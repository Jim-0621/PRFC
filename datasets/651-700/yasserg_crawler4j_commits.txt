File: crawler4j-examples/crawler4j-examples-base/src/test/java/edu/uci/ics/crawler4j/examples/localdata/Downloader.java
Patch:
@@ -41,13 +41,13 @@ public class Downloader {
     private final PageFetcher pageFetcher;
     private final CrawlConfig config = new CrawlConfig();
 
-    public Downloader() throws InstantiationException, IllegalAccessException {
+    public Downloader() throws Exception {
         config.setFollowRedirects(true);
         parser = new Parser(config);
         pageFetcher = new PageFetcher(config);
     }
 
-    public static void main(String[] args) throws InstantiationException, IllegalAccessException {
+    public static void main(String[] args) throws Exception {
         Downloader downloader = new Downloader();
         downloader.processUrl("https://en.wikipedia.org/wiki/Main_Page/");
         downloader.processUrl("https://api.ipify.org?format=html");

File: crawler4j/src/main/java/edu/uci/ics/crawler4j/crawler/Page.java
Patch:
@@ -158,9 +158,9 @@ protected byte[] toByteArray(HttpEntity entity, int maxBytes) throws IOException
      *
      * @param entity HttpEntity
      * @param maxBytes The maximum number of bytes to read
-     * @throws Exception when load fails
+     * @throws IOException when load fails
      */
-    public void load(HttpEntity entity, int maxBytes) throws Exception {
+    public void load(HttpEntity entity, int maxBytes) throws IOException {
 
         contentType = null;
         Header type = entity.getContentType();

File: crawler4j/src/test/java/edu/uci/ics/crawler4j/tests/URLCanonicalizerTest.java
Patch:
@@ -2,6 +2,7 @@
 
 import static org.junit.Assert.assertEquals;
 
+import java.io.UnsupportedEncodingException;
 import java.nio.charset.Charset;
 
 import org.junit.Test;
@@ -11,7 +12,7 @@
 public class URLCanonicalizerTest {
 
     @Test
-    public void testCanonizalier() {
+    public void testCanonizalier() throws UnsupportedEncodingException {
 
         assertEquals("http://www.example.com/display?category=foo%2Fbar%2Bbaz",
                      URLCanonicalizer.getCanonicalURL(

File: crawler4j/src/main/java/edu/uci/ics/crawler4j/crawler/CrawlConfig.java
Patch:
@@ -226,7 +226,6 @@ public DnsResolver getDnsResolver() {
 
     private DnsResolver dnsResolver = new SystemDefaultDnsResolver();
 
-
     private boolean haltOnError = false;
 
     private boolean allowSingleLevelDomain = false;
@@ -701,8 +700,8 @@ public boolean isHaltOnError() {
     public void setHaltOnError(boolean haltOnError) {
         this.haltOnError = haltOnError;
     }
-  
-    /**   
+
+    /**
      * Are single level domains (e.g. http://localhost) considered valid?
      *
      * @return

File: crawler4j/src/main/java/edu/uci/ics/crawler4j/parser/Parser.java
Patch:
@@ -39,7 +39,7 @@ public class Parser {
 
     private final HtmlParser htmlContentParser;
 
-    private Net net;
+    private final Net net;
 
     @Deprecated
     public Parser(CrawlConfig config) throws IllegalAccessException, InstantiationException {
@@ -58,7 +58,7 @@ public Parser(CrawlConfig config, HtmlParser htmlParser) {
     public Parser(CrawlConfig config, HtmlParser htmlParser, TLDList tldList) {
         this.config = config;
         this.htmlContentParser = htmlParser;
-        this.net = new Net(tldList);
+        this.net = new Net(config, tldList);
     }
 
     public void parse(Page page, String contextURL) throws NotAllowedContentException, ParseException {

File: crawler4j/src/main/java/edu/uci/ics/crawler4j/parser/Parser.java
Patch:
@@ -39,7 +39,7 @@ public class Parser {
 
     private final HtmlParser htmlContentParser;
 
-    private Net net;
+    private final Net net;
 
     @Deprecated
     public Parser(CrawlConfig config) throws IllegalAccessException, InstantiationException {
@@ -58,7 +58,7 @@ public Parser(CrawlConfig config, HtmlParser htmlParser) {
     public Parser(CrawlConfig config, HtmlParser htmlParser, TLDList tldList) {
         this.config = config;
         this.htmlContentParser = htmlParser;
-        this.net = new Net(tldList);
+        this.net = new Net(config, tldList);
     }
 
     public void parse(Page page, String contextURL)

File: crawler4j-examples/crawler4j-examples-base/src/test/java/edu/uci/ics/crawler4j/examples/basic/BasicCrawlController.java
Patch:
@@ -111,9 +111,9 @@ public static void main(String[] args) throws Exception {
      * URLs that are fetched and then the crawler starts following links
      * which are found in these pages
      */
-        controller.addSeed("http://www.ics.uci.edu/");
-        controller.addSeed("http://www.ics.uci.edu/~lopes/");
-        controller.addSeed("http://www.ics.uci.edu/~welling/");
+        controller.addSeed("https://www.ics.uci.edu/");
+        controller.addSeed("https://www.ics.uci.edu/~lopes/");
+        controller.addSeed("https://www.ics.uci.edu/~welling/");
 
     /*
      * Start the crawl. This is a blocking operation, meaning that your code

File: crawler4j-examples/crawler4j-examples-base/src/test/java/edu/uci/ics/crawler4j/examples/basic/BasicCrawler.java
Patch:
@@ -47,7 +47,7 @@ public boolean shouldVisit(Page referringPage, WebURL url) {
         }
 
         // Only accept the url if it is in the "www.ics.uci.edu" domain and protocol is "http".
-        return href.startsWith("http://www.ics.uci.edu/");
+        return href.startsWith("https://www.ics.uci.edu/");
     }
 
     /**

File: crawler4j-examples/crawler4j-examples-base/src/test/java/edu/uci/ics/crawler4j/examples/localdata/LocalDataCollectorController.java
Patch:
@@ -53,7 +53,7 @@ public static void main(String[] args) throws Exception {
         RobotstxtServer robotstxtServer = new RobotstxtServer(robotstxtConfig, pageFetcher);
         CrawlController controller = new CrawlController(config, pageFetcher, robotstxtServer);
 
-        controller.addSeed("http://www.ics.uci.edu/");
+        controller.addSeed("https://www.ics.uci.edu/");
         controller.start(LocalDataCollectorCrawler.class, numberOfCrawlers);
 
         List<Object> crawlersLocalData = controller.getCrawlersLocalData();

File: crawler4j-examples/crawler4j-examples-base/src/test/java/edu/uci/ics/crawler4j/examples/localdata/LocalDataCollectorCrawler.java
Patch:
@@ -45,7 +45,7 @@ public LocalDataCollectorCrawler() {
     @Override
     public boolean shouldVisit(Page referringPage, WebURL url) {
         String href = url.getURL().toLowerCase();
-        return !FILTERS.matcher(href).matches() && href.startsWith("http://www.ics.uci.edu/");
+        return !FILTERS.matcher(href).matches() && href.startsWith("https://www.ics.uci.edu/");
     }
 
     @Override

File: crawler4j-examples/crawler4j-examples-base/src/test/java/edu/uci/ics/crawler4j/examples/shutdown/BasicCrawler.java
Patch:
@@ -39,7 +39,7 @@ public class BasicCrawler extends WebCrawler {
         ".*(\\.(css|js|bmp|gif|jpe?g|png|tiff?|mid|mp2|mp3|mp4|wav|avi|mov|mpeg|ram|m4v|pdf" +
         "|rm|smil|wmv|swf|wma|zip|rar|gz))$");
 
-    private static final String DOMAIN = "http://www.ics.uci.edu/";
+    private static final String DOMAIN = "https://www.ics.uci.edu/";
 
     @Override
     public boolean shouldVisit(Page referringPage, WebURL url) {

File: crawler4j-examples/crawler4j-examples-base/src/test/java/edu/uci/ics/crawler4j/examples/shutdown/ControllerWithShutdown.java
Patch:
@@ -75,9 +75,9 @@ public static void main(String[] args) throws Exception {
      * URLs that are fetched and then the crawler starts following links
      * which are found in these pages
      */
-        controller.addSeed("http://www.ics.uci.edu/~welling/");
-        controller.addSeed("http://www.ics.uci.edu/~lopes/");
-        controller.addSeed("http://www.ics.uci.edu/");
+        controller.addSeed("https://www.ics.uci.edu/~welling/");
+        controller.addSeed("https://www.ics.uci.edu/~lopes/");
+        controller.addSeed("https://www.ics.uci.edu/");
 
     /*
      * Start the crawl. This is a blocking operation, meaning that your code

File: crawler4j-examples/crawler4j-examples-base/src/test/java/edu/uci/ics/crawler4j/examples/statushandler/StatusHandlerCrawlController.java
Patch:
@@ -106,9 +106,9 @@ public static void main(String[] args) throws Exception {
      * URLs that are fetched and then the crawler starts following links
      * which are found in these pages
      */
-        controller.addSeed("http://www.ics.uci.edu/~welling/");
-        controller.addSeed("http://www.ics.uci.edu/~lopes/");
-        controller.addSeed("http://www.ics.uci.edu/");
+        controller.addSeed("https://www.ics.uci.edu/~welling/");
+        controller.addSeed("https://www.ics.uci.edu/~lopes/");
+        controller.addSeed("https://www.ics.uci.edu/");
 
     /*
      * Start the crawl. This is a blocking operation, meaning that your code

File: crawler4j-examples/crawler4j-examples-base/src/test/java/edu/uci/ics/crawler4j/examples/statushandler/StatusHandlerCrawler.java
Patch:
@@ -45,7 +45,7 @@ public class StatusHandlerCrawler extends WebCrawler {
     @Override
     public boolean shouldVisit(Page referringPage, WebURL url) {
         String href = url.getURL().toLowerCase();
-        return !FILTERS.matcher(href).matches() && href.startsWith("http://www.ics.uci.edu/");
+        return !FILTERS.matcher(href).matches() && href.startsWith("https://www.ics.uci.edu/");
     }
 
     /**

File: crawler4j/src/main/java/edu/uci/ics/crawler4j/crawler/authentication/AuthInfo.java
Patch:
@@ -2,6 +2,7 @@
 
 import java.net.MalformedURLException;
 import java.net.URL;
+
 import javax.swing.text.html.FormSubmitEvent.MethodType;
 
 /**

File: crawler4j/src/main/java/edu/uci/ics/crawler4j/fetcher/PageFetcher.java
Patch:
@@ -27,6 +27,7 @@
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
+
 import javax.net.ssl.SSLContext;
 
 import org.apache.http.Header;

File: crawler4j/src/test/java/edu/uci/ics/crawler4j/tests/fetcher/PageFetcherHtmlOnly.java
Patch:
@@ -2,8 +2,10 @@
 
 import java.io.IOException;
 import java.util.Date;
+
 import org.apache.http.HttpResponse;
 import org.apache.http.client.methods.HttpHead;
+
 import edu.uci.ics.crawler4j.crawler.CrawlConfig;
 import edu.uci.ics.crawler4j.crawler.exceptions.PageBiggerThanMaxSizeException;
 import edu.uci.ics.crawler4j.fetcher.PageFetchResult;

File: crawler4j/src/test/java/edu/uci/ics/crawler4j/tests/fetcher/PageFetcherHtmlTest.java
Patch:
@@ -1,11 +1,14 @@
 package edu.uci.ics.crawler4j.tests.fetcher;
 
 import java.io.IOException;
+
 import org.junit.Rule;
 import org.junit.Test;
+
 import com.github.tomakehurst.wiremock.client.WireMock;
 import com.github.tomakehurst.wiremock.core.WireMockConfiguration;
 import com.github.tomakehurst.wiremock.junit.WireMockRule;
+
 import edu.uci.ics.crawler4j.crawler.CrawlConfig;
 import edu.uci.ics.crawler4j.crawler.Page;
 import edu.uci.ics.crawler4j.crawler.exceptions.PageBiggerThanMaxSizeException;

File: crawler4j/src/main/java/edu/uci/ics/crawler4j/crawler/WebCrawler.java
Patch:
@@ -420,6 +420,7 @@ private void processPage(WebURL curURL) {
                         }
 
                         WebURL webURL = new WebURL();
+                        webURL.setTldList(myController.getTldList());
                         webURL.setURL(movedToUrl);
                         webURL.setParentDocid(curURL.getParentDocid());
                         webURL.setParentUrl(curURL.getParentUrl());

File: crawler4j-examples/crawler4j-examples-base/src/test/java/edu/uci/ics/crawler4j/examples/basic/BasicCrawlController.java
Patch:
@@ -119,9 +119,9 @@ public static void main(String[] args) throws Exception {
      * URLs that are fetched and then the crawler starts following links
      * which are found in these pages
      */
-        controller.addSeed("http://www.ics.uci.edu/");
-        controller.addSeed("http://www.ics.uci.edu/~lopes/");
-        controller.addSeed("http://www.ics.uci.edu/~welling/");
+        controller.addSeed("https://www.ics.uci.edu/");
+        controller.addSeed("https://www.ics.uci.edu/~lopes/");
+        controller.addSeed("https://www.ics.uci.edu/~welling/");
 
     /*
      * Start the crawl. This is a blocking operation, meaning that your code

File: crawler4j-examples/crawler4j-examples-base/src/test/java/edu/uci/ics/crawler4j/examples/basic/BasicCrawler.java
Patch:
@@ -47,7 +47,7 @@ public boolean shouldVisit(Page referringPage, WebURL url) {
         }
 
         // Only accept the url if it is in the "www.ics.uci.edu" domain and protocol is "http".
-        return href.startsWith("http://www.ics.uci.edu/");
+        return href.startsWith("https://www.ics.uci.edu/");
     }
 
     /**

File: crawler4j-examples/crawler4j-examples-base/src/test/java/edu/uci/ics/crawler4j/examples/localdata/LocalDataCollectorController.java
Patch:
@@ -53,7 +53,7 @@ public static void main(String[] args) throws Exception {
         RobotstxtServer robotstxtServer = new RobotstxtServer(robotstxtConfig, pageFetcher);
         CrawlController controller = new CrawlController(config, pageFetcher, robotstxtServer);
 
-        controller.addSeed("http://www.ics.uci.edu/");
+        controller.addSeed("https://www.ics.uci.edu/");
         controller.start(LocalDataCollectorCrawler.class, numberOfCrawlers);
 
         List<Object> crawlersLocalData = controller.getCrawlersLocalData();

File: crawler4j-examples/crawler4j-examples-base/src/test/java/edu/uci/ics/crawler4j/examples/localdata/LocalDataCollectorCrawler.java
Patch:
@@ -45,7 +45,7 @@ public LocalDataCollectorCrawler() {
     @Override
     public boolean shouldVisit(Page referringPage, WebURL url) {
         String href = url.getURL().toLowerCase();
-        return !FILTERS.matcher(href).matches() && href.startsWith("http://www.ics.uci.edu/");
+        return !FILTERS.matcher(href).matches() && href.startsWith("https://www.ics.uci.edu/");
     }
 
     @Override

File: crawler4j-examples/crawler4j-examples-base/src/test/java/edu/uci/ics/crawler4j/examples/shutdown/BasicCrawler.java
Patch:
@@ -39,7 +39,7 @@ public class BasicCrawler extends WebCrawler {
         ".*(\\.(css|js|bmp|gif|jpe?g|png|tiff?|mid|mp2|mp3|mp4|wav|avi|mov|mpeg|ram|m4v|pdf" +
         "|rm|smil|wmv|swf|wma|zip|rar|gz))$");
 
-    private static final String DOMAIN = "http://www.ics.uci.edu/";
+    private static final String DOMAIN = "https://www.ics.uci.edu/";
 
     @Override
     public boolean shouldVisit(Page referringPage, WebURL url) {

File: crawler4j-examples/crawler4j-examples-base/src/test/java/edu/uci/ics/crawler4j/examples/shutdown/ControllerWithShutdown.java
Patch:
@@ -75,9 +75,9 @@ public static void main(String[] args) throws Exception {
      * URLs that are fetched and then the crawler starts following links
      * which are found in these pages
      */
-        controller.addSeed("http://www.ics.uci.edu/~welling/");
-        controller.addSeed("http://www.ics.uci.edu/~lopes/");
-        controller.addSeed("http://www.ics.uci.edu/");
+        controller.addSeed("https://www.ics.uci.edu/~welling/");
+        controller.addSeed("https://www.ics.uci.edu/~lopes/");
+        controller.addSeed("https://www.ics.uci.edu/");
 
     /*
      * Start the crawl. This is a blocking operation, meaning that your code

File: crawler4j-examples/crawler4j-examples-base/src/test/java/edu/uci/ics/crawler4j/examples/statushandler/StatusHandlerCrawlController.java
Patch:
@@ -106,9 +106,9 @@ public static void main(String[] args) throws Exception {
      * URLs that are fetched and then the crawler starts following links
      * which are found in these pages
      */
-        controller.addSeed("http://www.ics.uci.edu/~welling/");
-        controller.addSeed("http://www.ics.uci.edu/~lopes/");
-        controller.addSeed("http://www.ics.uci.edu/");
+        controller.addSeed("https://www.ics.uci.edu/~welling/");
+        controller.addSeed("https://www.ics.uci.edu/~lopes/");
+        controller.addSeed("https://www.ics.uci.edu/");
 
     /*
      * Start the crawl. This is a blocking operation, meaning that your code

File: crawler4j-examples/crawler4j-examples-base/src/test/java/edu/uci/ics/crawler4j/examples/statushandler/StatusHandlerCrawler.java
Patch:
@@ -45,7 +45,7 @@ public class StatusHandlerCrawler extends WebCrawler {
     @Override
     public boolean shouldVisit(Page referringPage, WebURL url) {
         String href = url.getURL().toLowerCase();
-        return !FILTERS.matcher(href).matches() && href.startsWith("http://www.ics.uci.edu/");
+        return !FILTERS.matcher(href).matches() && href.startsWith("https://www.ics.uci.edu/");
     }
 
     /**

File: crawler4j/src/main/java/edu/uci/ics/crawler4j/crawler/CrawlConfig.java
Patch:
@@ -28,6 +28,7 @@
 import org.apache.http.conn.DnsResolver;
 import org.apache.http.impl.conn.SystemDefaultDnsResolver;
 import org.apache.http.message.BasicHeader;
+
 import edu.uci.ics.crawler4j.crawler.authentication.AuthInfo;
 
 public class CrawlConfig {

File: crawler4j/src/main/java/edu/uci/ics/crawler4j/crawler/CrawlController.java
Patch:
@@ -26,8 +26,10 @@
 
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
+
 import com.sleepycat.je.Environment;
 import com.sleepycat.je.EnvironmentConfig;
+
 import edu.uci.ics.crawler4j.fetcher.PageFetcher;
 import edu.uci.ics.crawler4j.frontier.DocIDServer;
 import edu.uci.ics.crawler4j.frontier.Frontier;

File: crawler4j/src/main/java/edu/uci/ics/crawler4j/crawler/WebCrawler.java
Patch:
@@ -22,10 +22,12 @@
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Locale;
+
 import org.apache.http.HttpStatus;
 import org.apache.http.impl.EnglishReasonPhraseCatalog;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
+
 import edu.uci.ics.crawler4j.crawler.exceptions.ContentFetchException;
 import edu.uci.ics.crawler4j.crawler.exceptions.PageBiggerThanMaxSizeException;
 import edu.uci.ics.crawler4j.crawler.exceptions.ParseException;

File: crawler4j/src/main/java/edu/uci/ics/crawler4j/crawler/authentication/AuthInfo.java
Patch:
@@ -2,6 +2,7 @@
 
 import java.net.MalformedURLException;
 import java.net.URL;
+
 import javax.swing.text.html.FormSubmitEvent.MethodType;
 
 /**

File: crawler4j/src/main/java/edu/uci/ics/crawler4j/fetcher/PageFetcher.java
Patch:
@@ -30,6 +30,7 @@
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
+
 import javax.net.ssl.SSLContext;
 
 import org.apache.http.Header;

File: crawler4j/src/main/java/edu/uci/ics/crawler4j/parser/Parser.java
Patch:
@@ -20,6 +20,7 @@
 import org.apache.tika.language.LanguageIdentifier;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
+
 import edu.uci.ics.crawler4j.crawler.CrawlConfig;
 import edu.uci.ics.crawler4j.crawler.Page;
 import edu.uci.ics.crawler4j.crawler.exceptions.ParseException;

File: crawler4j/src/main/java/edu/uci/ics/crawler4j/parser/TikaHtmlParser.java
Patch:
@@ -7,13 +7,15 @@
 import java.nio.charset.StandardCharsets;
 import java.util.HashSet;
 import java.util.Set;
+
 import org.apache.tika.metadata.DublinCore;
 import org.apache.tika.metadata.Metadata;
 import org.apache.tika.parser.ParseContext;
 import org.apache.tika.parser.html.HtmlMapper;
 import org.apache.tika.parser.html.HtmlParser;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
+
 import edu.uci.ics.crawler4j.crawler.CrawlConfig;
 import edu.uci.ics.crawler4j.crawler.Page;
 import edu.uci.ics.crawler4j.crawler.exceptions.ParseException;

File: crawler4j/src/main/java/edu/uci/ics/crawler4j/url/TLDList.java
Patch:
@@ -12,6 +12,7 @@
 import java.util.Map;
 import java.util.Set;
 import java.util.function.Supplier;
+
 import org.apache.commons.io.FileUtils;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;

File: crawler4j/src/main/java/edu/uci/ics/crawler4j/url/WebURL.java
Patch:
@@ -18,7 +18,6 @@
 package edu.uci.ics.crawler4j.url;
 
 import java.io.Serializable;
-
 import java.util.Map;
 
 import com.sleepycat.persist.model.Entity;

File: crawler4j/src/test/java/edu/uci/ics/crawler4j/tests/fetcher/PageFetcherHtmlOnly.java
Patch:
@@ -5,8 +5,10 @@
 import java.security.KeyStoreException;
 import java.security.NoSuchAlgorithmException;
 import java.util.Date;
+
 import org.apache.http.HttpResponse;
 import org.apache.http.client.methods.HttpHead;
+
 import edu.uci.ics.crawler4j.crawler.CrawlConfig;
 import edu.uci.ics.crawler4j.crawler.exceptions.PageBiggerThanMaxSizeException;
 import edu.uci.ics.crawler4j.fetcher.PageFetchResult;

File: crawler4j/src/test/java/edu/uci/ics/crawler4j/tests/fetcher/PageFetcherHtmlTest.java
Patch:
@@ -1,10 +1,13 @@
 package edu.uci.ics.crawler4j.tests.fetcher;
 
+
 import org.junit.Rule;
 import org.junit.Test;
+
 import com.github.tomakehurst.wiremock.client.WireMock;
 import com.github.tomakehurst.wiremock.core.WireMockConfiguration;
 import com.github.tomakehurst.wiremock.junit.WireMockRule;
+
 import edu.uci.ics.crawler4j.crawler.CrawlConfig;
 import edu.uci.ics.crawler4j.crawler.Page;
 import edu.uci.ics.crawler4j.fetcher.PageFetcher;

File: crawler4j-examples/crawler4j-examples-base/src/test/java/edu/uci/ics/crawler4j/examples/localdata/Downloader.java
Patch:
@@ -41,13 +41,13 @@ public class Downloader {
     private final PageFetcher pageFetcher;
     private final CrawlConfig config = new CrawlConfig();
 
-    public Downloader() throws InstantiationException, IllegalAccessException {
+    public Downloader() throws Exception {
         config.setFollowRedirects(true);
         parser = new Parser(config);
         pageFetcher = new PageFetcher(config);
     }
 
-    public static void main(String[] args) throws InstantiationException, IllegalAccessException {
+    public static void main(String[] args) throws Exception {
         Downloader downloader = new Downloader();
         downloader.processUrl("https://en.wikipedia.org/wiki/Main_Page/");
         downloader.processUrl("https://api.ipify.org?format=html");

File: crawler4j/src/main/java/edu/uci/ics/crawler4j/crawler/Page.java
Patch:
@@ -158,9 +158,9 @@ protected byte[] toByteArray(HttpEntity entity, int maxBytes) throws IOException
      *
      * @param entity HttpEntity
      * @param maxBytes The maximum number of bytes to read
-     * @throws Exception when load fails
+     * @throws IOException when load fails
      */
-    public void load(HttpEntity entity, int maxBytes) throws Exception {
+    public void load(HttpEntity entity, int maxBytes) throws IOException {
 
         contentType = null;
         Header type = entity.getContentType();

File: crawler4j/src/test/java/edu/uci/ics/crawler4j/tests/URLCanonicalizerTest.java
Patch:
@@ -2,6 +2,7 @@
 
 import static org.junit.Assert.assertEquals;
 
+import java.io.UnsupportedEncodingException;
 import java.nio.charset.Charset;
 
 import org.junit.Test;
@@ -11,7 +12,7 @@
 public class URLCanonicalizerTest {
 
     @Test
-    public void testCanonizalier() {
+    public void testCanonizalier() throws UnsupportedEncodingException {
 
         assertEquals("http://www.example.com/display?category=foo%2Fbar%2Bbaz",
                      URLCanonicalizer.getCanonicalURL(

File: crawler4j/src/main/java/edu/uci/ics/crawler4j/crawler/CrawlConfig.java
Patch:
@@ -28,7 +28,6 @@
 import org.apache.http.conn.DnsResolver;
 import org.apache.http.impl.conn.SystemDefaultDnsResolver;
 import org.apache.http.message.BasicHeader;
-
 import edu.uci.ics.crawler4j.crawler.authentication.AuthInfo;
 
 public class CrawlConfig {

File: crawler4j/src/main/java/edu/uci/ics/crawler4j/crawler/WebCrawler.java
Patch:
@@ -21,12 +21,10 @@
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Locale;
-
 import org.apache.http.HttpStatus;
 import org.apache.http.impl.EnglishReasonPhraseCatalog;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
-
 import edu.uci.ics.crawler4j.crawler.exceptions.ContentFetchException;
 import edu.uci.ics.crawler4j.crawler.exceptions.PageBiggerThanMaxSizeException;
 import edu.uci.ics.crawler4j.crawler.exceptions.ParseException;
@@ -118,7 +116,7 @@ public void init(int id, CrawlController crawlController)
         this.robotstxtServer = crawlController.getRobotstxtServer();
         this.docIdServer = crawlController.getDocIdServer();
         this.frontier = crawlController.getFrontier();
-        this.parser = new Parser(crawlController.getConfig());
+        this.parser = crawlController.getParser();
         this.myController = crawlController;
         this.isWaitingForNewURLs = false;
     }

File: crawler4j/src/main/java/edu/uci/ics/crawler4j/crawler/WebCrawler.java
Patch:
@@ -21,12 +21,10 @@
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Locale;
-
 import org.apache.http.HttpStatus;
 import org.apache.http.impl.EnglishReasonPhraseCatalog;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
-
 import edu.uci.ics.crawler4j.crawler.exceptions.ContentFetchException;
 import edu.uci.ics.crawler4j.crawler.exceptions.PageBiggerThanMaxSizeException;
 import edu.uci.ics.crawler4j.crawler.exceptions.ParseException;

File: crawler4j/src/main/java/edu/uci/ics/crawler4j/parser/HtmlParseData.java
Patch:
@@ -17,11 +17,11 @@
 
 package edu.uci.ics.crawler4j.parser;
 
-import edu.uci.ics.crawler4j.url.WebURL;
-
 import java.util.Map;
 import java.util.Set;
 
+import edu.uci.ics.crawler4j.url.WebURL;
+
 public class HtmlParseData implements ParseData {
 
     private String html;

File: crawler4j/src/main/java/edu/uci/ics/crawler4j/parser/Parser.java
Patch:
@@ -17,14 +17,14 @@
 
 package edu.uci.ics.crawler4j.parser;
 
+import org.apache.tika.language.LanguageIdentifier;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 import edu.uci.ics.crawler4j.crawler.CrawlConfig;
 import edu.uci.ics.crawler4j.crawler.Page;
 import edu.uci.ics.crawler4j.crawler.exceptions.ParseException;
 import edu.uci.ics.crawler4j.util.Net;
 import edu.uci.ics.crawler4j.util.Util;
-import org.apache.tika.language.LanguageIdentifier;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
 
 /**
  * @author Yasser Ganjisaffar

File: crawler4j/src/main/java/edu/uci/ics/crawler4j/url/URLCanonicalizer.java
Patch:
@@ -64,7 +64,9 @@ public static String getCanonicalURL(String href, String context, Charset charse
        * ".", and no segments equal to ".." that are preceded by a segment
        * not equal to "..".
        */
-            path = new URI(path.replace("\\", "/")).normalize().toString();
+            path = new URI(path.replace("\\", "/")
+                    .replace(String.valueOf((char)12288), "%E3%80%80")
+                    .replace(String.valueOf((char)32), "%20")).normalize().toString();
 
             int idx = path.indexOf("//");
             while (idx >= 0) {

File: crawler4j/src/main/java/edu/uci/ics/crawler4j/url/URLCanonicalizer.java
Patch:
@@ -64,7 +64,9 @@ public static String getCanonicalURL(String href, String context, Charset charse
        * ".", and no segments equal to ".." that are preceded by a segment
        * not equal to "..".
        */
-            path = new URI(path.replace("\\", "/")).normalize().toString();
+            path = new URI(path.replace("\\", "/")
+                    .replace(String.valueOf((char)12288), "%E3%80%80")
+                    .replace(String.valueOf((char)32), "%20")).normalize().toString();
 
             int idx = path.indexOf("//");
             while (idx >= 0) {

File: crawler4j/src/main/java/edu/uci/ics/crawler4j/crawler/Page.java
Patch:
@@ -91,7 +91,7 @@ public class Page {
     /**
      * Headers which were present in the response of the fetch request
      */
-    protected Header[] fetchResponseHeaders;
+    protected Header[] fetchResponseHeaders = new Header[0];
 
     /**
      * The parsed data populated by parsers

File: crawler4j/src/main/java/edu/uci/ics/crawler4j/robotstxt/HostDirectives.java
Patch:
@@ -27,7 +27,7 @@
 public class HostDirectives {
     // If we fetched the directives for this host more than
     // 24 hours, we have to re-fetch it.
-    private static final long EXPIRATION_DELAY = TimeUnit.MILLISECONDS.convert(1, TimeUnit.DAYS);;
+    private static final long EXPIRATION_DELAY = TimeUnit.MILLISECONDS.convert(1, TimeUnit.DAYS);
 
     public static final int ALLOWED = 1;
     public static final int DISALLOWED = 2;

File: crawler4j/src/main/java/edu/uci/ics/crawler4j/robotstxt/HostDirectives.java
Patch:
@@ -19,14 +19,15 @@
 
 import java.util.Set;
 import java.util.TreeSet;
+import java.util.concurrent.TimeUnit;
 
 /**
  * @author Yasser Ganjisaffar
  */
 public class HostDirectives {
     // If we fetched the directives for this host more than
     // 24 hours, we have to re-fetch it.
-    private static final long EXPIRATION_DELAY = 24 * 60 * 1000L;
+    private static final long EXPIRATION_DELAY = TimeUnit.MILLISECONDS.convert(1, TimeUnit.DAYS);;
 
     public static final int ALLOWED = 1;
     public static final int DISALLOWED = 2;

File: src/main/java/edu/uci/ics/crawler4j/crawler/Page.java
Patch:
@@ -25,11 +25,11 @@
 import org.apache.http.HttpEntity;
 import org.apache.http.entity.ContentType;
 import org.apache.http.util.ByteArrayBuffer;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 import edu.uci.ics.crawler4j.parser.ParseData;
 import edu.uci.ics.crawler4j.url.WebURL;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
 
 /**
  * This class contains the data for a fetched and parsed page.
@@ -38,7 +38,7 @@
  */
 public class Page {
 
-    private final static Logger logger = LoggerFactory.getLogger(Page.class);
+    protected final Logger logger = LoggerFactory.getLogger(Page.class);
 
     /**
      * The URL of this page.

File: src/main/java/edu/uci/ics/crawler4j/crawler/Page.java
Patch:
@@ -25,11 +25,11 @@
 import org.apache.http.HttpEntity;
 import org.apache.http.entity.ContentType;
 import org.apache.http.util.ByteArrayBuffer;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 import edu.uci.ics.crawler4j.parser.ParseData;
 import edu.uci.ics.crawler4j.url.WebURL;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
 
 /**
  * This class contains the data for a fetched and parsed page.
@@ -38,7 +38,7 @@
  */
 public class Page {
 
-    private final static Logger logger = LoggerFactory.getLogger(Page.class);
+    protected final Logger logger = LoggerFactory.getLogger(Page.class);
 
     /**
      * The URL of this page.

File: crawler4j/src/main/java/edu/uci/ics/crawler4j/robotstxt/HostDirectives.java
Patch:
@@ -27,7 +27,7 @@
 public class HostDirectives {
     // If we fetched the directives for this host more than
     // 24 hours, we have to re-fetch it.
-    private static final long EXPIRATION_DELAY = TimeUnit.MILLISECONDS.convert(1, TimeUnit.DAYS);;
+    private static final long EXPIRATION_DELAY = TimeUnit.MILLISECONDS.convert(1, TimeUnit.DAYS);
 
     public static final int ALLOWED = 1;
     public static final int DISALLOWED = 2;

File: crawler4j/src/main/java/edu/uci/ics/crawler4j/robotstxt/HostDirectives.java
Patch:
@@ -19,14 +19,15 @@
 
 import java.util.Set;
 import java.util.TreeSet;
+import java.util.concurrent.TimeUnit;
 
 /**
  * @author Yasser Ganjisaffar
  */
 public class HostDirectives {
     // If we fetched the directives for this host more than
     // 24 hours, we have to re-fetch it.
-    private static final long EXPIRATION_DELAY = 24 * 60 * 1000L;
+    private static final long EXPIRATION_DELAY = TimeUnit.MILLISECONDS.convert(1, TimeUnit.DAYS);;
 
     public static final int ALLOWED = 1;
     public static final int DISALLOWED = 2;

File: src/main/java/edu/uci/ics/crawler4j/crawler/CrawlConfig.java
Patch:
@@ -633,6 +633,4 @@ public String toString() {
         sb.append("Respect noindex: " + isRespectNoIndex() + "\n");
         return sb.toString();
     }
-
-
 }

File: src/main/java/edu/uci/ics/crawler4j/crawler/Page.java
Patch:
@@ -25,11 +25,11 @@
 import org.apache.http.HttpEntity;
 import org.apache.http.entity.ContentType;
 import org.apache.http.util.ByteArrayBuffer;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 import edu.uci.ics.crawler4j.parser.ParseData;
 import edu.uci.ics.crawler4j.url.WebURL;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
 
 /**
  * This class contains the data for a fetched and parsed page.
@@ -38,7 +38,7 @@
  */
 public class Page {
 
-    private final static Logger logger = LoggerFactory.getLogger(Page.class);
+    protected final Logger logger = LoggerFactory.getLogger(Page.class);
 
     /**
      * The URL of this page.

File: src/main/java/edu/uci/ics/crawler4j/fetcher/PageFetcher.java
Patch:
@@ -48,6 +48,7 @@
 import org.apache.http.config.RegistryBuilder;
 import org.apache.http.conn.socket.ConnectionSocketFactory;
 import org.apache.http.conn.socket.PlainConnectionSocketFactory;
+import org.apache.http.conn.ssl.NoopHostnameVerifier;
 import org.apache.http.conn.ssl.SSLConnectionSocketFactory;
 import org.apache.http.conn.ssl.TrustStrategy;
 import org.apache.http.impl.client.BasicCredentialsProvider;
@@ -105,9 +106,8 @@ public boolean isTrusted(final X509Certificate[] chain, String authType) {
                             return true;
                         }
                     }).build();
-                SSLConnectionSocketFactory sslsf =
-                    new SSLConnectionSocketFactory(
-                        sslContext, SSLConnectionSocketFactory.ALLOW_ALL_HOSTNAME_VERIFIER);
+                SSLConnectionSocketFactory sslsf = new SSLConnectionSocketFactory(
+                    sslContext, NoopHostnameVerifier.INSTANCE);
                 connRegistryBuilder.register("https", sslsf);
             } catch (Exception e) {
                 logger.warn("Exception thrown while trying to register https");

File: src/main/java/edu/uci/ics/crawler4j/fetcher/PageFetcher.java
Patch:
@@ -48,6 +48,7 @@
 import org.apache.http.config.RegistryBuilder;
 import org.apache.http.conn.socket.ConnectionSocketFactory;
 import org.apache.http.conn.socket.PlainConnectionSocketFactory;
+import org.apache.http.conn.ssl.NoopHostnameVerifier;
 import org.apache.http.conn.ssl.SSLConnectionSocketFactory;
 import org.apache.http.conn.ssl.TrustStrategy;
 import org.apache.http.impl.client.BasicCredentialsProvider;
@@ -105,9 +106,8 @@ public boolean isTrusted(final X509Certificate[] chain, String authType) {
                             return true;
                         }
                     }).build();
-                SSLConnectionSocketFactory sslsf =
-                    new SSLConnectionSocketFactory(
-                        sslContext, SSLConnectionSocketFactory.ALLOW_ALL_HOSTNAME_VERIFIER);
+                SSLConnectionSocketFactory sslsf = new SSLConnectionSocketFactory(
+                    sslContext, NoopHostnameVerifier.INSTANCE);
                 connRegistryBuilder.register("https", sslsf);
             } catch (Exception e) {
                 logger.warn("Exception thrown while trying to register https");

File: src/main/java/edu/uci/ics/crawler4j/crawler/Page.java
Patch:
@@ -92,7 +92,6 @@ public class Page {
      */
     protected ParseData parseData;
 
-
     public Page(WebURL url) {
         this.url = url;
     }

File: src/main/java/edu/uci/ics/crawler4j/fetcher/PageFetcher.java
Patch:
@@ -105,8 +105,9 @@ public boolean isTrusted(final X509Certificate[] chain, String authType) {
                             return true;
                         }
                     }).build();
-                SSLConnectionSocketFactory sslsf = new SSLConnectionSocketFactory(sslContext,
-                                                                                  SSLConnectionSocketFactory.ALLOW_ALL_HOSTNAME_VERIFIER);
+                SSLConnectionSocketFactory sslsf =
+                    new SSLConnectionSocketFactory(
+                        sslContext, SSLConnectionSocketFactory.ALLOW_ALL_HOSTNAME_VERIFIER);
                 connRegistryBuilder.register("https", sslsf);
             } catch (Exception e) {
                 logger.warn("Exception thrown while trying to register https");

File: src/main/java/edu/uci/ics/crawler4j/frontier/InProcessPagesDB.java
Patch:
@@ -17,7 +17,6 @@
 
 package edu.uci.ics.crawler4j.frontier;
 
-
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 

File: src/main/java/edu/uci/ics/crawler4j/robotstxt/RobotstxtParser.java
Patch:
@@ -22,8 +22,6 @@
 /**
  * @author Yasser Ganjisaffar
  */
-
-
 public class RobotstxtParser {
 
     private static final String PATTERNS_USERAGENT = "(?i)^User-agent:.*";

File: src/main/java/edu/uci/ics/crawler4j/url/TLDList.java
Patch:
@@ -95,8 +95,8 @@ public static TLDList getInstance() {
     }
 
     /**
-     * If {@code online} is set to true, the list of TLD files will be downloaded and refreshed, otherwise the one
-     * cached in src/main/resources/tld-names.txt will be used.
+     * If {@code online} is set to true, the list of TLD files will be downloaded and refreshed,
+     * otherwise the one cached in src/main/resources/tld-names.txt will be used.
      */
     public static void setUseOnline(boolean online) {
         onlineUpdate = online;

File: src/main/java/edu/uci/ics/crawler4j/url/WebURL.java
Patch:
@@ -45,7 +45,6 @@ public class WebURL implements Serializable {
     private byte priority;
     private String tag;
 
-
     /**
      * @return unique document id assigned to this Url.
      */

File: src/main/java/edu/uci/ics/crawler4j/util/Util.java
Patch:
@@ -17,7 +17,6 @@
 
 package edu.uci.ics.crawler4j.util;
 
-
 /**
  * @author Yasser Ganjisaffar
  */

File: src/test/java/edu/uci/ics/crawler4j/tests/WebURLTest.java
Patch:
@@ -4,7 +4,6 @@
 
 import edu.uci.ics.crawler4j.url.WebURL;
 
-
 /**
  * Created by Avi on 8/19/2014.
  *

File: src/main/java/edu/uci/ics/crawler4j/fetcher/PageFetcher.java
Patch:
@@ -86,7 +86,7 @@ public PageFetcher(CrawlConfig config) {
     super(config);
 
     RequestConfig requestConfig =
-        RequestConfig.custom().setExpectContinueEnabled(false).setCookieSpec(CookieSpecs.DEFAULT)
+        RequestConfig.custom().setExpectContinueEnabled(false).setCookieSpec(CookieSpecs.STANDARD)
                      .setRedirectsEnabled(false).setSocketTimeout(config.getSocketTimeout())
                      .setConnectTimeout(config.getConnectionTimeout()).build();
 

File: src/main/java/edu/uci/ics/crawler4j/crawler/WebCrawler.java
Patch:
@@ -369,7 +369,8 @@ private void processPage(WebURL curURL) {
           String description = EnglishReasonPhraseCatalog.INSTANCE
               .getReason(fetchResult.getStatusCode(), Locale.ENGLISH); // Finds the status reason for all known statuses
           String contentType =
-              fetchResult.getEntity() == null ? "" : fetchResult.getEntity().getContentType().getValue();
+              fetchResult.getEntity() == null ? "" : 
+            	  fetchResult.getEntity().getContentType() == null ? "" : fetchResult.getEntity().getContentType().getValue();
           onUnexpectedStatusCode(curURL.getURL(), fetchResult.getStatusCode(), contentType, description);
         }
 

File: src/main/java/edu/uci/ics/crawler4j/crawler/WebCrawler.java
Patch:
@@ -369,7 +369,8 @@ private void processPage(WebURL curURL) {
           String description = EnglishReasonPhraseCatalog.INSTANCE
               .getReason(fetchResult.getStatusCode(), Locale.ENGLISH); // Finds the status reason for all known statuses
           String contentType =
-              fetchResult.getEntity() == null ? "" : fetchResult.getEntity().getContentType().getValue();
+              fetchResult.getEntity() == null ? "" : 
+            	  fetchResult.getEntity().getContentType() == null ? "" : fetchResult.getEntity().getContentType().getValue();
           onUnexpectedStatusCode(curURL.getURL(), fetchResult.getStatusCode(), contentType, description);
         }
 

File: src/main/java/edu/uci/ics/crawler4j/fetcher/PageFetcher.java
Patch:
@@ -86,7 +86,7 @@ public PageFetcher(CrawlConfig config) {
     super(config);
 
     RequestConfig requestConfig =
-        RequestConfig.custom().setExpectContinueEnabled(false).setCookieSpec(CookieSpecs.DEFAULT)
+        RequestConfig.custom().setExpectContinueEnabled(false).setCookieSpec(CookieSpecs.STANDARD)
                      .setRedirectsEnabled(false).setSocketTimeout(config.getSocketTimeout())
                      .setConnectTimeout(config.getConnectionTimeout()).build();
 

File: src/main/java/edu/uci/ics/crawler4j/robotstxt/RobotstxtParser.java
Patch:
@@ -59,7 +59,7 @@ public static HostDirectives parse(String content, String myUserAgent) {
 
       if (line.matches(PATTERNS_USERAGENT)) {
         String ua = line.substring(PATTERNS_USERAGENT_LENGTH).trim().toLowerCase();
-        inMatchingUserAgent = "*".equals(ua) || ua.contains(myUserAgent);
+        inMatchingUserAgent = "*".equals(ua) || ua.contains(myUserAgent.toLowerCase());
       } else if (line.matches(PATTERNS_DISALLOW)) {
         if (!inMatchingUserAgent) {
           continue;

File: src/main/java/edu/uci/ics/crawler4j/parser/HtmlContentHandler.java
Patch:
@@ -124,14 +124,14 @@ public void startElement(String uri, String localName, String qName, Attributes
           int pos = content.toLowerCase().indexOf("url=");
           if (pos != -1) {
             metaRefresh = content.substring(pos + 4);
+            addToOutgoingUrls(metaRefresh, localName);
           }
-          addToOutgoingUrls(metaRefresh, localName);
         }
 
         // http-equiv="location" content="http://foo.bar/..."
         if ("location".equals(equiv) && (metaLocation == null)) {
           metaLocation = content;
-          addToOutgoingUrls(metaRefresh, localName);
+          addToOutgoingUrls(metaLocation, localName);
         }
       }
     } else if (element == Element.BODY) {
@@ -194,4 +194,4 @@ public String getBaseUrl() {
   public Map<String, String> getMetaTags() {
     return metaTags;
   }
-}
\ No newline at end of file
+}

File: src/main/java/edu/uci/ics/crawler4j/url/TLDList.java
Patch:
@@ -21,7 +21,7 @@
 public class TLDList {
 
   private static final String TLD_NAMES_ONLINE_URL = "https://publicsuffix.org/list/effective_tld_names.dat";
-  private static final String TLD_NAMES_TXT_FILENAME = "/tld-names.txt";
+  private static final String TLD_NAMES_TXT_FILENAME = "tld-names.txt";
   private static final Logger logger = LoggerFactory.getLogger(TLDList.class);
 
   private static boolean onlineUpdate = false;
@@ -101,4 +101,4 @@ public static void setUseOnline(boolean online) {
   public boolean contains(String str) {
     return tldSet.contains(str);
   }
-}
\ No newline at end of file
+}

File: src/main/java/edu/uci/ics/crawler4j/url/TLDList.java
Patch:
@@ -21,7 +21,7 @@
 public class TLDList {
 
   private static final String TLD_NAMES_ONLINE_URL = "https://publicsuffix.org/list/effective_tld_names.dat";
-  private static final String TLD_NAMES_TXT_FILENAME = "/tld-names.txt";
+  private static final String TLD_NAMES_TXT_FILENAME = "tld-names.txt";
   private static final Logger logger = LoggerFactory.getLogger(TLDList.class);
 
   private static boolean onlineUpdate = false;
@@ -101,4 +101,4 @@ public static void setUseOnline(boolean online) {
   public boolean contains(String str) {
     return tldSet.contains(str);
   }
-}
\ No newline at end of file
+}

File: src/main/java/edu/uci/ics/crawler4j/parser/HtmlContentHandler.java
Patch:
@@ -124,14 +124,14 @@ public void startElement(String uri, String localName, String qName, Attributes
           int pos = content.toLowerCase().indexOf("url=");
           if (pos != -1) {
             metaRefresh = content.substring(pos + 4);
+            addToOutgoingUrls(metaRefresh, localName);
           }
-          addToOutgoingUrls(metaRefresh, localName);
         }
 
         // http-equiv="location" content="http://foo.bar/..."
         if ("location".equals(equiv) && (metaLocation == null)) {
           metaLocation = content;
-          addToOutgoingUrls(metaRefresh, localName);
+          addToOutgoingUrls(metaLocation, localName);
         }
       }
     } else if (element == Element.BODY) {
@@ -194,4 +194,4 @@ public String getBaseUrl() {
   public Map<String, String> getMetaTags() {
     return metaTags;
   }
-}
\ No newline at end of file
+}

File: src/main/java/edu/uci/ics/crawler4j/crawler/CrawlController.java
Patch:
@@ -93,8 +93,7 @@ public CrawlController(CrawlConfig config, PageFetcher pageFetcher, RobotstxtSer
     }
 
     TLDList.setUseOnline(config.isOnlineTldListUpdate());
-    TLDList.getInstance();
-    
+
     boolean resumable = config.isResumableCrawling();
 
     EnvironmentConfig envConfig = new EnvironmentConfig();

File: src/main/java/edu/uci/ics/crawler4j/fetcher/PageFetcher.java
Patch:
@@ -119,6 +119,7 @@ public boolean isTrusted(final X509Certificate[] chain, String authType) {
     clientBuilder.setDefaultRequestConfig(requestConfig);
     clientBuilder.setConnectionManager(connectionManager);
     clientBuilder.setUserAgent(config.getUserAgentString());
+    clientBuilder.setDefaultHeaders(config.getDefaultHeaders());
 
     if (config.getProxyHost() != null) {
       if (config.getProxyUsername() != null) {

File: src/main/java/edu/uci/ics/crawler4j/fetcher/PageFetcher.java
Patch:
@@ -115,6 +115,7 @@ public boolean isTrusted(final X509Certificate[] chain, String authType) {
     clientBuilder.setDefaultRequestConfig(requestConfig);
     clientBuilder.setConnectionManager(connectionManager);
     clientBuilder.setUserAgent(config.getUserAgentString());
+    clientBuilder.setDefaultHeaders(config.getDefaultHeaders());
 
     if (config.getProxyHost() != null) {
       if (config.getProxyUsername() != null) {

File: src/test/java/edu/uci/ics/crawler4j/examples/basic/BasicCrawler.java
Patch:
@@ -65,7 +65,7 @@ public void visit(Page page) {
     String anchor = page.getWebURL().getAnchor();
 
     logger.debug("Docid: {}", docid);
-    logger.info("URL: ", url);
+    logger.info("URL: {}", url);
     logger.debug("Domain: '{}'", domain);
     logger.debug("Sub-domain: '{}'", subDomain);
     logger.debug("Path: '{}'", path);
@@ -93,4 +93,4 @@ public void visit(Page page) {
 
     logger.debug("=============");
   }
-}
\ No newline at end of file
+}

File: src/main/java/edu/uci/ics/crawler4j/robotstxt/RobotstxtServer.java
Patch:
@@ -22,10 +22,7 @@
 import java.util.Map;
 import java.util.Map.Entry;
 
-import edu.uci.ics.crawler4j.crawler.CrawlConfig;
 import edu.uci.ics.crawler4j.crawler.exceptions.PageBiggerThanMaxSizeException;
-import edu.uci.ics.crawler4j.parser.HtmlParseData;
-import edu.uci.ics.crawler4j.parser.Parser;
 import org.apache.http.HttpStatus;
 
 import edu.uci.ics.crawler4j.crawler.Page;

File: src/main/java/edu/uci/ics/crawler4j/robotstxt/RobotstxtParser.java
Patch:
@@ -39,7 +39,7 @@ public static HostDirectives parse(String content, String myUserAgent) {
     HostDirectives directives = null;
     boolean inMatchingUserAgent = false;
 
-    StringTokenizer st = new StringTokenizer(content, "\n");
+    StringTokenizer st = new StringTokenizer(content, "\n\r");
     while (st.hasMoreTokens()) {
       String line = st.nextToken();
 

File: src/main/java/edu/uci/ics/crawler4j/crawler/Page.java
Patch:
@@ -83,8 +83,7 @@ public class Page {
   private String language;
 
   /**
-   * Headers which were present in the response of the
-   * fetch request
+   * Headers which were present in the response of the fetch request
    */
   protected Header[] fetchResponseHeaders;
 
@@ -123,7 +122,6 @@ public void load(HttpEntity entity) throws Exception {
       contentCharset = charset.displayName();
     }
 
-
     contentData = EntityUtils.toByteArray(entity);
   }
 

File: src/main/java/edu/uci/ics/crawler4j/crawler/WebCrawler.java
Patch:
@@ -368,9 +368,7 @@ private void processPage(WebURL curURL) {
           throw new ContentFetchException();
         }
 
-        if (!parser.parse(page, curURL.getURL())) {
-          throw new ParseException();
-        }
+        parser.parse(page, curURL.getURL());
 
         ParseData parseData = page.getParseData();
         List<WebURL> toSchedule = new ArrayList<>();

File: src/main/java/edu/uci/ics/crawler4j/fetcher/IdleConnectionMonitorThread.java
Patch:
@@ -39,8 +39,7 @@ public void run() {
           wait(5000);
           // Close expired connections
           connMgr.closeExpiredConnections();
-          // Optionally, close connections
-          // that have been idle longer than 30 sec
+          // Optionally, close connections that have been idle longer than 30 sec
           connMgr.closeIdleConnections(30, TimeUnit.SECONDS);
         }
       }

File: src/main/java/edu/uci/ics/crawler4j/frontier/InProcessPagesDB.java
Patch:
@@ -81,7 +81,7 @@ public boolean removeURL(WebURL webUrl) {
           }
         }
       } catch (Exception e) {
-        e.printStackTrace();
+        logger.error("Error while manipulating the DB of links from previous crawls", e);
       }
     }
     return false;

File: src/main/java/edu/uci/ics/crawler4j/url/TLDList.java
Patch:
@@ -50,13 +50,13 @@ private TLDList() {
         }
         tldSet.add(line);
       }
+
       reader.close();
       stream.close();
     } catch (Exception e) {
       logger.error("Couldn't find " + TLD_NAMES_TXT_FILENAME, e);
       System.exit(-1);
     }
-
   }
 
   public static TLDList getInstance() {

File: src/main/java/edu/uci/ics/crawler4j/url/UrlResolver.java
Patch:
@@ -33,11 +33,12 @@ public static String resolveUrl(final String baseUrl, final String relativeUrl)
     if (baseUrl == null) {
       throw new IllegalArgumentException("Base URL must not be null");
     }
+
     if (relativeUrl == null) {
       throw new IllegalArgumentException("Relative URL must not be null");
     }
-    final Url url = resolveUrl(parseUrl(baseUrl.trim()), relativeUrl.trim());
 
+    final Url url = resolveUrl(parseUrl(baseUrl.trim()), relativeUrl.trim());
     return url.toString();
   }
 

File: src/main/java/edu/uci/ics/crawler4j/crawler/exceptions/PageBiggerThanMaxSizeException.java
Patch:
@@ -8,6 +8,7 @@ public class PageBiggerThanMaxSizeException extends Exception {
   long pageSize;
 
   public PageBiggerThanMaxSizeException(long pageSize) {
+    super("Aborted fetching of this URL as it's size ( " + pageSize + " ) exceeds the maximum size");
     this.pageSize = pageSize;
   }
 

File: src/main/java/edu/uci/ics/crawler4j/crawler/CrawlController.java
Patch:
@@ -105,7 +105,7 @@ public CrawlController(CrawlConfig config, PageFetcher pageFetcher, RobotstxtSer
 
     env = new Environment(envHome, envConfig);
     docIdServer = new DocIDServer(env, config);
-    frontier = new Frontier(env, config, docIdServer);
+    frontier = new Frontier(env, config);
 
     this.pageFetcher = pageFetcher;
     this.robotstxtServer = robotstxtServer;

File: src/main/java/edu/uci/ics/crawler4j/fetcher/PageFetcher.java
Patch:
@@ -34,6 +34,7 @@
 import edu.uci.ics.crawler4j.crawler.authentication.AuthInfo;
 import edu.uci.ics.crawler4j.crawler.authentication.BasicAuthInfo;
 import edu.uci.ics.crawler4j.crawler.authentication.FormAuthInfo;
+import edu.uci.ics.crawler4j.crawler.exceptions.PageBiggerThanMaxSizeException;
 import org.apache.http.*;
 import org.apache.http.auth.AuthScope;
 import org.apache.http.auth.UsernamePasswordCredentials;
@@ -311,6 +312,7 @@ public PageFetchResult fetchHeader(WebURL webUrl) {
         e.printStackTrace();
       }
     }
+
     fetchResult.setStatusCode(CustomFetchStatus.UnknownError);
     logger.error("Failed: Unknown error occurred while fetching {}", webUrl.getURL());
     return fetchResult;

File: src/test/java/edu/uci/ics/crawler4j/examples/imagecrawler/ImageCrawler.java
Patch:
@@ -99,6 +99,6 @@ public void visit(Page page) {
     // store image
     IO.writeBytesToFile(page.getContentData(), storageFolder.getAbsolutePath() + "/" + hashedName);
 
-    System.out.println("Stored: " + url);
+    logger.info("Stored: {}", url);
   }
 }
\ No newline at end of file

File: src/main/java/edu/uci/ics/crawler4j/crawler/Configurable.java
Patch:
@@ -21,7 +21,7 @@
  * Several core components of crawler4j extend this class
  * to make them configurable.
  *
- * @author Yasser Ganjisaffar <lastname at gmail dot com>
+ * @author Yasser Ganjisaffar [lastname at gmail dot com]
  */
 public abstract class Configurable {
 

File: src/main/java/edu/uci/ics/crawler4j/crawler/authentication/BasicAuthInfo.java
Patch:
@@ -22,6 +22,8 @@ public class BasicAuthInfo extends AuthInfo {
    * @param username Username used for Authentication
    * @param password Password used for Authentication
    * @param loginUrl Full Login URL beginning with "http..." till the end of the url
+   *
+   * @throws MalformedURLException Make sure your URL is valid
    */
   public BasicAuthInfo(String username, String password, String loginUrl) throws MalformedURLException {
     super(AuthenticationType.BASIC_AUTHENTICATION, MethodType.GET, loginUrl, username, password);

File: src/main/java/edu/uci/ics/crawler4j/fetcher/CustomFetchStatus.java
Patch:
@@ -20,7 +20,7 @@
 import org.apache.http.HttpStatus;
 
 /**
- * @author Yasser Ganjisaffar <lastname at gmail dot com>
+ * @author Yasser Ganjisaffar [lastname at gmail dot com]
  */
 public class CustomFetchStatus {
 

File: src/main/java/edu/uci/ics/crawler4j/fetcher/PageFetchResult.java
Patch:
@@ -29,7 +29,7 @@
 import org.slf4j.LoggerFactory;
 
 /**
- * @author Yasser Ganjisaffar <lastname at gmail dot com>
+ * @author Yasser Ganjisaffar [lastname at gmail dot com]
  */
 public class PageFetchResult {
 

File: src/main/java/edu/uci/ics/crawler4j/fetcher/PageFetcher.java
Patch:
@@ -65,7 +65,7 @@
 import edu.uci.ics.crawler4j.url.WebURL;
 
 /**
- * @author Yasser Ganjisaffar <lastname at gmail dot com>
+ * @author Yasser Ganjisaffar [lastname at gmail dot com]
  */
 public class PageFetcher extends Configurable {
 

File: src/main/java/edu/uci/ics/crawler4j/frontier/Counters.java
Patch:
@@ -26,7 +26,7 @@
 import java.util.Map;
 
 /**
- * @author Yasser Ganjisaffar <lastname at gmail dot com>
+ * @author Yasser Ganjisaffar [lastname at gmail dot com]
  */
 
 public class Counters extends Configurable {

File: src/main/java/edu/uci/ics/crawler4j/frontier/DocIDServer.java
Patch:
@@ -26,7 +26,7 @@
 import org.slf4j.LoggerFactory;
 
 /**
- * @author Yasser Ganjisaffar <lastname at gmail dot com>
+ * @author Yasser Ganjisaffar [lastname at gmail dot com]
  */
 
 public class DocIDServer extends Configurable {

File: src/main/java/edu/uci/ics/crawler4j/frontier/Frontier.java
Patch:
@@ -29,7 +29,7 @@
 import java.util.List;
 
 /**
- * @author Yasser Ganjisaffar <lastname at gmail dot com>
+ * @author Yasser Ganjisaffar [lastname at gmail dot com]
  */
 
 public class Frontier extends Configurable {

File: src/main/java/edu/uci/ics/crawler4j/frontier/InProcessPagesDB.java
Patch:
@@ -34,7 +34,7 @@
  * assigned to crawlers but are not yet processed.
  * It is used for resuming a previous crawl. 
  * 
- * @author Yasser Ganjisaffar <lastname at gmail dot com>
+ * @author Yasser Ganjisaffar [lastname at gmail dot com]
  */
 public class InProcessPagesDB extends WorkQueues {
 

File: src/main/java/edu/uci/ics/crawler4j/frontier/WebURLTupleBinding.java
Patch:
@@ -24,7 +24,7 @@
 import edu.uci.ics.crawler4j.url.WebURL;
 
 /**
- * @author Yasser Ganjisaffar <lastname at gmail dot com>
+ * @author Yasser Ganjisaffar [lastname at gmail dot com]
  */
 public class WebURLTupleBinding extends TupleBinding<WebURL> {
 

File: src/main/java/edu/uci/ics/crawler4j/frontier/WorkQueues.java
Patch:
@@ -25,7 +25,7 @@
 import java.util.List;
 
 /**
- * @author Yasser Ganjisaffar <lastname at gmail dot com>
+ * @author Yasser Ganjisaffar [lastname at gmail dot com]
  */
 public class WorkQueues {
 

File: src/main/java/edu/uci/ics/crawler4j/parser/NotAllowedContentException.java
Patch:
@@ -3,7 +3,7 @@
 /**
  * Created by Avi on 8/19/2014.
  *
- * This Exception will be thrown whenever the parser tries to parse not allowed content<br/>
+ * This Exception will be thrown whenever the parser tries to parse not allowed content<br>
  * For example when the parser tries to parse binary content although the user configured it not to do it
  */
 public class NotAllowedContentException extends Exception {

File: src/main/java/edu/uci/ics/crawler4j/parser/Parser.java
Patch:
@@ -41,7 +41,7 @@
 import org.slf4j.LoggerFactory;
 
 /**
- * @author Yasser Ganjisaffar <lastname at gmail dot com>
+ * @author Yasser Ganjisaffar [lastname at gmail dot com]
  */
 public class Parser extends Configurable {
 

File: src/main/java/edu/uci/ics/crawler4j/robotstxt/HostDirectives.java
Patch:
@@ -18,7 +18,7 @@
 package edu.uci.ics.crawler4j.robotstxt;
 
 /**
- * @author Yasser Ganjisaffar <lastname at gmail dot com>
+ * @author Yasser Ganjisaffar [lastname at gmail dot com]
  */
 public class HostDirectives {
 

File: src/main/java/edu/uci/ics/crawler4j/robotstxt/RobotstxtParser.java
Patch:
@@ -20,7 +20,7 @@
 import java.util.StringTokenizer;
 
 /**
- * @author Yasser Ganjisaffar <lastname at gmail dot com>
+ * @author Yasser Ganjisaffar [lastname at gmail dot com]
  */
 
 

File: src/main/java/edu/uci/ics/crawler4j/robotstxt/RobotstxtServer.java
Patch:
@@ -34,7 +34,7 @@
 import org.slf4j.LoggerFactory;
 
 /**
- * @author Yasser Ganjisaffar <lastname at gmail dot com>
+ * @author Yasser Ganjisaffar [lastname at gmail dot com]
  */
 public class RobotstxtServer {
 

File: src/main/java/edu/uci/ics/crawler4j/url/URLCanonicalizer.java
Patch:
@@ -32,7 +32,7 @@
  * See http://en.wikipedia.org/wiki/URL_normalization for a reference Note: some
  * parts of the code are adapted from: http://stackoverflow.com/a/4057470/405418
  * 
- * @author Yasser Ganjisaffar <lastname at gmail dot com>
+ * @author Yasser Ganjisaffar [lastname at gmail dot com]
  */
 public class URLCanonicalizer {
 

File: src/main/java/edu/uci/ics/crawler4j/util/IO.java
Patch:
@@ -22,7 +22,7 @@
 import java.nio.channels.FileChannel;
 
 /**
- * @author Yasser Ganjisaffar <lastname at gmail dot com>
+ * @author Yasser Ganjisaffar [lastname at gmail dot com]
  */
 public class IO {
 

File: src/main/java/edu/uci/ics/crawler4j/util/Util.java
Patch:
@@ -19,7 +19,7 @@
 
 
 /**
- * @author Yasser Ganjisaffar <lastname at gmail dot com>
+ * @author Yasser Ganjisaffar [lastname at gmail dot com]
  */
 public class Util {
 

File: src/test/java/edu/uci/ics/crawler4j/examples/basic/BasicCrawlController.java
Patch:
@@ -24,7 +24,7 @@
 import edu.uci.ics.crawler4j.robotstxt.RobotstxtServer;
 
 /**
- * @author Yasser Ganjisaffar <lastname at gmail dot com>
+ * @author Yasser Ganjisaffar [lastname at gmail dot com]
  */
 public class BasicCrawlController {
 

File: src/test/java/edu/uci/ics/crawler4j/examples/basic/BasicCrawler.java
Patch:
@@ -28,7 +28,7 @@
 import org.apache.http.Header;
 
 /**
- * @author Yasser Ganjisaffar <lastname at gmail dot com>
+ * @author Yasser Ganjisaffar [lastname at gmail dot com]
  */
 public class BasicCrawler extends WebCrawler {
 

File: src/test/java/edu/uci/ics/crawler4j/examples/imagecrawler/Cryptography.java
Patch:
@@ -20,7 +20,7 @@
 import java.security.MessageDigest;
 
 /**
- * @author Yasser Ganjisaffar <lastname at gmail dot com>
+ * @author Yasser Ganjisaffar [lastname at gmail dot com]
  */
 public class Cryptography {
 

File: src/test/java/edu/uci/ics/crawler4j/examples/imagecrawler/ImageCrawlController.java
Patch:
@@ -24,7 +24,7 @@
 import edu.uci.ics.crawler4j.robotstxt.RobotstxtServer;
 
 /**
- * @author Yasser Ganjisaffar <lastname at gmail dot com>
+ * @author Yasser Ganjisaffar [lastname at gmail dot com]
  */
 
 /*

File: src/test/java/edu/uci/ics/crawler4j/examples/imagecrawler/ImageCrawler.java
Patch:
@@ -27,7 +27,7 @@
 import edu.uci.ics.crawler4j.util.IO;
 
 /**
- * @author Yasser Ganjisaffar <lastname at gmail dot com>
+ * @author Yasser Ganjisaffar [lastname at gmail dot com]
  */
 
 /*

File: src/test/java/edu/uci/ics/crawler4j/examples/multiple/BasicCrawler.java
Patch:
@@ -26,7 +26,7 @@
 import edu.uci.ics.crawler4j.url.WebURL;
 
 /**
- * @author Yasser Ganjisaffar <lastname at gmail dot com>
+ * @author Yasser Ganjisaffar [lastname at gmail dot com]
  */
 public class BasicCrawler extends WebCrawler {
 

File: src/test/java/edu/uci/ics/crawler4j/examples/multiple/MultipleCrawlerController.java
Patch:
@@ -24,7 +24,7 @@
 import edu.uci.ics.crawler4j.robotstxt.RobotstxtServer;
 
 /**
- * @author Yasser Ganjisaffar <lastname at gmail dot com>
+ * @author Yasser Ganjisaffar [lastname at gmail dot com]
  */
 
 public class MultipleCrawlerController {

File: src/test/java/edu/uci/ics/crawler4j/examples/shutdown/BasicCrawler.java
Patch:
@@ -26,7 +26,7 @@
 import java.util.regex.Pattern;
 
 /**
- * @author Yasser Ganjisaffar <lastname at gmail dot com>
+ * @author Yasser Ganjisaffar [lastname at gmail dot com]
  */
 
 public class BasicCrawler extends WebCrawler {

File: src/test/java/edu/uci/ics/crawler4j/examples/shutdown/ControllerWithShutdown.java
Patch:
@@ -24,7 +24,7 @@
 import edu.uci.ics.crawler4j.robotstxt.RobotstxtServer;
 
 /**
- * @author Yasser Ganjisaffar <lastname at gmail dot com>
+ * @author Yasser Ganjisaffar [lastname at gmail dot com]
  */
 
 public class ControllerWithShutdown {

File: src/test/java/edu/uci/ics/crawler4j/examples/statushandler/StatusHandlerCrawlController.java
Patch:
@@ -24,7 +24,7 @@
 import edu.uci.ics.crawler4j.robotstxt.RobotstxtServer;
 
 /**
- * @author Yasser Ganjisaffar <lastname at gmail dot com>
+ * @author Yasser Ganjisaffar [lastname at gmail dot com]
  */
 public class StatusHandlerCrawlController {
 

File: src/test/java/edu/uci/ics/crawler4j/examples/statushandler/StatusHandlerCrawler.java
Patch:
@@ -26,7 +26,7 @@
 import edu.uci.ics.crawler4j.url.WebURL;
 
 /**
- * @author Yasser Ganjisaffar <lastname at gmail dot com>
+ * @author Yasser Ganjisaffar [lastname at gmail dot com]
  */
 public class StatusHandlerCrawler extends WebCrawler {
 

File: src/main/java/edu/uci/ics/crawler4j/parser/HtmlParseData.java
Patch:
@@ -63,10 +63,12 @@ public void setMetaTags(Map<String, String> metaTags) {
     this.metaTags = metaTags;
   }
 
+  @Override
   public Set<WebURL> getOutgoingUrls() {
     return outgoingUrls;
   }
 
+  @Override
   public void setOutgoingUrls(Set<WebURL> outgoingUrls) {
     this.outgoingUrls = outgoingUrls;
   }

File: src/main/java/edu/uci/ics/crawler4j/crawler/WebCrawler.java
Patch:
@@ -338,6 +338,8 @@ private void processPage(WebURL curURL) {
             if (shouldVisit(page, webURL) && robotstxtServer.allows(webURL)) {
               webURL.setDocid(docIdServer.getNewDocID(movedToUrl));
               frontier.schedule(webURL);
+            } else {
+              logger.debug("Not visiting: {} as per your \"shouldVisit\" policy", webURL.getURL());
             }
           }
         } else if (fetchResult.getStatusCode() == CustomFetchStatus.PageTooBig) {

File: src/main/java/edu/uci/ics/crawler4j/fetcher/PageFetcher.java
Patch:
@@ -152,7 +152,7 @@ public PageFetchResult fetchHeader(WebURL webUrl) {
         }
         lastFetchTime = (new Date()).getTime();
       }
-      get.addHeader("Accept-Encoding", "gzip");
+
       HttpResponse response = httpClient.execute(get);
       fetchResult.setEntity(response.getEntity());
       fetchResult.setResponseHeaders(response.getAllHeaders());

File: src/main/java/edu/uci/ics/crawler4j/crawler/CrawlController.java
Patch:
@@ -443,6 +443,7 @@ public boolean isShuttingDown() {
   public void shutdown() {
     logger.info("Shutting down...");
     this.shuttingDown = true;
+    getPageFetcher().shutDown();
     frontier.finish();
   }
 }

File: src/main/java/edu/uci/ics/crawler4j/parser/Parser.java
Patch:
@@ -133,6 +133,7 @@ public boolean parse(Page page, String contextURL) throws NotAllowedContentExcep
         if (url != null) {
           WebURL webURL = new WebURL();
           webURL.setURL(url);
+          webURL.setTag(urlAnchorPair.getTag());
           webURL.setAnchor(urlAnchorPair.getAnchor());
           outgoingUrls.add(webURL);
           urlCount++;

File: src/main/java/edu/uci/ics/crawler4j/crawler/Page.java
Patch:
@@ -121,6 +121,7 @@ public void load(HttpEntity entity) throws Exception {
       contentCharset = charset.displayName();
     }
 
+
     contentData = EntityUtils.toByteArray(entity);
   }
 

File: src/main/java/edu/uci/ics/crawler4j/parser/Parser.java
Patch:
@@ -109,6 +109,7 @@ public boolean parse(Page page, String contextURL) throws NotAllowedContentExcep
     HtmlParseData parseData = new HtmlParseData();
     parseData.setText(contentHandler.getBodyText().trim());
     parseData.setTitle(metadata.get(DublinCore.TITLE));
+    parseData.setMetaTags(contentHandler.getMetaTags());
     LanguageIdentifier languageIdentifier = new LanguageIdentifier(parseData.getText());
     page.setLanguage(languageIdentifier.getLanguage());
 

File: src/main/java/edu/uci/ics/crawler4j/parser/Parser.java
Patch:
@@ -24,6 +24,7 @@
 import java.util.HashSet;
 import java.util.Set;
 
+import org.apache.tika.language.LanguageIdentifier;
 import org.apache.tika.metadata.DublinCore;
 import org.apache.tika.metadata.Metadata;
 import org.apache.tika.parser.ParseContext;
@@ -108,6 +109,8 @@ public boolean parse(Page page, String contextURL) throws NotAllowedContentExcep
     HtmlParseData parseData = new HtmlParseData();
     parseData.setText(contentHandler.getBodyText().trim());
     parseData.setTitle(metadata.get(DublinCore.TITLE));
+    LanguageIdentifier languageIdentifier = new LanguageIdentifier(parseData.getText());
+    page.setLanguage(languageIdentifier.getLanguage());
 
     Set<WebURL> outgoingUrls = new HashSet<>();
 

File: src/test/java/edu/uci/ics/crawler4j/examples/basic/BasicCrawler.java
Patch:
@@ -40,7 +40,7 @@ public class BasicCrawler extends WebCrawler {
    * should be crawled or not (based on your crawling logic).
    */
   @Override
-  public boolean shouldVisit(WebURL url) {
+  public boolean shouldVisit(Page page, WebURL url) {
     String href = url.getURL().toLowerCase();
     return !FILTERS.matcher(href).matches() && href.startsWith("http://www.ics.uci.edu/");
   }

File: src/test/java/edu/uci/ics/crawler4j/examples/imagecrawler/ImageCrawler.java
Patch:
@@ -56,7 +56,7 @@ public static void configure(String[] domain, String storageFolderName) {
   }
 
   @Override
-  public boolean shouldVisit(WebURL url) {
+  public boolean shouldVisit(Page page, WebURL url) {
     String href = url.getURL().toLowerCase();
     if (filters.matcher(href).matches()) {
       return false;

File: src/test/java/edu/uci/ics/crawler4j/examples/localdata/LocalDataCollectorCrawler.java
Patch:
@@ -38,7 +38,7 @@ public LocalDataCollectorCrawler() {
   }
 
   @Override
-  public boolean shouldVisit(WebURL url) {
+  public boolean shouldVisit(Page page, WebURL url) {
     String href = url.getURL().toLowerCase();
     return !filters.matcher(href).matches() && href.startsWith("http://www.ics.uci.edu/");
   }

File: src/test/java/edu/uci/ics/crawler4j/examples/multiple/BasicCrawler.java
Patch:
@@ -41,7 +41,7 @@ public void onStart() {
   }
 
   @Override
-  public boolean shouldVisit(WebURL url) {
+  public boolean shouldVisit(Page page, WebURL url) {
     String href = url.getURL().toLowerCase();
     if (FILTERS.matcher(href).matches()) {
       return false;

File: src/test/java/edu/uci/ics/crawler4j/examples/shutdown/BasicCrawler.java
Patch:
@@ -37,7 +37,7 @@ public class BasicCrawler extends WebCrawler {
   private final static String DOMAIN = "http://www.ics.uci.edu/";
 
   @Override
-  public boolean shouldVisit(WebURL url) {
+  public boolean shouldVisit(Page page, WebURL url) {
     String href = url.getURL().toLowerCase();
     return !FILTERS.matcher(href).matches() && href.startsWith(DOMAIN);
   }

File: src/test/java/edu/uci/ics/crawler4j/examples/statushandler/StatusHandlerCrawler.java
Patch:
@@ -39,7 +39,7 @@ public class StatusHandlerCrawler extends WebCrawler {
 	 * crawling logic).
 	 */
 	@Override
-	public boolean shouldVisit(WebURL url) {
+	public boolean shouldVisit(Page page, WebURL url) {
 		String href = url.getURL().toLowerCase();
 		return !FILTERS.matcher(href).matches() && href.startsWith("http://www.ics.uci.edu/");
 	}

File: src/main/java/edu/uci/ics/crawler4j/parser/Parser.java
Patch:
@@ -118,13 +118,13 @@ public boolean parse(Page page, String contextURL) throws NotAllowedContentExcep
 
     int urlCount = 0;
     for (ExtractedUrlAnchorPair urlAnchorPair : contentHandler.getOutgoingUrls()) {
-      String href = urlAnchorPair.getHref();
-      href = href.trim().toLowerCase();
+      String href = urlAnchorPair.getHref().trim();
       if (href.length() == 0) {
         continue;
       }
 
-      if (!href.contains("javascript:") && !href.contains("mailto:") && !href.contains("@")) {
+      String hrefLoweredCase = href.toLowerCase();
+      if (!hrefLoweredCase.contains("javascript:") && !hrefLoweredCase.contains("mailto:") && !hrefLoweredCase.contains("@")) {
         String url = URLCanonicalizer.getCanonicalURL(href, contextURL);
         if (url != null) {
           WebURL webURL = new WebURL();

File: src/main/java/edu/uci/ics/crawler4j/parser/Parser.java
Patch:
@@ -21,8 +21,8 @@
 import java.io.IOException;
 import java.io.InputStream;
 import java.io.UnsupportedEncodingException;
-import java.util.ArrayList;
-import java.util.List;
+import java.util.HashSet;
+import java.util.Set;
 
 import org.apache.tika.metadata.DublinCore;
 import org.apache.tika.metadata.Metadata;
@@ -109,7 +109,7 @@ public boolean parse(Page page, String contextURL) throws NotAllowedContentExcep
     parseData.setText(contentHandler.getBodyText().trim());
     parseData.setTitle(metadata.get(DublinCore.TITLE));
 
-    List<WebURL> outgoingUrls = new ArrayList<>();
+    Set<WebURL> outgoingUrls = new HashSet<>();
 
     String baseURL = contentHandler.getBaseUrl();
     if (baseURL != null) {

File: src/test/java/edu/uci/ics/crawler4j/examples/localdata/LocalDataCollectorCrawler.java
Patch:
@@ -23,7 +23,7 @@
 import edu.uci.ics.crawler4j.url.WebURL;
 
 import java.io.UnsupportedEncodingException;
-import java.util.List;
+import java.util.Set;
 import java.util.regex.Pattern;
 
 public class LocalDataCollectorCrawler extends WebCrawler {
@@ -50,7 +50,7 @@ public void visit(Page page) {
 
     if (page.getParseData() instanceof HtmlParseData) {
       HtmlParseData parseData = (HtmlParseData) page.getParseData();
-      List<WebURL> links = parseData.getOutgoingUrls();
+      Set<WebURL> links = parseData.getOutgoingUrls();
       myCrawlStat.incTotalLinks(links.size());
       try {
         myCrawlStat.incTotalTextSize(parseData.getText().getBytes("UTF-8").length);

File: src/test/java/edu/uci/ics/crawler4j/examples/multiple/BasicCrawler.java
Patch:
@@ -17,7 +17,7 @@
 
 package edu.uci.ics.crawler4j.examples.multiple;
 
-import java.util.List;
+import java.util.Set;
 import java.util.regex.Pattern;
 
 import edu.uci.ics.crawler4j.crawler.Page;
@@ -70,7 +70,7 @@ public void visit(Page page) {
       HtmlParseData htmlParseData = (HtmlParseData) page.getParseData();
       String text = htmlParseData.getText();
       String html = htmlParseData.getHtml();
-      List<WebURL> links = htmlParseData.getOutgoingUrls();
+      Set<WebURL> links = htmlParseData.getOutgoingUrls();
 
       System.out.println("Text length: " + text.length());
       System.out.println("Html length: " + html.length());

File: src/test/java/edu/uci/ics/crawler4j/examples/shutdown/BasicCrawler.java
Patch:
@@ -22,7 +22,7 @@
 import edu.uci.ics.crawler4j.parser.HtmlParseData;
 import edu.uci.ics.crawler4j.url.WebURL;
 
-import java.util.List;
+import java.util.Set;
 import java.util.regex.Pattern;
 
 /**
@@ -56,7 +56,7 @@ public void visit(Page page) {
       HtmlParseData htmlParseData = (HtmlParseData) page.getParseData();
       String text = htmlParseData.getText();
       String html = htmlParseData.getHtml();
-      List<WebURL> links = htmlParseData.getOutgoingUrls();
+      Set<WebURL> links = htmlParseData.getOutgoingUrls();
 
       System.out.println("Text length: " + text.length());
       System.out.println("Html length: " + html.length());

File: src/main/java/edu/uci/ics/crawler4j/crawler/WebCrawler.java
Patch:
@@ -303,7 +303,9 @@ private void processPage(WebURL curURL) {
       int statusCode = fetchResult.getStatusCode();
       handlePageStatusCode(curURL, statusCode, CustomFetchStatus.getStatusDescription(statusCode));
       if (statusCode != HttpStatus.SC_OK) {
-        if (statusCode == HttpStatus.SC_MOVED_PERMANENTLY || statusCode == HttpStatus.SC_MOVED_TEMPORARILY) {
+        if (statusCode == HttpStatus.SC_MOVED_PERMANENTLY || statusCode == HttpStatus.SC_MOVED_TEMPORARILY
+            || statusCode == HttpStatus.SC_MULTIPLE_CHOICES || statusCode == HttpStatus.SC_SEE_OTHER
+            || statusCode == HttpStatus.SC_TEMPORARY_REDIRECT || statusCode == CustomFetchStatus.SC_PERMANENT_REDIRECT) {
           if (myController.getConfig().isFollowRedirects()) {
             String movedToUrl = fetchResult.getMovedToUrl();
             if (movedToUrl == null) {

File: src/main/java/edu/uci/ics/crawler4j/fetcher/CustomFetchStatus.java
Patch:
@@ -24,6 +24,7 @@
  */
 public class CustomFetchStatus {
 
+  public static final int SC_PERMANENT_REDIRECT = 308; // todo follow https://issues.apache.org/jira/browse/HTTPCORE-389
   public static final int PageTooBig = 1001;
   public static final int FatalTransportError = 1005;
   public static final int UnknownError = 1006;

File: src/main/java/edu/uci/ics/crawler4j/fetcher/PageFetcher.java
Patch:
@@ -160,7 +160,9 @@ public PageFetchResult fetchHeader(WebURL webUrl) {
       int statusCode = response.getStatusLine().getStatusCode();
       if (statusCode != HttpStatus.SC_OK) {
         if (statusCode != HttpStatus.SC_NOT_FOUND) {
-          if (statusCode == HttpStatus.SC_MOVED_PERMANENTLY || statusCode == HttpStatus.SC_MOVED_TEMPORARILY) {
+          if (statusCode == HttpStatus.SC_MOVED_PERMANENTLY || statusCode == HttpStatus.SC_MOVED_TEMPORARILY
+              || statusCode == HttpStatus.SC_MULTIPLE_CHOICES || statusCode == HttpStatus.SC_SEE_OTHER
+              || statusCode == HttpStatus.SC_TEMPORARY_REDIRECT || statusCode == CustomFetchStatus.SC_PERMANENT_REDIRECT) {
             Header header = response.getFirstHeader("Location");
             if (header != null) {
               String movedToUrl = header.getValue();

File: src/main/java/edu/uci/ics/crawler4j/parser/NotAllowedContentException.java
Patch:
@@ -8,6 +8,6 @@
  */
 public class NotAllowedContentException extends Exception {
     public NotAllowedContentException() {
-        super("Not allowed to parse this type of content");
+      super("Not allowed to parse this type of content");
     }
 }
\ No newline at end of file

File: src/test/java/edu/uci/ics/crawler4j/examples/localdata/LocalDataCollectorController.java
Patch:
@@ -34,6 +34,7 @@ public static void main(String[] args) throws Exception {
 			System.out.println("\t numberOfCralwers (number of concurrent threads)");
 			return;
 		}
+
 		String rootFolder = args[0];
 		int numberOfCrawlers = Integer.parseInt(args[1]);
 
@@ -60,10 +61,10 @@ public static void main(String[] args) throws Exception {
 			totalTextSize += stat.getTotalTextSize();
 			totalProcessedPages += stat.getTotalProcessedPages();
 		}
+
 		System.out.println("Aggregated Statistics:");
 		System.out.println("   Processed Pages: " + totalProcessedPages);
 		System.out.println("   Total Links found: " + totalLinks);
 		System.out.println("   Total Text Size: " + totalTextSize);
 	}
-
-}
+}
\ No newline at end of file

File: src/main/java/edu/uci/ics/crawler4j/crawler/CrawlConfig.java
Patch:
@@ -58,7 +58,7 @@ public class CrawlConfig {
 	/**
 	 * Should we also crawl https pages?
 	 */
-	private boolean includeHttpsPages = false;
+	private boolean includeHttpsPages = true;
 
 	/**
 	 * Should we fetch binary content such as images, audio, ...?

File: src/main/java/edu/uci/ics/crawler4j/crawler/WebCrawler.java
Patch:
@@ -29,7 +29,6 @@
 import edu.uci.ics.crawler4j.robotstxt.RobotstxtServer;
 import edu.uci.ics.crawler4j.url.WebURL;
 
-import org.apache.http.Header;
 import org.apache.http.HttpStatus;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;

File: src/main/java/edu/uci/ics/crawler4j/fetcher/PageFetcher.java
Patch:
@@ -217,7 +217,9 @@ public PageFetchResult fetchHeader(WebURL webUrl) {
 			get.abort();
 			
 		} catch (IOException e) {
-			logger.error("Fatal transport error: {} while fetching {} (link found in doc #{})", e.getMessage(), toFetchURL, webUrl.getParentDocid());
+			logger.error("Fatal transport error: {} while fetching {} (link found in doc #{})",
+                    e.getMessage() != null ? e.getMessage() : e.getCause(), toFetchURL, webUrl.getParentDocid());
+            logger.debug("Stacktrace: ", e);
 			fetchResult.setStatusCode(CustomFetchStatus.FatalTransportError);
 			return fetchResult;
 		} catch (IllegalStateException e) {

File: src/main/java/edu/uci/ics/crawler4j/url/WebURL.java
Patch:
@@ -91,6 +91,7 @@ public void setURL(String url) {
 
 		int domainStartIdx = url.indexOf("//") + 2;
 		int domainEndIdx = url.indexOf('/', domainStartIdx);
+        domainEndIdx = domainEndIdx > domainStartIdx ? domainEndIdx : url.length();
 		domain = url.substring(domainStartIdx, domainEndIdx);
 		subDomain = "";
 		String[] parts = domain.split("\\.");

File: src/main/java/edu/uci/ics/crawler4j/fetcher/CustomFetchStatus.java
Patch:
@@ -93,7 +93,7 @@ public static String getStatusDescription(int code) {
 		case PageTooBig:
 			return "Page size was too big";
 		case FatalTransportError:
-			return "Fatal transport error";
+			return "Fatal transport error - server is probably down"; // todo follow https://issues.apache.org/jira/browse/HTTPCORE-388
 		case UnknownError:
 			return "Unknown error";
 		default:

File: src/test/java/edu/uci/ics/crawler4j/examples/multiple/MultipleCrawlerController.java
Patch:
@@ -89,7 +89,7 @@ public static void main(String[] args) throws Exception {
 		controller2.addSeed("http://en.wikipedia.org/wiki/Bing");
 
 		/*
-		 * The first crawler will have 5 cuncurrent threads and the second
+		 * The first crawler will have 5 concurrent threads and the second
 		 * crawler will have 7 threads.
 		 */
 		controller1.startNonBlocking(BasicCrawler.class, 5);

File: src/main/java/edu/uci/ics/crawler4j/parser/BinaryParseData.java
Patch:
@@ -44,7 +44,6 @@ public class BinaryParseData implements ParseData {
     private static final String DEFAULT_ENCODING = "UTF-8";
     private static final String DEFAULT_OUTPUT_FORMAT = "html";
 
-    private static final Metadata METADATA = new Metadata();
     private static final Parser AUTO_DETECT_PARSER = new AutoDetectParser();
     private static final SAXTransformerFactory SAX_TRANSFORMER_FACTORY = (SAXTransformerFactory) SAXTransformerFactory.newInstance();
 
@@ -61,7 +60,7 @@ public void setBinaryContent(byte[] data) {
 
         try {
             TransformerHandler handler = getTransformerHandler(outputStream, DEFAULT_OUTPUT_FORMAT, DEFAULT_ENCODING);
-            AUTO_DETECT_PARSER.parse(inputStream, handler, METADATA, context);
+            AUTO_DETECT_PARSER.parse(inputStream, handler, new Metadata(), context);
 
             setHtml(new String(outputStream.toByteArray(), DEFAULT_ENCODING));
         } catch (TransformerConfigurationException e) {

File: src/main/java/edu/uci/ics/crawler4j/fetcher/PageFetcher.java
Patch:
@@ -204,7 +204,8 @@ public PageFetchResult fetchHeader(WebURL webUrl) {
 				if (size > config.getMaxDownloadSize()) {
 					fetchResult.setStatusCode(CustomFetchStatus.PageTooBig);
 					get.abort();
-					logger.error("Failed: Page Size ({}) exceeded max-download-size ({})", size, config.getMaxDownloadSize());
+					logger.error("Failed: Page Size ({}) exceeded max-download-size ({}), at URL: {}",
+                            size, config.getMaxDownloadSize(), webUrl.getURL());
 					return fetchResult;
 				}
 

File: src/main/java/edu/uci/ics/crawler4j/url/URLCanonicalizer.java
Patch:
@@ -58,7 +58,7 @@ public static String getCanonicalURL(String href, String context) {
 			 * ".", and no segments equal to ".." that are preceded by a segment
 			 * not equal to "..".
 			 */
-			path = new URI(path).normalize().toString();
+			path = new URI(path.replace("\\", "/")).normalize().toString();
 
 			/*
 			 * Convert '//' -> '/'

File: src/main/java/edu/uci/ics/crawler4j/frontier/Frontier.java
Patch:
@@ -194,7 +194,9 @@ public void close() {
 		sync();
 		workQueues.close();
         counters.close();
-		inProcessPages.close();
+        if (inProcessPages != null) {
+            inProcessPages.close();
+        }
 	}
 
 	public void finish() {

File: src/main/java/edu/uci/ics/crawler4j/crawler/CrawlController.java
Patch:
@@ -26,7 +26,8 @@
 import edu.uci.ics.crawler4j.url.URLCanonicalizer;
 import edu.uci.ics.crawler4j.url.WebURL;
 import edu.uci.ics.crawler4j.util.IO;
-import org.apache.log4j.Logger;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 import java.io.File;
 import java.util.ArrayList;
@@ -40,7 +41,7 @@
  */
 public class CrawlController extends Configurable {
 
-	static final Logger logger = Logger.getLogger(CrawlController.class.getName());
+	static final Logger logger = LoggerFactory.getLogger(CrawlController.class);
 
 	/**
 	 * The 'customData' object can be used for passing custom crawl-related

File: src/main/java/edu/uci/ics/crawler4j/crawler/WebCrawler.java
Patch:
@@ -29,7 +29,8 @@
 import edu.uci.ics.crawler4j.url.WebURL;
 
 import org.apache.http.HttpStatus;
-import org.apache.log4j.Logger;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 import java.util.ArrayList;
 import java.util.List;
@@ -42,7 +43,7 @@
  */
 public class WebCrawler implements Runnable {
 
-	protected static final Logger logger = Logger.getLogger(WebCrawler.class.getName());
+	protected static final Logger logger = LoggerFactory.getLogger(WebCrawler.class);
 
 	/**
 	 * The id associated to the crawler thread running this instance

File: src/main/java/edu/uci/ics/crawler4j/fetcher/PageFetchResult.java
Patch:
@@ -23,16 +23,17 @@
 import org.apache.http.Header;
 import org.apache.http.HttpEntity;
 import org.apache.http.util.EntityUtils;
-import org.apache.log4j.Logger;
 
 import edu.uci.ics.crawler4j.crawler.Page;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 /**
  * @author Yasser Ganjisaffar <lastname at gmail dot com>
  */
 public class PageFetchResult {
 
-	protected static final Logger logger = Logger.getLogger(PageFetchResult.class);
+	protected static final Logger logger = LoggerFactory.getLogger(PageFetchResult.class);
 
 	protected int statusCode;
 	protected HttpEntity entity = null;

File: src/main/java/edu/uci/ics/crawler4j/fetcher/PageFetcher.java
Patch:
@@ -51,19 +51,20 @@
 import org.apache.http.params.HttpParams;
 import org.apache.http.params.HttpProtocolParamBean;
 import org.apache.http.protocol.HttpContext;
-import org.apache.log4j.Logger;
 
 import edu.uci.ics.crawler4j.crawler.Configurable;
 import edu.uci.ics.crawler4j.crawler.CrawlConfig;
 import edu.uci.ics.crawler4j.url.URLCanonicalizer;
 import edu.uci.ics.crawler4j.url.WebURL;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 /**
  * @author Yasser Ganjisaffar <lastname at gmail dot com>
  */
 public class PageFetcher extends Configurable {
 
-	protected static final Logger logger = Logger.getLogger(PageFetcher.class);
+	protected static final Logger logger = LoggerFactory.getLogger(PageFetcher.class);
 
 	protected PoolingClientConnectionManager connectionManager;
 

File: src/main/java/edu/uci/ics/crawler4j/frontier/DocIDServer.java
Patch:
@@ -17,21 +17,21 @@
 
 package edu.uci.ics.crawler4j.frontier;
 
-import org.apache.log4j.Logger;
-
 import com.sleepycat.je.*;
 
 import edu.uci.ics.crawler4j.crawler.Configurable;
 import edu.uci.ics.crawler4j.crawler.CrawlConfig;
 import edu.uci.ics.crawler4j.util.Util;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 /**
  * @author Yasser Ganjisaffar <lastname at gmail dot com>
  */
 
 public class DocIDServer extends Configurable {
 
-	protected static final Logger logger = Logger.getLogger(DocIDServer.class.getName());
+	protected static final Logger logger = LoggerFactory.getLogger(DocIDServer.class);
 	
 	protected Database docIDsDB = null;
 	

File: src/main/java/edu/uci/ics/crawler4j/frontier/Frontier.java
Patch:
@@ -23,7 +23,8 @@
 import edu.uci.ics.crawler4j.crawler.CrawlConfig;
 import edu.uci.ics.crawler4j.frontier.Counters.ReservedCounterNames;
 import edu.uci.ics.crawler4j.url.WebURL;
-import org.apache.log4j.Logger;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 import java.util.List;
 
@@ -33,7 +34,7 @@
 
 public class Frontier extends Configurable {
 
-	protected static final Logger logger = Logger.getLogger(Frontier.class.getName());
+	protected static final Logger logger = LoggerFactory.getLogger(Frontier.class);
 
 	protected WorkQueues workQueues;
 

File: src/main/java/edu/uci/ics/crawler4j/frontier/InProcessPagesDB.java
Patch:
@@ -17,7 +17,6 @@
 
 package edu.uci.ics.crawler4j.frontier;
 
-import org.apache.log4j.Logger;
 
 import com.sleepycat.je.Cursor;
 import com.sleepycat.je.DatabaseEntry;
@@ -27,6 +26,8 @@
 import com.sleepycat.je.Transaction;
 
 import edu.uci.ics.crawler4j.url.WebURL;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 /**
  * This class maintains the list of pages which are
@@ -37,7 +38,7 @@
  */
 public class InProcessPagesDB extends WorkQueues {
 
-	private static final Logger logger = Logger.getLogger(InProcessPagesDB.class.getName());
+	private static final Logger logger = LoggerFactory.getLogger(InProcessPagesDB.class);
 		
 	public InProcessPagesDB(Environment env) throws DatabaseException {
 		super(env, "InProcessPagesDB", true);

File: src/main/java/edu/uci/ics/crawler4j/parser/Parser.java
Patch:
@@ -24,7 +24,6 @@
 import java.util.ArrayList;
 import java.util.List;
 
-import org.apache.log4j.Logger;
 import org.apache.tika.metadata.DublinCore;
 import org.apache.tika.metadata.Metadata;
 import org.apache.tika.parser.ParseContext;
@@ -36,13 +35,15 @@
 import edu.uci.ics.crawler4j.url.URLCanonicalizer;
 import edu.uci.ics.crawler4j.url.WebURL;
 import edu.uci.ics.crawler4j.util.Util;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 /**
  * @author Yasser Ganjisaffar <lastname at gmail dot com>
  */
 public class Parser extends Configurable {
 
-	protected static final Logger logger = Logger.getLogger(Parser.class.getName());
+	protected static final Logger logger = LoggerFactory.getLogger(Parser.class);
 
 	private HtmlParser htmlParser;
 	private ParseContext parseContext;

File: src/main/java/edu/uci/ics/crawler4j/crawler/WebCrawler.java
Patch:
@@ -284,6 +284,7 @@ private void processPage(WebURL curURL) {
 						webURL.setParentUrl(curURL.getParentUrl());
 						webURL.setDepth(curURL.getDepth());
 						webURL.setDocid(-1);
+						webURL.setAnchor(curURL.getAnchor());
 						if (shouldVisit(webURL) && robotstxtServer.allows(webURL)) {
 							webURL.setDocid(docIdServer.getNewDocID(movedToUrl));
 							frontier.schedule(webURL);

File: src/main/java/edu/uci/ics/crawler4j/frontier/WebURLTupleBinding.java
Patch:
@@ -37,6 +37,7 @@ public WebURL entryToObject(TupleInput input) {
 		webURL.setParentUrl(input.readString());
 		webURL.setDepth(input.readShort());
 		webURL.setPriority(input.readByte());
+		webURL.setAnchor(input.readString());
 		return webURL;
 	}
 
@@ -48,5 +49,6 @@ public void objectToEntry(WebURL url, TupleOutput output) {
 		output.writeString(url.getParentUrl());
 		output.writeShort(url.getDepth());
 		output.writeByte(url.getPriority());
+		output.writeString(url.getAnchor());
 	}
 }

File: src/test/java/edu/uci/ics/crawler4j/examples/basic/BasicCrawler.java
Patch:
@@ -57,13 +57,15 @@ public void visit(Page page) {
 		String path = page.getWebURL().getPath();
 		String subDomain = page.getWebURL().getSubDomain();
 		String parentUrl = page.getWebURL().getParentUrl();
+		String anchor = page.getWebURL().getAnchor();
 
 		System.out.println("Docid: " + docid);
 		System.out.println("URL: " + url);
 		System.out.println("Domain: '" + domain + "'");
 		System.out.println("Sub-domain: '" + subDomain + "'");
 		System.out.println("Path: '" + path + "'");
 		System.out.println("Parent page: " + parentUrl);
+		System.out.println("Anchor text: " + anchor);
 		
 		if (page.getParseData() instanceof HtmlParseData) {
 			HtmlParseData htmlParseData = (HtmlParseData) page.getParseData();

File: src/main/java/edu/uci/ics/crawler4j/fetcher/PageFetcher.java
Patch:
@@ -35,6 +35,8 @@
 import org.apache.http.auth.UsernamePasswordCredentials;
 import org.apache.http.client.HttpClient;
 import org.apache.http.client.methods.HttpGet;
+import org.apache.http.client.params.ClientPNames;
+import org.apache.http.client.params.CookiePolicy;
 import org.apache.http.conn.params.ConnRoutePNames;
 import org.apache.http.conn.scheme.PlainSocketFactory;
 import org.apache.http.conn.scheme.Scheme;
@@ -82,6 +84,7 @@ public PageFetcher(CrawlConfig config) {
 		paramsBean.setContentCharset("UTF-8");
 		paramsBean.setUseExpectContinue(false);
 
+		params.setParameter(ClientPNames.COOKIE_POLICY, CookiePolicy.BROWSER_COMPATIBILITY);
 		params.setParameter(CoreProtocolPNames.USER_AGENT, config.getUserAgentString());
 		params.setIntParameter(CoreConnectionPNames.SO_TIMEOUT, config.getSocketTimeout());
 		params.setIntParameter(CoreConnectionPNames.CONNECTION_TIMEOUT, config.getConnectionTimeout());

File: src/main/java/edu/uci/ics/crawler4j/url/WebURL.java
Patch:
@@ -97,7 +97,7 @@ public void setURL(String url) {
 		if (parts.length > 2) {
 			domain = parts[parts.length - 2] + "." + parts[parts.length - 1];
 			int limit = 2;
-			if (TLDList.contains(domain)) {
+			if (TLDList.getInstance().contains(domain)) {
 				domain = parts[parts.length - 3] + "." + domain;
 				limit = 3;
 			}

File: src/main/java/edu/uci/ics/crawler4j/fetcher/PageFetcher.java
Patch:
@@ -198,6 +198,7 @@ public PageFetchResult fetchHeader(WebURL webUrl) {
 				}
 				if (size > config.getMaxDownloadSize()) {
 					fetchResult.setStatusCode(CustomFetchStatus.PageTooBig);
+					get.abort();
 					return fetchResult;
 				}
 

File: src/main/java/edu/uci/ics/crawler4j/frontier/InProcessPagesDB.java
Patch:
@@ -27,7 +27,6 @@
 import com.sleepycat.je.Transaction;
 
 import edu.uci.ics.crawler4j.url.WebURL;
-import edu.uci.ics.crawler4j.util.Util;
 
 /**
  * This class maintains the list of pages which are
@@ -51,7 +50,7 @@ public InProcessPagesDB(Environment env) throws DatabaseException {
 	public boolean removeURL(WebURL webUrl) {
 		synchronized (mutex) {
 			try {
-				DatabaseEntry key = new DatabaseEntry(Util.int2ByteArray(webUrl.getDocid()));				
+				DatabaseEntry key = getDatabaseEntryKey(webUrl);				
 				Cursor cursor = null;
 				OperationStatus result;
 				DatabaseEntry value = new DatabaseEntry();

