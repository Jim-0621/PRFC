File: core/src/main/java/org/apache/iceberg/rest/RESTSessionCatalog.java
Patch:
@@ -151,6 +151,7 @@ public class RESTSessionCatalog extends BaseViewSessionCatalog
           .add(Endpoint.V1_RENAME_TABLE)
           .add(Endpoint.V1_REGISTER_TABLE)
           .add(Endpoint.V1_REPORT_METRICS)
+          .add(Endpoint.V1_COMMIT_TRANSACTION)
           .build();
 
   private static final Set<Endpoint> VIEW_ENDPOINTS =

File: spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/source/BaseRowReader.java
Patch:
@@ -32,9 +32,9 @@
 import org.apache.iceberg.orc.ORC;
 import org.apache.iceberg.parquet.Parquet;
 import org.apache.iceberg.relocated.com.google.common.collect.Sets;
-import org.apache.iceberg.spark.data.SparkAvroReader;
 import org.apache.iceberg.spark.data.SparkOrcReader;
 import org.apache.iceberg.spark.data.SparkParquetReaders;
+import org.apache.iceberg.spark.data.SparkPlannedAvroReader;
 import org.apache.iceberg.types.TypeUtil;
 import org.apache.spark.sql.catalyst.InternalRow;
 
@@ -77,7 +77,7 @@ private CloseableIterable<InternalRow> newAvroIterable(
         .reuseContainers()
         .project(projection)
         .split(start, length)
-        .createReaderFunc(readSchema -> new SparkAvroReader(projection, readSchema, idToConstant))
+        .createResolvingReader(schema -> SparkPlannedAvroReader.create(schema, idToConstant))
         .withNameMapping(nameMapping())
         .build();
   }

File: spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkAvroEnums.java
Patch:
@@ -78,7 +78,7 @@ public void writeAndValidateEnums() throws IOException {
     List<InternalRow> rows;
     try (AvroIterable<InternalRow> reader =
         Avro.read(Files.localInput(testFile))
-            .createReaderFunc(SparkAvroReader::new)
+            .createResolvingReader(SparkPlannedAvroReader::create)
             .project(schema)
             .build()) {
       rows = Lists.newArrayList(reader);

File: spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeOnReadDelete.java
Patch:
@@ -31,6 +31,7 @@
 import java.util.Set;
 import java.util.stream.Collectors;
 import org.apache.iceberg.DeleteFile;
+import org.apache.iceberg.FileFormat;
 import org.apache.iceberg.ParameterizedTestExtension;
 import org.apache.iceberg.RowDelta;
 import org.apache.iceberg.RowLevelOperationMode;
@@ -232,6 +233,7 @@ public void testDeleteWithDVAndHistoricalPositionDeletes() {
         deleteFiles.stream().filter(ContentFileUtil::isDV).collect(Collectors.toList());
     assertThat(dvs).hasSize(1);
     assertThat(dvs).allMatch(dv -> dv.recordCount() == 3);
+    assertThat(dvs).allMatch(dv -> FileFormat.fromFileName(dv.location()) == FileFormat.PUFFIN);
   }
 
   private void checkDeleteFileGranularity(DeleteGranularity deleteGranularity)

File: spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeOnReadMerge.java
Patch:
@@ -27,6 +27,7 @@
 import java.util.stream.Collectors;
 import java.util.stream.IntStream;
 import org.apache.iceberg.DeleteFile;
+import org.apache.iceberg.FileFormat;
 import org.apache.iceberg.RowLevelOperationMode;
 import org.apache.iceberg.Snapshot;
 import org.apache.iceberg.Table;
@@ -129,6 +130,7 @@ public void testMergeWithDVAndHistoricalPositionDeletes() {
         deleteFiles.stream().filter(ContentFileUtil::isDV).collect(Collectors.toList());
     assertThat(dvs).hasSize(1);
     assertThat(dvs).allMatch(dv -> dv.recordCount() == 3);
+    assertThat(dvs).allMatch(dv -> FileFormat.fromFileName(dv.location()) == FileFormat.PUFFIN);
   }
 
   private void checkMergeDeleteGranularity(DeleteGranularity deleteGranularity) {

File: spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeOnReadUpdate.java
Patch:
@@ -26,6 +26,7 @@
 import java.util.Set;
 import java.util.stream.Collectors;
 import org.apache.iceberg.DeleteFile;
+import org.apache.iceberg.FileFormat;
 import org.apache.iceberg.ParameterizedTestExtension;
 import org.apache.iceberg.RowLevelOperationMode;
 import org.apache.iceberg.Snapshot;
@@ -209,6 +210,7 @@ public void testUpdateWithDVAndHistoricalPositionDeletes() {
         deleteFiles.stream().filter(ContentFileUtil::isDV).collect(Collectors.toList());
     assertThat(dvs).hasSize(1);
     assertThat(dvs.get(0).recordCount()).isEqualTo(3);
+    assertThat(dvs).allMatch(dv -> FileFormat.fromFileName(dv.location()) == FileFormat.PUFFIN);
   }
 
   private void initTable(String partitionedBy, DeleteGranularity deleteGranularity) {

File: spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/source/SparkPositionDeltaWrite.java
Patch:
@@ -786,7 +786,6 @@ private static class Context implements Serializable {
     private final String queryId;
     private final boolean useFanoutWriter;
     private final boolean inputOrdered;
-    private final boolean useDVs;
 
     Context(
         Schema dataSchema,
@@ -805,7 +804,6 @@ private static class Context implements Serializable {
       this.queryId = info.queryId();
       this.useFanoutWriter = writeConf.useFanoutWriter(writeRequirements);
       this.inputOrdered = writeRequirements.hasOrdering();
-      this.useDVs = writeConf.useDVs();
     }
 
     Schema dataSchema() {
@@ -853,7 +851,7 @@ boolean inputOrdered() {
     }
 
     boolean useDVs() {
-      return useDVs;
+      return deleteFileFormat == FileFormat.PUFFIN;
     }
 
     int specIdOrdinal() {

File: spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/source/BaseRowReader.java
Patch:
@@ -32,9 +32,9 @@
 import org.apache.iceberg.orc.ORC;
 import org.apache.iceberg.parquet.Parquet;
 import org.apache.iceberg.relocated.com.google.common.collect.Sets;
-import org.apache.iceberg.spark.data.SparkAvroReader;
 import org.apache.iceberg.spark.data.SparkOrcReader;
 import org.apache.iceberg.spark.data.SparkParquetReaders;
+import org.apache.iceberg.spark.data.SparkPlannedAvroReader;
 import org.apache.iceberg.types.TypeUtil;
 import org.apache.spark.sql.catalyst.InternalRow;
 
@@ -77,7 +77,7 @@ private CloseableIterable<InternalRow> newAvroIterable(
         .reuseContainers()
         .project(projection)
         .split(start, length)
-        .createReaderFunc(readSchema -> new SparkAvroReader(projection, readSchema, idToConstant))
+        .createResolvingReader(schema -> SparkPlannedAvroReader.create(schema, idToConstant))
         .withNameMapping(nameMapping())
         .build();
   }

File: spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkAvroEnums.java
Patch:
@@ -78,7 +78,7 @@ public void writeAndValidateEnums() throws IOException {
     List<InternalRow> rows;
     try (AvroIterable<InternalRow> reader =
         Avro.read(Files.localInput(testFile))
-            .createReaderFunc(SparkAvroReader::new)
+            .createResolvingReader(SparkPlannedAvroReader::create)
             .project(schema)
             .build()) {
       rows = Lists.newArrayList(reader);

File: spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/source/SparkScanBuilder.java
Patch:
@@ -561,10 +561,11 @@ public Scan buildChangelogScan() {
 
     boolean emptyScan = false;
     if (startTimestamp != null) {
-      startSnapshotId = getStartSnapshotId(startTimestamp);
-      if (startSnapshotId == null && endTimestamp == null) {
+      if (table.currentSnapshot() == null
+          || startTimestamp > table.currentSnapshot().timestampMillis()) {
         emptyScan = true;
       }
+      startSnapshotId = getStartSnapshotId(startTimestamp);
     }
 
     if (endTimestamp != null) {

File: spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/source/SparkScanBuilder.java
Patch:
@@ -561,10 +561,11 @@ public Scan buildChangelogScan() {
 
     boolean emptyScan = false;
     if (startTimestamp != null) {
-      startSnapshotId = getStartSnapshotId(startTimestamp);
-      if (startSnapshotId == null && endTimestamp == null) {
+      if (table.currentSnapshot() == null
+          || startTimestamp > table.currentSnapshot().timestampMillis()) {
         emptyScan = true;
       }
+      startSnapshotId = getStartSnapshotId(startTimestamp);
     }
 
     if (endTimestamp != null) {

File: core/src/main/java/org/apache/iceberg/rest/auth/AuthProperties.java
Patch:
@@ -26,11 +26,14 @@ private AuthProperties() {}
 
   public static final String AUTH_TYPE_NONE = "none";
   public static final String AUTH_TYPE_BASIC = "basic";
+  public static final String AUTH_TYPE_OAUTH2 = "oauth2";
 
   public static final String AUTH_MANAGER_IMPL_NONE =
       "org.apache.iceberg.rest.auth.NoopAuthManager";
   public static final String AUTH_MANAGER_IMPL_BASIC =
       "org.apache.iceberg.rest.auth.BasicAuthManager";
+  public static final String AUTH_MANAGER_IMPL_OAUTH2 =
+      "org.apache.iceberg.rest.auth.OAuth2Manager";
 
   public static final String BASIC_USERNAME = "rest.auth.basic.username";
   public static final String BASIC_PASSWORD = "rest.auth.basic.password";

File: api/src/main/java/org/apache/iceberg/ScanTask.java
Patch:
@@ -28,7 +28,7 @@ public interface ScanTask extends Serializable {
    * @return the total number of bytes to read
    */
   default long sizeBytes() {
-    return 4 * 1028 * 1028; // 4 MB
+    return 4 * 1024 * 1024; // 4 MB
   }
 
   /**

File: core/src/main/java/org/apache/iceberg/CatalogProperties.java
Patch:
@@ -29,6 +29,7 @@ private CatalogProperties() {}
   public static final String WAREHOUSE_LOCATION = "warehouse";
   public static final String TABLE_DEFAULT_PREFIX = "table-default.";
   public static final String TABLE_OVERRIDE_PREFIX = "table-override.";
+  public static final String VIEW_DEFAULT_PREFIX = "view-default.";
   public static final String METRICS_REPORTER_IMPL = "metrics-reporter-impl";
 
   /**

File: core/src/test/java/org/apache/iceberg/jdbc/TestJdbcViewCatalog.java
Patch:
@@ -57,6 +57,8 @@ public void before() {
     properties.put(JdbcCatalog.PROPERTY_PREFIX + "password", "password");
     properties.put(CatalogProperties.WAREHOUSE_LOCATION, tableDir.toAbsolutePath().toString());
     properties.put(JdbcUtil.SCHEMA_VERSION_PROPERTY, JdbcUtil.SchemaVersion.V1.name());
+    properties.put(CatalogProperties.VIEW_DEFAULT_PREFIX + "key1", "catalog-default-key1");
+    properties.put(CatalogProperties.VIEW_DEFAULT_PREFIX + "key2", "catalog-default-key2");
 
     catalog = new JdbcCatalog();
     catalog.setConf(new Configuration());

File: flink/v1.18/flink/src/main/java/org/apache/iceberg/flink/sink/shuffle/DataStatisticsCoordinator.java
Patch:
@@ -370,7 +370,8 @@ public void resetToCheckpoint(long checkpointId, byte[] checkpointData) {
         "Restoring data statistic coordinator {} from checkpoint {}", operatorName, checkpointId);
     this.completedStatistics =
         StatisticsUtil.deserializeCompletedStatistics(
-            checkpointData, completedStatisticsSerializer);
+            checkpointData, (CompletedStatisticsSerializer) completedStatisticsSerializer);
+
     // recompute global statistics in case downstream parallelism changed
     this.globalStatistics =
         globalStatistics(

File: flink/v1.18/flink/src/test/java/org/apache/iceberg/flink/sink/shuffle/TestSortKeySerializerPrimitives.java
Patch:
@@ -80,8 +80,8 @@ public void testSerializationSize() throws Exception {
     byte[] serializedBytes = output.getCopyOfBuffer();
     assertThat(serializedBytes.length)
         .as(
-            "Serialized bytes for sort key should be 38 bytes (34 UUID text + 4 byte integer of string length")
-        .isEqualTo(38);
+            "Serialized bytes for sort key should be 39 bytes (34 UUID text + 4 byte integer of string length + 1 byte of isnull flag")
+        .isEqualTo(39);
 
     DataInputDeserializer input = new DataInputDeserializer(serializedBytes);
     SortKey deserialized = serializer.deserialize(input);

File: flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/sink/shuffle/DataStatisticsCoordinator.java
Patch:
@@ -370,7 +370,8 @@ public void resetToCheckpoint(long checkpointId, byte[] checkpointData) {
         "Restoring data statistic coordinator {} from checkpoint {}", operatorName, checkpointId);
     this.completedStatistics =
         StatisticsUtil.deserializeCompletedStatistics(
-            checkpointData, completedStatisticsSerializer);
+            checkpointData, (CompletedStatisticsSerializer) completedStatisticsSerializer);
+
     // recompute global statistics in case downstream parallelism changed
     this.globalStatistics =
         globalStatistics(

File: flink/v1.19/flink/src/test/java/org/apache/iceberg/flink/sink/shuffle/TestSortKeySerializerPrimitives.java
Patch:
@@ -80,8 +80,8 @@ public void testSerializationSize() throws Exception {
     byte[] serializedBytes = output.getCopyOfBuffer();
     assertThat(serializedBytes.length)
         .as(
-            "Serialized bytes for sort key should be 38 bytes (34 UUID text + 4 byte integer of string length")
-        .isEqualTo(38);
+            "Serialized bytes for sort key should be 39 bytes (34 UUID text + 4 byte integer of string length + 1 byte of isnull flag")
+        .isEqualTo(39);
 
     DataInputDeserializer input = new DataInputDeserializer(serializedBytes);
     SortKey deserialized = serializer.deserialize(input);

File: core/src/main/java/org/apache/iceberg/hadoop/HadoopTableOperations.java
Patch:
@@ -344,7 +344,7 @@ int findVersion() {
 
         return maxVersion;
       } catch (IOException io) {
-        LOG.warn("Error trying to recover version-hint.txt data for {}", versionHintFile, e);
+        LOG.warn("Error trying to recover the latest version number for {}", versionHintFile, io);
         return 0;
       }
     }

File: spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/source/BaseRowReader.java
Patch:
@@ -77,7 +77,7 @@ private CloseableIterable<InternalRow> newAvroIterable(
         .reuseContainers()
         .project(projection)
         .split(start, length)
-        .createReaderFunc(readSchema -> SparkPlannedAvroReader.create(projection, idToConstant))
+        .createResolvingReader(schema -> SparkPlannedAvroReader.create(schema, idToConstant))
         .withNameMapping(nameMapping())
         .build();
   }

File: core/src/main/java/org/apache/iceberg/data/avro/PlannedDataReader.java
Patch:
@@ -32,6 +32,7 @@
 import org.apache.iceberg.avro.SupportsRowPosition;
 import org.apache.iceberg.avro.ValueReader;
 import org.apache.iceberg.avro.ValueReaders;
+import org.apache.iceberg.data.GenericDataUtil;
 import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;
 import org.apache.iceberg.types.Type;
 import org.apache.iceberg.types.Types;
@@ -97,7 +98,8 @@ public ValueReader<?> record(Type partner, Schema record, List<ValueReader<?>> f
 
       Types.StructType expected = partner.asStructType();
       List<Pair<Integer, ValueReader<?>>> readPlan =
-          ValueReaders.buildReadPlan(expected, record, fieldReaders, idToConstant);
+          ValueReaders.buildReadPlan(
+              expected, record, fieldReaders, idToConstant, GenericDataUtil::internalToGeneric);
 
       return GenericReaders.struct(readPlan, expected);
     }

File: spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java
Patch:
@@ -287,14 +287,15 @@ private static String toExecutorLocation(BlockManagerId id) {
    * @param value a value that is an instance of {@link Type.TypeID#javaClass()}
    * @return the value converted for Spark
    */
-  public static Object convertConstant(Type type, Object value) {
+  public static Object internalToSpark(Type type, Object value) {
     if (value == null) {
       return null;
     }
 
     switch (type.typeId()) {
       case DECIMAL:
         return Decimal.apply((BigDecimal) value);
+      case UUID:
       case STRING:
         if (value instanceof Utf8) {
           Utf8 utf8 = (Utf8) value;
@@ -325,7 +326,7 @@ public static Object convertConstant(Type type, Object value) {
           Types.NestedField field = fields.get(index);
           Type fieldType = field.type();
           values[index] =
-              convertConstant(fieldType, struct.get(index, fieldType.typeId().javaClass()));
+              internalToSpark(fieldType, struct.get(index, fieldType.typeId().javaClass()));
         }
 
         return new GenericInternalRow(values);

File: spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/data/SparkParquetReaders.java
Patch:
@@ -186,7 +186,7 @@ public ParquetValueReader<?> struct(
         } else if (field.initialDefault() != null) {
           reorderedFields.add(
               ParquetValueReaders.constant(
-                  SparkUtil.convertConstant(field.type(), field.initialDefault()),
+                  SparkUtil.internalToSpark(field.type(), field.initialDefault()),
                   maxDefinitionLevelsById.getOrDefault(id, defaultMaxDefinitionLevel)));
           types.add(typesById.get(id));
         } else if (field.isOptional()) {

File: spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/data/SparkPlannedAvroReader.java
Patch:
@@ -32,6 +32,7 @@
 import org.apache.iceberg.avro.ValueReader;
 import org.apache.iceberg.avro.ValueReaders;
 import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;
+import org.apache.iceberg.spark.SparkUtil;
 import org.apache.iceberg.types.Type;
 import org.apache.iceberg.types.Types;
 import org.apache.iceberg.util.Pair;
@@ -97,7 +98,8 @@ public ValueReader<?> record(Type partner, Schema record, List<ValueReader<?>> f
 
       Types.StructType expected = partner.asStructType();
       List<Pair<Integer, ValueReader<?>>> readPlan =
-          ValueReaders.buildReadPlan(expected, record, fieldReaders, idToConstant);
+          ValueReaders.buildReadPlan(
+              expected, record, fieldReaders, idToConstant, SparkUtil::internalToSpark);
 
       // TODO: should this pass expected so that struct.get can reuse containers?
       return SparkValueReaders.struct(readPlan, expected.fields().size());

File: spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/source/BaseReader.java
Patch:
@@ -184,9 +184,9 @@ private Map<String, InputFile> inputFiles() {
   protected Map<Integer, ?> constantsMap(ContentScanTask<?> task, Schema readSchema) {
     if (readSchema.findField(MetadataColumns.PARTITION_COLUMN_ID) != null) {
       StructType partitionType = Partitioning.partitionType(table);
-      return PartitionUtil.constantsMap(task, partitionType, SparkUtil::convertConstant);
+      return PartitionUtil.constantsMap(task, partitionType, SparkUtil::internalToSpark);
     } else {
-      return PartitionUtil.constantsMap(task, SparkUtil::convertConstant);
+      return PartitionUtil.constantsMap(task, SparkUtil::internalToSpark);
     }
   }
 

File: core/src/main/java/org/apache/iceberg/data/avro/IcebergDecoder.java
Patch:
@@ -103,9 +103,7 @@ public void addSchema(org.apache.iceberg.Schema writeSchema) {
 
   private void addSchema(Schema writeSchema) {
     long fp = SchemaNormalization.parsingFingerprint64(writeSchema);
-    RawDecoder decoder =
-        new RawDecoder<>(
-            readSchema, avroSchema -> DataReader.create(readSchema, avroSchema), writeSchema);
+    RawDecoder<D> decoder = RawDecoder.create(readSchema, PlannedDataReader::create, writeSchema);
     decoders.put(fp, decoder);
   }
 

File: core/src/main/java/org/apache/iceberg/encryption/KeyMetadataDecoder.java
Patch:
@@ -66,7 +66,7 @@ public StandardKeyMetadata decode(InputStream stream, StandardKeyMetadata reuse)
     RawDecoder<StandardKeyMetadata> decoder = decoders.get(writeSchemaVersion);
 
     if (decoder == null) {
-      decoder = new RawDecoder<>(readSchema, GenericAvroReader::create, writeSchema);
+      decoder = RawDecoder.create(readSchema, GenericAvroReader::create, writeSchema);
 
       decoders.put(writeSchemaVersion, decoder);
     }

File: core/src/test/java/org/apache/iceberg/avro/TestAvroDataWriter.java
Patch:
@@ -32,7 +32,7 @@
 import org.apache.iceberg.SortOrder;
 import org.apache.iceberg.data.GenericRecord;
 import org.apache.iceberg.data.Record;
-import org.apache.iceberg.data.avro.DataReader;
+import org.apache.iceberg.data.avro.PlannedDataReader;
 import org.apache.iceberg.io.DataWriter;
 import org.apache.iceberg.io.OutputFile;
 import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;
@@ -103,7 +103,7 @@ public void testDataWriter() throws IOException {
     try (AvroIterable<Record> reader =
         Avro.read(file.toInputFile())
             .project(SCHEMA)
-            .createReaderFunc(DataReader::create)
+            .createResolvingReader(PlannedDataReader::create)
             .build()) {
       writtenRecords = Lists.newArrayList(reader);
     }

File: core/src/test/java/org/apache/iceberg/avro/TestAvroFileSplit.java
Patch:
@@ -30,8 +30,8 @@
 import org.apache.iceberg.TableProperties;
 import org.apache.iceberg.data.GenericRecord;
 import org.apache.iceberg.data.Record;
-import org.apache.iceberg.data.avro.DataReader;
 import org.apache.iceberg.data.avro.DataWriter;
+import org.apache.iceberg.data.avro.PlannedDataReader;
 import org.apache.iceberg.io.FileAppender;
 import org.apache.iceberg.io.InputFile;
 import org.apache.iceberg.io.OutputFile;
@@ -186,7 +186,7 @@ public List<Record> readAvro(InputFile in, Schema projection, long start, long l
       throws IOException {
     try (AvroIterable<Record> reader =
         Avro.read(in)
-            .createReaderFunc(DataReader::create)
+            .createResolvingReader(PlannedDataReader::create)
             .split(start, length)
             .project(projection)
             .build()) {

File: core/src/test/java/org/apache/iceberg/avro/TestEncryptedAvroFileSplit.java
Patch:
@@ -30,8 +30,8 @@
 import org.apache.iceberg.TableProperties;
 import org.apache.iceberg.data.GenericRecord;
 import org.apache.iceberg.data.Record;
-import org.apache.iceberg.data.avro.DataReader;
 import org.apache.iceberg.data.avro.DataWriter;
+import org.apache.iceberg.data.avro.PlannedDataReader;
 import org.apache.iceberg.encryption.EncryptedFiles;
 import org.apache.iceberg.encryption.EncryptedInputFile;
 import org.apache.iceberg.encryption.EncryptedOutputFile;
@@ -199,7 +199,7 @@ public List<Record> readAvro(InputFile in, Schema projection, long start, long l
       throws IOException {
     try (AvroIterable<Record> reader =
         Avro.read(in)
-            .createReaderFunc(DataReader::create)
+            .createResolvingReader(PlannedDataReader::create)
             .split(start, length)
             .project(projection)
             .build()) {

File: data/src/main/java/org/apache/iceberg/data/BaseDeleteLoader.java
Patch:
@@ -31,7 +31,7 @@
 import org.apache.iceberg.Schema;
 import org.apache.iceberg.StructLike;
 import org.apache.iceberg.avro.Avro;
-import org.apache.iceberg.data.avro.DataReader;
+import org.apache.iceberg.data.avro.PlannedDataReader;
 import org.apache.iceberg.data.orc.GenericOrcReader;
 import org.apache.iceberg.data.parquet.GenericParquetReaders;
 import org.apache.iceberg.deletes.Deletes;
@@ -235,7 +235,7 @@ private CloseableIterable<Record> openDeletes(
         return Avro.read(inputFile)
             .project(projection)
             .reuseContainers()
-            .createReaderFunc(DataReader::create)
+            .createResolvingReader(PlannedDataReader::create)
             .build();
 
       case PARQUET:

File: data/src/main/java/org/apache/iceberg/data/GenericReader.java
Patch:
@@ -26,7 +26,7 @@
 import org.apache.iceberg.Schema;
 import org.apache.iceberg.TableScan;
 import org.apache.iceberg.avro.Avro;
-import org.apache.iceberg.data.avro.DataReader;
+import org.apache.iceberg.data.avro.PlannedDataReader;
 import org.apache.iceberg.data.orc.GenericOrcReader;
 import org.apache.iceberg.data.parquet.GenericParquetReaders;
 import org.apache.iceberg.expressions.Evaluator;
@@ -101,8 +101,7 @@ private CloseableIterable<Record> openFile(FileScanTask task, Schema fileProject
         Avro.ReadBuilder avro =
             Avro.read(input)
                 .project(fileProjection)
-                .createReaderFunc(
-                    avroSchema -> DataReader.create(fileProjection, avroSchema, partition))
+                .createResolvingReader(schema -> PlannedDataReader.create(schema, partition))
                 .split(task.start(), task.length());
 
         if (reuseContainers) {

File: data/src/test/java/org/apache/iceberg/data/avro/TestGenericReadProjection.java
Patch:
@@ -48,7 +48,7 @@ protected Record writeAndRead(String desc, Schema writeSchema, Schema readSchema
     Iterable<Record> records =
         Avro.read(Files.localInput(file))
             .project(readSchema)
-            .createReaderFunc(DataReader::create)
+            .createResolvingReader(PlannedDataReader::create)
             .build();
 
     return Iterables.getOnlyElement(records);

File: flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/data/AbstractTestFlinkAvroReaderWriter.java
Patch:
@@ -36,8 +36,8 @@
 import org.apache.iceberg.data.GenericRecord;
 import org.apache.iceberg.data.RandomGenericData;
 import org.apache.iceberg.data.Record;
-import org.apache.iceberg.data.avro.DataReader;
 import org.apache.iceberg.data.avro.DataWriter;
+import org.apache.iceberg.data.avro.PlannedDataReader;
 import org.apache.iceberg.flink.FlinkSchemaUtil;
 import org.apache.iceberg.flink.TestHelpers;
 import org.apache.iceberg.io.CloseableIterable;
@@ -116,7 +116,7 @@ private void writeAndValidate(Schema schema, List<Record> expectedRecords, int n
     try (CloseableIterable<Record> reader =
         Avro.read(Files.localInput(rowDataFile))
             .project(schema)
-            .createReaderFunc(DataReader::create)
+            .createResolvingReader(PlannedDataReader::create)
             .build()) {
       Iterator<RowData> expected = expectedRows.iterator();
       Iterator<Record> records = reader.iterator();

File: spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewritePositionDeleteFilesAction.java
Patch:
@@ -275,7 +275,7 @@ public void testRewriteFilter() throws Exception {
             .execute();
 
     List<DeleteFile> newDeleteFiles = except(deleteFiles(table), deleteFiles);
-    assertThat(newDeleteFiles).as("Should have 4 delete files").hasSize(2);
+    assertThat(newDeleteFiles).as("Delete files").hasSize(2);
 
     List<DeleteFile> expectedRewrittenFiles =
         filterFiles(table, deleteFiles, ImmutableList.of(1), ImmutableList.of(2));
@@ -469,7 +469,7 @@ public void testRewriteFilterRemoveDangling() throws Exception {
             .execute();
 
     List<DeleteFile> newDeleteFiles = except(deleteFiles(table), deleteFiles);
-    assertThat(newDeleteFiles).as("Should have 2 new delete files").hasSize(0);
+    assertThat(newDeleteFiles).as("New delete files").hasSize(0);
 
     List<DeleteFile> expectedRewrittenFiles =
         filterFiles(table, deleteFiles, ImmutableList.of(0), ImmutableList.of(1));

File: spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/PartitionedWritesTestBase.java
Patch:
@@ -53,7 +53,7 @@ public void removeTables() {
   @TestTemplate
   public void testInsertAppend() {
     assertThat(scalarSql("SELECT count(*) FROM %s", selectTarget()))
-        .as("Should have 5 rows after insert")
+        .as("Rows before insert")
         .isEqualTo(3L);
 
     sql("INSERT INTO %s VALUES (4, 'd'), (5, 'e')", commitTarget());
@@ -74,7 +74,7 @@ public void testInsertAppend() {
   @TestTemplate
   public void testInsertOverwrite() {
     assertThat(scalarSql("SELECT count(*) FROM %s", selectTarget()))
-        .as("Should have 5 rows after insert")
+        .as("Rows before overwrite")
         .isEqualTo(3L);
 
     // 4 and 5 replace 3 in the partition (id - (id % 3)) = 3

File: flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/shuffle/DataStatisticsCoordinator.java
Patch:
@@ -370,7 +370,8 @@ public void resetToCheckpoint(long checkpointId, byte[] checkpointData) {
         "Restoring data statistic coordinator {} from checkpoint {}", operatorName, checkpointId);
     this.completedStatistics =
         StatisticsUtil.deserializeCompletedStatistics(
-            checkpointData, completedStatisticsSerializer);
+            checkpointData, (CompletedStatisticsSerializer) completedStatisticsSerializer);
+
     // recompute global statistics in case downstream parallelism changed
     this.globalStatistics =
         globalStatistics(

File: flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/sink/shuffle/TestSortKeySerializerPrimitives.java
Patch:
@@ -80,8 +80,8 @@ public void testSerializationSize() throws Exception {
     byte[] serializedBytes = output.getCopyOfBuffer();
     assertThat(serializedBytes.length)
         .as(
-            "Serialized bytes for sort key should be 38 bytes (34 UUID text + 4 byte integer of string length")
-        .isEqualTo(38);
+            "Serialized bytes for sort key should be 39 bytes (34 UUID text + 4 byte integer of string length + 1 byte of isnull flag")
+        .isEqualTo(39);
 
     DataInputDeserializer input = new DataInputDeserializer(serializedBytes);
     SortKey deserialized = serializer.deserialize(input);

File: core/src/main/java/org/apache/iceberg/rest/RESTSessionCatalog.java
Patch:
@@ -228,12 +228,12 @@ public void initialize(String name, Map<String, String> unresolved) {
         && (hasInitToken || hasCredential)
         && !PropertyUtil.propertyAsBoolean(props, "rest.sigv4-enabled", false)) {
       LOG.warn(
-          "Iceberg REST client is missing the OAuth2 server URI configuration and defaults to {}{}. "
+          "Iceberg REST client is missing the OAuth2 server URI configuration and defaults to {}/{}. "
               + "This automatic fallback will be removed in a future Iceberg release."
               + "It is recommended to configure the OAuth2 endpoint using the '{}' property to be prepared. "
               + "This warning will disappear if the OAuth2 endpoint is explicitly configured. "
               + "See https://github.com/apache/iceberg/issues/10537",
-          props.get(CatalogProperties.URI),
+          RESTUtil.stripTrailingSlash(props.get(CatalogProperties.URI)),
           ResourcePaths.tokens(),
           OAuth2Properties.OAUTH2_SERVER_URI);
     }

File: api/src/main/java/org/apache/iceberg/ManifestFile.java
Patch:
@@ -49,7 +49,7 @@ public interface ManifestFile {
           Types.LongType.get(),
           "Lowest sequence number in the manifest");
   Types.NestedField SNAPSHOT_ID =
-      optional(
+      required(
           503, "added_snapshot_id", Types.LongType.get(), "Snapshot ID that added the manifest");
   Types.NestedField ADDED_FILES_COUNT =
       optional(504, "added_files_count", Types.IntegerType.get(), "Added entry count");

File: core/src/main/java/org/apache/iceberg/BaseSnapshot.java
Patch:
@@ -138,7 +138,8 @@ private void cacheManifests(FileIO fileIO) {
       allManifests =
           Lists.transform(
               Arrays.asList(v1ManifestLocations),
-              location -> new GenericManifestFile(fileIO.newInputFile(location), 0));
+              location ->
+                  new GenericManifestFile(fileIO.newInputFile(location), 0, this.snapshotId));
     }
 
     if (allManifests == null) {

File: core/src/main/java/org/apache/iceberg/V2Metadata.java
Patch:
@@ -40,7 +40,7 @@ private V2Metadata() {}
           ManifestFile.MANIFEST_CONTENT.asRequired(),
           ManifestFile.SEQUENCE_NUMBER.asRequired(),
           ManifestFile.MIN_SEQUENCE_NUMBER.asRequired(),
-          ManifestFile.SNAPSHOT_ID.asRequired(),
+          ManifestFile.SNAPSHOT_ID,
           ManifestFile.ADDED_FILES_COUNT.asRequired(),
           ManifestFile.EXISTING_FILES_COUNT.asRequired(),
           ManifestFile.DELETED_FILES_COUNT.asRequired(),

File: core/src/main/java/org/apache/iceberg/V3Metadata.java
Patch:
@@ -40,7 +40,7 @@ private V3Metadata() {}
           ManifestFile.MANIFEST_CONTENT.asRequired(),
           ManifestFile.SEQUENCE_NUMBER.asRequired(),
           ManifestFile.MIN_SEQUENCE_NUMBER.asRequired(),
-          ManifestFile.SNAPSHOT_ID.asRequired(),
+          ManifestFile.SNAPSHOT_ID,
           ManifestFile.ADDED_FILES_COUNT.asRequired(),
           ManifestFile.EXISTING_FILES_COUNT.asRequired(),
           ManifestFile.DELETED_FILES_COUNT.asRequired(),

File: core/src/test/java/org/apache/iceberg/TestMetadataUpdateParser.java
Patch:
@@ -1244,8 +1244,8 @@ private String createManifestListWithManifestFiles(long snapshotId, Long parentS
 
     List<ManifestFile> manifests =
         ImmutableList.of(
-            new GenericManifestFile(localInput("file:/tmp/manifest1.avro"), 0),
-            new GenericManifestFile(localInput("file:/tmp/manifest2.avro"), 0));
+            new GenericManifestFile(localInput("file:/tmp/manifest1.avro"), 0, snapshotId),
+            new GenericManifestFile(localInput("file:/tmp/manifest2.avro"), 0, snapshotId));
 
     try (ManifestListWriter writer =
         ManifestLists.write(1, Files.localOutput(manifestList), snapshotId, parentSnapshotId, 0)) {

File: core/src/test/java/org/apache/iceberg/TestSnapshotJson.java
Patch:
@@ -166,8 +166,8 @@ private String createManifestListWithManifestFiles(long snapshotId, Long parentS
 
     List<ManifestFile> manifests =
         ImmutableList.of(
-            new GenericManifestFile(localInput("file:/tmp/manifest1.avro"), 0),
-            new GenericManifestFile(localInput("file:/tmp/manifest2.avro"), 0));
+            new GenericManifestFile(localInput("file:/tmp/manifest1.avro"), 0, snapshotId),
+            new GenericManifestFile(localInput("file:/tmp/manifest2.avro"), 0, snapshotId));
 
     try (ManifestListWriter writer =
         ManifestLists.write(1, Files.localOutput(manifestList), snapshotId, parentSnapshotId, 0)) {

File: core/src/test/java/org/apache/iceberg/TestTableMetadata.java
Patch:
@@ -1663,7 +1663,8 @@ private String createManifestListWithManifestFile(
     try (ManifestListWriter writer =
         ManifestLists.write(1, Files.localOutput(manifestList), snapshotId, parentSnapshotId, 0)) {
       writer.addAll(
-          ImmutableList.of(new GenericManifestFile(localInput(manifestFile), SPEC_5.specId())));
+          ImmutableList.of(
+              new GenericManifestFile(localInput(manifestFile), SPEC_5.specId(), snapshotId)));
     }
 
     return localInput(manifestList).location();

File: kafka-connect/kafka-connect/src/main/java/org/apache/iceberg/connect/channel/Worker.java
Patch:
@@ -51,7 +51,7 @@ class Worker extends Channel {
     // pass transient consumer group ID to which we never commit offsets
     super(
         "worker",
-        IcebergSinkConfig.DEFAULT_CONTROL_GROUP_PREFIX + UUID.randomUUID(),
+        config.controlGroupIdPrefix() + UUID.randomUUID(),
         config,
         clientFactory,
         context);

File: aws/src/test/java/org/apache/iceberg/aws/glue/TestIcebergToGlueConverter.java
Patch:
@@ -238,7 +238,7 @@ public void testSetTableInputInformationWithRemovedColumns() {
 
     Schema newSchema =
         new Schema(Types.NestedField.required(1, "x", Types.StringType.get(), "comment1"));
-    tableMetadata = tableMetadata.updateSchema(newSchema, 3);
+    tableMetadata = tableMetadata.updateSchema(newSchema);
     IcebergToGlueConverter.setTableInputInformation(actualTableInputBuilder, tableMetadata);
     TableInput actualTableInput = actualTableInputBuilder.build();
 

File: core/src/main/java/org/apache/iceberg/SchemaUpdate.java
Patch:
@@ -444,7 +444,7 @@ public Schema apply() {
 
   @Override
   public void commit() {
-    TableMetadata update = applyChangesToMetadata(base.updateSchema(apply(), lastColumnId));
+    TableMetadata update = applyChangesToMetadata(base.updateSchema(apply()));
     ops.commit(base, update);
   }
 

File: core/src/main/java/org/apache/iceberg/rest/RESTSessionCatalog.java
Patch:
@@ -954,7 +954,7 @@ private static List<MetadataUpdate> createChanges(TableMetadata meta) {
     changes.add(new MetadataUpdate.UpgradeFormatVersion(meta.formatVersion()));
 
     Schema schema = meta.schema();
-    changes.add(new MetadataUpdate.AddSchema(schema, schema.highestFieldId()));
+    changes.add(new MetadataUpdate.AddSchema(schema));
     changes.add(new MetadataUpdate.SetCurrentSchema(-1));
 
     PartitionSpec spec = meta.spec();

File: spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReadProjection.java
Patch:
@@ -152,8 +152,7 @@ protected Record writeAndRead(String desc, Schema writeSchema, Schema readSchema
       Schema expectedSchema = reassignIds(readSchema, idMapping);
 
       // Set the schema to the expected schema directly to simulate the table schema evolving
-      TestTables.replaceMetadata(
-          desc, TestTables.readMetadata(desc).updateSchema(expectedSchema, 100));
+      TestTables.replaceMetadata(desc, TestTables.readMetadata(desc).updateSchema(expectedSchema));
 
       Dataset<Row> df =
           spark

File: spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestComputeTableStatsAction.java
Patch:
@@ -110,6 +110,7 @@ public void testComputeTableStatsAction() throws NoSuchTableException, ParseExce
             new SimpleRecord(4, "d"));
     spark.createDataset(records, Encoders.bean(SimpleRecord.class)).writeTo(tableName).append();
     SparkActions actions = SparkActions.get();
+    table.refresh();
     ComputeTableStats.Result results =
         actions.computeTableStats(table).columns("id", "data").execute();
     assertThat(results).isNotNull();

File: hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java
Patch:
@@ -292,6 +292,7 @@ protected void doCommit(TableMetadata base, TableMetadata metadata) {
             database,
             tableName,
             e);
+        commitStatus = BaseMetastoreOperations.CommitStatus.UNKNOWN;
         commitStatus =
             BaseMetastoreOperations.CommitStatus.valueOf(
                 checkCommitStatus(newMetadataLocation, metadata).name());

File: hive-metastore/src/main/java/org/apache/iceberg/hive/HiveViewOperations.java
Patch:
@@ -33,6 +33,7 @@
 import org.apache.hadoop.hive.metastore.api.InvalidObjectException;
 import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;
 import org.apache.hadoop.hive.metastore.api.Table;
+import org.apache.iceberg.BaseMetastoreOperations;
 import org.apache.iceberg.BaseMetastoreTableOperations;
 import org.apache.iceberg.CatalogUtil;
 import org.apache.iceberg.ClientPool;
@@ -226,6 +227,7 @@ public void doCommit(ViewMetadata base, ViewMetadata metadata) {
             database,
             viewName,
             e);
+        commitStatus = BaseMetastoreOperations.CommitStatus.UNKNOWN;
         commitStatus =
             checkCommitStatus(
                 viewName,

File: data/src/main/java/org/apache/iceberg/data/TableMigrationUtil.java
Patch:
@@ -25,6 +25,7 @@
 import java.util.Map;
 import java.util.concurrent.ExecutorService;
 import java.util.stream.Collectors;
+import javax.annotation.Nullable;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
@@ -263,6 +264,7 @@ private static DataFile buildDataFile(
    * <p><b>Important:</b> Callers are responsible for shutting down the returned executor service
    * when it is no longer needed to prevent resource leaks.
    */
+  @Nullable
   public static ExecutorService migrationService(int parallelism) {
     return parallelism == 1 ? null : ThreadPools.newFixedThreadPool("table-migration", parallelism);
   }

File: spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/procedures/MigrateTableProcedure.java
Patch:
@@ -22,6 +22,7 @@
 import org.apache.iceberg.actions.MigrateTable;
 import org.apache.iceberg.relocated.com.google.common.base.Preconditions;
 import org.apache.iceberg.relocated.com.google.common.collect.Maps;
+import org.apache.iceberg.spark.SparkTableUtil;
 import org.apache.iceberg.spark.actions.MigrateTableSparkAction;
 import org.apache.iceberg.spark.actions.SparkActions;
 import org.apache.iceberg.spark.procedures.SparkProcedures.ProcedureBuilder;
@@ -110,7 +111,7 @@ public InternalRow[] call(InternalRow args) {
       int parallelism = args.getInt(4);
       Preconditions.checkArgument(parallelism > 0, "Parallelism should be larger than 0");
       migrateTableSparkAction =
-          migrateTableSparkAction.executeWith(executorService(parallelism, "table-migration"));
+          migrateTableSparkAction.executeWith(SparkTableUtil.migrationService(parallelism));
     }
 
     MigrateTable.Result result = migrateTableSparkAction.execute();

File: spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/procedures/SnapshotTableProcedure.java
Patch:
@@ -22,6 +22,7 @@
 import org.apache.iceberg.actions.SnapshotTable;
 import org.apache.iceberg.relocated.com.google.common.base.Preconditions;
 import org.apache.iceberg.relocated.com.google.common.collect.Maps;
+import org.apache.iceberg.spark.SparkTableUtil;
 import org.apache.iceberg.spark.actions.SparkActions;
 import org.apache.spark.sql.catalyst.InternalRow;
 import org.apache.spark.sql.connector.catalog.TableCatalog;
@@ -106,7 +107,7 @@ public InternalRow[] call(InternalRow args) {
     if (!args.isNullAt(4)) {
       int parallelism = args.getInt(4);
       Preconditions.checkArgument(parallelism > 0, "Parallelism should be larger than 0");
-      action = action.executeWith(executorService(parallelism, "table-snapshot"));
+      action = action.executeWith(SparkTableUtil.migrationService(parallelism));
     }
 
     SnapshotTable.Result result = action.tableProperties(properties).execute();

File: api/src/main/java/org/apache/iceberg/Schema.java
Patch:
@@ -60,7 +60,7 @@ public class Schema implements Serializable {
 
   @VisibleForTesting
   static final Map<Type.TypeID, Integer> MIN_FORMAT_VERSIONS =
-      ImmutableMap.of(Type.TypeID.TIMESTAMP_NANO, 3);
+      ImmutableMap.of(Type.TypeID.TIMESTAMP_NANO, 3, Type.TypeID.VARIANT, 3);
 
   private final StructType struct;
   private final int schemaId;

File: api/src/main/java/org/apache/iceberg/expressions/ExpressionUtil.java
Patch:
@@ -534,7 +534,8 @@ private static String sanitize(Type type, Object value, long now, int today) {
       case DECIMAL:
       case FIXED:
       case BINARY:
-        // for boolean, uuid, decimal, fixed, and binary, match the string result
+      case VARIANT:
+        // for boolean, uuid, decimal, fixed, variant, and binary, match the string result
         return sanitizeSimpleString(value.toString());
     }
     throw new UnsupportedOperationException(
@@ -562,7 +563,7 @@ private static String sanitize(Literal<?> literal, long now, int today) {
     } else if (literal instanceof Literals.DoubleLiteral) {
       return sanitizeNumber(((Literals.DoubleLiteral) literal).value(), "float");
     } else {
-      // for uuid, decimal, fixed, and binary, match the string result
+      // for uuid, decimal, fixed, variant, and binary, match the string result
       return sanitizeSimpleString(literal.value().toString());
     }
   }

File: api/src/main/java/org/apache/iceberg/transforms/Identity.java
Patch:
@@ -38,6 +38,9 @@ class Identity<T> implements Transform<T, T> {
    */
   @Deprecated
   public static <I> Identity<I> get(Type type) {
+    Preconditions.checkArgument(
+        type.typeId() != Type.TypeID.VARIANT, "Unsupported type for identity: %s", type);
+
     return new Identity<>(type);
   }
 

File: api/src/main/java/org/apache/iceberg/types/Type.java
Patch:
@@ -45,7 +45,8 @@ enum TypeID {
     DECIMAL(BigDecimal.class),
     STRUCT(StructLike.class),
     LIST(List.class),
-    MAP(Map.class);
+    MAP(Map.class),
+    VARIANT(Object.class);
 
     private final Class<?> javaClass;
 

File: core/src/main/java/org/apache/iceberg/TableMetadata.java
Patch:
@@ -295,8 +295,9 @@ public String toString() {
         sortOrders != null && !sortOrders.isEmpty(), "Sort orders cannot be null or empty");
     Preconditions.checkArgument(
         formatVersion <= SUPPORTED_TABLE_FORMAT_VERSION,
-        "Unsupported format version: v%s",
-        formatVersion);
+        "Unsupported format version: v%s (supported: v%s)",
+        formatVersion,
+        SUPPORTED_TABLE_FORMAT_VERSION);
     Preconditions.checkArgument(
         formatVersion == 1 || uuid != null, "UUID is required in format v%s", formatVersion);
     Preconditions.checkArgument(

File: flink/v1.18/flink/src/main/java/org/apache/iceberg/flink/source/split/SplitComparators.java
Patch:
@@ -18,6 +18,7 @@
  */
 package org.apache.iceberg.flink.source.split;
 
+import org.apache.iceberg.flink.FlinkReadOptions;
 import org.apache.iceberg.flink.source.reader.SplitWatermarkExtractor;
 import org.apache.iceberg.relocated.com.google.common.base.Preconditions;
 
@@ -35,7 +36,8 @@ public static SerializableComparator<IcebergSourceSplit> fileSequenceNumber() {
     return (IcebergSourceSplit o1, IcebergSourceSplit o2) -> {
       Preconditions.checkArgument(
           o1.task().files().size() == 1 && o2.task().files().size() == 1,
-          "Could not compare combined task. Please use 'split-open-file-cost' to prevent combining multiple files to a split");
+          "Could not compare combined task. Please use '%s' to prevent combining multiple files to a split",
+          FlinkReadOptions.SPLIT_FILE_OPEN_COST);
 
       Long seq1 = o1.task().files().iterator().next().file().fileSequenceNumber();
       Long seq2 = o2.task().files().iterator().next().file().fileSequenceNumber();

File: flink/v1.18/flink/src/test/java/org/apache/iceberg/flink/source/assigner/TestFileSequenceNumberBasedSplitAssigner.java
Patch:
@@ -43,7 +43,7 @@ public void testMultipleFilesInAnIcebergSplit() {
             () -> assigner.onDiscoveredSplits(createSplits(4, 2, "2")),
             "Multiple files in a split is not allowed")
         .isInstanceOf(IllegalArgumentException.class)
-        .hasMessageContaining("Please use 'split-open-file-cost'");
+        .hasMessageContaining("Please use 'split-file-open-cost'");
   }
 
   /** Test sorted splits */

File: flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/source/split/SplitComparators.java
Patch:
@@ -18,6 +18,7 @@
  */
 package org.apache.iceberg.flink.source.split;
 
+import org.apache.iceberg.flink.FlinkReadOptions;
 import org.apache.iceberg.flink.source.reader.SplitWatermarkExtractor;
 import org.apache.iceberg.relocated.com.google.common.base.Preconditions;
 
@@ -35,7 +36,8 @@ public static SerializableComparator<IcebergSourceSplit> fileSequenceNumber() {
     return (IcebergSourceSplit o1, IcebergSourceSplit o2) -> {
       Preconditions.checkArgument(
           o1.task().files().size() == 1 && o2.task().files().size() == 1,
-          "Could not compare combined task. Please use 'split-open-file-cost' to prevent combining multiple files to a split");
+          "Could not compare combined task. Please use '%s' to prevent combining multiple files to a split",
+          FlinkReadOptions.SPLIT_FILE_OPEN_COST);
 
       Long seq1 = o1.task().files().iterator().next().file().fileSequenceNumber();
       Long seq2 = o2.task().files().iterator().next().file().fileSequenceNumber();

File: flink/v1.19/flink/src/test/java/org/apache/iceberg/flink/source/assigner/TestFileSequenceNumberBasedSplitAssigner.java
Patch:
@@ -43,7 +43,7 @@ public void testMultipleFilesInAnIcebergSplit() {
             () -> assigner.onDiscoveredSplits(createSplits(4, 2, "2")),
             "Multiple files in a split is not allowed")
         .isInstanceOf(IllegalArgumentException.class)
-        .hasMessageContaining("Please use 'split-open-file-cost'");
+        .hasMessageContaining("Please use 'split-file-open-cost'");
   }
 
   /** Test sorted splits */

File: flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/source/split/SplitComparators.java
Patch:
@@ -18,6 +18,7 @@
  */
 package org.apache.iceberg.flink.source.split;
 
+import org.apache.iceberg.flink.FlinkReadOptions;
 import org.apache.iceberg.flink.source.reader.SplitWatermarkExtractor;
 import org.apache.iceberg.relocated.com.google.common.base.Preconditions;
 
@@ -35,7 +36,8 @@ public static SerializableComparator<IcebergSourceSplit> fileSequenceNumber() {
     return (IcebergSourceSplit o1, IcebergSourceSplit o2) -> {
       Preconditions.checkArgument(
           o1.task().files().size() == 1 && o2.task().files().size() == 1,
-          "Could not compare combined task. Please use 'split-open-file-cost' to prevent combining multiple files to a split");
+          "Could not compare combined task. Please use '%s' to prevent combining multiple files to a split",
+          FlinkReadOptions.SPLIT_FILE_OPEN_COST);
 
       Long seq1 = o1.task().files().iterator().next().file().fileSequenceNumber();
       Long seq2 = o2.task().files().iterator().next().file().fileSequenceNumber();

File: flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/source/assigner/TestFileSequenceNumberBasedSplitAssigner.java
Patch:
@@ -43,7 +43,7 @@ public void testMultipleFilesInAnIcebergSplit() {
             () -> assigner.onDiscoveredSplits(createSplits(4, 2, "2")),
             "Multiple files in a split is not allowed")
         .isInstanceOf(IllegalArgumentException.class)
-        .hasMessageContaining("Please use 'split-open-file-cost'");
+        .hasMessageContaining("Please use 'split-file-open-cost'");
   }
 
   /** Test sorted splits */

File: core/src/test/java/org/apache/iceberg/FileGenerationUtil.java
Patch:
@@ -102,13 +102,13 @@ public static DeleteFile generateEqualityDeleteFile(Table table, StructLike part
   }
 
   public static DeleteFile generatePositionDeleteFile(Table table, DataFile dataFile) {
-    PartitionSpec spec = table.spec();
+    PartitionSpec spec = table.specs().get(dataFile.specId());
     StructLike partition = dataFile.partition();
     LocationProvider locations = table.locationProvider();
     String path = locations.newDataLocation(spec, partition, generateFileName());
     long fileSize = generateFileSize();
     Metrics metrics = generatePositionDeleteMetrics(dataFile);
-    return FileMetadata.deleteFileBuilder(table.spec())
+    return FileMetadata.deleteFileBuilder(spec)
         .ofPositionDeletes()
         .withPath(path)
         .withPartition(partition)

File: core/src/test/java/org/apache/iceberg/TestTableMetadata.java
Patch:
@@ -1672,7 +1672,8 @@ public void testV3TimestampNanoTypeSupport() {
                       unsupportedFormatVersion))
           .isInstanceOf(IllegalStateException.class)
           .hasMessage(
-              "Invalid type in v%s schema: struct.ts_nanos timestamptz_ns is not supported until v3",
+              "Invalid schema for v%s:\n"
+                  + "- Invalid type for struct.ts_nanos: timestamptz_ns is not supported until v3",
               unsupportedFormatVersion);
     }
 

File: flink/v1.19/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkRangeDistributionBucketing.java
Patch:
@@ -58,9 +58,9 @@
 import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;
 import org.apache.iceberg.relocated.com.google.common.collect.Lists;
 import org.apache.iceberg.types.Types;
-import org.junit.Ignore;
 import org.junit.jupiter.api.AfterEach;
 import org.junit.jupiter.api.BeforeEach;
+import org.junit.jupiter.api.Disabled;
 import org.junit.jupiter.api.Test;
 import org.junit.jupiter.api.Timeout;
 import org.junit.jupiter.api.extension.RegisterExtension;
@@ -78,7 +78,7 @@
  * </ul>
  */
 @Timeout(value = 30)
-@Ignore // https://github.com/apache/iceberg/pull/11305#issuecomment-2415207097
+@Disabled // https://github.com/apache/iceberg/pull/11305#issuecomment-2415207097
 public class TestFlinkIcebergSinkRangeDistributionBucketing {
   private static final Configuration DISABLE_CLASSLOADER_CHECK_CONFIG =
       new Configuration()

File: flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkRangeDistributionBucketing.java
Patch:
@@ -58,9 +58,9 @@
 import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;
 import org.apache.iceberg.relocated.com.google.common.collect.Lists;
 import org.apache.iceberg.types.Types;
-import org.junit.Ignore;
 import org.junit.jupiter.api.AfterEach;
 import org.junit.jupiter.api.BeforeEach;
+import org.junit.jupiter.api.Disabled;
 import org.junit.jupiter.api.Test;
 import org.junit.jupiter.api.Timeout;
 import org.junit.jupiter.api.extension.RegisterExtension;
@@ -78,7 +78,7 @@
  * </ul>
  */
 @Timeout(value = 30)
-@Ignore // https://github.com/apache/iceberg/pull/11305#issuecomment-2415207097
+@Disabled // https://github.com/apache/iceberg/pull/11305#issuecomment-2415207097
 public class TestFlinkIcebergSinkRangeDistributionBucketing {
   private static final Configuration DISABLE_CLASSLOADER_CHECK_CONFIG =
       new Configuration()

File: flink/v1.18/flink/src/main/java/org/apache/iceberg/flink/source/IcebergSource.java
Patch:
@@ -27,7 +27,6 @@
 import java.util.concurrent.ExecutorService;
 import java.util.concurrent.TimeUnit;
 import javax.annotation.Nullable;
-import org.apache.flink.annotation.Experimental;
 import org.apache.flink.api.common.eventtime.WatermarkStrategy;
 import org.apache.flink.api.common.typeinfo.TypeInformation;
 import org.apache.flink.api.connector.source.Boundedness;
@@ -86,7 +85,6 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-@Experimental
 public class IcebergSource<T> implements Source<T, IcebergSourceSplit, IcebergEnumeratorState> {
   private static final Logger LOG = LoggerFactory.getLogger(IcebergSource.class);
 

File: flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/source/IcebergSource.java
Patch:
@@ -27,7 +27,6 @@
 import java.util.concurrent.ExecutorService;
 import java.util.concurrent.TimeUnit;
 import javax.annotation.Nullable;
-import org.apache.flink.annotation.Experimental;
 import org.apache.flink.api.common.eventtime.WatermarkStrategy;
 import org.apache.flink.api.common.typeinfo.TypeInformation;
 import org.apache.flink.api.connector.source.Boundedness;
@@ -86,7 +85,6 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-@Experimental
 public class IcebergSource<T> implements Source<T, IcebergSourceSplit, IcebergEnumeratorState> {
   private static final Logger LOG = LoggerFactory.getLogger(IcebergSource.class);
 

File: flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/FlinkConfigOptions.java
Patch:
@@ -88,7 +88,7 @@ private FlinkConfigOptions() {}
   public static final ConfigOption<Boolean> TABLE_EXEC_ICEBERG_USE_FLIP27_SOURCE =
       ConfigOptions.key("table.exec.iceberg.use-flip27-source")
           .booleanType()
-          .defaultValue(false)
+          .defaultValue(true)
           .withDescription("Use the FLIP-27 based Iceberg source implementation.");
 
   public static final ConfigOption<SplitAssignerType> TABLE_EXEC_SPLIT_ASSIGNER_TYPE =

File: flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/source/IcebergSource.java
Patch:
@@ -27,7 +27,6 @@
 import java.util.concurrent.ExecutorService;
 import java.util.concurrent.TimeUnit;
 import javax.annotation.Nullable;
-import org.apache.flink.annotation.Experimental;
 import org.apache.flink.api.common.eventtime.WatermarkStrategy;
 import org.apache.flink.api.common.typeinfo.TypeInformation;
 import org.apache.flink.api.connector.source.Boundedness;
@@ -86,7 +85,6 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-@Experimental
 public class IcebergSource<T> implements Source<T, IcebergSourceSplit, IcebergEnumeratorState> {
   private static final Logger LOG = LoggerFactory.getLogger(IcebergSource.class);
 

File: api/src/test/java/org/apache/iceberg/types/TestSerializableTypes.java
Patch:
@@ -112,13 +112,13 @@ public void testMaps() throws Exception {
 
   @Test
   public void testLists() throws Exception {
-    Type[] maps =
+    Type[] lists =
         new Type[] {
           Types.ListType.ofOptional(2, Types.DoubleType.get()),
           Types.ListType.ofRequired(5, Types.DoubleType.get())
         };
 
-    for (Type list : maps) {
+    for (Type list : lists) {
       Type copy = TestHelpers.roundTripSerialize(list);
       assertThat(copy).as("List serialization should be equal to starting type").isEqualTo(list);
       assertThat(list.asNestedType().asListType().elementType())

File: core/src/main/java/org/apache/iceberg/io/ContentCache.java
Patch:
@@ -140,7 +140,7 @@ public void invalidate(String key) {
   }
 
   /**
-   * @deprecated since 1.6.0, will be removed in 1.7.0; This method does only best-effort
+   * @deprecated since 1.7.0, will be removed in 2.0.0; This method does only best-effort
    *     invalidation and is susceptible to a race condition. If the caller changed the state that
    *     could be cached (perhaps files on the storage) and calls this method, there is no guarantee
    *     that the cache will not contain stale entries some time after this method returns.

File: api/src/main/java/org/apache/iceberg/types/Comparators.java
Patch:
@@ -323,9 +323,9 @@ private CharSeqComparator() {}
      * represented using two Java characters (using UTF-16 surrogate pairs). Character by character
      * comparison may yield incorrect results while comparing a 4 byte UTF-8 character to a java
      * char. Character by character comparison works as expected if both characters are <= 3 byte
-     * UTF-8 character or both characters are 4 byte UTF-8 characters.
-     * isCharInUTF16HighSurrogateRange method detects a 4-byte character and considers that
-     * character to be lexicographically greater than any 3 byte or lower UTF-8 character.
+     * UTF-8 character or both characters are 4 byte UTF-8 characters. isCharHighSurrogate method
+     * detects a high surrogate (4-byte character) and considers that character to be
+     * lexicographically greater than any 3 byte or lower UTF-8 character.
      */
     @Override
     public int compare(CharSequence s1, CharSequence s2) {

File: api/src/main/java/org/apache/iceberg/expressions/BoundReference.java
Patch:
@@ -18,6 +18,7 @@
  */
 package org.apache.iceberg.expressions;
 
+import java.util.Locale;
 import org.apache.iceberg.Accessor;
 import org.apache.iceberg.StructLike;
 import org.apache.iceberg.types.Type;
@@ -82,6 +83,7 @@ public Accessor<StructLike> accessor() {
 
   @Override
   public String toString() {
-    return String.format("ref(id=%d, accessor-type=%s)", field.fieldId(), accessor.type());
+    return String.format(
+        Locale.ROOT, "ref(id=%d, accessor-type=%s)", field.fieldId(), accessor.type());
   }
 }

File: api/src/main/java/org/apache/iceberg/io/BulkDeletionFailureException.java
Patch:
@@ -18,11 +18,13 @@
  */
 package org.apache.iceberg.io;
 
+import java.util.Locale;
+
 public class BulkDeletionFailureException extends RuntimeException {
   private final int numberFailedObjects;
 
   public BulkDeletionFailureException(int numberFailedObjects) {
-    super(String.format("Failed to delete %d files", numberFailedObjects));
+    super(String.format(Locale.ROOT, "Failed to delete %d files", numberFailedObjects));
     this.numberFailedObjects = numberFailedObjects;
   }
 

File: api/src/main/java/org/apache/iceberg/util/DateTimeUtil.java
Patch:
@@ -27,6 +27,7 @@
 import java.time.format.DateTimeFormatter;
 import java.time.format.DateTimeFormatterBuilder;
 import java.time.temporal.ChronoUnit;
+import java.util.Locale;
 
 public class DateTimeUtil {
   private DateTimeUtil() {}
@@ -43,7 +44,7 @@ private DateTimeUtil() {}
           .parseCaseInsensitive()
           .append(DateTimeFormatter.ISO_LOCAL_DATE_TIME)
           .appendOffset("+HH:MM:ss", "+00:00")
-          .toFormatter();
+          .toFormatter(Locale.ROOT);
 
   public static LocalDate dateFromDays(int daysFromEpoch) {
     return ChronoUnit.DAYS.addTo(EPOCH_DAY, daysFromEpoch);

File: core/src/main/java/org/apache/iceberg/BaseMetastoreTableOperations.java
Patch:
@@ -18,6 +18,7 @@
  */
 package org.apache.iceberg;
 
+import java.util.Locale;
 import java.util.Set;
 import java.util.UUID;
 import java.util.concurrent.atomic.AtomicReference;
@@ -327,7 +328,8 @@ private String newTableMetadataFilePath(TableMetadata meta, int newVersion) {
             TableProperties.METADATA_COMPRESSION, TableProperties.METADATA_COMPRESSION_DEFAULT);
     String fileExtension = TableMetadataParser.getFileExtension(codecName);
     return metadataFileLocation(
-        meta, String.format("%05d-%s%s", newVersion, UUID.randomUUID(), fileExtension));
+        meta,
+        String.format(Locale.ROOT, "%05d-%s%s", newVersion, UUID.randomUUID(), fileExtension));
   }
 
   /**

File: core/src/main/java/org/apache/iceberg/MetricsModes.java
Patch:
@@ -114,7 +114,7 @@ public int length() {
 
     @Override
     public String toString() {
-      return String.format("truncate(%d)", length);
+      return String.format(Locale.ROOT, "truncate(%d)", length);
     }
 
     @Override

File: core/src/main/java/org/apache/iceberg/ScanSummary.java
Patch:
@@ -21,6 +21,7 @@
 import java.io.IOException;
 import java.util.Comparator;
 import java.util.List;
+import java.util.Locale;
 import java.util.Map;
 import java.util.NavigableMap;
 import java.util.Set;
@@ -346,7 +347,7 @@ public void update(K key, Function<V, V> updateFunc) {
       while (map.size() > maxSize) {
         if (throwIfLimited) {
           throw new IllegalStateException(
-              String.format("Too many matching keys: more than %d", maxSize));
+              String.format(Locale.ROOT, "Too many matching keys: more than %d", maxSize));
         }
         this.cut = map.lastKey();
         map.remove(cut);

File: core/src/main/java/org/apache/iceberg/data/avro/IcebergDecoder.java
Patch:
@@ -23,6 +23,7 @@
 import java.io.UncheckedIOException;
 import java.nio.ByteBuffer;
 import java.nio.ByteOrder;
+import java.util.Locale;
 import java.util.Map;
 import org.apache.avro.AvroRuntimeException;
 import org.apache.avro.Schema;
@@ -138,7 +139,8 @@ public D decode(InputStream stream, D reuse) throws IOException {
 
     if (IcebergEncoder.V1_HEADER[0] != header[0] || IcebergEncoder.V1_HEADER[1] != header[1]) {
       throw new BadHeaderException(
-          String.format("Unrecognized header bytes: 0x%02X 0x%02X", header[0], header[1]));
+          String.format(
+              Locale.ROOT, "Unrecognized header bytes: 0x%02X 0x%02X", header[0], header[1]));
     }
 
     RawDecoder<D> decoder = getDecoder(FP_BUFFER.get().getLong(2));

File: core/src/main/java/org/apache/iceberg/io/OutputFileFactory.java
Patch:
@@ -21,6 +21,7 @@
 import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;
 import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;
 
+import java.util.Locale;
 import java.util.UUID;
 import java.util.concurrent.atomic.AtomicInteger;
 import java.util.function.Supplier;
@@ -92,6 +93,7 @@ public static Builder builderFor(Table table, int partitionId, long taskId) {
   private String generateFilename() {
     return format.addExtension(
         String.format(
+            Locale.ROOT,
             "%05d-%d-%s-%05d%s",
             partitionId,
             taskId,

File: core/src/main/java/org/apache/iceberg/view/BaseViewOperations.java
Patch:
@@ -18,6 +18,7 @@
  */
 package org.apache.iceberg.view;
 
+import java.util.Locale;
 import java.util.UUID;
 import java.util.concurrent.atomic.AtomicReference;
 import java.util.function.Function;
@@ -157,7 +158,8 @@ private String newMetadataFilePath(ViewMetadata metadata, int newVersion) {
                 ViewProperties.METADATA_COMPRESSION, ViewProperties.METADATA_COMPRESSION_DEFAULT);
     String fileExtension = TableMetadataParser.getFileExtension(codecName);
     return metadataFileLocation(
-        metadata, String.format("%05d-%s%s", newVersion, UUID.randomUUID(), fileExtension));
+        metadata,
+        String.format(Locale.ROOT, "%05d-%s%s", newVersion, UUID.randomUUID(), fileExtension));
   }
 
   private String metadataFileLocation(ViewMetadata metadata, String filename) {

File: flink/v1.18/flink/src/main/java/org/apache/iceberg/flink/sink/ManifestOutputFileFactory.java
Patch:
@@ -18,6 +18,7 @@
  */
 package org.apache.iceberg.flink.sink;
 
+import java.util.Locale;
 import java.util.Map;
 import java.util.concurrent.atomic.AtomicInteger;
 import java.util.function.Supplier;
@@ -59,6 +60,7 @@ class ManifestOutputFileFactory {
   private String generatePath(long checkpointId) {
     return FileFormat.AVRO.addExtension(
         String.format(
+            Locale.ROOT,
             "%s-%s-%05d-%d-%d-%05d",
             flinkJobId,
             operatorUniqueId,

File: flink/v1.18/flink/src/main/java/org/apache/iceberg/flink/source/reader/RecordAndPosition.java
Patch:
@@ -18,6 +18,7 @@
  */
 package org.apache.iceberg.flink.source.reader;
 
+import java.util.Locale;
 import org.apache.flink.annotation.Internal;
 
 /**
@@ -73,6 +74,6 @@ public void record(T nextRecord) {
 
   @Override
   public String toString() {
-    return String.format("%s @ %d + %d", record, fileOffset, recordOffset);
+    return String.format(Locale.ROOT, "%s @ %d + %d", record, fileOffset, recordOffset);
   }
 }

File: flink/v1.18/flink/src/main/java/org/apache/iceberg/flink/source/split/IcebergSourceSplitSerializer.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.iceberg.flink.source.split;
 
 import java.io.IOException;
+import java.util.Locale;
 import org.apache.flink.annotation.Internal;
 import org.apache.flink.core.io.SimpleVersionedSerializer;
 
@@ -54,6 +55,7 @@ public IcebergSourceSplit deserialize(int version, byte[] serialized) throws IOE
       default:
         throw new IOException(
             String.format(
+                Locale.ROOT,
                 "Failed to deserialize IcebergSourceSplit. "
                     + "Encountered unsupported version: %d. Supported version are [1]",
                 version));

File: flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/sink/ManifestOutputFileFactory.java
Patch:
@@ -18,6 +18,7 @@
  */
 package org.apache.iceberg.flink.sink;
 
+import java.util.Locale;
 import java.util.Map;
 import java.util.concurrent.atomic.AtomicInteger;
 import java.util.function.Supplier;
@@ -59,6 +60,7 @@ class ManifestOutputFileFactory {
   private String generatePath(long checkpointId) {
     return FileFormat.AVRO.addExtension(
         String.format(
+            Locale.ROOT,
             "%s-%s-%05d-%d-%d-%05d",
             flinkJobId,
             operatorUniqueId,

File: flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/source/reader/RecordAndPosition.java
Patch:
@@ -18,6 +18,7 @@
  */
 package org.apache.iceberg.flink.source.reader;
 
+import java.util.Locale;
 import org.apache.flink.annotation.Internal;
 
 /**
@@ -73,6 +74,6 @@ public void record(T nextRecord) {
 
   @Override
   public String toString() {
-    return String.format("%s @ %d + %d", record, fileOffset, recordOffset);
+    return String.format(Locale.ROOT, "%s @ %d + %d", record, fileOffset, recordOffset);
   }
 }

File: flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/source/split/IcebergSourceSplitSerializer.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.iceberg.flink.source.split;
 
 import java.io.IOException;
+import java.util.Locale;
 import org.apache.flink.annotation.Internal;
 import org.apache.flink.core.io.SimpleVersionedSerializer;
 
@@ -54,6 +55,7 @@ public IcebergSourceSplit deserialize(int version, byte[] serialized) throws IOE
       default:
         throw new IOException(
             String.format(
+                Locale.ROOT,
                 "Failed to deserialize IcebergSourceSplit. "
                     + "Encountered unsupported version: %d. Supported version are [1]",
                 version));

File: flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/ManifestOutputFileFactory.java
Patch:
@@ -18,6 +18,7 @@
  */
 package org.apache.iceberg.flink.sink;
 
+import java.util.Locale;
 import java.util.Map;
 import java.util.concurrent.atomic.AtomicInteger;
 import java.util.function.Supplier;
@@ -59,6 +60,7 @@ class ManifestOutputFileFactory {
   private String generatePath(long checkpointId) {
     return FileFormat.AVRO.addExtension(
         String.format(
+            Locale.ROOT,
             "%s-%s-%05d-%d-%d-%05d",
             flinkJobId,
             operatorUniqueId,

File: flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/source/reader/RecordAndPosition.java
Patch:
@@ -18,6 +18,7 @@
  */
 package org.apache.iceberg.flink.source.reader;
 
+import java.util.Locale;
 import org.apache.flink.annotation.Internal;
 
 /**
@@ -73,6 +74,6 @@ public void record(T nextRecord) {
 
   @Override
   public String toString() {
-    return String.format("%s @ %d + %d", record, fileOffset, recordOffset);
+    return String.format(Locale.ROOT, "%s @ %d + %d", record, fileOffset, recordOffset);
   }
 }

File: flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/source/split/IcebergSourceSplitSerializer.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.iceberg.flink.source.split;
 
 import java.io.IOException;
+import java.util.Locale;
 import org.apache.flink.annotation.Internal;
 import org.apache.flink.core.io.SimpleVersionedSerializer;
 
@@ -54,6 +55,7 @@ public IcebergSourceSplit deserialize(int version, byte[] serialized) throws IOE
       default:
         throw new IOException(
             String.format(
+                Locale.ROOT,
                 "Failed to deserialize IcebergSourceSplit. "
                     + "Encountered unsupported version: %d. Supported version are [1]",
                 version));

File: kafka-connect/kafka-connect/src/main/java/org/apache/iceberg/connect/data/RecordConverter.java
Patch:
@@ -37,6 +37,7 @@
 import java.util.Base64;
 import java.util.Date;
 import java.util.List;
+import java.util.Locale;
 import java.util.Map;
 import java.util.UUID;
 import java.util.stream.Collectors;
@@ -71,7 +72,7 @@ class RecordConverter {
       new DateTimeFormatterBuilder()
           .append(DateTimeFormatter.ISO_LOCAL_DATE_TIME)
           .appendOffset("+HHmm", "Z")
-          .toFormatter();
+          .toFormatter(Locale.ROOT);
 
   private final Schema tableSchema;
   private final NameMapping nameMapping;

File: nessie/src/main/java/org/apache/iceberg/nessie/NessieIcebergClient.java
Patch:
@@ -272,8 +272,10 @@ public List<Namespace> listNamespaces(Namespace namespace) throws NoSuchNamespac
             org.projectnessie.model.Namespace.of(namespace.levels());
         filter +=
             String.format(
+                Locale.ROOT,
                 "size(entry.keyElements) == %d && entry.encodedKey.startsWith('%s.')",
-                root.getElementCount() + 1, root.name());
+                root.getElementCount() + 1,
+                root.name());
       }
       List<ContentKey> entries =
           withReference(api.getEntries()).filter(filter).stream()

File: spark/v3.3/spark/src/jmh/java/org/apache/iceberg/spark/action/DeleteOrphanFilesBenchmark.java
Patch:
@@ -22,6 +22,7 @@
 
 import java.sql.Timestamp;
 import java.util.List;
+import java.util.Locale;
 import java.util.UUID;
 import java.util.concurrent.TimeUnit;
 import org.apache.iceberg.AppendFiles;
@@ -124,7 +125,7 @@ private void appendData() {
     for (int i = 0; i < NUM_SNAPSHOTS; i++) {
       AppendFiles appendFiles = table().newFastAppend();
       for (int j = 0; j < NUM_FILES; j++) {
-        String path = String.format("%s/path/to/data-%d-%d.parquet", location, i, j);
+        String path = String.format(Locale.ROOT, "%s/path/to/data-%d-%d.parquet", location, i, j);
         validAndOrphanPaths.add(path);
         DataFile dataFile =
             DataFiles.builder(partitionSpec)

File: spark/v3.4/spark/src/jmh/java/org/apache/iceberg/spark/action/DeleteOrphanFilesBenchmark.java
Patch:
@@ -22,6 +22,7 @@
 
 import java.sql.Timestamp;
 import java.util.List;
+import java.util.Locale;
 import java.util.UUID;
 import java.util.concurrent.TimeUnit;
 import org.apache.iceberg.AppendFiles;
@@ -124,7 +125,7 @@ private void appendData() {
     for (int i = 0; i < NUM_SNAPSHOTS; i++) {
       AppendFiles appendFiles = table().newFastAppend();
       for (int j = 0; j < NUM_FILES; j++) {
-        String path = String.format("%s/path/to/data-%d-%d.parquet", location, i, j);
+        String path = String.format(Locale.ROOT, "%s/path/to/data-%d-%d.parquet", location, i, j);
         validAndOrphanPaths.add(path);
         DataFile dataFile =
             DataFiles.builder(partitionSpec)

File: spark/v3.5/spark/src/jmh/java/org/apache/iceberg/spark/action/DeleteOrphanFilesBenchmark.java
Patch:
@@ -22,6 +22,7 @@
 
 import java.sql.Timestamp;
 import java.util.List;
+import java.util.Locale;
 import java.util.UUID;
 import java.util.concurrent.TimeUnit;
 import org.apache.iceberg.AppendFiles;
@@ -124,7 +125,7 @@ private void appendData() {
     for (int i = 0; i < NUM_SNAPSHOTS; i++) {
       AppendFiles appendFiles = table().newFastAppend();
       for (int j = 0; j < NUM_FILES; j++) {
-        String path = String.format("%s/path/to/data-%d-%d.parquet", location, i, j);
+        String path = String.format(Locale.ROOT, "%s/path/to/data-%d-%d.parquet", location, i, j);
         validAndOrphanPaths.add(path);
         DataFile dataFile =
             DataFiles.builder(partitionSpec)

File: aws/src/main/java/org/apache/iceberg/aws/AssumeRoleAwsClientFactory.java
Patch:
@@ -47,6 +47,7 @@ public S3Client s3() {
         .applyMutation(s3FileIOProperties::applyEndpointConfigurations)
         .applyMutation(s3FileIOProperties::applyServiceConfigurations)
         .applyMutation(s3FileIOProperties::applySignerConfiguration)
+        .applyMutation(s3FileIOProperties::applyRetryConfigurations)
         .build();
   }
 

File: aws/src/main/java/org/apache/iceberg/aws/AwsClientFactories.java
Patch:
@@ -114,6 +114,7 @@ public S3Client s3() {
           .applyMutation(s3FileIOProperties::applySignerConfiguration)
           .applyMutation(s3FileIOProperties::applyS3AccessGrantsConfigurations)
           .applyMutation(s3FileIOProperties::applyUserAgentConfigurations)
+          .applyMutation(s3FileIOProperties::applyRetryConfigurations)
           .build();
     }
 

File: aws/src/main/java/org/apache/iceberg/aws/lakeformation/LakeFormationAwsClientFactory.java
Patch:
@@ -81,6 +81,7 @@ public S3Client s3() {
           .applyMutation(httpClientProperties()::applyHttpClientConfigurations)
           .applyMutation(s3FileIOProperties()::applyEndpointConfigurations)
           .applyMutation(s3FileIOProperties()::applyServiceConfigurations)
+          .applyMutation(s3FileIOProperties()::applyRetryConfigurations)
           .credentialsProvider(
               new LakeFormationCredentialsProvider(lakeFormation(), buildTableArn()))
           .region(Region.of(region()))

File: aws/src/main/java/org/apache/iceberg/aws/s3/DefaultS3FileIOAwsClientFactory.java
Patch:
@@ -55,6 +55,7 @@ public S3Client s3() {
         .applyMutation(s3FileIOProperties::applySignerConfiguration)
         .applyMutation(s3FileIOProperties::applyS3AccessGrantsConfigurations)
         .applyMutation(s3FileIOProperties::applyUserAgentConfigurations)
+        .applyMutation(s3FileIOProperties::applyRetryConfigurations)
         .build();
   }
 }

File: aws/src/main/java/org/apache/iceberg/aws/s3/S3FileIO.java
Patch:
@@ -342,7 +342,7 @@ private ExecutorService executorService() {
       synchronized (S3FileIO.class) {
         if (executorService == null) {
           executorService =
-              ThreadPools.newWorkerPool(
+              ThreadPools.newExitingWorkerPool(
                   "iceberg-s3fileio-delete", s3FileIOProperties.deleteThreads());
         }
       }

File: core/src/jmh/java/org/apache/iceberg/metrics/CountersBenchmark.java
Patch:
@@ -50,7 +50,7 @@ public class CountersBenchmark {
   public void defaultCounterMultipleThreads(Blackhole blackhole) {
     Counter counter = new DefaultCounter(Unit.BYTES);
 
-    ExecutorService workerPool = ThreadPools.newWorkerPool("bench-pool", WORKER_POOL_SIZE);
+    ExecutorService workerPool = ThreadPools.newFixedThreadPool("bench-pool", WORKER_POOL_SIZE);
 
     try {
       Tasks.range(WORKER_POOL_SIZE)

File: core/src/main/java/org/apache/iceberg/hadoop/HadoopFileIO.java
Patch:
@@ -192,7 +192,8 @@ private ExecutorService executorService() {
     if (executorService == null) {
       synchronized (HadoopFileIO.class) {
         if (executorService == null) {
-          executorService = ThreadPools.newWorkerPool(DELETE_FILE_POOL_NAME, deleteThreads());
+          executorService =
+              ThreadPools.newExitingWorkerPool(DELETE_FILE_POOL_NAME, deleteThreads());
         }
       }
     }

File: flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergCommitter.java
Patch:
@@ -102,7 +102,7 @@ class IcebergCommitter implements Committer<IcebergCommittable> {
     Preconditions.checkArgument(
         maxContinuousEmptyCommits > 0, MAX_CONTINUOUS_EMPTY_COMMITS + " must be positive");
     this.workerPool =
-        ThreadPools.newWorkerPool(
+        ThreadPools.newFixedThreadPool(
             "iceberg-committer-pool-" + table.name() + "-" + sinkId, workerPoolSize);
     this.continuousEmptyCheckpoints = 0;
   }
@@ -307,5 +307,6 @@ private void commitOperation(
   @Override
   public void close() throws IOException {
     tableLoader.close();
+    workerPool.shutdown();
   }
 }

File: flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java
Patch:
@@ -455,7 +455,7 @@ public void open() throws Exception {
 
     final String operatorID = getRuntimeContext().getOperatorUniqueID();
     this.workerPool =
-        ThreadPools.newWorkerPool("iceberg-worker-pool-" + operatorID, workerPoolSize);
+        ThreadPools.newFixedThreadPool("iceberg-worker-pool-" + operatorID, workerPoolSize);
   }
 
   @Override

File: flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/source/FlinkInputFormat.java
Patch:
@@ -93,7 +93,7 @@ public FlinkInputSplit[] createInputSplits(int minNumSplits) throws IOException
     // Called in Job manager, so it is OK to load table from catalog.
     tableLoader.open();
     final ExecutorService workerPool =
-        ThreadPools.newWorkerPool("iceberg-plan-worker-pool", context.planParallelism());
+        ThreadPools.newFixedThreadPool("iceberg-plan-worker-pool", context.planParallelism());
     try (TableLoader loader = tableLoader) {
       Table table = loader.loadTable();
       return FlinkSplitPlanner.planInputSplits(table, context, workerPool);

File: flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/source/IcebergSource.java
Patch:
@@ -153,7 +153,7 @@ private List<IcebergSourceSplit> planSplitsForBatch(String threadName) {
     }
 
     ExecutorService workerPool =
-        ThreadPools.newWorkerPool(threadName, scanContext.planParallelism());
+        ThreadPools.newFixedThreadPool(threadName, scanContext.planParallelism());
     try (TableLoader loader = tableLoader.clone()) {
       loader.open();
       this.batchSplits =

File: flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/source/StreamingMonitorFunction.java
Patch:
@@ -106,7 +106,7 @@ public void open(Configuration parameters) throws Exception {
         "context should be instance of StreamingRuntimeContext");
     final String operatorID = ((StreamingRuntimeContext) runtimeContext).getOperatorUniqueID();
     this.workerPool =
-        ThreadPools.newWorkerPool(
+        ThreadPools.newFixedThreadPool(
             "iceberg-worker-pool-" + operatorID, scanContext.planParallelism());
   }
 

File: flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/source/enumerator/ContinuousSplitPlannerImpl.java
Patch:
@@ -64,7 +64,7 @@ public ContinuousSplitPlannerImpl(
     this.workerPool =
         isSharedPool
             ? ThreadPools.getWorkerPool()
-            : ThreadPools.newWorkerPool(
+            : ThreadPools.newFixedThreadPool(
                 "iceberg-plan-worker-pool-" + threadName, scanContext.planParallelism());
   }
 

File: kafka-connect/kafka-connect/src/main/java/org/apache/iceberg/connect/channel/Coordinator.java
Patch:
@@ -90,7 +90,7 @@ class Coordinator extends Channel {
     this.snapshotOffsetsProp =
         String.format(
             "kafka.connect.offsets.%s.%s", config.controlTopic(), config.connectGroupId());
-    this.exec = ThreadPools.newWorkerPool("iceberg-committer", config.commitThreads());
+    this.exec = ThreadPools.newFixedThreadPool("iceberg-committer", config.commitThreads());
     this.commitState = new CommitState(config);
   }
 

File: mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java
Patch:
@@ -108,7 +108,7 @@ public List<InputSplit> getSplits(JobContext context) {
                 HiveIcebergStorageHandler.table(conf, conf.get(InputFormatConfig.TABLE_IDENTIFIER)))
             .orElseGet(() -> Catalogs.loadTable(conf));
     final ExecutorService workerPool =
-        ThreadPools.newWorkerPool(
+        ThreadPools.newFixedThreadPool(
             "iceberg-plan-worker-pool",
             conf.getInt(
                 SystemConfigs.WORKER_THREAD_POOL_SIZE.propertyKey(),

File: core/src/test/java/org/apache/iceberg/TestFastAppend.java
Patch:
@@ -54,7 +54,7 @@ public void testAddManyFiles() {
       dataFiles.add(dataFile);
     }
 
-    AppendFiles append = table.newAppend();
+    AppendFiles append = table.newFastAppend();
     dataFiles.forEach(append::appendFile);
     append.commit();
 

File: parquet/src/main/java/org/apache/iceberg/parquet/ParquetSchemaUtil.java
Patch:
@@ -78,7 +78,9 @@ private static Schema convertInternal(
   public static MessageType pruneColumns(MessageType fileSchema, Schema expectedSchema) {
     // column order must match the incoming type, so it doesn't matter that the ids are unordered
     Set<Integer> selectedIds = TypeUtil.getProjectedIds(expectedSchema);
-    return (MessageType) ParquetTypeVisitor.visit(fileSchema, new PruneColumns(selectedIds));
+    return (MessageType)
+        TypeWithSchemaVisitor.visit(
+            expectedSchema.asStruct(), fileSchema, new PruneColumns(selectedIds));
   }
 
   /**

File: aws/src/main/java/org/apache/iceberg/aws/s3/S3FileIOProperties.java
Patch:
@@ -283,7 +283,7 @@ public class S3FileIOProperties implements Serializable {
    * catalog property. After set, x-amz-storage-class header will be set to this property
    *
    * <p>For more details, see
-   * https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/userguide/storage-class-intro.html
+   * https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html
    *
    * <p>Example: s3.write.storage-class=INTELLIGENT_TIERING
    */

File: flink/v1.19/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkRangeDistributionBucketing.java
Patch:
@@ -98,7 +98,7 @@ public class TestFlinkIcebergSinkRangeDistributionBucketing {
       new HadoopCatalogExtension(TestFixtures.DATABASE, TestFixtures.TABLE);
 
   private static final int NUM_BUCKETS = 4;
-  private static final int NUM_OF_CHECKPOINTS = 4;
+  private static final int NUM_OF_CHECKPOINTS = 6;
   private static final int ROW_COUNT_PER_CHECKPOINT = 200;
   private static final Schema SCHEMA =
       new Schema(

File: flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkRangeDistributionBucketing.java
Patch:
@@ -98,7 +98,7 @@ public class TestFlinkIcebergSinkRangeDistributionBucketing {
       new HadoopCatalogExtension(TestFixtures.DATABASE, TestFixtures.TABLE);
 
   private static final int NUM_BUCKETS = 4;
-  private static final int NUM_OF_CHECKPOINTS = 4;
+  private static final int NUM_OF_CHECKPOINTS = 6;
   private static final int ROW_COUNT_PER_CHECKPOINT = 200;
   private static final Schema SCHEMA =
       new Schema(

File: core/src/main/java/org/apache/iceberg/MergingSnapshotProducer.java
Patch:
@@ -984,11 +984,11 @@ private List<ManifestFile> newDataFilesAsManifests() {
                 writer.close();
               }
               this.cachedNewDataManifests.addAll(writer.toManifestFiles());
-              this.hasNewDataFiles = false;
             } catch (IOException e) {
               throw new RuntimeIOException(e, "Failed to close manifest writer");
             }
           });
+      this.hasNewDataFiles = false;
     }
 
     return cachedNewDataManifests;

File: flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/maintenance/operator/TableChange.java
Patch:
@@ -192,6 +192,8 @@ static class Builder {
     private long eqDeleteRecordCount = 0L;
     private int commitCount = 0;
 
+    private Builder() {}
+
     public Builder dataFileCount(int newDataFileCount) {
       this.dataFileCount = newDataFileCount;
       return this;

File: flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/maintenance/operator/TableChange.java
Patch:
@@ -192,6 +192,8 @@ static class Builder {
     private long eqDeleteRecordCount = 0L;
     private int commitCount = 0;
 
+    private Builder() {}
+
     public Builder dataFileCount(int newDataFileCount) {
       this.dataFileCount = newDataFileCount;
       return this;

File: api/src/main/java/org/apache/iceberg/expressions/BoundLiteralPredicate.java
Patch:
@@ -31,6 +31,7 @@ public class BoundLiteralPredicate<T> extends BoundPredicate<T> {
           Type.TypeID.LONG,
           Type.TypeID.DATE,
           Type.TypeID.TIME,
+          Type.TypeID.TIMESTAMP_NANO,
           Type.TypeID.TIMESTAMP);
 
   private static long toLong(Literal<?> lit) {

File: api/src/main/java/org/apache/iceberg/types/Comparators.java
Patch:
@@ -41,6 +41,8 @@ private Comparators() {}
           .put(Types.TimeType.get(), Comparator.naturalOrder())
           .put(Types.TimestampType.withZone(), Comparator.naturalOrder())
           .put(Types.TimestampType.withoutZone(), Comparator.naturalOrder())
+          .put(Types.TimestampNanoType.withZone(), Comparator.naturalOrder())
+          .put(Types.TimestampNanoType.withoutZone(), Comparator.naturalOrder())
           .put(Types.StringType.get(), Comparators.charSequences())
           .put(Types.UUIDType.get(), Comparator.naturalOrder())
           .put(Types.BinaryType.get(), Comparators.unsignedBytes())

File: api/src/main/java/org/apache/iceberg/types/Conversions.java
Patch:
@@ -97,6 +97,7 @@ public static ByteBuffer toByteBuffer(Type.TypeID typeId, Object value) {
       case LONG:
       case TIME:
       case TIMESTAMP:
+      case TIMESTAMP_NANO:
         return ByteBuffer.allocate(8).order(ByteOrder.LITTLE_ENDIAN).putLong(0, (long) value);
       case FLOAT:
         return ByteBuffer.allocate(4).order(ByteOrder.LITTLE_ENDIAN).putFloat(0, (float) value);
@@ -146,6 +147,7 @@ private static Object internalFromByteBuffer(Type type, ByteBuffer buffer) {
       case LONG:
       case TIME:
       case TIMESTAMP:
+      case TIMESTAMP_NANO:
         if (tmp.remaining() < 8) {
           // type was later promoted to long
           return (long) tmp.getInt();

File: api/src/main/java/org/apache/iceberg/types/Type.java
Patch:
@@ -37,6 +37,7 @@ enum TypeID {
     DATE(Integer.class),
     TIME(Long.class),
     TIMESTAMP(Long.class),
+    TIMESTAMP_NANO(Long.class),
     STRING(CharSequence.class),
     UUID(java.util.UUID.class),
     FIXED(ByteBuffer.class),

File: api/src/main/java/org/apache/iceberg/types/TypeUtil.java
Patch:
@@ -522,6 +522,7 @@ private static int estimateSize(Type type) {
       case DOUBLE:
       case TIME:
       case TIMESTAMP:
+      case TIMESTAMP_NANO:
         // longs and doubles occupy 8 bytes
         // times and timestamps are internally represented as longs
         return 8;

File: api/src/test/java/org/apache/iceberg/TestAccessors.java
Patch:
@@ -180,6 +180,8 @@ public void testTime() {
   public void testTimestamp() {
     assertAccessorReturns(Types.TimestampType.withoutZone(), 123L);
     assertAccessorReturns(Types.TimestampType.withZone(), 123L);
+    assertAccessorReturns(Types.TimestampNanoType.withoutZone(), 123L);
+    assertAccessorReturns(Types.TimestampNanoType.withZone(), 123L);
   }
 
   @Test

File: api/src/test/java/org/apache/iceberg/transforms/TestIdentity.java
Patch:
@@ -106,7 +106,7 @@ public void testTimestampWithZoneHumanString() {
     // value will always be in UTC
     assertThat(identity.toHumanString(timestamptz, ts.value()))
         .as("Should produce timestamp with time zone adjusted to UTC")
-        .isEqualTo("2017-12-01T18:12:55.038194Z");
+        .isEqualTo("2017-12-01T18:12:55.038194+00:00");
   }
 
   @Test

File: api/src/test/java/org/apache/iceberg/types/TestReadabilityChecks.java
Patch:
@@ -39,6 +39,8 @@ public class TestReadabilityChecks {
         Types.TimeType.get(),
         Types.TimestampType.withoutZone(),
         Types.TimestampType.withZone(),
+        Types.TimestampNanoType.withoutZone(),
+        Types.TimestampNanoType.withZone(),
         Types.StringType.get(),
         Types.UUIDType.get(),
         Types.FixedType.ofLength(3),

File: api/src/test/java/org/apache/iceberg/types/TestSerializableTypes.java
Patch:
@@ -41,6 +41,8 @@ public void testIdentityTypes() throws Exception {
           Types.TimeType.get(),
           Types.TimestampType.withoutZone(),
           Types.TimestampType.withZone(),
+          Types.TimestampNanoType.withoutZone(),
+          Types.TimestampNanoType.withZone(),
           Types.StringType.get(),
           Types.UUIDType.get(),
           Types.BinaryType.get()

File: core/src/main/java/org/apache/iceberg/TableMetadata.java
Patch:
@@ -1494,6 +1494,8 @@ private int addSchemaInternal(Schema schema, int newLastColumnId) {
           newLastColumnId,
           lastColumnId);
 
+      Schema.checkCompatibility(schema, formatVersion);
+
       int newSchemaId = reuseOrCreateNewSchemaId(schema);
       boolean schemaFound = schemasById.containsKey(newSchemaId);
       if (schemaFound && newLastColumnId == lastColumnId) {

File: open-api/src/testFixtures/java/org/apache/iceberg/rest/RESTCatalogServer.java
Patch:
@@ -106,7 +106,7 @@ public void start(boolean join) throws Exception {
     httpServer.setHandler(context);
     httpServer.start();
 
-    if(join) {
+    if (join) {
       httpServer.join();
     }
   }

File: open-api/src/testFixtures/java/org/apache/iceberg/rest/RESTServerExtension.java
Patch:
@@ -28,9 +28,7 @@ public class RESTServerExtension implements BeforeAllCallback, AfterAllCallback
   @Override
   public void beforeAll(ExtensionContext extensionContext) throws Exception {
     if (Boolean.parseBoolean(
-        extensionContext
-            .getConfigurationParameter(RCKUtils.RCK_LOCAL)
-            .orElse("true"))) {
+        extensionContext.getConfigurationParameter(RCKUtils.RCK_LOCAL).orElse("true"))) {
       this.localServer = new RESTCatalogServer();
       this.localServer.start(false);
     }

File: flink/v1.19/flink/src/jmh/java/org/apache/iceberg/flink/sink/shuffle/MapRangePartitionerBenchmark.java
Patch:
@@ -18,6 +18,7 @@
  */
 package org.apache.iceberg.flink.sink.shuffle;
 
+import java.nio.charset.StandardCharsets;
 import java.util.Comparator;
 import java.util.List;
 import java.util.Map;
@@ -139,7 +140,7 @@ private static String randomString(String prefix) {
       buffer[i] = (byte) CHARS.charAt(ThreadLocalRandom.current().nextInt(CHARS.length()));
     }
 
-    return prefix + new String(buffer);
+    return prefix + new String(buffer, StandardCharsets.US_ASCII);
   }
 
   /** find the index where weightsUDF[index] < weight && weightsUDF[index+1] >= weight */

File: kafka-connect/kafka-connect/src/main/java/org/apache/iceberg/connect/data/SinkWriter.java
Patch:
@@ -23,6 +23,7 @@
 import java.time.ZoneOffset;
 import java.util.Collection;
 import java.util.List;
+import java.util.Locale;
 import java.util.Map;
 import java.util.regex.Pattern;
 import java.util.stream.Collectors;
@@ -119,7 +120,7 @@ private void routeRecordDynamically(SinkRecord record) {
 
     String routeValue = extractRouteValue(record.value(), routeField);
     if (routeValue != null) {
-      String tableName = routeValue.toLowerCase();
+      String tableName = routeValue.toLowerCase(Locale.ROOT);
       writerForTable(tableName, record, true).write(record);
     }
   }

File: core/src/test/java/org/apache/iceberg/TestMetadataTableFilters.java
Patch:
@@ -317,7 +317,7 @@ public void testPartitionSpecEvolutionRemovalV1() {
             .withPartition(data10Key)
             .build();
     PartitionKey data11Key = new PartitionKey(newSpec, table.schema());
-    data10Key.set(1, 11);
+    data11Key.set(1, 11);
     DataFile data11 =
         DataFiles.builder(newSpec)
             .withPath("/path/to/data-11.parquet")
@@ -465,8 +465,8 @@ public void testPartitionSpecEvolutionAdditiveV1() {
             .withPartition(data10Key)
             .build();
     PartitionKey data11Key = new PartitionKey(newSpec, table.schema());
-    data11Key.set(0, 1); // data=0
-    data10Key.set(1, 11); // id=11
+    data11Key.set(0, 1); // data=1
+    data11Key.set(1, 11); // id=11
     DataFile data11 =
         DataFiles.builder(newSpec)
             .withPath("/path/to/data-11.parquet")

File: core/src/test/java/org/apache/iceberg/TestMetadataTableScans.java
Patch:
@@ -939,8 +939,8 @@ public void testPartitionSpecEvolutionAdditive() {
             .withPartition(data10Key)
             .build();
     PartitionKey data11Key = new PartitionKey(newSpec, table.schema());
-    data11Key.set(0, 1); // data=0
-    data10Key.set(1, 11); // id=11
+    data11Key.set(0, 1); // data=1
+    data11Key.set(1, 11); // id=11
     DataFile data11 =
         DataFiles.builder(newSpec)
             .withPath("/path/to/data-11.parquet")

File: api/src/main/java/org/apache/iceberg/Transaction.java
Patch:
@@ -52,7 +52,7 @@ public interface Transaction {
   UpdateProperties updateProperties();
 
   /**
-   * Create a new {@link ReplaceSortOrder} to set a table sort order and commit the change.
+   * Create a new {@link ReplaceSortOrder} to set a table sort order.
    *
    * @return a new {@link ReplaceSortOrder}
    */
@@ -131,7 +131,7 @@ default AppendFiles newFastAppend() {
   ReplacePartitions newReplacePartitions();
 
   /**
-   * Create a new {@link DeleteFiles delete API} to replace files in this table.
+   * Create a new {@link DeleteFiles delete API} to delete files in this table.
    *
    * @return a new {@link DeleteFiles}
    */
@@ -160,7 +160,7 @@ default UpdatePartitionStatistics updatePartitionStatistics() {
   }
 
   /**
-   * Create a new {@link ExpireSnapshots expire API} to manage snapshots in this table.
+   * Create a new {@link ExpireSnapshots expire API} to expire snapshots in this table.
    *
    * @return a new {@link ExpireSnapshots}
    */

File: core/src/main/java/org/apache/iceberg/BaseEntriesTable.java
Patch:
@@ -262,7 +262,7 @@ private <T> boolean fileContent(BoundReference<T> ref) {
         return ref.fieldId() == DataFile.CONTENT.fieldId();
       }
 
-      private <T> boolean contentMatch(Integer fileContentId) {
+      private boolean contentMatch(Integer fileContentId) {
         if (FileContent.DATA.id() == fileContentId) {
           return ManifestContent.DATA.id() == manifestContentId;
         } else if (FileContent.EQUALITY_DELETES.id() == fileContentId

File: core/src/main/java/org/apache/iceberg/actions/SizeBasedFileRewriter.java
Patch:
@@ -229,7 +229,8 @@ protected long numOutputFiles(long inputSize) {
       // the remainder file is of a valid size for this rewrite so keep it
       return numFilesWithRemainder;
 
-    } else if (avgFileSizeWithoutRemainder < Math.min(1.1 * targetFileSize, writeMaxFileSize())) {
+    } else if (avgFileSizeWithoutRemainder
+        < Math.min(1.1 * targetFileSize, (double) writeMaxFileSize())) {
       // if the reminder is distributed amongst other files,
       // the average file size will be no more than 10% bigger than the target file size
       // so round down and distribute remainder amongst other files

File: core/src/main/java/org/apache/iceberg/rest/ExponentialHttpRequestRetryStrategy.java
Patch:
@@ -149,7 +149,7 @@ public TimeValue getRetryInterval(HttpResponse response, int execCount, HttpCont
       }
     }
 
-    int delayMillis = 1000 * (int) Math.min(Math.pow(2.0, (long) execCount - 1), 64.0);
+    int delayMillis = 1000 * (int) Math.min(Math.pow(2.0, (long) execCount - 1.0), 64.0);
     int jitter = ThreadLocalRandom.current().nextInt(Math.max(1, (int) (delayMillis * 0.1)));
 
     return TimeValue.ofMilliseconds(delayMillis + jitter);

File: core/src/main/java/org/apache/iceberg/util/ParallelIterable.java
Patch:
@@ -101,6 +101,7 @@ private ParallelIterator(
     }
 
     @Override
+    @SuppressWarnings("FutureReturnValueIgnored")
     public void close() {
       // close first, avoid new task submit
       this.closed.set(true);

File: core/src/main/java/org/apache/iceberg/util/Tasks.java
Patch:
@@ -450,7 +450,9 @@ private <E extends Exception> void runTaskWithRetry(Task<I, E> task, I item) thr
           }
 
           int delayMs =
-              (int) Math.min(minSleepTimeMs * Math.pow(scaleFactor, attempt - 1), maxSleepTimeMs);
+              (int)
+                  Math.min(
+                      minSleepTimeMs * Math.pow(scaleFactor, attempt - 1), (double) maxSleepTimeMs);
           int jitter = ThreadLocalRandom.current().nextInt(Math.max(1, (int) (delayMs * 0.1)));
 
           LOG.warn("Retrying task after failure: {}", e.getMessage(), e);

File: core/src/main/java/org/apache/iceberg/view/BaseViewOperations.java
Patch:
@@ -102,6 +102,7 @@ public ViewMetadata refresh() {
   }
 
   @Override
+  @SuppressWarnings("ImmutablesReferenceEquality")
   public void commit(ViewMetadata base, ViewMetadata metadata) {
     // if the metadata is already out of date, reject it
     if (base != current()) {

File: flink/v1.20/flink/src/jmh/java/org/apache/iceberg/flink/sink/shuffle/MapRangePartitionerBenchmark.java
Patch:
@@ -18,6 +18,7 @@
  */
 package org.apache.iceberg.flink.sink.shuffle;
 
+import java.nio.charset.StandardCharsets;
 import java.util.Comparator;
 import java.util.List;
 import java.util.Map;
@@ -139,7 +140,7 @@ private static String randomString(String prefix) {
       buffer[i] = (byte) CHARS.charAt(ThreadLocalRandom.current().nextInt(CHARS.length()));
     }
 
-    return prefix + new String(buffer);
+    return prefix + new String(buffer, StandardCharsets.UTF_8);
   }
 
   /** find the index where weightsUDF[index] < weight && weightsUDF[index+1] >= weight */

File: aws/src/test/java/org/apache/iceberg/aws/s3/signer/S3SignerServlet.java
Patch:
@@ -148,7 +148,7 @@ private OAuthTokenResponse handleOAuth(Map<String, String> requestMap) {
                 .withToken("client-credentials-token:sub=" + requestMap.get("client_id"))
                 .withIssuedTokenType("urn:ietf:params:oauth:token-type:access_token")
                 .withTokenType("Bearer")
-                .setExpirationInSeconds(100)
+                .setExpirationInSeconds(10000)
                 .build());
 
       case "urn:ietf:params:oauth:grant-type:token-exchange":
@@ -163,7 +163,7 @@ private OAuthTokenResponse handleOAuth(Map<String, String> requestMap) {
                 .withToken(token)
                 .withIssuedTokenType("urn:ietf:params:oauth:token-type:access_token")
                 .withTokenType("Bearer")
-                .setExpirationInSeconds(100)
+                .setExpirationInSeconds(10000)
                 .build());
 
       default:

File: flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/source/TestIcebergSpeculativeExecutionSupport.java
Patch:
@@ -165,7 +165,7 @@ private static Configuration configure() {
     Configuration configuration = new Configuration();
     configuration.set(CoreOptions.CHECK_LEAKED_CLASSLOADER, false);
     configuration.set(RestOptions.BIND_PORT, "0");
-    configuration.set(JobManagerOptions.SLOT_REQUEST_TIMEOUT, 5000L);
+    configuration.set(JobManagerOptions.SLOT_REQUEST_TIMEOUT, Duration.ofSeconds(5));
 
     // Use FLIP-27 source
     configuration.set(FlinkConfigOptions.TABLE_EXEC_ICEBERG_USE_FLIP27_SOURCE, true);

File: flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/util/TestFlinkPackage.java
Patch:
@@ -29,7 +29,7 @@ public class TestFlinkPackage {
   /** This unit test would need to be adjusted as new Flink version is supported. */
   @Test
   public void testVersion() {
-    assertThat(FlinkPackage.version()).isEqualTo("1.19.0");
+    assertThat(FlinkPackage.version()).isEqualTo("1.20.0");
   }
 
   @Test

File: api/src/main/java/org/apache/iceberg/RewriteManifests.java
Patch:
@@ -54,7 +54,7 @@ public interface RewriteManifests extends SnapshotUpdate<RewriteManifests> {
    * then all manifests will be rewritten.
    *
    * @param predicate Predicate used to determine which manifests to rewrite. If true then the
-   *     manifest file will be included for rewrite. If false then then manifest is kept as-is.
+   *     manifest file will be included for rewrite. If false then the manifest is kept as-is.
    * @return this for method chaining
    */
   RewriteManifests rewriteIf(Predicate<ManifestFile> predicate);

File: api/src/main/java/org/apache/iceberg/Table.java
Patch:
@@ -270,7 +270,7 @@ default AppendFiles newFastAppend() {
   ReplacePartitions newReplacePartitions();
 
   /**
-   * Create a new {@link DeleteFiles delete API} to replace files in this table and commit.
+   * Create a new {@link DeleteFiles delete API} to delete files in this table and commit.
    *
    * @return a new {@link DeleteFiles}
    */
@@ -299,7 +299,7 @@ default UpdatePartitionStatistics updatePartitionStatistics() {
   }
 
   /**
-   * Create a new {@link ExpireSnapshots expire API} to manage snapshots in this table and commit.
+   * Create a new {@link ExpireSnapshots expire API} to expire snapshots in this table and commit.
    *
    * @return a new {@link ExpireSnapshots}
    */

File: hive3/src/main/java/org/apache/iceberg/mr/hive/serde/objectinspector/IcebergTimestampObjectInspectorHive3.java
Patch:
@@ -51,6 +51,7 @@ public LocalDateTime convert(Object o) {
   }
 
   @Override
+  @SuppressWarnings("JavaLocalDateTimeGetNano")
   public Timestamp getPrimitiveJavaObject(Object o) {
     if (o == null) {
       return null;

File: core/src/main/java/org/apache/iceberg/util/PartitionSet.java
Patch:
@@ -200,7 +200,7 @@ public String toString() {
           StringBuilder partitionStringBuilder = new StringBuilder();
           partitionStringBuilder.append(structType.fields().get(i).name());
           partitionStringBuilder.append("=");
-          partitionStringBuilder.append(s.get(i, Object.class).toString());
+          partitionStringBuilder.append(s.get(i, Object.class));
           partitionDataJoiner.add(partitionStringBuilder.toString());
         }
       }

File: kafka-connect/kafka-connect-events/src/test/java/org/apache/iceberg/connect/events/EventTestUtil.java
Patch:
@@ -44,8 +44,7 @@ private EventTestUtil() {}
   static final Schema SCHEMA =
       new Schema(ImmutableList.of(Types.NestedField.required(1, "id", Types.LongType.get())));
 
-  static final PartitionSpec SPEC =
-      PartitionSpec.builderFor(SCHEMA).identity("id").withSpecId(1).build();
+  static final PartitionSpec SPEC = PartitionSpec.builderFor(SCHEMA).identity("id").build();
 
   static final SortOrder ORDER =
       SortOrder.builderFor(SCHEMA).sortBy("id", SortDirection.ASC, NullOrder.NULLS_FIRST).build();

File: kafka-connect/kafka-connect/src/main/java/org/apache/iceberg/connect/IcebergSinkConnector.java
Patch:
@@ -44,9 +44,7 @@ public void start(Map<String, String> connectorProps) {
 
   @Override
   public Class<? extends Task> taskClass() {
-    // FIXME: update this when the connector channel is added
-    //  return IcebergSinkTask.class;
-    return null;
+    return IcebergSinkTask.class;
   }
 
   @Override

File: kafka-connect/kafka-connect/src/main/java/org/apache/iceberg/connect/data/IcebergWriterResult.java
Patch:
@@ -24,14 +24,14 @@
 import org.apache.iceberg.catalog.TableIdentifier;
 import org.apache.iceberg.types.Types.StructType;
 
-public class WriterResult {
+public class IcebergWriterResult {
 
   private final TableIdentifier tableIdentifier;
   private final List<DataFile> dataFiles;
   private final List<DeleteFile> deleteFiles;
   private final StructType partitionStruct;
 
-  public WriterResult(
+  public IcebergWriterResult(
       TableIdentifier tableIdentifier,
       List<DataFile> dataFiles,
       List<DeleteFile> deleteFiles,

File: kafka-connect/kafka-connect/src/main/java/org/apache/iceberg/connect/data/NoOpWriter.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.iceberg.connect.data;
 
 import java.util.List;
+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;
 import org.apache.kafka.connect.sink.SinkRecord;
 
 class NoOpWriter implements RecordWriter {
@@ -28,9 +29,9 @@ public void write(SinkRecord record) {
   }
 
   @Override
-  public List<WriterResult> complete() {
+  public List<IcebergWriterResult> complete() {
     // NO-OP
-    return null;
+    return ImmutableList.of();
   }
 
   @Override

File: kafka-connect/kafka-connect/src/main/java/org/apache/iceberg/connect/data/RecordWriter.java
Patch:
@@ -21,11 +21,11 @@
 import java.util.List;
 import org.apache.kafka.connect.sink.SinkRecord;
 
-public interface RecordWriter extends Cloneable {
+interface RecordWriter extends Cloneable {
 
   void write(SinkRecord record);
 
-  List<WriterResult> complete();
+  List<IcebergWriterResult> complete();
 
   void close();
 }

File: kafka-connect/kafka-connect/src/test/java/org/apache/iceberg/connect/data/BaseWriterTest.java
Patch:
@@ -73,7 +73,7 @@ public void before() {
 
   protected WriteResult writeTest(
       List<Record> rows, IcebergSinkConfig config, Class<?> expectedWriterClass) {
-    try (TaskWriter<Record> writer = Utilities.createTableWriter(table, "name", config)) {
+    try (TaskWriter<Record> writer = RecordUtils.createTableWriter(table, "name", config)) {
       assertThat(writer.getClass()).isEqualTo(expectedWriterClass);
 
       rows.forEach(

File: flink/v1.17/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java
Patch:
@@ -422,7 +422,7 @@ public TimestampData read(TimestampData ignored) {
       long value = readLong();
       return TimestampData.fromLocalDateTime(
           Instant.ofEpochSecond(
-                  Math.floorDiv(value, 1000_000), Math.floorMod(value, 1000_000) * 1000)
+                  Math.floorDiv(value, 1000_000L), Math.floorMod(value, 1000_000L) * 1000L)
               .atOffset(ZoneOffset.UTC)
               .toLocalDateTime());
     }
@@ -444,7 +444,7 @@ public TimestampData read(TimestampData ignored) {
       long value = readLong();
       return TimestampData.fromInstant(
           Instant.ofEpochSecond(
-              Math.floorDiv(value, 1000_000), Math.floorMod(value, 1000_000) * 1000));
+              Math.floorDiv(value, 1000_000L), Math.floorMod(value, 1000_000L) * 1000L));
     }
 
     @Override
@@ -517,7 +517,7 @@ private static class LossyMicrosToMillisTimeReader
     @Override
     public Integer read(Integer reuse) {
       // Discard microseconds since Flink uses millisecond unit for TIME type.
-      return (int) Math.floorDiv(column.nextLong(), 1000);
+      return (int) Math.floorDiv(column.nextLong(), 1000L);
     }
   }
 

File: flink/v1.17/flink/src/test/java/org/apache/iceberg/flink/source/assigner/TestWatermarkBasedSplitAssigner.java
Patch:
@@ -123,7 +123,7 @@ protected List<IcebergSourceSplit> createSplits(
                         .mapToObj(
                             fileNum ->
                                 RandomGenericData.generate(
-                                    SCHEMA, 2, splitNum * filesPerSplit + fileNum))
+                                    SCHEMA, 2, (long) splitNum * filesPerSplit + fileNum))
                         .collect(Collectors.toList())))
         .collect(Collectors.toList());
   }

File: flink/v1.18/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java
Patch:
@@ -422,7 +422,7 @@ public TimestampData read(TimestampData ignored) {
       long value = readLong();
       return TimestampData.fromLocalDateTime(
           Instant.ofEpochSecond(
-                  Math.floorDiv(value, 1000_000), Math.floorMod(value, 1000_000) * 1000)
+                  Math.floorDiv(value, 1000_000L), Math.floorMod(value, 1000_000L) * 1000L)
               .atOffset(ZoneOffset.UTC)
               .toLocalDateTime());
     }
@@ -444,7 +444,7 @@ public TimestampData read(TimestampData ignored) {
       long value = readLong();
       return TimestampData.fromInstant(
           Instant.ofEpochSecond(
-              Math.floorDiv(value, 1000_000), Math.floorMod(value, 1000_000) * 1000));
+              Math.floorDiv(value, 1000_000L), Math.floorMod(value, 1000_000L) * 1000L));
     }
 
     @Override
@@ -517,7 +517,7 @@ private static class LossyMicrosToMillisTimeReader
     @Override
     public Integer read(Integer reuse) {
       // Discard microseconds since Flink uses millisecond unit for TIME type.
-      return (int) Math.floorDiv(column.nextLong(), 1000);
+      return (int) Math.floorDiv(column.nextLong(), 1000L);
     }
   }
 

File: flink/v1.18/flink/src/test/java/org/apache/iceberg/flink/source/assigner/TestWatermarkBasedSplitAssigner.java
Patch:
@@ -123,7 +123,7 @@ protected List<IcebergSourceSplit> createSplits(
                         .mapToObj(
                             fileNum ->
                                 RandomGenericData.generate(
-                                    SCHEMA, 2, splitNum * filesPerSplit + fileNum))
+                                    SCHEMA, 2, (long) splitNum * filesPerSplit + fileNum))
                         .collect(Collectors.toList())))
         .collect(Collectors.toList());
   }

File: flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java
Patch:
@@ -422,7 +422,7 @@ public TimestampData read(TimestampData ignored) {
       long value = readLong();
       return TimestampData.fromLocalDateTime(
           Instant.ofEpochSecond(
-                  Math.floorDiv(value, 1000_000), Math.floorMod(value, 1000_000) * 1000L)
+                  Math.floorDiv(value, 1000_000L), Math.floorMod(value, 1000_000L) * 1000L)
               .atOffset(ZoneOffset.UTC)
               .toLocalDateTime());
     }
@@ -444,7 +444,7 @@ public TimestampData read(TimestampData ignored) {
       long value = readLong();
       return TimestampData.fromInstant(
           Instant.ofEpochSecond(
-              Math.floorDiv(value, 1000_000), Math.floorMod(value, 1000_000) * 1000L));
+              Math.floorDiv(value, 1000_000L), Math.floorMod(value, 1000_000L) * 1000L));
     }
 
     @Override
@@ -517,7 +517,7 @@ private static class LossyMicrosToMillisTimeReader
     @Override
     public Integer read(Integer reuse) {
       // Discard microseconds since Flink uses millisecond unit for TIME type.
-      return (int) Math.floorDiv(column.nextLong(), 1000);
+      return (int) Math.floorDiv(column.nextLong(), 1000L);
     }
   }
 

File: core/src/test/java/org/apache/iceberg/rest/RESTCatalogAdapter.java
Patch:
@@ -254,7 +254,6 @@ private static OAuthTokenResponse handleOAuthRequest(Object body) {
       case "client_credentials":
         return OAuthTokenResponse.builder()
             .withToken("client-credentials-token:sub=" + request.get("client_id"))
-            .withIssuedTokenType("urn:ietf:params:oauth:token-type:access_token")
             .withTokenType("Bearer")
             .build();
 

File: api/src/test/java/org/apache/iceberg/metrics/TestFixedReservoirHistogram.java
Patch:
@@ -113,7 +113,7 @@ public void testMultipleThreadWriters() throws InterruptedException {
                           try {
                             barrier.await(30, SECONDS);
                             for (int i = 1; i <= 100; ++i) {
-                              histogram.update(threadIndex * samplesPerThread + i);
+                              histogram.update((long) threadIndex * samplesPerThread + i);
                             }
                             return threadIndex;
                           } catch (Exception e) {

File: flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java
Patch:
@@ -422,7 +422,7 @@ public TimestampData read(TimestampData ignored) {
       long value = readLong();
       return TimestampData.fromLocalDateTime(
           Instant.ofEpochSecond(
-                  Math.floorDiv(value, 1000_000), Math.floorMod(value, 1000_000) * 1000)
+                  Math.floorDiv(value, 1000_000), Math.floorMod(value, 1000_000) * 1000L)
               .atOffset(ZoneOffset.UTC)
               .toLocalDateTime());
     }
@@ -444,7 +444,7 @@ public TimestampData read(TimestampData ignored) {
       long value = readLong();
       return TimestampData.fromInstant(
           Instant.ofEpochSecond(
-              Math.floorDiv(value, 1000_000), Math.floorMod(value, 1000_000) * 1000));
+              Math.floorDiv(value, 1000_000), Math.floorMod(value, 1000_000) * 1000L));
     }
 
     @Override

File: flink/v1.19/flink/src/test/java/org/apache/iceberg/flink/source/assigner/TestWatermarkBasedSplitAssigner.java
Patch:
@@ -123,7 +123,7 @@ protected List<IcebergSourceSplit> createSplits(
                         .mapToObj(
                             fileNum ->
                                 RandomGenericData.generate(
-                                    SCHEMA, 2, splitNum * filesPerSplit + fileNum))
+                                    SCHEMA, 2, (long) splitNum * filesPerSplit + fileNum))
                         .collect(Collectors.toList())))
         .collect(Collectors.toList());
   }

File: orc/src/main/java/org/apache/iceberg/orc/ExpressionToSearchArgument.java
Patch:
@@ -332,7 +332,7 @@ private <T> Object literal(Type icebergType, T icebergLiteral) {
         return Timestamp.from(
             Instant.ofEpochSecond(
                 Math.floorDiv(microsFromEpoch, 1_000_000),
-                Math.floorMod(microsFromEpoch, 1_000_000) * 1_000));
+                Math.floorMod(microsFromEpoch, 1_000_000) * 1_000L));
       case DECIMAL:
         return new HiveDecimalWritable(HiveDecimal.create((BigDecimal) icebergLiteral, false));
       default:

File: flink/v1.17/flink/src/main/java/org/apache/iceberg/flink/source/split/IcebergSourceSplit.java
Patch:
@@ -32,7 +32,7 @@
 import org.apache.iceberg.BaseCombinedScanTask;
 import org.apache.iceberg.CombinedScanTask;
 import org.apache.iceberg.FileScanTask;
-import org.apache.iceberg.FileScanTaskParser;
+import org.apache.iceberg.ScanTaskParser;
 import org.apache.iceberg.relocated.com.google.common.base.MoreObjects;
 import org.apache.iceberg.relocated.com.google.common.base.Preconditions;
 import org.apache.iceberg.relocated.com.google.common.collect.Iterables;
@@ -154,7 +154,7 @@ private byte[] serialize(int version) throws IOException {
       out.writeInt(fileScanTasks.size());
 
       for (FileScanTask fileScanTask : fileScanTasks) {
-        String taskJson = FileScanTaskParser.toJson(fileScanTask);
+        String taskJson = ScanTaskParser.toJson(fileScanTask);
         writeTaskJson(out, taskJson, version);
       }
 
@@ -199,7 +199,7 @@ private static IcebergSourceSplit deserialize(
     List<FileScanTask> tasks = Lists.newArrayListWithCapacity(taskCount);
     for (int i = 0; i < taskCount; ++i) {
       String taskJson = readTaskJson(in, version);
-      FileScanTask task = FileScanTaskParser.fromJson(taskJson, caseSensitive);
+      FileScanTask task = ScanTaskParser.fromJson(taskJson, caseSensitive);
       tasks.add(task);
     }
 

File: flink/v1.18/flink/src/main/java/org/apache/iceberg/flink/source/split/IcebergSourceSplit.java
Patch:
@@ -32,7 +32,7 @@
 import org.apache.iceberg.BaseCombinedScanTask;
 import org.apache.iceberg.CombinedScanTask;
 import org.apache.iceberg.FileScanTask;
-import org.apache.iceberg.FileScanTaskParser;
+import org.apache.iceberg.ScanTaskParser;
 import org.apache.iceberg.relocated.com.google.common.base.MoreObjects;
 import org.apache.iceberg.relocated.com.google.common.base.Preconditions;
 import org.apache.iceberg.relocated.com.google.common.collect.Iterables;
@@ -154,7 +154,7 @@ private byte[] serialize(int version) throws IOException {
       out.writeInt(fileScanTasks.size());
 
       for (FileScanTask fileScanTask : fileScanTasks) {
-        String taskJson = FileScanTaskParser.toJson(fileScanTask);
+        String taskJson = ScanTaskParser.toJson(fileScanTask);
         writeTaskJson(out, taskJson, version);
       }
 
@@ -199,7 +199,7 @@ private static IcebergSourceSplit deserialize(
     List<FileScanTask> tasks = Lists.newArrayListWithCapacity(taskCount);
     for (int i = 0; i < taskCount; ++i) {
       String taskJson = readTaskJson(in, version);
-      FileScanTask task = FileScanTaskParser.fromJson(taskJson, caseSensitive);
+      FileScanTask task = ScanTaskParser.fromJson(taskJson, caseSensitive);
       tasks.add(task);
     }
 

File: flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/source/split/IcebergSourceSplit.java
Patch:
@@ -32,7 +32,7 @@
 import org.apache.iceberg.BaseCombinedScanTask;
 import org.apache.iceberg.CombinedScanTask;
 import org.apache.iceberg.FileScanTask;
-import org.apache.iceberg.FileScanTaskParser;
+import org.apache.iceberg.ScanTaskParser;
 import org.apache.iceberg.relocated.com.google.common.base.MoreObjects;
 import org.apache.iceberg.relocated.com.google.common.base.Preconditions;
 import org.apache.iceberg.relocated.com.google.common.collect.Iterables;
@@ -154,7 +154,7 @@ private byte[] serialize(int version) throws IOException {
       out.writeInt(fileScanTasks.size());
 
       for (FileScanTask fileScanTask : fileScanTasks) {
-        String taskJson = FileScanTaskParser.toJson(fileScanTask);
+        String taskJson = ScanTaskParser.toJson(fileScanTask);
         writeTaskJson(out, taskJson, version);
       }
 
@@ -199,7 +199,7 @@ private static IcebergSourceSplit deserialize(
     List<FileScanTask> tasks = Lists.newArrayListWithCapacity(taskCount);
     for (int i = 0; i < taskCount; ++i) {
       String taskJson = readTaskJson(in, version);
-      FileScanTask task = FileScanTaskParser.fromJson(taskJson, caseSensitive);
+      FileScanTask task = ScanTaskParser.fromJson(taskJson, caseSensitive);
       tasks.add(task);
     }
 

File: flink/v1.17/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergFilesCommitter.java
Patch:
@@ -766,7 +766,7 @@ public void testFlinkManifests() throws Exception {
   public void testDeleteFiles() throws Exception {
     assumeThat(formatVersion)
         .as("Only support equality-delete in format v2 or later.")
-        .isGreaterThan(2);
+        .isGreaterThan(1);
 
     long timestamp = 0;
     long checkpoint = 10;
@@ -837,7 +837,7 @@ public void testDeleteFiles() throws Exception {
   public void testCommitTwoCheckpointsInSingleTxn() throws Exception {
     assumeThat(formatVersion)
         .as("Only support equality-delete in format v2 or later.")
-        .isGreaterThan(2);
+        .isGreaterThan(1);
 
     long timestamp = 0;
     long checkpoint = 10;

File: flink/v1.18/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergFilesCommitter.java
Patch:
@@ -766,7 +766,7 @@ public void testFlinkManifests() throws Exception {
   public void testDeleteFiles() throws Exception {
     assumeThat(formatVersion)
         .as("Only support equality-delete in format v2 or later.")
-        .isGreaterThan(2);
+        .isGreaterThan(1);
 
     long timestamp = 0;
     long checkpoint = 10;
@@ -837,7 +837,7 @@ public void testDeleteFiles() throws Exception {
   public void testCommitTwoCheckpointsInSingleTxn() throws Exception {
     assumeThat(formatVersion)
         .as("Only support equality-delete in format v2 or later.")
-        .isGreaterThan(2);
+        .isGreaterThan(1);
 
     long timestamp = 0;
     long checkpoint = 10;

File: flink/v1.19/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergFilesCommitter.java
Patch:
@@ -766,7 +766,7 @@ public void testFlinkManifests() throws Exception {
   public void testDeleteFiles() throws Exception {
     assumeThat(formatVersion)
         .as("Only support equality-delete in format v2 or later.")
-        .isGreaterThan(2);
+        .isGreaterThan(1);
 
     long timestamp = 0;
     long checkpoint = 10;
@@ -837,7 +837,7 @@ public void testDeleteFiles() throws Exception {
   public void testCommitTwoCheckpointsInSingleTxn() throws Exception {
     assumeThat(formatVersion)
         .as("Only support equality-delete in format v2 or later.")
-        .isGreaterThan(2);
+        .isGreaterThan(1);
 
     long timestamp = 0;
     long checkpoint = 10;

File: flink/v1.17/flink/src/jmh/java/org/apache/iceberg/flink/sink/shuffle/MapRangePartitionerBenchmark.java
Patch:
@@ -18,6 +18,7 @@
  */
 package org.apache.iceberg.flink.sink.shuffle;
 
+import java.nio.charset.StandardCharsets;
 import java.util.List;
 import java.util.Map;
 import java.util.NavigableMap;
@@ -133,7 +134,8 @@ private static String randomString(String prefix) {
       buffer[i] = (byte) CHARS.charAt(ThreadLocalRandom.current().nextInt(CHARS.length()));
     }
 
-    return prefix + new String(buffer);
+    // CHARS is all ASCII
+    return prefix + new String(buffer, StandardCharsets.US_ASCII);
   }
 
   /** find the index where weightsUDF[index] < weight && weightsUDF[index+1] >= weight */

File: flink/v1.18/flink/src/jmh/java/org/apache/iceberg/flink/sink/shuffle/MapRangePartitionerBenchmark.java
Patch:
@@ -18,6 +18,7 @@
  */
 package org.apache.iceberg.flink.sink.shuffle;
 
+import java.nio.charset.StandardCharsets;
 import java.util.List;
 import java.util.Map;
 import java.util.NavigableMap;
@@ -133,7 +134,8 @@ private static String randomString(String prefix) {
       buffer[i] = (byte) CHARS.charAt(ThreadLocalRandom.current().nextInt(CHARS.length()));
     }
 
-    return prefix + new String(buffer);
+    // CHARS is all ASCII
+    return prefix + new String(buffer, StandardCharsets.US_ASCII);
   }
 
   /** find the index where weightsUDF[index] < weight && weightsUDF[index+1] >= weight */

File: aliyun/src/main/java/org/apache/iceberg/aliyun/oss/OSSURI.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.iceberg.aliyun.oss;
 
 import com.aliyun.oss.internal.OSSUtils;
+import java.util.Locale;
 import java.util.Set;
 import org.apache.iceberg.exceptions.ValidationException;
 import org.apache.iceberg.relocated.com.google.common.base.Preconditions;
@@ -64,7 +65,7 @@ public OSSURI(String location) {
 
     String scheme = schemeSplit[0];
     ValidationException.check(
-        VALID_SCHEMES.contains(scheme.toLowerCase()),
+        VALID_SCHEMES.contains(scheme.toLowerCase(Locale.ROOT)),
         "Invalid scheme: %s in OSS location %s",
         scheme,
         location);

File: api/src/main/java/org/apache/iceberg/catalog/TableIdentifier.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.iceberg.catalog;
 
 import java.util.Arrays;
+import java.util.Locale;
 import java.util.Objects;
 import org.apache.iceberg.relocated.com.google.common.base.Preconditions;
 import org.apache.iceberg.relocated.com.google.common.base.Splitter;
@@ -80,7 +81,7 @@ public String name() {
   public TableIdentifier toLowerCase() {
     String[] newLevels =
         Arrays.stream(namespace().levels()).map(String::toLowerCase).toArray(String[]::new);
-    String newName = name().toLowerCase();
+    String newName = name().toLowerCase(Locale.ROOT);
     return TableIdentifier.of(Namespace.of(newLevels), newName);
   }
 

File: aws/src/test/java/org/apache/iceberg/aws/s3/signer/S3SignerServlet.java
Patch:
@@ -31,6 +31,7 @@
 import java.time.ZoneId;
 import java.util.Arrays;
 import java.util.List;
+import java.util.Locale;
 import java.util.Map;
 import java.util.Set;
 import java.util.function.Predicate;
@@ -185,12 +186,12 @@ private S3SignResponse signRequest(S3SignRequest request) {
 
     Map<String, List<String>> unsignedHeaders =
         request.headers().entrySet().stream()
-            .filter(e -> UNSIGNED_HEADERS.contains(e.getKey().toLowerCase()))
+            .filter(e -> UNSIGNED_HEADERS.contains(e.getKey().toLowerCase(Locale.ROOT)))
             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));
 
     Map<String, List<String>> signedHeaders =
         request.headers().entrySet().stream()
-            .filter(e -> !UNSIGNED_HEADERS.contains(e.getKey().toLowerCase()))
+            .filter(e -> !UNSIGNED_HEADERS.contains(e.getKey().toLowerCase(Locale.ROOT)))
             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));
 
     SdkHttpFullRequest sign =

File: core/src/main/java/org/apache/iceberg/avro/AvroSchemaUtil.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.iceberg.avro;
 
 import java.util.List;
+import java.util.Locale;
 import java.util.Map;
 import java.util.Set;
 import java.util.function.BiFunction;
@@ -528,6 +529,6 @@ private static String sanitize(char character) {
     if (Character.isDigit(character)) {
       return "_" + character;
     }
-    return "_x" + Integer.toHexString(character).toUpperCase();
+    return "_x" + Integer.toHexString(character).toUpperCase(Locale.ROOT);
   }
 }

File: core/src/test/java/org/apache/iceberg/encryption/KeyStoreKmsClient.java
Patch:
@@ -28,6 +28,7 @@
 import java.security.UnrecoverableKeyException;
 import java.security.cert.CertificateException;
 import java.util.Enumeration;
+import java.util.Locale;
 import java.util.Map;
 import javax.crypto.SecretKey;
 import org.apache.iceberg.relocated.com.google.common.base.Preconditions;
@@ -50,13 +51,13 @@ public class KeyStoreKmsClient extends MemoryMockKMS {
   @Override
   public ByteBuffer wrapKey(ByteBuffer key, String wrappingKeyId) {
     // keytool keeps key names in lower case
-    return super.wrapKey(key, wrappingKeyId.toLowerCase());
+    return super.wrapKey(key, wrappingKeyId.toLowerCase(Locale.ROOT));
   }
 
   @Override
   public ByteBuffer unwrapKey(ByteBuffer wrappedKey, String wrappingKeyId) {
     // keytool keeps key names in lower case
-    return super.unwrapKey(wrappedKey, wrappingKeyId.toLowerCase());
+    return super.unwrapKey(wrappedKey, wrappingKeyId.toLowerCase(Locale.ROOT));
   }
 
   @Override

File: core/src/test/java/org/apache/iceberg/hadoop/TestStaticTable.java
Patch:
@@ -21,6 +21,7 @@
 import static org.assertj.core.api.Assertions.assertThat;
 import static org.assertj.core.api.Assertions.assertThatThrownBy;
 
+import java.util.Locale;
 import org.apache.iceberg.HasTableOperations;
 import org.apache.iceberg.MetadataTableType;
 import org.apache.iceberg.StaticTableOperations;
@@ -113,8 +114,8 @@ public void testImmutable() {
   @Test
   public void testMetadataTables() {
     for (MetadataTableType type : MetadataTableType.values()) {
-      String enumName = type.name().replace("_", "").toLowerCase();
-      assertThat(getStaticTable(type).getClass().getName().toLowerCase())
+      String enumName = type.name().replace("_", "").toLowerCase(Locale.ROOT);
+      assertThat(getStaticTable(type).getClass().getName().toLowerCase(Locale.ROOT))
           .as("Should be able to get MetadataTable of type : " + type)
           .contains(enumName);
     }

File: data/src/test/java/org/apache/iceberg/io/TestBaseTaskWriter.java
Patch:
@@ -27,6 +27,7 @@
 import java.nio.file.Paths;
 import java.util.Arrays;
 import java.util.List;
+import java.util.Locale;
 import java.util.stream.Collectors;
 import org.apache.iceberg.FileFormat;
 import org.apache.iceberg.Parameter;
@@ -175,7 +176,7 @@ public void testRollIfExceedTargetFileSize() throws IOException {
 
         int id = record.get(0, Integer.class);
         String data = record.get(1, String.class);
-        Record newRecord = createRecord(id, data.toUpperCase());
+        Record newRecord = createRecord(id, data.toUpperCase(Locale.ROOT));
         expected.add(newRecord);
         taskWriter.write(newRecord);
       }

File: flink/v1.17/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergStreamWriter.java
Patch:
@@ -24,6 +24,7 @@
 import java.io.IOException;
 import java.util.Arrays;
 import java.util.List;
+import java.util.Locale;
 import java.util.Map;
 import java.util.Set;
 import org.apache.flink.streaming.api.operators.BoundedOneInput;
@@ -201,7 +202,7 @@ private Set<String> scanDataFiles() throws IOException {
         LocatedFileStatus status = iterators.next();
         if (status.isFile()) {
           Path path = status.getPath();
-          if (path.getName().endsWith("." + format.toString().toLowerCase())) {
+          if (path.getName().endsWith("." + format.toString().toLowerCase(Locale.ROOT))) {
             paths.add(path.toString());
           }
         }

File: flink/v1.18/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergStreamWriter.java
Patch:
@@ -24,6 +24,7 @@
 import java.io.IOException;
 import java.util.Arrays;
 import java.util.List;
+import java.util.Locale;
 import java.util.Map;
 import java.util.Set;
 import org.apache.flink.streaming.api.operators.BoundedOneInput;
@@ -201,7 +202,7 @@ private Set<String> scanDataFiles() throws IOException {
         LocatedFileStatus status = iterators.next();
         if (status.isFile()) {
           Path path = status.getPath();
-          if (path.getName().endsWith("." + format.toString().toLowerCase())) {
+          if (path.getName().endsWith("." + format.toString().toLowerCase(Locale.ROOT))) {
             paths.add(path.toString());
           }
         }

File: flink/v1.19/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergStreamWriter.java
Patch:
@@ -24,6 +24,7 @@
 import java.io.IOException;
 import java.util.Arrays;
 import java.util.List;
+import java.util.Locale;
 import java.util.Map;
 import java.util.Set;
 import org.apache.flink.streaming.api.operators.BoundedOneInput;
@@ -201,7 +202,7 @@ private Set<String> scanDataFiles() throws IOException {
         LocatedFileStatus status = iterators.next();
         if (status.isFile()) {
           Path path = status.getPath();
-          if (path.getName().endsWith("." + format.toString().toLowerCase())) {
+          if (path.getName().endsWith("." + format.toString().toLowerCase(Locale.ROOT))) {
             paths.add(path.toString());
           }
         }

File: hive-metastore/src/main/java/org/apache/iceberg/hive/CachedClientPool.java
Patch:
@@ -148,7 +148,7 @@ static Key extractKey(String cacheKeys, Configuration conf) {
             !confElements.containsKey(key), "Conf key element %s already specified", key);
         confElements.put(key, conf.get(key));
       } else {
-        KeyElementType type = KeyElementType.valueOf(trimmed.toUpperCase());
+        KeyElementType type = KeyElementType.valueOf(trimmed.toUpperCase(Locale.ROOT));
         switch (type) {
           case UGI:
           case USER_NAME:

File: mr/src/main/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedSupport.java
Patch:
@@ -18,6 +18,7 @@
  */
 package org.apache.hadoop.hive.ql.exec.vector;
 
+import java.util.Locale;
 import java.util.Map;
 import org.apache.iceberg.relocated.com.google.common.collect.Maps;
 
@@ -30,7 +31,7 @@ public enum Support {
     final String lowerCaseName;
 
     Support() {
-      this.lowerCaseName = name().toLowerCase();
+      this.lowerCaseName = name().toLowerCase(Locale.ROOT);
     }
 
     public static final Map<String, Support> nameToSupportMap = Maps.newHashMap();

File: mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.iceberg.mr.hive;
 
 import java.util.List;
+import java.util.Locale;
 import java.util.Properties;
 import java.util.Set;
 import org.apache.hadoop.conf.Configuration;
@@ -84,7 +85,7 @@ public void preCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable)
         .getParameters()
         .put(
             BaseMetastoreTableOperations.TABLE_TYPE_PROP,
-            BaseMetastoreTableOperations.ICEBERG_TABLE_TYPE_VALUE.toUpperCase());
+            BaseMetastoreTableOperations.ICEBERG_TABLE_TYPE_VALUE.toUpperCase(Locale.ROOT));
 
     if (!Catalogs.hiveCatalog(conf, catalogProperties)) {
       // For non-HiveCatalog tables too, we should set the input and output format

File: mr/src/main/java/org/apache/iceberg/mr/hive/serde/objectinspector/IcebergRecordObjectInspector.java
Patch:
@@ -20,6 +20,7 @@
 
 import java.util.Collections;
 import java.util.List;
+import java.util.Locale;
 import java.util.Objects;
 import java.util.stream.Collectors;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
@@ -52,7 +53,7 @@ public IcebergRecordObjectInspector(
           Types.NestedField.of(
               field.fieldId(),
               field.isOptional(),
-              field.name().toLowerCase(),
+              field.name().toLowerCase(Locale.ROOT),
               field.type(),
               field.doc());
       IcebergRecordStructField structField =

File: mr/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergStorageHandlerNoScan.java
Patch:
@@ -28,6 +28,7 @@
 import java.util.Collection;
 import java.util.Collections;
 import java.util.List;
+import java.util.Locale;
 import java.util.Map;
 import java.util.Properties;
 import java.util.Set;
@@ -785,7 +786,7 @@ public void testIcebergAndHmsTableProperties() throws Exception {
               hive_metastoreConstants.META_TABLE_STORAGE, HiveIcebergStorageHandler.class.getName())
           .containsEntry(
               BaseMetastoreTableOperations.TABLE_TYPE_PROP,
-              BaseMetastoreTableOperations.ICEBERG_TABLE_TYPE_VALUE.toUpperCase())
+              BaseMetastoreTableOperations.ICEBERG_TABLE_TYPE_VALUE.toUpperCase(Locale.ROOT))
           .containsEntry(
               BaseMetastoreTableOperations.METADATA_LOCATION_PROP,
               getCurrentSnapshotForHiveCatalogTable(icebergTable))

File: aws/src/main/java/org/apache/iceberg/aws/s3/signer/S3ObjectMapper.java
Patch:
@@ -29,7 +29,7 @@
 import com.fasterxml.jackson.databind.JsonNode;
 import com.fasterxml.jackson.databind.JsonSerializer;
 import com.fasterxml.jackson.databind.ObjectMapper;
-import com.fasterxml.jackson.databind.PropertyNamingStrategy;
+import com.fasterxml.jackson.databind.PropertyNamingStrategies;
 import com.fasterxml.jackson.databind.SerializerProvider;
 import com.fasterxml.jackson.databind.module.SimpleModule;
 import java.io.IOException;
@@ -57,7 +57,7 @@ static ObjectMapper mapper() {
           // even though using new PropertyNamingStrategy.KebabCaseStrategy() is deprecated
           // and PropertyNamingStrategies.KebabCaseStrategy.INSTANCE (introduced in jackson 2.14) is
           // recommended, we can't use it because Spark still relies on jackson 2.13.x stuff
-          MAPPER.setPropertyNamingStrategy(new PropertyNamingStrategy.KebabCaseStrategy());
+          MAPPER.setPropertyNamingStrategy(new PropertyNamingStrategies.KebabCaseStrategy());
           MAPPER.registerModule(initModule());
           isInitialized = true;
         }

File: core/src/main/java/org/apache/iceberg/rest/RESTObjectMapper.java
Patch:
@@ -23,7 +23,7 @@
 import com.fasterxml.jackson.core.JsonFactory;
 import com.fasterxml.jackson.databind.DeserializationFeature;
 import com.fasterxml.jackson.databind.ObjectMapper;
-import com.fasterxml.jackson.databind.PropertyNamingStrategy;
+import com.fasterxml.jackson.databind.PropertyNamingStrategies;
 
 class RESTObjectMapper {
   private static final JsonFactory FACTORY = new JsonFactory();
@@ -38,7 +38,7 @@ static ObjectMapper mapper() {
         if (!isInitialized) {
           MAPPER.setVisibility(PropertyAccessor.FIELD, JsonAutoDetect.Visibility.ANY);
           MAPPER.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false);
-          MAPPER.setPropertyNamingStrategy(new PropertyNamingStrategy.KebabCaseStrategy());
+          MAPPER.setPropertyNamingStrategy(new PropertyNamingStrategies.KebabCaseStrategy());
           RESTSerializers.registerAll(MAPPER);
           isInitialized = true;
         }

File: api/src/main/java/org/apache/iceberg/expressions/CountNonNull.java
Patch:
@@ -39,7 +39,8 @@ protected Long countFor(StructLike row) {
 
   @Override
   protected boolean hasValue(DataFile file) {
-    return file.valueCounts().containsKey(fieldId) && file.nullValueCounts().containsKey(fieldId);
+    return safeContainsKey(file.valueCounts(), fieldId)
+        && file.nullValueCounts().containsKey(fieldId);
   }
 
   @Override

File: api/src/main/java/org/apache/iceberg/expressions/MaxAggregate.java
Patch:
@@ -40,7 +40,7 @@ protected MaxAggregate(BoundTerm<T> term) {
 
   @Override
   protected boolean hasValue(DataFile file) {
-    boolean hasBound = file.upperBounds().containsKey(fieldId);
+    boolean hasBound = safeContainsKey(file.upperBounds(), fieldId);
     Long valueCount = safeGet(file.valueCounts(), fieldId);
     Long nullCount = safeGet(file.nullValueCounts(), fieldId);
     boolean boundAllNull =

File: api/src/main/java/org/apache/iceberg/expressions/MinAggregate.java
Patch:
@@ -40,7 +40,7 @@ protected MinAggregate(BoundTerm<T> term) {
 
   @Override
   protected boolean hasValue(DataFile file) {
-    boolean hasBound = file.lowerBounds().containsKey(fieldId);
+    boolean hasBound = safeContainsKey(file.lowerBounds(), fieldId);
     Long valueCount = safeGet(file.valueCounts(), fieldId);
     Long nullCount = safeGet(file.nullValueCounts(), fieldId);
     boolean boundAllNull =

File: spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/procedures/RemoveOrphanFilesProcedure.java
Patch:
@@ -128,7 +128,7 @@ public InternalRow[] call(InternalRow args) {
               DataTypes.StringType,
               DataTypes.StringType,
               (k, v) -> {
-                equalSchemes.put(k.toString(), v.toString());
+                equalAuthorities.put(k.toString(), v.toString());
                 return BoxedUnit.UNIT;
               });
     }

File: spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/procedures/RemoveOrphanFilesProcedure.java
Patch:
@@ -128,7 +128,7 @@ public InternalRow[] call(InternalRow args) {
               DataTypes.StringType,
               DataTypes.StringType,
               (k, v) -> {
-                equalSchemes.put(k.toString(), v.toString());
+                equalAuthorities.put(k.toString(), v.toString());
                 return BoxedUnit.UNIT;
               });
     }

File: data/src/test/java/org/apache/iceberg/RecordWrapperTest.java
Patch:
@@ -44,8 +44,8 @@ public abstract class RecordWrapperTest {
           optional(113, "bytes", Types.BinaryType.get()),
           required(114, "dec_9_0", Types.DecimalType.of(9, 0)),
           required(115, "dec_11_2", Types.DecimalType.of(11, 2)),
-          required(116, "dec_38_10", Types.DecimalType.of(38, 10)) // maximum precision
-          );
+          required(116, "dec_38_10", Types.DecimalType.of(38, 10)), // maximum precision
+          optional(117, "uuid", Types.UUIDType.get()));
 
   private static final Types.StructType TIMESTAMP_WITHOUT_ZONE =
       Types.StructType.of(

File: spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/source/BaseReader.java
Patch:
@@ -259,7 +259,9 @@ protected class SparkDeleteFilter extends DeleteFilter<InternalRow> {
 
     SparkDeleteFilter(String filePath, List<DeleteFile> deletes, DeleteCounter counter) {
       super(filePath, deletes, tableSchema, expectedSchema, counter);
-      this.asStructLike = new InternalRowWrapper(SparkSchemaUtil.convert(requiredSchema()));
+      this.asStructLike =
+          new InternalRowWrapper(
+              SparkSchemaUtil.convert(requiredSchema()), requiredSchema().asStruct());
     }
 
     @Override

File: spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/source/SparkPartitionedFanoutWriter.java
Patch:
@@ -44,7 +44,7 @@ public SparkPartitionedFanoutWriter(
       StructType sparkSchema) {
     super(spec, format, appenderFactory, fileFactory, io, targetFileSize);
     this.partitionKey = new PartitionKey(spec, schema);
-    this.internalRowWrapper = new InternalRowWrapper(sparkSchema);
+    this.internalRowWrapper = new InternalRowWrapper(sparkSchema, schema.asStruct());
   }
 
   @Override

File: spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/source/SparkPartitionedWriter.java
Patch:
@@ -44,7 +44,7 @@ public SparkPartitionedWriter(
       StructType sparkSchema) {
     super(spec, format, appenderFactory, fileFactory, io, targetFileSize);
     this.partitionKey = new PartitionKey(spec, schema);
-    this.internalRowWrapper = new InternalRowWrapper(sparkSchema);
+    this.internalRowWrapper = new InternalRowWrapper(sparkSchema, schema.asStruct());
   }
 
   @Override

File: spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/source/SparkPositionDeltaWrite.java
Patch:
@@ -375,7 +375,7 @@ private abstract static class BaseDeltaWriter implements DeltaWriter<InternalRow
 
     protected InternalRowWrapper initPartitionRowWrapper(Types.StructType partitionType) {
       StructType sparkPartitionType = (StructType) SparkSchemaUtil.convert(partitionType);
-      return new InternalRowWrapper(sparkPartitionType);
+      return new InternalRowWrapper(sparkPartitionType, partitionType);
     }
 
     protected Map<Integer, StructProjection> buildPartitionProjections(
@@ -645,7 +645,8 @@ private static class PartitionedDeltaWriter extends DeleteAndDataDeltaWriter {
 
       this.dataSpec = table.spec();
       this.dataPartitionKey = new PartitionKey(dataSpec, context.dataSchema());
-      this.internalRowDataWrapper = new InternalRowWrapper(context.dataSparkType());
+      this.internalRowDataWrapper =
+          new InternalRowWrapper(context.dataSparkType(), context.dataSchema().asStruct());
     }
 
     @Override

File: spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/source/SparkWrite.java
Patch:
@@ -741,7 +741,7 @@ private PartitionedDataWriter(
       this.io = io;
       this.spec = spec;
       this.partitionKey = new PartitionKey(spec, dataSchema);
-      this.internalRowWrapper = new InternalRowWrapper(dataSparkType);
+      this.internalRowWrapper = new InternalRowWrapper(dataSparkType, dataSchema.asStruct());
     }
 
     @Override

File: spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/source/TestInternalRowWrapper.java
Patch:
@@ -53,7 +53,8 @@ protected void generateAndValidate(Schema schema, AssertMethod assertMethod) {
     Iterable<InternalRow> rowList = RandomData.generateSpark(schema, numRecords, 101L);
 
     InternalRecordWrapper recordWrapper = new InternalRecordWrapper(schema.asStruct());
-    InternalRowWrapper rowWrapper = new InternalRowWrapper(SparkSchemaUtil.convert(schema));
+    InternalRowWrapper rowWrapper =
+        new InternalRowWrapper(SparkSchemaUtil.convert(schema), schema.asStruct());
 
     Iterator<Record> actual = recordList.iterator();
     Iterator<InternalRow> expected = rowList.iterator();

File: spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkAppenderFactory.java
Patch:
@@ -56,7 +56,7 @@ protected InternalRow createRow(Integer id, String data) {
   protected StructLikeSet expectedRowSet(Iterable<InternalRow> rows) {
     StructLikeSet set = StructLikeSet.create(table.schema().asStruct());
     for (InternalRow row : rows) {
-      InternalRowWrapper wrapper = new InternalRowWrapper(sparkType);
+      InternalRowWrapper wrapper = new InternalRowWrapper(sparkType, table.schema().asStruct());
       set.add(wrapper.wrap(row));
     }
     return set;

File: spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkFileWriterFactory.java
Patch:
@@ -61,7 +61,7 @@ protected StructLikeSet toSet(Iterable<InternalRow> rows) {
     StructLikeSet set = StructLikeSet.create(table.schema().asStruct());
     StructType sparkType = SparkSchemaUtil.convert(table.schema());
     for (InternalRow row : rows) {
-      InternalRowWrapper wrapper = new InternalRowWrapper(sparkType);
+      InternalRowWrapper wrapper = new InternalRowWrapper(sparkType, table.schema().asStruct());
       set.add(wrapper.wrap(row));
     }
     return set;

File: spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkPartitioningWriters.java
Patch:
@@ -61,7 +61,7 @@ protected StructLikeSet toSet(Iterable<InternalRow> rows) {
     StructLikeSet set = StructLikeSet.create(table.schema().asStruct());
     StructType sparkType = SparkSchemaUtil.convert(table.schema());
     for (InternalRow row : rows) {
-      InternalRowWrapper wrapper = new InternalRowWrapper(sparkType);
+      InternalRowWrapper wrapper = new InternalRowWrapper(sparkType, table.schema().asStruct());
       set.add(wrapper.wrap(row));
     }
     return set;

File: spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkPositionDeltaWriters.java
Patch:
@@ -61,7 +61,7 @@ protected StructLikeSet toSet(Iterable<InternalRow> rows) {
     StructLikeSet set = StructLikeSet.create(table.schema().asStruct());
     StructType sparkType = SparkSchemaUtil.convert(table.schema());
     for (InternalRow row : rows) {
-      InternalRowWrapper wrapper = new InternalRowWrapper(sparkType);
+      InternalRowWrapper wrapper = new InternalRowWrapper(sparkType, table.schema().asStruct());
       set.add(wrapper.wrap(row));
     }
     return set;

File: spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReaderDeletes.java
Patch:
@@ -315,7 +315,8 @@ public void testReadEqualityDeleteRows() throws IOException {
           new EqualityDeleteRowReader(task, table, null, table.schema(), false)) {
         while (reader.next()) {
           actualRowSet.add(
-              new InternalRowWrapper(SparkSchemaUtil.convert(table.schema()))
+              new InternalRowWrapper(
+                      SparkSchemaUtil.convert(table.schema()), table.schema().asStruct())
                   .wrap(reader.get().copy()));
         }
       }

File: spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/source/BaseReader.java
Patch:
@@ -264,7 +264,9 @@ protected class SparkDeleteFilter extends DeleteFilter<InternalRow> {
 
     SparkDeleteFilter(String filePath, List<DeleteFile> deletes, DeleteCounter counter) {
       super(filePath, deletes, tableSchema, expectedSchema, counter);
-      this.asStructLike = new InternalRowWrapper(SparkSchemaUtil.convert(requiredSchema()));
+      this.asStructLike =
+          new InternalRowWrapper(
+              SparkSchemaUtil.convert(requiredSchema()), requiredSchema().asStruct());
     }
 
     @Override

File: spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/source/SparkPartitionedFanoutWriter.java
Patch:
@@ -44,7 +44,7 @@ public SparkPartitionedFanoutWriter(
       StructType sparkSchema) {
     super(spec, format, appenderFactory, fileFactory, io, targetFileSize);
     this.partitionKey = new PartitionKey(spec, schema);
-    this.internalRowWrapper = new InternalRowWrapper(sparkSchema);
+    this.internalRowWrapper = new InternalRowWrapper(sparkSchema, schema.asStruct());
   }
 
   @Override

File: spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/source/SparkPartitionedWriter.java
Patch:
@@ -44,7 +44,7 @@ public SparkPartitionedWriter(
       StructType sparkSchema) {
     super(spec, format, appenderFactory, fileFactory, io, targetFileSize);
     this.partitionKey = new PartitionKey(spec, schema);
-    this.internalRowWrapper = new InternalRowWrapper(sparkSchema);
+    this.internalRowWrapper = new InternalRowWrapper(sparkSchema, schema.asStruct());
   }
 
   @Override

File: spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/source/SparkPositionDeltaWrite.java
Patch:
@@ -391,7 +391,7 @@ private abstract static class BaseDeltaWriter implements DeltaWriter<InternalRow
 
     protected InternalRowWrapper initPartitionRowWrapper(Types.StructType partitionType) {
       StructType sparkPartitionType = (StructType) SparkSchemaUtil.convert(partitionType);
-      return new InternalRowWrapper(sparkPartitionType);
+      return new InternalRowWrapper(sparkPartitionType, partitionType);
     }
 
     protected Map<Integer, StructProjection> buildPartitionProjections(
@@ -653,7 +653,8 @@ private static class PartitionedDeltaWriter extends DeleteAndDataDeltaWriter {
 
       this.dataSpec = table.spec();
       this.dataPartitionKey = new PartitionKey(dataSpec, context.dataSchema());
-      this.internalRowDataWrapper = new InternalRowWrapper(context.dataSparkType());
+      this.internalRowDataWrapper =
+          new InternalRowWrapper(context.dataSparkType(), context.dataSchema().asStruct());
     }
 
     @Override

File: spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/source/SparkWrite.java
Patch:
@@ -756,7 +756,7 @@ private PartitionedDataWriter(
       this.io = io;
       this.spec = spec;
       this.partitionKey = new PartitionKey(spec, dataSchema);
-      this.internalRowWrapper = new InternalRowWrapper(dataSparkType);
+      this.internalRowWrapper = new InternalRowWrapper(dataSparkType, dataSchema.asStruct());
     }
 
     @Override

File: spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestInternalRowWrapper.java
Patch:
@@ -53,7 +53,8 @@ protected void generateAndValidate(Schema schema, AssertMethod assertMethod) {
     Iterable<InternalRow> rowList = RandomData.generateSpark(schema, numRecords, 101L);
 
     InternalRecordWrapper recordWrapper = new InternalRecordWrapper(schema.asStruct());
-    InternalRowWrapper rowWrapper = new InternalRowWrapper(SparkSchemaUtil.convert(schema));
+    InternalRowWrapper rowWrapper =
+        new InternalRowWrapper(SparkSchemaUtil.convert(schema), schema.asStruct());
 
     Iterator<Record> actual = recordList.iterator();
     Iterator<InternalRow> expected = rowList.iterator();

File: spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkAppenderFactory.java
Patch:
@@ -56,7 +56,7 @@ protected InternalRow createRow(Integer id, String data) {
   protected StructLikeSet expectedRowSet(Iterable<InternalRow> rows) {
     StructLikeSet set = StructLikeSet.create(table.schema().asStruct());
     for (InternalRow row : rows) {
-      InternalRowWrapper wrapper = new InternalRowWrapper(sparkType);
+      InternalRowWrapper wrapper = new InternalRowWrapper(sparkType, table.schema().asStruct());
       set.add(wrapper.wrap(row));
     }
     return set;

File: spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkFileWriterFactory.java
Patch:
@@ -61,7 +61,7 @@ protected StructLikeSet toSet(Iterable<InternalRow> rows) {
     StructLikeSet set = StructLikeSet.create(table.schema().asStruct());
     StructType sparkType = SparkSchemaUtil.convert(table.schema());
     for (InternalRow row : rows) {
-      InternalRowWrapper wrapper = new InternalRowWrapper(sparkType);
+      InternalRowWrapper wrapper = new InternalRowWrapper(sparkType, table.schema().asStruct());
       set.add(wrapper.wrap(row));
     }
     return set;

File: spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkPartitioningWriters.java
Patch:
@@ -61,7 +61,7 @@ protected StructLikeSet toSet(Iterable<InternalRow> rows) {
     StructLikeSet set = StructLikeSet.create(table.schema().asStruct());
     StructType sparkType = SparkSchemaUtil.convert(table.schema());
     for (InternalRow row : rows) {
-      InternalRowWrapper wrapper = new InternalRowWrapper(sparkType);
+      InternalRowWrapper wrapper = new InternalRowWrapper(sparkType, table.schema().asStruct());
       set.add(wrapper.wrap(row));
     }
     return set;

File: spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkPositionDeltaWriters.java
Patch:
@@ -61,7 +61,7 @@ protected StructLikeSet toSet(Iterable<InternalRow> rows) {
     StructLikeSet set = StructLikeSet.create(table.schema().asStruct());
     StructType sparkType = SparkSchemaUtil.convert(table.schema());
     for (InternalRow row : rows) {
-      InternalRowWrapper wrapper = new InternalRowWrapper(sparkType);
+      InternalRowWrapper wrapper = new InternalRowWrapper(sparkType, table.schema().asStruct());
       set.add(wrapper.wrap(row));
     }
     return set;

File: spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReaderDeletes.java
Patch:
@@ -324,7 +324,8 @@ public void testReadEqualityDeleteRows() throws IOException {
           new EqualityDeleteRowReader(task, table, null, table.schema(), false)) {
         while (reader.next()) {
           actualRowSet.add(
-              new InternalRowWrapper(SparkSchemaUtil.convert(table.schema()))
+              new InternalRowWrapper(
+                      SparkSchemaUtil.convert(table.schema()), table.schema().asStruct())
                   .wrap(reader.get().copy()));
         }
       }

File: spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/source/BaseReader.java
Patch:
@@ -251,7 +251,9 @@ protected class SparkDeleteFilter extends DeleteFilter<InternalRow> {
 
     SparkDeleteFilter(String filePath, List<DeleteFile> deletes, DeleteCounter counter) {
       super(filePath, deletes, tableSchema, expectedSchema, counter);
-      this.asStructLike = new InternalRowWrapper(SparkSchemaUtil.convert(requiredSchema()));
+      this.asStructLike =
+          new InternalRowWrapper(
+              SparkSchemaUtil.convert(requiredSchema()), requiredSchema().asStruct());
     }
 
     @Override

File: spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/source/SparkPartitionedFanoutWriter.java
Patch:
@@ -44,7 +44,7 @@ public SparkPartitionedFanoutWriter(
       StructType sparkSchema) {
     super(spec, format, appenderFactory, fileFactory, io, targetFileSize);
     this.partitionKey = new PartitionKey(spec, schema);
-    this.internalRowWrapper = new InternalRowWrapper(sparkSchema);
+    this.internalRowWrapper = new InternalRowWrapper(sparkSchema, schema.asStruct());
   }
 
   @Override

File: spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/source/SparkPartitionedWriter.java
Patch:
@@ -44,7 +44,7 @@ public SparkPartitionedWriter(
       StructType sparkSchema) {
     super(spec, format, appenderFactory, fileFactory, io, targetFileSize);
     this.partitionKey = new PartitionKey(spec, schema);
-    this.internalRowWrapper = new InternalRowWrapper(sparkSchema);
+    this.internalRowWrapper = new InternalRowWrapper(sparkSchema, schema.asStruct());
   }
 
   @Override

File: spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/source/SparkPositionDeltaWrite.java
Patch:
@@ -402,7 +402,7 @@ private abstract static class BaseDeltaWriter implements DeltaWriter<InternalRow
 
     protected InternalRowWrapper initPartitionRowWrapper(Types.StructType partitionType) {
       StructType sparkPartitionType = (StructType) SparkSchemaUtil.convert(partitionType);
-      return new InternalRowWrapper(sparkPartitionType);
+      return new InternalRowWrapper(sparkPartitionType, partitionType);
     }
 
     protected Map<Integer, StructProjection> buildPartitionProjections(
@@ -663,7 +663,8 @@ private static class PartitionedDeltaWriter extends DeleteAndDataDeltaWriter {
 
       this.dataSpec = table.spec();
       this.dataPartitionKey = new PartitionKey(dataSpec, context.dataSchema());
-      this.internalRowDataWrapper = new InternalRowWrapper(context.dataSparkType());
+      this.internalRowDataWrapper =
+          new InternalRowWrapper(context.dataSparkType(), context.dataSchema().asStruct());
     }
 
     @Override

File: spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/source/SparkWrite.java
Patch:
@@ -772,7 +772,7 @@ private PartitionedDataWriter(
       this.io = io;
       this.spec = spec;
       this.partitionKey = new PartitionKey(spec, dataSchema);
-      this.internalRowWrapper = new InternalRowWrapper(dataSparkType);
+      this.internalRowWrapper = new InternalRowWrapper(dataSparkType, dataSchema.asStruct());
     }
 
     @Override

File: spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestInternalRowWrapper.java
Patch:
@@ -54,7 +54,8 @@ protected void generateAndValidate(Schema schema, AssertMethod assertMethod) {
     Iterable<InternalRow> rowList = RandomData.generateSpark(schema, numRecords, 101L);
 
     InternalRecordWrapper recordWrapper = new InternalRecordWrapper(schema.asStruct());
-    InternalRowWrapper rowWrapper = new InternalRowWrapper(SparkSchemaUtil.convert(schema));
+    InternalRowWrapper rowWrapper =
+        new InternalRowWrapper(SparkSchemaUtil.convert(schema), schema.asStruct());
 
     Iterator<Record> actual = recordList.iterator();
     Iterator<InternalRow> expected = rowList.iterator();

File: spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkAppenderFactory.java
Patch:
@@ -56,7 +56,7 @@ protected InternalRow createRow(Integer id, String data) {
   protected StructLikeSet expectedRowSet(Iterable<InternalRow> rows) {
     StructLikeSet set = StructLikeSet.create(table.schema().asStruct());
     for (InternalRow row : rows) {
-      InternalRowWrapper wrapper = new InternalRowWrapper(sparkType);
+      InternalRowWrapper wrapper = new InternalRowWrapper(sparkType, table.schema().asStruct());
       set.add(wrapper.wrap(row));
     }
     return set;

File: spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkFileWriterFactory.java
Patch:
@@ -61,7 +61,7 @@ protected StructLikeSet toSet(Iterable<InternalRow> rows) {
     StructLikeSet set = StructLikeSet.create(table.schema().asStruct());
     StructType sparkType = SparkSchemaUtil.convert(table.schema());
     for (InternalRow row : rows) {
-      InternalRowWrapper wrapper = new InternalRowWrapper(sparkType);
+      InternalRowWrapper wrapper = new InternalRowWrapper(sparkType, table.schema().asStruct());
       set.add(wrapper.wrap(row));
     }
     return set;

File: spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkPartitioningWriters.java
Patch:
@@ -61,7 +61,7 @@ protected StructLikeSet toSet(Iterable<InternalRow> rows) {
     StructLikeSet set = StructLikeSet.create(table.schema().asStruct());
     StructType sparkType = SparkSchemaUtil.convert(table.schema());
     for (InternalRow row : rows) {
-      InternalRowWrapper wrapper = new InternalRowWrapper(sparkType);
+      InternalRowWrapper wrapper = new InternalRowWrapper(sparkType, table.schema().asStruct());
       set.add(wrapper.wrap(row));
     }
     return set;

File: spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkPositionDeltaWriters.java
Patch:
@@ -61,7 +61,7 @@ protected StructLikeSet toSet(Iterable<InternalRow> rows) {
     StructLikeSet set = StructLikeSet.create(table.schema().asStruct());
     StructType sparkType = SparkSchemaUtil.convert(table.schema());
     for (InternalRow row : rows) {
-      InternalRowWrapper wrapper = new InternalRowWrapper(sparkType);
+      InternalRowWrapper wrapper = new InternalRowWrapper(sparkType, table.schema().asStruct());
       set.add(wrapper.wrap(row));
     }
     return set;

File: spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReaderDeletes.java
Patch:
@@ -324,7 +324,8 @@ public void testReadEqualityDeleteRows() throws IOException {
           new EqualityDeleteRowReader(task, table, null, table.schema(), false)) {
         while (reader.next()) {
           actualRowSet.add(
-              new InternalRowWrapper(SparkSchemaUtil.convert(table.schema()))
+              new InternalRowWrapper(
+                      SparkSchemaUtil.convert(table.schema()), table.schema().asStruct())
                   .wrap(reader.get().copy()));
         }
       }

File: spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/procedures/RemoveOrphanFilesProcedure.java
Patch:
@@ -128,7 +128,7 @@ public InternalRow[] call(InternalRow args) {
               DataTypes.StringType,
               DataTypes.StringType,
               (k, v) -> {
-                equalSchemes.put(k.toString(), v.toString());
+                equalAuthorities.put(k.toString(), v.toString());
                 return BoxedUnit.UNIT;
               });
     }

File: core/src/main/java/org/apache/iceberg/jdbc/JdbcUtil.java
Patch:
@@ -43,6 +43,8 @@ final class JdbcUtil {
   static final String INIT_CATALOG_TABLES_PROPERTY =
       JdbcCatalog.PROPERTY_PREFIX + "init-catalog-tables";
 
+  static final String RETRYABLE_STATUS_CODES = "retryable_status_codes";
+
   enum SchemaVersion {
     V0,
     V1

File: data/src/test/java/org/apache/iceberg/data/DataTest.java
Patch:
@@ -38,7 +38,7 @@ public abstract class DataTest {
 
   protected abstract void writeAndValidate(Schema schema) throws IOException;
 
-  private static final StructType SUPPORTED_PRIMITIVES =
+  protected static final StructType SUPPORTED_PRIMITIVES =
       StructType.of(
           required(100, "id", LongType.get()),
           optional(101, "data", Types.StringType.get()),

File: aws/src/integration/java/org/apache/iceberg/aws/glue/TestGlueCatalogTable.java
Patch:
@@ -94,7 +94,7 @@ public void testCreateTable() {
     assertThat(response.table().storageDescriptor().columns()).hasSameSizeAs(schema.columns());
     assertThat(response.table().partitionKeys()).hasSameSizeAs(partitionSpec.fields());
     assertThat(response.table().storageDescriptor().additionalLocations())
-        .isEqualTo(tableLocationProperties.values());
+        .containsExactlyInAnyOrderElementsOf(tableLocationProperties.values());
     // verify metadata file exists in S3
     String metaLocation =
         response.table().parameters().get(BaseMetastoreTableOperations.METADATA_LOCATION_PROP);

File: core/src/main/java/org/apache/iceberg/jdbc/JdbcUtil.java
Patch:
@@ -335,7 +335,7 @@ enum SchemaVersion {
           + TABLE_NAMESPACE
           + " = ? OR "
           + TABLE_NAMESPACE
-          + " LIKE ? ESCAPE '\\')"
+          + " LIKE ? ESCAPE '!')"
           + " LIMIT 1";
   static final String LIST_NAMESPACES_SQL =
       "SELECT DISTINCT "
@@ -426,7 +426,7 @@ enum SchemaVersion {
           + NAMESPACE_NAME
           + " = ? OR "
           + NAMESPACE_NAME
-          + " LIKE ? ESCAPE '\\' "
+          + " LIKE ? ESCAPE '!' "
           + " ) ";
   static final String INSERT_NAMESPACE_PROPERTIES_SQL =
       "INSERT INTO "
@@ -783,7 +783,7 @@ static boolean namespaceExists(
     // when namespace has sub-namespace then additionally checking it with LIKE statement.
     // catalog.db can exists as: catalog.db.ns1 or catalog.db.ns1.ns2
     String namespaceStartsWith =
-        namespaceEquals.replace("\\", "\\\\").replace("_", "\\_").replace("%", "\\%") + ".%";
+        namespaceEquals.replace("!", "!!").replace("_", "!_").replace("%", "!%") + ".%";
     if (exists(connections, GET_NAMESPACE_SQL, catalogName, namespaceEquals, namespaceStartsWith)) {
       return true;
     }

File: spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/functions/BucketFunction.java
Patch:
@@ -25,7 +25,6 @@
 import org.apache.iceberg.util.BucketUtil;
 import org.apache.spark.sql.catalyst.InternalRow;
 import org.apache.spark.sql.connector.catalog.functions.BoundFunction;
-import org.apache.spark.sql.connector.catalog.functions.ScalarFunction;
 import org.apache.spark.sql.connector.catalog.functions.UnboundFunction;
 import org.apache.spark.sql.types.BinaryType;
 import org.apache.spark.sql.types.ByteType;
@@ -115,7 +114,7 @@ public String name() {
     return "bucket";
   }
 
-  public abstract static class BucketBase implements ScalarFunction<Integer> {
+  public abstract static class BucketBase extends BaseScalarFunction<Integer> {
     public static int apply(int numBuckets, int hashedValue) {
       return (hashedValue & Integer.MAX_VALUE) % numBuckets;
     }

File: spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/functions/DaysFunction.java
Patch:
@@ -21,7 +21,6 @@
 import org.apache.iceberg.util.DateTimeUtil;
 import org.apache.spark.sql.catalyst.InternalRow;
 import org.apache.spark.sql.connector.catalog.functions.BoundFunction;
-import org.apache.spark.sql.connector.catalog.functions.ScalarFunction;
 import org.apache.spark.sql.types.DataType;
 import org.apache.spark.sql.types.DataTypes;
 import org.apache.spark.sql.types.DateType;
@@ -61,7 +60,7 @@ public String name() {
     return "days";
   }
 
-  private abstract static class BaseToDaysFunction implements ScalarFunction<Integer> {
+  private abstract static class BaseToDaysFunction extends BaseScalarFunction<Integer> {
     @Override
     public String name() {
       return "days";

File: spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/functions/HoursFunction.java
Patch:
@@ -21,7 +21,6 @@
 import org.apache.iceberg.util.DateTimeUtil;
 import org.apache.spark.sql.catalyst.InternalRow;
 import org.apache.spark.sql.connector.catalog.functions.BoundFunction;
-import org.apache.spark.sql.connector.catalog.functions.ScalarFunction;
 import org.apache.spark.sql.types.DataType;
 import org.apache.spark.sql.types.DataTypes;
 import org.apache.spark.sql.types.TimestampNTZType;
@@ -58,7 +57,7 @@ public String name() {
     return "hours";
   }
 
-  public static class TimestampToHoursFunction implements ScalarFunction<Integer> {
+  public static class TimestampToHoursFunction extends BaseScalarFunction<Integer> {
     // magic method used in codegen
     public static int invoke(long micros) {
       return DateTimeUtil.microsToHours(micros);
@@ -91,7 +90,7 @@ public Integer produceResult(InternalRow input) {
     }
   }
 
-  public static class TimestampNtzToHoursFunction implements ScalarFunction<Integer> {
+  public static class TimestampNtzToHoursFunction extends BaseScalarFunction<Integer> {
     // magic method used in codegen
     public static int invoke(long micros) {
       return DateTimeUtil.microsToHours(micros);

File: spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/functions/IcebergVersionFunction.java
Patch:
@@ -21,7 +21,6 @@
 import org.apache.iceberg.IcebergBuild;
 import org.apache.spark.sql.catalyst.InternalRow;
 import org.apache.spark.sql.connector.catalog.functions.BoundFunction;
-import org.apache.spark.sql.connector.catalog.functions.ScalarFunction;
 import org.apache.spark.sql.connector.catalog.functions.UnboundFunction;
 import org.apache.spark.sql.types.DataType;
 import org.apache.spark.sql.types.DataTypes;
@@ -55,7 +54,7 @@ public String name() {
 
   // Implementing class cannot be private, otherwise Spark is unable to access the static invoke
   // function during code-gen and calling the function fails
-  static class IcebergVersionFunctionImpl implements ScalarFunction<UTF8String> {
+  static class IcebergVersionFunctionImpl extends BaseScalarFunction<UTF8String> {
     private static final UTF8String VERSION = UTF8String.fromString(IcebergBuild.version());
 
     // magic function used in code-gen. must be named `invoke`.

File: spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/functions/MonthsFunction.java
Patch:
@@ -21,7 +21,6 @@
 import org.apache.iceberg.util.DateTimeUtil;
 import org.apache.spark.sql.catalyst.InternalRow;
 import org.apache.spark.sql.connector.catalog.functions.BoundFunction;
-import org.apache.spark.sql.connector.catalog.functions.ScalarFunction;
 import org.apache.spark.sql.types.DataType;
 import org.apache.spark.sql.types.DataTypes;
 import org.apache.spark.sql.types.DateType;
@@ -61,7 +60,7 @@ public String name() {
     return "months";
   }
 
-  private abstract static class BaseToMonthsFunction implements ScalarFunction<Integer> {
+  private abstract static class BaseToMonthsFunction extends BaseScalarFunction<Integer> {
     @Override
     public String name() {
       return "months";

File: spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/functions/TruncateFunction.java
Patch:
@@ -27,7 +27,6 @@
 import org.apache.iceberg.util.TruncateUtil;
 import org.apache.spark.sql.catalyst.InternalRow;
 import org.apache.spark.sql.connector.catalog.functions.BoundFunction;
-import org.apache.spark.sql.connector.catalog.functions.ScalarFunction;
 import org.apache.spark.sql.connector.catalog.functions.UnboundFunction;
 import org.apache.spark.sql.types.BinaryType;
 import org.apache.spark.sql.types.ByteType;
@@ -108,7 +107,7 @@ public String name() {
     return "truncate";
   }
 
-  public abstract static class TruncateBase<T> implements ScalarFunction<T> {
+  public abstract static class TruncateBase<T> extends BaseScalarFunction<T> {
     @Override
     public String name() {
       return "truncate";

File: spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/functions/YearsFunction.java
Patch:
@@ -21,7 +21,6 @@
 import org.apache.iceberg.util.DateTimeUtil;
 import org.apache.spark.sql.catalyst.InternalRow;
 import org.apache.spark.sql.connector.catalog.functions.BoundFunction;
-import org.apache.spark.sql.connector.catalog.functions.ScalarFunction;
 import org.apache.spark.sql.types.DataType;
 import org.apache.spark.sql.types.DataTypes;
 import org.apache.spark.sql.types.DateType;
@@ -61,7 +60,7 @@ public String name() {
     return "years";
   }
 
-  private abstract static class BaseToYearsFunction implements ScalarFunction<Integer> {
+  private abstract static class BaseToYearsFunction extends BaseScalarFunction<Integer> {
     @Override
     public String name() {
       return "years";

File: core/src/main/java/org/apache/iceberg/jdbc/JdbcTableOperations.java
Patch:
@@ -138,7 +138,7 @@ public void doCommit(TableMetadata base, TableMetadata metadata) {
       throw new UncheckedSQLException(e, "Database warning");
     } catch (SQLException e) {
       // SQLite doesn't set SQLState or throw SQLIntegrityConstraintViolationException
-      if (e.getMessage().contains("constraint failed")) {
+      if (e.getMessage() != null && e.getMessage().contains("constraint failed")) {
         throw new AlreadyExistsException("Table already exists: %s", tableIdentifier);
       }
 

File: core/src/main/java/org/apache/iceberg/jdbc/JdbcViewOperations.java
Patch:
@@ -129,7 +129,7 @@ protected void doCommit(ViewMetadata base, ViewMetadata metadata) {
       throw new UncheckedSQLException(e, "Database warning");
     } catch (SQLException e) {
       // SQLite doesn't set SQLState or throw SQLIntegrityConstraintViolationException
-      if (e.getMessage().contains("constraint failed")) {
+      if (e.getMessage() != null && e.getMessage().contains("constraint failed")) {
         throw new AlreadyExistsException("View already exists: %s", viewIdentifier);
       }
 

File: hive-metastore/src/main/java/org/apache/iceberg/hive/HiveClientPool.java
Patch:
@@ -73,7 +73,8 @@ protected IMetaStoreClient newClient() {
     } catch (MetaException e) {
       throw new RuntimeMetaException(e, "Failed to connect to Hive Metastore");
     } catch (Throwable t) {
-      if (t.getMessage().contains("Another instance of Derby may have already booted")) {
+      if (t.getMessage() != null
+          && t.getMessage().contains("Another instance of Derby may have already booted")) {
         throw new RuntimeMetaException(
             t,
             "Failed to start an embedded metastore because embedded "

File: core/src/main/java/org/apache/iceberg/rest/RESTSessionCatalog.java
Patch:
@@ -602,7 +602,7 @@ private void shutdownRefreshExecutor() {
           });
 
       try {
-        if (service.awaitTermination(1, TimeUnit.MINUTES)) {
+        if (!service.awaitTermination(1, TimeUnit.MINUTES)) {
           LOG.warn("Timed out waiting for refresh executor to terminate");
         }
       } catch (InterruptedException e) {

File: api/src/test/java/org/apache/iceberg/expressions/TestStrictMetricsEvaluator.java
Patch:
@@ -592,7 +592,7 @@ public void testIntegerNotIn() {
     boolean shouldRead =
         new StrictMetricsEvaluator(SCHEMA, notIn("id", INT_MIN_VALUE - 25, INT_MIN_VALUE - 24))
             .eval(FILE);
-    assertThat(shouldRead).as("Should not match: all values !=5 and !=6").isTrue();
+    assertThat(shouldRead).as("Should match: all values !=5 and !=6").isTrue();
 
     shouldRead =
         new StrictMetricsEvaluator(SCHEMA, notIn("id", INT_MIN_VALUE - 1, INT_MIN_VALUE))

File: spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/functions/BucketFunction.java
Patch:
@@ -25,7 +25,6 @@
 import org.apache.iceberg.util.BucketUtil;
 import org.apache.spark.sql.catalyst.InternalRow;
 import org.apache.spark.sql.connector.catalog.functions.BoundFunction;
-import org.apache.spark.sql.connector.catalog.functions.ScalarFunction;
 import org.apache.spark.sql.connector.catalog.functions.UnboundFunction;
 import org.apache.spark.sql.types.BinaryType;
 import org.apache.spark.sql.types.ByteType;
@@ -115,7 +114,7 @@ public String name() {
     return "bucket";
   }
 
-  public abstract static class BucketBase implements ScalarFunction<Integer> {
+  public abstract static class BucketBase extends BaseScalarFunction<Integer> {
     public static int apply(int numBuckets, int hashedValue) {
       return (hashedValue & Integer.MAX_VALUE) % numBuckets;
     }

File: spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/functions/DaysFunction.java
Patch:
@@ -21,7 +21,6 @@
 import org.apache.iceberg.util.DateTimeUtil;
 import org.apache.spark.sql.catalyst.InternalRow;
 import org.apache.spark.sql.connector.catalog.functions.BoundFunction;
-import org.apache.spark.sql.connector.catalog.functions.ScalarFunction;
 import org.apache.spark.sql.types.DataType;
 import org.apache.spark.sql.types.DataTypes;
 import org.apache.spark.sql.types.DateType;
@@ -61,7 +60,7 @@ public String name() {
     return "days";
   }
 
-  private abstract static class BaseToDaysFunction implements ScalarFunction<Integer> {
+  private abstract static class BaseToDaysFunction extends BaseScalarFunction<Integer> {
     @Override
     public String name() {
       return "days";

File: spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/functions/HoursFunction.java
Patch:
@@ -21,7 +21,6 @@
 import org.apache.iceberg.util.DateTimeUtil;
 import org.apache.spark.sql.catalyst.InternalRow;
 import org.apache.spark.sql.connector.catalog.functions.BoundFunction;
-import org.apache.spark.sql.connector.catalog.functions.ScalarFunction;
 import org.apache.spark.sql.types.DataType;
 import org.apache.spark.sql.types.DataTypes;
 import org.apache.spark.sql.types.TimestampNTZType;
@@ -58,7 +57,7 @@ public String name() {
     return "hours";
   }
 
-  public static class TimestampToHoursFunction implements ScalarFunction<Integer> {
+  public static class TimestampToHoursFunction extends BaseScalarFunction<Integer> {
     // magic method used in codegen
     public static int invoke(long micros) {
       return DateTimeUtil.microsToHours(micros);
@@ -91,7 +90,7 @@ public Integer produceResult(InternalRow input) {
     }
   }
 
-  public static class TimestampNtzToHoursFunction implements ScalarFunction<Integer> {
+  public static class TimestampNtzToHoursFunction extends BaseScalarFunction<Integer> {
     // magic method used in codegen
     public static int invoke(long micros) {
       return DateTimeUtil.microsToHours(micros);

File: spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/functions/IcebergVersionFunction.java
Patch:
@@ -21,7 +21,6 @@
 import org.apache.iceberg.IcebergBuild;
 import org.apache.spark.sql.catalyst.InternalRow;
 import org.apache.spark.sql.connector.catalog.functions.BoundFunction;
-import org.apache.spark.sql.connector.catalog.functions.ScalarFunction;
 import org.apache.spark.sql.connector.catalog.functions.UnboundFunction;
 import org.apache.spark.sql.types.DataType;
 import org.apache.spark.sql.types.DataTypes;
@@ -55,7 +54,7 @@ public String name() {
 
   // Implementing class cannot be private, otherwise Spark is unable to access the static invoke
   // function during code-gen and calling the function fails
-  static class IcebergVersionFunctionImpl implements ScalarFunction<UTF8String> {
+  static class IcebergVersionFunctionImpl extends BaseScalarFunction<UTF8String> {
     private static final UTF8String VERSION = UTF8String.fromString(IcebergBuild.version());
 
     // magic function used in code-gen. must be named `invoke`.

File: spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/functions/MonthsFunction.java
Patch:
@@ -21,7 +21,6 @@
 import org.apache.iceberg.util.DateTimeUtil;
 import org.apache.spark.sql.catalyst.InternalRow;
 import org.apache.spark.sql.connector.catalog.functions.BoundFunction;
-import org.apache.spark.sql.connector.catalog.functions.ScalarFunction;
 import org.apache.spark.sql.types.DataType;
 import org.apache.spark.sql.types.DataTypes;
 import org.apache.spark.sql.types.DateType;
@@ -61,7 +60,7 @@ public String name() {
     return "months";
   }
 
-  private abstract static class BaseToMonthsFunction implements ScalarFunction<Integer> {
+  private abstract static class BaseToMonthsFunction extends BaseScalarFunction<Integer> {
     @Override
     public String name() {
       return "months";

File: spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/functions/TruncateFunction.java
Patch:
@@ -27,7 +27,6 @@
 import org.apache.iceberg.util.TruncateUtil;
 import org.apache.spark.sql.catalyst.InternalRow;
 import org.apache.spark.sql.connector.catalog.functions.BoundFunction;
-import org.apache.spark.sql.connector.catalog.functions.ScalarFunction;
 import org.apache.spark.sql.connector.catalog.functions.UnboundFunction;
 import org.apache.spark.sql.types.BinaryType;
 import org.apache.spark.sql.types.ByteType;
@@ -108,7 +107,7 @@ public String name() {
     return "truncate";
   }
 
-  public abstract static class TruncateBase<T> implements ScalarFunction<T> {
+  public abstract static class TruncateBase<T> extends BaseScalarFunction<T> {
     @Override
     public String name() {
       return "truncate";

File: spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/functions/YearsFunction.java
Patch:
@@ -21,7 +21,6 @@
 import org.apache.iceberg.util.DateTimeUtil;
 import org.apache.spark.sql.catalyst.InternalRow;
 import org.apache.spark.sql.connector.catalog.functions.BoundFunction;
-import org.apache.spark.sql.connector.catalog.functions.ScalarFunction;
 import org.apache.spark.sql.types.DataType;
 import org.apache.spark.sql.types.DataTypes;
 import org.apache.spark.sql.types.DateType;
@@ -61,7 +60,7 @@ public String name() {
     return "years";
   }
 
-  private abstract static class BaseToYearsFunction implements ScalarFunction<Integer> {
+  private abstract static class BaseToYearsFunction extends BaseScalarFunction<Integer> {
     @Override
     public String name() {
       return "years";

File: core/src/main/java/org/apache/iceberg/CatalogUtil.java
Patch:
@@ -334,7 +334,7 @@ public static FileIO loadFileIO(String impl, Map<String, String> properties, Obj
               .buildChecked();
     } catch (NoSuchMethodException e) {
       throw new IllegalArgumentException(
-          String.format("Cannot initialize FileIO, missing no-arg constructor: %s", impl), e);
+          String.format("Cannot initialize FileIO implementation %s: %s", impl, e.getMessage()), e);
     }
 
     FileIO fileIO;

File: core/src/test/java/org/apache/iceberg/TestCatalogUtil.java
Patch:
@@ -160,7 +160,9 @@ public void loadCustomFileIO_badArg() {
     Assertions.assertThatThrownBy(
             () -> CatalogUtil.loadFileIO(TestFileIOBadArg.class.getName(), Maps.newHashMap(), null))
         .isInstanceOf(IllegalArgumentException.class)
-        .hasMessageStartingWith("Cannot initialize FileIO, missing no-arg constructor");
+        .hasMessageStartingWith(
+            "Cannot initialize FileIO implementation "
+                + "org.apache.iceberg.TestCatalogUtil$TestFileIOBadArg: Cannot find constructor");
   }
 
   @Test

File: core/src/main/java/org/apache/iceberg/actions/BaseCommitService.java
Patch:
@@ -138,7 +138,7 @@ public void start() {
 
   /**
    * Places a file group in the queue and commits a batch of file groups if {@link
-   * #rewritesPerCommit} number of file groups are present in the queue.
+   * BaseCommitService#rewritesPerCommit} number of file groups are present in the queue.
    *
    * @param group file group to eventually be committed
    */

File: spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java
Patch:
@@ -201,7 +201,7 @@ TableCatalog tableCatalog() {
 
   /**
    * Closes this procedure's executor service if a new one was created with {@link
-   * #executorService(int, String)}. Does not block for any remaining tasks.
+   * BaseProcedure#executorService(int, String)}. Does not block for any remaining tasks.
    */
   protected void closeService() {
     if (executorService != null) {

File: spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java
Patch:
@@ -201,7 +201,7 @@ TableCatalog tableCatalog() {
 
   /**
    * Closes this procedure's executor service if a new one was created with {@link
-   * #executorService(int, String)}. Does not block for any remaining tasks.
+   * BaseProcedure#executorService(int, String)}. Does not block for any remaining tasks.
    */
   protected void closeService() {
     if (executorService != null) {

File: spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java
Patch:
@@ -201,7 +201,7 @@ TableCatalog tableCatalog() {
 
   /**
    * Closes this procedure's executor service if a new one was created with {@link
-   * #executorService(int, String)}. Does not block for any remaining tasks.
+   * BaseProcedure#executorService(int, String)}. Does not block for any remaining tasks.
    */
   protected void closeService() {
     if (executorService != null) {

File: spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestViews.java
Patch:
@@ -505,9 +505,8 @@ public void readFromViewReferencingTempFunction() throws NoSuchTableException {
     // reading from a view that references a TEMP FUNCTION shouldn't be possible
     assertThatThrownBy(() -> sql("SELECT * FROM %s", viewName))
         .isInstanceOf(AnalysisException.class)
-        .hasMessageContaining("The function")
-        .hasMessageContaining(functionName)
-        .hasMessageContaining("cannot be found");
+        .hasMessageStartingWith(
+            String.format("Cannot load function: %s.%s.%s", catalogName, NAMESPACE, functionName));
   }
 
   @Test

File: spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/SupportsFunctions.java
Patch:
@@ -24,6 +24,7 @@
 import org.apache.spark.sql.connector.catalog.FunctionCatalog;
 import org.apache.spark.sql.connector.catalog.Identifier;
 import org.apache.spark.sql.connector.catalog.functions.UnboundFunction;
+import scala.Option;
 
 interface SupportsFunctions extends FunctionCatalog {
 
@@ -58,6 +59,7 @@ default UnboundFunction loadFunction(Identifier ident) throws NoSuchFunctionExce
       }
     }
 
-    throw new NoSuchFunctionException(ident);
+    throw new NoSuchFunctionException(
+        String.format("Cannot load function: %s.%s", name(), ident), Option.empty());
   }
 }

File: spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestFunctionCatalog.java
Patch:
@@ -89,13 +89,13 @@ public void testLoadFunctions() throws NoSuchFunctionException {
                 asFunctionCatalog.loadFunction(Identifier.of(DEFAULT_NAMESPACE, "iceberg_version")))
         .isInstanceOf(NoSuchFunctionException.class)
         .hasMessageStartingWith(
-            "[ROUTINE_NOT_FOUND] The function default.iceberg_version cannot be found.");
+            String.format("Cannot load function: %s.default.iceberg_version", catalogName));
 
     Identifier undefinedFunction = Identifier.of(SYSTEM_NAMESPACE, "undefined_function");
     Assertions.assertThatThrownBy(() -> asFunctionCatalog.loadFunction(undefinedFunction))
         .isInstanceOf(NoSuchFunctionException.class)
         .hasMessageStartingWith(
-            "[ROUTINE_NOT_FOUND] The function system.undefined_function cannot be found.");
+            String.format("Cannot load function: %s.system.undefined_function", catalogName));
 
     Assertions.assertThatThrownBy(() -> sql("SELECT undefined_function(1, 2)"))
         .isInstanceOf(AnalysisException.class)

File: spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestViews.java
Patch:
@@ -504,9 +504,8 @@ public void readFromViewReferencingTempFunction() throws NoSuchTableException {
     // reading from a view that references a TEMP FUNCTION shouldn't be possible
     assertThatThrownBy(() -> sql("SELECT * FROM %s", viewName))
         .isInstanceOf(AnalysisException.class)
-        .hasMessageContaining("The function")
-        .hasMessageContaining(functionName)
-        .hasMessageContaining("cannot be found");
+        .hasMessageStartingWith(
+            String.format("Cannot load function: %s.%s.%s", catalogName, NAMESPACE, functionName));
   }
 
   @TestTemplate

File: spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SupportsFunctions.java
Patch:
@@ -24,6 +24,7 @@
 import org.apache.spark.sql.connector.catalog.FunctionCatalog;
 import org.apache.spark.sql.connector.catalog.Identifier;
 import org.apache.spark.sql.connector.catalog.functions.UnboundFunction;
+import scala.Option;
 
 interface SupportsFunctions extends FunctionCatalog {
 
@@ -58,6 +59,7 @@ default UnboundFunction loadFunction(Identifier ident) throws NoSuchFunctionExce
       }
     }
 
-    throw new NoSuchFunctionException(ident);
+    throw new NoSuchFunctionException(
+        String.format("Cannot load function: %s.%s", name(), ident), Option.empty());
   }
 }

File: spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestFunctionCatalog.java
Patch:
@@ -87,13 +87,13 @@ public void testLoadFunctions() throws NoSuchFunctionException {
                 asFunctionCatalog.loadFunction(Identifier.of(DEFAULT_NAMESPACE, "iceberg_version")))
         .isInstanceOf(NoSuchFunctionException.class)
         .hasMessageStartingWith(
-            "[ROUTINE_NOT_FOUND] The function default.iceberg_version cannot be found.");
+            String.format("Cannot load function: %s.default.iceberg_version", catalogName));
 
     Identifier undefinedFunction = Identifier.of(SYSTEM_NAMESPACE, "undefined_function");
     assertThatThrownBy(() -> asFunctionCatalog.loadFunction(undefinedFunction))
         .isInstanceOf(NoSuchFunctionException.class)
         .hasMessageStartingWith(
-            "[ROUTINE_NOT_FOUND] The function system.undefined_function cannot be found.");
+            String.format("Cannot load function: %s.system.undefined_function", catalogName));
 
     assertThatThrownBy(() -> sql("SELECT undefined_function(1, 2)"))
         .isInstanceOf(AnalysisException.class)

File: api/src/main/java/org/apache/iceberg/SortOrder.java
Patch:
@@ -212,7 +212,7 @@ public Builder asc(Term term, NullOrder nullOrder) {
     }
 
     /**
-     * Add an expression term to the sort, ascending with the given null order.
+     * Add an expression term to the sort, descending with the given null order.
      *
      * @param term an expression term
      * @param nullOrder a null order (first or last)

File: flink/v1.16/flink/src/main/java/org/apache/iceberg/flink/FlinkReadConf.java
Patch:
@@ -178,6 +178,7 @@ public long limit() {
   public int workerPoolSize() {
     return confParser
         .intConf()
+        .option(FlinkConfigOptions.TABLE_EXEC_ICEBERG_WORKER_POOL_SIZE.key())
         .flinkConfig(FlinkConfigOptions.TABLE_EXEC_ICEBERG_WORKER_POOL_SIZE)
         .defaultValue(FlinkConfigOptions.TABLE_EXEC_ICEBERG_WORKER_POOL_SIZE.defaultValue())
         .parse();

File: flink/v1.17/flink/src/main/java/org/apache/iceberg/flink/FlinkReadConf.java
Patch:
@@ -178,6 +178,7 @@ public long limit() {
   public int workerPoolSize() {
     return confParser
         .intConf()
+        .option(FlinkConfigOptions.TABLE_EXEC_ICEBERG_WORKER_POOL_SIZE.key())
         .flinkConfig(FlinkConfigOptions.TABLE_EXEC_ICEBERG_WORKER_POOL_SIZE)
         .defaultValue(FlinkConfigOptions.TABLE_EXEC_ICEBERG_WORKER_POOL_SIZE.defaultValue())
         .parse();

File: flink/v1.16/flink/src/main/java/org/apache/iceberg/flink/source/enumerator/ContinuousIcebergEnumerator.java
Patch:
@@ -75,6 +75,7 @@ public ContinuousIcebergEnumerator(
 
     if (enumState != null) {
       this.enumeratorPosition.set(enumState.lastEnumeratedPosition());
+      this.enumerationHistory.restore(enumState.enumerationSplitCountHistory());
     }
   }
 

File: flink/v1.17/flink/src/main/java/org/apache/iceberg/flink/source/enumerator/ContinuousIcebergEnumerator.java
Patch:
@@ -75,6 +75,7 @@ public ContinuousIcebergEnumerator(
 
     if (enumState != null) {
       this.enumeratorPosition.set(enumState.lastEnumeratedPosition());
+      this.enumerationHistory.restore(enumState.enumerationSplitCountHistory());
     }
   }
 

File: core/src/test/java/org/apache/iceberg/jdbc/TestJdbcViewCatalog.java
Patch:
@@ -43,6 +43,7 @@ public void before() {
     properties.put(JdbcCatalog.PROPERTY_PREFIX + "username", "user");
     properties.put(JdbcCatalog.PROPERTY_PREFIX + "password", "password");
     properties.put(CatalogProperties.WAREHOUSE_LOCATION, tableDir.toAbsolutePath().toString());
+    properties.put(JdbcUtil.SCHEMA_VERSION_PROPERTY, JdbcUtil.SchemaVersion.V1.name());
 
     catalog = new JdbcCatalog();
     catalog.setConf(new Configuration());

File: spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/source/BaseReader.java
Patch:
@@ -182,7 +182,7 @@ protected InputFile getInputFile(String location) {
   private Map<String, InputFile> inputFiles() {
     if (lazyInputFiles == null) {
       this.lazyInputFiles =
-          EncryptingFileIO.create(table().io(), table().encryption())
+          EncryptingFileIO.combine(table().io(), table().encryption())
               .bulkDecrypt(
                   () -> taskGroup.tasks().stream().flatMap(this::referencedFiles).iterator());
     }

File: flink/v1.18/flink/src/main/java/org/apache/iceberg/flink/FlinkReadConf.java
Patch:
@@ -178,6 +178,7 @@ public long limit() {
   public int workerPoolSize() {
     return confParser
         .intConf()
+        .option(FlinkConfigOptions.TABLE_EXEC_ICEBERG_WORKER_POOL_SIZE.key())
         .flinkConfig(FlinkConfigOptions.TABLE_EXEC_ICEBERG_WORKER_POOL_SIZE)
         .defaultValue(FlinkConfigOptions.TABLE_EXEC_ICEBERG_WORKER_POOL_SIZE.defaultValue())
         .parse();

File: flink/v1.18/flink/src/main/java/org/apache/iceberg/flink/source/enumerator/ContinuousIcebergEnumerator.java
Patch:
@@ -83,6 +83,7 @@ public ContinuousIcebergEnumerator(
 
     if (enumState != null) {
       this.enumeratorPosition.set(enumState.lastEnumeratedPosition());
+      this.enumerationHistory.restore(enumState.enumerationSplitCountHistory());
     }
   }
 

File: spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestBaseWithCatalog.java
Patch:
@@ -101,7 +101,7 @@ public void before() {
     catalogConfig.forEach(
         (key, value) -> spark.conf().set("spark.sql.catalog." + catalogName + "." + key, value));
 
-    if (catalogConfig.get("type").equalsIgnoreCase("hadoop")) {
+    if ("hadoop".equalsIgnoreCase(catalogConfig.get("type"))) {
       spark.conf().set("spark.sql.catalog." + catalogName + ".warehouse", "file:" + warehouse);
     }
 

File: spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/SparkCatalog.java
Patch:
@@ -639,7 +639,7 @@ public View replaceView(
                 .withSchema(icebergSchema)
                 .withLocation(properties.get("location"))
                 .withProperties(props)
-                .replace();
+                .createOrReplace();
         return new SparkView(catalogName, view);
       } catch (org.apache.iceberg.exceptions.NoSuchNamespaceException e) {
         throw new NoSuchNamespaceException(currentNamespace);

File: spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkCatalog.java
Patch:
@@ -639,7 +639,7 @@ public View replaceView(
                 .withSchema(icebergSchema)
                 .withLocation(properties.get("location"))
                 .withProperties(props)
-                .replace();
+                .createOrReplace();
         return new SparkView(catalogName, view);
       } catch (org.apache.iceberg.exceptions.NoSuchNamespaceException e) {
         throw new NoSuchNamespaceException(currentNamespace);

File: spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/actions/RewriteManifestsSparkAction.java
Patch:
@@ -179,7 +179,7 @@ private RewriteManifests.Result doExecute() {
     Dataset<Row> manifestEntryDF = buildManifestEntryDF(matchingManifests);
 
     List<ManifestFile> newManifests;
-    if (spec.fields().size() < 1) {
+    if (spec.isUnpartitioned()) {
       newManifests = writeManifestsForUnpartitionedTable(manifestEntryDF, targetNumManifests);
     } else {
       newManifests = writeManifestsForPartitionedTable(manifestEntryDF, targetNumManifests);

File: data/src/main/java/org/apache/iceberg/data/BaseDeleteLoader.java
Patch:
@@ -151,6 +151,7 @@ public PositionDeleteIndex loadPositionDeletes(
     return PositionDeleteIndexUtil.merge(deletes);
   }
 
+  @SuppressWarnings("CollectionUndefinedEquality")
   private PositionDeleteIndex getOrReadPosDeletes(DeleteFile deleteFile, CharSequence filePath) {
     long estimatedSize = estimatePosDeletesSize(deleteFile);
     if (canCache(estimatedSize)) {

File: spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReaderDeletes.java
Patch:
@@ -123,6 +123,7 @@ public static void startMetastoreAndSpark() {
         SparkSession.builder()
             .master("local[2]")
             .config("spark.appStateStore.asyncTracking.enable", false)
+            .config("spark.ui.liveUpdate.period", 0)
             .config(SQLConf.PARTITION_OVERWRITE_MODE().key(), "dynamic")
             .config("spark.hadoop." + METASTOREURIS.varname, hiveConf.get(METASTOREURIS.varname))
             .enableHiveSupport()

File: spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReaderDeletes.java
Patch:
@@ -127,6 +127,7 @@ public static void startMetastoreAndSpark() {
         SparkSession.builder()
             .master("local[2]")
             .config("spark.appStateStore.asyncTracking.enable", false)
+            .config("spark.ui.liveUpdate.period", 0)
             .config(SQLConf.PARTITION_OVERWRITE_MODE().key(), "dynamic")
             .config("spark.hadoop." + METASTOREURIS.varname, hiveConf.get(METASTOREURIS.varname))
             .enableHiveSupport()

File: spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReaderDeletes.java
Patch:
@@ -127,6 +127,7 @@ public static void startMetastoreAndSpark() {
         SparkSession.builder()
             .master("local[2]")
             .config("spark.appStateStore.asyncTracking.enable", false)
+            .config("spark.ui.liveUpdate.period", 0)
             .config(SQLConf.PARTITION_OVERWRITE_MODE().key(), "dynamic")
             .config("spark.hadoop." + METASTOREURIS.varname, hiveConf.get(METASTOREURIS.varname))
             .enableHiveSupport()

File: api/src/main/java/org/apache/iceberg/util/CharSequenceSet.java
Patch:
@@ -166,6 +166,7 @@ public void clear() {
     wrapperSet.clear();
   }
 
+  @SuppressWarnings("CollectionUndefinedEquality")
   @Override
   public boolean equals(Object other) {
     if (this == other) {

File: api/src/main/java/org/apache/iceberg/util/CharSequenceWrapper.java
Patch:
@@ -44,6 +44,7 @@ public CharSequence get() {
   }
 
   @Override
+  @SuppressWarnings("UndefinedEquals")
   public boolean equals(Object other) {
     if (this == other) {
       return true;

File: core/src/main/java/org/apache/iceberg/DeleteFileIndex.java
Patch:
@@ -161,6 +161,7 @@ private DeleteFile[] findEqPartitionDeletes(long seq, DataFile dataFile) {
     return deletes == null ? EMPTY_DELETES : deletes.filter(seq, dataFile);
   }
 
+  @SuppressWarnings("CollectionUndefinedEquality")
   private DeleteFile[] findPathDeletes(long seq, DataFile dataFile) {
     if (posDeletesByPath == null) {
       return EMPTY_DELETES;

File: core/src/main/java/org/apache/iceberg/avro/AvroWithPartnerByStructureVisitor.java
Patch:
@@ -109,7 +109,7 @@ private static <P, T> T visitUnion(
         // types match according to the following pattern:
         // Before NULL, branch type i in the union maps to struct field i + 1.
         // After NULL, branch type i in the union maps to struct field i.
-        int structFieldIndex = (encounteredNull) ? i : i + 1;
+        int structFieldIndex = encounteredNull ? i : i + 1;
         if (types.get(i).getType() == Schema.Type.NULL) {
           visit(visitor.nullType(), types.get(i), visitor);
           encounteredNull = true;

File: core/src/main/java/org/apache/iceberg/deletes/SortingPositionOnlyDeleteWriter.java
Patch:
@@ -116,6 +116,7 @@ private DeleteWriteResult writeFileDeletes() throws IOException {
     return new DeleteWriteResult(deleteFiles, referencedDataFiles);
   }
 
+  @SuppressWarnings("CollectionUndefinedEquality")
   private DeleteWriteResult writeDeletes(Collection<CharSequence> paths) throws IOException {
     FileWriter<PositionDelete<T>, DeleteWriteResult> writer = writers.get();
 

File: flink/v1.16/flink/src/test/java/org/apache/iceberg/flink/source/TestIcebergSourceWithWatermarkExtractor.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.iceberg.flink.source;
 
 import static org.apache.flink.connector.testframe.utils.ConnectorTestConstants.DEFAULT_COLLECT_DATA_TIMEOUT;
+import static org.apache.iceberg.flink.MiniClusterResource.DISABLE_CLASSLOADER_CHECK_CONFIG;
 import static org.assertj.core.api.AssertionsForClassTypes.assertThat;
 
 import java.io.Serializable;
@@ -39,7 +40,6 @@
 import org.apache.flink.api.common.eventtime.SerializableTimestampAssigner;
 import org.apache.flink.api.common.eventtime.WatermarkStrategy;
 import org.apache.flink.api.common.typeinfo.TypeInformation;
-import org.apache.flink.configuration.Configuration;
 import org.apache.flink.core.execution.JobClient;
 import org.apache.flink.metrics.Gauge;
 import org.apache.flink.runtime.metrics.MetricNames;
@@ -94,7 +94,7 @@ public class TestIcebergSourceWithWatermarkExtractor implements Serializable {
               .setNumberTaskManagers(1)
               .setNumberSlotsPerTaskManager(PARALLELISM)
               .setRpcServiceSharing(RpcServiceSharing.DEDICATED)
-              .setConfiguration(reporter.addToConfiguration(new Configuration()))
+              .setConfiguration(reporter.addToConfiguration(DISABLE_CLASSLOADER_CHECK_CONFIG))
               .withHaLeadershipControl()
               .build());
 

File: flink/v1.17/flink/src/test/java/org/apache/iceberg/flink/source/TestIcebergSourceWithWatermarkExtractor.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.iceberg.flink.source;
 
 import static org.apache.flink.connector.testframe.utils.ConnectorTestConstants.DEFAULT_COLLECT_DATA_TIMEOUT;
+import static org.apache.iceberg.flink.MiniClusterResource.DISABLE_CLASSLOADER_CHECK_CONFIG;
 import static org.assertj.core.api.AssertionsForClassTypes.assertThat;
 
 import java.io.Serializable;
@@ -39,7 +40,6 @@
 import org.apache.flink.api.common.eventtime.SerializableTimestampAssigner;
 import org.apache.flink.api.common.eventtime.WatermarkStrategy;
 import org.apache.flink.api.common.typeinfo.TypeInformation;
-import org.apache.flink.configuration.Configuration;
 import org.apache.flink.core.execution.JobClient;
 import org.apache.flink.metrics.Gauge;
 import org.apache.flink.runtime.metrics.MetricNames;
@@ -94,7 +94,7 @@ public class TestIcebergSourceWithWatermarkExtractor implements Serializable {
               .setNumberTaskManagers(1)
               .setNumberSlotsPerTaskManager(PARALLELISM)
               .setRpcServiceSharing(RpcServiceSharing.DEDICATED)
-              .setConfiguration(reporter.addToConfiguration(new Configuration()))
+              .setConfiguration(reporter.addToConfiguration(DISABLE_CLASSLOADER_CHECK_CONFIG))
               .withHaLeadershipControl()
               .build());
 

File: flink/v1.18/flink/src/test/java/org/apache/iceberg/flink/source/TestIcebergSourceWithWatermarkExtractor.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.iceberg.flink.source;
 
 import static org.apache.flink.connector.testframe.utils.ConnectorTestConstants.DEFAULT_COLLECT_DATA_TIMEOUT;
+import static org.apache.iceberg.flink.MiniClusterResource.DISABLE_CLASSLOADER_CHECK_CONFIG;
 import static org.assertj.core.api.AssertionsForClassTypes.assertThat;
 
 import java.io.Serializable;
@@ -39,7 +40,6 @@
 import org.apache.flink.api.common.eventtime.SerializableTimestampAssigner;
 import org.apache.flink.api.common.eventtime.WatermarkStrategy;
 import org.apache.flink.api.common.typeinfo.TypeInformation;
-import org.apache.flink.configuration.Configuration;
 import org.apache.flink.core.execution.JobClient;
 import org.apache.flink.metrics.Gauge;
 import org.apache.flink.runtime.metrics.MetricNames;
@@ -94,7 +94,7 @@ public class TestIcebergSourceWithWatermarkExtractor implements Serializable {
               .setNumberTaskManagers(1)
               .setNumberSlotsPerTaskManager(PARALLELISM)
               .setRpcServiceSharing(RpcServiceSharing.DEDICATED)
-              .setConfiguration(reporter.addToConfiguration(new Configuration()))
+              .setConfiguration(reporter.addToConfiguration(DISABLE_CLASSLOADER_CHECK_CONFIG))
               .withHaLeadershipControl()
               .build());
 

File: spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/source/SparkWrite.java
Patch:
@@ -643,11 +643,11 @@ public DataWriter<InternalRow> createWriter(int partitionId, long taskId, long e
       Table table = tableBroadcast.value();
       PartitionSpec spec = table.specs().get(outputSpecId);
       FileIO io = table.io();
-
+      String operationId = queryId + "-" + epochId;
       OutputFileFactory fileFactory =
           OutputFileFactory.builderFor(table, partitionId, taskId)
               .format(format)
-              .operationId(queryId)
+              .operationId(operationId)
               .build();
       SparkFileWriterFactory writerFactory =
           SparkFileWriterFactory.builderFor(table)

File: spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/source/SparkWrite.java
Patch:
@@ -657,11 +657,11 @@ public DataWriter<InternalRow> createWriter(int partitionId, long taskId, long e
       Table table = tableBroadcast.value();
       PartitionSpec spec = table.specs().get(outputSpecId);
       FileIO io = table.io();
-
+      String operationId = queryId + "-" + epochId;
       OutputFileFactory fileFactory =
           OutputFileFactory.builderFor(table, partitionId, taskId)
               .format(format)
-              .operationId(queryId)
+              .operationId(operationId)
               .build();
       SparkFileWriterFactory writerFactory =
           SparkFileWriterFactory.builderFor(table)

File: spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/source/SparkWrite.java
Patch:
@@ -673,11 +673,11 @@ public DataWriter<InternalRow> createWriter(int partitionId, long taskId, long e
       Table table = tableBroadcast.value();
       PartitionSpec spec = table.specs().get(outputSpecId);
       FileIO io = table.io();
-
+      String operationId = queryId + "-" + epochId;
       OutputFileFactory fileFactory =
           OutputFileFactory.builderFor(table, partitionId, taskId)
               .format(format)
-              .operationId(queryId)
+              .operationId(operationId)
               .build();
       SparkFileWriterFactory writerFactory =
           SparkFileWriterFactory.builderFor(table)

File: spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/procedures/AddFilesProcedure.java
Patch:
@@ -196,7 +196,7 @@ private void importFileTable(
       importPartitions(table, ImmutableList.of(partition), checkDuplicateFiles);
     } else {
       Preconditions.checkArgument(
-          !partitions.isEmpty(), "Cannot find any matching partitions in table %s", partitions);
+          !partitions.isEmpty(), "Cannot find any matching partitions in table %s", table.name());
       importPartitions(table, partitions, checkDuplicateFiles);
     }
   }

File: spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/procedures/AddFilesProcedure.java
Patch:
@@ -196,7 +196,7 @@ private void importFileTable(
       importPartitions(table, ImmutableList.of(partition), checkDuplicateFiles);
     } else {
       Preconditions.checkArgument(
-          !partitions.isEmpty(), "Cannot find any matching partitions in table %s", partitions);
+          !partitions.isEmpty(), "Cannot find any matching partitions in table %s", table.name());
       importPartitions(table, partitions, checkDuplicateFiles);
     }
   }

File: spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/procedures/AddFilesProcedure.java
Patch:
@@ -196,7 +196,7 @@ private void importFileTable(
       importPartitions(table, ImmutableList.of(partition), checkDuplicateFiles);
     } else {
       Preconditions.checkArgument(
-          !partitions.isEmpty(), "Cannot find any matching partitions in table %s", partitions);
+          !partitions.isEmpty(), "Cannot find any matching partitions in table %s", table.name());
       importPartitions(table, partitions, checkDuplicateFiles);
     }
   }

File: core/src/main/java/org/apache/iceberg/BaseMetadataTable.java
Patch:
@@ -44,13 +44,15 @@ public abstract class BaseMetadataTable extends BaseReadOnlyTable
   private final SortOrder sortOrder = SortOrder.unsorted();
   private final BaseTable table;
   private final String name;
+  private final UUID uuid;
 
   protected BaseMetadataTable(Table table, String name) {
     super("metadata");
     Preconditions.checkArgument(
         table instanceof BaseTable, "Cannot create metadata table for non-data table: %s", table);
     this.table = (BaseTable) table;
     this.name = name;
+    this.uuid = UUID.randomUUID();
   }
 
   /**
@@ -202,7 +204,7 @@ public Map<String, SnapshotRef> refs() {
 
   @Override
   public UUID uuid() {
-    return UUID.randomUUID();
+    return uuid;
   }
 
   @Override

File: core/src/test/java/org/apache/iceberg/TestRemoveSnapshots.java
Patch:
@@ -795,7 +795,7 @@ public void dataFilesCleanupWithParallelTasks() throws IOException {
     rewriteManifests.addManifest(newManifest);
     rewriteManifests.commit();
 
-    Set<String> deletedFiles = Sets.newHashSet();
+    Set<String> deletedFiles = ConcurrentHashMap.newKeySet();
     Set<String> deleteThreads = ConcurrentHashMap.newKeySet();
     AtomicInteger deleteThreadsIndex = new AtomicInteger(0);
     AtomicInteger planThreadsIndex = new AtomicInteger(0);

File: spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/actions/TestExpireSnapshotsAction.java
Patch:
@@ -207,7 +207,7 @@ public void dataFilesCleanupWithParallelTasks() throws IOException {
 
     long t4 = rightAfterSnapshot();
 
-    Set<String> deletedFiles = Sets.newHashSet();
+    Set<String> deletedFiles = ConcurrentHashMap.newKeySet();
     Set<String> deleteThreads = ConcurrentHashMap.newKeySet();
     AtomicInteger deleteThreadsIndex = new AtomicInteger(0);
 

File: spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveOrphanFilesAction.java
Patch:
@@ -271,7 +271,7 @@ public void orphanedFileRemovedWithParallelTasks() throws InterruptedException,
 
     waitUntilAfter(System.currentTimeMillis());
 
-    Set<String> deletedFiles = Sets.newHashSet();
+    Set<String> deletedFiles = ConcurrentHashMap.newKeySet();
     Set<String> deleteThreads = ConcurrentHashMap.newKeySet();
     AtomicInteger deleteThreadsIndex = new AtomicInteger(0);
 

File: spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestExpireSnapshotsAction.java
Patch:
@@ -207,7 +207,7 @@ public void dataFilesCleanupWithParallelTasks() throws IOException {
 
     long t4 = rightAfterSnapshot();
 
-    Set<String> deletedFiles = Sets.newHashSet();
+    Set<String> deletedFiles = ConcurrentHashMap.newKeySet();
     Set<String> deleteThreads = ConcurrentHashMap.newKeySet();
     AtomicInteger deleteThreadsIndex = new AtomicInteger(0);
 

File: spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveOrphanFilesAction.java
Patch:
@@ -270,7 +270,7 @@ public void orphanedFileRemovedWithParallelTasks() throws InterruptedException,
 
     waitUntilAfter(System.currentTimeMillis());
 
-    Set<String> deletedFiles = Sets.newHashSet();
+    Set<String> deletedFiles = ConcurrentHashMap.newKeySet();
     Set<String> deleteThreads = ConcurrentHashMap.newKeySet();
     AtomicInteger deleteThreadsIndex = new AtomicInteger(0);
 

File: spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestExpireSnapshotsAction.java
Patch:
@@ -207,7 +207,7 @@ public void dataFilesCleanupWithParallelTasks() throws IOException {
 
     long t4 = rightAfterSnapshot();
 
-    Set<String> deletedFiles = Sets.newHashSet();
+    Set<String> deletedFiles = ConcurrentHashMap.newKeySet();
     Set<String> deleteThreads = ConcurrentHashMap.newKeySet();
     AtomicInteger deleteThreadsIndex = new AtomicInteger(0);
 

File: spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveOrphanFilesAction.java
Patch:
@@ -270,7 +270,7 @@ public void orphanedFileRemovedWithParallelTasks() throws InterruptedException,
 
     waitUntilAfter(System.currentTimeMillis());
 
-    Set<String> deletedFiles = Sets.newHashSet();
+    Set<String> deletedFiles = ConcurrentHashMap.newKeySet();
     Set<String> deleteThreads = ConcurrentHashMap.newKeySet();
     AtomicInteger deleteThreadsIndex = new AtomicInteger(0);
 

File: flink/v1.17/flink/src/main/java/org/apache/iceberg/flink/source/enumerator/ContinuousSplitPlannerImpl.java
Patch:
@@ -56,9 +56,9 @@ public class ContinuousSplitPlannerImpl implements ContinuousSplitPlanner {
    */
   public ContinuousSplitPlannerImpl(
       TableLoader tableLoader, ScanContext scanContext, String threadName) {
-    this.tableLoader = tableLoader;
+    this.tableLoader = tableLoader.clone();
     this.tableLoader.open();
-    this.table = tableLoader.loadTable();
+    this.table = this.tableLoader.loadTable();
     this.scanContext = scanContext;
     this.isSharedPool = threadName == null;
     this.workerPool =

File: core/src/main/java/org/apache/iceberg/util/PartitionSet.java
Patch:
@@ -63,7 +63,7 @@ public boolean contains(Object o) {
     if (o instanceof Pair) {
       Object first = ((Pair<?, ?>) o).first();
       Object second = ((Pair<?, ?>) o).second();
-      if (first instanceof Integer && second instanceof StructLike) {
+      if (first instanceof Integer && (second == null || second instanceof StructLike)) {
         return contains((Integer) first, (StructLike) second);
       }
     }
@@ -98,7 +98,7 @@ public boolean remove(Object o) {
     if (o instanceof Pair) {
       Object first = ((Pair<?, ?>) o).first();
       Object second = ((Pair<?, ?>) o).second();
-      if (first instanceof Integer && second instanceof StructLike) {
+      if (first instanceof Integer && (second == null || second instanceof StructLike)) {
         return remove((Integer) first, (StructLike) second);
       }
     }

File: core/src/test/java/org/apache/iceberg/util/TestPartitionMap.java
Patch:
@@ -274,9 +274,11 @@ public void testKeyAndEntrySetEquality() {
 
     map1.put(BY_DATA_SPEC.specId(), Row.of("aaa"), "v1");
     map1.put(BY_DATA_SPEC.specId(), Row.of("bbb"), "v2");
+    map1.put(UNPARTITIONED_SPEC.specId(), null, "v3");
 
     map2.put(BY_DATA_SPEC.specId(), CustomRow.of("aaa"), "v1");
     map2.put(BY_DATA_SPEC.specId(), CustomRow.of("bbb"), "v2");
+    map2.put(UNPARTITIONED_SPEC.specId(), null, "v3");
 
     assertThat(map1.keySet()).isEqualTo(map2.keySet());
     assertThat(map1.entrySet()).isEqualTo(map2.entrySet());

File: core/src/main/java/org/apache/iceberg/rest/ErrorHandlers.java
Patch:
@@ -202,6 +202,9 @@ public ErrorResponse parseResponse(int code, String json) {
     public void accept(ErrorResponse error) {
       switch (error.code()) {
         case 400:
+          if (IllegalArgumentException.class.getSimpleName().equals(error.type())) {
+            throw new IllegalArgumentException(error.message());
+          }
           throw new BadRequestException("Malformed request: %s", error.message());
         case 401:
           throw new NotAuthorizedException("Not authorized: %s", error.message());

File: core/src/test/java/org/apache/iceberg/view/ViewCatalogTests.java
Patch:
@@ -252,7 +252,7 @@ public void createViewErrorCases() {
                     .withQuery(trino.dialect(), trino.sql())
                     .withQuery(trino.dialect(), trino.sql())
                     .create())
-        .isInstanceOf(Exception.class)
+        .isInstanceOf(IllegalArgumentException.class)
         .hasMessageContaining(
             "Invalid view version: Cannot add multiple queries for dialect trino");
   }

File: spark/v3.3/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/SparkRowLevelOperationsTestBase.java
Patch:
@@ -114,7 +114,7 @@ public static Object[][] parameters() {
         "parquet",
         true,
         WRITE_DISTRIBUTION_MODE_NONE,
-        null,
+        "test",
       },
       {
         "testhadoop",

File: spark/v3.3/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestDelete.java
Patch:
@@ -144,9 +144,8 @@ public void testDeleteWithoutScanningTable() throws Exception {
   public void testDeleteFileThenMetadataDelete() throws Exception {
     Assume.assumeFalse("Avro does not support metadata delete", fileFormat.equals("avro"));
     createAndInitUnpartitionedTable();
-
-    sql("INSERT INTO TABLE %s VALUES (1, 'hr'), (2, 'hardware'), (null, 'hr')", tableName);
     createBranchIfNeeded();
+    sql("INSERT INTO TABLE %s VALUES (1, 'hr'), (2, 'hardware'), (null, 'hr')", commitTarget());
 
     // MOR mode: writes a delete file as null cannot be deleted by metadata
     sql("DELETE FROM %s AS t WHERE t.id IS NULL", commitTarget());

File: spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/source/SparkTable.java
Patch:
@@ -330,7 +330,7 @@ private boolean canDeleteUsingMetadata(Expression deleteExpr) {
             .ignoreResiduals();
 
     if (branch != null) {
-      scan.useRef(branch);
+      scan = scan.useRef(branch);
     }
 
     try (CloseableIterable<FileScanTask> tasks = scan.planFiles()) {

File: spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/data/TestHelpers.java
Patch:
@@ -790,7 +790,7 @@ public static List<DataFile> dataFiles(Table table) {
   public static List<DataFile> dataFiles(Table table, String branch) {
     TableScan scan = table.newScan();
     if (branch != null) {
-      scan.useRef(branch);
+      scan = scan.useRef(branch);
     }
 
     CloseableIterable<FileScanTask> tasks = scan.includeColumnStats().planFiles();

File: spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/SparkRowLevelOperationsTestBase.java
Patch:
@@ -135,7 +135,7 @@ public static Object[][] parameters() {
         true,
         WRITE_DISTRIBUTION_MODE_NONE,
         false,
-        null,
+        "test",
         DISTRIBUTED
       },
       {

File: spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestDelete.java
Patch:
@@ -312,9 +312,8 @@ public void testDeleteWithoutScanningTable() throws Exception {
   public void testDeleteFileThenMetadataDelete() throws Exception {
     Assume.assumeFalse("Avro does not support metadata delete", fileFormat.equals("avro"));
     createAndInitUnpartitionedTable();
-
-    sql("INSERT INTO TABLE %s VALUES (1, 'hr'), (2, 'hardware'), (null, 'hr')", tableName);
     createBranchIfNeeded();
+    sql("INSERT INTO TABLE %s VALUES (1, 'hr'), (2, 'hardware'), (null, 'hr')", commitTarget());
 
     // MOR mode: writes a delete file as null cannot be deleted by metadata
     sql("DELETE FROM %s AS t WHERE t.id IS NULL", commitTarget());

File: spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/source/SparkTable.java
Patch:
@@ -330,7 +330,7 @@ private boolean canDeleteUsingMetadata(Expression deleteExpr) {
             .ignoreResiduals();
 
     if (branch != null) {
-      scan.useRef(branch);
+      scan = scan.useRef(branch);
     }
 
     try (CloseableIterable<FileScanTask> tasks = scan.planFiles()) {

File: spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestHelpers.java
Patch:
@@ -806,7 +806,7 @@ public static List<DataFile> dataFiles(Table table) {
   public static List<DataFile> dataFiles(Table table, String branch) {
     TableScan scan = table.newScan();
     if (branch != null) {
-      scan.useRef(branch);
+      scan = scan.useRef(branch);
     }
 
     CloseableIterable<FileScanTask> tasks = scan.includeColumnStats().planFiles();

File: spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/SparkRowLevelOperationsTestBase.java
Patch:
@@ -135,7 +135,7 @@ public static Object[][] parameters() {
         true,
         WRITE_DISTRIBUTION_MODE_NONE,
         false,
-        null,
+        "test",
         DISTRIBUTED
       },
       {

File: spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestDelete.java
Patch:
@@ -326,9 +326,8 @@ public void testDeleteWithoutScanningTable() throws Exception {
   public void testDeleteFileThenMetadataDelete() throws Exception {
     Assume.assumeFalse("Avro does not support metadata delete", fileFormat.equals("avro"));
     createAndInitUnpartitionedTable();
-
-    sql("INSERT INTO TABLE %s VALUES (1, 'hr'), (2, 'hardware'), (null, 'hr')", tableName);
     createBranchIfNeeded();
+    sql("INSERT INTO TABLE %s VALUES (1, 'hr'), (2, 'hardware'), (null, 'hr')", commitTarget());
 
     // MOR mode: writes a delete file as null cannot be deleted by metadata
     sql("DELETE FROM %s AS t WHERE t.id IS NULL", commitTarget());

File: spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/source/SparkTable.java
Patch:
@@ -330,7 +330,7 @@ private boolean canDeleteUsingMetadata(Expression deleteExpr) {
             .ignoreResiduals();
 
     if (branch != null) {
-      scan.useRef(branch);
+      scan = scan.useRef(branch);
     }
 
     try (CloseableIterable<FileScanTask> tasks = scan.planFiles()) {

File: spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/TestHelpers.java
Patch:
@@ -806,7 +806,7 @@ public static List<DataFile> dataFiles(Table table) {
   public static List<DataFile> dataFiles(Table table, String branch) {
     TableScan scan = table.newScan();
     if (branch != null) {
-      scan.useRef(branch);
+      scan = scan.useRef(branch);
     }
 
     CloseableIterable<FileScanTask> tasks = scan.includeColumnStats().planFiles();

File: spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/actions/RewriteManifestsSparkAction.java
Patch:
@@ -179,7 +179,7 @@ private RewriteManifests.Result doExecute() {
     Dataset<Row> manifestEntryDF = buildManifestEntryDF(matchingManifests);
 
     List<ManifestFile> newManifests;
-    if (spec.fields().size() < 1) {
+    if (spec.isUnpartitioned()) {
       newManifests = writeManifestsForUnpartitionedTable(manifestEntryDF, targetNumManifests);
     } else {
       newManifests = writeManifestsForPartitionedTable(manifestEntryDF, targetNumManifests);

File: core/src/main/java/org/apache/iceberg/util/LockManagers.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.iceberg.util;
 
 import java.util.Map;
+import java.util.Optional;
 import java.util.concurrent.Executors;
 import java.util.concurrent.ScheduledExecutorService;
 import java.util.concurrent.ScheduledFuture;
@@ -253,7 +254,7 @@ public boolean release(String entityId, String ownerId) {
         return false;
       }
 
-      HEARTBEATS.remove(entityId).cancel(false);
+      Optional.ofNullable(HEARTBEATS.remove(entityId)).ifPresent(future -> future.cancel(false));
       LOCKS.remove(entityId);
       return true;
     }

File: aliyun/src/test/java/org/apache/iceberg/aliyun/oss/mock/AliyunOSSMockLocalStore.java
Patch:
@@ -110,7 +110,7 @@ Bucket getBucket(String bucketName) {
         findBucketsByFilter(
             file -> Files.isDirectory(file) && file.getFileName().endsWith(bucketName));
 
-    return buckets.size() > 0 ? buckets.get(0) : null;
+    return !buckets.isEmpty() ? buckets.get(0) : null;
   }
 
   void deleteBucket(String bucketName) throws IOException {

File: api/src/main/java/org/apache/iceberg/expressions/ResidualEvaluator.java
Patch:
@@ -82,7 +82,7 @@ public static ResidualEvaluator unpartitioned(Expression expr) {
    * @return a residual evaluator for the expression
    */
   public static ResidualEvaluator of(PartitionSpec spec, Expression expr, boolean caseSensitive) {
-    if (spec.fields().size() > 0) {
+    if (!spec.fields().isEmpty()) {
       return new ResidualEvaluator(spec, expr, caseSensitive);
     } else {
       return unpartitioned(expr);

File: aws/src/integration/java/org/apache/iceberg/aws/glue/TestGlueCatalogNamespace.java
Patch:
@@ -105,7 +105,7 @@ public void testNamespaceExists() {
   public void testListNamespace() {
     String namespace = createNamespace();
     List<Namespace> namespaceList = glueCatalog.listNamespaces();
-    Assert.assertTrue(namespaceList.size() > 0);
+    Assert.assertFalse(namespaceList.isEmpty());
     Assert.assertTrue(namespaceList.contains(Namespace.of(namespace)));
     namespaceList = glueCatalog.listNamespaces(Namespace.of(namespace));
     Assert.assertTrue(namespaceList.isEmpty());

File: data/src/test/java/org/apache/iceberg/data/TestMetricsRowGroupFilter.java
Patch:
@@ -975,7 +975,7 @@ private boolean shouldReadOrc(Expression expression, boolean caseSensitive) {
             .filter(expression)
             .caseSensitive(caseSensitive)
             .build()) {
-      return Lists.newArrayList(reader).size() > 0;
+      return !Lists.newArrayList(reader).isEmpty();
     } catch (IOException e) {
       throw new UncheckedIOException(e);
     }

File: data/src/test/java/org/apache/iceberg/data/TestMetricsRowGroupFilterTypes.java
Patch:
@@ -310,7 +310,7 @@ private boolean shouldReadOrc(Object value) {
             .createReaderFunc(fileSchema -> GenericOrcReader.buildReader(SCHEMA, fileSchema))
             .filter(Expressions.equal(column, value))
             .build()) {
-      return Lists.newArrayList(reader).size() > 0;
+      return !Lists.newArrayList(reader).isEmpty();
     } catch (IOException e) {
       throw new UncheckedIOException(e);
     }

File: delta-lake/src/main/java/org/apache/iceberg/delta/BaseSnapshotDeltaLakeTableAction.java
Patch:
@@ -312,18 +312,18 @@ private void commitDeltaVersionLogToIcebergTransaction(
       migratedDataFilesBuilder.add(dataFile.path().toString());
     }
 
-    if (filesToAdd.size() > 0 && filesToRemove.size() > 0) {
+    if (!filesToAdd.isEmpty() && !filesToRemove.isEmpty()) {
       // OverwriteFiles case
       OverwriteFiles overwriteFiles = transaction.newOverwrite();
       filesToAdd.forEach(overwriteFiles::addFile);
       filesToRemove.forEach(overwriteFiles::deleteFile);
       overwriteFiles.commit();
-    } else if (filesToAdd.size() > 0) {
+    } else if (!filesToAdd.isEmpty()) {
       // AppendFiles case
       AppendFiles appendFiles = transaction.newAppend();
       filesToAdd.forEach(appendFiles::appendFile);
       appendFiles.commit();
-    } else if (filesToRemove.size() > 0) {
+    } else if (!filesToRemove.isEmpty()) {
       // DeleteFiles case
       DeleteFiles deleteFiles = transaction.newDelete();
       filesToRemove.forEach(deleteFiles::deleteFile);

File: flink/v1.15/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java
Patch:
@@ -386,7 +386,7 @@ private String operatorName(String suffix) {
     @VisibleForTesting
     List<Integer> checkAndGetEqualityFieldIds() {
       List<Integer> equalityFieldIds = Lists.newArrayList(table.schema().identifierFieldIds());
-      if (equalityFieldColumns != null && equalityFieldColumns.size() > 0) {
+      if (equalityFieldColumns != null && !equalityFieldColumns.isEmpty()) {
         Set<Integer> equalityFieldSet =
             Sets.newHashSetWithExpectedSize(equalityFieldColumns.size());
         for (String column : equalityFieldColumns) {

File: flink/v1.16/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java
Patch:
@@ -386,7 +386,7 @@ private String operatorName(String suffix) {
     @VisibleForTesting
     List<Integer> checkAndGetEqualityFieldIds() {
       List<Integer> equalityFieldIds = Lists.newArrayList(table.schema().identifierFieldIds());
-      if (equalityFieldColumns != null && equalityFieldColumns.size() > 0) {
+      if (equalityFieldColumns != null && !equalityFieldColumns.isEmpty()) {
         Set<Integer> equalityFieldSet =
             Sets.newHashSetWithExpectedSize(equalityFieldColumns.size());
         for (String column : equalityFieldColumns) {

File: flink/v1.17/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java
Patch:
@@ -386,7 +386,7 @@ private String operatorName(String suffix) {
     @VisibleForTesting
     List<Integer> checkAndGetEqualityFieldIds() {
       List<Integer> equalityFieldIds = Lists.newArrayList(table.schema().identifierFieldIds());
-      if (equalityFieldColumns != null && equalityFieldColumns.size() > 0) {
+      if (equalityFieldColumns != null && !equalityFieldColumns.isEmpty()) {
         Set<Integer> equalityFieldSet =
             Sets.newHashSetWithExpectedSize(equalityFieldColumns.size());
         for (String column : equalityFieldColumns) {

File: flink/v1.17/flink/src/main/java/org/apache/iceberg/flink/sink/shuffle/DataStatisticsCoordinator.java
Patch:
@@ -340,7 +340,7 @@ private void unregisterSubtaskGateway(int subtaskIndex, int attemptNumber) {
 
     private OperatorCoordinator.SubtaskGateway getSubtaskGateway(int subtaskIndex) {
       Preconditions.checkState(
-          gateways[subtaskIndex].size() > 0,
+          !gateways[subtaskIndex].isEmpty(),
           "Coordinator of %s subtask %d is not ready yet to receive events",
           operatorName,
           subtaskIndex);

File: hive3/src/main/java/org/apache/hadoop/hive/ql/io/orc/OrcSplit.java
Patch:
@@ -238,7 +238,7 @@ public long getFileLength() {
    * @return true if is ACID
    */
   public boolean isAcid() {
-    return hasBase || deltas.size() > 0;
+    return hasBase || !deltas.isEmpty();
   }
 
   public long getProjectedColumnsUncompressedSize() {

File: mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputCommitter.java
Patch:
@@ -269,7 +269,7 @@ public void abortJob(JobContext originalContext, int status) throws IOException
                     dataFiles(fileExecutor, table.location(), jobContext, table.io(), false);
 
                 // Check if we have files already committed and remove data files if there are any
-                if (dataFiles.size() > 0) {
+                if (!dataFiles.isEmpty()) {
                   Tasks.foreach(dataFiles)
                       .retry(3)
                       .suppressFailureWhenFinished()
@@ -327,7 +327,7 @@ private void commitTable(
 
     Collection<DataFile> dataFiles = dataFiles(executor, location, jobContext, io, true);
 
-    if (dataFiles.size() > 0) {
+    if (!dataFiles.isEmpty()) {
       // Appending data files to the table
       AppendFiles append = table.newAppend();
       dataFiles.forEach(append::appendFile);

File: parquet/src/main/java/org/apache/iceberg/parquet/ParquetBloomRowGroupFilter.java
Patch:
@@ -116,7 +116,7 @@ private boolean eval(
           Binder.boundReferences(schema.asStruct(), ImmutableList.of(expr), caseSensitive);
       // If the filter's column set doesn't overlap with any bloom filter columns, exit early with
       // ROWS_MIGHT_MATCH
-      if (filterRefs.size() > 0 && Sets.intersection(fieldsWithBloomFilter, filterRefs).isEmpty()) {
+      if (!filterRefs.isEmpty() && Sets.intersection(fieldsWithBloomFilter, filterRefs).isEmpty()) {
         return ROWS_MIGHT_MATCH;
       }
 

File: aws/src/integration/java/org/apache/iceberg/aws/glue/TestGlueCatalogCommitFailure.java
Patch:
@@ -319,6 +319,7 @@ public void testExceptionThrownInConcurrentCommit() {
         ops.current().schema().columns().size());
   }
 
+  @SuppressWarnings("unchecked")
   private void concurrentCommitAndThrowException(
       GlueTableOperations realOps, GlueTableOperations spyOperations, Table table) {
     // Simulate a communication error after a successful commit
@@ -492,6 +493,7 @@ private TableMetadata updateTable(Table table, GlueTableOperations ops) {
     return metadataV2;
   }
 
+  @SuppressWarnings("unchecked")
   private void commitAndThrowException(GlueTableOperations realOps, GlueTableOperations spyOps) {
     Mockito.doAnswer(
             i -> {

File: azure/src/test/java/org/apache/iceberg/azure/adlsv2/ADLSFileIOTest.java
Patch:
@@ -101,6 +101,7 @@ public void testGetClient() {
   }
 
   /** Azurite does not support ADLSv2 directory operations yet so use mocks here. */
+  @SuppressWarnings("unchecked")
   @Test
   public void testListPrefixOperations() {
     String prefix = "abfs://container@account.dfs.core.windows.net/dir";
@@ -136,6 +137,7 @@ public void testListPrefixOperations() {
   }
 
   /** Azurite does not support ADLSv2 directory operations yet so use mocks here. */
+  @SuppressWarnings("unchecked")
   @Test
   public void testDeletePrefixOperations() {
     String prefix = "abfs://container@account.dfs.core.windows.net/dir";

File: gcp/src/test/java/org/apache/iceberg/gcp/gcs/GCSFileIOTest.java
Patch:
@@ -57,6 +57,7 @@ public class GCSFileIOTest {
   private final Storage storage = spy(LocalStorageHelper.getOptions().getService());
   private GCSFileIO io;
 
+  @SuppressWarnings("unchecked")
   @BeforeEach
   public void before() {
     // LocalStorageHelper doesn't support batch operations, so mock that here

File: hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java
Patch:
@@ -121,7 +121,7 @@ public static String translateToIcebergProp(String hmsProp) {
 
   protected HiveTableOperations(
       Configuration conf,
-      ClientPool metaClients,
+      ClientPool<IMetaStoreClient, TException> metaClients,
       FileIO fileIO,
       String catalogName,
       String database,

File: spark/v3.5/spark/src/jmh/java/org/apache/iceberg/spark/action/IcebergSortCompactionBenchmark.java
Patch:
@@ -294,10 +294,11 @@ protected final void initTable() {
             optional(9, "timestampCol", Types.TimestampType.withZone()),
             optional(10, "stringCol", Types.StringType.get()));
 
-    SparkSessionCatalog catalog;
+    SparkSessionCatalog<?> catalog;
     try {
       catalog =
-          (SparkSessionCatalog) Spark3Util.catalogAndIdentifier(spark(), "spark_catalog").catalog();
+          (SparkSessionCatalog<?>)
+              Spark3Util.catalogAndIdentifier(spark(), "spark_catalog").catalog();
       catalog.dropTable(IDENT);
       catalog.createTable(
           IDENT, SparkSchemaUtil.convert(schema), new Transform[0], Collections.emptyMap());

File: core/src/jmh/java/org/apache/iceberg/metrics/CountersBenchmark.java
Patch:
@@ -56,7 +56,7 @@ public void defaultCounterMultipleThreads(Blackhole blackhole) {
       Tasks.range(WORKER_POOL_SIZE)
           .executeWith(workerPool)
           .run(
-              (id) -> {
+              id -> {
                 for (int operation = 0; operation < NUM_OPERATIONS; operation++) {
                   counter.increment(INCREMENT_AMOUNT);
                 }

File: flink/v1.17/flink/src/main/java/org/apache/iceberg/flink/sink/shuffle/DataStatisticsCoordinator.java
Patch:
@@ -172,6 +172,7 @@ private void handleDataStatisticRequest(int subtask, DataStatisticsEvent<D, S> e
     }
   }
 
+  @SuppressWarnings("FutureReturnValueIgnored")
   private void sendDataStatisticsToSubtasks(
       long checkpointId, DataStatistics<D, S> globalDataStatistics) {
     callInCoordinatorThread(

File: core/src/main/java/org/apache/iceberg/view/ViewMetadata.java
Patch:
@@ -38,13 +38,12 @@
 import org.apache.iceberg.relocated.com.google.common.collect.Sets;
 import org.apache.iceberg.util.PropertyUtil;
 import org.immutables.value.Value;
-import org.immutables.value.Value.Style.ImplementationVisibility;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings("ImmutablesStyle")
 @Value.Immutable(builder = false)
-@Value.Style(allParameters = true, visibility = ImplementationVisibility.PACKAGE)
+@Value.Style(allParameters = true, visibilityString = "PACKAGE")
 public interface ViewMetadata extends Serializable {
   Logger LOG = LoggerFactory.getLogger(ViewMetadata.class);
   int SUPPORTED_VIEW_FORMAT_VERSION = 1;

File: spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/SparkV2Filters.java
Patch:
@@ -49,6 +49,7 @@
 import org.apache.spark.sql.connector.expressions.filter.Not;
 import org.apache.spark.sql.connector.expressions.filter.Or;
 import org.apache.spark.sql.connector.expressions.filter.Predicate;
+import org.apache.spark.sql.types.Decimal;
 import org.apache.spark.unsafe.types.UTF8String;
 
 public class SparkV2Filters {
@@ -285,6 +286,8 @@ private static boolean isLiteral(org.apache.spark.sql.connector.expressions.Expr
   private static Object convertLiteral(Literal<?> literal) {
     if (literal.value() instanceof UTF8String) {
       return ((UTF8String) literal.value()).toString();
+    } else if (literal.value() instanceof Decimal) {
+      return ((Decimal) literal.value()).toJavaBigDecimal();
     }
     return literal.value();
   }

File: spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/SparkV2Filters.java
Patch:
@@ -64,6 +64,7 @@
 import org.apache.spark.sql.connector.expressions.filter.Not;
 import org.apache.spark.sql.connector.expressions.filter.Or;
 import org.apache.spark.sql.connector.expressions.filter.Predicate;
+import org.apache.spark.sql.types.Decimal;
 import org.apache.spark.unsafe.types.UTF8String;
 
 public class SparkV2Filters {
@@ -378,6 +379,8 @@ private static boolean isLiteral(org.apache.spark.sql.connector.expressions.Expr
   private static Object convertLiteral(Literal<?> literal) {
     if (literal.value() instanceof UTF8String) {
       return ((UTF8String) literal.value()).toString();
+    } else if (literal.value() instanceof Decimal) {
+      return ((Decimal) literal.value()).toJavaBigDecimal();
     }
     return literal.value();
   }

File: spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkV2Filters.java
Patch:
@@ -64,6 +64,7 @@
 import org.apache.spark.sql.connector.expressions.filter.Not;
 import org.apache.spark.sql.connector.expressions.filter.Or;
 import org.apache.spark.sql.connector.expressions.filter.Predicate;
+import org.apache.spark.sql.types.Decimal;
 import org.apache.spark.unsafe.types.UTF8String;
 
 public class SparkV2Filters {
@@ -378,6 +379,8 @@ private static boolean isLiteral(org.apache.spark.sql.connector.expressions.Expr
   private static Object convertLiteral(Literal<?> literal) {
     if (literal.value() instanceof UTF8String) {
       return ((UTF8String) literal.value()).toString();
+    } else if (literal.value() instanceof Decimal) {
+      return ((Decimal) literal.value()).toJavaBigDecimal();
     }
     return literal.value();
   }

File: core/src/main/java/org/apache/iceberg/TableProperties.java
Patch:
@@ -304,6 +304,9 @@ private TableProperties() {}
   public static final String SPARK_WRITE_ACCEPT_ANY_SCHEMA = "write.spark.accept-any-schema";
   public static final boolean SPARK_WRITE_ACCEPT_ANY_SCHEMA_DEFAULT = false;
 
+  public static final String SPARK_WRITE_ADVISORY_PARTITION_SIZE_BYTES =
+      "write.spark.advisory-partition-size-bytes";
+
   public static final String SNAPSHOT_ID_INHERITANCE_ENABLED =
       "compatibility.snapshot-id-inheritance.enabled";
   public static final boolean SNAPSHOT_ID_INHERITANCE_ENABLED_DEFAULT = false;

File: spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkConfParser.java
Patch:
@@ -106,7 +106,7 @@ public int parse() {
     }
 
     public Integer parseOptional() {
-      return parse(Integer::parseInt, null);
+      return parse(Integer::parseInt, defaultValue);
     }
   }
 
@@ -129,7 +129,7 @@ public long parse() {
     }
 
     public Long parseOptional() {
-      return parse(Long::parseLong, null);
+      return parse(Long::parseLong, defaultValue);
     }
   }
 
@@ -152,7 +152,7 @@ public String parse() {
     }
 
     public String parseOptional() {
-      return parse(Function.identity(), null);
+      return parse(Function.identity(), defaultValue);
     }
   }
 

File: spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkSQLProperties.java
Patch:
@@ -64,4 +64,7 @@ private SparkSQLProperties() {}
 
   // Overrides the delete planning mode
   public static final String DELETE_PLANNING_MODE = "spark.sql.iceberg.delete-planning-mode";
+
+  // Overrides the advisory partition size
+  public static final String ADVISORY_PARTITION_SIZE = "spark.sql.iceberg.advisory-partition-size";
 }

File: spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkWriteOptions.java
Patch:
@@ -80,4 +80,7 @@ private SparkWriteOptions() {}
   public static final String COMPRESSION_CODEC = "compression-codec";
   public static final String COMPRESSION_LEVEL = "compression-level";
   public static final String COMPRESSION_STRATEGY = "compression-strategy";
+
+  // Overrides the advisory partition size
+  public static final String ADVISORY_PARTITION_SIZE = "advisory-partition-size";
 }

File: spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRequiredDistributionAndOrdering.java
Patch:
@@ -195,6 +195,7 @@ public void testDisabledDistributionAndOrdering() {
             inputDF
                 .writeTo(tableName)
                 .option(SparkWriteOptions.USE_TABLE_DISTRIBUTION_AND_ORDERING, "false")
+                .option(SparkWriteOptions.FANOUT_ENABLED, "false")
                 .append();
           } catch (NoSuchTableException e) {
             throw new RuntimeException(e);

File: spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestWriteAborts.java
Patch:
@@ -112,6 +112,7 @@ public void testBatchAppend() throws Exception {
                     .sortWithinPartitions("id")
                     .writeTo(tableName)
                     .option(SparkWriteOptions.USE_TABLE_DISTRIBUTION_AND_ORDERING, "false")
+                    .option(SparkWriteOptions.FANOUT_ENABLED, "false")
                     .append())
         .isInstanceOf(SparkException.class)
         .hasMessageContaining("Encountered records that belong to already closed files");

File: spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestRequiredDistributionAndOrdering.java
Patch:
@@ -167,6 +167,7 @@ public void testDisabledDistributionAndOrdering() {
                 inputDF
                     .writeTo(tableName)
                     .option(SparkWriteOptions.USE_TABLE_DISTRIBUTION_AND_ORDERING, "false")
+                    .option(SparkWriteOptions.FANOUT_ENABLED, "false")
                     .append())
         .cause()
         .isInstanceOf(IllegalStateException.class)

File: aws/src/integration/java/org/apache/iceberg/aws/TestDefaultAwsClientFactory.java
Patch:
@@ -55,7 +55,7 @@ public void testS3FileIoEndpointOverride() {
     AssertHelpers.assertThrowsCause(
         "Should refuse connection to unknown endpoint",
         SdkClientException.class,
-        "Unable to execute HTTP request: unknown",
+        "Unable to execute HTTP request: bucket.unknown",
         () -> s3Client.getObject(GetObjectRequest.builder().bucket("bucket").key("key").build()));
   }
 

File: flink/v1.17/flink/src/test/java/org/apache/iceberg/flink/sink/TestCompressionSettings.java
Patch:
@@ -215,7 +215,7 @@ private static OneInputStreamOperatorTestHarness<RowData, WriteResult> createIce
             icebergTable, override, new org.apache.flink.configuration.Configuration());
 
     IcebergStreamWriter<RowData> streamWriter =
-        FlinkSink.createStreamWriter(icebergTable, flinkWriteConfig, flinkRowType, null);
+        FlinkSink.createStreamWriter(() -> icebergTable, flinkWriteConfig, flinkRowType, null);
     OneInputStreamOperatorTestHarness<RowData, WriteResult> harness =
         new OneInputStreamOperatorTestHarness<>(streamWriter, 1, 1, 0);
 

File: flink/v1.17/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergStreamWriter.java
Patch:
@@ -376,7 +376,7 @@ private OneInputStreamOperatorTestHarness<RowData, WriteResult> createIcebergStr
             icebergTable, Maps.newHashMap(), new org.apache.flink.configuration.Configuration());
 
     IcebergStreamWriter<RowData> streamWriter =
-        FlinkSink.createStreamWriter(icebergTable, flinkWriteConfig, flinkRowType, null);
+        FlinkSink.createStreamWriter(() -> icebergTable, flinkWriteConfig, flinkRowType, null);
     OneInputStreamOperatorTestHarness<RowData, WriteResult> harness =
         new OneInputStreamOperatorTestHarness<>(streamWriter, 1, 1, 0);
 

File: core/src/main/java/org/apache/iceberg/TableProperties.java
Patch:
@@ -143,6 +143,7 @@ private TableProperties() {}
   public static final String PARQUET_COMPRESSION = "write.parquet.compression-codec";
   public static final String DELETE_PARQUET_COMPRESSION = "write.delete.parquet.compression-codec";
   public static final String PARQUET_COMPRESSION_DEFAULT = "gzip";
+  public static final String PARQUET_COMPRESSION_DEFAULT_SINCE_1_4_0 = "zstd";
 
   public static final String PARQUET_COMPRESSION_LEVEL = "write.parquet.compression-level";
   public static final String DELETE_PARQUET_COMPRESSION_LEVEL =

File: flink/v1.15/flink/src/test/java/org/apache/iceberg/flink/sink/TestCompressionSettings.java
Patch:
@@ -134,11 +134,11 @@ public void testCompressionParquet() throws Exception {
 
     if (initProperties.get(TableProperties.PARQUET_COMPRESSION) == null) {
       Assert.assertEquals(
-          TableProperties.PARQUET_COMPRESSION_DEFAULT,
+          TableProperties.PARQUET_COMPRESSION_DEFAULT_SINCE_1_4_0,
           resultProperties.get(TableProperties.PARQUET_COMPRESSION));
       Assert.assertEquals(
           TableProperties.PARQUET_COMPRESSION_LEVEL_DEFAULT,
-          resultProperties.get(TableProperties.PARQUET_COMPRESSION_LEVEL));
+          resultProperties.get(TableProperties.PARQUET_COMPRESSION_LEVEL_DEFAULT));
     } else {
       Assert.assertEquals(
           initProperties.get(TableProperties.PARQUET_COMPRESSION),

File: flink/v1.16/flink/src/test/java/org/apache/iceberg/flink/sink/TestCompressionSettings.java
Patch:
@@ -134,7 +134,7 @@ public void testCompressionParquet() throws Exception {
 
     if (initProperties.get(TableProperties.PARQUET_COMPRESSION) == null) {
       Assert.assertEquals(
-          TableProperties.PARQUET_COMPRESSION_DEFAULT,
+          TableProperties.PARQUET_COMPRESSION_DEFAULT_SINCE_1_4_0,
           resultProperties.get(TableProperties.PARQUET_COMPRESSION));
       Assert.assertEquals(
           TableProperties.PARQUET_COMPRESSION_LEVEL_DEFAULT,

File: flink/v1.17/flink/src/test/java/org/apache/iceberg/flink/sink/TestCompressionSettings.java
Patch:
@@ -134,11 +134,11 @@ public void testCompressionParquet() throws Exception {
 
     if (initProperties.get(TableProperties.PARQUET_COMPRESSION) == null) {
       Assert.assertEquals(
-          TableProperties.PARQUET_COMPRESSION_DEFAULT,
+          TableProperties.PARQUET_COMPRESSION_DEFAULT_SINCE_1_4_0,
           resultProperties.get(TableProperties.PARQUET_COMPRESSION));
       Assert.assertEquals(
           TableProperties.PARQUET_COMPRESSION_LEVEL_DEFAULT,
-          resultProperties.get(TableProperties.PARQUET_COMPRESSION_LEVEL));
+          resultProperties.get(TableProperties.PARQUET_COMPRESSION_DEFAULT_SINCE_1_4_0));
     } else {
       Assert.assertEquals(
           initProperties.get(TableProperties.PARQUET_COMPRESSION),

File: mr/src/test/java/org/apache/iceberg/mr/TestCatalogs.java
Patch:
@@ -19,9 +19,9 @@
 package org.apache.iceberg.mr;
 
 import static org.apache.iceberg.types.Types.NestedField.required;
+import static org.assertj.core.api.Assertions.assertThat;
 
 import java.io.IOException;
-import java.util.Collections;
 import java.util.Optional;
 import java.util.Properties;
 import org.apache.hadoop.conf.Configuration;
@@ -126,7 +126,7 @@ public void testCreateDropTableToLocation() throws IOException {
     Assert.assertEquals(properties.getProperty("location"), table.location());
     Assert.assertEquals(SchemaParser.toJson(SCHEMA), SchemaParser.toJson(table.schema()));
     Assert.assertEquals(PartitionSpecParser.toJson(SPEC), PartitionSpecParser.toJson(table.spec()));
-    Assert.assertEquals(Collections.singletonMap("dummy", "test"), table.properties());
+    assertThat(table.properties()).containsEntry("dummy", "test");
 
     Assertions.assertThatThrownBy(() -> Catalogs.dropTable(conf, new Properties()))
         .isInstanceOf(NullPointerException.class)
@@ -178,7 +178,7 @@ public void testCreateDropTableToCatalog() throws IOException {
 
     Assert.assertEquals(SchemaParser.toJson(SCHEMA), SchemaParser.toJson(table.schema()));
     Assert.assertEquals(PartitionSpecParser.toJson(SPEC), PartitionSpecParser.toJson(table.spec()));
-    Assert.assertEquals(Collections.singletonMap("dummy", "test"), table.properties());
+    assertThat(table.properties()).containsEntry("dummy", "test");
 
     Assertions.assertThatThrownBy(() -> Catalogs.dropTable(conf, new Properties()))
         .isInstanceOf(NullPointerException.class)

File: core/src/test/java/org/apache/iceberg/TestCommitReporting.java
Patch:
@@ -21,7 +21,7 @@
 import static org.assertj.core.api.Assertions.assertThat;
 
 import java.io.IOException;
-import org.apache.iceberg.TestScanPlanningAndReporting.TestMetricsReporter;
+import org.apache.iceberg.ScanPlanningAndReportingTestBase.TestMetricsReporter;
 import org.apache.iceberg.metrics.CommitMetricsResult;
 import org.apache.iceberg.metrics.CommitReport;
 import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;

File: core/src/main/java/org/apache/iceberg/TableOperations.java
Patch:
@@ -121,9 +121,10 @@ default long newSnapshotId() {
    * Whether to clean up uncommitted metadata files only when a commit fails with a {@link
    * CleanableFailure} exception.
    *
-   * <p>This defaults to false: any unexpected exception will cause metadata files to be cleaned up.
+   * <p>This defaults to true: cleanup will only occur for exceptions marked as {@link
+   * CleanableFailure}
    */
   default boolean requireStrictCleanup() {
-    return false;
+    return true;
   }
 }

File: parquet/src/main/java/org/apache/iceberg/parquet/ParquetUtil.java
Patch:
@@ -133,9 +133,7 @@ public static Metrics footerMetrics(
         increment(valueCounts, fieldId, column.getValueCount());
 
         Statistics stats = column.getStatistics();
-        if (stats == null) {
-          missingStats.add(fieldId);
-        } else if (!stats.isEmpty()) {
+        if (stats != null && !stats.isEmpty()) {
           increment(nullValueCounts, fieldId, stats.getNumNulls());
 
           // when there are metrics gathered by Iceberg for a column, we should use those instead
@@ -153,6 +151,8 @@ public static Metrics footerMetrics(
               updateMax(upperBounds, fieldId, field.type(), max, metricsMode);
             }
           }
+        } else {
+          missingStats.add(fieldId);
         }
       }
     }

File: spark/v3.2/spark/src/main/java/org/apache/iceberg/spark/procedures/ExpireSnapshotsProcedure.java
Patch:
@@ -126,7 +126,7 @@ public InternalRow[] call(InternalRow args) {
           if (maxConcurrentDeletes != null) {
             if (table.io() instanceof SupportsBulkOperations) {
               LOG.warn(
-                  "max_concurrent_deletes only works with FileIOs that do not support bulk deletes. This"
+                  "max_concurrent_deletes only works with FileIOs that do not support bulk deletes. This "
                       + "table is currently using {} which supports bulk deletes so the parameter will be ignored. "
                       + "See that IO's documentation to learn how to adjust parallelism for that particular "
                       + "IO's bulk delete.",

File: spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/procedures/ExpireSnapshotsProcedure.java
Patch:
@@ -126,7 +126,7 @@ public InternalRow[] call(InternalRow args) {
           if (maxConcurrentDeletes != null) {
             if (table.io() instanceof SupportsBulkOperations) {
               LOG.warn(
-                  "max_concurrent_deletes only works with FileIOs that do not support bulk deletes. This"
+                  "max_concurrent_deletes only works with FileIOs that do not support bulk deletes. This "
                       + "table is currently using {} which supports bulk deletes so the parameter will be ignored. "
                       + "See that IO's documentation to learn how to adjust parallelism for that particular "
                       + "IO's bulk delete.",

File: spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/procedures/ExpireSnapshotsProcedure.java
Patch:
@@ -126,7 +126,7 @@ public InternalRow[] call(InternalRow args) {
           if (maxConcurrentDeletes != null) {
             if (table.io() instanceof SupportsBulkOperations) {
               LOG.warn(
-                  "max_concurrent_deletes only works with FileIOs that do not support bulk deletes. This"
+                  "max_concurrent_deletes only works with FileIOs that do not support bulk deletes. This "
                       + "table is currently using {} which supports bulk deletes so the parameter will be ignored. "
                       + "See that IO's documentation to learn how to adjust parallelism for that particular "
                       + "IO's bulk delete.",

File: hive-metastore/src/main/java/org/apache/iceberg/hive/MetastoreLock.java
Patch:
@@ -310,7 +310,7 @@ private LockInfo createLock() throws LockException {
                 try {
                   // If we can not check for lock, or we do not find it, then rethrow the exception
                   // Otherwise we are happy as the findLock sets the lockId and the state correctly
-                  if (!HiveVersion.min(HiveVersion.HIVE_2)) {
+                  if (HiveVersion.min(HiveVersion.HIVE_2)) {
                     LockInfo lockFound = findLock();
                     if (lockFound != null) {
                       lockInfo.lockId = lockFound.lockId;

File: spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkWriteConf.java
Patch:
@@ -418,8 +418,7 @@ private void testWriteProperties(List<Map<String, String>> propertiesSuite) {
 
           updateProperties.commit();
 
-          Map<String, String> writeOptions = ImmutableMap.of();
-          SparkWriteConf writeConf = new SparkWriteConf(spark, table, writeOptions);
+          SparkWriteConf writeConf = new SparkWriteConf(spark, table, ImmutableMap.of());
           Map<String, String> writeProperties = writeConf.writeProperties();
           Map<String, String> expectedProperties = propertiesSuite.get(2);
           Assert.assertEquals(expectedProperties.size(), writeConf.writeProperties().size());

File: spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/source/SparkPositionDeletesRewrite.java
Patch:
@@ -108,7 +108,7 @@ public class SparkPositionDeletesRewrite implements Write {
     this.fileSetId = writeConf.rewrittenFileSetId();
     this.specId = specId;
     this.partition = partition;
-    this.writeProperties = writeConf.writeProperties(format);
+    this.writeProperties = writeConf.writeProperties();
   }
 
   @Override
@@ -221,6 +221,7 @@ public DataWriter<InternalRow> createWriter(int partitionId, long taskId) {
               .deleteFileFormat(format)
               .positionDeleteRowSchema(positionDeleteRowSchema)
               .positionDeleteSparkType(deleteSparkType)
+              .writeProperties(writeProperties)
               .build();
       SparkFileWriterFactory writerFactoryWithoutRow =
           SparkFileWriterFactory.builderFor(table)

File: spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/source/SparkPositionDeltaWrite.java
Patch:
@@ -127,7 +127,7 @@ class SparkPositionDeltaWrite implements DeltaWrite, RequiresDistributionAndOrde
     this.extraSnapshotMetadata = writeConf.extraSnapshotMetadata();
     this.writeRequirements = writeConf.positionDeltaRequirements(command);
     this.context = new Context(dataSchema, writeConf, info, writeRequirements);
-    this.writeProperties = writeConf.writeProperties(context.dataFileFormat);
+    this.writeProperties = writeConf.writeProperties();
   }
 
   @Override

File: spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/source/SparkWrite.java
Patch:
@@ -129,7 +129,7 @@ abstract class SparkWrite implements Write, RequiresDistributionAndOrdering {
     this.partitionedFanoutEnabled = writeConf.fanoutWriterEnabled();
     this.writeRequirements = writeRequirements;
     this.outputSpecId = writeConf.outputSpecId();
-    this.writeProperties = writeConf.writeProperties(format);
+    this.writeProperties = writeConf.writeProperties();
   }
 
   @Override

File: core/src/main/java/org/apache/iceberg/MergingSnapshotProducer.java
Patch:
@@ -682,9 +682,9 @@ protected void setNewDataFilesDataSequenceNumber(long sequenceNumber) {
     this.newDataFilesDataSequenceNumber = sequenceNumber;
   }
 
-  private long startingSequenceNumber(TableMetadata metadata, Long staringSnapshotId) {
-    if (staringSnapshotId != null && metadata.snapshot(staringSnapshotId) != null) {
-      Snapshot startingSnapshot = metadata.snapshot(staringSnapshotId);
+  private long startingSequenceNumber(TableMetadata metadata, Long startingSnapshotId) {
+    if (startingSnapshotId != null && metadata.snapshot(startingSnapshotId) != null) {
+      Snapshot startingSnapshot = metadata.snapshot(startingSnapshotId);
       return startingSnapshot.sequenceNumber();
     } else {
       return TableMetadata.INITIAL_SEQUENCE_NUMBER;

File: core/src/main/java/org/apache/iceberg/TableMetadata.java
Patch:
@@ -1141,7 +1141,9 @@ public Builder addSnapshot(Snapshot snapshot) {
           snapshot.snapshotId());
 
       ValidationException.check(
-          formatVersion == 1 || snapshot.sequenceNumber() > lastSequenceNumber,
+          formatVersion == 1
+              || snapshot.sequenceNumber() > lastSequenceNumber
+              || snapshot.parentId() == null,
           "Cannot add snapshot with sequence number %s older than last sequence number %s",
           snapshot.sequenceNumber(),
           lastSequenceNumber);

File: spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/source/SparkCopyOnWriteScan.java
Patch:
@@ -107,6 +107,7 @@ public void filter(Filter[] filters) {
         Objects.equals(snapshotId(), currentSnapshotId()),
         "Runtime file filtering is not possible: the table has been concurrently modified. "
             + "Row-level operation scan snapshot ID: %s, current table snapshot ID: %s. "
+            + "If an external process modifies the table, enable table caching in the catalog. "
             + "If multiple threads modify the table, use independent Spark sessions in each thread.",
         snapshotId(),
         currentSnapshotId());

File: spark/v3.1/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java
Patch:
@@ -80,6 +80,7 @@
 import org.apache.iceberg.io.OutputFile;
 import org.apache.iceberg.relocated.com.google.common.base.Preconditions;
 import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;
+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;
 import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;
 import org.apache.iceberg.relocated.com.google.common.collect.Iterables;
 import org.apache.iceberg.relocated.com.google.common.collect.Lists;
@@ -334,7 +335,8 @@ public void testBinPackWithStartingSequenceNumber() {
 
   @Test
   public void testBinPackWithStartingSequenceNumberV1Compatibility() {
-    Table table = createTablePartitioned(4, 2);
+    Map<String, String> properties = ImmutableMap.of(TableProperties.FORMAT_VERSION, "1");
+    Table table = createTablePartitioned(4, 2, SCALE, properties);
     shouldHaveFiles(table, 8);
     List<Object[]> expectedRecords = currentData();
     table.refresh();

File: spark/v3.1/spark/src/test/java/org/apache/iceberg/spark/sql/TestCreateTableAsSelect.java
Patch:
@@ -388,7 +388,6 @@ public void testCreateRTASWithPartitionSpecChanging() {
 
     PartitionSpec expectedSpec =
         PartitionSpec.builderFor(expectedSchema)
-            .alwaysNull("part", "part_1000")
             .identity("part")
             .identity("id")
             .withSpecId(2) // The Spec is new

File: spark/v3.2/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateChangelogViewProcedure.java
Patch:
@@ -47,19 +47,16 @@ public void removeTable() {
 
   public void createTableWith2Columns() {
     sql("CREATE TABLE %s (id INT, data STRING) USING iceberg", tableName);
-    sql("ALTER TABLE %s SET TBLPROPERTIES ('format-version'='%d')", tableName, 1);
     sql("ALTER TABLE %s ADD PARTITION FIELD data", tableName);
   }
 
   private void createTableWith3Columns() {
     sql("CREATE TABLE %s (id INT, data STRING, age INT) USING iceberg", tableName);
-    sql("ALTER TABLE %s SET TBLPROPERTIES ('format-version'='%d')", tableName, 1);
     sql("ALTER TABLE %s ADD PARTITION FIELD id", tableName);
   }
 
   private void createTableWithIdentifierField() {
     sql("CREATE TABLE %s (id INT NOT NULL, data STRING) USING iceberg", tableName);
-    sql("ALTER TABLE %s SET TBLPROPERTIES ('format-version'='%d')", tableName, 1);
     sql("ALTER TABLE %s SET IDENTIFIER FIELDS id", tableName);
   }
 

File: spark/v3.2/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java
Patch:
@@ -84,6 +84,7 @@
 import org.apache.iceberg.io.OutputFile;
 import org.apache.iceberg.relocated.com.google.common.base.Preconditions;
 import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;
+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;
 import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;
 import org.apache.iceberg.relocated.com.google.common.collect.Iterables;
 import org.apache.iceberg.relocated.com.google.common.collect.Lists;
@@ -370,7 +371,8 @@ public void testBinPackWithStartingSequenceNumber() {
 
   @Test
   public void testBinPackWithStartingSequenceNumberV1Compatibility() {
-    Table table = createTablePartitioned(4, 2);
+    Map<String, String> properties = ImmutableMap.of(TableProperties.FORMAT_VERSION, "1");
+    Table table = createTablePartitioned(4, 2, SCALE, properties);
     shouldHaveFiles(table, 8);
     List<Object[]> expectedRecords = currentData();
     table.refresh();

File: spark/v3.2/spark/src/test/java/org/apache/iceberg/spark/sql/TestCreateTableAsSelect.java
Patch:
@@ -388,7 +388,6 @@ public void testCreateRTASWithPartitionSpecChanging() {
 
     PartitionSpec expectedSpec =
         PartitionSpec.builderFor(expectedSchema)
-            .alwaysNull("part", "part_1000")
             .identity("part")
             .identity("id")
             .withSpecId(2) // The Spec is new

File: spark/v3.3/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateChangelogViewProcedure.java
Patch:
@@ -49,19 +49,16 @@ public void removeTable() {
 
   public void createTableWithTwoColumns() {
     sql("CREATE TABLE %s (id INT, data STRING) USING iceberg", tableName);
-    sql("ALTER TABLE %s SET TBLPROPERTIES ('format-version'='%d')", tableName, 1);
     sql("ALTER TABLE %s ADD PARTITION FIELD data", tableName);
   }
 
   private void createTableWithThreeColumns() {
     sql("CREATE TABLE %s (id INT, data STRING, age INT) USING iceberg", tableName);
-    sql("ALTER TABLE %s SET TBLPROPERTIES ('format-version'='%d')", tableName, 1);
     sql("ALTER TABLE %s ADD PARTITION FIELD id", tableName);
   }
 
   private void createTableWithIdentifierField() {
     sql("CREATE TABLE %s (id INT NOT NULL, data STRING) USING iceberg", tableName);
-    sql("ALTER TABLE %s SET TBLPROPERTIES ('format-version'='%d')", tableName, 1);
     sql("ALTER TABLE %s SET IDENTIFIER FIELDS id", tableName);
   }
 

File: spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java
Patch:
@@ -84,6 +84,7 @@
 import org.apache.iceberg.io.OutputFile;
 import org.apache.iceberg.relocated.com.google.common.base.Preconditions;
 import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;
+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;
 import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;
 import org.apache.iceberg.relocated.com.google.common.collect.Iterables;
 import org.apache.iceberg.relocated.com.google.common.collect.Lists;
@@ -371,7 +372,8 @@ public void testBinPackWithStartingSequenceNumber() {
 
   @Test
   public void testBinPackWithStartingSequenceNumberV1Compatibility() {
-    Table table = createTablePartitioned(4, 2);
+    Map<String, String> properties = ImmutableMap.of(TableProperties.FORMAT_VERSION, "1");
+    Table table = createTablePartitioned(4, 2, SCALE, properties);
     shouldHaveFiles(table, 8);
     List<Object[]> expectedRecords = currentData();
     table.refresh();

File: spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/sql/TestCreateTableAsSelect.java
Patch:
@@ -388,7 +388,6 @@ public void testCreateRTASWithPartitionSpecChanging() {
 
     PartitionSpec expectedSpec =
         PartitionSpec.builderFor(expectedSchema)
-            .alwaysNull("part", "part_1000")
             .identity("part")
             .identity("id")
             .withSpecId(2) // The Spec is new

File: spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateChangelogViewProcedure.java
Patch:
@@ -49,19 +49,16 @@ public void removeTable() {
 
   public void createTableWithTwoColumns() {
     sql("CREATE TABLE %s (id INT, data STRING) USING iceberg", tableName);
-    sql("ALTER TABLE %s SET TBLPROPERTIES ('format-version'='%d')", tableName, 1);
     sql("ALTER TABLE %s ADD PARTITION FIELD data", tableName);
   }
 
   private void createTableWithThreeColumns() {
     sql("CREATE TABLE %s (id INT, data STRING, age INT) USING iceberg", tableName);
-    sql("ALTER TABLE %s SET TBLPROPERTIES ('format-version'='%d')", tableName, 1);
     sql("ALTER TABLE %s ADD PARTITION FIELD id", tableName);
   }
 
   private void createTableWithIdentifierField() {
     sql("CREATE TABLE %s (id INT NOT NULL, data STRING) USING iceberg", tableName);
-    sql("ALTER TABLE %s SET TBLPROPERTIES ('format-version'='%d')", tableName, 1);
     sql("ALTER TABLE %s SET IDENTIFIER FIELDS id", tableName);
   }
 

File: spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java
Patch:
@@ -83,6 +83,7 @@
 import org.apache.iceberg.io.OutputFile;
 import org.apache.iceberg.relocated.com.google.common.base.Preconditions;
 import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;
+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;
 import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;
 import org.apache.iceberg.relocated.com.google.common.collect.Iterables;
 import org.apache.iceberg.relocated.com.google.common.collect.Lists;
@@ -381,7 +382,8 @@ public void testBinPackWithStartingSequenceNumber() {
 
   @Test
   public void testBinPackWithStartingSequenceNumberV1Compatibility() {
-    Table table = createTablePartitioned(4, 2);
+    Map<String, String> properties = ImmutableMap.of(TableProperties.FORMAT_VERSION, "1");
+    Table table = createTablePartitioned(4, 2, SCALE, properties);
     shouldHaveFiles(table, 8);
     List<Object[]> expectedRecords = currentData();
     table.refresh();

File: spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestCreateTableAsSelect.java
Patch:
@@ -388,7 +388,6 @@ public void testCreateRTASWithPartitionSpecChanging() {
 
     PartitionSpec expectedSpec =
         PartitionSpec.builderFor(expectedSchema)
-            .alwaysNull("part", "part_1000")
             .identity("part")
             .identity("id")
             .withSpecId(2) // The Spec is new

File: spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/source/SparkPositionDeltaWrite.java
Patch:
@@ -102,9 +102,9 @@ class SparkPositionDeltaWrite implements DeltaWrite, RequiresDistributionAndOrde
   private final Map<String, String> extraSnapshotMetadata;
   private final SparkWriteRequirements writeRequirements;
   private final Context context;
+  private final Map<String, String> writeProperties;
 
   private boolean cleanupOnAbort = true;
-  private final Map<String, String> writeProperties;
 
   SparkPositionDeltaWrite(
       SparkSession spark,

File: nessie/src/test/java/org/apache/iceberg/nessie/TestNessieIcebergClient.java
Patch:
@@ -70,7 +70,7 @@ public void testWithReference() throws NessieNotFoundException {
   public void testWithReferenceAfterRecreatingBranch()
       throws NessieConflictException, NessieNotFoundException {
     String branch = "branchToBeDropped";
-    createBranch(branch, null);
+    createBranch(branch);
     NessieIcebergClient client = new NessieIcebergClient(api, branch, null, ImmutableMap.of());
 
     // just create a new commit on the branch and then delete & re-create it
@@ -82,7 +82,7 @@ public void testWithReferenceAfterRecreatingBranch()
         .deleteBranch()
         .branch((Branch) client.getApi().getReference().refName(branch).get())
         .delete();
-    createBranch(branch, null);
+    createBranch(branch);
 
     // make sure the client uses the re-created branch
     Reference ref = client.getApi().getReference().refName(branch).get();

File: parquet/src/main/java/org/apache/iceberg/parquet/ParquetAvroValueReaders.java
Patch:
@@ -191,9 +191,9 @@ public ParquetValueReader<?> primitive(
               case FIXED_LEN_BYTE_ARRAY:
                 return new DecimalReader(desc, decimal.getScale());
               case INT64:
-                return new IntegerAsDecimalReader(desc, decimal.getScale());
-              case INT32:
                 return new LongAsDecimalReader(desc, decimal.getScale());
+              case INT32:
+                return new IntegerAsDecimalReader(desc, decimal.getScale());
               default:
                 throw new UnsupportedOperationException(
                     "Unsupported base type for decimal: " + primitive.getPrimitiveTypeName());

File: spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestParquetAvroReader.java
Patch:
@@ -84,7 +84,8 @@ public class TestParquetAvroReader {
                       optional(22, "jumpy", Types.DoubleType.get()),
                       required(23, "koala", Types.TimeType.get()),
                       required(24, "couch rope", Types.IntegerType.get())))),
-          optional(2, "slide", Types.StringType.get()));
+          optional(2, "slide", Types.StringType.get()),
+          required(25, "foo", Types.DecimalType.of(7, 5)));
 
   @Ignore
   public void testStructSchema() throws IOException {

File: api/src/main/java/org/apache/iceberg/view/ViewVersion.java
Patch:
@@ -78,9 +78,7 @@ default String defaultCatalog() {
   }
 
   /** The default namespace to use when the SQL does not contain a namespace. */
-  default Namespace defaultNamespace() {
-    return null;
-  }
+  Namespace defaultNamespace();
 
   default void check() {
     Preconditions.checkArgument(

File: core/src/test/java/org/apache/iceberg/view/TestViewMetadataParser.java
Patch:
@@ -25,6 +25,7 @@
 import java.nio.file.Path;
 import java.nio.file.Paths;
 import org.apache.iceberg.Schema;
+import org.apache.iceberg.catalog.Namespace;
 import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;
 import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;
 import org.apache.iceberg.types.Types;
@@ -63,6 +64,7 @@ public void readAndWriteValidViewMetadata() throws Exception {
             .summary(ImmutableMap.of("operation", "create"))
             .schemaId(1)
             .defaultCatalog("some-catalog")
+            .defaultNamespace(Namespace.empty())
             .addRepresentations(
                 ImmutableSQLViewRepresentation.builder()
                     .sql("select 'foo' foo")
@@ -80,6 +82,7 @@ public void readAndWriteValidViewMetadata() throws Exception {
             .timestampMillis(5555L)
             .summary(ImmutableMap.of("operation", "replace"))
             .defaultCatalog("some-catalog")
+            .defaultNamespace(Namespace.empty())
             .addRepresentations(
                 ImmutableSQLViewRepresentation.builder()
                     .sql("select 1 id, 'abc' data")

File: api/src/main/java/org/apache/iceberg/ContentFile.java
Patch:
@@ -63,7 +63,8 @@ public interface ContentFile<F> {
   Map<Integer, Long> columnSizes();
 
   /**
-   * Returns if collected, map from column ID to the count of its non-null values, null otherwise.
+   * Returns if collected, map from column ID to the count of its values (including null and NaN
+   * values), null otherwise.
    */
   Map<Integer, Long> valueCounts();
 

File: spark/v3.2/spark/src/main/java/org/apache/iceberg/spark/source/SparkScanBuilder.java
Patch:
@@ -420,7 +420,7 @@ public Scan buildCopyOnWriteScan() {
 
   @Override
   public Statistics estimateStatistics() {
-    return ((SparkScan) build()).estimateStatistics();
+    return ((SupportsReportStatistics) build()).estimateStatistics();
   }
 
   @Override

File: spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/source/SparkScanBuilder.java
Patch:
@@ -658,7 +658,7 @@ public Scan buildCopyOnWriteScan() {
 
   @Override
   public Statistics estimateStatistics() {
-    return ((SparkScan) build()).estimateStatistics();
+    return ((SupportsReportStatistics) build()).estimateStatistics();
   }
 
   @Override

File: spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/source/SparkScanBuilder.java
Patch:
@@ -700,7 +700,7 @@ public Scan buildCopyOnWriteScan() {
 
   @Override
   public Statistics estimateStatistics() {
-    return ((SparkScan) build()).estimateStatistics();
+    return ((SupportsReportStatistics) build()).estimateStatistics();
   }
 
   @Override

File: spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/actions/SparkBinPackPositionDeletesRewriter.java
Patch:
@@ -131,7 +131,7 @@ private Dataset<Row> dataFiles(Types.StructType partitionType, StructLike partit
                   Type type = fields.get(i).type();
                   Object value = partition.get(i, type.typeId().javaClass());
                   Object convertedValue = SparkValueConverter.convertToSpark(type, value);
-                  Column col = col("partition." + fields.get(i).name());
+                  Column col = col("partition.`" + fields.get(i).name() + "`");
                   return col.eqNullSafe(lit(convertedValue));
                 })
             .reduce(Column::and);

File: spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/actions/SparkBinPackPositionDeletesRewriter.java
Patch:
@@ -131,7 +131,7 @@ private Dataset<Row> dataFiles(Types.StructType partitionType, StructLike partit
                   Type type = fields.get(i).type();
                   Object value = partition.get(i, type.typeId().javaClass());
                   Object convertedValue = SparkValueConverter.convertToSpark(type, value);
-                  Column col = col("partition." + fields.get(i).name());
+                  Column col = col("partition.`" + fields.get(i).name() + "`");
                   return col.eqNullSafe(lit(convertedValue));
                 })
             .reduce(Column::and);

File: hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveMetastore.java
Patch:
@@ -112,7 +112,7 @@ public class TestHiveMetastore {
                     FileSystem fs = Util.getFs(localDirPath, new Configuration());
                     String errMsg = "Failed to delete " + localDirPath;
                     try {
-                      assertThat(fs.delete(localDirPath, true)).isEqualTo(errMsg);
+                      assertThat(fs.delete(localDirPath, true)).as(errMsg).isTrue();
                     } catch (IOException e) {
                       throw new RuntimeException(errMsg, e);
                     }

File: gcp/src/main/java/org/apache/iceberg/gcp/gcs/GCSInputStream.java
Patch:
@@ -117,7 +117,7 @@ public int read() throws IOException {
     readBytes.increment();
     readOperations.increment();
 
-    return singleByteBuffer.array()[0];
+    return singleByteBuffer.array()[0] & 0xFF;
   }
 
   @Override

File: core/src/test/java/org/apache/iceberg/catalog/CatalogTests.java
Patch:
@@ -74,19 +74,19 @@ public abstract class CatalogTests<C extends Catalog & SupportsNamespaces> {
   // Schema passed to create tables
   protected static final Schema SCHEMA =
       new Schema(
-          required(3, "id", Types.IntegerType.get(), "unique ID"),
+          required(3, "id", Types.IntegerType.get(), "unique ID "),
           required(4, "data", Types.StringType.get()));
 
   // This is the actual schema for the table, with column IDs reassigned
   private static final Schema TABLE_SCHEMA =
       new Schema(
-          required(1, "id", Types.IntegerType.get(), "unique ID"),
+          required(1, "id", Types.IntegerType.get(), "unique ID "),
           required(2, "data", Types.StringType.get()));
 
   // This is the actual schema for the table, with column IDs reassigned
   private static final Schema REPLACE_SCHEMA =
       new Schema(
-          required(2, "id", Types.IntegerType.get(), "unique ID"),
+          required(2, "id", Types.IntegerType.get(), "unique ID "),
           required(3, "data", Types.StringType.get()));
 
   // another schema that is not the same

File: core/src/main/java/org/apache/iceberg/io/RollingFileWriter.java
Patch:
@@ -109,7 +109,7 @@ protected void openCurrentWriter() {
   }
 
   private EncryptedOutputFile newFile() {
-    if (partition == null) {
+    if (spec.isUnpartitioned() || partition == null) {
       return fileFactory.newOutputFile();
     } else {
       return fileFactory.newOutputFile(spec, partition);

File: api/src/test/java/org/apache/iceberg/types/TestTypes.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.iceberg.types;
 
 import org.assertj.core.api.Assertions;
-import org.junit.Test;
+import org.junit.jupiter.api.Test;
 
 public class TestTypes {
 

File: api/src/test/java/org/apache/iceberg/util/TestCharSequenceSet.java
Patch:
@@ -22,7 +22,7 @@
 import java.util.Set;
 import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;
 import org.assertj.core.api.Assertions;
-import org.junit.Test;
+import org.junit.jupiter.api.Test;
 
 public class TestCharSequenceSet {
 

File: api/src/test/java/org/apache/iceberg/util/TestDateTimeUtil.java
Patch:
@@ -20,7 +20,7 @@
 
 import java.time.ZonedDateTime;
 import org.assertj.core.api.Assertions;
-import org.junit.Test;
+import org.junit.jupiter.api.Test;
 
 public class TestDateTimeUtil {
 

File: api/src/test/java/org/apache/iceberg/util/TestExceptionUtil.java
Patch:
@@ -21,7 +21,7 @@
 import java.io.IOException;
 import java.util.Arrays;
 import org.assertj.core.api.Assertions;
-import org.junit.Test;
+import org.junit.jupiter.api.Test;
 
 public class TestExceptionUtil {
 

File: aws/src/test/java/org/apache/iceberg/aws/s3/signer/TestS3RestSigner.java
Patch:
@@ -134,6 +134,7 @@ public void before() throws Exception {
                     s3ClientBuilder.httpClientBuilder(
                         software.amazon.awssdk.http.apache.ApacheHttpClient.builder()))
             .endpointOverride(minioContainer.getURI())
+            .forcePathStyle(true) // OSX won't resolve subdomains
             .overrideConfiguration(
                 c -> c.putAdvancedOption(SdkAdvancedClientOption.SIGNER, validatingSigner))
             .build();

File: core/src/main/java/org/apache/iceberg/BaseFilesTable.java
Patch:
@@ -185,7 +185,7 @@ private CloseableIterable<? extends ContentFile<?>> files(Schema fileProjection)
      * Given content file metadata, append a 'readable_metrics' column that return the file's
      * metrics in human-readable form.
      *
-     * @file content file metadata
+     * @param file content file metadata
      * @param readableMetricsField projected "readable_metrics" field
      * @return struct representing content file, with appended readable_metrics field
      */

File: core/src/main/java/org/apache/iceberg/TableProperties.java
Patch:
@@ -135,6 +135,9 @@ private TableProperties() {}
   public static final String DELETE_PARQUET_PAGE_ROW_LIMIT = "write.delete.parquet.page-row-limit";
   public static final int PARQUET_PAGE_ROW_LIMIT_DEFAULT = 20_000;
 
+  public static final String PARQUET_DICT_ENABLED = "write.parquet.enable.dictionary";
+  public static final boolean PARQUET_DICT_ENABLED_DEFAULT = true;
+
   public static final String PARQUET_DICT_SIZE_BYTES = "write.parquet.dict-size-bytes";
   public static final String DELETE_PARQUET_DICT_SIZE_BYTES =
       "write.delete.parquet.dict-size-bytes";

File: parquet/src/test/java/org/apache/iceberg/parquet/TestBloomRowGroupFilter.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.iceberg.parquet;
 
 import static org.apache.iceberg.TableProperties.PARQUET_BLOOM_FILTER_COLUMN_ENABLED_PREFIX;
+import static org.apache.iceberg.TableProperties.PARQUET_DICT_ENABLED;
 import static org.apache.iceberg.avro.AvroSchemaUtil.convert;
 import static org.apache.iceberg.expressions.Expressions.and;
 import static org.apache.iceberg.expressions.Expressions.equal;
@@ -197,6 +198,7 @@ public void createInputFile() throws IOException {
     try (FileAppender<Record> appender =
         Parquet.write(outFile)
             .schema(FILE_SCHEMA)
+            .set(PARQUET_DICT_ENABLED, "false")
             .set(PARQUET_BLOOM_FILTER_COLUMN_ENABLED_PREFIX + "_id", "true")
             .set(PARQUET_BLOOM_FILTER_COLUMN_ENABLED_PREFIX + "_long", "true")
             .set(PARQUET_BLOOM_FILTER_COLUMN_ENABLED_PREFIX + "_double", "true")

File: snowflake/src/main/java/org/apache/iceberg/snowflake/SnowflakeCatalog.java
Patch:
@@ -209,7 +209,7 @@ public Map<String, String> loadNamespaceMetadata(Namespace namespace)
       default:
         throw new IllegalArgumentException(
             String.format(
-                "loadNamespaceMetadat must be at either DATABASE or SCHEMA level; got %s from namespace %s",
+                "loadNamespaceMetadata must be at either DATABASE or SCHEMA level; got %s from namespace %s",
                 id, namespace));
     }
     if (namespaceExists) {

File: spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/actions/SparkActions.java
Patch:
@@ -93,7 +93,7 @@ public DeleteReachableFilesSparkAction deleteReachableFiles(String metadataLocat
   }
 
   @Override
-  public RewritePositionDeleteSparkAction rewritePositionDeletes(Table table) {
-    return new RewritePositionDeleteSparkAction(spark, table);
+  public RewritePositionDeleteFilesSparkAction rewritePositionDeletes(Table table) {
+    return new RewritePositionDeleteFilesSparkAction(spark, table);
   }
 }

File: spark/v3.3/spark/src/jmh/java/org/apache/iceberg/spark/data/parquet/SparkParquetReadersFlatDataBenchmark.java
Patch:
@@ -156,6 +156,7 @@ public void readUsingSparkReader(Blackhole blackhole) throws IOException {
             .set("spark.sql.parquet.int96AsTimestamp", "false")
             .set("spark.sql.caseSensitive", "false")
             .set("spark.sql.parquet.fieldId.write.enabled", "false")
+            .set("spark.sql.legacy.parquet.nanosAsLong", "false")
             .callInit()
             .build()) {
 
@@ -214,6 +215,7 @@ public void readWithProjectionUsingSparkReader(Blackhole blackhole) throws IOExc
             .set("spark.sql.parquet.binaryAsString", "false")
             .set("spark.sql.parquet.int96AsTimestamp", "false")
             .set("spark.sql.caseSensitive", "false")
+            .set("spark.sql.legacy.parquet.nanosAsLong", "false")
             .callInit()
             .build()) {
 

File: spark/v3.3/spark/src/jmh/java/org/apache/iceberg/spark/data/parquet/SparkParquetReadersNestedDataBenchmark.java
Patch:
@@ -154,6 +154,7 @@ public void readUsingSparkReader(Blackhole blackhole) throws IOException {
             .set("spark.sql.parquet.int96AsTimestamp", "false")
             .set("spark.sql.caseSensitive", "false")
             .set("spark.sql.parquet.fieldId.write.enabled", "false")
+            .set("spark.sql.legacy.parquet.nanosAsLong", "false")
             .callInit()
             .build()) {
 
@@ -212,6 +213,7 @@ public void readWithProjectionUsingSparkReader(Blackhole blackhole) throws IOExc
             .set("spark.sql.parquet.binaryAsString", "false")
             .set("spark.sql.parquet.int96AsTimestamp", "false")
             .set("spark.sql.caseSensitive", "false")
+            .set("spark.sql.legacy.parquet.nanosAsLong", "false")
             .callInit()
             .build()) {
 

File: flink/v1.15/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogDatabase.java
Patch:
@@ -37,8 +37,8 @@
 
 public class TestFlinkCatalogDatabase extends FlinkCatalogTestBase {
 
-  public TestFlinkCatalogDatabase(String catalogName, Namespace baseNamepace) {
-    super(catalogName, baseNamepace);
+  public TestFlinkCatalogDatabase(String catalogName, Namespace baseNamespace) {
+    super(catalogName, baseNamespace);
   }
 
   @After

File: flink/v1.15/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogTable.java
Patch:
@@ -62,8 +62,8 @@
 
 public class TestFlinkCatalogTable extends FlinkCatalogTestBase {
 
-  public TestFlinkCatalogTable(String catalogName, Namespace baseNamepace) {
-    super(catalogName, baseNamepace);
+  public TestFlinkCatalogTable(String catalogName, Namespace baseNamespace) {
+    super(catalogName, baseNamespace);
   }
 
   @Override

File: flink/v1.16/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogDatabase.java
Patch:
@@ -37,8 +37,8 @@
 
 public class TestFlinkCatalogDatabase extends FlinkCatalogTestBase {
 
-  public TestFlinkCatalogDatabase(String catalogName, Namespace baseNamepace) {
-    super(catalogName, baseNamepace);
+  public TestFlinkCatalogDatabase(String catalogName, Namespace baseNamespace) {
+    super(catalogName, baseNamespace);
   }
 
   @After

File: flink/v1.16/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogTable.java
Patch:
@@ -62,8 +62,8 @@
 
 public class TestFlinkCatalogTable extends FlinkCatalogTestBase {
 
-  public TestFlinkCatalogTable(String catalogName, Namespace baseNamepace) {
-    super(catalogName, baseNamepace);
+  public TestFlinkCatalogTable(String catalogName, Namespace baseNamespace) {
+    super(catalogName, baseNamespace);
   }
 
   @Override

File: flink/v1.17/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogDatabase.java
Patch:
@@ -37,8 +37,8 @@
 
 public class TestFlinkCatalogDatabase extends FlinkCatalogTestBase {
 
-  public TestFlinkCatalogDatabase(String catalogName, Namespace baseNamepace) {
-    super(catalogName, baseNamepace);
+  public TestFlinkCatalogDatabase(String catalogName, Namespace baseNamespace) {
+    super(catalogName, baseNamespace);
   }
 
   @After

File: flink/v1.17/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogTable.java
Patch:
@@ -62,8 +62,8 @@
 
 public class TestFlinkCatalogTable extends FlinkCatalogTestBase {
 
-  public TestFlinkCatalogTable(String catalogName, Namespace baseNamepace) {
-    super(catalogName, baseNamepace);
+  public TestFlinkCatalogTable(String catalogName, Namespace baseNamespace) {
+    super(catalogName, baseNamespace);
   }
 
   @Override

File: core/src/main/java/org/apache/iceberg/CatalogProperties.java
Patch:
@@ -44,6 +44,8 @@ private CatalogProperties() {}
   /** Controls whether the caching catalog will cache table entries using case sensitive keys. */
   public static final String CACHE_CASE_SENSITIVE = "cache.case-sensitive";
 
+  public static final boolean CACHE_CASE_SENSITIVE_DEFAULT = true;
+
   /**
    * Controls the duration for which entries in the catalog are cached.
    *

File: delta-lake/src/main/java/org/apache/iceberg/delta/SnapshotDeltaLakeTable.java
Patch:
@@ -23,8 +23,10 @@
 import org.apache.iceberg.actions.Action;
 import org.apache.iceberg.catalog.Catalog;
 import org.apache.iceberg.catalog.TableIdentifier;
+import org.immutables.value.Value;
 
 /** Snapshot an existing Delta Lake table to Iceberg in place. */
+@Value.Enclosing
 public interface SnapshotDeltaLakeTable
     extends Action<SnapshotDeltaLakeTable, SnapshotDeltaLakeTable.Result> {
 
@@ -81,6 +83,7 @@ public interface SnapshotDeltaLakeTable
   SnapshotDeltaLakeTable deltaLakeConfiguration(Configuration conf);
 
   /** The action result that contains a summary of the execution. */
+  @Value.Immutable
   interface Result {
 
     /** Returns the number of migrated data files. */

File: spark/v3.1/spark/src/main/java/org/apache/iceberg/spark/SparkReadOptions.java
Patch:
@@ -44,7 +44,7 @@ private SparkReadOptions() {}
   // Overrides the table's read.split.open-file-cost
   public static final String FILE_OPEN_COST = "file-open-cost";
 
-  // Overrides the table's read.split.open-file-cost
+  // Overrides table's vectorization enabled properties
   public static final String VECTORIZATION_ENABLED = "vectorization-enabled";
 
   // Overrides the table's read.parquet.vectorization.batch-size

File: spark/v3.2/spark/src/main/java/org/apache/iceberg/spark/SparkReadOptions.java
Patch:
@@ -50,7 +50,7 @@ private SparkReadOptions() {}
   // Overrides the table's read.split.open-file-cost
   public static final String FILE_OPEN_COST = "file-open-cost";
 
-  // Overrides the table's read.split.open-file-cost
+  // Overrides table's vectorization enabled properties
   public static final String VECTORIZATION_ENABLED = "vectorization-enabled";
 
   // Overrides the table's read.parquet.vectorization.batch-size

File: spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/SparkReadOptions.java
Patch:
@@ -56,7 +56,7 @@ private SparkReadOptions() {}
   // Overrides the table's read.split.open-file-cost
   public static final String FILE_OPEN_COST = "file-open-cost";
 
-  // Overrides the table's read.split.open-file-cost
+  // Overrides table's vectorization enabled properties
   public static final String VECTORIZATION_ENABLED = "vectorization-enabled";
 
   // Overrides the table's read.parquet.vectorization.batch-size

File: spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/SparkReadOptions.java
Patch:
@@ -56,7 +56,7 @@ private SparkReadOptions() {}
   // Overrides the table's read.split.open-file-cost
   public static final String FILE_OPEN_COST = "file-open-cost";
 
-  // Overrides the table's read.split.open-file-cost
+  // Overrides table's vectorization enabled properties
   public static final String VECTORIZATION_ENABLED = "vectorization-enabled";
 
   // Overrides the table's read.parquet.vectorization.batch-size

File: arrow/src/main/java/org/apache/iceberg/arrow/vectorized/ArrowReader.java
Patch:
@@ -113,7 +113,8 @@ public class ArrowReader extends CloseableGroup {
           TypeID.BINARY,
           TypeID.DATE,
           TypeID.UUID,
-          TypeID.TIME);
+          TypeID.TIME,
+          TypeID.DECIMAL);
 
   private final Schema schema;
   private final FileIO io;

File: arrow/src/main/java/org/apache/iceberg/arrow/vectorized/ColumnarBatch.java
Patch:
@@ -55,7 +55,7 @@ public class ColumnarBatch implements AutoCloseable {
    */
   public VectorSchemaRoot createVectorSchemaRootFromVectors() {
     return VectorSchemaRoot.of(
-        Arrays.stream(columns).map(ColumnVector::getFieldVector).toArray(FieldVector[]::new));
+        Arrays.stream(columns).map(ColumnVector::getArrowVector).toArray(FieldVector[]::new));
   }
 
   /**

File: arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedArrowReader.java
Patch:
@@ -197,7 +197,7 @@ public VectorHolder read(VectorHolder reuse, int numValsToRead) {
         vec.getValueCount(),
         numValsToRead);
     return new VectorHolder(
-        columnDescriptor, vec, dictEncoded, dictionary, nullabilityHolder, icebergField.type());
+        columnDescriptor, vec, dictEncoded, dictionary, nullabilityHolder, icebergField);
   }
 
   private void allocateFieldVector(boolean dictionaryEncodedVector) {
@@ -513,7 +513,7 @@ public VectorHolder read(VectorHolder reuse, int numValsToRead) {
       rowStart += numValsToRead;
       vec.setValueCount(numValsToRead);
 
-      return new VectorHolder.PositionVectorHolder(vec, MetadataColumns.ROW_POSITION.type(), nulls);
+      return new VectorHolder.PositionVectorHolder(vec, MetadataColumns.ROW_POSITION, nulls);
     }
 
     private static BigIntVector newVector(int valueCount) {

File: spark/v3.1/spark/src/main/java/org/apache/iceberg/spark/SparkReadConf.java
Patch:
@@ -201,7 +201,7 @@ public boolean handleTimestampWithoutZone() {
         .parse();
   }
 
-  public Long streamFromTimestamp() {
+  public long streamFromTimestamp() {
     return confParser
         .longConf()
         .option(SparkReadOptions.STREAM_FROM_TIMESTAMP)

File: spark/v3.2/spark/src/main/java/org/apache/iceberg/spark/SparkReadConf.java
Patch:
@@ -201,7 +201,7 @@ public boolean handleTimestampWithoutZone() {
         .parse();
   }
 
-  public Long streamFromTimestamp() {
+  public long streamFromTimestamp() {
     return confParser
         .longConf()
         .option(SparkReadOptions.STREAM_FROM_TIMESTAMP)

File: spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/SparkReadConf.java
Patch:
@@ -239,7 +239,7 @@ public boolean handleTimestampWithoutZone() {
         .parse();
   }
 
-  public Long streamFromTimestamp() {
+  public long streamFromTimestamp() {
     return confParser
         .longConf()
         .option(SparkReadOptions.STREAM_FROM_TIMESTAMP)
@@ -255,15 +255,15 @@ public Long endTimestamp() {
     return confParser.longConf().option(SparkReadOptions.END_TIMESTAMP).parseOptional();
   }
 
-  public Integer maxFilesPerMicroBatch() {
+  public int maxFilesPerMicroBatch() {
     return confParser
         .intConf()
         .option(SparkReadOptions.STREAMING_MAX_FILES_PER_MICRO_BATCH)
         .defaultValue(Integer.MAX_VALUE)
         .parse();
   }
 
-  public Integer maxRecordsPerMicroBatch() {
+  public int maxRecordsPerMicroBatch() {
     return confParser
         .intConf()
         .option(SparkReadOptions.STREAMING_MAX_ROWS_PER_MICRO_BATCH)

File: spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/SparkReadConf.java
Patch:
@@ -239,7 +239,7 @@ public boolean handleTimestampWithoutZone() {
         .parse();
   }
 
-  public Long streamFromTimestamp() {
+  public long streamFromTimestamp() {
     return confParser
         .longConf()
         .option(SparkReadOptions.STREAM_FROM_TIMESTAMP)

File: core/src/test/java/org/apache/iceberg/view/TestViewRepresentationParser.java
Patch:
@@ -33,8 +33,8 @@ public void testParseUnknownViewRepresentation() {
         ImmutableUnknownViewRepresentation.builder().type("unknown-sql-representation").build());
 
     Assertions.assertThatThrownBy(() -> ViewRepresentationParser.toJson(unknownRepresentation))
-        .isInstanceOf(IllegalArgumentException.class)
-        .hasMessage("Cannot serialize view representation type: unknown-sql-representation");
+        .isInstanceOf(UnsupportedOperationException.class)
+        .hasMessage("Cannot serialize unsupported view representation: unknown-sql-representation");
   }
 
   @Test

File: aws/src/main/java/org/apache/iceberg/aws/dynamodb/DynamoDbCatalog.java
Patch:
@@ -178,7 +178,7 @@ protected String defaultWarehouseLocation(TableIdentifier tableIdentifier) {
     String defaultLocationCol = toPropertyCol(PROPERTY_DEFAULT_LOCATION);
     if (response.item().containsKey(defaultLocationCol)) {
       return String.format(
-          "%s/%s", response.item().get(defaultLocationCol), tableIdentifier.name());
+          "%s/%s", response.item().get(defaultLocationCol).s(), tableIdentifier.name());
     } else {
       return String.format(
           "%s/%s.db/%s", warehousePath, tableIdentifier.namespace(), tableIdentifier.name());

File: api/src/main/java/org/apache/iceberg/metrics/LoggingMetricsReporter.java
Patch:
@@ -18,7 +18,6 @@
  */
 package org.apache.iceberg.metrics;
 
-import org.apache.iceberg.relocated.com.google.common.base.Preconditions;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -36,7 +35,6 @@ public static LoggingMetricsReporter instance() {
 
   @Override
   public void report(MetricsReport report) {
-    Preconditions.checkArgument(null != report, "Invalid metrics report: null");
     LOG.info("Received metrics report: {}", report);
   }
 }

File: flink/v1.17/flink/src/main/java/org/apache/iceberg/flink/source/IcebergSource.java
Patch:
@@ -449,7 +449,8 @@ public IcebergSource<T> build() {
                   context.nameMapping(),
                   context.caseSensitive(),
                   table.io(),
-                  table.encryption());
+                  table.encryption(),
+                  context.filters());
           this.readerFunction = (ReaderFunction<T>) rowDataReaderFunction;
         }
       }

File: flink/v1.17/flink/src/main/java/org/apache/iceberg/flink/source/RowDataRewriter.java
Patch:
@@ -21,6 +21,7 @@
 import static org.apache.iceberg.TableProperties.DEFAULT_NAME_MAPPING;
 
 import java.util.Collection;
+import java.util.Collections;
 import java.util.List;
 import java.util.stream.Collectors;
 import org.apache.flink.api.common.functions.RichMapFunction;
@@ -125,7 +126,8 @@ public RewriteMap(
       this.encryptionManager = encryptionManager;
       this.taskWriterFactory = taskWriterFactory;
       this.rowDataReader =
-          new RowDataFileScanTaskReader(schema, schema, nameMapping, caseSensitive);
+          new RowDataFileScanTaskReader(
+              schema, schema, nameMapping, caseSensitive, Collections.emptyList());
     }
 
     @Override

File: flink/v1.17/flink/src/test/java/org/apache/iceberg/flink/source/TestIcebergSourceBoundedGenericRecord.java
Patch:
@@ -163,7 +163,8 @@ private List<Row> run(
             null,
             false,
             table.io(),
-            table.encryption());
+            table.encryption(),
+            filters);
 
     IcebergSource.Builder<GenericRecord> sourceBuilder =
         IcebergSource.<GenericRecord>builder()

File: flink/v1.17/flink/src/test/java/org/apache/iceberg/flink/source/reader/ReaderUtil.java
Patch:
@@ -20,6 +20,7 @@
 
 import java.io.File;
 import java.io.IOException;
+import java.util.Collections;
 import java.util.List;
 import org.apache.flink.table.data.RowData;
 import org.apache.iceberg.BaseCombinedScanTask;
@@ -83,7 +84,8 @@ public static FileScanTask createFileTask(
 
   public static DataIterator<RowData> createDataIterator(CombinedScanTask combinedTask) {
     return new DataIterator<>(
-        new RowDataFileScanTaskReader(TestFixtures.SCHEMA, TestFixtures.SCHEMA, null, true),
+        new RowDataFileScanTaskReader(
+            TestFixtures.SCHEMA, TestFixtures.SCHEMA, null, true, Collections.emptyList()),
         combinedTask,
         new HadoopFileIO(new org.apache.hadoop.conf.Configuration()),
         new PlaintextEncryptionManager());

File: flink/v1.17/flink/src/test/java/org/apache/iceberg/flink/source/reader/TestIcebergSourceReader.java
Patch:
@@ -19,6 +19,7 @@
 package org.apache.iceberg.flink.source.reader;
 
 import java.util.Arrays;
+import java.util.Collections;
 import java.util.List;
 import org.apache.flink.api.connector.source.SourceReaderContext;
 import org.apache.flink.configuration.Configuration;
@@ -106,7 +107,8 @@ private IcebergSourceReader createReader(
             null,
             true,
             new HadoopFileIO(new org.apache.hadoop.conf.Configuration()),
-            new PlaintextEncryptionManager());
+            new PlaintextEncryptionManager(),
+            Collections.emptyList());
     return new IcebergSourceReader<>(readerMetrics, readerFunction, readerContext);
   }
 }

File: flink/v1.17/flink/src/test/java/org/apache/iceberg/flink/source/reader/TestRowDataReaderFunction.java
Patch:
@@ -18,6 +18,7 @@
  */
 package org.apache.iceberg.flink.source.reader;
 
+import java.util.Collections;
 import java.util.List;
 import java.util.stream.Collectors;
 import org.apache.flink.configuration.Configuration;
@@ -55,7 +56,8 @@ protected ReaderFunction<RowData> readerFunction() {
         null,
         true,
         new HadoopFileIO(new org.apache.hadoop.conf.Configuration()),
-        new PlaintextEncryptionManager());
+        new PlaintextEncryptionManager(),
+        Collections.emptyList());
   }
 
   @Override

File: core/src/main/java/org/apache/iceberg/MetadataUpdate.java
Patch:
@@ -126,8 +126,8 @@ public void applyTo(TableMetadata.Builder metadataBuilder) {
   class SetDefaultPartitionSpec implements MetadataUpdate {
     private final int specId;
 
-    public SetDefaultPartitionSpec(int schemaId) {
-      this.specId = schemaId;
+    public SetDefaultPartitionSpec(int specId) {
+      this.specId = specId;
     }
 
     public int specId() {

File: api/src/test/java/org/apache/iceberg/io/TestCloseableIterable.java
Patch:
@@ -195,10 +195,11 @@ public CloseableIterator<Integer> iterator() {
                   }
                 });
 
+    AtomicInteger consumedCounter = new AtomicInteger(0);
     try (CloseableIterable<Integer> concat = CloseableIterable.concat(transform)) {
-      concat.forEach(c -> c++);
+      concat.forEach(count -> consumedCounter.getAndIncrement());
     }
-    Assertions.assertThat(counter.get()).isEqualTo(items.size());
+    Assertions.assertThat(counter.get()).isEqualTo(items.size()).isEqualTo(consumedCounter.get());
   }
 
   @Test

File: core/src/test/java/org/apache/iceberg/TestMetrics.java
Patch:
@@ -454,7 +454,7 @@ public void testMetricsForTopLevelWithMultipleRowGroup() throws Exception {
 
     for (int i = 0; i < recordCount; i++) {
       Record newRecord = GenericRecord.create(SIMPLE_SCHEMA);
-      newRecord.setField("booleanCol", i == 0 ? false : true);
+      newRecord.setField("booleanCol", i != 0);
       newRecord.setField("intCol", i + 1);
       newRecord.setField("longCol", i == 0 ? null : i + 1L);
       newRecord.setField("floatCol", i + 1.0F);

File: flink/v1.15/flink/src/test/java/org/apache/iceberg/flink/DataGenerators.java
Patch:
@@ -113,6 +113,7 @@ private org.apache.avro.Schema fixupAvroSchemaConvertedFromIcebergSchema(
           schemaConvertedFromIceberg.getFields().stream()
               .map(
                   field -> {
+                    org.apache.avro.Schema.Field updatedField = field;
                     if (field.name().equals("time_field")) {
                       // Iceberg's AvroSchemaUtil uses timestamp-micros with Long value for time
                       // field, while AvroToRowDataConverters#convertToTime() always looks for
@@ -124,10 +125,10 @@ private org.apache.avro.Schema fixupAvroSchemaConvertedFromIcebergSchema(
                           LogicalTypes.timeMillis()
                               .addToSchema(
                                   org.apache.avro.Schema.create(org.apache.avro.Schema.Type.INT));
-                      field = new org.apache.avro.Schema.Field("time_field", fieldSchema);
+                      updatedField = new org.apache.avro.Schema.Field("time_field", fieldSchema);
                     }
 
-                    return new org.apache.avro.Schema.Field(field, field.schema());
+                    return new org.apache.avro.Schema.Field(updatedField, updatedField.schema());
                   })
               .collect(Collectors.toList());
       return org.apache.avro.Schema.createRecord(

File: flink/v1.16/flink/src/test/java/org/apache/iceberg/flink/DataGenerators.java
Patch:
@@ -113,6 +113,7 @@ private org.apache.avro.Schema fixupAvroSchemaConvertedFromIcebergSchema(
           schemaConvertedFromIceberg.getFields().stream()
               .map(
                   field -> {
+                    org.apache.avro.Schema.Field updatedField = field;
                     if (field.name().equals("time_field")) {
                       // Iceberg's AvroSchemaUtil uses timestamp-micros with Long value for time
                       // field, while AvroToRowDataConverters#convertToTime() always looks for
@@ -124,10 +125,10 @@ private org.apache.avro.Schema fixupAvroSchemaConvertedFromIcebergSchema(
                           LogicalTypes.timeMillis()
                               .addToSchema(
                                   org.apache.avro.Schema.create(org.apache.avro.Schema.Type.INT));
-                      field = new org.apache.avro.Schema.Field("time_field", fieldSchema);
+                      updatedField = new org.apache.avro.Schema.Field("time_field", fieldSchema);
                     }
 
-                    return new org.apache.avro.Schema.Field(field, field.schema());
+                    return new org.apache.avro.Schema.Field(updatedField, updatedField.schema());
                   })
               .collect(Collectors.toList());
       return org.apache.avro.Schema.createRecord(

File: flink/v1.17/flink/src/test/java/org/apache/iceberg/flink/DataGenerators.java
Patch:
@@ -113,6 +113,7 @@ private org.apache.avro.Schema fixupAvroSchemaConvertedFromIcebergSchema(
           schemaConvertedFromIceberg.getFields().stream()
               .map(
                   field -> {
+                    org.apache.avro.Schema.Field updatedField = field;
                     if (field.name().equals("time_field")) {
                       // Iceberg's AvroSchemaUtil uses timestamp-micros with Long value for time
                       // field, while AvroToRowDataConverters#convertToTime() always looks for
@@ -124,10 +125,10 @@ private org.apache.avro.Schema fixupAvroSchemaConvertedFromIcebergSchema(
                           LogicalTypes.timeMillis()
                               .addToSchema(
                                   org.apache.avro.Schema.create(org.apache.avro.Schema.Type.INT));
-                      field = new org.apache.avro.Schema.Field("time_field", fieldSchema);
+                      updatedField = new org.apache.avro.Schema.Field("time_field", fieldSchema);
                     }
 
-                    return new org.apache.avro.Schema.Field(field, field.schema());
+                    return new org.apache.avro.Schema.Field(updatedField, updatedField.schema());
                   })
               .collect(Collectors.toList());
       return org.apache.avro.Schema.createRecord(

File: parquet/src/test/java/org/apache/iceberg/parquet/TestBloomRowGroupFilter.java
Patch:
@@ -248,7 +248,7 @@ public void createInputFile() throws IOException {
         structNotNull.put("_int_field", INT_MIN_VALUE + i);
         builder.set("_struct_not_null", structNotNull); // struct with int
         builder.set("_no_stats", TOO_LONG_FOR_STATS); // value longer than 4k will produce no stats
-        builder.set("_boolean", (i % 2 == 0) ? true : false);
+        builder.set("_boolean", i % 2 == 0);
         builder.set("_time", instant.plusSeconds(i * 86400).toEpochMilli());
         builder.set("_date", instant.plusSeconds(i * 86400).getEpochSecond());
         builder.set("_timestamp", instant.plusSeconds(i * 86400).toEpochMilli());

File: spark/v3.2/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReaderWithBloomFilter.java
Patch:
@@ -140,7 +140,7 @@ public void writeTestDataFile() throws IOException {
                   "id_string",
                   BINARY_PREFIX + (INT_MIN_VALUE + i),
                   "id_boolean",
-                  (i % 2 == 0) ? true : false,
+                  i % 2 == 0,
                   "id_date",
                   LocalDate.parse("2021-09-05"),
                   "id_int_decimal",

File: spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReaderWithBloomFilter.java
Patch:
@@ -140,7 +140,7 @@ public void writeTestDataFile() throws IOException {
                   "id_string",
                   BINARY_PREFIX + (INT_MIN_VALUE + i),
                   "id_boolean",
-                  (i % 2 == 0) ? true : false,
+                  i % 2 == 0,
                   "id_date",
                   LocalDate.parse("2021-09-05"),
                   "id_int_decimal",

File: core/src/main/java/org/apache/iceberg/SnapshotProducer.java
Patch:
@@ -137,7 +137,7 @@ protected ThisT reportWith(MetricsReporter newReporter) {
   }
 
   /**
-   * * A setter for the target branch on which snapshot producer operation should be performed
+   * A setter for the target branch on which snapshot producer operation should be performed
    *
    * @param branch to set as target branch
    */
@@ -146,7 +146,8 @@ protected void targetBranch(String branch) {
     boolean refExists = base.ref(branch) != null;
     Preconditions.checkArgument(
         !refExists || base.ref(branch).isBranch(),
-        "%s is a tag, not a branch. Tags cannot be targets for producing snapshots");
+        "%s is a tag, not a branch. Tags cannot be targets for producing snapshots",
+        branch);
     this.targetBranch = branch;
   }
 

File: nessie/src/test/java/org/apache/iceberg/nessie/TestMultipleClients.java
Patch:
@@ -98,7 +98,7 @@ public void testLoadNamespaceMetadata() {
 
   @Test
   public void testListTables() {
-    catalog.createTable(TableIdentifier.parse("foo.tbl1"), schema);
+    createTable(TableIdentifier.parse("foo.tbl1"), schema);
     Assertions.assertThat(catalog.listTables(Namespace.of("foo")))
         .containsExactlyInAnyOrder(TableIdentifier.parse("foo.tbl1"));
 
@@ -116,7 +116,7 @@ public void testListTables() {
   @Test
   public void testCommits() {
     TableIdentifier identifier = TableIdentifier.parse("foo.tbl1");
-    catalog.createTable(identifier, schema);
+    createTable(identifier, schema);
     Table tableFromCatalog = catalog.loadTable(identifier);
     tableFromCatalog.updateSchema().addColumn("x1", Types.LongType.get()).commit();
 
@@ -133,7 +133,7 @@ public void testCommits() {
   @Test
   public void testConcurrentCommitsWithRefresh() {
     TableIdentifier identifier = TableIdentifier.parse("foo.tbl1");
-    catalog.createTable(identifier, schema);
+    createTable(identifier, schema);
 
     String hashBefore = catalog.currentHash();
 

File: core/src/main/java/org/apache/iceberg/inmemory/InMemoryInputFile.java
Patch:
@@ -63,7 +63,7 @@ public boolean exists() {
 
   private static class InMemorySeekableInputStream extends SeekableInputStream {
 
-    private final int length;
+    private final long length;
     private final ByteArrayInputStream delegate;
     private boolean closed = false;
 

File: core/src/test/java/org/apache/iceberg/hadoop/HadoopFileIOTest.java
Patch:
@@ -27,6 +27,7 @@
 import java.util.List;
 import java.util.Random;
 import java.util.UUID;
+import java.util.Vector;
 import java.util.stream.Collectors;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
@@ -48,7 +49,7 @@ public class HadoopFileIOTest {
   private FileSystem fs;
   private HadoopFileIO hadoopFileIO;
 
-  @TempDir static File tempDir;
+  @TempDir private File tempDir;
 
   @BeforeEach
   public void before() throws Exception {
@@ -165,7 +166,7 @@ public void testHadoopFileIOJavaSerialization() throws IOException, ClassNotFoun
   }
 
   private List<Path> createRandomFiles(Path parent, int count) {
-    List<Path> paths = Lists.newArrayList();
+    Vector<Path> paths = new Vector<>();
     random
         .ints(count)
         .parallel()

File: core/src/main/java/org/apache/iceberg/rest/RESTSessionCatalog.java
Patch:
@@ -327,6 +327,7 @@ public Table loadTable(SessionContext context, TableIdentifier identifier) {
       tableMetadata =
           TableMetadata.buildFrom(response.tableMetadata())
               .withMetadataLocation(response.metadataLocation())
+              .setPreviousFileLocation(null)
               .setSnapshotsSupplier(
                   () ->
                       loadInternal(context, identifier, SnapshotMode.ALL)

File: spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/source/SparkTable.java
Patch:
@@ -163,7 +163,7 @@ public Table table() {
 
   @Override
   public String name() {
-    return String.format("Iceberg %s", icebergTable.name());
+    return icebergTable.toString();
   }
 
   public Long snapshotId() {

File: flink/v1.16/flink/src/main/java/org/apache/iceberg/flink/CatalogLoader.java
Patch:
@@ -33,7 +33,7 @@
 import org.apache.iceberg.rest.RESTCatalog;
 
 /** Serializable loader to load an Iceberg {@link Catalog}. */
-public interface CatalogLoader extends Serializable {
+public interface CatalogLoader extends Serializable, Cloneable {
 
   /**
    * Create a new catalog with the provided properties. NOTICE: for flink, we may initialize the

File: api/src/main/java/org/apache/iceberg/actions/RewriteDataFiles.java
Patch:
@@ -76,7 +76,7 @@ public interface RewriteDataFiles
    */
   String MAX_CONCURRENT_FILE_GROUP_REWRITES = "max-concurrent-file-group-rewrites";
 
-  int MAX_CONCURRENT_FILE_GROUP_REWRITES_DEFAULT = 1;
+  int MAX_CONCURRENT_FILE_GROUP_REWRITES_DEFAULT = 5;
 
   /**
    * The output file size that this rewrite strategy will attempt to generate when rewriting files.

File: spark/v3.1/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java
Patch:
@@ -18,6 +18,7 @@
  */
 package org.apache.iceberg.spark.actions;
 
+import static org.apache.iceberg.TableProperties.COMMIT_NUM_RETRIES;
 import static org.apache.iceberg.types.Types.NestedField.optional;
 import static org.assertj.core.api.Assertions.assertThat;
 import static org.mockito.ArgumentMatchers.any;
@@ -495,6 +496,8 @@ public void testPartialProgressEnabled() {
     Table table = createTable(20);
     int fileSize = averageFileSize(table);
 
+    table.updateProperties().set(COMMIT_NUM_RETRIES, "10").commit();
+
     List<Object[]> originalData = currentData();
     long dataSizeBefore = testDataSize(table);
 

File: spark/v3.2/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java
Patch:
@@ -18,6 +18,7 @@
  */
 package org.apache.iceberg.spark.actions;
 
+import static org.apache.iceberg.TableProperties.COMMIT_NUM_RETRIES;
 import static org.apache.iceberg.types.Types.NestedField.optional;
 import static org.apache.iceberg.types.Types.NestedField.required;
 import static org.apache.spark.sql.functions.current_date;
@@ -534,6 +535,8 @@ public void testPartialProgressEnabled() {
     Table table = createTable(20);
     int fileSize = averageFileSize(table);
 
+    table.updateProperties().set(COMMIT_NUM_RETRIES, "10").commit();
+
     List<Object[]> originalData = currentData();
     long dataSizeBefore = testDataSize(table);
 

File: spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java
Patch:
@@ -18,6 +18,7 @@
  */
 package org.apache.iceberg.spark.actions;
 
+import static org.apache.iceberg.TableProperties.COMMIT_NUM_RETRIES;
 import static org.apache.iceberg.types.Types.NestedField.optional;
 import static org.apache.iceberg.types.Types.NestedField.required;
 import static org.apache.spark.sql.functions.current_date;
@@ -534,6 +535,8 @@ public void testPartialProgressEnabled() {
     Table table = createTable(20);
     int fileSize = averageFileSize(table);
 
+    table.updateProperties().set(COMMIT_NUM_RETRIES, "10").commit();
+
     List<Object[]> originalData = currentData();
     long dataSizeBefore = testDataSize(table);
 

File: core/src/main/java/org/apache/iceberg/rest/requests/ReportMetricsRequestParser.java
Patch:
@@ -94,6 +94,6 @@ public static ReportMetricsRequest fromJson(JsonNode json) {
           .build();
     }
 
-    throw new IllegalArgumentException(String.format("Cannot build metrics request from %s", json));
+    return ReportMetricsRequest.unknown();
   }
 }

File: aws/src/main/java/org/apache/iceberg/aws/AwsProperties.java
Patch:
@@ -372,7 +372,7 @@ public class AwsProperties implements Serializable {
    */
   public static final String HTTP_CLIENT_TYPE_APACHE = "apache";
 
-  public static final String HTTP_CLIENT_TYPE_DEFAULT = HTTP_CLIENT_TYPE_URLCONNECTION;
+  public static final String HTTP_CLIENT_TYPE_DEFAULT = HTTP_CLIENT_TYPE_APACHE;
 
   /**
    * Used to configure the connection timeout in milliseconds for {@link

File: core/src/main/java/org/apache/iceberg/BaseMetadataTable.java
Patch:
@@ -108,7 +108,7 @@ protected BaseTable table() {
     return table;
   }
 
-  /** @deprecated will be removed in 2.0.0; do not use metadata table TableOperations */
+  /** @deprecated will be removed in 1.4.0; do not use metadata table TableOperations */
   @Override
   @Deprecated
   public TableOperations operations() {

File: spark/v3.2/spark/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java
Patch:
@@ -148,9 +148,8 @@ protected SparkTable loadSparkTable(Identifier ident) {
     }
   }
 
-  protected Dataset<Row> loadDataSetFromTable(Identifier tableIdent, Map<String, String> options) {
+  protected Dataset<Row> loadRows(Identifier tableIdent, Map<String, String> options) {
     String tableName = Spark3Util.quotedFullIdentifier(tableCatalog().name(), tableIdent);
-    // no need to validate the read options here since the reader will validate them
     return spark().read().options(options).table(tableName);
   }
 

File: spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java
Patch:
@@ -148,9 +148,8 @@ protected SparkTable loadSparkTable(Identifier ident) {
     }
   }
 
-  protected Dataset<Row> loadDataSetFromTable(Identifier tableIdent, Map<String, String> options) {
+  protected Dataset<Row> loadRows(Identifier tableIdent, Map<String, String> options) {
     String tableName = Spark3Util.quotedFullIdentifier(tableCatalog().name(), tableIdent);
-    // no need to validate the read options here since the reader will validate them
     return spark().read().options(options).table(tableName);
   }
 

File: core/src/main/java/org/apache/iceberg/rest/auth/OAuth2Util.java
Patch:
@@ -566,7 +566,7 @@ public static AuthSession fromAccessToken(
         expiresAtMillis = defaultExpiresAtMillis;
       }
 
-      if (null != expiresAtMillis) {
+      if (null != executor && null != expiresAtMillis) {
         scheduleTokenRefresh(client, executor, session, expiresAtMillis);
       }
 
@@ -614,7 +614,7 @@ private static AuthSession fromTokenResponse(
         expiresAtMillis = startTimeMillis + TimeUnit.SECONDS.toMillis(response.expiresInSeconds());
       }
 
-      if (null != expiresAtMillis) {
+      if (null != executor && null != expiresAtMillis) {
         scheduleTokenRefresh(client, executor, session, expiresAtMillis);
       }
 

File: api/src/main/java/org/apache/iceberg/expressions/AggregateEvaluator.java
Patch:
@@ -108,6 +108,7 @@ private ArrayStructLike(Object[] values) {
       this.values = values;
     }
 
+    @Override
     public int size() {
       return values.length;
     }

File: api/src/main/java/org/apache/iceberg/expressions/ValueAggregate.java
Patch:
@@ -33,6 +33,7 @@ public T eval(StructLike struct) {
     return term().eval(struct);
   }
 
+  @Override
   public T eval(DataFile file) {
     valueStruct.setValue(evaluateRef(file));
     return term().eval(valueStruct);

File: api/src/main/java/org/apache/iceberg/view/SQLViewRepresentation.java
Patch:
@@ -18,8 +18,8 @@
  */
 package org.apache.iceberg.view;
 
-import edu.umd.cs.findbugs.annotations.Nullable;
 import java.util.List;
+import javax.annotation.Nullable;
 import org.apache.iceberg.catalog.Namespace;
 import org.immutables.value.Value;
 

File: core/src/main/java/org/apache/iceberg/metrics/CommitMetricsResult.java
Patch:
@@ -18,8 +18,8 @@
  */
 package org.apache.iceberg.metrics;
 
-import edu.umd.cs.findbugs.annotations.Nullable;
 import java.util.Map;
+import javax.annotation.Nullable;
 import org.apache.iceberg.SnapshotSummary;
 import org.apache.iceberg.metrics.MetricsContext.Unit;
 import org.apache.iceberg.relocated.com.google.common.base.Preconditions;

File: core/src/main/java/org/apache/iceberg/metrics/ScanMetricsResult.java
Patch:
@@ -18,7 +18,7 @@
  */
 package org.apache.iceberg.metrics;
 
-import edu.umd.cs.findbugs.annotations.Nullable;
+import javax.annotation.Nullable;
 import org.apache.iceberg.relocated.com.google.common.base.Preconditions;
 import org.immutables.value.Value;
 

File: flink/v1.14/flink/src/main/java/org/apache/iceberg/flink/source/IcebergSource.java
Patch:
@@ -224,7 +224,7 @@ public Builder<T> tableLoader(TableLoader loader) {
       return this;
     }
 
-    public Builder table(Table newTable) {
+    public Builder<T> table(Table newTable) {
       this.table = newTable;
       return this;
     }

File: flink/v1.15/flink/src/main/java/org/apache/iceberg/flink/source/IcebergSource.java
Patch:
@@ -224,7 +224,7 @@ public Builder<T> tableLoader(TableLoader loader) {
       return this;
     }
 
-    public Builder table(Table newTable) {
+    public Builder<T> table(Table newTable) {
       this.table = newTable;
       return this;
     }

File: flink/v1.16/flink/src/main/java/org/apache/iceberg/flink/source/IcebergSource.java
Patch:
@@ -224,7 +224,7 @@ public Builder<T> tableLoader(TableLoader loader) {
       return this;
     }
 
-    public Builder table(Table newTable) {
+    public Builder<T> table(Table newTable) {
       this.table = newTable;
       return this;
     }

File: api/src/main/java/org/apache/iceberg/types/Types.java
Patch:
@@ -53,8 +53,9 @@ private Types() {}
           .put(BinaryType.get().toString(), BinaryType.get())
           .buildOrThrow();
 
-  private static final Pattern FIXED = Pattern.compile("fixed\\[(\\d+)\\]");
-  private static final Pattern DECIMAL = Pattern.compile("decimal\\((\\d+),\\s+(\\d+)\\)");
+  private static final Pattern FIXED = Pattern.compile("fixed\\[\\s*(\\d+)\\s*\\]");
+  private static final Pattern DECIMAL =
+      Pattern.compile("decimal\\(\\s*(\\d+)\\s*,\\s*(\\d+)\\s*\\)");
 
   public static PrimitiveType fromPrimitiveString(String typeString) {
     String lowerTypeString = typeString.toLowerCase(Locale.ROOT);

File: aws/src/main/java/org/apache/iceberg/aws/AwsProperties.java
Patch:
@@ -129,7 +129,7 @@ public class AwsProperties implements Serializable {
    */
   public static final String GLUE_CATALOG_SKIP_ARCHIVE = "glue.skip-archive";
 
-  public static final boolean GLUE_CATALOG_SKIP_ARCHIVE_DEFAULT = false;
+  public static final boolean GLUE_CATALOG_SKIP_ARCHIVE_DEFAULT = true;
 
   /**
    * If Glue should skip name validations It is recommended to stick to Glue best practice in

File: spark/v3.2/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceTablesBase.java
Patch:
@@ -65,6 +65,7 @@
 import org.apache.iceberg.spark.SparkSchemaUtil;
 import org.apache.iceberg.spark.SparkTableUtil;
 import org.apache.iceberg.spark.SparkTestBase;
+import org.apache.iceberg.spark.SparkWriteOptions;
 import org.apache.iceberg.spark.actions.SparkActions;
 import org.apache.iceberg.spark.data.TestHelpers;
 import org.apache.iceberg.types.Types;
@@ -1009,6 +1010,7 @@ public void testManifestsTable() {
         .write()
         .format("iceberg")
         .mode("append")
+        .option(SparkWriteOptions.DISTRIBUTION_MODE, TableProperties.WRITE_DISTRIBUTION_MODE_NONE)
         .save(loadLocation(tableIdentifier));
 
     table.updateProperties().set(TableProperties.FORMAT_VERSION, "2").commit();

File: spark/v3.2/spark/src/test/java/org/apache/iceberg/spark/source/TestPartitionValues.java
Patch:
@@ -491,6 +491,7 @@ public void testReadPartitionColumn() throws Exception {
             .option(SparkReadOptions.VECTORIZATION_ENABLED, String.valueOf(vectorized))
             .load(baseLocation)
             .select("struct.innerName")
+            .orderBy("struct.innerName")
             .as(Encoders.STRING())
             .collectAsList();
 

File: api/src/main/java/org/apache/iceberg/view/ViewRepresentation.java
Patch:
@@ -18,9 +18,6 @@
  */
 package org.apache.iceberg.view;
 
-import org.immutables.value.Value;
-
-@Value.Immutable
 public interface ViewRepresentation {
 
   class Type {

File: spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceTablesBase.java
Patch:
@@ -65,6 +65,7 @@
 import org.apache.iceberg.spark.SparkSchemaUtil;
 import org.apache.iceberg.spark.SparkTableUtil;
 import org.apache.iceberg.spark.SparkTestBase;
+import org.apache.iceberg.spark.SparkWriteOptions;
 import org.apache.iceberg.spark.actions.SparkActions;
 import org.apache.iceberg.spark.data.TestHelpers;
 import org.apache.iceberg.types.Types;
@@ -1010,6 +1011,7 @@ public void testManifestsTable() {
         .write()
         .format("iceberg")
         .mode("append")
+        .option(SparkWriteOptions.DISTRIBUTION_MODE, TableProperties.WRITE_DISTRIBUTION_MODE_NONE)
         .save(loadLocation(tableIdentifier));
 
     table.updateProperties().set(TableProperties.FORMAT_VERSION, "2").commit();

File: spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/source/TestPartitionValues.java
Patch:
@@ -491,6 +491,7 @@ public void testReadPartitionColumn() throws Exception {
             .option(SparkReadOptions.VECTORIZATION_ENABLED, String.valueOf(vectorized))
             .load(baseLocation)
             .select("struct.innerName")
+            .orderBy("struct.innerName")
             .as(Encoders.STRING())
             .collectAsList();
 

File: parquet/src/main/java/org/apache/iceberg/data/parquet/BaseParquetReaders.java
Patch:
@@ -42,8 +42,8 @@
 import org.apache.iceberg.relocated.com.google.common.collect.Maps;
 import org.apache.iceberg.types.Types;
 import org.apache.parquet.column.ColumnDescriptor;
-import org.apache.parquet.schema.DecimalMetadata;
 import org.apache.parquet.schema.GroupType;
+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;
 import org.apache.parquet.schema.MessageType;
 import org.apache.parquet.schema.PrimitiveType;
 import org.apache.parquet.schema.Type;
@@ -276,7 +276,8 @@ public ParquetValueReader<?> primitive(
           case TIME_MILLIS:
             return new TimeMillisReader(desc);
           case DECIMAL:
-            DecimalMetadata decimal = primitive.getDecimalMetadata();
+            DecimalLogicalTypeAnnotation decimal =
+                (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();
             switch (primitive.getPrimitiveTypeName()) {
               case BINARY:
               case FIXED_LEN_BYTE_ARRAY:

File: parquet/src/main/java/org/apache/iceberg/parquet/ParquetAvroValueReaders.java
Patch:
@@ -45,8 +45,8 @@
 import org.apache.iceberg.types.Types;
 import org.apache.iceberg.util.UUIDUtil;
 import org.apache.parquet.column.ColumnDescriptor;
-import org.apache.parquet.schema.DecimalMetadata;
 import org.apache.parquet.schema.GroupType;
+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;
 import org.apache.parquet.schema.MessageType;
 import org.apache.parquet.schema.PrimitiveType;
 import org.apache.parquet.schema.Type;
@@ -184,7 +184,8 @@ public ParquetValueReader<?> primitive(
           case TIMESTAMP_MILLIS:
             return new TimestampMillisReader(desc);
           case DECIMAL:
-            DecimalMetadata decimal = primitive.getDecimalMetadata();
+            DecimalLogicalTypeAnnotation decimal =
+                (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();
             switch (primitive.getPrimitiveTypeName()) {
               case BINARY:
               case FIXED_LEN_BYTE_ARRAY:

File: parquet/src/main/java/org/apache/iceberg/parquet/ParquetAvroWriter.java
Patch:
@@ -26,8 +26,8 @@
 import org.apache.iceberg.relocated.com.google.common.collect.Lists;
 import org.apache.parquet.column.ColumnDescriptor;
 import org.apache.parquet.io.api.Binary;
-import org.apache.parquet.schema.DecimalMetadata;
 import org.apache.parquet.schema.GroupType;
+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;
 import org.apache.parquet.schema.MessageType;
 import org.apache.parquet.schema.PrimitiveType;
 import org.apache.parquet.schema.Type;
@@ -123,7 +123,8 @@ public ParquetValueWriter<?> primitive(PrimitiveType primitive) {
           case TIMESTAMP_MICROS:
             return ParquetValueWriters.longs(desc);
           case DECIMAL:
-            DecimalMetadata decimal = primitive.getDecimalMetadata();
+            DecimalLogicalTypeAnnotation decimal =
+                (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();
             switch (primitive.getPrimitiveTypeName()) {
               case INT32:
                 return ParquetValueWriters.decimalAsInteger(

File: parquet/src/main/java/org/apache/iceberg/parquet/ParquetBloomRowGroupFilter.java
Patch:
@@ -44,7 +44,7 @@
 import org.apache.parquet.hadoop.metadata.BlockMetaData;
 import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;
 import org.apache.parquet.io.api.Binary;
-import org.apache.parquet.schema.DecimalMetadata;
+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;
 import org.apache.parquet.schema.MessageType;
 import org.apache.parquet.schema.PrimitiveType;
 
@@ -314,7 +314,8 @@ private <T> boolean shouldRead(
               hashValue = bloom.hash(Binary.fromConstantByteBuffer((ByteBuffer) value));
               return bloom.findHash(hashValue);
             case DECIMAL:
-              DecimalMetadata metadata = primitiveType.getDecimalMetadata();
+              DecimalLogicalTypeAnnotation metadata =
+                  (DecimalLogicalTypeAnnotation) primitiveType.getLogicalTypeAnnotation();
               int scale = metadata.getScale();
               int precision = metadata.getPrecision();
               byte[] requiredBytes = new byte[TypeUtil.decimalRequiredBytes(precision)];

File: pig/src/main/java/org/apache/iceberg/pig/PigParquetReader.java
Patch:
@@ -48,8 +48,8 @@
 import org.apache.iceberg.types.Type.TypeID;
 import org.apache.iceberg.types.Types;
 import org.apache.parquet.column.ColumnDescriptor;
-import org.apache.parquet.schema.DecimalMetadata;
 import org.apache.parquet.schema.GroupType;
+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;
 import org.apache.parquet.schema.MessageType;
 import org.apache.parquet.schema.PrimitiveType;
 import org.apache.parquet.schema.Type;
@@ -247,7 +247,8 @@ public ParquetValueReader<?> primitive(
           case TIMESTAMP_MICROS:
             return new TimestampMicrosReader(desc);
           case DECIMAL:
-            DecimalMetadata decimal = primitive.getDecimalMetadata();
+            DecimalLogicalTypeAnnotation decimal =
+                (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();
             switch (primitive.getPrimitiveTypeName()) {
               case BINARY:
               case FIXED_LEN_BYTE_ARRAY:

File: spark/v3.1/spark/src/main/java/org/apache/iceberg/spark/data/SparkParquetReaders.java
Patch:
@@ -48,8 +48,8 @@
 import org.apache.iceberg.types.Types;
 import org.apache.parquet.column.ColumnDescriptor;
 import org.apache.parquet.io.api.Binary;
-import org.apache.parquet.schema.DecimalMetadata;
 import org.apache.parquet.schema.GroupType;
+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;
 import org.apache.parquet.schema.MessageType;
 import org.apache.parquet.schema.PrimitiveType;
 import org.apache.parquet.schema.Type;
@@ -258,7 +258,8 @@ public ParquetValueReader<?> primitive(
           case TIMESTAMP_MILLIS:
             return new TimestampMillisReader(desc);
           case DECIMAL:
-            DecimalMetadata decimal = primitive.getDecimalMetadata();
+            DecimalLogicalTypeAnnotation decimal =
+                (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();
             switch (primitive.getPrimitiveTypeName()) {
               case BINARY:
               case FIXED_LEN_BYTE_ARRAY:

File: spark/v3.1/spark/src/main/java/org/apache/iceberg/spark/data/SparkParquetWriters.java
Patch:
@@ -34,8 +34,8 @@
 import org.apache.iceberg.util.DecimalUtil;
 import org.apache.parquet.column.ColumnDescriptor;
 import org.apache.parquet.io.api.Binary;
-import org.apache.parquet.schema.DecimalMetadata;
 import org.apache.parquet.schema.GroupType;
+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;
 import org.apache.parquet.schema.MessageType;
 import org.apache.parquet.schema.PrimitiveType;
 import org.apache.parquet.schema.Type;
@@ -151,7 +151,8 @@ public ParquetValueWriter<?> primitive(DataType sType, PrimitiveType primitive)
           case TIMESTAMP_MICROS:
             return ParquetValueWriters.longs(desc);
           case DECIMAL:
-            DecimalMetadata decimal = primitive.getDecimalMetadata();
+            DecimalLogicalTypeAnnotation decimal =
+                (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();
             switch (primitive.getPrimitiveTypeName()) {
               case INT32:
                 return decimalAsInteger(desc, decimal.getPrecision(), decimal.getScale());

File: spark/v3.2/spark/src/main/java/org/apache/iceberg/spark/data/SparkParquetReaders.java
Patch:
@@ -48,8 +48,8 @@
 import org.apache.iceberg.types.Types;
 import org.apache.parquet.column.ColumnDescriptor;
 import org.apache.parquet.io.api.Binary;
-import org.apache.parquet.schema.DecimalMetadata;
 import org.apache.parquet.schema.GroupType;
+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;
 import org.apache.parquet.schema.MessageType;
 import org.apache.parquet.schema.PrimitiveType;
 import org.apache.parquet.schema.Type;
@@ -257,7 +257,8 @@ public ParquetValueReader<?> primitive(
           case TIMESTAMP_MILLIS:
             return new TimestampMillisReader(desc);
           case DECIMAL:
-            DecimalMetadata decimal = primitive.getDecimalMetadata();
+            DecimalLogicalTypeAnnotation decimal =
+                (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();
             switch (primitive.getPrimitiveTypeName()) {
               case BINARY:
               case FIXED_LEN_BYTE_ARRAY:

File: spark/v3.2/spark/src/main/java/org/apache/iceberg/spark/data/SparkParquetWriters.java
Patch:
@@ -34,8 +34,8 @@
 import org.apache.iceberg.util.DecimalUtil;
 import org.apache.parquet.column.ColumnDescriptor;
 import org.apache.parquet.io.api.Binary;
-import org.apache.parquet.schema.DecimalMetadata;
 import org.apache.parquet.schema.GroupType;
+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;
 import org.apache.parquet.schema.MessageType;
 import org.apache.parquet.schema.PrimitiveType;
 import org.apache.parquet.schema.Type;
@@ -151,7 +151,8 @@ public ParquetValueWriter<?> primitive(DataType sType, PrimitiveType primitive)
           case TIMESTAMP_MICROS:
             return ParquetValueWriters.longs(desc);
           case DECIMAL:
-            DecimalMetadata decimal = primitive.getDecimalMetadata();
+            DecimalLogicalTypeAnnotation decimal =
+                (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();
             switch (primitive.getPrimitiveTypeName()) {
               case INT32:
                 return decimalAsInteger(desc, decimal.getPrecision(), decimal.getScale());

File: spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/data/SparkParquetReaders.java
Patch:
@@ -48,8 +48,8 @@
 import org.apache.iceberg.types.Types;
 import org.apache.parquet.column.ColumnDescriptor;
 import org.apache.parquet.io.api.Binary;
-import org.apache.parquet.schema.DecimalMetadata;
 import org.apache.parquet.schema.GroupType;
+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;
 import org.apache.parquet.schema.MessageType;
 import org.apache.parquet.schema.PrimitiveType;
 import org.apache.parquet.schema.Type;
@@ -257,7 +257,8 @@ public ParquetValueReader<?> primitive(
           case TIMESTAMP_MILLIS:
             return new TimestampMillisReader(desc);
           case DECIMAL:
-            DecimalMetadata decimal = primitive.getDecimalMetadata();
+            DecimalLogicalTypeAnnotation decimal =
+                (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();
             switch (primitive.getPrimitiveTypeName()) {
               case BINARY:
               case FIXED_LEN_BYTE_ARRAY:

File: spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/data/SparkParquetWriters.java
Patch:
@@ -34,8 +34,8 @@
 import org.apache.iceberg.util.DecimalUtil;
 import org.apache.parquet.column.ColumnDescriptor;
 import org.apache.parquet.io.api.Binary;
-import org.apache.parquet.schema.DecimalMetadata;
 import org.apache.parquet.schema.GroupType;
+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;
 import org.apache.parquet.schema.MessageType;
 import org.apache.parquet.schema.PrimitiveType;
 import org.apache.parquet.schema.Type;
@@ -151,7 +151,8 @@ public ParquetValueWriter<?> primitive(DataType sType, PrimitiveType primitive)
           case TIMESTAMP_MICROS:
             return ParquetValueWriters.longs(desc);
           case DECIMAL:
-            DecimalMetadata decimal = primitive.getDecimalMetadata();
+            DecimalLogicalTypeAnnotation decimal =
+                (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();
             switch (primitive.getPrimitiveTypeName()) {
               case INT32:
                 return decimalAsInteger(desc, decimal.getPrecision(), decimal.getScale());

File: api/src/main/java/org/apache/iceberg/view/SQLViewRepresentation.java
Patch:
@@ -18,8 +18,8 @@
  */
 package org.apache.iceberg.view;
 
+import edu.umd.cs.findbugs.annotations.Nullable;
 import java.util.List;
-import javax.annotation.Nullable;
 import org.apache.iceberg.catalog.Namespace;
 import org.immutables.value.Value;
 

File: spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/source/SparkBatchQueryScan.java
Patch:
@@ -221,9 +221,8 @@ public boolean equals(Object o) {
 
     SparkBatchQueryScan that = (SparkBatchQueryScan) o;
     return table().name().equals(that.table().name())
-        && readSchema().equals(that.readSchema())
-        && // compare Spark schemas to ignore field ids
-        filterExpressions().toString().equals(that.filterExpressions().toString())
+        && readSchema().equals(that.readSchema()) // compare Spark schemas to ignore field ids
+        && filterExpressions().toString().equals(that.filterExpressions().toString())
         && runtimeFilterExpressions.toString().equals(that.runtimeFilterExpressions.toString())
         && Objects.equals(snapshotId, that.snapshotId)
         && Objects.equals(startSnapshotId, that.startSnapshotId)

File: spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/source/SparkCopyOnWriteScan.java
Patch:
@@ -155,9 +155,8 @@ public boolean equals(Object o) {
 
     SparkCopyOnWriteScan that = (SparkCopyOnWriteScan) o;
     return table().name().equals(that.table().name())
-        && readSchema().equals(that.readSchema())
-        && // compare Spark schemas to ignore field ids
-        filterExpressions().toString().equals(that.filterExpressions().toString())
+        && readSchema().equals(that.readSchema()) // compare Spark schemas to ignore field ids
+        && filterExpressions().toString().equals(that.filterExpressions().toString())
         && Objects.equals(snapshotId(), that.snapshotId())
         && Objects.equals(filteredLocations, that.filteredLocations);
   }

File: spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/source/SparkFileWriterFactory.java
Patch:
@@ -159,8 +159,7 @@ private StructType equalityDeleteSparkType() {
 
   private StructType positionDeleteSparkType() {
     if (positionDeleteSparkType == null) {
-      // wrap the optional row schema into the position delete schema that contains path and
-      // position
+      // wrap the optional row schema into the position delete schema containing path and position
       Schema positionDeleteSchema = DeleteSchemaUtil.posDeleteSchema(positionDeleteRowSchema());
       this.positionDeleteSparkType = SparkSchemaUtil.convert(positionDeleteSchema);
     }

File: spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/source/SparkMicroBatchStream.java
Patch:
@@ -118,9 +118,8 @@ public Offset latestOffset() {
     Snapshot latestSnapshot = table.currentSnapshot();
     long addedFilesCount =
         PropertyUtil.propertyAsLong(latestSnapshot.summary(), SnapshotSummary.ADDED_FILES_PROP, -1);
-    // If snapshotSummary doesn't have SnapshotSummary.ADDED_FILES_PROP, iterate through addedFiles
-    // iterator to find
-    // addedFilesCount.
+    // if the latest snapshot summary doesn't contain SnapshotSummary.ADDED_FILES_PROP,
+    // iterate through addedDataFiles to compute addedFilesCount
     addedFilesCount =
         addedFilesCount == -1
             ? Iterables.size(latestSnapshot.addedDataFiles(table.io()))

File: aws/src/main/java/org/apache/iceberg/aws/AssumeRoleAwsClientFactory.java
Patch:
@@ -43,6 +43,7 @@ public S3Client s3() {
         .applyMutation(awsProperties::applyHttpClientConfigurations)
         .applyMutation(awsProperties::applyS3EndpointConfigurations)
         .applyMutation(awsProperties::applyS3ServiceConfigurations)
+        .applyMutation(awsProperties::applyS3SignerConfiguration)
         .build();
   }
 

File: aws/src/main/java/org/apache/iceberg/aws/AwsClientFactories.java
Patch:
@@ -102,6 +102,7 @@ public S3Client s3() {
           .applyMutation(awsProperties::applyS3EndpointConfigurations)
           .applyMutation(awsProperties::applyS3ServiceConfigurations)
           .applyMutation(awsProperties::applyS3CredentialConfigurations)
+          .applyMutation(awsProperties::applyS3SignerConfiguration)
           .build();
     }
 

File: nessie/src/main/java/org/apache/iceberg/nessie/NessieCatalog.java
Patch:
@@ -271,11 +271,10 @@ public List<Namespace> listNamespaces(Namespace namespace) throws NoSuchNamespac
   }
 
   /**
-   * Load the given namespace but return an empty map because namespace properties are currently not
-   * supported.
+   * Load the given namespace and return its properties.
    *
    * @param namespace a namespace. {@link Namespace}
-   * @return an empty map
+   * @return a string map of properties for the given namespace
    * @throws NoSuchNamespaceException If the namespace does not exist
    */
   @Override

File: core/src/main/java/org/apache/iceberg/SnapshotScan.java
Patch:
@@ -31,7 +31,6 @@
 import org.apache.iceberg.metrics.ScanMetricsResult;
 import org.apache.iceberg.metrics.ScanReport;
 import org.apache.iceberg.metrics.Timer;
-import org.apache.iceberg.relocated.com.google.common.annotations.VisibleForTesting;
 import org.apache.iceberg.relocated.com.google.common.base.MoreObjects;
 import org.apache.iceberg.relocated.com.google.common.base.Preconditions;
 import org.apache.iceberg.relocated.com.google.common.collect.Lists;
@@ -67,8 +66,7 @@ protected Long snapshotId() {
 
   protected abstract CloseableIterable<T> doPlanFiles();
 
-  @VisibleForTesting
-  ScanMetrics scanMetrics() {
+  protected ScanMetrics scanMetrics() {
     if (scanMetrics == null) {
       this.scanMetrics = ScanMetrics.of(new DefaultMetricsContext());
     }

File: spark/v3.3/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/SparkRowLevelOperationsTestBase.java
Patch:
@@ -180,7 +180,7 @@ protected void createAndInitTable(String schema, String partitioning, String jso
     if (jsonData != null) {
       try {
         Dataset<Row> ds = toDS(schema, jsonData);
-        ds.writeTo(tableName).append();
+        ds.coalesce(1).writeTo(tableName).append();
       } catch (NoSuchTableException e) {
         throw new RuntimeException("Failed to write data", e);
       }

File: core/src/main/java/org/apache/iceberg/rest/auth/OAuth2Util.java
Patch:
@@ -329,14 +329,14 @@ static Long expiresAtMillis(String token) {
       return null;
     }
 
-    String[] parts = token.split("\\.");
-    if (parts.length != 3) {
+    List<String> parts = Splitter.on('.').splitToList(token);
+    if (parts.size() != 3) {
       return null;
     }
 
     JsonNode node;
     try {
-      node = JsonUtil.mapper().readTree(Base64.getUrlDecoder().decode(parts[1]));
+      node = JsonUtil.mapper().readTree(Base64.getUrlDecoder().decode(parts.get(1)));
     } catch (IOException e) {
       return null;
     }

File: core/src/main/java/org/apache/iceberg/BaseMetastoreCatalog.java
Patch:
@@ -83,7 +83,7 @@ public Table registerTable(TableIdentifier identifier, String metadataFileLocati
     TableMetadata metadata = TableMetadataParser.read(ops.io(), metadataFile);
     ops.commit(null, metadata);
 
-    return new BaseTable(ops, identifier.toString());
+    return new BaseTable(ops, fullTableName(name(), identifier));
   }
 
   @Override

File: core/src/main/java/org/apache/iceberg/jdbc/JdbcClientPool.java
Patch:
@@ -27,12 +27,12 @@
 import org.apache.iceberg.CatalogProperties;
 import org.apache.iceberg.ClientPoolImpl;
 
-class JdbcClientPool extends ClientPoolImpl<Connection, SQLException> {
+public class JdbcClientPool extends ClientPoolImpl<Connection, SQLException> {
 
   private final String dbUrl;
   private final Map<String, String> properties;
 
-  JdbcClientPool(String dbUrl, Map<String, String> props) {
+  public JdbcClientPool(String dbUrl, Map<String, String> props) {
     this(
         Integer.parseInt(
             props.getOrDefault(
@@ -42,7 +42,7 @@ class JdbcClientPool extends ClientPoolImpl<Connection, SQLException> {
         props);
   }
 
-  JdbcClientPool(int poolSize, String dbUrl, Map<String, String> props) {
+  public JdbcClientPool(int poolSize, String dbUrl, Map<String, String> props) {
     super(poolSize, SQLNonTransientConnectionException.class, true);
     properties = props;
     this.dbUrl = dbUrl;

File: spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/source/SparkTable.java
Patch:
@@ -145,7 +145,7 @@ public Table table() {
 
   @Override
   public String name() {
-    return icebergTable.toString();
+    return String.format("Iceberg %s", icebergTable.name());
   }
 
   public Long snapshotId() {

File: api/src/main/java/org/apache/iceberg/view/SQLViewRepresentation.java
Patch:
@@ -44,9 +44,9 @@ default Type type() {
   /** The query output schema at version create time, without aliases. */
   Schema schema();
 
-  /** The view field aliases. */
+  /** The view field comments. */
   List<String> fieldComments();
 
-  /** The view field comments. */
+  /** The view field aliases. */
   List<String> fieldAliases();
 }

File: flink/v1.14/flink/src/test/java/org/apache/iceberg/flink/SimpleDataUtil.java
Patch:
@@ -224,7 +224,7 @@ public static List<Record> tableRecords(Table table) throws IOException {
     return records;
   }
 
-  private static boolean equalsRecords(List<Record> expected, List<Record> actual, Schema schema) {
+  public static boolean equalsRecords(List<Record> expected, List<Record> actual, Schema schema) {
     if (expected.size() != actual.size()) {
       return false;
     }
@@ -236,8 +236,7 @@ private static boolean equalsRecords(List<Record> expected, List<Record> actual,
     return expectedSet.equals(actualSet);
   }
 
-  private static void assertRecordsEqual(
-      List<Record> expected, List<Record> actual, Schema schema) {
+  public static void assertRecordsEqual(List<Record> expected, List<Record> actual, Schema schema) {
     Assert.assertEquals(expected.size(), actual.size());
     Types.StructType type = schema.asStruct();
     StructLikeSet expectedSet = StructLikeSet.create(type);

File: flink/v1.15/flink/src/test/java/org/apache/iceberg/flink/SimpleDataUtil.java
Patch:
@@ -226,7 +226,7 @@ public static List<Record> tableRecords(Table table) throws IOException {
     return records;
   }
 
-  private static boolean equalsRecords(List<Record> expected, List<Record> actual, Schema schema) {
+  public static boolean equalsRecords(List<Record> expected, List<Record> actual, Schema schema) {
     if (expected.size() != actual.size()) {
       return false;
     }
@@ -238,8 +238,7 @@ private static boolean equalsRecords(List<Record> expected, List<Record> actual,
     return expectedSet.equals(actualSet);
   }
 
-  private static void assertRecordsEqual(
-      List<Record> expected, List<Record> actual, Schema schema) {
+  public static void assertRecordsEqual(List<Record> expected, List<Record> actual, Schema schema) {
     Assert.assertEquals(expected.size(), actual.size());
     Types.StructType type = schema.asStruct();
     StructLikeSet expectedSet = StructLikeSet.create(type);

File: flink/v1.16/flink/src/test/java/org/apache/iceberg/flink/SimpleDataUtil.java
Patch:
@@ -226,7 +226,7 @@ public static List<Record> tableRecords(Table table) throws IOException {
     return records;
   }
 
-  private static boolean equalsRecords(List<Record> expected, List<Record> actual, Schema schema) {
+  public static boolean equalsRecords(List<Record> expected, List<Record> actual, Schema schema) {
     if (expected.size() != actual.size()) {
       return false;
     }
@@ -238,8 +238,7 @@ private static boolean equalsRecords(List<Record> expected, List<Record> actual,
     return expectedSet.equals(actualSet);
   }
 
-  private static void assertRecordsEqual(
-      List<Record> expected, List<Record> actual, Schema schema) {
+  public static void assertRecordsEqual(List<Record> expected, List<Record> actual, Schema schema) {
     Assert.assertEquals(expected.size(), actual.size());
     Types.StructType type = schema.asStruct();
     StructLikeSet expectedSet = StructLikeSet.create(type);

File: core/src/main/java/org/apache/iceberg/rest/CatalogHandlers.java
Patch:
@@ -223,7 +223,7 @@ public static LoadTableResponse createTable(
   }
 
   public static void dropTable(Catalog catalog, TableIdentifier ident) {
-    boolean dropped = catalog.dropTable(ident);
+    boolean dropped = catalog.dropTable(ident, false);
     if (!dropped) {
       throw new NoSuchTableException("Table does not exist: %s", ident);
     }

File: core/src/main/java/org/apache/iceberg/util/SnapshotUtil.java
Patch:
@@ -149,7 +149,8 @@ public static Iterable<Snapshot> ancestorsOf(long snapshotId, Function<Long, Sna
   }
 
   /**
-   * Finds the oldest snapshot of a table that was committed either at or after a given time.
+   * Traverses the history of the table's current snapshot, finds the oldest snapshot that was
+   * committed either at or after a given time.
    *
    * @param table a table
    * @param timestampMillis a timestamp in milliseconds

File: aws/src/main/java/org/apache/iceberg/aws/glue/GlueCatalog.java
Patch:
@@ -261,6 +261,7 @@ protected String defaultWarehouseLocation(TableIdentifier tableIdentifier) {
     GetDatabaseResponse response =
         glue.getDatabase(
             GetDatabaseRequest.builder()
+                .catalogId(awsProperties.glueCatalogId())
                 .name(
                     IcebergToGlueConverter.getDatabaseName(
                         tableIdentifier, awsProperties.glueCatalogSkipNameValidation()))

File: spark/v3.2/spark/src/main/java/org/apache/iceberg/spark/procedures/RewriteDataFilesProcedure.java
Patch:
@@ -183,8 +183,10 @@ private RewriteDataFiles checkAndApplyStrategy(
                 .flatMap(zOrder -> zOrder.refs().stream().map(NamedReference::name))
                 .toArray(String[]::new);
         return action.zOrder(columnNames);
-      } else {
+      } else if (!sortOrderFields.isEmpty()) {
         return action.sort(buildSortOrder(sortOrderFields, schema));
+      } else {
+        return action.sort();
       }
     }
     if (strategy.equalsIgnoreCase("binpack")) {

File: flink/v1.14/flink/src/main/java/org/apache/iceberg/flink/util/FlinkPackage.java
Patch:
@@ -26,7 +26,7 @@ public class FlinkPackage {
 
   private FlinkPackage() {}
 
-  /** @return Flink version string like x.y.z */
+  /** Returns Flink version string like x.y.z */
   public static String version() {
     return VERSION;
   }

File: flink/v1.15/flink/src/main/java/org/apache/iceberg/flink/util/FlinkPackage.java
Patch:
@@ -26,7 +26,7 @@ public class FlinkPackage {
 
   private FlinkPackage() {}
 
-  /** @return Flink version string like x.y.z */
+  /** Returns Flink version string like x.y.z */
   public static String version() {
     return VERSION;
   }

File: flink/v1.16/flink/src/main/java/org/apache/iceberg/flink/util/FlinkPackage.java
Patch:
@@ -26,7 +26,7 @@ public class FlinkPackage {
 
   private FlinkPackage() {}
 
-  /** @return Flink version string like x.y.z */
+  /** Returns Flink version string like x.y.z */
   public static String version() {
     return VERSION;
   }

File: spark/v3.3/spark/src/jmh/java/org/apache/iceberg/spark/data/parquet/SparkParquetReadersFlatDataBenchmark.java
Patch:
@@ -88,7 +88,7 @@ public class SparkParquetReadersFlatDataBenchmark {
           required(1, "longCol", Types.LongType.get()),
           optional(5, "decimalCol", Types.DecimalType.of(20, 5)),
           optional(8, "stringCol", Types.StringType.get()));
-  private static final int NUM_RECORDS = 10000000;
+  private static final int NUM_RECORDS = 1000000;
   private File dataFile;
 
   @Setup
@@ -155,6 +155,7 @@ public void readUsingSparkReader(Blackhole blackhole) throws IOException {
             .set("spark.sql.parquet.binaryAsString", "false")
             .set("spark.sql.parquet.int96AsTimestamp", "false")
             .set("spark.sql.caseSensitive", "false")
+            .set("spark.sql.parquet.fieldId.write.enabled", "false")
             .callInit()
             .build()) {
 

File: spark/v3.3/spark/src/jmh/java/org/apache/iceberg/spark/data/parquet/SparkParquetReadersNestedDataBenchmark.java
Patch:
@@ -86,7 +86,7 @@ public class SparkParquetReadersNestedDataBenchmark {
   private static final Schema PROJECTED_SCHEMA =
       new Schema(
           optional(4, "nested", Types.StructType.of(required(1, "col1", Types.StringType.get()))));
-  private static final int NUM_RECORDS = 10000000;
+  private static final int NUM_RECORDS = 1000000;
   private File dataFile;
 
   @Setup
@@ -153,6 +153,7 @@ public void readUsingSparkReader(Blackhole blackhole) throws IOException {
             .set("spark.sql.parquet.binaryAsString", "false")
             .set("spark.sql.parquet.int96AsTimestamp", "false")
             .set("spark.sql.caseSensitive", "false")
+            .set("spark.sql.parquet.fieldId.write.enabled", "false")
             .callInit()
             .build()) {
 

File: spark/v3.3/spark/src/jmh/java/org/apache/iceberg/spark/data/parquet/SparkParquetWritersFlatDataBenchmark.java
Patch:
@@ -120,6 +120,7 @@ public void writeUsingSparkWriter() throws IOException {
             .set("spark.sql.parquet.int96AsTimestamp", "false")
             .set("spark.sql.parquet.outputTimestampType", "TIMESTAMP_MICROS")
             .set("spark.sql.caseSensitive", "false")
+            .set("spark.sql.parquet.fieldId.write.enabled", "false")
             .schema(SCHEMA)
             .build()) {
 

File: spark/v3.3/spark/src/jmh/java/org/apache/iceberg/spark/data/parquet/SparkParquetWritersNestedDataBenchmark.java
Patch:
@@ -120,6 +120,7 @@ public void writeUsingSparkWriter() throws IOException {
             .set("spark.sql.parquet.int96AsTimestamp", "false")
             .set("spark.sql.parquet.outputTimestampType", "TIMESTAMP_MICROS")
             .set("spark.sql.caseSensitive", "false")
+            .set("spark.sql.parquet.fieldId.write.enabled", "false")
             .schema(SCHEMA)
             .build()) {
 

File: api/src/main/java/org/apache/iceberg/types/TypeUtil.java
Patch:
@@ -332,10 +332,10 @@ public static boolean isPromotionAllowed(Type from, Type.PrimitiveType to) {
 
     switch (from.typeId()) {
       case INTEGER:
-        return to == Types.LongType.get();
+        return to.typeId() == Type.TypeID.LONG;
 
       case FLOAT:
-        return to == Types.DoubleType.get();
+        return to.typeId() == Type.TypeID.DOUBLE;
 
       case DECIMAL:
         Types.DecimalType fromDecimal = (Types.DecimalType) from;

File: spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/source/EqualityDeleteRowReader.java
Patch:
@@ -31,7 +31,7 @@
 public class EqualityDeleteRowReader extends RowDataReader {
   public EqualityDeleteRowReader(
       CombinedScanTask task, Table table, Schema expectedSchema, boolean caseSensitive) {
-    super(task, table, expectedSchema, caseSensitive);
+    super(table, task, expectedSchema, caseSensitive);
   }
 
   @Override

File: spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/source/SparkMicroBatchStream.java
Patch:
@@ -47,7 +47,6 @@
 import org.apache.iceberg.relocated.com.google.common.collect.Lists;
 import org.apache.iceberg.spark.SparkReadConf;
 import org.apache.iceberg.spark.SparkReadOptions;
-import org.apache.iceberg.spark.source.SparkScan.ReaderFactory;
 import org.apache.iceberg.util.PropertyUtil;
 import org.apache.iceberg.util.SnapshotUtil;
 import org.apache.iceberg.util.TableScanUtil;
@@ -172,7 +171,7 @@ public InputPartition[] planInputPartitions(Offset start, Offset end) {
 
   @Override
   public PartitionReaderFactory createReaderFactory() {
-    return new ReaderFactory(0);
+    return new SparkRowReaderFactory();
   }
 
   @Override

File: api/src/main/java/org/apache/iceberg/io/CloseableIterable.java
Patch:
@@ -279,7 +279,9 @@ public boolean hasNext() {
       @Override
       public void close() throws IOException {
         if (!closed) {
-          currentIterable.close();
+          if (null != currentIterable) {
+            currentIterable.close();
+          }
           this.closed = true;
           this.currentIterator = null;
           this.currentIterable = null;

File: api/src/main/java/org/apache/iceberg/transforms/Dates.java
Patch:
@@ -18,6 +18,7 @@
  */
 package org.apache.iceberg.transforms;
 
+import com.google.errorprone.annotations.Immutable;
 import java.time.temporal.ChronoUnit;
 import org.apache.iceberg.expressions.BoundPredicate;
 import org.apache.iceberg.expressions.BoundTransform;
@@ -35,6 +36,7 @@ enum Dates implements Transform<Integer, Integer> {
   MONTH(ChronoUnit.MONTHS, "month"),
   DAY(ChronoUnit.DAYS, "day");
 
+  @Immutable
   static class Apply implements SerializableFunction<Integer, Integer> {
     private final ChronoUnit granularity;
 

File: api/src/main/java/org/apache/iceberg/transforms/Timestamps.java
Patch:
@@ -18,6 +18,7 @@
  */
 package org.apache.iceberg.transforms;
 
+import com.google.errorprone.annotations.Immutable;
 import java.time.temporal.ChronoUnit;
 import org.apache.iceberg.expressions.BoundPredicate;
 import org.apache.iceberg.expressions.BoundTransform;
@@ -36,6 +37,7 @@ enum Timestamps implements Transform<Long, Integer> {
   DAY(ChronoUnit.DAYS, "day"),
   HOUR(ChronoUnit.HOURS, "hour");
 
+  @Immutable
   static class Apply implements SerializableFunction<Long, Integer> {
     private final ChronoUnit granularity;
 
@@ -66,7 +68,7 @@ public Integer apply(Long timestampMicros) {
 
   private final ChronoUnit granularity;
   private final String name;
-  private final SerializableFunction<Long, Integer> apply;
+  private final Apply apply;
 
   Timestamps(ChronoUnit granularity, String name) {
     this.granularity = granularity;

File: flink/v1.16/flink/src/test/java/org/apache/iceberg/flink/FlinkCatalogTestBase.java
Patch:
@@ -65,7 +65,7 @@ public void before() {
 
   @After
   public void clean() {
-    sql("DROP CATALOG IF EXISTS %s", catalogName);
+    dropCatalog(catalogName, true);
   }
 
   @Parameterized.Parameters(name = "catalogName = {0} baseNamespace = {1}")

File: flink/v1.16/flink/src/test/java/org/apache/iceberg/flink/TestChangeLogTable.java
Patch:
@@ -99,7 +99,7 @@ public void before() {
   public void clean() {
     sql("DROP TABLE IF EXISTS %s", TABLE_NAME);
     sql("DROP DATABASE IF EXISTS %s", DATABASE_NAME);
-    sql("DROP CATALOG IF EXISTS %s", CATALOG_NAME);
+    dropCatalog(CATALOG_NAME, true);
     BoundedTableFactory.clearDataSets();
   }
 

File: flink/v1.16/flink/src/test/java/org/apache/iceberg/flink/TestFlinkHiveCatalog.java
Patch:
@@ -100,6 +100,6 @@ private void checkSQLQuery(Map<String, String> catalogProperties, File warehouse
 
     sql("DROP TABLE test_table");
     sql("DROP DATABASE test_db");
-    sql("DROP CATALOG test_catalog");
+    dropCatalog("test_catalog", false);
   }
 }

File: flink/v1.16/flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkTableSource.java
Patch:
@@ -97,7 +97,7 @@ public void before() {
   public void clean() {
     sql("DROP TABLE IF EXISTS %s.%s", DATABASE_NAME, TABLE_NAME);
     sql("DROP DATABASE IF EXISTS %s", DATABASE_NAME);
-    sql("DROP CATALOG IF EXISTS %s", CATALOG_NAME);
+    dropCatalog(CATALOG_NAME, true);
   }
 
   @Test

File: api/src/main/java/org/apache/iceberg/SortOrder.java
Patch:
@@ -284,7 +284,7 @@ SortOrder buildUnchecked() {
 
     private Transform<?, ?> toTransform(BoundTerm<?> term) {
       if (term instanceof BoundReference) {
-        return Transforms.identity();
+        return Transforms.identity(term.type());
       } else if (term instanceof BoundTransform) {
         return ((BoundTransform<?, ?>) term).transform();
       } else {

File: api/src/main/java/org/apache/iceberg/transforms/Transforms.java
Patch:
@@ -81,7 +81,7 @@ private Transforms() {}
     }
 
     if (transform.equalsIgnoreCase("identity")) {
-      return Identity.get();
+      return Identity.get(type);
     }
 
     try {
@@ -111,7 +111,7 @@ private Transforms() {}
    */
   @Deprecated
   public static <T> Transform<T, T> identity(Type type) {
-    return Identity.get();
+    return Identity.get(type);
   }
 
   /**

File: api/src/main/java/org/apache/iceberg/PartitionField.java
Patch:
@@ -73,7 +73,7 @@ public boolean equals(Object other) {
     return sourceId == that.sourceId
         && fieldId == that.fieldId
         && name.equals(that.name)
-        && transform.equals(that.transform);
+        && transform.toString().equals(that.transform.toString());
   }
 
   @Override

File: api/src/main/java/org/apache/iceberg/SortField.java
Patch:
@@ -96,7 +96,7 @@ public boolean equals(Object other) {
     }
 
     SortField that = (SortField) other;
-    return transform.equals(that.transform)
+    return transform.toString().equals(that.transform.toString())
         && sourceId == that.sourceId
         && direction == that.direction
         && nullOrder == that.nullOrder;

File: api/src/main/java/org/apache/iceberg/transforms/ProjectionUtil.java
Patch:
@@ -231,7 +231,9 @@ static <S, T> UnboundPredicate<T> truncateArrayStrict(
   static <T> UnboundPredicate<T> projectTransformPredicate(
       Transform<?, T> transform, String partitionName, BoundPredicate<?> pred) {
     if (pred.term() instanceof BoundTransform
-        && transform.equals(((BoundTransform<?, ?>) pred.term()).transform())) {
+        && transform
+            .toString()
+            .equals(((BoundTransform<?, ?>) pred.term()).transform().toString())) {
       // the bound value must be a T because the transform matches
       return (UnboundPredicate<T>) removeTransform(partitionName, pred);
     }

File: api/src/main/java/org/apache/iceberg/TableScan.java
Patch:
@@ -45,7 +45,9 @@ public interface TableScan extends Scan<TableScan, FileScanTask, CombinedScanTas
    * @return a new scan based on the given reference.
    * @throws IllegalArgumentException if a reference with the given name could not be found
    */
-  TableScan useRef(String ref);
+  default TableScan useRef(String ref) {
+    throw new UnsupportedOperationException("Using a reference is not supported");
+  }
 
   /**
    * Create a new {@link TableScan} from this scan's configuration that will use the most recent

File: api/src/main/java/org/apache/iceberg/actions/MigrateTable.java
Patch:
@@ -46,7 +46,9 @@ public interface MigrateTable extends Action<MigrateTable, MigrateTable.Result>
    *
    * @return this for method chaining
    */
-  MigrateTable dropBackup();
+  default MigrateTable dropBackup() {
+    throw new UnsupportedOperationException("Dropping a backup is not supported");
+  }
 
   /** The action result that contains a summary of the execution. */
   interface Result {

File: aws/src/integration/java/org/apache/iceberg/aws/lakeformation/LakeFormationTestBase.java
Patch:
@@ -233,7 +233,7 @@ public static void beforeClass() throws Exception {
         AwsProperties.CLIENT_ASSUME_ROLE_TAGS_PREFIX
             + LakeFormationAwsClientFactory.LF_AUTHORIZED_CALLER,
         LF_AUTHORIZED_CALLER_VALUE);
-    glueCatalogPrivilegedRole.initialize("test_registered", assumeRoleProperties);
+    glueCatalogPrivilegedRole.initialize("test_privileged", assumeRoleProperties);
 
     // Build test glueCatalog with lfRegisterPathRole
     assumeRoleProperties.put(AwsProperties.GLUE_LAKEFORMATION_ENABLED, "false");
@@ -244,7 +244,7 @@ public static void beforeClass() throws Exception {
     assumeRoleProperties.put(
         AwsProperties.CLIENT_FACTORY, AssumeRoleAwsClientFactory.class.getName());
     glueCatalogRegisterPathRole = new GlueCatalog();
-    glueCatalogRegisterPathRole.initialize("test_privileged", assumeRoleProperties);
+    glueCatalogRegisterPathRole.initialize("test_registered", assumeRoleProperties);
     // register S3 test bucket path
     deregisterResource(testBucketPath);
     registerResource(testBucketPath);

File: core/src/main/java/org/apache/iceberg/rest/HTTPClient.java
Patch:
@@ -272,10 +272,11 @@ public <T extends RESTResponse> T post(
   @Override
   public <T extends RESTResponse> T delete(
       String path,
+      Map<String, String> queryParams,
       Class<T> responseType,
       Map<String, String> headers,
       Consumer<ErrorResponse> errorHandler) {
-    return execute(Method.DELETE, path, null, null, responseType, headers, errorHandler);
+    return execute(Method.DELETE, path, queryParams, null, responseType, headers, errorHandler);
   }
 
   @Override

File: core/src/test/java/org/apache/iceberg/rest/TestHTTPClient.java
Patch:
@@ -219,7 +219,7 @@ private static Item doExecuteRequest(
         restClient.head(path, headers, onError);
         return null;
       case DELETE:
-        return restClient.delete(path, Item.class, headers, onError);
+        return restClient.delete(path, Item.class, () -> headers, onError);
       default:
         throw new IllegalArgumentException(String.format("Invalid method: %s", method));
     }

File: aws/src/main/java/org/apache/iceberg/aws/AwsProperties.java
Patch:
@@ -21,6 +21,7 @@
 import java.io.Serializable;
 import java.net.URI;
 import java.time.Duration;
+import java.util.Collections;
 import java.util.Map;
 import java.util.Set;
 import java.util.stream.Collectors;
@@ -32,7 +33,6 @@
 import org.apache.iceberg.relocated.com.google.common.annotations.VisibleForTesting;
 import org.apache.iceberg.relocated.com.google.common.base.Preconditions;
 import org.apache.iceberg.relocated.com.google.common.base.Strings;
-import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;
 import org.apache.iceberg.relocated.com.google.common.collect.Sets;
 import org.apache.iceberg.util.PropertyUtil;
 import software.amazon.awssdk.auth.credentials.AwsBasicCredentials;
@@ -717,7 +717,7 @@ public AwsProperties() {
     this.s3DeleteTags = Sets.newHashSet();
     this.s3FileIoDeleteThreads = Runtime.getRuntime().availableProcessors();
     this.isS3DeleteEnabled = S3_DELETE_ENABLED_DEFAULT;
-    this.s3BucketToAccessPointMapping = ImmutableMap.of();
+    this.s3BucketToAccessPointMapping = Collections.emptyMap();
     this.s3PreloadClientEnabled = S3_PRELOAD_CLIENT_ENABLED_DEFAULT;
     this.s3DualStackEnabled = S3_DUALSTACK_ENABLED_DEFAULT;
     this.s3PathStyleAccess = S3FILEIO_PATH_STYLE_ACCESS_DEFAULT;

File: core/src/main/java/org/apache/iceberg/util/PropertyUtil.java
Patch:
@@ -18,12 +18,12 @@
  */
 package org.apache.iceberg.util;
 
+import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
 import java.util.stream.Collectors;
 import org.apache.iceberg.relocated.com.google.common.base.Preconditions;
-import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;
 import org.apache.iceberg.relocated.com.google.common.collect.Maps;
 
 public class PropertyUtil {
@@ -110,7 +110,7 @@ public static String propertyAsString(
   public static Map<String, String> propertiesWithPrefix(
       Map<String, String> properties, String prefix) {
     if (properties == null || properties.isEmpty()) {
-      return ImmutableMap.of();
+      return Collections.emptyMap();
     }
 
     Preconditions.checkArgument(prefix != null, "Invalid prefix: null");

File: core/src/test/java/org/apache/iceberg/metrics/TestScanReportParser.java
Patch:
@@ -162,14 +162,14 @@ public void invalidSchema() {
                 ScanReportParser.fromJson(
                     "{\"table-name\":\"roundTripTableName\",\"snapshot-id\":23,\"filter\":true,\"schema-id\":23,\"projected-field-ids\": [\"1\"],\"metrics\":{}}"))
         .isInstanceOf(IllegalArgumentException.class)
-        .hasMessage("Cannot parse integer from non-int value: \"1\"");
+        .hasMessage("Cannot parse integer from non-int value in projected-field-ids: \"1\"");
 
     Assertions.assertThatThrownBy(
             () ->
                 ScanReportParser.fromJson(
                     "{\"table-name\":\"roundTripTableName\",\"snapshot-id\":23,\"filter\":true,\"schema-id\":23,\"projected-field-ids\": [1],\"projected-field-names\": [1],\"metrics\":{}}"))
         .isInstanceOf(IllegalArgumentException.class)
-        .hasMessage("Cannot parse string from non-text value: 1");
+        .hasMessage("Cannot parse string from non-text value in projected-field-names: 1");
   }
 
   @Test

File: core/src/test/java/org/apache/iceberg/puffin/TestFileMetadataParser.java
Patch:
@@ -173,7 +173,7 @@ public void testFieldNumberOutOfRange() {
                         + "  } ]\n"
                         + "}"))
         .isInstanceOf(IllegalArgumentException.class)
-        .hasMessage("Cannot parse integer from non-int value: 2147483648");
+        .hasMessage("Cannot parse integer from non-int value in fields: 2147483648");
   }
 
   private void testJsonSerialization(FileMetadata fileMetadata, String json) {

File: core/src/main/java/org/apache/iceberg/FastAppend.java
Patch:
@@ -197,7 +197,7 @@ private ManifestFile writeManifest() throws IOException {
     }
 
     if (newManifest == null && newFiles.size() > 0) {
-      ManifestWriter writer = newManifestWriter(spec);
+      ManifestWriter<DataFile> writer = newManifestWriter(spec);
       try {
         writer.addAll(newFiles);
       } finally {

File: spark/v2.4/spark/src/test/java/org/apache/iceberg/TestManifestFileSerialization.java
Patch:
@@ -199,7 +199,7 @@ private ManifestFile writeManifest(DataFile... files) throws IOException {
     Assert.assertTrue(manifestFile.delete());
     OutputFile outputFile = FILE_IO.newOutputFile(manifestFile.getCanonicalPath());
 
-    ManifestWriter writer = ManifestFiles.write(SPEC, outputFile);
+    ManifestWriter<DataFile> writer = ManifestFiles.write(SPEC, outputFile);
     try {
       for (DataFile file : files) {
         writer.add(file);

File: spark/v2.4/spark/src/test/java/org/apache/iceberg/spark/source/TestForwardCompatibility.java
Patch:
@@ -193,7 +193,7 @@ public void testSparkCanReadUnknownTransform() throws IOException {
             .build();
 
     OutputFile manifestFile = localOutput(FileFormat.AVRO.addExtension(temp.newFile().toString()));
-    ManifestWriter manifestWriter = ManifestFiles.write(FAKE_SPEC, manifestFile);
+    ManifestWriter<DataFile> manifestWriter = ManifestFiles.write(FAKE_SPEC, manifestFile);
     try {
       manifestWriter.add(file);
     } finally {

File: spark/v3.0/spark/src/test/java/org/apache/iceberg/TestManifestFileSerialization.java
Patch:
@@ -199,7 +199,7 @@ private ManifestFile writeManifest(DataFile... files) throws IOException {
     Assert.assertTrue(manifestFile.delete());
     OutputFile outputFile = FILE_IO.newOutputFile(manifestFile.getCanonicalPath());
 
-    ManifestWriter writer = ManifestFiles.write(SPEC, outputFile);
+    ManifestWriter<DataFile> writer = ManifestFiles.write(SPEC, outputFile);
     try {
       for (DataFile file : files) {
         writer.add(file);

File: spark/v3.0/spark/src/test/java/org/apache/iceberg/spark/source/TestForwardCompatibility.java
Patch:
@@ -195,7 +195,7 @@ public void testSparkCanReadUnknownTransform() throws IOException {
             .build();
 
     OutputFile manifestFile = localOutput(FileFormat.AVRO.addExtension(temp.newFile().toString()));
-    ManifestWriter manifestWriter = ManifestFiles.write(FAKE_SPEC, manifestFile);
+    ManifestWriter<DataFile> manifestWriter = ManifestFiles.write(FAKE_SPEC, manifestFile);
     try {
       manifestWriter.add(file);
     } finally {

File: spark/v3.1/spark/src/test/java/org/apache/iceberg/TestManifestFileSerialization.java
Patch:
@@ -199,7 +199,7 @@ private ManifestFile writeManifest(DataFile... files) throws IOException {
     Assert.assertTrue(manifestFile.delete());
     OutputFile outputFile = FILE_IO.newOutputFile(manifestFile.getCanonicalPath());
 
-    ManifestWriter writer = ManifestFiles.write(SPEC, outputFile);
+    ManifestWriter<DataFile> writer = ManifestFiles.write(SPEC, outputFile);
     try {
       for (DataFile file : files) {
         writer.add(file);

File: spark/v3.1/spark/src/test/java/org/apache/iceberg/spark/source/TestForwardCompatibility.java
Patch:
@@ -195,7 +195,7 @@ public void testSparkCanReadUnknownTransform() throws IOException {
             .build();
 
     OutputFile manifestFile = localOutput(FileFormat.AVRO.addExtension(temp.newFile().toString()));
-    ManifestWriter manifestWriter = ManifestFiles.write(FAKE_SPEC, manifestFile);
+    ManifestWriter<DataFile> manifestWriter = ManifestFiles.write(FAKE_SPEC, manifestFile);
     try {
       manifestWriter.add(file);
     } finally {

File: spark/v3.2/spark/src/test/java/org/apache/iceberg/TestManifestFileSerialization.java
Patch:
@@ -199,7 +199,7 @@ private ManifestFile writeManifest(DataFile... files) throws IOException {
     Assert.assertTrue(manifestFile.delete());
     OutputFile outputFile = FILE_IO.newOutputFile(manifestFile.getCanonicalPath());
 
-    ManifestWriter writer = ManifestFiles.write(SPEC, outputFile);
+    ManifestWriter<DataFile> writer = ManifestFiles.write(SPEC, outputFile);
     try {
       for (DataFile file : files) {
         writer.add(file);

File: spark/v3.2/spark/src/test/java/org/apache/iceberg/spark/source/TestForwardCompatibility.java
Patch:
@@ -195,7 +195,7 @@ public void testSparkCanReadUnknownTransform() throws IOException {
             .build();
 
     OutputFile manifestFile = localOutput(FileFormat.AVRO.addExtension(temp.newFile().toString()));
-    ManifestWriter manifestWriter = ManifestFiles.write(FAKE_SPEC, manifestFile);
+    ManifestWriter<DataFile> manifestWriter = ManifestFiles.write(FAKE_SPEC, manifestFile);
     try {
       manifestWriter.add(file);
     } finally {

File: spark/v3.3/spark/src/test/java/org/apache/iceberg/TestManifestFileSerialization.java
Patch:
@@ -199,7 +199,7 @@ private ManifestFile writeManifest(DataFile... files) throws IOException {
     Assert.assertTrue(manifestFile.delete());
     OutputFile outputFile = FILE_IO.newOutputFile(manifestFile.getCanonicalPath());
 
-    ManifestWriter writer = ManifestFiles.write(SPEC, outputFile);
+    ManifestWriter<DataFile> writer = ManifestFiles.write(SPEC, outputFile);
     try {
       for (DataFile file : files) {
         writer.add(file);

File: spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/source/TestForwardCompatibility.java
Patch:
@@ -195,7 +195,7 @@ public void testSparkCanReadUnknownTransform() throws IOException {
             .build();
 
     OutputFile manifestFile = localOutput(FileFormat.AVRO.addExtension(temp.newFile().toString()));
-    ManifestWriter manifestWriter = ManifestFiles.write(FAKE_SPEC, manifestFile);
+    ManifestWriter<DataFile> manifestWriter = ManifestFiles.write(FAKE_SPEC, manifestFile);
     try {
       manifestWriter.add(file);
     } finally {

File: core/src/main/java/org/apache/iceberg/util/SnapshotUtil.java
Patch:
@@ -72,7 +72,7 @@ public static boolean isAncestorOf(Table table, long ancestorSnapshotId) {
   public static boolean isParentAncestorOf(
       Table table, long snapshotId, long ancestorParentSnapshotId) {
     for (Snapshot snapshot : ancestorsOf(snapshotId, table::snapshot)) {
-      if (snapshot.parentId() == ancestorParentSnapshotId) {
+      if (snapshot.parentId() != null && snapshot.parentId() == ancestorParentSnapshotId) {
         return true;
       }
     }

File: api/src/main/java/org/apache/iceberg/transforms/Transforms.java
Patch:
@@ -178,11 +178,11 @@ public static <T> Transform<T, Integer> day(Type type) {
   }
 
   /**
-   * Returns a hour {@link Transform} for timestamps.
+   * Returns an hour {@link Transform} for timestamps.
    *
    * @param type the {@link Type source type} for the transform
    * @param <T> Java type passed to this transform
-   * @return a hour transform
+   * @return an hour transform
    * @deprecated use {@link #hour()} instead; will be removed in 2.0.0
    */
   @Deprecated
@@ -262,7 +262,7 @@ public static <T> Transform<T, Integer> day() {
   }
 
   /**
-   * Returns an hour {@link Transform} for date or timestamp types.
+   * Returns an hour {@link Transform} for timestamp types.
    *
    * @param <T> Java type passed to this transform
    * @return an hour transform

File: spark/v2.4/spark/src/jmh/java/org/apache/iceberg/spark/source/parquet/IcebergSourceFlatParquetDataWriteBenchmark.java
Patch:
@@ -81,7 +81,7 @@ private Dataset<Row> benchmarkData() {
         .withColumn("floatCol", expr("CAST(longCol AS FLOAT)"))
         .withColumn("doubleCol", expr("CAST(longCol AS DOUBLE)"))
         .withColumn("decimalCol", expr("CAST(longCol AS DECIMAL(20, 5))"))
-        .withColumn("dateCol", expr("DATE_ADD(CURRENT_DATE(), (longCol % 20))"))
+        .withColumn("dateCol", expr("DATE_ADD(CURRENT_DATE(), (intCol % 20))"))
         .withColumn("timestampCol", expr("TO_TIMESTAMP(dateCol)"))
         .withColumn("stringCol", expr("CAST(dateCol AS STRING)"))
         .coalesce(1);

File: spark/v3.0/spark/src/jmh/java/org/apache/iceberg/spark/source/parquet/IcebergSourceFlatParquetDataWriteBenchmark.java
Patch:
@@ -81,7 +81,7 @@ private Dataset<Row> benchmarkData() {
         .withColumn("floatCol", expr("CAST(longCol AS FLOAT)"))
         .withColumn("doubleCol", expr("CAST(longCol AS DOUBLE)"))
         .withColumn("decimalCol", expr("CAST(longCol AS DECIMAL(20, 5))"))
-        .withColumn("dateCol", expr("DATE_ADD(CURRENT_DATE(), (longCol % 20))"))
+        .withColumn("dateCol", expr("DATE_ADD(CURRENT_DATE(), (intCol % 20))"))
         .withColumn("timestampCol", expr("TO_TIMESTAMP(dateCol)"))
         .withColumn("stringCol", expr("CAST(dateCol AS STRING)"))
         .coalesce(1);

File: spark/v3.1/spark/src/jmh/java/org/apache/iceberg/spark/source/parquet/IcebergSourceFlatParquetDataWriteBenchmark.java
Patch:
@@ -81,7 +81,7 @@ private Dataset<Row> benchmarkData() {
         .withColumn("floatCol", expr("CAST(longCol AS FLOAT)"))
         .withColumn("doubleCol", expr("CAST(longCol AS DOUBLE)"))
         .withColumn("decimalCol", expr("CAST(longCol AS DECIMAL(20, 5))"))
-        .withColumn("dateCol", expr("DATE_ADD(CURRENT_DATE(), (longCol % 20))"))
+        .withColumn("dateCol", expr("DATE_ADD(CURRENT_DATE(), (intCol % 20))"))
         .withColumn("timestampCol", expr("TO_TIMESTAMP(dateCol)"))
         .withColumn("stringCol", expr("CAST(dateCol AS STRING)"))
         .coalesce(1);

File: spark/v3.2/spark/src/jmh/java/org/apache/iceberg/spark/source/parquet/IcebergSourceFlatParquetDataWriteBenchmark.java
Patch:
@@ -81,7 +81,7 @@ private Dataset<Row> benchmarkData() {
         .withColumn("floatCol", expr("CAST(longCol AS FLOAT)"))
         .withColumn("doubleCol", expr("CAST(longCol AS DOUBLE)"))
         .withColumn("decimalCol", expr("CAST(longCol AS DECIMAL(20, 5))"))
-        .withColumn("dateCol", expr("DATE_ADD(CURRENT_DATE(), (longCol % 20))"))
+        .withColumn("dateCol", expr("DATE_ADD(CURRENT_DATE(), (intCol % 20))"))
         .withColumn("timestampCol", expr("TO_TIMESTAMP(dateCol)"))
         .withColumn("stringCol", expr("CAST(dateCol AS STRING)"))
         .coalesce(1);

File: spark/v3.3/spark/src/jmh/java/org/apache/iceberg/spark/source/parquet/IcebergSourceFlatParquetDataWriteBenchmark.java
Patch:
@@ -81,7 +81,7 @@ private Dataset<Row> benchmarkData() {
         .withColumn("floatCol", expr("CAST(longCol AS FLOAT)"))
         .withColumn("doubleCol", expr("CAST(longCol AS DOUBLE)"))
         .withColumn("decimalCol", expr("CAST(longCol AS DECIMAL(20, 5))"))
-        .withColumn("dateCol", expr("DATE_ADD(CURRENT_DATE(), (longCol % 20))"))
+        .withColumn("dateCol", expr("DATE_ADD(CURRENT_DATE(), (intCol % 20))"))
         .withColumn("timestampCol", expr("TO_TIMESTAMP(dateCol)"))
         .withColumn("stringCol", expr("CAST(dateCol AS STRING)"))
         .coalesce(1);

File: aws/src/main/java/org/apache/iceberg/aws/AwsProperties.java
Patch:
@@ -829,8 +829,9 @@ public AwsProperties(Map<String, String> properties) {
               properties, S3FILEIO_MULTIPART_SIZE, S3FILEIO_MULTIPART_SIZE_DEFAULT);
     } catch (NumberFormatException e) {
       throw new IllegalArgumentException(
-          "Input malformed or exceeded maximum multipart upload size 5GB: %s"
-              + properties.get(S3FILEIO_MULTIPART_SIZE));
+          String.format(
+              "Input malformed or exceeded maximum multipart upload size 5GB: %s",
+              properties.get(S3FILEIO_MULTIPART_SIZE)));
     }
     this.s3FileIoMultipartThresholdFactor =
         PropertyUtil.propertyAsDouble(

File: spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java
Patch:
@@ -47,9 +47,9 @@
 /**
  * The IcebergSource loads/writes tables with format "iceberg". It can load paths and tables.
  *
- * <p>How paths/tables are loaded when using spark.read().format("iceberg").path(table)
+ * <p>How paths/tables are loaded when using spark.read().format("iceberg").load(table)
  *
- * <p>table = "file:/path/to/table" -&gt; loads a HadoopTable at given path table = "tablename"
+ * <p>table = "file:///path/to/table" -&gt; loads a HadoopTable at given path table = "tablename"
  * -&gt; loads currentCatalog.currentNamespace.tablename table = "catalog.tablename" -&gt; load
  * "tablename" from the specified catalog. table = "namespace.tablename" -&gt; load
  * "namespace.tablename" from current catalog table = "catalog.namespace.tablename" -&gt;

File: core/src/main/java/org/apache/iceberg/SerializableTable.java
Patch:
@@ -82,7 +82,7 @@ protected SerializableTable(Table table) {
     this.io = fileIO(table);
     this.encryption = table.encryption();
     this.locationProvider = table.locationProvider();
-    this.refs = table.refs();
+    this.refs = SerializableMap.copyOf(table.refs());
   }
 
   /**

File: api/src/main/java/org/apache/iceberg/ContentScanTask.java
Patch:
@@ -66,7 +66,8 @@ public interface ContentScanTask<F extends ContentFile<F>> extends ScanTask {
 
   @Override
   default long estimatedRowsCount() {
-    double scannedFileFraction = ((double) length()) / file().fileSizeInBytes();
+    long splitOffset = (file().splitOffsets() != null) ? file().splitOffsets().get(0) : 0L;
+    double scannedFileFraction = ((double) length()) / (file().fileSizeInBytes() - splitOffset);
     return (long) (scannedFileFraction * file().recordCount());
   }
 }

File: core/src/main/java/org/apache/iceberg/BaseFile.java
Patch:
@@ -249,7 +249,7 @@ public void put(int i, Object value) {
         this.filePath = value.toString();
         return;
       case 2:
-        this.format = FileFormat.valueOf(value.toString());
+        this.format = FileFormat.fromString(value.toString());
         return;
       case 3:
         this.partitionSpecId = (value != null) ? (Integer) value : -1;

File: core/src/main/java/org/apache/iceberg/DataFiles.java
Patch:
@@ -20,7 +20,6 @@
 
 import java.nio.ByteBuffer;
 import java.util.List;
-import java.util.Locale;
 import java.util.Map;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.iceberg.encryption.EncryptedOutputFile;
@@ -210,7 +209,7 @@ public Builder withPath(String newFilePath) {
     }
 
     public Builder withFormat(String newFormat) {
-      this.format = FileFormat.valueOf(newFormat.toUpperCase(Locale.ENGLISH));
+      this.format = FileFormat.fromString(newFormat);
       return this;
     }
 

File: core/src/main/java/org/apache/iceberg/FileMetadata.java
Patch:
@@ -19,7 +19,6 @@
 package org.apache.iceberg;
 
 import java.nio.ByteBuffer;
-import java.util.Locale;
 import java.util.Map;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.iceberg.encryption.EncryptedOutputFile;
@@ -145,7 +144,7 @@ public Builder withPath(String newFilePath) {
     }
 
     public Builder withFormat(String newFormat) {
-      this.format = FileFormat.valueOf(newFormat.toUpperCase(Locale.ENGLISH));
+      this.format = FileFormat.fromString(newFormat);
       return this;
     }
 

File: core/src/main/java/org/apache/iceberg/io/OutputFileFactory.java
Patch:
@@ -21,7 +21,6 @@
 import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;
 import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;
 
-import java.util.Locale;
 import java.util.UUID;
 import java.util.concurrent.atomic.AtomicInteger;
 import org.apache.iceberg.FileFormat;
@@ -126,7 +125,7 @@ private Builder(Table table, int partitionId, long taskId) {
 
       String formatAsString =
           table.properties().getOrDefault(DEFAULT_FILE_FORMAT, DEFAULT_FILE_FORMAT_DEFAULT);
-      this.format = FileFormat.valueOf(formatAsString.toUpperCase(Locale.ROOT));
+      this.format = FileFormat.fromString(formatAsString);
     }
 
     public Builder defaultSpec(PartitionSpec newDefaultSpec) {

File: data/src/test/java/org/apache/iceberg/TestSplitScan.java
Patch:
@@ -23,7 +23,6 @@
 import java.io.File;
 import java.io.IOException;
 import java.util.List;
-import java.util.Locale;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.iceberg.data.GenericAppenderFactory;
 import org.apache.iceberg.data.IcebergGenerics;
@@ -66,7 +65,7 @@ public static Object[] parameters() {
   private final FileFormat format;
 
   public TestSplitScan(String format) {
-    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));
+    this.format = FileFormat.fromString(format);
   }
 
   @Before

File: data/src/test/java/org/apache/iceberg/data/FileHelpers.java
Patch:
@@ -24,7 +24,6 @@
 import java.io.Closeable;
 import java.io.IOException;
 import java.util.List;
-import java.util.Locale;
 import java.util.Map;
 import org.apache.iceberg.DataFile;
 import org.apache.iceberg.DataFiles;
@@ -146,6 +145,6 @@ private static EncryptedOutputFile encrypt(OutputFile out) {
 
   private static FileFormat defaultFormat(Map<String, String> properties) {
     String formatString = properties.getOrDefault(DEFAULT_FILE_FORMAT, DEFAULT_FILE_FORMAT_DEFAULT);
-    return FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));
+    return FileFormat.fromString(formatString);
   }
 }

File: data/src/test/java/org/apache/iceberg/data/TestLocalScan.java
Patch:
@@ -34,7 +34,6 @@
 import java.nio.ByteOrder;
 import java.util.Iterator;
 import java.util.List;
-import java.util.Locale;
 import java.util.Set;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
@@ -87,7 +86,7 @@ public static Object[] parameters() {
   private final FileFormat format;
 
   public TestLocalScan(String format) {
-    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));
+    this.format = FileFormat.fromString(format);
   }
 
   private String sharedTableLocation = null;

File: data/src/test/java/org/apache/iceberg/data/TestMetricsRowGroupFilter.java
Patch:
@@ -43,7 +43,6 @@
 import java.io.IOException;
 import java.io.UncheckedIOException;
 import java.util.List;
-import java.util.Locale;
 import java.util.UUID;
 import org.apache.avro.generic.GenericData.Record;
 import org.apache.avro.generic.GenericRecordBuilder;
@@ -98,7 +97,7 @@ public static Object[] parameters() {
   private final FileFormat format;
 
   public TestMetricsRowGroupFilter(String format) {
-    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));
+    this.format = FileFormat.fromString(format);
   }
 
   private static final Types.StructType structFieldType =

File: data/src/test/java/org/apache/iceberg/data/TestMetricsRowGroupFilterTypes.java
Patch:
@@ -33,7 +33,6 @@
 import java.time.OffsetDateTime;
 import java.time.format.DateTimeFormatter;
 import java.util.List;
-import java.util.Locale;
 import java.util.UUID;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
@@ -277,7 +276,7 @@ public static Object[][] parameters() {
 
   public TestMetricsRowGroupFilterTypes(
       String format, String column, Object readValue, Object skipValue) {
-    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));
+    this.format = FileFormat.fromString(format);
     this.column = column;
     this.readValue = readValue;
     this.skipValue = skipValue;

File: data/src/test/java/org/apache/iceberg/io/TestAppenderFactory.java
Patch:
@@ -21,7 +21,6 @@
 import java.io.File;
 import java.io.IOException;
 import java.util.List;
-import java.util.Locale;
 import java.util.Set;
 import org.apache.iceberg.DataFile;
 import org.apache.iceberg.FileFormat;
@@ -77,7 +76,7 @@ public static Object[] parameters() {
 
   public TestAppenderFactory(String fileFormat, boolean partitioned) {
     super(FORMAT_V2);
-    this.format = FileFormat.valueOf(fileFormat.toUpperCase(Locale.ENGLISH));
+    this.format = FileFormat.fromString(fileFormat);
     this.partitioned = partitioned;
   }
 

File: data/src/test/java/org/apache/iceberg/io/TestBaseTaskWriter.java
Patch:
@@ -25,7 +25,6 @@
 import java.nio.file.Paths;
 import java.util.Arrays;
 import java.util.List;
-import java.util.Locale;
 import java.util.stream.Collectors;
 import org.apache.iceberg.FileFormat;
 import org.apache.iceberg.PartitionSpec;
@@ -60,7 +59,7 @@ public static Object[][] parameters() {
 
   public TestBaseTaskWriter(String fileFormat) {
     super(FORMAT_V2);
-    this.format = FileFormat.valueOf(fileFormat.toUpperCase(Locale.ENGLISH));
+    this.format = FileFormat.fromString(fileFormat);
   }
 
   @Override

File: data/src/test/java/org/apache/iceberg/io/TestGenericSortedPosDeleteWriter.java
Patch:
@@ -21,7 +21,6 @@
 import java.io.File;
 import java.io.IOException;
 import java.util.List;
-import java.util.Locale;
 import org.apache.iceberg.DataFile;
 import org.apache.iceberg.DeleteFile;
 import org.apache.iceberg.FileFormat;
@@ -66,7 +65,7 @@ public static Object[] parameters() {
 
   public TestGenericSortedPosDeleteWriter(String fileFormat) {
     super(FORMAT_V2);
-    this.format = FileFormat.valueOf(fileFormat.toUpperCase(Locale.ENGLISH));
+    this.format = FileFormat.fromString(fileFormat);
   }
 
   @Override

File: data/src/test/java/org/apache/iceberg/io/TestTaskEqualityDeltaWriter.java
Patch:
@@ -22,7 +22,6 @@
 import java.io.IOException;
 import java.util.Arrays;
 import java.util.List;
-import java.util.Locale;
 import java.util.function.Function;
 import org.apache.iceberg.DataFile;
 import org.apache.iceberg.DeleteFile;
@@ -75,7 +74,7 @@ public static Object[][] parameters() {
 
   public TestTaskEqualityDeltaWriter(String fileFormat) {
     super(FORMAT_V2);
-    this.format = FileFormat.valueOf(fileFormat.toUpperCase(Locale.ENGLISH));
+    this.format = FileFormat.fromString(fileFormat);
   }
 
   @Override

File: flink/v1.13/flink/src/main/java/org/apache/iceberg/flink/FlinkWriteConf.java
Patch:
@@ -18,7 +18,6 @@
  */
 package org.apache.iceberg.flink;
 
-import java.util.Locale;
 import java.util.Map;
 import org.apache.flink.configuration.ReadableConfig;
 import org.apache.iceberg.DistributionMode;
@@ -81,7 +80,7 @@ public FileFormat dataFileFormat() {
             .tableProperty(TableProperties.DEFAULT_FILE_FORMAT)
             .defaultValue(TableProperties.DEFAULT_FILE_FORMAT_DEFAULT)
             .parse();
-    return FileFormat.valueOf(valueAsString.toUpperCase(Locale.ENGLISH));
+    return FileFormat.fromString(valueAsString);
   }
 
   public long targetDataFileSize() {

File: flink/v1.13/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkFileWriterFactory.java
Patch:
@@ -24,7 +24,6 @@
 import static org.apache.iceberg.TableProperties.DELETE_DEFAULT_FILE_FORMAT;
 
 import java.io.Serializable;
-import java.util.Locale;
 import java.util.Map;
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.data.StringData;
@@ -192,11 +191,11 @@ static class Builder {
 
       String dataFileFormatName =
           properties.getOrDefault(DEFAULT_FILE_FORMAT, DEFAULT_FILE_FORMAT_DEFAULT);
-      this.dataFileFormat = FileFormat.valueOf(dataFileFormatName.toUpperCase(Locale.ENGLISH));
+      this.dataFileFormat = FileFormat.fromString(dataFileFormatName);
 
       String deleteFileFormatName =
           properties.getOrDefault(DELETE_DEFAULT_FILE_FORMAT, dataFileFormatName);
-      this.deleteFileFormat = FileFormat.valueOf(deleteFileFormatName.toUpperCase(Locale.ENGLISH));
+      this.deleteFileFormat = FileFormat.fromString(deleteFileFormatName);
     }
 
     Builder dataFileFormat(FileFormat newDataFileFormat) {

File: flink/v1.13/flink/src/main/java/org/apache/iceberg/flink/source/RowDataRewriter.java
Patch:
@@ -22,7 +22,6 @@
 
 import java.util.Collection;
 import java.util.List;
-import java.util.Locale;
 import java.util.stream.Collectors;
 import org.apache.flink.api.common.functions.RichMapFunction;
 import org.apache.flink.configuration.Configuration;
@@ -74,7 +73,7 @@ public RowDataRewriter(
             table.properties(),
             TableProperties.DEFAULT_FILE_FORMAT,
             TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);
-    FileFormat format = FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));
+    FileFormat format = FileFormat.fromString(formatString);
     RowType flinkSchema = FlinkSchemaUtil.convert(table.schema());
     this.taskWriterFactory =
         new RowDataTaskWriterFactory(

File: flink/v1.13/flink/src/test/java/org/apache/iceberg/flink/sink/TestDeltaTaskWriter.java
Patch:
@@ -31,7 +31,6 @@
 import java.nio.file.Paths;
 import java.util.Arrays;
 import java.util.List;
-import java.util.Locale;
 import java.util.stream.Collectors;
 import org.apache.flink.table.data.RowData;
 import org.apache.iceberg.FileContent;
@@ -68,7 +67,7 @@ public static Object[][] parameters() {
 
   public TestDeltaTaskWriter(String fileFormat) {
     super(FORMAT_V2);
-    this.format = FileFormat.valueOf(fileFormat.toUpperCase(Locale.ENGLISH));
+    this.format = FileFormat.fromString(fileFormat);
   }
 
   @Override

File: flink/v1.13/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSink.java
Patch:
@@ -21,7 +21,6 @@
 import java.io.File;
 import java.io.IOException;
 import java.util.List;
-import java.util.Locale;
 import java.util.Map;
 import java.util.stream.Collectors;
 import org.apache.flink.api.common.typeinfo.TypeInformation;
@@ -97,7 +96,7 @@ public static Object[][] parameters() {
   }
 
   public TestFlinkIcebergSink(String format, int parallelism, boolean partitioned) {
-    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));
+    this.format = FileFormat.fromString(format);
     this.parallelism = parallelism;
     this.partitioned = partitioned;
   }
@@ -378,7 +377,7 @@ public void testOverrideWriteConfigWithUnknownFileFormat() {
     AssertHelpers.assertThrows(
         "Should fail with invalid file format.",
         IllegalArgumentException.class,
-        "No enum constant org.apache.iceberg.FileFormat.UNRECOGNIZED",
+        "Invalid file format: UNRECOGNIZED",
         () -> {
           builder.append();
 

File: flink/v1.13/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkV2.java
Patch:
@@ -21,7 +21,6 @@
 import java.io.File;
 import java.io.IOException;
 import java.util.List;
-import java.util.Locale;
 import java.util.Map;
 import org.apache.flink.api.common.typeinfo.TypeInformation;
 import org.apache.flink.api.java.functions.KeySelector;
@@ -112,7 +111,7 @@ public static Object[][] parameters() {
   public TestFlinkIcebergSinkV2(
       String format, int parallelism, boolean partitioned, String writeDistributionMode) {
     super(FORMAT_V2);
-    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));
+    this.format = FileFormat.fromString(format);
     this.parallelism = parallelism;
     this.partitioned = partitioned;
     this.writeDistributionMode = writeDistributionMode;

File: flink/v1.13/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergFilesCommitter.java
Patch:
@@ -27,7 +27,6 @@
 import java.nio.file.Files;
 import java.nio.file.Path;
 import java.util.List;
-import java.util.Locale;
 import java.util.stream.Collectors;
 import org.apache.flink.api.common.ExecutionConfig;
 import org.apache.flink.api.common.JobID;
@@ -90,7 +89,7 @@ public static Object[][] parameters() {
 
   public TestIcebergFilesCommitter(String format, int formatVersion) {
     super(formatVersion);
-    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));
+    this.format = FileFormat.fromString(format);
   }
 
   @Override

File: flink/v1.13/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergStreamWriter.java
Patch:
@@ -22,7 +22,6 @@
 import java.io.IOException;
 import java.util.Arrays;
 import java.util.List;
-import java.util.Locale;
 import java.util.Map;
 import java.util.Set;
 import org.apache.flink.streaming.api.operators.BoundedOneInput;
@@ -88,7 +87,7 @@ public static Object[][] parameters() {
   }
 
   public TestIcebergStreamWriter(String format, boolean partitioned) {
-    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));
+    this.format = FileFormat.fromString(format);
     this.partitioned = partitioned;
   }
 

File: flink/v1.13/flink/src/test/java/org/apache/iceberg/flink/sink/TestTaskWriters.java
Patch:
@@ -21,7 +21,6 @@
 import java.io.File;
 import java.io.IOException;
 import java.util.List;
-import java.util.Locale;
 import java.util.Map;
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.types.logical.RowType;
@@ -74,7 +73,7 @@ public static Object[][] parameters() {
   private Table table;
 
   public TestTaskWriters(String format, boolean partitioned) {
-    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));
+    this.format = FileFormat.fromString(format);
     this.partitioned = partitioned;
   }
 

File: flink/v1.13/flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkScan.java
Patch:
@@ -26,7 +26,6 @@
 import java.util.Arrays;
 import java.util.Collections;
 import java.util.List;
-import java.util.Locale;
 import java.util.Map;
 import org.apache.flink.test.util.MiniClusterWithClientResource;
 import org.apache.flink.types.Row;
@@ -83,7 +82,7 @@ public static Object[] parameters() {
   }
 
   TestFlinkScan(String fileFormat) {
-    this.fileFormat = FileFormat.valueOf(fileFormat.toUpperCase(Locale.ENGLISH));
+    this.fileFormat = FileFormat.fromString(fileFormat);
   }
 
   @Before

File: flink/v1.14/flink/src/main/java/org/apache/iceberg/flink/FlinkWriteConf.java
Patch:
@@ -18,7 +18,6 @@
  */
 package org.apache.iceberg.flink;
 
-import java.util.Locale;
 import java.util.Map;
 import org.apache.flink.configuration.ReadableConfig;
 import org.apache.iceberg.DistributionMode;
@@ -81,7 +80,7 @@ public FileFormat dataFileFormat() {
             .tableProperty(TableProperties.DEFAULT_FILE_FORMAT)
             .defaultValue(TableProperties.DEFAULT_FILE_FORMAT_DEFAULT)
             .parse();
-    return FileFormat.valueOf(valueAsString.toUpperCase(Locale.ENGLISH));
+    return FileFormat.fromString(valueAsString);
   }
 
   public long targetDataFileSize() {

File: flink/v1.14/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkFileWriterFactory.java
Patch:
@@ -24,7 +24,6 @@
 import static org.apache.iceberg.TableProperties.DELETE_DEFAULT_FILE_FORMAT;
 
 import java.io.Serializable;
-import java.util.Locale;
 import java.util.Map;
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.data.StringData;
@@ -192,11 +191,11 @@ static class Builder {
 
       String dataFileFormatName =
           properties.getOrDefault(DEFAULT_FILE_FORMAT, DEFAULT_FILE_FORMAT_DEFAULT);
-      this.dataFileFormat = FileFormat.valueOf(dataFileFormatName.toUpperCase(Locale.ENGLISH));
+      this.dataFileFormat = FileFormat.fromString(dataFileFormatName);
 
       String deleteFileFormatName =
           properties.getOrDefault(DELETE_DEFAULT_FILE_FORMAT, dataFileFormatName);
-      this.deleteFileFormat = FileFormat.valueOf(deleteFileFormatName.toUpperCase(Locale.ENGLISH));
+      this.deleteFileFormat = FileFormat.fromString(deleteFileFormatName);
     }
 
     Builder dataFileFormat(FileFormat newDataFileFormat) {

File: flink/v1.14/flink/src/main/java/org/apache/iceberg/flink/source/RowDataRewriter.java
Patch:
@@ -22,7 +22,6 @@
 
 import java.util.Collection;
 import java.util.List;
-import java.util.Locale;
 import java.util.stream.Collectors;
 import org.apache.flink.api.common.functions.RichMapFunction;
 import org.apache.flink.configuration.Configuration;
@@ -74,7 +73,7 @@ public RowDataRewriter(
             table.properties(),
             TableProperties.DEFAULT_FILE_FORMAT,
             TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);
-    FileFormat format = FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));
+    FileFormat format = FileFormat.fromString(formatString);
     RowType flinkSchema = FlinkSchemaUtil.convert(table.schema());
     this.taskWriterFactory =
         new RowDataTaskWriterFactory(

File: flink/v1.14/flink/src/test/java/org/apache/iceberg/flink/sink/TestDeltaTaskWriter.java
Patch:
@@ -31,7 +31,6 @@
 import java.nio.file.Paths;
 import java.util.Arrays;
 import java.util.List;
-import java.util.Locale;
 import java.util.stream.Collectors;
 import org.apache.flink.table.data.RowData;
 import org.apache.iceberg.FileContent;
@@ -68,7 +67,7 @@ public static Object[][] parameters() {
 
   public TestDeltaTaskWriter(String fileFormat) {
     super(FORMAT_V2);
-    this.format = FileFormat.valueOf(fileFormat.toUpperCase(Locale.ENGLISH));
+    this.format = FileFormat.fromString(fileFormat);
   }
 
   @Override

File: flink/v1.14/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSink.java
Patch:
@@ -22,7 +22,6 @@
 import java.io.IOException;
 import java.util.Collections;
 import java.util.List;
-import java.util.Locale;
 import java.util.Map;
 import java.util.stream.Collectors;
 import org.apache.flink.api.common.typeinfo.TypeInformation;
@@ -98,7 +97,7 @@ public static Object[][] parameters() {
   }
 
   public TestFlinkIcebergSink(String format, int parallelism, boolean partitioned) {
-    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));
+    this.format = FileFormat.fromString(format);
     this.parallelism = parallelism;
     this.partitioned = partitioned;
   }
@@ -390,7 +389,7 @@ public void testOverrideWriteConfigWithUnknownFileFormat() {
     AssertHelpers.assertThrows(
         "Should fail with invalid file format.",
         IllegalArgumentException.class,
-        "No enum constant org.apache.iceberg.FileFormat.UNRECOGNIZED",
+        "Invalid file format: UNRECOGNIZED",
         () -> {
           builder.append();
 

File: flink/v1.14/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkV2.java
Patch:
@@ -21,7 +21,6 @@
 import java.io.File;
 import java.io.IOException;
 import java.util.List;
-import java.util.Locale;
 import java.util.Map;
 import org.apache.flink.api.common.typeinfo.TypeInformation;
 import org.apache.flink.api.java.functions.KeySelector;
@@ -112,7 +111,7 @@ public static Object[][] parameters() {
   public TestFlinkIcebergSinkV2(
       String format, int parallelism, boolean partitioned, String writeDistributionMode) {
     super(FORMAT_V2);
-    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));
+    this.format = FileFormat.fromString(format);
     this.parallelism = parallelism;
     this.partitioned = partitioned;
     this.writeDistributionMode = writeDistributionMode;

File: flink/v1.14/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergFilesCommitter.java
Patch:
@@ -28,7 +28,6 @@
 import java.nio.file.Path;
 import java.util.Collections;
 import java.util.List;
-import java.util.Locale;
 import java.util.stream.Collectors;
 import org.apache.flink.api.common.ExecutionConfig;
 import org.apache.flink.api.common.JobID;
@@ -92,7 +91,7 @@ public static Object[][] parameters() {
 
   public TestIcebergFilesCommitter(String format, int formatVersion) {
     super(formatVersion);
-    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));
+    this.format = FileFormat.fromString(format);
   }
 
   @Override

File: flink/v1.14/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergStreamWriter.java
Patch:
@@ -22,7 +22,6 @@
 import java.io.IOException;
 import java.util.Arrays;
 import java.util.List;
-import java.util.Locale;
 import java.util.Map;
 import java.util.Set;
 import org.apache.flink.streaming.api.operators.BoundedOneInput;
@@ -88,7 +87,7 @@ public static Object[][] parameters() {
   }
 
   public TestIcebergStreamWriter(String format, boolean partitioned) {
-    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));
+    this.format = FileFormat.fromString(format);
     this.partitioned = partitioned;
   }
 

File: flink/v1.14/flink/src/test/java/org/apache/iceberg/flink/sink/TestTaskWriters.java
Patch:
@@ -21,7 +21,6 @@
 import java.io.File;
 import java.io.IOException;
 import java.util.List;
-import java.util.Locale;
 import java.util.Map;
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.types.logical.RowType;
@@ -74,7 +73,7 @@ public static Object[][] parameters() {
   private Table table;
 
   public TestTaskWriters(String format, boolean partitioned) {
-    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));
+    this.format = FileFormat.fromString(format);
     this.partitioned = partitioned;
   }
 

File: flink/v1.14/flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkScan.java
Patch:
@@ -24,7 +24,6 @@
 import java.util.Arrays;
 import java.util.Collections;
 import java.util.List;
-import java.util.Locale;
 import java.util.Map;
 import org.apache.flink.test.util.MiniClusterWithClientResource;
 import org.apache.flink.types.Row;
@@ -79,7 +78,7 @@ public static Object[] parameters() {
   }
 
   TestFlinkScan(String fileFormat) {
-    this.fileFormat = FileFormat.valueOf(fileFormat.toUpperCase(Locale.ENGLISH));
+    this.fileFormat = FileFormat.fromString(fileFormat);
   }
 
   protected TableLoader tableLoader() {

File: flink/v1.15/flink/src/main/java/org/apache/iceberg/flink/FlinkWriteConf.java
Patch:
@@ -18,7 +18,6 @@
  */
 package org.apache.iceberg.flink;
 
-import java.util.Locale;
 import java.util.Map;
 import org.apache.flink.configuration.ReadableConfig;
 import org.apache.iceberg.DistributionMode;
@@ -81,7 +80,7 @@ public FileFormat dataFileFormat() {
             .tableProperty(TableProperties.DEFAULT_FILE_FORMAT)
             .defaultValue(TableProperties.DEFAULT_FILE_FORMAT_DEFAULT)
             .parse();
-    return FileFormat.valueOf(valueAsString.toUpperCase(Locale.ENGLISH));
+    return FileFormat.fromString(valueAsString);
   }
 
   public long targetDataFileSize() {

File: flink/v1.15/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkFileWriterFactory.java
Patch:
@@ -24,7 +24,6 @@
 import static org.apache.iceberg.TableProperties.DELETE_DEFAULT_FILE_FORMAT;
 
 import java.io.Serializable;
-import java.util.Locale;
 import java.util.Map;
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.data.StringData;
@@ -192,11 +191,11 @@ static class Builder {
 
       String dataFileFormatName =
           properties.getOrDefault(DEFAULT_FILE_FORMAT, DEFAULT_FILE_FORMAT_DEFAULT);
-      this.dataFileFormat = FileFormat.valueOf(dataFileFormatName.toUpperCase(Locale.ENGLISH));
+      this.dataFileFormat = FileFormat.fromString(dataFileFormatName);
 
       String deleteFileFormatName =
           properties.getOrDefault(DELETE_DEFAULT_FILE_FORMAT, dataFileFormatName);
-      this.deleteFileFormat = FileFormat.valueOf(deleteFileFormatName.toUpperCase(Locale.ENGLISH));
+      this.deleteFileFormat = FileFormat.fromString(deleteFileFormatName);
     }
 
     Builder dataFileFormat(FileFormat newDataFileFormat) {

File: flink/v1.15/flink/src/main/java/org/apache/iceberg/flink/source/RowDataRewriter.java
Patch:
@@ -22,7 +22,6 @@
 
 import java.util.Collection;
 import java.util.List;
-import java.util.Locale;
 import java.util.stream.Collectors;
 import org.apache.flink.api.common.functions.RichMapFunction;
 import org.apache.flink.configuration.Configuration;
@@ -74,7 +73,7 @@ public RowDataRewriter(
             table.properties(),
             TableProperties.DEFAULT_FILE_FORMAT,
             TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);
-    FileFormat format = FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));
+    FileFormat format = FileFormat.fromString(formatString);
     RowType flinkSchema = FlinkSchemaUtil.convert(table.schema());
     this.taskWriterFactory =
         new RowDataTaskWriterFactory(

File: flink/v1.15/flink/src/test/java/org/apache/iceberg/flink/sink/TestDeltaTaskWriter.java
Patch:
@@ -31,7 +31,6 @@
 import java.nio.file.Paths;
 import java.util.Arrays;
 import java.util.List;
-import java.util.Locale;
 import java.util.stream.Collectors;
 import org.apache.flink.table.data.RowData;
 import org.apache.iceberg.FileContent;
@@ -68,7 +67,7 @@ public static Object[][] parameters() {
 
   public TestDeltaTaskWriter(String fileFormat) {
     super(FORMAT_V2);
-    this.format = FileFormat.valueOf(fileFormat.toUpperCase(Locale.ENGLISH));
+    this.format = FileFormat.fromString(fileFormat);
   }
 
   @Override

File: flink/v1.15/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSink.java
Patch:
@@ -22,7 +22,6 @@
 import java.io.IOException;
 import java.util.Collections;
 import java.util.List;
-import java.util.Locale;
 import java.util.Map;
 import java.util.stream.Collectors;
 import org.apache.flink.api.common.typeinfo.TypeInformation;
@@ -97,7 +96,7 @@ public static Object[][] parameters() {
   }
 
   public TestFlinkIcebergSink(String format, int parallelism, boolean partitioned) {
-    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));
+    this.format = FileFormat.fromString(format);
     this.parallelism = parallelism;
     this.partitioned = partitioned;
   }
@@ -389,7 +388,7 @@ public void testOverrideWriteConfigWithUnknownFileFormat() {
     AssertHelpers.assertThrows(
         "Should fail with invalid file format.",
         IllegalArgumentException.class,
-        "No enum constant org.apache.iceberg.FileFormat.UNRECOGNIZED",
+        "Invalid file format: UNRECOGNIZED",
         () -> {
           builder.append();
 

File: flink/v1.15/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkV2.java
Patch:
@@ -21,7 +21,6 @@
 import java.io.File;
 import java.io.IOException;
 import java.util.List;
-import java.util.Locale;
 import java.util.Map;
 import org.apache.flink.api.common.typeinfo.TypeInformation;
 import org.apache.flink.api.java.functions.KeySelector;
@@ -112,7 +111,7 @@ public static Object[][] parameters() {
   public TestFlinkIcebergSinkV2(
       String format, int parallelism, boolean partitioned, String writeDistributionMode) {
     super(FORMAT_V2);
-    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));
+    this.format = FileFormat.fromString(format);
     this.parallelism = parallelism;
     this.partitioned = partitioned;
     this.writeDistributionMode = writeDistributionMode;

File: flink/v1.15/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergFilesCommitter.java
Patch:
@@ -28,7 +28,6 @@
 import java.nio.file.Path;
 import java.util.Collections;
 import java.util.List;
-import java.util.Locale;
 import java.util.stream.Collectors;
 import org.apache.flink.api.common.ExecutionConfig;
 import org.apache.flink.api.common.JobID;
@@ -91,7 +90,7 @@ public static Object[][] parameters() {
 
   public TestIcebergFilesCommitter(String format, int formatVersion) {
     super(formatVersion);
-    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));
+    this.format = FileFormat.fromString(format);
   }
 
   @Override

File: flink/v1.15/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergStreamWriter.java
Patch:
@@ -22,7 +22,6 @@
 import java.io.IOException;
 import java.util.Arrays;
 import java.util.List;
-import java.util.Locale;
 import java.util.Map;
 import java.util.Set;
 import org.apache.flink.streaming.api.operators.BoundedOneInput;
@@ -87,7 +86,7 @@ public static Object[][] parameters() {
   }
 
   public TestIcebergStreamWriter(String format, boolean partitioned) {
-    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));
+    this.format = FileFormat.fromString(format);
     this.partitioned = partitioned;
   }
 

File: flink/v1.15/flink/src/test/java/org/apache/iceberg/flink/sink/TestTaskWriters.java
Patch:
@@ -21,7 +21,6 @@
 import java.io.File;
 import java.io.IOException;
 import java.util.List;
-import java.util.Locale;
 import java.util.Map;
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.types.logical.RowType;
@@ -73,7 +72,7 @@ public static Object[][] parameters() {
   private Table table;
 
   public TestTaskWriters(String format, boolean partitioned) {
-    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));
+    this.format = FileFormat.fromString(format);
     this.partitioned = partitioned;
   }
 

File: flink/v1.15/flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkScan.java
Patch:
@@ -24,7 +24,6 @@
 import java.util.Arrays;
 import java.util.Collections;
 import java.util.List;
-import java.util.Locale;
 import java.util.Map;
 import org.apache.flink.test.util.MiniClusterWithClientResource;
 import org.apache.flink.types.Row;
@@ -79,7 +78,7 @@ public static Object[] parameters() {
   }
 
   TestFlinkScan(String fileFormat) {
-    this.fileFormat = FileFormat.valueOf(fileFormat.toUpperCase(Locale.ENGLISH));
+    this.fileFormat = FileFormat.fromString(fileFormat);
   }
 
   protected TableLoader tableLoader() {

File: mr/src/test/java/org/apache/iceberg/mr/TestIcebergInputFormats.java
Patch:
@@ -24,7 +24,6 @@
 import java.io.IOException;
 import java.io.UncheckedIOException;
 import java.util.List;
-import java.util.Locale;
 import java.util.Map;
 import java.util.Set;
 import java.util.function.Function;
@@ -133,7 +132,7 @@ public static Object[][] parameters() {
   public TestIcebergInputFormats(
       TestInputFormat.Factory<Record> testInputFormat, String fileFormat) {
     this.testInputFormat = testInputFormat;
-    this.fileFormat = FileFormat.valueOf(fileFormat.toUpperCase(Locale.ENGLISH));
+    this.fileFormat = FileFormat.fromString(fileFormat);
   }
 
   @Test

File: spark/v2.4/spark/src/main/java/org/apache/iceberg/spark/SparkDataFile.java
Patch:
@@ -20,7 +20,6 @@
 
 import java.nio.ByteBuffer;
 import java.util.List;
-import java.util.Locale;
 import java.util.Map;
 import org.apache.iceberg.DataFile;
 import org.apache.iceberg.FileFormat;
@@ -109,8 +108,7 @@ public CharSequence path() {
 
   @Override
   public FileFormat format() {
-    String formatAsString = wrapped.getString(fileFormatPosition).toUpperCase(Locale.ROOT);
-    return FileFormat.valueOf(formatAsString);
+    return FileFormat.fromString(wrapped.getString(fileFormatPosition));
   }
 
   @Override

File: spark/v2.4/spark/src/main/java/org/apache/iceberg/spark/SparkWriteConf.java
Patch:
@@ -115,7 +115,7 @@ public FileFormat dataFileFormat() {
             .tableProperty(TableProperties.DEFAULT_FILE_FORMAT)
             .defaultValue(TableProperties.DEFAULT_FILE_FORMAT_DEFAULT)
             .parse();
-    return FileFormat.valueOf(valueAsString.toUpperCase(Locale.ENGLISH));
+    return FileFormat.fromString(valueAsString);
   }
 
   public long targetDataFileSize() {

File: spark/v2.4/spark/src/main/java/org/apache/iceberg/spark/source/RowDataRewriter.java
Patch:
@@ -21,7 +21,6 @@
 import java.io.Serializable;
 import java.util.Collection;
 import java.util.List;
-import java.util.Locale;
 import java.util.Map;
 import java.util.stream.Collectors;
 import org.apache.iceberg.CombinedScanTask;
@@ -66,7 +65,7 @@ public RowDataRewriter(
             .properties()
             .getOrDefault(
                 TableProperties.DEFAULT_FILE_FORMAT, TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);
-    this.format = FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));
+    this.format = FileFormat.fromString(formatString);
   }
 
   public List<DataFile> rewriteDataForTasks(JavaRDD<CombinedScanTask> taskRDD) {

File: spark/v2.4/spark/src/main/java/org/apache/iceberg/spark/source/SparkFileWriterFactory.java
Patch:
@@ -23,7 +23,6 @@
 import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;
 import static org.apache.iceberg.TableProperties.DELETE_DEFAULT_FILE_FORMAT;
 
-import java.util.Locale;
 import java.util.Map;
 import org.apache.iceberg.FileFormat;
 import org.apache.iceberg.Schema;
@@ -190,11 +189,11 @@ static class Builder {
 
       String dataFileFormatName =
           properties.getOrDefault(DEFAULT_FILE_FORMAT, DEFAULT_FILE_FORMAT_DEFAULT);
-      this.dataFileFormat = FileFormat.valueOf(dataFileFormatName.toUpperCase(Locale.ENGLISH));
+      this.dataFileFormat = FileFormat.fromString(dataFileFormatName);
 
       String deleteFileFormatName =
           properties.getOrDefault(DELETE_DEFAULT_FILE_FORMAT, dataFileFormatName);
-      this.deleteFileFormat = FileFormat.valueOf(deleteFileFormatName.toUpperCase(Locale.ENGLISH));
+      this.deleteFileFormat = FileFormat.fromString(deleteFileFormatName);
     }
 
     Builder dataFileFormat(FileFormat newDataFileFormat) {

File: spark/v2.4/spark/src/test/java/org/apache/iceberg/spark/source/TestFilteredScan.java
Patch:
@@ -28,7 +28,6 @@
 import java.sql.Timestamp;
 import java.time.OffsetDateTime;
 import java.util.List;
-import java.util.Locale;
 import java.util.UUID;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.iceberg.DataFile;
@@ -180,7 +179,7 @@ public void writeUnpartitionedTable() throws IOException {
     Table table = TABLES.create(SCHEMA, PartitionSpec.unpartitioned(), unpartitioned.toString());
     Schema tableSchema = table.schema(); // use the table schema because ids are reassigned
 
-    FileFormat fileFormat = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));
+    FileFormat fileFormat = FileFormat.fromString(format);
 
     File testFile = new File(dataFolder, fileFormat.addExtension(UUID.randomUUID().toString()));
 

File: spark/v2.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDataWrite.java
Patch:
@@ -27,7 +27,6 @@
 import java.io.File;
 import java.io.IOException;
 import java.util.List;
-import java.util.Locale;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.iceberg.AppendFiles;
 import org.apache.iceberg.AssertHelpers;
@@ -95,7 +94,7 @@ public static void stopSpark() {
   }
 
   public TestSparkDataWrite(String format) {
-    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));
+    this.format = FileFormat.fromString(format);
   }
 
   @Test

File: spark/v2.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReadProjection.java
Patch:
@@ -25,7 +25,6 @@
 import java.io.File;
 import java.io.IOException;
 import java.util.List;
-import java.util.Locale;
 import java.util.Map;
 import java.util.UUID;
 import org.apache.iceberg.DataFile;
@@ -75,7 +74,7 @@ public static Object[][] parameters() {
 
   public TestSparkReadProjection(String format, boolean vectorized) {
     super(format);
-    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ROOT));
+    this.format = FileFormat.fromString(format);
     this.vectorized = vectorized;
   }
 

File: spark/v2.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java
Patch:
@@ -146,7 +146,7 @@ public static Object[] parameters() {
     }
 
     public TableImport(String format) {
-      this.format = FileFormat.valueOf(format.toUpperCase());
+      this.format = FileFormat.fromString(format);
     }
 
     @Before

File: spark/v2.4/spark/src/test/java/org/apache/iceberg/spark/source/TestTimestampWithoutZone.java
Patch:
@@ -24,7 +24,6 @@
 import java.io.IOException;
 import java.time.LocalDateTime;
 import java.util.List;
-import java.util.Locale;
 import java.util.UUID;
 import java.util.stream.Collectors;
 import java.util.stream.Stream;
@@ -122,7 +121,7 @@ public void writeUnpartitionedTable() throws IOException {
     Table table = TABLES.create(SCHEMA, PartitionSpec.unpartitioned(), unpartitioned.toString());
     Schema tableSchema = table.schema(); // use the table schema because ids are reassigned
 
-    FileFormat fileFormat = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));
+    FileFormat fileFormat = FileFormat.fromString(format);
 
     File testFile = new File(dataFolder, fileFormat.addExtension(UUID.randomUUID().toString()));
 

File: spark/v3.0/spark/src/main/java/org/apache/iceberg/spark/SparkDataFile.java
Patch:
@@ -20,7 +20,6 @@
 
 import java.nio.ByteBuffer;
 import java.util.List;
-import java.util.Locale;
 import java.util.Map;
 import org.apache.iceberg.DataFile;
 import org.apache.iceberg.FileFormat;
@@ -109,8 +108,7 @@ public CharSequence path() {
 
   @Override
   public FileFormat format() {
-    String formatAsString = wrapped.getString(fileFormatPosition).toUpperCase(Locale.ROOT);
-    return FileFormat.valueOf(formatAsString);
+    return FileFormat.fromString(wrapped.getString(fileFormatPosition));
   }
 
   @Override

File: spark/v3.0/spark/src/main/java/org/apache/iceberg/spark/SparkWriteConf.java
Patch:
@@ -115,7 +115,7 @@ public FileFormat dataFileFormat() {
             .tableProperty(TableProperties.DEFAULT_FILE_FORMAT)
             .defaultValue(TableProperties.DEFAULT_FILE_FORMAT_DEFAULT)
             .parse();
-    return FileFormat.valueOf(valueAsString.toUpperCase(Locale.ENGLISH));
+    return FileFormat.fromString(valueAsString);
   }
 
   public long targetDataFileSize() {

File: spark/v3.0/spark/src/main/java/org/apache/iceberg/spark/source/RowDataRewriter.java
Patch:
@@ -21,7 +21,6 @@
 import java.io.Serializable;
 import java.util.Collection;
 import java.util.List;
-import java.util.Locale;
 import java.util.Map;
 import java.util.stream.Collectors;
 import org.apache.iceberg.CombinedScanTask;
@@ -66,7 +65,7 @@ public RowDataRewriter(
             .properties()
             .getOrDefault(
                 TableProperties.DEFAULT_FILE_FORMAT, TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);
-    this.format = FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));
+    this.format = FileFormat.fromString(formatString);
   }
 
   public List<DataFile> rewriteDataForTasks(JavaRDD<CombinedScanTask> taskRDD) {

File: spark/v3.0/spark/src/main/java/org/apache/iceberg/spark/source/SparkFileWriterFactory.java
Patch:
@@ -23,7 +23,6 @@
 import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;
 import static org.apache.iceberg.TableProperties.DELETE_DEFAULT_FILE_FORMAT;
 
-import java.util.Locale;
 import java.util.Map;
 import org.apache.iceberg.FileFormat;
 import org.apache.iceberg.Schema;
@@ -190,11 +189,11 @@ static class Builder {
 
       String dataFileFormatName =
           properties.getOrDefault(DEFAULT_FILE_FORMAT, DEFAULT_FILE_FORMAT_DEFAULT);
-      this.dataFileFormat = FileFormat.valueOf(dataFileFormatName.toUpperCase(Locale.ENGLISH));
+      this.dataFileFormat = FileFormat.fromString(dataFileFormatName);
 
       String deleteFileFormatName =
           properties.getOrDefault(DELETE_DEFAULT_FILE_FORMAT, dataFileFormatName);
-      this.deleteFileFormat = FileFormat.valueOf(deleteFileFormatName.toUpperCase(Locale.ENGLISH));
+      this.deleteFileFormat = FileFormat.fromString(deleteFileFormatName);
     }
 
     Builder dataFileFormat(FileFormat newDataFileFormat) {

File: spark/v3.0/spark/src/test/java/org/apache/iceberg/spark/source/TestFilteredScan.java
Patch:
@@ -28,7 +28,6 @@
 import java.sql.Timestamp;
 import java.time.OffsetDateTime;
 import java.util.List;
-import java.util.Locale;
 import java.util.UUID;
 import java.util.stream.Collectors;
 import org.apache.hadoop.conf.Configuration;
@@ -181,7 +180,7 @@ public void writeUnpartitionedTable() throws IOException {
     Table table = TABLES.create(SCHEMA, PartitionSpec.unpartitioned(), unpartitioned.toString());
     Schema tableSchema = table.schema(); // use the table schema because ids are reassigned
 
-    FileFormat fileFormat = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));
+    FileFormat fileFormat = FileFormat.fromString(format);
 
     File testFile = new File(dataFolder, fileFormat.addExtension(UUID.randomUUID().toString()));
 

File: spark/v3.0/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDataWrite.java
Patch:
@@ -27,7 +27,6 @@
 import java.io.File;
 import java.io.IOException;
 import java.util.List;
-import java.util.Locale;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.iceberg.AppendFiles;
 import org.apache.iceberg.AssertHelpers;
@@ -95,7 +94,7 @@ public static void stopSpark() {
   }
 
   public TestSparkDataWrite(String format) {
-    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));
+    this.format = FileFormat.fromString(format);
   }
 
   @Test

File: spark/v3.0/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReadProjection.java
Patch:
@@ -25,7 +25,6 @@
 import java.io.File;
 import java.io.IOException;
 import java.util.List;
-import java.util.Locale;
 import java.util.Map;
 import java.util.UUID;
 import org.apache.iceberg.DataFile;
@@ -75,7 +74,7 @@ public static Object[][] parameters() {
 
   public TestSparkReadProjection(String format, boolean vectorized) {
     super(format);
-    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ROOT));
+    this.format = FileFormat.fromString(format);
     this.vectorized = vectorized;
   }
 

File: spark/v3.0/spark/src/test/java/org/apache/iceberg/spark/source/TestTimestampWithoutZone.java
Patch:
@@ -24,7 +24,6 @@
 import java.io.IOException;
 import java.time.LocalDateTime;
 import java.util.List;
-import java.util.Locale;
 import java.util.UUID;
 import java.util.stream.Collectors;
 import java.util.stream.Stream;
@@ -122,7 +121,7 @@ public void writeUnpartitionedTable() throws IOException {
     Table table = TABLES.create(SCHEMA, PartitionSpec.unpartitioned(), unpartitioned.toString());
     Schema tableSchema = table.schema(); // use the table schema because ids are reassigned
 
-    FileFormat fileFormat = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));
+    FileFormat fileFormat = FileFormat.fromString(format);
 
     File testFile = new File(dataFolder, fileFormat.addExtension(UUID.randomUUID().toString()));
 

File: spark/v3.1/spark/src/main/java/org/apache/iceberg/spark/SparkDataFile.java
Patch:
@@ -20,7 +20,6 @@
 
 import java.nio.ByteBuffer;
 import java.util.List;
-import java.util.Locale;
 import java.util.Map;
 import org.apache.iceberg.DataFile;
 import org.apache.iceberg.FileFormat;
@@ -109,8 +108,7 @@ public CharSequence path() {
 
   @Override
   public FileFormat format() {
-    String formatAsString = wrapped.getString(fileFormatPosition).toUpperCase(Locale.ROOT);
-    return FileFormat.valueOf(formatAsString);
+    return FileFormat.fromString(wrapped.getString(fileFormatPosition));
   }
 
   @Override

File: spark/v3.1/spark/src/main/java/org/apache/iceberg/spark/SparkWriteConf.java
Patch:
@@ -123,7 +123,7 @@ public FileFormat dataFileFormat() {
             .tableProperty(TableProperties.DEFAULT_FILE_FORMAT)
             .defaultValue(TableProperties.DEFAULT_FILE_FORMAT_DEFAULT)
             .parse();
-    return FileFormat.valueOf(valueAsString.toUpperCase(Locale.ENGLISH));
+    return FileFormat.fromString(valueAsString);
   }
 
   public long targetDataFileSize() {

File: spark/v3.1/spark/src/main/java/org/apache/iceberg/spark/source/RowDataRewriter.java
Patch:
@@ -21,7 +21,6 @@
 import java.io.Serializable;
 import java.util.Collection;
 import java.util.List;
-import java.util.Locale;
 import java.util.Map;
 import java.util.stream.Collectors;
 import org.apache.iceberg.CombinedScanTask;
@@ -66,7 +65,7 @@ public RowDataRewriter(
             .properties()
             .getOrDefault(
                 TableProperties.DEFAULT_FILE_FORMAT, TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);
-    this.format = FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));
+    this.format = FileFormat.fromString(formatString);
   }
 
   public List<DataFile> rewriteDataForTasks(JavaRDD<CombinedScanTask> taskRDD) {

File: spark/v3.1/spark/src/main/java/org/apache/iceberg/spark/source/SparkFileWriterFactory.java
Patch:
@@ -23,7 +23,6 @@
 import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;
 import static org.apache.iceberg.TableProperties.DELETE_DEFAULT_FILE_FORMAT;
 
-import java.util.Locale;
 import java.util.Map;
 import org.apache.iceberg.FileFormat;
 import org.apache.iceberg.Schema;
@@ -190,11 +189,11 @@ static class Builder {
 
       String dataFileFormatName =
           properties.getOrDefault(DEFAULT_FILE_FORMAT, DEFAULT_FILE_FORMAT_DEFAULT);
-      this.dataFileFormat = FileFormat.valueOf(dataFileFormatName.toUpperCase(Locale.ENGLISH));
+      this.dataFileFormat = FileFormat.fromString(dataFileFormatName);
 
       String deleteFileFormatName =
           properties.getOrDefault(DELETE_DEFAULT_FILE_FORMAT, dataFileFormatName);
-      this.deleteFileFormat = FileFormat.valueOf(deleteFileFormatName.toUpperCase(Locale.ENGLISH));
+      this.deleteFileFormat = FileFormat.fromString(deleteFileFormatName);
     }
 
     Builder dataFileFormat(FileFormat newDataFileFormat) {

File: spark/v3.1/spark/src/test/java/org/apache/iceberg/spark/source/TestFilteredScan.java
Patch:
@@ -28,7 +28,6 @@
 import java.sql.Timestamp;
 import java.time.OffsetDateTime;
 import java.util.List;
-import java.util.Locale;
 import java.util.UUID;
 import java.util.stream.Collectors;
 import org.apache.hadoop.conf.Configuration;
@@ -181,7 +180,7 @@ public void writeUnpartitionedTable() throws IOException {
     Table table = TABLES.create(SCHEMA, PartitionSpec.unpartitioned(), unpartitioned.toString());
     Schema tableSchema = table.schema(); // use the table schema because ids are reassigned
 
-    FileFormat fileFormat = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));
+    FileFormat fileFormat = FileFormat.fromString(format);
 
     File testFile = new File(dataFolder, fileFormat.addExtension(UUID.randomUUID().toString()));
 

File: spark/v3.1/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDataWrite.java
Patch:
@@ -27,7 +27,6 @@
 import java.io.File;
 import java.io.IOException;
 import java.util.List;
-import java.util.Locale;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.iceberg.AppendFiles;
 import org.apache.iceberg.AssertHelpers;
@@ -95,7 +94,7 @@ public static void stopSpark() {
   }
 
   public TestSparkDataWrite(String format) {
-    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));
+    this.format = FileFormat.fromString(format);
   }
 
   @Test

File: spark/v3.1/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReadProjection.java
Patch:
@@ -25,7 +25,6 @@
 import java.io.File;
 import java.io.IOException;
 import java.util.List;
-import java.util.Locale;
 import java.util.Map;
 import java.util.UUID;
 import org.apache.iceberg.DataFile;
@@ -75,7 +74,7 @@ public static Object[][] parameters() {
 
   public TestSparkReadProjection(String format, boolean vectorized) {
     super(format);
-    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ROOT));
+    this.format = FileFormat.fromString(format);
     this.vectorized = vectorized;
   }
 

File: spark/v3.1/spark/src/test/java/org/apache/iceberg/spark/source/TestTimestampWithoutZone.java
Patch:
@@ -24,7 +24,6 @@
 import java.io.IOException;
 import java.time.LocalDateTime;
 import java.util.List;
-import java.util.Locale;
 import java.util.UUID;
 import java.util.stream.Collectors;
 import java.util.stream.Stream;
@@ -122,7 +121,7 @@ public void writeUnpartitionedTable() throws IOException {
     Table table = TABLES.create(SCHEMA, PartitionSpec.unpartitioned(), unpartitioned.toString());
     Schema tableSchema = table.schema(); // use the table schema because ids are reassigned
 
-    FileFormat fileFormat = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));
+    FileFormat fileFormat = FileFormat.fromString(format);
 
     File testFile = new File(dataFolder, fileFormat.addExtension(UUID.randomUUID().toString()));
 

File: spark/v3.2/spark/src/main/java/org/apache/iceberg/spark/SparkDataFile.java
Patch:
@@ -20,7 +20,6 @@
 
 import java.nio.ByteBuffer;
 import java.util.List;
-import java.util.Locale;
 import java.util.Map;
 import org.apache.iceberg.DataFile;
 import org.apache.iceberg.FileFormat;
@@ -126,8 +125,7 @@ public CharSequence path() {
 
   @Override
   public FileFormat format() {
-    String formatAsString = wrapped.getString(fileFormatPosition).toUpperCase(Locale.ROOT);
-    return FileFormat.valueOf(formatAsString);
+    return FileFormat.fromString(wrapped.getString(fileFormatPosition));
   }
 
   @Override

File: spark/v3.2/spark/src/main/java/org/apache/iceberg/spark/source/RowDataRewriter.java
Patch:
@@ -21,7 +21,6 @@
 import java.io.Serializable;
 import java.util.Collection;
 import java.util.List;
-import java.util.Locale;
 import java.util.Map;
 import java.util.stream.Collectors;
 import org.apache.iceberg.CombinedScanTask;
@@ -66,7 +65,7 @@ public RowDataRewriter(
             .properties()
             .getOrDefault(
                 TableProperties.DEFAULT_FILE_FORMAT, TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);
-    this.format = FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));
+    this.format = FileFormat.fromString(formatString);
   }
 
   public List<DataFile> rewriteDataForTasks(JavaRDD<CombinedScanTask> taskRDD) {

File: spark/v3.2/spark/src/main/java/org/apache/iceberg/spark/source/SparkFileWriterFactory.java
Patch:
@@ -23,7 +23,6 @@
 import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;
 import static org.apache.iceberg.TableProperties.DELETE_DEFAULT_FILE_FORMAT;
 
-import java.util.Locale;
 import java.util.Map;
 import org.apache.iceberg.FileFormat;
 import org.apache.iceberg.Schema;
@@ -190,11 +189,11 @@ static class Builder {
 
       String dataFileFormatName =
           properties.getOrDefault(DEFAULT_FILE_FORMAT, DEFAULT_FILE_FORMAT_DEFAULT);
-      this.dataFileFormat = FileFormat.valueOf(dataFileFormatName.toUpperCase(Locale.ENGLISH));
+      this.dataFileFormat = FileFormat.fromString(dataFileFormatName);
 
       String deleteFileFormatName =
           properties.getOrDefault(DELETE_DEFAULT_FILE_FORMAT, dataFileFormatName);
-      this.deleteFileFormat = FileFormat.valueOf(deleteFileFormatName.toUpperCase(Locale.ENGLISH));
+      this.deleteFileFormat = FileFormat.fromString(deleteFileFormatName);
     }
 
     Builder dataFileFormat(FileFormat newDataFileFormat) {

File: spark/v3.2/spark/src/test/java/org/apache/iceberg/spark/source/TestFilteredScan.java
Patch:
@@ -28,7 +28,6 @@
 import java.sql.Timestamp;
 import java.time.OffsetDateTime;
 import java.util.List;
-import java.util.Locale;
 import java.util.UUID;
 import java.util.stream.Collectors;
 import org.apache.hadoop.conf.Configuration;
@@ -181,7 +180,7 @@ public void writeUnpartitionedTable() throws IOException {
     Table table = TABLES.create(SCHEMA, PartitionSpec.unpartitioned(), unpartitioned.toString());
     Schema tableSchema = table.schema(); // use the table schema because ids are reassigned
 
-    FileFormat fileFormat = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));
+    FileFormat fileFormat = FileFormat.fromString(format);
 
     File testFile = new File(dataFolder, fileFormat.addExtension(UUID.randomUUID().toString()));
 

File: spark/v3.2/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDataWrite.java
Patch:
@@ -27,7 +27,6 @@
 import java.io.File;
 import java.io.IOException;
 import java.util.List;
-import java.util.Locale;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.iceberg.AppendFiles;
 import org.apache.iceberg.AssertHelpers;
@@ -95,7 +94,7 @@ public static void stopSpark() {
   }
 
   public TestSparkDataWrite(String format) {
-    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));
+    this.format = FileFormat.fromString(format);
   }
 
   @Test

File: spark/v3.2/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReadProjection.java
Patch:
@@ -25,7 +25,6 @@
 import java.io.File;
 import java.io.IOException;
 import java.util.List;
-import java.util.Locale;
 import java.util.Map;
 import java.util.UUID;
 import org.apache.iceberg.DataFile;
@@ -75,7 +74,7 @@ public static Object[][] parameters() {
 
   public TestSparkReadProjection(String format, boolean vectorized) {
     super(format);
-    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ROOT));
+    this.format = FileFormat.fromString(format);
     this.vectorized = vectorized;
   }
 

File: spark/v3.2/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReaderWithBloomFilter.java
Patch:
@@ -30,7 +30,6 @@
 import java.math.BigDecimal;
 import java.time.LocalDate;
 import java.util.List;
-import java.util.Locale;
 import java.util.Map;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.iceberg.BaseTable;
@@ -332,7 +331,7 @@ private DataFile writeDataFile(OutputFile out, StructLike partition, List<Record
 
   private FileFormat defaultFormat(Map<String, String> properties) {
     String formatString = properties.getOrDefault(DEFAULT_FILE_FORMAT, DEFAULT_FILE_FORMAT_DEFAULT);
-    return FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));
+    return FileFormat.fromString(formatString);
   }
 
   @Test

File: spark/v3.2/spark/src/test/java/org/apache/iceberg/spark/source/TestTimestampWithoutZone.java
Patch:
@@ -24,7 +24,6 @@
 import java.io.IOException;
 import java.time.LocalDateTime;
 import java.util.List;
-import java.util.Locale;
 import java.util.UUID;
 import java.util.stream.Collectors;
 import java.util.stream.Stream;
@@ -122,7 +121,7 @@ public void writeUnpartitionedTable() throws IOException {
     Table table = TABLES.create(SCHEMA, PartitionSpec.unpartitioned(), unpartitioned.toString());
     Schema tableSchema = table.schema(); // use the table schema because ids are reassigned
 
-    FileFormat fileFormat = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));
+    FileFormat fileFormat = FileFormat.fromString(format);
 
     File testFile = new File(dataFolder, fileFormat.addExtension(UUID.randomUUID().toString()));
 

File: spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/SparkDataFile.java
Patch:
@@ -20,7 +20,6 @@
 
 import java.nio.ByteBuffer;
 import java.util.List;
-import java.util.Locale;
 import java.util.Map;
 import org.apache.iceberg.DataFile;
 import org.apache.iceberg.FileFormat;
@@ -126,8 +125,7 @@ public CharSequence path() {
 
   @Override
   public FileFormat format() {
-    String formatAsString = wrapped.getString(fileFormatPosition).toUpperCase(Locale.ROOT);
-    return FileFormat.valueOf(formatAsString);
+    return FileFormat.fromString(wrapped.getString(fileFormatPosition));
   }
 
   @Override

File: spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/source/RowDataRewriter.java
Patch:
@@ -21,7 +21,6 @@
 import java.io.Serializable;
 import java.util.Collection;
 import java.util.List;
-import java.util.Locale;
 import java.util.Map;
 import java.util.stream.Collectors;
 import org.apache.iceberg.CombinedScanTask;
@@ -66,7 +65,7 @@ public RowDataRewriter(
             .properties()
             .getOrDefault(
                 TableProperties.DEFAULT_FILE_FORMAT, TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);
-    this.format = FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));
+    this.format = FileFormat.fromString(formatString);
   }
 
   public List<DataFile> rewriteDataForTasks(JavaRDD<CombinedScanTask> taskRDD) {

File: spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/source/SparkFileWriterFactory.java
Patch:
@@ -23,7 +23,6 @@
 import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;
 import static org.apache.iceberg.TableProperties.DELETE_DEFAULT_FILE_FORMAT;
 
-import java.util.Locale;
 import java.util.Map;
 import org.apache.iceberg.FileFormat;
 import org.apache.iceberg.Schema;
@@ -190,11 +189,11 @@ static class Builder {
 
       String dataFileFormatName =
           properties.getOrDefault(DEFAULT_FILE_FORMAT, DEFAULT_FILE_FORMAT_DEFAULT);
-      this.dataFileFormat = FileFormat.valueOf(dataFileFormatName.toUpperCase(Locale.ENGLISH));
+      this.dataFileFormat = FileFormat.fromString(dataFileFormatName);
 
       String deleteFileFormatName =
           properties.getOrDefault(DELETE_DEFAULT_FILE_FORMAT, dataFileFormatName);
-      this.deleteFileFormat = FileFormat.valueOf(deleteFileFormatName.toUpperCase(Locale.ENGLISH));
+      this.deleteFileFormat = FileFormat.fromString(deleteFileFormatName);
     }
 
     Builder dataFileFormat(FileFormat newDataFileFormat) {

File: spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/source/TestFilteredScan.java
Patch:
@@ -28,7 +28,6 @@
 import java.sql.Timestamp;
 import java.time.OffsetDateTime;
 import java.util.List;
-import java.util.Locale;
 import java.util.UUID;
 import java.util.function.Function;
 import java.util.stream.Collectors;
@@ -181,7 +180,7 @@ public void writeUnpartitionedTable() throws IOException {
     Table table = TABLES.create(SCHEMA, PartitionSpec.unpartitioned(), unpartitioned.toString());
     Schema tableSchema = table.schema(); // use the table schema because ids are reassigned
 
-    FileFormat fileFormat = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));
+    FileFormat fileFormat = FileFormat.fromString(format);
 
     File testFile = new File(dataFolder, fileFormat.addExtension(UUID.randomUUID().toString()));
 

File: spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDataWrite.java
Patch:
@@ -27,7 +27,6 @@
 import java.io.File;
 import java.io.IOException;
 import java.util.List;
-import java.util.Locale;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.iceberg.AppendFiles;
 import org.apache.iceberg.AssertHelpers;
@@ -95,7 +94,7 @@ public static void stopSpark() {
   }
 
   public TestSparkDataWrite(String format) {
-    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));
+    this.format = FileFormat.fromString(format);
   }
 
   @Test

File: spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReadProjection.java
Patch:
@@ -25,7 +25,6 @@
 import java.io.File;
 import java.io.IOException;
 import java.util.List;
-import java.util.Locale;
 import java.util.Map;
 import java.util.UUID;
 import org.apache.iceberg.DataFile;
@@ -75,7 +74,7 @@ public static Object[][] parameters() {
 
   public TestSparkReadProjection(String format, boolean vectorized) {
     super(format);
-    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ROOT));
+    this.format = FileFormat.fromString(format);
     this.vectorized = vectorized;
   }
 

File: spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReaderWithBloomFilter.java
Patch:
@@ -30,7 +30,6 @@
 import java.math.BigDecimal;
 import java.time.LocalDate;
 import java.util.List;
-import java.util.Locale;
 import java.util.Map;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.iceberg.BaseTable;
@@ -332,7 +331,7 @@ private DataFile writeDataFile(OutputFile out, StructLike partition, List<Record
 
   private FileFormat defaultFormat(Map<String, String> properties) {
     String formatString = properties.getOrDefault(DEFAULT_FILE_FORMAT, DEFAULT_FILE_FORMAT_DEFAULT);
-    return FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));
+    return FileFormat.fromString(formatString);
   }
 
   @Test

File: spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/source/TestTimestampWithoutZone.java
Patch:
@@ -24,7 +24,6 @@
 import java.io.IOException;
 import java.time.LocalDateTime;
 import java.util.List;
-import java.util.Locale;
 import java.util.UUID;
 import java.util.stream.Collectors;
 import java.util.stream.Stream;
@@ -122,7 +121,7 @@ public void writeUnpartitionedTable() throws IOException {
     Table table = TABLES.create(SCHEMA, PartitionSpec.unpartitioned(), unpartitioned.toString());
     Schema tableSchema = table.schema(); // use the table schema because ids are reassigned
 
-    FileFormat fileFormat = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));
+    FileFormat fileFormat = FileFormat.fromString(format);
 
     File testFile = new File(dataFolder, fileFormat.addExtension(UUID.randomUUID().toString()));
 

File: api/src/main/java/org/apache/iceberg/ManifestFile.java
Patch:
@@ -126,7 +126,7 @@ static Schema schema() {
   /** Returns the sequence number of the commit that added the manifest file. */
   long sequenceNumber();
 
-  /** Returns the lowest sequence number of any data file in the manifest. */
+  /** Returns the lowest data sequence number of any live file in the manifest. */
   long minSequenceNumber();
 
   /** Returns iD of the snapshot that added the manifest file to table metadata. */

File: core/src/test/java/org/apache/iceberg/TestManifestWriterVersions.java
Patch:
@@ -197,6 +197,7 @@ public void testV2ManifestRewriteWithInheritance() throws IOException {
   void checkEntry(ManifestEntry<?> entry, Long expectedSequenceNumber, FileContent content) {
     Assert.assertEquals("Status", ManifestEntry.Status.ADDED, entry.status());
     Assert.assertEquals("Snapshot ID", (Long) SNAPSHOT_ID, entry.snapshotId());
+    Assert.assertEquals("Data sequence number", expectedSequenceNumber, entry.dataSequenceNumber());
     Assert.assertEquals("Sequence number", expectedSequenceNumber, entry.sequenceNumber());
     checkDataFile(entry.file(), content);
   }
@@ -205,6 +206,7 @@ void checkRewrittenEntry(
       ManifestEntry<DataFile> entry, Long expectedSequenceNumber, FileContent content) {
     Assert.assertEquals("Status", ManifestEntry.Status.EXISTING, entry.status());
     Assert.assertEquals("Snapshot ID", (Long) SNAPSHOT_ID, entry.snapshotId());
+    Assert.assertEquals("Data sequence number", expectedSequenceNumber, entry.dataSequenceNumber());
     Assert.assertEquals("Sequence number", expectedSequenceNumber, entry.sequenceNumber());
     checkDataFile(entry.file(), content);
   }

File: core/src/test/java/org/apache/iceberg/TestMergeAppend.java
Patch:
@@ -580,7 +580,7 @@ public void testMergeWithExistingManifestAfterDelete() {
 
     validateManifest(
         deleteManifest,
-        seqs(2, 1),
+        seqs(1, 1),
         ids(deleteId, baseId),
         files(FILE_A, FILE_B),
         statuses(Status.DELETED, Status.EXISTING));

File: core/src/test/java/org/apache/iceberg/TestRewriteManifests.java
Patch:
@@ -962,7 +962,7 @@ public void testInvalidUsage() throws IOException {
     ManifestEntry<DataFile> appendEntry =
         manifestEntry(ManifestEntry.Status.ADDED, snapshot.snapshotId(), FILE_A);
     // update the entry's sequence number or else it will be rejected by the writer
-    appendEntry.setSequenceNumber(snapshot.sequenceNumber());
+    appendEntry.setDataSequenceNumber(snapshot.sequenceNumber());
 
     ManifestFile invalidAddedFileManifest = writeManifest("manifest-file-2.avro", appendEntry);
 
@@ -980,7 +980,7 @@ public void testInvalidUsage() throws IOException {
     ManifestEntry<DataFile> deleteEntry =
         manifestEntry(ManifestEntry.Status.DELETED, snapshot.snapshotId(), FILE_A);
     // update the entry's sequence number or else it will be rejected by the writer
-    deleteEntry.setSequenceNumber(snapshot.sequenceNumber());
+    deleteEntry.setDataSequenceNumber(snapshot.sequenceNumber());
 
     ManifestFile invalidDeletedFileManifest = writeManifest("manifest-file-3.avro", deleteEntry);
 

File: core/src/test/java/org/apache/iceberg/TestSequenceNumberForV2Table.java
Patch:
@@ -69,12 +69,12 @@ public void testRewrite() {
     for (ManifestEntry<DataFile> entry : ManifestFiles.read(newManifest, FILE_IO).entries()) {
       if (entry.file().path().equals(FILE_A.path())) {
         V2Assert.assertEquals(
-            "FILE_A sequence number should be 1", 1, entry.sequenceNumber().longValue());
+            "FILE_A sequence number should be 1", 1, entry.dataSequenceNumber().longValue());
       }
 
       if (entry.file().path().equals(FILE_B.path())) {
         V2Assert.assertEquals(
-            "FILE_b sequence number should be 2", 2, entry.sequenceNumber().longValue());
+            "FILE_b sequence number should be 2", 2, entry.dataSequenceNumber().longValue());
       }
     }
   }
@@ -238,7 +238,7 @@ public void testConcurrentTransaction() {
             .collect(Collectors.toList())
             .get(0);
     validateManifest(
-        manifestFile, seqs(4), ids(commitId4), files(FILE_A), statuses(Status.DELETED));
+        manifestFile, seqs(1), ids(commitId4), files(FILE_A), statuses(Status.DELETED));
     V2Assert.assertEquals("Snapshot sequence number should be 4", 4, snap4.sequenceNumber());
     V2Assert.assertEquals(
         "Last sequence number should be 4", 4, readMetadata().lastSequenceNumber());

File: core/src/main/java/org/apache/iceberg/MetadataUpdateParser.java
Patch:
@@ -461,8 +461,7 @@ private static MetadataUpdate readRemoveSnapshots(JsonNode node) {
   private static MetadataUpdate readSetSnapshotRef(JsonNode node) {
     String refName = JsonUtil.getString(REF_NAME, node);
     long snapshotId = JsonUtil.getLong(SNAPSHOT_ID, node);
-    SnapshotRefType type =
-        SnapshotRefType.valueOf(JsonUtil.getString(TYPE, node).toUpperCase(Locale.ENGLISH));
+    SnapshotRefType type = SnapshotRefType.fromString(JsonUtil.getString(TYPE, node));
     Integer minSnapshotsToKeep = JsonUtil.getIntOrNull(MIN_SNAPSHOTS_TO_KEEP, node);
     Long maxSnapshotAgeMs = JsonUtil.getLongOrNull(MAX_SNAPSHOT_AGE_MS, node);
     Long maxRefAgeMs = JsonUtil.getLongOrNull(MAX_REF_AGE_MS, node);

File: core/src/main/java/org/apache/iceberg/SnapshotRefParser.java
Patch:
@@ -74,8 +74,7 @@ public static SnapshotRef fromJson(JsonNode node) {
     Preconditions.checkArgument(
         node.isObject(), "Cannot parse snapshot reference from a non-object: %s", node);
     long snapshotId = JsonUtil.getLong(SNAPSHOT_ID, node);
-    SnapshotRefType type =
-        SnapshotRefType.valueOf(JsonUtil.getString(TYPE, node).toUpperCase(Locale.ENGLISH));
+    SnapshotRefType type = SnapshotRefType.fromString(JsonUtil.getString(TYPE, node));
     Integer minSnapshotsToKeep = JsonUtil.getIntOrNull(MIN_SNAPSHOTS_TO_KEEP, node);
     Long maxSnapshotAgeMs = JsonUtil.getLongOrNull(MAX_SNAPSHOT_AGE_MS, node);
     Long maxRefAgeMs = JsonUtil.getLongOrNull(MAX_REF_AGE_MS, node);

File: core/src/main/java/org/apache/iceberg/expressions/ExpressionParser.java
Patch:
@@ -230,7 +230,7 @@ private void unboundLiteral(Object object) throws IOException {
     }
 
     private String operationType(Expression.Operation op) {
-      return op.toString().replaceAll("_", "-").toLowerCase(Locale.ROOT);
+      return op.toString().replaceAll("_", "-").toLowerCase(Locale.ENGLISH);
     }
 
     private void term(Term term) throws IOException {
@@ -312,7 +312,7 @@ static Expression fromJson(JsonNode json, Schema schema) {
   }
 
   private static Expression.Operation fromType(String type) {
-    return Expression.Operation.valueOf(type.replaceAll("-", "_").toUpperCase(Locale.ROOT));
+    return Expression.Operation.fromString(type.replaceAll("-", "_"));
   }
 
   @SuppressWarnings("unchecked")

File: core/src/main/java/org/apache/iceberg/rest/requests/ReportMetricsRequestParser.java
Patch:
@@ -57,11 +57,11 @@ public static void toJson(ReportMetricsRequest request, JsonGenerator gen) throw
   }
 
   private static String fromReportType(ReportType reportType) {
-    return reportType.name().replaceAll("_", "-").toLowerCase(Locale.ROOT);
+    return reportType.name().replaceAll("_", "-").toLowerCase(Locale.ENGLISH);
   }
 
   private static ReportType toReportType(String type) {
-    return ReportType.valueOf(type.replaceAll("-", "_").toUpperCase(Locale.ROOT));
+    return ReportType.fromString(type.replaceAll("-", "_"));
   }
 
   public static ReportMetricsRequest fromJson(String json) {

File: core/src/test/java/org/apache/iceberg/TableMetadataParserCodecTest.java
Patch:
@@ -42,7 +42,7 @@ public void testCompressionCodec() {
   public void testInvalidCodecName() {
     Assertions.assertThatThrownBy(() -> Codec.fromName("invalid"))
         .isInstanceOf(IllegalArgumentException.class)
-        .hasMessage("No enum constant org.apache.iceberg.TableMetadataParser.Codec.INVALID");
+        .hasMessage("Invalid codec name: invalid");
   }
 
   @Test

File: core/src/test/java/org/apache/iceberg/TestSnapshotRefParser.java
Patch:
@@ -151,7 +151,7 @@ public void testFailWhenFieldsHaveInvalidValues() {
     AssertHelpers.assertThrows(
         "SnapshotRefParser should fail to deserialize ref with invalid tag",
         IllegalArgumentException.class,
-        "No enum constant",
+        "Invalid snapshot ref type: not-a-valid-tag-type",
         () -> SnapshotRefParser.fromJson(invalidTagType));
 
     String invalidRefAge =

File: core/src/test/java/org/apache/iceberg/expressions/TestExpressionParser.java
Patch:
@@ -301,7 +301,7 @@ public void invalidOperationType() {
                         + "  }\n"
                         + "}"))
         .isInstanceOf(IllegalArgumentException.class)
-        .hasMessage("No enum constant org.apache.iceberg.expressions.Expression.Operation.ILLEGAL");
+        .hasMessage("Invalid operation type: illegal");
 
     Assertions.assertThatThrownBy(
             () ->
@@ -315,7 +315,7 @@ public void invalidOperationType() {
                         + "  }\n"
                         + "}"))
         .isInstanceOf(IllegalArgumentException.class)
-        .hasMessage("No enum constant org.apache.iceberg.expressions.Expression.Operation.ILLEGAL");
+        .hasMessage("Invalid operation type: ILLEGAL");
   }
 
   @Test

File: core/src/test/java/org/apache/iceberg/metrics/TestCounterResultParser.java
Patch:
@@ -60,7 +60,7 @@ public void unsupportedUnit() {
     Assertions.assertThatThrownBy(
             () -> CounterResultParser.fromJson("{\"unit\":\"unknown\",\"value\":23}"))
         .isInstanceOf(IllegalArgumentException.class)
-        .hasMessage("No enum constant org.apache.iceberg.metrics.MetricsContext.Unit.UNKNOWN");
+        .hasMessage("Invalid unit: unknown");
   }
 
   @Test

File: core/src/test/java/org/apache/iceberg/metrics/TestTimerResultParser.java
Patch:
@@ -80,7 +80,7 @@ public void unsupportedTimeUnit() {
                 TimerResultParser.fromJson(
                     "{\"count\":44,\"time-unit\":\"unknown\",\"total-duration\":24}"))
         .isInstanceOf(IllegalArgumentException.class)
-        .hasMessage("No enum constant java.util.concurrent.TimeUnit.UNKNOWN");
+        .hasMessage("Invalid time unit: unknown");
   }
 
   @Test

File: core/src/test/java/org/apache/iceberg/rest/requests/TestReportMetricsRequestParser.java
Patch:
@@ -67,8 +67,7 @@ public void invalidReportType() {
     Assertions.assertThatThrownBy(
             () -> ReportMetricsRequestParser.fromJson("{\"report-type\":\"invalid\"}"))
         .isInstanceOf(IllegalArgumentException.class)
-        .hasMessage(
-            "No enum constant org.apache.iceberg.rest.requests.ReportMetricsRequest.ReportType.INVALID");
+        .hasMessage("Invalid report type: invalid");
 
     Assertions.assertThatThrownBy(
             () ->

File: flink/v1.13/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSink.java
Patch:
@@ -350,7 +350,7 @@ public void testOverrideWriteConfigWithUnknownDistributionMode() {
     AssertHelpers.assertThrows(
         "Should fail with invalid distribution mode.",
         IllegalArgumentException.class,
-        "No enum constant org.apache.iceberg.DistributionMode.UNRECOGNIZED",
+        "Invalid distribution mode: UNRECOGNIZED",
         () -> {
           builder.append();
 

File: flink/v1.14/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSink.java
Patch:
@@ -362,7 +362,7 @@ public void testOverrideWriteConfigWithUnknownDistributionMode() {
     AssertHelpers.assertThrows(
         "Should fail with invalid distribution mode.",
         IllegalArgumentException.class,
-        "No enum constant org.apache.iceberg.DistributionMode.UNRECOGNIZED",
+        "Invalid distribution mode: UNRECOGNIZED",
         () -> {
           builder.append();
 

File: flink/v1.15/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSink.java
Patch:
@@ -361,7 +361,7 @@ public void testOverrideWriteConfigWithUnknownDistributionMode() {
     AssertHelpers.assertThrows(
         "Should fail with invalid distribution mode.",
         IllegalArgumentException.class,
-        "No enum constant org.apache.iceberg.DistributionMode.UNRECOGNIZED",
+        "Invalid distribution mode: UNRECOGNIZED",
         () -> {
           builder.append();
 

File: spark/v3.0/spark/src/main/java/org/apache/iceberg/spark/source/SparkMergeBuilder.java
Patch:
@@ -25,7 +25,6 @@
 import static org.apache.iceberg.TableProperties.UPDATE_ISOLATION_LEVEL;
 import static org.apache.iceberg.TableProperties.UPDATE_ISOLATION_LEVEL_DEFAULT;
 
-import java.util.Locale;
 import java.util.Map;
 import org.apache.iceberg.IsolationLevel;
 import org.apache.iceberg.Table;
@@ -70,7 +69,7 @@ private IsolationLevel getIsolationLevel(Map<String, String> props, String opera
     } else {
       throw new IllegalArgumentException("Unsupported operation: " + operation);
     }
-    return IsolationLevel.valueOf(isolationLevelAsString.toUpperCase(Locale.ROOT));
+    return IsolationLevel.fromName(isolationLevelAsString);
   }
 
   @Override

File: spark/v3.1/spark/src/main/java/org/apache/iceberg/spark/source/SparkMergeBuilder.java
Patch:
@@ -25,7 +25,6 @@
 import static org.apache.iceberg.TableProperties.UPDATE_ISOLATION_LEVEL;
 import static org.apache.iceberg.TableProperties.UPDATE_ISOLATION_LEVEL_DEFAULT;
 
-import java.util.Locale;
 import java.util.Map;
 import org.apache.iceberg.IsolationLevel;
 import org.apache.iceberg.Table;
@@ -70,7 +69,7 @@ private IsolationLevel getIsolationLevel(Map<String, String> props, String opera
     } else {
       throw new IllegalArgumentException("Unsupported operation: " + operation);
     }
-    return IsolationLevel.valueOf(isolationLevelAsString.toUpperCase(Locale.ROOT));
+    return IsolationLevel.fromName(isolationLevelAsString);
   }
 
   @Override

File: aliyun/src/main/java/org/apache/iceberg/aliyun/oss/OSSInputStream.java
Patch:
@@ -60,7 +60,7 @@ class OSSInputStream extends SeekableInputStream {
     this.createStack = Thread.currentThread().getStackTrace();
 
     this.readBytes = metrics.counter(FileIOMetricsContext.READ_BYTES, Unit.BYTES);
-    this.readOperations = metrics.counter(FileIOMetricsContext.READ_OPERATIONS, Unit.COUNT);
+    this.readOperations = metrics.counter(FileIOMetricsContext.READ_OPERATIONS);
   }
 
   @Override

File: aliyun/src/main/java/org/apache/iceberg/aliyun/oss/OSSOutputStream.java
Patch:
@@ -67,7 +67,7 @@ public class OSSOutputStream extends PositionOutputStream {
     this.currentStagingFile = newStagingFile(aliyunProperties.ossStagingDirectory());
     this.stream = newStream(currentStagingFile);
     this.writeBytes = metrics.counter(FileIOMetricsContext.WRITE_BYTES, Unit.BYTES);
-    this.writeOperations = metrics.counter(FileIOMetricsContext.WRITE_OPERATIONS, Unit.COUNT);
+    this.writeOperations = metrics.counter(FileIOMetricsContext.WRITE_OPERATIONS);
   }
 
   private static File newStagingFile(String ossStagingDirectory) {

File: aws/src/main/java/org/apache/iceberg/aws/s3/S3InputStream.java
Patch:
@@ -68,7 +68,7 @@ class S3InputStream extends SeekableInputStream implements RangeReadable {
     this.awsProperties = awsProperties;
 
     this.readBytes = metrics.counter(FileIOMetricsContext.READ_BYTES, Unit.BYTES);
-    this.readOperations = metrics.counter(FileIOMetricsContext.READ_OPERATIONS, Unit.COUNT);
+    this.readOperations = metrics.counter(FileIOMetricsContext.READ_OPERATIONS);
 
     this.createStack = Thread.currentThread().getStackTrace();
   }

File: aws/src/main/java/org/apache/iceberg/aws/s3/S3OutputStream.java
Patch:
@@ -146,7 +146,7 @@ class S3OutputStream extends PositionOutputStream {
     }
 
     this.writeBytes = metrics.counter(FileIOMetricsContext.WRITE_BYTES, Unit.BYTES);
-    this.writeOperations = metrics.counter(FileIOMetricsContext.WRITE_OPERATIONS, Unit.COUNT);
+    this.writeOperations = metrics.counter(FileIOMetricsContext.WRITE_OPERATIONS);
 
     newStream();
   }

File: dell/src/main/java/org/apache/iceberg/dell/ecs/EcsAppendOutputStream.java
Patch:
@@ -57,7 +57,7 @@ private EcsAppendOutputStream(
     this.uri = uri;
     this.localCache = ByteBuffer.wrap(localCache);
     this.writeBytes = metrics.counter(FileIOMetricsContext.WRITE_BYTES, Unit.BYTES);
-    this.writeOperations = metrics.counter(FileIOMetricsContext.WRITE_OPERATIONS, Unit.COUNT);
+    this.writeOperations = metrics.counter(FileIOMetricsContext.WRITE_OPERATIONS);
   }
 
   /** Use built-in 1 KiB byte buffer */

File: dell/src/main/java/org/apache/iceberg/dell/ecs/EcsSeekableInputStream.java
Patch:
@@ -58,7 +58,7 @@ class EcsSeekableInputStream extends SeekableInputStream {
     this.client = client;
     this.uri = uri;
     this.readBytes = metrics.counter(FileIOMetricsContext.READ_BYTES, Unit.BYTES);
-    this.readOperations = metrics.counter(FileIOMetricsContext.READ_OPERATIONS, Unit.COUNT);
+    this.readOperations = metrics.counter(FileIOMetricsContext.READ_OPERATIONS);
   }
 
   @Override

File: gcp/src/main/java/org/apache/iceberg/gcp/gcs/GCSInputStream.java
Patch:
@@ -67,7 +67,7 @@ class GCSInputStream extends SeekableInputStream {
     this.gcpProperties = gcpProperties;
 
     this.readBytes = metrics.counter(FileIOMetricsContext.READ_BYTES, Unit.BYTES);
-    this.readOperations = metrics.counter(FileIOMetricsContext.READ_OPERATIONS, Unit.COUNT);
+    this.readOperations = metrics.counter(FileIOMetricsContext.READ_OPERATIONS);
 
     createStack = Thread.currentThread().getStackTrace();
 

File: gcp/src/main/java/org/apache/iceberg/gcp/gcs/GCSOutputStream.java
Patch:
@@ -69,7 +69,7 @@ class GCSOutputStream extends PositionOutputStream {
     createStack = Thread.currentThread().getStackTrace();
 
     this.writeBytes = metrics.counter(FileIOMetricsContext.WRITE_BYTES, Unit.BYTES);
-    this.writeOperations = metrics.counter(FileIOMetricsContext.WRITE_OPERATIONS, Unit.COUNT);
+    this.writeOperations = metrics.counter(FileIOMetricsContext.WRITE_OPERATIONS);
 
     openStream();
   }

File: spark/v3.3/spark/src/jmh/java/org/apache/iceberg/spark/data/parquet/SparkParquetReadersFlatDataBenchmark.java
Patch:
@@ -154,6 +154,7 @@ public void readUsingSparkReader(Blackhole blackhole) throws IOException {
             .set("org.apache.spark.sql.parquet.row.requested_schema", sparkSchema.json())
             .set("spark.sql.parquet.binaryAsString", "false")
             .set("spark.sql.parquet.int96AsTimestamp", "false")
+            .set("spark.sql.caseSensitive", "false")
             .callInit()
             .build()) {
 
@@ -211,6 +212,7 @@ public void readWithProjectionUsingSparkReader(Blackhole blackhole) throws IOExc
             .set("org.apache.spark.sql.parquet.row.requested_schema", sparkSchema.json())
             .set("spark.sql.parquet.binaryAsString", "false")
             .set("spark.sql.parquet.int96AsTimestamp", "false")
+            .set("spark.sql.caseSensitive", "false")
             .callInit()
             .build()) {
 

File: spark/v3.3/spark/src/jmh/java/org/apache/iceberg/spark/data/parquet/SparkParquetReadersNestedDataBenchmark.java
Patch:
@@ -152,6 +152,7 @@ public void readUsingSparkReader(Blackhole blackhole) throws IOException {
             .set("org.apache.spark.sql.parquet.row.requested_schema", sparkSchema.json())
             .set("spark.sql.parquet.binaryAsString", "false")
             .set("spark.sql.parquet.int96AsTimestamp", "false")
+            .set("spark.sql.caseSensitive", "false")
             .callInit()
             .build()) {
 
@@ -209,6 +210,7 @@ public void readWithProjectionUsingSparkReader(Blackhole blackhole) throws IOExc
             .set("org.apache.spark.sql.parquet.row.requested_schema", sparkSchema.json())
             .set("spark.sql.parquet.binaryAsString", "false")
             .set("spark.sql.parquet.int96AsTimestamp", "false")
+            .set("spark.sql.caseSensitive", "false")
             .callInit()
             .build()) {
 

File: spark/v3.3/spark/src/jmh/java/org/apache/iceberg/spark/data/parquet/SparkParquetWritersFlatDataBenchmark.java
Patch:
@@ -119,6 +119,7 @@ public void writeUsingSparkWriter() throws IOException {
             .set("spark.sql.parquet.binaryAsString", "false")
             .set("spark.sql.parquet.int96AsTimestamp", "false")
             .set("spark.sql.parquet.outputTimestampType", "TIMESTAMP_MICROS")
+            .set("spark.sql.caseSensitive", "false")
             .schema(SCHEMA)
             .build()) {
 

File: spark/v3.3/spark/src/jmh/java/org/apache/iceberg/spark/data/parquet/SparkParquetWritersNestedDataBenchmark.java
Patch:
@@ -119,6 +119,7 @@ public void writeUsingSparkWriter() throws IOException {
             .set("spark.sql.parquet.binaryAsString", "false")
             .set("spark.sql.parquet.int96AsTimestamp", "false")
             .set("spark.sql.parquet.outputTimestampType", "TIMESTAMP_MICROS")
+            .set("spark.sql.caseSensitive", "false")
             .schema(SCHEMA)
             .build()) {
 

File: aws/src/integration/java/org/apache/iceberg/aws/lakeformation/LakeFormationTestBase.java
Patch:
@@ -417,7 +417,7 @@ private static void registerResource(String s3Location) {
       // propagation delay
       waitForIamConsistency();
     } catch (AlreadyExistsException e) {
-      LOG.warn("Resource {} already registered. Error: {}", arn, e);
+      LOG.warn("Resource {} already registered. Error: {}", arn, e.getMessage(), e);
     } catch (Exception e) {
       // ignore exception
     }
@@ -429,7 +429,7 @@ private static void deregisterResource(String s3Location) {
       lakeformation.deregisterResource(
           DeregisterResourceRequest.builder().resourceArn(arn).build());
     } catch (EntityNotFoundException e) {
-      LOG.info("Resource {} not found. Error: {}", arn, e);
+      LOG.info("Resource {} not found. Error: {}", arn, e.getMessage(), e);
     }
   }
 
@@ -454,7 +454,7 @@ private static void attachRolePolicyIfNotExists(
           GetRolePolicyRequest.builder().roleName(roleName).policyName(policyName).build());
       LOG.info("Policy {} already attached to role {}", policyName, roleName);
     } catch (NoSuchEntityException e) {
-      LOG.info("Attaching policy {} to role {} {}", policyName, roleName, e);
+      LOG.info("Attaching policy {} to role {}", policyName, roleName, e);
       iam.attachRolePolicy(
           AttachRolePolicyRequest.builder().roleName(roleName).policyArn(policyArn).build());
     }

File: aws/src/main/java/org/apache/iceberg/aws/dynamodb/DynamoDbLockManager.java
Patch:
@@ -278,7 +278,7 @@ public boolean release(String entityId, String ownerId) {
           e);
     } catch (DynamoDbException e) {
       LOG.error(
-          "Failed to release lock {} by for entity: {}, owner: {}, encountered unexpected DynamoDB exception",
+          "Failed to release lock for entity: {}, owner: {}, encountered unexpected DynamoDB exception",
           entityId,
           ownerId,
           e);

File: bundled-guava/src/main/java/org/apache/iceberg/GuavaClasses.java
Patch:
@@ -56,6 +56,7 @@
 
 // inspired in part by
 // https://github.com/apache/avro/blob/release-1.8.2/lang/java/guava/src/main/java/org/apache/avro/GuavaClasses.java
+@SuppressWarnings("ReturnValueIgnored")
 public class GuavaClasses {
 
   /*

File: core/src/main/java/org/apache/iceberg/CatalogUtil.java
Patch:
@@ -169,7 +169,7 @@ private static void deleteFiles(
         SupportsBulkOperations bulkIO = (SupportsBulkOperations) io;
         bulkIO.deleteFiles(files);
       } catch (RuntimeException e) {
-        LOG.warn("Failed to bulk delete {} files: {}", type, e);
+        LOG.warn("Failed to bulk delete {} files", type, e);
       }
     } else {
       if (concurrent) {
@@ -185,15 +185,15 @@ private static void deleteFiles(FileIO io, Iterable<String> files, String type)
         .executeWith(ThreadPools.getWorkerPool())
         .noRetry()
         .suppressFailureWhenFinished()
-        .onFailure((file, exc) -> LOG.warn("Failed to delete {} file {}: {}", type, file, exc))
+        .onFailure((file, exc) -> LOG.warn("Failed to delete {} file {}", type, file, exc))
         .run(io::deleteFile);
   }
 
   private static void deleteFile(FileIO io, String file, String type) {
     try {
       io.deleteFile(file);
     } catch (RuntimeException e) {
-      LOG.warn("Failed to delete {} file {}: {}", type, file, e);
+      LOG.warn("Failed to delete {} file {}", type, file, e);
     }
   }
 

File: core/src/main/java/org/apache/iceberg/hadoop/HadoopCatalog.java
Patch:
@@ -162,7 +162,7 @@ private boolean isTableDir(Path path) {
       return false;
     } catch (IOException e) {
       if (shouldSuppressPermissionError(e)) {
-        LOG.warn("Unable to list metadata directory {}: {}", metadataPath, e);
+        LOG.warn("Unable to list metadata directory {}", metadataPath, e);
         return false;
       } else {
         throw new UncheckedIOException(e);
@@ -177,7 +177,7 @@ private boolean isDirectory(Path path) {
       return false;
     } catch (IOException e) {
       if (shouldSuppressPermissionError(e)) {
-        LOG.warn("Unable to list directory {}: {}", path, e);
+        LOG.warn("Unable to list directory {}", path, e);
         return false;
       } else {
         throw new UncheckedIOException(e);

File: hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveMetastore.java
Patch:
@@ -267,6 +267,9 @@ private void initConf(HiveConf conf, int port) {
     conf.set(HiveConf.ConfVars.METASTORE_TRY_DIRECT_SQL.varname, "false");
     conf.set(HiveConf.ConfVars.METASTORE_DISALLOW_INCOMPATIBLE_COL_TYPE_CHANGES.varname, "false");
     conf.set("iceberg.hive.client-pool-size", "2");
+    // Setting this to avoid thrift exception during running Iceberg tests outside Iceberg.
+    conf.set(
+        HiveConf.ConfVars.HIVE_IN_TEST.varname, HiveConf.ConfVars.HIVE_IN_TEST.getDefaultValue());
   }
 
   private static void setupMetastoreDB(String dbURL) throws SQLException, IOException {

File: core/src/test/java/org/apache/iceberg/TestManifestCaching.java
Patch:
@@ -39,6 +39,7 @@
 import org.apache.iceberg.relocated.com.google.common.collect.Lists;
 import org.apache.iceberg.types.Types;
 import org.junit.Assert;
+import org.junit.Ignore;
 import org.junit.Test;
 
 public class TestManifestCaching extends HadoopTableTestBase {
@@ -162,6 +163,7 @@ public void testRecreateCache() throws Exception {
   }
 
   @Test
+  @Ignore("will be fixed by https://github.com/apache/iceberg/issues/5861")
   public void testWeakFileIOReferenceCleanUp() {
     Cache<FileIO, ContentCache> manifestCache = ManifestFiles.newManifestCache();
     int maxIO = SystemProperties.IO_MANIFEST_CACHE_MAX_FILEIO_DEFAULT;

File: aws/src/integration/java/org/apache/iceberg/aws/TestAssumeRoleAwsClientFactory.java
Patch:
@@ -81,7 +81,8 @@ public void before() {
     assumeRoleProperties.put(
         AwsProperties.CLIENT_FACTORY, AssumeRoleAwsClientFactory.class.getName());
     assumeRoleProperties.put(AwsProperties.HTTP_CLIENT_TYPE, AwsProperties.HTTP_CLIENT_TYPE_APACHE);
-    assumeRoleProperties.put(AwsProperties.CLIENT_ASSUME_ROLE_REGION, "us-east-1");
+    assumeRoleProperties.put(
+        AwsProperties.CLIENT_ASSUME_ROLE_REGION, AwsIntegTestUtil.testRegion());
     assumeRoleProperties.put(AwsProperties.CLIENT_ASSUME_ROLE_ARN, response.role().arn());
     assumeRoleProperties.put(AwsProperties.CLIENT_ASSUME_ROLE_TAGS_PREFIX + "key1", "value1");
     assumeRoleProperties.put(AwsProperties.CLIENT_ASSUME_ROLE_TAGS_PREFIX + "key2", "value2");

File: aws/src/integration/java/org/apache/iceberg/aws/glue/TestGlueCatalogLock.java
Patch:
@@ -35,6 +35,7 @@
 import org.apache.iceberg.aws.dynamodb.DynamoDbLockManager;
 import org.apache.iceberg.aws.s3.S3FileIO;
 import org.apache.iceberg.catalog.TableIdentifier;
+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;
 import org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors;
 import org.apache.iceberg.util.Tasks;
 import org.junit.AfterClass;
@@ -64,7 +65,8 @@ public static void beforeClass() {
         awsProperties,
         glue,
         new DynamoDbLockManager(dynamo, lockTableName),
-        fileIO);
+        fileIO,
+        ImmutableMap.of());
   }
 
   @AfterClass

File: core/src/main/java/org/apache/iceberg/rest/auth/OAuth2Util.java
Patch:
@@ -132,7 +132,7 @@ private static OAuthTokenResponse refreshToken(
             request,
             OAuthTokenResponse.class,
             headers,
-            ErrorHandlers.defaultErrorHandler());
+            ErrorHandlers.oauthErrorHandler());
     response.validate();
 
     return response;
@@ -160,7 +160,7 @@ public static OAuthTokenResponse exchangeToken(
             request,
             OAuthTokenResponse.class,
             headers,
-            ErrorHandlers.defaultErrorHandler());
+            ErrorHandlers.oauthErrorHandler());
     response.validate();
 
     return response;
@@ -178,7 +178,7 @@ public static OAuthTokenResponse fetchToken(
             request,
             OAuthTokenResponse.class,
             headers,
-            ErrorHandlers.defaultErrorHandler());
+            ErrorHandlers.oauthErrorHandler());
     response.validate();
 
     return response;

File: core/src/main/java/org/apache/iceberg/DeleteFileIndex.java
Patch:
@@ -364,7 +364,7 @@ static class Builder {
     private PartitionSet partitionSet = null;
     private boolean caseSensitive = true;
     private ExecutorService executorService = null;
-    private ScanReport.ScanMetrics scanMetrics = ScanReport.ScanMetrics.NOOP;
+    private ScanReport.ScanMetrics scanMetrics = ScanReport.ScanMetrics.noop();
 
     Builder(FileIO io, Set<ManifestFile> deleteManifests) {
       this.io = io;

File: core/src/main/java/org/apache/iceberg/ManifestGroup.java
Patch:
@@ -85,7 +85,7 @@ class ManifestGroup {
     this.caseSensitive = true;
     this.manifestPredicate = m -> true;
     this.manifestEntryPredicate = e -> true;
-    this.scanMetrics = ScanReport.ScanMetrics.NOOP;
+    this.scanMetrics = ScanReport.ScanMetrics.noop();
   }
 
   ManifestGroup specsById(Map<Integer, PartitionSpec> newSpecsById) {

File: core/src/main/java/org/apache/iceberg/ManifestReader.java
Patch:
@@ -93,7 +93,7 @@ private String fileClass() {
   private Schema fileProjection = null;
   private Collection<String> columns = null;
   private boolean caseSensitive = true;
-  private ScanReport.ScanMetrics scanMetrics = ScanReport.ScanMetrics.NOOP;
+  private ScanReport.ScanMetrics scanMetrics = ScanReport.ScanMetrics.noop();
 
   // lazily initialized
   private Evaluator lazyEvaluator = null;

File: core/src/main/java/org/apache/iceberg/metrics/CounterResultParser.java
Patch:
@@ -63,7 +63,7 @@ static CounterResult fromJson(JsonNode json) {
 
     String unit = JsonUtil.getString(UNIT, json);
     long value = JsonUtil.getLong(VALUE, json);
-    return new CounterResult(Unit.fromDisplayName(unit), value);
+    return CounterResult.of(Unit.fromDisplayName(unit), value);
   }
 
   /**
@@ -88,6 +88,6 @@ static CounterResult fromJson(String counterName, JsonNode json) {
 
     String unit = JsonUtil.getString(UNIT, counter);
     long value = JsonUtil.getLong(VALUE, counter);
-    return new CounterResult(Unit.fromDisplayName(unit), value);
+    return CounterResult.of(Unit.fromDisplayName(unit), value);
   }
 }

File: core/src/main/java/org/apache/iceberg/metrics/TimerResultParser.java
Patch:
@@ -69,7 +69,7 @@ static TimerResult fromJson(JsonNode json) {
     long count = JsonUtil.getLong(COUNT, json);
     TimeUnit unit = TimeUnit.valueOf(JsonUtil.getString(TIME_UNIT, json).toUpperCase(Locale.ROOT));
     long duration = JsonUtil.getLong(TOTAL_DURATION, json);
-    return new TimerResult(unit, toDuration(duration, unit), count);
+    return TimerResult.of(unit, toDuration(duration, unit), count);
   }
 
   /**
@@ -98,7 +98,7 @@ static TimerResult fromJson(String timerName, JsonNode json) {
     long count = JsonUtil.getLong(COUNT, timer);
     TimeUnit unit = TimeUnit.valueOf(JsonUtil.getString(TIME_UNIT, timer).toUpperCase(Locale.ROOT));
     long duration = JsonUtil.getLong(TOTAL_DURATION, timer);
-    return new TimerResult(unit, toDuration(duration, unit), count);
+    return TimerResult.of(unit, toDuration(duration, unit), count);
   }
 
   @VisibleForTesting

File: core/src/test/java/org/apache/iceberg/metrics/TestCounterResultParser.java
Patch:
@@ -52,7 +52,7 @@ public void missingFields() {
   public void extraFields() {
     Assertions.assertThat(
             CounterResultParser.fromJson("{\"unit\":\"bytes\",\"value\":23,\"extra\": \"value\"}"))
-        .isEqualTo(new CounterResult(Unit.BYTES, 23L));
+        .isEqualTo(CounterResult.of(Unit.BYTES, 23L));
   }
 
   @Test
@@ -73,7 +73,7 @@ public void invalidValue() {
 
   @Test
   public void roundTripSerde() {
-    CounterResult counter = new CounterResult(Unit.BYTES, Long.MAX_VALUE);
+    CounterResult counter = CounterResult.of(Unit.BYTES, Long.MAX_VALUE);
 
     String json = CounterResultParser.toJson(counter);
     Assertions.assertThat(CounterResultParser.fromJson(json)).isEqualTo(counter);

File: core/src/test/java/org/apache/iceberg/metrics/TestTimerResultParser.java
Patch:
@@ -60,7 +60,7 @@ public void extraFields() {
     Assertions.assertThat(
             TimerResultParser.fromJson(
                 "{\"count\":44,\"time-unit\":\"hours\",\"total-duration\":24,\"extra\": \"value\"}"))
-        .isEqualTo(new TimerResult(TimeUnit.HOURS, Duration.ofHours(24), 44));
+        .isEqualTo(TimerResult.of(TimeUnit.HOURS, Duration.ofHours(24), 44));
   }
 
   @Test
@@ -95,7 +95,7 @@ public void invalidCount() {
 
   @Test
   public void roundTripSerde() {
-    TimerResult timer = new TimerResult(TimeUnit.HOURS, Duration.ofHours(23), 44);
+    TimerResult timer = TimerResult.of(TimeUnit.HOURS, Duration.ofHours(23), 44);
 
     String json = TimerResultParser.toJson(timer);
     Assertions.assertThat(TimerResultParser.fromJson(json)).isEqualTo(timer);

File: core/src/main/java/org/apache/iceberg/rest/RESTTableOperations.java
Patch:
@@ -21,6 +21,7 @@
 import java.util.List;
 import java.util.Map;
 import java.util.Objects;
+import java.util.function.Consumer;
 import java.util.function.Supplier;
 import org.apache.iceberg.LocationProviders;
 import org.apache.iceberg.MetadataUpdate;
@@ -34,6 +35,7 @@
 import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;
 import org.apache.iceberg.relocated.com.google.common.collect.Lists;
 import org.apache.iceberg.rest.requests.UpdateTableRequest;
+import org.apache.iceberg.rest.responses.ErrorResponse;
 import org.apache.iceberg.rest.responses.LoadTableResponse;
 
 class RESTTableOperations implements TableOperations {
@@ -100,7 +102,7 @@ public TableMetadata refresh() {
   public void commit(TableMetadata base, TableMetadata metadata) {
     UpdateTableRequest.Builder requestBuilder;
     List<MetadataUpdate> baseChanges;
-    ErrorHandler errorHandler;
+    Consumer<ErrorResponse> errorHandler;
     switch (updateType) {
       case CREATE:
         Preconditions.checkState(

File: core/src/main/java/org/apache/iceberg/rest/auth/OAuth2Util.java
Patch:
@@ -133,7 +133,7 @@ private static OAuthTokenResponse refreshToken(
             request,
             OAuthTokenResponse.class,
             headers,
-            ErrorHandlers.oauthErrorHandler());
+            ErrorHandlers.defaultErrorHandler());
     response.validate();
 
     return response;
@@ -161,7 +161,7 @@ public static OAuthTokenResponse exchangeToken(
             request,
             OAuthTokenResponse.class,
             headers,
-            ErrorHandlers.oauthErrorHandler());
+            ErrorHandlers.defaultErrorHandler());
     response.validate();
 
     return response;
@@ -179,7 +179,7 @@ public static OAuthTokenResponse fetchToken(
             request,
             OAuthTokenResponse.class,
             headers,
-            ErrorHandlers.oauthErrorHandler());
+            ErrorHandlers.defaultErrorHandler());
     response.validate();
 
     return response;

File: core/src/main/java/org/apache/iceberg/rest/responses/ErrorResponse.java
Patch:
@@ -23,10 +23,10 @@
 import java.util.Arrays;
 import java.util.List;
 import org.apache.iceberg.relocated.com.google.common.base.Preconditions;
-import org.apache.iceberg.rest.RESTErrorResponse;
+import org.apache.iceberg.rest.RESTResponse;
 
 /** Standard response body for all API errors */
-public class ErrorResponse implements RESTErrorResponse {
+public class ErrorResponse implements RESTResponse {
 
   private String message;
   private String type;

File: core/src/main/java/org/apache/iceberg/rest/RESTTableOperations.java
Patch:
@@ -21,7 +21,6 @@
 import java.util.List;
 import java.util.Map;
 import java.util.Objects;
-import java.util.function.Consumer;
 import java.util.function.Supplier;
 import org.apache.iceberg.LocationProviders;
 import org.apache.iceberg.MetadataUpdate;
@@ -35,7 +34,6 @@
 import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;
 import org.apache.iceberg.relocated.com.google.common.collect.Lists;
 import org.apache.iceberg.rest.requests.UpdateTableRequest;
-import org.apache.iceberg.rest.responses.ErrorResponse;
 import org.apache.iceberg.rest.responses.LoadTableResponse;
 
 class RESTTableOperations implements TableOperations {
@@ -102,7 +100,7 @@ public TableMetadata refresh() {
   public void commit(TableMetadata base, TableMetadata metadata) {
     UpdateTableRequest.Builder requestBuilder;
     List<MetadataUpdate> baseChanges;
-    Consumer<ErrorResponse> errorHandler;
+    ErrorHandler errorHandler;
     switch (updateType) {
       case CREATE:
         Preconditions.checkState(

File: core/src/main/java/org/apache/iceberg/rest/auth/OAuth2Util.java
Patch:
@@ -133,7 +133,7 @@ private static OAuthTokenResponse refreshToken(
             request,
             OAuthTokenResponse.class,
             headers,
-            ErrorHandlers.defaultErrorHandler());
+            ErrorHandlers.oauthErrorHandler());
     response.validate();
 
     return response;
@@ -161,7 +161,7 @@ public static OAuthTokenResponse exchangeToken(
             request,
             OAuthTokenResponse.class,
             headers,
-            ErrorHandlers.defaultErrorHandler());
+            ErrorHandlers.oauthErrorHandler());
     response.validate();
 
     return response;
@@ -179,7 +179,7 @@ public static OAuthTokenResponse fetchToken(
             request,
             OAuthTokenResponse.class,
             headers,
-            ErrorHandlers.defaultErrorHandler());
+            ErrorHandlers.oauthErrorHandler());
     response.validate();
 
     return response;

File: core/src/main/java/org/apache/iceberg/rest/responses/ErrorResponse.java
Patch:
@@ -23,10 +23,10 @@
 import java.util.Arrays;
 import java.util.List;
 import org.apache.iceberg.relocated.com.google.common.base.Preconditions;
-import org.apache.iceberg.rest.RESTResponse;
+import org.apache.iceberg.rest.RESTErrorResponse;
 
 /** Standard response body for all API errors */
-public class ErrorResponse implements RESTResponse {
+public class ErrorResponse implements RESTErrorResponse {
 
   private String message;
   private String type;

File: api/src/main/java/org/apache/iceberg/transforms/UnknownTransform.java
Patch:
@@ -86,6 +86,6 @@ public boolean equals(Object other) {
 
   @Override
   public int hashCode() {
-    return Objects.hash(transform);
+    return Objects.hashCode(transform);
   }
 }

File: api/src/main/java/org/apache/iceberg/expressions/ExpressionUtil.java
Patch:
@@ -18,17 +18,17 @@
  */
 package org.apache.iceberg.expressions;
 
+import java.util.function.Function;
 import java.util.regex.Pattern;
 import java.util.stream.Collectors;
 import org.apache.iceberg.PartitionSpec;
-import org.apache.iceberg.transforms.Transform;
 import org.apache.iceberg.transforms.Transforms;
 import org.apache.iceberg.types.Types;
 
 /** Expression utility methods. */
 public class ExpressionUtil {
-  private static final Transform<CharSequence, Integer> HASH_FUNC =
-      Transforms.bucket(Types.StringType.get(), Integer.MAX_VALUE);
+  private static final Function<Object, Integer> HASH_FUNC =
+      Transforms.bucket(Integer.MAX_VALUE).bind(Types.StringType.get());
   private static final Pattern DATE = Pattern.compile("\\d\\d\\d\\d-\\d\\d-\\d\\d");
   private static final Pattern TIME = Pattern.compile("\\d\\d:\\d\\d(:\\d\\d(.\\d{1,6})?)?");
   private static final Pattern TIMESTAMP =

File: api/src/test/java/org/apache/iceberg/expressions/TestExpressionHelpers.java
Patch:
@@ -237,6 +237,6 @@ private void assertInvalidateNaNThrows(Callable<UnboundPredicate<Double>> callab
   }
 
   private <T> UnboundTerm<T> self(String name) {
-    return new UnboundTransform<>(ref(name), Transforms.identity(Types.DoubleType.get()));
+    return new UnboundTransform<>(ref(name), Transforms.identity());
   }
 }

File: core/src/main/java/org/apache/iceberg/BaseUpdatePartitionSpec.java
Patch:
@@ -343,7 +343,7 @@ public void commit() {
 
   private Transform<?, ?> toTransform(BoundTerm<?> term) {
     if (term instanceof BoundReference) {
-      return Transforms.identity(term.type());
+      return Transforms.identity();
     } else if (term instanceof BoundTransform) {
       return ((BoundTransform<?, ?>) term).transform();
     } else {

File: core/src/main/java/org/apache/iceberg/LocationProviders.java
Patch:
@@ -19,11 +19,11 @@
 package org.apache.iceberg;
 
 import java.util.Map;
+import java.util.function.Function;
 import org.apache.hadoop.fs.Path;
 import org.apache.iceberg.common.DynConstructors;
 import org.apache.iceberg.io.LocationProvider;
 import org.apache.iceberg.relocated.com.google.common.base.Preconditions;
-import org.apache.iceberg.transforms.Transform;
 import org.apache.iceberg.transforms.Transforms;
 import org.apache.iceberg.types.Types;
 import org.apache.iceberg.util.LocationUtil;
@@ -104,8 +104,8 @@ public String newDataLocation(String filename) {
   }
 
   static class ObjectStoreLocationProvider implements LocationProvider {
-    private static final Transform<String, Integer> HASH_FUNC =
-        Transforms.bucket(Types.StringType.get(), Integer.MAX_VALUE);
+    private static final Function<Object, Integer> HASH_FUNC =
+        Transforms.bucket(Integer.MAX_VALUE).bind(Types.StringType.get());
 
     private final String storageLocation;
     private final String context;

File: core/src/main/java/org/apache/iceberg/ManifestsTable.java
Patch:
@@ -130,12 +130,14 @@ static List<StaticDataTask.Row> partitionSummariesToRows(
                   .get(i)
                   .transform()
                   .toHumanString(
+                      spec.partitionType().fields().get(i).type(),
                       Conversions.fromByteBuffer(
                           spec.partitionType().fields().get(i).type(), summary.lowerBound())),
               spec.fields()
                   .get(i)
                   .transform()
                   .toHumanString(
+                      spec.partitionType().fields().get(i).type(),
                       Conversions.fromByteBuffer(
                           spec.partitionType().fields().get(i).type(), summary.upperBound()))));
     }

File: core/src/main/java/org/apache/iceberg/TableMetadata.java
Patch:
@@ -109,8 +109,7 @@ static TableMetadata newTableMetadata(
       // look up the name of the source field in the old schema to get the new schema's id
       String sourceName = schema.findColumnName(field.sourceId());
       // reassign all partition fields with fresh partition field Ids to ensure consistency
-      specBuilder.add(
-          freshSchema.findField(sourceName).fieldId(), field.name(), field.transform().toString());
+      specBuilder.add(freshSchema.findField(sourceName).fieldId(), field.name(), field.transform());
     }
     PartitionSpec freshSpec = specBuilder.build();
 

File: core/src/test/java/org/apache/iceberg/TestReplaceTransaction.java
Patch:
@@ -96,7 +96,7 @@ public void testReplaceTransactionWithCustomSortOrder() {
     Assert.assertEquals("Direction must match ", ASC, sortOrder.fields().get(0).direction());
     Assert.assertEquals(
         "Null order must match ", NULLS_FIRST, sortOrder.fields().get(0).nullOrder());
-    Transform<?, ?> transform = Transforms.identity(Types.IntegerType.get());
+    Transform<?, ?> transform = Transforms.identity();
     Assert.assertEquals("Transform must match", transform, sortOrder.fields().get(0).transform());
   }
 

File: core/src/test/java/org/apache/iceberg/hadoop/TestHadoopCatalog.java
Patch:
@@ -51,7 +51,6 @@
 import org.apache.iceberg.relocated.com.google.common.collect.Sets;
 import org.apache.iceberg.transforms.Transform;
 import org.apache.iceberg.transforms.Transforms;
-import org.apache.iceberg.types.Types;
 import org.assertj.core.api.Assertions;
 import org.junit.Assert;
 import org.junit.Test;
@@ -181,7 +180,7 @@ public void testCreateTableCustomSortOrder() throws Exception {
     Assert.assertEquals("Direction must match ", ASC, sortOrder.fields().get(0).direction());
     Assert.assertEquals(
         "Null order must match ", NULLS_FIRST, sortOrder.fields().get(0).nullOrder());
-    Transform<?, ?> transform = Transforms.identity(Types.IntegerType.get());
+    Transform<?, ?> transform = Transforms.identity();
     Assert.assertEquals("Transform must match", transform, sortOrder.fields().get(0).transform());
   }
 

File: core/src/test/java/org/apache/iceberg/hadoop/TestHadoopTables.java
Patch:
@@ -143,7 +143,7 @@ public void testCustomSortOrder() {
     Assert.assertEquals("Direction must match ", ASC, sortOrder.fields().get(0).direction());
     Assert.assertEquals(
         "Null order must match ", NULLS_FIRST, sortOrder.fields().get(0).nullOrder());
-    Transform<?, ?> transform = Transforms.identity(Types.IntegerType.get());
+    Transform<?, ?> transform = Transforms.identity();
     Assert.assertEquals("Transform must match", transform, sortOrder.fields().get(0).transform());
   }
 

File: core/src/test/java/org/apache/iceberg/jdbc/TestJdbcCatalog.java
Patch:
@@ -250,7 +250,7 @@ public void testCreateTableCustomSortOrder() {
     Assert.assertEquals("Direction must match ", ASC, sortOrder.fields().get(0).direction());
     Assert.assertEquals(
         "Null order must match ", NULLS_FIRST, sortOrder.fields().get(0).nullOrder());
-    Transform<?, ?> transform = Transforms.identity(Types.IntegerType.get());
+    Transform<?, ?> transform = Transforms.identity();
     Assert.assertEquals("Transform must match", transform, sortOrder.fields().get(0).transform());
   }
 

File: spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/IcebergSpark.java
Patch:
@@ -18,7 +18,7 @@
  */
 package org.apache.iceberg.spark;
 
-import org.apache.iceberg.transforms.Transform;
+import java.util.function.Function;
 import org.apache.iceberg.transforms.Transforms;
 import org.apache.iceberg.types.Type;
 import org.apache.spark.sql.SparkSession;
@@ -32,7 +32,7 @@ public static void registerBucketUDF(
       SparkSession session, String funcName, DataType sourceType, int numBuckets) {
     SparkTypeToType typeConverter = new SparkTypeToType();
     Type sourceIcebergType = typeConverter.atomic(sourceType);
-    Transform<Object, Integer> bucket = Transforms.bucket(sourceIcebergType, numBuckets);
+    Function<Object, Integer> bucket = Transforms.bucket(numBuckets).bind(sourceIcebergType);
     session
         .udf()
         .register(
@@ -45,7 +45,7 @@ public static void registerTruncateUDF(
       SparkSession session, String funcName, DataType sourceType, int width) {
     SparkTypeToType typeConverter = new SparkTypeToType();
     Type sourceIcebergType = typeConverter.atomic(sourceType);
-    Transform<Object, Object> truncate = Transforms.truncate(sourceIcebergType, width);
+    Function<Object, Object> truncate = Transforms.truncate(width).bind(sourceIcebergType);
     session
         .udf()
         .register(

File: aws/src/main/java/org/apache/iceberg/aws/AssumeRoleAwsClientFactory.java
Patch:
@@ -96,7 +96,7 @@ public void initialize(Map<String, String> properties) {
     this.s3UseArnRegionEnabled =
         PropertyUtil.propertyAsBoolean(
             properties,
-            AwsProperties.S3_ACCESS_POINTS_PREFIX,
+            AwsProperties.S3_USE_ARN_REGION_ENABLED,
             AwsProperties.S3_USE_ARN_REGION_ENABLED_DEFAULT);
     this.dynamoDbEndpoint = properties.get(AwsProperties.DYNAMODB_ENDPOINT);
     this.httpClientType =

File: core/src/test/java/org/apache/iceberg/TestIncrementalDataTableScan.java
Patch:
@@ -190,7 +190,7 @@ public void testRollbacks() {
     add(table.newAppend(), files("B"));
     add(table.newAppend(), files("C")); // 3
     // Go back to snapshot "B"
-    table.rollback().toSnapshotId(2).commit(); // 2
+    table.manageSnapshots().rollbackTo(2).commit(); // 2
     Assert.assertEquals(2, table.currentSnapshot().snapshotId());
     filesMatch(Lists.newArrayList("B"), appendsBetweenScan(1, 2));
     filesMatch(Lists.newArrayList("B"), appendsAfterScan(1));
@@ -201,7 +201,7 @@ public void testRollbacks() {
     add(transaction.newAppend(), files("F"));
     transaction.commitTransaction();
     // Go back to snapshot "E"
-    table.rollback().toSnapshotId(5).commit();
+    table.manageSnapshots().rollbackTo(5).commit();
     Assert.assertEquals(5, table.currentSnapshot().snapshotId());
     filesMatch(Lists.newArrayList("B", "D", "E"), appendsBetweenScan(1, 5));
     filesMatch(Lists.newArrayList("B", "D", "E"), appendsAfterScan(1));

File: spark/v2.4/spark/src/test/java/org/apache/iceberg/examples/SnapshotFunctionalityTest.java
Patch:
@@ -91,7 +91,7 @@ public void before() throws IOException {
   public void rollbackToPreviousSnapshotAndReadData() {
     long oldId = table.history().get(0).snapshotId();
 
-    table.rollback().toSnapshotId(oldId).commit();
+    table.manageSnapshots().rollbackTo(oldId).commit();
     table.refresh();
 
     Dataset<Row> results = spark.read().format("iceberg").load(tableLocation.toString());

File: spark/v2.4/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceTablesBase.java
Patch:
@@ -778,7 +778,7 @@ public void testHistoryTable() {
     long secondSnapshotId = table.currentSnapshot().snapshotId();
 
     // rollback the table state to the first snapshot
-    table.rollback().toSnapshotId(firstSnapshotId).commit();
+    table.manageSnapshots().rollbackTo(firstSnapshotId).commit();
     long rollbackTimestamp = Iterables.getLast(table.history()).timestampMillis();
 
     inputDf
@@ -864,7 +864,7 @@ public void testSnapshotsTable() {
     String secondManifestList = table.currentSnapshot().manifestListLocation();
 
     // rollback the table state to the first snapshot
-    table.rollback().toSnapshotId(firstSnapshotId).commit();
+    table.manageSnapshots().rollbackTo(firstSnapshotId).commit();
 
     List<Row> actual =
         spark
@@ -937,7 +937,7 @@ public void testPrunedSnapshotsTable() {
     long secondSnapshotTimestamp = table.currentSnapshot().timestampMillis();
 
     // rollback the table state to the first snapshot
-    table.rollback().toSnapshotId(firstSnapshotId).commit();
+    table.manageSnapshots().rollbackTo(firstSnapshotId).commit();
 
     Dataset<Row> actualDf =
         spark

File: spark/v3.0/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceTablesBase.java
Patch:
@@ -782,7 +782,7 @@ public void testHistoryTable() {
     long secondSnapshotId = table.currentSnapshot().snapshotId();
 
     // rollback the table state to the first snapshot
-    table.rollback().toSnapshotId(firstSnapshotId).commit();
+    table.manageSnapshots().rollbackTo(firstSnapshotId).commit();
     long rollbackTimestamp = Iterables.getLast(table.history()).timestampMillis();
 
     inputDf
@@ -868,7 +868,7 @@ public void testSnapshotsTable() {
     String secondManifestList = table.currentSnapshot().manifestListLocation();
 
     // rollback the table state to the first snapshot
-    table.rollback().toSnapshotId(firstSnapshotId).commit();
+    table.manageSnapshots().rollbackTo(firstSnapshotId).commit();
 
     List<Row> actual =
         spark
@@ -941,7 +941,7 @@ public void testPrunedSnapshotsTable() {
     long secondSnapshotTimestamp = table.currentSnapshot().timestampMillis();
 
     // rollback the table state to the first snapshot
-    table.rollback().toSnapshotId(firstSnapshotId).commit();
+    table.manageSnapshots().rollbackTo(firstSnapshotId).commit();
 
     Dataset<Row> actualDf =
         spark

File: spark/v3.1/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceTablesBase.java
Patch:
@@ -782,7 +782,7 @@ public void testHistoryTable() {
     long secondSnapshotId = table.currentSnapshot().snapshotId();
 
     // rollback the table state to the first snapshot
-    table.rollback().toSnapshotId(firstSnapshotId).commit();
+    table.manageSnapshots().rollbackTo(firstSnapshotId).commit();
     long rollbackTimestamp = Iterables.getLast(table.history()).timestampMillis();
 
     inputDf
@@ -868,7 +868,7 @@ public void testSnapshotsTable() {
     String secondManifestList = table.currentSnapshot().manifestListLocation();
 
     // rollback the table state to the first snapshot
-    table.rollback().toSnapshotId(firstSnapshotId).commit();
+    table.manageSnapshots().rollbackTo(firstSnapshotId).commit();
 
     List<Row> actual =
         spark
@@ -941,7 +941,7 @@ public void testPrunedSnapshotsTable() {
     long secondSnapshotTimestamp = table.currentSnapshot().timestampMillis();
 
     // rollback the table state to the first snapshot
-    table.rollback().toSnapshotId(firstSnapshotId).commit();
+    table.manageSnapshots().rollbackTo(firstSnapshotId).commit();
 
     Dataset<Row> actualDf =
         spark

File: spark/v3.2/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceTablesBase.java
Patch:
@@ -782,7 +782,7 @@ public void testHistoryTable() {
     long secondSnapshotId = table.currentSnapshot().snapshotId();
 
     // rollback the table state to the first snapshot
-    table.rollback().toSnapshotId(firstSnapshotId).commit();
+    table.manageSnapshots().rollbackTo(firstSnapshotId).commit();
     long rollbackTimestamp = Iterables.getLast(table.history()).timestampMillis();
 
     inputDf
@@ -869,7 +869,7 @@ public void testSnapshotsTable() {
     String secondManifestList = table.currentSnapshot().manifestListLocation();
 
     // rollback the table state to the first snapshot
-    table.rollback().toSnapshotId(firstSnapshotId).commit();
+    table.manageSnapshots().rollbackTo(firstSnapshotId).commit();
 
     List<Row> actual =
         spark
@@ -942,7 +942,7 @@ public void testPrunedSnapshotsTable() {
     long secondSnapshotTimestamp = table.currentSnapshot().timestampMillis();
 
     // rollback the table state to the first snapshot
-    table.rollback().toSnapshotId(firstSnapshotId).commit();
+    table.manageSnapshots().rollbackTo(firstSnapshotId).commit();
 
     Dataset<Row> actualDf =
         spark

File: spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceTablesBase.java
Patch:
@@ -782,7 +782,7 @@ public void testHistoryTable() {
     long secondSnapshotId = table.currentSnapshot().snapshotId();
 
     // rollback the table state to the first snapshot
-    table.rollback().toSnapshotId(firstSnapshotId).commit();
+    table.manageSnapshots().rollbackTo(firstSnapshotId).commit();
     long rollbackTimestamp = Iterables.getLast(table.history()).timestampMillis();
 
     inputDf
@@ -869,7 +869,7 @@ public void testSnapshotsTable() {
     String secondManifestList = table.currentSnapshot().manifestListLocation();
 
     // rollback the table state to the first snapshot
-    table.rollback().toSnapshotId(firstSnapshotId).commit();
+    table.manageSnapshots().rollbackTo(firstSnapshotId).commit();
 
     List<Row> actual =
         spark
@@ -942,7 +942,7 @@ public void testPrunedSnapshotsTable() {
     long secondSnapshotTimestamp = table.currentSnapshot().timestampMillis();
 
     // rollback the table state to the first snapshot
-    table.rollback().toSnapshotId(firstSnapshotId).commit();
+    table.manageSnapshots().rollbackTo(firstSnapshotId).commit();
 
     Dataset<Row> actualDf =
         spark

File: core/src/main/java/org/apache/iceberg/MetadataUpdateParser.java
Patch:
@@ -150,7 +150,7 @@ public static void toJson(MetadataUpdate metadataUpdate, JsonGenerator generator
     // which is required
     Preconditions.checkArgument(
         updateAction != null,
-        "Cannot convert metadata update to json. Unrecognized metadata update type: {}",
+        "Cannot convert metadata update to json. Unrecognized metadata update type: %s",
         metadataUpdate.getClass().getName());
 
     generator.writeStartObject();

File: flink/v1.14/flink/src/main/java/org/apache/iceberg/flink/source/enumerator/ContinuousSplitPlannerImpl.java
Patch:
@@ -78,7 +78,7 @@ public ContinuousEnumerationResult planSplits(IcebergEnumeratorPosition lastPosi
     }
   }
 
-  /** Discover incremental changes between @{code lastPosition} and current table snapshot */
+  /** Discover incremental changes between {@code lastPosition} and current table snapshot */
   private ContinuousEnumerationResult discoverIncrementalSplits(
       IcebergEnumeratorPosition lastPosition) {
     Snapshot currentSnapshot = table.currentSnapshot();

File: flink/v1.15/flink/src/main/java/org/apache/iceberg/flink/source/enumerator/ContinuousSplitPlannerImpl.java
Patch:
@@ -78,7 +78,7 @@ public ContinuousEnumerationResult planSplits(IcebergEnumeratorPosition lastPosi
     }
   }
 
-  /** Discover incremental changes between @{code lastPosition} and current table snapshot */
+  /** Discover incremental changes between {@code lastPosition} and current table snapshot */
   private ContinuousEnumerationResult discoverIncrementalSplits(
       IcebergEnumeratorPosition lastPosition) {
     Snapshot currentSnapshot = table.currentSnapshot();

File: core/src/test/java/org/apache/iceberg/TestMetadataUpdateParser.java
Patch:
@@ -675,7 +675,7 @@ public void testSetPropertiesFromJsonFailsWhenDeserializingNullValues() {
     AssertHelpers.assertThrows(
         "Parsing updates from SetProperties with a property set to null should throw",
         IllegalArgumentException.class,
-        "Cannot parse prop2 to a string value: null",
+        "Cannot parse to a string value: prop2: null",
         () -> MetadataUpdateParser.fromJson(json));
   }
 

File: core/src/test/java/org/apache/iceberg/avro/TestAvroNameMapping.java
Patch:
@@ -44,6 +44,7 @@
 import org.junit.Assert;
 import org.junit.Test;
 
+@SuppressWarnings("unchecked")
 public class TestAvroNameMapping extends TestAvroReadProjection {
   @Test
   public void testMapProjections() throws IOException {

File: api/src/main/java/org/apache/iceberg/io/SupportsBulkOperations.java
Patch:
@@ -23,7 +23,7 @@ public interface SupportsBulkOperations {
    * Delete the files at the given paths.
    *
    * @param pathsToDelete The paths to delete
-   * @throws BulkDeletionFailureException in
+   * @throws BulkDeletionFailureException in case of failure to delete at least 1 file
    */
   void deleteFiles(Iterable<String> pathsToDelete) throws BulkDeletionFailureException;
 }

File: aws/src/integration/java/org/apache/iceberg/aws/s3/TestS3FileIOIntegration.java
Patch:
@@ -378,8 +378,8 @@ private void testDeleteFiles(int numObjects, S3FileIO s3FileIO) throws Exception
     List<String> paths = Lists.newArrayList();
     for (int i = 1; i <= numObjects; i++) {
       String deletionKey = objectKey + "-deletion-" + i;
-      write(s3FileIO, String.format("s3://%s/%s", bucketName, deletionKey));
-      paths.add(String.format("s3://%s/%s", bucketName, deletionKey));
+      write(s3FileIO, String.format("s3://%s/%s/%s", bucketName, prefix, deletionKey));
+      paths.add(String.format("s3://%s/%s/%s", bucketName, prefix, deletionKey));
     }
     s3FileIO.deleteFiles(paths);
     for (String path : paths) {

File: aws/src/integration/java/org/apache/iceberg/aws/s3/TestS3FileIOIntegration.java
Patch:
@@ -334,6 +334,7 @@ public void testDeleteFilesSingleBatchWithRemainder() throws Exception {
     testDeleteFiles(5, s3FileIO);
   }
 
+  @SuppressWarnings("DangerousParallelStreamUsage")
   @Test
   public void testPrefixList() {
     S3FileIO s3FileIO = new S3FileIO(clientFactory::s3);
@@ -350,6 +351,7 @@ public void testPrefixList() {
     Assertions.assertEquals(totalFiles, Streams.stream(s3FileIO.listPrefix(listPrefix)).count());
   }
 
+  @SuppressWarnings("DangerousParallelStreamUsage")
   @Test
   public void testPrefixDelete() {
     AwsProperties properties = new AwsProperties();

File: api/src/main/java/org/apache/iceberg/io/CloseableIterator.java
Patch:
@@ -59,7 +59,7 @@ public E next() {
 
   static <I, O> CloseableIterator<O> transform(
       CloseableIterator<I> iterator, Function<I, O> transform) {
-    Preconditions.checkNotNull(transform, "Cannot apply a null transform");
+    Preconditions.checkNotNull(transform, "Invalid transform: null");
 
     return new CloseableIterator<O>() {
       @Override

File: aws/src/integration/java/org/apache/iceberg/aws/s3/TestS3FileIOIntegration.java
Patch:
@@ -338,7 +338,7 @@ public void testDeleteFilesSingleBatchWithRemainder() throws Exception {
   public void testPrefixList() {
     S3FileIO s3FileIO = new S3FileIO(clientFactory::s3);
     List<Integer> scaleSizes = Lists.newArrayList(1, 1000, 2500);
-    String listPrefix = String.format("s3://%s/%s", bucketName, "prefix-list-test");
+    String listPrefix = String.format("s3://%s/%s/%s", bucketName, prefix, "prefix-list-test");
 
     scaleSizes.parallelStream().forEach(scale -> {
       String scalePrefix = String.format("%s/%s/", listPrefix, scale);
@@ -355,7 +355,7 @@ public void testPrefixDelete() {
     AwsProperties properties = new AwsProperties();
     properties.setS3FileIoDeleteBatchSize(100);
     S3FileIO s3FileIO = new S3FileIO(clientFactory::s3, properties);
-    String deletePrefix = String.format("s3://%s/%s", bucketName, "prefix-delete-test");
+    String deletePrefix = String.format("s3://%s/%s/%s", bucketName, prefix, "prefix-delete-test");
 
     List<Integer> scaleSizes = Lists.newArrayList(0, 5, 1000, 2500);
     scaleSizes.parallelStream().forEach(scale -> {

File: api/src/main/java/org/apache/iceberg/types/TypeUtil.java
Patch:
@@ -289,7 +289,7 @@ public static Schema reassignIds(Schema schema, Schema idSourceSchema) {
   }
 
   public static Schema reassignOrRefreshIds(Schema schema, Schema idSourceSchema) {
-    AtomicInteger highest = new AtomicInteger(schema.highestFieldId());
+    AtomicInteger highest = new AtomicInteger(idSourceSchema.highestFieldId());
     Types.StructType struct =
         visit(schema, new ReassignIds(idSourceSchema, highest::incrementAndGet)).asStructType();
     return new Schema(struct.fields(), refreshIdentifierFields(struct, schema));

File: aws/src/integration/java/org/apache/iceberg/aws/s3/TestS3FileIOIntegration.java
Patch:
@@ -341,7 +341,7 @@ public void testPrefixList() {
     String listPrefix = String.format("s3://%s/%s", bucketName, "prefix-list-test");
 
     scaleSizes.parallelStream().forEach(scale -> {
-      String scalePrefix = String.format("%s/%s/", prefix, scale);
+      String scalePrefix = String.format("%s/%s/", listPrefix, scale);
       createRandomObjects(scalePrefix, scale);
       assertEquals((long) scale, Streams.stream(s3FileIO.listPrefix(scalePrefix)).count());
     });

File: aws/src/main/java/org/apache/iceberg/aws/AwsProperties.java
Patch:
@@ -122,7 +122,7 @@ public class AwsProperties implements Serializable {
   public static final String GLUE_LAKEFORMATION_ENABLED = "glue.lakeformation-enabled";
   public static final boolean GLUE_LAKEFORMATION_ENABLED_DEFAULT = false;
 
-  /*
+  /**
    * Configure an alternative endpoint of the Glue service for GlueCatalog to access.
    * <p>
    * This could be used to use GlueCatalog with any glue-compatible metastore service that has a different endpoint

File: spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/SparkTableUtil.java
Patch:
@@ -106,7 +106,7 @@ public class SparkTableUtil {
 
   private static final String DUPLICATE_FILE_MESSAGE = "Cannot complete import because data files " +
       "to be imported already exist within the target table: %s.  " +
-      "This is disabled by default as Iceberg is not designed for mulitple references to the same file" +
+      "This is disabled by default as Iceberg is not designed for multiple references to the same file" +
       " within the same table.  If you are sure, you may set 'check_duplicate_files' to false to force the import.";
 
 

File: bundled-guava/src/main/java/org/apache/iceberg/GuavaClasses.java
Patch:
@@ -25,6 +25,7 @@
 import com.google.common.base.Objects;
 import com.google.common.base.Preconditions;
 import com.google.common.base.Splitter;
+import com.google.common.base.Stopwatch;
 import com.google.common.base.Suppliers;
 import com.google.common.base.Throwables;
 import com.google.common.collect.BiMap;
@@ -96,6 +97,7 @@ public class GuavaClasses {
     Iterables.class.getName();
     CountingOutputStream.class.getName();
     Suppliers.class.getName();
+    Stopwatch.class.getName();
   }
 
 }

File: core/src/main/java/org/apache/iceberg/hadoop/HadoopMetricsContext.java
Patch:
@@ -51,8 +51,8 @@ public void initialize(Map<String, String> properties) {
   }
 
   /**
-   * The Hadoop implementation delegates to the Hadoop delegates to the
-   * FileSystem.Statistics implementation and therefore does not require
+   * The Hadoop implementation delegates to the FileSystem.Statistics
+   * implementation and therefore does not require
    * support for operations like unit() and count() as the counter
    * values are not directly consumed.
    *

File: spark/v3.2/spark/src/test/java/org/apache/iceberg/TestHadoopMetricsContextSerialization.java
Patch:
@@ -28,7 +28,7 @@
 
 public class TestHadoopMetricsContextSerialization {
 
-  @Test(expected = Test.None.class)
+  @Test
   public void testHadoopMetricsContextKryoSerialization() throws IOException {
     MetricsContext metricsContext = new HadoopMetricsContext("s3");
 
@@ -41,7 +41,7 @@ public void testHadoopMetricsContextKryoSerialization() throws IOException {
         .increment();
   }
 
-  @Test(expected = Test.None.class)
+  @Test
   public void testHadoopMetricsContextJavaSerialization() throws IOException, ClassNotFoundException {
     MetricsContext metricsContext = new HadoopMetricsContext("s3");
 

File: spark/v3.3/spark/src/test/java/org/apache/iceberg/TestHadoopMetricsContextSerialization.java
Patch:
@@ -28,7 +28,7 @@
 
 public class TestHadoopMetricsContextSerialization {
 
-  @Test(expected = Test.None.class)
+  @Test
   public void testHadoopMetricsContextKryoSerialization() throws IOException {
     MetricsContext metricsContext = new HadoopMetricsContext("s3");
 
@@ -41,7 +41,7 @@ public void testHadoopMetricsContextKryoSerialization() throws IOException {
         .increment();
   }
 
-  @Test(expected = Test.None.class)
+  @Test
   public void testHadoopMetricsContextJavaSerialization() throws IOException, ClassNotFoundException {
     MetricsContext metricsContext = new HadoopMetricsContext("s3");
 

File: aws/src/test/java/org/apache/iceberg/aws/s3/TestS3FileIO.java
Patch:
@@ -173,6 +173,7 @@ private void testBatchDelete(int numObjects) {
       for (int j = 1; j <= numObjects; j++) {
         String key = "object-" + j;
         paths.add("s3://" + bucketName + "/" + key);
+        s3mock.putObject(builder -> builder.bucket(bucketName).key(key).build(), RequestBody.empty());
       }
     }
     s3FileIO.deleteFiles(paths);
@@ -217,9 +218,9 @@ public void testPrefixList() {
   @Test
   public void testPrefixDelete() {
     String prefix = "s3://bucket/path/to/delete";
-    List<Integer> scaleSizes = Lists.newArrayList(0, 5, 1000, 2500);
+    List<Integer> scaleSizes = Lists.newArrayList(0, 5, 1001);
 
-    scaleSizes.parallelStream().forEach(scale -> {
+    scaleSizes.forEach(scale -> {
       String scalePrefix = String.format("%s/%s/", prefix, scale);
 
       createRandomObjects(scalePrefix, scale);

File: mr/src/main/java/org/apache/iceberg/mr/Catalogs.java
Patch:
@@ -243,7 +243,7 @@ private static Map<String, String> getCatalogProperties(Configuration conf, Stri
   /**
    * This method is used for backward-compatible catalog configuration.
    * Collect all the catalog specific configuration from the global hive configuration.
-   * Note: this should be removed when the old catalog configuration is depracated.
+   * Note: this should be removed when the old catalog configuration is deprecated.
    * @param conf global hive configuration
    * @param catalogType type of the catalog
    * @param catalogProperties pre-populated catalog properties

File: dell/src/main/java/org/apache/iceberg/dell/DellClientFactory.java
Patch:
@@ -20,9 +20,10 @@
 package org.apache.iceberg.dell;
 
 import com.emc.object.s3.S3Client;
+import java.io.Serializable;
 import java.util.Map;
 
-public interface DellClientFactory {
+public interface DellClientFactory extends Serializable {
 
   /**
    * Create a Dell EMC ECS S3 client

File: dell/src/main/java/org/apache/iceberg/dell/ecs/PropertiesSerDesUtil.java
Patch:
@@ -58,7 +58,7 @@ private PropertiesSerDesUtil() {
    * @param version is the version of {@link PropertiesSerDesUtil}
    */
   public static Map<String, String> read(byte[] content, String version) {
-    Preconditions.checkArgument(version.equals(CURRENT_VERSION),
+    Preconditions.checkArgument(CURRENT_VERSION.equals(version),
         "Properties version is not match", version);
     Properties jdkProperties = new Properties();
     try (Reader reader = new InputStreamReader(new ByteArrayInputStream(content), StandardCharsets.UTF_8)) {

File: api/src/main/java/org/apache/iceberg/util/ExceptionUtil.java
Patch:
@@ -78,6 +78,7 @@ public static <R, E1 extends Exception, E2 extends Exception> R runSafely(
     return runSafely(block, catchBlock, finallyBlock, e1Class, e2Class, RuntimeException.class);
   }
 
+  @SuppressWarnings("Finally")
   public static <R, E1 extends Exception, E2 extends Exception, E3 extends Exception> R runSafely(
       Block<R, E1, E2, E3> block,
       CatchBlock catchBlock,
@@ -113,15 +114,15 @@ public static <R, E1 extends Exception, E2 extends Exception, E3 extends Excepti
         try {
           finallyBlock.run();
         } catch (Exception e) {
-          LOG.warn("Suppressing failure in finally block", e);
           if (failure != null) {
+            LOG.warn("Suppressing failure in finally block", e);
             failure.addSuppressed(e);
           } else {
             tryThrowAs(e, e1Class);
             tryThrowAs(e, e2Class);
             tryThrowAs(e, e3Class);
             tryThrowAs(e, RuntimeException.class);
-            throw new RuntimeException("Unknown exception in finally block", failure);
+            throw new RuntimeException("Unknown exception in finally block", e);
           }
         }
       }

File: core/src/main/java/org/apache/iceberg/MetadataTableUtils.java
Patch:
@@ -19,6 +19,7 @@
 
 package org.apache.iceberg;
 
+import java.util.Locale;
 import org.apache.iceberg.catalog.TableIdentifier;
 import org.apache.iceberg.exceptions.NoSuchTableException;
 
@@ -93,6 +94,6 @@ public static Table createMetadataTableInstance(TableOperations ops,
   }
 
   private static String metadataTableName(String tableName, MetadataTableType type) {
-    return tableName + (tableName.contains("/") ? "#" : ".") + type;
+    return tableName + (tableName.contains("/") ? "#" : ".") + type.name().toLowerCase(Locale.ROOT);
   }
 }

File: core/src/main/java/org/apache/iceberg/actions/BaseRewriteDataFilesAction.java
Patch:
@@ -259,9 +259,10 @@ private Map<StructLikeWrapper, Collection<FileScanTask>> groupTasksByPartition(
       CloseableIterator<FileScanTask> tasksIter) {
     ListMultimap<StructLikeWrapper, FileScanTask> tasksGroupedByPartition = Multimaps.newListMultimap(
         Maps.newHashMap(), Lists::newArrayList);
+    StructLikeWrapper partitionWrapper = StructLikeWrapper.forType(spec.partitionType());
     try (CloseableIterator<FileScanTask> iterator = tasksIter) {
       iterator.forEachRemaining(task -> {
-        StructLikeWrapper structLike = StructLikeWrapper.forType(spec.partitionType()).set(task.file().partition());
+        StructLikeWrapper structLike = partitionWrapper.copyFor(task.file().partition());
         tasksGroupedByPartition.put(structLike, task);
       });
     } catch (IOException e) {

File: core/src/main/java/org/apache/iceberg/util/StructLikeMap.java
Patch:
@@ -84,7 +84,7 @@ public T get(Object key) {
 
   @Override
   public T put(StructLike key, T value) {
-    return wrapperMap.put(StructLikeWrapper.forType(type).set(key), value);
+    return wrapperMap.put(wrappers.get().copyFor(key), value);
   }
 
   @Override

File: core/src/main/java/org/apache/iceberg/util/StructLikeSet.java
Patch:
@@ -100,7 +100,7 @@ public <T> T[] toArray(T[] destArray) {
 
   @Override
   public boolean add(StructLike struct) {
-    return wrapperSet.add(StructLikeWrapper.forType(type).set(struct));
+    return wrapperSet.add(wrappers.get().copyFor(struct));
   }
 
   @Override
@@ -126,7 +126,7 @@ public boolean containsAll(Collection<?> objects) {
   public boolean addAll(Collection<? extends StructLike> structs) {
     if (structs != null) {
       return Iterables.addAll(wrapperSet,
-          Iterables.transform(structs, struct -> StructLikeWrapper.forType(type).set(struct)));
+          Iterables.transform(structs, struct -> wrappers.get().copyFor(struct)));
     }
     return false;
   }

File: data/src/main/java/org/apache/iceberg/data/DeleteFilter.java
Patch:
@@ -147,6 +147,7 @@ private List<Predicate<T>> applyEqDeletes() {
       Iterable<DeleteFile> deletes = entry.getValue();
 
       Schema deleteSchema = TypeUtil.select(requiredSchema, ids);
+      InternalRecordWrapper wrapper = new InternalRecordWrapper(deleteSchema.asStruct());
 
       // a projection to select and reorder fields of the file schema to match the delete rows
       StructProjection projectRow = StructProjection.create(requiredSchema, deleteSchema);
@@ -159,9 +160,7 @@ private List<Predicate<T>> applyEqDeletes() {
           CloseableIterable.concat(deleteRecords), Record::copy);
 
       StructLikeSet deleteSet = Deletes.toEqualitySet(
-          CloseableIterable.transform(
-              records, record -> new InternalRecordWrapper(deleteSchema.asStruct()).wrap(record)),
-          deleteSchema.asStruct());
+          CloseableIterable.transform(records, wrapper::copyFor), deleteSchema.asStruct());
 
       Predicate<T> isInDeleteSet = record -> deleteSet.contains(projectRow.wrap(asStructLike(record)));
       isInDeleteSets.add(isInDeleteSet);

File: flink/v1.14/flink/src/main/java/org/apache/iceberg/flink/source/reader/RowDataReaderFunction.java
Patch:
@@ -19,7 +19,7 @@
 
 package org.apache.iceberg.flink.source.reader;
 
-import org.apache.flink.configuration.Configuration;
+import org.apache.flink.configuration.ReadableConfig;
 import org.apache.flink.table.data.RowData;
 import org.apache.iceberg.Schema;
 import org.apache.iceberg.encryption.EncryptionManager;
@@ -39,7 +39,7 @@ public class RowDataReaderFunction extends DataIteratorReaderFunction<RowData> {
   private final EncryptionManager encryption;
 
   public RowDataReaderFunction(
-      Configuration config, Schema tableSchema, Schema projectedSchema,
+      ReadableConfig config, Schema tableSchema, Schema projectedSchema,
       String nameMapping, boolean caseSensitive, FileIO io, EncryptionManager encryption) {
     super(new ArrayPoolDataIteratorBatcher<>(config, new RowDataRecordFactory(
         FlinkSchemaUtil.convert(readSchema(tableSchema, projectedSchema)))));

File: api/src/main/java/org/apache/iceberg/io/CredentialSupplier.java
Patch:
@@ -28,7 +28,7 @@
  */
 public interface CredentialSupplier {
   /**
-   * @return the credential string
+   * Returns the credential string
    */
   String getCredential();
 }

File: api/src/main/java/org/apache/iceberg/io/FileIO.java
Patch:
@@ -69,7 +69,7 @@ default void deleteFile(OutputFile file) {
   }
 
   /**
-   * @return the property map used to configure this FileIO
+   * Returns the property map used to configure this FileIO
    * @throws UnsupportedOperationException if this FileIO does not expose its configuration properties
    */
   default Map<String, String> properties() {

File: core/src/main/java/org/apache/iceberg/rest/CatalogHandlers.java
Patch:
@@ -244,7 +244,7 @@ public static LoadTableResponse updateTable(Catalog catalog, TableIdentifier ide
       Transaction transaction = catalog.buildTable(ident, EMPTY_SCHEMA).createOrReplaceTransaction();
       if (transaction instanceof BaseTransaction) {
         BaseTransaction baseTransaction = (BaseTransaction) transaction;
-        finalMetadata = create(baseTransaction.underlyingOps(), baseTransaction.startMetadata(), request);
+        finalMetadata = create(baseTransaction.underlyingOps(), request);
       } else {
         throw new IllegalStateException("Cannot wrap catalog that does not produce BaseTransaction");
       }
@@ -283,11 +283,11 @@ private static boolean isCreate(UpdateTableRequest request) {
     return isCreate;
   }
 
-  private static TableMetadata create(TableOperations ops, TableMetadata start, UpdateTableRequest request) {
+  private static TableMetadata create(TableOperations ops, UpdateTableRequest request) {
     // the only valid requirement is that the table will be created
     request.requirements().forEach(requirement -> requirement.validate(ops.current()));
 
-    TableMetadata.Builder builder = TableMetadata.buildFrom(start);
+    TableMetadata.Builder builder = TableMetadata.buildFromEmpty();
     request.updates().forEach(update -> update.applyTo(builder));
 
     // create transactions do not retry. if the table exists, retrying is not a solution

File: core/src/test/java/org/apache/iceberg/rest/responses/TestLoadTableResponse.java
Patch:
@@ -78,6 +78,7 @@ public LoadTableResponse createExampleInstance() {
         TableMetadata
             .buildFrom(
                 TableMetadata.newTableMetadata(SCHEMA_7, SPEC_5, SORT_ORDER_3, TEST_TABLE_LOCATION, TABLE_PROPS))
+            .discardChanges()
             .withMetadataLocation(TEST_METADATA_LOCATION)
             .build();
 

File: flink/v1.15/flink/src/main/java/org/apache/iceberg/flink/source/reader/RowDataReaderFunction.java
Patch:
@@ -19,7 +19,7 @@
 
 package org.apache.iceberg.flink.source.reader;
 
-import org.apache.flink.configuration.Configuration;
+import org.apache.flink.configuration.ReadableConfig;
 import org.apache.flink.table.data.RowData;
 import org.apache.iceberg.Schema;
 import org.apache.iceberg.encryption.EncryptionManager;
@@ -39,7 +39,7 @@ public class RowDataReaderFunction extends DataIteratorReaderFunction<RowData> {
   private final EncryptionManager encryption;
 
   public RowDataReaderFunction(
-      Configuration config, Schema tableSchema, Schema projectedSchema,
+      ReadableConfig config, Schema tableSchema, Schema projectedSchema,
       String nameMapping, boolean caseSensitive, FileIO io, EncryptionManager encryption) {
     super(new ArrayPoolDataIteratorBatcher<>(config, new RowDataRecordFactory(
         FlinkSchemaUtil.convert(readSchema(tableSchema, projectedSchema)))));

File: arrow/src/main/java/org/apache/iceberg/arrow/vectorized/ArrowReader.java
Patch:
@@ -268,6 +268,7 @@ private static final class VectorizedCombinedScanIterator implements CloseableIt
           .map(entry -> EncryptedFiles.encryptedInput(io.newInputFile(entry.getKey()), entry.getValue()));
 
       // decrypt with the batch call to avoid multiple RPCs to a key server, if possible
+      @SuppressWarnings("StreamToIterable")
       Iterable<InputFile> decryptedFiles = encryptionManager.decrypt(encrypted::iterator);
 
       Map<String, InputFile> files = Maps.newHashMapWithExpectedSize(fileTasks.size());

File: aws/src/integration/java/org/apache/iceberg/aws/glue/TestGlueCatalogTable.java
Patch:
@@ -404,7 +404,7 @@ public void testTablePropsDefinedAtCatalogLevel() {
         NestedField.required(4, "data", Types.StringType.get())
     );
 
-    org.apache.iceberg.Table table = glueCatalog.buildTable(tableIdent, schema)
+    Table table = glueCatalog.buildTable(tableIdent, schema)
         .withProperty("key2", "table-key2")
         .withProperty("key3", "table-key3")
         .withProperty("key5", "table-key5")

File: aws/src/main/java/org/apache/iceberg/aws/glue/GlueTableOperations.java
Patch:
@@ -133,7 +133,7 @@ protected void doCommit(TableMetadata base, TableMetadata metadata) {
     } catch (software.amazon.awssdk.services.glue.model.AlreadyExistsException e) {
       throw new AlreadyExistsException(e,
           "Cannot commit %s because its Glue table already exists when trying to create one", tableName());
-    } catch (software.amazon.awssdk.services.glue.model.EntityNotFoundException e) {
+    } catch (EntityNotFoundException e) {
       throw new NotFoundException(e,
           "Cannot commit %s because Glue cannot find the requested entity", tableName());
     } catch (software.amazon.awssdk.services.glue.model.AccessDeniedException e) {

File: core/src/main/java/org/apache/iceberg/util/ZOrderByteUtils.java
Patch:
@@ -166,7 +166,8 @@ static byte[] interleaveBits(byte[][] columnsBinary, int interleavedSize) {
    * @param interleavedSize the number of bytes to use in the output
    * @return the columnbytes interleaved
    */
-  @SuppressWarnings("ByteBufferBackingArray")
+  // NarrowingCompoundAssignment is intended here. See https://github.com/apache/iceberg/pull/5200#issuecomment-1176226163
+  @SuppressWarnings({"ByteBufferBackingArray", "NarrowingCompoundAssignment"})
   public static byte[] interleaveBits(byte[][] columnsBinary, int interleavedSize, ByteBuffer reuse) {
     byte[] interleavedBytes = reuse.array();
     Arrays.fill(interleavedBytes, 0, interleavedSize, (byte) 0x00);

File: hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java
Patch:
@@ -502,6 +502,7 @@ private StorageDescriptor storageDescriptor(TableMetadata metadata, boolean hive
     return storageDescriptor;
   }
 
+  @SuppressWarnings("ReverseDnsLookup")
   @VisibleForTesting
   long acquireLock() throws UnknownHostException, TException, InterruptedException {
     final LockComponent lockComponent = new LockComponent(LockType.EXCLUSIVE, LockLevel.TABLE, database);

File: core/src/main/java/org/apache/iceberg/ManifestFiles.java
Patch:
@@ -226,6 +226,7 @@ static ManifestFile copyRewriteManifest(int formatVersion,
     }
   }
 
+  @SuppressWarnings("Finally")
   private static ManifestFile copyManifestInternal(int formatVersion, ManifestReader<DataFile> reader,
                                                    OutputFile outputFile, long snapshotId,
                                                    SnapshotSummary.Builder summaryBuilder,

File: core/src/main/java/org/apache/iceberg/MetricsModes.java
Patch:
@@ -22,7 +22,6 @@
 import java.io.ObjectStreamException;
 import java.io.Serializable;
 import java.util.Locale;
-import java.util.Objects;
 import java.util.regex.Matcher;
 import java.util.regex.Pattern;
 import org.apache.iceberg.relocated.com.google.common.base.Preconditions;
@@ -134,7 +133,7 @@ public boolean equals(Object other) {
 
     @Override
     public int hashCode() {
-      return Objects.hash(length);
+      return Integer.hashCode(length);
     }
   }
 

File: core/src/main/java/org/apache/iceberg/avro/Avro.java
Patch:
@@ -577,6 +577,7 @@ public static class ReadBuilder {
     private org.apache.iceberg.Schema schema = null;
     private Function<Schema, DatumReader<?>> createReaderFunc = null;
     private BiFunction<org.apache.iceberg.Schema, Schema, DatumReader<?>> createReaderBiFunc = null;
+    @SuppressWarnings("UnnecessaryLambda")
     private final Function<Schema, DatumReader<?>> defaultCreateReaderFunc = readSchema -> {
       GenericAvroReader<?> reader = new GenericAvroReader<>(readSchema);
       reader.setClassLoader(loader);

File: core/src/main/java/org/apache/iceberg/encryption/InputFilesDecryptor.java
Patch:
@@ -43,6 +43,7 @@ public InputFilesDecryptor(CombinedScanTask combinedTask, FileIO io, EncryptionM
         .map(entry -> EncryptedFiles.encryptedInput(io.newInputFile(entry.getKey()), entry.getValue()));
 
     // decrypt with the batch call to avoid multiple RPCs to a key server, if possible
+    @SuppressWarnings("StreamToIterable")
     Iterable<InputFile> decryptedFiles = encryption.decrypt(encrypted::iterator);
 
     Map<String, InputFile> files = Maps.newHashMapWithExpectedSize(keyMetadata.size());

File: core/src/main/java/org/apache/iceberg/jdbc/JdbcCatalog.java
Patch:
@@ -196,7 +196,7 @@ public void renameTable(TableIdentifier from, TableIdentifier to) {
         err -> {
           // SQLite doesn't set SQLState or throw SQLIntegrityConstraintViolationException
           if (err instanceof SQLIntegrityConstraintViolationException ||
-              err.getMessage() != null && err.getMessage().contains("constraint failed")) {
+              (err.getMessage() != null && err.getMessage().contains("constraint failed"))) {
             throw new AlreadyExistsException("Table already exists: %s", to);
           }
         },

File: core/src/main/java/org/apache/iceberg/rest/CatalogHandlers.java
Patch:
@@ -20,6 +20,7 @@
 package org.apache.iceberg.rest;
 
 import java.time.OffsetDateTime;
+import java.time.ZoneOffset;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
@@ -166,7 +167,7 @@ public static LoadTableResponse stageTableCreate(Catalog catalog, Namespace name
     }
 
     Map<String, String> properties = Maps.newHashMap();
-    properties.put("created-at", OffsetDateTime.now().toString());
+    properties.put("created-at", OffsetDateTime.now(ZoneOffset.UTC).toString());
     properties.putAll(request.properties());
 
     String location;

File: core/src/main/java/org/apache/iceberg/rest/RESTSessionCatalog.java
Patch:
@@ -352,6 +352,7 @@ private ScheduledExecutorService tokenRefreshExecutor() {
     return refreshExecutor;
   }
 
+  @SuppressWarnings("FutureReturnValueIgnored")
   private void scheduleTokenRefresh(
       AuthSession session, long startTimeMillis, long expiresIn, TimeUnit unit) {
     // convert expiration interval to milliseconds

File: core/src/main/java/org/apache/iceberg/util/Tasks.java
Patch:
@@ -569,6 +569,7 @@ public static <I> Builder<I> foreach(I... items) {
     return new Builder<>(Arrays.asList(items));
   }
 
+  @SuppressWarnings("StreamToIterable")
   public static <I> Builder<I> foreach(Stream<I> items) {
     return new Builder<>(items::iterator);
   }

File: core/src/main/java/org/apache/iceberg/util/ZOrderByteUtils.java
Patch:
@@ -123,6 +123,7 @@ public static ByteBuffer doubleToOrderedBytes(double val, ByteBuffer reuse) {
    * This implementation just uses a set size to for all output byte representations. Truncating longer strings
    * and right padding 0 for shorter strings.
    */
+  @SuppressWarnings("ByteBufferBackingArray")
   public static ByteBuffer stringToOrderedBytes(String val, int length, ByteBuffer reuse, CharsetEncoder encoder) {
     Preconditions.checkArgument(encoder.charset().equals(StandardCharsets.UTF_8),
         "Cannot use an encoder not using UTF_8 as it's Charset");
@@ -140,6 +141,7 @@ public static ByteBuffer stringToOrderedBytes(String val, int length, ByteBuffer
    * Return a bytebuffer with the given bytes truncated to length, or filled with 0's to length depending on whether
    * the given bytes are larger or smaller than the given length.
    */
+  @SuppressWarnings("ByteBufferBackingArray")
   public static ByteBuffer byteTruncateOrFill(byte[] val, int length, ByteBuffer reuse) {
     ByteBuffer bytes = ByteBuffers.reuse(reuse, length);
     if (val.length < length) {
@@ -164,6 +166,7 @@ static byte[] interleaveBits(byte[][] columnsBinary, int interleavedSize) {
    * @param interleavedSize the number of bytes to use in the output
    * @return the columnbytes interleaved
    */
+  @SuppressWarnings("ByteBufferBackingArray")
   public static byte[] interleaveBits(byte[][] columnsBinary, int interleavedSize, ByteBuffer reuse) {
     byte[] interleavedBytes = reuse.array();
     Arrays.fill(interleavedBytes, 0, interleavedSize, (byte) 0x00);

File: data/src/test/java/org/apache/iceberg/data/TestReadProjection.java
Patch:
@@ -21,6 +21,7 @@
 
 import java.io.IOException;
 import java.time.OffsetDateTime;
+import java.time.ZoneOffset;
 import java.util.List;
 import java.util.Map;
 import org.apache.iceberg.Schema;
@@ -200,7 +201,7 @@ public void testBasicProjection() throws Exception {
     Record record = GenericRecord.create(writeSchema.asStruct());
     record.setField("id", 34L);
     record.setField("data", "test");
-    record.setField("time", OffsetDateTime.now());
+    record.setField("time", OffsetDateTime.now(ZoneOffset.UTC));
 
     Schema idOnly = new Schema(
         Types.NestedField.required(0, "id", Types.LongType.get())

File: flink/v1.13/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java
Patch:
@@ -491,7 +491,7 @@ static IcebergStreamWriter<RowData> createStreamWriter(Table table,
                                                          RowType flinkRowType,
                                                          List<Integer> equalityFieldIds,
                                                          boolean upsert) {
-    Preconditions.checkArgument(table != null, "Iceberg table should't be null");
+    Preconditions.checkArgument(table != null, "Iceberg table shouldn't be null");
     Map<String, String> props = table.properties();
     long targetFileSize = getTargetFileSizeBytes(props);
     FileFormat fileFormat = getFileFormat(props);

File: flink/v1.14/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java
Patch:
@@ -514,7 +514,7 @@ static IcebergStreamWriter<RowData> createStreamWriter(Table table,
                                                          RowType flinkRowType,
                                                          List<Integer> equalityFieldIds,
                                                          boolean upsert) {
-    Preconditions.checkArgument(table != null, "Iceberg table should't be null");
+    Preconditions.checkArgument(table != null, "Iceberg table shouldn't be null");
     Map<String, String> props = table.properties();
     long targetFileSize = getTargetFileSizeBytes(props);
     FileFormat fileFormat = getFileFormat(props);

File: flink/v1.15/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java
Patch:
@@ -511,7 +511,7 @@ static IcebergStreamWriter<RowData> createStreamWriter(Table table,
                                                          FlinkWriteConf flinkWriteConf,
                                                          RowType flinkRowType,
                                                          List<Integer> equalityFieldIds) {
-    Preconditions.checkArgument(table != null, "Iceberg table should't be null");
+    Preconditions.checkArgument(table != null, "Iceberg table shouldn't be null");
 
     Table serializableTable = SerializableTable.copyOf(table);
     TaskWriterFactory<RowData> taskWriterFactory = new RowDataTaskWriterFactory(

File: parquet/src/main/java/org/apache/iceberg/parquet/ParquetUtil.java
Patch:
@@ -345,7 +345,7 @@ public static boolean hasNonDictionaryPages(ColumnChunkMetaData meta) {
   }
 
   public static boolean hasNoBloomFilterPages(ColumnChunkMetaData meta) {
-    return meta.getBloomFilterOffset() == -1;
+    return meta.getBloomFilterOffset() <= 0;
   }
 
   public static Dictionary readDictionary(ColumnDescriptor desc, PageReader pageSource) {

File: api/src/main/java/org/apache/iceberg/util/ByteBuffers.java
Patch:
@@ -51,7 +51,7 @@ public static ByteBuffer reuse(ByteBuffer reuse, int length) {
     Preconditions.checkArgument(reuse.hasArray(), "Cannot reuse a buffer not backed by an array");
     Preconditions.checkArgument(reuse.arrayOffset() == 0, "Cannot reuse a buffer whose array offset is not 0");
     Preconditions.checkArgument(reuse.capacity() == length,
-        "Canout use a buffer whose capacity (%s) is not equal to the requested length (%s)", length, reuse.capacity());
+        "Cannot use a buffer whose capacity (%s) is not equal to the requested length (%s)", length, reuse.capacity());
     reuse.position(0);
     reuse.limit(length);
     return reuse;

File: spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/actions/TestExpireSnapshotsAction.java
Patch:
@@ -1147,7 +1147,7 @@ public void testUseLocalIterator() {
       checkExpirationResults(1L, 0L, 0L, 1L, 2L, results);
 
       Assert.assertEquals("Expected total number of jobs with stream-results should match the expected number",
-          5L, jobsRunDuringStreamResults);
+          4L, jobsRunDuringStreamResults);
     });
   }
 }

File: spark/v3.2/spark/src/test/java/org/apache/iceberg/spark/actions/TestExpireSnapshotsAction.java
Patch:
@@ -1147,7 +1147,7 @@ public void testUseLocalIterator() {
       checkExpirationResults(1L, 0L, 0L, 1L, 2L, results);
 
       Assert.assertEquals("Expected total number of jobs with stream-results should match the expected number",
-          5L, jobsRunDuringStreamResults);
+          4L, jobsRunDuringStreamResults);
     });
   }
 }

File: core/src/main/java/org/apache/iceberg/rest/RESTObjectMapper.java
Patch:
@@ -24,6 +24,7 @@
 import com.fasterxml.jackson.core.JsonFactory;
 import com.fasterxml.jackson.databind.DeserializationFeature;
 import com.fasterxml.jackson.databind.ObjectMapper;
+import com.fasterxml.jackson.databind.PropertyNamingStrategy;
 
 class RESTObjectMapper {
   private static final JsonFactory FACTORY = new JsonFactory();
@@ -39,6 +40,7 @@ static ObjectMapper mapper() {
         if (!isInitialized) {
           MAPPER.setVisibility(PropertyAccessor.FIELD, JsonAutoDetect.Visibility.ANY);
           MAPPER.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false);
+          MAPPER.setPropertyNamingStrategy(new PropertyNamingStrategy.KebabCaseStrategy());
           RESTSerializers.registerAll(MAPPER);
           isInitialized = true;
         }

File: core/src/main/java/org/apache/iceberg/actions/RewriteDataFilesCommitManager.java
Patch:
@@ -74,8 +74,8 @@ public void commitFileGroups(Set<RewriteFileGroup> fileGroups) {
     Set<DataFile> rewrittenDataFiles = Sets.newHashSet();
     Set<DataFile> addedDataFiles = Sets.newHashSet();
     for (RewriteFileGroup group : fileGroups) {
-      rewrittenDataFiles = Sets.union(rewrittenDataFiles, group.rewrittenFiles());
-      addedDataFiles = Sets.union(addedDataFiles, group.addedFiles());
+      rewrittenDataFiles.addAll(group.rewrittenFiles());
+      addedDataFiles.addAll(group.addedFiles());
     }
 
     RewriteFiles rewrite = table.newRewrite().validateFromSnapshot(startingSnapshotId);

File: core/src/main/java/org/apache/iceberg/rest/requests/RenameTableRequest.java
Patch:
@@ -43,6 +43,7 @@ private RenameTableRequest(TableIdentifier source, TableIdentifier destination)
     validate();
   }
 
+  @Override
   public void validate() {
     Preconditions.checkArgument(source != null, "Invalid source table: null");
     Preconditions.checkArgument(destination != null, "Invalid destination table: null");

File: core/src/main/java/org/apache/iceberg/rest/responses/OAuthTokenResponse.java
Patch:
@@ -80,6 +80,9 @@ public static class Builder {
     private Integer expiresInSeconds;
     private final List<String> scopes = Lists.newArrayList();
 
+    private Builder() {
+    }
+
     public Builder withToken(String token) {
       this.accessToken = token;
       return this;

File: core/src/main/java/org/apache/iceberg/PartitionData.java
Patch:
@@ -110,16 +110,15 @@ public int size() {
   }
 
   @Override
-  @SuppressWarnings("unchecked")
   public <T> T get(int pos, Class<T> javaClass) {
     Object value = get(pos);
     if (value == null || javaClass.isInstance(value)) {
       return javaClass.cast(value);
     }
 
     throw new IllegalArgumentException(String.format(
-        "Wrong class, %s, for object: %s",
-        javaClass.getName(), String.valueOf(value)));
+        "Wrong class, expected %s, but was %s, for object: %s",
+        javaClass.getName(), value.getClass().getName(), value));
   }
 
   @Override

File: parquet/src/main/java/org/apache/iceberg/parquet/PruneColumns.java
Patch:
@@ -164,8 +164,8 @@ private boolean isStruct(Type field) {
     } else {
       GroupType groupType = field.asGroupType();
       LogicalTypeAnnotation logicalTypeAnnotation = groupType.getLogicalTypeAnnotation();
-      return !logicalTypeAnnotation.equals(LogicalTypeAnnotation.mapType()) &&
-          !logicalTypeAnnotation.equals(LogicalTypeAnnotation.listType());
+      return !LogicalTypeAnnotation.mapType().equals(logicalTypeAnnotation) &&
+          !LogicalTypeAnnotation.listType().equals(logicalTypeAnnotation);
     }
   }
 }

File: spark/v3.2/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveOrphanFilesAction.java
Patch:
@@ -420,7 +420,7 @@ public void testOlderThanTimestamp() throws InterruptedException {
 
     long timestamp = System.currentTimeMillis();
 
-    waitUntilAfter(System.currentTimeMillis());
+    waitUntilAfter(System.currentTimeMillis() + 1000L);
 
     df.write().mode("append").parquet(tableLocation + "/data/c2_trunc=AA/c3=AAAA");
 

File: parquet/src/main/java/org/apache/iceberg/parquet/BasePageIterator.java
Patch:
@@ -163,7 +163,7 @@ int nextInt() {
   IntIterator newRLEIterator(int maxLevel, BytesInput bytes) {
     try {
       if (maxLevel == 0) {
-        return new PageIterator.NullIntIterator();
+        return new NullIntIterator();
       }
       return new RLEIntIterator(
           new RunLengthBitPackingHybridDecoder(

File: orc/src/main/java/org/apache/iceberg/orc/IdToOrcName.java
Patch:
@@ -36,7 +36,7 @@
  * <p>
  * This visitor also enclose column names in backticks i.e. ` so that ORC can correctly parse column names with
  * special characters. A comparison of ORC convention with Iceberg convention is provided below
- * <pre><code>
+ * <pre>{@code
  *                                      Iceberg           ORC
  * field                                field             field
  * struct -> field                      struct.field      struct.field
@@ -46,7 +46,7 @@
  * map -> value                         map.value         map._value
  * map -> struct key -> field           map.key.field     map._key.field
  * map -> struct value -> field         map.field         map._value.field
- * </code></pre>
+ * }</pre>
  */
 class IdToOrcName extends TypeUtil.SchemaVisitor<Map<Integer, String>> {
   private static final Joiner DOT = Joiner.on(".");

File: core/src/main/java/org/apache/iceberg/rest/RESTSessionCatalog.java
Patch:
@@ -369,6 +369,7 @@ public void close() throws IOException {
       ScheduledExecutorService service = refreshExecutor;
       this.refreshExecutor = null;
 
+      service.shutdown();
       try {
         if (service.awaitTermination(1, TimeUnit.MINUTES)) {
           LOG.warn("Timed out waiting for refresh executor to terminate");

File: spark/v3.2/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveOrphanFilesAction.java
Patch:
@@ -303,7 +303,7 @@ public void orphanedFileRemovedWithParallelTasks() throws InterruptedException,
 
     DeleteOrphanFiles.Result result = SparkActions.get().deleteOrphanFiles(table)
             .executeDeleteWith(executorService)
-            .olderThan(System.currentTimeMillis())
+            .olderThan(System.currentTimeMillis() + 5000)  // Ensure all orphan files are selected
             .deleteWith(file -> {
               deleteThreads.add(Thread.currentThread().getName());
               deletedFiles.add(file);

File: aws/src/integration/java/org/apache/iceberg/aws/glue/TestGlueCatalogCommitFailure.java
Patch:
@@ -263,7 +263,7 @@ public void testCreateTableWithInvalidDB() {
     AssertHelpers.assertThrows(
             "Should throw not found exception",
             NotFoundException.class,
-            "Cannot commit because Glue cannot find the requested entity",
+            "because Glue cannot find the requested entity",
             () -> spyOps.commit(metadataV2, metadataV1));
 
     ops.refresh();
@@ -286,7 +286,7 @@ public void testGlueAccessDeniedException() {
     AssertHelpers.assertThrows(
             "Should throw forbidden exception",
             ForbiddenException.class,
-            "Cannot commit because Glue cannot access the requested resources",
+            "because Glue cannot access the requested resources",
             () -> spyOps.commit(metadataV2, metadataV1));
 
     ops.refresh();
@@ -309,7 +309,7 @@ public void testGlueValidationException() {
     AssertHelpers.assertThrows(
             "Should throw validation exception",
             org.apache.iceberg.exceptions.ValidationException.class,
-            "Cannot commit because Glue encountered a validation exception while accessing requested resources",
+            "because Glue encountered a validation exception while accessing requested resources",
             () -> spyOps.commit(metadataV2, metadataV1));
 
     ops.refresh();

File: flink/v1.13/flink/src/main/java/org/apache/iceberg/flink/sink/BaseDeltaTaskWriter.java
Patch:
@@ -87,7 +87,7 @@ public void write(RowData row) throws IOException {
 
       case UPDATE_BEFORE:
         if (upsert) {
-          break;  // UPDATE_BEFORE is not necessary for UPDATE, we do nothing to prevent delete one row twice
+          break;  // UPDATE_BEFORE is not necessary for UPSERT, we do nothing to prevent delete one row twice
         }
         writer.delete(row);
         break;

File: flink/v1.14/flink/src/main/java/org/apache/iceberg/flink/sink/BaseDeltaTaskWriter.java
Patch:
@@ -87,7 +87,7 @@ public void write(RowData row) throws IOException {
 
       case UPDATE_BEFORE:
         if (upsert) {
-          break;  // UPDATE_BEFORE is not necessary for UPDATE, we do nothing to prevent delete one row twice
+          break;  // UPDATE_BEFORE is not necessary for UPSERT, we do nothing to prevent delete one row twice
         }
         writer.delete(row);
         break;

File: flink/v1.15/flink/src/main/java/org/apache/iceberg/flink/sink/BaseDeltaTaskWriter.java
Patch:
@@ -87,7 +87,7 @@ public void write(RowData row) throws IOException {
 
       case UPDATE_BEFORE:
         if (upsert) {
-          break;  // UPDATE_BEFORE is not necessary for UPDATE, we do nothing to prevent delete one row twice
+          break;  // UPDATE_BEFORE is not necessary for UPSERT, we do nothing to prevent delete one row twice
         }
         writer.delete(row);
         break;

File: core/src/main/java/org/apache/iceberg/PartitionsTable.java
Patch:
@@ -152,7 +152,7 @@ static CloseableIterable<FileScanTask> planFiles(StaticTableScan scan) {
 
     LoadingCache<Integer, ManifestEvaluator> evalCache = Caffeine.newBuilder().build(specId -> {
       PartitionSpec spec = table.specs().get(specId);
-      PartitionSpec transformedSpec = transformSpec(scan.schema(), spec);
+      PartitionSpec transformedSpec = transformSpec(scan.tableSchema(), spec);
       return ManifestEvaluator.forRowFilter(scan.filter(), transformedSpec, caseSensitive);
     });
 

File: api/src/main/java/org/apache/iceberg/PendingUpdate.java
Patch:
@@ -20,6 +20,7 @@
 package org.apache.iceberg;
 
 import org.apache.iceberg.exceptions.CommitFailedException;
+import org.apache.iceberg.exceptions.CommitStateUnknownException;
 import org.apache.iceberg.exceptions.ValidationException;
 
 /**
@@ -49,6 +50,8 @@ public interface PendingUpdate<T> {
    *
    * @throws ValidationException If the update cannot be applied to the current table metadata.
    * @throws CommitFailedException If the update cannot be committed due to conflicts.
+   * @throws CommitStateUnknownException If the update success or failure is unknown, no cleanup should be done in
+   * this case.
    */
   void commit();
 

File: core/src/main/java/org/apache/iceberg/util/JsonUtil.java
Patch:
@@ -59,7 +59,7 @@ public static int getInt(String property, JsonNode node) {
   }
 
   public static Integer getIntOrNull(String property, JsonNode node) {
-    if (!node.has(property)) {
+    if (!node.hasNonNull(property)) {
       return null;
     }
     JsonNode pNode = node.get(property);
@@ -69,7 +69,7 @@ public static Integer getIntOrNull(String property, JsonNode node) {
   }
 
   public static Long getLongOrNull(String property, JsonNode node) {
-    if (!node.has(property)) {
+    if (!node.hasNonNull(property)) {
       return null;
     }
     JsonNode pNode = node.get(property);

File: core/src/main/java/org/apache/iceberg/actions/ConvertEqualityDeleteStrategy.java
Patch:
@@ -49,7 +49,7 @@ public interface ConvertEqualityDeleteStrategy {
   /**
    * Sets options to be used with this strategy
    */
-  RewritePositionDeleteStrategy options(Map<String, String> options);
+  ConvertEqualityDeleteStrategy options(Map<String, String> options);
 
   /**
    * Select the delete files to convert.

File: core/src/test/java/org/apache/iceberg/rest/requests/TestCreateNamespaceRequest.java
Patch:
@@ -90,7 +90,8 @@ public void testDeserializeInvalidRequest() {
     String jsonMisspelledKeys = "{\"namepsace\":[\"accounting\",\"tax\"],\"propertiezzzz\":{\"owner\":\"Hank\"}}";
     AssertHelpers.assertThrows(
         "A JSON request with the keys spelled incorrectly should fail to deserialize and validate",
-        JsonProcessingException.class, "Unrecognized field \"namepsace\"",
+        IllegalArgumentException.class,
+        "Invalid namespace: null",
         () -> deserialize(jsonMisspelledKeys)
     );
 

File: core/src/test/java/org/apache/iceberg/rest/responses/TestCreateNamespaceResponse.java
Patch:
@@ -101,7 +101,8 @@ public void testDeserializeInvalidResponse() {
     String jsonMisspelledKeys = "{\"namepsace\":[\"accounting\",\"tax\"],\"propertiezzzz\":{\"owner\":\"Hank\"}}";
     AssertHelpers.assertThrows(
         "A JSON response with the keys spelled incorrectly should fail to deserialize and validate",
-        JsonProcessingException.class,
+        IllegalArgumentException.class,
+        "Invalid namespace: null",
         () -> deserialize(jsonMisspelledKeys)
     );
 

File: core/src/test/java/org/apache/iceberg/rest/responses/TestGetNamespaceResponse.java
Patch:
@@ -88,7 +88,8 @@ public void testDeserializeInvalidResponse() {
         "{\"namepsace\":[\"accounting\",\"tax\"],\"propertiezzzz\":{\"owner\":\"Hank\"}}";
     AssertHelpers.assertThrows(
         "A JSON response with the keys spelled incorrectly should fail to deserialize",
-        JsonProcessingException.class,
+        IllegalArgumentException.class,
+        "Invalid namespace: null",
         () -> deserialize(jsonWithKeysSpelledIncorrectly)
     );
 

File: core/src/test/java/org/apache/iceberg/rest/responses/TestListNamespacesResponse.java
Patch:
@@ -67,7 +67,8 @@ public void testDeserializeInvalidResponseThrows() {
         "{\"namepsacezz\":[\"accounting\",\"tax\"]}";
     AssertHelpers.assertThrows(
         "A JSON response with the keys spelled incorrectly should fail to deserialize",
-        JsonProcessingException.class,
+        IllegalArgumentException.class,
+        "Invalid namespace: null",
         () -> deserialize(jsonWithKeysSpelledIncorrectly)
     );
 

File: core/src/test/java/org/apache/iceberg/rest/responses/TestListTablesResponse.java
Patch:
@@ -64,7 +64,8 @@ public void testDeserializeInvalidResponsesThrows() {
         "{\"identifyrezzzz\":[{\"namespace\":[\"accounting\",\"tax\"],\"name\":\"paid\"}]}";
     AssertHelpers.assertThrows(
         "A JSON response with the keys spelled incorrectly should fail to deserialize",
-        JsonProcessingException.class,
+        IllegalArgumentException.class,
+        "Invalid identifier list: null",
         () -> deserialize(jsonWithKeysSpelledIncorrectly));
 
     String jsonWithInvalidIdentifiersInList =

File: mr/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergStorageHandlerNoScan.java
Patch:
@@ -624,7 +624,7 @@ public void testIcebergAndHmsTableProperties() throws Exception {
     Assert.assertEquals(expectedIcebergProperties, icebergTable.properties());
 
     if (Catalogs.hiveCatalog(shell.getHiveConf(), tableProperties)) {
-      Assert.assertEquals(12, hmsParams.size());
+      Assert.assertEquals(13, hmsParams.size());
       Assert.assertEquals("initial_val", hmsParams.get("custom_property"));
       Assert.assertEquals("TRUE", hmsParams.get(InputFormatConfig.EXTERNAL_TABLE_PURGE));
       Assert.assertEquals("TRUE", hmsParams.get("EXTERNAL"));
@@ -662,7 +662,7 @@ public void testIcebergAndHmsTableProperties() throws Exception {
         .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));
 
     if (Catalogs.hiveCatalog(shell.getHiveConf(), tableProperties)) {
-      Assert.assertEquals(15, hmsParams.size()); // 2 newly-added properties + previous_metadata_location prop
+      Assert.assertEquals(16, hmsParams.size()); // 2 newly-added properties + previous_metadata_location prop
       Assert.assertEquals("true", hmsParams.get("new_prop_1"));
       Assert.assertEquals("false", hmsParams.get("new_prop_2"));
       Assert.assertEquals("new_val", hmsParams.get("custom_property"));

File: nessie/src/main/java/org/apache/iceberg/nessie/UpdateableReference.java
Patch:
@@ -19,7 +19,6 @@
 
 package org.apache.iceberg.nessie;
 
-import java.util.Objects;
 import org.apache.iceberg.relocated.com.google.common.base.Preconditions;
 import org.projectnessie.client.api.NessieApiV1;
 import org.projectnessie.error.NessieNotFoundException;
@@ -51,7 +50,7 @@ public boolean refresh(NessieApiV1 api) throws NessieNotFoundException {
 
   public void updateReference(Reference ref) {
     Preconditions.checkState(mutable, "Hash references cannot be updated.");
-    this.reference = Objects.requireNonNull(ref);
+    this.reference = Preconditions.checkNotNull(ref, "ref is null");
   }
 
   public boolean isBranch() {

File: spark/v3.2/spark/src/test/java/org/apache/iceberg/spark/actions/TestCreateActions.java
Patch:
@@ -28,7 +28,6 @@
 import java.util.Collections;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 import java.util.stream.Collectors;
 import org.apache.commons.io.FileUtils;
 import org.apache.commons.io.filefilter.TrueFileFilter;
@@ -37,6 +36,7 @@
 import org.apache.iceberg.Table;
 import org.apache.iceberg.actions.MigrateTable;
 import org.apache.iceberg.actions.SnapshotTable;
+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;
 import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;
 import org.apache.iceberg.relocated.com.google.common.collect.Lists;
 import org.apache.iceberg.relocated.com.google.common.collect.Maps;
@@ -671,7 +671,7 @@ public void testTwoLevelList() throws IOException {
             JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(testData))
         .coalesce(1).write().format("parquet").mode(SaveMode.Append).save(location.getPath());
 
-    File parquetFile = Arrays.stream(Objects.requireNonNull(location.listFiles(new FilenameFilter() {
+    File parquetFile = Arrays.stream(Preconditions.checkNotNull(location.listFiles(new FilenameFilter() {
       @Override
       public boolean accept(File dir, String name) {
         return name.endsWith("parquet");

File: api/src/test/java/org/apache/iceberg/types/TestReadabilityChecks.java
Patch:
@@ -362,7 +362,6 @@ public void testStructWriteReordering() {
     List<String> errors = CheckCompatibility.writeCompatibilityErrors(read, write);
     Assert.assertEquals("Should produce 1 error message", 1, errors.size());
 
-    System.err.println(errors);
     Assert.assertTrue("Should complain about field_b before field_a",
         errors.get(0).contains("field_b is out of order, before field_a"));
   }

File: core/src/test/java/org/apache/iceberg/TestSchemaUnionByFieldName.java
Patch:
@@ -198,7 +198,6 @@ public void testDetectInvalidTopLevelMapValue() {
     Schema newSchema = new Schema(optional(1, "aMap",
         Types.MapType.ofOptional(2, 3, StringType.get(), LongType.get())));
     Schema apply = new SchemaUpdate(currentSchema, 3).unionByNameWith(newSchema).apply();
-    System.out.println(apply.toString());
   }
 
   @Test

File: core/src/test/java/org/apache/iceberg/jdbc/TestJdbcCatalog.java
Patch:
@@ -486,7 +486,6 @@ public void testListNamespace() {
 
     List<Namespace> nsp3 = catalog.listNamespaces();
     Set<String> tblSet2 = Sets.newHashSet(nsp3.stream().map(Namespace::toString).iterator());
-    System.out.println(tblSet2.toString());
     Assert.assertEquals(tblSet2.size(), 3);
     Assert.assertTrue(tblSet2.contains("db"));
     Assert.assertTrue(tblSet2.contains("db2"));

File: spark/v2.4/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkDateTimes.java
Patch:
@@ -46,7 +46,6 @@ public void testSparkDate() {
   public void checkSparkDate(String dateString) {
     Literal<Integer> date = Literal.of(dateString).to(Types.DateType.get());
     String sparkDate = DateTimeUtils.toJavaDate(date.value()).toString();
-    System.err.println(dateString + ": " + date.value());
     Assert.assertEquals("Should be the same date (" + date.value() + ")", dateString, sparkDate);
   }
 
@@ -66,7 +65,6 @@ public void testSparkTimestamp() {
   public void checkSparkTimestamp(String timestampString, String sparkRepr) {
     Literal<Long> ts = Literal.of(timestampString).to(Types.TimestampType.withZone());
     String sparkTimestamp = DateTimeUtils.timestampToString(ts.value());
-    System.err.println(timestampString + ": " + ts.value());
     Assert.assertEquals("Should be the same timestamp (" + ts.value() + ")",
         sparkRepr, sparkTimestamp);
   }

File: spark/v3.0/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkDateTimes.java
Patch:
@@ -48,7 +48,6 @@ public void testSparkDate() {
   public void checkSparkDate(String dateString) {
     Literal<Integer> date = Literal.of(dateString).to(Types.DateType.get());
     String sparkDate = DateTimeUtils.toJavaDate(date.value()).toString();
-    System.err.println(dateString + ": " + date.value());
     Assert.assertEquals("Should be the same date (" + date.value() + ")", dateString, sparkDate);
   }
 
@@ -70,7 +69,6 @@ public void checkSparkTimestamp(String timestampString, String sparkRepr) {
     ZoneId zoneId = DateTimeUtils.getZoneId("UTC");
     TimestampFormatter formatter = TimestampFormatter.getFractionFormatter(zoneId);
     String sparkTimestamp = DateTimeUtils.timestampToString(formatter, ts.value());
-    System.err.println(timestampString + ": " + ts.value());
     Assert.assertEquals("Should be the same timestamp (" + ts.value() + ")",
         sparkRepr, sparkTimestamp);
   }

File: spark/v3.1/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkDateTimes.java
Patch:
@@ -48,7 +48,6 @@ public void testSparkDate() {
   public void checkSparkDate(String dateString) {
     Literal<Integer> date = Literal.of(dateString).to(Types.DateType.get());
     String sparkDate = DateTimeUtils.toJavaDate(date.value()).toString();
-    System.err.println(dateString + ": " + date.value());
     Assert.assertEquals("Should be the same date (" + date.value() + ")", dateString, sparkDate);
   }
 
@@ -70,7 +69,6 @@ public void checkSparkTimestamp(String timestampString, String sparkRepr) {
     ZoneId zoneId = DateTimeUtils.getZoneId("UTC");
     TimestampFormatter formatter = TimestampFormatter.getFractionFormatter(zoneId);
     String sparkTimestamp = formatter.format(ts.value());
-    System.err.println(timestampString + ": " + ts.value());
     Assert.assertEquals("Should be the same timestamp (" + ts.value() + ")",
         sparkRepr, sparkTimestamp);
   }

File: spark/v3.2/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkDateTimes.java
Patch:
@@ -48,7 +48,6 @@ public void testSparkDate() {
   public void checkSparkDate(String dateString) {
     Literal<Integer> date = Literal.of(dateString).to(Types.DateType.get());
     String sparkDate = DateTimeUtils.toJavaDate(date.value()).toString();
-    System.err.println(dateString + ": " + date.value());
     Assert.assertEquals("Should be the same date (" + date.value() + ")", dateString, sparkDate);
   }
 
@@ -70,7 +69,6 @@ public void checkSparkTimestamp(String timestampString, String sparkRepr) {
     ZoneId zoneId = DateTimeUtils.getZoneId("UTC");
     TimestampFormatter formatter = TimestampFormatter.getFractionFormatter(zoneId);
     String sparkTimestamp = formatter.format(ts.value());
-    System.err.println(timestampString + ": " + ts.value());
     Assert.assertEquals("Should be the same timestamp (" + ts.value() + ")",
         sparkRepr, sparkTimestamp);
   }

File: core/src/main/java/org/apache/iceberg/MergingSnapshotProducer.java
Patch:
@@ -690,7 +690,7 @@ public List<ManifestFile> apply(TableMetadata base) {
         base.schema(), current != null ? current.dataManifests() : null);
     long minDataSequenceNumber = filtered.stream()
         .map(ManifestFile::minSequenceNumber)
-        .filter(seq -> seq > 0) // filter out unassigned sequence numbers in rewritten manifests
+        .filter(seq -> seq != ManifestWriter.UNASSIGNED_SEQ) // filter out unassigned in rewritten manifests
         .reduce(base.lastSequenceNumber(), Math::min);
     deleteFilterManager.dropDeleteFilesOlderThan(minDataSequenceNumber);
     List<ManifestFile> filteredDeletes = deleteFilterManager.filterManifests(

File: nessie/src/test/java/org/apache/iceberg/nessie/TestBranchVisibility.java
Patch:
@@ -63,7 +63,6 @@ public TestBranchVisibility() {
   public void before() throws NessieNotFoundException, NessieConflictException {
     createTable(tableIdentifier1, 1); // table 1
     createTable(tableIdentifier2, 1); // table 2
-    catalog.refresh();
     createBranch("test", catalog.currentHash());
     testCatalog = initCatalog("test");
   }
@@ -72,7 +71,6 @@ public void before() throws NessieNotFoundException, NessieConflictException {
   public void after() throws NessieNotFoundException, NessieConflictException {
     catalog.dropTable(tableIdentifier1);
     catalog.dropTable(tableIdentifier2);
-    catalog.refresh();
     for (Reference reference : api.getAllReferences().get().getReferences()) {
       if (!reference.getName().equals("main")) {
         api.deleteBranch().branch((Branch) reference).delete();
@@ -147,7 +145,6 @@ public void testSchemaSnapshot() throws Exception {
     String metadataOnTest = addRow(catalog, tableIdentifier1, "initial-data",
         ImmutableMap.of("id0", 4L));
     long snapshotIdOnTest = snapshotIdFromMetadata(catalog, metadataOnTest);
-    catalog.refresh();
 
     String hashOnTest = catalog.currentHash();
     createBranch(branch1, hashOnTest, branchTest);

File: spark/v2.4/spark/src/main/java/org/apache/iceberg/spark/SparkFixupTimestampType.java
Patch:
@@ -29,7 +29,7 @@
  * By default Spark type {@link org.apache.iceberg.types.Types.TimestampType} should be converted to
  * {@link Types.TimestampType#withZone()} iceberg type. But we also can convert
  * {@link org.apache.iceberg.types.Types.TimestampType} to {@link Types.TimestampType#withoutZone()} iceberg type
- * by setting {@link SparkUtil#USE_TIMESTAMP_WITHOUT_TIME_ZONE_IN_NEW_TABLES} to 'true'
+ * by setting {@link SparkSQLProperties#USE_TIMESTAMP_WITHOUT_TIME_ZONE_IN_NEW_TABLES} to 'true'
  */
 class SparkFixupTimestampType extends FixupTypes {
 

File: spark/v3.0/spark/src/main/java/org/apache/iceberg/spark/SparkFixupTimestampType.java
Patch:
@@ -29,7 +29,7 @@
  * By default Spark type {@link org.apache.iceberg.types.Types.TimestampType} should be converted to
  * {@link Types.TimestampType#withZone()} iceberg type. But we also can convert
  * {@link org.apache.iceberg.types.Types.TimestampType} to {@link Types.TimestampType#withoutZone()} iceberg type
- * by setting {@link SparkUtil#USE_TIMESTAMP_WITHOUT_TIME_ZONE_IN_NEW_TABLES} to 'true'
+ * by setting {@link SparkSQLProperties#USE_TIMESTAMP_WITHOUT_TIME_ZONE_IN_NEW_TABLES} to 'true'
  */
 class SparkFixupTimestampType extends FixupTypes {
 

File: spark/v3.1/spark/src/main/java/org/apache/iceberg/spark/SparkFixupTimestampType.java
Patch:
@@ -29,7 +29,7 @@
  * By default Spark type {@link org.apache.iceberg.types.Types.TimestampType} should be converted to
  * {@link Types.TimestampType#withZone()} iceberg type. But we also can convert
  * {@link org.apache.iceberg.types.Types.TimestampType} to {@link Types.TimestampType#withoutZone()} iceberg type
- * by setting {@link SparkUtil#USE_TIMESTAMP_WITHOUT_TIME_ZONE_IN_NEW_TABLES} to 'true'
+ * by setting {@link SparkSQLProperties#USE_TIMESTAMP_WITHOUT_TIME_ZONE_IN_NEW_TABLES} to 'true'
  */
 class SparkFixupTimestampType extends FixupTypes {
 

File: spark/v3.2/spark/src/main/java/org/apache/iceberg/spark/SparkFixupTimestampType.java
Patch:
@@ -29,7 +29,7 @@
  * By default Spark type {@link org.apache.iceberg.types.Types.TimestampType} should be converted to
  * {@link Types.TimestampType#withZone()} iceberg type. But we also can convert
  * {@link org.apache.iceberg.types.Types.TimestampType} to {@link Types.TimestampType#withoutZone()} iceberg type
- * by setting {@link SparkUtil#USE_TIMESTAMP_WITHOUT_TIME_ZONE_IN_NEW_TABLES} to 'true'
+ * by setting {@link SparkSQLProperties#USE_TIMESTAMP_WITHOUT_TIME_ZONE_IN_NEW_TABLES} to 'true'
  */
 class SparkFixupTimestampType extends FixupTypes {
 

File: api/src/main/java/org/apache/iceberg/util/UUIDUtil.java
Patch:
@@ -37,9 +37,9 @@ public static UUID convert(byte[] buf) {
 
   public static UUID convert(byte[] buf, int offset) {
     Preconditions.checkArgument(offset >= 0 && offset < buf.length,
-        "Offset overflow, offset=%d, length=%d", offset, buf.length);
+        "Offset overflow, offset=%s, length=%s", offset, buf.length);
     Preconditions.checkArgument(offset + 16 <= buf.length,
-        "UUID require 16 bytes, offset=%d, length=%d", offset, buf.length);
+        "UUID require 16 bytes, offset=%s, length=%s", offset, buf.length);
 
     ByteBuffer bb = ByteBuffer.wrap(buf, offset, 16);
     bb.order(ByteOrder.BIG_ENDIAN);

File: flink/v1.13/flink/src/main/java/org/apache/iceberg/flink/actions/RewriteDataFilesAction.java
Patch:
@@ -65,7 +65,7 @@ protected RewriteDataFilesAction self() {
   }
 
   public RewriteDataFilesAction maxParallelism(int parallelism) {
-    Preconditions.checkArgument(parallelism > 0, "Invalid max parallelism %d", parallelism);
+    Preconditions.checkArgument(parallelism > 0, "Invalid max parallelism %s", parallelism);
     this.maxParallelism = parallelism;
     return this;
   }

File: flink/v1.14/flink/src/main/java/org/apache/iceberg/flink/actions/RewriteDataFilesAction.java
Patch:
@@ -65,7 +65,7 @@ protected RewriteDataFilesAction self() {
   }
 
   public RewriteDataFilesAction maxParallelism(int parallelism) {
-    Preconditions.checkArgument(parallelism > 0, "Invalid max parallelism %d", parallelism);
+    Preconditions.checkArgument(parallelism > 0, "Invalid max parallelism %s", parallelism);
     this.maxParallelism = parallelism;
     return this;
   }

File: flink/v1.14/flink/src/main/java/org/apache/iceberg/flink/source/DataIterator.java
Patch:
@@ -76,7 +76,7 @@ public void seek(int startingFileOffset, long startingRecordOffset) {
         "Seek should be called before any other iterator actions");
     // skip files
     Preconditions.checkState(startingFileOffset < combinedTask.files().size(),
-        "Invalid starting file offset %d for combined scan task with %d files: %s",
+        "Invalid starting file offset %s for combined scan task with %s files: %s",
         startingFileOffset, combinedTask.files().size(), combinedTask);
     for (long i = 0L; i < startingFileOffset; ++i) {
       tasks.next();

File: spark/v2.4/spark/src/main/java/org/apache/iceberg/spark/actions/BaseRewriteManifestsSparkAction.java
Patch:
@@ -123,7 +123,7 @@ protected RewriteManifests self() {
 
   @Override
   public RewriteManifests specId(int specId) {
-    Preconditions.checkArgument(table.specs().containsKey(specId), "Invalid spec id %d", specId);
+    Preconditions.checkArgument(table.specs().containsKey(specId), "Invalid spec id %s", specId);
     this.spec = table.specs().get(specId);
     return this;
   }

File: spark/v3.0/spark/src/main/java/org/apache/iceberg/spark/actions/BaseRewriteManifestsSparkAction.java
Patch:
@@ -123,7 +123,7 @@ protected RewriteManifests self() {
 
   @Override
   public RewriteManifests specId(int specId) {
-    Preconditions.checkArgument(table.specs().containsKey(specId), "Invalid spec id %d", specId);
+    Preconditions.checkArgument(table.specs().containsKey(specId), "Invalid spec id %s", specId);
     this.spec = table.specs().get(specId);
     return this;
   }

File: spark/v3.1/spark/src/main/java/org/apache/iceberg/spark/actions/BaseRewriteManifestsSparkAction.java
Patch:
@@ -123,7 +123,7 @@ protected RewriteManifests self() {
 
   @Override
   public RewriteManifests specId(int specId) {
-    Preconditions.checkArgument(table.specs().containsKey(specId), "Invalid spec id %d", specId);
+    Preconditions.checkArgument(table.specs().containsKey(specId), "Invalid spec id %s", specId);
     this.spec = table.specs().get(specId);
     return this;
   }

File: spark/v3.2/spark/src/main/java/org/apache/iceberg/spark/actions/BaseRewriteManifestsSparkAction.java
Patch:
@@ -123,7 +123,7 @@ protected RewriteManifests self() {
 
   @Override
   public RewriteManifests specId(int specId) {
-    Preconditions.checkArgument(table.specs().containsKey(specId), "Invalid spec id %d", specId);
+    Preconditions.checkArgument(table.specs().containsKey(specId), "Invalid spec id %s", specId);
     this.spec = table.specs().get(specId);
     return this;
   }

File: core/src/main/java/org/apache/iceberg/PartitionsTable.java
Patch:
@@ -111,7 +111,7 @@ static CloseableIterable<FileScanTask> planFiles(StaticTableScan scan) {
 
     // use an inclusive projection to remove the partition name prefix and filter out any non-partition expressions
     Expression partitionFilter = Projections
-        .inclusive(transformSpec(scan.schema(), table.spec(), PARTITION_FIELD_PREFIX), caseSensitive)
+        .inclusive(transformSpec(scan.schema(), table.spec()), caseSensitive)
         .project(scan.filter());
 
     ManifestGroup manifestGroup = new ManifestGroup(table.io(), snapshot.dataManifests(), snapshot.deleteManifests())

File: core/src/main/java/org/apache/iceberg/io/FileAppenderFactory.java
Patch:
@@ -67,7 +67,7 @@ public interface FileAppenderFactory<T> {
    * @param outputFile an OutputFile used to create an output stream.
    * @param format     a file format
    * @param partition  a tuple of partition values
-   * @return a newly created {@link EqualityDeleteWriter} for position deletes
+   * @return a newly created {@link PositionDeleteWriter} for position deletes
    */
   PositionDeleteWriter<T> newPosDeleteWriter(EncryptedOutputFile outputFile, FileFormat format, StructLike partition);
 }

File: nessie/src/main/java/org/apache/iceberg/nessie/NessieTableOperations.java
Patch:
@@ -89,7 +89,8 @@ private TableMetadata loadTableMetadata(String metadataLocation) {
     TableMetadata.Builder builder = TableMetadata.buildFrom(TableMetadataParser.read(io(), metadataLocation))
         .setCurrentSchema(table.getSchemaId())
         .setDefaultSortOrder(table.getSortOrderId())
-        .setDefaultPartitionSpec(table.getSpecId());
+        .setDefaultPartitionSpec(table.getSpecId())
+        .withMetadataLocation(metadataLocation);
     if (table.getSnapshotId() != -1) {
       builder.setBranchSnapshot(table.getSnapshotId(), SnapshotRef.MAIN_BRANCH);
     }

File: core/src/main/java/org/apache/iceberg/util/JsonUtil.java
Patch:
@@ -149,7 +149,7 @@ public static List<String> getStringList(String property, JsonNode node) {
   }
 
   public static List<String> getStringListOrNull(String property, JsonNode node) {
-    if (!node.has(property)) {
+    if (!node.has(property) || node.get(property).isNull()) {
       return null;
     }
 
@@ -159,7 +159,7 @@ public static List<String> getStringListOrNull(String property, JsonNode node) {
   }
 
   public static Set<Integer> getIntegerSetOrNull(String property, JsonNode node) {
-    if (!node.has(property)) {
+    if (!node.has(property) || node.get(property).isNull()) {
       return null;
     }
 

File: core/src/main/java/org/apache/iceberg/BaseFileScanTask.java
Patch:
@@ -19,7 +19,6 @@
 
 package org.apache.iceberg;
 
-import java.util.Collections;
 import java.util.Iterator;
 import java.util.List;
 import java.util.NoSuchElementException;
@@ -228,7 +227,7 @@ public boolean isAdjacent(SplitScanTask other) {
 
   static List<FileScanTask> combineAdjacentTasks(List<FileScanTask> tasks) {
     if (tasks.isEmpty()) {
-      return Collections.emptyList();
+      return tasks;
     }
 
     List<FileScanTask> combinedScans = Lists.newArrayList();

File: flink/v1.14/flink/src/main/java/org/apache/iceberg/flink/source/FlinkSplitPlanner.java
Patch:
@@ -69,8 +69,7 @@ static FlinkInputSplit[] planInputSplits(Table table, ScanContext context, Execu
   public static List<IcebergSourceSplit> planIcebergSourceSplits(
       Table table, ScanContext context, ExecutorService workerPool) {
     try (CloseableIterable<CombinedScanTask> tasksIterable = planTasks(table, context, workerPool)) {
-      return Lists.newArrayList(CloseableIterable.transform(tasksIterable,
-          task -> IcebergSourceSplit.fromCombinedScanTask(task)));
+      return Lists.newArrayList(CloseableIterable.transform(tasksIterable, IcebergSourceSplit::fromCombinedScanTask));
     } catch (IOException e) {
       throw new UncheckedIOException("Failed to process task iterable: ", e);
     }

File: flink/v1.14/flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkScan.java
Patch:
@@ -311,7 +311,8 @@ public void testPartitionTypes() throws Exception {
   public void testCustomizedFlinkDataTypes() throws Exception {
     Schema schema = new Schema(
         Types.NestedField.required(
-            1, "map", Types.MapType.ofRequired(2, 3, Types.StringType.get(), Types.StringType.get())));
+            1, "map", Types.MapType.ofRequired(2, 3, Types.StringType.get(), Types.StringType.get())),
+        Types.NestedField.required(4, "arr", Types.ListType.ofRequired(5, Types.StringType.get())));
     Table table = catalog.createTable(TestFixtures.TABLE_IDENTIFIER, schema);
     List<Record> records = RandomGenericData.generate(schema, 10, 0L);
     GenericAppenderHelper helper = new GenericAppenderHelper(table, fileFormat, TEMPORARY_FOLDER);

File: core/src/main/java/org/apache/iceberg/avro/BuildAvroProjection.java
Patch:
@@ -49,7 +49,7 @@ class BuildAvroProjection extends AvroCustomOrderSchemaVisitor<Schema, Schema.Fi
     this.current = expectedSchema.asStruct();
   }
 
-  BuildAvroProjection(org.apache.iceberg.types.Type expectedType, Map<String, String> renames) {
+  BuildAvroProjection(Type expectedType, Map<String, String> renames) {
     this.renames = renames;
     this.current = expectedType;
   }

File: core/src/test/java/org/apache/iceberg/rest/RESTCatalogAdapter.java
Patch:
@@ -318,7 +318,8 @@ public static void configureResponseFromException(Exception exc, ErrorResponse.B
     errorBuilder
         .responseCode(EXCEPTION_ERROR_CODES.getOrDefault(exc.getClass(), 500))
         .withType(exc.getClass().getSimpleName())
-        .withMessage(exc.getMessage());
+        .withMessage(exc.getMessage())
+        .withStackTrace(exc);
   }
 
   private static Namespace namespaceFromPathVars(Map<String, String> pathVars) {

File: aws/src/integration/java/org/apache/iceberg/aws/TestAssumeRoleAwsClientFactory.java
Patch:
@@ -74,6 +74,7 @@ public void before() {
         .build());
     assumeRoleProperties = Maps.newHashMap();
     assumeRoleProperties.put(AwsProperties.CLIENT_FACTORY, AssumeRoleAwsClientFactory.class.getName());
+    assumeRoleProperties.put(AwsProperties.HTTP_CLIENT_TYPE, AwsProperties.HTTP_CLIENT_TYPE_APACHE);
     assumeRoleProperties.put(AwsProperties.CLIENT_ASSUME_ROLE_REGION, "us-east-1");
     assumeRoleProperties.put(AwsProperties.CLIENT_ASSUME_ROLE_ARN, response.role().arn());
     assumeRoleProperties.put(AwsProperties.CLIENT_ASSUME_ROLE_TAGS_PREFIX + "key1", "value1");

File: api/src/main/java/org/apache/iceberg/Schema.java
Patch:
@@ -109,6 +109,8 @@ public Schema(int schemaId, List<NestedField> columns, Map<String, Integer> alia
   static void validateIdentifierField(int fieldId, Map<Integer, Types.NestedField> idToField,
                                               Map<Integer, Integer> idToParent) {
     Types.NestedField field = idToField.get(fieldId);
+    Preconditions.checkArgument(field != null,
+        "Cannot add fieldId %s as an identifier field: field does not exist", fieldId);
     Preconditions.checkArgument(field.type().isPrimitiveType(),
         "Cannot add field %s as an identifier field: not a primitive type field", field.name());
     Preconditions.checkArgument(field.isRequired(),

File: aws/src/main/java/org/apache/iceberg/aws/s3/S3FileIO.java
Patch:
@@ -67,7 +67,7 @@ public class S3FileIO implements FileIO, SupportsBulkOperations {
   private transient S3Client client;
   private MetricsContext metrics = MetricsContext.nullMetrics();
   private final AtomicBoolean isResourceClosed = new AtomicBoolean(false);
-  private Set<Tag> writeTags;
+  private Set<Tag> writeTags = Sets.newHashSet();
 
   /**
    * No-arg constructor to load the FileIO dynamically.

File: flink/v1.14/flink/src/main/java/org/apache/iceberg/flink/source/ScanContext.java
Patch:
@@ -200,8 +200,9 @@ ScanContext copyWithAppendsBetween(long newStartSnapshotId, long newEndSnapshotI
         .project(schema)
         .filters(filters)
         .limit(limit)
-        .exposeLocality(exposeLocality)
         .includeColumnStats(includeColumnStats)
+        .exposeLocality(exposeLocality)
+        .planParallelism(planParallelism)
         .build();
   }
 
@@ -223,6 +224,7 @@ ScanContext copyWithSnapshotId(long newSnapshotId) {
         .limit(limit)
         .includeColumnStats(includeColumnStats)
         .exposeLocality(exposeLocality)
+        .planParallelism(planParallelism)
         .build();
   }
 

File: orc/src/main/java/org/apache/iceberg/orc/ORCSchemaUtil.java
Patch:
@@ -107,8 +107,8 @@ public static TypeDescription convert(Schema schema) {
     final TypeDescription root = TypeDescription.createStruct();
     final Types.StructType schemaRoot = schema.asStruct();
     for (Types.NestedField field : schemaRoot.asStructType().fields()) {
-      TypeDescription orcColumType = convert(field.fieldId(), field.type(), field.isRequired());
-      root.addField(field.name(), orcColumType);
+      TypeDescription orcColumnType = convert(field.fieldId(), field.type(), field.isRequired());
+      root.addField(field.name(), orcColumnType);
     }
     return root;
   }

File: core/src/main/java/org/apache/iceberg/TableProperties.java
Patch:
@@ -148,7 +148,7 @@ private TableProperties() {
   public static final long SPLIT_OPEN_FILE_COST_DEFAULT = 4 * 1024 * 1024; // 4MB
 
   public static final String PARQUET_VECTORIZATION_ENABLED = "read.parquet.vectorization.enabled";
-  public static final boolean PARQUET_VECTORIZATION_ENABLED_DEFAULT = false;
+  public static final boolean PARQUET_VECTORIZATION_ENABLED_DEFAULT = true;
 
   public static final String PARQUET_BATCH_SIZE = "read.parquet.vectorization.batch-size";
   public static final int PARQUET_BATCH_SIZE_DEFAULT = 5000;

File: spark/v2.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtilWithInMemoryCatalog.java
Patch:
@@ -451,9 +451,10 @@ public void testImportTableWithInt96Timestamp() throws IOException {
       );
       Table table = TABLES.create(schema, PartitionSpec.unpartitioned(), tableLocation);
 
-      // assign a custom metrics config
+      // assign a custom metrics config and disable vectorized reads
       table.updateProperties()
           .set(TableProperties.DEFAULT_WRITE_METRICS_MODE, "full")
+          .set(TableProperties.PARQUET_VECTORIZATION_ENABLED, "false")
           .commit();
 
       File stagingDir = temp.newFolder("staging-dir");

File: core/src/test/java/org/apache/iceberg/TestDeleteFileIndex.java
Patch:
@@ -263,7 +263,7 @@ public void testPartitionedTableWithUnrelatedPartitionDeletes() {
     FileScanTask task = tasks.get(0);
     Assert.assertEquals("Should have the correct data file path",
         FILE_B.path(), task.file().path());
-    Assert.assertEquals("Should have one associated delete file",
+    Assert.assertEquals("Should have no delete files to apply",
         0, task.deletes().size());
   }
 

File: api/src/main/java/org/apache/iceberg/actions/DeleteOrphanFiles.java
Patch:
@@ -23,9 +23,9 @@
 import java.util.function.Consumer;
 
 /**
- * An action that deletes orphan files in a table.
+ * An action that deletes orphan metadata, data and delete files in a table.
  * <p>
- * A metadata or data file is considered orphan if it is not reachable by any valid snapshot.
+ * A file is considered orphan if it is not reachable by any valid snapshot.
  * The set of actual files is built by listing the underlying storage which makes this operation
  * expensive.
  */

File: spark/v3.2/spark/src/main/java/org/apache/iceberg/spark/actions/BaseSparkAction.java
Patch:
@@ -111,7 +111,8 @@ protected Table newStaticTable(TableMetadata metadata, FileIO io) {
     return new BaseTable(ops, metadataFileLocation);
   }
 
-  protected Dataset<Row> buildValidDataFileDF(Table table) {
+  // builds a DF of delete and data file locations by reading all manifests
+  protected Dataset<Row> buildValidContentFileDF(Table table) {
     JavaSparkContext context = JavaSparkContext.fromSparkContext(spark.sparkContext());
     Broadcast<FileIO> ioBroadcast = context.broadcast(SparkUtil.serializableFileIO(table));
 

File: spark/v2.4/spark/src/jmh/java/org/apache/iceberg/spark/data/parquet/SparkParquetWritersNestedDataBenchmark.java
Patch:
@@ -35,6 +35,7 @@
 import org.openjdk.jmh.annotations.Benchmark;
 import org.openjdk.jmh.annotations.BenchmarkMode;
 import org.openjdk.jmh.annotations.Fork;
+import org.openjdk.jmh.annotations.Level;
 import org.openjdk.jmh.annotations.Measurement;
 import org.openjdk.jmh.annotations.Mode;
 import org.openjdk.jmh.annotations.Scope;
@@ -84,7 +85,7 @@ public void setupBenchmark() throws IOException {
     dataFile.delete();
   }
 
-  @TearDown
+  @TearDown(Level.Iteration)
   public void tearDownBenchmark() {
     if (dataFile != null) {
       dataFile.delete();

File: spark/v3.0/spark/src/jmh/java/org/apache/iceberg/spark/data/parquet/SparkParquetWritersNestedDataBenchmark.java
Patch:
@@ -35,6 +35,7 @@
 import org.openjdk.jmh.annotations.Benchmark;
 import org.openjdk.jmh.annotations.BenchmarkMode;
 import org.openjdk.jmh.annotations.Fork;
+import org.openjdk.jmh.annotations.Level;
 import org.openjdk.jmh.annotations.Measurement;
 import org.openjdk.jmh.annotations.Mode;
 import org.openjdk.jmh.annotations.Scope;
@@ -84,7 +85,7 @@ public void setupBenchmark() throws IOException {
     dataFile.delete();
   }
 
-  @TearDown
+  @TearDown(Level.Iteration)
   public void tearDownBenchmark() {
     if (dataFile != null) {
       dataFile.delete();

File: spark/v3.1/spark/src/jmh/java/org/apache/iceberg/spark/data/parquet/SparkParquetWritersNestedDataBenchmark.java
Patch:
@@ -35,6 +35,7 @@
 import org.openjdk.jmh.annotations.Benchmark;
 import org.openjdk.jmh.annotations.BenchmarkMode;
 import org.openjdk.jmh.annotations.Fork;
+import org.openjdk.jmh.annotations.Level;
 import org.openjdk.jmh.annotations.Measurement;
 import org.openjdk.jmh.annotations.Mode;
 import org.openjdk.jmh.annotations.Scope;
@@ -84,7 +85,7 @@ public void setupBenchmark() throws IOException {
     dataFile.delete();
   }
 
-  @TearDown
+  @TearDown(Level.Iteration)
   public void tearDownBenchmark() {
     if (dataFile != null) {
       dataFile.delete();

File: spark/v3.2/spark/src/jmh/java/org/apache/iceberg/spark/data/parquet/SparkParquetWritersNestedDataBenchmark.java
Patch:
@@ -35,6 +35,7 @@
 import org.openjdk.jmh.annotations.Benchmark;
 import org.openjdk.jmh.annotations.BenchmarkMode;
 import org.openjdk.jmh.annotations.Fork;
+import org.openjdk.jmh.annotations.Level;
 import org.openjdk.jmh.annotations.Measurement;
 import org.openjdk.jmh.annotations.Mode;
 import org.openjdk.jmh.annotations.Scope;
@@ -84,7 +85,7 @@ public void setupBenchmark() throws IOException {
     dataFile.delete();
   }
 
-  @TearDown
+  @TearDown(Level.Iteration)
   public void tearDownBenchmark() {
     if (dataFile != null) {
       dataFile.delete();

File: flink/v1.12/flink/src/test/java/org/apache/iceberg/flink/SimpleDataUtil.java
Patch:
@@ -275,10 +275,10 @@ public static Map<Long, List<DataFile>> snapshotToDataFiles(Table table) throws
       TableScan tableScan = table.newScan();
       if (current.parentId() != null) {
         // Collect the data files that was added only in current snapshot.
-        tableScan.appendsBetween(current.parentId(), current.snapshotId());
+        tableScan = tableScan.appendsBetween(current.parentId(), current.snapshotId());
       } else {
         // Collect the data files that was added in the oldest snapshot.
-        tableScan.useSnapshot(current.snapshotId());
+        tableScan = tableScan.useSnapshot(current.snapshotId());
       }
       try (CloseableIterable<FileScanTask> scanTasks = tableScan.planFiles()) {
         result.put(current.snapshotId(), ImmutableList.copyOf(Iterables.transform(scanTasks, FileScanTask::file)));

File: flink/v1.13/flink/src/test/java/org/apache/iceberg/flink/SimpleDataUtil.java
Patch:
@@ -275,10 +275,10 @@ public static Map<Long, List<DataFile>> snapshotToDataFiles(Table table) throws
       TableScan tableScan = table.newScan();
       if (current.parentId() != null) {
         // Collect the data files that was added only in current snapshot.
-        tableScan.appendsBetween(current.parentId(), current.snapshotId());
+        tableScan = tableScan.appendsBetween(current.parentId(), current.snapshotId());
       } else {
         // Collect the data files that was added in the oldest snapshot.
-        tableScan.useSnapshot(current.snapshotId());
+        tableScan = tableScan.useSnapshot(current.snapshotId());
       }
       try (CloseableIterable<FileScanTask> scanTasks = tableScan.planFiles()) {
         result.put(current.snapshotId(), ImmutableList.copyOf(Iterables.transform(scanTasks, FileScanTask::file)));

File: flink/v1.14/flink/src/test/java/org/apache/iceberg/flink/SimpleDataUtil.java
Patch:
@@ -275,10 +275,10 @@ public static Map<Long, List<DataFile>> snapshotToDataFiles(Table table) throws
       TableScan tableScan = table.newScan();
       if (current.parentId() != null) {
         // Collect the data files that was added only in current snapshot.
-        tableScan.appendsBetween(current.parentId(), current.snapshotId());
+        tableScan = tableScan.appendsBetween(current.parentId(), current.snapshotId());
       } else {
         // Collect the data files that was added in the oldest snapshot.
-        tableScan.useSnapshot(current.snapshotId());
+        tableScan = tableScan.useSnapshot(current.snapshotId());
       }
       try (CloseableIterable<FileScanTask> scanTasks = tableScan.planFiles()) {
         result.put(current.snapshotId(), ImmutableList.copyOf(Iterables.transform(scanTasks, FileScanTask::file)));

File: api/src/test/java/org/apache/iceberg/transforms/TestBucketing.java
Patch:
@@ -39,7 +39,7 @@
 import org.junit.Test;
 
 public class TestBucketing {
-  private static final HashFunction MURMUR3 = Hashing.murmur3_32();
+  private static final HashFunction MURMUR3 = Hashing.murmur3_32_fixed();
   private static Constructor<UUID> uuidBytesConstructor;
 
   @BeforeClass

File: parquet/src/main/java/org/apache/iceberg/parquet/Parquet.java
Patch:
@@ -235,7 +235,9 @@ public <D> FileAppender<D> build() throws IOException {
             config.put("compression.brotli.quality", compressionLevel);
             break;
           case ZSTD:
+            // keep "io.compression.codec.zstd.level" for backwards compatibility
             config.put("io.compression.codec.zstd.level", compressionLevel);
+            config.put("parquet.compression.codec.zstd.level", compressionLevel);
             break;
           default:
             // compression level is not supported; ignore it

File: hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveMetastore.java
Patch:
@@ -125,15 +125,15 @@ public void start() {
 
   /**
    * Starts a TestHiveMetastore with the default connection pool size (5) with the provided HiveConf.
-   * @param hiveConf The hive configuration to use
+   * @param conf The hive configuration to use
    */
   public void start(HiveConf conf) {
     start(conf, DEFAULT_POOL_SIZE);
   }
 
   /**
    * Starts a TestHiveMetastore with a provided connection pool size and HiveConf.
-   * @param hiveConf The hive configuration to use
+   * @param conf The hive configuration to use
    * @param poolSize The number of threads in the executor pool
    */
   public void start(HiveConf conf, int poolSize) {

File: aws/src/integration/java/org/apache/iceberg/aws/s3/TestS3MultipartUpload.java
Patch:
@@ -56,6 +56,7 @@ public static void beforeClass() {
     prefix = UUID.randomUUID().toString();
     properties = new AwsProperties();
     properties.setS3FileIoMultiPartSize(AwsProperties.S3FILEIO_MULTIPART_SIZE_MIN);
+    properties.setS3ChecksumEnabled(true);
     io = new S3FileIO(() -> s3, properties);
   }
 

File: aws/src/main/java/org/apache/iceberg/aws/glue/GlueTableOperations.java
Patch:
@@ -114,6 +114,8 @@ protected void doCommit(TableMetadata base, TableMetadata metadata) {
       Map<String, String> properties = prepareProperties(glueTable, newMetadataLocation);
       persistGlueTable(glueTable, properties, metadata);
       commitStatus = CommitStatus.SUCCESS;
+    } catch (CommitFailedException e) {
+      throw e;
     } catch (ConcurrentModificationException e) {
       throw new CommitFailedException(e, "Cannot commit %s because Glue detected concurrent update", tableName());
     } catch (software.amazon.awssdk.services.glue.model.AlreadyExistsException e) {

File: core/src/main/java/org/apache/iceberg/DeleteFileIndex.java
Patch:
@@ -462,7 +462,7 @@ private Iterable<CloseableIterable<ManifestEntry<DeleteFile>>> deleteManifestRea
       Iterable<ManifestFile> matchingManifests = evalCache == null ? deleteManifests :
           Iterables.filter(deleteManifests, manifest ->
               manifest.content() == ManifestContent.DELETES &&
-                  (manifest.hasAddedFiles() || manifest.hasDeletedFiles()) &&
+                  (manifest.hasAddedFiles() || manifest.hasExistingFiles()) &&
                   evalCache.get(manifest.partitionSpecId()).eval(manifest));
 
       return Iterables.transform(

File: core/src/test/java/org/apache/iceberg/TableTestBase.java
Patch:
@@ -54,9 +54,11 @@ public class TableTestBase {
       required(4, "data", Types.StringType.get())
   );
 
+  protected static final int BUCKETS_NUMBER = 16;
+
   // Partition spec used to create tables
   protected static final PartitionSpec SPEC = PartitionSpec.builderFor(SCHEMA)
-      .bucket("data", 16)
+      .bucket("data", BUCKETS_NUMBER)
       .build();
 
   static final DataFile FILE_A = DataFiles.builder(SPEC)

File: core/src/main/java/org/apache/iceberg/actions/BinPackStrategy.java
Patch:
@@ -260,7 +260,7 @@ private void validateOptions() {
 
     Preconditions.checkArgument(targetFileSize < maxFileSize,
         "Cannot set %s is greater than or equal to %s, all files written will be larger than the threshold, %d >= %d",
-        MAX_FILE_SIZE_BYTES, RewriteDataFiles.TARGET_FILE_SIZE_BYTES, maxFileSize, targetFileSize);
+        RewriteDataFiles.TARGET_FILE_SIZE_BYTES, MAX_FILE_SIZE_BYTES, targetFileSize, maxFileSize);
 
     Preconditions.checkArgument(minInputFiles > 0,
         "Cannot set %s is less than 1. All values less than 1 have the same effect as 1. %d < 1",

File: spark/v3.0/spark/src/main/java/org/apache/iceberg/spark/SparkCatalog.java
Patch:
@@ -493,7 +493,7 @@ private Pair<Table, Long> load(Identifier ident) {
       Table table;
       try {
         table = icebergCatalog.loadTable(namespaceAsIdent);
-      } catch (org.apache.iceberg.exceptions.NoSuchTableException ignored) {
+      } catch (Exception ignored) {
         // the namespace does not identify a table, so it cannot be a table with a snapshot selector
         // throw the original exception
         throw e;

File: spark/v3.1/spark/src/main/java/org/apache/iceberg/spark/SparkCatalog.java
Patch:
@@ -493,7 +493,7 @@ private Pair<Table, Long> load(Identifier ident) {
       Table table;
       try {
         table = icebergCatalog.loadTable(namespaceAsIdent);
-      } catch (org.apache.iceberg.exceptions.NoSuchTableException ignored) {
+      } catch (Exception ignored) {
         // the namespace does not identify a table, so it cannot be a table with a snapshot selector
         // throw the original exception
         throw e;

File: spark/v3.2/spark/src/main/java/org/apache/iceberg/spark/SparkCatalog.java
Patch:
@@ -493,7 +493,7 @@ private Pair<Table, Long> load(Identifier ident) {
       Table table;
       try {
         table = icebergCatalog.loadTable(namespaceAsIdent);
-      } catch (org.apache.iceberg.exceptions.NoSuchTableException ignored) {
+      } catch (Exception ignored) {
         // the namespace does not identify a table, so it cannot be a table with a snapshot selector
         // throw the original exception
         throw e;

File: core/src/main/java/org/apache/iceberg/ManifestReader.java
Patch:
@@ -99,6 +99,7 @@ protected ManifestReader(InputFile file, Map<Integer, PartitionSpec> specsById,
     try {
       try (AvroIterable<ManifestEntry<F>> headerReader = Avro.read(file)
           .project(ManifestEntry.getSchema(Types.StructType.of()).select("status"))
+          .classLoader(GenericManifestEntry.class.getClassLoader())
           .build()) {
         this.metadata = headerReader.getMetadata();
       }

File: core/src/main/java/org/apache/iceberg/BaseCombinedScanTask.java
Patch:
@@ -36,7 +36,7 @@ public BaseCombinedScanTask(FileScanTask... tasks) {
 
   public BaseCombinedScanTask(List<FileScanTask> tasks) {
     Preconditions.checkNotNull(tasks, "tasks cannot be null");
-    this.tasks = tasks.stream().toArray(FileScanTask[]::new);
+    this.tasks = BaseFileScanTask.combineAdjacentTasks(tasks).stream().toArray(FileScanTask[]::new);
   }
 
   @Override

File: nessie/src/main/java/org/apache/iceberg/nessie/NessieUtil.java
Patch:
@@ -37,10 +37,12 @@
 
 public final class NessieUtil {
 
+  public static final String NESSIE_CONFIG_PREFIX = "nessie.";
+  public static final String CONFIG_CLIENT_BUILDER_IMPL = NESSIE_CONFIG_PREFIX + "client-builder-impl";
+
   static final String APPLICATION_TYPE = "application-type";
 
   private NessieUtil() {
-
   }
 
   static Predicate<EntriesResponse.Entry> namespacePredicate(Namespace ns) {

File: nessie/src/test/java/org/apache/iceberg/nessie/BaseTestIceberg.java
Patch:
@@ -84,7 +84,7 @@ public abstract class BaseTestIceberg {
   protected NessieApiV1 api;
   protected Configuration hadoopConfig;
   protected final String branch;
-  private String uri;
+  protected String uri;
 
   public BaseTestIceberg(String branch) {
     this.branch = branch;

File: core/src/main/java/org/apache/iceberg/BaseMetadataTable.java
Patch:
@@ -62,7 +62,7 @@ protected BaseMetadataTable(TableOperations ops, Table table, String name) {
    * @return a spec used to rewrite the metadata table filters to partition filters using an inclusive projection
    */
   static PartitionSpec transformSpec(Schema metadataTableSchema, PartitionSpec spec, String partitionPrefix) {
-    PartitionSpec.Builder identitySpecBuilder = PartitionSpec.builderFor(metadataTableSchema);
+    PartitionSpec.Builder identitySpecBuilder = PartitionSpec.builderFor(metadataTableSchema).checkConflicts(false);
     spec.fields().forEach(pf -> identitySpecBuilder.identity(partitionPrefix + pf.name(), pf.name()));
     return identitySpecBuilder.build();
   }

File: api/src/main/java/org/apache/iceberg/expressions/Evaluator.java
Patch:
@@ -149,7 +149,8 @@ public <T> Boolean notIn(Bound<T> valueExpr, Set<T> literalSet) {
 
     @Override
     public <T> Boolean startsWith(Bound<T> valueExpr, Literal<T> lit) {
-      return ((String) valueExpr.eval(struct)).startsWith((String) lit.value());
+      T evalRes = valueExpr.eval(struct);
+      return evalRes != null && ((String) evalRes).startsWith((String) lit.value());
     }
 
     @Override

File: api/src/test/java/org/apache/iceberg/expressions/TestEvaluator.java
Patch:
@@ -231,6 +231,8 @@ public void testStartsWith() {
     Assert.assertFalse("Abc startsWith abc should be false", evaluator.eval(TestHelpers.Row.of("Abc")));
     Assert.assertFalse("a startsWith abc should be false", evaluator.eval(TestHelpers.Row.of("a")));
     Assert.assertTrue("abcd startsWith abc should be true", evaluator.eval(TestHelpers.Row.of("abcd")));
+    Assert.assertFalse("null startsWith abc should be false",
+        evaluator.eval(TestHelpers.Row.of((String) null)));
   }
 
   @Test

File: core/src/main/java/org/apache/iceberg/StaticTableScan.java
Patch:
@@ -34,7 +34,7 @@ class StaticTableScan extends BaseMetadataTableScan {
     this.tableType = tableType;
   }
 
-  private StaticTableScan(TableOperations ops, Table table, Schema schema, String tableType,
+  StaticTableScan(TableOperations ops, Table table, Schema schema, String tableType,
                           Function<StaticTableScan, DataTask> buildTask, TableScanContext context) {
     super(ops, table, schema, context);
     this.buildTask = buildTask;

File: parquet/src/main/java/org/apache/iceberg/parquet/ApplyNameMapping.java
Patch:
@@ -96,10 +96,12 @@ public Type primitive(PrimitiveType primitive) {
     return field == null ? primitive : primitive.withId(field.id());
   }
 
+  @Override
   public void beforeField(Type type) {
     fieldNames.push(type.getName());
   }
 
+  @Override
   public void afterField(Type type) {
     fieldNames.pop();
   }

File: flink/v1.12/flink/src/main/java/org/apache/iceberg/flink/data/FlinkValueWriters.java
Patch:
@@ -109,7 +109,7 @@ private static class TimeMicrosWriter implements ValueWriter<Integer> {
 
     @Override
     public void write(Integer timeMills, Encoder encoder) throws IOException {
-      encoder.writeLong(timeMills * 1000);
+      encoder.writeLong(timeMills * 1000L);
     }
   }
 

File: flink/v1.13/flink/src/main/java/org/apache/iceberg/flink/data/FlinkValueWriters.java
Patch:
@@ -109,7 +109,7 @@ private static class TimeMicrosWriter implements ValueWriter<Integer> {
 
     @Override
     public void write(Integer timeMills, Encoder encoder) throws IOException {
-      encoder.writeLong(timeMills * 1000);
+      encoder.writeLong(timeMills * 1000L);
     }
   }
 

File: flink/v1.14/flink/src/main/java/org/apache/iceberg/flink/data/FlinkValueWriters.java
Patch:
@@ -109,7 +109,7 @@ private static class TimeMicrosWriter implements ValueWriter<Integer> {
 
     @Override
     public void write(Integer timeMills, Encoder encoder) throws IOException {
-      encoder.writeLong(timeMills * 1000);
+      encoder.writeLong(timeMills * 1000L);
     }
   }
 

File: spark/v3.1/spark/src/main/java/org/apache/iceberg/spark/Spark3Util.java
Patch:
@@ -849,7 +849,7 @@ public static List<SparkPartition> getPartitions(SparkSession spark, Path rootPa
             int fieldIndex = schema.fieldIndex(field.name());
             Object catalystValue = partition.values().get(fieldIndex, field.dataType());
             Object value = CatalystTypeConverters.convertToScala(catalystValue, field.dataType());
-            values.put(field.name(), value.toString());
+            values.put(field.name(), String.valueOf(value));
           });
           return new SparkPartition(values, partition.path().toString(), format);
         }).collect(Collectors.toList());

File: spark/v3.0/spark/src/main/java/org/apache/iceberg/spark/Spark3Util.java
Patch:
@@ -849,7 +849,7 @@ public static List<SparkPartition> getPartitions(SparkSession spark, Path rootPa
             int fieldIndex = schema.fieldIndex(field.name());
             Object catalystValue = partition.values().get(fieldIndex, field.dataType());
             Object value = CatalystTypeConverters.convertToScala(catalystValue, field.dataType());
-            values.put(field.name(), value.toString());
+            values.put(field.name(), String.valueOf(value));
           });
           return new SparkPartition(values, partition.path().toString(), format);
         }).collect(Collectors.toList());

File: spark/v3.2/spark/src/main/java/org/apache/iceberg/spark/Spark3Util.java
Patch:
@@ -779,7 +779,7 @@ public static List<SparkPartition> getPartitions(SparkSession spark, Path rootPa
             int fieldIndex = schema.fieldIndex(field.name());
             Object catalystValue = partition.values().get(fieldIndex, field.dataType());
             Object value = CatalystTypeConverters.convertToScala(catalystValue, field.dataType());
-            values.put(field.name(), value.toString());
+            values.put(field.name(), String.valueOf(value));
           });
           return new SparkPartition(values, partition.path().toString(), format);
         }).collect(Collectors.toList());

File: flink/v1.12/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java
Patch:
@@ -62,7 +62,7 @@
 import static org.apache.iceberg.TableProperties.UPSERT_ENABLED;
 import static org.apache.iceberg.TableProperties.UPSERT_ENABLED_DEFAULT;
 import static org.apache.iceberg.TableProperties.WRITE_DISTRIBUTION_MODE;
-import static org.apache.iceberg.TableProperties.WRITE_DISTRIBUTION_MODE_DEFAULT;
+import static org.apache.iceberg.TableProperties.WRITE_DISTRIBUTION_MODE_NONE;
 import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;
 import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;
 
@@ -403,7 +403,7 @@ private DataStream<RowData> distributeDataStream(DataStream<RowData> input,
         // Fallback to use distribution mode parsed from table properties if don't specify in job level.
         String modeName = PropertyUtil.propertyAsString(properties,
             WRITE_DISTRIBUTION_MODE,
-            WRITE_DISTRIBUTION_MODE_DEFAULT);
+            WRITE_DISTRIBUTION_MODE_NONE);
 
         writeMode = DistributionMode.fromName(modeName);
       } else {

File: flink/v1.13/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java
Patch:
@@ -62,7 +62,7 @@
 import static org.apache.iceberg.TableProperties.UPSERT_ENABLED;
 import static org.apache.iceberg.TableProperties.UPSERT_ENABLED_DEFAULT;
 import static org.apache.iceberg.TableProperties.WRITE_DISTRIBUTION_MODE;
-import static org.apache.iceberg.TableProperties.WRITE_DISTRIBUTION_MODE_DEFAULT;
+import static org.apache.iceberg.TableProperties.WRITE_DISTRIBUTION_MODE_NONE;
 import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;
 import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;
 
@@ -403,7 +403,7 @@ private DataStream<RowData> distributeDataStream(DataStream<RowData> input,
         // Fallback to use distribution mode parsed from table properties if don't specify in job level.
         String modeName = PropertyUtil.propertyAsString(properties,
             WRITE_DISTRIBUTION_MODE,
-            WRITE_DISTRIBUTION_MODE_DEFAULT);
+            WRITE_DISTRIBUTION_MODE_NONE);
 
         writeMode = DistributionMode.fromName(modeName);
       } else {

File: flink/v1.14/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java
Patch:
@@ -62,7 +62,7 @@
 import static org.apache.iceberg.TableProperties.UPSERT_ENABLED;
 import static org.apache.iceberg.TableProperties.UPSERT_ENABLED_DEFAULT;
 import static org.apache.iceberg.TableProperties.WRITE_DISTRIBUTION_MODE;
-import static org.apache.iceberg.TableProperties.WRITE_DISTRIBUTION_MODE_DEFAULT;
+import static org.apache.iceberg.TableProperties.WRITE_DISTRIBUTION_MODE_NONE;
 import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;
 import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;
 
@@ -403,7 +403,7 @@ private DataStream<RowData> distributeDataStream(DataStream<RowData> input,
         // Fallback to use distribution mode parsed from table properties if don't specify in job level.
         String modeName = PropertyUtil.propertyAsString(properties,
             WRITE_DISTRIBUTION_MODE,
-            WRITE_DISTRIBUTION_MODE_DEFAULT);
+            WRITE_DISTRIBUTION_MODE_NONE);
 
         writeMode = DistributionMode.fromName(modeName);
       } else {

File: spark/v3.0/spark/src/main/java/org/apache/iceberg/spark/Spark3Util.java
Patch:
@@ -94,7 +94,7 @@
 import scala.collection.Seq;
 
 import static org.apache.iceberg.TableProperties.WRITE_DISTRIBUTION_MODE;
-import static org.apache.iceberg.TableProperties.WRITE_DISTRIBUTION_MODE_DEFAULT;
+import static org.apache.iceberg.TableProperties.WRITE_DISTRIBUTION_MODE_NONE;
 import static org.apache.iceberg.TableProperties.WRITE_DISTRIBUTION_MODE_RANGE;
 
 public class Spark3Util {
@@ -337,7 +337,7 @@ public static SortOrder[] buildRequiredOrdering(Distribution distribution, org.a
 
   public static DistributionMode distributionModeFor(org.apache.iceberg.Table table) {
     boolean isSortedTable = !table.sortOrder().isUnsorted();
-    String defaultModeName = isSortedTable ? WRITE_DISTRIBUTION_MODE_RANGE : WRITE_DISTRIBUTION_MODE_DEFAULT;
+    String defaultModeName = isSortedTable ? WRITE_DISTRIBUTION_MODE_RANGE : WRITE_DISTRIBUTION_MODE_NONE;
     String modeName = table.properties().getOrDefault(WRITE_DISTRIBUTION_MODE, defaultModeName);
     return DistributionMode.fromName(modeName);
   }

File: spark/v3.1/spark/src/main/java/org/apache/iceberg/spark/Spark3Util.java
Patch:
@@ -94,7 +94,7 @@
 import scala.collection.Seq;
 
 import static org.apache.iceberg.TableProperties.WRITE_DISTRIBUTION_MODE;
-import static org.apache.iceberg.TableProperties.WRITE_DISTRIBUTION_MODE_DEFAULT;
+import static org.apache.iceberg.TableProperties.WRITE_DISTRIBUTION_MODE_NONE;
 import static org.apache.iceberg.TableProperties.WRITE_DISTRIBUTION_MODE_RANGE;
 
 public class Spark3Util {
@@ -337,7 +337,7 @@ public static SortOrder[] buildRequiredOrdering(Distribution distribution, org.a
 
   public static DistributionMode distributionModeFor(org.apache.iceberg.Table table) {
     boolean isSortedTable = !table.sortOrder().isUnsorted();
-    String defaultModeName = isSortedTable ? WRITE_DISTRIBUTION_MODE_RANGE : WRITE_DISTRIBUTION_MODE_DEFAULT;
+    String defaultModeName = isSortedTable ? WRITE_DISTRIBUTION_MODE_RANGE : WRITE_DISTRIBUTION_MODE_NONE;
     String modeName = table.properties().getOrDefault(WRITE_DISTRIBUTION_MODE, defaultModeName);
     return DistributionMode.fromName(modeName);
   }

File: parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueReaders.java
Patch:
@@ -330,7 +330,7 @@ public ByteBuffer read(ByteBuffer reuse) {
       if (reuse != null && reuse.hasArray() && reuse.capacity() >= data.remaining()) {
         data.get(reuse.array(), reuse.arrayOffset(), data.remaining());
         reuse.position(0);
-        reuse.limit(data.remaining());
+        reuse.limit(binary.length());
         return reuse;
       } else {
         byte[] array = new byte[data.remaining()];

File: api/src/main/java/org/apache/iceberg/catalog/SupportsNamespaces.java
Patch:
@@ -100,7 +100,7 @@ default List<Namespace> listNamespaces() {
    *
    * @param namespace a namespace. {@link Namespace}
    * @return true if the namespace was dropped, false otherwise.
-   * @throws NamespaceNotEmptyException If the namespace does not empty
+   * @throws NamespaceNotEmptyException If the namespace is not empty
    */
   boolean dropNamespace(Namespace namespace) throws NamespaceNotEmptyException;
 

File: core/src/main/java/org/apache/iceberg/DataTableScan.java
Patch:
@@ -30,7 +30,7 @@
 public class DataTableScan extends BaseTableScan {
   static final ImmutableList<String> SCAN_COLUMNS = ImmutableList.of(
       "snapshot_id", "file_path", "file_ordinal", "file_format", "block_size_in_bytes",
-      "file_size_in_bytes", "record_count", "partition", "key_metadata"
+      "file_size_in_bytes", "record_count", "partition", "key_metadata", "split_offsets"
   );
   static final ImmutableList<String> SCAN_WITH_STATS_COLUMNS = ImmutableList.<String>builder()
       .addAll(SCAN_COLUMNS)

File: spark/v2.4/spark/src/test/java/org/apache/iceberg/spark/source/TestDataSourceOptions.java
Patch:
@@ -188,6 +188,7 @@ public void testSplitOptionsOverridesTableProperties() throws IOException {
     PartitionSpec spec = PartitionSpec.unpartitioned();
     Map<String, String> options = Maps.newHashMap();
     options.put(TableProperties.SPLIT_SIZE, String.valueOf(128L * 1024 * 1024)); // 128Mb
+    options.put(TableProperties.DEFAULT_FILE_FORMAT, String.valueOf(FileFormat.AVRO)); // Arbitrarily splittable
     Table icebergTable = tables.create(SCHEMA, spec, options, tableLocation);
 
     List<SimpleRecord> expectedRecords = Lists.newArrayList(

File: spark/v3.0/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java
Patch:
@@ -230,9 +230,9 @@ public void testBinPackSplitLargeFile() {
         .execute();
 
     Assert.assertEquals("Action should delete 1 data files", 1, result.rewrittenDataFilesCount());
-    Assert.assertEquals("Action should add 2 data files", 2, result.addedDataFilesCount());
+    Assert.assertEquals("Action should add 3 data files", 3, result.addedDataFilesCount());
 
-    shouldHaveFiles(table, 2);
+    shouldHaveFiles(table, 3);
 
     List<Object[]> actualRecords = currentData();
     assertEquals("Rows must match", expectedRecords, actualRecords);

File: spark/v3.0/spark/src/test/java/org/apache/iceberg/spark/source/TestDataSourceOptions.java
Patch:
@@ -188,6 +188,7 @@ public void testSplitOptionsOverridesTableProperties() throws IOException {
     PartitionSpec spec = PartitionSpec.unpartitioned();
     Map<String, String> options = Maps.newHashMap();
     options.put(TableProperties.SPLIT_SIZE, String.valueOf(128L * 1024 * 1024)); // 128Mb
+    options.put(TableProperties.DEFAULT_FILE_FORMAT, String.valueOf(FileFormat.AVRO)); // Arbitrarily splittable
     Table icebergTable = tables.create(SCHEMA, spec, options, tableLocation);
 
     List<SimpleRecord> expectedRecords = Lists.newArrayList(

File: spark/v3.1/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java
Patch:
@@ -230,9 +230,9 @@ public void testBinPackSplitLargeFile() {
         .execute();
 
     Assert.assertEquals("Action should delete 1 data files", 1, result.rewrittenDataFilesCount());
-    Assert.assertEquals("Action should add 2 data files", 2, result.addedDataFilesCount());
+    Assert.assertEquals("Action should add 3 data files", 3, result.addedDataFilesCount());
 
-    shouldHaveFiles(table, 2);
+    shouldHaveFiles(table, 3);
 
     List<Object[]> actualRecords = currentData();
     assertEquals("Rows must match", expectedRecords, actualRecords);

File: spark/v3.1/spark/src/test/java/org/apache/iceberg/spark/source/TestDataSourceOptions.java
Patch:
@@ -188,6 +188,7 @@ public void testSplitOptionsOverridesTableProperties() throws IOException {
     PartitionSpec spec = PartitionSpec.unpartitioned();
     Map<String, String> options = Maps.newHashMap();
     options.put(TableProperties.SPLIT_SIZE, String.valueOf(128L * 1024 * 1024)); // 128Mb
+    options.put(TableProperties.DEFAULT_FILE_FORMAT, String.valueOf(FileFormat.AVRO)); // Arbitrarily splittable
     Table icebergTable = tables.create(SCHEMA, spec, options, tableLocation);
 
     List<SimpleRecord> expectedRecords = Lists.newArrayList(

File: spark/v3.2/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java
Patch:
@@ -348,9 +348,9 @@ public void testBinPackSplitLargeFile() {
         .execute();
 
     Assert.assertEquals("Action should delete 1 data files", 1, result.rewrittenDataFilesCount());
-    Assert.assertEquals("Action should add 2 data files", 2, result.addedDataFilesCount());
+    Assert.assertEquals("Action should add 3 data files", 3, result.addedDataFilesCount());
 
-    shouldHaveFiles(table, 2);
+    shouldHaveFiles(table, 3);
 
     List<Object[]> actualRecords = currentData();
     assertEquals("Rows must match", expectedRecords, actualRecords);

File: spark/v3.2/spark/src/test/java/org/apache/iceberg/spark/source/TestDataSourceOptions.java
Patch:
@@ -188,6 +188,7 @@ public void testSplitOptionsOverridesTableProperties() throws IOException {
     PartitionSpec spec = PartitionSpec.unpartitioned();
     Map<String, String> options = Maps.newHashMap();
     options.put(TableProperties.SPLIT_SIZE, String.valueOf(128L * 1024 * 1024)); // 128Mb
+    options.put(TableProperties.DEFAULT_FILE_FORMAT, String.valueOf(FileFormat.AVRO)); // Arbitrarily splittable
     Table icebergTable = tables.create(SCHEMA, spec, options, tableLocation);
 
     List<SimpleRecord> expectedRecords = Lists.newArrayList(

File: core/src/main/java/org/apache/iceberg/BaseMetadataTable.java
Patch:
@@ -56,13 +56,13 @@ protected BaseMetadataTable(TableOperations ops, Table table, String name) {
    * When this spec is used to project an expression for the given metadata table, the projection will remove
    * predicates for non-partition fields (not in the spec) and will remove the "$partitionPrefix." prefix from fields.
    *
-   * @param metadataTableSchena schema of the metadata table
+   * @param metadataTableSchema schema of the metadata table
    * @param spec spec on which the metadata table schema is based
    * @param partitionPrefix prefix to remove from each field in the partition spec
    * @return a spec used to rewrite the metadata table filters to partition filters using an inclusive projection
    */
-  static PartitionSpec transformSpec(Schema metadataTableSchena, PartitionSpec spec, String partitionPrefix) {
-    PartitionSpec.Builder identitySpecBuilder = PartitionSpec.builderFor(metadataTableSchena);
+  static PartitionSpec transformSpec(Schema metadataTableSchema, PartitionSpec spec, String partitionPrefix) {
+    PartitionSpec.Builder identitySpecBuilder = PartitionSpec.builderFor(metadataTableSchema);
     spec.fields().forEach(pf -> identitySpecBuilder.identity(partitionPrefix + pf.name(), pf.name()));
     return identitySpecBuilder.build();
   }

File: core/src/main/java/org/apache/iceberg/ManifestFilterManager.java
Patch:
@@ -345,7 +345,7 @@ private boolean manifestHasDeletedFiles(
     Evaluator inclusive = inclusiveDeleteEvaluator(reader.spec());
     Evaluator strict = strictDeleteEvaluator(reader.spec());
     boolean hasDeletedFiles = false;
-    for (ManifestEntry<F> entry : reader.entries()) {
+    for (ManifestEntry<F> entry : reader.liveEntries()) {
       F file = entry.file();
       boolean fileDelete = deletePaths.contains(file.path()) ||
           dropPartitions.contains(file.specId(), file.partition()) ||

File: spark/v2.4/spark/src/main/java/org/apache/iceberg/spark/IcebergSpark.java
Patch:
@@ -34,6 +34,7 @@ public static void registerBucketUDF(SparkSession session, String funcName, Data
     SparkTypeToType typeConverter = new SparkTypeToType();
     Type sourceIcebergType = typeConverter.atomic(sourceType);
     Transform<Object, Integer> bucket = Transforms.bucket(sourceIcebergType, numBuckets);
-    session.udf().register(funcName, bucket::apply, DataTypes.IntegerType);
+    session.udf().register(funcName,
+        value -> bucket.apply(SparkValueConverter.convert(sourceIcebergType, value)), DataTypes.IntegerType);
   }
 }

File: spark/v2.4/spark/src/main/java/org/apache/iceberg/spark/SparkValueConverter.java
Patch:
@@ -79,8 +79,9 @@ public static Object convert(Type type, Object object) {
         return DateTimeUtils.fromJavaTimestamp((Timestamp) object);
       case BINARY:
         return ByteBuffer.wrap((byte[]) object);
-      case BOOLEAN:
       case INTEGER:
+        return ((Number) object).intValue();
+      case BOOLEAN:
       case LONG:
       case FLOAT:
       case DOUBLE:

File: spark/v3.0/spark/src/main/java/org/apache/iceberg/spark/IcebergSpark.java
Patch:
@@ -34,6 +34,7 @@ public static void registerBucketUDF(SparkSession session, String funcName, Data
     SparkTypeToType typeConverter = new SparkTypeToType();
     Type sourceIcebergType = typeConverter.atomic(sourceType);
     Transform<Object, Integer> bucket = Transforms.bucket(sourceIcebergType, numBuckets);
-    session.udf().register(funcName, bucket::apply, DataTypes.IntegerType);
+    session.udf().register(funcName,
+        value -> bucket.apply(SparkValueConverter.convert(sourceIcebergType, value)), DataTypes.IntegerType);
   }
 }

File: spark/v3.0/spark/src/main/java/org/apache/iceberg/spark/SparkValueConverter.java
Patch:
@@ -79,8 +79,9 @@ public static Object convert(Type type, Object object) {
         return DateTimeUtils.fromJavaTimestamp((Timestamp) object);
       case BINARY:
         return ByteBuffer.wrap((byte[]) object);
-      case BOOLEAN:
       case INTEGER:
+        return ((Number) object).intValue();
+      case BOOLEAN:
       case LONG:
       case FLOAT:
       case DOUBLE:

File: spark/v3.1/spark/src/main/java/org/apache/iceberg/spark/IcebergSpark.java
Patch:
@@ -34,6 +34,7 @@ public static void registerBucketUDF(SparkSession session, String funcName, Data
     SparkTypeToType typeConverter = new SparkTypeToType();
     Type sourceIcebergType = typeConverter.atomic(sourceType);
     Transform<Object, Integer> bucket = Transforms.bucket(sourceIcebergType, numBuckets);
-    session.udf().register(funcName, bucket::apply, DataTypes.IntegerType);
+    session.udf().register(funcName,
+        value -> bucket.apply(SparkValueConverter.convert(sourceIcebergType, value)), DataTypes.IntegerType);
   }
 }

File: spark/v3.1/spark/src/main/java/org/apache/iceberg/spark/SparkValueConverter.java
Patch:
@@ -79,8 +79,9 @@ public static Object convert(Type type, Object object) {
         return DateTimeUtils.fromJavaTimestamp((Timestamp) object);
       case BINARY:
         return ByteBuffer.wrap((byte[]) object);
-      case BOOLEAN:
       case INTEGER:
+        return ((Number) object).intValue();
+      case BOOLEAN:
       case LONG:
       case FLOAT:
       case DOUBLE:

File: hive-metastore/src/main/java/org/apache/iceberg/hive/HiveSchemaConverter.java
Patch:
@@ -64,7 +64,7 @@ List<Types.NestedField> convertInternal(List<String> names, List<TypeInfo> typeI
     List<Types.NestedField> result = new ArrayList<>(names.size());
     for (int i = 0; i < names.size(); ++i) {
       result.add(Types.NestedField.optional(id++, names.get(i), convertType(typeInfos.get(i)),
-          comments.isEmpty() ? null : comments.get(i)));
+          (comments.isEmpty() || i >= comments.size()) ? null : comments.get(i)));
     }
 
     return result;

File: mr/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergStorageHandlerNoScan.java
Patch:
@@ -547,7 +547,8 @@ public void testCreateTableWithColumnComments() {
     TableIdentifier identifier = TableIdentifier.of("default", "comment_table");
     shell.executeStatement("CREATE EXTERNAL TABLE comment_table (" +
         "t_int INT COMMENT 'int column',  " +
-        "t_string STRING COMMENT 'string column') " +
+        "t_string STRING COMMENT 'string column', " +
+        "t_string_2 STRING) " +
         "STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' " +
         testTables.locationForCreateTableSQL(identifier) +
         testTables.propertiesForCreateTableSQL(ImmutableMap.of()));
@@ -558,7 +559,7 @@ public void testCreateTableWithColumnComments() {
     for (int i = 0; i < icebergTable.schema().columns().size(); i++) {
       Types.NestedField field = icebergTable.schema().columns().get(i);
       Assert.assertArrayEquals(new Object[] {field.name(), HiveSchemaUtil.convert(field.type()).getTypeName(),
-          field.doc()}, rows.get(i));
+          (field.doc() != null ? field.doc() : "from deserializer")}, rows.get(i));
     }
   }
 

File: api/src/main/java/org/apache/iceberg/ManageSnapshots.java
Patch:
@@ -76,7 +76,7 @@ public interface ManageSnapshots extends PendingUpdate<Snapshot> {
    * @param snapshotId a snapshotId whose changes to apply
    * @return this for method chaining
    * @throws IllegalArgumentException If the table has no snapshot with the given id
-   * @throws DuplicateWAPCommitException In case of a WAP workflow and if the table has has a duplicate commit with same
+   * @throws DuplicateWAPCommitException In case of a WAP workflow and if the table has a duplicate commit with same
    * wapId
    */
   ManageSnapshots cherrypick(long snapshotId);

File: spark/v3.0/spark/src/main/java/org/apache/iceberg/spark/procedures/AddFilesProcedure.java
Patch:
@@ -141,7 +141,7 @@ private long importToIceberg(Identifier destIdent, Identifier sourceIdent, Map<S
       }
 
       Snapshot snapshot = table.currentSnapshot();
-      return Long.parseLong(snapshot.summary().get(SnapshotSummary.ADDED_FILES_PROP));
+      return Long.parseLong(snapshot.summary().getOrDefault(SnapshotSummary.ADDED_FILES_PROP, "0"));
     });
   }
 

File: spark/v3.1/spark/src/main/java/org/apache/iceberg/spark/procedures/AddFilesProcedure.java
Patch:
@@ -141,7 +141,7 @@ private long importToIceberg(Identifier destIdent, Identifier sourceIdent, Map<S
       }
 
       Snapshot snapshot = table.currentSnapshot();
-      return Long.parseLong(snapshot.summary().get(SnapshotSummary.ADDED_FILES_PROP));
+      return Long.parseLong(snapshot.summary().getOrDefault(SnapshotSummary.ADDED_FILES_PROP, "0"));
     });
   }
 

File: spark/v3.2/spark/src/main/java/org/apache/iceberg/spark/procedures/AddFilesProcedure.java
Patch:
@@ -141,7 +141,7 @@ private long importToIceberg(Identifier destIdent, Identifier sourceIdent, Map<S
       }
 
       Snapshot snapshot = table.currentSnapshot();
-      return Long.parseLong(snapshot.summary().get(SnapshotSummary.ADDED_FILES_PROP));
+      return Long.parseLong(snapshot.summary().getOrDefault(SnapshotSummary.ADDED_FILES_PROP, "0"));
     });
   }
 

File: aws/src/main/java/org/apache/iceberg/aws/glue/GlueCatalog.java
Patch:
@@ -260,7 +260,8 @@ public void renameTable(TableIdentifier from, TableIdentifier to) {
     TableInput.Builder tableInputBuilder = TableInput.builder()
         .owner(fromTable.owner())
         .tableType(fromTable.tableType())
-        .parameters(fromTable.parameters());
+        .parameters(fromTable.parameters())
+        .storageDescriptor(fromTable.storageDescriptor());
 
     glue.createTable(CreateTableRequest.builder()
         .catalogId(awsProperties.glueCatalogId())

File: spark/v3.2/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSpark.java
Patch:
@@ -147,7 +147,7 @@ public void testRegisterBinaryBucketUDF() {
         spark.sql("SELECT iceberg_bucket_binary_16(X'0020001F')").collectAsList();
     Assert.assertEquals(1, results.size());
     Assert.assertEquals((int) Transforms.bucket(Types.BinaryType.get(), 16)
-            .apply(ByteBuffer.wrap((new byte[]{0x00, 0x20, 0x00, 0x1F}))),
+            .apply(ByteBuffer.wrap(new byte[]{0x00, 0x20, 0x00, 0x1F})),
         results.get(0).getInt(0));
   }
 

File: spark/v3.2/spark/src/main/java/org/apache/iceberg/spark/IcebergSpark.java
Patch:
@@ -34,6 +34,7 @@ public static void registerBucketUDF(SparkSession session, String funcName, Data
     SparkTypeToType typeConverter = new SparkTypeToType();
     Type sourceIcebergType = typeConverter.atomic(sourceType);
     Transform<Object, Integer> bucket = Transforms.bucket(sourceIcebergType, numBuckets);
-    session.udf().register(funcName, bucket::apply, DataTypes.IntegerType);
+    session.udf().register(funcName,
+        value -> bucket.apply(SparkValueConverter.convert(sourceIcebergType, value)), DataTypes.IntegerType);
   }
 }

File: spark/v3.2/spark/src/main/java/org/apache/iceberg/spark/SparkValueConverter.java
Patch:
@@ -79,8 +79,9 @@ public static Object convert(Type type, Object object) {
         return DateTimeUtils.fromJavaTimestamp((Timestamp) object);
       case BINARY:
         return ByteBuffer.wrap((byte[]) object);
-      case BOOLEAN:
       case INTEGER:
+        return ((Number) object).intValue();
+      case BOOLEAN:
       case LONG:
       case FLOAT:
       case DOUBLE:

File: flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergFilesCommitter.java
Patch:
@@ -40,7 +40,6 @@
 import org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness;
 import org.apache.flink.table.data.RowData;
 import org.apache.hadoop.conf.Configuration;
-import org.apache.iceberg.AssertHelpers;
 import org.apache.iceberg.DataFile;
 import org.apache.iceberg.DeleteFile;
 import org.apache.iceberg.FileFormat;
@@ -49,7 +48,6 @@
 import org.apache.iceberg.ManifestFile;
 import org.apache.iceberg.PartitionSpec;
 import org.apache.iceberg.TableTestBase;
-import org.apache.iceberg.exceptions.ValidationException;
 import org.apache.iceberg.flink.FlinkSchemaUtil;
 import org.apache.iceberg.flink.SimpleDataUtil;
 import org.apache.iceberg.flink.TestHelpers;

File: core/src/main/java/org/apache/iceberg/util/SortOrderUtil.java
Patch:
@@ -44,7 +44,7 @@ public static SortOrder buildSortOrder(Table table) {
     return buildSortOrder(table.schema(), table.spec(), table.sortOrder());
   }
 
-  static SortOrder buildSortOrder(Schema schema, PartitionSpec spec, SortOrder sortOrder) {
+  public static SortOrder buildSortOrder(Schema schema, PartitionSpec spec, SortOrder sortOrder) {
     if (sortOrder.isUnsorted() && spec.isUnpartitioned()) {
       return SortOrder.unsorted();
     }

File: spark/src/jmh/java/org/apache/iceberg/spark/data/parquet/SparkParquetReadersFlatDataBenchmark.java
Patch:
@@ -61,7 +61,7 @@
  *
  * To run this benchmark for either spark-2 or spark-3:
  * <code>
- *   ./gradlew :iceberg-spark[2|3]:jmh
+ *   ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh
  *       -PjmhIncludeRegex=SparkParquetReadersFlatDataBenchmark
  *       -PjmhOutputPath=benchmark/spark-parquet-readers-flat-data-benchmark-result.txt
  * </code>

File: spark/src/jmh/java/org/apache/iceberg/spark/data/parquet/SparkParquetReadersNestedDataBenchmark.java
Patch:
@@ -61,7 +61,7 @@
  *
  * To run this benchmark for either spark-2 or spark-3:
  * <code>
- *   ./gradlew :iceberg-spark[2|3]:jmh
+ *   ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh
  *       -PjmhIncludeRegex=SparkParquetReadersNestedDataBenchmark
  *       -PjmhOutputPath=benchmark/spark-parquet-readers-nested-data-benchmark-result.txt
  * </code>

File: spark/src/jmh/java/org/apache/iceberg/spark/data/parquet/SparkParquetWritersFlatDataBenchmark.java
Patch:
@@ -53,7 +53,7 @@
  *
  * To run this benchmark for either spark-2 or spark-3:
  * <code>
- *   ./gradlew :iceberg-spark[2|3]:jmh
+ *   ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh
  *       -PjmhIncludeRegex=SparkParquetWritersFlatDataBenchmark
  *       -PjmhOutputPath=benchmark/spark-parquet-writers-flat-data-benchmark-result.txt
  * </code>

File: spark/src/jmh/java/org/apache/iceberg/spark/data/parquet/SparkParquetWritersNestedDataBenchmark.java
Patch:
@@ -53,7 +53,7 @@
  *
  * To run this benchmark for either spark-2 or spark-3:
  * <code>
- *   ./gradlew :iceberg-spark[2|3]:jmh
+ *   ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh
  *       -PjmhIncludeRegex=SparkParquetWritersNestedDataBenchmark
  *       -PjmhOutputPath=benchmark/spark-parquet-writers-nested-data-benchmark-result.txt
  * </code>

File: spark/src/jmh/java/org/apache/iceberg/spark/source/avro/AvroWritersBenchmark.java
Patch:
@@ -27,7 +27,7 @@
  *
  * To run this benchmark for either spark-2 or spark-3:
  * <code>
- *   ./gradlew :iceberg-spark[2|3]:jmh
+ *   ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh
  *       -PjmhIncludeRegex=AvroWritersBenchmark
  *       -PjmhOutputPath=benchmark/avro-writers-benchmark-result.txt
  * </code>

File: spark/src/jmh/java/org/apache/iceberg/spark/source/avro/IcebergSourceFlatAvroDataReadBenchmark.java
Patch:
@@ -43,7 +43,7 @@
  *
  * To run this benchmark for either spark-2 or spark-3:
  * <code>
- *   ./gradlew :iceberg-spark[2|3]:jmh
+ *   ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh
  *       -PjmhIncludeRegex=IcebergSourceFlatAvroDataReadBenchmark
  *       -PjmhOutputPath=benchmark/iceberg-source-flat-avro-data-read-benchmark-result.txt
  * </code>

File: spark/src/jmh/java/org/apache/iceberg/spark/source/avro/IcebergSourceNestedAvroDataReadBenchmark.java
Patch:
@@ -44,7 +44,7 @@
  *
  * To run this benchmark for either spark-2 or spark-3:
  * <code>
- *   ./gradlew :iceberg-spark[2|3]:jmh
+ *   ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh
  *       -PjmhIncludeRegex=IcebergSourceNestedAvroDataReadBenchmark
  *       -PjmhOutputPath=benchmark/iceberg-source-nested-avro-data-read-benchmark-result.txt
  * </code>

File: spark/src/jmh/java/org/apache/iceberg/spark/source/orc/IcebergSourceFlatORCDataReadBenchmark.java
Patch:
@@ -43,7 +43,7 @@
  *
  * To run this benchmark for either spark-2 or spark-3:
  * <code>
- *   ./gradlew :iceberg-spark[2|3]:jmh
+ *   ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh
  *       -PjmhIncludeRegex=IcebergSourceFlatORCDataReadBenchmark
  *       -PjmhOutputPath=benchmark/iceberg-source-flat-orc-data-read-benchmark-result.txt
  * </code>

File: spark/src/jmh/java/org/apache/iceberg/spark/source/orc/IcebergSourceNestedListORCDataWriteBenchmark.java
Patch:
@@ -42,7 +42,7 @@
  *
  * To run this benchmark for either spark-2 or spark-3:
  * <code>
- *   ./gradlew :iceberg-spark[2|3]:jmh
+ *   ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh
  *       -PjmhIncludeRegex=IcebergSourceNestedListORCDataWriteBenchmark
  *       -PjmhOutputPath=benchmark/iceberg-source-nested-list-orc-data-write-benchmark-result.txt
  * </code>

File: spark/src/jmh/java/org/apache/iceberg/spark/source/orc/IcebergSourceNestedORCDataReadBenchmark.java
Patch:
@@ -45,7 +45,7 @@
  *
  * To run this benchmark for either spark-2 or spark-3:
  * <code>
- *   ./gradlew :iceberg-spark[2|3]:jmh
+ *   ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh
  *       -PjmhIncludeRegex=IcebergSourceNestedORCDataReadBenchmark
  *       -PjmhOutputPath=benchmark/iceberg-source-nested-orc-data-read-benchmark-result.txt
  * </code>

File: spark/src/jmh/java/org/apache/iceberg/spark/source/parquet/IcebergSourceFlatParquetDataFilterBenchmark.java
Patch:
@@ -46,7 +46,7 @@
  *
  * To run this benchmark for either spark-2 or spark-3:
  * <code>
- *   ./gradlew :iceberg-spark[2|3]:jmh
+ *   ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh
  *       -PjmhIncludeRegex=IcebergSourceFlatParquetDataFilterBenchmark
  *       -PjmhOutputPath=benchmark/iceberg-source-flat-parquet-data-filter-benchmark-result.txt
  * </code>

File: spark/src/jmh/java/org/apache/iceberg/spark/source/parquet/IcebergSourceFlatParquetDataReadBenchmark.java
Patch:
@@ -42,7 +42,7 @@
  *
  * To run this benchmark for either spark-2 or spark-3:
  * <code>
- *   ./gradlew :iceberg-spark[2|3]:jmh
+ *   ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh
  *       -PjmhIncludeRegex=IcebergSourceFlatParquetDataReadBenchmark
  *       -PjmhOutputPath=benchmark/iceberg-source-flat-parquet-data-read-benchmark-result.txt
  * </code>

File: spark/src/jmh/java/org/apache/iceberg/spark/source/parquet/IcebergSourceFlatParquetDataWriteBenchmark.java
Patch:
@@ -40,7 +40,7 @@
  *
  * To run this benchmark for either spark-2 or spark-3:
  * <code>
- *   ./gradlew :iceberg-spark[2|3]:jmh
+ *   ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh
  *       -PjmhIncludeRegex=IcebergSourceFlatParquetDataWriteBenchmark
  *       -PjmhOutputPath=benchmark/iceberg-source-flat-parquet-data-write-benchmark-result.txt
  * </code>

File: spark/src/jmh/java/org/apache/iceberg/spark/source/parquet/IcebergSourceNestedListParquetDataWriteBenchmark.java
Patch:
@@ -43,7 +43,7 @@
  *
  * To run this benchmark for either spark-2 or spark-3:
  * <code>
- *   ./gradlew :iceberg-spark[2|3]:jmh
+ *   ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh
  *       -PjmhIncludeRegex=IcebergSourceNestedListParquetDataWriteBenchmark
  *       -PjmhOutputPath=benchmark/iceberg-source-nested-list-parquet-data-write-benchmark-result.txt
  * </code>

File: spark/src/jmh/java/org/apache/iceberg/spark/source/parquet/IcebergSourceNestedParquetDataFilterBenchmark.java
Patch:
@@ -46,7 +46,7 @@
  *
  * To run this benchmark for either spark-2 or spark-3:
  * <code>
- *   ./gradlew :iceberg-spark[2|3]:jmh
+ *   ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh
  *       -PjmhIncludeRegex=IcebergSourceNestedParquetDataFilterBenchmark
  *       -PjmhOutputPath=benchmark/iceberg-source-nested-parquet-data-filter-benchmark-result.txt
  * </code>

File: spark/src/jmh/java/org/apache/iceberg/spark/source/parquet/IcebergSourceNestedParquetDataReadBenchmark.java
Patch:
@@ -42,7 +42,7 @@
  *
  * To run this benchmark for either spark-2 or spark-3:
  * <code>
- *   ./gradlew :iceberg-spark[2|3]:jmh
+ *   ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh
  *       -PjmhIncludeRegex=IcebergSourceNestedParquetDataReadBenchmark
  *       -PjmhOutputPath=benchmark/iceberg-source-nested-parquet-data-read-benchmark-result.txt
  * </code>

File: spark/src/jmh/java/org/apache/iceberg/spark/source/parquet/IcebergSourceNestedParquetDataWriteBenchmark.java
Patch:
@@ -41,7 +41,7 @@
  *
  * To run this benchmark for either spark-2 or spark-3:
  * <code>
- *   ./gradlew :iceberg-spark[2|3]:jmh
+ *   ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh
  *       -PjmhIncludeRegex=IcebergSourceNestedParquetDataWriteBenchmark
  *       -PjmhOutputPath=benchmark/iceberg-source-nested-parquet-data-write-benchmark-result.txt
  * </code>

File: spark/src/jmh/java/org/apache/iceberg/spark/source/parquet/ParquetWritersBenchmark.java
Patch:
@@ -27,7 +27,7 @@
  *
  * To run this benchmark for either spark-2 or spark-3:
  * <code>
- *   ./gradlew :iceberg-spark[2|3]:jmh
+ *   ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh
  *       -PjmhIncludeRegex=ParquetWritersBenchmark
  *       -PjmhOutputPath=benchmark/parquet-writers-benchmark-result.txt
  * </code>

File: spark/src/jmh/java/org/apache/iceberg/spark/source/parquet/vectorized/VectorizedReadDictionaryEncodedFlatParquetDataBenchmark.java
Patch:
@@ -43,7 +43,7 @@
  * <p>
  * To run this benchmark for either spark-2 or spark-3:
  * <code>
- *   ./gradlew :iceberg-spark[2|3]:jmh
+ *   ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh
  *       -PjmhIncludeRegex=VectorizedReadDictionaryEncodedFlatParquetDataBenchmark
  *       -PjmhOutputPath=benchmark/results.txt
  * </code>

File: spark/src/jmh/java/org/apache/iceberg/spark/source/parquet/vectorized/VectorizedReadFlatParquetDataBenchmark.java
Patch:
@@ -53,7 +53,7 @@
  * <p>
  * To run this benchmark for either spark-2 or spark-3:
  * <code>
- *   ./gradlew :iceberg-spark[2|3]:jmh
+ *   ./gradlew :iceberg-spark:iceberg-spark[2|3]:jmh
  *       -PjmhIncludeRegex=VectorizedReadFlatParquetDataBenchmark
  *       -PjmhOutputPath=benchmark/results.txt
  * </code>

File: api/src/main/java/org/apache/iceberg/Metrics.java
Patch:
@@ -24,8 +24,8 @@
 import java.io.ObjectOutputStream;
 import java.io.Serializable;
 import java.nio.ByteBuffer;
-import java.util.HashMap;
 import java.util.Map;
+import org.apache.iceberg.relocated.com.google.common.collect.Maps;
 import org.apache.iceberg.util.ByteBuffers;
 
 /**
@@ -230,7 +230,7 @@ private static Map<Integer, ByteBuffer> readByteBufferMap(ObjectInputStream in)
       return null;
 
     } else {
-      Map<Integer, ByteBuffer> result = new HashMap<>(size);
+      Map<Integer, ByteBuffer> result = Maps.newHashMapWithExpectedSize(size);
 
       for (int i = 0; i < size; ++i) {
         Integer key = (Integer) in.readObject();

File: mr/src/main/java/org/apache/iceberg/mr/Catalogs.java
Patch:
@@ -19,7 +19,6 @@
 
 package org.apache.iceberg.mr;
 
-import java.util.HashMap;
 import java.util.Map;
 import java.util.Optional;
 import java.util.Properties;
@@ -39,6 +38,7 @@
 import org.apache.iceberg.relocated.com.google.common.annotations.VisibleForTesting;
 import org.apache.iceberg.relocated.com.google.common.base.Preconditions;
 import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;
+import org.apache.iceberg.relocated.com.google.common.collect.Maps;
 import org.apache.iceberg.relocated.com.google.common.collect.Streams;
 
 /**
@@ -150,7 +150,7 @@ public static Table createTable(Configuration conf, Properties props) {
     String catalogName = props.getProperty(InputFormatConfig.CATALOG_NAME);
 
     // Create a table property map without the controlling properties
-    Map<String, String> map = new HashMap<>(props.size());
+    Map<String, String> map = Maps.newHashMapWithExpectedSize(props.size());
     for (Object key : props.keySet()) {
       if (!PROPERTIES_TO_REMOVE.contains(key)) {
         map.put(key.toString(), props.get(key).toString());

File: mr/src/main/java/org/apache/iceberg/mr/hive/Deserializer.java
Patch:
@@ -33,6 +33,7 @@
 import org.apache.iceberg.data.GenericRecord;
 import org.apache.iceberg.data.Record;
 import org.apache.iceberg.mr.hive.serde.objectinspector.WriteObjectInspector;
+import org.apache.iceberg.relocated.com.google.common.collect.Maps;
 import org.apache.iceberg.schema.SchemaWithPartnerVisitor;
 import org.apache.iceberg.types.Type.PrimitiveType;
 import org.apache.iceberg.types.Types.ListType;
@@ -232,7 +233,7 @@ private static class FixNameMappingObjectInspectorPair extends ObjectInspectorPa
     FixNameMappingObjectInspectorPair(Schema schema, ObjectInspectorPair pair) {
       super(pair.writerInspector(), pair.sourceInspector());
 
-      this.sourceNameMap = new HashMap<>(schema.columns().size());
+      this.sourceNameMap = Maps.newHashMapWithExpectedSize(schema.columns().size());
 
       List<? extends StructField> fields = ((StructObjectInspector) sourceInspector()).getAllStructFieldRefs();
       for (int i = 0; i < schema.columns().size(); ++i) {

File: mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergSerDe.java
Patch:
@@ -22,7 +22,6 @@
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collections;
-import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.Properties;
@@ -46,6 +45,7 @@
 import org.apache.iceberg.mr.InputFormatConfig;
 import org.apache.iceberg.mr.hive.serde.objectinspector.IcebergObjectInspector;
 import org.apache.iceberg.mr.mapred.Container;
+import org.apache.iceberg.relocated.com.google.common.collect.Maps;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -55,7 +55,7 @@ public class HiveIcebergSerDe extends AbstractSerDe {
 
   private ObjectInspector inspector;
   private Schema tableSchema;
-  private Map<ObjectInspector, Deserializer> deserializers = new HashMap<>(1);
+  private Map<ObjectInspector, Deserializer> deserializers = Maps.newHashMapWithExpectedSize(1);
   private Container<Record> row = new Container<>();
 
   @Override

File: core/src/main/java/org/apache/iceberg/GenericDataFile.java
Patch:
@@ -30,7 +30,7 @@ class GenericDataFile extends BaseFile<DataFile> implements DataFile {
   /**
    * Used by Avro reflection to instantiate this class when reading manifest files.
    */
-  GenericDataFile(org.apache.avro.Schema avroSchema) {
+  GenericDataFile(Schema avroSchema) {
     super(avroSchema);
   }
 

File: core/src/main/java/org/apache/iceberg/GenericManifestFile.java
Patch:
@@ -65,7 +65,7 @@ public class GenericManifestFile
   /**
    * Used by Avro reflection to instantiate this class when reading manifest files.
    */
-  public GenericManifestFile(org.apache.avro.Schema avroSchema) {
+  public GenericManifestFile(Schema avroSchema) {
     this.avroSchema = avroSchema;
 
     List<Types.NestedField> fields = AvroSchemaUtil.convert(avroSchema).asStructType().fields();

File: core/src/main/java/org/apache/iceberg/avro/PruneColumns.java
Patch:
@@ -26,7 +26,6 @@
 import java.util.Set;
 import org.apache.avro.JsonProperties;
 import org.apache.avro.Schema;
-import org.apache.avro.Schema.Type;
 import org.apache.avro.SchemaNormalization;
 import org.apache.iceberg.mapping.NameMapping;
 import org.apache.iceberg.relocated.com.google.common.base.Preconditions;
@@ -274,9 +273,9 @@ private static Schema copyRecord(Schema record, List<Schema.Field> newFields) {
 
   private boolean isRecord(Schema field) {
     if (AvroSchemaUtil.isOptionSchema(field)) {
-      return AvroSchemaUtil.fromOption(field).getType().equals(Type.RECORD);
+      return AvroSchemaUtil.fromOption(field).getType().equals(Schema.Type.RECORD);
     } else {
-      return field.getType().equals(Type.RECORD);
+      return field.getType().equals(Schema.Type.RECORD);
     }
   }
 

File: core/src/main/java/org/apache/iceberg/avro/RemoveIds.java
Patch:
@@ -82,11 +82,11 @@ private static Schema.Field copyField(Schema.Field field, Schema newSchema) {
     return copy;
   }
 
-  static org.apache.avro.Schema removeIds(org.apache.iceberg.Schema schema) {
+  static Schema removeIds(org.apache.iceberg.Schema schema) {
     return AvroSchemaVisitor.visit(AvroSchemaUtil.convert(schema.asStruct(), "table"), new RemoveIds());
   }
 
-  public static org.apache.avro.Schema removeIds(org.apache.avro.Schema schema) {
+  public static Schema removeIds(Schema schema) {
     return AvroSchemaVisitor.visit(schema, new RemoveIds());
   }
 }

File: core/src/main/java/org/apache/iceberg/avro/ValueReaders.java
Patch:
@@ -664,7 +664,7 @@ public S read(Decoder decoder, Object reuse) throws IOException {
 
       if (decoder instanceof ResolvingDecoder) {
         // this may not set all of the fields. nulls are set by default.
-        for (org.apache.avro.Schema.Field field : ((ResolvingDecoder) decoder).readFieldOrder()) {
+        for (Schema.Field field : ((ResolvingDecoder) decoder).readFieldOrder()) {
           Object reusedValue = get(struct, field.pos());
           set(struct, field.pos(), readers[field.pos()].read(decoder, reusedValue));
         }

File: core/src/main/java/org/apache/iceberg/data/avro/IcebergDecoder.java
Patch:
@@ -104,7 +104,7 @@ public void addSchema(org.apache.iceberg.Schema writeSchema) {
     addSchema(AvroSchemaUtil.convert(writeSchema, "table"));
   }
 
-  private void addSchema(org.apache.avro.Schema writeSchema) {
+  private void addSchema(Schema writeSchema) {
     long fp = SchemaNormalization.parsingFingerprint64(writeSchema);
     decoders.put(fp, new RawDecoder<>(readSchema, writeSchema));
   }
@@ -165,7 +165,7 @@ private static class RawDecoder<D> extends MessageDecoder.BaseDecoder<D> {
      * @param readSchema the schema used to construct datum instances
      * @param writeSchema the schema used to decode buffers
      */
-    private RawDecoder(org.apache.iceberg.Schema readSchema, org.apache.avro.Schema writeSchema) {
+    private RawDecoder(org.apache.iceberg.Schema readSchema, Schema writeSchema) {
       this.reader = new ProjectionDatumReader<>(
           avroSchema -> DataReader.create(readSchema, avroSchema),
           readSchema, ImmutableMap.of(), null);

File: data/src/main/java/org/apache/iceberg/data/orc/GenericOrcReader.java
Patch:
@@ -38,7 +38,7 @@ public class GenericOrcReader implements OrcRowReader<Record> {
   private final OrcValueReader<?> reader;
 
   public GenericOrcReader(
-      org.apache.iceberg.Schema expectedSchema, TypeDescription readOrcSchema, Map<Integer, ?> idToConstant) {
+      Schema expectedSchema, TypeDescription readOrcSchema, Map<Integer, ?> idToConstant) {
     this.reader = OrcSchemaWithTypeVisitor.visit(expectedSchema, readOrcSchema, new ReadBuilder(idToConstant));
   }
 

File: flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java
Patch:
@@ -278,7 +278,7 @@ public void alterDatabase(String name, CatalogDatabase newDatabase, boolean igno
           asNamespaceCatalog.removeProperties(namespace, removals);
         }
 
-      } catch (org.apache.iceberg.exceptions.NoSuchNamespaceException e) {
+      } catch (NoSuchNamespaceException e) {
         if (!ignoreIfNotExists) {
           throw new DatabaseNotExistException(getName(), name, e);
         }

File: flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetWriters.java
Patch:
@@ -116,7 +116,7 @@ public ParquetValueWriter<?> map(MapType sMap, GroupType map,
     }
 
 
-    private ParquetValueWriter<?> newOption(org.apache.parquet.schema.Type fieldType, ParquetValueWriter<?> writer) {
+    private ParquetValueWriter<?> newOption(Type fieldType, ParquetValueWriter<?> writer) {
       int maxD = type.getMaxDefinitionLevel(path(fieldType.getName()));
       return ParquetValueWriters.option(fieldType, maxD, writer);
     }

File: mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergFilterFactory.java
Patch:
@@ -130,7 +130,7 @@ private static Object leafToLiteral(PredicateLeaf leaf) {
       case FLOAT:
         return leaf.getLiteral();
       case DATE:
-        if (leaf.getLiteral() instanceof java.sql.Date) {
+        if (leaf.getLiteral() instanceof Date) {
           return daysFromDate((Date) leaf.getLiteral());
         }
         return daysFromTimestamp((Timestamp) leaf.getLiteral());

File: mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergInputFormat.java
Patch:
@@ -112,7 +112,7 @@ public RecordReader<Void, Container<Record>> getRecordReader(InputSplit split, J
       IcebergSplit icebergSplit = ((IcebergSplitContainer) split).icebergSplit();
       // bogus cast for favouring code reuse over syntax
       return (RecordReader) HIVE_VECTORIZED_RECORDREADER_CTOR.newInstance(
-              new org.apache.iceberg.mr.mapreduce.IcebergInputFormat<>(),
+              new IcebergInputFormat<>(),
               icebergSplit,
               job,
               reporter);

File: parquet/src/main/java/org/apache/iceberg/data/parquet/BaseParquetWriter.java
Patch:
@@ -85,7 +85,7 @@ public ParquetValueWriter<?> list(GroupType array, ParquetValueWriter<?> element
       int repeatedD = type.getMaxDefinitionLevel(repeatedPath);
       int repeatedR = type.getMaxRepetitionLevel(repeatedPath);
 
-      org.apache.parquet.schema.Type elementType = repeated.getType(0);
+      Type elementType = repeated.getType(0);
       int elementD = type.getMaxDefinitionLevel(path(elementType.getName()));
 
       return ParquetValueWriters.collections(repeatedD, repeatedR,
@@ -102,9 +102,9 @@ public ParquetValueWriter<?> map(GroupType map,
       int repeatedD = type.getMaxDefinitionLevel(repeatedPath);
       int repeatedR = type.getMaxRepetitionLevel(repeatedPath);
 
-      org.apache.parquet.schema.Type keyType = repeatedKeyValue.getType(0);
+      Type keyType = repeatedKeyValue.getType(0);
       int keyD = type.getMaxDefinitionLevel(path(keyType.getName()));
-      org.apache.parquet.schema.Type valueType = repeatedKeyValue.getType(1);
+      Type valueType = repeatedKeyValue.getType(1);
       int valueD = type.getMaxDefinitionLevel(path(valueType.getName()));
 
       return ParquetValueWriters.maps(repeatedD, repeatedR,

File: parquet/src/main/java/org/apache/iceberg/parquet/ApplyNameMapping.java
Patch:
@@ -40,7 +40,7 @@ class ApplyNameMapping extends ParquetTypeVisitor<Type> {
 
   @Override
   public Type message(MessageType message, List<Type> fields) {
-    Types.MessageTypeBuilder builder = org.apache.parquet.schema.Types.buildMessage();
+    Types.MessageTypeBuilder builder = Types.buildMessage();
     fields.stream().filter(Objects::nonNull).forEach(builder::addField);
 
     return builder.named(message.getName());
@@ -61,7 +61,7 @@ public Type list(GroupType list, Type elementType) {
         "List type must have element field");
 
     MappedField field = nameMapping.find(currentPath());
-    Type listType = org.apache.parquet.schema.Types.list(list.getRepetition())
+    Type listType = Types.list(list.getRepetition())
         .element(elementType)
         .named(list.getName());
 
@@ -74,7 +74,7 @@ public Type map(GroupType map, Type keyType, Type valueType) {
         "Map type must have both key field and value field");
 
     MappedField field = nameMapping.find(currentPath());
-    Type mapType = org.apache.parquet.schema.Types.map(map.getRepetition())
+    Type mapType = Types.map(map.getRepetition())
         .key(keyType)
         .value(valueType)
         .named(map.getName());

File: parquet/src/main/java/org/apache/iceberg/parquet/ParquetAvroWriter.java
Patch:
@@ -77,7 +77,7 @@ public ParquetValueWriter<?> list(GroupType array, ParquetValueWriter<?> element
       int repeatedD = type.getMaxDefinitionLevel(repeatedPath);
       int repeatedR = type.getMaxRepetitionLevel(repeatedPath);
 
-      org.apache.parquet.schema.Type elementType = repeated.getType(0);
+      Type elementType = repeated.getType(0);
       int elementD = type.getMaxDefinitionLevel(path(elementType.getName()));
 
       return ParquetValueWriters.collections(repeatedD, repeatedR,
@@ -94,9 +94,9 @@ public ParquetValueWriter<?> map(GroupType map,
       int repeatedD = type.getMaxDefinitionLevel(repeatedPath);
       int repeatedR = type.getMaxRepetitionLevel(repeatedPath);
 
-      org.apache.parquet.schema.Type keyType = repeatedKeyValue.getType(0);
+      Type keyType = repeatedKeyValue.getType(0);
       int keyD = type.getMaxDefinitionLevel(path(keyType.getName()));
-      org.apache.parquet.schema.Type valueType = repeatedKeyValue.getType(1);
+      Type valueType = repeatedKeyValue.getType(1);
       int valueD = type.getMaxDefinitionLevel(path(valueType.getName()));
 
       return ParquetValueWriters.maps(repeatedD, repeatedR,

File: spark/src/main/java/org/apache/iceberg/spark/data/SparkParquetWriters.java
Patch:
@@ -118,7 +118,7 @@ public ParquetValueWriter<?> map(MapType sMap, GroupType map,
           sMap.keyType(), sMap.valueType());
     }
 
-    private ParquetValueWriter<?> newOption(org.apache.parquet.schema.Type fieldType, ParquetValueWriter<?> writer) {
+    private ParquetValueWriter<?> newOption(Type fieldType, ParquetValueWriter<?> writer) {
       int maxD = type.getMaxDefinitionLevel(path(fieldType.getName()));
       return ParquetValueWriters.option(fieldType, maxD, writer);
     }

File: data/src/test/java/org/apache/iceberg/data/TestGenericReaderDeletes.java
Patch:
@@ -26,6 +26,7 @@
 import org.apache.iceberg.Table;
 import org.apache.iceberg.TestTables;
 import org.apache.iceberg.io.CloseableIterable;
+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;
 import org.apache.iceberg.util.StructLikeSet;
 import org.junit.Assert;
 
@@ -48,7 +49,8 @@ protected void dropTable(String name) {
   public StructLikeSet rowSet(String name, Table table, String... columns) throws IOException {
     StructLikeSet set = StructLikeSet.create(table.schema().asStruct());
     try (CloseableIterable<Record> reader = IcebergGenerics.read(table).select(columns).build()) {
-      reader.forEach(set::add);
+      Iterables.addAll(set, CloseableIterable.transform(
+          reader, record -> new InternalRecordWrapper(table.schema().asStruct()).wrap(record)));
     }
     return set;
   }

File: mr/src/test/java/org/apache/iceberg/mr/TestInputFormatReaderDeletes.java
Patch:
@@ -32,6 +32,7 @@
 import org.apache.iceberg.TableMetadata;
 import org.apache.iceberg.TableOperations;
 import org.apache.iceberg.data.DeleteReadTests;
+import org.apache.iceberg.data.InternalRecordWrapper;
 import org.apache.iceberg.hadoop.HadoopTables;
 import org.apache.iceberg.util.StructLikeSet;
 import org.junit.Assert;
@@ -104,6 +105,7 @@ public StructLikeSet rowSet(String name, Table table, String... columns) {
         .filter(recordFactory -> recordFactory.name().equals(inputFormat))
         .map(recordFactory -> recordFactory.create(builder.project(projected).conf()).getRecords())
         .flatMap(List::stream)
+        .map(record -> new InternalRecordWrapper(projected.asStruct()).wrap(record))
         .collect(Collectors.toList())
     );
 

File: core/src/main/java/org/apache/iceberg/jdbc/JdbcClientPool.java
Patch:
@@ -47,8 +47,7 @@ class JdbcClientPool extends ClientPoolImpl<Connection, SQLException> {
   @Override
   protected Connection newClient() {
     try {
-      Properties dbProps = new Properties();
-      properties.forEach((key, value) -> dbProps.put(key.replace(JdbcCatalog.PROPERTY_PREFIX, ""), value));
+      Properties dbProps = JdbcUtil.filterAndRemovePrefix(properties, JdbcCatalog.PROPERTY_PREFIX);
       return DriverManager.getConnection(dbUrl, dbProps);
     } catch (SQLException e) {
       throw new UncheckedSQLException(e, "Failed to connect: %s", dbUrl);

File: mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java
Patch:
@@ -373,8 +373,8 @@ private CloseableIterable<T> newOrcIterable(InputFile inputFile, FileScanTask ta
       // ORC does not support reuse containers yet
       switch (inMemoryDataModel) {
         case PIG:
-          // TODO: implement value readers for Pig and Hive
-          throw new UnsupportedOperationException("ORC support not yet supported for Pig and Hive");
+          // TODO: implement value readers for Pig
+          throw new UnsupportedOperationException("ORC support not yet supported for Pig");
         case HIVE:
           if (MetastoreUtil.hive3PresentOnClasspath()) {
             orcIterator = HIVE_VECTORIZED_READER_BUILDER.invoke(inputFile, task, idToConstant, context);

File: core/src/main/java/org/apache/iceberg/BaseTableScan.java
Patch:
@@ -21,7 +21,7 @@
 
 import java.time.Instant;
 import java.time.LocalDateTime;
-import java.time.ZoneId;
+import java.time.ZoneOffset;
 import java.time.format.DateTimeFormatter;
 import java.util.Collection;
 import java.util.Collections;
@@ -306,6 +306,6 @@ private Schema lazyColumnProjection() {
   }
 
   private static String formatTimestampMillis(long millis) {
-    return DATE_FORMAT.format(LocalDateTime.ofInstant(Instant.ofEpochMilli(millis), ZoneId.systemDefault()));
+    return DATE_FORMAT.format(LocalDateTime.ofInstant(Instant.ofEpochMilli(millis), ZoneOffset.UTC));
   }
 }

File: core/src/main/java/org/apache/iceberg/BaseTransaction.java
Patch:
@@ -454,6 +454,7 @@ public TableMetadata refresh() {
     }
 
     @Override
+    @SuppressWarnings("ConsistentOverrides")
     public void commit(TableMetadata underlyingBase, TableMetadata metadata) {
       if (underlyingBase != current) {
         // trigger a refresh and retry

File: core/src/main/java/org/apache/iceberg/FindFiles.java
Patch:
@@ -21,7 +21,7 @@
 
 import java.time.Instant;
 import java.time.LocalDateTime;
-import java.time.ZoneId;
+import java.time.ZoneOffset;
 import java.time.format.DateTimeFormatter;
 import java.util.Arrays;
 import java.util.List;
@@ -109,7 +109,7 @@ public Builder asOfTime(long timestampMillis) {
       // case, there is no valid snapshot to read.
       Preconditions.checkArgument(lastSnapshotId != null,
           "Cannot find a snapshot older than %s",
-          DATE_FORMAT.format(LocalDateTime.ofInstant(Instant.ofEpochMilli(timestampMillis), ZoneId.systemDefault())));
+          DATE_FORMAT.format(LocalDateTime.ofInstant(Instant.ofEpochMilli(timestampMillis), ZoneOffset.UTC)));
       return inSnapshot(lastSnapshotId);
     }
 

File: core/src/main/java/org/apache/iceberg/IncrementalDataTableScan.java
Patch:
@@ -54,10 +54,10 @@ public TableScan useSnapshot(long scanSnapshotId) {
   }
 
   @Override
-  public TableScan appendsBetween(long newFromSnapshotId, long newToSnapshotId) {
-    validateSnapshotIdsRefinement(newFromSnapshotId, newToSnapshotId);
+  public TableScan appendsBetween(long fromSnapshotId, long toSnapshotId) {
+    validateSnapshotIdsRefinement(fromSnapshotId, toSnapshotId);
     return new IncrementalDataTableScan(tableOps(), table(), schema(),
-        context().fromSnapshotId(newFromSnapshotId).toSnapshotId(newToSnapshotId));
+        context().fromSnapshotId(fromSnapshotId).toSnapshotId(toSnapshotId));
   }
 
   @Override

File: hive-metastore/src/main/java/org/apache/iceberg/hive/HiveCatalog.java
Patch:
@@ -72,9 +72,9 @@ public HiveCatalog() {
   /**
    * Hive Catalog constructor.
    *
+   * @param conf Hadoop Configuration
    * @deprecated please use the no-arg constructor, setConf and initialize to construct the catalog. Will be removed in
    * v0.13.0
-   * @param conf Hadoop Configuration
    */
   @Deprecated
   public HiveCatalog(Configuration conf) {

File: hive3/src/main/java/org/apache/iceberg/mr/hive/vector/CompatibilityHiveVectorUtils.java
Patch:
@@ -59,7 +59,7 @@ private CompatibilityHiveVectorUtils() {
    * Returns serialized mapwork instance from a job conf - ported from Hive source code LlapHiveUtils#findMapWork
    *
    * @param job JobConf instance
-   * @return
+   * @return a serialized {@link MapWork} based on the given job conf
    */
   public static MapWork findMapWork(JobConf job) {
     String inputName = job.get(Utilities.INPUT_NAME, null);

File: mr/src/test/java/org/apache/iceberg/mr/hive/TestTables.java
Patch:
@@ -114,8 +114,8 @@ public String catalogName() {
 
   /**
    * The table properties string needed for the CREATE TABLE ... commands,
-   * like "TBLPROPERTIES('iceberg.catalog'='mycatalog')
-   * @return
+   * like {@code TBLPROPERTIES('iceberg.catalog'='mycatalog')}
+   * @return the tables properties string, such as {@code TBLPROPERTIES('iceberg.catalog'='mycatalog')}
    */
   public String propertiesForCreateTableSQL(Map<String, String> tableProperties) {
     Map<String, String> properties = new HashMap<>(tableProperties);

File: arrow/src/main/java/org/apache/iceberg/arrow/vectorized/ColumnVector.java
Patch:
@@ -40,6 +40,8 @@
  *     <li>{@link Types.BinaryType}</li>
  *     <li>{@link Types.TimestampType} (with and without timezone)</li>
  *     <li>{@link Types.DateType}</li>
+ *     <li>{@link Types.TimeType}</li>
+ *     <li>{@link Types.UUIDType}</li>
  *   </ul>
  */
 public class ColumnVector implements AutoCloseable {

File: core/src/test/java/org/apache/iceberg/hadoop/TestHadoopCatalog.java
Patch:
@@ -244,7 +244,7 @@ public void testDropNonIcebergTable() throws Exception {
     HadoopCatalog catalog = new HadoopCatalog(conf, warehousePath);
     TableIdentifier testTable = TableIdentifier.of("db", "ns1", "ns2", "tbl");
     String metaLocation = catalog.defaultWarehouseLocation(testTable);
-    //testing with non existent directory
+    // testing with non existent directory
     Assert.assertFalse(catalog.dropTable(testTable));
 
     FileSystem fs = Util.getFs(new Path(metaLocation), conf);

File: aws/src/main/java/org/apache/iceberg/aws/dynamodb/DynamoDbCatalog.java
Patch:
@@ -256,7 +256,7 @@ public boolean dropNamespace(Namespace namespace) throws NamespaceNotEmptyExcept
       dynamo.deleteItem(DeleteItemRequest.builder()
           .tableName(awsProperties.dynamoDbTableName())
           .key(namespacePrimaryKey(namespace))
-          .conditionExpression("attribute_exists(" + namespace + ")")
+          .conditionExpression("attribute_exists(" + COL_NAMESPACE + ")")
           .build());
       return true;
     } catch (ConditionalCheckFailedException e) {

File: core/src/main/java/org/apache/iceberg/PropertiesUpdate.java
Patch:
@@ -50,7 +50,7 @@ class PropertiesUpdate implements UpdateProperties {
   @Override
   public UpdateProperties set(String key, String value) {
     Preconditions.checkNotNull(key, "Key cannot be null");
-    Preconditions.checkNotNull(key, "Value cannot be null");
+    Preconditions.checkNotNull(value, "Value cannot be null");
     Preconditions.checkArgument(!removals.contains(key),
         "Cannot remove and update the same key: %s", key);
 

File: hive-metastore/src/main/java/org/apache/iceberg/hive/HiveCatalog.java
Patch:
@@ -507,7 +507,7 @@ Database convertToDatabase(Namespace namespace, Map<String, String> meta) {
   public String toString() {
     return MoreObjects.toStringHelper(this)
         .add("name", name)
-        .add("uri", this.conf.get(HiveConf.ConfVars.METASTOREURIS.varname))
+        .add("uri", this.conf == null ? "" : this.conf.get(HiveConf.ConfVars.METASTOREURIS.varname))
         .toString();
   }
 

File: orc/src/main/java/org/apache/iceberg/orc/OrcMetrics.java
Patch:
@@ -146,8 +146,8 @@ private static Metrics buildOrcMetrics(final long numOfRows, final TypeDescripti
       if (icebergColOpt.isPresent()) {
         final Types.NestedField icebergCol = icebergColOpt.get();
         final int fieldId = icebergCol.fieldId();
-        final MetricsMode metricsMode = effectiveMetricsConfig.columnMode(icebergCol.name());
 
+        final MetricsMode metricsMode = MetricsUtil.metricsMode(schema, effectiveMetricsConfig, icebergCol.fieldId());
         columnSizes.put(fieldId, colStat.getBytesOnDisk());
 
         if (metricsMode == MetricsModes.None.get()) {

File: spark/src/test/java/org/apache/iceberg/actions/TestDeleteReachableFilesAction.java
Patch:
@@ -131,7 +131,7 @@ public void dataFilesCleanupWithParallelTasks() {
         .rewriteFiles(ImmutableSet.of(FILE_A), ImmutableSet.of(FILE_C))
         .commit();
 
-    Set<String> deletedFiles = Sets.newHashSet();
+    Set<String> deletedFiles = ConcurrentHashMap.newKeySet();
     Set<String> deleteThreads = ConcurrentHashMap.newKeySet();
     AtomicInteger deleteThreadsIndex = new AtomicInteger(0);
 

File: core/src/main/java/org/apache/iceberg/HistoryTable.java
Patch:
@@ -68,8 +68,9 @@ private DataTask task(TableScan scan) {
     TableOperations ops = operations();
     return StaticDataTask.of(
         ops.io().newInputFile(ops.current().metadataFileLocation()),
-        ops.current().snapshotLog(),
-        convertHistoryEntryFunc(table()));
+        schema(), scan.schema(), ops.current().snapshotLog(),
+        convertHistoryEntryFunc(table())
+    );
   }
 
   private class HistoryScan extends StaticTableScan {

File: core/src/main/java/org/apache/iceberg/ManifestsTable.java
Patch:
@@ -75,8 +75,9 @@ protected DataTask task(TableScan scan) {
     String location = scan.snapshot().manifestListLocation();
     return StaticDataTask.of(
         ops.io().newInputFile(location != null ? location : ops.current().metadataFileLocation()),
-        scan.snapshot().allManifests(),
-        manifest -> ManifestsTable.manifestFileToRow(spec, manifest));
+        schema(), scan.schema(), scan.snapshot().allManifests(),
+        manifest -> ManifestsTable.manifestFileToRow(spec, manifest)
+    );
   }
 
   private class ManifestsTableScan extends StaticTableScan {

File: core/src/main/java/org/apache/iceberg/SnapshotsTable.java
Patch:
@@ -60,8 +60,9 @@ private DataTask task(BaseTableScan scan) {
     TableOperations ops = operations();
     return StaticDataTask.of(
         ops.io().newInputFile(ops.current().metadataFileLocation()),
-        ops.current().snapshots(),
-        SnapshotsTable::snapshotToRow);
+        schema(), scan.schema(), ops.current().snapshots(),
+        SnapshotsTable::snapshotToRow
+    );
   }
 
   @Override

File: core/src/main/java/org/apache/iceberg/DataFiles.java
Patch:
@@ -121,7 +121,7 @@ public static class Builder {
     private FileFormat format = null;
     private long recordCount = -1L;
     private long fileSizeInBytes = -1L;
-    private int sortOrderId = SortOrder.unsorted().orderId();
+    private Integer sortOrderId = SortOrder.unsorted().orderId();
 
     // optional fields
     private Map<Integer, Long> columnSizes = null;

File: nessie/src/test/java/org/apache/iceberg/nessie/BaseTestIceberg.java
Patch:
@@ -108,7 +108,7 @@ NessieCatalog initCatalog(String ref) {
     newCatalog.setConf(hadoopConfig);
     newCatalog.initialize("nessie", ImmutableMap.of("ref", ref,
         CatalogProperties.URI, uri,
-        "auth_type", "NONE",
+        "auth-type", "NONE",
         CatalogProperties.WAREHOUSE_LOCATION, temp.toUri().toString()
     ));
     return newCatalog;

File: nessie/src/test/java/org/apache/iceberg/nessie/NessieUtilTest.java
Patch:
@@ -22,7 +22,7 @@
 import org.apache.iceberg.CatalogProperties;
 import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;
 import org.assertj.core.api.Assertions;
-import org.junit.Test;
+import org.junit.jupiter.api.Test;
 import org.projectnessie.model.CommitMeta;
 
 public class NessieUtilTest {

File: nessie/src/test/java/org/apache/iceberg/nessie/TestNamespace.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.iceberg.catalog.Namespace;
 import org.apache.iceberg.catalog.TableIdentifier;
 import org.assertj.core.api.Assertions;
-import org.junit.Test;
+import org.junit.jupiter.api.Test;
 
 public class TestNamespace extends BaseTestIceberg {
   private static final String BRANCH = "test-namespace";

File: nessie/src/test/java/org/apache/iceberg/nessie/TestTableReference.java
Patch:
@@ -20,7 +20,7 @@
 package org.apache.iceberg.nessie;
 
 import org.assertj.core.api.Assertions;
-import org.junit.Test;
+import org.junit.jupiter.api.Test;
 
 public class TestTableReference {
 

File: api/src/main/java/org/apache/iceberg/actions/RewriteStrategy.java
Patch:
@@ -71,8 +71,9 @@ interface RewriteStrategy extends Serializable {
    * Method which will rewrite files based on this particular RewriteStrategy's algorithm.
    * This will most likely be Action framework specific (Spark/Presto/Flink ....).
    *
+   * @param groupID an identifier for this set of files
    * @param filesToRewrite a group of files to be rewritten together
-   * @return a list of newly written files
+   * @return a set of newly written files
    */
-  List<DataFile> rewriteFiles(List<FileScanTask> filesToRewrite);
+  Set<DataFile> rewriteFiles(String groupID, List<FileScanTask> filesToRewrite);
 }

File: spark3/src/main/java/org/apache/iceberg/spark/source/SparkMicroBatchStream.java
Patch:
@@ -96,7 +96,8 @@ public class SparkMicroBatchStream implements MicroBatchStream {
 
     long tableSplitOpenFileCost = PropertyUtil.propertyAsLong(
         table.properties(), SPLIT_OPEN_FILE_COST, SPLIT_OPEN_FILE_COST_DEFAULT);
-    this.splitOpenFileCost = Spark3Util.propertyAsLong(options, SPLIT_OPEN_FILE_COST, tableSplitOpenFileCost);
+    this.splitOpenFileCost = Spark3Util.propertyAsLong(
+        options, SparkReadOptions.FILE_OPEN_COST, tableSplitOpenFileCost);
 
     InitialOffsetStore initialOffsetStore = new InitialOffsetStore(table, checkpointLocation);
     this.initialOffset = initialOffsetStore.initialOffset();

File: arrow/src/main/java/org/apache/iceberg/arrow/ArrowSchemaUtil.java
Patch:
@@ -98,6 +98,9 @@ public static Field convert(final NestedField field) {
       case TIME:
         arrowType = new ArrowType.Time(TimeUnit.MICROSECOND, Long.SIZE);
         break;
+      case UUID:
+        arrowType = new ArrowType.FixedSizeBinary(16);
+        break;
       case TIMESTAMP:
         arrowType = new ArrowType.Timestamp(TimeUnit.MICROSECOND,
             ((Types.TimestampType) field.type()).shouldAdjustToUTC() ? "UTC" : null);

File: arrow/src/main/java/org/apache/iceberg/arrow/vectorized/ArrowReader.java
Patch:
@@ -80,8 +80,8 @@
  *     <li>Columns with constant values are physically encoded as a dictionary. The Arrow vector
  *     type is int32 instead of the type as per the schema.
  *     See https://github.com/apache/iceberg/issues/2484.</li>
- *     <li>Data types: {@link Types.TimeType}, {@link Types.ListType}, {@link Types.MapType},
- *     {@link Types.StructType}, {@link Types.UUIDType}, {@link Types.FixedType} and
+ *     <li>Data types: {@link Types.ListType}, {@link Types.MapType},
+ *     {@link Types.StructType}, {@link Types.FixedType} and
  *     {@link Types.DecimalType}
  *     See https://github.com/apache/iceberg/issues/2485 and https://github.com/apache/iceberg/issues/2486.</li>
  *     <li>Iceberg v2 spec is not supported.

File: spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAlterTablePartitionFields.java
Patch:
@@ -265,7 +265,8 @@ public void testDropPartitionByName() {
 
     Assert.assertEquals("Table should have 1 partition field", 1, table.spec().fields().size());
 
-    sql("ALTER TABLE %s DROP PARTITION FIELD shard", tableName);
+    // Should be recognized as iceberg command even with extra white spaces
+    sql("ALTER TABLE %s DROP  PARTITION \n FIELD shard", tableName);
 
     table.refresh();
 

File: core/src/main/java/org/apache/iceberg/MetricsUtil.java
Patch:
@@ -34,7 +34,7 @@ private MetricsUtil() {
    * Construct mapping relationship between column id to NaN value counts from input metrics and metrics config.
    */
   public static Map<Integer, Long> createNanValueCounts(
-      Stream<FieldMetrics> fieldMetrics, MetricsConfig metricsConfig, Schema inputSchema) {
+      Stream<FieldMetrics<?>> fieldMetrics, MetricsConfig metricsConfig, Schema inputSchema) {
     Preconditions.checkNotNull(metricsConfig, "metricsConfig is required");
 
     if (fieldMetrics == null || inputSchema == null) {

File: data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriter.java
Patch:
@@ -129,7 +129,7 @@ public void write(Record value, VectorizedRowBatch output) {
   }
 
   @Override
-  public Stream<FieldMetrics> metrics() {
+  public Stream<FieldMetrics<?>> metrics() {
     return writer.metrics();
   }
 
@@ -160,7 +160,7 @@ public void nonNullWrite(int rowId, Record data, ColumnVector output) {
     }
 
     @Override
-    public Stream<FieldMetrics> metrics() {
+    public Stream<FieldMetrics<?>> metrics() {
       return writers.stream().flatMap(OrcValueWriter::metrics);
     }
   }

File: flink/src/main/java/org/apache/iceberg/flink/data/FlinkOrcWriter.java
Patch:
@@ -68,7 +68,7 @@ public void write(RowData row, VectorizedRowBatch output) {
   }
 
   @Override
-  public Stream<FieldMetrics> metrics() {
+  public Stream<FieldMetrics<?>> metrics() {
     return writer.metrics();
   }
 

File: flink/src/main/java/org/apache/iceberg/flink/data/FlinkOrcWriters.java
Patch:
@@ -258,7 +258,7 @@ public void nonNullWrite(int rowId, ArrayData data, ColumnVector output) {
     }
 
     @Override
-    public Stream<FieldMetrics> metrics() {
+    public Stream<FieldMetrics<?>> metrics() {
       return elementWriter.metrics();
     }
 
@@ -306,7 +306,7 @@ public void nonNullWrite(int rowId, MapData data, ColumnVector output) {
     }
 
     @Override
-    public Stream<FieldMetrics> metrics() {
+    public Stream<FieldMetrics<?>> metrics() {
       return Stream.concat(keyWriter.metrics(), valueWriter.metrics());
     }
   }
@@ -344,7 +344,7 @@ public void nonNullWrite(int rowId, RowData data, ColumnVector output) {
     }
 
     @Override
-    public Stream<FieldMetrics> metrics() {
+    public Stream<FieldMetrics<?>> metrics() {
       return writers.stream().flatMap(OrcValueWriter::metrics);
     }
   }

File: orc/src/main/java/org/apache/iceberg/orc/OrcRowWriter.java
Patch:
@@ -41,7 +41,7 @@ public interface OrcRowWriter<T> {
   /**
    * Returns a stream of {@link FieldMetrics} that this OrcRowWriter keeps track of.
    */
-  default Stream<FieldMetrics> metrics() {
+  default Stream<FieldMetrics<?>> metrics() {
     return Stream.empty();
   }
 }

File: parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriter.java
Patch:
@@ -34,7 +34,7 @@ public interface ParquetValueWriter<T> {
   /**
    * Returns a stream of {@link FieldMetrics} that this ParquetValueWriter keeps track of.
    */
-  default Stream<FieldMetrics> metrics() {
+  default Stream<FieldMetrics<?>> metrics() {
     return Stream.empty();
   }
 }

File: spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueWriter.java
Patch:
@@ -53,7 +53,7 @@ default void write(int rowId, int column, SpecializedGetters data, ColumnVector
    * counters, and only return non-empty stream if the writer writes double or float values either by itself or
    * transitively.
    */
-  default Stream<FieldMetrics> metrics() {
+  default Stream<FieldMetrics<?>> metrics() {
     return Stream.empty();
   }
 }

File: spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcWriter.java
Patch:
@@ -65,7 +65,7 @@ public void write(InternalRow value, VectorizedRowBatch output) {
   }
 
   @Override
-  public Stream<FieldMetrics> metrics() {
+  public Stream<FieldMetrics<?>> metrics() {
     return writer.metrics();
   }
 
@@ -146,7 +146,7 @@ public void nonNullWrite(int rowId, int column, SpecializedGetters data, ColumnV
     }
 
     @Override
-    public Stream<FieldMetrics> metrics() {
+    public Stream<FieldMetrics<?>> metrics() {
       return writers.stream().flatMap(SparkOrcValueWriter::metrics);
     }
 

File: api/src/main/java/org/apache/iceberg/util/CharSequenceSet.java
Patch:
@@ -32,11 +32,11 @@ public class CharSequenceSet implements Set<CharSequence>, Serializable {
   private static final ThreadLocal<CharSequenceWrapper> wrappers = ThreadLocal.withInitial(
       () -> CharSequenceWrapper.wrap(null));
 
-  public static Set<CharSequence> of(Iterable<CharSequence> charSequences) {
+  public static CharSequenceSet of(Iterable<CharSequence> charSequences) {
     return new CharSequenceSet(charSequences);
   }
 
-  public static Set<CharSequence> empty() {
+  public static CharSequenceSet empty() {
     return new CharSequenceSet(ImmutableList.of());
   }
 
@@ -116,6 +116,7 @@ public boolean remove(Object obj) {
   }
 
   @Override
+  @SuppressWarnings("CollectionUndefinedEquality")
   public boolean containsAll(Collection<?> objects) {
     if (objects != null) {
       return Iterables.all(objects, this::contains);

File: core/src/main/java/org/apache/iceberg/BaseRowDelta.java
Patch:
@@ -19,14 +19,13 @@
 
 package org.apache.iceberg;
 
-import java.util.Set;
 import org.apache.iceberg.expressions.Expression;
 import org.apache.iceberg.relocated.com.google.common.base.Preconditions;
 import org.apache.iceberg.util.CharSequenceSet;
 
 class BaseRowDelta extends MergingSnapshotProducer<RowDelta> implements RowDelta {
   private Long startingSnapshotId = null; // check all versions by default
-  private final Set<CharSequence> referencedDataFiles = CharSequenceSet.empty();
+  private final CharSequenceSet referencedDataFiles = CharSequenceSet.empty();
   private boolean validateDeletes = false;
   private Expression conflictDetectionFilter = null;
   private boolean caseSensitive = true;

File: core/src/main/java/org/apache/iceberg/MergingSnapshotProducer.java
Patch:
@@ -40,6 +40,7 @@
 import org.apache.iceberg.relocated.com.google.common.collect.Iterators;
 import org.apache.iceberg.relocated.com.google.common.collect.Lists;
 import org.apache.iceberg.relocated.com.google.common.collect.Sets;
+import org.apache.iceberg.util.CharSequenceSet;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -296,8 +297,9 @@ protected void validateAddedDataFiles(TableMetadata base, Long startingSnapshotI
     }
   }
 
+  @SuppressWarnings("CollectionUndefinedEquality")
   protected void validateDataFilesExist(TableMetadata base, Long startingSnapshotId,
-                                        Set<CharSequence> requiredDataFiles, boolean skipDeletes) {
+                                        CharSequenceSet requiredDataFiles, boolean skipDeletes) {
     // if there is no current table state, no files have been removed
     if (base.currentSnapshot() == null) {
       return;

File: core/src/main/java/org/apache/iceberg/deletes/PositionDeleteWriter.java
Patch:
@@ -22,7 +22,6 @@
 import java.io.Closeable;
 import java.io.IOException;
 import java.nio.ByteBuffer;
-import java.util.Set;
 import org.apache.iceberg.DeleteFile;
 import org.apache.iceberg.FileFormat;
 import org.apache.iceberg.FileMetadata;
@@ -41,7 +40,7 @@ public class PositionDeleteWriter<T> implements Closeable {
   private final StructLike partition;
   private final ByteBuffer keyMetadata;
   private final PositionDelete<T> delete;
-  private final Set<CharSequence> pathSet;
+  private final CharSequenceSet pathSet;
   private DeleteFile deleteFile = null;
 
   public PositionDeleteWriter(FileAppender<StructLike> appender, FileFormat format, String location,
@@ -81,7 +80,7 @@ public void close() throws IOException {
     }
   }
 
-  public Set<CharSequence> referencedDataFiles() {
+  public CharSequenceSet referencedDataFiles() {
     return pathSet;
   }
 

File: core/src/main/java/org/apache/iceberg/io/BaseTaskWriter.java
Patch:
@@ -23,7 +23,6 @@
 import java.io.IOException;
 import java.util.List;
 import java.util.Map;
-import java.util.Set;
 import org.apache.iceberg.DataFile;
 import org.apache.iceberg.DeleteFile;
 import org.apache.iceberg.FileFormat;
@@ -44,7 +43,7 @@
 public abstract class BaseTaskWriter<T> implements TaskWriter<T> {
   private final List<DataFile> completedDataFiles = Lists.newArrayList();
   private final List<DeleteFile> completedDeleteFiles = Lists.newArrayList();
-  private final Set<CharSequence> referencedDataFiles = CharSequenceSet.empty();
+  private final CharSequenceSet referencedDataFiles = CharSequenceSet.empty();
 
   private final PartitionSpec spec;
   private final FileFormat format;

File: core/src/main/java/org/apache/iceberg/io/SortedPosDeleteWriter.java
Patch:
@@ -25,7 +25,6 @@
 import java.util.Comparator;
 import java.util.List;
 import java.util.Map;
-import java.util.Set;
 import org.apache.iceberg.DeleteFile;
 import org.apache.iceberg.FileFormat;
 import org.apache.iceberg.StructLike;
@@ -42,7 +41,7 @@ class SortedPosDeleteWriter<T> implements Closeable {
 
   private final Map<CharSequenceWrapper, List<PosRow<T>>> posDeletes = Maps.newHashMap();
   private final List<DeleteFile> completedFiles = Lists.newArrayList();
-  private final Set<CharSequence> referencedDataFiles = CharSequenceSet.empty();
+  private final CharSequenceSet referencedDataFiles = CharSequenceSet.empty();
   private final CharSequenceWrapper wrapper = CharSequenceWrapper.wrap(null);
 
   private final FileAppenderFactory<T> appenderFactory;
@@ -98,7 +97,7 @@ public List<DeleteFile> complete() throws IOException {
     return completedFiles;
   }
 
-  public Set<CharSequence> referencedDataFiles() {
+  public CharSequenceSet referencedDataFiles() {
     return referencedDataFiles;
   }
 

File: core/src/main/java/org/apache/iceberg/io/WriteResult.java
Patch:
@@ -22,7 +22,6 @@
 import java.io.Serializable;
 import java.util.Collections;
 import java.util.List;
-import java.util.Set;
 import org.apache.iceberg.DataFile;
 import org.apache.iceberg.DeleteFile;
 import org.apache.iceberg.relocated.com.google.common.collect.Iterables;
@@ -36,7 +35,7 @@ public class WriteResult implements Serializable {
 
   private WriteResult(List<DataFile> dataFiles,
                       List<DeleteFile> deleteFiles,
-                      Set<CharSequence> referencedDataFiles) {
+                      CharSequenceSet referencedDataFiles) {
     this.dataFiles = dataFiles.toArray(new DataFile[0]);
     this.deleteFiles = deleteFiles.toArray(new DeleteFile[0]);
     this.referencedDataFiles = referencedDataFiles.toArray(new CharSequence[0]);
@@ -61,7 +60,7 @@ public static Builder builder() {
   public static class Builder {
     private final List<DataFile> dataFiles;
     private final List<DeleteFile> deleteFiles;
-    private final Set<CharSequence> referencedDataFiles;
+    private final CharSequenceSet referencedDataFiles;
 
     private Builder() {
       this.dataFiles = Lists.newArrayList();

File: data/src/test/java/org/apache/iceberg/data/DeleteReadTests.java
Patch:
@@ -33,6 +33,7 @@
 import org.apache.iceberg.relocated.com.google.common.collect.Sets;
 import org.apache.iceberg.types.Types;
 import org.apache.iceberg.util.ArrayUtil;
+import org.apache.iceberg.util.CharSequenceSet;
 import org.apache.iceberg.util.Pair;
 import org.apache.iceberg.util.StructLikeSet;
 import org.apache.iceberg.util.StructProjection;
@@ -193,7 +194,7 @@ public void testPositionDeletes() throws IOException {
         Pair.of(dataFile.path(), 6L) // id = 122
     );
 
-    Pair<DeleteFile, Set<CharSequence>> posDeletes = FileHelpers.writeDeleteFile(
+    Pair<DeleteFile, CharSequenceSet> posDeletes = FileHelpers.writeDeleteFile(
         table, Files.localOutput(temp.newFile()), Row.of(0), deletes);
 
     table.newRowDelta()
@@ -225,7 +226,7 @@ public void testMixedPositionAndEqualityDeletes() throws IOException {
         Pair.of(dataFile.path(), 5L) // id = 121
     );
 
-    Pair<DeleteFile, Set<CharSequence>> posDeletes = FileHelpers.writeDeleteFile(
+    Pair<DeleteFile, CharSequenceSet> posDeletes = FileHelpers.writeDeleteFile(
         table, Files.localOutput(temp.newFile()), Row.of(0), deletes);
 
     table.newRowDelta()

File: data/src/test/java/org/apache/iceberg/data/FileHelpers.java
Patch:
@@ -22,7 +22,6 @@
 import java.io.Closeable;
 import java.io.IOException;
 import java.util.List;
-import java.util.Set;
 import org.apache.iceberg.DataFile;
 import org.apache.iceberg.DataFiles;
 import org.apache.iceberg.DeleteFile;
@@ -37,19 +36,20 @@
 import org.apache.iceberg.io.OutputFile;
 import org.apache.iceberg.parquet.Parquet;
 import org.apache.iceberg.types.Types;
+import org.apache.iceberg.util.CharSequenceSet;
 import org.apache.iceberg.util.Pair;
 
 public class FileHelpers {
   private FileHelpers() {
   }
 
-  public static Pair<DeleteFile, Set<CharSequence>> writeDeleteFile(Table table, OutputFile out,
+  public static Pair<DeleteFile, CharSequenceSet> writeDeleteFile(Table table, OutputFile out,
                                                                     List<Pair<CharSequence, Long>> deletes)
       throws IOException {
     return writeDeleteFile(table, out, null, deletes);
   }
 
-  public static Pair<DeleteFile, Set<CharSequence>> writeDeleteFile(Table table, OutputFile out, StructLike partition,
+  public static Pair<DeleteFile, CharSequenceSet> writeDeleteFile(Table table, OutputFile out, StructLike partition,
                                                                     List<Pair<CharSequence, Long>> deletes)
       throws IOException {
     PositionDeleteWriter<?> writer = Parquet.writeDeletes(out)

File: data/src/test/java/org/apache/iceberg/data/TestDataFileIndexStatsFilters.java
Patch:
@@ -22,7 +22,6 @@
 import java.io.File;
 import java.io.IOException;
 import java.util.List;
-import java.util.Set;
 import java.util.stream.Collectors;
 import org.apache.iceberg.DataFile;
 import org.apache.iceberg.DeleteFile;
@@ -35,6 +34,7 @@
 import org.apache.iceberg.io.CloseableIterable;
 import org.apache.iceberg.relocated.com.google.common.collect.Lists;
 import org.apache.iceberg.types.Types;
+import org.apache.iceberg.util.CharSequenceSet;
 import org.apache.iceberg.util.Pair;
 import org.junit.After;
 import org.junit.Assert;
@@ -97,7 +97,7 @@ public void testPositionDeletePlanningPath() throws IOException {
     deletes.add(Pair.of(dataFile.path(), 0L));
     deletes.add(Pair.of(dataFile.path(), 1L));
 
-    Pair<DeleteFile, Set<CharSequence>> posDeletes = FileHelpers.writeDeleteFile(
+    Pair<DeleteFile, CharSequenceSet> posDeletes = FileHelpers.writeDeleteFile(
         table, Files.localOutput(temp.newFile()), deletes);
     table.newRowDelta()
         .addDeletes(posDeletes.first())
@@ -124,7 +124,7 @@ public void testPositionDeletePlanningPathFilter() throws IOException {
     deletes.add(Pair.of("some-other-file.parquet", 0L));
     deletes.add(Pair.of("some-other-file.parquet", 1L));
 
-    Pair<DeleteFile, Set<CharSequence>> posDeletes = FileHelpers.writeDeleteFile(
+    Pair<DeleteFile, CharSequenceSet> posDeletes = FileHelpers.writeDeleteFile(
         table, Files.localOutput(temp.newFile()), deletes);
     table.newRowDelta()
         .addDeletes(posDeletes.first())

File: core/src/main/java/org/apache/iceberg/HistoryTable.java
Patch:
@@ -74,7 +74,7 @@ private DataTask task(TableScan scan) {
 
   private class HistoryScan extends StaticTableScan {
     HistoryScan(TableOperations ops, Table table) {
-      super(ops, table, HISTORY_SCHEMA, HistoryTable.this::task);
+      super(ops, table, HISTORY_SCHEMA, HistoryTable.this.metadataTableType().name(), HistoryTable.this::task);
     }
 
     @Override

File: core/src/main/java/org/apache/iceberg/ManifestsTable.java
Patch:
@@ -81,7 +81,7 @@ protected DataTask task(TableScan scan) {
 
   private class ManifestsTableScan extends StaticTableScan {
     ManifestsTableScan(TableOperations ops, Table table) {
-      super(ops, table, SNAPSHOT_SCHEMA, ManifestsTable.this::task);
+      super(ops, table, SNAPSHOT_SCHEMA, ManifestsTable.this.metadataTableType().name(), ManifestsTable.this::task);
     }
   }
 

File: core/src/main/java/org/apache/iceberg/PartitionsTable.java
Patch:
@@ -97,7 +97,8 @@ private static Iterable<Partition> partitions(Table table, Long snapshotId) {
 
   private class PartitionsScan extends StaticTableScan {
     PartitionsScan(TableOperations ops, Table table) {
-      super(ops, table, PartitionsTable.this.schema(), PartitionsTable.this::task);
+      super(ops, table, PartitionsTable.this.schema(), PartitionsTable.this.metadataTableType().name(),
+            PartitionsTable.this::task);
     }
   }
 

File: core/src/main/java/org/apache/iceberg/SnapshotsTable.java
Patch:
@@ -71,7 +71,7 @@ MetadataTableType metadataTableType() {
 
   private class SnapshotsTableScan extends StaticTableScan {
     SnapshotsTableScan(TableOperations ops, Table table) {
-      super(ops, table, SNAPSHOT_SCHEMA, SnapshotsTable.this::task);
+      super(ops, table, SNAPSHOT_SCHEMA, SnapshotsTable.this.metadataTableType().name(), SnapshotsTable.this::task);
     }
 
     @Override

File: core/src/main/java/org/apache/iceberg/SchemaUpdate.java
Patch:
@@ -677,7 +677,7 @@ private static List<Types.NestedField> addFields(List<Types.NestedField> fields,
     return newFields;
   }
 
-  @SuppressWarnings("checkstyle:IllegalType")
+  @SuppressWarnings({"checkstyle:IllegalType", "JdkObsolete"})
   private static List<Types.NestedField> moveFields(List<Types.NestedField> fields,
                                                     Collection<Move> moves) {
     LinkedList<Types.NestedField> reordered = Lists.newLinkedList(fields);

File: parquet/src/main/java/org/apache/iceberg/parquet/TypeWithSchemaVisitor.java
Patch:
@@ -19,7 +19,7 @@
 
 package org.apache.iceberg.parquet;
 
-import java.util.LinkedList;
+import java.util.ArrayDeque;
 import java.util.List;
 import org.apache.iceberg.relocated.com.google.common.base.Preconditions;
 import org.apache.iceberg.relocated.com.google.common.collect.Lists;
@@ -36,8 +36,8 @@
  * @param <T> the Java class returned by the visitor
  */
 public class TypeWithSchemaVisitor<T> {
-  @SuppressWarnings({"checkstyle:VisibilityModifier", "checkstyle:IllegalType"})
-  protected LinkedList<String> fieldNames = Lists.newLinkedList();
+  @SuppressWarnings("checkstyle:VisibilityModifier")
+  protected ArrayDeque<String> fieldNames = new ArrayDeque<>();
 
   @SuppressWarnings("checkstyle:CyclomaticComplexity")
   public static <T> T visit(org.apache.iceberg.types.Type iType, Type type, TypeWithSchemaVisitor<T> visitor) {

File: core/src/main/java/org/apache/iceberg/MicroBatches.java
Patch:
@@ -242,7 +242,7 @@ private CloseableIterable<FileScanTask> open(ManifestFile manifestFile, boolean
       ManifestGroup manifestGroup = new ManifestGroup(io, ImmutableList.of(manifestFile))
           .specsById(specsById)
           .caseSensitive(caseSensitive);
-      if (scanAllFiles) {
+      if (!scanAllFiles) {
         manifestGroup = manifestGroup
             .filterManifestEntries(entry ->
                 entry.snapshotId() == snapshot.snapshotId() && entry.status() == ManifestEntry.Status.ADDED)

File: parquet/src/main/java/org/apache/iceberg/parquet/PruneColumns.java
Patch:
@@ -21,6 +21,7 @@
 
 import java.util.List;
 import java.util.Set;
+import org.apache.iceberg.relocated.com.google.common.base.Objects;
 import org.apache.iceberg.relocated.com.google.common.base.Preconditions;
 import org.apache.iceberg.relocated.com.google.common.collect.Lists;
 import org.apache.parquet.schema.GroupType;
@@ -102,7 +103,7 @@ public Type list(GroupType list, Type element) {
     if (elementId != null && selectedIds.contains(elementId)) {
       return list;
     } else if (element != null) {
-      if (element != originalElement) {
+      if (!Objects.equal(element, originalElement)) {
         Integer listId = getId(list);
         // the element type was projected
         Type listType = Types.list(list.getRepetition())
@@ -129,7 +130,7 @@ public Type map(GroupType map, Type key, Type value) {
       return map;
     } else if (value != null) {
       Integer mapId = getId(map);
-      if (value != originalValue) {
+      if (!Objects.equal(value, originalValue)) {
         Type mapType =  Types.map(map.getRepetition())
             .key(originalKey)
             .value(value)

File: api/src/main/java/org/apache/iceberg/catalog/Namespace.java
Patch:
@@ -21,6 +21,7 @@
 
 import java.util.Arrays;
 import org.apache.iceberg.relocated.com.google.common.base.Joiner;
+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;
 
 /**
  * A namespace in a {@link Catalog}.
@@ -34,6 +35,7 @@ public static Namespace empty() {
   }
 
   public static Namespace of(String... levels) {
+    Preconditions.checkArgument(null != levels, "Cannot create Namespace from null array");
     if (levels.length == 0) {
       return empty();
     }

File: api/src/test/java/org/apache/iceberg/AssertHelpers.java
Patch:
@@ -41,7 +41,7 @@ public static void assertThrows(String message,
                                   String containedInMessage,
                                   Callable callable) {
     AbstractThrowableAssert<?, ? extends Throwable> check = Assertions.assertThatThrownBy(callable::call)
-        .withFailMessage(message)
+        .as(message)
         .isInstanceOf(expected);
     if (null != containedInMessage) {
       check.hasMessageContaining(containedInMessage);
@@ -61,7 +61,7 @@ public static void assertThrows(String message,
                                   String containedInMessage,
                                   Runnable runnable) {
     AbstractThrowableAssert<?, ? extends Throwable> check = Assertions.assertThatThrownBy(runnable::run)
-        .withFailMessage(message)
+        .as(message)
         .isInstanceOf(expected);
     if (null != containedInMessage) {
       check.hasMessageContaining(containedInMessage);
@@ -105,7 +105,7 @@ public static void assertThrowsCause(String message,
                                        String containedInMessage,
                                        Runnable runnable) {
     Assertions.assertThatThrownBy(runnable::run)
-        .withFailMessage(message)
+        .as(message)
         .getCause()
         .isInstanceOf(expected)
         .hasMessageContaining(containedInMessage);

File: aws/src/main/java/org/apache/iceberg/aws/s3/S3OutputFile.java
Patch:
@@ -61,7 +61,7 @@ public PositionOutputStream createOrOverwrite() {
     try {
       return new S3OutputStream(client(), uri(), awsProperties());
     } catch (IOException e) {
-      throw new UncheckedIOException("Filed to create output stream for location: " + uri(), e);
+      throw new UncheckedIOException("Failed to create output stream for location: " + uri(), e);
     }
   }
 

File: core/src/main/java/org/apache/iceberg/BaseMetastoreTableOperations.java
Patch:
@@ -69,9 +69,7 @@ protected BaseMetastoreTableOperations() {
    * catalogName + "." + database + "." + table.
    * @return The full name
    */
-  protected String tableName() {
-    return null;
-  }
+  protected abstract String tableName();
 
   @Override
   public TableMetadata current() {

File: spark/src/main/java/org/apache/iceberg/spark/data/vectorized/RowPositionColumnVector.java
Patch:
@@ -27,11 +27,11 @@
 import org.apache.spark.sql.vectorized.ColumnarMap;
 import org.apache.spark.unsafe.types.UTF8String;
 
-public class RowPostitionColumnVector extends ColumnVector {
+public class RowPositionColumnVector extends ColumnVector {
 
   private final long batchOffsetInFile;
 
-  RowPostitionColumnVector(long batchOffsetInFile) {
+  RowPositionColumnVector(long batchOffsetInFile) {
     super(SparkSchemaUtil.convert(Types.LongType.get()));
     this.batchOffsetInFile = batchOffsetInFile;
   }

File: spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkOrcReaders.java
Patch:
@@ -405,7 +405,7 @@ public ColumnVector convert(org.apache.orc.storage.ql.exec.vector.ColumnVector v
         if (idToConstant.containsKey(field.fieldId())) {
           fieldVectors.add(new ConstantColumnVector(field.type(), batchSize, idToConstant.get(field.fieldId())));
         } else if (field.equals(MetadataColumns.ROW_POSITION)) {
-          fieldVectors.add(new RowPostitionColumnVector(batchOffsetInFile));
+          fieldVectors.add(new RowPositionColumnVector(batchOffsetInFile));
         } else {
           fieldVectors.add(fieldConverters.get(vectorIndex)
               .convert(structVector.fields[vectorIndex], batchSize, batchOffsetInFile));

File: data/src/test/java/org/apache/iceberg/data/DeleteReadTests.java
Patch:
@@ -329,7 +329,7 @@ private StructLikeSet rowSetWithoutIds(int... idsToRemove) {
     return set;
   }
 
-  protected StructLikeSet rowSetWitIds(int... idsToRetain) {
+  protected StructLikeSet rowSetWithIds(int... idsToRetain) {
     Set<Integer> deletedIds = Sets.newHashSet(ArrayUtil.toIntList(idsToRetain));
     StructLikeSet set = StructLikeSet.create(table.schema().asStruct());
     records.stream()

File: spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReaderDeletes.java
Patch:
@@ -193,7 +193,7 @@ public void testReadEqualityDeleteRows() throws IOException {
         .addDeletes(eqDelete2)
         .commit();
 
-    StructLikeSet expectedRowSet = rowSetWitIds(29, 89, 121, 122);
+    StructLikeSet expectedRowSet = rowSetWithIds(29, 89, 121, 122);
 
     Types.StructType type = table.schema().asStruct();
     StructLikeSet actualRowSet = StructLikeSet.create(type);

File: parquet/src/main/java/org/apache/iceberg/parquet/ParquetDictionaryRowGroupFilter.java
Patch:
@@ -406,6 +406,8 @@ private <T> Set<T> dict(int id, Comparator<T> comparator) {
 
       for (int i = 0; i <= dict.getMaxId(); i++) {
         switch (col.getPrimitiveType().getPrimitiveTypeName()) {
+          case FIXED_LEN_BYTE_ARRAY: dictSet.add((T) conversion.apply(dict.decodeToBinary(i)));
+            break;
           case BINARY: dictSet.add((T) conversion.apply(dict.decodeToBinary(i)));
             break;
           case INT32: dictSet.add((T) conversion.apply(dict.decodeToInt(i)));

File: core/src/main/java/org/apache/iceberg/TableMetadata.java
Patch:
@@ -48,6 +48,7 @@
  */
 public class TableMetadata implements Serializable {
   static final long INITIAL_SEQUENCE_NUMBER = 0;
+  static final long INVALID_SEQUENCE_NUMBER = -1;
   static final int DEFAULT_TABLE_FORMAT_VERSION = 1;
   static final int SUPPORTED_TABLE_FORMAT_VERSION = 2;
   static final int INITIAL_SPEC_ID = 0;

File: api/src/main/java/org/apache/iceberg/transforms/ProjectionUtil.java
Patch:
@@ -265,7 +265,7 @@ static <S, T> UnboundPredicate<T> transformSet(String fieldName,
    * to 1 larger than the correct value. For example, day(1969-12-31 10:00:00) produced 0 instead of -1. To read data
    * written by versions with this bug, this method adjusts the inclusive projection. The current inclusive projection
    * is correct, so this modifies the "correct" projection when needed. For example, < day(1969-12-31 10:00:00) will
-   * produce <= -1 (= 1969-12-31) and is adjusted to <= 0 (= 1969-01-01) because the incorrect transformed value was 0.
+   * produce <= -1 (= 1969-12-31) and is adjusted to <= 0 (= 1970-01-01) because the incorrect transformed value was 0.
    */
   static UnboundPredicate<Integer> fixInclusiveTimeProjection(UnboundPredicate<Integer> projected) {
     if (projected == null) {

File: spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java
Patch:
@@ -579,7 +579,7 @@ public String field(Types.NestedField field, String fieldResult) {
 
     @Override
     public String list(Types.ListType list, String elementResult) {
-      return "map<" + elementResult + ">";
+      return "list<" + elementResult + ">";
     }
 
     @Override

File: core/src/main/java/org/apache/iceberg/ManifestsTable.java
Patch:
@@ -38,7 +38,7 @@ public class ManifestsTable extends BaseMetadataTable {
       Types.NestedField.required(7, "deleted_data_files_count", Types.IntegerType.get()),
       Types.NestedField.required(8, "partition_summaries", Types.ListType.ofRequired(9, Types.StructType.of(
           Types.NestedField.required(10, "contains_null", Types.BooleanType.get()),
-          Types.NestedField.required(11, "contains_nan", Types.BooleanType.get()),
+          Types.NestedField.optional(11, "contains_nan", Types.BooleanType.get()),
           Types.NestedField.optional(12, "lower_bound", Types.StringType.get()),
           Types.NestedField.optional(13, "upper_bound", Types.StringType.get())
       )))

File: core/src/main/java/org/apache/iceberg/util/ManifestFileUtil.java
Patch:
@@ -51,7 +51,7 @@ private static class FieldSummary<T> {
       this.lowerBound = Conversions.fromByteBuffer(primitive, summary.lowerBound());
       this.upperBound = Conversions.fromByteBuffer(primitive, summary.upperBound());
       this.containsNull = summary.containsNull();
-      this.containsNaN = summary.containsNaN();
+      this.containsNaN = summary.containsNaN() == null ? true : summary.containsNaN();
     }
 
     boolean canContain(Object value) {

File: spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCallStatementParser.java
Patch:
@@ -30,6 +30,7 @@
 import org.apache.spark.sql.catalyst.expressions.Literal$;
 import org.apache.spark.sql.catalyst.parser.ParseException;
 import org.apache.spark.sql.catalyst.parser.ParserInterface;
+import org.apache.spark.sql.catalyst.parser.extensions.IcebergParseException;
 import org.apache.spark.sql.catalyst.plans.logical.CallArgument;
 import org.apache.spark.sql.catalyst.plans.logical.CallStatement;
 import org.apache.spark.sql.catalyst.plans.logical.NamedArgument;
@@ -131,7 +132,7 @@ public void testCallWithVarSubstitution() throws ParseException {
 
   @Test
   public void testCallParseError() {
-    AssertHelpers.assertThrows("Should fail with a sensible parse error", ParseException.class,
+    AssertHelpers.assertThrows("Should fail with a sensible parse error", IcebergParseException.class,
         "missing '(' at 'radish'",
         () -> parser.parsePlan("CALL cat.system radish kebab"));
   }

File: mr/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergStorageHandlerNoScan.java
Patch:
@@ -653,7 +653,7 @@ public void testIcebergAndHmsTableProperties() throws Exception {
   @Test
   public void testIcebergHMSPropertiesTranslation() throws Exception {
     Assume.assumeTrue("Iceberg - HMS property translation is only relevant for HiveCatalog",
-        Catalogs.hiveCatalog(shell.getHiveConf()));
+        testTableType == TestTables.TestTableType.HIVE_CATALOG);
 
     TableIdentifier identifier = TableIdentifier.of("default", "customers");
 

File: mr/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergStorageHandlerWithEngine.java
Patch:
@@ -34,7 +34,6 @@
 import org.apache.iceberg.data.GenericRecord;
 import org.apache.iceberg.data.Record;
 import org.apache.iceberg.hive.HiveSchemaUtil;
-import org.apache.iceberg.mr.Catalogs;
 import org.apache.iceberg.mr.TestHelper;
 import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;
 import org.apache.iceberg.relocated.com.google.common.collect.Lists;
@@ -667,7 +666,8 @@ public void testMultiTableInsert() throws IOException {
   @Test
   public void testWriteWithDefaultWriteFormat() {
     Assume.assumeTrue("Testing the default file format is enough for a single scenario.",
-        executionEngine.equals("mr") && Catalogs.hiveCatalog(shell.getHiveConf()) && fileFormat == FileFormat.ORC);
+        executionEngine.equals("mr") && testTableType == TestTables.TestTableType.HIVE_CATALOG &&
+                fileFormat == FileFormat.ORC);
 
     TableIdentifier identifier = TableIdentifier.of("default", "customers");
 

File: mr/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergSerDe.java
Patch:
@@ -28,6 +28,8 @@
 import org.apache.iceberg.data.RandomGenericData;
 import org.apache.iceberg.data.Record;
 import org.apache.iceberg.hadoop.HadoopTables;
+import org.apache.iceberg.mr.Catalogs;
+import org.apache.iceberg.mr.InputFormatConfig;
 import org.apache.iceberg.mr.hive.serde.objectinspector.IcebergObjectInspector;
 import org.apache.iceberg.mr.mapred.Container;
 import org.apache.iceberg.types.Types;
@@ -54,6 +56,7 @@ public void testInitialize() throws IOException, SerDeException {
 
     Properties properties = new Properties();
     properties.setProperty("location", location.toString());
+    properties.setProperty(InputFormatConfig.CATALOG_NAME, Catalogs.ICEBERG_HADOOP_TABLE_NAME);
 
     HadoopTables tables = new HadoopTables(conf);
     tables.create(schema, location.toString());

File: spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java
Patch:
@@ -139,7 +139,6 @@ private static void setupDefaultSparkCatalog(SparkSession spark) {
     ImmutableMap<String, String> config = ImmutableMap.of(
         "type", "hive",
         "default-namespace", "default",
-        "parquet-enabled", "true",
         "cache-enabled", "false" // the source should not use a cache
     );
     String catalogName = "org.apache.iceberg.spark.SparkCatalog";

File: spark3/src/main/java/org/apache/iceberg/actions/Spark3SnapshotAction.java
Patch:
@@ -59,10 +59,10 @@ private Long doExecute() {
     Table icebergTable = stagedTable.table();
     // TODO Check table location here against source location
 
-    ensureNameMappingPresent(icebergTable);
-
     boolean threw = true;
     try {
+      ensureNameMappingPresent(icebergTable);
+
       String stagingLocation = getMetadataLocation(icebergTable);
       LOG.info("Beginning snapshot of {} to {} using metadata location {}", sourceTableIdent(), destTableIdent(),
           stagingLocation);

File: core/src/main/java/org/apache/iceberg/BaseMetastoreCatalog.java
Patch:
@@ -25,6 +25,7 @@
 import org.apache.iceberg.exceptions.AlreadyExistsException;
 import org.apache.iceberg.exceptions.CommitFailedException;
 import org.apache.iceberg.exceptions.NoSuchTableException;
+import org.apache.iceberg.relocated.com.google.common.base.MoreObjects;
 import org.apache.iceberg.relocated.com.google.common.base.Preconditions;
 import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;
 import org.slf4j.Logger;
@@ -146,7 +147,7 @@ protected boolean isValidIdentifier(TableIdentifier tableIdentifier) {
 
   @Override
   public String toString() {
-    return getClass().getSimpleName() + "(" + name() + ")";
+    return MoreObjects.toStringHelper(this).toString();
   }
 
   protected abstract TableOperations newTableOps(TableIdentifier tableIdentifier);

File: flink/src/test/java/org/apache/iceberg/flink/TestHelpers.java
Patch:
@@ -325,6 +325,7 @@ public static void assertEquals(ContentFile<?> expected, ContentFile<?> actual)
     Assert.assertEquals("Content", expected.content(), actual.content());
     Assert.assertEquals("Path", expected.path(), actual.path());
     Assert.assertEquals("Format", expected.format(), actual.format());
+    Assert.assertEquals("Partition size", expected.partition().size(), actual.partition().size());
     for (int i = 0; i < expected.partition().size(); i++) {
       Assert.assertEquals("Partition data at index " + i,
           expected.partition().get(i, Object.class),

File: core/src/test/java/org/apache/iceberg/TestMergeAppend.java
Patch:
@@ -749,7 +749,7 @@ public void testChangedPartitionSpecMergeExisting() {
     V2Assert.assertEquals("Last sequence number should be 2", 2, readMetadata().lastSequenceNumber());
     V1Assert.assertEquals("Table should end with last-sequence-number 0", 0, readMetadata().lastSequenceNumber());
 
-    DataFile newFileY = DataFiles.builder(newSpec)
+    DataFile newFileY = DataFiles.builder(table.spec())
         .withPath("/path/to/data-y.parquet")
         .withFileSizeInBytes(10)
         .withPartitionPath("data_bucket=2/id_bucket=3")

File: spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java
Patch:
@@ -62,7 +62,7 @@ protected List<DataFile> rewriteDataForTasks(List<CombinedScanTask> combinedScan
     Broadcast<FileIO> io = sparkContext.broadcast(fileIO());
     Broadcast<EncryptionManager> encryption = sparkContext.broadcast(encryptionManager());
     RowDataRewriter rowDataRewriter =
-        new RowDataRewriter(table(), table().spec(), caseSensitive(), io, encryption);
+        new RowDataRewriter(table(), spec(), caseSensitive(), io, encryption);
     return rowDataRewriter.rewriteDataForTasks(taskRDD);
   }
 }

File: flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergFilesCommitter.java
Patch:
@@ -52,6 +52,7 @@
 import org.apache.iceberg.exceptions.ValidationException;
 import org.apache.iceberg.flink.FlinkSchemaUtil;
 import org.apache.iceberg.flink.SimpleDataUtil;
+import org.apache.iceberg.flink.TestHelpers;
 import org.apache.iceberg.flink.TestTableLoader;
 import org.apache.iceberg.io.FileAppenderFactory;
 import org.apache.iceberg.io.WriteResult;
@@ -578,7 +579,7 @@ public void testFlinkManifests() throws Exception {
       // 2. Read the data files from manifests and assert.
       List<DataFile> dataFiles = FlinkManifestUtil.readDataFiles(createTestingManifestFile(manifestPath), table.io());
       Assert.assertEquals(1, dataFiles.size());
-      TestFlinkManifest.checkContentFile(dataFile1, dataFiles.get(0));
+      TestHelpers.assertEquals(dataFile1, dataFiles.get(0));
 
       // 3. notifyCheckpointComplete for checkpoint#1
       harness.notifyOfCompletedCheckpoint(checkpoint);
@@ -619,7 +620,7 @@ public void testDeleteFiles() throws Exception {
       // 2. Read the data files from manifests and assert.
       List<DataFile> dataFiles = FlinkManifestUtil.readDataFiles(createTestingManifestFile(manifestPath), table.io());
       Assert.assertEquals(1, dataFiles.size());
-      TestFlinkManifest.checkContentFile(dataFile1, dataFiles.get(0));
+      TestHelpers.assertEquals(dataFile1, dataFiles.get(0));
 
       // 3. notifyCheckpointComplete for checkpoint#1
       harness.notifyOfCompletedCheckpoint(checkpoint);

File: core/src/main/java/org/apache/iceberg/SnapshotProducer.java
Patch:
@@ -32,6 +32,7 @@
 import java.util.function.Consumer;
 import org.apache.iceberg.events.Listeners;
 import org.apache.iceberg.exceptions.CommitFailedException;
+import org.apache.iceberg.exceptions.CommitStateUnknownException;
 import org.apache.iceberg.exceptions.RuntimeIOException;
 import org.apache.iceberg.io.OutputFile;
 import org.apache.iceberg.relocated.com.google.common.base.Preconditions;
@@ -299,6 +300,8 @@ public void commit() {
             taskOps.commit(base, updated.withUUID());
           });
 
+    } catch (CommitStateUnknownException commitStateUnknownException) {
+      throw commitStateUnknownException;
     } catch (RuntimeException e) {
       Exceptions.suppressAndThrow(e, this::cleanAll);
     }

File: core/src/main/java/org/apache/iceberg/TableProperties.java
Patch:
@@ -36,6 +36,9 @@ private TableProperties() {
   public static final String COMMIT_TOTAL_RETRY_TIME_MS = "commit.retry.total-timeout-ms";
   public static final int COMMIT_TOTAL_RETRY_TIME_MS_DEFAULT = 1800000; // 30 minutes
 
+  public static final String COMMIT_NUM_STATUS_CHECKS = "commit.num-status-checks";
+  public static final int COMMIT_NUM_STATUS_CHECKS_DEFAULT = 3;
+
   public static final String MANIFEST_TARGET_SIZE_BYTES = "commit.manifest.target-size-bytes";
   public static final long MANIFEST_TARGET_SIZE_BYTES_DEFAULT = 8388608; // 8 MB
 

File: spark3/src/main/java/org/apache/iceberg/spark/procedures/SparkProcedures.java
Patch:
@@ -50,6 +50,7 @@ private static Map<String, Supplier<ProcedureBuilder>> initProcedureBuilders() {
     mapBuilder.put("expire_snapshots", ExpireSnapshotsProcedure::builder);
     mapBuilder.put("migrate", MigrateTableProcedure::builder);
     mapBuilder.put("snapshot", SnapshotTableProcedure::builder);
+    mapBuilder.put("add_files", AddFilesProcedure::builder);
     return mapBuilder.build();
   }
 

File: hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveClientPool.java
Patch:
@@ -42,14 +42,12 @@ public class TestHiveClientPool {
   public void testConf() {
     HiveConf conf = createHiveConf();
     conf.set(HiveConf.ConfVars.METASTOREWAREHOUSE.varname, "file:/mywarehouse/");
-    conf.setInt("iceberg.hive.client-pool-size", 10);
 
-    HiveClientPool clientPool = new HiveClientPool(conf);
+    HiveClientPool clientPool = new HiveClientPool(10, conf);
     HiveConf clientConf = clientPool.hiveConf();
 
     Assert.assertEquals(conf.get(HiveConf.ConfVars.METASTOREWAREHOUSE.varname),
             clientConf.get(HiveConf.ConfVars.METASTOREWAREHOUSE.varname));
-    Assert.assertEquals(conf.get("iceberg.hive.client-pool-size"), clientConf.get("iceberg.hive.client-pool-size"));
     Assert.assertEquals(10, clientPool.poolSize());
 
     // 'hive.metastore.sasl.enabled' should be 'true' as defined in xml

File: nessie/src/main/java/org/apache/iceberg/nessie/NessieUtil.java
Patch:
@@ -19,14 +19,14 @@
 
 package org.apache.iceberg.nessie;
 
-import com.dremio.nessie.model.ContentsKey;
-import com.dremio.nessie.model.EntriesResponse;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.List;
 import java.util.function.Predicate;
 import org.apache.iceberg.catalog.Namespace;
 import org.apache.iceberg.catalog.TableIdentifier;
+import org.projectnessie.model.ContentsKey;
+import org.projectnessie.model.EntriesResponse;
 
 public final class NessieUtil {
 
@@ -78,7 +78,7 @@ static ContentsKey toKey(TableIdentifier tableIdentifier) {
     }
     identifiers.add(tableIdentifier.name());
 
-    ContentsKey key = new ContentsKey(identifiers);
+    ContentsKey key = ContentsKey.of(identifiers);
     return key;
   }
 

File: nessie/src/test/java/org/apache/iceberg/nessie/TestBranchVisibility.java
Patch:
@@ -19,14 +19,14 @@
 
 package org.apache.iceberg.nessie;
 
-import com.dremio.nessie.error.NessieConflictException;
-import com.dremio.nessie.error.NessieNotFoundException;
 import org.apache.iceberg.catalog.TableIdentifier;
 import org.apache.iceberg.types.Types;
 import org.junit.After;
 import org.junit.Assert;
 import org.junit.Before;
 import org.junit.Test;
+import org.projectnessie.error.NessieConflictException;
+import org.projectnessie.error.NessieNotFoundException;
 
 public class TestBranchVisibility extends BaseTestIceberg {
 

File: mr/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergFilterFactory.java
Patch:
@@ -23,7 +23,6 @@
 import java.sql.Date;
 import java.sql.Timestamp;
 import java.time.LocalDate;
-import java.time.ZoneOffset;
 import org.apache.hadoop.hive.ql.io.sarg.PredicateLeaf;
 import org.apache.hadoop.hive.ql.io.sarg.SearchArgument;
 import org.apache.hadoop.hive.ql.io.sarg.SearchArgumentFactory;
@@ -231,7 +230,7 @@ public void testDateType() {
   public void testTimestampType() {
     Literal<Long> timestampLiteral = Literal.of("2012-10-02T05:16:17.123456").to(Types.TimestampType.withoutZone());
     long timestampMicros = timestampLiteral.value();
-    Timestamp ts = Timestamp.from(DateTimeUtil.timestampFromMicros(timestampMicros).toInstant(ZoneOffset.UTC));
+    Timestamp ts = Timestamp.valueOf(DateTimeUtil.timestampFromMicros(timestampMicros));
 
     SearchArgument.Builder builder = SearchArgumentFactory.newBuilder();
     SearchArgument arg = builder.startAnd().equals("timestamp", PredicateLeaf.Type.TIMESTAMP, ts).end().build();

File: mr/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergFilterFactory.java
Patch:
@@ -218,7 +218,7 @@ public void testBooleanType() {
   @Test
   public void testDateType() {
     SearchArgument.Builder builder = SearchArgumentFactory.newBuilder();
-    Date gmtDate = new Date(LocalDate.of(2015, 11, 12).atStartOfDay(ZoneOffset.UTC).toInstant().toEpochMilli());
+    Date gmtDate = Date.valueOf(LocalDate.of(2015, 11, 12));
     SearchArgument arg = builder.startAnd().equals("date", PredicateLeaf.Type.DATE, gmtDate).end().build();
 
     UnboundPredicate expected = Expressions.equal("date", Literal.of("2015-11-12").to(Types.DateType.get()).value());

File: core/src/main/java/org/apache/iceberg/TableProperties.java
Patch:
@@ -127,7 +127,7 @@ private TableProperties() {
   public static final String WRITE_AUDIT_PUBLISH_ENABLED_DEFAULT = "false";
 
   public static final String WRITE_TARGET_FILE_SIZE_BYTES = "write.target-file-size-bytes";
-  public static final long WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT = Long.MAX_VALUE;
+  public static final long WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT = 536870912; // 512 MB
 
   public static final String SPARK_WRITE_PARTITIONED_FANOUT_ENABLED = "write.spark.fanout.enabled";
   public static final boolean SPARK_WRITE_PARTITIONED_FANOUT_ENABLED_DEFAULT = false;

File: hive-metastore/src/main/java/org/apache/iceberg/hive/HiveCatalog.java
Patch:
@@ -56,6 +56,7 @@
 import org.apache.iceberg.relocated.com.google.common.base.Preconditions;
 import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;
 import org.apache.iceberg.relocated.com.google.common.collect.Maps;
+import org.apache.iceberg.util.PropertyUtil;
 import org.apache.thrift.TException;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -158,8 +159,8 @@ public void initialize(String inputName, Map<String, String> properties) {
       this.conf.set(HiveConf.ConfVars.METASTOREWAREHOUSE.varname, properties.get(CatalogProperties.WAREHOUSE_LOCATION));
     }
 
-    int clientPoolSize = Integer.parseInt(
-        properties.getOrDefault(CatalogProperties.CLIENT_POOL_SIZE, "5"));
+    int clientPoolSize = PropertyUtil.propertyAsInt(properties,
+        CatalogProperties.CLIENT_POOL_SIZE, CatalogProperties.CLIENT_POOL_SIZE_DEFAULT);
     this.clients = new HiveClientPool(clientPoolSize, this.conf);
     this.createStack = Thread.currentThread().getStackTrace();
     this.closed = false;

File: core/src/main/java/org/apache/iceberg/CatalogUtil.java
Patch:
@@ -155,7 +155,7 @@ public static Catalog loadCatalog(
       ctor = DynConstructors.builder(Catalog.class).impl(impl).buildChecked();
     } catch (NoSuchMethodException e) {
       throw new IllegalArgumentException(String.format(
-          "Cannot initialize Catalog, missing no-arg constructor: %s", impl), e);
+          "Cannot initialize Catalog implementation %s: %s", impl, e.getMessage()), e);
     }
 
     Catalog catalog;

File: mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputFormat.java
Patch:
@@ -77,7 +77,7 @@ private static HiveIcebergRecordWriter writer(JobConf jc) {
         new OutputFileFactory(spec, fileFormat, location, io, encryption, taskAttemptID.getTaskID().getId(),
             taskAttemptID.getId(), jc.get(HiveConf.ConfVars.HIVEQUERYID.varname) + "-" + taskAttemptID.getJobID());
     HiveIcebergRecordWriter writer = new HiveIcebergRecordWriter(schema, spec, fileFormat,
-        new GenericAppenderFactory(schema), outputFileFactory, io, targetFileSize, taskAttemptID);
+        new GenericAppenderFactory(schema, spec), outputFileFactory, io, targetFileSize, taskAttemptID);
 
     return writer;
   }

File: mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputFormat.java
Patch:
@@ -74,7 +74,7 @@ private static HiveIcebergRecordWriter writer(JobConf jc) {
     LocationProvider location = HiveIcebergStorageHandler.location(jc);
     EncryptionManager encryption = HiveIcebergStorageHandler.encryption(jc);
     OutputFileFactory outputFileFactory =
-        new OutputFileFactory(spec, FileFormat.PARQUET, location, io, encryption, taskAttemptID.getTaskID().getId(),
+        new OutputFileFactory(spec, fileFormat, location, io, encryption, taskAttemptID.getTaskID().getId(),
             taskAttemptID.getId(), jc.get(HiveConf.ConfVars.HIVEQUERYID.varname) + "-" + taskAttemptID.getJobID());
     HiveIcebergRecordWriter writer = new HiveIcebergRecordWriter(schema, spec, fileFormat,
         new GenericAppenderFactory(schema), outputFileFactory, io, targetFileSize, taskAttemptID);

File: mr/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergStorageHandlerWithEngine.java
Patch:
@@ -34,6 +34,7 @@
 import org.apache.iceberg.Schema;
 import org.apache.iceberg.SchemaParser;
 import org.apache.iceberg.Table;
+import org.apache.iceberg.TableProperties;
 import org.apache.iceberg.catalog.TableIdentifier;
 import org.apache.iceberg.data.GenericRecord;
 import org.apache.iceberg.data.Record;
@@ -535,7 +536,7 @@ public void testPartitionedWrite() throws IOException {
         SchemaParser.toJson(HiveIcebergStorageHandlerTestUtils.CUSTOMER_SCHEMA) + "', " +
         "'" + InputFormatConfig.PARTITION_SPEC + "'='" +
         PartitionSpecParser.toJson(spec) + "', " +
-        "'" + InputFormatConfig.WRITE_FILE_FORMAT + "'='" + fileFormat + "')");
+        "'" + TableProperties.DEFAULT_FILE_FORMAT + "'='" + fileFormat + "')");
 
     List<Record> records = TestHelper.generateRandomRecords(HiveIcebergStorageHandlerTestUtils.CUSTOMER_SCHEMA, 4, 0L);
 

File: core/src/main/java/org/apache/iceberg/V1Metadata.java
Patch:
@@ -62,7 +62,7 @@ public org.apache.avro.Schema getSchema() {
 
     @Override
     public void put(int i, Object v) {
-      throw new UnsupportedOperationException("Cannot read using IndexedManifestFile");
+      throw new UnsupportedOperationException("Cannot modify IndexedManifestFile wrapper via put");
     }
 
     @Override
@@ -243,7 +243,7 @@ public org.apache.avro.Schema getSchema() {
 
     @Override
     public void put(int i, Object v) {
-      throw new UnsupportedOperationException("Cannot read using IndexedManifestEntry");
+      throw new UnsupportedOperationException("Cannot modify IndexedManifestEntry wrapper via put");
     }
 
     @Override
@@ -359,7 +359,7 @@ public Object get(int pos) {
 
     @Override
     public void put(int i, Object v) {
-      throw new UnsupportedOperationException("Cannot read into IndexedDataFile");
+      throw new UnsupportedOperationException("Cannot modify IndexedDataFile wrapper via put");
     }
 
     @Override

File: core/src/main/java/org/apache/iceberg/V2Metadata.java
Patch:
@@ -81,7 +81,7 @@ public org.apache.avro.Schema getSchema() {
 
     @Override
     public void put(int i, Object v) {
-      throw new UnsupportedOperationException("Cannot read using IndexedManifestFile");
+      throw new UnsupportedOperationException("Cannot modify IndexedManifestFile wrapper via put");
     }
 
     @Override
@@ -283,7 +283,7 @@ public org.apache.avro.Schema getSchema() {
 
     @Override
     public void put(int i, Object v) {
-      throw new UnsupportedOperationException("Cannot read using IndexedManifestEntry");
+      throw new UnsupportedOperationException("Cannot modify IndexedManifestEntry wrapper via put");
     }
 
     @Override
@@ -415,7 +415,7 @@ public Object get(int pos) {
 
     @Override
     public void put(int i, Object v) {
-      throw new UnsupportedOperationException("Cannot read into IndexedDataFile");
+      throw new UnsupportedOperationException("Cannot modify IndexedDataFile wrapper via put");
     }
 
     @Override

File: mr/src/main/java/org/apache/iceberg/mr/hive/serde/objectinspector/IcebergTimeObjectInspector.java
Patch:
@@ -19,6 +19,7 @@
 
 package org.apache.iceberg.mr.hive.serde.objectinspector;
 
+import java.time.LocalTime;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveJavaObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
@@ -49,8 +50,8 @@ public Text getPrimitiveWritableObject(Object o) {
   }
 
   @Override
-  public Object convert(Object o) {
-    return o == null ? null : o.toString();
+  public LocalTime convert(Object o) {
+    return o == null ? null : LocalTime.parse((String) o);
   }
 
   @Override

File: mr/src/main/java/org/apache/iceberg/mr/hive/serde/objectinspector/IcebergUUIDObjectInspector.java
Patch:
@@ -19,6 +19,7 @@
 
 package org.apache.iceberg.mr.hive.serde.objectinspector;
 
+import java.util.UUID;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveJavaObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
@@ -49,8 +50,8 @@ public Text getPrimitiveWritableObject(Object o) {
   }
 
   @Override
-  public String convert(Object o) {
-    return o == null ? null : o.toString();
+  public UUID convert(Object o) {
+    return o == null ? null : UUID.fromString(o.toString());
   }
 
   @Override

File: mr/src/test/java/org/apache/iceberg/mr/hive/serde/objectinspector/TestIcebergTimeObjectInspector.java
Patch:
@@ -48,12 +48,13 @@ public void testIcebergTimeObjectInspector() {
     Assert.assertNull(oi.getPrimitiveWritableObject(null));
     Assert.assertNull(oi.convert(null));
 
-    String time = LocalTime.now().toString();
+    LocalTime localTime = LocalTime.now();
+    String time = localTime.toString();
     Text text = new Text(time);
 
     Assert.assertEquals(time, oi.getPrimitiveJavaObject(text));
     Assert.assertEquals(text, oi.getPrimitiveWritableObject(time));
-    Assert.assertEquals(time, oi.convert(text));
+    Assert.assertEquals(localTime, oi.convert(time));
 
     Text copy = (Text) oi.copyObject(text);
 

File: data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java
Patch:
@@ -428,7 +428,7 @@ public void nonNullWrite(int rowId, List<T> value, ColumnVector output) {
       // record the length and start of the list elements
       cv.lengths[rowId] = value.size();
       cv.offsets[rowId] = cv.childCount;
-      cv.childCount += cv.lengths[rowId];
+      cv.childCount = (int) (cv.childCount + cv.lengths[rowId]);
       // make sure the child is big enough
       cv.child.ensureSize(cv.childCount, true);
       // Add each element
@@ -464,7 +464,7 @@ public void nonNullWrite(int rowId, Map<K, V> map, ColumnVector output) {
       // record the length and start of the list elements
       cv.lengths[rowId] = map.size();
       cv.offsets[rowId] = cv.childCount;
-      cv.childCount += cv.lengths[rowId];
+      cv.childCount = (int) (cv.childCount + cv.lengths[rowId]);
       // make sure the child is big enough
       cv.keys.ensureSize(cv.childCount, true);
       cv.values.ensureSize(cv.childCount, true);

File: flink/src/main/java/org/apache/iceberg/flink/data/FlinkOrcWriters.java
Patch:
@@ -245,7 +245,7 @@ public void nonNullWrite(int rowId, ArrayData data, ColumnVector output) {
       ListColumnVector cv = (ListColumnVector) output;
       cv.lengths[rowId] = data.size();
       cv.offsets[rowId] = cv.childCount;
-      cv.childCount += cv.lengths[rowId];
+      cv.childCount = (int) (cv.childCount + cv.lengths[rowId]);
       // make sure the child is big enough.
       cv.child.ensureSize(cv.childCount, true);
 
@@ -285,7 +285,7 @@ public void nonNullWrite(int rowId, MapData data, ColumnVector output) {
       // record the length and start of the list elements
       cv.lengths[rowId] = data.size();
       cv.offsets[rowId] = cv.childCount;
-      cv.childCount += cv.lengths[rowId];
+      cv.childCount = (int) (cv.childCount + cv.lengths[rowId]);
       // make sure the child is big enough
       cv.keys.ensureSize(cv.childCount, true);
       cv.values.ensureSize(cv.childCount, true);

File: spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueWriters.java
Patch:
@@ -236,7 +236,7 @@ public void nonNullWrite(int rowId, int column, SpecializedGetters data, ColumnV
       // record the length and start of the list elements
       cv.lengths[rowId] = value.numElements();
       cv.offsets[rowId] = cv.childCount;
-      cv.childCount += cv.lengths[rowId];
+      cv.childCount = (int) (cv.childCount + cv.lengths[rowId]);
       // make sure the child is big enough
       cv.child.ensureSize(cv.childCount, true);
       // Add each element
@@ -264,7 +264,7 @@ public void nonNullWrite(int rowId, int column, SpecializedGetters data, ColumnV
       // record the length and start of the list elements
       cv.lengths[rowId] = value.numElements();
       cv.offsets[rowId] = cv.childCount;
-      cv.childCount += cv.lengths[rowId];
+      cv.childCount = (int) (cv.childCount + cv.lengths[rowId]);
       // make sure the child is big enough
       cv.keys.ensureSize(cv.childCount, true);
       cv.values.ensureSize(cv.childCount, true);

File: api/src/main/java/org/apache/iceberg/transforms/TransformUtil.java
Patch:
@@ -41,7 +41,8 @@ static String humanYear(int yearOrdinal) {
   }
 
   static String humanMonth(int monthOrdinal) {
-    return String.format("%04d-%02d", EPOCH_YEAR + (monthOrdinal / 12), 1 + (monthOrdinal % 12));
+    return String.format("%04d-%02d",
+        EPOCH_YEAR + Math.floorDiv(monthOrdinal, 12), 1 + Math.floorMod(monthOrdinal, 12));
   }
 
   static String humanDay(int dayOrdinal) {

File: mr/src/main/java/org/apache/iceberg/mr/InputFormatConfig.java
Patch:
@@ -41,6 +41,7 @@ private InputFormatConfig() {
   public static final String READ_SCHEMA = "iceberg.mr.read.schema";
   public static final String SNAPSHOT_ID = "iceberg.mr.snapshot.id";
   public static final String SPLIT_SIZE = "iceberg.mr.split.size";
+  public static final String SCHEMA_AUTO_CONVERSION = "iceberg.mr.schema.auto.conversion";
   public static final String TABLE_IDENTIFIER = "iceberg.mr.table.identifier";
   public static final String TABLE_LOCATION = "iceberg.mr.table.location";
   public static final String TABLE_SCHEMA = "iceberg.mr.table.schema";

File: mr/src/test/java/org/apache/iceberg/mr/hive/TestHiveShell.java
Patch:
@@ -78,7 +78,8 @@ public void setHiveSessionValue(String key, boolean value) {
   }
 
   public void start() {
-    metastore.start();
+    // Create a copy of the HiveConf for the metastore
+    metastore.start(new HiveConf(hs2Conf));
     hs2Conf.setVar(HiveConf.ConfVars.METASTOREURIS, metastore.hiveConf().getVar(HiveConf.ConfVars.METASTOREURIS));
     hs2Conf.setVar(HiveConf.ConfVars.METASTOREWAREHOUSE,
         metastore.hiveConf().getVar(HiveConf.ConfVars.METASTOREWAREHOUSE));

File: orc/src/main/java/org/apache/iceberg/orc/ORC.java
Patch:
@@ -148,6 +148,9 @@ private ReadBuilder(InputFile file) {
       } else {
         this.conf = new Configuration();
       }
+
+      // We need to turn positional schema evolution off since we use column name based schema evolution for projection
+      this.conf.setBoolean(OrcConf.FORCE_POSITIONAL_EVOLUTION.getHiveConfName(), false);
     }
 
     /**

File: spark2/src/test/java/org/apache/iceberg/spark/source/TestFilteredScan.java
Patch:
@@ -575,7 +575,7 @@ private List<Record> testRecords(Schema schema) {
     return Lists.newArrayList(
         record(schema, 0L, parse("2017-12-22T09:20:44.294658+00:00"), "junction"),
         record(schema, 1L, parse("2017-12-22T07:15:34.582910+00:00"), "alligator"),
-        record(schema, 2L, parse("2017-12-22T06:02:09.243857+00:00"), "forrest"),
+        record(schema, 2L, parse("2017-12-22T06:02:09.243857+00:00"), ""),
         record(schema, 3L, parse("2017-12-22T03:10:11.134509+00:00"), "clapping"),
         record(schema, 4L, parse("2017-12-22T00:34:00.184671+00:00"), "brush"),
         record(schema, 5L, parse("2017-12-21T22:20:08.935889+00:00"), "trap"),

File: spark3/src/test/java/org/apache/iceberg/spark/source/TestFilteredScan.java
Patch:
@@ -529,7 +529,7 @@ private List<Record> testRecords(Schema schema) {
     return Lists.newArrayList(
         record(schema, 0L, parse("2017-12-22T09:20:44.294658+00:00"), "junction"),
         record(schema, 1L, parse("2017-12-22T07:15:34.582910+00:00"), "alligator"),
-        record(schema, 2L, parse("2017-12-22T06:02:09.243857+00:00"), "forrest"),
+        record(schema, 2L, parse("2017-12-22T06:02:09.243857+00:00"), ""),
         record(schema, 3L, parse("2017-12-22T03:10:11.134509+00:00"), "clapping"),
         record(schema, 4L, parse("2017-12-22T00:34:00.184671+00:00"), "brush"),
         record(schema, 5L, parse("2017-12-21T22:20:08.935889+00:00"), "trap"),

File: hive3/src/main/java/org/apache/iceberg/mr/hive/serde/objectinspector/IcebergTimestampObjectInspectorHive3.java
Patch:
@@ -45,7 +45,7 @@ public LocalDateTime convert(Object o) {
     if (o == null) {
       return null;
     }
-    Timestamp timestamp = ((TimestampWritableV2) o).getTimestamp();
+    Timestamp timestamp = (Timestamp) o;
     return LocalDateTime.ofEpochSecond(timestamp.toEpochSecond(), timestamp.getNanos(), ZoneOffset.UTC);
   }
 

File: hive3/src/main/java/org/apache/iceberg/mr/hive/serde/objectinspector/IcebergTimestampWithZoneObjectInspectorHive3.java
Patch:
@@ -47,7 +47,7 @@ public OffsetDateTime convert(Object o) {
     if (o == null) {
       return null;
     }
-    ZonedDateTime zdt = ((TimestampLocalTZWritable) o).getTimestampTZ().getZonedDateTime();
+    ZonedDateTime zdt = ((TimestampTZ) o).getZonedDateTime();
     return OffsetDateTime.of(zdt.toLocalDateTime(), zdt.getOffset());
   }
 

File: hive3/src/test/java/org/apache/iceberg/mr/hive/serde/objectinspector/TestIcebergTimestampObjectInspectorHive3.java
Patch:
@@ -65,7 +65,7 @@ public void testIcebergTimestampObjectInspector() {
 
     Assert.assertFalse(oi.preferWritable());
 
-    Assert.assertEquals(local, oi.convert(new TimestampWritableV2(ts)));
+    Assert.assertEquals(local, oi.convert(ts));
   }
 
 }

File: hive3/src/test/java/org/apache/iceberg/mr/hive/serde/objectinspector/TestIcebergTimestampWithZoneObjectInspectorHive3.java
Patch:
@@ -70,7 +70,7 @@ public void testIcebergTimestampLocalTZObjectInspector() {
 
     Assert.assertFalse(oi.preferWritable());
 
-    Assert.assertEquals(OffsetDateTime.of(dateTimeAtUTC, ZoneOffset.UTC), oi.convert(new TimestampLocalTZWritable(ts)));
+    Assert.assertEquals(OffsetDateTime.of(dateTimeAtUTC, ZoneOffset.UTC), oi.convert(ts));
   }
 
 }

File: mr/src/main/java/org/apache/iceberg/mr/hive/serde/objectinspector/IcebergBinaryObjectInspector.java
Patch:
@@ -38,7 +38,7 @@ byte[] toByteArray(Object o) {
 
     @Override
     public byte[] convert(Object o) {
-      return o == null ? null : ((BytesWritable) o).getBytes();
+      return (byte[]) o;
     }
   };
 
@@ -50,7 +50,7 @@ byte[] toByteArray(Object o) {
 
     @Override
     public ByteBuffer convert(Object o) {
-      return o == null ? null : ByteBuffer.wrap(((BytesWritable) o).getBytes());
+      return o == null ? null : ByteBuffer.wrap((byte[]) o);
     }
   };
 

File: mr/src/main/java/org/apache/iceberg/mr/hive/serde/objectinspector/IcebergDateObjectInspector.java
Patch:
@@ -67,6 +67,6 @@ public Object copyObject(Object o) {
 
   @Override
   public LocalDate convert(Object o) {
-    return o == null ? null : ((DateWritable) o).get().toLocalDate();
+    return o == null ? null : ((Date) o).toLocalDate();
   }
 }

File: mr/src/main/java/org/apache/iceberg/mr/hive/serde/objectinspector/IcebergDecimalObjectInspector.java
Patch:
@@ -80,6 +80,6 @@ public Object copyObject(Object o) {
 
   @Override
   public BigDecimal convert(Object o) {
-    return o == null ? null : ((HiveDecimalWritable) o).getHiveDecimal().bigDecimalValue();
+    return o == null ? null : ((HiveDecimal) o).bigDecimalValue();
   }
 }

File: mr/src/main/java/org/apache/iceberg/mr/hive/serde/objectinspector/IcebergTimestampObjectInspector.java
Patch:
@@ -41,7 +41,7 @@ private IcebergTimestampObjectInspector() {
 
   @Override
   public LocalDateTime convert(Object o) {
-    return o == null ? null : ((TimestampWritable) o).getTimestamp().toLocalDateTime();
+    return o == null ? null : ((Timestamp) o).toLocalDateTime();
   }
 
   @Override

File: mr/src/main/java/org/apache/iceberg/mr/hive/serde/objectinspector/IcebergTimestampWithZoneObjectInspector.java
Patch:
@@ -42,8 +42,7 @@ private IcebergTimestampWithZoneObjectInspector() {
 
   @Override
   public OffsetDateTime convert(Object o) {
-    return o == null ? null :
-        OffsetDateTime.ofInstant(((TimestampWritable) o).getTimestamp().toInstant(), ZoneOffset.UTC);
+    return o == null ? null : OffsetDateTime.ofInstant(((Timestamp) o).toInstant(), ZoneOffset.UTC);
   }
 
   @Override

File: mr/src/test/java/org/apache/iceberg/mr/hive/serde/objectinspector/TestIcebergTimestampObjectInspector.java
Patch:
@@ -61,7 +61,7 @@ public void testIcebergTimestampObjectInspector() {
 
     Assert.assertFalse(oi.preferWritable());
 
-    Assert.assertEquals(local, oi.convert(new TimestampWritable(ts)));
+    Assert.assertEquals(local, oi.convert(ts));
   }
 
 }

File: mr/src/test/java/org/apache/iceberg/mr/hive/serde/objectinspector/TestIcebergTimestampWithZoneObjectInspector.java
Patch:
@@ -65,10 +65,10 @@ public void testIcebergTimestampObjectInspectorWithUTCAdjustment() {
     Assert.assertFalse(oi.preferWritable());
 
     Assert.assertEquals(OffsetDateTime.ofInstant(local.toInstant(ZoneOffset.ofHours(-5)), ZoneOffset.UTC),
-            oi.convert(new TimestampWritable(ts)));
+            oi.convert(ts));
 
     Assert.assertEquals(offsetDateTime.withOffsetSameInstant(ZoneOffset.UTC),
-            oi.convert(new TimestampWritable(Timestamp.from(offsetDateTime.toInstant()))));
+            oi.convert(Timestamp.from(offsetDateTime.toInstant())));
   }
 
 }

File: core/src/main/java/org/apache/iceberg/BaseUpdatePartitionSpec.java
Patch:
@@ -69,7 +69,8 @@ class BaseUpdatePartitionSpec implements UpdatePartitionSpec {
     this.schema = spec.schema();
     this.nameToField = indexSpecByName(spec);
     this.transformToField = indexSpecByTransform(spec);
-    this.lastAssignedPartitionId = spec.fields().stream().mapToInt(PartitionField::fieldId).max().orElse(999);
+    this.lastAssignedPartitionId =
+        base.specs().stream().mapToInt(PartitionSpec::lastAssignedFieldId).max().orElse(999);
 
     spec.fields().stream()
         .filter(field -> field.transform() instanceof UnknownTransform)

File: api/src/main/java/org/apache/iceberg/transforms/Dates.java
Patch:
@@ -94,7 +94,7 @@ public boolean satisfiesOrderOf(Transform<?, ?> other) {
   @Override
   public UnboundPredicate<Integer> project(String fieldName, BoundPredicate<Integer> pred) {
     if (pred.term() instanceof BoundTransform) {
-      return ProjectionUtil.projectTransformPredicate(this, name, pred);
+      return ProjectionUtil.projectTransformPredicate(this, fieldName, pred);
     }
 
     if (pred.isUnaryPredicate()) {
@@ -110,7 +110,7 @@ public UnboundPredicate<Integer> project(String fieldName, BoundPredicate<Intege
   @Override
   public UnboundPredicate<Integer> projectStrict(String fieldName, BoundPredicate<Integer> pred) {
     if (pred.term() instanceof BoundTransform) {
-      return ProjectionUtil.projectTransformPredicate(this, name, pred);
+      return ProjectionUtil.projectTransformPredicate(this, fieldName, pred);
     }
 
     if (pred.isUnaryPredicate()) {

File: api/src/main/java/org/apache/iceberg/transforms/Timestamps.java
Patch:
@@ -96,7 +96,7 @@ public boolean satisfiesOrderOf(Transform<?, ?> other) {
   @Override
   public UnboundPredicate<Integer> project(String fieldName, BoundPredicate<Long> pred) {
     if (pred.term() instanceof BoundTransform) {
-      return ProjectionUtil.projectTransformPredicate(this, name, pred);
+      return ProjectionUtil.projectTransformPredicate(this, fieldName, pred);
     }
 
     if (pred.isUnaryPredicate()) {
@@ -112,7 +112,7 @@ public UnboundPredicate<Integer> project(String fieldName, BoundPredicate<Long>
   @Override
   public UnboundPredicate<Integer> projectStrict(String fieldName, BoundPredicate<Long> pred) {
     if (pred.term() instanceof BoundTransform) {
-      return ProjectionUtil.projectTransformPredicate(this, name, pred);
+      return ProjectionUtil.projectTransformPredicate(this, fieldName, pred);
     }
 
     if (pred.isUnaryPredicate()) {

File: aws/src/main/java/org/apache/iceberg/aws/s3/S3OutputStream.java
Patch:
@@ -85,10 +85,10 @@ class S3OutputStream extends PositionOutputStream {
   private long pos = 0;
   private boolean closed = false;
 
-  @SuppressWarnings({"StaticAssignmentInConstructor", "StaticGuardedByInstance"})
+  @SuppressWarnings("StaticAssignmentInConstructor")
   S3OutputStream(S3Client s3, S3URI location, AwsProperties awsProperties) throws IOException {
     if (executorService == null) {
-      synchronized (this) {
+      synchronized (S3OutputStream.class) {
         if (executorService == null) {
           executorService = MoreExecutors.getExitingExecutorService(
               (ThreadPoolExecutor) Executors.newFixedThreadPool(

File: core/src/main/java/org/apache/iceberg/BaseTable.java
Patch:
@@ -119,6 +119,7 @@ public UpdateSchema updateSchema() {
     return new SchemaUpdate(ops);
   }
 
+  @Override
   public UpdatePartitionSpec updateSpec() {
     return new BaseUpdatePartitionSpec(ops);
   }

File: core/src/main/java/org/apache/iceberg/BaseTableScan.java
Patch:
@@ -45,7 +45,7 @@
  * Base class for {@link TableScan} implementations.
  */
 abstract class BaseTableScan implements TableScan {
-  private static final Logger LOG = LoggerFactory.getLogger(TableScan.class);
+  private static final Logger LOG = LoggerFactory.getLogger(BaseTableScan.class);
 
   private static final DateTimeFormatter DATE_FORMAT = DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss.SSS");
 

File: core/src/main/java/org/apache/iceberg/actions/BaseRewriteDataFilesAction.java
Patch:
@@ -67,7 +67,7 @@ public abstract class BaseRewriteDataFilesAction<ThisT>
   private int splitLookback;
   private long splitOpenFileCost;
 
-  public BaseRewriteDataFilesAction(Table table) {
+  protected BaseRewriteDataFilesAction(Table table) {
     this.table = table;
     this.spec = table.spec();
     this.filter = Expressions.alwaysTrue();

File: core/src/main/java/org/apache/iceberg/io/BaseTaskWriter.java
Patch:
@@ -100,7 +100,7 @@ protected abstract class BaseEqualityDeltaWriter implements Closeable {
     private SortedPosDeleteWriter<T> posDeleteWriter;
     private Map<StructLike, PathOffset> insertedRowMap;
 
-    public BaseEqualityDeltaWriter(PartitionKey partition, Schema schema, Schema deleteSchema) {
+    protected BaseEqualityDeltaWriter(PartitionKey partition, Schema schema, Schema deleteSchema) {
       Preconditions.checkNotNull(schema, "Iceberg table schema cannot be null.");
       Preconditions.checkNotNull(deleteSchema, "Equality-delete schema cannot be null.");
       this.structProjection = StructProjection.create(schema, deleteSchema);

File: core/src/main/java/org/apache/iceberg/io/PartitionedWriter.java
Patch:
@@ -37,7 +37,7 @@ public abstract class PartitionedWriter<T> extends BaseTaskWriter<T> {
   private PartitionKey currentKey = null;
   private RollingFileWriter currentWriter = null;
 
-  public PartitionedWriter(PartitionSpec spec, FileFormat format, FileAppenderFactory<T> appenderFactory,
+  protected PartitionedWriter(PartitionSpec spec, FileFormat format, FileAppenderFactory<T> appenderFactory,
                            OutputFileFactory fileFactory, FileIO io, long targetFileSize) {
     super(spec, format, appenderFactory, fileFactory, io, targetFileSize);
   }

File: spark3/src/main/java/org/apache/iceberg/spark/source/SparkMergeBuilder.java
Patch:
@@ -79,6 +79,7 @@ public ScanBuilder asScanBuilder() {
   private ScanBuilder scanBuilder() {
     if (lazyScanBuilder == null) {
       SparkScanBuilder scanBuilder = new SparkScanBuilder(spark, table, writeInfo.options()) {
+        @Override
         public Scan build() {
           Scan scan = super.buildMergeScan();
           SparkMergeBuilder.this.configuredScan = scan;

File: core/src/main/java/org/apache/iceberg/hadoop/HadoopTables.java
Patch:
@@ -339,9 +339,9 @@ private Transaction newReplaceTableTransaction(boolean orCreate) {
       Map<String, String> properties = propertiesBuilder.build();
       TableMetadata metadata;
       if (ops.current() != null) {
-        metadata = ops.current().buildReplacement(schema, spec, SortOrder.unsorted(), location, properties);
+        metadata = ops.current().buildReplacement(schema, spec, sortOrder, location, properties);
       } else {
-        metadata = tableMetadata(schema, spec, null, properties, location);
+        metadata = tableMetadata(schema, spec, sortOrder, properties, location);
       }
 
       if (orCreate) {

File: parquet/src/main/java/org/apache/iceberg/parquet/ParquetUtil.java
Patch:
@@ -277,7 +277,7 @@ public static boolean hasNonDictionaryPages(ColumnChunkMetaData meta) {
     }
 
     // without EncodingStats, fall back to testing the encoding list
-    Set<Encoding> encodings = new HashSet<Encoding>(meta.getEncodings());
+    Set<Encoding> encodings = new HashSet<>(meta.getEncodings());
     if (encodings.remove(Encoding.PLAIN_DICTIONARY)) {
       // if remove returned true, PLAIN_DICTIONARY was present, which means at
       // least one page was dictionary encoded and 1.0 encodings are used

File: flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java
Patch:
@@ -92,7 +92,7 @@ public void testLimitPushDown() {
     String sqlLimitExceed = String.format("SELECT * FROM %s LIMIT 3", TABLE_NAME);
     List<Object[]> resultExceed = sql(sqlLimitExceed);
     Assert.assertEquals("should have 2 record", 2, resultExceed.size());
-    List expectedList = Lists.newArrayList();
+    List<Object[]> expectedList = Lists.newArrayList();
     expectedList.add(new Object[] {1, "a"});
     expectedList.add(new Object[] {2, "b"});
     Assert.assertArrayEquals("Should produce the expected records", resultExceed.toArray(), expectedList.toArray());

File: mr/src/main/java/org/apache/iceberg/mr/hive/serde/objectinspector/IcebergTimestampWithZoneObjectInspector.java
Patch:
@@ -43,12 +43,12 @@ private IcebergTimestampWithZoneObjectInspector() {
   @Override
   public OffsetDateTime convert(Object o) {
     return o == null ? null :
-        OffsetDateTime.of(((TimestampWritable) o).getTimestamp().toLocalDateTime(), ZoneOffset.UTC);
+        OffsetDateTime.ofInstant(((TimestampWritable) o).getTimestamp().toInstant(), ZoneOffset.UTC);
   }
 
   @Override
   public Timestamp getPrimitiveJavaObject(Object o) {
-    return o == null ? null : Timestamp.valueOf(((OffsetDateTime) o).toLocalDateTime());
+    return o == null ? null : Timestamp.from(((OffsetDateTime) o).toInstant());
   }
 
   @Override
@@ -66,7 +66,7 @@ public Object copyObject(Object o) {
       return copy;
     } else if (o instanceof OffsetDateTime) {
       OffsetDateTime odt = (OffsetDateTime) o;
-      return OffsetDateTime.of(odt.toLocalDateTime(), odt.getOffset());
+      return OffsetDateTime.ofInstant(odt.toInstant(), odt.getOffset());
     } else {
       return o;
     }

File: arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedArrowReader.java
Patch:
@@ -384,6 +384,7 @@ public VectorHolder read(VectorHolder reuse, int numValsToRead) {
         nulls = new NullabilityHolder(numValsToRead);
       }
 
+      rowStart += numValsToRead;
       vec.setValueCount(numValsToRead);
       nulls.setNotNulls(0, numValsToRead);
 

File: aws/src/integration/java/org/apache/iceberg/aws/s3/S3MultipartUploadTest.java
Patch:
@@ -24,7 +24,7 @@
 import java.util.UUID;
 import java.util.function.Supplier;
 import java.util.stream.IntStream;
-import org.apache.iceberg.aws.AwsClientUtil;
+import org.apache.iceberg.aws.AwsClientFactories;
 import org.apache.iceberg.aws.AwsIntegTestUtil;
 import org.apache.iceberg.aws.AwsProperties;
 import org.apache.iceberg.io.PositionOutputStream;
@@ -51,7 +51,7 @@ public class S3MultipartUploadTest {
 
   @BeforeClass
   public static void beforeClass() {
-    s3 = AwsClientUtil.defaultS3Client();
+    s3 = AwsClientFactories.defaultFactory().s3();
     bucketName = AwsIntegTestUtil.testBucketName();
     prefix = UUID.randomUUID().toString();
     properties = new AwsProperties();

File: aws/src/main/java/org/apache/iceberg/aws/AwsProperties.java
Patch:
@@ -19,12 +19,13 @@
 
 package org.apache.iceberg.aws;
 
+import java.io.Serializable;
 import java.util.Map;
 import org.apache.iceberg.relocated.com.google.common.base.Preconditions;
 import org.apache.iceberg.util.PropertyUtil;
 import software.amazon.awssdk.services.s3.model.ObjectCannedACL;
 
-public class AwsProperties {
+public class AwsProperties implements Serializable {
 
   /**
    * Type of S3 Server side encryption used, default to {@link AwsProperties#S3FILEIO_SSE_TYPE_NONE}.

File: spark/src/main/java/org/apache/iceberg/spark/source/StructInternalRow.java
Patch:
@@ -278,7 +278,7 @@ private ArrayData collectionToArrayData(Type elementType, Collection<?> values)
             array[pos] = new StructInternalRow(elementType.asStructType(), tuple));
       case LIST:
         return fillArray(values, array -> (BiConsumer<Integer, Collection<?>>) (pos, list) ->
-            array[pos] = collectionToArrayData(elementType.asListType(), list));
+            array[pos] = collectionToArrayData(elementType.asListType().elementType(), list));
       case MAP:
         return fillArray(values, array -> (BiConsumer<Integer, Map<?, ?>>) (pos, map) ->
             array[pos] = mapToMapData(elementType.asMapType(), map));

File: spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestDelete.java
Patch:
@@ -327,7 +327,7 @@ public void testDeleteWithInAndNotInConditions() {
 
     sql("DELETE FROM %s WHERE id NOT IN (1, 10)", tableName);
     assertEquals("Should have expected rows",
-        ImmutableList.of(),
+        ImmutableList.of(row(null, "hr")),
         sql("SELECT * FROM %s ORDER BY id ASC NULLS LAST", tableName));
   }
 

File: aws/src/main/java/org/apache/iceberg/aws/glue/GlueTableOperations.java
Patch:
@@ -163,6 +163,7 @@ private void persistGlueTable(Table glueTable, Map<String, String> parameters) {
           .skipArchive(awsProperties.glueCatalogSkipArchive())
           .tableInput(TableInput.builder()
               .name(tableName)
+              .tableType(GLUE_EXTERNAL_TABLE_TYPE)
               .parameters(parameters)
               .build())
           .build());

File: spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/SparkExtensionsTestBase.java
Patch:
@@ -44,6 +44,7 @@ public static void startMetastoreAndSpark() {
 
     SparkTestBase.spark = SparkSession.builder()
         .master("local[2]")
+        .config("spark.testing", "true")
         .config(SQLConf.PARTITION_OVERWRITE_MODE().key(), "dynamic")
         .config("spark.sql.extensions", IcebergSparkSessionExtensions.class.getName())
         .config("spark.hadoop." + METASTOREURIS.varname, hiveConf.get(METASTOREURIS.varname))

File: spark3/src/main/java/org/apache/iceberg/spark/procedures/SparkProcedures.java
Patch:
@@ -46,6 +46,7 @@ private static Map<String, Supplier<ProcedureBuilder>> initProcedureBuilders() {
     mapBuilder.put("set_current_snapshot", SetCurrentSnapshotProcedure::builder);
     mapBuilder.put("cherrypick_snapshot", CherrypickSnapshotProcedure::builder);
     mapBuilder.put("rewrite_manifests", RewriteManifestsProcedure::builder);
+    mapBuilder.put("remove_orphan_files", RemoveOrphanFilesProcedure::builder);
     return mapBuilder.build();
   }
 

File: spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCherrypickSnapshotProcedure.java
Patch:
@@ -174,7 +174,7 @@ public void testInvalidCherrypickSnapshotCases() {
         () -> sql("CALL %s.system.cherrypick_snapshot('n', 't')", catalogName));
 
     AssertHelpers.assertThrows("Should reject calls with invalid arg types",
-        AnalysisException.class, "Wrong arg type for snapshot_id: expected LongType",
+        AnalysisException.class, "Wrong arg type for snapshot_id: cannot cast",
         () -> sql("CALL %s.system.cherrypick_snapshot('n', 't', 2.2)", catalogName));
 
     AssertHelpers.assertThrows("Should reject empty namespace",

File: spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToSnapshotProcedure.java
Patch:
@@ -258,7 +258,7 @@ public void testInvalidRollbackToSnapshotCases() {
         () -> sql("CALL %s.system.rollback_to_snapshot(table => 't', snapshot_id => 1L)", catalogName));
 
     AssertHelpers.assertThrows("Should reject calls with invalid arg types",
-        AnalysisException.class, "Wrong arg type for snapshot_id: expected LongType",
+        AnalysisException.class, "Wrong arg type for snapshot_id: cannot cast",
         () -> sql("CALL %s.system.rollback_to_snapshot('n', 't', 2.2)", catalogName));
 
     AssertHelpers.assertThrows("Should reject empty namespace",

File: spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToTimestampProcedure.java
Patch:
@@ -266,7 +266,7 @@ public void testInvalidRollbackToTimestampCases() {
         () -> sql("CALL %s.system.rollback_to_timestamp('n', 't', %s, 1L)", catalogName, timestamp));
 
     AssertHelpers.assertThrows("Should reject calls with invalid arg types",
-        AnalysisException.class, "Wrong arg type for timestamp: expected TimestampType",
+        AnalysisException.class, "Wrong arg type for timestamp: cannot cast",
         () -> sql("CALL %s.system.rollback_to_timestamp('n', 't', 2.2)", catalogName));
 
     AssertHelpers.assertThrows("Should reject empty namespace",

File: spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSetCurrentSnapshotProcedure.java
Patch:
@@ -211,7 +211,7 @@ public void testInvalidRollbackToSnapshotCases() {
         () -> sql("CALL %s.system.set_current_snapshot(table => 't', snapshot_id => 1L)", catalogName));
 
     AssertHelpers.assertThrows("Should reject calls with invalid arg types",
-        AnalysisException.class, "Wrong arg type for snapshot_id: expected LongType",
+        AnalysisException.class, "Wrong arg type for snapshot_id: cannot cast",
         () -> sql("CALL %s.system.set_current_snapshot('n', 't', 2.2)", catalogName));
 
     AssertHelpers.assertThrows("Should reject empty namespace",

File: core/src/main/java/org/apache/iceberg/util/StructLikeSet.java
Patch:
@@ -19,6 +19,7 @@
 
 package org.apache.iceberg.util;
 
+import java.util.AbstractSet;
 import java.util.Collection;
 import java.util.Iterator;
 import java.util.Objects;
@@ -29,7 +30,7 @@
 import org.apache.iceberg.relocated.com.google.common.collect.Sets;
 import org.apache.iceberg.types.Types;
 
-public class StructLikeSet implements Set<StructLike> {
+public class StructLikeSet extends AbstractSet<StructLike> implements Set<StructLike> {
   public static StructLikeSet create(Types.StructType type) {
     return new StructLikeSet(type);
   }
@@ -104,7 +105,7 @@ public boolean add(StructLike struct) {
 
   @Override
   public boolean remove(Object obj) {
-    if (obj instanceof CharSequence) {
+    if (obj instanceof StructLike) {
       StructLikeWrapper wrapper = wrappers.get();
       boolean result = wrapperSet.remove(wrapper.set((StructLike) obj));
       wrapper.set(null); // don't hold a reference to the value

File: aws/src/main/java/org/apache/iceberg/aws/s3/S3FileIO.java
Patch:
@@ -39,7 +39,6 @@
 public class S3FileIO implements FileIO {
   private final SerializableSupplier<S3Client> s3;
   private AwsProperties awsProperties;
-
   private transient S3Client client;
 
   public S3FileIO() {

File: bundled-guava/src/main/java/org/apache/iceberg/GuavaClasses.java
Patch:
@@ -46,6 +46,7 @@
 import com.google.common.hash.HashFunction;
 import com.google.common.hash.Hasher;
 import com.google.common.hash.Hashing;
+import com.google.common.io.CountingOutputStream;
 import com.google.common.io.Files;
 import com.google.common.primitives.Bytes;
 import com.google.common.util.concurrent.MoreExecutors;
@@ -90,6 +91,7 @@ public class GuavaClasses {
     MoreExecutors.class.getName();
     ThreadFactoryBuilder.class.getName();
     Iterables.class.getName();
+    CountingOutputStream.class.getName();
   }
 
 }

File: api/src/test/java/org/apache/iceberg/expressions/TestMiscLiteralConversions.java
Patch:
@@ -231,7 +231,6 @@ public void testInvalidDecimalConversions() {
         Types.TimeType.get(),
         Types.TimestampType.withZone(),
         Types.TimestampType.withoutZone(),
-        Types.DecimalType.of(9, 4),
         Types.StringType.get(),
         Types.UUIDType.get(),
         Types.FixedType.ofLength(1),

File: api/src/main/java/org/apache/iceberg/expressions/Expressions.java
Patch:
@@ -120,7 +120,7 @@ public static <T> UnboundPredicate<T> notNull(String name) {
   }
 
   public static <T> UnboundPredicate<T> notNull(UnboundTerm<T> expr) {
-    return new UnboundPredicate<>(Expression.Operation.IS_NULL, expr);
+    return new UnboundPredicate<>(Expression.Operation.NOT_NULL, expr);
   }
 
   public static <T> UnboundPredicate<T> lessThan(String name, T value) {

File: core/src/main/java/org/apache/iceberg/ManifestReader.java
Patch:
@@ -271,6 +271,7 @@ private InclusiveMetricsEvaluator metricsEvaluator() {
   private static boolean requireStatsProjection(Expression rowFilter, Collection<String> columns) {
     // Make sure we have all stats columns for metrics evaluator
     return rowFilter != Expressions.alwaysTrue() &&
+        columns != null &&
         !columns.containsAll(ManifestReader.ALL_COLUMNS) &&
         !columns.containsAll(STATS_COLUMNS);
   }
@@ -279,6 +280,7 @@ static boolean dropStats(Expression rowFilter, Collection<String> columns) {
     // Make sure we only drop all stats if we had projected all stats
     // We do not drop stats even if we had partially added some stats columns
     return rowFilter != Expressions.alwaysTrue() &&
+        columns != null &&
         !columns.containsAll(ManifestReader.ALL_COLUMNS) &&
         Sets.intersection(Sets.newHashSet(columns), STATS_COLUMNS).isEmpty();
   }

File: core/src/main/java/org/apache/iceberg/SnapshotManager.java
Patch:
@@ -235,7 +235,7 @@ public Snapshot apply() {
   }
 
   private static void validateCurrentSnapshot(TableMetadata meta, Long requiredSnapshotId) {
-    if (requiredSnapshotId != null) {
+    if (requiredSnapshotId != null && meta.currentSnapshot() != null) {
       ValidationException.check(meta.currentSnapshot().snapshotId() == requiredSnapshotId,
           "Cannot fast-forward to non-append snapshot; current has changed: current=%s != required=%s",
           meta.currentSnapshot().snapshotId(), requiredSnapshotId);

File: core/src/main/java/org/apache/iceberg/SnapshotSummary.java
Patch:
@@ -83,7 +83,7 @@ public void clear() {
      * Sets the maximum number of changed partitions before partition summaries will be excluded.
      * <p>
      * If the number of changed partitions is over this max, summaries will not be included. If the number of changed
-     * partitions is <= this limit, then partition-level summaries will be included in the summary if they are
+     * partitions is &lt;= this limit, then partition-level summaries will be included in the summary if they are
      * available, and "partition-summaries-included" will be set to "true".
      *
      * @param max maximum number of changed partitions

File: core/src/main/java/org/apache/iceberg/AllEntriesTable.java
Patch:
@@ -115,7 +115,7 @@ protected CloseableIterable<FileScanTask> planFiles(
       ResidualEvaluator residuals = ResidualEvaluator.unpartitioned(filter);
 
       return CloseableIterable.transform(manifests, manifest -> new ManifestEntriesTable.ManifestReadTask(
-          ops.io(), manifest, fileSchema, schemaString, specString, residuals));
+          ops.io(), manifest, fileSchema, schemaString, specString, residuals, ops.current().specsById()));
     }
   }
 

File: core/src/main/java/org/apache/iceberg/SnapshotManager.java
Patch:
@@ -255,7 +255,7 @@ private static void validateNonAncestor(TableMetadata meta, long snapshotId) {
 
   private static void validateReplacedPartitions(TableMetadata meta, Long parentId,
                                                  PartitionSet replacedPartitions) {
-    if (replacedPartitions != null) {
+    if (replacedPartitions != null && meta.currentSnapshot() != null) {
       ValidationException.check(parentId == null || isCurrentAncestor(meta, parentId),
           "Cannot cherry-pick overwrite, based on non-ancestor of the current state: %s", parentId);
       List<DataFile> newFiles = SnapshotUtil.newFiles(parentId, meta.currentSnapshot().snapshotId(), meta::snapshot);

File: core/src/main/java/org/apache/iceberg/ManifestsTable.java
Patch:
@@ -114,7 +114,7 @@ static List<StaticDataTask.Row> partitionSummariesToRows(PartitionSpec spec,
 
     List<StaticDataTask.Row> rows = Lists.newArrayList();
 
-    for (int i = 0; i < spec.fields().size(); i += 1) {
+    for (int i = 0; i < summaries.size(); i += 1) {
       ManifestFile.PartitionFieldSummary summary = summaries.get(i);
       rows.add(StaticDataTask.Row.of(
           summary.containsNull(),

File: parquet/src/main/java/org/apache/iceberg/parquet/VectorizedReader.java
Patch:
@@ -41,7 +41,7 @@ public interface VectorizedReader<T> {
   void setBatchSize(int batchSize);
 
   /**
-   * Sets the row group information toe be used with this reader
+   * Sets the row group information to be used with this reader
    *
    * @param pages    row group information for all the columns
    * @param metadata map of {@link ColumnPath} -&gt; {@link ColumnChunkMetaData} for the row group

File: core/src/main/java/org/apache/iceberg/BaseOverwriteFiles.java
Patch:
@@ -92,6 +92,7 @@ public OverwriteFiles caseSensitive(boolean isCaseSensitive) {
     return this;
   }
 
+  @Override
   public OverwriteFiles validateNoConflictingAppends(Expression newConflictDetectionFilter) {
     Preconditions.checkArgument(newConflictDetectionFilter != null, "Conflict detection filter cannot be null");
     this.conflictDetectionFilter = newConflictDetectionFilter;

File: core/src/main/java/org/apache/iceberg/types/FixupTypes.java
Patch:
@@ -33,7 +33,7 @@ public abstract class FixupTypes extends TypeUtil.CustomOrderSchemaVisitor<Type>
   private final Schema referenceSchema;
   private Type sourceType;
 
-  public FixupTypes(Schema referenceSchema) {
+  protected FixupTypes(Schema referenceSchema) {
     this.referenceSchema = referenceSchema;
     this.sourceType = referenceSchema.asStruct();
   }

File: data/src/main/java/org/apache/iceberg/data/DeleteFilter.java
Patch:
@@ -65,7 +65,7 @@ public abstract class DeleteFilter<T> {
   private final Schema requiredSchema;
   private final Accessor<StructLike> posAccessor;
 
-  public DeleteFilter(FileScanTask task, Schema tableSchema, Schema requestedSchema) {
+  protected DeleteFilter(FileScanTask task, Schema tableSchema, Schema requestedSchema) {
     this.setFilterThreshold = DEFAULT_SET_FILTER_THRESHOLD;
     this.dataFile = task.file();
 

File: spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueReaders.java
Patch:
@@ -181,7 +181,7 @@ private TimestampTzReader() {
     @Override
     public Long nonNullRead(ColumnVector vector, int row) {
       TimestampColumnVector tcv = (TimestampColumnVector) vector;
-      return (Math.floorDiv(tcv.time[row], 1_000)) * 1_000_000 + Math.floorDiv(tcv.nanos[row], 1000);
+      return Math.floorDiv(tcv.time[row], 1_000) * 1_000_000 + Math.floorDiv(tcv.nanos[row], 1000);
     }
   }
 

File: spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueWriters.java
Patch:
@@ -185,7 +185,7 @@ public void nonNullWrite(int rowId, int column, SpecializedGetters data, ColumnV
       TimestampColumnVector cv = (TimestampColumnVector) output;
       long micros = data.getLong(column); // it could be negative.
       cv.time[rowId] = Math.floorDiv(micros, 1_000); // millis
-      cv.nanos[rowId] = (int) (Math.floorMod(micros, 1_000_000)) * 1_000; // nanos
+      cv.nanos[rowId] = (int) Math.floorMod(micros, 1_000_000) * 1_000; // nanos
     }
   }
 

File: hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveMetastore.java
Patch:
@@ -166,8 +166,8 @@ private TServer newThriftServer(TServerSocket socket, HiveConf conf) throws Exce
         .processor(new TSetIpAddressProcessor<>(handler))
         .transportFactory(new TTransportFactory())
         .protocolFactory(new TBinaryProtocol.Factory())
-        .minWorkerThreads(3)
-        .maxWorkerThreads(5);
+        .minWorkerThreads(5)
+        .maxWorkerThreads(15);
 
     return new TThreadPoolServer(args);
   }

File: spark2/src/jmh/java/org/apache/iceberg/spark/data/parquet/SparkParquetReadersFlatDataBenchmark.java
Patch:
@@ -61,7 +61,7 @@
  *
  * To run this benchmark:
  * <code>
- *   ./gradlew :iceberg-spark:jmh
+ *   ./gradlew :iceberg-spark2:jmh
  *       -PjmhIncludeRegex=SparkParquetReadersFlatDataBenchmark
  *       -PjmhOutputPath=benchmark/spark-parquet-readers-flat-data-benchmark-result.txt
  * </code>
@@ -95,6 +95,7 @@ public class SparkParquetReadersFlatDataBenchmark {
   @Setup
   public void setupBenchmark() throws IOException {
     dataFile = File.createTempFile("parquet-flat-data-benchmark", ".parquet");
+    dataFile.delete();
     List<GenericData.Record> records = RandomData.generateList(SCHEMA, NUM_RECORDS, 0L);
     try (FileAppender<GenericData.Record> writer = Parquet.write(Files.localOutput(dataFile))
         .schema(SCHEMA)

File: spark2/src/jmh/java/org/apache/iceberg/spark/data/parquet/SparkParquetReadersNestedDataBenchmark.java
Patch:
@@ -61,7 +61,7 @@
  *
  * To run this benchmark:
  * <code>
- *   ./gradlew :iceberg-spark:jmh
+ *   ./gradlew :iceberg-spark2:jmh
  *       -PjmhIncludeRegex=SparkParquetReadersNestedDataBenchmark
  *       -PjmhOutputPath=benchmark/spark-parquet-readers-nested-data-benchmark-result.txt
  * </code>
@@ -95,6 +95,7 @@ public class SparkParquetReadersNestedDataBenchmark {
   @Setup
   public void setupBenchmark() throws IOException {
     dataFile = File.createTempFile("parquet-nested-data-benchmark", ".parquet");
+    dataFile.delete();
     List<GenericData.Record> records = RandomData.generateList(SCHEMA, NUM_RECORDS, 0L);
     try (FileAppender<GenericData.Record> writer = Parquet.write(Files.localOutput(dataFile))
         .schema(SCHEMA)

File: spark2/src/jmh/java/org/apache/iceberg/spark/data/parquet/SparkParquetWritersFlatDataBenchmark.java
Patch:
@@ -53,7 +53,7 @@
  *
  * To run this benchmark:
  * <code>
- *   ./gradlew :iceberg-spark:jmh
+ *   ./gradlew :iceberg-spark2:jmh
  *       -PjmhIncludeRegex=SparkParquetWritersFlatDataBenchmark
  *       -PjmhOutputPath=benchmark/spark-parquet-writers-flat-data-benchmark-result.txt
  * </code>
@@ -82,6 +82,7 @@ public class SparkParquetWritersFlatDataBenchmark {
   public void setupBenchmark() throws IOException {
     rows = RandomData.generateSpark(SCHEMA, NUM_RECORDS, 0L);
     dataFile = File.createTempFile("parquet-flat-data-benchmark", ".parquet");
+    dataFile.delete();
   }
 
   @TearDown

File: spark2/src/jmh/java/org/apache/iceberg/spark/data/parquet/SparkParquetWritersNestedDataBenchmark.java
Patch:
@@ -53,7 +53,7 @@
  *
  * To run this benchmark:
  * <code>
- *   ./gradlew :iceberg-spark:jmh
+ *   ./gradlew :iceberg-spark2:jmh
  *       -PjmhIncludeRegex=SparkParquetWritersNestedDataBenchmark
  *       -PjmhOutputPath=benchmark/spark-parquet-writers-nested-data-benchmark-result.txt
  * </code>
@@ -81,6 +81,7 @@ public class SparkParquetWritersNestedDataBenchmark {
   public void setupBenchmark() throws IOException {
     rows = RandomData.generateSpark(SCHEMA, NUM_RECORDS, 0L);
     dataFile = File.createTempFile("parquet-nested-data-benchmark", ".parquet");
+    dataFile.delete();
   }
 
   @TearDown

File: spark2/src/jmh/java/org/apache/iceberg/spark/source/avro/IcebergSourceFlatAvroDataReadBenchmark.java
Patch:
@@ -43,7 +43,7 @@
  *
  * To run this benchmark:
  * <code>
- *   ./gradlew :iceberg-spark:jmh
+ *   ./gradlew :iceberg-spark2:jmh
  *       -PjmhIncludeRegex=IcebergSourceFlatAvroDataReadBenchmark
  *       -PjmhOutputPath=benchmark/iceberg-source-flat-avro-data-read-benchmark-result.txt
  * </code>

File: spark2/src/jmh/java/org/apache/iceberg/spark/source/avro/IcebergSourceNestedAvroDataReadBenchmark.java
Patch:
@@ -44,7 +44,7 @@
  *
  * To run this benchmark:
  * <code>
- *   ./gradlew :iceberg-spark:jmh
+ *   ./gradlew :iceberg-spark2:jmh
  *       -PjmhIncludeRegex=IcebergSourceNestedAvroDataReadBenchmark
  *       -PjmhOutputPath=benchmark/iceberg-source-nested-avro-data-read-benchmark-result.txt
  * </code>

File: spark2/src/jmh/java/org/apache/iceberg/spark/source/orc/IcebergSourceFlatORCDataReadBenchmark.java
Patch:
@@ -42,7 +42,7 @@
  *
  * To run this benchmark:
  * <code>
- *   ./gradlew :iceberg-spark:jmh
+ *   ./gradlew :iceberg-spark2:jmh
  *       -PjmhIncludeRegex=IcebergSourceFlatORCDataReadBenchmark
  *       -PjmhOutputPath=benchmark/iceberg-source-flat-orc-data-read-benchmark-result.txt
  * </code>

File: spark2/src/jmh/java/org/apache/iceberg/spark/source/orc/IcebergSourceNestedORCDataReadBenchmark.java
Patch:
@@ -44,7 +44,7 @@
  *
  * To run this benchmark:
  * <code>
- *   ./gradlew :iceberg-spark:jmh
+ *   ./gradlew :iceberg-spark2:jmh
  *       -PjmhIncludeRegex=IcebergSourceNestedORCDataReadBenchmark
  *       -PjmhOutputPath=benchmark/iceberg-source-nested-orc-data-read-benchmark-result.txt
  * </code>

File: spark2/src/jmh/java/org/apache/iceberg/spark/source/parquet/IcebergSourceFlatParquetDataFilterBenchmark.java
Patch:
@@ -46,7 +46,7 @@
  *
  * To run this benchmark:
  * <code>
- *   ./gradlew :iceberg-spark:jmh
+ *   ./gradlew :iceberg-spark2:jmh
  *       -PjmhIncludeRegex=IcebergSourceFlatParquetDataFilterBenchmark
  *       -PjmhOutputPath=benchmark/iceberg-source-flat-parquet-data-filter-benchmark-result.txt
  * </code>

File: spark2/src/jmh/java/org/apache/iceberg/spark/source/parquet/IcebergSourceFlatParquetDataReadBenchmark.java
Patch:
@@ -42,7 +42,7 @@
  *
  * To run this benchmark:
  * <code>
- *   ./gradlew :iceberg-spark:jmh
+ *   ./gradlew :iceberg-spark2:jmh
  *       -PjmhIncludeRegex=IcebergSourceFlatParquetDataReadBenchmark
  *       -PjmhOutputPath=benchmark/iceberg-source-flat-parquet-data-read-benchmark-result.txt
  * </code>

File: spark2/src/jmh/java/org/apache/iceberg/spark/source/parquet/IcebergSourceFlatParquetDataWriteBenchmark.java
Patch:
@@ -40,7 +40,7 @@
  *
  * To run this benchmark:
  * <code>
- *   ./gradlew :iceberg-spark:jmh
+ *   ./gradlew :iceberg-spark2:jmh
  *       -PjmhIncludeRegex=IcebergSourceFlatParquetDataWriteBenchmark
  *       -PjmhOutputPath=benchmark/iceberg-source-flat-parquet-data-write-benchmark-result.txt
  * </code>

File: spark2/src/jmh/java/org/apache/iceberg/spark/source/parquet/IcebergSourceNestedParquetDataFilterBenchmark.java
Patch:
@@ -46,7 +46,7 @@
  *
  * To run this benchmark:
  * <code>
- *   ./gradlew :iceberg-spark:jmh
+ *   ./gradlew :iceberg-spark2:jmh
  *       -PjmhIncludeRegex=IcebergSourceNestedParquetDataFilterBenchmark
  *       -PjmhOutputPath=benchmark/iceberg-source-nested-parquet-data-filter-benchmark-result.txt
  * </code>

File: spark2/src/jmh/java/org/apache/iceberg/spark/source/parquet/IcebergSourceNestedParquetDataReadBenchmark.java
Patch:
@@ -42,7 +42,7 @@
  *
  * To run this benchmark:
  * <code>
- *   ./gradlew :iceberg-spark:jmh
+ *   ./gradlew :iceberg-spark2:jmh
  *       -PjmhIncludeRegex=IcebergSourceNestedParquetDataReadBenchmark
  *       -PjmhOutputPath=benchmark/iceberg-source-nested-parquet-data-read-benchmark-result.txt
  * </code>

File: spark2/src/jmh/java/org/apache/iceberg/spark/source/parquet/IcebergSourceNestedParquetDataWriteBenchmark.java
Patch:
@@ -41,7 +41,7 @@
  *
  * To run this benchmark:
  * <code>
- *   ./gradlew :iceberg-spark:jmh
+ *   ./gradlew :iceberg-spark2:jmh
  *       -PjmhIncludeRegex=IcebergSourceNestedParquetDataWriteBenchmark
  *       -PjmhOutputPath=benchmark/iceberg-source-nested-parquet-data-write-benchmark-result.txt
  * </code>

File: spark2/src/jmh/java/org/apache/iceberg/spark/source/parquet/vectorized/VectorizedReadDictionaryEncodedFlatParquetDataBenchmark.java
Patch:
@@ -43,7 +43,7 @@
  * <p>
  * To run the benchmark:
  * <code>
- * ./gradlew :iceberg-spark:jmh -PjmhIncludeRegex=VectorizedReadDictionaryEncodedFlatParquetDataBenchmark
+ * ./gradlew :iceberg-spark2:jmh -PjmhIncludeRegex=VectorizedReadDictionaryEncodedFlatParquetDataBenchmark
  * -PjmhOutputPath=benchmark/results.txt
  * </code>
  */

File: spark2/src/jmh/java/org/apache/iceberg/spark/source/parquet/vectorized/VectorizedReadFlatParquetDataBenchmark.java
Patch:
@@ -53,7 +53,7 @@
  * <p>
  * To run the benchmark:
  * <code>
- * ./gradlew :iceberg-spark:jmh -PjmhIncludeRegex=VectorizedReadFlatParquetDataBenchmark
+ * ./gradlew :iceberg-spark2:jmh -PjmhIncludeRegex=VectorizedReadFlatParquetDataBenchmark
  * -PjmhOutputPath=benchmark/results.txt
  * </code>
  */

File: mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergFilterFactory.java
Patch:
@@ -22,6 +22,7 @@
 import java.math.BigDecimal;
 import java.sql.Date;
 import java.sql.Timestamp;
+import java.time.Instant;
 import java.util.List;
 import java.util.stream.Collectors;
 import org.apache.hadoop.hive.ql.io.sarg.ExpressionTree;
@@ -169,7 +170,7 @@ private static BigDecimal hiveDecimalToBigDecimal(HiveDecimalWritable hiveDecima
   }
 
   private static int daysFromDate(Date date) {
-    return DateTimeUtil.daysFromDate(date.toLocalDate());
+    return DateTimeUtil.daysFromInstant(Instant.ofEpochMilli(date.getTime()));
   }
 
   private static int daysFromTimestamp(Timestamp timestamp) {

File: flink/src/main/java/org/apache/iceberg/flink/data/FlinkOrcWriters.java
Patch:
@@ -132,7 +132,7 @@ public Class<?> getJavaClass() {
     public void nonNullWrite(int rowId, Integer millis, ColumnVector output) {
       // The time in flink is in millisecond, while the standard time in iceberg is microsecond.
       // So we need to transform it to microsecond.
-      ((LongColumnVector) output).vector[rowId] = millis * 1000;
+      ((LongColumnVector) output).vector[rowId] = millis * 1000L;
     }
   }
 

File: mr/src/test/java/org/apache/iceberg/mr/TestCatalogs.java
Patch:
@@ -107,7 +107,7 @@ public void testCreateDropTableToLocation() throws IOException {
         "location not set", () -> Catalogs.createTable(conf, missingLocation));
 
     Properties properties = new Properties();
-    properties.put("location", temp.toString() + "/hadoop_tables");
+    properties.put("location", temp.getRoot() + "/hadoop_tables");
     properties.put(InputFormatConfig.TABLE_SCHEMA, SchemaParser.toJson(SCHEMA));
     properties.put(InputFormatConfig.PARTITION_SPEC, PartitionSpecParser.toJson(SPEC));
     properties.put("dummy", "test");
@@ -127,7 +127,7 @@ public void testCreateDropTableToLocation() throws IOException {
         "location not set", () -> Catalogs.dropTable(conf, new Properties()));
 
     Properties dropProperties = new Properties();
-    dropProperties.put("location", temp.toString() + "/hadoop_tables");
+    dropProperties.put("location", temp.getRoot() + "/hadoop_tables");
     Catalogs.dropTable(conf, dropProperties);
 
     AssertHelpers.assertThrows(

File: flink/src/test/java/org/apache/iceberg/flink/TestFlinkSchemaUtil.java
Patch:
@@ -252,7 +252,7 @@ public void testInconsistentTypes() {
         Types.BinaryType.get(), new VarBinaryType(VarBinaryType.MAX_LENGTH),
         new VarBinaryType(100), Types.BinaryType.get());
     checkInconsistentType(
-        Types.TimeType.get(), new TimeType(6),
+        Types.TimeType.get(), new TimeType(),
         new TimeType(3), Types.TimeType.get());
     checkInconsistentType(
         Types.TimestampType.withoutZone(), new TimestampType(6),

File: core/src/test/java/org/apache/iceberg/TestReplaceTransaction.java
Patch:
@@ -150,7 +150,7 @@ public void testReplaceWithIncompatibleSchemaUpdate() {
     Assert.assertEquals("Version should be 2", 2L, (long) version());
     Assert.assertNull("Table should not have a current snapshot", table.currentSnapshot());
     Assert.assertEquals("Schema should use new schema, not compatible with previous",
-        new Schema(required(1, "obj_id", Types.IntegerType.get())).asStruct(),
+        new Schema(required(3, "obj_id", Types.IntegerType.get())).asStruct(),
         table.schema().asStruct());
   }
 

File: api/src/main/java/org/apache/iceberg/DataTask.java
Patch:
@@ -36,7 +36,7 @@ default DataTask asDataTask() {
   }
 
   /**
-   * @return an iterable of {@link StructLike} rows
+   * Returns an iterable of {@link StructLike} rows.
    */
   CloseableIterable<StructLike> rows();
 }

File: api/src/main/java/org/apache/iceberg/ExpireSnapshots.java
Patch:
@@ -60,11 +60,11 @@ public interface ExpireSnapshots extends PendingUpdate<List<Snapshot>> {
    * Retains the most recent ancestors of the current snapshot.
    * <p>
    * If a snapshot would be expired because it is older than the expiration timestamp, but is one of
-   * the {@code numSnapshot} most recent ancestors of the current state, it will be retained. This
+   * the {@code numSnapshots} most recent ancestors of the current state, it will be retained. This
    * will not cause snapshots explicitly identified by id from expiring.
    * <p>
-   * This may keep more than {@code numSnapshot} ancestors if snapshots are added concurrently. This
-   * may keep less than {@code numSnapshot} ancestors if the current table state does not have that many.
+   * This may keep more than {@code numSnapshots} ancestors if snapshots are added concurrently. This
+   * may keep less than {@code numSnapshots} ancestors if the current table state does not have that many.
    *
    * @param numSnapshots the number of snapshots to retain
    * @return this for method chaining

File: api/src/main/java/org/apache/iceberg/Files.java
Patch:
@@ -60,9 +60,8 @@ public PositionOutputStream create() {
 
       if (!file.getParentFile().isDirectory() && !file.getParentFile().mkdirs()) {
         throw new RuntimeIOException(
-            String.format(
                 "Failed to create the file's directory at %s.",
-                file.getParentFile().getAbsolutePath()));
+                file.getParentFile().getAbsolutePath());
       }
 
       try {
@@ -76,7 +75,7 @@ public PositionOutputStream create() {
     public PositionOutputStream createOrOverwrite() {
       if (file.exists()) {
         if (!file.delete()) {
-          throw new RuntimeIOException("Failed to delete: " + file);
+          throw new RuntimeIOException("Failed to delete: %s", file);
         }
       }
       return create();

File: api/src/main/java/org/apache/iceberg/HistoryEntry.java
Patch:
@@ -29,12 +29,12 @@
  */
 public interface HistoryEntry extends Serializable {
   /**
-   * @return the timestamp in milliseconds of the change
+   * Returns the timestamp in milliseconds of the change.
    */
   long timestampMillis();
 
   /**
-   * @return ID of the new current snapshot
+   * Returns ID of the new current snapshot.
    */
   long snapshotId();
 }

File: api/src/main/java/org/apache/iceberg/Schema.java
Patch:
@@ -127,7 +127,7 @@ public StructType asStruct() {
   }
 
   /**
-   * @return a List of the {@link NestedField columns} in this Schema.
+   * Returns a List of the {@link NestedField columns} in this Schema.
    */
   public List<NestedField> columns() {
     return struct.fields();

File: api/src/main/java/org/apache/iceberg/SortField.java
Patch:
@@ -53,21 +53,21 @@ public <S, T> Transform<S, T> transform() {
   }
 
   /**
-   * @return the field id of the source field in the {@link SortOrder sort order's} table schema
+   * Returns the field id of the source field in the {@link SortOrder sort order's} table schema
    */
   public int sourceId() {
     return sourceId;
   }
 
   /**
-   * @return the sort direction
+   * Returns the sort direction
    */
   public SortDirection direction() {
     return direction;
   }
 
   /**
-   * @return the null order
+   * Returns the null order
    */
   public NullOrder nullOrder() {
     return nullOrder;

File: api/src/main/java/org/apache/iceberg/catalog/TableIdentifier.java
Patch:
@@ -64,14 +64,14 @@ public boolean hasNamespace() {
   }
 
   /**
-   * @return the identifier namespace
+   * Returns the identifier namespace.
    */
   public Namespace namespace() {
     return namespace;
   }
 
   /**
-   * @return the identifier name
+   * Returns the identifier name.
    */
   public String name() {
     return name;

File: api/src/main/java/org/apache/iceberg/exceptions/RuntimeIOException.java
Patch:
@@ -19,6 +19,7 @@
 
 package org.apache.iceberg.exceptions;
 
+import com.google.errorprone.annotations.FormatMethod;
 import java.io.IOException;
 import java.io.UncheckedIOException;
 
@@ -34,10 +35,12 @@ public RuntimeIOException(IOException cause) {
     super(cause);
   }
 
+  @FormatMethod
   public RuntimeIOException(IOException cause, String message, Object... args) {
     super(String.format(message, args), cause);
   }
 
+  @FormatMethod
   public RuntimeIOException(String message, Object...args) {
     super(new IOException(String.format(message, args)));
   }

File: api/src/main/java/org/apache/iceberg/expressions/Bound.java
Patch:
@@ -28,7 +28,7 @@
  */
 public interface Bound<T> {
   /**
-   * @return the underlying reference
+   * Returns the underlying reference.
    */
   BoundReference<?> ref();
 

File: api/src/main/java/org/apache/iceberg/expressions/BoundTerm.java
Patch:
@@ -30,12 +30,12 @@
  */
 public interface BoundTerm<T> extends Bound<T>, Term {
   /**
-   * @return the type produced by this expression
+   * Returns the type produced by this expression.
    */
   Type type();
 
   /**
-   * @return a {@link Comparator} for values produced by this term
+   * Returns a {@link Comparator} for values produced by this term.
    */
   default Comparator<T> comparator() {
     return Comparators.forType(type().asPrimitiveType());

File: api/src/main/java/org/apache/iceberg/expressions/Literal.java
Patch:
@@ -72,7 +72,7 @@ static Literal<BigDecimal> of(BigDecimal value) {
   }
 
   /**
-   * @return the value wrapped by this literal
+   * Returns the value wrapped by this literal.
    */
   T value();
 

File: api/src/main/java/org/apache/iceberg/expressions/Unbound.java
Patch:
@@ -37,7 +37,7 @@ public interface Unbound<T, B> {
   B bind(Types.StructType struct, boolean caseSensitive);
 
   /**
-   * @return this expression's underlying reference
+   * Returns this expression's underlying reference.
    */
   NamedReference<?> ref();
 }

File: api/src/main/java/org/apache/iceberg/expressions/UnboundPredicate.java
Patch:
@@ -136,9 +136,8 @@ private Expression bindLiteralOperation(BoundTerm<T> boundTerm) {
     Literal<T> lit = literal().to(boundTerm.type());
 
     if (lit == null) {
-      throw new ValidationException(String.format(
-          "Invalid value for conversion to type %s: %s (%s)",
-          boundTerm.type(), literal().value(), literal().value().getClass().getName()));
+      throw new ValidationException("Invalid value for conversion to type %s: %s (%s)",
+          boundTerm.type(), literal().value(), literal().value().getClass().getName());
 
     } else if (lit == Literals.aboveMax()) {
       switch (op()) {

File: api/src/main/java/org/apache/iceberg/io/InputFile.java
Patch:
@@ -30,6 +30,8 @@
  */
 public interface InputFile {
   /**
+   * Returns the total length of the file, in bytes
+   *
    * @return the total length of the file, in bytes
    * @throws RuntimeIOException If the implementation throws an {@link IOException}
    */

File: api/src/main/java/org/apache/iceberg/types/Conversions.java
Patch:
@@ -110,7 +110,7 @@ public static ByteBuffer toByteBuffer(Type.TypeID typeId, Object value) {
         try {
           return ENCODER.get().encode(buffer);
         } catch (CharacterCodingException e) {
-          throw new RuntimeIOException(e, "Failed to encode value as UTF-8: " + value);
+          throw new RuntimeIOException(e, "Failed to encode value as UTF-8: %s", value);
         }
       case UUID:
         return UUIDUtil.convertToByteBuffer((UUID) value);
@@ -166,7 +166,7 @@ private static Object internalFromByteBuffer(Type type, ByteBuffer buffer) {
         try {
           return DECODER.get().decode(tmp);
         } catch (CharacterCodingException e) {
-          throw new RuntimeIOException(e, "Failed to decode value as UTF-8: " + buffer);
+          throw new RuntimeIOException(e, "Failed to decode value as UTF-8: %s", buffer);
         }
       case UUID:
         return UUIDUtil.convert(tmp);

File: arrow/src/main/java/org/apache/iceberg/arrow/vectorized/NullabilityHolder.java
Patch:
@@ -60,7 +60,7 @@ public void setNotNulls(int startIndex, int num) {
   }
 
   /**
-   * @return 1 if null, 0 otherwise
+   * Returns 1 if null, 0 otherwise.
    */
   public byte isNullAt(int index) {
     return isNull[index];

File: common/src/main/java/org/apache/iceberg/common/DynFields.java
Patch:
@@ -110,14 +110,14 @@ public StaticField<T> asStatic() {
     }
 
     /**
-     * @return whether the field is a static field
+     * Returns whether the field is a static field.
      */
     public boolean isStatic() {
       return Modifier.isStatic(field.getModifiers());
     }
 
     /**
-     * @return whether the field is always null
+     * Returns whether the field is always null.
      */
     public boolean isAlwaysNull() {
       return this == AlwaysNull.INSTANCE;

File: common/src/main/java/org/apache/iceberg/common/DynMethods.java
Patch:
@@ -102,14 +102,14 @@ public BoundMethod bind(Object receiver) {
     }
 
     /**
-     * @return whether the method is a static method
+     * Returns whether the method is a static method.
      */
     public boolean isStatic() {
       return Modifier.isStatic(method.getModifiers());
     }
 
     /**
-     * @return whether the method is a noop
+     * Returns whether the method is a noop.
      */
     public boolean isNoop() {
       return this == NOOP;

File: core/src/main/java/org/apache/iceberg/BaseRewriteManifests.java
Patch:
@@ -208,7 +208,7 @@ private void keepActiveManifests(List<ManifestFile> currentManifests) {
     keptManifests.clear();
     currentManifests.stream()
         .filter(manifest -> !rewrittenManifests.contains(manifest) && !deletedManifests.contains(manifest))
-        .forEach(manifest -> keptManifests.add(manifest));
+        .forEach(keptManifests::add);
   }
 
   private void reset() {
@@ -247,7 +247,7 @@ private void performRewrite(List<ManifestFile> currentManifests) {
             }
           });
     } finally {
-      Tasks.foreach(writers.values()).executeWith(ThreadPools.getWorkerPool()).run(writer -> writer.close());
+      Tasks.foreach(writers.values()).executeWith(ThreadPools.getWorkerPool()).run(WriterWrapper::close);
     }
   }
 

File: core/src/main/java/org/apache/iceberg/DeleteFileIndex.java
Patch:
@@ -361,7 +361,7 @@ DeleteFileIndex build() {
                 deleteEntries.add(entry.copy());
               }
             } catch (IOException e) {
-              throw new RuntimeIOException("Failed to close", e);
+              throw new RuntimeIOException(e, "Failed to close");
             }
           });
 

File: core/src/main/java/org/apache/iceberg/FindFiles.java
Patch:
@@ -185,7 +185,7 @@ public Builder inPartitions(PartitionSpec spec, List<StructLike> partitions) {
     }
 
     /**
-     * @return all files in the table that match all of the filters
+     * Returns all files in the table that match all of the filters.
      */
     public CloseableIterable<DataFile> collect() {
       Snapshot snapshot = snapshotId != null ?

File: core/src/main/java/org/apache/iceberg/IncrementalDataTableScan.java
Patch:
@@ -74,7 +74,7 @@ public CloseableIterable<FileScanTask> planFiles() {
     Set<Long> snapshotIds = Sets.newHashSet(Iterables.transform(snapshots, Snapshot::snapshotId));
     Set<ManifestFile> manifests = FluentIterable
         .from(snapshots)
-        .transformAndConcat(s -> s.dataManifests())
+        .transformAndConcat(Snapshot::dataManifests)
         .filter(manifestFile -> snapshotIds.contains(manifestFile.snapshotId()))
         .toSet();
 

File: core/src/main/java/org/apache/iceberg/ManifestFilterManager.java
Patch:
@@ -300,7 +300,7 @@ private ManifestFile filterManifest(StrictMetricsEvaluator metricsEvaluator, Man
       return filterManifestWithDeletedFiles(metricsEvaluator, manifest, reader);
 
     } catch (IOException e) {
-      throw new RuntimeIOException("Failed to close manifest: " + manifest, e);
+      throw new RuntimeIOException(e, "Failed to close manifest: %s", manifest);
     }
   }
 
@@ -422,7 +422,7 @@ private ManifestFile filterManifestWithDeletedFiles(
       return filtered;
 
     } catch (IOException e) {
-      throw new RuntimeIOException("Failed to close manifest writer", e);
+      throw new RuntimeIOException(e, "Failed to close manifest writer");
     }
   }
 

File: core/src/main/java/org/apache/iceberg/ManifestListWriter.java
Patch:
@@ -98,7 +98,7 @@ protected FileAppender<ManifestFile> newAppender(OutputFile file, Map<String, St
             .build();
 
       } catch (IOException e) {
-        throw new RuntimeIOException(e, "Failed to create snapshot list writer for path: " + file);
+        throw new RuntimeIOException(e, "Failed to create snapshot list writer for path: %s", file);
       }
     }
   }
@@ -131,7 +131,7 @@ protected FileAppender<ManifestFile> newAppender(OutputFile file, Map<String, St
             .build();
 
       } catch (IOException e) {
-        throw new RuntimeIOException(e, "Failed to create snapshot list writer for path: " + file);
+        throw new RuntimeIOException(e, "Failed to create snapshot list writer for path: %s", file);
       }
     }
   }

File: core/src/main/java/org/apache/iceberg/ManifestMergeManager.java
Patch:
@@ -172,7 +172,7 @@ private ManifestFile createManifest(int specId, List<ManifestFile> bin) {
             }
           }
         } catch (IOException e) {
-          throw new RuntimeIOException("Failed to close manifest reader", e);
+          throw new RuntimeIOException(e, "Failed to close manifest reader");
         }
       }
       threw = false;

File: core/src/main/java/org/apache/iceberg/ManifestWriter.java
Patch:
@@ -193,7 +193,7 @@ protected FileAppender<ManifestEntry<DataFile>> newAppender(PartitionSpec spec,
             .overwrite()
             .build();
       } catch (IOException e) {
-        throw new RuntimeIOException(e, "Failed to create manifest writer for path: " + file);
+        throw new RuntimeIOException(e, "Failed to create manifest writer for path: %s", file);
       }
     }
   }
@@ -226,7 +226,7 @@ protected FileAppender<ManifestEntry<DeleteFile>> newAppender(PartitionSpec spec
             .overwrite()
             .build();
       } catch (IOException e) {
-        throw new RuntimeIOException(e, "Failed to create manifest writer for path: " + file);
+        throw new RuntimeIOException(e, "Failed to create manifest writer for path: %s", file);
       }
     }
 
@@ -263,7 +263,7 @@ protected FileAppender<ManifestEntry<DataFile>> newAppender(PartitionSpec spec,
             .overwrite()
             .build();
       } catch (IOException e) {
-        throw new RuntimeIOException(e, "Failed to create manifest writer for path: " + file);
+        throw new RuntimeIOException(e, "Failed to create manifest writer for path: %s", file);
       }
     }
   }

File: core/src/main/java/org/apache/iceberg/MergingSnapshotProducer.java
Patch:
@@ -327,7 +327,7 @@ private ManifestFile newFilesAsManifest() {
         this.cachedNewManifest = writer.toManifestFile();
         this.hasNewFiles = false;
       } catch (IOException e) {
-        throw new RuntimeIOException("Failed to close manifest writer", e);
+        throw new RuntimeIOException(e, "Failed to close manifest writer");
       }
     }
 
@@ -360,7 +360,7 @@ private ManifestFile newDeleteFilesAsManifest() {
         this.cachedNewDeleteManifest = writer.toManifestFile();
         this.hasNewDeleteFiles = false;
       } catch (IOException e) {
-        throw new RuntimeIOException("Failed to close manifest writer", e);
+        throw new RuntimeIOException(e, "Failed to close manifest writer");
       }
     }
 

File: core/src/main/java/org/apache/iceberg/PartitionSpecParser.java
Patch:
@@ -131,7 +131,7 @@ static PartitionSpec fromJsonFields(Schema schema, int specId, String json) {
     try {
       return fromJsonFields(schema, specId, JsonUtil.mapper().readValue(json, JsonNode.class));
     } catch (IOException e) {
-      throw new RuntimeIOException(e, "Failed to parse partition spec fields: " + json);
+      throw new RuntimeIOException(e, "Failed to parse partition spec fields: %s", json);
     }
   }
 

File: core/src/main/java/org/apache/iceberg/TableOperations.java
Patch:
@@ -59,13 +59,12 @@ public interface TableOperations {
   void commit(TableMetadata base, TableMetadata metadata);
 
   /**
-   * @return a {@link FileIO} to read and write table data and metadata files
+   * Returns a {@link FileIO} to read and write table data and metadata files.
    */
   FileIO io();
 
   /**
-   * @return a {@link org.apache.iceberg.encryption.EncryptionManager} to encrypt and decrypt
-   * data files.
+   * Returns a {@link org.apache.iceberg.encryption.EncryptionManager} to encrypt and decrypt data files.
    */
   default EncryptionManager encryption() {
     return new PlaintextEncryptionManager();

File: core/src/main/java/org/apache/iceberg/hadoop/HadoopCatalog.java
Patch:
@@ -246,7 +246,7 @@ public List<Namespace> listNamespaces(Namespace namespace) {
     try {
       return Stream.of(fs.listStatus(nsPath))
         .map(FileStatus::getPath)
-        .filter(path -> isNamespace(path))
+        .filter(this::isNamespace)
         .map(path -> append(namespace, path.getName()))
         .collect(Collectors.toList());
     } catch (IOException ioe) {
@@ -270,7 +270,7 @@ public boolean dropNamespace(Namespace namespace) {
 
     try {
       if (fs.listStatusIterator(nsPath).hasNext()) {
-        throw new NamespaceNotEmptyException("Namespace " + namespace + " is not empty.");
+        throw new NamespaceNotEmptyException("Namespace %s is not empty.", namespace);
       }
 
       return fs.delete(nsPath, false /* recursive */);

File: core/src/main/java/org/apache/iceberg/hadoop/HadoopTableOperations.java
Patch:
@@ -157,7 +157,7 @@ public void commit(TableMetadata base, TableMetadata metadata) {
       }
     } catch (IOException e) {
       throw new RuntimeIOException(e,
-          "Failed to check if next version exists: " + finalMetadataFile);
+          "Failed to check if next version exists: %s", finalMetadataFile);
     }
 
     // this rename operation is the atomic commit operation

File: core/src/main/java/org/apache/iceberg/mapping/MappingUtil.java
Patch:
@@ -133,7 +133,7 @@ private MappedFields addNewFields(MappedFields mapping, int parentId) {
       }
 
       ImmutableMap.Builder<String, Integer> builder = ImmutableMap.builder();
-      fieldsToAdd.stream().forEach(field -> builder.put(field.name(), field.fieldId()));
+      fieldsToAdd.forEach(field -> builder.put(field.name(), field.fieldId()));
       Map<String, Integer> assignments = builder.build();
 
       // create a copy of fields that can be updated (append new fields, replace existing for reassignment)

File: core/src/main/java/org/apache/iceberg/util/Exceptions.java
Patch:
@@ -32,7 +32,7 @@ public static void close(Closeable closeable, boolean suppressExceptions) {
       closeable.close();
     } catch (IOException e) {
       if (!suppressExceptions) {
-        throw new RuntimeIOException("Failed calling close", e);
+        throw new RuntimeIOException(e, "Failed calling close");
       }
       // otherwise, ignore the exception
     }

File: core/src/main/java/org/apache/iceberg/util/Filter.java
Patch:
@@ -23,6 +23,7 @@
 import org.apache.iceberg.io.FilterIterator;
 
 /**
+ * A Class for generic filters
  *
  * @param <T> the type of objects filtered by this Filter
  */

File: hive-metastore/src/main/java/org/apache/iceberg/hive/HiveCatalog.java
Patch:
@@ -277,7 +277,7 @@ public boolean dropNamespace(Namespace namespace) {
       return true;
 
     } catch (InvalidOperationException e) {
-      throw new NamespaceNotEmptyException("Namespace " + namespace + " is not empty. One or more tables exist.", e);
+      throw new NamespaceNotEmptyException(e, "Namespace %s is not empty. One or more tables exist.", namespace);
 
     } catch (NoSuchObjectException e) {
       return false;

File: hive-metastore/src/main/java/org/apache/iceberg/hive/RuntimeMetaException.java
Patch:
@@ -19,6 +19,7 @@
 
 package org.apache.iceberg.hive;
 
+import com.google.errorprone.annotations.FormatMethod;
 import org.apache.hadoop.hive.metastore.api.MetaException;
 
 /**
@@ -29,10 +30,12 @@ public RuntimeMetaException(MetaException cause) {
     super(cause);
   }
 
+  @FormatMethod
   public RuntimeMetaException(MetaException cause, String message, Object... args) {
     super(String.format(message, args), cause);
   }
 
+  @FormatMethod
   public RuntimeMetaException(Throwable throwable, String message, Object... args) {
     super(String.format(message, args), throwable);
   }

File: mr/src/main/java/org/apache/iceberg/mr/Catalogs.java
Patch:
@@ -107,7 +107,7 @@ static Optional<Catalog> loadCatalog(Configuration conf) {
           LOG.info("Loaded Hive Metastore catalog {}", catalog);
           return Optional.of(catalog);
         default:
-          throw new NoSuchNamespaceException("Catalog " + catalogName + " is not supported.");
+          throw new NoSuchNamespaceException("Catalog %s is not supported.", catalogName);
       }
     }
 

File: orc/src/main/java/org/apache/iceberg/orc/IdToOrcName.java
Patch:
@@ -36,7 +36,7 @@
  * <p>
  * This visitor also enclose column names in backticks i.e. ` so that ORC can correctly parse column names with
  * special characters. A comparison of ORC convention with Iceberg convention is provided below
- * <pre>
+ * <pre><code>
  *                                      Iceberg           ORC
  * field                                field             field
  * struct -> field                      struct.field      struct.field
@@ -46,7 +46,7 @@
  * map -> value                         map.value         map._value
  * map -> struct key -> field           map.key.field     map._key.field
  * map -> struct value -> field         map.field         map._value.field
- * </pre>
+ * </code></pre>
  */
 class IdToOrcName extends TypeUtil.SchemaVisitor<Map<Integer, String>> {
   private static final Joiner DOT = Joiner.on(".");

File: orc/src/main/java/org/apache/iceberg/orc/OrcFileAppender.java
Patch:
@@ -86,7 +86,7 @@ public void add(D datum) {
         batch.reset();
       }
     } catch (IOException ioe) {
-      throw new RuntimeIOException(ioe, "Problem writing to ORC file " + file.location());
+      throw new RuntimeIOException(ioe, "Problem writing to ORC file %s", file.location());
     }
   }
 
@@ -138,7 +138,7 @@ private static Writer newOrcWriter(OutputFile file,
     try {
       writer = OrcFile.createWriter(locPath, options);
     } catch (IOException ioe) {
-      throw new RuntimeIOException(ioe, "Can't create file " + locPath);
+      throw new RuntimeIOException(ioe, "Can't create file %s", locPath);
     }
 
     metadata.forEach((key, value) -> writer.addUserMetadata(key, ByteBuffer.wrap(value)));

File: orc/src/main/java/org/apache/iceberg/orc/VectorizedRowBatchIterator.java
Patch:
@@ -56,7 +56,7 @@ private void advance() {
         batchOffsetInFile = rows.getRowNumber();
         rows.nextBatch(batch);
       } catch (IOException ioe) {
-        throw new RuntimeIOException(ioe, "Problem reading ORC file " + fileLocation);
+        throw new RuntimeIOException(ioe, "Problem reading ORC file %s", fileLocation);
       }
       advanced = true;
     }

File: parquet/src/main/java/org/apache/iceberg/parquet/ParquetUtil.java
Patch:
@@ -167,8 +167,7 @@ private static MessageType getParquetTypeWithIds(ParquetMetadata metadata, NameM
   }
 
   /**
-   * @return a list of offsets in ascending order determined by the starting position
-   * of the row groups
+   * Returns a list of offsets in ascending order determined by the starting position of the row groups.
    */
   public static List<Long> getSplitOffsets(ParquetMetadata md) {
     List<Long> splitOffsets = new ArrayList<>(md.getBlocks().size());

File: parquet/src/main/java/org/apache/iceberg/parquet/ValuesAsBytesReader.java
Patch:
@@ -92,8 +92,7 @@ public final boolean readBoolean() {
   }
 
   /**
-   *
-   * @return 1 if true, 0 otherwise
+   * Returns 1 if true, 0 otherwise.
    */
   public final int readBooleanAsInt() {
     if (bitOffset == 0) {

File: parquet/src/main/java/org/apache/iceberg/parquet/VectorizedReader.java
Patch:
@@ -41,6 +41,8 @@ public interface VectorizedReader<T> {
   void setBatchSize(int batchSize);
 
   /**
+   * Sets the row group information toe be used with this reader
+   *
    * @param pages    row group information for all the columns
    * @param metadata map of {@link ColumnPath} -&gt; {@link ColumnChunkMetaData} for the row group
    */

File: spark/src/main/java/org/apache/iceberg/actions/RewriteManifestsAction.java
Patch:
@@ -174,7 +174,7 @@ public RewriteManifestsActionResult execute() {
     int numEntries = 0;
 
     for (ManifestFile manifest : matchingManifests) {
-      ValidationException.check(hasFileCounts(manifest), "No file counts in manifest: " + manifest.path());
+      ValidationException.check(hasFileCounts(manifest), "No file counts in manifest: %s", manifest.path());
 
       totalSizeBytes += manifest.length();
       numEntries += manifest.addedFilesCount() + manifest.existingFilesCount() + manifest.deletedFilesCount();

File: spark/src/main/java/org/apache/iceberg/spark/SparkTableUtil.java
Patch:
@@ -508,8 +508,7 @@ public static void importSparkTable(
     TableIdentifier sourceTableIdentWithDB = new TableIdentifier(sourceTableIdent.table(), Some.apply(db));
 
     if (!catalog.tableExists(sourceTableIdentWithDB)) {
-      throw new org.apache.iceberg.exceptions.NoSuchTableException(
-          String.format("Table %s does not exist", sourceTableIdentWithDB));
+      throw new org.apache.iceberg.exceptions.NoSuchTableException("Table %s does not exist", sourceTableIdentWithDB);
     }
 
     try {

File: spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ArrowVectorAccessors.java
Patch:
@@ -40,6 +40,7 @@
 import org.apache.parquet.Preconditions;
 import org.apache.parquet.column.ColumnDescriptor;
 import org.apache.parquet.column.Dictionary;
+import org.apache.parquet.io.api.Binary;
 import org.apache.parquet.schema.PrimitiveType;
 import org.apache.spark.sql.types.Decimal;
 import org.apache.spark.sql.vectorized.ArrowColumnVector;
@@ -350,7 +351,7 @@ private static class DictionaryBinaryAccessor extends ArrowVectorAccessor {
       this.offsetVector = vector;
       this.decodedDictionary = IntStream.rangeClosed(0, dictionary.getMaxId())
           .mapToObj(dictionary::decodeToBinary)
-          .map(binary -> binary.getBytes())
+          .map(Binary::getBytes)
           .toArray(byte[][]::new);
     }
 

File: spark2/src/main/java/org/apache/iceberg/spark/source/Writer.java
Patch:
@@ -150,7 +150,7 @@ protected void commitOperation(SnapshotUpdate<?> operation, int numFiles, String
     }
 
     if (!extraSnapshotMetadata.isEmpty()) {
-      extraSnapshotMetadata.forEach((key, value) -> operation.set(key, value));
+      extraSnapshotMetadata.forEach(operation::set);
     }
 
     if (isWapTable() && wapId != null) {

File: spark3/src/main/java/org/apache/iceberg/spark/source/SparkBatchWrite.java
Patch:
@@ -154,7 +154,7 @@ protected void commitOperation(SnapshotUpdate<?> operation, int numFiles, String
     }
 
     if (!extraSnapshotMetadata.isEmpty()) {
-      extraSnapshotMetadata.forEach((key, value) -> operation.set(key, value));
+      extraSnapshotMetadata.forEach(operation::set);
     }
 
     if (isWapTable() && wapId != null) {

File: mr/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergFilterFactory.java
Patch:
@@ -22,6 +22,7 @@
 import java.math.BigDecimal;
 import java.sql.Date;
 import java.sql.Timestamp;
+import java.time.LocalDate;
 import java.time.ZoneOffset;
 import org.apache.hadoop.hive.ql.io.sarg.PredicateLeaf;
 import org.apache.hadoop.hive.ql.io.sarg.SearchArgument;
@@ -206,8 +207,8 @@ public void testBooleanType() {
   @Test
   public void testDateType() {
     SearchArgument.Builder builder = SearchArgumentFactory.newBuilder();
-    SearchArgument arg = builder.startAnd().equals("date", PredicateLeaf.Type.DATE,
-            Date.valueOf("2015-11-12")).end().build();
+    Date gmtDate = new Date(LocalDate.of(2015, 11, 12).atStartOfDay(ZoneOffset.UTC).toInstant().toEpochMilli());
+    SearchArgument arg = builder.startAnd().equals("date", PredicateLeaf.Type.DATE, gmtDate).end().build();
 
     UnboundPredicate expected = Expressions.equal("date", Literal.of("2015-11-12").to(Types.DateType.get()).value());
     UnboundPredicate actual = (UnboundPredicate) HiveIcebergFilterFactory.generateFilterExpression(arg);

File: core/src/main/java/org/apache/iceberg/DeleteFileIndex.java
Patch:
@@ -76,7 +76,7 @@ class DeleteFileIndex {
     ImmutableMap.Builder<Integer, Types.StructType> builder = ImmutableMap.builder();
     specsById.forEach((specId, spec) -> builder.put(specId, spec.partitionType()));
     this.partitionTypeById = builder.build();
-    this.wrapperById = Maps.newHashMap();
+    this.wrapperById = Maps.newConcurrentMap();
     this.globalSeqs = globalSeqs;
     this.globalDeletes = globalDeletes;
     this.sortedDeletesByPartition = sortedDeletesByPartition;

File: core/src/main/java/org/apache/iceberg/avro/ValueReaders.java
Patch:
@@ -272,6 +272,7 @@ private UUIDReader() {
     }
 
     @Override
+    @SuppressWarnings("ByteBufferBackingArray")
     public UUID read(Decoder decoder, Object ignored) throws IOException {
       ByteBuffer buffer = BUFFER.get();
       buffer.rewind();

File: core/src/main/java/org/apache/iceberg/avro/ValueWriters.java
Patch:
@@ -272,6 +272,7 @@ private UUIDWriter() {
     }
 
     @Override
+    @SuppressWarnings("ByteBufferBackingArray")
     public void write(UUID uuid, Encoder encoder) throws IOException {
       // TODO: direct conversion from string to byte buffer
       ByteBuffer buffer = BUFFER.get();

File: pig/src/main/java/org/apache/iceberg/pig/IcebergPigInputFormat.java
Patch:
@@ -51,6 +51,7 @@
 import org.apache.iceberg.types.Type;
 import org.apache.iceberg.types.TypeUtil;
 import org.apache.iceberg.types.Types;
+import org.apache.iceberg.util.ByteBuffers;
 import org.apache.pig.data.DataByteArray;
 import org.apache.pig.impl.util.ObjectSerializer;
 import org.slf4j.Logger;
@@ -245,8 +246,7 @@ private boolean advance() throws IOException {
 
     private Object convertPartitionValue(Type type, Object value) {
       if (type.typeId() == Types.BinaryType.get().typeId()) {
-        ByteBuffer buffer = (ByteBuffer) value;
-        return new DataByteArray(buffer.get(new byte[buffer.remaining()]).array());
+        return new DataByteArray(ByteBuffers.toByteArray((ByteBuffer) value));
       }
 
       return value;

File: spark/src/main/java/org/apache/iceberg/spark/data/SparkValueReaders.java
Patch:
@@ -134,6 +134,7 @@ private UUIDReader() {
     }
 
     @Override
+    @SuppressWarnings("ByteBufferBackingArray")
     public UTF8String read(Decoder decoder, Object reuse) throws IOException {
       ByteBuffer buffer = BUFFER.get();
       buffer.rewind();

File: spark/src/main/java/org/apache/iceberg/spark/data/SparkValueWriters.java
Patch:
@@ -100,6 +100,7 @@ private UUIDWriter() {
     }
 
     @Override
+    @SuppressWarnings("ByteBufferBackingArray")
     public void write(UTF8String s, Encoder encoder) throws IOException {
       // TODO: direct conversion from string to byte buffer
       UUID uuid = UUID.fromString(s.toString());

File: spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetVectorizedReads.java
Patch:
@@ -91,7 +91,7 @@ FileAppender<GenericData.Record> getParquetWriter(Schema schema, File testFile)
         .build();
   }
 
-  private void assertRecordsMatch(
+  void assertRecordsMatch(
       Schema schema, int expectedSize, Iterable<GenericData.Record> expected, File testFile,
       boolean setAndCheckArrowValidityBuffer, boolean reuseContainers)
       throws IOException {

File: spark/src/main/java/org/apache/iceberg/spark/SparkSchemaUtil.java
Patch:
@@ -163,7 +163,7 @@ public static Schema convert(Schema baseSchema, StructType sparkType) {
     // reassign ids to match the base schema
     Schema schema = TypeUtil.reassignIds(new Schema(struct.fields()), baseSchema);
     // fix types that can't be represented in Spark (UUID and Fixed)
-    return FixupTypes.fixup(schema, baseSchema);
+    return SparkFixupTypes.fixup(schema, baseSchema);
   }
 
   /**

File: orc/src/main/java/org/apache/iceberg/orc/OrcMetrics.java
Patch:
@@ -99,7 +99,8 @@ private static Metrics buildOrcMetrics(long numOfRows, TypeDescription orcSchema
                                          ColumnStatistics[] colStats, MetricsConfig metricsConfig) {
     final Schema schema = ORCSchemaUtil.convert(orcSchema);
     final Set<Integer> statsColumns = statsColumns(orcSchema);
-    final MetricsConfig effectiveMetricsConfig = Optional.ofNullable(metricsConfig).orElse(MetricsConfig.getDefault());
+    final MetricsConfig effectiveMetricsConfig = Optional.ofNullable(metricsConfig)
+        .orElseGet(MetricsConfig::getDefault);
     Map<Integer, Long> columnSizes = Maps.newHashMapWithExpectedSize(colStats.length);
     Map<Integer, Long> valueCounts = Maps.newHashMapWithExpectedSize(colStats.length);
     Map<Integer, Long> nullCounts = Maps.newHashMapWithExpectedSize(colStats.length);

File: data/src/test/java/org/apache/iceberg/orc/TestOrcMetrics.java
Patch:
@@ -101,7 +101,7 @@ protected <T> void assertBounds(int fieldId, Type type, T lowerBound, T upperBou
       Assert.assertFalse("ORC binary field should not have lower bounds.",
           metrics.lowerBounds().containsKey(fieldId));
       Assert.assertFalse("ORC binary field should not have upper bounds.",
-          metrics.lowerBounds().containsKey(fieldId));
+          metrics.upperBounds().containsKey(fieldId));
       return;
     }
     super.assertBounds(fieldId, type, lowerBound, upperBound, metrics);

File: api/src/main/java/org/apache/iceberg/Files.java
Patch:
@@ -34,7 +34,8 @@
 
 public class Files {
 
-  private Files() {}
+  private Files() {
+  }
 
   public static OutputFile localOutput(File file) {
     return new LocalOutputFile(file);

File: api/src/main/java/org/apache/iceberg/expressions/ExpressionVisitors.java
Patch:
@@ -27,7 +27,8 @@
  */
 public class ExpressionVisitors {
 
-  private ExpressionVisitors() {}
+  private ExpressionVisitors() {
+  }
 
   public abstract static class ExpressionVisitor<R> {
     public R alwaysTrue() {

File: api/src/main/java/org/apache/iceberg/transforms/ProjectionUtil.java
Patch:
@@ -34,7 +34,8 @@
 
 class ProjectionUtil {
 
-  private ProjectionUtil() {}
+  private ProjectionUtil() {
+  }
 
   static <T> UnboundPredicate<T> truncateInteger(
       String name, BoundLiteralPredicate<Integer> pred, Transform<Integer, T> transform) {

File: api/src/main/java/org/apache/iceberg/transforms/TransformUtil.java
Patch:
@@ -30,7 +30,8 @@
 
 class TransformUtil {
 
-  private TransformUtil() {}
+  private TransformUtil() {
+  }
 
   private static final OffsetDateTime EPOCH = Instant.ofEpochSecond(0).atOffset(ZoneOffset.UTC);
   private static final int EPOCH_YEAR = EPOCH.getYear();

File: api/src/main/java/org/apache/iceberg/types/Comparators.java
Patch:
@@ -30,7 +30,8 @@
 
 public class Comparators {
 
-  private Comparators() {}
+  private Comparators() {
+  }
 
   private static final ImmutableMap<Type.PrimitiveType, Comparator<?>> COMPARATORS = ImmutableMap
       .<Type.PrimitiveType, Comparator<?>>builder()

File: api/src/main/java/org/apache/iceberg/types/Conversions.java
Patch:
@@ -36,7 +36,8 @@
 
 public class Conversions {
 
-  private Conversions() {}
+  private Conversions() {
+  }
 
   private static final String HIVE_NULL = "__HIVE_DEFAULT_PARTITION__";
 

File: api/src/main/java/org/apache/iceberg/types/TypeUtil.java
Patch:
@@ -37,7 +37,8 @@
 
 public class TypeUtil {
 
-  private TypeUtil() {}
+  private TypeUtil() {
+  }
 
   public static Schema select(Schema schema, Set<Integer> fieldIds) {
     Preconditions.checkNotNull(schema, "Schema cannot be null");

File: api/src/main/java/org/apache/iceberg/types/Types.java
Patch:
@@ -36,7 +36,8 @@
 
 public class Types {
 
-  private Types() {}
+  private Types() {
+  }
 
   private static final ImmutableMap<String, PrimitiveType> TYPES = ImmutableMap
       .<String, PrimitiveType>builder()

File: api/src/test/java/org/apache/iceberg/AssertHelpers.java
Patch:
@@ -24,7 +24,8 @@
 
 public class AssertHelpers {
 
-  private AssertHelpers() {}
+  private AssertHelpers() {
+  }
 
   /**
    * A convenience method to avoid a large number of @Test(expected=...) tests

File: api/src/test/java/org/apache/iceberg/TestHelpers.java
Patch:
@@ -37,7 +37,8 @@
 
 public class TestHelpers {
 
-  private TestHelpers() {}
+  private TestHelpers() {
+  }
 
   public static <T> T assertAndUnwrap(Expression expr, Class<T> expected) {
     Assert.assertTrue("Expression should have expected type: " + expected,

File: arrow/src/main/java/org/apache/iceberg/arrow/ArrowSchemaUtil.java
Patch:
@@ -42,7 +42,8 @@ public class ArrowSchemaUtil {
   private static final String ORIGINAL_TYPE = "originalType";
   private static final String MAP_TYPE = "mapType";
 
-  private ArrowSchemaUtil() { }
+  private ArrowSchemaUtil() {
+  }
 
   /**
    * Convert Iceberg schema to Arrow Schema.

File: bundled-guava/src/main/java/org/apache/iceberg/GuavaClasses.java
Patch:
@@ -51,7 +51,7 @@
 import com.google.common.util.concurrent.MoreExecutors;
 import com.google.common.util.concurrent.ThreadFactoryBuilder;
 
-//inspired in part by https://github.com/apache/avro/blob/release-1.8.2/lang/java/guava/src/main/java/org/apache/avro/GuavaClasses.java
+// inspired in part by https://github.com/apache/avro/blob/release-1.8.2/lang/java/guava/src/main/java/org/apache/avro/GuavaClasses.java
 public class GuavaClasses {
 
   /*

File: common/src/main/java/org/apache/iceberg/common/DynClasses.java
Patch:
@@ -25,7 +25,8 @@
 
 public class DynClasses {
 
-  private DynClasses() {}
+  private DynClasses() {
+  }
 
   public static Builder builder() {
     return new Builder();

File: common/src/main/java/org/apache/iceberg/common/DynConstructors.java
Patch:
@@ -35,7 +35,8 @@
  */
 public class DynConstructors {
 
-  private DynConstructors() {}
+  private DynConstructors() {
+  }
 
   public static class Ctor<C> extends DynMethods.UnboundMethod {
     private final Constructor<C> ctor;

File: common/src/main/java/org/apache/iceberg/common/DynFields.java
Patch:
@@ -34,7 +34,8 @@
 
 public class DynFields {
 
-  private DynFields() {}
+  private DynFields() {
+  }
 
   /**
    * Convenience wrapper class around {@link java.lang.reflect.Field}.

File: common/src/main/java/org/apache/iceberg/common/DynMethods.java
Patch:
@@ -34,7 +34,8 @@
  */
 public class DynMethods {
 
-  private DynMethods() {}
+  private DynMethods() {
+  }
 
   /**
    * Convenience wrapper class around {@link java.lang.reflect.Method}.

File: core/src/main/java/org/apache/iceberg/BaseMetastoreTableOperations.java
Patch:
@@ -51,7 +51,8 @@ public abstract class BaseMetastoreTableOperations implements TableOperations {
   private boolean shouldRefresh = true;
   private int version = -1;
 
-  protected BaseMetastoreTableOperations() { }
+  protected BaseMetastoreTableOperations() {
+  }
 
   @Override
   public TableMetadata current() {

File: core/src/main/java/org/apache/iceberg/BaseRewriteManifests.java
Patch:
@@ -207,8 +207,8 @@ private void keepActiveManifests(List<ManifestFile> currentManifests) {
     // keep any existing manifests as-is that were not processed
     keptManifests.clear();
     currentManifests.stream()
-      .filter(manifest -> !rewrittenManifests.contains(manifest) && !deletedManifests.contains(manifest))
-      .forEach(manifest -> keptManifests.add(manifest));
+        .filter(manifest -> !rewrittenManifests.contains(manifest) && !deletedManifests.contains(manifest))
+        .forEach(manifest -> keptManifests.add(manifest));
   }
 
   private void reset() {

File: core/src/main/java/org/apache/iceberg/DataFiles.java
Patch:
@@ -35,7 +35,8 @@
 
 public class DataFiles {
 
-  private DataFiles() {}
+  private DataFiles() {
+  }
 
   static PartitionData newPartitionData(PartitionSpec spec) {
     return new PartitionData(spec.partitionType());

File: core/src/main/java/org/apache/iceberg/IncrementalDataTableScan.java
Patch:
@@ -68,7 +68,7 @@ public TableScan appendsAfter(long newFromSnapshotId) {
 
   @Override
   public CloseableIterable<FileScanTask> planFiles() {
-    //TODO publish an incremental appends scan event
+    // TODO publish an incremental appends scan event
     List<Snapshot> snapshots = snapshotsWithin(table(),
         context().fromSnapshotId(), context().toSnapshotId());
     Set<Long> snapshotIds = Sets.newHashSet(Iterables.transform(snapshots, Snapshot::snapshotId));

File: core/src/main/java/org/apache/iceberg/LocationProviders.java
Patch:
@@ -32,7 +32,8 @@
 
 public class LocationProviders {
 
-  private LocationProviders() {}
+  private LocationProviders() {
+  }
 
   public static LocationProvider locationsFor(String location, Map<String, String> properties) {
     if (PropertyUtil.propertyAsBoolean(properties,

File: core/src/main/java/org/apache/iceberg/MetricsConfig.java
Patch:
@@ -36,7 +36,8 @@ public class MetricsConfig implements Serializable {
   private Map<String, MetricsMode> columnModes = Maps.newHashMap();
   private MetricsMode defaultMode;
 
-  private MetricsConfig() {}
+  private MetricsConfig() {
+  }
 
   public static MetricsConfig getDefault() {
     MetricsConfig spec = new MetricsConfig();

File: core/src/main/java/org/apache/iceberg/SchemaParser.java
Patch:
@@ -36,7 +36,8 @@
 
 public class SchemaParser {
 
-  private SchemaParser() {}
+  private SchemaParser() {
+  }
 
   private static final String TYPE = "type";
   private static final String STRUCT = "struct";

File: core/src/main/java/org/apache/iceberg/SnapshotParser.java
Patch:
@@ -35,7 +35,8 @@
 
 public class SnapshotParser {
 
-  private SnapshotParser() {}
+  private SnapshotParser() {
+  }
 
   private static final String SEQUENCE_NUMBER = "sequence-number";
   private static final String SNAPSHOT_ID = "snapshot-id";

File: core/src/main/java/org/apache/iceberg/SystemProperties.java
Patch:
@@ -24,7 +24,8 @@
  */
 public class SystemProperties {
 
-  private SystemProperties() {}
+  private SystemProperties() {
+  }
 
   /**
    * Sets the size of the worker pool. The worker pool limits the number of tasks concurrently

File: core/src/main/java/org/apache/iceberg/TableMetadataParser.java
Patch:
@@ -77,7 +77,8 @@ public static Codec fromFileName(String fileName) {
     }
   }
 
-  private TableMetadataParser() {}
+  private TableMetadataParser() {
+  }
 
   // visible for testing
   static final String FORMAT_VERSION = "format-version";

File: core/src/main/java/org/apache/iceberg/TableProperties.java
Patch:
@@ -21,7 +21,8 @@
 
 public class TableProperties {
 
-  private TableProperties() {}
+  private TableProperties() {
+  }
 
   public static final String COMMIT_NUM_RETRIES = "commit.retry.num-retries";
   public static final int COMMIT_NUM_RETRIES_DEFAULT = 4;

File: core/src/main/java/org/apache/iceberg/Transactions.java
Patch:
@@ -23,7 +23,8 @@
 import org.apache.iceberg.relocated.com.google.common.base.Preconditions;
 
 public final class Transactions {
-  private Transactions() {}
+  private Transactions() {
+  }
 
   public static Transaction createOrReplaceTableTransaction(
       String tableName, TableOperations ops, TableMetadata start) {

File: core/src/main/java/org/apache/iceberg/avro/AvroSchemaUtil.java
Patch:
@@ -38,7 +38,8 @@
 
 public class AvroSchemaUtil {
 
-  private AvroSchemaUtil() {}
+  private AvroSchemaUtil() {
+  }
 
   // Original Iceberg field name corresponding to a sanitized Avro name
   public static final String ICEBERG_FIELD_NAME_PROP = "iceberg-field-name";

File: core/src/main/java/org/apache/iceberg/data/avro/DecoderResolver.java
Patch:
@@ -39,7 +39,8 @@ public class DecoderResolver {
   private static final ThreadLocal<Map<Schema, Map<Schema, ResolvingDecoder>>> DECODER_CACHES =
       ThreadLocal.withInitial(() -> new MapMaker().weakKeys().makeMap());
 
-  private DecoderResolver() {}
+  private DecoderResolver() {
+  }
 
   public static <T> T resolveAndRead(
       Decoder decoder, Schema readSchema, Schema fileSchema, ValueReader<T> reader, T reuse) throws IOException {

File: core/src/main/java/org/apache/iceberg/encryption/EncryptedFiles.java
Patch:
@@ -53,5 +53,6 @@ public static EncryptedOutputFile encryptedOutput(OutputFile encryptedOutputFile
     return encryptedOutput(encryptedOutputFile, BaseEncryptionKeyMetadata.fromByteArray(keyMetadata));
   }
 
-  private EncryptedFiles() {}
+  private EncryptedFiles() {
+  }
 }

File: core/src/main/java/org/apache/iceberg/encryption/EncryptionKeyMetadatas.java
Patch:
@@ -31,5 +31,6 @@ public static EncryptionKeyMetadata of(byte[] keyMetadata) {
     return BaseEncryptionKeyMetadata.fromByteArray(keyMetadata);
   }
 
-  private EncryptionKeyMetadatas() {}
+  private EncryptionKeyMetadatas() {
+  }
 }

File: core/src/main/java/org/apache/iceberg/hadoop/HadoopStreams.java
Patch:
@@ -41,7 +41,8 @@
  */
 class HadoopStreams {
 
-  private HadoopStreams() {}
+  private HadoopStreams() {
+  }
 
   private static final Logger LOG = LoggerFactory.getLogger(HadoopStreams.class);
 

File: core/src/main/java/org/apache/iceberg/hadoop/HiddenPathFilter.java
Patch:
@@ -30,7 +30,8 @@ public class HiddenPathFilter implements PathFilter {
 
   private static final HiddenPathFilter INSTANCE = new HiddenPathFilter();
 
-  private HiddenPathFilter() {}
+  private HiddenPathFilter() {
+  }
 
   public static HiddenPathFilter get() {
     return INSTANCE;

File: core/src/main/java/org/apache/iceberg/io/BaseTaskWriter.java
Patch:
@@ -105,7 +105,7 @@ private void openCurrent() {
     }
 
     private boolean shouldRollToNewFile() {
-      //TODO: ORC file now not support target file size before closed
+      // TODO: ORC file now not support target file size before closed
       return !format.equals(FileFormat.ORC) &&
           currentRows % ROWS_DIVISOR == 0 && currentAppender.length() >= targetFileSize;
     }

File: core/src/main/java/org/apache/iceberg/util/ByteBuffers.java
Patch:
@@ -58,5 +58,6 @@ public static ByteBuffer copy(ByteBuffer buffer) {
     return ByteBuffer.wrap(copyArray);
   }
 
-  private ByteBuffers() {}
+  private ByteBuffers() {
+  }
 }

File: core/src/main/java/org/apache/iceberg/util/ExceptionUtil.java
Patch:
@@ -21,7 +21,8 @@
 
 public class ExceptionUtil {
 
-  private ExceptionUtil() {}
+  private ExceptionUtil() {
+  }
 
   @SuppressWarnings("unchecked")
   static <E extends Exception> void castAndThrow(

File: core/src/main/java/org/apache/iceberg/util/JsonUtil.java
Patch:
@@ -31,7 +31,8 @@
 
 public class JsonUtil {
 
-  private JsonUtil() {}
+  private JsonUtil() {
+  }
 
   private static final JsonFactory FACTORY = new JsonFactory();
   private static final ObjectMapper MAPPER = new ObjectMapper(FACTORY);

File: core/src/main/java/org/apache/iceberg/util/PropertyUtil.java
Patch:
@@ -23,7 +23,8 @@
 
 public class PropertyUtil {
 
-  private PropertyUtil() {}
+  private PropertyUtil() {
+  }
 
   public static boolean propertyAsBoolean(Map<String, String> properties,
                                           String property, boolean defaultValue) {

File: core/src/main/java/org/apache/iceberg/util/TableScanUtil.java
Patch:
@@ -28,7 +28,8 @@
 
 public class TableScanUtil {
 
-  private TableScanUtil() {}
+  private TableScanUtil() {
+  }
 
   public static CloseableIterable<FileScanTask> splitFiles(CloseableIterable<FileScanTask> tasks, long splitSize) {
     Iterable<FileScanTask> splitTasks = FluentIterable

File: core/src/main/java/org/apache/iceberg/util/Tasks.java
Patch:
@@ -42,7 +42,8 @@
 public class Tasks {
   private static final Logger LOG = LoggerFactory.getLogger(Tasks.class);
 
-  private Tasks() {}
+  private Tasks() {
+  }
 
   public static class UnrecoverableException extends RuntimeException {
     public UnrecoverableException(String message) {

File: core/src/main/java/org/apache/iceberg/util/ThreadPools.java
Patch:
@@ -28,7 +28,8 @@
 
 public class ThreadPools {
 
-  private ThreadPools() {}
+  private ThreadPools() {
+  }
 
   public static final String WORKER_THREAD_POOL_SIZE_PROP =
       SystemProperties.WORKER_THREAD_POOL_SIZE_PROP;

File: core/src/test/java/org/apache/iceberg/TestRemoveSnapshots.java
Patch:
@@ -618,13 +618,13 @@ public void testWithExpiringDanglingStageCommit() {
     // ManifestList should be deleted too
     expectedDeletes.add(snapshotB.manifestListLocation());
     snapshotB.dataManifests().forEach(file -> {
-      //Only the manifest of B should be deleted.
+      // Only the manifest of B should be deleted.
       if (file.snapshotId() == snapshotB.snapshotId()) {
         expectedDeletes.add(file.path());
       }
     });
     Assert.assertSame("Files deleted count should be expected", expectedDeletes.size(), deletedFiles.size());
-    //Take the diff
+    // Take the diff
     expectedDeletes.removeAll(deletedFiles);
     Assert.assertTrue("Exactly same files should be deleted", expectedDeletes.isEmpty());
   }

File: core/src/test/java/org/apache/iceberg/TestTables.java
Patch:
@@ -36,7 +36,8 @@
 
 public class TestTables {
 
-  private TestTables() {}
+  private TestTables() {
+  }
 
   private static TestTable upgrade(File temp, String name, int newFormatVersion) {
     TestTable table = load(temp, name);

File: core/src/test/java/org/apache/iceberg/avro/AvroTestHelpers.java
Patch:
@@ -33,7 +33,8 @@
 
 class AvroTestHelpers {
 
-  private AvroTestHelpers() {}
+  private AvroTestHelpers() {
+  }
 
   static Schema.Field optionalField(int id, String name, Schema schema) {
     return addId(id, new Schema.Field(name, toOption(schema), null, JsonProperties.NULL_VALUE));

File: core/src/test/java/org/apache/iceberg/avro/RandomAvroData.java
Patch:
@@ -40,7 +40,8 @@
 
 public class RandomAvroData {
 
-  private RandomAvroData() {}
+  private RandomAvroData() {
+  }
 
   public static List<Record> generate(Schema schema, int numRecords, long seed) {
     RandomDataGenerator generator = new RandomDataGenerator(schema, seed);

File: data/src/test/java/org/apache/iceberg/data/DataTestHelpers.java
Patch:
@@ -26,7 +26,8 @@
 import org.junit.Assert;
 
 public class DataTestHelpers {
-  private DataTestHelpers() {}
+  private DataTestHelpers() {
+  }
 
   public static void assertEquals(Types.StructType struct, Record expected, Record actual) {
     List<Types.NestedField> fields = struct.fields();

File: data/src/test/java/org/apache/iceberg/data/RandomGenericData.java
Patch:
@@ -45,7 +45,8 @@
 import static java.time.temporal.ChronoUnit.MICROS;
 
 public class RandomGenericData {
-  private RandomGenericData() {}
+  private RandomGenericData() {
+  }
 
   public static List<Record> generate(Schema schema, int numRecords, long seed) {
     return Lists.newArrayList(generateIcebergGenerics(schema, numRecords, () -> new RandomRecordGenerator(seed)));

File: flink/src/main/java/org/apache/iceberg/flink/data/FlinkValueReaders.java
Patch:
@@ -44,7 +44,8 @@
 
 public class FlinkValueReaders {
 
-  private FlinkValueReaders() {}
+  private FlinkValueReaders() {
+  }
 
   static ValueReader<StringData> strings() {
     return StringReader.INSTANCE;

File: flink/src/main/java/org/apache/iceberg/flink/data/FlinkValueWriters.java
Patch:
@@ -37,7 +37,8 @@
 
 public class FlinkValueWriters {
 
-  private FlinkValueWriters() {}
+  private FlinkValueWriters() {
+  }
 
   static ValueWriter<StringData> strings() {
     return StringWriter.INSTANCE;

File: flink/src/test/java/org/apache/iceberg/flink/data/TestHelpers.java
Patch:
@@ -47,7 +47,8 @@
 import org.junit.Assert;
 
 public class TestHelpers {
-  private TestHelpers() {}
+  private TestHelpers() {
+  }
 
   public static void assertRowData(Types.StructType structType, LogicalType rowType, Record expectedRecord,
                                    RowData actualRowData) {

File: hive/src/main/java/org/apache/iceberg/hive/HiveCatalogs.java
Patch:
@@ -33,7 +33,8 @@ public final class HiveCatalogs {
       .removalListener((RemovalListener<String, HiveCatalog>) (uri, catalog, cause) -> catalog.close())
       .build();
 
-  private HiveCatalogs() {}
+  private HiveCatalogs() {
+  }
 
   public static HiveCatalog loadCatalog(Configuration conf) {
     // metastore URI can be null in local mode

File: mr/src/main/java/org/apache/iceberg/mr/Catalogs.java
Patch:
@@ -41,7 +41,8 @@ public final class Catalogs {
   private static final String NAME = "name";
   private static final String LOCATION = "location";
 
-  private Catalogs() {}
+  private Catalogs() {
+  }
 
   /**
    * Load an Iceberg table using the catalog and table identifier (or table path) specified by the configuration.

File: mr/src/main/java/org/apache/iceberg/mr/InputFormatConfig.java
Patch:
@@ -27,7 +27,8 @@
 
 public class InputFormatConfig {
 
-  private InputFormatConfig() {}
+  private InputFormatConfig() {
+  }
 
   // configuration values for Iceberg input formats
   public static final String REUSE_CONTAINERS = "iceberg.mr.reuse.containers";

File: mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergSplit.java
Patch:
@@ -40,7 +40,8 @@ public class HiveIcebergSplit extends FileSplit implements IcebergSplitContainer
   private String tableLocation;
 
   // public no-argument constructor for deserialization
-  public HiveIcebergSplit() {}
+  public HiveIcebergSplit() {
+  }
 
   HiveIcebergSplit(IcebergSplit split, String tableLocation) {
     this.innerSplit = split;

File: mr/src/main/java/org/apache/iceberg/mr/mapred/MapredIcebergInputFormat.java
Patch:
@@ -148,14 +148,14 @@ public void close() throws IOException {
 
   private static JobContext newJobContext(JobConf job) {
     JobID jobID = Optional.ofNullable(JobID.forName(job.get(JobContext.ID)))
-                          .orElse(new JobID());
+                          .orElseGet(JobID::new);
 
     return new JobContextImpl(job, jobID);
   }
 
   private static TaskAttemptContext newTaskAttemptContext(JobConf job, Reporter reporter) {
     TaskAttemptID taskAttemptID = Optional.ofNullable(TaskAttemptID.forName(job.get(JobContext.TASK_ATTEMPT_ID)))
-                                          .orElse(new TaskAttemptID());
+                                          .orElseGet(TaskAttemptID::new);
 
     return new TaskAttemptContextImpl(job, taskAttemptID, toStatusReporter(reporter));
   }

File: mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergSplit.java
Patch:
@@ -42,7 +42,8 @@ public class IcebergSplit extends InputSplit implements org.apache.hadoop.mapred
   private transient Configuration conf;
 
   // public no-argument constructor for deserialization
-  public IcebergSplit() {}
+  public IcebergSplit() {
+  }
 
   IcebergSplit(Configuration conf, CombinedScanTask task) {
     this.task = task;

File: orc/src/main/java/org/apache/iceberg/orc/ORCSchemaUtil.java
Patch:
@@ -96,7 +96,8 @@ public TypeDescription type() {
           .put(Type.TypeID.DECIMAL, TypeDescription.Category.DECIMAL)
           .build();
 
-  private ORCSchemaUtil() {}
+  private ORCSchemaUtil() {
+  }
 
   public static TypeDescription convert(Schema schema) {
     final TypeDescription root = TypeDescription.createStruct();
@@ -267,7 +268,7 @@ private static TypeDescription buildOrcProjection(Integer fieldId, Type type, bo
           // e.g. renaming column c -> d and adding new column d
           String name = Optional.ofNullable(mapping.get(nestedField.fieldId()))
               .map(OrcField::name)
-              .orElse(nestedField.name() + "_r" + nestedField.fieldId());
+              .orElseGet(() -> nestedField.name() + "_r" + nestedField.fieldId());
           TypeDescription childType = buildOrcProjection(nestedField.fieldId(), nestedField.type(),
               isRequired && nestedField.isRequired(), mapping);
           orcType.addField(name, childType);

File: parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueReaders.java
Patch:
@@ -697,7 +697,7 @@ public final T read(T reuse) {
 
       for (int i = 0; i < readers.length; i += 1) {
         set(intermediate, i, readers[i].read(get(intermediate, i)));
-        //setters[i].set(intermediate, i, get(intermediate, i));
+        // setters[i].set(intermediate, i, get(intermediate, i));
       }
 
       return buildStruct(intermediate);

File: parquet/src/test/java/org/apache/iceberg/parquet/ParquetWritingTestUtils.java
Patch:
@@ -40,7 +40,8 @@
  */
 class ParquetWritingTestUtils {
 
-  private ParquetWritingTestUtils() {}
+  private ParquetWritingTestUtils() {
+  }
 
   static File writeRecords(TemporaryFolder temp, Schema schema, GenericData.Record... records) throws IOException {
     return writeRecords(temp, schema, Collections.emptyMap(), null, records);

File: pig/src/main/java/org/apache/iceberg/pig/IcebergPigInputFormat.java
Patch:
@@ -84,7 +84,7 @@ public List<InputSplit> getSplits(JobContext context) throws IOException {
 
     TableScan scan = table.newScan();
 
-    //Apply Filters
+    // Apply Filters
     Expression filterExpression =
         (Expression) ObjectSerializer.deserialize(context.getConfiguration().get(scope(ICEBERG_FILTER_EXPRESSION)));
     LOG.info("[{}]: iceberg filter expressions: {}", signature, filterExpression);
@@ -94,7 +94,7 @@ public List<InputSplit> getSplits(JobContext context) throws IOException {
       scan = scan.filter(filterExpression);
     }
 
-    //Wrap in Splits
+    // Wrap in Splits
     try (CloseableIterable<CombinedScanTask> tasks = scan.planTasks()) {
       tasks.forEach(scanTask -> splits.add(new IcebergSplit(scanTask)));
     }

File: pig/src/main/java/org/apache/iceberg/pig/SchemaUtil.java
Patch:
@@ -115,7 +115,7 @@ private static ResourceSchema convertComplex(Type type) throws IOException {
         if (listType.elementType().isStructType()) {
           result.setFields(elementFieldSchemas);
         } else {
-          //Wrap non-struct types in tuples
+          // Wrap non-struct types in tuples
           ResourceSchema elementSchema = new ResourceSchema();
           elementSchema.setFields(elementFieldSchemas);
 

File: spark/src/main/java/org/apache/iceberg/spark/SparkValueConverter.java
Patch:
@@ -39,7 +39,8 @@
  */
 public class SparkValueConverter {
 
-  private SparkValueConverter() {}
+  private SparkValueConverter() {
+  }
 
   public static Record convert(Schema schema, Row row) {
     return convert(schema.asStruct(), row);

File: spark/src/main/java/org/apache/iceberg/spark/data/SparkValueReaders.java
Patch:
@@ -44,7 +44,8 @@
 
 public class SparkValueReaders {
 
-  private SparkValueReaders() {}
+  private SparkValueReaders() {
+  }
 
   static ValueReader<UTF8String> strings() {
     return StringReader.INSTANCE;

File: spark/src/main/java/org/apache/iceberg/spark/data/SparkValueWriters.java
Patch:
@@ -39,7 +39,8 @@
 
 public class SparkValueWriters {
 
-  private SparkValueWriters() {}
+  private SparkValueWriters() {
+  }
 
   static ValueWriter<UTF8String> strings() {
     return StringWriter.INSTANCE;

File: spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ArrowVectorAccessors.java
Patch:
@@ -49,7 +49,8 @@
 
 public class ArrowVectorAccessors {
 
-  private ArrowVectorAccessors() {}
+  private ArrowVectorAccessors() {
+  }
 
   static ArrowVectorAccessor getVectorAccessor(VectorHolder holder) {
     Dictionary dictionary = holder.dictionary();

File: spark/src/test/java/org/apache/iceberg/actions/TestExpireSnapshotsAction.java
Patch:
@@ -554,13 +554,13 @@ public void testWithExpiringDanglingStageCommit() {
     // ManifestList should be deleted too
     expectedDeletes.add(snapshotB.manifestListLocation());
     snapshotB.dataManifests().forEach(file -> {
-      //Only the manifest of B should be deleted.
+      // Only the manifest of B should be deleted.
       if (file.snapshotId() == snapshotB.snapshotId()) {
         expectedDeletes.add(file.path());
       }
     });
     Assert.assertSame("Files deleted count should be expected", expectedDeletes.size(), deletedFiles.size());
-    //Take the diff
+    // Take the diff
     expectedDeletes.removeAll(deletedFiles);
     Assert.assertTrue("Exactly same files should be deleted", expectedDeletes.isEmpty());
   }

File: spark/src/test/java/org/apache/iceberg/spark/data/AvroDataTest.java
Patch:
@@ -51,7 +51,7 @@ public abstract class AvroDataTest {
       optional(107, "date", Types.DateType.get()),
       required(108, "ts", Types.TimestampType.withZone()),
       required(110, "s", Types.StringType.get()),
-      //required(111, "uuid", Types.UUIDType.get()),
+      // required(111, "uuid", Types.UUIDType.get()),
       required(112, "fixed", Types.FixedType.ofLength(7)),
       optional(113, "bytes", Types.BinaryType.get()),
       required(114, "dec_9_0", Types.DecimalType.of(9, 0)),

File: spark/src/test/java/org/apache/iceberg/spark/data/RandomData.java
Patch:
@@ -53,7 +53,8 @@ public class RandomData {
   // Default percentage of number of values that are null for optional fields
   public static final float DEFAULT_NULL_PERCENTAGE = 0.05f;
 
-  private RandomData() {}
+  private RandomData() {
+  }
 
   public static List<Record> generateList(Schema schema, int numRecords, long seed) {
     RandomDataGenerator generator = new RandomDataGenerator(schema, seed, DEFAULT_NULL_PERCENTAGE);

File: spark/src/test/java/org/apache/iceberg/spark/data/TestHelpers.java
Patch:
@@ -69,7 +69,8 @@
 @SuppressWarnings("checkstyle:OverloadMethodsDeclarationOrder")
 public class TestHelpers {
 
-  private TestHelpers() {}
+  private TestHelpers() {
+  }
 
   public static void assertEqualsSafe(Types.StructType struct, Record rec, Row row) {
     List<Types.NestedField> fields = struct.fields();

File: spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetReadMetadataColumns.java
Patch:
@@ -109,7 +109,7 @@ public class TestSparkParquetReadMetadataColumns {
   public static Object[][] parameters() {
     return new Object[][] {
         new Object[] { false },
-        //new Object[] { true }
+        // new Object[] { true }
     };
   }
 

File: spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetDictionaryFallbackToPlainEncodingVectorizedReads.java
Patch:
@@ -41,7 +41,7 @@ protected int getNumRows() {
 
   @Override
   Iterable<GenericData.Record> generateData(Schema schema, int numRecords, long seed, float nullPercentage) {
-    //TODO: take into account nullPercentage when generating fallback encoding data
+    // TODO: take into account nullPercentage when generating fallback encoding data
     return RandomData.generateFallbackData(schema, numRecords, seed, numRecords / 20);
   }
 

File: spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDataWrite.java
Patch:
@@ -128,7 +128,7 @@ public void testBasicWrite() throws IOException {
           Assert.assertNotNull("Split offsets not present", file.splitOffsets());
         }
         Assert.assertEquals("Should have reported record count as 1", 1, file.recordCount());
-        //TODO: append more metric info
+        // TODO: append more metric info
         if (format.equals(FileFormat.PARQUET)) {
           Assert.assertNotNull("Column sizes metric not present", file.columnSizes());
           Assert.assertNotNull("Counts metric not present", file.valueCounts());

File: spark2/src/jmh/java/org/apache/iceberg/spark/SparkBenchmarkUtil.java
Patch:
@@ -32,7 +32,8 @@
 
 public class SparkBenchmarkUtil {
 
-  private SparkBenchmarkUtil() {}
+  private SparkBenchmarkUtil() {
+  }
 
   public static UnsafeProjection projection(Schema expectedSchema, Schema actualSchema) {
     StructType struct = SparkSchemaUtil.convert(actualSchema);

File: spark2/src/main/java/org/apache/iceberg/spark/source/Writer.java
Patch:
@@ -118,7 +118,7 @@ class Writer implements DataSourceWriter {
   private FileFormat getFileFormat(Map<String, String> tableProperties, DataSourceOptions options) {
     Optional<String> formatOption = options.get("write-format");
     String formatString = formatOption
-        .orElse(tableProperties.getOrDefault(DEFAULT_FILE_FORMAT, DEFAULT_FILE_FORMAT_DEFAULT));
+        .orElseGet(() -> tableProperties.getOrDefault(DEFAULT_FILE_FORMAT, DEFAULT_FILE_FORMAT_DEFAULT));
     return FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));
   }
 

File: spark2/src/test/java/org/apache/iceberg/examples/SnapshotFunctionalityTest.java
Patch:
@@ -124,7 +124,7 @@ public void retireAllSnapshotsOlderThanTimestamp() {
     Iterator<Snapshot> beforeIterator = table.snapshots().iterator();
     List<Snapshot> beforeSnapshots = IteratorUtils.toList(beforeIterator);
 
-    //Delete the 2 oldest snapshots
+    // Delete the 2 oldest snapshots
     table.expireSnapshots().expireOlderThan(secondLatestTimestamp).commit();
     table.refresh();
 

File: spark3/src/main/java/org/apache/iceberg/spark/source/SparkBatchWrite.java
Patch:
@@ -120,7 +120,7 @@ class SparkBatchWrite implements BatchWrite {
   protected FileFormat getFileFormat(Map<String, String> tableProperties, Map<String, String> options) {
     Optional<String> formatOption = Optional.ofNullable(options.get("write-format"));
     String formatString = formatOption
-        .orElse(tableProperties.getOrDefault(DEFAULT_FILE_FORMAT, DEFAULT_FILE_FORMAT_DEFAULT));
+        .orElseGet(() -> tableProperties.getOrDefault(DEFAULT_FILE_FORMAT, DEFAULT_FILE_FORMAT_DEFAULT));
     return FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));
   }
 

File: api/src/main/java/org/apache/iceberg/PartitionSpec.java
Patch:
@@ -222,7 +222,7 @@ public boolean equals(Object other) {
 
   @Override
   public int hashCode() {
-    return Integer.hashCode(Arrays.hashCode(fields));
+    return 31 * Integer.hashCode(specId) + Arrays.hashCode(fields);
   }
 
   private List<PartitionField> lazyFieldList() {

File: core/src/main/java/org/apache/iceberg/BaseReplacePartitions.java
Patch:
@@ -42,7 +42,7 @@ protected String operation() {
 
   @Override
   public ReplacePartitions addFile(DataFile file) {
-    dropPartition(file.partition());
+    dropPartition(file.specId(), file.partition());
     add(file);
     return this;
   }

File: core/src/main/java/org/apache/iceberg/ManifestGroup.java
Patch:
@@ -172,10 +172,10 @@ public CloseableIterable<FileScanTask> planFiles() {
       ResidualEvaluator residuals = residualCache.get(specId);
       if (dropStats) {
         return CloseableIterable.transform(entries, e -> new BaseFileScanTask(
-            e.file().copyWithoutStats(), deleteFiles.forEntry(specId, e), schemaString, specString, residuals));
+            e.file().copyWithoutStats(), deleteFiles.forEntry(e), schemaString, specString, residuals));
       } else {
         return CloseableIterable.transform(entries, e -> new BaseFileScanTask(
-            e.file().copy(), deleteFiles.forEntry(specId, e), schemaString, specString, residuals));
+            e.file().copy(), deleteFiles.forEntry(e), schemaString, specString, residuals));
       }
     });
 

File: data/src/main/java/org/apache/iceberg/data/parquet/BaseParquetReaders.java
Patch:
@@ -356,22 +356,22 @@ public LocalDateTime read(LocalDateTime reuse) {
     }
   }
 
-  private static class TimestampInt96Reader extends ParquetValueReaders.PrimitiveReader<LocalDateTime> {
+  private static class TimestampInt96Reader extends ParquetValueReaders.PrimitiveReader<OffsetDateTime> {
     private static final long UNIX_EPOCH_JULIAN = 2_440_588L;
 
     private TimestampInt96Reader(ColumnDescriptor desc) {
       super(desc);
     }
 
     @Override
-    public LocalDateTime read(LocalDateTime reuse) {
+    public OffsetDateTime read(OffsetDateTime reuse) {
       final ByteBuffer byteBuffer = column.nextBinary().toByteBuffer().order(ByteOrder.LITTLE_ENDIAN);
       final long timeOfDayNanos = byteBuffer.getLong();
       final int julianDay = byteBuffer.getInt();
 
       return Instant
               .ofEpochMilli(TimeUnit.DAYS.toMillis(julianDay - UNIX_EPOCH_JULIAN))
-              .plusNanos(timeOfDayNanos).atOffset(ZoneOffset.UTC).toLocalDateTime();
+              .plusNanos(timeOfDayNanos).atOffset(ZoneOffset.UTC);
     }
   }
 

File: parquet/src/main/java/org/apache/iceberg/parquet/MessageTypeToType.java
Patch:
@@ -156,6 +156,8 @@ public Type primitive(PrimitiveType primitive) {
         return Types.DoubleType.get();
       case FIXED_LEN_BYTE_ARRAY:
         return Types.FixedType.ofLength(primitive.getTypeLength());
+      case INT96:
+        return Types.TimestampType.withZone();
       case BINARY:
         return Types.BinaryType.get();
     }

File: spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java
Patch:
@@ -247,7 +247,7 @@ private Map<StructLikeWrapper, Collection<FileScanTask>> groupTasksByPartition(
 
     try {
       tasksIter.forEachRemaining(task -> {
-        StructLikeWrapper structLike = StructLikeWrapper.wrap(task.file().partition());
+        StructLikeWrapper structLike = StructLikeWrapper.forType(spec.partitionType()).set(task.file().partition());
         tasksGroupedByPartition.put(structLike, task);
       });
 

File: data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriter.java
Patch:
@@ -103,7 +103,7 @@ public OrcValueWriter<?> primitive(Type.PrimitiveType iPrimitive, TypeDescriptio
           return GenericOrcWriters.byteBuffers();
         case DECIMAL:
           Types.DecimalType decimalType = (Types.DecimalType) iPrimitive;
-          return GenericOrcWriters.decimal(decimalType.scale(), decimalType.precision());
+          return GenericOrcWriters.decimal(decimalType.precision(), decimalType.scale());
         default:
           throw new IllegalArgumentException(String.format("Invalid iceberg type %s corresponding to ORC type %s",
               iPrimitive, primitive));

File: spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueWriters.java
Patch:
@@ -183,9 +183,9 @@ private static class TimestampTzWriter implements SparkOrcValueWriter {
     @Override
     public void nonNullWrite(int rowId, int column, SpecializedGetters data, ColumnVector output) {
       TimestampColumnVector cv = (TimestampColumnVector) output;
-      long micros = data.getLong(column);
-      cv.time[rowId] = micros / 1_000; // millis
-      cv.nanos[rowId] = (int) (micros % 1_000_000) * 1_000; // nanos
+      long micros = data.getLong(column); // it could be negative.
+      cv.time[rowId] = Math.floorDiv(micros, 1_000); // millis
+      cv.nanos[rowId] = (int) (Math.floorMod(micros, 1_000_000)) * 1_000; // nanos
     }
   }
 

File: spark/src/test/java/org/apache/iceberg/spark/data/GenericsHelpers.java
Patch:
@@ -237,7 +237,7 @@ private static void assertEqualsUnsafe(Type type, Object expected, Object actual
         break;
       case DATE:
         Assert.assertTrue("Should expect a LocalDate", expected instanceof LocalDate);
-        long expectedDays = ChronoUnit.DAYS.between(EPOCH_DAY, (LocalDate) expected);
+        int expectedDays = (int) ChronoUnit.DAYS.between(EPOCH_DAY, (LocalDate) expected);
         Assert.assertEquals("Primitive value should be equal to expected", expectedDays, actual);
         break;
       case TIMESTAMP:

File: spark/src/main/java/org/apache/iceberg/spark/source/SparkAppenderFactory.java
Patch:
@@ -72,7 +72,7 @@ public FileAppender<InternalRow> newAppender(OutputFile file, FileFormat fileFor
 
         case ORC:
           return ORC.write(file)
-              .createWriterFunc((schema, typeDesc) -> new SparkOrcWriter(typeDesc))
+              .createWriterFunc(SparkOrcWriter::new)
               .setAll(properties)
               .schema(writeSchema)
               .overwrite()

File: spark/src/test/java/org/apache/iceberg/spark/data/TestOrcWrite.java
Patch:
@@ -50,7 +50,7 @@ public void splitOffsets() throws IOException {
 
     Iterable<InternalRow> rows = RandomData.generateSpark(SCHEMA, 1, 0L);
     FileAppender<InternalRow> writer = ORC.write(Files.localOutput(testFile))
-        .createWriterFunc((schema, typeDesc) -> new SparkOrcWriter(typeDesc))
+        .createWriterFunc(SparkOrcWriter::new)
         .schema(SCHEMA)
         .build();
 

File: spark/src/test/java/org/apache/iceberg/spark/data/TestSparkOrcReadMetadataColumns.java
Patch:
@@ -119,7 +119,7 @@ public void writeFile() throws IOException {
     Assert.assertTrue("Delete should succeed", testFile.delete());
 
     try (FileAppender<InternalRow> writer = ORC.write(Files.localOutput(testFile))
-        .createWriterFunc((icebergSchema, typeDesc) -> new SparkOrcWriter(typeDesc))
+        .createWriterFunc(SparkOrcWriter::new)
         .schema(DATA_SCHEMA)
         // write in such a way that the file contains 10 stripes each with 100 rows
         .config("iceberg.orc.vectorbatch.size", "100")

File: spark/src/test/java/org/apache/iceberg/spark/data/TestSparkOrcReader.java
Patch:
@@ -67,7 +67,7 @@ private void writeAndValidateRecords(Schema schema, Iterable<InternalRow> expect
     Assert.assertTrue("Delete should succeed", testFile.delete());
 
     try (FileAppender<InternalRow> writer = ORC.write(Files.localOutput(testFile))
-        .createWriterFunc((icebergSchema, typeDesc) -> new SparkOrcWriter(typeDesc))
+        .createWriterFunc(SparkOrcWriter::new)
         .schema(schema)
         .build()) {
       writer.addAll(expected);

File: orc/src/main/java/org/apache/iceberg/orc/ORC.java
Patch:
@@ -23,6 +23,7 @@
 import java.nio.charset.StandardCharsets;
 import java.util.HashMap;
 import java.util.Map;
+import java.util.function.BiFunction;
 import java.util.function.Function;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
@@ -59,7 +60,7 @@ public static class WriteBuilder {
     private final OutputFile file;
     private final Configuration conf;
     private Schema schema = null;
-    private Function<TypeDescription, OrcValueWriter<?>>  createWriterFunc;
+    private BiFunction<Schema, TypeDescription, OrcRowWriter<?>> createWriterFunc;
     private Map<String, byte[]> metadata = new HashMap<>();
 
     private WriteBuilder(OutputFile file) {
@@ -81,7 +82,7 @@ public WriteBuilder config(String property, String value) {
       return this;
     }
 
-    public WriteBuilder createWriterFunc(Function<TypeDescription, OrcValueWriter<?>> writerFunction) {
+    public WriteBuilder createWriterFunc(BiFunction<Schema, TypeDescription, OrcRowWriter<?>> writerFunction) {
       this.createWriterFunc = writerFunction;
       return this;
     }

File: spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcWriter.java
Patch:
@@ -20,7 +20,7 @@
 package org.apache.iceberg.spark.data;
 
 import java.util.List;
-import org.apache.iceberg.orc.OrcValueWriter;
+import org.apache.iceberg.orc.OrcRowWriter;
 import org.apache.orc.TypeDescription;
 import org.apache.orc.storage.common.type.HiveDecimal;
 import org.apache.orc.storage.ql.exec.vector.BytesColumnVector;
@@ -42,7 +42,7 @@
  * This class acts as an adaptor from an OrcFileAppender to a
  * FileAppender&lt;InternalRow&gt;.
  */
-public class SparkOrcWriter implements OrcValueWriter<InternalRow> {
+public class SparkOrcWriter implements OrcRowWriter<InternalRow> {
 
   private final Converter[] converters;
 

File: spark/src/main/java/org/apache/iceberg/spark/source/SparkAppenderFactory.java
Patch:
@@ -70,7 +70,7 @@ public FileAppender<InternalRow> newAppender(OutputFile file, FileFormat fileFor
 
         case ORC:
           return ORC.write(file)
-              .createWriterFunc(SparkOrcWriter::new)
+              .createWriterFunc((schema, typeDesc) -> new SparkOrcWriter(typeDesc))
               .setAll(properties)
               .schema(writeSchema)
               .overwrite()

File: spark/src/test/java/org/apache/iceberg/spark/data/TestOrcWrite.java
Patch:
@@ -50,7 +50,7 @@ public void splitOffsets() throws IOException {
 
     Iterable<InternalRow> rows = RandomData.generateSpark(SCHEMA, 1, 0L);
     FileAppender<InternalRow> writer = ORC.write(Files.localOutput(testFile))
-        .createWriterFunc(SparkOrcWriter::new)
+        .createWriterFunc((schema, typeDesc) -> new SparkOrcWriter(typeDesc))
         .schema(SCHEMA)
         .build();
 

File: spark/src/test/java/org/apache/iceberg/spark/data/TestSparkOrcReadMetadataColumns.java
Patch:
@@ -119,7 +119,7 @@ public void writeFile() throws IOException {
     Assert.assertTrue("Delete should succeed", testFile.delete());
 
     try (FileAppender<InternalRow> writer = ORC.write(Files.localOutput(testFile))
-        .createWriterFunc(SparkOrcWriter::new)
+        .createWriterFunc((icebergSchema, typeDesc) -> new SparkOrcWriter(typeDesc))
         .schema(DATA_SCHEMA)
         // write in such a way that the file contains 10 stripes each with 100 rows
         .config("iceberg.orc.vectorbatch.size", "100")

File: spark/src/test/java/org/apache/iceberg/spark/data/TestSparkOrcReader.java
Patch:
@@ -67,7 +67,7 @@ private void writeAndValidateRecords(Schema schema, Iterable<InternalRow> expect
     Assert.assertTrue("Delete should succeed", testFile.delete());
 
     try (FileAppender<InternalRow> writer = ORC.write(Files.localOutput(testFile))
-        .createWriterFunc(SparkOrcWriter::new)
+        .createWriterFunc((icebergSchema, typeDesc) -> new SparkOrcWriter(typeDesc))
         .schema(schema)
         .build()) {
       writer.addAll(expected);

File: hive/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java
Patch:
@@ -144,6 +144,7 @@ protected void doCommit(TableMetadata base, TableMetadata metadata) {
       Table tbl;
       if (base != null) {
         tbl = metaClients.run(client -> client.getTable(database, tableName));
+        tbl.setSd(storageDescriptor(metadata)); // set to pickup any schema changes
       } else {
         final long currentTimeMillis = System.currentTimeMillis();
         tbl = new Table(tableName,
@@ -161,7 +162,6 @@ protected void doCommit(TableMetadata base, TableMetadata metadata) {
         tbl.getParameters().put("EXTERNAL", "TRUE"); // using the external table type also requires this
       }
 
-      tbl.setSd(storageDescriptor(metadata)); // set to pickup any schema changes
       String metadataLocation = tbl.getParameters().get(METADATA_LOCATION_PROP);
       String baseMetadataLocation = base != null ? base.metadataFileLocation() : null;
       if (!Objects.equals(baseMetadataLocation, metadataLocation)) {

File: spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java
Patch:
@@ -37,6 +37,7 @@
 import org.apache.iceberg.io.InputFile;
 import org.apache.iceberg.relocated.com.google.common.base.Joiner;
 import org.apache.iceberg.relocated.com.google.common.base.Preconditions;
+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;
 import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;
 import org.apache.iceberg.transforms.PartitionSpecVisitor;
 import org.apache.iceberg.types.Type;
@@ -52,7 +53,6 @@
 import org.apache.spark.sql.types.IntegerType;
 import org.apache.spark.sql.types.LongType;
 import org.apache.spark.sql.util.CaseInsensitiveStringMap;
-import org.sparkproject.guava.collect.ImmutableMap;
 
 public class Spark3Util {
 

File: spark3/src/main/java/org/apache/iceberg/spark/SparkCatalog.java
Patch:
@@ -39,6 +39,8 @@
 import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;
 import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;
 import org.apache.iceberg.relocated.com.google.common.collect.Lists;
+import org.apache.iceberg.relocated.com.google.common.collect.Maps;
+import org.apache.iceberg.relocated.com.google.common.collect.Sets;
 import org.apache.iceberg.spark.source.SparkTable;
 import org.apache.iceberg.spark.source.StagedSparkTable;
 import org.apache.spark.sql.SparkSession;
@@ -58,8 +60,6 @@
 import org.apache.spark.sql.connector.expressions.Transform;
 import org.apache.spark.sql.types.StructType;
 import org.apache.spark.sql.util.CaseInsensitiveStringMap;
-import org.glassfish.jersey.internal.guava.Sets;
-import org.sparkproject.guava.collect.Maps;
 
 /**
  * A Spark TableCatalog implementation that wraps an Iceberg {@link Catalog}.

File: mr/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergInputFormat.java
Patch:
@@ -134,7 +134,7 @@ public void testScanTable() {
     // Adding the ORDER BY clause will cause Hive to spawn a local MR job this time.
     List<Object[]> descRows = shell.executeStatement("SELECT * FROM default.customers ORDER BY customer_id DESC");
 
-    Assert.assertEquals(3, rows.size());
+    Assert.assertEquals(3, descRows.size());
     Assert.assertArrayEquals(new Object[] {2L, "Trudy"}, descRows.get(0));
     Assert.assertArrayEquals(new Object[] {1L, "Bob"}, descRows.get(1));
     Assert.assertArrayEquals(new Object[] {0L, "Alice"}, descRows.get(2));

File: spark/src/main/java/org/apache/iceberg/spark/SparkTableUtil.java
Patch:
@@ -49,6 +49,7 @@
 import org.apache.iceberg.hadoop.HadoopFileIO;
 import org.apache.iceberg.hadoop.HadoopInputFile;
 import org.apache.iceberg.hadoop.SerializableConfiguration;
+import org.apache.iceberg.hadoop.Util;
 import org.apache.iceberg.io.FileIO;
 import org.apache.iceberg.io.OutputFile;
 import org.apache.iceberg.orc.OrcMetrics;
@@ -386,7 +387,7 @@ private static SparkPartition toSparkPartition(CatalogTablePartition partition,
     Preconditions.checkArgument(serde.nonEmpty() || table.provider().nonEmpty(),
         "Partition format should be defined");
 
-    String uri = String.valueOf(locationUri.get());
+    String uri = Util.uriToString(locationUri.get());
     String format = serde.nonEmpty() ? serde.get() : table.provider().get();
 
     Map<String, String> partitionSpec = JavaConverters.mapAsJavaMapConverter(partition.spec()).asJava();
@@ -495,7 +496,7 @@ private static void importUnpartitionedSparkTable(
       MetricsConfig metricsConfig = MetricsConfig.fromProperties(targetTable.properties());
 
       List<DataFile> files = listPartition(
-          partition, sourceTable.location().toString(), format.get(), spec, conf, metricsConfig);
+          partition, Util.uriToString(sourceTable.location()), format.get(), spec, conf, metricsConfig);
 
       AppendFiles append = targetTable.newAppend();
       files.forEach(append::appendFile);

File: core/src/main/java/org/apache/iceberg/AllEntriesTable.java
Patch:
@@ -28,6 +28,7 @@
 import org.apache.iceberg.io.CloseableIterable;
 import org.apache.iceberg.relocated.com.google.common.collect.Iterables;
 import org.apache.iceberg.relocated.com.google.common.collect.Sets;
+import org.apache.iceberg.types.Type;
 import org.apache.iceberg.types.TypeUtil;
 import org.apache.iceberg.util.ParallelIterable;
 import org.apache.iceberg.util.ThreadPools;
@@ -100,7 +101,8 @@ protected CloseableIterable<FileScanTask> planFiles(
         TableOperations ops, Snapshot snapshot, Expression rowFilter,
         boolean ignoreResiduals, boolean caseSensitive, boolean colStats) {
       CloseableIterable<ManifestFile> manifests = allManifestFiles(ops.current().snapshots());
-      Schema fileSchema = new Schema(schema().findType("data_file").asStructType().fields());
+      Type fileProjection = schema().findType("data_file");
+      Schema fileSchema = fileProjection != null ? new Schema(fileProjection.asStructType().fields()) : new Schema();
       String schemaString = SchemaParser.toJson(schema());
       String specString = PartitionSpecParser.toJson(PartitionSpec.unpartitioned());
       Expression filter = ignoreResiduals ? Expressions.alwaysTrue() : rowFilter;

File: core/src/main/java/org/apache/iceberg/ManifestEntriesTable.java
Patch:
@@ -26,6 +26,7 @@
 import org.apache.iceberg.io.FileIO;
 import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;
 import org.apache.iceberg.relocated.com.google.common.collect.Sets;
+import org.apache.iceberg.types.Type;
 import org.apache.iceberg.types.TypeUtil;
 
 /**
@@ -97,7 +98,8 @@ protected CloseableIterable<FileScanTask> planFiles(
         boolean ignoreResiduals, boolean caseSensitive, boolean colStats) {
       // return entries from both data and delete manifests
       CloseableIterable<ManifestFile> manifests = CloseableIterable.withNoopClose(snapshot.allManifests());
-      Schema fileSchema = new Schema(schema().findType("data_file").asStructType().fields());
+      Type fileProjection = schema().findType("data_file");
+      Schema fileSchema = fileProjection != null ? new Schema(fileProjection.asStructType().fields()) : new Schema();
       String schemaString = SchemaParser.toJson(schema());
       String specString = PartitionSpecParser.toJson(PartitionSpec.unpartitioned());
       Expression filter = ignoreResiduals ? Expressions.alwaysTrue() : rowFilter;

File: core/src/main/java/org/apache/iceberg/BaseMetastoreCatalog.java
Patch:
@@ -282,7 +282,7 @@ private static void deleteFiles(FileIO io, Set<ManifestFile> allManifests) {
         });
   }
 
-  private static String fullTableName(String catalogName, TableIdentifier identifier) {
+  protected static String fullTableName(String catalogName, TableIdentifier identifier) {
     StringBuilder sb = new StringBuilder();
 
     if (catalogName.contains("/") || catalogName.contains(":")) {

File: spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java
Patch:
@@ -57,7 +57,6 @@ public boolean supportsExternalMetadata() {
 
   @Override
   public SparkTable getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {
-    // TODO: if partitioning is non-null, the table is being created?
     // Get Iceberg table from options
     Configuration conf = new Configuration(SparkSession.active().sparkContext().hadoopConfiguration());
     Table icebergTable = getTableAndResolveHadoopConfiguration(options, conf);

File: data/src/test/java/org/apache/iceberg/data/TestLocalScan.java
Patch:
@@ -456,6 +456,8 @@ private DataFile writeFile(String location, String filename, Schema schema, List
 
   @Test
   public void testFilterWithDateAndTimestamp() throws IOException {
+    // TODO: Add multiple timestamp tests - there's an issue with ORC caching TZ in ThreadLocal, so it's not possible
+    //   to change TZ and test with ORC as they will produce incompatible values.
     Schema schema = new Schema(
         required(1, "timestamp_with_zone", Types.TimestampType.withZone()),
         required(2, "timestamp_without_zone", Types.TimestampType.withoutZone()),

File: core/src/main/java/org/apache/iceberg/BaseReplacePartitions.java
Patch:
@@ -27,6 +27,7 @@ public class BaseReplacePartitions
     extends MergingSnapshotProducer<ReplacePartitions> implements ReplacePartitions {
   BaseReplacePartitions(String tableName, TableOperations ops) {
     super(tableName, ops);
+    set(SnapshotSummary.REPLACE_PARTITIONS_PROP, "true");
   }
 
   @Override

File: core/src/main/java/org/apache/iceberg/SnapshotSummary.java
Patch:
@@ -37,6 +37,7 @@ public class SnapshotSummary {
   public static final String STAGED_WAP_ID_PROP = "wap.id";
   public static final String PUBLISHED_WAP_ID_PROP = "published-wap-id";
   public static final String SOURCE_SNAPSHOT_ID_PROP = "source-snapshot-id";
+  public static final String REPLACE_PARTITIONS_PROP = "replace-partitions";
 
   private SnapshotSummary() {
   }

File: data/src/test/java/org/apache/iceberg/data/TestLocalScan.java
Patch:
@@ -56,7 +56,6 @@
 import org.apache.iceberg.relocated.com.google.common.collect.Sets;
 import org.apache.iceberg.types.Types;
 import org.junit.Assert;
-import org.junit.Assume;
 import org.junit.Before;
 import org.junit.Rule;
 import org.junit.Test;
@@ -457,7 +456,6 @@ private DataFile writeFile(String location, String filename, Schema schema, List
 
   @Test
   public void testFilterWithDateAndTimestamp() throws IOException {
-    Assume.assumeFalse(format == FileFormat.ORC);
     Schema schema = new Schema(
         required(1, "timestamp_with_zone", Types.TimestampType.withZone()),
         required(2, "timestamp_without_zone", Types.TimestampType.withoutZone()),

File: api/src/main/java/org/apache/iceberg/Accessors.java
Patch:
@@ -178,7 +178,7 @@ private static Accessor<StructLike> newAccessor(int pos, boolean isOptional,
     if (isOptional) {
       // the wrapped position handles null layers
       return new WrappedPositionAccessor(pos, accessor);
-    } else if (accessor instanceof PositionAccessor) {
+    } else if (accessor.getClass() == PositionAccessor.class) {
       return new Position2Accessor(pos, (PositionAccessor) accessor);
     } else if (accessor instanceof Position2Accessor) {
       return new Position3Accessor(pos, (Position2Accessor) accessor);

File: spark/src/main/java/org/apache/iceberg/spark/source/PartitionKey.java
Patch:
@@ -182,7 +182,7 @@ private static Accessor<InternalRow> newAccessor(int position, boolean isOptiona
     if (isOptional) {
       // the wrapped position handles null layers
       return new WrappedPositionAccessor(position, size, accessor);
-    } else if (accessor instanceof PositionAccessor) {
+    } else if (accessor.getClass() == PositionAccessor.class) {
       return new Position2Accessor(position, size, (PositionAccessor) accessor);
     } else if (accessor instanceof Position2Accessor) {
       return new Position3Accessor(position, size, (Position2Accessor) accessor);

File: core/src/main/java/org/apache/iceberg/ManifestListWriter.java
Patch:
@@ -19,6 +19,7 @@
 
 package org.apache.iceberg;
 
+import com.google.common.base.Preconditions;
 import com.google.common.collect.ImmutableMap;
 import java.io.IOException;
 import java.util.Iterator;
@@ -114,6 +115,8 @@ static class V1Writer extends ManifestListWriter {
 
     @Override
     protected ManifestFile prepare(ManifestFile manifest) {
+      Preconditions.checkArgument(manifest.content() == ManifestContent.DATA,
+          "Cannot store delete manifests in a v1 table");
       return wrapper.wrap(manifest);
     }
 

File: core/src/main/java/org/apache/iceberg/ManifestWriter.java
Patch:
@@ -164,7 +164,8 @@ public ManifestFile toManifestFile() {
     // if the minSequenceNumber is null, then no manifests with a sequence number have been written, so the min
     // sequence number is the one that will be assigned when this is committed. pass UNASSIGNED_SEQ to inherit it.
     long minSeqNumber = minSequenceNumber != null ? minSequenceNumber : UNASSIGNED_SEQ;
-    return new GenericManifestFile(file.location(), writer.length(), specId, UNASSIGNED_SEQ, minSeqNumber, snapshotId,
+    return new GenericManifestFile(file.location(), writer.length(), specId, ManifestContent.DATA,
+        UNASSIGNED_SEQ, minSeqNumber, snapshotId,
         addedFiles, addedRows, existingFiles, existingRows, deletedFiles, deletedRows, stats.summaries());
   }
 

File: core/src/main/java/org/apache/iceberg/SnapshotProducer.java
Patch:
@@ -393,8 +393,8 @@ private static ManifestFile addMetadata(TableOperations ops, ManifestFile manife
       }
 
       return new GenericManifestFile(manifest.path(), manifest.length(), manifest.partitionSpecId(),
-          manifest.sequenceNumber(), manifest.minSequenceNumber(), snapshotId, addedFiles, addedRows, existingFiles,
-          existingRows, deletedFiles, deletedRows, stats.summaries());
+          ManifestContent.DATA, manifest.sequenceNumber(), manifest.minSequenceNumber(), snapshotId,
+          addedFiles, addedRows, existingFiles, existingRows, deletedFiles, deletedRows, stats.summaries());
 
     } catch (IOException e) {
       throw new RuntimeIOException(e, "Failed to read manifest: %s", manifest.path());

File: core/src/main/java/org/apache/iceberg/avro/GenericAvroReader.java
Patch:
@@ -158,6 +158,9 @@ public ValueReader<?> primitive(Schema primitive) {
             // Spark uses the same representation
             return ValueReaders.ints();
 
+          case "time-micros":
+            return ValueReaders.longs();
+
           case "timestamp-millis":
             // adjust to microseconds
             ValueReader<Long> longs = ValueReaders.longs();

File: core/src/main/java/org/apache/iceberg/avro/GenericAvroWriter.java
Patch:
@@ -91,6 +91,9 @@ public ValueWriter<?> primitive(Schema primitive) {
           case "date":
             return ValueWriters.ints();
 
+          case "time-micros":
+            return ValueWriters.longs();
+
           case "timestamp-micros":
             return ValueWriters.longs();
 

File: core/src/test/java/org/apache/iceberg/avro/AvroDataTest.java
Patch:
@@ -55,7 +55,8 @@ public abstract class AvroDataTest {
       optional(113, "bytes", Types.BinaryType.get()),
       required(114, "dec_9_0", Types.DecimalType.of(9, 0)),
       required(115, "dec_11_2", Types.DecimalType.of(11, 2)),
-      required(116, "dec_38_10", Types.DecimalType.of(38, 10)) // maximum precision
+      required(116, "dec_38_10", Types.DecimalType.of(38, 10)), // maximum precision
+      required(117, "time", Types.TimeType.get())
   );
 
   @Rule

File: data/src/test/java/org/apache/iceberg/data/DataTest.java
Patch:
@@ -48,7 +48,8 @@ public abstract class DataTest {
       optional(105, "f", Types.FloatType.get()),
       required(106, "d", Types.DoubleType.get()),
       optional(107, "date", Types.DateType.get()),
-      required(108, "ts", Types.TimestampType.withZone()),
+      required(108, "ts_tz", Types.TimestampType.withZone()),
+      required(109, "ts", Types.TimestampType.withoutZone()),
       required(110, "s", Types.StringType.get()),
       required(112, "fixed", Types.FixedType.ofLength(7)),
       optional(113, "bytes", Types.BinaryType.get()),

File: core/src/main/java/org/apache/iceberg/avro/AvroSchemaUtil.java
Patch:
@@ -119,7 +119,7 @@ public static boolean isTimestamptz(Schema schema) {
     return false;
   }
 
-  static boolean isOptionSchema(Schema schema) {
+  public static boolean isOptionSchema(Schema schema) {
     if (schema.getType() == UNION && schema.getTypes().size() == 2) {
       if (schema.getTypes().get(0).getType() == Schema.Type.NULL) {
         return true;
@@ -162,7 +162,7 @@ static Schema fromOptions(List<Schema> options) {
     }
   }
 
-  static boolean isKeyValueSchema(Schema schema) {
+  public static boolean isKeyValueSchema(Schema schema) {
     return schema.getType() == RECORD && schema.getFields().size() == 2;
   }
 

File: spark/src/jmh/java/org/apache/iceberg/spark/data/parquet/SparkParquetWritersFlatDataBenchmark.java
Patch:
@@ -95,7 +95,7 @@ public void tearDownBenchmark() {
   @Threads(1)
   public void writeUsingIcebergWriter() throws IOException {
     try (FileAppender<InternalRow> writer = Parquet.write(Files.localOutput(dataFile))
-        .createWriterFunc(msgType -> SparkParquetWriters.buildWriter(SCHEMA, msgType))
+        .createWriterFunc(msgType -> SparkParquetWriters.buildWriter(SparkSchemaUtil.convert(SCHEMA), msgType))
         .schema(SCHEMA)
         .build()) {
 

File: spark/src/jmh/java/org/apache/iceberg/spark/data/parquet/SparkParquetWritersNestedDataBenchmark.java
Patch:
@@ -94,7 +94,7 @@ public void tearDownBenchmark() {
   @Threads(1)
   public void writeUsingIcebergWriter() throws IOException {
     try (FileAppender<InternalRow> writer = Parquet.write(Files.localOutput(dataFile))
-        .createWriterFunc(msgType -> SparkParquetWriters.buildWriter(SCHEMA, msgType))
+        .createWriterFunc(msgType -> SparkParquetWriters.buildWriter(SparkSchemaUtil.convert(SCHEMA), msgType))
         .schema(SCHEMA)
         .build()) {
 

File: spark/src/main/java/org/apache/iceberg/spark/source/StreamingWriter.java
Patch:
@@ -35,6 +35,7 @@
 import org.apache.spark.sql.sources.v2.writer.WriterCommitMessage;
 import org.apache.spark.sql.sources.v2.writer.streaming.StreamWriter;
 import org.apache.spark.sql.streaming.OutputMode;
+import org.apache.spark.sql.types.StructType;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -49,8 +50,8 @@ public class StreamingWriter extends Writer implements StreamWriter {
 
   StreamingWriter(Table table, Broadcast<FileIO> io, Broadcast<EncryptionManager> encryptionManager,
                   DataSourceOptions options, String queryId, OutputMode mode, String applicationId,
-                  Schema dsSchema) {
-    super(table, io, encryptionManager, options, false, applicationId, dsSchema);
+                  Schema writeSchema, StructType dsSchema) {
+    super(table, io, encryptionManager, options, false, applicationId, writeSchema, dsSchema);
     this.queryId = queryId;
     this.mode = mode;
   }

File: spark/src/test/java/org/apache/iceberg/TestDataFileSerialization.java
Patch:
@@ -38,6 +38,7 @@
 import java.util.UUID;
 import org.apache.iceberg.io.FileAppender;
 import org.apache.iceberg.parquet.Parquet;
+import org.apache.iceberg.spark.SparkSchemaUtil;
 import org.apache.iceberg.spark.data.RandomData;
 import org.apache.iceberg.spark.data.SparkParquetWriters;
 import org.apache.iceberg.types.Types;
@@ -164,7 +165,7 @@ public void testParquetWriterSplitOffsets() throws IOException {
     FileAppender<InternalRow> writer =
         Parquet.write(Files.localOutput(parquetFile))
             .schema(DATE_SCHEMA)
-            .createWriterFunc(msgType -> SparkParquetWriters.buildWriter(DATE_SCHEMA, msgType))
+            .createWriterFunc(msgType -> SparkParquetWriters.buildWriter(SparkSchemaUtil.convert(DATE_SCHEMA), msgType))
             .build();
     try {
       writer.addAll(records);

File: spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetWriter.java
Patch:
@@ -27,6 +27,7 @@
 import org.apache.iceberg.io.CloseableIterable;
 import org.apache.iceberg.io.FileAppender;
 import org.apache.iceberg.parquet.Parquet;
+import org.apache.iceberg.spark.SparkSchemaUtil;
 import org.apache.iceberg.types.Types;
 import org.apache.spark.sql.catalyst.InternalRow;
 import org.junit.Assert;
@@ -78,7 +79,7 @@ public void testCorrectness() throws IOException {
 
     try (FileAppender<InternalRow> writer = Parquet.write(Files.localOutput(testFile))
         .schema(COMPLEX_SCHEMA)
-        .createWriterFunc(msgType -> SparkParquetWriters.buildWriter(COMPLEX_SCHEMA, msgType))
+        .createWriterFunc(msgType -> SparkParquetWriters.buildWriter(SparkSchemaUtil.convert(COMPLEX_SCHEMA), msgType))
         .build()) {
       writer.addAll(records);
     }

File: data/src/main/java/org/apache/iceberg/data/orc/GenericOrcReader.java
Patch:
@@ -101,7 +101,7 @@ default T convert(ColumnVector vector, int row) {
       if (!vector.noNulls && vector.isNull[rowIndex]) {
         return null;
       } else {
-        return convertNonNullValue(vector, row);
+        return convertNonNullValue(vector, rowIndex);
       }
     }
 

File: spark/src/test/java/org/apache/iceberg/TestRewriteManifestsAction.java
Patch:
@@ -146,7 +146,6 @@ public void testRewriteSmallManifestsNonPartitionedTable() throws IOException {
 
     RewriteManifestsActionResult result = actions.rewriteManifests()
         .rewriteIf(manifest -> true)
-        .stagingLocation(temp.newFolder().toString())
         .execute();
 
     Assert.assertEquals("Action should rewrite 2 manifests", 2, result.deletedManifests().size());
@@ -224,7 +223,6 @@ public void testRewriteSmallManifestsPartitionedTable() throws IOException {
 
     RewriteManifestsActionResult result = actions.rewriteManifests()
         .rewriteIf(manifest -> true)
-        .stagingLocation(temp.newFolder().toString())
         .execute();
 
     Assert.assertEquals("Action should rewrite 4 manifests", 4, result.deletedManifests().size());

File: core/src/main/java/org/apache/iceberg/ManifestListWriter.java
Patch:
@@ -117,6 +117,7 @@ protected ManifestFile prepare(ManifestFile manifest) {
       return wrapper.wrap(manifest);
     }
 
+    @Override
     protected FileAppender<ManifestFile> newAppender(OutputFile file, Map<String, String> meta) {
       try {
         return Avro.write(file)

File: core/src/main/java/org/apache/iceberg/ManifestWriter.java
Patch:
@@ -25,14 +25,11 @@
 import org.apache.iceberg.exceptions.RuntimeIOException;
 import org.apache.iceberg.io.FileAppender;
 import org.apache.iceberg.io.OutputFile;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
 
 /**
  * Writer for manifest files.
  */
 public abstract class ManifestWriter implements FileAppender<DataFile> {
-  private static final Logger LOG = LoggerFactory.getLogger(ManifestWriter.class);
   // stand-in for the current sequence number that will be assigned when the commit is successful
   // this is replaced when writing a manifest list by the ManifestFile wrapper
   static final long UNASSIGNED_SEQ = -1L;

File: core/src/main/java/org/apache/iceberg/GenericDataFile.java
Patch:
@@ -377,7 +377,7 @@ private static org.apache.avro.Schema getAvroSchema(Types.StructType partitionTy
 
   @Override
   public int size() {
-    return 15;
+    return 13;
   }
 
   @Override

File: core/src/main/java/org/apache/iceberg/AllDataFilesTable.java
Patch:
@@ -75,7 +75,7 @@ public Schema schema() {
     }
   }
 
-  public static class AllDataFilesTableScan extends BaseTableScan {
+  public static class AllDataFilesTableScan extends BaseAllMetadataTableScan {
     private final Schema fileSchema;
 
     AllDataFilesTableScan(TableOperations ops, Table table, Schema fileSchema) {

File: core/src/main/java/org/apache/iceberg/AllEntriesTable.java
Patch:
@@ -68,7 +68,7 @@ public Schema schema() {
     }
   }
 
-  private static class Scan extends BaseTableScan {
+  private static class Scan extends BaseAllMetadataTableScan {
 
     Scan(TableOperations ops, Table table, Schema schema) {
       super(ops, table, schema);

File: core/src/main/java/org/apache/iceberg/AllManifestsTable.java
Patch:
@@ -83,7 +83,7 @@ public Schema schema() {
     return MANIFEST_FILE_SCHEMA;
   }
 
-  public static class AllManifestsTableScan extends BaseTableScan {
+  public static class AllManifestsTableScan extends BaseAllMetadataTableScan {
 
     AllManifestsTableScan(TableOperations ops, Table table, Schema fileSchema) {
       super(ops, table, fileSchema);

File: data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriter.java
Patch:
@@ -167,6 +167,7 @@ public Class<LocalTime> getJavaClass() {
       return LocalTime.class;
     }
 
+    @Override
     public void addValue(int rowId, LocalTime data, ColumnVector output) {
       if (data == null) {
         output.noNulls = false;
@@ -257,6 +258,7 @@ public Class<ByteBuffer> getJavaClass() {
       return ByteBuffer.class;
     }
 
+    @Override
     public void addValue(int rowId, ByteBuffer data, ColumnVector output) {
       if (data == null) {
         output.noNulls = false;
@@ -274,6 +276,7 @@ public Class<UUID> getJavaClass() {
       return UUID.class;
     }
 
+    @Override
     public void addValue(int rowId, UUID data, ColumnVector output) {
       if (data == null) {
         output.noNulls = false;

File: parquet/src/main/java/org/apache/iceberg/parquet/PageIterator.java
Patch:
@@ -91,6 +91,7 @@ private PageIterator(ColumnDescriptor desc, String writerVersion) {
     super(desc, writerVersion);
   }
 
+  @Override
   public void setPage(DataPage page) {
     super.setPage(page);
     advance();

File: spark/src/test/java/org/apache/iceberg/spark/source/TestIdentityPartitionData.java
Patch:
@@ -54,7 +54,8 @@ public class TestIdentityPartitionData  {
   public static Object[][] parameters() {
     return new Object[][] {
         new Object[] { "parquet" },
-        new Object[] { "avro" }
+        new Object[] { "avro" },
+        new Object[] { "orc" }
     };
   }
 

File: spark/src/test/java/org/apache/iceberg/spark/source/TestPartitionValues.java
Patch:
@@ -57,7 +57,8 @@ public class TestPartitionValues {
   public static Object[][] parameters() {
     return new Object[][] {
         new Object[] { "parquet" },
-        new Object[] { "avro" }
+        new Object[] { "avro" },
+        new Object[] { "orc" }
     };
   }
 

File: core/src/main/java/org/apache/iceberg/AllManifestsTable.java
Patch:
@@ -167,6 +167,7 @@ public CloseableIterable<StructLike> rows() {
           .rename("partitions", GenericPartitionFieldSummary.class.getName())
           .rename("r508", GenericPartitionFieldSummary.class.getName())
           .project(ManifestFile.schema())
+          .classLoader(GenericManifestFile.class.getClassLoader())
           .reuseContainers(false)
           .build()) {
 

File: core/src/main/java/org/apache/iceberg/BaseSnapshot.java
Patch:
@@ -119,6 +119,7 @@ public List<ManifestFile> manifests() {
           .rename("manifest_file", GenericManifestFile.class.getName())
           .rename("partitions", GenericPartitionFieldSummary.class.getName())
           .rename("r508", GenericPartitionFieldSummary.class.getName())
+          .classLoader(GenericManifestFile.class.getClassLoader())
           .project(ManifestFile.schema())
           .reuseContainers(false)
           .build()) {

File: core/src/main/java/org/apache/iceberg/ManifestReader.java
Patch:
@@ -252,6 +252,7 @@ CloseableIterable<ManifestEntry> entries(Schema fileProjection) {
             .rename("r102", PartitionData.class.getName())
             .rename("data_file", GenericDataFile.class.getName())
             .rename("r2", GenericDataFile.class.getName())
+            .classLoader(ManifestEntry.class.getClassLoader())
             .reuseContainers()
             .build();
 

File: core/src/main/java/org/apache/iceberg/RemoveSnapshots.java
Patch:
@@ -380,6 +380,7 @@ private CloseableIterable<ManifestFile> readManifestFiles(Snapshot snapshot) {
     if (snapshot.manifestListLocation() != null) {
       return Avro.read(ops.io().newInputFile(snapshot.manifestListLocation()))
           .rename("manifest_file", GenericManifestFile.class.getName())
+          .classLoader(GenericManifestFile.class.getClassLoader())
           .project(MANIFEST_PROJECTION)
           .reuseContainers(true)
           .build();

File: core/src/test/java/org/apache/iceberg/TestGenericManifestFile.java
Patch:
@@ -68,6 +68,7 @@ public void testManifestsWithoutRowStats() throws IOException {
         .rename("manifest_file", GenericManifestFile.class.getName())
         .rename("partitions", GenericPartitionFieldSummary.class.getName())
         .rename("r508", GenericPartitionFieldSummary.class.getName())
+        .classLoader(GenericManifestFile.class.getClassLoader())
         .project(ManifestFile.schema())
         .reuseContainers(false)
         .build()) {

File: orc/src/main/java/org/apache/iceberg/orc/ORC.java
Patch:
@@ -183,7 +183,7 @@ static Reader newFileReader(String location, ReaderOptions readerOptions) {
   }
 
   static Reader newFileReader(InputFile file, Configuration config) {
-    ReaderOptions readerOptions = OrcFile.readerOptions(config);
+    ReaderOptions readerOptions = OrcFile.readerOptions(config).useUTCTimestamp(true);
     if (file instanceof HadoopInputFile) {
       readerOptions.filesystem(((HadoopInputFile) file).getFileSystem());
     }

File: orc/src/main/java/org/apache/iceberg/orc/OrcFileAppender.java
Patch:
@@ -67,7 +67,7 @@ class OrcFileAppender<D> implements FileAppender<D> {
     TypeDescription orcSchema = ORCSchemaUtil.convert(this.schema);
     this.batch = orcSchema.createRowBatch(this.batchSize);
 
-    OrcFile.WriterOptions options = OrcFile.writerOptions(conf);
+    OrcFile.WriterOptions options = OrcFile.writerOptions(conf).useUTCTimestamp(true);
     if (file instanceof HadoopOutputFile) {
       options.fileSystem(((HadoopOutputFile) file).getFileSystem());
     }

File: spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcWriter.java
Patch:
@@ -204,7 +204,7 @@ public void addValue(int rowId, int column, SpecializedGetters data,
     }
   }
 
-  static class TimestampConverter implements Converter {
+  static class TimestampTzConverter implements Converter {
     @Override
     public void addValue(int rowId, int column, SpecializedGetters data,
                          ColumnVector output) {
@@ -391,8 +391,8 @@ private static Converter buildConverter(TypeDescription schema) {
         return schema.getPrecision() <= 18 ?
             new Decimal18Converter(schema) :
             new Decimal38Converter(schema);
-      case TIMESTAMP:
-        return new TimestampConverter();
+      case TIMESTAMP_INSTANT:
+        return new TimestampTzConverter();
       case STRUCT:
         return new StructConverter(schema);
       case LIST:

File: core/src/main/java/org/apache/iceberg/RemoveSnapshots.java
Patch:
@@ -23,11 +23,11 @@
 import com.google.common.base.Preconditions;
 import com.google.common.collect.Lists;
 import com.google.common.collect.Sets;
-import io.netty.util.internal.ConcurrentSet;
 import java.io.IOException;
 import java.util.Date;
 import java.util.List;
 import java.util.Set;
+import java.util.concurrent.ConcurrentHashMap;
 import java.util.function.Consumer;
 import org.apache.iceberg.avro.Avro;
 import org.apache.iceberg.exceptions.CommitFailedException;
@@ -330,7 +330,7 @@ private void deleteDataFiles(Set<ManifestFile> manifestsToScan, Set<ManifestFile
 
   private Set<String> findFilesToDelete(Set<ManifestFile> manifestsToScan, Set<ManifestFile> manifestsToRevert,
                                         Set<Long> validIds) {
-    Set<String> filesToDelete = new ConcurrentSet<>();
+    Set<String> filesToDelete = ConcurrentHashMap.newKeySet();
     Tasks.foreach(manifestsToScan)
         .noRetry().suppressFailureWhenFinished()
         .executeWith(ThreadPools.getWorkerPool())

File: api/src/main/java/org/apache/iceberg/types/TypeUtil.java
Patch:
@@ -391,7 +391,7 @@ static int decimalMaxPrecision(int numBytes) {
     return MAX_PRECISION[numBytes];
   }
 
-  public static int decimalRequriedBytes(int precision) {
+  public static int decimalRequiredBytes(int precision) {
     Preconditions.checkArgument(precision >= 0 && precision < 40,
         "Unsupported decimal precision: %s", precision);
     return REQUIRED_LENGTH[precision];

File: api/src/test/java/org/apache/iceberg/transforms/TestBucketing.java
Patch:
@@ -42,7 +42,7 @@ public class TestBucketing {
   private static Constructor<UUID> uuidBytesConstructor;
 
   @BeforeClass
-  public static void getUUIDConstrutor() {
+  public static void getUUIDConstructor() {
     try {
       uuidBytesConstructor = UUID.class.getDeclaredConstructor(byte[].class);
       uuidBytesConstructor.setAccessible(true);

File: api/src/test/java/org/apache/iceberg/transforms/TestTruncatesResiduals.java
Patch:
@@ -42,7 +42,7 @@ public class TestTruncatesResiduals {
 
   /**
    * Test helper method to compute residual for a given partitionValue against a predicate
-   * and assert the resulting residual expression is same as the exprectedOp
+   * and assert the resulting residual expression is same as the expectedOp
    *
    * @param spec the partition spec
    * @param predicate predicate to calculate the residual against

File: api/src/test/java/org/apache/iceberg/types/TestSerializableTypes.java
Patch:
@@ -149,7 +149,7 @@ public void testSchema() throws Exception {
             Types.StringType.get()))
     );
 
-    Assert.assertEquals("Schema serialziation should be equal to starting schema",
+    Assert.assertEquals("Schema serialization should be equal to starting schema",
         schema.asStruct(), TestHelpers.roundTripSerialize(schema).asStruct());
   }
 

File: core/src/main/java/org/apache/iceberg/avro/TypeToSchema.java
Patch:
@@ -221,7 +221,7 @@ public Schema primitive(Type.PrimitiveType primitive) {
         primitiveSchema = LogicalTypes.decimal(decimal.precision(), decimal.scale())
             .addToSchema(Schema.createFixed(
                 "decimal_" + decimal.precision() + "_" + decimal.scale(),
-                null, null, TypeUtil.decimalRequriedBytes(decimal.precision())));
+                null, null, TypeUtil.decimalRequiredBytes(decimal.precision())));
         break;
       default:
         throw new UnsupportedOperationException(

File: core/src/main/java/org/apache/iceberg/avro/ValueWriters.java
Patch:
@@ -312,7 +312,7 @@ private static class DecimalWriter implements ValueWriter<BigDecimal> {
     private DecimalWriter(int precision, int scale) {
       this.precision = precision;
       this.scale = scale;
-      this.length = TypeUtil.decimalRequriedBytes(precision);
+      this.length = TypeUtil.decimalRequiredBytes(precision);
       this.bytes = ThreadLocal.withInitial(() -> new byte[length]);
     }
 

File: parquet/src/main/java/org/apache/iceberg/parquet/ParquetAvro.java
Patch:
@@ -304,7 +304,7 @@ public Schema primitive(Schema primitive) {
         } else {
           return new ParquetDecimal(decimal.getPrecision(), decimal.getScale())
               .addToSchema(Schema.createFixed(primitive.getName(),
-                  null, null, TypeUtil.decimalRequriedBytes(decimal.getPrecision())));
+                  null, null, TypeUtil.decimalRequiredBytes(decimal.getPrecision())));
         }
       }
 

File: parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueWriters.java
Patch:
@@ -190,7 +190,7 @@ private FixedDecimalWriter(ColumnDescriptor desc, int precision, int scale) {
       super(desc);
       this.precision = precision;
       this.scale = scale;
-      this.length = TypeUtil.decimalRequriedBytes(precision);
+      this.length = TypeUtil.decimalRequiredBytes(precision);
       this.bytes = ThreadLocal.withInitial(() -> new byte[length]);
     }
 

File: parquet/src/main/java/org/apache/iceberg/parquet/TypeToMessageType.java
Patch:
@@ -165,7 +165,7 @@ public Type primitive(PrimitiveType primitive, Type.Repetition repetition, int i
 
         } else {
           // store as a fixed-length array
-          int minLength = TypeUtil.decimalRequriedBytes(decimal.precision());
+          int minLength = TypeUtil.decimalRequiredBytes(decimal.precision());
           return Types.primitive(FIXED_LEN_BYTE_ARRAY, repetition).length(minLength)
               .as(DECIMAL)
               .precision(decimal.precision())

File: spark/src/main/java/org/apache/iceberg/spark/data/SparkParquetWriters.java
Patch:
@@ -299,7 +299,7 @@ private FixedDecimalWriter(ColumnDescriptor desc, int precision, int scale) {
       super(desc);
       this.precision = precision;
       this.scale = scale;
-      this.length = TypeUtil.decimalRequriedBytes(precision);
+      this.length = TypeUtil.decimalRequiredBytes(precision);
       this.bytes = ThreadLocal.withInitial(() -> new byte[length]);
     }
 

File: spark/src/main/java/org/apache/iceberg/spark/data/SparkValueWriters.java
Patch:
@@ -120,7 +120,7 @@ private static class DecimalWriter implements ValueWriter<Decimal> {
     private DecimalWriter(int precision, int scale) {
       this.precision = precision;
       this.scale = scale;
-      this.length = TypeUtil.decimalRequriedBytes(precision);
+      this.length = TypeUtil.decimalRequiredBytes(precision);
       this.bytes = ThreadLocal.withInitial(() -> new byte[length]);
     }
 

File: api/src/main/java/org/apache/iceberg/catalog/TableIdentifier.java
Patch:
@@ -57,7 +57,7 @@ private TableIdentifier(Namespace namespace, String name) {
 
   /**
    * Whether the namespace is empty.
-   * @return true if the namespace is empty, false otherwise
+   * @return true if the namespace is not empty, false otherwise
    */
   public boolean hasNamespace() {
     return !namespace.isEmpty();

File: spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcReader.java
Patch:
@@ -56,10 +56,12 @@ public class SparkOrcReader implements OrcValueReader<InternalRow> {
   private static final int INITIAL_SIZE = 128 * 1024;
   private final List<TypeDescription> columns;
   private final Converter[] converters;
+  private final UnsafeRowWriter rowWriter;
 
   public SparkOrcReader(TypeDescription readOrcSchema) {
     columns = readOrcSchema.getChildren();
     converters = buildConverters();
+    rowWriter = new UnsafeRowWriter(columns.size(), INITIAL_SIZE);
   }
 
   private Converter[] buildConverters() {
@@ -72,7 +74,6 @@ private Converter[] buildConverters() {
 
   @Override
   public InternalRow read(VectorizedRowBatch batch, int row) {
-    final UnsafeRowWriter rowWriter = new UnsafeRowWriter(columns.size(), INITIAL_SIZE);
     rowWriter.reset();
     rowWriter.zeroOutNullBytes();
     for (int c = 0; c < batch.cols.length; ++c) {

File: core/src/main/java/org/apache/iceberg/RemoveSnapshots.java
Patch:
@@ -198,7 +198,7 @@ private void cleanExpiredFiles(List<Snapshot> snapshots, Set<Long> validIds, Set
           boolean fromValidSnapshots = validIds.contains(manifest.snapshotId());
           boolean isFromAncestor = ancestorIds.contains(manifest.snapshotId());
           if (!fromValidSnapshots && isFromAncestor && manifest.hasDeletedFiles()) {
-            manifestsToScan.add(manifest);
+            manifestsToScan.add(manifest.copy());
           }
         }
 
@@ -228,7 +228,7 @@ private void cleanExpiredFiles(List<Snapshot> snapshots, Set<Long> validIds, Set
                 // snapshot is an ancestor of the current table state. Otherwise, a snapshot that
                 // deleted files and was rolled back will delete files that could be in the current
                 // table state.
-                manifestsToScan.add(manifest);
+                manifestsToScan.add(manifest.copy());
               }
 
               if (!isFromAncestor && isFromExpiringSnapshot && manifest.hasAddedFiles()) {
@@ -239,7 +239,7 @@ private void cleanExpiredFiles(List<Snapshot> snapshots, Set<Long> validIds, Set
                 // written and this expiration is known and there is no missing history. If history
                 // were missing, then the snapshot could be an ancestor of the table state but the
                 // ancestor ID set would not contain it and this would be unsafe.
-                manifestsToRevert.add(manifest);
+                manifestsToRevert.add(manifest.copy());
               }
             }
           }

File: core/src/main/java/org/apache/iceberg/BaseMetastoreTableOperations.java
Patch:
@@ -148,8 +148,8 @@ protected void refreshFromMetadataLocation(String newLocation, Predicate<Excepti
               TableMetadataParser.read(io(), metadataLocation)));
 
       String newUUID = newMetadata.get().uuid();
-      if (currentMetadata != null) {
-        Preconditions.checkState(newUUID == null || newUUID.equals(currentMetadata.uuid()),
+      if (currentMetadata != null && currentMetadata.uuid() != null && newUUID != null) {
+        Preconditions.checkState(newUUID.equals(currentMetadata.uuid()),
             "Table UUID does not match: current=%s != refreshed=%s", currentMetadata.uuid(), newUUID);
       }
 

File: core/src/main/java/org/apache/iceberg/ManifestsTable.java
Patch:
@@ -79,8 +79,9 @@ public Schema schema() {
   }
 
   protected DataTask task(TableScan scan) {
+    String manifestListLocation = scan.snapshot().manifestListLocation();
     return StaticDataTask.of(
-        ops.io().newInputFile(scan.snapshot().manifestListLocation()),
+        ops.io().newInputFile(manifestListLocation != null ? manifestListLocation : ops.current().file().location()),
         scan.snapshot().manifests(),
         this::manifestFileToRow);
   }

File: core/src/main/java/org/apache/iceberg/hadoop/HadoopTableOperations.java
Patch:
@@ -110,7 +110,7 @@ public TableMetadata refresh() {
   }
 
   @Override
-  public void commit(TableMetadata base, TableMetadata metadata) {
+  public synchronized void commit(TableMetadata base, TableMetadata metadata) {
     if (base != current()) {
       throw new CommitFailedException("Cannot commit changes based on stale table metadata");
     }

File: parquet/src/main/java/org/apache/iceberg/parquet/VectorizedReader.java
Patch:
@@ -39,7 +39,7 @@ public interface VectorizedReader<T> {
   /**
    *
    * @param pages row group information for all the columns
-   * @param metadata map of {@link ColumnPath} -> {@link ColumnChunkMetaData} for the row group
+   * @param metadata map of {@link ColumnPath} -&gt; {@link ColumnChunkMetaData} for the row group
    */
   void setRowGroupInfo(PageReadStore pages, Map<ColumnPath, ColumnChunkMetaData> metadata);
 

File: core/src/main/java/org/apache/iceberg/CachingCatalog.java
Patch:
@@ -107,7 +107,7 @@ public Transaction newReplaceTableTransaction(TableIdentifier ident, Schema sche
 
   @Override
   public boolean dropTable(TableIdentifier ident, boolean purge) {
-    boolean dropped = catalog.dropTable(ident, false);
+    boolean dropped = catalog.dropTable(ident, purge);
     tableCache.invalidate(canonicalizeIdentifier(ident));
     return dropped;
   }

File: core/src/main/java/org/apache/iceberg/BaseMetastoreCatalog.java
Patch:
@@ -153,7 +153,7 @@ private Table loadMetadataTable(TableIdentifier identifier) {
         throw new NoSuchTableException("Table does not exist: " + baseTableIdentifier);
       }
 
-      Table baseTable = new BaseTable(ops, fullTableName(name(), identifier));
+      Table baseTable = new BaseTable(ops, fullTableName(name(), baseTableIdentifier));
 
       switch (type) {
         case ENTRIES:

File: api/src/main/java/org/apache/iceberg/ExpireSnapshots.java
Patch:
@@ -58,12 +58,12 @@ public interface ExpireSnapshots extends PendingUpdate<List<Snapshot>> {
   /**
    * Retains the most recent ancestors of the current snapshot.
    * <p>
-   * If a snapshot would be expired becuase it is older than the expiration timestamp, but is one of
+   * If a snapshot would be expired because it is older than the expiration timestamp, but is one of
    * the {@code numSnapshot} most recent ancestors of the current state, it will be retained. This
    * will not cause snapshots explicitly identified by id from expiring.
    * <p>
    * This may keep more than {@code numSnapshot} ancestors if snapshots are added concurrently. This
-   * may keep less than {@numSnapshot} ancestors if the current table state does not have that many.
+   * may keep less than {@code numSnapshot} ancestors if the current table state does not have that many.
    *
    * @param numSnapshots the number of snapshots to retain
    * @return this for method chaining

File: api/src/main/java/org/apache/iceberg/expressions/Reference.java
Patch:
@@ -19,12 +19,10 @@
 
 package org.apache.iceberg.expressions;
 
-import java.io.Serializable;
-
 /**
  * Represents a variable reference in an {@link Expression expression}.
  * @see BoundReference
  * @see NamedReference
  */
-public interface Reference extends Serializable {
+public interface Reference<T> extends Term {
 }

File: api/src/main/java/org/apache/iceberg/expressions/BoundSetPredicate.java
Patch:
@@ -53,6 +53,7 @@ public Set<T> literalSet() {
     return literalSet;
   }
 
+  @Override
   public boolean test(T value) {
     switch (op()) {
       case IN:

File: spark/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java
Patch:
@@ -71,9 +71,9 @@ public DataSourceReader createReader(DataSourceOptions options) {
   public DataSourceReader createReader(StructType readSchema, DataSourceOptions options) {
     Configuration conf = new Configuration(lazyBaseConf());
     Table table = getTableAndResolveHadoopConfiguration(options, conf);
-    String caseSensitive = lazySparkSession().conf().get("spark.sql.caseSensitive", "true");
+    String caseSensitive = lazySparkSession().conf().get("spark.sql.caseSensitive");
 
-    Reader reader = new Reader(table, Boolean.valueOf(caseSensitive), options);
+    Reader reader = new Reader(table, Boolean.parseBoolean(caseSensitive), options);
     if (readSchema != null) {
       // convert() will fail if readSchema contains fields not in table.schema()
       SparkSchemaUtil.convert(table.schema(), readSchema);

File: core/src/test/java/org/apache/iceberg/TestScanSummary.java
Patch:
@@ -45,7 +45,7 @@ public void testSnapshotTimeRangeValidation() {
         .commit();
 
     long t1 = System.currentTimeMillis();
-    while (t1 == t0) {
+    while (t1 <= table.currentSnapshot().timestampMillis()) {
       t1 = System.currentTimeMillis();
     }
 
@@ -56,7 +56,7 @@ public void testSnapshotTimeRangeValidation() {
     long secondSnapshotId = table.currentSnapshot().snapshotId();
 
     long t2 = System.currentTimeMillis();
-    while (t2 == t1) {
+    while (t2 <= table.currentSnapshot().timestampMillis()) {
       t2 = System.currentTimeMillis();
     }
 

File: core/src/main/java/org/apache/iceberg/avro/BuildAvroProjection.java
Patch:
@@ -87,7 +87,7 @@ public Schema record(Schema record, List<String> names, Iterable<Schema.Field> s
         hasChange = true;
       }
 
-      Schema.Field avroField = updateMap.get(field.name());
+      Schema.Field avroField = updateMap.get(AvroSchemaUtil.makeCompatibleName(field.name()));
 
       if (avroField != null) {
         updatedFields.add(avroField);
@@ -131,7 +131,7 @@ public Schema.Field field(Schema.Field field, Supplier<Schema> fieldResult) {
 
       if (schema != field.schema() || !expectedName.equals(field.name())) {
         // add an alias for the field
-        return AvroSchemaUtil.copyField(field, schema, expectedName);
+        return AvroSchemaUtil.copyField(field, schema, AvroSchemaUtil.makeCompatibleName(expectedName));
       } else {
         // always copy because fields can't be reused
         return AvroSchemaUtil.copyField(field, field.schema(), field.name());

File: data/src/main/java/org/apache/iceberg/data/avro/IcebergDecoder.java
Patch:
@@ -166,7 +166,9 @@ private static class RawDecoder<D> extends MessageDecoder.BaseDecoder<D> {
      * @param writeSchema the schema used to decode buffers
      */
     private RawDecoder(org.apache.iceberg.Schema readSchema, org.apache.avro.Schema writeSchema) {
-      this.reader = new ProjectionDatumReader<>(DataReader::create, readSchema, ImmutableMap.of(), null);
+      this.reader = new ProjectionDatumReader<>(
+          avroSchema -> DataReader.create(readSchema, avroSchema),
+          readSchema, ImmutableMap.of(), null);
       this.reader.setSchema(writeSchema);
     }
 

File: spark/src/test/java/org/apache/iceberg/spark/data/TestParquetAvroReader.java
Patch:
@@ -66,7 +66,8 @@ public class TestParquetAvroReader {
       optional(19, "renovate", Types.MapType.ofRequired(20, 21,
           Types.StringType.get(), Types.StructType.of(
               optional(22, "jumpy", Types.DoubleType.get()),
-              required(23, "koala", Types.TimeType.get())
+              required(23, "koala", Types.TimeType.get()),
+              required(24, "couch rope", Types.IntegerType.get())
           ))),
       optional(2, "slide", Types.StringType.get())
   );

File: spark/src/test/java/org/apache/iceberg/spark/data/TestParquetAvroWriter.java
Patch:
@@ -66,7 +66,8 @@ public class TestParquetAvroWriter {
       optional(19, "renovate", Types.MapType.ofRequired(20, 21,
           Types.StringType.get(), Types.StructType.of(
               optional(22, "jumpy", Types.DoubleType.get()),
-              required(23, "koala", Types.TimeType.get())
+              required(23, "koala", Types.TimeType.get()),
+              required(24, "couch rope", Types.IntegerType.get())
           ))),
       optional(2, "slide", Types.StringType.get())
   );

File: spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetWriter.java
Patch:
@@ -62,7 +62,8 @@ public class TestSparkParquetWriter {
       optional(19, "renovate", Types.MapType.ofRequired(20, 21,
           Types.StringType.get(), Types.StructType.of(
               optional(22, "jumpy", Types.DoubleType.get()),
-              required(23, "koala", Types.IntegerType.get())
+              required(23, "koala", Types.IntegerType.get()),
+              required(24, "couch rope", Types.IntegerType.get())
           ))),
       optional(2, "slide", Types.StringType.get())
   );

File: core/src/main/java/org/apache/iceberg/avro/AvroSchemaVisitor.java
Patch:
@@ -58,7 +58,7 @@ public static <T> T visit(Schema schema, AvroSchemaVisitor<T> visitor) {
         return visitor.union(schema, options);
 
       case ARRAY:
-        if (schema.getLogicalType() instanceof LogicalMap || AvroSchemaUtil.isKeyValueSchema(schema.getElementType())) {
+        if (schema.getLogicalType() instanceof LogicalMap) {
           return visitor.array(schema, visit(schema.getElementType(), visitor));
         } else {
           return visitor.array(schema, visitWithName("element", schema.getElementType(), visitor));

File: core/src/main/java/org/apache/iceberg/avro/PruneColumns.java
Patch:
@@ -121,7 +121,7 @@ public Schema union(Schema union, List<Schema> options) {
   @Override
   @SuppressWarnings("checkstyle:CyclomaticComplexity")
   public Schema array(Schema array, Schema element) {
-    if (array.getLogicalType() instanceof LogicalMap || AvroSchemaUtil.isKeyValueSchema(array.getElementType())) {
+    if (array.getLogicalType() instanceof LogicalMap) {
       Schema keyValue = array.getElementType();
       Integer keyId = AvroSchemaUtil.getFieldId(keyValue.getField("key"), nameMapping, fieldNames());
       Integer valueId = AvroSchemaUtil.getFieldId(keyValue.getField("value"), nameMapping, fieldNames());

File: api/src/main/java/org/apache/iceberg/PartitionSpec.java
Patch:
@@ -39,6 +39,7 @@
 import org.apache.iceberg.transforms.UnknownTransform;
 import org.apache.iceberg.types.Type;
 import org.apache.iceberg.types.Types;
+import org.apache.iceberg.types.Types.StructType;
 
 /**
  * Represents how to produce partition data for a table.
@@ -99,9 +100,9 @@ public List<PartitionField> getFieldsBySourceId(int fieldId) {
   }
 
   /**
-   * @return a {@link Types.StructType} for partition data defined by this spec.
+   * @return a {@link StructType} for partition data defined by this spec.
    */
-  public Types.StructType partitionType() {
+  public StructType partitionType() {
     List<Types.NestedField> structFields = Lists.newArrayListWithExpectedSize(fields.length);
 
     for (int i = 0; i < fields.length; i += 1) {

File: api/src/main/java/org/apache/iceberg/transforms/Transforms.java
Patch:
@@ -31,7 +31,7 @@
  * Factory methods for transforms.
  * <p>
  * Most users should create transforms using a
- * {@link PartitionSpec.Builder#builderFor(Schema)} partition spec builder}.
+ * {@link PartitionSpec#builderFor(Schema)} partition spec builder}.
  *
  * @see PartitionSpec#builderFor(Schema) The partition spec builder.
  */

File: api/src/main/java/org/apache/iceberg/PartitionSpec.java
Patch:
@@ -465,7 +465,7 @@ public Builder truncate(String sourceName, int width) {
     Builder add(int sourceId, String name, String transform) {
       Types.NestedField column = schema.findField(sourceId);
       checkAndAddPartitionName(name, column.fieldId());
-      Preconditions.checkNotNull(column, "Cannot find source column: %d", sourceId);
+      Preconditions.checkNotNull(column, "Cannot find source column: %s", sourceId);
       fields.add(new PartitionField(sourceId, name, Transforms.fromString(column.type(), transform)));
       return this;
     }

File: core/src/main/java/org/apache/iceberg/TableMetadataParser.java
Patch:
@@ -209,7 +209,7 @@ static TableMetadata fromJson(TableOperations ops, InputFile file, JsonNode node
 
     int formatVersion = JsonUtil.getInt(FORMAT_VERSION, node);
     Preconditions.checkArgument(formatVersion == TableMetadata.TABLE_FORMAT_VERSION,
-        "Cannot read unsupported version %d", formatVersion);
+        "Cannot read unsupported version %s", formatVersion);
 
     String uuid = JsonUtil.getStringOrNull(TABLE_UUID, node);
     String location = JsonUtil.getString(LOCATION, node);

File: api/src/main/java/org/apache/iceberg/PartitionSpec.java
Patch:
@@ -33,7 +33,6 @@
 import java.util.Arrays;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 import java.util.Set;
 import org.apache.iceberg.exceptions.ValidationException;
 import org.apache.iceberg.transforms.Transforms;
@@ -214,7 +213,7 @@ public boolean equals(Object other) {
 
   @Override
   public int hashCode() {
-    return Objects.hashCode(Arrays.hashCode(fields));
+    return Integer.hashCode(Arrays.hashCode(fields));
   }
 
   private List<PartitionField> lazyFieldList() {

File: api/src/main/java/org/apache/iceberg/types/CheckCompatibility.java
Patch:
@@ -52,7 +52,7 @@ public static List<String> readCompatibilityErrors(Schema readSchema, Schema wri
     return TypeUtil.visit(readSchema, new CheckCompatibility(writeSchema, false));
   }
 
-  private static final List<String> NO_ERRORS = ImmutableList.of();
+  private static final ImmutableList<String> NO_ERRORS = ImmutableList.of();
 
   private final Schema schema;
   private final boolean checkOrdering;

File: core/src/main/java/org/apache/iceberg/DataTableScan.java
Patch:
@@ -25,7 +25,6 @@
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.Iterables;
 import java.util.Collection;
-import java.util.List;
 import org.apache.iceberg.expressions.Expression;
 import org.apache.iceberg.expressions.ManifestEvaluator;
 import org.apache.iceberg.expressions.ResidualEvaluator;
@@ -38,11 +37,11 @@
 public class DataTableScan extends BaseTableScan {
   private static final Logger LOG = LoggerFactory.getLogger(DataTableScan.class);
 
-  private static final List<String> SCAN_COLUMNS = ImmutableList.of(
+  private static final ImmutableList<String> SCAN_COLUMNS = ImmutableList.of(
       "snapshot_id", "file_path", "file_ordinal", "file_format", "block_size_in_bytes",
       "file_size_in_bytes", "record_count", "partition", "key_metadata"
   );
-  private static final List<String> SCAN_WITH_STATS_COLUMNS = ImmutableList.<String>builder()
+  private static final ImmutableList<String> SCAN_WITH_STATS_COLUMNS = ImmutableList.<String>builder()
       .addAll(SCAN_COLUMNS)
       .add("value_counts", "null_value_counts", "lower_bounds", "upper_bounds", "column_sizes")
       .build();

File: core/src/main/java/org/apache/iceberg/FileHistory.java
Patch:
@@ -33,7 +33,7 @@
 import org.apache.iceberg.util.CharSequenceWrapper;
 
 public class FileHistory {
-  private static final List<String> HISTORY_COLUMNS = ImmutableList.of("file_path");
+  private static final ImmutableList<String> HISTORY_COLUMNS = ImmutableList.of("file_path");
 
   private FileHistory() {
   }

File: core/src/main/java/org/apache/iceberg/ManifestReader.java
Patch:
@@ -49,10 +49,10 @@
 public class ManifestReader extends CloseableGroup implements Filterable<FilteredManifest> {
   private static final Logger LOG = LoggerFactory.getLogger(ManifestReader.class);
 
-  private static final List<String> ALL_COLUMNS = ImmutableList.of("*");
-  static final List<String> CHANGE_COLUMNS = ImmutableList.of(
+  private static final ImmutableList<String> ALL_COLUMNS = ImmutableList.of("*");
+  static final ImmutableList<String> CHANGE_COLUMNS = ImmutableList.of(
       "file_path", "file_format", "partition", "record_count", "file_size_in_bytes");
-  static final List<String> CHANGE_WITH_STATS_COLUMNS = ImmutableList.<String>builder()
+  static final ImmutableList<String> CHANGE_WITH_STATS_COLUMNS = ImmutableList.<String>builder()
       .addAll(CHANGE_COLUMNS)
       .add("value_counts", "null_value_counts", "lower_bounds", "upper_bounds")
       .build();

File: core/src/main/java/org/apache/iceberg/ScanSummary.java
Patch:
@@ -51,7 +51,7 @@ public class ScanSummary {
   private ScanSummary() {
   }
 
-  private static final List<String> SCAN_SUMMARY_COLUMNS = ImmutableList.of(
+  private static final ImmutableList<String> SCAN_SUMMARY_COLUMNS = ImmutableList.of(
       "partition", "record_count", "file_size_in_bytes");
 
   /**

File: parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueReaders.java
Patch:
@@ -59,7 +59,7 @@ public static <C> ParquetValueReader<C> constant(C value) {
 
   private static class NullReader<T> implements ParquetValueReader<T> {
     private static final NullReader<Void> INSTANCE = new NullReader<>();
-    private static final List<TripleIterator<?>> COLUMNS = ImmutableList.of();
+    private static final ImmutableList<TripleIterator<?>> COLUMNS = ImmutableList.of();
     private static final TripleIterator<?> NULL_COLUMN = new TripleIterator<Object>() {
       @Override
       public int currentDefinitionLevel() {

File: spark/src/main/java/org/apache/iceberg/spark/PruneColumnsWithReordering.java
Patch:
@@ -240,7 +240,7 @@ public Type primitive(Type.PrimitiveType primitive) {
     return primitive;
   }
 
-  private static final Map<TypeID, Class<? extends DataType>> TYPES = ImmutableMap
+  private static final ImmutableMap<TypeID, Class<? extends DataType>> TYPES = ImmutableMap
       .<TypeID, Class<? extends DataType>>builder()
       .put(TypeID.BOOLEAN, BooleanType.class)
       .put(TypeID.INTEGER, IntegerType.class)

File: spark/src/main/java/org/apache/iceberg/spark/PruneColumnsWithoutReordering.java
Patch:
@@ -23,7 +23,6 @@
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.Lists;
 import java.util.List;
-import java.util.Map;
 import java.util.Set;
 import java.util.function.Supplier;
 import org.apache.iceberg.Schema;
@@ -214,7 +213,7 @@ public Type primitive(Type.PrimitiveType primitive) {
     return primitive;
   }
 
-  private static final Map<TypeID, Class<? extends DataType>> TYPES = ImmutableMap
+  private static final ImmutableMap<TypeID, Class<? extends DataType>> TYPES = ImmutableMap
       .<TypeID, Class<? extends DataType>>builder()
       .put(TypeID.BOOLEAN, BooleanType.class)
       .put(TypeID.INTEGER, IntegerType.class)

File: spark/src/main/java/org/apache/iceberg/spark/SparkFilters.java
Patch:
@@ -23,7 +23,6 @@
 import com.google.common.collect.ImmutableMap;
 import java.sql.Date;
 import java.sql.Timestamp;
-import java.util.Map;
 import org.apache.iceberg.expressions.Expression;
 import org.apache.iceberg.expressions.Expression.Operation;
 import org.apache.spark.sql.catalyst.util.DateTimeUtils;
@@ -59,7 +58,7 @@ public class SparkFilters {
   private SparkFilters() {
   }
 
-  private static final Map<Class<? extends Filter>, Operation> FILTERS = ImmutableMap
+  private static final ImmutableMap<Class<? extends Filter>, Operation> FILTERS = ImmutableMap
       .<Class<? extends Filter>, Operation>builder()
       .put(EqualTo.class, Operation.EQ)
       .put(EqualNullSafe.class, Operation.EQ)

File: core/src/main/java/org/apache/iceberg/avro/Avro.java
Patch:
@@ -55,7 +55,7 @@ private enum CodecName {
     BROTLI(null),
     ZSTD(null);
 
-    private CodecFactory avroCodec;
+    private final CodecFactory avroCodec;
 
     CodecName(CodecFactory avroCodec) {
       this.avroCodec = avroCodec;

File: api/src/main/java/org/apache/iceberg/PartitionField.java
Patch:
@@ -67,8 +67,7 @@ public String toString() {
   public boolean equals(Object other) {
     if (this == other) {
       return true;
-    }
-    if (other == null || getClass() != other.getClass()) {
+    } else if (!(other instanceof PartitionField)) {
       return false;
     }
 

File: api/src/main/java/org/apache/iceberg/PartitionSpec.java
Patch:
@@ -201,8 +201,7 @@ public boolean compatibleWith(PartitionSpec other) {
   public boolean equals(Object other) {
     if (this == other) {
       return true;
-    }
-    if (other == null || getClass() != other.getClass()) {
+    } else if (!(other instanceof PartitionSpec)) {
       return false;
     }
 

File: api/src/main/java/org/apache/iceberg/transforms/Bucket.java
Patch:
@@ -89,8 +89,7 @@ public Integer apply(T value) {
   public boolean equals(Object o) {
     if (this == o) {
       return true;
-    }
-    if (o == null || getClass() != o.getClass()) {
+    } else if (!(o instanceof Bucket)) {
       return false;
     }
 

File: api/src/main/java/org/apache/iceberg/transforms/Identity.java
Patch:
@@ -108,8 +108,7 @@ public String toString() {
   public boolean equals(Object o) {
     if (this == o) {
       return true;
-    }
-    if (o == null || getClass() != o.getClass()) {
+    } else if (!(o instanceof Identity)) {
       return false;
     }
 

File: api/src/main/java/org/apache/iceberg/transforms/UnknownTransform.java
Patch:
@@ -73,9 +73,7 @@ public String toString() {
   public boolean equals(Object other) {
     if (this == other) {
       return true;
-    }
-
-    if (other == null || getClass() != other.getClass()) {
+    } else if (!(other instanceof UnknownTransform)) {
       return false;
     }
 

File: api/src/main/java/org/apache/iceberg/util/CharSequenceWrapper.java
Patch:
@@ -49,8 +49,7 @@ public CharSequence get() {
   public boolean equals(Object other) {
     if (this == other) {
       return true;
-    }
-    if (other == null || getClass() != other.getClass()) {
+    } else if (!(other instanceof CharSequenceWrapper)) {
       return false;
     }
 

File: core/src/main/java/org/apache/iceberg/GenericManifestFile.java
Patch:
@@ -282,8 +282,7 @@ public Schema getSchema() {
   public boolean equals(Object other) {
     if (this == other) {
       return true;
-    }
-    if (other == null || getClass() != other.getClass()) {
+    } else if (!(other instanceof GenericManifestFile)) {
       return false;
     }
     GenericManifestFile that = (GenericManifestFile) other;

File: core/src/main/java/org/apache/iceberg/PartitionData.java
Patch:
@@ -180,8 +180,7 @@ public PartitionData copy() {
   public boolean equals(Object o) {
     if (this == o) {
       return true;
-    }
-    if (o == null || getClass() != o.getClass()) {
+    } else if (!(o instanceof PartitionData)) {
       return false;
     }
 

File: core/src/main/java/org/apache/iceberg/TableMetadata.java
Patch:
@@ -104,8 +104,7 @@ public long snapshotId() {
     public boolean equals(Object other) {
       if (this == other) {
         return true;
-      }
-      if (other == null || getClass() != other.getClass()) {
+      } else if (!(other instanceof SnapshotLogEntry)) {
         return false;
       }
       SnapshotLogEntry that = (SnapshotLogEntry) other;

File: core/src/main/java/org/apache/iceberg/mapping/MappedField.java
Patch:
@@ -71,9 +71,7 @@ public MappedFields nestedMapping() {
   public boolean equals(Object other) {
     if (this == other) {
       return true;
-    }
-
-    if (other == null || getClass() != other.getClass()) {
+    } else if (!(other instanceof MappedField)) {
       return false;
     }
 

File: core/src/main/java/org/apache/iceberg/mapping/MappedFields.java
Patch:
@@ -89,9 +89,7 @@ public List<MappedField> fields() {
   public boolean equals(Object other) {
     if (this == other) {
       return true;
-    }
-
-    if (other == null || getClass() != other.getClass()) {
+    } else if (!(other instanceof MappedFields)) {
       return false;
     }
 

File: core/src/main/java/org/apache/iceberg/util/Pair.java
Patch:
@@ -119,8 +119,7 @@ public int hashCode() {
   public boolean equals(Object other) {
     if (this == other) {
       return true;
-    }
-    if (getClass() != other.getClass()) {
+    } else if (!(other instanceof Pair)) {
       return false;
     }
     Pair<?, ?> otherPair = (Pair<?, ?>) other;

File: core/src/main/java/org/apache/iceberg/util/StructLikeWrapper.java
Patch:
@@ -50,9 +50,7 @@ public StructLike get() {
   public boolean equals(Object other) {
     if (this == other) {
       return true;
-    }
-
-    if (other == null || getClass() != other.getClass()) {
+    } else if (!(other instanceof StructLikeWrapper)) {
       return false;
     }
 

File: data/src/main/java/org/apache/iceberg/data/GenericRecord.java
Patch:
@@ -157,9 +157,7 @@ public String toString() {
   public boolean equals(Object other) {
     if (this == other) {
       return true;
-    }
-
-    if (other == null || getClass() != other.getClass()) {
+    } else if (!(other instanceof GenericRecord)) {
       return false;
     }
 

File: spark/src/main/java/org/apache/iceberg/spark/source/PartitionKey.java
Patch:
@@ -142,8 +142,7 @@ public <T> void set(int pos, T value) {
   public boolean equals(Object o) {
     if (this == o) {
       return true;
-    }
-    if (o == null || getClass() != o.getClass()) {
+    } else if (!(o instanceof PartitionKey)) {
       return false;
     }
 

File: core/src/main/java/org/apache/iceberg/hadoop/SerializableConfiguration.java
Patch:
@@ -32,8 +32,8 @@ public class SerializableConfiguration implements Serializable {
 
   private transient Configuration hadoopConf;
 
-  public SerializableConfiguration(Configuration hadoopCOnf) {
-    this.hadoopConf = hadoopCOnf;
+  public SerializableConfiguration(Configuration hadoopConf) {
+    this.hadoopConf = hadoopConf;
   }
 
   private void writeObject(ObjectOutputStream out) throws IOException {

File: api/src/main/java/org/apache/iceberg/catalog/TableIdentifier.java
Patch:
@@ -76,6 +76,7 @@ public String name() {
     return name;
   }
 
+  @Override
   public String toString() {
     return namespace.toString() + "." + name;
   }

File: core/src/main/java/org/apache/iceberg/DataTableScan.java
Patch:
@@ -68,6 +68,7 @@ protected TableScan newRefinedScan(
         ops, table, snapshotId, schema, rowFilter, caseSensitive, colStats, selectedColumns, options);
   }
 
+  @Override
   public CloseableIterable<FileScanTask> planFiles(TableOperations ops, Snapshot snapshot,
                                                    Expression rowFilter, boolean caseSensitive, boolean colStats) {
     LoadingCache<Integer, ManifestEvaluator> evalCache = Caffeine.newBuilder().build(specId -> {
@@ -105,6 +106,7 @@ public CloseableIterable<FileScanTask> planFiles(TableOperations ops, Snapshot s
     }
   }
 
+  @Override
   protected long targetSplitSize(TableOperations ops) {
     return ops.current().propertyAsLong(
         TableProperties.SPLIT_SIZE, TableProperties.SPLIT_SIZE_DEFAULT);

File: core/src/main/java/org/apache/iceberg/ManifestReader.java
Patch:
@@ -152,6 +152,7 @@ public FilteredManifest filterRows(Expression expr) {
     return new FilteredManifest(this, alwaysTrue(), expr, fileSchema, ALL_COLUMNS, true);
   }
 
+  @Override
   public FilteredManifest caseSensitive(boolean caseSensitive) {
     return new FilteredManifest(this, alwaysTrue(), alwaysTrue(), fileSchema, ALL_COLUMNS, caseSensitive);
   }

File: hive/src/main/java/org/apache/iceberg/hive/HiveCatalog.java
Patch:
@@ -144,6 +144,7 @@ public TableOperations newTableOps(TableIdentifier tableIdentifier) {
     return new HiveTableOperations(conf, clients, dbName, tableName);
   }
 
+  @Override
   protected String defaultWarehouseLocation(TableIdentifier tableIdentifier) {
     String warehouseLocation = conf.get("hive.metastore.warehouse.dir");
     Preconditions.checkNotNull(

File: data/src/main/java/org/apache/iceberg/data/avro/IcebergDecoder.java
Patch:
@@ -166,7 +166,7 @@ private static class RawDecoder<D> extends MessageDecoder.BaseDecoder<D> {
      * @param writeSchema the schema used to decode buffers
      */
     private RawDecoder(org.apache.iceberg.Schema readSchema, org.apache.avro.Schema writeSchema) {
-      this.reader = new ProjectionDatumReader<>(DataReader::create, readSchema, ImmutableMap.of());
+      this.reader = new ProjectionDatumReader<>(DataReader::create, readSchema, ImmutableMap.of(), null);
       this.reader.setSchema(writeSchema);
     }
 

File: parquet/src/main/java/org/apache/iceberg/parquet/ParquetUtil.java
Patch:
@@ -19,7 +19,6 @@
 
 package org.apache.iceberg.parquet;
 
-import com.google.common.collect.ImmutableList;
 import com.google.common.collect.Maps;
 import com.google.common.collect.Sets;
 import java.io.IOException;
@@ -141,7 +140,7 @@ public static List<Long> getSplitOffsets(ParquetMetadata md) {
       splitOffsets.add(blockMetaData.getStartingPos());
     }
     Collections.sort(splitOffsets);
-    return ImmutableList.copyOf(splitOffsets);
+    return splitOffsets;
   }
 
   // we allow struct nesting, but not maps or arrays

File: core/src/main/java/org/apache/iceberg/MergingSnapshotProducer.java
Patch:
@@ -87,6 +87,7 @@ public String partition() {
   private final AtomicInteger manifestCount = new AtomicInteger(0);
   private final List<DataFile> newFiles = Lists.newArrayList();
   private final List<ManifestFile> appendManifests = Lists.newArrayList();
+  private final SnapshotSummary.Builder appendedManifestsSummary = SnapshotSummary.builder();
   private final Set<CharSequenceWrapper> deletePaths = Sets.newHashSet();
   private final Set<StructLikeWrapper> deleteFilePartitions = Sets.newHashSet();
   private final Set<StructLikeWrapper> dropPartitions = Sets.newHashSet();
@@ -205,7 +206,7 @@ protected void add(ManifestFile manifest) {
     try (ManifestReader reader = ManifestReader.read(
         ops.io().newInputFile(manifest.path()), ops.current()::spec)) {
       appendManifests.add(ManifestWriter.copyAppendManifest(
-          reader, manifestPath(manifestCount.getAndIncrement()), snapshotId(), summaryBuilder));
+          reader, manifestPath(manifestCount.getAndIncrement()), snapshotId(), appendedManifestsSummary));
     } catch (IOException e) {
       throw new RuntimeIOException(e, "Failed to close manifest: %s", manifest);
     }
@@ -219,6 +220,7 @@ protected Map<String, String> summary() {
   @Override
   public List<ManifestFile> apply(TableMetadata base) {
     summaryBuilder.clear();
+    summaryBuilder.merge(appendedManifestsSummary);
 
     if (filterUpdated) {
       cleanUncommittedFilters(SnapshotProducer.EMPTY_SET);

File: core/src/main/java/org/apache/iceberg/BaseTransaction.java
Patch:
@@ -72,13 +72,13 @@ enum TransactionType {
   BaseTransaction(TableOperations ops, TransactionType type, TableMetadata start) {
     this.ops = ops;
     this.transactionTable = new TransactionTable();
+    this.current = start;
     this.transactionOps = new TransactionTableOperations();
     this.updates = Lists.newArrayList();
     this.intermediateSnapshotIds = Sets.newHashSet();
     this.base = ops.current();
     this.type = type;
     this.lastBase = null;
-    this.current = start;
   }
 
   @Override

File: data/src/main/java/org/apache/iceberg/data/GenericRecord.java
Patch:
@@ -117,7 +117,7 @@ public Object get(int pos) {
   @Override
   public <T> T get(int pos, Class<T> javaClass) {
     Object value = get(pos);
-    if (javaClass.isInstance(value)) {
+    if (value == null || javaClass.isInstance(value)) {
       return javaClass.cast(value);
     } else {
       throw new IllegalStateException("Not an instance of " + javaClass.getName() + ": " + value);

File: hive/src/main/java/org/apache/iceberg/hive/ClientPool.java
Patch:
@@ -26,7 +26,7 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-abstract class ClientPool<C, E extends Exception> implements Closeable {
+public abstract class ClientPool<C, E extends Exception> implements Closeable {
   private static final Logger LOG = LoggerFactory.getLogger(ClientPool.class);
 
   private final int poolSize;
@@ -45,7 +45,7 @@ abstract class ClientPool<C, E extends Exception> implements Closeable {
     this.closed = false;
   }
 
-  interface Action<R, C, E extends Exception> {
+  public interface Action<R, C, E extends Exception> {
     R run(C client) throws E;
   }
 

File: hive/src/main/java/org/apache/iceberg/hive/HiveClientPool.java
Patch:
@@ -26,14 +26,14 @@
 import org.apache.thrift.TException;
 import org.apache.thrift.transport.TTransportException;
 
-class HiveClientPool extends ClientPool<HiveMetaStoreClient, TException> {
+public class HiveClientPool extends ClientPool<HiveMetaStoreClient, TException> {
   private final HiveConf hiveConf;
 
   HiveClientPool(Configuration conf) {
     this(conf.getInt("iceberg.hive.client-pool-size", 5), conf);
   }
 
-  HiveClientPool(int poolSize, Configuration conf) {
+  public HiveClientPool(int poolSize, Configuration conf) {
     super(poolSize, TTransportException.class);
     this.hiveConf = new HiveConf(conf, HiveClientPool.class);
   }

File: api/src/main/java/org/apache/iceberg/types/Conversions.java
Patch:
@@ -51,7 +51,7 @@ public static Object fromPartitionString(Type type, String asString) {
       case LONG:
         return Long.valueOf(asString);
       case FLOAT:
-        return Long.valueOf(asString);
+        return Float.valueOf(asString);
       case DOUBLE:
         return Double.valueOf(asString);
       case STRING:

File: api/src/main/java/org/apache/iceberg/expressions/Literals.java
Patch:
@@ -95,7 +95,7 @@ static <T> BelowMin<T> belowMin() {
 
   private abstract static class BaseLiteral<T> implements Literal<T> {
     private final T value;
-    private volatile ByteBuffer byteBuffer = null;
+    private transient volatile ByteBuffer byteBuffer = null;
 
     BaseLiteral(T value) {
       Preconditions.checkNotNull(value, "Literal values cannot be null");

File: api/src/main/java/org/apache/iceberg/transforms/Dates.java
Patch:
@@ -66,6 +66,9 @@ public boolean canTransform(Type type) {
 
   @Override
   public Type getResultType(Type sourceType) {
+    if (granularity == ChronoUnit.DAYS) {
+      return Types.DateType.get();
+    }
     return Types.IntegerType.get();
   }
 

File: api/src/main/java/org/apache/iceberg/transforms/Timestamps.java
Patch:
@@ -68,6 +68,9 @@ public boolean canTransform(Type type) {
 
   @Override
   public Type getResultType(Type sourceType) {
+    if (granularity == ChronoUnit.DAYS) {
+      return Types.DateType.get();
+    }
     return Types.IntegerType.get();
   }
 

File: api/src/main/java/org/apache/iceberg/transforms/Transform.java
Patch:
@@ -43,7 +43,7 @@ public interface Transform<S, T> extends Serializable {
   T apply(S value);
 
   /**
-   * Checks whether this function can be applied to the give {@link Type}.
+   * Checks whether this function can be applied to the given {@link Type}.
    *
    * @param type a type
    * @return true if this transform can be applied to the type, false otherwise

File: core/src/main/java/org/apache/iceberg/GenericPartitionFieldSummary.java
Patch:
@@ -29,6 +29,7 @@
 import org.apache.iceberg.ManifestFile.PartitionFieldSummary;
 import org.apache.iceberg.avro.AvroSchemaUtil;
 import org.apache.iceberg.types.Types;
+import org.apache.iceberg.util.ByteBuffers;
 
 public class GenericPartitionFieldSummary
     implements PartitionFieldSummary, StructLike, IndexedRecord, SchemaConstructable, Serializable {
@@ -87,8 +88,8 @@ public GenericPartitionFieldSummary(boolean containsNull, ByteBuffer lowerBound,
   private GenericPartitionFieldSummary(GenericPartitionFieldSummary toCopy) {
     this.avroSchema = toCopy.avroSchema;
     this.containsNull = toCopy.containsNull;
-    this.lowerBound = toCopy.lowerBound;
-    this.upperBound = toCopy.upperBound;
+    this.lowerBound = ByteBuffers.copy(toCopy.lowerBound);
+    this.upperBound = ByteBuffers.copy(toCopy.upperBound);
     this.fromProjectionPos = toCopy.fromProjectionPos;
   }
 

File: hive/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java
Patch:
@@ -92,7 +92,9 @@ public TableMetadata refresh() {
       String tableType = table.getParameters().get(TABLE_TYPE_PROP);
 
       if (tableType == null || !tableType.equalsIgnoreCase(ICEBERG_TABLE_TYPE_VALUE)) {
-        throw new IllegalArgumentException(String.format("Invalid tableName, not Iceberg: %s.%s", database, table));
+        throw new IllegalArgumentException(String.format("Type of %s.%s is %s, not %s",
+            database, tableName,
+            tableType /* actual type */, ICEBERG_TABLE_TYPE_VALUE /* expected type */));
       }
 
       metadataLocation = table.getParameters().get(METADATA_LOCATION_PROP);

File: core/src/main/java/org/apache/iceberg/SnapshotProducer.java
Patch:
@@ -270,11 +270,11 @@ public void commit() {
       } else {
         // saved may not be present if the latest metadata couldn't be loaded due to eventual
         // consistency problems in refresh. in that case, don't clean up.
-        LOG.info("Failed to load committed snapshot, skipping manifest clean-up");
+        LOG.warn("Failed to load committed snapshot, skipping manifest clean-up");
       }
 
     } catch (RuntimeException e) {
-      LOG.info("Failed to load committed table metadata, skipping manifest clean-up", e);
+      LOG.warn("Failed to load committed table metadata, skipping manifest clean-up", e);
     }
   }
 

File: api/src/main/java/org/apache/iceberg/util/UnicodeUtil.java
Patch:
@@ -79,11 +79,11 @@ public static Literal<CharSequence> truncateStringMax(Literal<CharSequence> inpu
 
     // Try incrementing the code points from the end
     for (int i = length - 1; i >= 0; i--) {
-      int nextCodePoint = truncatedStringBuffer.codePointAt(i) + 1;
+      // Get the offset in the truncated string buffer where the number of unicode characters = i
+      int offsetByCodePoint = truncatedStringBuffer.offsetByCodePoints(0, i);
+      int nextCodePoint = truncatedStringBuffer.codePointAt(offsetByCodePoint) + 1;
       // No overflow
       if (nextCodePoint != 0 && Character.isValidCodePoint(nextCodePoint)) {
-        // Get the offset in the truncated string buffer where the number of unicode characters = i
-        int offsetByCodePoint = truncatedStringBuffer.offsetByCodePoints(0, i);
         truncatedStringBuffer.setLength(offsetByCodePoint);
         // Append next code point to the truncated substring
         truncatedStringBuffer.appendCodePoint(nextCodePoint);

File: hive/src/main/java/org/apache/iceberg/hive/HiveTypeConverter.java
Patch:
@@ -45,7 +45,7 @@ public static String convert(Type type) {
       case DATE:
         return "date";
       case TIME:
-        throw new UnsupportedOperationException("Hive does not support time fields");
+        return "string";
       case TIMESTAMP:
         return "timestamp";
       case STRING:

File: api/src/main/java/org/apache/iceberg/transforms/Transforms.java
Patch:
@@ -131,7 +131,7 @@ public static <T> Transform<T, Integer> day(Type type) {
         return (Transform<T, Integer>) Timestamps.DAY;
       default:
         throw new IllegalArgumentException(
-            "Cannot partition type " + type + " by month");
+            "Cannot partition type " + type + " by day");
     }
   }
 

File: core/src/main/java/org/apache/iceberg/TableProperties.java
Patch:
@@ -80,7 +80,7 @@ private TableProperties() {}
 
   // This only applies to files written after this property is set. Files previously written aren't
   // relocated to reflect this parameter.
-  // If not set, defaults to a "meatdata" folder underneath the root path of the table.
+  // If not set, defaults to a "metadata" folder underneath the root path of the table.
   public static final String WRITE_METADATA_LOCATION = "write.metadata.path";
 
   public static final String MANIFEST_LISTS_ENABLED = "write.manifest-lists.enabled";

File: hive/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java
Patch:
@@ -208,8 +208,8 @@ private StorageDescriptor storageDescriptor(TableMetadata metadata) {
     final StorageDescriptor storageDescriptor = new StorageDescriptor();
     storageDescriptor.setCols(columns(metadata.schema()));
     storageDescriptor.setLocation(metadata.location());
-    storageDescriptor.setOutputFormat("org.apache.hadoop.mapred.FileInputFormat");
-    storageDescriptor.setInputFormat("org.apache.hadoop.mapred.FileOutputFormat");
+    storageDescriptor.setOutputFormat("org.apache.hadoop.mapred.FileOutputFormat");
+    storageDescriptor.setInputFormat("org.apache.hadoop.mapred.FileInputFormat");
     SerDeInfo serDeInfo = new SerDeInfo();
     serDeInfo.setSerializationLib("org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe");
     storageDescriptor.setSerdeInfo(serDeInfo);

File: core/src/main/java/org/apache/iceberg/ReplaceManifests.java
Patch:
@@ -52,7 +52,7 @@ public class ReplaceManifests extends SnapshotProducer<RewriteManifests> impleme
   private final Set<ManifestFile> replacedManifests = Collections.synchronizedSet(new HashSet<>());
   private final Map<Object, WriterWrapper> writers = Collections.synchronizedMap(new HashMap<>());
 
-  private final AtomicInteger manifestCount = new AtomicInteger(0);
+  private final AtomicInteger manifestSuffix = new AtomicInteger(0);
   private final AtomicLong entryCount = new AtomicLong(0);
 
   private final Map<String, String> summaryProps = new HashMap<>();
@@ -155,7 +155,6 @@ private void addExistingFromNewCommit(List<ManifestFile> currentManifests) {
   private void reset() {
     cleanAll();
     entryCount.set(0);
-    manifestCount.set(0);
     keptManifests.clear();
     replacedManifests.clear();
     newManifests.clear();
@@ -240,7 +239,7 @@ synchronized void addEntry(ManifestEntry entry) {
     }
 
     private ManifestWriter newWriter() {
-      return new ManifestWriter(spec, manifestPath(manifestCount.getAndIncrement()), snapshotId());
+      return new ManifestWriter(spec, manifestPath(manifestSuffix.getAndIncrement()), snapshotId());
     }
 
     synchronized void close() {

File: spark/src/main/java/org/apache/iceberg/spark/SparkSchemaUtil.java
Patch:
@@ -70,17 +70,18 @@ public static Schema schemaForTable(SparkSession spark, String name) {
    *
    * @param spark a Spark session
    * @param name a table name and (optional) database
-   * @return a PartitionSpec for the table, if found
+   * @return a PartitionSpec for the table
    * @throws AnalysisException if thrown by the Spark catalog
    */
   public static PartitionSpec specForTable(SparkSession spark, String name) throws AnalysisException {
     List<String> parts = Lists.newArrayList(Splitter.on('.').limit(2).split(name));
     String db = parts.size() == 1 ? "default" : parts.get(0);
     String table = parts.get(parts.size() == 1 ? 0 : 1);
 
-    return identitySpec(
+    PartitionSpec spec = identitySpec(
         schemaForTable(spark, name),
         spark.catalog().listColumns(db, table).collectAsList());
+    return spec == null ? PartitionSpec.unpartitioned() : spec;
   }
 
   /**

File: core/src/main/java/org/apache/iceberg/ManifestEntry.java
Patch:
@@ -166,7 +166,7 @@ static Schema projectSchema(StructType partitionType, Collection<String> columns
         new Schema(DataFile.getType(partitionType).fields()).select(columns).asStruct());
   }
 
-  private static Schema wrapFileSchema(StructType fileStruct) {
+  static Schema wrapFileSchema(StructType fileStruct) {
     // ids for top-level columns are assigned from 1000
     return new Schema(
         required(0, "status", IntegerType.get()),

File: parquet/src/main/java/org/apache/iceberg/parquet/Parquet.java
Patch:
@@ -195,7 +195,7 @@ public <D> FileAppender<D> build() throws IOException {
 
         ParquetProperties parquetProperties = ParquetProperties.builder()
             .withWriterVersion(writerVersion)
-            .withDictionaryPageSize(pageSize)
+            .withPageSize(pageSize)
             .withDictionaryPageSize(dictionaryPageSize)
             .build();
 

File: core/src/main/java/org/apache/iceberg/TableProperties.java
Patch:
@@ -85,4 +85,7 @@ private TableProperties() {}
 
   public static final String MANIFEST_LISTS_ENABLED = "write.manifest-lists.enabled";
   public static final boolean MANIFEST_LISTS_ENABLED_DEFAULT = true;
+
+  public static final String WRITE_METADATA_TRUNCATE_BYTES = "write.metadata.truncate-length";
+  public static final int WRITE_METADATA_TRUNCATE_BYTES_DEFAULT = 16;
 }

File: spark/src/test/java/org/apache/iceberg/spark/source/TestParquetScan.java
Patch:
@@ -49,9 +49,7 @@
 import org.junit.Rule;
 import org.junit.rules.TemporaryFolder;
 
-import static org.apache.iceberg.Files.localInput;
 import static org.apache.iceberg.Files.localOutput;
-import static org.apache.iceberg.parquet.ParquetUtil.fileMetrics;
 
 public class TestParquetScan extends AvroDataTest {
   private static final Configuration CONF = new Configuration();
@@ -106,7 +104,7 @@ protected void writeAndValidate(Schema schema) throws IOException {
     DataFile file = DataFiles.builder(PartitionSpec.unpartitioned())
         .withFileSizeInBytes(parquetFile.length())
         .withPath(parquetFile.toString())
-        .withMetrics(fileMetrics(localInput(parquetFile)))
+        .withRecordCount(100)
         .build();
 
     table.newAppend().appendFile(file).commit();

File: parquet/src/main/java/org/apache/iceberg/parquet/ParquetReader.java
Patch:
@@ -245,6 +245,7 @@ private void advance() {
       }
 
       nextRowGroupStart += pages.getRowCount();
+      nextRowGroup += 1;
 
       model.setPageSource(pages);
     }

File: core/src/main/java/org/apache/iceberg/MergingSnapshotProducer.java
Patch:
@@ -116,8 +116,6 @@ public String partition() {
         .propertyAsInt(MANIFEST_MIN_MERGE_COUNT, MANIFEST_MIN_MERGE_COUNT_DEFAULT);
   }
 
-  protected abstract ThisT self();
-
   @Override
   public ThisT set(String property, String value) {
     summaryBuilder.set(property, value);

File: common/src/main/java/org/apache/iceberg/common/DynFields.java
Patch:
@@ -20,7 +20,7 @@
 package org.apache.iceberg.common;
 
 import com.google.common.base.Joiner;
-import com.google.common.base.Objects;
+import com.google.common.base.MoreObjects;
 import com.google.common.base.Preconditions;
 import com.google.common.base.Throwables;
 import com.google.common.collect.Sets;
@@ -69,7 +69,7 @@ public void set(Object target, T value) {
 
     @Override
     public String toString() {
-      return Objects.toStringHelper(this)
+      return MoreObjects.toStringHelper(this)
           .add("class", field.getDeclaringClass().toString())
           .add("name", name)
           .add("type", field.getType())

File: core/src/main/java/org/apache/iceberg/BaseFileScanTask.java
Patch:
@@ -20,7 +20,7 @@
 package org.apache.iceberg;
 
 import com.google.common.annotations.VisibleForTesting;
-import com.google.common.base.Objects;
+import com.google.common.base.MoreObjects;
 import com.google.common.collect.ImmutableList;
 import java.util.ArrayList;
 import java.util.Iterator;
@@ -86,7 +86,7 @@ public Iterable<FileScanTask> split(long targetSplitSize) {
 
   @Override
   public String toString() {
-    return Objects.toStringHelper(this)
+    return MoreObjects.toStringHelper(this)
         .add("file", file.path())
         .add("partition_data", file.partition())
         .add("residual", residual())

File: core/src/main/java/org/apache/iceberg/BaseSnapshot.java
Patch:
@@ -19,7 +19,7 @@
 
 package org.apache.iceberg;
 
-import com.google.common.base.Objects;
+import com.google.common.base.MoreObjects;
 import com.google.common.collect.Iterables;
 import com.google.common.collect.Lists;
 import java.io.IOException;
@@ -182,7 +182,7 @@ private void cacheChanges() {
 
   @Override
   public String toString() {
-    return Objects.toStringHelper(this)
+    return MoreObjects.toStringHelper(this)
         .add("id", snapshotId)
         .add("timestamp_ms", timestampMillis)
         .add("operation", operation)

File: core/src/main/java/org/apache/iceberg/BaseTableScan.java
Patch:
@@ -21,7 +21,7 @@
 
 import com.github.benmanes.caffeine.cache.Caffeine;
 import com.github.benmanes.caffeine.cache.LoadingCache;
-import com.google.common.base.Objects;
+import com.google.common.base.MoreObjects;
 import com.google.common.base.Preconditions;
 import com.google.common.collect.FluentIterable;
 import com.google.common.collect.ImmutableList;
@@ -241,7 +241,7 @@ public boolean isCaseSensitive() {
 
   @Override
   public String toString() {
-    return Objects.toStringHelper(this)
+    return MoreObjects.toStringHelper(this)
         .add("table", table)
         .add("projection", schema().asStruct())
         .add("filter", rowFilter)

File: core/src/main/java/org/apache/iceberg/GenericDataFile.java
Patch:
@@ -19,7 +19,7 @@
 
 package org.apache.iceberg;
 
-import com.google.common.base.Objects;
+import com.google.common.base.MoreObjects;
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import java.io.IOException;
@@ -406,7 +406,7 @@ public int size() {
 
   @Override
   public String toString() {
-    return Objects.toStringHelper(this)
+    return MoreObjects.toStringHelper(this)
         .add("file_path", filePath)
         .add("file_format", format)
         .add("partition", partitionData)

File: core/src/main/java/org/apache/iceberg/GenericManifestFile.java
Patch:
@@ -19,6 +19,7 @@
 
 package org.apache.iceberg;
 
+import com.google.common.base.MoreObjects;
 import com.google.common.base.Objects;
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.Iterables;
@@ -296,7 +297,7 @@ public int hashCode() {
 
   @Override
   public String toString() {
-    return Objects.toStringHelper(this)
+    return MoreObjects.toStringHelper(this)
         .add("path", manifestPath)
         .add("length", length)
         .add("partition_spec_id", specId)

File: core/src/main/java/org/apache/iceberg/GenericPartitionFieldSummary.java
Patch:
@@ -19,7 +19,7 @@
 
 package org.apache.iceberg;
 
-import com.google.common.base.Objects;
+import com.google.common.base.MoreObjects;
 import java.io.Serializable;
 import java.nio.ByteBuffer;
 import java.util.List;
@@ -182,7 +182,7 @@ public Schema getSchema() {
 
   @Override
   public String toString() {
-    return Objects.toStringHelper(this)
+    return MoreObjects.toStringHelper(this)
         .add("contains_null", containsNull)
         .add("lower_bound", lowerBound)
         .add("upper_bound", upperBound)

File: core/src/main/java/org/apache/iceberg/ManifestEntry.java
Patch:
@@ -19,7 +19,7 @@
 
 package org.apache.iceberg;
 
-import com.google.common.base.Objects;
+import com.google.common.base.MoreObjects;
 import java.util.Collection;
 import org.apache.avro.generic.IndexedRecord;
 import org.apache.avro.specific.SpecificData;
@@ -176,7 +176,7 @@ private static Schema wrapFileSchema(StructType fileStruct) {
 
   @Override
   public String toString() {
-    return Objects.toStringHelper(this)
+    return MoreObjects.toStringHelper(this)
         .add("status", status)
         .add("snapshot_id", snapshotId)
         .add("file", file)

File: core/src/main/java/org/apache/iceberg/TableMetadata.java
Patch:
@@ -19,6 +19,7 @@
 
 package org.apache.iceberg;
 
+import com.google.common.base.MoreObjects;
 import com.google.common.base.Objects;
 import com.google.common.base.Preconditions;
 import com.google.common.collect.ImmutableList;
@@ -115,7 +116,7 @@ public int hashCode() {
 
     @Override
     public String toString() {
-      return Objects.toStringHelper(this)
+      return MoreObjects.toStringHelper(this)
           .add("timestampMillis", timestampMillis)
           .add("snapshotId", snapshotId)
           .toString();

File: core/src/main/java/org/apache/iceberg/BaseSnapshot.java
Patch:
@@ -163,12 +163,12 @@ private void cacheChanges() {
           ops.current()::spec)) {
         for (ManifestEntry add : reader.addedFiles()) {
           if (add.snapshotId() == snapshotId) {
-            adds.add(add.file().copy());
+            adds.add(add.file().copyWithoutStats());
           }
         }
         for (ManifestEntry delete : reader.deletedFiles()) {
           if (delete.snapshotId() == snapshotId) {
-            deletes.add(delete.file().copy());
+            deletes.add(delete.file().copyWithoutStats());
           }
         }
       } catch (IOException e) {

File: core/src/main/java/org/apache/iceberg/ManifestReader.java
Patch:
@@ -183,10 +183,10 @@ private void cacheChanges() {
     for (ManifestEntry entry : entries(CHANGE_COLUNNS)) {
       switch (entry.status()) {
         case ADDED:
-          adds.add(entry.copy());
+          adds.add(entry.copyWithoutStats());
           break;
         case DELETED:
-          deletes.add(entry.copy());
+          deletes.add(entry.copyWithoutStats());
           break;
         default:
       }

File: core/src/main/java/org/apache/iceberg/MergingSnapshotProducer.java
Patch:
@@ -434,7 +434,7 @@ private ManifestFile filterManifestWithDeletedFiles(
             } else {
               // only add the file to deletes if it is a new delete
               // this keeps the snapshot summary accurate for non-duplicate data
-              deletedFiles.add(entry.file().copy());
+              deletedFiles.add(entry.file().copyWithoutStats());
             }
             deletedPaths.add(wrapper);
 

File: api/src/main/java/org/apache/iceberg/io/CloseableIterable.java
Patch:
@@ -20,12 +20,12 @@
 package org.apache.iceberg.io;
 
 import com.google.common.base.Preconditions;
-import org.apache.iceberg.exceptions.RuntimeIOException;
 import java.io.Closeable;
 import java.io.IOException;
 import java.util.Collections;
 import java.util.Iterator;
 import java.util.function.Function;
+import org.apache.iceberg.exceptions.RuntimeIOException;
 
 public interface CloseableIterable<T> extends Iterable<T>, Closeable {
   static <E> CloseableIterable<E> withNoopClose(Iterable<E> iterable) {

File: api/src/main/java/org/apache/iceberg/transforms/Timestamps.java
Patch:
@@ -53,7 +53,8 @@ public Integer apply(Long timestampMicros) {
     OffsetDateTime timestamp = Instant
         .ofEpochSecond(timestampMicros / 1_000_000)
         .atOffset(ZoneOffset.UTC);
-    return (int) granularity.between(EPOCH, timestamp);
+    Integer year = Long.valueOf(granularity.between(EPOCH, timestamp)).intValue();
+    return year;
   }
 
   @Override

File: core/src/main/java/org/apache/iceberg/MergingSnapshotUpdate.java
Patch:
@@ -305,7 +305,7 @@ private void cleanUncommittedFilters(Set<ManifestFile> committed) {
 
   @Override
   protected void cleanUncommitted(Set<ManifestFile> committed) {
-    if (!committed.contains(newManifest)) {
+    if (newManifest != null && !committed.contains(newManifest)) {
       deleteFile(newManifest.path());
       this.newManifest = null;
     }

File: spark/src/main/java/org/apache/iceberg/spark/source/Reader.java
Patch:
@@ -58,6 +58,7 @@
 import org.apache.iceberg.spark.data.SparkParquetReaders;
 import org.apache.iceberg.types.TypeUtil;
 import org.apache.iceberg.types.Types;
+import org.apache.iceberg.util.ByteBuffers;
 import org.apache.spark.sql.catalyst.InternalRow;
 import org.apache.spark.sql.catalyst.expressions.Attribute;
 import org.apache.spark.sql.catalyst.expressions.AttributeReference;
@@ -521,8 +522,7 @@ private static Object convert(Object value, DataType type) {
       if (type instanceof StringType) {
         return UTF8String.fromString(value.toString());
       } else if (type instanceof BinaryType) {
-        ByteBuffer buffer = (ByteBuffer) value;
-        return buffer.get(new byte[buffer.remaining()]);
+        return ByteBuffers.toByteArray((ByteBuffer) value);
       } else if (type instanceof DecimalType) {
         return Decimal.fromDecimal(value);
       }

File: core/src/main/java/org/apache/iceberg/avro/AvroIterable.java
Patch:
@@ -80,12 +80,12 @@ public Iterator<D> iterator() {
       reader = new AvroRangeIterator<>(reader, start, end);
     }
 
+    addCloseable(reader);
+
     if (reuseContainers) {
       return new AvroReuseIterator<>(reader);
     }
 
-    addCloseable(reader);
-
     return reader;
   }
 

File: parquet/src/main/java/com/netflix/iceberg/parquet/ParquetMetrics.java
Patch:
@@ -86,9 +86,9 @@ public static Metrics fromMetadata(ParquetMetadata metadata) {
           Types.NestedField field = fileSchema.asStruct().field(fieldId);
           if (field != null && stats.hasNonNullValue()) {
             updateMin(lowerBounds, fieldId,
-                fromParquetPrimitive(field.type(), stats.genericGetMin()));
+                fromParquetPrimitive(field.type(), column.getPrimitiveType(), stats.genericGetMin()));
             updateMax(upperBounds, fieldId,
-                fromParquetPrimitive(field.type(), stats.genericGetMax()));
+                fromParquetPrimitive(field.type(), column.getPrimitiveType(), stats.genericGetMax()));
           }
         }
       }

File: core/src/main/java/com/netflix/iceberg/util/StructLikeWrapper.java
Patch:
@@ -20,6 +20,7 @@
 package com.netflix.iceberg.util;
 
 import com.netflix.iceberg.StructLike;
+import java.util.Objects;
 
 /**
  * Wrapper to adapt StructLike for use in maps and sets by implementing equals and hashCode.
@@ -68,7 +69,7 @@ public boolean equals(Object other) {
     }
 
     for (int i = 0; i < len; i += 1) {
-      if (!struct.get(i, Object.class).equals(that.struct.get(i, Object.class))) {
+      if (!Objects.equals(struct.get(i, Object.class), that.struct.get(i, Object.class))) {
         return false;
       }
     }
@@ -82,7 +83,7 @@ public int hashCode() {
     int len = struct.size();
     result = 41 * result + len;
     for (int i = 0; i < len; i += 1) {
-      result = 41 * result + struct.get(i, Object.class).hashCode();
+      result = 41 * result + Objects.hashCode(struct.get(i, Object.class));
     }
     return result;
   }

File: api/src/main/java/com/netflix/iceberg/expressions/Binder.java
Patch:
@@ -86,7 +86,7 @@ public static Expression bind(StructType struct,
    * @throws IllegalStateException if any references are already bound
    */
   static Expression bind(StructType struct,
-                                   Expression expr) {
+                         Expression expr) {
     return Binder.bind(struct, expr, true);
   }
 

File: api/src/main/java/com/netflix/iceberg/expressions/Evaluator.java
Patch:
@@ -44,7 +44,7 @@ private EvalVisitor visitor() {
   }
 
   public Evaluator(Types.StructType struct, Expression unbound) {
-    this.expr = Binder.bind(struct, unbound);
+    this.expr = Binder.bind(struct, unbound, true);
   }
 
   public boolean eval(StructLike data) {

File: api/src/main/java/com/netflix/iceberg/expressions/InclusiveManifestEvaluator.java
Patch:
@@ -54,7 +54,7 @@ private ManifestEvalVisitor visitor() {
 
   public InclusiveManifestEvaluator(PartitionSpec spec, Expression rowFilter) {
     this.struct = spec.partitionType();
-    this.expr = Binder.bind(struct, rewriteNot(Projections.inclusive(spec).project(rowFilter)));
+    this.expr = Binder.bind(struct, rewriteNot(Projections.inclusive(spec).project(rowFilter)), true);
   }
 
   /**

File: api/src/main/java/com/netflix/iceberg/expressions/InclusiveMetricsEvaluator.java
Patch:
@@ -56,7 +56,7 @@ private MetricsEvalVisitor visitor() {
   public InclusiveMetricsEvaluator(Schema schema, Expression unbound) {
     this.schema = schema;
     this.struct = schema.asStruct();
-    this.expr = Binder.bind(struct, rewriteNot(unbound));
+    this.expr = Binder.bind(struct, rewriteNot(unbound), true);
   }
 
   /**

File: api/src/main/java/com/netflix/iceberg/expressions/Projections.java
Patch:
@@ -135,7 +135,7 @@ public Expression or(Expression leftResult, Expression rightResult) {
 
     @Override
     public <T> Expression predicate(UnboundPredicate<T> pred) {
-      Expression bound = pred.bind(spec.schema().asStruct());
+      Expression bound = pred.bind(spec.schema().asStruct(), true);
 
       if (bound instanceof BoundPredicate) {
         return predicate((BoundPredicate<?>) bound);

File: api/src/main/java/com/netflix/iceberg/expressions/ResidualEvaluator.java
Patch:
@@ -170,7 +170,7 @@ public <T> Expression predicate(BoundPredicate<T> pred) {
           .projectStrict(part.name(), pred);
 
       if (strictProjection != null) {
-        Expression bound = strictProjection.bind(spec.partitionType());
+        Expression bound = strictProjection.bind(spec.partitionType(), true);
         if (bound instanceof BoundPredicate) {
           // the predicate methods will evaluate and return alwaysTrue or alwaysFalse
           return super.predicate((BoundPredicate<?>) bound);
@@ -184,7 +184,7 @@ public <T> Expression predicate(BoundPredicate<T> pred) {
 
     @Override
     public <T> Expression predicate(UnboundPredicate<T> pred) {
-      Expression bound = pred.bind(spec.schema().asStruct());
+      Expression bound = pred.bind(spec.schema().asStruct(), true);
 
       if (bound instanceof BoundPredicate) {
         Expression boundResidual = predicate((BoundPredicate<?>) bound);

File: api/src/main/java/com/netflix/iceberg/expressions/StrictMetricsEvaluator.java
Patch:
@@ -57,7 +57,7 @@ private MetricsEvalVisitor visitor() {
   public StrictMetricsEvaluator(Schema schema, Expression unbound) {
     this.schema = schema;
     this.struct = schema.asStruct();
-    this.expr = Binder.bind(struct, rewriteNot(unbound));
+    this.expr = Binder.bind(struct, rewriteNot(unbound), true);
   }
 
   /**

File: api/src/test/java/com/netflix/iceberg/expressions/TestPredicateBinding.java
Patch:
@@ -179,7 +179,7 @@ public void testLongToIntegerConversion() {
     Assert.assertEquals("Less than or equal below min should be alwaysFalse",
         Expressions.alwaysFalse(), lteqMin.bind(struct));
 
-    Expression ltExpr = new UnboundPredicate<>(LT, ref("i"), (long) Integer.MAX_VALUE).bind(struct);
+    Expression ltExpr = new UnboundPredicate<>(LT, ref("i"), (long) Integer.MAX_VALUE).bind(struct, true);
     BoundPredicate<Integer> ltMax = assertAndUnwrap(ltExpr);
     Assert.assertEquals("Should translate bound to Integer",
         (Integer) Integer.MAX_VALUE, ltMax.literal().value());

File: api/src/test/java/com/netflix/iceberg/transforms/TestProjection.java
Patch:
@@ -71,7 +71,7 @@ public void testIdentityProjection() {
       UnboundPredicate<?> projected = assertAndUnwrapUnbound(expr);
 
       // check inclusive the bound predicate to ensure the types are correct
-      BoundPredicate<?> bound = assertAndUnwrap(predicate.bind(spec.schema().asStruct()));
+      BoundPredicate<?> bound = assertAndUnwrap(predicate.bind(spec.schema().asStruct(), true));
 
       Assert.assertEquals("Field name should match partition struct field",
           "id", projected.ref().name());
@@ -109,7 +109,7 @@ public void testStrictIdentityProjection() {
       UnboundPredicate<?> projected = assertAndUnwrapUnbound(expr);
 
       // check inclusive the bound predicate to ensure the types are correct
-      BoundPredicate<?> bound = assertAndUnwrap(predicate.bind(spec.schema().asStruct()));
+      BoundPredicate<?> bound = assertAndUnwrap(predicate.bind(spec.schema().asStruct(), true));
 
       Assert.assertEquals("Field name should match partition struct field",
           "id", projected.ref().name());

File: core/src/main/java/com/netflix/iceberg/BaseTableScan.java
Patch:
@@ -129,7 +129,7 @@ public TableScan select(Collection<String> columns) {
 
     // all of the filter columns are required
     requiredFieldIds.addAll(
-        Binder.boundReferences(table.schema().asStruct(), Collections.singletonList(rowFilter)));
+        Binder.boundReferences(table.schema().asStruct(), Collections.singletonList(rowFilter), true));
 
     // all of the projection columns are required
     requiredFieldIds.addAll(TypeUtil.getProjectedIds(table.schema().select(columns)));

File: parquet/src/main/java/com/netflix/iceberg/parquet/ParquetDictionaryRowGroupFilter.java
Patch:
@@ -68,7 +68,7 @@ private EvalVisitor visitor() {
   public ParquetDictionaryRowGroupFilter(Schema schema, Expression unbound) {
     this.schema = schema;
     this.struct = schema.asStruct();
-    this.expr = Binder.bind(struct, rewriteNot(unbound));
+    this.expr = Binder.bind(struct, rewriteNot(unbound), true);
   }
 
   /**

File: parquet/src/main/java/com/netflix/iceberg/parquet/ParquetFilters.java
Patch:
@@ -161,7 +161,7 @@ public <T> FilterPredicate predicate(BoundPredicate<T> pred) {
     }
 
     protected Expression bind(UnboundPredicate<?> pred) {
-      return pred.bind(schema.asStruct());
+      return pred.bind(schema.asStruct(), true);
     }
 
     @Override
@@ -189,7 +189,7 @@ private ConvertColumnFilterToParquet(Schema schema, String column) {
 
     protected Expression bind(UnboundPredicate<?> pred) {
       // instead of binding the predicate using the top-level schema, bind it to the partition data
-      return pred.bind(partitionStruct);
+      return pred.bind(partitionStruct, true);
     }
   }
 

File: parquet/src/main/java/com/netflix/iceberg/parquet/ParquetMetricsRowGroupFilter.java
Patch:
@@ -57,7 +57,7 @@ private MetricsEvalVisitor visitor() {
   public ParquetMetricsRowGroupFilter(Schema schema, Expression unbound) {
     this.schema = schema;
     this.struct = schema.asStruct();
-    this.expr = Binder.bind(struct, rewriteNot(unbound));
+    this.expr = Binder.bind(struct, rewriteNot(unbound), true);
   }
 
   /**

File: spark/src/main/java/com/netflix/iceberg/spark/SparkExpressions.java
Patch:
@@ -336,7 +336,7 @@ private static com.netflix.iceberg.expressions.Expression filter(
 
   public static Expression convert(com.netflix.iceberg.expressions.Expression filter,
                                    Schema schema) {
-    return visit(Binder.bind(schema.asStruct(), filter), new ExpressionToSpark(schema));
+    return visit(Binder.bind(schema.asStruct(), filter, true), new ExpressionToSpark(schema));
   }
 
   private static class ExpressionToSpark extends ExpressionVisitors.

File: spark/src/main/java/com/netflix/iceberg/spark/SparkSchemaUtil.java
Patch:
@@ -202,7 +202,7 @@ public static Schema prune(Schema schema, StructType requestedType) {
    * @throws IllegalArgumentException if the Spark type does not match the Schema
    */
   public static Schema prune(Schema schema, StructType requestedType, List<Expression> filters) {
-    Set<Integer> filterRefs = Binder.boundReferences(schema.asStruct(), filters);
+    Set<Integer> filterRefs = Binder.boundReferences(schema.asStruct(), filters, true);
     return new Schema(visit(schema, new PruneColumnsWithoutReordering(requestedType, filterRefs))
         .asNestedType()
         .asStructType()
@@ -225,7 +225,7 @@ public static Schema prune(Schema schema, StructType requestedType, List<Express
    * @throws IllegalArgumentException if the Spark type does not match the Schema
    */
   public static Schema prune(Schema schema, StructType requestedType, Expression filter) {
-    Set<Integer> filterRefs = Binder.boundReferences(schema.asStruct(), Collections.singletonList(filter));
+    Set<Integer> filterRefs = Binder.boundReferences(schema.asStruct(), Collections.singletonList(filter), true);
     return new Schema(visit(schema, new PruneColumnsWithoutReordering(requestedType, filterRefs))
         .asNestedType()
         .asStructType()

File: parquet/src/main/java/com/netflix/iceberg/parquet/Parquet.java
Patch:
@@ -28,6 +28,7 @@
 import com.netflix.iceberg.exceptions.RuntimeIOException;
 import com.netflix.iceberg.expressions.Expression;
 import com.netflix.iceberg.hadoop.HadoopInputFile;
+import com.netflix.iceberg.hadoop.HadoopOutputFile;
 import com.netflix.iceberg.io.CloseableIterable;
 import com.netflix.iceberg.io.FileAppender;
 import com.netflix.iceberg.io.InputFile;
@@ -172,8 +173,8 @@ public <D> FileAppender<D> build() throws IOException {
         Preconditions.checkArgument(writeSupport == null,
             "Cannot write with both write support and Parquet value writer");
         Configuration conf;
-        if (file instanceof HadoopInputFile) {
-          conf = ((HadoopInputFile) file).getConf();
+        if (file instanceof HadoopOutputFile) {
+          conf = ((HadoopOutputFile) file).getConf();
         } else {
           conf = new Configuration();
         }

File: parquet/src/main/java/com/netflix/iceberg/parquet/ColumnIterator.java
Patch:
@@ -32,7 +32,7 @@
 public abstract class ColumnIterator<T> implements TripleIterator<T> {
   @SuppressWarnings("unchecked")
   static <T> ColumnIterator<T> newIterator(ColumnDescriptor desc, String writerVersion) {
-    switch (desc.getType()) {
+    switch (desc.getPrimitiveType().getPrimitiveTypeName()) {
       case BOOLEAN:
         return (ColumnIterator<T>) new ColumnIterator<Boolean>(desc, writerVersion) {
           @Override
@@ -77,7 +77,8 @@ public Binary next() {
           }
         };
       default:
-        throw new UnsupportedOperationException("Unsupported primitive type: " + desc.getType());
+        throw new UnsupportedOperationException("Unsupported primitive type: "
+                + desc.getPrimitiveType().getPrimitiveTypeName());
     }
   }
 

File: parquet/src/main/java/com/netflix/iceberg/parquet/ColumnWriter.java
Patch:
@@ -26,7 +26,7 @@
 public abstract class ColumnWriter<T> implements TripleWriter<T> {
   @SuppressWarnings("unchecked")
   static <T> ColumnWriter<T> newWriter(ColumnDescriptor desc) {
-    switch (desc.getType()) {
+    switch (desc.getPrimitiveType().getPrimitiveTypeName()) {
       case BOOLEAN:
         return (ColumnWriter<T>) new ColumnWriter<Boolean>(desc) {
           @Override
@@ -71,7 +71,8 @@ public void write(int rl, Binary value) {
           }
         };
       default:
-        throw new UnsupportedOperationException("Unsupported primitive type: " + desc.getType());
+        throw new UnsupportedOperationException("Unsupported primitive type: "
+                + desc.getPrimitiveType().getPrimitiveTypeName());
     }
   }
 

File: parquet/src/main/java/com/netflix/iceberg/parquet/PageIterator.java
Patch:
@@ -50,7 +50,7 @@ abstract class PageIterator<T> implements TripleIterator<T> {
 
   @SuppressWarnings("unchecked")
   static <T> PageIterator<T> newIterator(ColumnDescriptor desc, String writerVersion) {
-    switch (desc.getType()) {
+    switch (desc.getPrimitiveType().getPrimitiveTypeName()) {
       case BOOLEAN:
         return (PageIterator<T>) new PageIterator<Boolean>(desc, writerVersion) {
           @Override
@@ -95,7 +95,8 @@ public Binary next() {
           }
         };
       default:
-        throw new UnsupportedOperationException("Unsupported primitive type: " + desc.getType());
+        throw new UnsupportedOperationException("Unsupported primitive type: "
+                + desc.getPrimitiveType().getPrimitiveTypeName());
     }
   }
 

File: parquet/src/main/java/com/netflix/iceberg/parquet/ParquetDictionaryRowGroupFilter.java
Patch:
@@ -325,7 +325,7 @@ private <T> Set<T> dict(int id, Comparator<T> comparator) {
       Set<T> dictSet = Sets.newTreeSet(comparator);;
 
       for (int i=0; i<=dict.getMaxId(); i++) {
-        switch (col.getType()) {
+        switch (col.getPrimitiveType().getPrimitiveTypeName()) {
           case BINARY: dictSet.add((T) conversion.apply(dict.decodeToBinary(i)));
             break;
           case INT32: dictSet.add((T) conversion.apply(dict.decodeToInt(i)));
@@ -338,7 +338,7 @@ private <T> Set<T> dict(int id, Comparator<T> comparator) {
             break;
           default:
             throw new IllegalArgumentException(
-                "Cannot decode dictionary of type: " + col.getType());
+                "Cannot decode dictionary of type: " + col.getPrimitiveType().getPrimitiveTypeName());
         }
       }
 

File: api/src/main/java/com/netflix/iceberg/Schema.java
Patch:
@@ -226,7 +226,7 @@ public Schema select(Collection<String> names) {
   public String toString() {
     return String.format("table {\n%s\n}",
         NEWLINE.join(struct.fields().stream()
-            .map(f -> "  " + f)
+            .map(f -> "  " + f + (f.doc() == null ? "" : " COMMENT '" + f.doc() + "'"))
             .collect(Collectors.toList())));
   }
 }

File: api/src/main/java/com/netflix/iceberg/types/AssignFreshIds.java
Patch:
@@ -53,9 +53,9 @@ public Type struct(Types.StructType struct, Iterable<Type> futures) {
       Types.NestedField field = fields.get(i);
       Type type = types.next();
       if (field.isOptional()) {
-        newFields.add(Types.NestedField.optional(newIds.get(i), field.name(), type));
+        newFields.add(Types.NestedField.optional(newIds.get(i), field.name(), type, field.doc()));
       } else {
-        newFields.add(Types.NestedField.required(newIds.get(i), field.name(), type));
+        newFields.add(Types.NestedField.required(newIds.get(i), field.name(), type, field.doc()));
       }
     }
 

File: api/src/main/java/com/netflix/iceberg/types/PruneColumns.java
Patch:
@@ -53,10 +53,10 @@ public Type struct(Types.StructType struct, List<Type> fieldResults) {
         sameTypes = false; // signal that some types were altered
         if (field.isOptional()) {
           selectedFields.add(
-              Types.NestedField.optional(field.fieldId(), field.name(), projectedType));
+              Types.NestedField.optional(field.fieldId(), field.name(), projectedType, field.doc()));
         } else {
           selectedFields.add(
-              Types.NestedField.required(field.fieldId(), field.name(), projectedType));
+              Types.NestedField.required(field.fieldId(), field.name(), projectedType, field.doc()));
         }
       }
     }

File: api/src/main/java/com/netflix/iceberg/types/ReassignIds.java
Patch:
@@ -58,9 +58,9 @@ public Type struct(Types.StructType struct, Iterable<Type> fieldTypes) {
       Types.NestedField field = fields.get(i);
       int sourceFieldId = sourceStruct.field(field.name()).fieldId();
       if (field.isRequired()) {
-        newFields.add(Types.NestedField.required(sourceFieldId, field.name(), types.get(i)));
+        newFields.add(Types.NestedField.required(sourceFieldId, field.name(), types.get(i), field.doc()));
       } else {
-        newFields.add(Types.NestedField.optional(sourceFieldId, field.name(), types.get(i)));
+        newFields.add(Types.NestedField.optional(sourceFieldId, field.name(), types.get(i), field.doc()));
       }
     }
 

File: core/src/test/java/com/netflix/iceberg/TestTableMetadataJson.java
Patch:
@@ -60,7 +60,7 @@ public class TestTableMetadataJson {
   public void testJsonConversion() throws Exception {
     Schema schema = new Schema(
         Types.NestedField.required(1, "x", Types.LongType.get()),
-        Types.NestedField.required(2, "y", Types.LongType.get()),
+        Types.NestedField.required(2, "y", Types.LongType.get(), "comment"),
         Types.NestedField.required(3, "z", Types.LongType.get())
     );
 

File: core/src/test/java/com/netflix/iceberg/hadoop/HadoopTableTestBase.java
Patch:
@@ -47,18 +47,18 @@
 public class HadoopTableTestBase {
   // Schema passed to create tables
   static final Schema SCHEMA = new Schema(
-      required(3, "id", Types.IntegerType.get()),
+      required(3, "id", Types.IntegerType.get(), "unique ID"),
       required(4, "data", Types.StringType.get())
   );
 
   // This is the actual schema for the table, with column IDs reassigned
   static final Schema TABLE_SCHEMA = new Schema(
-      required(1, "id", Types.IntegerType.get()),
+      required(1, "id", Types.IntegerType.get(), "unique ID"),
       required(2, "data", Types.StringType.get())
   );
 
   static final Schema UPDATED_SCHEMA = new Schema(
-      required(1, "id", Types.IntegerType.get()),
+      required(1, "id", Types.IntegerType.get(), "unique ID"),
       required(2, "data", Types.StringType.get()),
       optional(3, "n", Types.IntegerType.get())
   );

File: api/src/main/java/com/netflix/iceberg/Files.java
Patch:
@@ -99,6 +99,9 @@ public static InputFile localInput(File file) {
   }
 
   public static InputFile localInput(String file) {
+    if (file.startsWith("file:")) {
+      return localInput(new File(file.replaceFirst("file:", "")));
+    }
     return localInput(new File(file));
   }
 

File: core/src/main/java/com/netflix/iceberg/BaseSnapshot.java
Patch:
@@ -138,7 +138,7 @@ private void cacheChanges() {
 
     // accumulate adds and deletes from all manifests.
     // because manifests can be reused in newer snapshots, filter the changes by snapshot id.
-    for (String manifest : Iterables.transform(manifests, ManifestFile::path)) {
+    for (String manifest : Iterables.transform(manifests(), ManifestFile::path)) {
       try (ManifestReader reader = ManifestReader.read(ops.io().newInputFile(manifest))) {
         for (ManifestEntry add : reader.addedFiles()) {
           if (add.snapshotId() == snapshotId) {
@@ -164,7 +164,7 @@ public String toString() {
     return Objects.toStringHelper(this)
         .add("id", snapshotId)
         .add("timestamp_ms", timestampMillis)
-        .add("manifests", manifests)
+        .add("manifests", manifests())
         .toString();
   }
 }

File: core/src/main/java/com/netflix/iceberg/TableProperties.java
Patch:
@@ -73,5 +73,5 @@ public class TableProperties {
   public static final String WRITE_NEW_DATA_LOCATION = "write.folder-storage.path";
 
   public static final String MANIFEST_LISTS_ENABLED = "write.manifest-lists.enabled";
-  public static final boolean MANIFEST_LISTS_ENABLED_DEFAULT = false;
+  public static final boolean MANIFEST_LISTS_ENABLED_DEFAULT = true;
 }

File: core/src/test/java/com/netflix/iceberg/TestFastAppend.java
Patch:
@@ -32,7 +32,7 @@ public class TestFastAppend extends TableTestBase {
 
   @Test
   public void testEmptyTableAppend() {
-    Assert.assertEquals("Table should start empty", 0, listMetadataFiles("avro").size());
+    Assert.assertEquals("Table should start empty", 0, listManifestFiles().size());
 
     TableMetadata base = readMetadata();
     Assert.assertNull("Should not have a current snapshot", base.currentSnapshot());

File: core/src/test/java/com/netflix/iceberg/TestReplaceTransaction.java
Patch:
@@ -289,7 +289,7 @@ public void testReplaceToCreateAndAppend() throws IOException {
     Assert.assertEquals("Should have metadata version 0",
         0, (int) TestTables.metadataVersion("test_append"));
     Assert.assertEquals("Should have 1 manifest file",
-        1, listMetadataFiles(tableDir, "avro").size());
+        1, listManifestFiles(tableDir).size());
 
     Assert.assertEquals("Table schema should match with reassigned IDs",
         assignFreshIds(SCHEMA).asStruct(), meta.schema().asStruct());

File: api/src/main/java/com/netflix/iceberg/expressions/Literals.java
Patch:
@@ -96,6 +96,7 @@ private abstract static class BaseLiteral<T> implements Literal<T> {
     private final T value;
 
     BaseLiteral(T value) {
+      Preconditions.checkNotNull(value, "Literal values cannot be null");
       this.value = value;
     }
 

File: api/src/test/java/com/netflix/iceberg/TestPartitionPaths.java
Patch:
@@ -34,6 +34,7 @@ public class TestPartitionPaths {
   );
 
   @Test
+  @SuppressWarnings("unchecked")
   public void testPartitionPath() {
     PartitionSpec spec = PartitionSpec.builderFor(SCHEMA)
         .hour("ts")

File: core/src/main/java/com/netflix/iceberg/OverwriteData.java
Patch:
@@ -52,7 +52,7 @@ public OverwriteFiles validateAddedFiles() {
   }
 
   @Override
-  public List<String> apply(TableMetadata base) {
+  public List<ManifestFile> apply(TableMetadata base) {
     if (validateAddedFiles) {
       PartitionSpec spec = writeSpec();
       Expression rowFilter = rowFilter();

File: core/src/main/java/com/netflix/iceberg/ReplacePartitionsOperation.java
Patch:
@@ -42,7 +42,7 @@ public ReplacePartitions validateAppendOnly() {
   }
 
   @Override
-  public List<String> apply(TableMetadata base) {
+  public List<ManifestFile> apply(TableMetadata base) {
     if (writeSpec().fields().size() <= 0) {
       // replace all data in an unpartitioned table
       deleteByRowFilter(Expressions.alwaysTrue());

File: core/src/main/java/com/netflix/iceberg/TableProperties.java
Patch:
@@ -66,4 +66,7 @@ public class TableProperties {
   public static final boolean OBJECT_STORE_ENABLED_DEFAULT = false;
 
   public static final String OBJECT_STORE_PATH = "write.object-storage.path";
+
+  public static final String MANIFEST_LISTS_ENABLED = "write.manifest-lists.enabled";
+  public static final boolean MANIFEST_LISTS_ENABLED_DEFAULT = false;
 }

File: core/src/main/java/com/netflix/iceberg/ManifestWriter.java
Patch:
@@ -108,7 +108,8 @@ private static <D> FileAppender<D> newAppender(FileFormat format, PartitionSpec
               .schema(manifestSchema)
               .named("manifest_entry")
               .meta("schema", SchemaParser.toJson(spec.schema()))
-              .meta("partition-spec", PartitionSpecParser.toJson(spec))
+              .meta("partition-spec", PartitionSpecParser.toJsonFields(spec))
+              .meta("partition-spec-id", String.valueOf(spec.specId()))
               .build();
         default:
           throw new IllegalArgumentException("Unsupported format: " + format);

File: core/src/main/java/com/netflix/iceberg/ScanSummary.java
Patch:
@@ -138,10 +138,12 @@ private PartitionMetrics updateFromFile(DataFile file, Long timestampMillis) {
 
     @Override
     public String toString() {
+      String lastUpdated = lastUpdatedMillis != null ?
+          new Date(lastUpdatedMillis).toString() : null;
       return "PartitionMetrics(fileCount=" + fileCount +
           ", recordCount=" + recordCount +
           ", totalSize=" + totalSize +
-          ", lastUpdatedMillis=" + new Date(lastUpdatedMillis).toString() + ")";
+          ", lastUpdated=" + lastUpdated + ")";
     }
   }
 

File: core/src/main/java/com/netflix/iceberg/SnapshotUpdate.java
Patch:
@@ -18,7 +18,6 @@
 
 import com.google.common.collect.Sets;
 import com.netflix.iceberg.exceptions.CommitFailedException;
-import com.netflix.iceberg.exceptions.ValidationException;
 import com.netflix.iceberg.io.OutputFile;
 import com.netflix.iceberg.util.Exceptions;
 import com.netflix.iceberg.util.Tasks;
@@ -76,8 +75,8 @@ protected SnapshotUpdate(TableOperations ops) {
   public Snapshot apply() {
     this.base = ops.refresh();
     List<String> manifests = apply(base);
-    long currentSnapshotId = base.currentSnapshot() != null ?
-        base.currentSnapshot().snapshotId() : -1;
+    Long currentSnapshotId = base.currentSnapshot() != null ?
+        base.currentSnapshot().snapshotId() : null;
     return new BaseSnapshot(ops,
         snapshotId(), currentSnapshotId, System.currentTimeMillis(), manifests);
   }

File: core/src/main/java/com/netflix/iceberg/TableOperations.java
Patch:
@@ -44,6 +44,8 @@ public interface TableOperations {
    * This method should implement and document atomicity guarantees.
    * <p>
    * Implementations must check that the base metadata is current to avoid overwriting updates.
+   * Once the atomic commit operation succeeds, implementations must not perform any operations that
+   * may fail because failure in this method cannot be distinguished from commit failure.
    *
    * @param base     table metadata on which changes were based
    * @param metadata new table metadata with updates

File: core/src/main/java/com/netflix/iceberg/TableMetadata.java
Patch:
@@ -163,7 +163,7 @@ public String toString() {
     for (SnapshotLogEntry logEntry : snapshotLog) {
       if (last != null) {
         Preconditions.checkArgument(
-            (logEntry.timestampMillis() - last.timestampMillis()) > 0,
+            (logEntry.timestampMillis() - last.timestampMillis()) >= 0,
             "[BUG] Expected sorted snapshot log entries.");
       }
       last = logEntry;

File: spark/src/main/java/com/netflix/iceberg/spark/SparkExpressions.java
Patch:
@@ -201,7 +201,7 @@ private static List<Object> convertLiterals(List<Expression> values) {
     for (Expression value : values) {
       if (value instanceof Literal) {
         Literal lit = (Literal) value;
-        converted.add(lit.value());
+        converted.add(valueFromSpark(lit));
       } else {
         return null;
       }

File: parquet/src/main/java/com/netflix/iceberg/parquet/ParquetValueReaders.java
Patch:
@@ -203,8 +203,8 @@ public Double read(Double ignored) {
     }
 
     @Override
-    public long readLong() {
-      return super.readInteger();
+    public double readDouble() {
+      return super.readFloat();
     }
   }
 

File: core/src/main/java/com/netflix/iceberg/avro/LogicalMap.java
Patch:
@@ -27,7 +27,7 @@
 
 import static org.apache.avro.Schema.Type.ARRAY;
 
-class LogicalMap extends LogicalType {
+public class LogicalMap extends LogicalType {
   static final String NAME = "map";
   private static final LogicalMap INSTANCE = new LogicalMap();
 

File: core/src/main/java/com/netflix/iceberg/hadoop/HadoopTables.java
Patch:
@@ -40,6 +40,7 @@ public class HadoopTables implements Tables, Configurable {
   private Configuration conf;
 
   public HadoopTables() {
+    this(new Configuration());
   }
 
   public HadoopTables(Configuration conf) {

File: core/src/test/java/com/netflix/iceberg/TestOverwrite.java
Patch:
@@ -27,6 +27,7 @@
 import java.io.File;
 import java.io.IOException;
 import java.nio.ByteBuffer;
+import java.nio.ByteOrder;
 
 import static com.netflix.iceberg.expressions.Expressions.and;
 import static com.netflix.iceberg.expressions.Expressions.equal;
@@ -87,7 +88,7 @@ public class TestOverwrite extends TableTestBase {
       .build();
 
   private static ByteBuffer longToBuffer(long value) {
-    return ByteBuffer.allocate(8).putLong(0, value);
+    return ByteBuffer.allocate(8).order(ByteOrder.LITTLE_ENDIAN).putLong(0, value);
   }
 
   private Table table = null;

File: parquet/src/main/java/com/netflix/iceberg/parquet/MessageTypeToType.java
Patch:
@@ -43,7 +43,7 @@ class MessageTypeToType extends ParquetTypeVisitor<Type> {
 
   public MessageTypeToType(GroupType root) {
     this.root = root;
-    this.nextId = 1_000_000; // use ids that won't match other than for root
+    this.nextId = 1_000; // use ids that won't match other than for root
   }
 
   public Map<String, Integer> getAliases() {

File: core/src/main/java/com/netflix/iceberg/BaseMetastoreTableOperations.java
Patch:
@@ -99,7 +99,7 @@ protected void refreshFromMetadataLocation(String newLocation) {
       LOG.info("Refreshing table metadata from new version: " + newLocation);
 
       Tasks.foreach(newLocation)
-          .retry(4).exponentialBackoff(100, 5000, 5000, 4.0 /* 100, 400, 1600, ... */ )
+          .retry(20).exponentialBackoff(100, 5000, 600000, 4.0 /* 100, 400, 1600, ... */ )
           .suppressFailureWhenFinished()
           .run(location -> {
             this.currentMetadata = read(this, fromLocation(location, conf));

File: spark/src/main/java/com/netflix/iceberg/spark/source/Reader.java
Patch:
@@ -198,7 +198,7 @@ public Statistics getStatistics() {
     long sizeInBytes = 0L;
     long numRows = 0L;
 
-    for (CombinedScanTask task : tasks) {
+    for (CombinedScanTask task : tasks()) {
       for (FileScanTask file : task.files()) {
         sizeInBytes += file.length();
         numRows += file.file().recordCount();

File: api/src/test/java/com/netflix/iceberg/expressions/TestMiscLiteralConversions.java
Patch:
@@ -208,7 +208,6 @@ public void testInvalidTimestampConversions() {
         Types.LongType.get(),
         Types.FloatType.get(),
         Types.DoubleType.get(),
-        Types.DateType.get(),
         Types.TimeType.get(),
         Types.DecimalType.of(9, 4),
         Types.StringType.get(),

File: core/src/main/java/com/netflix/iceberg/TableProperties.java
Patch:
@@ -27,13 +27,13 @@ public class TableProperties {
   public static final int COMMIT_MAX_RETRY_WAIT_MS_DEFAULT = 60000; // 1 minute
 
   public static final String COMMIT_TOTAL_RETRY_TIME_MS = "commit.retry.total-timeout-ms";
-  public static final int COMMIT_TOTAL_RETRY_TIME_MS_DEFAULT = 600000; // 10 minutes
+  public static final int COMMIT_TOTAL_RETRY_TIME_MS_DEFAULT = 1800000; // 30 minutes
 
   public static final String MANIFEST_TARGET_SIZE_BYTES = "commit.manifest.target-size-bytes";
-  public static final long MANIFEST_TARGET_SIZE_BYTES_DEFAULT = 4194304; // 4 MB
+  public static final long MANIFEST_TARGET_SIZE_BYTES_DEFAULT = 8388608; // 8 MB
 
   public static final String MANIFEST_MIN_MERGE_COUNT = "commit.manifest.min-count-to-merge";
-  public static final int MANIFEST_MIN_MERGE_COUNT_DEFAULT = 20;
+  public static final int MANIFEST_MIN_MERGE_COUNT_DEFAULT = 100;
 
   public static final String DEFAULT_FILE_FORMAT = "write.format.default";
   public static final String DEFAULT_FILE_FORMAT_DEFAULT = "parquet";

File: parquet/src/main/java/com/netflix/iceberg/parquet/MessageTypeToType.java
Patch:
@@ -43,7 +43,7 @@ class MessageTypeToType extends ParquetTypeVisitor<Type> {
 
   public MessageTypeToType(GroupType root) {
     this.root = root;
-    this.nextId = root.getFieldCount() + 1; // reserve ids for the root struct
+    this.nextId = 1_000_000; // use ids that won't match other than for root
   }
 
   public Map<String, Integer> getAliases() {
@@ -185,7 +185,7 @@ private void addAlias(String name, int fieldId) {
     aliasToId.put(fullName, fieldId);
   }
 
-  private int nextId() {
+  protected int nextId() {
     int current = nextId;
     nextId += 1;
     return current;

File: spark/src/main/java/com/netflix/iceberg/spark/data/SparkParquetReaders.java
Patch:
@@ -58,6 +58,7 @@
 import java.util.List;
 import java.util.Map;
 
+import static com.netflix.iceberg.parquet.ParquetSchemaUtil.hasIds;
 import static com.netflix.iceberg.parquet.ParquetValueReaders.option;
 
 public class SparkParquetReaders {
@@ -67,8 +68,7 @@ private SparkParquetReaders() {
   @SuppressWarnings("unchecked")
   public static ParquetValueReader<InternalRow> buildReader(Schema expectedSchema,
                                                             MessageType fileSchema) {
-    List<Type> fileFields = fileSchema.getFields();
-    if (fileFields.size() > 0 && fileFields.get(0).getId() != null) {
+    if (hasIds(fileSchema)) {
       return (ParquetValueReader<InternalRow>)
           TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,
               new ReadBuilder(fileSchema));

File: parquet/src/main/java/com/netflix/iceberg/parquet/ParquetReader.java
Patch:
@@ -71,7 +71,7 @@ private static class ReadConf<T> {
       MessageType fileSchema = reader.getFileMetaData().getSchema();
 
       this.projection = projectionSchema(expectedSchema, fileSchema);
-      this.model = (ParquetValueReader<T>) readerFunc.apply(fileSchema);
+      this.model = (ParquetValueReader<T>) readerFunc.apply(projection);
       this.rowGroups = reader.getRowGroups();
       this.shouldSkip = new boolean[rowGroups.size()];
 

File: parquet/src/main/java/com/netflix/iceberg/parquet/ParquetSchemaUtil.java
Patch:
@@ -67,7 +67,7 @@ public static MessageType pruneColumnsFallback(MessageType fileSchema, Schema ex
     int ordinal = 1;
     for (Type type : fileSchema.getFields()) {
       if (selectedIds.contains(ordinal)) {
-        builder.addField(type);
+        builder.addField(type.withId(ordinal));
       }
       ordinal += 1;
     }

File: api/src/main/java/com/netflix/iceberg/UpdateProperties.java
Patch:
@@ -49,7 +49,7 @@ public interface UpdateProperties extends PendingUpdate<Map<String, String>> {
 
   /**
    * Set the default file format for the table.
-   * @param format
+   * @param format a file format
    * @return this
    */
   UpdateProperties defaultFormat(FileFormat format);

File: api/src/main/java/com/netflix/iceberg/transforms/Transforms.java
Patch:
@@ -64,6 +64,7 @@ private Transforms() {
   /**
    * Returns an identity {@link Transform} that can be used for any type.
    *
+   * @param type the {@link Type source type} for the transform
    * @param <T> Java type passed to this transform
    * @return an identity transform
    */

File: api/src/main/java/com/netflix/iceberg/expressions/And.java
Patch:
@@ -25,11 +25,11 @@ public class And implements Expression {
     this.right = right;
   }
 
-  public Expression getLeft() {
+  public Expression left() {
     return left;
   }
 
-  public Expression getRight() {
+  public Expression right() {
     return right;
   }
 

File: api/src/main/java/com/netflix/iceberg/expressions/ExpressionVisitors.java
Patch:
@@ -154,10 +154,10 @@ public static <R> R visit(Expression expr, ExpressionVisitor<R> visitor) {
           return visitor.not(visit(not.child(), visitor));
         case AND:
           And and = (And) expr;
-          return visitor.and(visit(and.getLeft(), visitor), visit(and.getRight(), visitor));
+          return visitor.and(visit(and.left(), visitor), visit(and.right(), visitor));
         case OR:
           Or or = (Or) expr;
-          return visitor.or(visit(or.getLeft(), visitor), visit(or.getRight(), visitor));
+          return visitor.or(visit(or.left(), visitor), visit(or.right(), visitor));
         default:
           throw new UnsupportedOperationException(
               "Unknown operation: " + expr.op());

File: api/src/main/java/com/netflix/iceberg/expressions/Or.java
Patch:
@@ -25,11 +25,11 @@ public class Or implements Expression {
     this.right = right;
   }
 
-  public Expression getLeft() {
+  public Expression left() {
     return left;
   }
 
-  public Expression getRight() {
+  public Expression right() {
     return right;
   }
 

File: api/src/main/java/com/netflix/iceberg/expressions/InclusiveMetricsEvaluator.java
Patch:
@@ -26,6 +26,8 @@
 import java.nio.ByteBuffer;
 import java.util.Map;
 
+import static com.netflix.iceberg.expressions.Expressions.rewriteNot;
+
 /**
  * Evaluates an {@link Expression} on a {@link DataFile} to test whether rows in the file may match.
  * <p>
@@ -51,7 +53,7 @@ private MetricsEvalVisitor visitor() {
   public InclusiveMetricsEvaluator(Schema schema, Expression unbound) {
     this.schema = schema;
     this.struct = schema.asStruct();
-    this.expr = Binder.bind(struct, unbound);
+    this.expr = Binder.bind(struct, rewriteNot(unbound));
   }
 
   /**

File: api/src/main/java/com/netflix/iceberg/expressions/StrictMetricsEvaluator.java
Patch:
@@ -26,6 +26,8 @@
 import java.nio.ByteBuffer;
 import java.util.Map;
 
+import static com.netflix.iceberg.expressions.Expressions.rewriteNot;
+
 /**
  * Evaluates an {@link Expression} on a {@link DataFile} to test whether all rows in the file match.
  * <p>
@@ -52,7 +54,7 @@ private MetricsEvalVisitor visitor() {
   public StrictMetricsEvaluator(Schema schema, Expression unbound) {
     this.schema = schema;
     this.struct = schema.asStruct();
-    this.expr = Binder.bind(struct, unbound);
+    this.expr = Binder.bind(struct, rewriteNot(unbound));
   }
 
   /**

File: core/src/main/java/com/netflix/iceberg/ManifestReader.java
Patch:
@@ -193,7 +193,9 @@ Iterable<ManifestEntry> entries(Collection<String> columns) {
             .project(schema)
             .rename("manifest_entry", ManifestEntry.class.getName())
             .rename("partition", PartitionData.class.getName())
+            .rename("r102", PartitionData.class.getName())
             .rename("data_file", GenericDataFile.class.getName())
+            .rename("r2", GenericDataFile.class.getName())
             .reuseContainers()
             .build();
 

File: core/src/main/java/com/netflix/iceberg/avro/TypeToSchema.java
Patch:
@@ -78,7 +78,7 @@ public Schema struct(Types.StructType struct, List<Schema> fieldSchemas) {
 
     String recordName = names.get(struct);
     if (recordName == null) {
-      recordName = fieldNames.peek();
+      recordName = "r" + fieldIds.peek();
     }
 
     List<Types.NestedField> structFields = struct.fields();

File: parquet/src/main/java/com/netflix/iceberg/parquet/TypeToMessageType.java
Patch:
@@ -170,7 +170,8 @@ public Type primitive(PrimitiveType primitive, Type.Repetition repetition, int i
         }
 
       case UUID:
-        // TODO: add UUID to upstream Parquet spec
+        return Types.primitive(FIXED_LEN_BYTE_ARRAY, repetition).length(16).id(id).named(name);
+
       default:
         throw new UnsupportedOperationException("Unsupported type for Parquet: " + primitive);
     }

File: api/src/main/java/com/netflix/iceberg/expressions/Literals.java
Patch:
@@ -24,14 +24,11 @@
 import java.math.BigDecimal;
 import java.math.RoundingMode;
 import java.nio.ByteBuffer;
-import java.sql.Date;
-import java.sql.Timestamp;
 import java.time.Instant;
 import java.time.LocalDate;
 import java.time.LocalDateTime;
 import java.time.LocalTime;
 import java.time.OffsetDateTime;
-import java.time.OffsetTime;
 import java.time.ZoneOffset;
 import java.time.format.DateTimeFormatter;
 import java.time.temporal.ChronoUnit;

File: api/src/main/java/com/netflix/iceberg/io/CloseableGroup.java
Patch:
@@ -21,7 +21,7 @@
 import java.io.IOException;
 import java.util.LinkedList;
 
-public abstract class ClosingIterable implements Closeable {
+public abstract class CloseableGroup implements Closeable {
   private final LinkedList<Closeable> closeables = Lists.newLinkedList();
 
   protected void addCloseable(Closeable closeable) {

File: core/src/main/java/com/netflix/iceberg/BaseSnapshot.java
Patch:
@@ -23,14 +23,14 @@
 import com.netflix.iceberg.exceptions.RuntimeIOException;
 import com.netflix.iceberg.expressions.Expression;
 import com.netflix.iceberg.expressions.Expressions;
-import com.netflix.iceberg.io.ClosingIterable;
+import com.netflix.iceberg.io.CloseableGroup;
 import java.io.IOException;
 import java.util.Arrays;
 import java.util.Collection;
 import java.util.Iterator;
 import java.util.List;
 
-class BaseSnapshot extends ClosingIterable implements Snapshot, SnapshotIterable {
+class BaseSnapshot extends CloseableGroup implements Snapshot, SnapshotIterable {
   private final TableOperations ops;
   private final long snapshotId;
   private final long timestampMillis;

File: core/src/main/java/com/netflix/iceberg/BaseTableScan.java
Patch:
@@ -21,7 +21,7 @@
 import com.netflix.iceberg.expressions.Expression;
 import com.netflix.iceberg.expressions.Expressions;
 import com.netflix.iceberg.expressions.ResidualEvaluator;
-import com.netflix.iceberg.io.ClosingIterable;
+import com.netflix.iceberg.io.CloseableGroup;
 import com.netflix.iceberg.util.BinPacking;
 import com.netflix.iceberg.util.ParallelIterable;
 import java.util.Collection;
@@ -33,7 +33,7 @@
 /**
  * Base class for {@link TableScan} implementations.
  */
-class BaseTableScan extends ClosingIterable implements TableScan {
+class BaseTableScan extends CloseableGroup implements TableScan {
   private final TableOperations ops;
   private final Table table;
   private final Collection<String> columns;

File: core/src/main/java/com/netflix/iceberg/ManifestReader.java
Patch:
@@ -25,7 +25,7 @@
 import com.netflix.iceberg.exceptions.RuntimeIOException;
 import com.netflix.iceberg.expressions.Expression;
 import com.netflix.iceberg.expressions.Projections;
-import com.netflix.iceberg.io.ClosingIterable;
+import com.netflix.iceberg.io.CloseableGroup;
 import com.netflix.iceberg.io.InputFile;
 import com.netflix.iceberg.types.Types;
 import org.slf4j.Logger;
@@ -44,7 +44,7 @@
  * <p>
  * Readers are created using the builder from {@link #read(InputFile)}.
  */
-public class ManifestReader extends ClosingIterable implements Filterable<FilteredManifest> {
+public class ManifestReader extends CloseableGroup implements Filterable<FilteredManifest> {
   private static final Logger LOG = LoggerFactory.getLogger(ManifestReader.class);
 
   private static final List<String> ALL_COLUMNS = Lists.newArrayList("*");

File: core/src/main/java/com/netflix/iceberg/avro/Avro.java
Patch:
@@ -19,6 +19,7 @@
 import com.google.common.base.Preconditions;
 import com.google.common.collect.Maps;
 import com.netflix.iceberg.SchemaParser;
+import com.netflix.iceberg.io.CloseableIterable;
 import com.netflix.iceberg.io.InputFile;
 import com.netflix.iceberg.io.OutputFile;
 import org.apache.avro.Conversions;

File: core/src/main/java/com/netflix/iceberg/avro/AvroIterable.java
Patch:
@@ -18,7 +18,8 @@
 
 import com.google.common.collect.Maps;
 import com.netflix.iceberg.exceptions.RuntimeIOException;
-import com.netflix.iceberg.io.ClosingIterable;
+import com.netflix.iceberg.io.CloseableGroup;
+import com.netflix.iceberg.io.CloseableIterable;
 import com.netflix.iceberg.io.InputFile;
 import org.apache.avro.Schema;
 import org.apache.avro.file.DataFileReader;
@@ -30,7 +31,7 @@
 import java.util.Map;
 import java.util.NoSuchElementException;
 
-public class AvroIterable<D> extends ClosingIterable implements Iterable<D> {
+public class AvroIterable<D> extends CloseableGroup implements CloseableIterable<D> {
   private final InputFile file;
   private final DatumReader<D> reader;
   private final Long start;

File: parquet/src/main/java/com/netflix/iceberg/parquet/ParquetIterable.java
Patch:
@@ -17,14 +17,15 @@
 package com.netflix.iceberg.parquet;
 
 import com.netflix.iceberg.exceptions.RuntimeIOException;
-import com.netflix.iceberg.io.ClosingIterable;
+import com.netflix.iceberg.io.CloseableGroup;
+import com.netflix.iceberg.io.CloseableIterable;
 import org.apache.parquet.hadoop.ParquetReader;
 import java.io.Closeable;
 import java.io.IOException;
 import java.util.Iterator;
 import java.util.NoSuchElementException;
 
-public class ParquetIterable<T> extends ClosingIterable implements Iterable<T>, Closeable {
+public class ParquetIterable<T> extends CloseableGroup implements CloseableIterable<T> {
   private final ParquetReader.Builder<T> builder;
 
   ParquetIterable(ParquetReader.Builder<T> builder) {

File: spark/src/main/java/com/netflix/iceberg/spark/source/Writer.java
Patch:
@@ -16,6 +16,7 @@
 
 package com.netflix.iceberg.spark.source;
 
+import com.google.common.base.Joiner;
 import com.google.common.base.Preconditions;
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.Iterables;
@@ -407,9 +408,9 @@ public void write(InternalRow row) throws IOException {
         closeCurrent();
 
         if (completedPartitions.contains(key)) {
+          // if rows are not correctly grouped, detect and fail the write
           PartitionKey existingKey = Iterables.find(completedPartitions, key::equals, null);
           LOG.warn("Duplicate key: {} == {}", existingKey, key);
-          // if rows are not correctly grouped, detect and fail the write
           throw new IllegalStateException("Already closed file for partition: " + key.toPath());
         }
 

File: spark/src/test/java/com/netflix/iceberg/spark/data/TestSparkAvroReader.java
Patch:
@@ -34,7 +34,7 @@
 
 public class TestSparkAvroReader extends AvroDataTest {
   protected void writeAndValidate(Schema schema) throws IOException {
-    List<Record> expected = RandomData.generate(schema, 100, 0L);
+    List<Record> expected = RandomData.generateList(schema, 100, 0L);
 
     File testFile = temp.newFile();
     Assert.assertTrue("Delete should succeed", testFile.delete());

File: spark/src/test/java/com/netflix/iceberg/spark/source/TestAvroScan.java
Patch:
@@ -81,7 +81,7 @@ protected void writeAndValidate(Schema schema) throws IOException {
     // When tables are created, the column ids are reassigned.
     Schema tableSchema = table.schema();
 
-    List<Record> expected = RandomData.generate(tableSchema, 100, 1L);
+    List<Record> expected = RandomData.generateList(tableSchema, 100, 1L);
 
     try (FileAppender<Record> writer = Avro.write(localOutput(avroFile))
         .schema(tableSchema)

File: spark/src/test/java/com/netflix/iceberg/spark/source/TestDataFrameWrites.java
Patch:
@@ -98,7 +98,7 @@ protected void writeAndValidate(Schema schema) throws IOException {
 
     table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format).commit();
 
-    List<Record> expected = RandomData.generate(tableSchema, 100, 0L);
+    List<Record> expected = RandomData.generateList(tableSchema, 100, 0L);
     Dataset<Row> df = createDataset(expected, tableSchema);
 
     df.write()

File: spark/src/test/java/com/netflix/iceberg/spark/source/TestParquetScan.java
Patch:
@@ -92,7 +92,7 @@ protected void writeAndValidate(Schema schema) throws IOException {
     // When tables are created, the column ids are reassigned.
     Schema tableSchema = table.schema();
 
-    List<GenericData.Record> expected = RandomData.generate(tableSchema, 100, 1L);
+    List<GenericData.Record> expected = RandomData.generateList(tableSchema, 100, 1L);
 
     try (FileAppender<GenericData.Record> writer = Parquet.write(localOutput(parquetFile))
         .schema(tableSchema)

File: api/src/main/java/com/netflix/iceberg/expressions/StrictMetricsEvaluator.java
Patch:
@@ -30,8 +30,8 @@
  * Evaluates an {@link Expression} on a {@link DataFile} to test whether all rows in the file match.
  * <p>
  * This evaluation is strict: it returns true if all rows in a file must match the expression. For
- * example, if a file's ts column has min X and max Y, this evaluator will return true for ts < Y+1
- * but not for ts < Y-1.
+ * example, if a file's ts column has min X and max Y, this evaluator will return true for ts &lt; Y+1
+ * but not for ts &lt; Y-1.
  * <p>
  * Files are passed to {@link #eval(DataFile)}, which returns true if all rows in the file must
  * contain matching rows and false if the file may contain rows that do not match.

File: core/src/main/java/com/netflix/iceberg/BaseTable.java
Patch:
@@ -94,7 +94,7 @@ public AppendFiles newFastAppend() {
 
   @Override
   public RewriteFiles newRewrite() {
-    throw new UnsupportedOperationException("File rewrites are not yet implemented.");
+    return new ReplaceFiles(ops);
   }
 
   @Override
@@ -112,7 +112,6 @@ public Rollback rollback() {
     return new RollbackToSnapshot(ops);
   }
 
-
   @Override
   public String toString() {
     return name;

File: core/src/main/java/com/netflix/iceberg/TableProperties.java
Patch:
@@ -27,7 +27,7 @@ public class TableProperties {
   public static final int COMMIT_MAX_RETRY_WAIT_MS_DEFAULT = 60000; // 1 minute
 
   public static final String COMMIT_TOTAL_RETRY_TIME_MS = "commit.retry.total-timeout-ms";
-  public static final int COMMIT_TOTAL_RETRY_TIME_MS_DEFAULT = 60000; // 1 minute
+  public static final int COMMIT_TOTAL_RETRY_TIME_MS_DEFAULT = 600000; // 10 minutes
 
   public static final String MANIFEST_TARGET_SIZE_BYTES = "commit.manifest.target-size-bytes";
   public static final long MANIFEST_TARGET_SIZE_BYTES_DEFAULT = 4194304; // 4 MB

File: spark/src/main/java/com/netflix/iceberg/spark/source/PartitionKey.java
Patch:
@@ -80,7 +80,8 @@ private PartitionKey(PartitionKey toCopy) {
   private Object defensiveCopyIfNeeded(Object obj) {
     if (obj instanceof UTF8String) {
       // bytes backing the UTF8 string might be reused
-      return UTF8String.fromBytes(((UTF8String) obj).getBytes());
+      byte[] bytes = ((UTF8String) obj).getBytes();
+      return UTF8String.fromBytes(Arrays.copyOf(bytes, bytes.length));
     }
     return obj;
   }

File: spark/src/main/java/com/netflix/iceberg/spark/source/Reader.java
Patch:
@@ -94,7 +94,8 @@ class Reader implements DataSourceReader, SupportsScanUnsafeRow,
   private static final Filter[] NO_FILTERS = new Filter[0];
   private static final List<String> SNAPSHOT_COLUMNS = ImmutableList.of(
       "snapshot_id", "file_path", "file_ordinal", "file_format", "block_size_in_bytes",
-      "file_size_in_bytes", "record_count", "partition"
+      "file_size_in_bytes", "record_count", "partition", "value_counts", "null_value_counts",
+      "lower_bounds", "upper_bounds"
   );
 
   private final Table table;

File: core/src/main/java/com/netflix/iceberg/avro/ValueWriters.java
Patch:
@@ -198,7 +198,7 @@ public void write(Object s, Encoder encoder) throws IOException {
       // use getBytes because it may return the backing byte array if available.
       // otherwise, it copies to a new byte array, which is still cheaper than Avro
       // calling toString, which incurs encoding costs
-      if (s == null || s instanceof Utf8) {
+      if (s instanceof Utf8) {
         encoder.writeString((Utf8) s);
       } else if (s instanceof String) {
         encoder.writeString(new Utf8((String) s));

File: core/src/test/java/com/netflix/iceberg/avro/RandomAvroData.java
Patch:
@@ -111,15 +111,15 @@ public Object map(Types.MapType map, Supplier<Object> keyResult, Supplier<Object
         keyFunc = keyResult;
       }
 
-      Set<Object> keys = Sets.newHashSet();
+      Set<Object> keySet = Sets.newHashSet();
       for (int i = 0; i < numEntries; i += 1) {
         Object key = keyFunc.get();
         // ensure no collisions
-        while (keys.contains(key)) {
+        while (keySet.contains(key)) {
           key = keyFunc.get();
         }
 
-        keys.add(key);
+        keySet.add(key);
 
         // return null 5% of the time when the value is optional
         if (map.isValueOptional() && random.nextInt(20) == 1) {

File: core/src/test/java/com/netflix/iceberg/avro/TestReadProjection.java
Patch:
@@ -320,7 +320,7 @@ public void testMapOfStructsProjection() throws IOException {
     record.put("id", 34L);
     Record l1 = new Record(AvroSchemaUtil.fromOption(
         AvroSchemaUtil.fromOption(record.getSchema().getField("locations").schema())
-            .getElementType().getField("value").schema()));
+            .getValueType()));
     l1.put("lat", 53.992811f);
     l1.put("long", -1.542616f);
     Record l2 = new Record(l1.getSchema());

File: parquet/src/main/java/com/netflix/iceberg/parquet/MessageTypeToType.java
Patch:
@@ -110,8 +110,6 @@ public Type map(GroupType map, Type keyType, Type valueType) {
     org.apache.parquet.schema.Type key = keyValue.getType(0);
     org.apache.parquet.schema.Type value = keyValue.getType(1);
 
-    Preconditions.checkArgument(Types.StringType.get() == keyType,
-        "Non-string map keys are not supported: " + key);
     Preconditions.checkArgument(
         !value.isRepetition(Repetition.REPEATED),
         "Values cannot have repetition REPEATED: {}", value);

File: parquet/src/test/java/com/netflix/iceberg/avro/TestReadProjection.java
Patch:
@@ -320,7 +320,7 @@ public void testMapOfStructsProjection() throws IOException {
     record.put("id", 34L);
     Record l1 = new Record(AvroSchemaUtil.fromOption(
         AvroSchemaUtil.fromOption(record.getSchema().getField("locations").schema())
-            .getElementType().getField("value").schema()));
+            .getValueType()));
     l1.put("lat", 53.992811f);
     l1.put("long", -1.542616f);
     Record l2 = new Record(l1.getSchema());

File: spark/src/main/java/com/netflix/iceberg/spark/PruneColumnsWithReordering.java
Patch:
@@ -183,7 +183,7 @@ public Type list(Types.ListType list, Supplier<Type> elementResult) {
   }
 
   @Override
-  public Type map(Types.MapType map, Supplier<Type> valueResult) {
+  public Type map(Types.MapType map, Supplier<Type> keyResult, Supplier<Type> valueResult) {
     Preconditions.checkArgument(current instanceof MapType, "Not a map: %s", current);
     MapType m = (MapType) current;
 
@@ -200,9 +200,9 @@ public Type map(Types.MapType map, Supplier<Type> valueResult) {
       }
 
       if (map.isValueOptional()) {
-        return Types.MapType.ofOptional(map.keyId(), map.valueId(), valueType);
+        return Types.MapType.ofOptional(map.keyId(), map.valueId(), map.keyType(), valueType);
       } else {
-        return Types.MapType.ofRequired(map.keyId(), map.valueId(), valueType);
+        return Types.MapType.ofRequired(map.keyId(), map.valueId(), map.keyType(), valueType);
       }
     } finally {
       this.current = m;

File: spark/src/main/java/com/netflix/iceberg/spark/TypeToSparkType.java
Patch:
@@ -73,8 +73,8 @@ public DataType list(Types.ListType list, DataType elementResult) {
   }
 
   @Override
-  public DataType map(Types.MapType map, DataType valueResult) {
-    return MapType$.MODULE$.apply(StringType$.MODULE$, valueResult, map.isValueOptional());
+  public DataType map(Types.MapType map, DataType keyResult, DataType valueResult) {
+    return MapType$.MODULE$.apply(keyResult, valueResult, map.isValueOptional());
   }
 
   @Override

File: core/src/main/java/com/netflix/iceberg/avro/GenericAvroWriter.java
Patch:
@@ -124,9 +124,9 @@ public ValueWriter<?> primitive(Schema primitive) {
         case STRING:
           return ValueWriters.strings();
         case FIXED:
-          return ValueWriters.fixed(primitive.getFixedSize());
+          return ValueWriters.genericFixed(primitive.getFixedSize());
         case BYTES:
-          return ValueWriters.bytes();
+          return ValueWriters.byteBuffers();
         default:
           throw new IllegalArgumentException("Unsupported type: " + primitive);
       }

File: core/src/main/java/com/netflix/iceberg/avro/ValueReader.java
Patch:
@@ -20,5 +20,5 @@
 import java.io.IOException;
 
 public interface ValueReader<T> {
-  T read(Decoder decoder) throws IOException;
+  T read(Decoder decoder, Object reuse) throws IOException;
 }

File: core/src/main/java/com/netflix/iceberg/avro/ValueReader.java
Patch:
@@ -14,7 +14,7 @@
  * limitations under the License.
  */
 
-package com.netflix.iceberg.spark.data;
+package com.netflix.iceberg.avro;
 
 import org.apache.avro.io.Decoder;
 import java.io.IOException;

File: core/src/main/java/com/netflix/iceberg/avro/ValueWriter.java
Patch:
@@ -14,7 +14,7 @@
  * limitations under the License.
  */
 
-package com.netflix.iceberg.spark.data;
+package com.netflix.iceberg.avro;
 
 import org.apache.avro.io.Encoder;
 import java.io.IOException;

File: api/src/main/java/com/netflix/iceberg/types/AssignFreshIds.java
Patch:
@@ -75,13 +75,13 @@ public Type list(Types.ListType list, Supplier<Type> future) {
   }
 
   @Override
-  public Type map(Types.MapType map, Supplier<Type> future) {
+  public Type map(Types.MapType map, Supplier<Type> keyFuture, Supplier<Type> valuefuture) {
     int newKeyId = nextId.get();
     int newValueId = nextId.get();
     if (map.isValueOptional()) {
-      return Types.MapType.ofOptional(newKeyId, newValueId, future.get());
+      return Types.MapType.ofOptional(newKeyId, newValueId, keyFuture.get(), valuefuture.get());
     } else {
-      return Types.MapType.ofRequired(newKeyId, newValueId, future.get());
+      return Types.MapType.ofRequired(newKeyId, newValueId, keyFuture.get(), valuefuture.get());
     }
   }
 

File: api/src/main/java/com/netflix/iceberg/types/GetProjectedIds.java
Patch:
@@ -53,7 +53,7 @@ public Set<Integer> list(Types.ListType list, Set<Integer> elementResult) {
   }
 
   @Override
-  public Set<Integer> map(Types.MapType map, Set<Integer> valueResult) {
+  public Set<Integer> map(Types.MapType map, Set<Integer> keyResult, Set<Integer> valueResult) {
     if (valueResult == null) {
       for (Types.NestedField field : map.fields()) {
         fieldIds.add(field.fieldId());

File: api/src/main/java/com/netflix/iceberg/types/IndexById.java
Patch:
@@ -49,7 +49,7 @@ public Map<Integer, Types.NestedField> list(Types.ListType list, Map<Integer, Ty
   }
 
   @Override
-  public Map<Integer, Types.NestedField> map(Types.MapType map, Map<Integer, Types.NestedField> valueResult) {
+  public Map<Integer, Types.NestedField> map(Types.MapType map, Map<Integer, Types.NestedField> keyResult, Map<Integer, Types.NestedField> valueResult) {
     for (Types.NestedField field : map.fields()) {
       index.put(field.fieldId(), field);
     }

File: api/src/main/java/com/netflix/iceberg/types/IndexByName.java
Patch:
@@ -52,7 +52,7 @@ public Map<String, Integer> list(Types.ListType list, Map<String, Integer> eleme
   }
 
   @Override
-  public Map<String, Integer> map(Types.MapType map, Map<String, Integer> valueResult) {
+  public Map<String, Integer> map(Types.MapType map, Map<String, Integer> keyResult, Map<String, Integer> valueResult) {
     for (Types.NestedField field : map.fields()) {
       addField(field.name(), field.fieldId());
     }

File: api/src/main/java/com/netflix/iceberg/types/PruneColumns.java
Patch:
@@ -98,16 +98,16 @@ public Type list(Types.ListType list, Type elementResult) {
   }
 
   @Override
-  public Type map(Types.MapType map, Type valueResult) {
+  public Type map(Types.MapType map, Type ignored, Type valueResult) {
     if (selected.contains(map.valueId())) {
       return map;
     } else if (valueResult != null) {
       if (map.valueType() == valueResult) {
         return map;
       } else if (map.isValueOptional()) {
-        return Types.MapType.ofOptional(map.keyId(), map.valueId(), valueResult);
+        return Types.MapType.ofOptional(map.keyId(), map.valueId(), map.keyType(), valueResult);
       } else {
-        return Types.MapType.ofRequired(map.keyId(), map.valueId(), valueResult);
+        return Types.MapType.ofRequired(map.keyId(), map.valueId(), map.keyType(), valueResult);
       }
     } else if (selected.contains(map.keyId())) {
       // right now, maps can't be selected without values

File: spark/src/main/java/com/netflix/iceberg/spark/source/Writer.java
Patch:
@@ -203,7 +203,7 @@ private static class WriterFactory implements DataWriterFactory<InternalRow> {
 
     @Override
     public DataWriter<InternalRow> createDataWriter(int partitionId, int attemptNumber) {
-      String filename = String.format("%05d-%s", partitionId, uuid);
+      String filename = String.format("%05d-%d-%s", partitionId, attemptNumber, uuid);
       AppenderFactory<InternalRow> factory = new SparkAppenderFactory<>();
       if (spec.fields().isEmpty()) {
         return new UnpartitionedWriter(lazyDataPath(), filename, format, conf.value(), factory);

File: core/src/main/java/com/netflix/iceberg/TableProperties.java
Patch:
@@ -30,7 +30,7 @@ public class TableProperties {
   public static final int COMMIT_TOTAL_RETRY_TIME_MS_DEFAULT = 60000; // 1 minute
 
   public static final String MANIFEST_TARGET_SIZE_BYTES = "commit.manifest.target-size-bytes";
-  public static final long MANIFEST_TARGET_SIZE_BYTES_DEFAULT = 8388608; // 8 MB
+  public static final long MANIFEST_TARGET_SIZE_BYTES_DEFAULT = 4194304; // 4 MB
 
   public static final String MANIFEST_MIN_MERGE_COUNT = "commit.manifest.min-count-to-merge";
   public static final int MANIFEST_MIN_MERGE_COUNT_DEFAULT = 20;

File: api/src/main/java/com/netflix/iceberg/FileFormat.java
Patch:
@@ -22,6 +22,7 @@
  * Enum of supported file formats.
  */
 public enum FileFormat {
+  ORC("orc"),
   PARQUET("parquet"),
   AVRO("avro");
 

File: spark/src/test/java/com/netflix/iceberg/spark/source/TestDataFrameWrites.java
Patch:
@@ -60,6 +60,7 @@ public class TestDataFrameWrites extends AvroDataTest {
   public static Object[][] parameters() {
     return new Object[][] {
         new Object[] { "parquet" },
+        new Object[] { "orc" },
         new Object[] { "avro" }
     };
   }

File: core/src/main/java/com/netflix/iceberg/TableProperties.java
Patch:
@@ -29,6 +29,9 @@ public class TableProperties {
   public static final String COMMIT_TOTAL_RETRY_TIME_MS = "commit.retry.total-timeout-ms";
   public static final int COMMIT_TOTAL_RETRY_TIME_MS_DEFAULT = 60000; // 1 minute
 
+  public static final String MANIFEST_TARGET_SIZE_BYTES = "commit.manifest.target-size-bytes";
+  public static final long MANIFEST_TARGET_SIZE_BYTES_DEFAULT = 8388608; // 8 MB
+
   public static final String DEFAULT_FILE_FORMAT = "write.format.default";
   public static final String DEFAULT_FILE_FORMAT_DEFAULT = "parquet";
 }

File: api/src/test/java/com/netflix/iceberg/expressions/TestLiteralSerialization.java
Patch:
@@ -33,8 +33,7 @@ public void testLiterals() throws Exception {
         Literal.of(36.75F),
         Literal.of(8.75D),
         Literal.of("2017-11-29").to(Types.DateType.get()),
-        Literal.of("11:30:07").to(Types.TimeType.withoutZone()),
-        Literal.of("11:30:07+01:00").to(Types.TimeType.withZone()),
+        Literal.of("11:30:07").to(Types.TimeType.get()),
         Literal.of("2017-11-29T11:30:07.123").to(Types.TimestampType.withoutZone()),
         Literal.of("2017-11-29T11:30:07.123+01:00").to(Types.TimestampType.withZone()),
         Literal.of("abc"),

File: api/src/test/java/com/netflix/iceberg/expressions/TestStringLiteralConversions.java
Patch:
@@ -61,7 +61,7 @@ public void testStringToTimeLiteral() {
         new TimeConversions.LossyTimeMicrosConversion();
 
     Literal<CharSequence> timeStr = Literal.of("14:21:01.919");
-    Literal<Long> time = timeStr.to(Types.TimeType.withoutZone());
+    Literal<Long> time = timeStr.to(Types.TimeType.get());
 
     long avroValue = avroConversion.toLong(
         new LocalTime(14, 21, 1, 919),

File: api/src/test/java/com/netflix/iceberg/transforms/TestTransformSerialization.java
Patch:
@@ -30,7 +30,7 @@ public void testTransforms() throws Exception {
         Types.NestedField.required(1, "i", Types.IntegerType.get()),
         Types.NestedField.required(2, "l", Types.LongType.get()),
         Types.NestedField.required(3, "d", Types.DateType.get()),
-        Types.NestedField.required(4, "t", Types.TimeType.withoutZone()),
+        Types.NestedField.required(4, "t", Types.TimeType.get()),
         Types.NestedField.required(5, "ts", Types.TimestampType.withoutZone()),
         Types.NestedField.required(6, "dec", Types.DecimalType.of(9, 2)),
         Types.NestedField.required(7, "s", Types.StringType.get()),

File: api/src/test/java/com/netflix/iceberg/types/TestReadabilityChecks.java
Patch:
@@ -32,8 +32,7 @@ public class TestReadabilityChecks {
       Types.FloatType.get(),
       Types.DoubleType.get(),
       Types.DateType.get(),
-      Types.TimeType.withoutZone(),
-      Types.TimeType.withZone(),
+      Types.TimeType.get(),
       Types.TimestampType.withoutZone(),
       Types.TimestampType.withZone(),
       Types.StringType.get(),

File: api/src/test/java/com/netflix/iceberg/types/TestSerializableTypes.java
Patch:
@@ -35,8 +35,7 @@ public void testIdentityTypes() throws Exception {
         Types.FloatType.get(),
         Types.DoubleType.get(),
         Types.DateType.get(),
-        Types.TimeType.withoutZone(),
-        Types.TimeType.withZone(),
+        Types.TimeType.get(),
         Types.TimestampType.withoutZone(),
         Types.TimestampType.withZone(),
         Types.StringType.get(),

File: core/src/test/java/com/netflix/iceberg/TestSchemaUpdate.java
Patch:
@@ -130,8 +130,7 @@ public void testUpdateFailure() {
 
     List<Type.PrimitiveType> primitives = Lists.newArrayList(
         Types.BooleanType.get(), Types.IntegerType.get(), Types.LongType.get(),
-        Types.FloatType.get(), Types.DoubleType.get(), Types.DateType.get(),
-        Types.TimeType.withZone(), Types.TimeType.withoutZone(),
+        Types.FloatType.get(), Types.DoubleType.get(), Types.DateType.get(), Types.TimeType.get(),
         Types.TimestampType.withZone(), Types.TimestampType.withoutZone(),
         Types.StringType.get(), Types.UUIDType.get(), Types.BinaryType.get(),
         Types.FixedType.ofLength(3), Types.FixedType.ofLength(4),

File: parquet/src/main/java/com/netflix/iceberg/parquet/MessageTypeToType.java
Patch:
@@ -137,7 +137,7 @@ public Type primitive(PrimitiveType primitive) {
         case DATE:
           return Types.DateType.get();
         case TIME_MICROS:
-          return Types.TimeType.withZone();
+          return Types.TimeType.get();
         case TIMESTAMP_MICROS:
           return Types.TimestampType.withZone();
         case UTF8:

File: spark/src/main/java/com/netflix/iceberg/spark/SparkSchemaUtil.java
Patch:
@@ -80,7 +80,7 @@ public static PartitionSpec specForTable(SparkSession spark, String name) throws
     String table = parts.get(parts.size() == 1 ? 0 : 1);
 
     return identitySpec(
-        schemaForTable(spark, table),
+        schemaForTable(spark, name),
         spark.catalog().listColumns(db, table).collectAsList());
   }
 

File: parquet/src/main/java/com/netflix/iceberg/parquet/ParquetReadSupport.java
Patch:
@@ -74,7 +74,7 @@ public ReadContext init(Configuration configuration, Map<String, String> keyValu
     org.apache.avro.Schema avroReadSchema = AvroSchemaUtil.buildAvroProjection(
         AvroSchemaUtil.convert(ParquetSchemaUtil.convert(projection), projection.getName()),
         expectedSchema, ImmutableMap.of());
-    AvroReadSupport.setAvroReadSchema(configuration, avroReadSchema);
+    AvroReadSupport.setAvroReadSchema(configuration, ParquetAvro.parquetAvroSchema(avroReadSchema));
 
     // let the context set up read support metadata, but always use the correct projection
     ReadContext context = null;

File: api/src/main/java/com/netflix/iceberg/transforms/Bucket.java
Patch:
@@ -109,6 +109,8 @@ public UnboundPredicate<Integer> project(String name, BoundPredicate<T> predicat
 //        return Expressions.predicate();
       default:
         // comparison predicates can't be projected, notEq can't be projected
+        // TODO: small ranges can be projected.
+        // for example, (x > 0) and (x < 3) can be turned into in({1, 2}) and projected.
         return null;
     }
   }

File: core/src/main/java/com/netflix/iceberg/SchemaUpdate.java
Patch:
@@ -100,7 +100,7 @@ public UpdateSchema addColumn(String parent, String name, Type type) {
     // assign new IDs in order
     int newId = assignNewColumnId();
     adds.put(parentId, Types.NestedField.optional(newId, name,
-        TypeUtil.reassignIds(type, this::assignNewColumnId)));
+        TypeUtil.assignFreshIds(type, this::assignNewColumnId)));
 
     return this;
   }

File: core/src/main/java/com/netflix/iceberg/TableMetadata.java
Patch:
@@ -40,7 +40,7 @@ public static TableMetadata newTableMetadata(TableOperations ops,
                                                String location) {
     // reassign all column ids to ensure consistency
     AtomicInteger lastColumnId = new AtomicInteger(0);
-    Schema freshSchema = TypeUtil.reassignIds(schema, lastColumnId::incrementAndGet);
+    Schema freshSchema = TypeUtil.assignFreshIds(schema, lastColumnId::incrementAndGet);
 
     // rebuild the partition spec using the new column ids
     PartitionSpec.Builder specBuilder = PartitionSpec.builderFor(freshSchema);

File: spark/src/main/java/com/netflix/iceberg/spark/source/IcebergSource.java
Patch:
@@ -63,13 +63,12 @@ public Optional<DataSourceV2Writer> createWriter(String jobId, StructType dfStru
 
     Table table = findTable(options);
 
-    // TODO: prune isn't quite correct. this should convert and fill in the right ids
-    Schema dfSchema = SparkSchemaUtil.prune(table.schema(), dfStruct);
+    Schema dfSchema = SparkSchemaUtil.convert(table.schema(), dfStruct);
     List<String> errors = CheckReadability.schemaCompatibilityErrors(table.schema(), dfSchema);
     if (!errors.isEmpty()) {
       StringBuilder sb = new StringBuilder();
       sb.append("Cannot write incompatible dataframe to table with schema:\n")
-          .append(table.schema()).append("\nProblems:\n");
+          .append(table.schema()).append("\nProblems:");
       for (String error : errors) {
         sb.append("\n* ").append(error);
       }

File: spark/src/test/java/com/netflix/iceberg/spark/source/TestParquetWrite.java
Patch:
@@ -45,7 +45,7 @@
 public class TestParquetWrite {
   private static final Configuration CONF = new Configuration();
   private static final Schema SCHEMA = new Schema(
-      required(1, "id", Types.IntegerType.get()),
+      optional(1, "id", Types.IntegerType.get()),
       optional(2, "data", Types.StringType.get())
   );
 

File: parquet/src/main/java/com/netflix/iceberg/parquet/ParquetIterable.java
Patch:
@@ -36,7 +36,9 @@ public class ParquetIterable<T> implements Iterable<T>, Closeable {
   @Override
   public Iterator<T> iterator() {
     try {
-      return new ParquetIterator<>(builder.build());
+      ParquetReader<T> reader = builder.build();
+      closeables.add(reader);
+      return new ParquetIterator<>(reader);
     } catch (IOException e) {
       throw new RuntimeIOException(e, "Failed to create Parquet reader");
     }

File: core/src/main/java/com/netflix/iceberg/BaseMetastoreTableOperations.java
Patch:
@@ -79,12 +79,12 @@ public String dataLocation() {
 
   protected String writeNewMetadata(TableMetadata metadata, int version) {
     String newFilename = newTableMetadataFilename(baseLocation, version + 1);
-    OutputFile newMetadataLocation = newMetadataFile(newFilename);
+    OutputFile newMetadataLocation = HadoopOutputFile.fromPath(new Path(newFilename), conf);
 
     // write the new metadata
     TableMetadataParser.write(metadata, newMetadataLocation);
 
-    return newMetadataLocation.toString();
+    return newFilename;
   }
 
   protected void refreshFromMetadataLocation(String newLocation) {

File: spark/src/test/java/com/netflix/iceberg/spark/data/AvroDataTest.java
Patch:
@@ -49,8 +49,8 @@ public abstract class AvroDataTest {
       //required(111, "uuid", Types.UUIDType.get()),
       //required(112, "fixed", Types.FixedType.ofLength(7)),
       optional(113, "bytes", Types.BinaryType.get()),
-      required(114, "dec_9_0", Types.DecimalType.of(9, 0)),
-      required(115, "dec_9_2", Types.DecimalType.of(9, 2)),
+      //required(114, "dec_9_0", Types.DecimalType.of(9, 0)),
+      //required(115, "dec_9_2", Types.DecimalType.of(9, 2)),
       required(116, "dec_38_10", Types.DecimalType.of(38, 10)) // spark's maximum precision
   );
 

File: spark/src/main/java/com/netflix/iceberg/spark/source/Writer.java
Patch:
@@ -222,6 +222,7 @@ public FileAppender<T> newAppender(OutputFile file, FileFormat format) {
                   .config("spark.sql.parquet.writeLegacyFormat", "false")
                   .config("spark.sql.parquet.binaryAsString", "false")
                   .config("spark.sql.parquet.int96AsTimestamp", "false")
+                  .config("spark.sql.parquet.outputTimestampType", "TIMESTAMP_MICROS")
                   .schema(schema)
                   .build();
 

File: spark/src/test/java/com/netflix/iceberg/spark/data/AvroDataTest.java
Patch:
@@ -46,8 +46,8 @@ public abstract class AvroDataTest {
       optional(107, "date", Types.DateType.get()),
       required(108, "ts", Types.TimestampType.withZone()),
       required(110, "s", Types.StringType.get()),
-      required(111, "uuid", Types.UUIDType.get()),
-      required(112, "fixed", Types.FixedType.ofLength(7)),
+      //required(111, "uuid", Types.UUIDType.get()),
+      //required(112, "fixed", Types.FixedType.ofLength(7)),
       optional(113, "bytes", Types.BinaryType.get()),
       required(114, "dec_9_0", Types.DecimalType.of(9, 0)),
       required(115, "dec_9_2", Types.DecimalType.of(9, 2)),

File: spark/src/main/java/com/netflix/iceberg/spark/SparkSchemaUtil.java
Patch:
@@ -80,7 +80,7 @@ public static PartitionSpec specForTable(SparkSession spark, String name) throws
    * @throws IllegalArgumentException if the type cannot be converted to Spark
    */
   public static StructType convert(Schema schema) {
-    return (StructType) TypeUtil.visit(schema, TypeToSparkType.get());
+    return (StructType) TypeUtil.visit(schema, new TypeToSparkType());
   }
 
   /**
@@ -91,7 +91,7 @@ public static StructType convert(Schema schema) {
    * @throws IllegalArgumentException if the type cannot be converted to Spark
    */
   public static DataType convert(Type type) {
-    return TypeUtil.visit(type, TypeToSparkType.get());
+    return TypeUtil.visit(type, new TypeToSparkType());
   }
 
   /**

File: parquet/src/main/java/com/netflix/iceberg/parquet/ParquetTypeVisitor.java
Patch:
@@ -61,7 +61,7 @@ public static <T> T visit(Type type, ParquetTypeVisitor<T> visitor) {
             try {
               T elementResult = null;
               if (repeatedElement.getFieldCount() > 0) {
-                elementResult = visit(repeatedElement.getType(0), visitor);
+                elementResult = visitField(repeatedElement.getType(0), visitor);
               }
 
               return visitor.list(group, elementResult);

File: avro/src/main/java/com/netflix/iceberg/avro/SchemaToType.java
Patch:
@@ -26,7 +26,6 @@
 import java.util.List;
 
 import static com.netflix.iceberg.avro.AvroSchemaUtil.ADJUST_TO_UTC_PROP;
-import static com.netflix.iceberg.avro.AvroSchemaUtil.fromOption;
 import static com.netflix.iceberg.avro.AvroSchemaUtil.isOptionSchema;
 
 class SchemaToType extends AvroSchemaVisitor<Type> {

File: avro/src/main/java/com/netflix/iceberg/avro/UUIDConversion.java
Patch:
@@ -26,7 +26,7 @@
 import java.nio.ByteOrder;
 import java.util.UUID;
 
-class UUIDConversion extends Conversion<UUID> {
+public class UUIDConversion extends Conversion<UUID> {
   @Override
   public Class<UUID> getConvertedType() {
     return UUID.class;

