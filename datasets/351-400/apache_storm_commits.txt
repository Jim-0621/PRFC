File: storm-client/src/jvm/org/apache/storm/blobstore/BlobStore.java
Patch:
@@ -444,7 +444,7 @@ public BlobStoreFileInputStream(BlobStoreFile part) throws IOException {
 
         @Override
         public long getVersion() throws IOException {
-            return part.getModTime();
+            return part.getVersion();
         }
 
         @Override

File: storm-server/src/main/java/org/apache/storm/blobstore/LocalFsBlobStore.java
Patch:
@@ -291,7 +291,7 @@ public ReadableBlobMeta getBlobMeta(String key, Subject who) throws Authorizatio
         rbm.set_settable(meta);
         try {
             LocalFsBlobStoreFile pf = fbs.read(DATA_PREFIX + key);
-            rbm.set_version(pf.getModTime());
+            rbm.set_version(pf.getVersion());
         } catch (IOException e) {
             throw new RuntimeException(e);
         }

File: external/storm-kafka-client/src/test/java/org/apache/storm/kafka/spout/MaxUncommittedOffsetTest.java
Patch:
@@ -39,7 +39,7 @@
 import org.apache.kafka.clients.consumer.ConsumerConfig;
 import org.apache.storm.kafka.KafkaUnitExtension;
 import org.apache.storm.kafka.spout.config.builder.SingleTopicKafkaSpoutConfiguration;
-import org.apache.storm.kafka.spout.internal.ConsumerFactoryDefault;
+import org.apache.storm.kafka.spout.internal.ClientFactoryDefault;
 import org.apache.storm.spout.SpoutOutputCollector;
 import org.apache.storm.task.TopologyContext;
 import org.apache.storm.utils.Time;
@@ -86,7 +86,7 @@ public void setUp() {
         //The spout must be able to reemit all retriable tuples, even if the maxPollRecords is set to a low value compared to maxUncommittedOffsets.
         assertThat("Current tests require maxPollRecords < maxUncommittedOffsets", maxPollRecords, lessThanOrEqualTo(maxUncommittedOffsets));
         spout = new KafkaSpout<>(spoutConfig);
-        new ConsumerFactoryDefault<String, String>().createConsumer(spoutConfig.getKafkaProps());
+        new ClientFactoryDefault<String, String>().createConsumer(spoutConfig.getKafkaProps());
     }
 
     private void prepareSpout(int msgCount) throws Exception {

File: storm-server/src/main/java/org/apache/storm/daemon/metrics/reporters/JmxPreparableReporter.java
Patch:
@@ -12,12 +12,12 @@
 
 package org.apache.storm.daemon.metrics.reporters;
 
-import com.codahale.metrics.JmxReporter;
 import com.codahale.metrics.MetricRegistry;
+import com.codahale.metrics.jmx.JmxReporter;
+
 import java.util.Map;
 import java.util.concurrent.TimeUnit;
 import org.apache.storm.DaemonConfig;
-import org.apache.storm.daemon.metrics.ClientMetricsUtils;
 import org.apache.storm.daemon.metrics.MetricsUtils;
 import org.apache.storm.utils.ObjectReader;
 import org.slf4j.Logger;

File: storm-server/src/main/java/org/apache/storm/security/auth/IHttpCredentialsPlugin.java
Patch:
@@ -19,7 +19,6 @@
 package org.apache.storm.security.auth;
 
 import java.util.Map;
-import javax.servlet.http.HttpServletRequest;
 
 /**
  * Interface for handling credentials in an HttpServletRequest.
@@ -39,13 +38,14 @@ public interface IHttpCredentialsPlugin {
      * @param req the servlet request
      * @return the authenticated user, or null if none is authenticated.
      */
-    String getUserName(HttpServletRequest req);
+    String getUserName(jakarta.servlet.http.HttpServletRequest req);
 
     /**
      * Populates a given context with credentials information from an HTTP request.
      *
      * @param req the servlet request
      * @return the context
      */
-    ReqContext populateContext(ReqContext context, HttpServletRequest req);
+    ReqContext populateContext(ReqContext context, jakarta.servlet.http.HttpServletRequest req);
+
 }

File: storm-server/src/test/java/org/apache/storm/security/auth/DefaultHttpCredentialsPluginTest.java
Patch:
@@ -16,7 +16,7 @@
 import java.util.HashMap;
 import java.util.HashSet;
 import javax.security.auth.Subject;
-import javax.servlet.http.HttpServletRequest;
+import jakarta.servlet.http.HttpServletRequest;
 import org.apache.storm.shade.com.google.common.collect.ImmutableSet;
 import org.junit.jupiter.api.Test;
 import org.mockito.Mockito;
@@ -32,7 +32,7 @@ public void test_getUserName() {
         DefaultHttpCredentialsPlugin handler = new DefaultHttpCredentialsPlugin();
         handler.prepare(new HashMap<>());
 
-        assertNull(handler.getUserName(null), "Should return null when request is null");
+        assertNull(handler.getUserName((HttpServletRequest) null), "Should return null when request is null");
 
         assertNull(handler.getUserName(Mockito.mock(HttpServletRequest.class)), "Should return null when user principal is null");
 

File: storm-server/src/test/java/org/apache/storm/security/auth/ServerAuthUtilsTest.java
Patch:
@@ -21,7 +21,6 @@
 import org.apache.storm.DaemonConfig;
 import org.junit.jupiter.api.Test;
 
-import javax.servlet.http.HttpServletRequest;
 import java.util.HashMap;
 import java.util.Map;
 
@@ -37,12 +36,12 @@ public void prepare(Map<String, Object> topoConf) {
         }
 
         @Override
-        public String getUserName(HttpServletRequest req) {
+        public String getUserName(jakarta.servlet.http.HttpServletRequest req) {
             return null;
         }
 
         @Override
-        public ReqContext populateContext(ReqContext context, HttpServletRequest req) {
+        public ReqContext populateContext(ReqContext context, jakarta.servlet.http.HttpServletRequest req) {
             return null;
         }
     }

File: storm-webapp/src/main/java/org/apache/storm/daemon/common/JsonResponseBuilder.java
Patch:
@@ -18,9 +18,10 @@
 
 package org.apache.storm.daemon.common;
 
+import jakarta.ws.rs.core.Response;
+
 import java.util.Collections;
 import java.util.Map;
-import javax.ws.rs.core.Response;
 
 import org.apache.storm.daemon.ui.UIHelpers;
 

File: storm-webapp/src/main/java/org/apache/storm/daemon/common/ReloadableSslContextFactory.java
Patch:
@@ -25,7 +25,7 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-public class ReloadableSslContextFactory extends SslContextFactory {
+public class ReloadableSslContextFactory extends SslContextFactory.Server {
 
     private static final Logger LOG = LoggerFactory.getLogger(ReloadableSslContextFactory.class);
 

File: storm-webapp/src/main/java/org/apache/storm/daemon/drpc/DRPCServer.java
Patch:
@@ -20,11 +20,11 @@
 
 import com.codahale.metrics.Meter;
 import com.google.common.annotations.VisibleForTesting;
+import jakarta.servlet.DispatcherType;
 import java.util.Arrays;
 import java.util.EnumSet;
 import java.util.List;
 import java.util.Map;
-import javax.servlet.DispatcherType;
 import org.apache.storm.Config;
 import org.apache.storm.DaemonConfig;
 import org.apache.storm.daemon.drpc.webapp.DRPCApplication;
@@ -119,7 +119,7 @@ private static Server mkHttpServer(StormMetricsRegistry metricsRegistry, Map<Str
 
             ServletHolder jerseyServlet = context.addServlet(ServletContainer.class, "/*");
             jerseyServlet.setInitOrder(1);
-            jerseyServlet.setInitParameter("javax.ws.rs.Application", DRPCApplication.class.getName());
+            jerseyServlet.setInitParameter("jakarta.ws.rs.Application", DRPCApplication.class.getName());
             
             UIHelpers.configFilters(context, filterConfigurations);
             addRequestContextFilter(context, DaemonConfig.DRPC_HTTP_CREDS_PLUGIN, conf);

File: storm-webapp/src/main/java/org/apache/storm/daemon/drpc/webapp/DRPCApplication.java
Patch:
@@ -18,12 +18,12 @@
 
 package org.apache.storm.daemon.drpc.webapp;
 
+import jakarta.ws.rs.ApplicationPath;
+import jakarta.ws.rs.core.Application;
+
 import java.util.HashSet;
 import java.util.Set;
 
-import javax.ws.rs.ApplicationPath;
-import javax.ws.rs.core.Application;
-
 import org.apache.storm.daemon.common.AuthorizationExceptionMapper;
 import org.apache.storm.daemon.drpc.DRPC;
 import org.apache.storm.metric.StormMetricsRegistry;

File: storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/LogviewerServer.java
Patch:
@@ -111,7 +111,7 @@ private static Server mkHttpServer(StormMetricsRegistry metricsRegistry, Map<Str
 
             ServletHolder jerseyServlet = context.addServlet(ServletContainer.class, "/api/v1/*");
             jerseyServlet.setInitOrder(2);
-            jerseyServlet.setInitParameter("javax.ws.rs.Application", LogviewerApplication.class.getName());
+            jerseyServlet.setInitParameter("jakarta.ws.rs.Application", LogviewerApplication.class.getName());
 
             UIHelpers.configFilters(context, filterConfigurations);
         }

File: storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/handler/LogviewerLogDownloadHandler.java
Patch:
@@ -18,8 +18,9 @@
 
 package org.apache.storm.daemon.logviewer.handler;
 
+import jakarta.ws.rs.core.Response;
+
 import java.io.IOException;
-import javax.ws.rs.core.Response;
 
 import org.apache.storm.daemon.logviewer.utils.LogFileDownloader;
 import org.apache.storm.daemon.logviewer.utils.ResourceAuthorizer;

File: storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/handler/LogviewerLogPageHandler.java
Patch:
@@ -41,6 +41,8 @@
 import j2html.attributes.Attr;
 import j2html.tags.DomContent;
 
+import jakarta.ws.rs.core.Response;
+
 import java.io.ByteArrayOutputStream;
 import java.io.File;
 import java.io.FileInputStream;
@@ -63,8 +65,6 @@
 import java.util.regex.Pattern;
 import java.util.zip.GZIPInputStream;
 
-import javax.ws.rs.core.Response;
-
 import org.apache.commons.lang.StringUtils;
 import org.apache.storm.daemon.logviewer.LogviewerConstant;
 import org.apache.storm.daemon.logviewer.utils.DirectoryCleaner;

File: storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/handler/LogviewerLogSearchHandler.java
Patch:
@@ -32,6 +32,8 @@
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.annotations.VisibleForTesting;
 
+import jakarta.ws.rs.core.Response;
+
 import java.io.BufferedInputStream;
 import java.io.IOException;
 import java.io.InputStream;
@@ -54,8 +56,6 @@
 import java.util.stream.Stream;
 import java.util.zip.GZIPInputStream;
 
-import javax.ws.rs.core.Response;
-
 import net.minidev.json.JSONAware;
 
 import org.apache.commons.lang.BooleanUtils;

File: storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/handler/LogviewerProfileHandler.java
Patch:
@@ -31,12 +31,13 @@
 import com.codahale.metrics.Meter;
 import j2html.tags.DomContent;
 
+import jakarta.ws.rs.core.Response;
+
 import java.io.File;
 import java.io.IOException;
 import java.nio.file.Path;
 import java.nio.file.Paths;
 import java.util.List;
-import javax.ws.rs.core.Response;
 
 import org.apache.commons.lang.StringUtils;
 import org.apache.storm.daemon.logviewer.utils.DirectoryCleaner;

File: storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/utils/LogFileDownloader.java
Patch:
@@ -21,12 +21,12 @@
 import com.codahale.metrics.Histogram;
 import com.codahale.metrics.Meter;
 
+import jakarta.ws.rs.core.Response;
+
 import java.io.IOException;
 import java.nio.file.Path;
 import java.nio.file.Paths;
 
-import javax.ws.rs.core.Response;
-
 import org.apache.commons.io.FileUtils;
 import org.apache.storm.metric.StormMetricsRegistry;
 

File: storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/webapp/LogviewerApplication.java
Patch:
@@ -20,15 +20,15 @@
 
 import static org.apache.storm.DaemonConfig.LOGVIEWER_APPENDER_NAME;
 
+import jakarta.ws.rs.ApplicationPath;
+import jakarta.ws.rs.core.Application;
+
 import java.io.File;
 import java.nio.file.Paths;
 import java.util.HashSet;
 import java.util.Map;
 import java.util.Set;
 
-import javax.ws.rs.ApplicationPath;
-import javax.ws.rs.core.Application;
-
 import org.apache.logging.log4j.LogManager;
 import org.apache.logging.log4j.core.Appender;
 import org.apache.logging.log4j.core.LoggerContext;

File: storm-webapp/src/main/java/org/apache/storm/daemon/ui/UIServer.java
Patch:
@@ -21,13 +21,15 @@
 import static org.apache.storm.utils.ConfigUtils.FILE_SEPARATOR;
 import static org.apache.storm.utils.ConfigUtils.STORM_HOME;
 
+import jakarta.servlet.DispatcherType;
+
 import java.nio.file.Files;
 import java.nio.file.Paths;
 import java.util.Arrays;
 import java.util.EnumSet;
 import java.util.List;
 import java.util.Map;
-import javax.servlet.DispatcherType;
+
 import org.apache.storm.DaemonConfig;
 import org.apache.storm.daemon.drpc.webapp.ReqContextFilter;
 import org.apache.storm.daemon.ui.exceptionmappers.AuthorizationExceptionMapper;

File: storm-webapp/src/main/java/org/apache/storm/daemon/ui/exceptionmappers/ExceptionMapperUtils.java
Patch:
@@ -18,9 +18,9 @@
 
 package org.apache.storm.daemon.ui.exceptionmappers;
 
-import javax.inject.Provider;
-import javax.servlet.http.HttpServletRequest;
-import javax.ws.rs.core.Response;
+import jakarta.inject.Provider;
+import jakarta.servlet.http.HttpServletRequest;
+import jakarta.ws.rs.core.Response;
 
 import org.apache.storm.daemon.common.JsonResponseBuilder;
 import org.apache.storm.daemon.ui.UIHelpers;

File: storm-webapp/src/test/java/org/apache/storm/daemon/logviewer/handler/LogviewerLogDownloadHandlerTest.java
Patch:
@@ -30,7 +30,7 @@
 import java.nio.file.Files;
 import java.nio.file.Path;
 import java.util.Map;
-import javax.ws.rs.core.Response;
+import jakarta.ws.rs.core.Response;
 import org.apache.storm.daemon.logviewer.utils.ResourceAuthorizer;
 import org.apache.storm.daemon.logviewer.utils.WorkerLogs;
 import org.apache.storm.metric.StormMetricsRegistry;

File: storm-webapp/src/test/java/org/apache/storm/daemon/logviewer/handler/LogviewerProfileHandlerTest.java
Patch:
@@ -31,7 +31,7 @@
 import java.nio.file.Files;
 import java.nio.file.Path;
 import java.util.Map;
-import javax.ws.rs.core.Response;
+import jakarta.ws.rs.core.Response;
 import org.apache.storm.daemon.logviewer.utils.ResourceAuthorizer;
 import org.apache.storm.metric.StormMetricsRegistry;
 import org.apache.storm.testing.TmpPath;

File: storm-client/src/jvm/org/apache/storm/generated/AccessControl.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class AccessControl implements org.apache.storm.thrift.TBase<AccessControl, AccessControl._Fields>, java.io.Serializable, Cloneable, Comparable<AccessControl> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("AccessControl");
 

File: storm-client/src/jvm/org/apache/storm/generated/AccessControlType.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public enum AccessControlType implements org.apache.storm.thrift.TEnum {
   OTHER(1),
   USER(2);

File: storm-client/src/jvm/org/apache/storm/generated/AlreadyAliveException.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class AlreadyAliveException extends org.apache.storm.thrift.TException implements org.apache.storm.thrift.TBase<AlreadyAliveException, AlreadyAliveException._Fields>, java.io.Serializable, Cloneable, Comparable<AlreadyAliveException> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("AlreadyAliveException");
 

File: storm-client/src/jvm/org/apache/storm/generated/Assignment.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class Assignment implements org.apache.storm.thrift.TBase<Assignment, Assignment._Fields>, java.io.Serializable, Cloneable, Comparable<Assignment> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("Assignment");
 

File: storm-client/src/jvm/org/apache/storm/generated/AuthorizationException.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class AuthorizationException extends org.apache.storm.thrift.TException implements org.apache.storm.thrift.TBase<AuthorizationException, AuthorizationException._Fields>, java.io.Serializable, Cloneable, Comparable<AuthorizationException> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("AuthorizationException");
 

File: storm-client/src/jvm/org/apache/storm/generated/BeginDownloadResult.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class BeginDownloadResult implements org.apache.storm.thrift.TBase<BeginDownloadResult, BeginDownloadResult._Fields>, java.io.Serializable, Cloneable, Comparable<BeginDownloadResult> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("BeginDownloadResult");
 

File: storm-client/src/jvm/org/apache/storm/generated/Bolt.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class Bolt implements org.apache.storm.thrift.TBase<Bolt, Bolt._Fields>, java.io.Serializable, Cloneable, Comparable<Bolt> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("Bolt");
 

File: storm-client/src/jvm/org/apache/storm/generated/BoltAggregateStats.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class BoltAggregateStats implements org.apache.storm.thrift.TBase<BoltAggregateStats, BoltAggregateStats._Fields>, java.io.Serializable, Cloneable, Comparable<BoltAggregateStats> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("BoltAggregateStats");
 

File: storm-client/src/jvm/org/apache/storm/generated/BoltStats.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class BoltStats implements org.apache.storm.thrift.TBase<BoltStats, BoltStats._Fields>, java.io.Serializable, Cloneable, Comparable<BoltStats> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("BoltStats");
 

File: storm-client/src/jvm/org/apache/storm/generated/ClusterSummary.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class ClusterSummary implements org.apache.storm.thrift.TBase<ClusterSummary, ClusterSummary._Fields>, java.io.Serializable, Cloneable, Comparable<ClusterSummary> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("ClusterSummary");
 

File: storm-client/src/jvm/org/apache/storm/generated/ClusterWorkerHeartbeat.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class ClusterWorkerHeartbeat implements org.apache.storm.thrift.TBase<ClusterWorkerHeartbeat, ClusterWorkerHeartbeat._Fields>, java.io.Serializable, Cloneable, Comparable<ClusterWorkerHeartbeat> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("ClusterWorkerHeartbeat");
 

File: storm-client/src/jvm/org/apache/storm/generated/CommonAggregateStats.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class CommonAggregateStats implements org.apache.storm.thrift.TBase<CommonAggregateStats, CommonAggregateStats._Fields>, java.io.Serializable, Cloneable, Comparable<CommonAggregateStats> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("CommonAggregateStats");
 

File: storm-client/src/jvm/org/apache/storm/generated/ComponentAggregateStats.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class ComponentAggregateStats implements org.apache.storm.thrift.TBase<ComponentAggregateStats, ComponentAggregateStats._Fields>, java.io.Serializable, Cloneable, Comparable<ComponentAggregateStats> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("ComponentAggregateStats");
 

File: storm-client/src/jvm/org/apache/storm/generated/ComponentCommon.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class ComponentCommon implements org.apache.storm.thrift.TBase<ComponentCommon, ComponentCommon._Fields>, java.io.Serializable, Cloneable, Comparable<ComponentCommon> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("ComponentCommon");
 

File: storm-client/src/jvm/org/apache/storm/generated/ComponentObject.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class ComponentObject extends org.apache.storm.thrift.TUnion<ComponentObject, ComponentObject._Fields> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("ComponentObject");
   private static final org.apache.storm.thrift.protocol.TField SERIALIZED_JAVA_FIELD_DESC = new org.apache.storm.thrift.protocol.TField("serialized_java", org.apache.storm.thrift.protocol.TType.STRING, (short)1);

File: storm-client/src/jvm/org/apache/storm/generated/ComponentPageInfo.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class ComponentPageInfo implements org.apache.storm.thrift.TBase<ComponentPageInfo, ComponentPageInfo._Fields>, java.io.Serializable, Cloneable, Comparable<ComponentPageInfo> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("ComponentPageInfo");
 

File: storm-client/src/jvm/org/apache/storm/generated/ComponentType.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public enum ComponentType implements org.apache.storm.thrift.TEnum {
   BOLT(1),
   SPOUT(2);

File: storm-client/src/jvm/org/apache/storm/generated/Credentials.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class Credentials implements org.apache.storm.thrift.TBase<Credentials, Credentials._Fields>, java.io.Serializable, Cloneable, Comparable<Credentials> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("Credentials");
 

File: storm-client/src/jvm/org/apache/storm/generated/DRPCExceptionType.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public enum DRPCExceptionType implements org.apache.storm.thrift.TEnum {
   INTERNAL_ERROR(0),
   SERVER_SHUTDOWN(1),

File: storm-client/src/jvm/org/apache/storm/generated/DRPCExecutionException.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class DRPCExecutionException extends org.apache.storm.thrift.TException implements org.apache.storm.thrift.TBase<DRPCExecutionException, DRPCExecutionException._Fields>, java.io.Serializable, Cloneable, Comparable<DRPCExecutionException> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("DRPCExecutionException");
 

File: storm-client/src/jvm/org/apache/storm/generated/DRPCRequest.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class DRPCRequest implements org.apache.storm.thrift.TBase<DRPCRequest, DRPCRequest._Fields>, java.io.Serializable, Cloneable, Comparable<DRPCRequest> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("DRPCRequest");
 

File: storm-client/src/jvm/org/apache/storm/generated/DebugOptions.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class DebugOptions implements org.apache.storm.thrift.TBase<DebugOptions, DebugOptions._Fields>, java.io.Serializable, Cloneable, Comparable<DebugOptions> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("DebugOptions");
 

File: storm-client/src/jvm/org/apache/storm/generated/DistributedRPC.java
Patch:
@@ -16,14 +16,14 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
 public class DistributedRPC {
 

File: storm-client/src/jvm/org/apache/storm/generated/DistributedRPCInvocations.java
Patch:
@@ -16,14 +16,14 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
 public class DistributedRPCInvocations {
 

File: storm-client/src/jvm/org/apache/storm/generated/ErrorInfo.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class ErrorInfo implements org.apache.storm.thrift.TBase<ErrorInfo, ErrorInfo._Fields>, java.io.Serializable, Cloneable, Comparable<ErrorInfo> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("ErrorInfo");
 

File: storm-client/src/jvm/org/apache/storm/generated/ExecutorAggregateStats.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class ExecutorAggregateStats implements org.apache.storm.thrift.TBase<ExecutorAggregateStats, ExecutorAggregateStats._Fields>, java.io.Serializable, Cloneable, Comparable<ExecutorAggregateStats> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("ExecutorAggregateStats");
 

File: storm-client/src/jvm/org/apache/storm/generated/ExecutorInfo.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class ExecutorInfo implements org.apache.storm.thrift.TBase<ExecutorInfo, ExecutorInfo._Fields>, java.io.Serializable, Cloneable, Comparable<ExecutorInfo> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("ExecutorInfo");
 

File: storm-client/src/jvm/org/apache/storm/generated/ExecutorSpecificStats.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class ExecutorSpecificStats extends org.apache.storm.thrift.TUnion<ExecutorSpecificStats, ExecutorSpecificStats._Fields> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("ExecutorSpecificStats");
   private static final org.apache.storm.thrift.protocol.TField BOLT_FIELD_DESC = new org.apache.storm.thrift.protocol.TField("bolt", org.apache.storm.thrift.protocol.TType.STRUCT, (short)1);

File: storm-client/src/jvm/org/apache/storm/generated/ExecutorStats.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class ExecutorStats implements org.apache.storm.thrift.TBase<ExecutorStats, ExecutorStats._Fields>, java.io.Serializable, Cloneable, Comparable<ExecutorStats> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("ExecutorStats");
 

File: storm-client/src/jvm/org/apache/storm/generated/ExecutorSummary.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class ExecutorSummary implements org.apache.storm.thrift.TBase<ExecutorSummary, ExecutorSummary._Fields>, java.io.Serializable, Cloneable, Comparable<ExecutorSummary> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("ExecutorSummary");
 

File: storm-client/src/jvm/org/apache/storm/generated/GetInfoOptions.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class GetInfoOptions implements org.apache.storm.thrift.TBase<GetInfoOptions, GetInfoOptions._Fields>, java.io.Serializable, Cloneable, Comparable<GetInfoOptions> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("GetInfoOptions");
 

File: storm-client/src/jvm/org/apache/storm/generated/GlobalStreamId.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class GlobalStreamId implements org.apache.storm.thrift.TBase<GlobalStreamId, GlobalStreamId._Fields>, java.io.Serializable, Cloneable, Comparable<GlobalStreamId> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("GlobalStreamId");
 

File: storm-client/src/jvm/org/apache/storm/generated/Grouping.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class Grouping extends org.apache.storm.thrift.TUnion<Grouping, Grouping._Fields> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("Grouping");
   private static final org.apache.storm.thrift.protocol.TField FIELDS_FIELD_DESC = new org.apache.storm.thrift.protocol.TField("fields", org.apache.storm.thrift.protocol.TType.LIST, (short)1);

File: storm-client/src/jvm/org/apache/storm/generated/HBAuthorizationException.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class HBAuthorizationException extends org.apache.storm.thrift.TException implements org.apache.storm.thrift.TBase<HBAuthorizationException, HBAuthorizationException._Fields>, java.io.Serializable, Cloneable, Comparable<HBAuthorizationException> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("HBAuthorizationException");
 

File: storm-client/src/jvm/org/apache/storm/generated/HBExecutionException.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class HBExecutionException extends org.apache.storm.thrift.TException implements org.apache.storm.thrift.TBase<HBExecutionException, HBExecutionException._Fields>, java.io.Serializable, Cloneable, Comparable<HBExecutionException> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("HBExecutionException");
 

File: storm-client/src/jvm/org/apache/storm/generated/HBMessage.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class HBMessage implements org.apache.storm.thrift.TBase<HBMessage, HBMessage._Fields>, java.io.Serializable, Cloneable, Comparable<HBMessage> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("HBMessage");
 

File: storm-client/src/jvm/org/apache/storm/generated/HBMessageData.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class HBMessageData extends org.apache.storm.thrift.TUnion<HBMessageData, HBMessageData._Fields> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("HBMessageData");
   private static final org.apache.storm.thrift.protocol.TField PATH_FIELD_DESC = new org.apache.storm.thrift.protocol.TField("path", org.apache.storm.thrift.protocol.TType.STRING, (short)1);

File: storm-client/src/jvm/org/apache/storm/generated/HBNodes.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class HBNodes implements org.apache.storm.thrift.TBase<HBNodes, HBNodes._Fields>, java.io.Serializable, Cloneable, Comparable<HBNodes> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("HBNodes");
 

File: storm-client/src/jvm/org/apache/storm/generated/HBPulse.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class HBPulse implements org.apache.storm.thrift.TBase<HBPulse, HBPulse._Fields>, java.io.Serializable, Cloneable, Comparable<HBPulse> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("HBPulse");
 

File: storm-client/src/jvm/org/apache/storm/generated/HBRecords.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class HBRecords implements org.apache.storm.thrift.TBase<HBRecords, HBRecords._Fields>, java.io.Serializable, Cloneable, Comparable<HBRecords> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("HBRecords");
 

File: storm-client/src/jvm/org/apache/storm/generated/HBServerMessageType.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public enum HBServerMessageType implements org.apache.storm.thrift.TEnum {
   CREATE_PATH(0),
   CREATE_PATH_RESPONSE(1),

File: storm-client/src/jvm/org/apache/storm/generated/IllegalStateException.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class IllegalStateException extends org.apache.storm.thrift.TException implements org.apache.storm.thrift.TBase<IllegalStateException, IllegalStateException._Fields>, java.io.Serializable, Cloneable, Comparable<IllegalStateException> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("IllegalStateException");
 

File: storm-client/src/jvm/org/apache/storm/generated/InvalidTopologyException.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class InvalidTopologyException extends org.apache.storm.thrift.TException implements org.apache.storm.thrift.TBase<InvalidTopologyException, InvalidTopologyException._Fields>, java.io.Serializable, Cloneable, Comparable<InvalidTopologyException> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("InvalidTopologyException");
 

File: storm-client/src/jvm/org/apache/storm/generated/JavaObject.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class JavaObject implements org.apache.storm.thrift.TBase<JavaObject, JavaObject._Fields>, java.io.Serializable, Cloneable, Comparable<JavaObject> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("JavaObject");
 

File: storm-client/src/jvm/org/apache/storm/generated/JavaObjectArg.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class JavaObjectArg extends org.apache.storm.thrift.TUnion<JavaObjectArg, JavaObjectArg._Fields> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("JavaObjectArg");
   private static final org.apache.storm.thrift.protocol.TField INT_ARG_FIELD_DESC = new org.apache.storm.thrift.protocol.TField("int_arg", org.apache.storm.thrift.protocol.TType.I32, (short)1);

File: storm-client/src/jvm/org/apache/storm/generated/KeyAlreadyExistsException.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class KeyAlreadyExistsException extends org.apache.storm.thrift.TException implements org.apache.storm.thrift.TBase<KeyAlreadyExistsException, KeyAlreadyExistsException._Fields>, java.io.Serializable, Cloneable, Comparable<KeyAlreadyExistsException> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("KeyAlreadyExistsException");
 

File: storm-client/src/jvm/org/apache/storm/generated/KeyNotFoundException.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class KeyNotFoundException extends org.apache.storm.thrift.TException implements org.apache.storm.thrift.TBase<KeyNotFoundException, KeyNotFoundException._Fields>, java.io.Serializable, Cloneable, Comparable<KeyNotFoundException> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("KeyNotFoundException");
 

File: storm-client/src/jvm/org/apache/storm/generated/KillOptions.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class KillOptions implements org.apache.storm.thrift.TBase<KillOptions, KillOptions._Fields>, java.io.Serializable, Cloneable, Comparable<KillOptions> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("KillOptions");
 

File: storm-client/src/jvm/org/apache/storm/generated/LSApprovedWorkers.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class LSApprovedWorkers implements org.apache.storm.thrift.TBase<LSApprovedWorkers, LSApprovedWorkers._Fields>, java.io.Serializable, Cloneable, Comparable<LSApprovedWorkers> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("LSApprovedWorkers");
 

File: storm-client/src/jvm/org/apache/storm/generated/LSSupervisorAssignments.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class LSSupervisorAssignments implements org.apache.storm.thrift.TBase<LSSupervisorAssignments, LSSupervisorAssignments._Fields>, java.io.Serializable, Cloneable, Comparable<LSSupervisorAssignments> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("LSSupervisorAssignments");
 

File: storm-client/src/jvm/org/apache/storm/generated/LSSupervisorId.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class LSSupervisorId implements org.apache.storm.thrift.TBase<LSSupervisorId, LSSupervisorId._Fields>, java.io.Serializable, Cloneable, Comparable<LSSupervisorId> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("LSSupervisorId");
 

File: storm-client/src/jvm/org/apache/storm/generated/LSTopoHistory.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class LSTopoHistory implements org.apache.storm.thrift.TBase<LSTopoHistory, LSTopoHistory._Fields>, java.io.Serializable, Cloneable, Comparable<LSTopoHistory> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("LSTopoHistory");
 

File: storm-client/src/jvm/org/apache/storm/generated/LSTopoHistoryList.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class LSTopoHistoryList implements org.apache.storm.thrift.TBase<LSTopoHistoryList, LSTopoHistoryList._Fields>, java.io.Serializable, Cloneable, Comparable<LSTopoHistoryList> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("LSTopoHistoryList");
 

File: storm-client/src/jvm/org/apache/storm/generated/LSWorkerHeartbeat.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class LSWorkerHeartbeat implements org.apache.storm.thrift.TBase<LSWorkerHeartbeat, LSWorkerHeartbeat._Fields>, java.io.Serializable, Cloneable, Comparable<LSWorkerHeartbeat> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("LSWorkerHeartbeat");
 

File: storm-client/src/jvm/org/apache/storm/generated/ListBlobsResult.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class ListBlobsResult implements org.apache.storm.thrift.TBase<ListBlobsResult, ListBlobsResult._Fields>, java.io.Serializable, Cloneable, Comparable<ListBlobsResult> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("ListBlobsResult");
 

File: storm-client/src/jvm/org/apache/storm/generated/LocalAssignment.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class LocalAssignment implements org.apache.storm.thrift.TBase<LocalAssignment, LocalAssignment._Fields>, java.io.Serializable, Cloneable, Comparable<LocalAssignment> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("LocalAssignment");
 

File: storm-client/src/jvm/org/apache/storm/generated/LocalStateData.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class LocalStateData implements org.apache.storm.thrift.TBase<LocalStateData, LocalStateData._Fields>, java.io.Serializable, Cloneable, Comparable<LocalStateData> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("LocalStateData");
 

File: storm-client/src/jvm/org/apache/storm/generated/LogConfig.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class LogConfig implements org.apache.storm.thrift.TBase<LogConfig, LogConfig._Fields>, java.io.Serializable, Cloneable, Comparable<LogConfig> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("LogConfig");
 

File: storm-client/src/jvm/org/apache/storm/generated/LogLevel.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class LogLevel implements org.apache.storm.thrift.TBase<LogLevel, LogLevel._Fields>, java.io.Serializable, Cloneable, Comparable<LogLevel> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("LogLevel");
 

File: storm-client/src/jvm/org/apache/storm/generated/LogLevelAction.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public enum LogLevelAction implements org.apache.storm.thrift.TEnum {
   UNCHANGED(1),
   UPDATE(2),

File: storm-client/src/jvm/org/apache/storm/generated/Nimbus.java
Patch:
@@ -16,14 +16,14 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
 public class Nimbus {
 

File: storm-client/src/jvm/org/apache/storm/generated/NimbusSummary.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class NimbusSummary implements org.apache.storm.thrift.TBase<NimbusSummary, NimbusSummary._Fields>, java.io.Serializable, Cloneable, Comparable<NimbusSummary> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("NimbusSummary");
 

File: storm-client/src/jvm/org/apache/storm/generated/NodeInfo.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class NodeInfo implements org.apache.storm.thrift.TBase<NodeInfo, NodeInfo._Fields>, java.io.Serializable, Cloneable, Comparable<NodeInfo> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("NodeInfo");
 

File: storm-client/src/jvm/org/apache/storm/generated/NotAliveException.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class NotAliveException extends org.apache.storm.thrift.TException implements org.apache.storm.thrift.TBase<NotAliveException, NotAliveException._Fields>, java.io.Serializable, Cloneable, Comparable<NotAliveException> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("NotAliveException");
 

File: storm-client/src/jvm/org/apache/storm/generated/NullStruct.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class NullStruct implements org.apache.storm.thrift.TBase<NullStruct, NullStruct._Fields>, java.io.Serializable, Cloneable, Comparable<NullStruct> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("NullStruct");
 

File: storm-client/src/jvm/org/apache/storm/generated/NumErrorsChoice.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public enum NumErrorsChoice implements org.apache.storm.thrift.TEnum {
   ALL(0),
   NONE(1),

File: storm-client/src/jvm/org/apache/storm/generated/OwnerResourceSummary.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class OwnerResourceSummary implements org.apache.storm.thrift.TBase<OwnerResourceSummary, OwnerResourceSummary._Fields>, java.io.Serializable, Cloneable, Comparable<OwnerResourceSummary> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("OwnerResourceSummary");
 

File: storm-client/src/jvm/org/apache/storm/generated/PrivateWorkerKey.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class PrivateWorkerKey implements org.apache.storm.thrift.TBase<PrivateWorkerKey, PrivateWorkerKey._Fields>, java.io.Serializable, Cloneable, Comparable<PrivateWorkerKey> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("PrivateWorkerKey");
 

File: storm-client/src/jvm/org/apache/storm/generated/ProfileAction.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public enum ProfileAction implements org.apache.storm.thrift.TEnum {
   JPROFILE_STOP(0),
   JPROFILE_START(1),

File: storm-client/src/jvm/org/apache/storm/generated/ProfileRequest.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class ProfileRequest implements org.apache.storm.thrift.TBase<ProfileRequest, ProfileRequest._Fields>, java.io.Serializable, Cloneable, Comparable<ProfileRequest> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("ProfileRequest");
 

File: storm-client/src/jvm/org/apache/storm/generated/ReadableBlobMeta.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class ReadableBlobMeta implements org.apache.storm.thrift.TBase<ReadableBlobMeta, ReadableBlobMeta._Fields>, java.io.Serializable, Cloneable, Comparable<ReadableBlobMeta> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("ReadableBlobMeta");
 

File: storm-client/src/jvm/org/apache/storm/generated/RebalanceOptions.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class RebalanceOptions implements org.apache.storm.thrift.TBase<RebalanceOptions, RebalanceOptions._Fields>, java.io.Serializable, Cloneable, Comparable<RebalanceOptions> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("RebalanceOptions");
 

File: storm-client/src/jvm/org/apache/storm/generated/SettableBlobMeta.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class SettableBlobMeta implements org.apache.storm.thrift.TBase<SettableBlobMeta, SettableBlobMeta._Fields>, java.io.Serializable, Cloneable, Comparable<SettableBlobMeta> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("SettableBlobMeta");
 

File: storm-client/src/jvm/org/apache/storm/generated/SharedMemory.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class SharedMemory implements org.apache.storm.thrift.TBase<SharedMemory, SharedMemory._Fields>, java.io.Serializable, Cloneable, Comparable<SharedMemory> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("SharedMemory");
 

File: storm-client/src/jvm/org/apache/storm/generated/ShellComponent.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class ShellComponent implements org.apache.storm.thrift.TBase<ShellComponent, ShellComponent._Fields>, java.io.Serializable, Cloneable, Comparable<ShellComponent> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("ShellComponent");
 

File: storm-client/src/jvm/org/apache/storm/generated/SpecificAggregateStats.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class SpecificAggregateStats extends org.apache.storm.thrift.TUnion<SpecificAggregateStats, SpecificAggregateStats._Fields> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("SpecificAggregateStats");
   private static final org.apache.storm.thrift.protocol.TField BOLT_FIELD_DESC = new org.apache.storm.thrift.protocol.TField("bolt", org.apache.storm.thrift.protocol.TType.STRUCT, (short)1);

File: storm-client/src/jvm/org/apache/storm/generated/SpoutAggregateStats.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class SpoutAggregateStats implements org.apache.storm.thrift.TBase<SpoutAggregateStats, SpoutAggregateStats._Fields>, java.io.Serializable, Cloneable, Comparable<SpoutAggregateStats> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("SpoutAggregateStats");
 

File: storm-client/src/jvm/org/apache/storm/generated/SpoutSpec.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class SpoutSpec implements org.apache.storm.thrift.TBase<SpoutSpec, SpoutSpec._Fields>, java.io.Serializable, Cloneable, Comparable<SpoutSpec> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("SpoutSpec");
 

File: storm-client/src/jvm/org/apache/storm/generated/SpoutStats.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class SpoutStats implements org.apache.storm.thrift.TBase<SpoutStats, SpoutStats._Fields>, java.io.Serializable, Cloneable, Comparable<SpoutStats> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("SpoutStats");
 

File: storm-client/src/jvm/org/apache/storm/generated/StateSpoutSpec.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class StateSpoutSpec implements org.apache.storm.thrift.TBase<StateSpoutSpec, StateSpoutSpec._Fields>, java.io.Serializable, Cloneable, Comparable<StateSpoutSpec> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("StateSpoutSpec");
 

File: storm-client/src/jvm/org/apache/storm/generated/StormBase.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class StormBase implements org.apache.storm.thrift.TBase<StormBase, StormBase._Fields>, java.io.Serializable, Cloneable, Comparable<StormBase> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("StormBase");
 

File: storm-client/src/jvm/org/apache/storm/generated/StormTopology.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class StormTopology implements org.apache.storm.thrift.TBase<StormTopology, StormTopology._Fields>, java.io.Serializable, Cloneable, Comparable<StormTopology> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("StormTopology");
 

File: storm-client/src/jvm/org/apache/storm/generated/StreamInfo.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class StreamInfo implements org.apache.storm.thrift.TBase<StreamInfo, StreamInfo._Fields>, java.io.Serializable, Cloneable, Comparable<StreamInfo> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("StreamInfo");
 

File: storm-client/src/jvm/org/apache/storm/generated/SubmitOptions.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class SubmitOptions implements org.apache.storm.thrift.TBase<SubmitOptions, SubmitOptions._Fields>, java.io.Serializable, Cloneable, Comparable<SubmitOptions> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("SubmitOptions");
 

File: storm-client/src/jvm/org/apache/storm/generated/Supervisor.java
Patch:
@@ -16,14 +16,14 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
 public class Supervisor {
 

File: storm-client/src/jvm/org/apache/storm/generated/SupervisorAssignments.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class SupervisorAssignments implements org.apache.storm.thrift.TBase<SupervisorAssignments, SupervisorAssignments._Fields>, java.io.Serializable, Cloneable, Comparable<SupervisorAssignments> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("SupervisorAssignments");
 

File: storm-client/src/jvm/org/apache/storm/generated/SupervisorInfo.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class SupervisorInfo implements org.apache.storm.thrift.TBase<SupervisorInfo, SupervisorInfo._Fields>, java.io.Serializable, Cloneable, Comparable<SupervisorInfo> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("SupervisorInfo");
 

File: storm-client/src/jvm/org/apache/storm/generated/SupervisorPageInfo.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class SupervisorPageInfo implements org.apache.storm.thrift.TBase<SupervisorPageInfo, SupervisorPageInfo._Fields>, java.io.Serializable, Cloneable, Comparable<SupervisorPageInfo> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("SupervisorPageInfo");
 

File: storm-client/src/jvm/org/apache/storm/generated/SupervisorSummary.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class SupervisorSummary implements org.apache.storm.thrift.TBase<SupervisorSummary, SupervisorSummary._Fields>, java.io.Serializable, Cloneable, Comparable<SupervisorSummary> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("SupervisorSummary");
 

File: storm-client/src/jvm/org/apache/storm/generated/SupervisorWorkerHeartbeat.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class SupervisorWorkerHeartbeat implements org.apache.storm.thrift.TBase<SupervisorWorkerHeartbeat, SupervisorWorkerHeartbeat._Fields>, java.io.Serializable, Cloneable, Comparable<SupervisorWorkerHeartbeat> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("SupervisorWorkerHeartbeat");
 

File: storm-client/src/jvm/org/apache/storm/generated/SupervisorWorkerHeartbeats.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class SupervisorWorkerHeartbeats implements org.apache.storm.thrift.TBase<SupervisorWorkerHeartbeats, SupervisorWorkerHeartbeats._Fields>, java.io.Serializable, Cloneable, Comparable<SupervisorWorkerHeartbeats> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("SupervisorWorkerHeartbeats");
 

File: storm-client/src/jvm/org/apache/storm/generated/ThriftSerializedObject.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class ThriftSerializedObject implements org.apache.storm.thrift.TBase<ThriftSerializedObject, ThriftSerializedObject._Fields>, java.io.Serializable, Cloneable, Comparable<ThriftSerializedObject> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("ThriftSerializedObject");
 

File: storm-client/src/jvm/org/apache/storm/generated/TopologyActionOptions.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class TopologyActionOptions extends org.apache.storm.thrift.TUnion<TopologyActionOptions, TopologyActionOptions._Fields> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("TopologyActionOptions");
   private static final org.apache.storm.thrift.protocol.TField KILL_OPTIONS_FIELD_DESC = new org.apache.storm.thrift.protocol.TField("kill_options", org.apache.storm.thrift.protocol.TType.STRUCT, (short)1);

File: storm-client/src/jvm/org/apache/storm/generated/TopologyHistoryInfo.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class TopologyHistoryInfo implements org.apache.storm.thrift.TBase<TopologyHistoryInfo, TopologyHistoryInfo._Fields>, java.io.Serializable, Cloneable, Comparable<TopologyHistoryInfo> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("TopologyHistoryInfo");
 

File: storm-client/src/jvm/org/apache/storm/generated/TopologyInfo.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class TopologyInfo implements org.apache.storm.thrift.TBase<TopologyInfo, TopologyInfo._Fields>, java.io.Serializable, Cloneable, Comparable<TopologyInfo> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("TopologyInfo");
 

File: storm-client/src/jvm/org/apache/storm/generated/TopologyInitialStatus.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public enum TopologyInitialStatus implements org.apache.storm.thrift.TEnum {
   ACTIVE(1),
   INACTIVE(2);

File: storm-client/src/jvm/org/apache/storm/generated/TopologyPageInfo.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class TopologyPageInfo implements org.apache.storm.thrift.TBase<TopologyPageInfo, TopologyPageInfo._Fields>, java.io.Serializable, Cloneable, Comparable<TopologyPageInfo> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("TopologyPageInfo");
 

File: storm-client/src/jvm/org/apache/storm/generated/TopologyStats.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class TopologyStats implements org.apache.storm.thrift.TBase<TopologyStats, TopologyStats._Fields>, java.io.Serializable, Cloneable, Comparable<TopologyStats> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("TopologyStats");
 

File: storm-client/src/jvm/org/apache/storm/generated/TopologyStatus.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public enum TopologyStatus implements org.apache.storm.thrift.TEnum {
   ACTIVE(1),
   INACTIVE(2),

File: storm-client/src/jvm/org/apache/storm/generated/TopologySummary.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class TopologySummary implements org.apache.storm.thrift.TBase<TopologySummary, TopologySummary._Fields>, java.io.Serializable, Cloneable, Comparable<TopologySummary> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("TopologySummary");
 

File: storm-client/src/jvm/org/apache/storm/generated/WorkerMetricList.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class WorkerMetricList implements org.apache.storm.thrift.TBase<WorkerMetricList, WorkerMetricList._Fields>, java.io.Serializable, Cloneable, Comparable<WorkerMetricList> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("WorkerMetricList");
 

File: storm-client/src/jvm/org/apache/storm/generated/WorkerMetricPoint.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class WorkerMetricPoint implements org.apache.storm.thrift.TBase<WorkerMetricPoint, WorkerMetricPoint._Fields>, java.io.Serializable, Cloneable, Comparable<WorkerMetricPoint> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("WorkerMetricPoint");
 

File: storm-client/src/jvm/org/apache/storm/generated/WorkerMetrics.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class WorkerMetrics implements org.apache.storm.thrift.TBase<WorkerMetrics, WorkerMetrics._Fields>, java.io.Serializable, Cloneable, Comparable<WorkerMetrics> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("WorkerMetrics");
 

File: storm-client/src/jvm/org/apache/storm/generated/WorkerResources.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class WorkerResources implements org.apache.storm.thrift.TBase<WorkerResources, WorkerResources._Fields>, java.io.Serializable, Cloneable, Comparable<WorkerResources> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("WorkerResources");
 

File: storm-client/src/jvm/org/apache/storm/generated/WorkerSummary.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class WorkerSummary implements org.apache.storm.thrift.TBase<WorkerSummary, WorkerSummary._Fields>, java.io.Serializable, Cloneable, Comparable<WorkerSummary> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("WorkerSummary");
 

File: storm-client/src/jvm/org/apache/storm/generated/WorkerToken.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class WorkerToken implements org.apache.storm.thrift.TBase<WorkerToken, WorkerToken._Fields>, java.io.Serializable, Cloneable, Comparable<WorkerToken> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("WorkerToken");
 

File: storm-client/src/jvm/org/apache/storm/generated/WorkerTokenInfo.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked", "unused"})
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public class WorkerTokenInfo implements org.apache.storm.thrift.TBase<WorkerTokenInfo, WorkerTokenInfo._Fields>, java.io.Serializable, Cloneable, Comparable<WorkerTokenInfo> {
   private static final org.apache.storm.thrift.protocol.TStruct STRUCT_DESC = new org.apache.storm.thrift.protocol.TStruct("WorkerTokenInfo");
 

File: storm-client/src/jvm/org/apache/storm/generated/WorkerTokenServiceType.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.18.1)
+ * Autogenerated by Thrift Compiler (0.19.0)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
  *  @generated
  */
 package org.apache.storm.generated;
 
 
-@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.18.1)")
+@javax.annotation.Generated(value = "Autogenerated by Thrift Compiler (0.19.0)")
 public enum WorkerTokenServiceType implements org.apache.storm.thrift.TEnum {
   NIMBUS(0),
   DRPC(1),

File: storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java
Patch:
@@ -598,7 +598,7 @@ public Nimbus(Map<String, Object> conf, INimbus inimbus, IStormClusterState stor
         }
         if (leaderElector == null) {
             leaderElector = Zookeeper.zkLeaderElector(conf, zkClient, blobStore, topoCache, stormClusterState, getNimbusAcls(conf),
-                metricsRegistry);
+                metricsRegistry, submitLock);
         }
         this.leaderElector = leaderElector;
         this.blobStore.setLeaderElector(this.leaderElector);

File: external/storm-kafka-client/src/test/java/org/apache/storm/kafka/KafkaUnit.java
Patch:
@@ -29,14 +29,14 @@
 import java.util.concurrent.TimeoutException;
 import kafka.server.KafkaConfig;
 import kafka.server.KafkaServer;
-import kafka.utils.MockTime;
 import kafka.utils.TestUtils;
 import org.apache.curator.test.TestingServer;
 import org.apache.kafka.clients.admin.AdminClient;
 import org.apache.kafka.clients.admin.AdminClientConfig;
 import org.apache.kafka.clients.admin.NewTopic;
 import org.apache.kafka.clients.producer.KafkaProducer;
 import org.apache.kafka.clients.producer.ProducerRecord;
+import org.apache.kafka.common.utils.MockTime;
 import org.apache.storm.testing.TmpPath;
 
 public class KafkaUnit {
@@ -64,8 +64,7 @@ public void setUp() throws Exception {
         brokerProps.setProperty("listeners", String.format("PLAINTEXT://%s:%d", KAFKA_HOST, KAFKA_PORT));
         brokerProps.setProperty("offsets.topic.replication.factor", "1");
         KafkaConfig config = new KafkaConfig(brokerProps);
-        MockTime mock = new MockTime();
-        kafkaServer = TestUtils.createServer(config, mock);
+        kafkaServer = TestUtils.createServer(config, new MockTime());
 
         // setup default Producer
         createProducer();

File: external/storm-kafka-client/src/test/java/org/apache/storm/kafka/spout/KafkaSpoutNullTupleTest.java
Patch:
@@ -39,6 +39,7 @@ public KafkaSpoutNullTupleTest() {
     KafkaSpoutConfig<String, String> createSpoutConfig() {
         return KafkaSpoutConfig.builder("127.0.0.1:" + kafkaUnitExtension.getKafkaUnit().getKafkaPort(),
                 Pattern.compile(SingleTopicKafkaSpoutConfiguration.TOPIC))
+                .setGroupId("test")
                 .setOffsetCommitPeriodMs(commitOffsetPeriodMs)
                 .setRecordTranslator(new NullRecordTranslator<>())
                 .build();

File: storm-server/src/test/java/org/apache/storm/scheduler/resource/TestResourceAwareScheduler.java
Patch:
@@ -762,8 +762,7 @@ public void testTopologyWorkerMaxHeapSize() {
             assertTrue(cluster.needsSchedulingRas(topology2));
             String status = cluster.getStatusMap().get(topology2.getId());
             String expectedStatusPrefix = "Not enough resources to schedule";
-            assertTrue("Expected status to start with \"" + expectedStatusPrefix + "\" but status is: " + status,
-                    status.startsWith(expectedStatusPrefix));
+            assertTrue(status.startsWith(expectedStatusPrefix), "Expected status to start with \"" + expectedStatusPrefix + "\" but status is: " + status);
             assertEquals(5, cluster.getUnassignedExecutors(topology2).size());
         } finally {
             rs.cleanup();
@@ -1137,7 +1136,7 @@ void append(TimeBlockResult other) {
 
     private long getMedianValue(List<Long> values) {
         final int numValues = values.size();
-        assertTrue("Expecting odd number of values to compute median, got " + numValues, (numValues % 2) == 1);
+        assertEquals(1, (numValues % 2), "Expecting odd number of values to compute median, got " + numValues);
         List<Long> sortedValues = new ArrayList<>();
         sortedValues.addAll(values);
         Collections.sort(sortedValues);

File: storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java
Patch:
@@ -2663,7 +2663,7 @@ private int getTopologyHeartbeatTimeoutSecs(String topoId) {
             return getTopologyHeartbeatTimeoutSecs(topoConf);
         } catch (Exception e) {
             // contain any exception
-            LOG.warn("Exception when getting heartbeat timeout.", e.getMessage());
+            LOG.warn("Exception when getting heartbeat timeout", e);
             return ObjectReader.getInt(conf.get(DaemonConfig.NIMBUS_TASK_TIMEOUT_SECS));
         }
     }

File: storm-client/src/jvm/org/apache/storm/trident/testing/MemoryBackingMap.java
Patch:
@@ -18,11 +18,11 @@
 import java.util.Map;
 import org.apache.storm.trident.state.map.IBackingMap;
 
-public class MemoryBackingMap implements IBackingMap<Object> {
+public class MemoryBackingMap<T> implements IBackingMap<T> {
     Map vals = new HashMap();
 
     @Override
-    public List<Object> multiGet(List<List<Object>> keys) {
+    public List<T> multiGet(List<List<Object>> keys) {
         List ret = new ArrayList();
         for (List key : keys) {
             ret.add(vals.get(key));
@@ -31,7 +31,7 @@ public List<Object> multiGet(List<List<Object>> keys) {
     }
 
     @Override
-    public void multiPut(List<List<Object>> keys, List<Object> vals) {
+    public void multiPut(List<List<Object>> keys, List<T> vals) {
         for (int i = 0; i < keys.size(); i++) {
             List key = keys.get(i);
             Object val = vals.get(i);

File: storm-client/src/jvm/org/apache/storm/ILocalCluster.java
Patch:
@@ -79,7 +79,7 @@ ILocalTopology submitTopologyWithOpts(String topologyName, Map<String, Object> c
      * @param options      for how to kill the topology
      * @throws TException on any error from nimbus
      */
-    void killTopologyWithOpts(String name, KillOptions options) throws TException;
+    void killTopologyWithOpts(String topologyName, KillOptions options) throws TException;
 
     /**
      * Activate a topology.
@@ -203,7 +203,7 @@ ILocalTopology submitTopologyWithOpts(String topologyName, Map<String, Object> c
      * @param secs  the number of seconds to advance time
      * @param steps the number of steps we should take when advancing simulated time
      */
-    void advanceClusterTime(int secs, int step) throws InterruptedException;
+    void advanceClusterTime(int secs, int steps) throws InterruptedException;
 
     /**
      * If the cluster is tracked get the id for the tracked cluster. This is intended for internal testing only.

File: storm-client/src/jvm/org/apache/storm/blobstore/BlobStore.java
Patch:
@@ -256,7 +256,6 @@ public void close() {
      * Filters keys based on the KeyFilter passed as the argument.
      *
      * @param filter KeyFilter
-     * @param <R>    Type
      * @return Set of filtered keys
      */
     public <R> Set<R> filterAndListKeys(KeyFilter<R> filter) {

File: storm-client/src/jvm/org/apache/storm/cluster/IStateStorage.java
Patch:
@@ -64,7 +64,6 @@ public interface IStateStorage extends Closeable {
      *
      * @param path The path to create, along with all its parents.
      * @param acls The acls to apply to the path. May be null.
-     * @return path
      */
     void mkdirs(String path, List<ACL> acls);
 

File: storm-client/src/jvm/org/apache/storm/streams/Pair.java
Patch:
@@ -16,9 +16,6 @@
 
 /**
  * A pair of values.
- *
- * @param <T1> the type of the first value
- * @param <T2> the type of the second value
  */
 public final class Pair<T1, T2> implements Serializable {
     /**

File: storm-client/src/jvm/org/apache/storm/topology/TopologyBuilder.java
Patch:
@@ -330,8 +330,7 @@ public <T extends State> BoltDeclarer setBolt(String id, IStatefulWindowedBolt<T
      *                         outputs.
      * @param bolt             the stateful windowed bolt
      * @param parallelismHint the number of tasks that should be assigned to execute this bolt. Each task will run on a thread in a process
-     *                         somwehere around the cluster.
-     * @param <T>              the type of the state (e.g. {@link org.apache.storm.state.KeyValueState})
+     *                         somewhere around the cluster.
      * @return use the returned object to declare the inputs to this component
      *
      * @throws IllegalArgumentException if {@code parallelism_hint} is not positive

File: storm-client/src/jvm/org/apache/storm/trident/Stream.java
Patch:
@@ -529,7 +529,6 @@ public Stream minBy(String inputFieldName) {
      *
      * @param inputFieldName input field name
      * @param comparator     comparator used in for finding minimum of two tuple values of {@code inputFieldName}.
-     * @param <T>            type of tuple's given input field value.
      * @return the new stream with this operation.
      */
     public <T> Stream minBy(String inputFieldName, Comparator<T> comparator) {
@@ -569,7 +568,6 @@ public Stream maxBy(String inputFieldName) {
      *
      * @param inputFieldName input field name
      * @param comparator     comparator used in for finding maximum of two tuple values of {@code inputFieldName}.
-     * @param <T>            type of tuple's given input field value.
      * @return the new stream with this operation.
      */
     public <T> Stream maxBy(String inputFieldName, Comparator<T> comparator) {

File: storm-client/src/jvm/org/apache/storm/trident/operation/ITridentResource.java
Patch:
@@ -18,8 +18,7 @@
 
 /**
  * This interface is implemented by various Trident classes in order to gather and propogate resources that have been set on them.
- *
- * @see ResourceDeclarer
+ * See {@link org.apache.storm.topology.ResourceDeclarer}
  */
 public interface ITridentResource {
     /**

File: storm-client/src/jvm/org/apache/storm/utils/DefaultShellLogHandler.java
Patch:
@@ -45,7 +45,6 @@ private Logger getLogger(final Class<?> ownerCls) {
      * @param ownerCls - the class which instantiated this ShellLogHandler.
      * @param process  - the current {@link ShellProcess}.
      * @param context  - the current {@link TopologyContext}.
-     * @see {@link ShellLogHandler#setUpContext}
      */
     @Override
     public void setUpContext(final Class<?> ownerCls, final ShellProcess process,
@@ -60,7 +59,6 @@ public void setUpContext(final Class<?> ownerCls, final ShellProcess process,
      * Log the given message.
      *
      * @param shellMsg - the {@link ShellMsg} to log.
-     * @see {@link ShellLogHandler#log}
      */
     @Override
     public void log(final ShellMsg shellMsg) {

File: storm-client/src/jvm/org/apache/storm/utils/Utils.java
Patch:
@@ -855,7 +855,6 @@ public static String serializeToString(Object obj) {
      *
      * @param str   the encoded string.
      * @param clazz the thrift class we are expecting.
-     * @param <T>   The type of clazz
      * @return the decoded object
      */
     public static <T> T deserializeFromString(String str, Class<T> clazz) {

File: storm-client/src/jvm/org/apache/storm/utils/VersionedStore.java
Patch:
@@ -28,7 +28,7 @@ public class VersionedStore {
     /**
      * Creates a store at the given path.
      *
-     * @param The path for the store
+     * @param path The path for the store
      * @param createRootDir option to create the path directory
      */
     public VersionedStore(String path, boolean createRootDir) throws IOException {

File: storm-server/src/main/java/org/apache/storm/nimbus/LeaderListenerCallback.java
Patch:
@@ -69,6 +69,8 @@ public class LeaderListenerCallback {
     private final int requeueDelayMs;
 
     /**
+     * Creates a {@link LeaderListenerCallback}.
+     *
      * @param conf config
      * @param zk zookeeper CuratorFramework client
      * @param blobStore BlobStore

File: storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/sorter/NodeSorter.java
Patch:
@@ -160,7 +160,8 @@ protected List<ObjectResourcesItem> sortObjectResources(
      *     above will be ranked after racks that have more balanced resource availability and nodes or racks that have
      *     resources that are not requested will be ranked below . So we will be less likely to pick a rack that
      *     have a lot of one resource but a low amount of another and have a lot of resources that are not requested by the executor.
-     *     This is similar to logic used in {@code #sortObjectResourcesGeneric(ObjectResourcesSummary, ExecutorDetails, ExistingScheduleFunc)}.
+     *     This is similar to logic used in
+     *     {@link NodeSorter#sortObjectResourcesGeneric(ObjectResourcesSummary, ExecutorDetails, ExistingScheduleFunc)}.
      * </li>
      *
      * <li>

File: storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/sorter/NodeSorterHostProximity.java
Patch:
@@ -169,13 +169,14 @@ protected Iterable<ObjectResourcesItem> sortObjectResources(
      *     above will be ranked after racks that have more balanced resource availability and nodes or racks that have
      *     resources that are not requested will be ranked below . So we will be less likely to pick a rack that
      *     have a lot of one resource but a low amount of another and have a lot of resources that are not requested by the executor.
-     *     This is similar to logic used in {@code #sortObjectResourcesGeneric(ObjectResourcesSummary, ExecutorDetails, ExistingScheduleFunc)}.
+     *     This is similar to logic used in
+     *     {@link NodeSorterHostProximity#sortObjectResourcesGeneric(ObjectResourcesSummary, ExecutorDetails, ExistingScheduleFunc)}.
      * </li>
      *
      * <li>
      *     The tie between two nodes with same resource availability is broken by using the node with lower minimum
      *     percentage used. This comparison was used in {@link #sortObjectResourcesDefault(ObjectResourcesSummary, ExistingScheduleFunc)}
-     *     but here it is made subservient to modified resource availbility used in
+     *     but here it is made subservient to modified resource availability used in
      *     {@code #sortObjectResourcesGeneric(ObjectResourcesSummary, ExecutorDetails, ExistingScheduleFunc)}.
      *
      * </li>

File: storm-server/src/main/java/org/apache/storm/metric/timed/TimerDecorated.java
Patch:
@@ -21,7 +21,7 @@ public interface TimerDecorated extends AutoCloseable {
     long stopTiming();
 
     /**
-     * Stop the timer for measured object. (Copied from {@link Timer.Context#stop()})
+     * Stop the timer for measured object.
      * Call to this method will not reset the start time.
      * Multiple calls result in multiple updates.
      *

File: storm-server/src/main/java/org/apache/storm/nimbus/AssignmentDistributionService.java
Patch:
@@ -107,7 +107,6 @@ public static AssignmentDistributionService getInstance(Map conf, INodeAssignmen
      * Function for initialization.
      *
      * @param conf config
-     * @param callback callback for sendAssignment results
      */
     public void prepare(Map conf, INodeAssignmentSentCallBack callBack) {
         this.conf = conf;

File: storm-server/src/main/java/org/apache/storm/nimbus/LeaderListenerCallback.java
Patch:
@@ -69,7 +69,6 @@ public class LeaderListenerCallback {
     private final int requeueDelayMs;
 
     /**
-     * Constructor for {@LeaderListenerCallback}.
      * @param conf config
      * @param zk zookeeper CuratorFramework client
      * @param blobStore BlobStore

File: storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/BaseResourceAwareStrategy.java
Patch:
@@ -75,7 +75,7 @@ public enum NodeSortType {
 
         /**
          * New and only node sorting type going forward.
-         * {@link NodeSorterHostProximity#NodeSorterHostProximity(Cluster, TopologyDetails)} for more details
+         * See {@link NodeSorterHostProximity} for more details.
          */
         COMMON,
     }

File: storm-server/src/main/java/org/apache/storm/utils/EnumUtil.java
Patch:
@@ -26,8 +26,6 @@ public static String toMetricName(Enum type) {
      * Create an Enum map with given lambda mapper.
      * @param klass the Enum class
      * @param mapper The mapper producing value with key (enum constant)
-     * @param <T> An Enum class
-     * @param <U> Mapped class
      * @return An Enum map
      */
     public static <T extends Enum<T>, U> EnumMap<T, U> toEnumMap(Class<T> klass, Function<? super T, ? extends U> mapper) {

File: storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/handler/LogviewerLogDownloadHandler.java
Patch:
@@ -47,13 +47,13 @@ public LogviewerLogDownloadHandler(String logRoot, String daemonLogRoot, WorkerL
     }
 
     /**
-     * Download an worker log.
+     * Download a worker log.
      *
      * @param host host address
      * @param fileName file to download
      * @param user username
      * @return a Response which lets browsers download that file.
-     * @see {@link LogFileDownloader#downloadFile(String, String, String, boolean)}
+     *
      */
     public Response downloadLogFile(String host, String fileName, String user) throws IOException {
         workerLogs.setLogFilePermission(fileName);
@@ -67,7 +67,6 @@ public Response downloadLogFile(String host, String fileName, String user) throw
      * @param fileName file to download
      * @param user username
      * @return a Response which lets browsers download that file.
-     * @see {@link LogFileDownloader#downloadFile(String, String, String, boolean)}
      */
     public Response downloadDaemonLogFile(String host, String fileName, String user) throws IOException {
         return logFileDownloadHelper.downloadFile(host, fileName, user, true);

File: storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/handler/LogviewerProfileHandler.java
Patch:
@@ -107,7 +107,6 @@ public Response listDumpFiles(String topologyId, String hostPort, String user) t
      * @param fileName dump file name
      * @param user username
      * @return a Response which lets browsers download that file.
-     * @see {@link org.apache.storm.daemon.logviewer.utils.LogFileDownloader#downloadFile(String, String, String, boolean)}
      */
     public Response downloadDumpFile(String topologyId, String hostPort, String fileName, String user) throws IOException {
         String[] hostPortSplit = hostPort.split(":");

File: storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/utils/LogviewerResponseBuilder.java
Patch:
@@ -65,7 +65,6 @@ public static Response buildSuccessHtmlResponse(String content) {
      * @param entity entity object to represent it as JSON
      * @param callback callbackParameterName for JSONP
      * @param origin origin
-     * @see {@link JsonResponseBuilder}
      */
     public static Response buildSuccessJsonResponse(Object entity, String callback, String origin) {
         return new JsonResponseBuilder().setData(entity).setCallback(callback)

File: storm-webapp/src/main/java/org/apache/storm/daemon/utils/ListFunctionalSupport.java
Patch:
@@ -97,7 +97,6 @@ public static <T> List<T> drop(List<T> list, int count) {
     /**
      * Drop the only first element and create a new list. equivalent to drop(list, 1).
      *
-     * @see {@link ListFunctionalSupport#drop(List, int)}
      * @param list the list
      * @return newly created sublist that drops the first element from origin list. null if list is null.
      */

File: storm-webapp/src/main/java/org/apache/storm/daemon/utils/StreamUtil.java
Patch:
@@ -33,7 +33,6 @@ private StreamUtil() {
      * <p/>
      * FileInputStream#skip may not work the first time, so ensure it successfully skips the given number of bytes.
      *
-     * @see {@link java.io.FileInputStream#skip(long)}
      * @param stream the stream to skip
      * @param n bytes to skip
      */

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/context/WorkerCtx.java
Patch:
@@ -66,9 +66,9 @@ public <T, K, V> T getWorkerBean(Class<T> clazz, Map<K, V> topoConf) {
      *
      * @param clazz the class of the bean.
      * @param topoConf the storm configuration
-     * @param force if <code>true</code>= create a new instance even if one already exist.
+     * @param force if {@code true}, create a new instance even if one already exist.
      *
-     * @return a instance of type {@link T}.
+     * @return an instance of the given type {@code T}.
      */
     public <T, K, V> T getWorkerBean(Class<T> clazz, Map<K, V> topoConf, boolean force) {
         if (force) {

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/bolt/KafkaBolt.java
Patch:
@@ -66,11 +66,11 @@ public class KafkaBolt<K, V> extends BaseTickTupleAwareRichBolt {
     private PreparableCallback providedCallback;
     private Properties boltSpecifiedProperties = new Properties();
     /**
-     * {@see KafkaBolt#setFireAndForget(boolean)} for more details on this. 
+     * {@link KafkaBolt#setFireAndForget(boolean)} for more details on this.
      */
     private boolean fireAndForget = false;
     /**
-     * {@see KafkaBolt#setAsync(boolean)} for more details on this. 
+     * {@link KafkaBolt#setAsync(boolean)} for more details on this.
      */
     private boolean async = true;
 

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaTuple.java
Patch:
@@ -22,7 +22,7 @@
 
 /**
  * A list of Values in a tuple that can be routed 
- * to a given stream. {@see org.apache.storm.kafka.spout.RecordTranslator#apply}
+ * to a given stream: {@link org.apache.storm.kafka.spout.RecordTranslator#apply}.
  */
 public class KafkaTuple extends Values {
     private static final long serialVersionUID = 4803794470450587992L;

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/subscription/TopicAssigner.java
Patch:
@@ -31,8 +31,6 @@ public class TopicAssigner implements Serializable {
     
     /**
      * Assign partitions to the KafkaConsumer.
-     * @param <K> The consumer key type
-     * @param <V> The consumer value type
      * @param consumer The Kafka consumer to assign partitions to
      * @param newAssignment The partitions to assign.
      * @param listener The rebalance listener to call back on when the assignment changes

File: examples/storm-starter/src/jvm/org/apache/storm/starter/SlidingWindowTopology.java
Patch:
@@ -58,8 +58,8 @@ public static void main(String[] args) throws Exception {
         StormSubmitter.submitTopologyWithProgressBar(topoName, conf, builder.createTopology());
     }
 
-    /*
-     * Computes tumbling window average
+    /**
+     * Computes tumbling window average.
      */
     private static class TumblingWindowAvgBolt extends BaseWindowedBolt {
         private OutputCollector collector;

File: examples/storm-starter/src/jvm/org/apache/storm/starter/streams/TypedTupleExample.java
Patch:
@@ -25,7 +25,7 @@
 import org.apache.storm.topology.base.BaseWindowedBolt.Count;
 
 /**
- * An example that illustrates the usage of typed tuples (TupleN<..>) and {@link TupleValueMappers}.
+ * An example that illustrates the usage of typed tuples (TupleN&lt;..&gt;) and {@link TupleValueMappers}.
  */
 public class TypedTupleExample {
 

File: examples/storm-starter/src/jvm/org/apache/storm/starter/tools/SlidingWindowCounter.java
Patch:
@@ -12,14 +12,16 @@
 
 package org.apache.storm.starter.tools;
 
+import org.apache.storm.starter.bolt.RollingCountBolt;
+
 import java.io.Serializable;
 import java.util.Map;
 
 /**
  * This class counts objects in a sliding window fashion.
  * <p/>
  * It is designed 1) to give multiple "producer" threads write access to the counter, i.e. being able to increment
- * counts of objects, and 2) to give a single "consumer" thread (e.g. {@link PeriodicSlidingWindowCounter}) read access
+ * counts of objects, and 2) to give a single "consumer" thread (e.g. {@link RollingCountBolt}) read access
  * to the counter. Whenever the consumer thread performs a read operation, this class will advance the head slot of the
  * sliding window counter. This means that the consumer thread indirectly controls where writes of the producer threads
  * will go to. Also, by itself this class will not advance the head slot.

File: storm-client/src/jvm/org/apache/storm/metric/cgroup/CGroupCpu.java
Patch:
@@ -15,6 +15,7 @@
 import java.io.BufferedReader;
 import java.io.IOException;
 import java.io.InputStreamReader;
+import java.nio.charset.StandardCharsets;
 import java.util.HashMap;
 import java.util.Map;
 import org.apache.storm.container.cgroup.SubSystemType;
@@ -40,7 +41,7 @@ public synchronized int getUserHZ() throws IOException {
         if (userHz < 0) {
             ProcessBuilder pb = new ProcessBuilder("getconf", "CLK_TCK");
             Process p = pb.start();
-            BufferedReader in = new BufferedReader(new InputStreamReader(p.getInputStream()));
+            BufferedReader in = new BufferedReader(new InputStreamReader(p.getInputStream(), StandardCharsets.UTF_8));
             String line = in.readLine().trim();
             userHz = Integer.valueOf(line);
         }

File: storm-server/src/main/java/org/apache/storm/localizer/LocalizedResource.java
Patch:
@@ -141,7 +141,7 @@ static Collection<String> getLocalizedUsers(Path localBaseDir) throws IOExceptio
         if (!Files.exists(userCacheDir)) {
             return Collections.emptyList();
         }
-        try (Stream<Path> stream = Files.list(userCacheDir)){
+        try (Stream<Path> stream = Files.list(userCacheDir)) {
             return stream.map((p) -> p.getFileName().toString()).collect(Collectors.toList());
         }
     }

File: storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/utils/LogCleaner.java
Patch:
@@ -115,8 +115,8 @@ public LogCleaner(Map<String, Object> stormConf, WorkerLogs workerLogs, Director
     }
     
     private long sizeOfDir(Path dir) {
-        try {
-            return Files.walk(dir)
+        try (Stream<Path> stream = Files.walk(dir)) {
+            return stream
                 .filter(Files::isRegularFile)
                 .mapToLong(p -> p.toFile().length())
                 .sum();

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/avro/AbstractAvroSerializer.java
Patch:
@@ -57,7 +57,7 @@ public void write(Kryo kryo, Output output, GenericContainer record) {
     }
 
     @Override
-    public GenericContainer read(Kryo kryo, Input input, Class<GenericContainer> someClass) {
+    public GenericContainer read(Kryo kryo, Input input, Class<? extends GenericContainer> someClass) {
         Schema theSchema = this.getSchema(input.readString());
         GenericDatumReader<GenericContainer> reader = new GenericDatumReader<>(theSchema);
         Decoder decoder = DecoderFactory

File: external/storm-kinesis/src/main/java/org/apache/storm/kinesis/spout/KinesisConnectionInfo.java
Patch:
@@ -88,6 +88,7 @@ public Regions getRegion() {
 
     private byte[] getKryoSerializedBytes(final Object obj) {
         final Kryo kryo = new Kryo();
+        kryo.setRegistrationRequired(false);
         final ByteArrayOutputStream os = new ByteArrayOutputStream();
         final Output output = new Output(os);
         kryo.setInstantiatorStrategy(new StdInstantiatorStrategy());
@@ -98,6 +99,7 @@ private byte[] getKryoSerializedBytes(final Object obj) {
 
     private Object getKryoDeserializedObject(final byte[] ser) {
         final Kryo kryo = new Kryo();
+        kryo.setRegistrationRequired(false);
         final Input input = new Input(new ByteArrayInputStream(ser));
         kryo.setInstantiatorStrategy(new StdInstantiatorStrategy());
         return kryo.readClassAndObject(input);

File: storm-client/src/jvm/org/apache/storm/security/serialization/BlowfishTupleSerializer.java
Patch:
@@ -44,7 +44,7 @@ public class BlowfishTupleSerializer extends Serializer<ListDelegate> {
     private static final Logger LOG = LoggerFactory.getLogger(BlowfishTupleSerializer.class);
     private BlowfishSerializer serializer;
 
-    public BlowfishTupleSerializer(Kryo kryo, Map<String, Object> topoConf) {
+    public BlowfishTupleSerializer(Kryo unused, Map<String, Object> topoConf) {
         String encryptionkey;
         try {
             encryptionkey = (String) topoConf.get(SECRET_KEY);
@@ -94,7 +94,7 @@ public void write(Kryo kryo, Output output, ListDelegate object) {
     }
 
     @Override
-    public ListDelegate read(Kryo kryo, Input input, Class<ListDelegate> type) {
+    public ListDelegate read(Kryo kryo, Input input, Class<? extends ListDelegate> type) {
         return kryo.readObject(input, ListDelegate.class, serializer);
     }
 }

File: storm-client/src/jvm/org/apache/storm/serialization/KryoTupleSerializer.java
Patch:
@@ -33,7 +33,7 @@ public KryoTupleSerializer(final Map<String, Object> conf, final GeneralTopology
     public byte[] serialize(Tuple tuple) {
         try {
 
-            kryoOut.clear();
+            kryoOut.reset();
             kryoOut.writeInt(tuple.getSourceTask(), true);
             kryoOut.writeInt(ids.getStreamId(tuple.getSourceComponent(), tuple.getSourceStreamId()), true);
             tuple.getMessageId().serialize(kryoOut);

File: storm-client/src/jvm/org/apache/storm/serialization/KryoValuesSerializer.java
Patch:
@@ -39,13 +39,13 @@ public void serializeInto(List<Object> values, Output out) {
     }
 
     public byte[] serialize(List<Object> values) {
-        kryoOut.clear();
+        kryoOut.reset();
         serializeInto(values, kryoOut);
         return kryoOut.toBytes();
     }
 
     public byte[] serializeObject(Object obj) {
-        kryoOut.clear();
+        kryoOut.reset();
         kryo.writeClassAndObject(kryoOut, obj);
         return kryoOut.toBytes();
     }

File: storm-client/src/jvm/org/apache/storm/serialization/types/ArrayListSerializer.java
Patch:
@@ -19,9 +19,9 @@
 import java.util.Collection;
 
 
-public class ArrayListSerializer extends CollectionSerializer {
+public class ArrayListSerializer extends CollectionSerializer<ArrayList> {
     @Override
-    public Collection create(Kryo kryo, Input input, Class<Collection> type) {
-        return new ArrayList();
+    public ArrayList create(Kryo kryo, Input input, Class<? extends ArrayList> type, int length) {
+        return new ArrayList(length);
     }
 }

File: storm-client/src/jvm/org/apache/storm/serialization/types/HashMapSerializer.java
Patch:
@@ -19,9 +19,9 @@
 import java.util.Map;
 
 
-public class HashMapSerializer extends MapSerializer {
+public class HashMapSerializer extends MapSerializer<HashMap> {
     @Override
-    public Map<String, Object> create(Kryo kryo, Input input, Class<Map> type) {
-        return new HashMap<>();
+    protected HashMap<String, Object> create(Kryo kryo, Input input, Class<? extends HashMap> type, int length) {
+        return new HashMap<>(length);
     }
 }

File: storm-client/src/jvm/org/apache/storm/serialization/types/HashSetSerializer.java
Patch:
@@ -19,9 +19,9 @@
 import java.util.HashSet;
 
 
-public class HashSetSerializer extends CollectionSerializer {
+public class HashSetSerializer extends CollectionSerializer<HashSet> {
     @Override
-    public Collection create(Kryo kryo, Input input, Class<Collection> type) {
-        return new HashSet();
+    protected HashSet create(Kryo kryo, Input input, Class<? extends HashSet> type, int length) {
+        return new HashSet(length);
     }
 }

File: storm-client/src/jvm/org/apache/storm/serialization/types/ListDelegateSerializer.java
Patch:
@@ -19,9 +19,9 @@
 import org.apache.storm.utils.ListDelegate;
 
 
-public class ListDelegateSerializer extends CollectionSerializer {
+public class ListDelegateSerializer extends CollectionSerializer<ListDelegate> {
     @Override
-    public Collection create(Kryo kryo, Input input, Class<Collection> type) {
+    public ListDelegate create(Kryo kryo, Input input, Class<? extends ListDelegate> type, int length) {
         return new ListDelegate();
     }
 }

File: storm-client/src/jvm/org/apache/storm/trident/windowing/WindowKryoSerializer.java
Patch:
@@ -41,7 +41,7 @@ public WindowKryoSerializer(Map<String, Object> topoConf) {
      * @param obj Object to be serialized.
      */
     public byte[] serialize(Object obj) {
-        output.clear();
+        output.reset();
         kryo.writeClassAndObject(output, obj);
         return output.toBytes();
     }
@@ -52,7 +52,7 @@ public byte[] serialize(Object obj) {
      * @param obj Object to be serialized.
      */
     public ByteBuffer serializeToByteBuffer(Object obj) {
-        output.clear();
+        output.reset();
         kryo.writeClassAndObject(output, obj);
         return ByteBuffer.wrap(output.getBuffer(), 0, output.position());
     }

File: storm-client/test/jvm/org/apache/storm/security/serialization/BlowfishTupleSerializerTest.java
Patch:
@@ -82,8 +82,9 @@ private void testEncryptsAndDecryptsMessage(Map<String, Object> topoConf) {
                           " lower plate, which are used for crushing the shells of crustaceans and" +
                           " mollusks, their natural prey.";
         Kryo kryo = new Kryo();
-        BlowfishTupleSerializer writerBTS = new BlowfishTupleSerializer(kryo, topoConf);
-        BlowfishTupleSerializer readerBTS = new BlowfishTupleSerializer(kryo, topoConf);
+        kryo.setRegistrationRequired(false);
+        BlowfishTupleSerializer writerBTS = new BlowfishTupleSerializer(null, topoConf);
+        BlowfishTupleSerializer readerBTS = new BlowfishTupleSerializer(null, topoConf);
         int bufferSize = 1024;
         Output output = new Output(bufferSize, bufferSize);
         Input input = new Input(bufferSize);

File: storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/utils/DirectoryCleaner.java
Patch:
@@ -173,6 +173,7 @@ public DeletionMeta deleteOldestWhileTooLarge(List<Path> dirs,
                 LOG.warn("No more files eligible to be deleted this round, but {} is over {} quota by {} MB",
                         forPerDir ? "worker directory: " + dirs.get(0).toAbsolutePath().normalize() : "log root directory",
                         forPerDir ? "per-worker" : "global", toDeleteSize * 1e-6);
+                break; // No entries left to delete
             }
         }
         return new DeletionMeta(deletedSize, deletedFiles);

File: storm-core/src/jvm/org/apache/storm/command/BasicDrpcClient.java
Patch:
@@ -27,7 +27,7 @@ public class BasicDrpcClient {
 
     private static void runAndPrint(DRPCClient drpc, String func, String arg) throws Exception {
         String result = drpc.execute(func, arg);
-        System.out.println(String.format("%s \"%s\" => \"%s\"", func, arg, result));
+        System.out.printf("%s \"%s\" => \"%s\"%n", func, arg, result);
     }
 
     /**

File: storm-core/src/jvm/org/apache/storm/command/CLI.java
Patch:
@@ -28,7 +28,7 @@ public class CLI {
     /**
      * Parse function to return an Integer.
      */
-    public static final Parse AS_INT = value -> Integer.valueOf(value);
+    public static final Parse AS_INT = Integer::valueOf;
 
     /**
      * Noop parse function, returns the String.

File: storm-core/src/jvm/org/apache/storm/command/GetErrors.java
Patch:
@@ -22,7 +22,6 @@
 import org.apache.storm.generated.NumErrorsChoice;
 import org.apache.storm.generated.TopologyInfo;
 import org.apache.storm.utils.NimbusClient;
-import org.apache.storm.utils.Utils;
 import org.json.simple.JSONValue;
 
 public class GetErrors {

File: storm-core/src/jvm/org/apache/storm/command/Heartbeats.java
Patch:
@@ -56,7 +56,7 @@ public static void main(String[] args) throws Exception {
         try {
             cluster.close();
         } catch (Exception e) {
-            LOG.info("Caught exception: {} on close.", e);
+            LOG.info("Caught exception: {} on close.", e.getMessage(), e);
         }
 
         // force process to be terminated

File: storm-core/src/jvm/org/apache/storm/command/UploadCredentials.java
Patch:
@@ -21,7 +21,6 @@
 import java.util.Set;
 import org.apache.storm.Config;
 import org.apache.storm.StormSubmitter;
-import org.apache.storm.generated.ClusterSummary;
 import org.apache.storm.generated.Nimbus;
 import org.apache.storm.generated.TopologySummary;
 import org.apache.storm.utils.NimbusClient;

File: storm-client/src/jvm/org/apache/storm/cluster/StormClusterStateImpl.java
Patch:
@@ -95,6 +95,7 @@ public StormClusterStateImpl(IStateStorage stateStorage, ILocalAssignmentsBacken
 
         stateId = this.stateStorage.register(new ZKStateChangedCallback() {
 
+            @Override
             public void changed(Watcher.Event.EventType type, String path) {
                 List<String> toks = tokenizePath(path);
                 int size = toks.size();
@@ -762,6 +763,7 @@ public void reportError(String stormId, String componentId, String node, Long po
         List<String> childrens = stateStorage.get_children(path, false);
 
         Collections.sort(childrens, new Comparator<String>() {
+            @Override
             public int compare(String arg0, String arg1) {
                 return Long.compare(Long.parseLong(arg0.substring(1)), Long.parseLong(arg1.substring(1)));
             }
@@ -797,6 +799,7 @@ public List<ErrorInfo> errors(String stormId, String componentId) {
             }
         }
         Collections.sort(errorInfos, new Comparator<ErrorInfo>() {
+            @Override
             public int compare(ErrorInfo arg0, ErrorInfo arg1) {
                 return Integer.compare(arg1.get_error_time_secs(), arg0.get_error_time_secs());
             }

File: storm-client/src/jvm/org/apache/storm/metrics2/StormMetricRegistry.java
Patch:
@@ -349,10 +349,12 @@ public int getRateCounterUpdateIntervalSeconds() {
         return RATE_COUNTER_UPDATE_INTERVAL_SECONDS;
     }
 
+    @Override
     public MetricRegistry getRegistry() {
         return registry;
     }
 
+    @Override
     public Map<TaskMetricDimensions, TaskMetricRepo> getTaskMetrics() {
         return taskMetrics;
     }

File: storm-client/src/jvm/org/apache/storm/streams/PairStream.java
Patch:
@@ -168,6 +168,7 @@ public PairStream<K, V> peek(Consumer<? super Pair<K, V>> action) {
     /**
      * {@inheritDoc}
      */
+    @Override
     public PairStream<K, V> filter(Predicate<? super Pair<K, V>> predicate) {
         return toPairStream(super.filter(predicate));
     }

File: storm-client/src/jvm/org/apache/storm/topology/BaseConfigurationDeclarer.java
Patch:
@@ -89,6 +89,7 @@ public T setCPULoad(Number amount) {
     }
 
     @SuppressWarnings("unchecked")
+    @Override
     public T addResource(String resourceName, Number resourceValue) {
         Map<String, Double> resourcesMap = (Map<String, Double>) getComponentConfiguration()
                 .computeIfAbsent(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP,

File: storm-client/src/jvm/org/apache/storm/utils/ConfigUtils.java
Patch:
@@ -80,6 +80,7 @@ public static ConfigUtils setInstance(ConfigUtils u) {
 
     public static Map<String, Object> maskPasswords(final Map<String, Object> conf) {
         Maps.EntryTransformer<String, Object, Object> maskPasswords = new Maps.EntryTransformer<String, Object, Object>() {
+            @Override
             public Object transformEntry(String key, Object value) {
                 return passwordConfigKeys.contains(key) ? "*****" : value;
             }

File: storm-client/src/jvm/org/apache/storm/utils/JCQueue.java
Patch:
@@ -254,6 +254,7 @@ private interface Inserter {
     }
 
     public interface Consumer extends MessagePassingQueue.Consumer<Object> {
+        @Override
         void accept(Object event);
 
         void flush() throws InterruptedException;

File: storm-client/src/jvm/org/apache/storm/utils/Utils.java
Patch:
@@ -386,6 +386,7 @@ public static SmartThread asyncLoop(final Callable afn, boolean isDaemon, final
                                         int priority, final boolean isFactory, boolean startImmediately,
                                         String threadName) {
         SmartThread thread = new SmartThread(new Runnable() {
+            @Override
             public void run() {
                 try {
                     final Callable<Long> fn = isFactory ? (Callable<Long>) afn.call() : afn;
@@ -416,6 +417,7 @@ public void run() {
             thread.setUncaughtExceptionHandler(eh);
         } else {
             thread.setUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler() {
+                @Override
                 public void uncaughtException(Thread t, Throwable e) {
                     LOG.error("Async loop died!", e);
                     Utils.exitProcess(1, "Async loop died!");

File: storm-client/src/jvm/org/apache/storm/windowing/persistence/SimpleWindowPartitionCache.java
Patch:
@@ -180,16 +180,19 @@ public static class SimpleWindowPartitionCacheBuilder<K, V> implements WindowPar
         private long maximumSize;
         private RemovalListener<K, V> removalListener;
 
+        @Override
         public SimpleWindowPartitionCacheBuilder<K, V> maximumSize(long size) {
             maximumSize = size;
             return this;
         }
 
+        @Override
         public SimpleWindowPartitionCacheBuilder<K, V> removalListener(RemovalListener<K, V> listener) {
             removalListener = listener;
             return this;
         }
 
+        @Override
         public SimpleWindowPartitionCache<K, V> build(CacheLoader<K, V> loader) {
             return new SimpleWindowPartitionCache<>(maximumSize, removalListener, loader);
         }

File: storm-server/src/main/java/org/apache/storm/LocalCluster.java
Patch:
@@ -1055,6 +1055,7 @@ public void sendSupervisorWorkerHeartbeat(SupervisorWorkerHeartbeat heatbeat) th
 
     }
 
+    @Override
     public void processWorkerMetrics(WorkerMetrics metrics) throws TException {
         getNimbus().processWorkerMetrics(metrics);
     }

File: storm-server/src/main/java/org/apache/storm/daemon/supervisor/Slot.java
Patch:
@@ -1185,6 +1185,7 @@ static class DynamicState {
             this.slotMetrics = slotMetrics;
         }
 
+        @Override
         public String toString() {
             StringBuffer sb = new StringBuffer();
             sb.append(state);

File: storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/sorter/ExecSorterByConnectionCount.java
Patch:
@@ -50,6 +50,7 @@ public ExecSorterByConnectionCount(TopologyDetails topologyDetails) {
      * @param unassignedExecutors an unmodifiable set of executors that need to be scheduled.
      * @return a list of executors in sorted order for scheduling.
      */
+    @Override
     public List<ExecutorDetails> sortExecutors(Set<ExecutorDetails> unassignedExecutors) {
         Map<String, Component> componentMap = topologyDetails.getUserTopolgyComponents(); // excludes system components
         LinkedHashSet<ExecutorDetails> orderedExecutorSet = new LinkedHashSet<>(); // in insert order

File: storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/sorter/ExecSorterByProximity.java
Patch:
@@ -54,6 +54,7 @@ public ExecSorterByProximity(TopologyDetails topologyDetails) {
      * @param unassignedExecutors an unmodifiable set of executors that need to be scheduled.
      * @return a list of executors in sorted order for scheduling.
      */
+    @Override
     public List<ExecutorDetails> sortExecutors(Set<ExecutorDetails> unassignedExecutors) {
         Map<String, Component> componentMap = topologyDetails.getUserTopolgyComponents(); // excludes system components
         LinkedHashSet<ExecutorDetails> orderedExecutorSet = new LinkedHashSet<>(); // in insert order

File: storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/sorter/NodeSorter.java
Patch:
@@ -587,6 +587,7 @@ private Map<String, AtomicInteger> getScheduledExecCntByRackId() {
      *
      * @return a sorted list of racks
      */
+    @Override
     public List<ObjectResourcesItem> getSortedRacks() {
 
         final ObjectResourcesSummary clusterResourcesSummary = createClusterSummarizedResources();

File: storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/sorter/NodeSorterHostProximity.java
Patch:
@@ -674,6 +674,7 @@ public Map<String, AtomicInteger> getScheduledExecCntByRackId() {
      *
      * @return an iterable of sorted racks
      */
+    @Override
     public Iterable<ObjectResourcesItem> getSortedRacks() {
 
         final ObjectResourcesSummary clusterResourcesSummary = createClusterSummarizedResources();

File: storm-server/src/test/java/org/apache/storm/scheduler/resource/strategies/scheduling/TestConstraintSolverStrategy.java
Patch:
@@ -352,6 +352,7 @@ public void testConstraintSolverForceBacktrackWithSpreadCoLocation() {
         }
 
         ConstraintSolverStrategy cs = new ConstraintSolverStrategy() {
+            @Override
             protected void prepareForScheduling(Cluster cluster, TopologyDetails topologyDetails) {
                 super.prepareForScheduling(cluster, topologyDetails);
 

File: examples/storm-starter/src/jvm/org/apache/storm/starter/AnchoredWordCount.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
  * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
  * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
@@ -32,6 +32,7 @@
 
 public class AnchoredWordCount extends ConfigurableTopology {
 
+    @Override
     protected int run(String[] args) throws Exception {
         TopologyBuilder builder = new TopologyBuilder();
 

File: examples/storm-starter/src/jvm/org/apache/storm/starter/ExclamationTopology.java
Patch:
@@ -33,6 +33,7 @@ public static void main(String[] args) throws Exception {
         ConfigurableTopology.start(new ExclamationTopology(), args);
     }
 
+    @Override
     protected int run(String[] args) {
         TopologyBuilder builder = new TopologyBuilder();
 

File: examples/storm-starter/src/jvm/org/apache/storm/starter/LambdaTopology.java
Patch:
@@ -12,7 +12,6 @@
 
 package org.apache.storm.starter;
 
-import java.io.Serializable;
 import java.util.UUID;
 import org.apache.storm.Config;
 import org.apache.storm.topology.ConfigurableTopology;

File: external/storm-hdfs-oci/src/main/java/org/apache/storm/container/oci/HdfsOciResourcesLocalizer.java
Patch:
@@ -47,6 +47,7 @@ public class HdfsOciResourcesLocalizer implements OciResourcesLocalizerInterface
      * @param conf the storm conf.
      * @throws IOException on I/O exception
      */
+    @Override
     public void init(Map<String, Object> conf) throws IOException {
         //login to hdfs
         HadoopLoginUtil.loginHadoop(conf);
@@ -66,6 +67,7 @@ public void init(Map<String, Object> conf) throws IOException {
      * @return the destination of the oci resource
      * @throws IOException on I/O exception
      */
+    @Override
     public synchronized String localize(OciResource ociResource) throws IOException {
         if (ociResource == null) {
             return null;

File: storm-client/src/jvm/org/apache/storm/executor/Executor.java
Patch:
@@ -443,7 +443,8 @@ private void addSnapshotDatapoints(String baseName, Snapshot snapshot, List<IMet
         addConvertedMetric(baseName, ".p999", snapshot.get999thPercentile(), dataPoints, true);
     }
 
-    private void addConvertedMetric(String baseName, String suffix, double value, List<IMetricsConsumer.DataPoint> dataPoints, boolean needConversion) {
+    private void addConvertedMetric(String baseName, String suffix, double value,
+                                    List<IMetricsConsumer.DataPoint> dataPoints, boolean needConversion) {
         IMetricsConsumer.DataPoint dataPoint
             = new IMetricsConsumer.DataPoint(baseName + suffix, needConversion ? convertDuration(value) : value);
         dataPoints.add(dataPoint);

File: storm-client/src/jvm/org/apache/storm/Config.java
Patch:
@@ -582,7 +582,7 @@ public class Config extends HashMap<String, Object> {
     @IsInteger
     public static final String TOPOLOGY_BUILTIN_METRICS_BUCKET_SIZE_SECS = "topology.builtin.metrics.bucket.size.secs";
     /**
-     * Whether or not to use Java serialization in a topology.
+     * Whether or not to use Java serialization in a topology. Default is set false for security reasons.
      */
     @IsBoolean
     public static final String TOPOLOGY_FALL_BACK_ON_JAVA_SERIALIZATION = "topology.fall.back.on.java.serialization";

File: external/storm-hdfs-oci/src/main/java/org/apache/storm/container/oci/HdfsOciResourcesLocalizer.java
Patch:
@@ -89,7 +89,7 @@ public synchronized String localize(OciResource ociResource) throws IOException
             // this allows the operation to be atomic in case the supervisor dies.
             File workingDir = new File(dst.getParent() + "/working");
             if (!workingDir.exists()) {
-                boolean dirCreated = workingDir.mkdir();
+                boolean dirCreated = workingDir.mkdirs();
                 if (!dirCreated) {
                     throw new IOException("Couldn't create the directory: " + workingDir);
                 }

File: storm-client/src/jvm/org/apache/storm/executor/bolt/BoltExecutor.java
Patch:
@@ -99,9 +99,6 @@ public void init(ArrayList<Task> idToTask, int idToTaskBase) throws InterruptedE
             Utils.sleepNoSimulation(100);
         }
 
-        if (!componentId.equals(StormCommon.SYSTEM_STREAM_ID)) { // System bolt doesn't call reportError()
-            this.errorReportingMetrics.registerAll(topoConf, idToTask.get(taskIds.get(0) - idToTaskBase).getUserContext());
-        }
         LOG.info("Preparing bolt {}:{}", componentId, getTaskIds());
         for (Task taskData : idToTask) {
             if (taskData == null) {

File: storm-client/src/jvm/org/apache/storm/executor/bolt/BoltOutputCollectorImpl.java
Patch:
@@ -187,7 +187,7 @@ public void flush() {
 
     @Override
     public void reportError(Throwable error) {
-        executor.getErrorReportingMetrics().incrReportedErrorCount();
+        executor.incrementReportedErrorCount();
         executor.getReportError().report(error);
     }
 

File: storm-client/src/jvm/org/apache/storm/executor/spout/SpoutExecutor.java
Patch:
@@ -123,7 +123,6 @@ public void expire(Long key, TupleInfo tupleInfo) {
             }
         });
 
-        this.errorReportingMetrics.registerAll(topoConf, idToTask.get(taskIds.get(0) - idToTaskBase).getUserContext());
         this.outputCollectors = new ArrayList<>();
         for (int i = 0; i < idToTask.size(); ++i) {
             Task taskData = idToTask.get(i);

File: storm-client/src/jvm/org/apache/storm/executor/spout/SpoutOutputCollectorImpl.java
Patch:
@@ -102,7 +102,7 @@ public long getPendingCount() {
 
     @Override
     public void reportError(Throwable error) {
-        executor.getErrorReportingMetrics().incrReportedErrorCount();
+        executor.incrementReportedErrorCount();
         executor.getReportError().report(error);
     }
 

File: storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/BaseResourceAwareStrategy.java
Patch:
@@ -527,7 +527,7 @@ protected SchedulingResult scheduleExecutorsOnNodes(List<ExecutorDetails> ordere
             if (execIndex == 0) {
                 break;
             } else {
-                searcherState.backtrack(execToComp, nodeForExec[execIndex - 1], workerSlotForExec[execIndex - 1]);
+                searcherState.backtrack(execToComp, nodeForExec, workerSlotForExec);
                 progressIdxForExec[execIndex] = -1;
             }
         }

File: storm-server/src/test/java/org/apache/storm/scheduler/resource/strategies/scheduling/TestLargeCluster.java
Patch:
@@ -354,7 +354,7 @@ private static Map<String, SupervisorDetails> createSupervisors(
         Map<String, SupervisorDetails> retList = new HashMap<>();
         Map<String, AtomicInteger> seenRacks = new HashMap<>();
         byRackId.forEach((rackId, list) -> {
-            int tmpRackSupervisorCnt = list.stream().mapToInt(x -> x.supervisorCnt).sum() - Math.abs(reducedSupervisorsPerRack);
+            int tmpRackSupervisorCnt = list.stream().mapToInt(x -> x.supervisorCnt).sum();
             if (tmpRackSupervisorCnt > Math.abs(reducedSupervisorsPerRack)) {
                 tmpRackSupervisorCnt -= Math.abs(reducedSupervisorsPerRack);
             }

File: storm-client/src/jvm/org/apache/storm/utils/Utils.java
Patch:
@@ -1086,7 +1086,7 @@ public static Double parseJvmHeapMemByChildOpts(List<String> options, Double def
                 }
                 Matcher m = optsPattern.matcher(option);
                 while (m.find()) {
-                    int value = Integer.parseInt(m.group(1));
+                    long value = Long.parseLong(m.group(1));
                     char unitChar = m.group(2).toLowerCase().charAt(0);
                     int unit;
                     switch (unitChar) {

File: storm-client/test/jvm/org/apache/storm/utils/UtilsTest.java
Patch:
@@ -89,7 +89,7 @@ private void doParseJvmHeapMemByChildOptsTest(String message, String opt, double
     }
 
     private void doParseJvmHeapMemByChildOptsTest(String message, List<String> opts, double expected) {
-        Assert.assertEquals(message, Utils.parseJvmHeapMemByChildOpts(opts, 123.0), expected, 0);
+        Assert.assertEquals(message, expected, Utils.parseJvmHeapMemByChildOpts(opts, 123.0), 0);
     }
 
     @Test
@@ -104,13 +104,15 @@ public void parseJvmHeapMemByChildOptsTestM() {
         doParseJvmHeapMemByChildOptsTest("Xmx100M results in 100 MB", "Xmx100m", 100.0);
         doParseJvmHeapMemByChildOptsTest("Xmx100m results in 100 MB", "Xmx100M", 100.0);
         doParseJvmHeapMemByChildOptsTest("-Xmx100M results in 100 MB", "-Xmx100m", 100.0);
+        doParseJvmHeapMemByChildOptsTest("-Xmx2048M results in 2048 MB", "-Xmx2048m", 2048.0);
     }
 
     @Test
     public void parseJvmHeapMemByChildOptsTestG() {
         doParseJvmHeapMemByChildOptsTest("Xmx1g results in 1024 MB", "Xmx1g", 1024.0);
         doParseJvmHeapMemByChildOptsTest("Xmx1G results in 1024 MB", "Xmx1G", 1024.0);
         doParseJvmHeapMemByChildOptsTest("-Xmx1g results in 1024 MB", "-Xmx1g", 1024.0);
+        doParseJvmHeapMemByChildOptsTest("-Xmx2g results in 2048 MB", "-Xmx2g", 2048.0);
     }
 
     @Test

File: storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java
Patch:
@@ -592,7 +592,7 @@ public Nimbus(Map<String, Object> conf, INimbus inimbus, IStormClusterState stor
         }
         if (leaderElector == null) {
             leaderElector = Zookeeper.zkLeaderElector(conf, zkClient, blobStore, topoCache, stormClusterState, getNimbusAcls(conf),
-                metricsRegistry);
+                metricsRegistry, submitLock);
         }
         this.leaderElector = leaderElector;
         this.blobStore.setLeaderElector(this.leaderElector);

File: storm-server/src/main/java/org/apache/storm/stats/StatsUtil.java
Patch:
@@ -660,7 +660,7 @@ private static TopologyPageInfo postAggregateTopoStats(Map task2comp, Map exec2n
             m.remove(EXEC_LAT_TOTAL);
             m.remove(PROC_LAT_TOTAL);
             String id = (String) e.getKey();
-            m.put("last-error", getLastError(clusterState, topologyId, id));
+            m.put(LAST_ERROR, getLastError(clusterState, topologyId, id));
 
             aggBolt2stats.put(id, thriftifyBoltAggStats(m));
         }
@@ -677,7 +677,7 @@ private static TopologyPageInfo postAggregateTopoStats(Map task2comp, Map exec2n
                 m.put(COMP_LATENCY, compLatencyTotal / acked);
             }
             m.remove(COMP_LAT_TOTAL);
-            m.put("last-error", getLastError(clusterState, topologyId, id));
+            m.put(LAST_ERROR, getLastError(clusterState, topologyId, id));
 
             aggSpout2stats.put(id, thriftifySpoutAggStats(m));
         }

File: storm-server/src/main/java/org/apache/storm/scheduler/resource/normalization/NormalizedResourceRequest.java
Patch:
@@ -124,7 +124,6 @@ private static Map<String, Double> getDefaultResources(Map<String, Object> topoC
     private static Map<String, Double> parseResources(String input) {
         Map<String, Double> topologyResources = new HashMap<>();
         JSONParser parser = new JSONParser();
-        LOG.debug("Input to parseResources {}", input);
         try {
             if (input != null) {
                 Object obj = parser.parse(input);

File: storm-server/src/main/java/org/apache/storm/scheduler/resource/normalization/NormalizedResourceRequest.java
Patch:
@@ -124,7 +124,6 @@ private static Map<String, Double> getDefaultResources(Map<String, Object> topoC
     private static Map<String, Double> parseResources(String input) {
         Map<String, Double> topologyResources = new HashMap<>();
         JSONParser parser = new JSONParser();
-        LOG.debug("Input to parseResources {}", input);
         try {
             if (input != null) {
                 Object obj = parser.parse(input);

File: storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java
Patch:
@@ -1642,12 +1642,12 @@ private static void notifySupervisorsAsKilled(IStormClusterState clusterState, A
 
     @VisibleForTesting
     static void validateTopologyWorkerMaxHeapSizeConfigs(
-        Map<String, Object> stormConf, StormTopology topology, double defaultWorkerMaxHeapSizeMb) {
+        Map<String, Object> stormConf, StormTopology topology, double defaultWorkerMaxHeapSizeMb) throws InvalidTopologyException {
         double largestMemReq = getMaxExecutorMemoryUsageForTopo(topology, stormConf);
         double topologyWorkerMaxHeapSize =
             ObjectReader.getDouble(stormConf.get(Config.TOPOLOGY_WORKER_MAX_HEAP_SIZE_MB), defaultWorkerMaxHeapSizeMb);
         if (topologyWorkerMaxHeapSize < largestMemReq) {
-            throw new IllegalArgumentException(
+            throw new InvalidTopologyException(
                 "Topology will not be able to be successfully scheduled: Config "
                 + "TOPOLOGY_WORKER_MAX_HEAP_SIZE_MB="
                 + topologyWorkerMaxHeapSize

File: storm-server/src/test/java/org/apache/storm/daemon/nimbus/NimbusTest.java
Patch:
@@ -25,6 +25,7 @@
 
 import org.apache.storm.Config;
 import org.apache.storm.DaemonConfig;
+import org.apache.storm.generated.InvalidTopologyException;
 import org.apache.storm.generated.StormTopology;
 import org.apache.storm.scheduler.resource.strategies.priority.DefaultSchedulingPriorityStrategy;
 import org.apache.storm.scheduler.resource.strategies.scheduling.DefaultResourceAwareStrategy;
@@ -58,7 +59,7 @@ public void testMemoryLoadLargerThanMaxHeapSize() throws Exception {
         try {
             Nimbus.validateTopologyWorkerMaxHeapSizeConfigs(config1, stormTopology1, 768.0);
             fail("Expected exception not thrown");
-        } catch (IllegalArgumentException e) {
+        } catch (InvalidTopologyException e) {
             //Expected...
         }
     }

File: storm-server/src/main/java/org/apache/storm/scheduler/DefaultScheduler.java
Patch:
@@ -18,6 +18,7 @@
 
 package org.apache.storm.scheduler;
 
+import java.util.Collections;
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.List;
@@ -109,6 +110,6 @@ public void schedule(Topologies topologies, Cluster cluster) {
 
     @Override
     public Map<String, Map<String, Double>> config() {
-        return new HashMap<>();
+        return Collections.emptyMap();
     }
 }

File: storm-server/src/main/java/org/apache/storm/scheduler/EvenScheduler.java
Patch:
@@ -173,7 +173,7 @@ public void schedule(Topologies topologies, Cluster cluster) {
 
     @Override
     public Map<String, Map<String, Double>> config() {
-        return new HashMap<>();
+        return Collections.emptyMap();
     }
 
 }

File: storm-server/src/main/java/org/apache/storm/scheduler/IsolationScheduler.java
Patch:
@@ -53,7 +53,7 @@ public void prepare(Map<String, Object> conf, StormMetricsRegistry metricsRegist
 
     @Override
     public Map<String, Map<String, Double>> config() {
-        return new HashMap<>();
+        return Collections.emptyMap();
     }
 
     // get host -> all assignable worker slots for non-blacklisted machines (assigned or not assigned)

File: storm-server/src/main/java/org/apache/storm/scheduler/utils/ArtifactoryConfigLoader.java
Patch:
@@ -44,6 +44,7 @@
 
 /**
  * A dynamic loader that can load scheduler configurations for user resource guarantees from Artifactory (an artifact repository manager).
+ * This is not thread-safe.
  */
 public class ArtifactoryConfigLoader implements IConfigLoader {
     protected static final String LOCAL_ARTIFACT_DIR = "scheduler_artifacts";

File: storm-server/src/main/java/org/apache/storm/daemon/supervisor/BasicContainer.java
Patch:
@@ -77,7 +77,7 @@ public class BasicContainer extends Container {
     protected final long mediumMemoryGracePeriodMs;
     protected volatile boolean exitedEarly = false;
     protected volatile long memoryLimitMb;
-    protected volatile long memoryLimitExceededStart;
+    protected volatile long memoryLimitExceededStart = -1;
 
     /**
      * Create a new BasicContainer.

File: storm-server/src/main/java/org/apache/storm/daemon/supervisor/BasicContainer.java
Patch:
@@ -77,7 +77,7 @@ public class BasicContainer extends Container {
     protected final long mediumMemoryGracePeriodMs;
     protected volatile boolean exitedEarly = false;
     protected volatile long memoryLimitMb;
-    protected volatile long memoryLimitExceededStart;
+    protected volatile long memoryLimitExceededStart = -1;
 
     /**
      * Create a new BasicContainer.

File: storm-client/src/jvm/org/apache/storm/daemon/worker/WorkerTransfer.java
Patch:
@@ -38,7 +38,7 @@
 import org.slf4j.LoggerFactory;
 
 // Transfers messages destined to other workers
-class WorkerTransfer implements JCQueue.Consumer {
+public class WorkerTransfer implements JCQueue.Consumer {
     static final Logger LOG = LoggerFactory.getLogger(WorkerTransfer.class);
 
     private final TransferDrainer drainer;
@@ -50,7 +50,7 @@ class WorkerTransfer implements JCQueue.Consumer {
 
     private final AtomicBoolean[] remoteBackPressureStatus; // [[remoteTaskId] -> true/false : indicates if remote task is under BP.
 
-    WorkerTransfer(WorkerState workerState, Map<String, Object> topologyConf, int maxTaskIdInTopo) {
+    public WorkerTransfer(WorkerState workerState, Map<String, Object> topologyConf, int maxTaskIdInTopo) {
         this.workerState = workerState;
         this.backPressureWaitStrategy = IWaitStrategy.createBackPressureWaitStrategy(topologyConf);
         this.drainer = new TransferDrainer();

File: storm-client/src/jvm/org/apache/storm/pacemaker/PacemakerClient.java
Patch:
@@ -182,7 +182,7 @@ public HBMessage send(HBMessage m) throws PacemakerConnectionException, Interrup
                     if (retry <= 0) {
                         throw e;
                     }
-                    LOG.error("error attempting to write to a channel {}.", e.getMessage());
+                    LOG.error("Error attempting to write to a channel to host {} - {}", host, e.getMessage());
                 }
                 if (retry <= 0) {
                     throw new PacemakerConnectionException("couldn't get response after " + maxRetries + " attempts.");

File: storm-client/src/jvm/org/apache/storm/pacemaker/PacemakerClient.java
Patch:
@@ -182,7 +182,7 @@ public HBMessage send(HBMessage m) throws PacemakerConnectionException, Interrup
                     if (retry <= 0) {
                         throw e;
                     }
-                    LOG.error("error attempting to write to a channel {}.", e.getMessage());
+                    LOG.error("Error attempting to write to a channel to host {} - {}", host, e.getMessage());
                 }
                 if (retry <= 0) {
                     throw new PacemakerConnectionException("couldn't get response after " + maxRetries + " attempts.");

File: storm-server/src/main/java/org/apache/storm/DaemonConfig.java
Patch:
@@ -485,13 +485,13 @@ public class DaemonConfig implements Validated {
     /**
      * A list of users allowed to view logs via the Log Viewer.
      */
-    @IsStringList
+    @IsStringOrStringList
     public static final String LOGS_USERS = "logs.users";
 
     /**
      * A list of groups allowed to view logs via the Log Viewer.
      */
-    @IsStringList
+    @IsStringOrStringList
     public static final String LOGS_GROUPS = "logs.groups";
 
     /**

File: storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java
Patch:
@@ -3145,8 +3145,7 @@ public void submitTopologyWithOpts(String topoName, String uploadedJarLocation,
             ReqContext req = ReqContext.context();
             Principal principal = req.principal();
             String submitterPrincipal = principal == null ? null : principal.toString();
-            @SuppressWarnings("unchecked")
-            Set<String> topoAcl = new HashSet<>((List<String>) topoConf.getOrDefault(Config.TOPOLOGY_USERS, Collections.emptyList()));
+            Set<String> topoAcl = new HashSet<>(ObjectReader.getStrings(topoConf.get(Config.TOPOLOGY_USERS)));
             topoAcl.add(submitterPrincipal);
             String submitterUser = principalToLocal.toLocal(principal);
             topoAcl.add(submitterUser);

File: storm-server/src/test/java/org/apache/storm/daemon/supervisor/ContainerTest.java
Patch:
@@ -30,6 +30,7 @@
 import org.apache.storm.daemon.supervisor.Container.ContainerType;
 import org.apache.storm.generated.LocalAssignment;
 import org.apache.storm.generated.ProfileRequest;
+import org.apache.storm.utils.ObjectReader;
 import org.junit.Test;
 import org.yaml.snakeyaml.Yaml;
 
@@ -156,11 +157,11 @@ public void testSetup() throws Exception {
         assertEquals(user, result.get(Config.TOPOLOGY_SUBMITTER_USER));
         HashSet<String> allowedUsers = new HashSet<>(topoUsers);
         allowedUsers.addAll(logUsers);
-        assertEquals(allowedUsers, new HashSet<String>((List<String>) result.get(DaemonConfig.LOGS_USERS)));
+        assertEquals(allowedUsers, new HashSet<>(ObjectReader.getStrings(result.get(DaemonConfig.LOGS_USERS))));
 
         HashSet<String> allowedGroups = new HashSet<>(topoGroups);
         allowedGroups.addAll(logGroups);
-        assertEquals(allowedGroups, new HashSet<String>((List<String>) result.get(DaemonConfig.LOGS_GROUPS)));
+        assertEquals(allowedGroups, new HashSet<>(ObjectReader.getStrings(result.get(DaemonConfig.LOGS_GROUPS))));
 
         //Save the current user to help with recovery
         verify(ops).dump(workerUserFile, user);

File: storm-server/src/main/java/org/apache/storm/DaemonConfig.java
Patch:
@@ -485,13 +485,13 @@ public class DaemonConfig implements Validated {
     /**
      * A list of users allowed to view logs via the Log Viewer.
      */
-    @IsStringList
+    @IsStringOrStringList
     public static final String LOGS_USERS = "logs.users";
 
     /**
      * A list of groups allowed to view logs via the Log Viewer.
      */
-    @IsStringList
+    @IsStringOrStringList
     public static final String LOGS_GROUPS = "logs.groups";
 
     /**

File: storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java
Patch:
@@ -3145,8 +3145,7 @@ public void submitTopologyWithOpts(String topoName, String uploadedJarLocation,
             ReqContext req = ReqContext.context();
             Principal principal = req.principal();
             String submitterPrincipal = principal == null ? null : principal.toString();
-            @SuppressWarnings("unchecked")
-            Set<String> topoAcl = new HashSet<>((List<String>) topoConf.getOrDefault(Config.TOPOLOGY_USERS, Collections.emptyList()));
+            Set<String> topoAcl = new HashSet<>(ObjectReader.getStrings(topoConf.get(Config.TOPOLOGY_USERS)));
             topoAcl.add(submitterPrincipal);
             String submitterUser = principalToLocal.toLocal(principal);
             topoAcl.add(submitterUser);

File: storm-server/src/test/java/org/apache/storm/daemon/supervisor/ContainerTest.java
Patch:
@@ -30,6 +30,7 @@
 import org.apache.storm.daemon.supervisor.Container.ContainerType;
 import org.apache.storm.generated.LocalAssignment;
 import org.apache.storm.generated.ProfileRequest;
+import org.apache.storm.utils.ObjectReader;
 import org.junit.Test;
 import org.yaml.snakeyaml.Yaml;
 
@@ -156,11 +157,11 @@ public void testSetup() throws Exception {
         assertEquals(user, result.get(Config.TOPOLOGY_SUBMITTER_USER));
         HashSet<String> allowedUsers = new HashSet<>(topoUsers);
         allowedUsers.addAll(logUsers);
-        assertEquals(allowedUsers, new HashSet<String>((List<String>) result.get(DaemonConfig.LOGS_USERS)));
+        assertEquals(allowedUsers, new HashSet<>(ObjectReader.getStrings(result.get(DaemonConfig.LOGS_USERS))));
 
         HashSet<String> allowedGroups = new HashSet<>(topoGroups);
         allowedGroups.addAll(logGroups);
-        assertEquals(allowedGroups, new HashSet<String>((List<String>) result.get(DaemonConfig.LOGS_GROUPS)));
+        assertEquals(allowedGroups, new HashSet<>(ObjectReader.getStrings(result.get(DaemonConfig.LOGS_GROUPS))));
 
         //Save the current user to help with recovery
         verify(ops).dump(workerUserFile, user);

File: storm-server/src/main/java/org/apache/storm/daemon/supervisor/ReadClusterState.java
Patch:
@@ -247,7 +247,7 @@ protected Map<Integer, LocalAssignment> readMyExecutors(String topoId, String as
         Map<NodeInfo, WorkerResources> nodeInfoWorkerResourcesMap = assignment.get_worker_resources();
         if (nodeInfoWorkerResourcesMap != null) {
             for (Map.Entry<NodeInfo, WorkerResources> entry : nodeInfoWorkerResourcesMap.entrySet()) {
-                if (entry.getKey().get_node().equals(assignmentId)) {
+                if (entry.getKey().get_node().startsWith(assignmentId)) {
                     Set<Long> ports = entry.getKey().get_port();
                     for (Long port : ports) {
                         slotsResources.put(port, entry.getValue());
@@ -267,7 +267,7 @@ protected Map<Integer, LocalAssignment> readMyExecutors(String topoId, String as
         Map<List<Long>, NodeInfo> executorNodePort = assignment.get_executor_node_port();
         if (executorNodePort != null) {
             for (Map.Entry<List<Long>, NodeInfo> entry : executorNodePort.entrySet()) {
-                if (entry.getValue().get_node().equals(assignmentId)) {
+                if (entry.getValue().get_node().startsWith(assignmentId)) {
                     for (Long port : entry.getValue().get_port()) {
                         LocalAssignment localAssignment = portTasks.get(port.intValue());
                         if (localAssignment == null) {

File: storm-server/src/main/java/org/apache/storm/daemon/supervisor/ReadClusterState.java
Patch:
@@ -247,7 +247,7 @@ protected Map<Integer, LocalAssignment> readMyExecutors(String topoId, String as
         Map<NodeInfo, WorkerResources> nodeInfoWorkerResourcesMap = assignment.get_worker_resources();
         if (nodeInfoWorkerResourcesMap != null) {
             for (Map.Entry<NodeInfo, WorkerResources> entry : nodeInfoWorkerResourcesMap.entrySet()) {
-                if (entry.getKey().get_node().equals(assignmentId)) {
+                if (entry.getKey().get_node().startsWith(assignmentId)) {
                     Set<Long> ports = entry.getKey().get_port();
                     for (Long port : ports) {
                         slotsResources.put(port, entry.getValue());
@@ -267,7 +267,7 @@ protected Map<Integer, LocalAssignment> readMyExecutors(String topoId, String as
         Map<List<Long>, NodeInfo> executorNodePort = assignment.get_executor_node_port();
         if (executorNodePort != null) {
             for (Map.Entry<List<Long>, NodeInfo> entry : executorNodePort.entrySet()) {
-                if (entry.getValue().get_node().equals(assignmentId)) {
+                if (entry.getValue().get_node().startsWith(assignmentId)) {
                     for (Long port : entry.getValue().get_port()) {
                         LocalAssignment localAssignment = portTasks.get(port.intValue());
                         if (localAssignment == null) {

File: storm-client/src/jvm/org/apache/storm/StormSubmitter.java
Patch:
@@ -252,8 +252,8 @@ public static void submitTopologyAs(String name, Map<String, Object> topoConf, S
         try {
             String serConf = JSONValue.toJSONString(topoConf);
             try (NimbusClient client = NimbusClient.getConfiguredClientAs(conf, asUser)) {
-                if (isTopologyNameAllowed(name, client)) {
-                    throw new RuntimeException("Topology with name `" + name + "` is either not allowed or it already exists on cluster");
+                if (!isTopologyNameAllowed(name, client)) {
+                    throw new RuntimeException("Topology name " + name + " is either not allowed or it already exists on the cluster");
                 }
 
                 // Dependency uploading only makes sense for distributed mode
@@ -438,7 +438,7 @@ public void onCompleted(String srcFile, String targetFile, long totalBytes) {
 
     private static boolean isTopologyNameAllowed(String name, NimbusClient client) {
         try {
-            return !client.getClient().isTopologyNameAllowed(name);
+            return client.getClient().isTopologyNameAllowed(name);
         } catch (Exception e) {
             throw new RuntimeException(e);
         }

File: storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceAwareScheduler.java
Patch:
@@ -58,6 +58,7 @@ public class ResourceAwareScheduler implements IScheduler {
     private int schedulingTimeoutSeconds;
     private ExecutorService backgroundScheduling;
     private Meter schedulingTimeoutMeter;
+    private Meter internalErrorMeter;
 
     private static void markFailedTopology(User u, Cluster c, TopologyDetails td, String message) {
         markFailedTopology(u, c, td, message, null);
@@ -78,6 +79,7 @@ private static void markFailedTopology(User u, Cluster c, TopologyDetails td, St
     public void prepare(Map<String, Object> conf, StormMetricsRegistry metricsRegistry) {
         this.conf = conf;
         schedulingTimeoutMeter = metricsRegistry.registerMeter("nimbus:num-scheduling-timeouts");
+        internalErrorMeter = metricsRegistry.registerMeter("nimbus:scheduler-internal-errors");
         schedulingPriorityStrategy = ReflectionUtils.newInstance(
             (String) conf.get(DaemonConfig.RESOURCE_AWARE_SCHEDULER_PRIORITY_STRATEGY));
         configLoader = ConfigLoaderFactoryService.createConfigLoader(conf);
@@ -235,6 +237,7 @@ private void scheduleTopology(TopologyDetails td, Cluster cluster, final User to
                     }
                 }
             } catch (Exception ex) {
+                internalErrorMeter.mark();
                 markFailedTopology(topologySubmitter, cluster, td,
                         "Internal Error - Exception thrown when scheduling. Please check logs for details", ex);
                 return;

File: storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceAwareScheduler.java
Patch:
@@ -58,6 +58,7 @@ public class ResourceAwareScheduler implements IScheduler {
     private int schedulingTimeoutSeconds;
     private ExecutorService backgroundScheduling;
     private Meter schedulingTimeoutMeter;
+    private Meter internalErrorMeter;
 
     private static void markFailedTopology(User u, Cluster c, TopologyDetails td, String message) {
         markFailedTopology(u, c, td, message, null);
@@ -78,6 +79,7 @@ private static void markFailedTopology(User u, Cluster c, TopologyDetails td, St
     public void prepare(Map<String, Object> conf, StormMetricsRegistry metricsRegistry) {
         this.conf = conf;
         schedulingTimeoutMeter = metricsRegistry.registerMeter("nimbus:num-scheduling-timeouts");
+        internalErrorMeter = metricsRegistry.registerMeter("nimbus:scheduler-internal-errors");
         schedulingPriorityStrategy = ReflectionUtils.newInstance(
             (String) conf.get(DaemonConfig.RESOURCE_AWARE_SCHEDULER_PRIORITY_STRATEGY));
         configLoader = ConfigLoaderFactoryService.createConfigLoader(conf);
@@ -235,6 +237,7 @@ private void scheduleTopology(TopologyDetails td, Cluster cluster, final User to
                     }
                 }
             } catch (Exception ex) {
+                internalErrorMeter.mark();
                 markFailedTopology(topologySubmitter, cluster, td,
                         "Internal Error - Exception thrown when scheduling. Please check logs for details", ex);
                 return;

File: storm-client/src/jvm/org/apache/storm/executor/bolt/BoltExecutor.java
Patch:
@@ -28,6 +28,7 @@
 import org.apache.storm.daemon.metrics.BuiltinMetricsUtil;
 import org.apache.storm.daemon.worker.WorkerState;
 import org.apache.storm.executor.Executor;
+import org.apache.storm.generated.Credentials;
 import org.apache.storm.generated.NodeInfo;
 import org.apache.storm.hooks.info.BoltExecuteInfo;
 import org.apache.storm.messaging.IConnection;
@@ -218,7 +219,8 @@ public void tupleActionFn(int taskId, TupleImpl tuple) throws Exception {
         } else if (Constants.CREDENTIALS_CHANGED_STREAM_ID.equals(streamId)) {
             Object taskObject = idToTask.get(taskId - idToTaskBase).getTaskObject();
             if (taskObject instanceof ICredentialsListener) {
-                ((ICredentialsListener) taskObject).setCredentials((Map<String, String>) tuple.getValue(0));
+                Credentials creds = (Credentials) tuple.getValue(0);
+                ((ICredentialsListener) taskObject).setCredentials(creds == null ? null : creds.get_creds());
             }
         } else {
             IBolt boltObject = (IBolt) idToTask.get(taskId - idToTaskBase).getTaskObject();

File: storm-client/src/jvm/org/apache/storm/executor/spout/SpoutExecutor.java
Patch:
@@ -30,6 +30,7 @@
 import org.apache.storm.daemon.worker.WorkerState;
 import org.apache.storm.executor.Executor;
 import org.apache.storm.executor.TupleInfo;
+import org.apache.storm.generated.Credentials;
 import org.apache.storm.hooks.info.SpoutAckInfo;
 import org.apache.storm.hooks.info.SpoutFailInfo;
 import org.apache.storm.policy.IWaitStrategy;
@@ -300,7 +301,8 @@ public void tupleActionFn(int taskId, TupleImpl tuple) throws Exception {
         } else if (streamId.equals(Constants.CREDENTIALS_CHANGED_STREAM_ID)) {
             Object spoutObj = idToTask.get(taskId - idToTaskBase).getTaskObject();
             if (spoutObj instanceof ICredentialsListener) {
-                ((ICredentialsListener) spoutObj).setCredentials((Map<String, String>) tuple.getValue(0));
+                Credentials creds = (Credentials) tuple.getValue(0);
+                ((ICredentialsListener) spoutObj).setCredentials(creds == null ? null : creds.get_creds());
             }
         } else if (streamId.equals(Acker.ACKER_RESET_TIMEOUT_STREAM_ID)) {
             Long id = (Long) tuple.getValue(0);

File: storm-client/src/jvm/org/apache/storm/executor/bolt/BoltExecutor.java
Patch:
@@ -28,6 +28,7 @@
 import org.apache.storm.daemon.metrics.BuiltinMetricsUtil;
 import org.apache.storm.daemon.worker.WorkerState;
 import org.apache.storm.executor.Executor;
+import org.apache.storm.generated.Credentials;
 import org.apache.storm.generated.NodeInfo;
 import org.apache.storm.hooks.info.BoltExecuteInfo;
 import org.apache.storm.messaging.IConnection;
@@ -218,7 +219,8 @@ public void tupleActionFn(int taskId, TupleImpl tuple) throws Exception {
         } else if (Constants.CREDENTIALS_CHANGED_STREAM_ID.equals(streamId)) {
             Object taskObject = idToTask.get(taskId - idToTaskBase).getTaskObject();
             if (taskObject instanceof ICredentialsListener) {
-                ((ICredentialsListener) taskObject).setCredentials((Map<String, String>) tuple.getValue(0));
+                Credentials creds = (Credentials) tuple.getValue(0);
+                ((ICredentialsListener) taskObject).setCredentials(creds == null ? null : creds.get_creds());
             }
         } else {
             IBolt boltObject = (IBolt) idToTask.get(taskId - idToTaskBase).getTaskObject();

File: storm-client/src/jvm/org/apache/storm/executor/spout/SpoutExecutor.java
Patch:
@@ -30,6 +30,7 @@
 import org.apache.storm.daemon.worker.WorkerState;
 import org.apache.storm.executor.Executor;
 import org.apache.storm.executor.TupleInfo;
+import org.apache.storm.generated.Credentials;
 import org.apache.storm.hooks.info.SpoutAckInfo;
 import org.apache.storm.hooks.info.SpoutFailInfo;
 import org.apache.storm.policy.IWaitStrategy;
@@ -300,7 +301,8 @@ public void tupleActionFn(int taskId, TupleImpl tuple) throws Exception {
         } else if (streamId.equals(Constants.CREDENTIALS_CHANGED_STREAM_ID)) {
             Object spoutObj = idToTask.get(taskId - idToTaskBase).getTaskObject();
             if (spoutObj instanceof ICredentialsListener) {
-                ((ICredentialsListener) spoutObj).setCredentials((Map<String, String>) tuple.getValue(0));
+                Credentials creds = (Credentials) tuple.getValue(0);
+                ((ICredentialsListener) spoutObj).setCredentials(creds == null ? null : creds.get_creds());
             }
         } else if (streamId.equals(Acker.ACKER_RESET_TIMEOUT_STREAM_ID)) {
             Long id = (Long) tuple.getValue(0);

File: storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/DefaultResourceAwareStrategy.java
Patch:
@@ -79,7 +79,8 @@ public SchedulingResult schedule(Cluster cluster, TopologyDetails td) {
         }
 
         executorsNotScheduled.removeAll(scheduledTasks);
-        LOG.debug("/* Scheduling left over task (most likely sys tasks) */");
+        LOG.debug("Scheduling left over tasks {} (most likely sys tasks) from topology {}",
+                executorsNotScheduled, td.getId());
         // schedule left over system tasks
         for (ExecutorDetails exec : executorsNotScheduled) {
             if (Thread.currentThread().isInterrupted()) {

File: storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/GenericResourceAwareStrategy.java
Patch:
@@ -120,7 +120,8 @@ public SchedulingResult schedule(Cluster cluster, TopologyDetails td) {
 
         executorsNotScheduled.removeAll(scheduledTasks);
         if (!executorsNotScheduled.isEmpty()) {
-            LOG.warn("Scheduling {} left over task (most likely sys tasks)", executorsNotScheduled);
+            LOG.debug("Scheduling left over tasks {} (most likely sys tasks) from topology {}",
+                        executorsNotScheduled, td.getId());
             // schedule left over system tasks
             for (ExecutorDetails exec : executorsNotScheduled) {
                 if (Thread.currentThread().isInterrupted()) {

File: storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/DefaultResourceAwareStrategy.java
Patch:
@@ -79,7 +79,8 @@ public SchedulingResult schedule(Cluster cluster, TopologyDetails td) {
         }
 
         executorsNotScheduled.removeAll(scheduledTasks);
-        LOG.debug("/* Scheduling left over task (most likely sys tasks) */");
+        LOG.debug("Scheduling left over tasks {} (most likely sys tasks) from topology {}",
+                executorsNotScheduled, td.getId());
         // schedule left over system tasks
         for (ExecutorDetails exec : executorsNotScheduled) {
             if (Thread.currentThread().isInterrupted()) {

File: storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/GenericResourceAwareStrategy.java
Patch:
@@ -120,7 +120,8 @@ public SchedulingResult schedule(Cluster cluster, TopologyDetails td) {
 
         executorsNotScheduled.removeAll(scheduledTasks);
         if (!executorsNotScheduled.isEmpty()) {
-            LOG.warn("Scheduling {} left over task (most likely sys tasks)", executorsNotScheduled);
+            LOG.debug("Scheduling left over tasks {} (most likely sys tasks) from topology {}",
+                        executorsNotScheduled, td.getId());
             // schedule left over system tasks
             for (ExecutorDetails exec : executorsNotScheduled) {
                 if (Thread.currentThread().isInterrupted()) {

File: storm-client/src/jvm/org/apache/storm/security/auth/kerberos/AutoTGT.java
Patch:
@@ -172,6 +172,7 @@ public void populateSubject(Subject subject, Map<String, String> credentials) {
 
     @SuppressWarnings("checkstyle:AbbreviationAsWordInName")
     private void populateSubjectWithTGT(Subject subject, Map<String, String> credentials) {
+        LOG.info("Populating TGT from credentials");
         KerberosTicket tgt = getTGT(credentials);
         if (tgt != null) {
             clearCredentials(subject, tgt);

File: storm-client/src/jvm/org/apache/storm/security/auth/kerberos/AutoTGT.java
Patch:
@@ -172,6 +172,7 @@ public void populateSubject(Subject subject, Map<String, String> credentials) {
 
     @SuppressWarnings("checkstyle:AbbreviationAsWordInName")
     private void populateSubjectWithTGT(Subject subject, Map<String, String> credentials) {
+        LOG.info("Populating TGT from credentials");
         KerberosTicket tgt = getTGT(credentials);
         if (tgt != null) {
             clearCredentials(subject, tgt);

File: storm-core/src/jvm/org/apache/storm/command/SetLogLevel.java
Patch:
@@ -92,7 +92,7 @@ public Object parse(String value) {
                 splits = splits[1].split(":");
                 Integer timeout = 0;
                 Level level = Level.valueOf(splits[0]);
-                logLevel.set_reset_log_level(level.toString());
+                logLevel.set_target_log_level(level.toString());
                 if (splits.length > 1) {
                     timeout = Integer.parseInt(splits[1]);
                 }

File: storm-core/test/jvm/org/apache/storm/command/SetLogLevelTest.java
Patch:
@@ -25,11 +25,11 @@ public void testUpdateLogLevelParser() {
         SetLogLevel.LogLevelsParser logLevelsParser = new SetLogLevel.LogLevelsParser(LogLevelAction.UPDATE);
         LogLevel logLevel = ((Map<String, LogLevel>) logLevelsParser.parse("com.foo.one=warn")).get("com.foo.one");
         Assert.assertEquals(0, logLevel.get_reset_log_level_timeout_secs());
-        Assert.assertEquals("WARN", logLevel.get_reset_log_level());
+        Assert.assertEquals("WARN", logLevel.get_target_log_level());
 
         logLevel = ((Map<String, LogLevel>) logLevelsParser.parse("com.foo.two=DEBUG:10")).get("com.foo.two");
         Assert.assertEquals(10, logLevel.get_reset_log_level_timeout_secs());
-        Assert.assertEquals("DEBUG", logLevel.get_reset_log_level());
+        Assert.assertEquals("DEBUG", logLevel.get_target_log_level());
     }
 
     @Test(expected = NumberFormatException.class)

File: storm-core/src/jvm/org/apache/storm/command/SetLogLevel.java
Patch:
@@ -92,7 +92,7 @@ public Object parse(String value) {
                 splits = splits[1].split(":");
                 Integer timeout = 0;
                 Level level = Level.valueOf(splits[0]);
-                logLevel.set_reset_log_level(level.toString());
+                logLevel.set_target_log_level(level.toString());
                 if (splits.length > 1) {
                     timeout = Integer.parseInt(splits[1]);
                 }

File: storm-core/test/jvm/org/apache/storm/command/SetLogLevelTest.java
Patch:
@@ -25,11 +25,11 @@ public void testUpdateLogLevelParser() {
         SetLogLevel.LogLevelsParser logLevelsParser = new SetLogLevel.LogLevelsParser(LogLevelAction.UPDATE);
         LogLevel logLevel = ((Map<String, LogLevel>) logLevelsParser.parse("com.foo.one=warn")).get("com.foo.one");
         Assert.assertEquals(0, logLevel.get_reset_log_level_timeout_secs());
-        Assert.assertEquals("WARN", logLevel.get_reset_log_level());
+        Assert.assertEquals("WARN", logLevel.get_target_log_level());
 
         logLevel = ((Map<String, LogLevel>) logLevelsParser.parse("com.foo.two=DEBUG:10")).get("com.foo.two");
         Assert.assertEquals(10, logLevel.get_reset_log_level_timeout_secs());
-        Assert.assertEquals("DEBUG", logLevel.get_reset_log_level());
+        Assert.assertEquals("DEBUG", logLevel.get_target_log_level());
     }
 
     @Test(expected = NumberFormatException.class)

File: storm-client/src/jvm/org/apache/storm/pacemaker/PacemakerClientHandler.java
Patch:
@@ -36,6 +36,7 @@ public void channelActive(ChannelHandlerContext ctx) {
         Channel channel = ctx.channel();
         LOG.info("Connection established from {} to {}",
                  channel.localAddress(), channel.remoteAddress());
+        client.channelReady(channel);
     }
 
     @Override
@@ -57,7 +58,7 @@ public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) {
         if (cause instanceof ConnectException) {
             LOG.warn("Connection to pacemaker failed. Trying to reconnect {}", cause.getMessage());
         } else {
-            LOG.error("Exception occurred in Pacemaker.", cause);
+            LOG.error("Exception occurred in Pacemaker: " + cause);
         }
         client.reconnect();
     }

File: storm-client/src/jvm/org/apache/storm/pacemaker/codec/ThriftNettyClientCodec.java
Patch:
@@ -72,7 +72,7 @@ protected void initChannel(Channel ch) throws Exception {
                 throw new RuntimeException(e);
             }
         } else {
-            client.channelReady(ch);
+            // no work for AuthMethod.NONE
         }
 
         pipeline.addLast("PacemakerClientHandler", new PacemakerClientHandler(client));

File: storm-client/src/jvm/org/apache/storm/pacemaker/PacemakerClientHandler.java
Patch:
@@ -36,6 +36,7 @@ public void channelActive(ChannelHandlerContext ctx) {
         Channel channel = ctx.channel();
         LOG.info("Connection established from {} to {}",
                  channel.localAddress(), channel.remoteAddress());
+        client.channelReady(channel);
     }
 
     @Override
@@ -57,7 +58,7 @@ public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) {
         if (cause instanceof ConnectException) {
             LOG.warn("Connection to pacemaker failed. Trying to reconnect {}", cause.getMessage());
         } else {
-            LOG.error("Exception occurred in Pacemaker.", cause);
+            LOG.error("Exception occurred in Pacemaker: " + cause);
         }
         client.reconnect();
     }

File: storm-client/src/jvm/org/apache/storm/pacemaker/codec/ThriftNettyClientCodec.java
Patch:
@@ -72,7 +72,7 @@ protected void initChannel(Channel ch) throws Exception {
                 throw new RuntimeException(e);
             }
         } else {
-            client.channelReady(ch);
+            // no work for AuthMethod.NONE
         }
 
         pipeline.addLast("PacemakerClientHandler", new PacemakerClientHandler(client));

File: storm-server/src/main/java/org/apache/storm/daemon/nimbus/TopologyResources.java
Patch:
@@ -94,7 +94,7 @@ private Map<String, Double> computeAssignedGenericResources(Collection<WorkerRes
         for (WorkerResources worker : workers) {
             genericResources = NormalizedResourceRequest.addResourceMap(genericResources, worker.get_resources());
         }
-        NormalizedResourceRequest.filterGenericResources(genericResources);
+        NormalizedResourceRequest.removeNonGenericResources(genericResources);
         return genericResources;
     }
 

File: storm-server/src/main/java/org/apache/storm/scheduler/SupervisorDetails.java
Patch:
@@ -18,7 +18,6 @@
 import java.util.Set;
 import org.apache.storm.scheduler.resource.normalization.NormalizedResourceOffer;
 import org.apache.storm.scheduler.resource.normalization.NormalizedResourceRequest;
-import org.apache.storm.scheduler.resource.normalization.ResourceMetrics;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -159,7 +158,7 @@ public double getTotalCpu() {
      */
     public Map<String, Double> getTotalGenericResources() {
         Map<String, Double> genericResources = totalResources.toNormalizedMap();
-        NormalizedResourceRequest.filterGenericResources(genericResources);
+        NormalizedResourceRequest.removeNonGenericResources(genericResources);
         return genericResources;
     }
 

File: storm-server/src/main/java/org/apache/storm/scheduler/SupervisorResources.java
Patch:
@@ -86,7 +86,7 @@ public Map<String, Double> getUsedGenericResources() {
 
     public SupervisorResources add(WorkerResources wr) {
         usedGenericResources = NormalizedResourceRequest.addResourceMap(usedGenericResources, wr.get_resources());
-        NormalizedResourceRequest.filterGenericResources(usedGenericResources);
+        NormalizedResourceRequest.removeNonGenericResources(usedGenericResources);
 
         return new SupervisorResources(
                 totalMem,

File: storm-server/src/main/java/org/apache/storm/scheduler/TopologyDetails.java
Patch:
@@ -457,7 +457,7 @@ public double getTotalRequestedCpu() {
 
     public Map<String, Double> getTotalRequestedGenericResources() {
         Map<String, Double> map = getApproximateTotalResources().toNormalizedMap();
-        NormalizedResourceRequest.filterGenericResources(map);
+        NormalizedResourceRequest.removeNonGenericResources(map);
         return map;
     }
 

File: storm-server/src/main/java/org/apache/storm/scheduler/resource/normalization/NormalizedResourceRequest.java
Patch:
@@ -179,9 +179,9 @@ public Map<String, Double> toNormalizedMap() {
     }
 
     /*
-     * return map with non generic resources removded
+     * return map with non generic resources removed
      */
-    public static void filterGenericResources(Map<String, Double> map) {
+    public static void removeNonGenericResources(Map<String, Double> map) {
         map.remove(Constants.COMMON_ONHEAP_MEMORY_RESOURCE_NAME);
         map.remove(Constants.COMMON_OFFHEAP_MEMORY_RESOURCE_NAME);
         map.remove(Constants.COMMON_TOTAL_MEMORY_RESOURCE_NAME);

File: storm-webapp/src/main/java/org/apache/storm/daemon/ui/UIHelpers.java
Patch:
@@ -654,7 +654,7 @@ private static String prettifyGenericResources(Map<String, Double> resourceMap)
         }
         TreeMap<String, Double> treeGenericResources = new TreeMap<>(); // use TreeMap for deterministic ordering
         treeGenericResources.putAll(resourceMap);
-        NormalizedResourceRequest.filterGenericResources(treeGenericResources);
+        NormalizedResourceRequest.removeNonGenericResources(treeGenericResources);
         return treeGenericResources.toString()
                 .replaceAll("[{}]", "")
                 .replace(",", "");

File: storm-client/src/jvm/org/apache/storm/topology/WindowedBoltExecutor.java
Patch:
@@ -16,6 +16,7 @@
 import static org.apache.storm.topology.base.BaseWindowedBolt.Duration;
 
 import java.util.Collection;
+import java.util.Collections;
 import java.util.HashSet;
 import java.util.Iterator;
 import java.util.List;
@@ -345,7 +346,7 @@ public void declareOutputFields(OutputFieldsDeclarer declarer) {
 
     @Override
     public Map<String, Object> getComponentConfiguration() {
-        return bolt.getComponentConfiguration();
+        return bolt.getComponentConfiguration() != null ? bolt.getComponentConfiguration() : Collections.emptyMap();
     }
 
     protected WindowLifecycleListener<Tuple> newWindowLifecycleListener() {

File: storm-client/src/jvm/org/apache/storm/topology/WindowedBoltExecutor.java
Patch:
@@ -16,6 +16,7 @@
 import static org.apache.storm.topology.base.BaseWindowedBolt.Duration;
 
 import java.util.Collection;
+import java.util.Collections;
 import java.util.HashSet;
 import java.util.Iterator;
 import java.util.List;
@@ -345,7 +346,7 @@ public void declareOutputFields(OutputFieldsDeclarer declarer) {
 
     @Override
     public Map<String, Object> getComponentConfiguration() {
-        return bolt.getComponentConfiguration();
+        return bolt.getComponentConfiguration() != null ? bolt.getComponentConfiguration() : Collections.emptyMap();
     }
 
     protected WindowLifecycleListener<Tuple> newWindowLifecycleListener() {

File: storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/priority/FIFOSchedulingPriorityStrategy.java
Patch:
@@ -55,8 +55,7 @@ public double getScore(double availableCpu, double availableMemory) {
     }
 
     /**
-     * Comparator that sorts topologies by priority and then by submission time.
-     * First sort by Topology Priority, if there is a tie for topology priority, topology uptime is used to sort.
+     * Comparator that sorts topologies by submission time.
      */
     private static class TopologyBySubmissionTimeComparator implements Comparator<TopologyDetails> {
 

File: storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/priority/FIFOSchedulingPriorityStrategy.java
Patch:
@@ -55,8 +55,7 @@ public double getScore(double availableCpu, double availableMemory) {
     }
 
     /**
-     * Comparator that sorts topologies by priority and then by submission time.
-     * First sort by Topology Priority, if there is a tie for topology priority, topology uptime is used to sort.
+     * Comparator that sorts topologies by submission time.
      */
     private static class TopologyBySubmissionTimeComparator implements Comparator<TopologyDetails> {
 

File: storm-client/src/jvm/org/apache/storm/Config.java
Patch:
@@ -1418,12 +1418,12 @@ public class Config extends HashMap<String, Object> {
     @IsInteger
     public static final String STORM_BLOBSTORE_INPUTSTREAM_BUFFER_SIZE_BYTES = "storm.blobstore.inputstream.buffer.size.bytes";
     /**
-     * What chuck size to use for storm client to upload dependency jars.
+     * What chunk size to use for storm client to upload dependency jars.
      */
     @IsPositiveNumber
     @IsInteger
-    public static final String STORM_BLOBSTORE_DEPENDENCY_JAR_UPLOAD_CHUCK_SIZE_BYTES =
-            "storm.blobstore.dependency.jar.upload.chuck.size.bytes";
+    public static final String STORM_BLOBSTORE_DEPENDENCY_JAR_UPLOAD_CHUNK_SIZE_BYTES =
+            "storm.blobstore.dependency.jar.upload.chunk.size.bytes";
     /**
      * FQCN of a class that implements {@code ISubmitterHook} @see ISubmitterHook for details.
      */

File: storm-client/src/jvm/org/apache/storm/dependency/DependencyUploader.java
Patch:
@@ -48,11 +48,11 @@ public class DependencyUploader {
 
     private final Map<String, Object> conf;
     private ClientBlobStore blobStore;
-    private int uploadChuckSize;
+    private final int uploadChunkSize;
 
     public DependencyUploader() {
         conf = Utils.readStormConfig();
-        this.uploadChuckSize = ObjectReader.getInt(conf.get(Config.STORM_BLOBSTORE_DEPENDENCY_JAR_UPLOAD_CHUCK_SIZE_BYTES), 1024 * 1024);
+        this.uploadChunkSize = ObjectReader.getInt(conf.get(Config.STORM_BLOBSTORE_DEPENDENCY_JAR_UPLOAD_CHUNK_SIZE_BYTES), 1024 * 1024);
     }
 
     public void init() {
@@ -164,7 +164,7 @@ private boolean uploadDependencyToBlobStore(String key, File dependency)
             try {
                 blob = getBlobStore().createBlob(key, new SettableBlobMeta(acls));
                 try (InputStream in = Files.newInputStream(dependency.toPath())) {
-                    IOUtils.copy(in, blob, this.uploadChuckSize);
+                    IOUtils.copy(in, blob, this.uploadChunkSize);
                 }
                 blob.close();
                 blob = null;

File: storm-client/src/jvm/org/apache/storm/Config.java
Patch:
@@ -1418,12 +1418,12 @@ public class Config extends HashMap<String, Object> {
     @IsInteger
     public static final String STORM_BLOBSTORE_INPUTSTREAM_BUFFER_SIZE_BYTES = "storm.blobstore.inputstream.buffer.size.bytes";
     /**
-     * What chuck size to use for storm client to upload dependency jars.
+     * What chunk size to use for storm client to upload dependency jars.
      */
     @IsPositiveNumber
     @IsInteger
-    public static final String STORM_BLOBSTORE_DEPENDENCY_JAR_UPLOAD_CHUCK_SIZE_BYTES =
-            "storm.blobstore.dependency.jar.upload.chuck.size.bytes";
+    public static final String STORM_BLOBSTORE_DEPENDENCY_JAR_UPLOAD_CHUNK_SIZE_BYTES =
+            "storm.blobstore.dependency.jar.upload.chunk.size.bytes";
     /**
      * FQCN of a class that implements {@code ISubmitterHook} @see ISubmitterHook for details.
      */

File: storm-client/src/jvm/org/apache/storm/dependency/DependencyUploader.java
Patch:
@@ -48,11 +48,11 @@ public class DependencyUploader {
 
     private final Map<String, Object> conf;
     private ClientBlobStore blobStore;
-    private int uploadChuckSize;
+    private final int uploadChunkSize;
 
     public DependencyUploader() {
         conf = Utils.readStormConfig();
-        this.uploadChuckSize = ObjectReader.getInt(conf.get(Config.STORM_BLOBSTORE_DEPENDENCY_JAR_UPLOAD_CHUCK_SIZE_BYTES), 1024 * 1024);
+        this.uploadChunkSize = ObjectReader.getInt(conf.get(Config.STORM_BLOBSTORE_DEPENDENCY_JAR_UPLOAD_CHUNK_SIZE_BYTES), 1024 * 1024);
     }
 
     public void init() {
@@ -164,7 +164,7 @@ private boolean uploadDependencyToBlobStore(String key, File dependency)
             try {
                 blob = getBlobStore().createBlob(key, new SettableBlobMeta(acls));
                 try (InputStream in = Files.newInputStream(dependency.toPath())) {
-                    IOUtils.copy(in, blob, this.uploadChuckSize);
+                    IOUtils.copy(in, blob, this.uploadChunkSize);
                 }
                 blob.close();
                 blob = null;

File: storm-core/test/jvm/org/apache/storm/nimbus/InMemoryTopologyActionNotifier.java
Patch:
@@ -25,7 +25,7 @@ public class InMemoryTopologyActionNotifier implements ITopologyActionNotifierPl
 
 
     @Override
-    public void prepare(Map<String, Object> StormConf) {
+    public void prepare(Map<String, Object> stormConf) {
         //no-op
     }
 

File: storm-server/src/main/java/org/apache/storm/ILocalClusterTrackedTopologyAware.java
Patch:
@@ -28,8 +28,9 @@
  * Topology.
  */
 public interface ILocalClusterTrackedTopologyAware extends ILocalCluster {
+
     /**
-     * Submit a tracked topology to be run in local mode
+     * Submit a tracked topology to be run in local mode.
      *
      * @param topologyName the name of the topology to use
      * @param conf         the config for the topology
@@ -41,7 +42,7 @@ public interface ILocalClusterTrackedTopologyAware extends ILocalCluster {
     ILocalTopology submitTopology(String topologyName, Map<String, Object> conf, TrackedTopology topology) throws TException;
 
     /**
-     * Submit a tracked topology to be run in local mode
+     * Submit a tracked topology to be run in local mode.
      *
      * @param topologyName the name of the topology to use
      * @param conf         the config for the topology

File: storm-server/src/main/java/org/apache/storm/LocalDRPC.java
Patch:
@@ -30,10 +30,11 @@
 import org.apache.storm.utils.ServiceRegistry;
 
 /**
- * A Local way to test DRPC
+ * A Local way to test DRPC.
  *
- * try (LocalDRPC drpc = new LocalDRPC()) { // Do tests }
+ * <p>try <code>(LocalDRPC drpc = new LocalDRPC()) { // Do tests }</code>
  */
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public class LocalDRPC implements ILocalDRPC {
 
     private final DRPC drpc;

File: storm-server/src/main/java/org/apache/storm/blobstore/BlobStoreUtils.java
Patch:
@@ -49,6 +49,7 @@ public static String getBlobStoreSubtree() {
         return BLOBSTORE_SUBTREE;
     }
 
+    @SuppressWarnings("checkstyle:AbbreviationAsWordInName")
     public static CuratorFramework createZKClient(Map<String, Object> conf, DaemonType type) {
         @SuppressWarnings("unchecked")
         List<String> zkServers = (List<String>) conf.get(Config.STORM_ZOOKEEPER_SERVERS);
@@ -191,7 +192,7 @@ public static boolean downloadUpdatedBlob(Map<String, Object> conf, BlobStore bl
                     out = null;
                 }
                 isSuccess = true;
-            } catch(FileNotFoundException fnf) {
+            } catch (FileNotFoundException fnf) {
                 LOG.warn("Blobstore file for key '{}' does not exist or got deleted before it could be downloaded.", key, fnf);
             } catch (IOException | AuthorizationException exception) {
                 throw new RuntimeException(exception);

File: storm-server/src/main/java/org/apache/storm/container/ResourceIsolationInterface.java
Patch:
@@ -23,7 +23,7 @@
 public interface ResourceIsolationInterface {
 
     /**
-     * Called when starting up
+     * Called when starting up.
      *
      * @param conf the cluster config
      * @throws IOException on any error.
@@ -85,6 +85,7 @@ public interface ResourceIsolationInterface {
     long getMemoryUsage(String workerId) throws IOException;
 
     /**
+     * Get the system free memory in MB.
      * @return The amount of memory in bytes that are free on the system. This might not be the entire box, it might be
      *     within a parent resource group.
      * @throws IOException on any error.

File: storm-server/src/main/java/org/apache/storm/daemon/drpc/RequestFactory.java
Patch:
@@ -21,5 +21,6 @@
 import org.apache.storm.generated.DRPCRequest;
 
 public interface RequestFactory<T extends OutstandingRequest> {
-    public T mkRequest(String function, DRPCRequest req);
+
+    T mkRequest(String function, DRPCRequest req);
 }

File: storm-server/src/main/java/org/apache/storm/daemon/nimbus/TopoCache.java
Patch:
@@ -12,6 +12,8 @@
 
 package org.apache.storm.daemon.nimbus;
 
+import static org.apache.storm.blobstore.BlobStoreAclHandler.READ;
+
 import java.io.IOException;
 import java.util.List;
 import java.util.Map;
@@ -31,8 +33,6 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import static org.apache.storm.blobstore.BlobStoreAclHandler.READ;
-
 /**
  * Cache topologies and topology confs from the blob store.
  * Makes reading this faster because it can skip
@@ -44,6 +44,7 @@ public class TopoCache {
     private final BlobStoreAclHandler aclHandler;
     private final ConcurrentHashMap<String, WithAcl<StormTopology>> topos = new ConcurrentHashMap<>();
     private final ConcurrentHashMap<String, WithAcl<Map<String, Object>>> confs = new ConcurrentHashMap<>();
+
     public TopoCache(BlobStore store, Map<String, Object> conf) {
         this.store = store;
         aclHandler = new BlobStoreAclHandler(conf);

File: storm-server/src/main/java/org/apache/storm/daemon/nimbus/TopologyActions.java
Patch:
@@ -13,7 +13,7 @@
 package org.apache.storm.daemon.nimbus;
 
 /**
- * Actions that can be done to a topology in nimbus
+ * Actions that can be done to a topology in nimbus.
  */
 public enum TopologyActions {
     STARTUP,

File: storm-server/src/main/java/org/apache/storm/daemon/nimbus/TopologyStateTransition.java
Patch:
@@ -15,8 +15,9 @@
 import org.apache.storm.generated.StormBase;
 
 /**
- * A transition from one state to another
+ * A transition from one state to another.
  */
 interface TopologyStateTransition {
+
     StormBase transition(Object argument, Nimbus nimbus, String topoId, StormBase base) throws Exception;
 }

File: storm-server/src/main/java/org/apache/storm/daemon/supervisor/ContainerLauncher.java
Patch:
@@ -28,7 +28,7 @@
 import org.slf4j.LoggerFactory;
 
 /**
- * Launches containers
+ * Launches containers.
  */
 public abstract class ContainerLauncher {
     private static final Logger LOG = LoggerFactory.getLogger(ContainerLauncher.class);
@@ -76,7 +76,7 @@ public static ContainerLauncher make(Map<String, Object> conf, String supervisor
     }
 
     /**
-     * Launch a container in a given slot
+     * Launch a container in a given slot.
      * @param port the port to run this on
      * @param assignment what to launch
      * @param state the current state of the supervisor
@@ -86,7 +86,7 @@ public static ContainerLauncher make(Map<String, Object> conf, String supervisor
     public abstract Container launchContainer(int port, LocalAssignment assignment, LocalState state) throws IOException;
 
     /**
-     * Recover a container for a running process
+     * Recover a container for a running process.
      * @param port the port the assignment is running on
      * @param assignment the assignment that was launched
      * @param state the current state of the supervisor

File: storm-server/src/main/java/org/apache/storm/daemon/supervisor/OnlyLatestExecutor.java
Patch:
@@ -39,7 +39,7 @@ public OnlyLatestExecutor(Executor exec) {
     }
 
     /**
-     * Run something in the future, but replace it with the latest if it is taking too long
+     * Run something in the future, but replace it with the latest if it is taking too long.
      *
      * @param key what to use to dedupe things.
      * @param r   what you want to run.

File: storm-server/src/main/java/org/apache/storm/localizer/BlobChangingCallback.java
Patch:
@@ -29,7 +29,7 @@ public interface BlobChangingCallback {
      * Informs the listener that a blob has changed and is ready to update and replace a localized blob that has been marked as tied to the
      * life cycle of the worker process.
      *
-     * If `go.getLatch()` is never called before the method completes it is assumed that the listener is good with the blob changing.
+     * <p>If `go.getLatch()` is never called before the method completes it is assumed that the listener is good with the blob changing.
      *
      * @param assignment the assignment this resource and callback are registered with.
      * @param port       the port that this resource and callback are registered with.

File: storm-server/src/main/java/org/apache/storm/localizer/GoodToGo.java
Patch:
@@ -29,6 +29,7 @@
 public class GoodToGo {
     private final GoodToGoLatch latch;
     private boolean gotLatch = false;
+
     public GoodToGo(CountDownLatch latch, Future<Void> doneChanging) {
         this.latch = new GoodToGoLatch(latch, doneChanging);
     }

File: storm-server/src/main/java/org/apache/storm/localizer/IOFunction.java
Patch:
@@ -17,6 +17,8 @@
 import java.io.IOException;
 
 @FunctionalInterface
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public interface IOFunction<T, R> {
+
     R apply(T t) throws IOException;
 }

File: storm-server/src/main/java/org/apache/storm/localizer/LocalizedResource.java
Patch:
@@ -220,7 +220,7 @@ Path getFilePathWithVersion() {
     private void setSize() {
         // we trust that the file exists
         Path withVersion = getFilePathWithVersion();
-        size = ServerUtils.getDU(withVersion.toFile());
+        size = ServerUtils.getDiskUsage(withVersion.toFile());
         LOG.debug("size of {} is: {}", withVersion, size);
     }
 

File: storm-server/src/main/java/org/apache/storm/localizer/LocalizedResourceRetentionSet.java
Patch:
@@ -138,6 +138,7 @@ public String toString() {
         return "Cache: " + currentSize;
     }
 
+    @SuppressWarnings("checkstyle:AbbreviationAsWordInName")
     static class LRUComparator implements Comparator<LocallyCachedBlob> {
         @Override
         public int compare(LocallyCachedBlob r1, LocallyCachedBlob r2) {

File: storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/StringMetadataCache.java
Patch:
@@ -109,10 +109,10 @@ public StringMetadata get(String s) {
     /**
      * Add the string metadata to the cache.
      *
-     * NOTE: this can cause data to be evicted from the cache when full.  When this occurs, the evictionCallback() method
+     * <p>NOTE: this can cause data to be evicted from the cache when full.  When this occurs, the evictionCallback() method
      * is called to store the metadata back into the RocksDB database.
      *
-     * This method is only exposed to the WritableStringMetadataCache interface.
+     * <p>This method is only exposed to the WritableStringMetadataCache interface.
      *
      * @param s   The string to add
      * @param stringMetadata  The string's metadata

File: storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/WritableStringMetadataCache.java
Patch:
@@ -24,10 +24,10 @@ public interface WritableStringMetadataCache extends ReadOnlyStringMetadataCache
     /**
      * Add the string metadata to the cache.
      *
-     * * NOTE: this can cause data to be evicted from the cache when full.  When this occurs, the evictionCallback() method
+     * <p>NOTE: this can cause data to be evicted from the cache when full.  When this occurs, the evictionCallback() method
      * is called to store the metadata back into the RocksDB database.
      *
-     * This method is only exposed to the WritableStringMetadataCache interface.
+     * <p>This method is only exposed to the WritableStringMetadataCache interface.
      *
      * @param s   The string to add
      * @param stringMetadata  The string's metadata

File: storm-server/src/main/java/org/apache/storm/nimbus/AssignmentDistributionService.java
Patch:
@@ -186,7 +186,6 @@ private LinkedBlockingQueue<NodeAssignments> getQueueById(Integer queueIndex) {
      * Get an assignments from the target queue with the specific index.
      * @param queueIndex index of the queue
      * @return an {@link NodeAssignments}
-     * @throws InterruptedException
      */
     public NodeAssignments nextAssignments(Integer queueIndex) throws InterruptedException {
         NodeAssignments target = null;

File: storm-server/src/main/java/org/apache/storm/nimbus/DefaultTopologyValidator.java
Patch:
@@ -24,7 +24,7 @@ public class DefaultTopologyValidator implements ITopologyValidator {
     private static final Logger LOG = LoggerFactory.getLogger(DefaultTopologyValidator.class);
 
     @Override
-    public void prepare(Map<String, Object> StormConf) {
+    public void prepare(Map<String, Object> stormConf) {
     }
 
     @Override

File: storm-server/src/main/java/org/apache/storm/nimbus/ITopologyValidator.java
Patch:
@@ -18,7 +18,7 @@
 
 public interface ITopologyValidator {
 
-    void prepare(Map<String, Object> StormConf);
+    void prepare(Map<String, Object> stormConf);
 
     void validate(String topologyName, Map<String, Object> topologyConf, StormTopology topology)
         throws InvalidTopologyException;

File: storm-server/src/main/java/org/apache/storm/nimbus/LeaderListenerCallback.java
Patch:
@@ -12,6 +12,8 @@
 
 package org.apache.storm.nimbus;
 
+import com.codahale.metrics.Meter;
+
 import java.io.IOException;
 import java.util.HashSet;
 import java.util.List;
@@ -20,7 +22,6 @@
 import java.util.TreeSet;
 import javax.security.auth.Subject;
 
-import com.codahale.metrics.Meter;
 import org.apache.commons.io.IOUtils;
 import org.apache.storm.Config;
 import org.apache.storm.DaemonConfig;
@@ -90,7 +91,7 @@ public LeaderListenerCallback(Map conf, CuratorFramework zk, BlobStore blobStore
         this.numLostLeader = metricsRegistry.registerMeter("nimbus:num-lost-leadership");
         //Since we only give up leadership if we're waiting for blobs to sync,
         //it makes sense to wait a full sync cycle before trying for leadership again.
-        this.requeueDelayMs = ObjectReader.getInt(conf.get(DaemonConfig.NIMBUS_CODE_SYNC_FREQ_SECS))*1000;
+        this.requeueDelayMs = ObjectReader.getInt(conf.get(DaemonConfig.NIMBUS_CODE_SYNC_FREQ_SECS)) * 1000;
     }
 
     /**

File: storm-server/src/main/java/org/apache/storm/nimbus/TimeOutWorkerHeartbeatsRecoveryStrategy.java
Patch:
@@ -12,6 +12,8 @@
 
 package org.apache.storm.nimbus;
 
+import static java.util.stream.Collectors.toSet;
+
 import java.util.HashSet;
 import java.util.Map;
 import java.util.Set;
@@ -21,8 +23,6 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import static java.util.stream.Collectors.toSet;
-
 /**
  * Wait for a node to report worker heartbeats until a configured timeout. For cases below we have strategies:
  *

File: storm-server/src/main/java/org/apache/storm/scheduler/Cluster.java
Patch:
@@ -1034,8 +1034,8 @@ public NormalizedResourceRequest getAllScheduledResourcesForNode(String nodeId)
                 totalScheduledResources.add(req);
             }
             // shared off heap node memory
-            for (Double offHeapNodeMemory : nodeToScheduledOffHeapNodeMemoryCache.
-                    computeIfAbsent(nid, Cluster::makeMap).values()) {
+            for (Double offHeapNodeMemory
+                    : nodeToScheduledOffHeapNodeMemoryCache.computeIfAbsent(nid, Cluster::makeMap).values()) {
                 totalScheduledResources.addOffHeap(offHeapNodeMemory);
             }
 

File: storm-server/src/main/java/org/apache/storm/scheduler/ISupervisor.java
Patch:
@@ -16,6 +16,7 @@
 import java.util.Map;
 
 public interface ISupervisor {
+
     void prepare(Map<String, Object> topoConf, String schedulerLocalDir);
 
     // for mesos, this is {hostname}-{topologyid}

File: storm-server/src/main/java/org/apache/storm/scheduler/TopologyDetails.java
Patch:
@@ -352,8 +352,7 @@ public NormalizedResourceRequest getApproximateTotalResources() {
     /**
      * Get the total CPU requirement for executor.
      *
-     * @param exec
-     * @return Map<String   ,       Double> generic resource mapping requirement for the executor
+     * @return generic resource mapping requirement for the executor
      */
     public Double getTotalCpuReqTask(ExecutorDetails exec) {
         if (hasExecInTopo(exec)) {

File: storm-server/src/main/java/org/apache/storm/scheduler/blacklist/strategies/RasBlacklistStrategy.java
Patch:
@@ -54,7 +54,7 @@ protected Set<String> releaseBlacklistWhenNeeded(Cluster cluster, final List<Str
                 if (cluster.needsSchedulingRas(td)) {
                     int slots = 0;
                     try {
-                        slots = ServerUtils.getEstimatedWorkerCountForRASTopo(td.getConf(), td.getTopology());
+                        slots = ServerUtils.getEstimatedWorkerCountForRasTopo(td.getConf(), td.getTopology());
                     } catch (InvalidTopologyException e) {
                         LOG.warn("Could not guess the number of slots needed for {}", td.getName(), e);
                     }

File: storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceAwareScheduler.java
Patch:
@@ -123,7 +123,7 @@ private void scheduleTopology(TopologyDetails td, Cluster cluster, final User to
                                   List<TopologyDetails> orderedTopologies) {
         //A copy of cluster that we can modify, but does not get committed back to cluster unless scheduling succeeds
         Cluster workingState = new Cluster(cluster);
-        RAS_Nodes nodes = new RAS_Nodes(workingState);
+        RasNodes nodes = new RasNodes(workingState);
         IStrategy rasStrategy = null;
         String strategyConf = (String) td.getConf().get(Config.TOPOLOGY_SCHEDULER_STRATEGY);
         try {

File: storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/priority/DefaultSchedulingPriorityStrategy.java
Patch:
@@ -137,8 +137,8 @@ public int compare(SimulatedUser o1, SimulatedUser o2) {
     }
 
     /**
-     * Comparator that sorts topologies by priority and then by submission time
-     * First sort by Topology Priority, if there is a tie for topology priority, topology uptime is used to sort
+     * Comparator that sorts topologies by priority and then by submission time.
+     * First sort by Topology Priority, if there is a tie for topology priority, topology uptime is used to sort.
      */
     private static class TopologyByPriorityAndSubmissionTimeComparator implements Comparator<TopologyDetails> {
 

File: storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/priority/FIFOSchedulingPriorityStrategy.java
Patch:
@@ -20,6 +20,7 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public class FIFOSchedulingPriorityStrategy extends DefaultSchedulingPriorityStrategy {
     private static final Logger LOG = LoggerFactory.getLogger(FIFOSchedulingPriorityStrategy.class);
 
@@ -54,8 +55,8 @@ public double getScore(double availableCpu, double availableMemory) {
     }
 
     /**
-     * Comparator that sorts topologies by priority and then by submission time
-     * First sort by Topology Priority, if there is a tie for topology priority, topology uptime is used to sort
+     * Comparator that sorts topologies by priority and then by submission time.
+     * First sort by Topology Priority, if there is a tie for topology priority, topology uptime is used to sort.
      */
     private static class TopologyBySubmissionTimeComparator implements Comparator<TopologyDetails> {
 

File: storm-server/src/main/java/org/apache/storm/security/auth/DefaultHttpCredentialsPlugin.java
Patch:
@@ -26,7 +26,7 @@ public class DefaultHttpCredentialsPlugin implements IHttpCredentialsPlugin {
         LoggerFactory.getLogger(DefaultHttpCredentialsPlugin.class);
 
     /**
-     * No-op
+     * No-op.
      *
      * @param topoConf Storm configuration
      */

File: storm-server/src/main/java/org/apache/storm/security/auth/IHttpCredentialsPlugin.java
Patch:
@@ -22,11 +22,12 @@
 import javax.servlet.http.HttpServletRequest;
 
 /**
- * Interface for handling credentials in an HttpServletRequest
+ * Interface for handling credentials in an HttpServletRequest.
  */
 public interface IHttpCredentialsPlugin {
+
     /**
-     * Invoked once immediately after construction
+     * Invoked once immediately after construction.
      *
      * @param topoConf Storm configuration
      */

File: storm-server/src/main/java/org/apache/storm/security/auth/ServerAuthUtils.java
Patch:
@@ -39,7 +39,7 @@ public static IHttpCredentialsPlugin getHttpCredentialsPlugin(Map<String, Object
     }
 
     /**
-     * Construct an HttpServletRequest credential plugin specified by the UI storm configuration
+     * Construct an HttpServletRequest credential plugin specified by the UI storm configuration.
      *
      * @param conf storm configuration
      * @return the plugin
@@ -50,7 +50,7 @@ public static IHttpCredentialsPlugin getUiHttpCredentialsPlugin(Map<String, Obje
     }
 
     /**
-     * Construct an HttpServletRequest credential plugin specified by the DRPC storm configuration
+     * Construct an HttpServletRequest credential plugin specified by the DRPC storm configuration.
      *
      * @param conf storm configuration
      * @return the plugin

File: storm-server/src/main/java/org/apache/storm/security/auth/workertoken/WorkerTokenManager.java
Patch:
@@ -128,9 +128,9 @@ public WorkerToken createOrUpdateTokenFor(WorkerTokenServiceType serviceType, St
      * @param topologyId the topology the credentials are for
      */
     public void upsertWorkerTokensInCredsForTopo(Map<String, String> creds, String user, String topologyId) {
-        Arrays.stream(WorkerTokenServiceType.values()).filter(type -> shouldRenewWorkerToken(creds, type))
-              .forEach(type -> {ClientAuthUtils.setWorkerToken(creds, createOrUpdateTokenFor(type, user, topologyId));
-              });
+        Arrays.stream(WorkerTokenServiceType.values())
+                .filter(type -> shouldRenewWorkerToken(creds, type))
+                .forEach(type -> ClientAuthUtils.setWorkerToken(creds, createOrUpdateTokenFor(type, user, topologyId)));
     }
 
     @VisibleForTesting

File: storm-server/src/main/java/org/apache/storm/testing/InProcessZookeeper.java
Patch:
@@ -34,6 +34,7 @@ public InProcessZookeeper() throws Exception {
     }
 
     /**
+     * Get port.
      * @return the port ZK is listening on (localhost)
      */
     public long getPort() {

File: storm-server/src/main/java/org/apache/storm/testing/TestJob.java
Patch:
@@ -19,7 +19,7 @@
  * we put our java unit testing logic in the run method. A sample
  * code will be:
  *
- * ```java
+ * <p>```java
  * Testing.withSimulatedTimeLocalCluster(new TestJob() {
  *     public void run(Cluster cluster) {
  *         // your testing logic here.

File: storm-server/src/main/java/org/apache/storm/zookeeper/AclEnforcement.java
Patch:
@@ -290,8 +290,8 @@ private static void verifyAclStrict(CuratorFramework zk, List<ACL> strictAcl, St
 
     private static boolean equivalent(List<ACL> a, List<ACL> b) {
         if (a.size() == b.size()) {
-            for (ACL aAcl : a) {
-                if (!b.contains(aAcl)) {
+            for (ACL acl : a) {
+                if (!b.contains(acl)) {
                     return false;
                 }
             }

File: storm-server/src/test/java/org/apache/storm/scheduler/resource/TestResourceAwareScheduler.java
Patch:
@@ -103,9 +103,9 @@ public void testRASNodeSlotAssign() {
         TopologyDetails topology2 = genTopology("topology2", config, 1, 0, 2, 0, 0, 0, "user");
         Topologies topologies = new Topologies(topology1, topology2);
         Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, config);
-        Map<String, RAS_Node> nodes = RAS_Nodes.getAllNodesFrom(cluster);
+        Map<String, RasNode> nodes = RasNodes.getAllNodesFrom(cluster);
         assertEquals(5, nodes.size());
-        RAS_Node node = nodes.get("r000s000");
+        RasNode node = nodes.get("r000s000");
 
         assertEquals("r000s000", node.getId());
         assertTrue(node.isAlive());
@@ -904,7 +904,7 @@ public void testNodeFreeSlot() {
         scheduler.prepare(config);
         scheduler.schedule(topologies, cluster);
 
-        Map<String, RAS_Node> nodes = RAS_Nodes.getAllNodesFrom(cluster);
+        Map<String, RasNode> nodes = RasNodes.getAllNodesFrom(cluster);
 
         for (SchedulerAssignment assignment : cluster.getAssignments().values()) {
             for (Entry<WorkerSlot, WorkerResources> entry : new HashMap<>(assignment.getScheduledResources()).entrySet()) {

File: storm-server/src/test/java/org/apache/storm/scheduler/resource/strategies/scheduling/TestDefaultResourceAwareStrategy.java
Patch:
@@ -35,7 +35,7 @@
 import org.apache.storm.scheduler.Topologies;
 import org.apache.storm.scheduler.TopologyDetails;
 import org.apache.storm.scheduler.WorkerSlot;
-import org.apache.storm.scheduler.resource.RAS_Node;
+import org.apache.storm.scheduler.resource.RasNode;
 import org.apache.storm.scheduler.resource.ResourceAwareScheduler;
 import org.apache.storm.scheduler.resource.SchedulingResult;
 import org.apache.storm.scheduler.resource.strategies.scheduling.BaseResourceAwareStrategy.ObjectResources;
@@ -474,7 +474,7 @@ public void testMultipleRacks() {
         List<String> nodeHostnames = rackToNodes.get("rack-1");
         for (int i = 0; i< topo2.getExecutors().size()/2; i++) {
             String nodeHostname = nodeHostnames.get(i % nodeHostnames.size());
-            RAS_Node node = rs.hostnameToNodes(nodeHostname).get(0);
+            RasNode node = rs.hostnameToNodes(nodeHostname).get(0);
             WorkerSlot targetSlot = node.getFreeSlots().iterator().next();
             ExecutorDetails targetExec = executorIterator.next();
             // to keep track of free slots
@@ -599,7 +599,7 @@ public void testMultipleRacksWithFavoritism() {
         List<String> nodeHostnames = rackToNodes.get("rack-1");
         for (int i = 0; i< topo2.getExecutors().size()/2; i++) {
             String nodeHostname = nodeHostnames.get(i % nodeHostnames.size());
-            RAS_Node node = rs.hostnameToNodes(nodeHostname).get(0);
+            RasNode node = rs.hostnameToNodes(nodeHostname).get(0);
             WorkerSlot targetSlot = node.getFreeSlots().iterator().next();
             ExecutorDetails targetExec = executorIterator.next();
             // to keep track of free slots

File: storm-webapp/src/main/java/org/apache/storm/daemon/ui/UIHelpers.java
Patch:
@@ -526,7 +526,6 @@ public static Response makeStandardResponse(
     private static final AtomicReference<List<Map<String, String>>> MEMORIZED_VERSIONS = new AtomicReference<>();
     private static final AtomicReference<Map<String, String>> MEMORIZED_FULL_VERSION = new AtomicReference<>();
 
-
     private static Map<String, String> toJsonStruct(IVersionInfo info) {
         Map<String, String> ret = new HashMap<>();
         ret.put("version", info.getVersion());

File: storm-webapp/src/main/java/org/apache/storm/daemon/ui/UIHelpers.java
Patch:
@@ -526,7 +526,6 @@ public static Response makeStandardResponse(
     private static final AtomicReference<List<Map<String, String>>> MEMORIZED_VERSIONS = new AtomicReference<>();
     private static final AtomicReference<Map<String, String>> MEMORIZED_FULL_VERSION = new AtomicReference<>();
 
-
     private static Map<String, String> toJsonStruct(IVersionInfo info) {
         Map<String, String> ret = new HashMap<>();
         ret.put("version", info.getVersion());

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/bolt/AbstractHdfsBolt.java
Patch:
@@ -247,7 +247,9 @@ public void declareOutputFields(OutputFieldsDeclarer outputFieldsDeclarer) {
     @Override
     public void cleanup() {
         doRotationAndRemoveAllWriters();
-        this.rotationTimer.cancel();
+        if (this.rotationTimer != null) {
+            this.rotationTimer.cancel();
+        }
     }
 
     private void doRotationAndRemoveAllWriters() {

File: external/storm-hdfs/src/test/java/org/apache/storm/hdfs/bolt/format/TestSimpleFileNameFormat.java
Patch:
@@ -69,7 +69,7 @@ public void testTimeFormat() {
     }
 
     private TopologyContext createTopologyContext(Map<String, Object> topoConf) {
-        Map<Integer, String> taskToComponent = new HashMap<Integer, String>();
+        Map<Integer, String> taskToComponent = new HashMap<>();
         taskToComponent.put(7, "Xcom");
         return new TopologyContext(null, topoConf, taskToComponent, null, null, null, null, null, null, 7, 6703, null, null, null, null,
                                    null, null, null);

File: external/storm-hdfs/src/test/java/org/apache/storm/hdfs/spout/TestHdfsSemantics.java
Patch:
@@ -12,6 +12,8 @@
 
 package org.apache.storm.hdfs.spout;
 
+import static org.hamcrest.core.IsNull.notNullValue;
+
 import java.io.IOException;
 import org.apache.hadoop.fs.CommonConfigurationKeys;
 import org.apache.hadoop.fs.FSDataOutputStream;
@@ -30,7 +32,6 @@
 
 import static org.junit.Assert.assertThat;
 import static org.junit.Assert.fail;
-import static org.mockito.ArgumentMatchers.notNull;
 
 public class TestHdfsSemantics {
 
@@ -124,7 +125,7 @@ public void testAppendSemantics() throws Exception {
 
         //2 try to append to a closed file
         try (FSDataOutputStream os2 = fs.append(file1)) {
-            assertThat(os2, notNull());
+            assertThat(os2, notNullValue());
         }
     }
 

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/bolt/AbstractHdfsBolt.java
Patch:
@@ -247,7 +247,9 @@ public void declareOutputFields(OutputFieldsDeclarer outputFieldsDeclarer) {
     @Override
     public void cleanup() {
         doRotationAndRemoveAllWriters();
-        this.rotationTimer.cancel();
+        if (this.rotationTimer != null) {
+            this.rotationTimer.cancel();
+        }
     }
 
     private void doRotationAndRemoveAllWriters() {

File: external/storm-hdfs/src/test/java/org/apache/storm/hdfs/bolt/format/TestSimpleFileNameFormat.java
Patch:
@@ -69,7 +69,7 @@ public void testTimeFormat() {
     }
 
     private TopologyContext createTopologyContext(Map<String, Object> topoConf) {
-        Map<Integer, String> taskToComponent = new HashMap<Integer, String>();
+        Map<Integer, String> taskToComponent = new HashMap<>();
         taskToComponent.put(7, "Xcom");
         return new TopologyContext(null, topoConf, taskToComponent, null, null, null, null, null, null, 7, 6703, null, null, null, null,
                                    null, null, null);

File: external/storm-hdfs/src/test/java/org/apache/storm/hdfs/spout/TestHdfsSemantics.java
Patch:
@@ -12,6 +12,8 @@
 
 package org.apache.storm.hdfs.spout;
 
+import static org.hamcrest.core.IsNull.notNullValue;
+
 import java.io.IOException;
 import org.apache.hadoop.fs.CommonConfigurationKeys;
 import org.apache.hadoop.fs.FSDataOutputStream;
@@ -30,7 +32,6 @@
 
 import static org.junit.Assert.assertThat;
 import static org.junit.Assert.fail;
-import static org.mockito.ArgumentMatchers.notNull;
 
 public class TestHdfsSemantics {
 
@@ -124,7 +125,7 @@ public void testAppendSemantics() throws Exception {
 
         //2 try to append to a closed file
         try (FSDataOutputStream os2 = fs.append(file1)) {
-            assertThat(os2, notNull());
+            assertThat(os2, notNullValue());
         }
     }
 

File: storm-client/src/jvm/org/apache/storm/security/auth/kerberos/AutoTGT.java
Patch:
@@ -137,15 +137,15 @@ public void populateCredentials(Map<String, String> credentials) {
                 }
 
                 if (!tgt.isForwardable()) {
-                    throw new RuntimeException("The TGT found is not forwardable. Please use -f option.");
+                    throw new RuntimeException("The TGT found is not forwardable. Please use -f option with 'kinit'.");
                 }
 
                 if (!tgt.isRenewable()) {
-                    throw new RuntimeException("The TGT found is not renewable. Please use -r option.");
+                    throw new RuntimeException("The TGT found is not renewable. Please use -r option with 'kinit'.");
                 }
 
                 if (tgt.getClientAddresses() != null) {
-                    throw new RuntimeException("The TGT found is not address-less. Please use -A option.");
+                    throw new RuntimeException("The TGT found is not address-less. Please use -A option with 'kinit'.");
                 }
 
                 LOG.info("Pushing TGT for " + tgt.getClient() + " to topology.");

File: storm-client/src/jvm/org/apache/storm/security/auth/sasl/SimpleSaslServerCallbackHandler.java
Patch:
@@ -16,6 +16,7 @@
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.List;
+import java.util.Objects;
 import java.util.Optional;
 import javax.security.auth.callback.Callback;
 import javax.security.auth.callback.CallbackHandler;
@@ -178,9 +179,9 @@ public void handle(Callback[] callbacks) throws UnsupportedCallbackException, IO
                 ac.setAuthorizedID(zid);
             }
 
-            //When zid and zid are not equal, nid is attempting to impersonate zid, We
+            //When nid and zid are not equal, nid is attempting to impersonate zid, We
             //add the nid as the real user in reqContext's subject which will be used during authorization.
-            if (!nid.equals(zid)) {
+            if (!Objects.equals(nid, zid)) {
                 LOG.info("Impersonation attempt  authenticationID = {} authorizationID = {}",
                          nid, zid);
                 if (!allowImpersonation) {

File: storm-client/src/jvm/org/apache/storm/security/auth/sasl/SimpleSaslServerCallbackHandler.java
Patch:
@@ -16,6 +16,7 @@
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.List;
+import java.util.Objects;
 import java.util.Optional;
 import javax.security.auth.callback.Callback;
 import javax.security.auth.callback.CallbackHandler;
@@ -178,9 +179,9 @@ public void handle(Callback[] callbacks) throws UnsupportedCallbackException, IO
                 ac.setAuthorizedID(zid);
             }
 
-            //When zid and zid are not equal, nid is attempting to impersonate zid, We
+            //When nid and zid are not equal, nid is attempting to impersonate zid, We
             //add the nid as the real user in reqContext's subject which will be used during authorization.
-            if (!nid.equals(zid)) {
+            if (!Objects.equals(nid, zid)) {
                 LOG.info("Impersonation attempt  authenticationID = {} authorizationID = {}",
                          nid, zid);
                 if (!allowImpersonation) {

File: examples/storm-redis-examples/src/main/java/org/apache/storm/redis/tools/Base64ToBinaryStateMigrationUtil.java
Patch:
@@ -38,7 +38,6 @@
 
 import redis.clients.util.SafeEncoder;
 
-
 public class Base64ToBinaryStateMigrationUtil {
     private static final Logger LOG = LoggerFactory.getLogger(Base64ToBinaryStateMigrationUtil.class);
     private static final String OPTION_REDIS_HOST_SHORT = "h";

File: examples/storm-redis-examples/src/main/java/org/apache/storm/redis/topology/PersistentWordCount.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.redis.topology;
 
 import org.apache.storm.Config;
@@ -36,8 +37,6 @@ public class PersistentWordCount {
     private static final int TEST_REDIS_PORT = 6379;
 
     public static void main(String[] args) throws Exception {
-        Config config = new Config();
-
         String host = TEST_REDIS_HOST;
         int port = TEST_REDIS_PORT;
 
@@ -68,6 +67,7 @@ public static void main(String[] args) throws Exception {
             System.out.println("Usage: PersistentWordCount <redis host> <redis port> (topology name)");
             return;
         }
+        Config config = new Config();
         StormSubmitter.submitTopology(topoName, config, builder.createTopology());
     }
 

File: examples/storm-redis-examples/src/main/java/org/apache/storm/redis/trident/WordCountStoreMapper.java
Patch:
@@ -15,11 +15,12 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.redis.trident;
 
-import org.apache.storm.tuple.ITuple;
 import org.apache.storm.redis.common.mapper.RedisDataTypeDescription;
 import org.apache.storm.redis.common.mapper.RedisStoreMapper;
+import org.apache.storm.tuple.ITuple;
 
 public class WordCountStoreMapper implements RedisStoreMapper {
     @Override

File: examples/storm-redis-examples/src/main/java/org/apache/storm/redis/trident/WordCountTridentRedis.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.redis.trident;
 
 import org.apache.storm.Config;
@@ -34,7 +35,8 @@
 import org.apache.storm.tuple.Values;
 
 public class WordCountTridentRedis {
-    public static StormTopology buildTopology(String redisHost, Integer redisPort){
+
+    public static StormTopology buildTopology(String redisHost, Integer redisPort) {
         Fields fields = new Fields("word", "count");
         FixedBatchSpout spout = new FixedBatchSpout(fields, 4,
                 new Values("storm", 1),

File: examples/storm-redis-examples/src/main/java/org/apache/storm/redis/trident/WordCountTridentRedisMap.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.redis.trident;
 
 import org.apache.storm.Config;
@@ -34,7 +35,8 @@
 import org.apache.storm.tuple.Values;
 
 public class WordCountTridentRedisMap {
-    public static StormTopology buildTopology(String redisHost, Integer redisPort){
+
+    public static StormTopology buildTopology(String redisHost, Integer redisPort) {
         Fields fields = new Fields("word", "count");
         FixedBatchSpout spout = new FixedBatchSpout(fields, 4,
                 new Values("storm", 1),

File: examples/storm-mongodb-examples/src/main/java/org/apache/storm/mongodb/topology/InsertWordCount.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.mongodb.topology;
 
 import org.apache.storm.Config;
@@ -35,8 +36,6 @@ public class InsertWordCount {
     
 
     public static void main(String[] args) throws Exception {
-        Config config = new Config();
-
         String url = TEST_MONGODB_URL;
         String collectionName = TEST_MONGODB_COLLECTION_NAME;
 
@@ -67,6 +66,7 @@ public static void main(String[] args) throws Exception {
             System.out.println("Usage: InsertWordCount <mongodb url> <mongodb collection> [topology name]");
             return;
         }
+        Config config = new Config();
         StormSubmitter.submitTopology(topoName, config, builder.createTopology());
     }
 }

File: examples/storm-mongodb-examples/src/main/java/org/apache/storm/mongodb/topology/LookupWordCount.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.mongodb.topology;
 
 import org.apache.storm.Config;
@@ -36,8 +37,6 @@ public class LookupWordCount {
     private static final String TEST_MONGODB_COLLECTION_NAME = "wordcount";
 
     public static void main(String[] args) throws Exception {
-        Config config = new Config();
-
         String url = TEST_MONGODB_URL;
         String collectionName = TEST_MONGODB_COLLECTION_NAME;
 
@@ -70,7 +69,8 @@ public static void main(String[] args) throws Exception {
             System.out.println("Usage: LookupWordCount <mongodb url> <mongodb collection> [topology name]");
             return;
         }
-        
+
+        Config config = new Config();
         StormSubmitter.submitTopology(topoName, config, builder.createTopology());
     }
 }

File: examples/storm-mongodb-examples/src/main/java/org/apache/storm/mongodb/topology/UpdateWordCount.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.mongodb.topology;
 
 import org.apache.storm.Config;
@@ -37,8 +38,6 @@ public class UpdateWordCount {
     
 
     public static void main(String[] args) throws Exception {
-        Config config = new Config();
-
         String url = TEST_MONGODB_URL;
         String collectionName = TEST_MONGODB_COLLECTION_NAME;
 
@@ -78,6 +77,7 @@ public static void main(String[] args) throws Exception {
             System.out.println("Usage: UpdateWordCount <mongodb url> <mongodb collection> [topology name]");
             return;
         }
+        Config config = new Config();
         StormSubmitter.submitTopology(topoName, config, builder.createTopology());
     }
 }

File: examples/storm-mongodb-examples/src/main/java/org/apache/storm/mongodb/trident/WordCountTrident.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.mongodb.trident;
 
 import org.apache.storm.Config;
@@ -36,7 +37,7 @@
 
 public class WordCountTrident {
 
-    public static StormTopology buildTopology(String url, String collectionName){
+    public static StormTopology buildTopology(String url, String collectionName) {
         Fields fields = new Fields("word", "count");
         FixedBatchSpout spout = new FixedBatchSpout(fields, 4,
                 new Values("storm", 1),

File: examples/storm-hdfs-examples/src/main/java/org/apache/storm/hdfs/bolt/HdfsFileTopology.java
Patch:
@@ -101,6 +101,7 @@ public static void waitForSeconds(int seconds) {
         try {
             Thread.sleep(seconds * 1000);
         } catch (InterruptedException e) {
+            //ignore
         }
     }
 

File: examples/storm-hdfs-examples/src/main/java/org/apache/storm/hdfs/bolt/SequenceFileTopology.java
Patch:
@@ -104,6 +104,7 @@ public static void waitForSeconds(int seconds) {
         try {
             Thread.sleep(seconds * 1000);
         } catch (InterruptedException e) {
+            //ignore
         }
     }
 

File: examples/storm-opentsdb-examples/src/main/java/org/apache/storm/opentsdb/SampleOpenTsdbBoltTopology.java
Patch:
@@ -16,6 +16,7 @@
  * specific language governing permissions and limitations
  * under the License.
  */
+
 package org.apache.storm.opentsdb;
 
 import java.util.Collections;
@@ -33,7 +34,7 @@
 public class SampleOpenTsdbBoltTopology {
 
     public static void main(String[] args) throws Exception {
-        if(args.length == 0) {
+        if (args.length == 0) {
             throw new IllegalArgumentException("There should be at least one argument. Run as `SampleOpenTsdbBoltTopology <tsdb-url>`");
         }
 

File: examples/storm-opentsdb-examples/src/main/java/org/apache/storm/opentsdb/SampleOpenTsdbTridentTopology.java
Patch:
@@ -16,6 +16,7 @@
  * specific language governing permissions and limitations
  * under the License.
  */
+
 package org.apache.storm.opentsdb;
 
 import java.util.Collections;
@@ -40,7 +41,7 @@ public class SampleOpenTsdbTridentTopology {
     private static final Logger LOG = LoggerFactory.getLogger(SampleOpenTsdbTridentTopology.class);
 
     public static void main(String[] args) throws Exception {
-        if(args.length == 0) {
+        if (args.length == 0) {
             throw new IllegalArgumentException("There should be at least one argument. Run as `SampleOpenTsdbTridentTopology <tsdb-url>`");
         }
 

File: examples/storm-starter/src/jvm/org/apache/storm/starter/streams/TypedTupleExample.java
Patch:
@@ -37,7 +37,7 @@ public static void main(String[] args) throws Exception {
         StreamBuilder builder = new StreamBuilder();
         Stream<Tuple3<Integer, Long, Long>> stream = builder.newStream(new RandomIntegerSpout(), TupleValueMappers.of(0, 1, 2));
 
-        PairStream<Long, Integer> pairs = stream.mapToPair(t -> Pair.of(t._2 / 10000, t._1));
+        PairStream<Long, Integer> pairs = stream.mapToPair(t -> Pair.of(t.value2 / 10000, t.value1));
 
         pairs.window(TumblingWindows.of(Count.of(10))).groupByKey().print();
 

File: storm-client/src/jvm/org/apache/storm/annotation/InterfaceStability.java
Patch:
@@ -18,7 +18,6 @@
 
 /**
  * Annotation to inform users of how much to rely on a particular package, class or method not changing over time.
- * </ul>
  */
 @InterfaceStability.Evolving
 public class InterfaceStability {

File: storm-client/src/jvm/org/apache/storm/assignments/InMemoryAssignmentBackend.java
Patch:
@@ -24,6 +24,7 @@
 
 /**
  * An assignment backend which will keep all assignments and id-info in memory. Only used if no backend is specified internal.
+ *
  * <p>About thread safe: idToAssignment,idToName,nameToId are all memory cache in nimbus local, for
  * <ul>
  * <li>idToAssignment: nimbus will modify it and supervisors will sync it at fixed interval,

File: storm-client/src/jvm/org/apache/storm/callback/ZKStateChangedCallback.java
Patch:
@@ -14,6 +14,7 @@
 
 import org.apache.storm.shade.org.apache.zookeeper.Watcher;
 
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public interface ZKStateChangedCallback {
     public void changed(Watcher.Event.EventType type, String path);
 }

File: storm-client/src/jvm/org/apache/storm/cluster/PaceMakerStateStorageFactory.java
Patch:
@@ -24,10 +24,10 @@
 
 public class PaceMakerStateStorageFactory implements StateStorageFactory {
     @Override
-    public IStateStorage mkStore(Map<String, Object> config, Map<String, Object> auth_conf, ClusterStateContext context) {
+    public IStateStorage mkStore(Map<String, Object> config, Map<String, Object> authConf, ClusterStateContext context) {
         try {
             ZKStateStorageFactory zkfact = new ZKStateStorageFactory();
-            IStateStorage zkState = zkfact.mkStore(config, auth_conf, context);
+            IStateStorage zkState = zkfact.mkStore(config, authConf, context);
             return new PaceMakerStateStorage(new PacemakerClientPool(config), zkState);
         } catch (Exception e) {
             throw Utils.wrapInRuntime(e);

File: storm-client/src/jvm/org/apache/storm/cluster/StateStorageFactory.java
Patch:
@@ -16,5 +16,5 @@
 
 public interface StateStorageFactory {
 
-    IStateStorage mkStore(Map<String, Object> config, Map<String, Object> auth_conf, ClusterStateContext context);
+    IStateStorage mkStore(Map<String, Object> config, Map<String, Object> authConf, ClusterStateContext context);
 }

File: storm-client/src/jvm/org/apache/storm/cluster/ZKStateStorage.java
Patch:
@@ -38,6 +38,7 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public class ZKStateStorage implements IStateStorage {
 
     private static Logger LOG = LoggerFactory.getLogger(ZKStateStorage.class);

File: storm-client/src/jvm/org/apache/storm/cluster/ZKStateStorageFactory.java
Patch:
@@ -21,12 +21,13 @@
 import java.util.Map;
 import org.apache.storm.utils.Utils;
 
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public class ZKStateStorageFactory implements StateStorageFactory {
 
     @Override
-    public IStateStorage mkStore(Map<String, Object> config, Map<String, Object> auth_conf, ClusterStateContext context) {
+    public IStateStorage mkStore(Map<String, Object> config, Map<String, Object> authConf, ClusterStateContext context) {
         try {
-            return new ZKStateStorage(config, auth_conf, context);
+            return new ZKStateStorage(config, authConf, context);
         } catch (Exception e) {
             throw Utils.wrapInRuntime(e);
         }

File: storm-client/src/jvm/org/apache/storm/container/cgroup/CgroupCenter.java
Patch:
@@ -37,7 +37,7 @@ private CgroupCenter() {
 
     }
 
-    public synchronized static CgroupCenter getInstance() {
+    public static synchronized CgroupCenter getInstance() {
         if (CgroupUtils.enabled()) {
             instance = new CgroupCenter();
             return instance;
@@ -82,10 +82,10 @@ public Set<SubSystem> getSubSystems() {
                 if (type == null) {
                     continue;
                 }
-                int hierarchyID = Integer.valueOf(split[1]);
+                int hierarchyId = Integer.valueOf(split[1]);
                 int cgroupNum = Integer.valueOf(split[2]);
                 boolean enable = Integer.valueOf(split[3]).intValue() == 1 ? true : false;
-                subSystems.add(new SubSystem(type, hierarchyID, cgroupNum, enable));
+                subSystems.add(new SubSystem(type, hierarchyId, cgroupNum, enable));
             }
             return subSystems;
         } catch (Exception e) {

File: storm-client/src/jvm/org/apache/storm/container/cgroup/CgroupCommon.java
Patch:
@@ -45,9 +45,6 @@ public CgroupCommon(String name, Hierarchy hierarchy, CgroupCommon parent) {
         this.isRoot = false;
     }
 
-    /**
-     * rootCgroup
-     */
     public CgroupCommon(Hierarchy hierarchy, String dir) {
         this.name = "";
         this.hierarchy = hierarchy;

File: storm-client/src/jvm/org/apache/storm/container/cgroup/CgroupUtils.java
Patch:
@@ -48,7 +48,7 @@ public static void deleteDir(String dir) {
     }
 
     /**
-     * Get a set of SubSystemType objects from a comma delimited list of subsystem names
+     * Get a set of SubSystemType objects from a comma delimited list of subsystem names.
      */
     public static Set<SubSystemType> getSubSystemsFromString(String str) {
         Set<SubSystemType> result = new HashSet<SubSystemType>();
@@ -64,7 +64,7 @@ public static Set<SubSystemType> getSubSystemsFromString(String str) {
     }
 
     /**
-     * Get a string that is a comma delimited list of subsystems
+     * Get a string that is a comma delimited list of subsystems.
      */
     public static String subSystemsToString(Set<SubSystemType> subSystems) {
         StringBuilder sb = new StringBuilder();

File: storm-client/src/jvm/org/apache/storm/container/cgroup/Device.java
Patch:
@@ -13,7 +13,7 @@
 package org.apache.storm.container.cgroup;
 
 /**
- * a class that represents a device in linux
+ * a class that represents a device in linux.
  */
 public class Device {
 

File: storm-client/src/jvm/org/apache/storm/container/cgroup/Hierarchy.java
Patch:
@@ -15,7 +15,7 @@
 import java.util.Set;
 
 /**
- * A class that describes a cgroup hierarchy
+ * A class that describes a cgroup hierarchy.
  */
 public class Hierarchy {
 
@@ -38,14 +38,14 @@ public Hierarchy(String name, Set<SubSystemType> subSystems, String dir) {
     }
 
     /**
-     * get subsystems
+     * get subsystems.
      */
     public Set<SubSystemType> getSubSystems() {
         return subSystems;
     }
 
     /**
-     * get all subsystems in hierarchy as a comma delimited list
+     * get all subsystems in hierarchy as a comma delimited list.
      */
     public String getType() {
         return type;

File: storm-client/src/jvm/org/apache/storm/container/cgroup/SubSystemType.java
Patch:
@@ -13,7 +13,7 @@
 package org.apache.storm.container.cgroup;
 
 /**
- * A enum class to described the subsystems that can be used
+ * A enum class to described the subsystems that can be used.
  */
 public enum SubSystemType {
 

File: storm-client/src/jvm/org/apache/storm/coordination/BatchOutputCollector.java
Patch:
@@ -43,7 +43,7 @@ public void emitDirect(int taskId, List<Object> tuple) {
     public abstract void emitDirect(int taskId, String streamId, List<Object> tuple);
 
     /**
-     * Flush any buffered tuples (when batching is enabled)
+     * Flush any buffered tuples (when batching is enabled).
      */
     public abstract void flush();
 

File: storm-client/src/jvm/org/apache/storm/daemon/GrouperFactory.java
Patch:
@@ -121,7 +121,7 @@ public static LoadAwareCustomStreamGrouping mkGrouper(WorkerTopologyContext cont
     }
 
     /**
-     * A bridge between CustomStreamGrouping and LoadAwareCustomStreamGrouping
+     * A bridge between CustomStreamGrouping and LoadAwareCustomStreamGrouping.
      */
     public static class BasicLoadAwareCustomStreamGrouping implements LoadAwareCustomStreamGrouping {
 

File: storm-client/src/jvm/org/apache/storm/daemon/StormCommon.java
Patch:
@@ -252,19 +252,19 @@ public static IBolt makeAckerBolt() {
 
     @SuppressWarnings("unchecked")
     public static void addAcker(Map<String, Object> conf, StormTopology topology) {
-        int ackerNum =
-            ObjectReader.getInt(conf.get(Config.TOPOLOGY_ACKER_EXECUTORS), ObjectReader.getInt(conf.get(Config.TOPOLOGY_WORKERS)));
-        Map<GlobalStreamId, Grouping> inputs = ackerInputs(topology);
 
         Map<String, StreamInfo> outputStreams = new HashMap<String, StreamInfo>();
         outputStreams.put(Acker.ACKER_ACK_STREAM_ID, Thrift.directOutputFields(Arrays.asList("id", "time-delta-ms")));
         outputStreams.put(Acker.ACKER_FAIL_STREAM_ID, Thrift.directOutputFields(Arrays.asList("id", "time-delta-ms")));
         outputStreams.put(Acker.ACKER_RESET_TIMEOUT_STREAM_ID, Thrift.directOutputFields(Arrays.asList("id", "time-delta-ms")));
 
         Map<String, Object> ackerConf = new HashMap<>();
+        int ackerNum =
+                ObjectReader.getInt(conf.get(Config.TOPOLOGY_ACKER_EXECUTORS), ObjectReader.getInt(conf.get(Config.TOPOLOGY_WORKERS)));
         ackerConf.put(Config.TOPOLOGY_TASKS, ackerNum);
         ackerConf.put(Config.TOPOLOGY_TICK_TUPLE_FREQ_SECS, ObjectReader.getInt(conf.get(Config.TOPOLOGY_MESSAGE_TIMEOUT_SECS)));
 
+        Map<GlobalStreamId, Grouping> inputs = ackerInputs(topology);
         Bolt acker = Thrift.prepareSerializedBoltDetails(inputs, makeAckerBolt(), outputStreams, ackerNum, ackerConf);
 
         for (Bolt bolt : topology.get_bolts().values()) {

File: storm-client/src/jvm/org/apache/storm/daemon/supervisor/ExitCodeCallback.java
Patch:
@@ -18,7 +18,7 @@
 public interface ExitCodeCallback {
 
     /**
-     * The process finished
+     * The process finished.
      *
      * @param exitCode the exit code of the finished process.
      */

File: storm-client/src/jvm/org/apache/storm/dependency/DependencyBlobStoreUtils.java
Patch:
@@ -30,6 +30,7 @@ public static String generateDependencyBlobKey(String key) {
         return BLOB_DEPENDENCIES_PREFIX + key;
     }
 
+    @SuppressWarnings("checkstyle:AbbreviationAsWordInName")
     public static String applyUUIDToFileName(String fileName) {
         String fileNameWithExt = Files.getNameWithoutExtension(fileName);
         String ext = Files.getFileExtension(fileName);

File: storm-client/src/jvm/org/apache/storm/dependency/DependencyUploader.java
Patch:
@@ -37,11 +37,11 @@
 import org.apache.storm.generated.KeyNotFoundException;
 import org.apache.storm.generated.SettableBlobMeta;
 import org.apache.storm.shade.com.google.common.annotations.VisibleForTesting;
+import org.apache.storm.shade.org.apache.commons.io.IOUtils;
 import org.apache.storm.utils.ObjectReader;
 import org.apache.storm.utils.Utils;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
-import org.apache.storm.shade.org.apache.commons.io.IOUtils;
 
 public class DependencyUploader {
     public static final Logger LOG = LoggerFactory.getLogger(DependencyUploader.class);
@@ -163,7 +163,7 @@ private boolean uploadDependencyToBlobStore(String key, File dependency)
             AtomicOutputStream blob = null;
             try {
                 blob = getBlobStore().createBlob(key, new SettableBlobMeta(acls));
-                try(InputStream in = Files.newInputStream(dependency.toPath())) {
+                try (InputStream in = Files.newInputStream(dependency.toPath())) {
                     IOUtils.copy(in, blob, this.uploadChuckSize);
                 }
                 blob.close();

File: storm-client/src/jvm/org/apache/storm/drpc/DRPCInvocationsClient.java
Patch:
@@ -25,6 +25,7 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public class DRPCInvocationsClient extends ThriftClient implements DistributedRPCInvocations.Iface {
     public static final Logger LOG = LoggerFactory.getLogger(DRPCInvocationsClient.class);
     private final AtomicReference<DistributedRPCInvocations.Client> client = new AtomicReference<>();
@@ -35,7 +36,7 @@ public DRPCInvocationsClient(Map<String, Object> conf, String host, int port) th
         super(conf, ThriftConnectionType.DRPC_INVOCATIONS, host, port, null);
         this.host = host;
         this.port = port;
-        client.set(new DistributedRPCInvocations.Client(_protocol));
+        client.set(new DistributedRPCInvocations.Client(protocol));
     }
 
     public String getHost() {
@@ -49,7 +50,7 @@ public int getPort() {
     public void reconnectClient() throws TException {
         if (client.get() == null) {
             reconnect();
-            client.set(new DistributedRPCInvocations.Client(_protocol));
+            client.set(new DistributedRPCInvocations.Client(protocol));
         }
     }
 

File: storm-client/src/jvm/org/apache/storm/drpc/LinearDRPCInputDeclarer.java
Patch:
@@ -16,6 +16,7 @@
 import org.apache.storm.topology.ComponentConfigurationDeclarer;
 import org.apache.storm.tuple.Fields;
 
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public interface LinearDRPCInputDeclarer extends ComponentConfigurationDeclarer<LinearDRPCInputDeclarer> {
     public LinearDRPCInputDeclarer fieldsGrouping(Fields fields);
 

File: storm-client/src/jvm/org/apache/storm/drpc/LinearDRPCTopologyBuilder.java
Patch:
@@ -41,7 +41,7 @@
 import org.apache.storm.topology.TopologyBuilder;
 import org.apache.storm.tuple.Fields;
 
-
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public class LinearDRPCTopologyBuilder {
     String function;
     List<Component> components = new ArrayList<>();

File: storm-client/src/jvm/org/apache/storm/executor/ExecutorTransfer.java
Patch:
@@ -36,7 +36,7 @@ public class ExecutorTransfer {
     private int indexingBase = 0;
     private ArrayList<JCQueue> localReceiveQueues; // [taskId-indexingBase] => queue : List of all recvQs local to this worker
     private AtomicReferenceArray<JCQueue> queuesToFlush;
-        // [taskId-indexingBase] => queue, some entries can be null. : outbound Qs for this executor instance
+    // [taskId-indexingBase] => queue, some entries can be null. : outbound Qs for this executor instance
 
 
     public ExecutorTransfer(WorkerState workerData, Map<String, Object> topoConf) {

File: storm-client/src/jvm/org/apache/storm/executor/bolt/BoltOutputCollectorImpl.java
Patch:
@@ -98,8 +98,8 @@ private List<Integer> boltEmit(String streamId, Collection<Tuple> anchors, List<
                     if (rootIds.size() > 0) {
                         long edgeId = MessageId.generateId(random);
                         ((TupleImpl) a).updateAckVal(edgeId);
-                        for (Long root_id : rootIds) {
-                            putXor(anchorsToIds, root_id, edgeId);
+                        for (Long rootId : rootIds) {
+                            putXor(anchorsToIds, rootId, edgeId);
                         }
                     }
                 }
@@ -202,7 +202,7 @@ private long tupleTimeDelta(TupleImpl tuple) {
     private void putXor(Map<Long, Long> pending, Long key, Long id) {
         Long curr = pending.get(key);
         if (curr == null) {
-            curr = 0l;
+            curr = 0L;
         }
         pending.put(key, Utils.bitXor(curr, id));
     }

File: storm-client/src/jvm/org/apache/storm/executor/spout/SpoutExecutor.java
Patch:
@@ -33,7 +33,7 @@
 import org.apache.storm.hooks.info.SpoutAckInfo;
 import org.apache.storm.hooks.info.SpoutFailInfo;
 import org.apache.storm.policy.IWaitStrategy;
-import org.apache.storm.policy.IWaitStrategy.WAIT_SITUATION;
+import org.apache.storm.policy.IWaitStrategy.WaitSituation;
 import org.apache.storm.shade.com.google.common.collect.ImmutableMap;
 import org.apache.storm.spout.ISpout;
 import org.apache.storm.spout.SpoutOutputCollector;
@@ -75,9 +75,9 @@ public class SpoutExecutor extends Executor {
     public SpoutExecutor(final WorkerState workerData, final List<Long> executorId, Map<String, String> credentials) {
         super(workerData, executorId, credentials, ClientStatsUtil.SPOUT);
         this.spoutWaitStrategy = ReflectionUtils.newInstance((String) topoConf.get(Config.TOPOLOGY_SPOUT_WAIT_STRATEGY));
-        this.spoutWaitStrategy.prepare(topoConf, WAIT_SITUATION.SPOUT_WAIT);
+        this.spoutWaitStrategy.prepare(topoConf, WaitSituation.SPOUT_WAIT);
         this.backPressureWaitStrategy = ReflectionUtils.newInstance((String) topoConf.get(Config.TOPOLOGY_BACKPRESSURE_WAIT_STRATEGY));
-        this.backPressureWaitStrategy.prepare(topoConf, WAIT_SITUATION.BACK_PRESSURE_WAIT);
+        this.backPressureWaitStrategy.prepare(topoConf, WaitSituation.BACK_PRESSURE_WAIT);
 
         this.lastActive = new AtomicBoolean(false);
         this.hasAckers = StormCommon.hasAckers(topoConf);

File: storm-client/src/jvm/org/apache/storm/grouping/CustomStreamGrouping.java
Patch:
@@ -23,7 +23,7 @@ public interface CustomStreamGrouping extends Serializable {
      * Tells the stream grouping at runtime the tasks in the target bolt. This information should be used in chooseTasks to determine the
      * target tasks.
      *
-     * It also tells the grouping the metadata on the stream this grouping will be used on.
+     * <p>It also tells the grouping the metadata on the stream this grouping will be used on.
      */
     void prepare(WorkerTopologyContext context, GlobalStreamId stream, List<Integer> targetTasks);
 

File: storm-client/src/jvm/org/apache/storm/hooks/IWorkerHook.java
Patch:
@@ -22,15 +22,15 @@
  */
 public interface IWorkerHook extends Serializable {
     /**
-     * This method is called when a worker is started
+     * This method is called when a worker is started.
      *
      * @param topoConf The Storm configuration for this worker
      * @param context  This object can be used to get information about this worker's place within the topology
      */
     void start(Map<String, Object> topoConf, WorkerTopologyContext context);
 
     /**
-     * This method is called right before a worker shuts down
+     * This method is called right before a worker shuts down.
      */
     void shutdown();
 }

File: storm-client/src/jvm/org/apache/storm/messaging/ConnectionWithStatus.java
Patch:
@@ -15,7 +15,7 @@
 public abstract class ConnectionWithStatus implements IConnection {
 
     /**
-     * whether this connection is available to transfer data
+     * whether this connection is available to transfer data.
      */
     public abstract Status status();
 

File: storm-client/src/jvm/org/apache/storm/messaging/DeserializingConnectionCallback.java
Patch:
@@ -36,7 +36,7 @@ public class DeserializingConnectionCallback implements IConnectionCallback, IMe
     private final Map<String, Object> conf;
     private final GeneralTopologyContext context;
 
-    private final ThreadLocal<KryoTupleDeserializer> _des =
+    private final ThreadLocal<KryoTupleDeserializer> des =
         new ThreadLocal<KryoTupleDeserializer>() {
             @Override
             protected KryoTupleDeserializer initialValue() {
@@ -60,7 +60,7 @@ public DeserializingConnectionCallback(final Map<String, Object> conf, final Gen
 
     @Override
     public void recv(List<TaskMessage> batch) {
-        KryoTupleDeserializer des = _des.get();
+        KryoTupleDeserializer des = this.des.get();
         ArrayList<AddressedTuple> ret = new ArrayList<>(batch.size());
         for (TaskMessage message : batch) {
             Tuple tuple = des.deserialize(message.message());

File: storm-client/src/jvm/org/apache/storm/messaging/IConnectionCallback.java
Patch:
@@ -19,7 +19,7 @@
  */
 public interface IConnectionCallback {
     /**
-     * A batch of new messages have arrived to be processed
+     * A batch of new messages have arrived to be processed.
      *
      * @param batch the messages to be processed
      */

File: storm-client/src/jvm/org/apache/storm/messaging/netty/KerberosSaslNettyClientState.java
Patch:
@@ -16,6 +16,7 @@
 
 final class KerberosSaslNettyClientState {
 
-    public static final AttributeKey<KerberosSaslNettyClient> KERBEROS_SASL_NETTY_CLIENT = AttributeKey.valueOf("kerberos.sasl.netty.client");
+    public static final AttributeKey<KerberosSaslNettyClient> KERBEROS_SASL_NETTY_CLIENT =
+            AttributeKey.valueOf("kerberos.sasl.netty.client");
 
 }

File: storm-client/src/jvm/org/apache/storm/messaging/netty/KerberosSaslNettyServerState.java
Patch:
@@ -16,5 +16,6 @@
 
 final class KerberosSaslNettyServerState {
 
-    public static final AttributeKey<KerberosSaslNettyServer> KERBOROS_SASL_NETTY_SERVER = AttributeKey.valueOf("kerboros.sasl.netty.server");
+    public static final AttributeKey<KerberosSaslNettyServer> KERBOROS_SASL_NETTY_SERVER =
+            AttributeKey.valueOf("kerboros.sasl.netty.server");
 }

File: storm-client/src/jvm/org/apache/storm/messaging/netty/SaslNettyClient.java
Patch:
@@ -85,11 +85,11 @@ public byte[] saslResponse(SaslMessageToken saslTokenMessage) {
      */
     private static class SaslClientCallbackHandler implements CallbackHandler {
         /**
-         * Generated username contained in TopologyToken
+         * Generated username contained in TopologyToken.
          */
         private final String userName;
         /**
-         * Generated password contained in TopologyToken
+         * Generated password contained in TopologyToken.
          */
         private final char[] userPassword;
 
@@ -106,7 +106,6 @@ public SaslClientCallbackHandler(String topologyToken, byte[] token) {
          * Implementation used to respond to SASL tokens from server.
          *
          * @param callbacks objects that indicate what credential information the server's SaslServer requires from the client.
-         * @throws UnsupportedCallbackException
          */
         @Override
         public void handle(Callback[] callbacks)

File: storm-client/src/jvm/org/apache/storm/messaging/netty/SaslNettyServer.java
Patch:
@@ -78,12 +78,12 @@ public byte[] response(byte[] token) {
     }
 
     /**
-     * CallbackHandler for SASL DIGEST-MD5 mechanism
+     * CallbackHandler for SASL DIGEST-MD5 mechanism.
      */
     public static class SaslDigestCallbackHandler implements CallbackHandler {
 
         /**
-         * Used to authenticate the clients
+         * Used to authenticate the clients.
          */
         private byte[] userPassword;
         private String userName;

File: storm-client/src/jvm/org/apache/storm/messaging/netty/SaslStormServerHandler.java
Patch:
@@ -128,6 +128,7 @@ public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) {
         ctx.close();
     }
 
+    @SuppressWarnings("checkstyle:AbbreviationAsWordInName")
     private void getSASLCredentials() throws IOException {
         String secretKey;
         topologyName = server.name();

File: storm-client/src/jvm/org/apache/storm/messaging/netty/StormClientHandler.java
Patch:
@@ -27,13 +27,13 @@
 public class StormClientHandler extends ChannelInboundHandlerAdapter {
     private static final Logger LOG = LoggerFactory.getLogger(StormClientHandler.class);
     private final Client client;
-    private final KryoValuesDeserializer _des;
+    private final KryoValuesDeserializer des;
     private final AtomicBoolean[] remoteBpStatus;
 
     StormClientHandler(Client client, AtomicBoolean[] remoteBpStatus, Map<String, Object> conf) {
         this.client = client;
         this.remoteBpStatus = remoteBpStatus;
-        _des = new KryoValuesDeserializer(conf);
+        des = new KryoValuesDeserializer(conf);
     }
 
     @Override
@@ -80,7 +80,7 @@ public void channelRead(ChannelHandlerContext ctx, Object message) throws Except
             if (tm.task() != Server.LOAD_METRICS_TASK_ID) {
                 throw new RuntimeException("Metrics messages are sent to the system task (" + client.getDstAddress() + ") " + tm);
             }
-            List<Object> metrics = _des.deserialize(tm.message());
+            List<Object> metrics = des.deserialize(tm.message());
             if (metrics.size() < 1) {
                 throw new RuntimeException("No metrics data in the metrics message (" + client.getDstAddress() + ") " + metrics);
             }

File: storm-client/src/jvm/org/apache/storm/messaging/netty/StormServerHandler.java
Patch:
@@ -28,11 +28,11 @@ public class StormServerHandler extends ChannelInboundHandlerAdapter {
     private static final Logger LOG = LoggerFactory.getLogger(StormServerHandler.class);
     private static final Set<Class<?>> ALLOWED_EXCEPTIONS = new HashSet<>(Arrays.asList(new Class<?>[]{ IOException.class }));
     private final IServer server;
-    private final AtomicInteger failure_count;
+    private final AtomicInteger failureCount;
 
     public StormServerHandler(IServer server) {
         this.server = server;
-        failure_count = new AtomicInteger(0);
+        failureCount = new AtomicInteger(0);
     }
 
     @Override
@@ -51,7 +51,7 @@ public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception
             server.received(msg, channel.remoteAddress().toString(), channel);
         } catch (InterruptedException e) {
             LOG.info("failed to enqueue a request message", e);
-            failure_count.incrementAndGet();
+            failureCount.incrementAndGet();
         }
     }
 

File: storm-client/src/jvm/org/apache/storm/metric/IEventLogger.java
Patch:
@@ -73,7 +73,7 @@ public List<Object> getValues() {
         }
 
         /**
-         * Returns a default formatted string with fields separated by ","
+         * Returns a default formatted string with fields separated by ",".
          *
          * @return a default formatted string with fields separated by ","
          */

File: storm-client/src/jvm/org/apache/storm/metric/api/IMetric.java
Patch:
@@ -17,6 +17,7 @@
  */
 public interface IMetric {
     /**
+     * Get value and reset.
      * @return an object that will be sent sent to {@link IMetricsConsumer#handleDataPoints(org.apache.storm.metric.api.IMetricsConsumer
      * .TaskInfo,
      *     java.util.Collection)}. If null is returned nothing will be sent. If this value can be reset, like with a counter, a side effect

File: storm-client/src/jvm/org/apache/storm/metric/api/StateMetric.java
Patch:
@@ -13,14 +13,14 @@
 package org.apache.storm.metric.api;
 
 public class StateMetric implements IMetric {
-    private IStatefulObject _obj;
+    private IStatefulObject obj;
 
     public StateMetric(IStatefulObject obj) {
-        _obj = obj;
+        this.obj = obj;
     }
 
     @Override
     public Object getValueAndReset() {
-        return _obj.getState();
+        return obj.getState();
     }
 }

File: storm-client/src/jvm/org/apache/storm/metric/cgroup/CGroupCpu.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.storm.container.cgroup.core.CpuacctCore.StatType;
 
 /**
- * Report CPU used in the cgroup
+ * Report CPU used in the cgroup.
  */
 public class CGroupCpu extends CGroupMetricsBase<Map<String, Long>> {
     long previousSystem = 0;
@@ -34,6 +34,7 @@ public CGroupCpu(Map<String, Object> conf) {
         super(conf, SubSystemType.cpuacct);
     }
 
+    @SuppressWarnings("checkstyle:AbbreviationAsWordInName")
     public synchronized int getUserHZ() throws IOException {
         if (userHz < 0) {
             ProcessBuilder pb = new ProcessBuilder("getconf", "CLK_TCK");
@@ -52,6 +53,7 @@ public Map<String, Long> getDataFrom(CgroupCore core) throws IOException {
         long systemHz = stat.get(StatType.system);
         long userHz = stat.get(StatType.user);
         long user = userHz - previousUser;
+        @SuppressWarnings("checkstyle:VariableDeclarationUsageDistance")
         long sys = systemHz - previousSystem;
         previousUser = userHz;
         previousSystem = systemHz;

File: storm-client/src/jvm/org/apache/storm/metric/cgroup/CGroupMemoryUsage.java
Patch:
@@ -18,7 +18,7 @@
 import org.apache.storm.container.cgroup.core.MemoryCore;
 
 /**
- * Reports the current memory usage of the cgroup for this worker
+ * Reports the current memory usage of the cgroup for this worker.
  */
 public class CGroupMemoryUsage extends CGroupMetricsBase<Long> {
 

File: storm-client/src/jvm/org/apache/storm/metric/internal/MetricStatTimer.java
Patch:
@@ -15,8 +15,8 @@
 import java.util.Timer;
 
 /**
- * Just holds a singleton metric/stat timer for use by metric/stat calculations
+ * Just holds a singleton metric/stat timer for use by metric/stat calculations.
  */
 class MetricStatTimer {
-    static Timer _timer = new Timer("metric/stat timer", true);
+    static Timer timer = new Timer("metric/stat timer", true);
 }

File: storm-client/src/jvm/org/apache/storm/multilang/BoltMsg.java
Patch:
@@ -18,8 +18,9 @@
  * BoltMsg is an object that represents the data sent from a shell component to a bolt process that implements a multi-language protocol. It
  * is the union of all data types that a bolt can receive from Storm.
  *
- * BoltMsgs are objects sent to the ISerializer interface, for serialization according to the wire protocol implemented by the serializer.
- * The BoltMsg class allows for a decoupling between the serialized representation of the data and the data itself.
+ * <p>BoltMsgs are objects sent to the ISerializer interface, for serialization according to the wire protocol
+ * implemented by the serializer. The BoltMsg class allows for a decoupling between the serialized representation of the
+ * data and the data itself.
  */
 public class BoltMsg {
     private String id;

File: storm-client/src/jvm/org/apache/storm/multilang/JsonSerializer.java
Patch:
@@ -123,8 +123,8 @@ public ShellMsg readShellMsg() throws IOException, NoOutputException {
             shellMsg.setTask(0);
         }
 
-        Object need_task_ids = msg.get("need_task_ids");
-        if (need_task_ids == null || ((Boolean) need_task_ids).booleanValue()) {
+        Object needTaskIds = msg.get("need_task_ids");
+        if (needTaskIds == null || ((Boolean) needTaskIds).booleanValue()) {
             shellMsg.setNeedTaskIds(true);
         } else {
             shellMsg.setNeedTaskIds(false);

File: storm-client/src/jvm/org/apache/storm/multilang/SpoutMsg.java
Patch:
@@ -16,7 +16,7 @@
  * SpoutMsg is an object that represents the data sent from a shell spout to a process that implements a multi-language spout. The SpoutMsg
  * is used to send a "next", "ack" or "fail" message to a spout.
  *
- * Spout messages are objects sent to the ISerializer interface, for serialization according to the wire protocol implemented by the
+ * <p>Spout messages are objects sent to the ISerializer interface, for serialization according to the wire protocol implemented by the
  * serializer. The SpoutMsg class allows for a decoupling between the serialized representation of the data and the data itself.
  */
 public class SpoutMsg {

File: storm-client/src/jvm/org/apache/storm/networktopography/DefaultRackDNSToSwitchMapping.java
Patch:
@@ -20,6 +20,7 @@
 /**
  * This class implements the {@link DNSToSwitchMapping} interface It returns the DEFAULT_RACK for every host.
  */
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public final class DefaultRackDNSToSwitchMapping extends AbstractDNSToSwitchMapping {
 
     private Map<String, String> mappingCache = new ConcurrentHashMap<>();

File: storm-client/src/jvm/org/apache/storm/pacemaker/codec/ThriftDecoder.java
Patch:
@@ -63,7 +63,7 @@ protected void decode(ChannelHandlerContext channelHandlerContext, ByteBuf buf,
             return;
         }
 
-        byte serialized[] = new byte[thriftLen];
+        byte[] serialized = new byte[thriftLen];
         buf.readBytes(serialized, 0, thriftLen);
         HBMessage m = (HBMessage) Utils.thriftDeserialize(HBMessage.class, serialized);
 

File: storm-client/src/jvm/org/apache/storm/security/auth/DefaultPrincipalToLocal.java
Patch:
@@ -19,7 +19,7 @@
  */
 public class DefaultPrincipalToLocal implements IPrincipalToLocal {
     /**
-     * Invoked once immediately after construction
+     * Invoked once immediately after construction.
      */
     @Override
     public void prepare(Map<String, Object> topoConf) {

File: storm-client/src/jvm/org/apache/storm/security/auth/IPrincipalToLocal.java
Patch:
@@ -20,7 +20,7 @@
  */
 public interface IPrincipalToLocal {
     /**
-     * Invoked once immediately after construction
+     * Invoked once immediately after construction.
      *
      * @param topoConf Storm configuration
      */
@@ -32,9 +32,9 @@ public interface IPrincipalToLocal {
      * @param principal the principal to convert
      * @return The local user name.
      */
-    default public String toLocal(Principal principal) {
+    default String toLocal(Principal principal) {
         return principal == null ? null : toLocal(principal.getName());
     }
 
-    public String toLocal(String principalName);
+    String toLocal(String principalName);
 }

File: storm-client/src/jvm/org/apache/storm/security/auth/KerberosPrincipalToLocal.java
Patch:
@@ -15,12 +15,12 @@
 import java.util.Map;
 
 /**
- * Map a kerberos principal to a local user
+ * Map a kerberos principal to a local user.
  */
 public class KerberosPrincipalToLocal implements IPrincipalToLocal {
 
     /**
-     * Invoked once immediately after construction
+     * Invoked once immediately after construction.
      *
      * @param topoConf Storm configuration
      */

File: storm-client/src/jvm/org/apache/storm/security/auth/ShellBasedGroupsMapping.java
Patch:
@@ -49,7 +49,7 @@ public ShellBasedGroupsMapping() {
     }
 
     /**
-     * Invoked once immediately after construction
+     * Invoked once immediately after construction.
      *
      * @param topoConf Storm configuration
      */
@@ -61,7 +61,7 @@ public void prepare(Map<String, Object> topoConf) {
     }
 
     /**
-     * Returns list of groups for a user
+     * Returns list of groups for a user.
      *
      * @param user get groups for this user
      * @return list of groups for a given user

File: storm-client/src/jvm/org/apache/storm/security/auth/ThriftServer.java
Patch:
@@ -66,6 +66,7 @@ public void stop() {
     }
 
     /**
+     * Check whether serving.
      * @return true if ThriftServer is listening to requests?
      */
     public boolean isServing() {
@@ -90,6 +91,7 @@ private void handleServerException(Exception ex) {
     }
 
     /**
+     * Get port.
      * @return The port this server is/will be listening on
      */
     public int getPort() {

File: storm-client/src/jvm/org/apache/storm/security/auth/authorizer/DenyAuthorizer.java
Patch:
@@ -17,12 +17,12 @@
 import org.apache.storm.security.auth.ReqContext;
 
 /**
- * An authorization implementation that denies everything, for testing purposes
+ * An authorization implementation that denies everything, for testing purposes.
  */
 public class DenyAuthorizer implements IAuthorizer {
 
     /**
-     * Invoked once immediately after construction
+     * Invoked once immediately after construction.
      *
      * @param conf Storm configuration
      */
@@ -31,7 +31,7 @@ public void prepare(Map<String, Object> conf) {
     }
 
     /**
-     * permit() method is invoked for each incoming Thrift request
+     * permit() method is invoked for each incoming Thrift request.
      *
      * @param context   request context
      * @param operation operation name

File: storm-client/src/jvm/org/apache/storm/security/auth/authorizer/NoopAuthorizer.java
Patch:
@@ -22,7 +22,7 @@
 public class NoopAuthorizer implements IAuthorizer {
 
     /**
-     * Invoked once immediately after construction
+     * Invoked once immediately after construction.
      *
      * @param conf Storm configuration
      */
@@ -31,7 +31,7 @@ public void prepare(Map<String, Object> conf) {
     }
 
     /**
-     * permit() method is invoked for each incoming Thrift request
+     * permit() method is invoked for each incoming Thrift request.
      *
      * @param context   request context includes info about
      * @param operation operation name

File: storm-client/src/jvm/org/apache/storm/security/auth/authorizer/SimpleACLAuthorizer.java
Patch:
@@ -30,6 +30,7 @@
 /**
  * An authorization implementation that simply checks if a user is allowed to perform specific operations.
  */
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public class SimpleACLAuthorizer implements IAuthorizer {
     private static final Logger LOG = LoggerFactory.getLogger(SimpleACLAuthorizer.class);
 

File: storm-client/src/jvm/org/apache/storm/security/auth/authorizer/SimpleWhitelistAuthorizer.java
Patch:
@@ -27,7 +27,7 @@ public class SimpleWhitelistAuthorizer implements IAuthorizer {
     protected Set<String> users;
 
     /**
-     * Invoked once immediately after construction
+     * Invoked once immediately after construction.
      *
      * @param conf Storm configuration
      */
@@ -40,7 +40,7 @@ public void prepare(Map<String, Object> conf) {
     }
 
     /**
-     * `permit()` method is invoked for each incoming Thrift request
+     * `permit()` method is invoked for each incoming Thrift request.
      *
      * @param context   request context includes info about
      * @param operation operation name

File: storm-client/src/jvm/org/apache/storm/security/auth/authorizer/SupervisorSimpleACLAuthorizer.java
Patch:
@@ -30,6 +30,7 @@
 /**
  * An authorization implementation that simply checks if a user is allowed to perform specific operations.
  */
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public class SupervisorSimpleACLAuthorizer implements IAuthorizer {
     private static final Logger LOG = LoggerFactory.getLogger(SupervisorSimpleACLAuthorizer.class);
 

File: storm-client/src/jvm/org/apache/storm/security/auth/kerberos/AutoTGTKrb5LoginModule.java
Patch:
@@ -25,8 +25,9 @@
 
 
 /**
- * Custom LoginModule to enable Auto Login based on cached ticket
+ * Custom LoginModule to enable Auto Login based on cached ticket.
  */
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public class AutoTGTKrb5LoginModule implements LoginModule {
     private static final Logger LOG = LoggerFactory.getLogger(AutoTGTKrb5LoginModule.class);
     protected KerberosTicket kerbTicket = null;

File: storm-client/src/jvm/org/apache/storm/security/auth/kerberos/AutoTGTKrb5LoginModuleTest.java
Patch:
@@ -18,6 +18,7 @@
 /**
  * Custom LoginModule extended for testing.
  */
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public class AutoTGTKrb5LoginModuleTest extends AutoTGTKrb5LoginModule {
 
     public Principal client = null;

File: storm-client/src/jvm/org/apache/storm/security/auth/plain/PlainSaslTransportPlugin.java
Patch:
@@ -40,7 +40,7 @@ public class PlainSaslTransportPlugin extends SaslTransportPlugin {
     protected TTransportFactory getServerTransportFactory(boolean impersonationAllowed) throws IOException {
         //create an authentication callback handler
         CallbackHandler serverCallbackHandler = new SimpleSaslServerCallbackHandler(impersonationAllowed,
-                                                                                    (userName) -> Optional.of("password".toCharArray()));
+            (userName) -> Optional.of("password".toCharArray()));
         if (Security.getProvider(SaslPlainServer.SecurityProvider.SASL_PLAIN_SERVER) == null) {
             Security.addProvider(new SaslPlainServer.SecurityProvider());
         }

File: storm-client/src/jvm/org/apache/storm/security/auth/sasl/SaslTransportPlugin.java
Patch:
@@ -117,6 +117,7 @@ public int getPort() {
      * Processor that pulls the SaslServer object out of the transport, and assumes the remote user's UGI before calling through to the
      * original processor. This is used on the server side to set the UGI for each specific call.
      */
+    @SuppressWarnings("checkstyle:AbbreviationAsWordInName")
     private static class TUGIWrapProcessor implements TProcessor {
         final TProcessor wrapped;
 

File: storm-client/src/jvm/org/apache/storm/serialization/DefaultKryoFactory.java
Patch:
@@ -42,15 +42,15 @@ public void postDecorate(Kryo k, Map<String, Object> conf) {
     }
 
     public static class KryoSerializableDefault extends Kryo {
-        boolean _override = false;
+        boolean override = false;
 
         public void overrideDefault(boolean value) {
-            _override = value;
+            override = value;
         }
 
         @Override
         public Serializer getDefaultSerializer(Class type) {
-            if (_override) {
+            if (override) {
                 return new SerializableSerializer();
             } else {
                 return super.getDefaultSerializer(type);

File: storm-client/src/jvm/org/apache/storm/serialization/GzipBridgeThriftSerializationDelegate.java
Patch:
@@ -49,7 +49,7 @@ public <T> T deserialize(byte[] bytes, Class<T> clazz) {
     }
 
     /**
-     * Looks ahead to see if the GZIP magic constant is heading {@code bytes}
+     * Looks ahead to see if the GZIP magic constant is heading {@code bytes}.
      */
     private boolean isGzipped(byte[] bytes) {
         return (bytes.length > 1) && (bytes[0] == GZIP_MAGIC_FIRST_BYTE)

File: storm-client/src/jvm/org/apache/storm/serialization/SerializationFactory.java
Patch:
@@ -224,9 +224,9 @@ public IdDictionary(StormTopology topology) {
         }
 
         /**
-         * "{:a 1  :b 2} -> {1 :a  2 :b}"
+         * "{:a 1  :b 2} -> {1 :a  2 :b}".
          *
-         * Note: Only one key wins if there are duplicate values. Which key wins is indeterminate: "{:a 1  :b 1} -> {1 :a} *or* {1 :b}"
+         * <p>Note: Only one key wins if there are duplicate values. Which key wins is indeterminate: "{:a 1  :b 1} -> {1 :a} *or* {1 :b}"
          */
         private static <K, V> Map<V, K> simpleReverseMap(Map<K, V> map) {
             Map<V, K> ret = new HashMap<V, K>();

File: storm-client/src/jvm/org/apache/storm/serialization/SerializationRegister.java
Patch:
@@ -26,7 +26,7 @@
  */
 public interface SerializationRegister {
     /**
-     * Register any serializers needed with the kryo instance
+     * Register any serializers needed with the kryo instance.
      *
      * @param kryo what to register the serializers with.
      */

File: storm-client/src/jvm/org/apache/storm/spout/RawMultiScheme.java
Patch:
@@ -18,13 +18,13 @@
 
 package org.apache.storm.spout;
 
+import static java.util.Arrays.asList;
+
 import java.nio.ByteBuffer;
 import java.util.List;
 import org.apache.storm.tuple.Fields;
 import org.apache.storm.utils.Utils;
 
-import static java.util.Arrays.asList;
-
 public class RawMultiScheme implements MultiScheme {
     @Override
     public Iterable<List<Object>> deserialize(ByteBuffer ser) {

File: storm-client/src/jvm/org/apache/storm/state/InMemoryKeyValueStateProvider.java
Patch:
@@ -17,7 +17,7 @@
 import org.apache.storm.task.TopologyContext;
 
 /**
- * Provides {@link InMemoryKeyValueState}
+ * Provides {@link InMemoryKeyValueState}.
  */
 public class InMemoryKeyValueStateProvider implements StateProvider {
     private final ConcurrentHashMap<String, State> states = new ConcurrentHashMap<>();

File: storm-client/src/jvm/org/apache/storm/state/KeyValueState.java
Patch:
@@ -19,15 +19,15 @@
  */
 public interface KeyValueState<K, V> extends State, Iterable<Map.Entry<K, V>> {
     /**
-     * Maps the value with the key
+     * Maps the value with the key.
      *
      * @param key   the key
      * @param value the value
      */
     void put(K key, V value);
 
     /**
-     * Returns the value mapped to the key
+     * Returns the value mapped to the key.
      *
      * @param key the key
      * @return the value or null if no mapping is found
@@ -44,7 +44,7 @@ public interface KeyValueState<K, V> extends State, Iterable<Map.Entry<K, V>> {
     V get(K key, V defaultValue);
 
     /**
-     * Deletes the value mapped to the key, if there is any
+     * Deletes the value mapped to the key, if there is any.
      *
      * @param key the key
      */

File: storm-client/src/jvm/org/apache/storm/state/StateFactory.java
Patch:
@@ -19,7 +19,7 @@
 import org.slf4j.LoggerFactory;
 
 /**
- * A factory for creating {@link State} instances
+ * A factory for creating {@link State} instances.
  */
 public class StateFactory {
     private static final Logger LOG = LoggerFactory.getLogger(StateFactory.class);
@@ -49,8 +49,8 @@ public static State getState(String namespace, Map<String, Object> topoConf, Top
             if (object instanceof StateProvider) {
                 state = ((StateProvider) object).newState(namespace, topoConf, context);
             } else {
-                String msg = "Invalid state provider '" + provider +
-                             "'. Should implement org.apache.storm.state.StateProvider";
+                String msg = "Invalid state provider '" + provider
+                        + "'. Should implement org.apache.storm.state.StateProvider";
                 LOG.error(msg);
                 throw new RuntimeException(msg);
             }

File: storm-client/src/jvm/org/apache/storm/streams/Edge.java
Patch:
@@ -15,7 +15,7 @@
 import java.io.Serializable;
 
 /**
- * An edge connects source and target nodes
+ * An edge connects source and target nodes.
  */
 class Edge implements Serializable {
     private final Node source;

File: storm-client/src/jvm/org/apache/storm/streams/GroupingInfo.java
Patch:
@@ -91,8 +91,8 @@ public int hashCode() {
 
     @Override
     public String toString() {
-        return "GroupingInfo{" +
-               "fields=" + fields +
-               '}';
+        return "GroupingInfo{"
+                + "fields=" + fields
+                + '}';
     }
 }

File: storm-client/src/jvm/org/apache/storm/streams/ProcessorBolt.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.storm.tuple.Tuple;
 
 /**
- * Stream bolt that executes the different processors (except windowed and stateful operations)
+ * Stream bolt that executes the different processors (except windowed and stateful operations).
  */
 class ProcessorBolt extends BaseRichBolt implements StreamBolt {
     private final ProcessorBoltDelegate delegate;

File: storm-client/src/jvm/org/apache/storm/streams/ProcessorBoltDelegate.java
Patch:
@@ -289,8 +289,8 @@ private boolean shouldPunctuate(ProcessorNode processorNode, String sourceStream
             for (String receivedStream : receivedStreams) {
                 Integer expected = streamToInputTaskCount.get(receivedStream);
                 if (expected == null) {
-                    throw new IllegalStateException("Punctuation received on unexpected stream '" + receivedStream +
-                                                    "' for which input task count is not set.");
+                    throw new IllegalStateException("Punctuation received on unexpected stream '" + receivedStream
+                            + "' for which input task count is not set.");
                 }
                 if (punctuationState.get(processorNode, receivedStream) < streamToInputTaskCount.get(receivedStream)) {
                     return false;

File: storm-client/src/jvm/org/apache/storm/streams/SpoutNode.java
Patch:
@@ -24,8 +24,8 @@ class SpoutNode extends Node {
     SpoutNode(IRichSpout spout) {
         super(Utils.DEFAULT_STREAM_ID, getOutputFields(spout, Utils.DEFAULT_STREAM_ID));
         if (outputFields.size() == 0) {
-            throw new IllegalArgumentException("Spout " + spout + " does not declare any fields" +
-                                               "for the stream '" + Utils.DEFAULT_STREAM_ID + "'");
+            throw new IllegalArgumentException("Spout " + spout + " does not declare any fields"
+                    + "for the stream '" + Utils.DEFAULT_STREAM_ID + "'");
         }
         this.spout = spout;
     }

File: storm-client/src/jvm/org/apache/storm/streams/StatefulProcessorBolt.java
Patch:
@@ -98,8 +98,8 @@ private Set<StatefulProcessor<K, V>> getStatefulProcessors(List<ProcessorNode> n
                 statefulProcessors.add((StatefulProcessor<K, V>) node.getProcessor());
                 if (node.getProcessor() instanceof UpdateStateByKeyProcessor) {
                     if (++updateStateByKeyCount > 1) {
-                        throw new IllegalArgumentException("Cannot have more than one updateStateByKey processor " +
-                                                           "in a StatefulProcessorBolt");
+                        throw new IllegalArgumentException("Cannot have more than one updateStateByKey processor "
+                                + "in a StatefulProcessorBolt");
                     }
                 }
 

File: storm-client/src/jvm/org/apache/storm/streams/StreamState.java
Patch:
@@ -15,7 +15,7 @@
 import java.io.Serializable;
 
 /**
- * A wrapper for the stream state which can be used to query the state via {@link Stream#stateQuery(StreamState)}
+ * A wrapper for the stream state which can be used to query the state via {@link Stream#stateQuery(StreamState)}.
  *
  * @param <K> the key type
  * @param <V> the value type

File: storm-client/src/jvm/org/apache/storm/streams/StreamUtil.java
Patch:
@@ -12,13 +12,13 @@
 
 package org.apache.storm.streams;
 
+import static org.apache.storm.streams.WindowNode.PUNCTUATION;
+
 import java.util.ArrayList;
 import java.util.List;
 import org.apache.storm.shade.org.jgrapht.DirectedGraph;
 import org.apache.storm.tuple.Fields;
 
-import static org.apache.storm.streams.WindowNode.PUNCTUATION;
-
 public class StreamUtil {
     @SuppressWarnings("unchecked")
     public static <T> List<T> getParents(DirectedGraph<Node, Edge> graph, Node node) {

File: storm-client/src/jvm/org/apache/storm/streams/WindowedProcessorBolt.java
Patch:
@@ -12,6 +12,8 @@
 
 package org.apache.storm.streams;
 
+import static org.apache.storm.streams.WindowNode.PUNCTUATION;
+
 import java.util.Date;
 import java.util.List;
 import java.util.Map;
@@ -29,8 +31,6 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import static org.apache.storm.streams.WindowNode.PUNCTUATION;
-
 /**
  * Stream bolt that executes windowing operations.
  */

File: storm-client/src/jvm/org/apache/storm/streams/operations/aggregators/LongSum.java
Patch:
@@ -15,7 +15,7 @@
 import org.apache.storm.streams.operations.CombinerAggregator;
 
 /**
- * Computes the long sum of the input values
+ * Computes the long sum of the input values.
  */
 public class LongSum implements CombinerAggregator<Number, Long, Long> {
     @Override

File: storm-client/src/jvm/org/apache/storm/streams/operations/mappers/ValuesMapper.java
Patch:
@@ -22,7 +22,7 @@ public class ValuesMapper implements TupleValueMapper<Values> {
     private final int[] indices;
 
     /**
-     * Constructs a new {@link ValuesMapper} that extracts value from a {@link Tuple} at specified indices
+     * Constructs a new {@link ValuesMapper} that extracts value from a {@link Tuple} at specified indices.
      *
      * @param indices the indices
      */

File: storm-client/src/jvm/org/apache/storm/streams/processors/ChainedProcessorContext.java
Patch:
@@ -19,7 +19,7 @@
 import org.apache.storm.streams.ProcessorNode;
 
 /**
- * A composite context that holds a chain of {@link ProcessorContext}
+ * A composite context that holds a chain of {@link ProcessorContext}.
  */
 public class ChainedProcessorContext implements ProcessorContext {
     private final ProcessorNode processorNode;

File: storm-client/src/jvm/org/apache/storm/streams/processors/CoGroupByKeyProcessor.java
Patch:
@@ -18,7 +18,7 @@
 import org.apache.storm.streams.Pair;
 
 /**
- * co-group by key implementation
+ * co-group by key implementation.
  */
 public class CoGroupByKeyProcessor<K, V1, V2> extends BaseProcessor<Pair<K, ?>> implements BatchProcessor {
     private final String firstStream;

File: storm-client/src/jvm/org/apache/storm/streams/processors/EmittingProcessorContext.java
Patch:
@@ -12,6 +12,8 @@
 
 package org.apache.storm.streams.processors;
 
+import static org.apache.storm.streams.WindowNode.PUNCTUATION;
+
 import java.util.ArrayList;
 import java.util.Collection;
 import java.util.List;
@@ -28,8 +30,6 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import static org.apache.storm.streams.WindowNode.PUNCTUATION;
-
 /**
  * A context that emits the results to downstream processors which are in another bolt.
  */

File: storm-client/src/jvm/org/apache/storm/streams/processors/ForwardingProcessorContext.java
Patch:
@@ -12,14 +12,14 @@
 
 package org.apache.storm.streams.processors;
 
+import static org.apache.storm.streams.WindowNode.PUNCTUATION;
+
 import java.util.Set;
 import org.apache.storm.shade.com.google.common.collect.Multimap;
 import org.apache.storm.streams.ProcessorNode;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import static org.apache.storm.streams.WindowNode.PUNCTUATION;
-
 /**
  * A context that emits the results to downstream processors which are in the same bolt.
  */

File: storm-client/src/jvm/org/apache/storm/streams/processors/JoinProcessor.java
Patch:
@@ -90,11 +90,11 @@ public String getRightStream() {
     private void joinAndForward(List<Pair<K, V1>> leftRows, List<Pair<K, V2>> rightRows) {
         if (leftRows.size() < rightRows.size()) {
             for (Tuple3<K, V1, V2> res : join(getJoinTable(leftRows), rightRows, leftType, rightType)) {
-                context.forward(Pair.of(res._1, valueJoiner.apply(res._2, res._3)));
+                context.forward(Pair.of(res.value1, valueJoiner.apply(res.value2, res.value3)));
             }
         } else {
             for (Tuple3<K, V2, V1> res : join(getJoinTable(rightRows), leftRows, rightType, leftType)) {
-                context.forward(Pair.of(res._1, valueJoiner.apply(res._3, res._2)));
+                context.forward(Pair.of(res.value1, valueJoiner.apply(res.value3, res.value2)));
             }
         }
     }

File: storm-client/src/jvm/org/apache/storm/streams/processors/StatefulProcessor.java
Patch:
@@ -22,7 +22,7 @@
  */
 public interface StatefulProcessor<K, V> {
     /**
-     * Initialize the state of the processor with the given {@link KeyValueState}
+     * Initialize the state of the processor with the given {@link KeyValueState}.
      *
      * @param keyValueState the key-value state
      */

File: storm-client/src/jvm/org/apache/storm/streams/windowing/TumblingWindows.java
Patch:
@@ -12,11 +12,11 @@
 
 package org.apache.storm.streams.windowing;
 
-import org.apache.storm.topology.base.BaseWindowedBolt;
-
 import static org.apache.storm.topology.base.BaseWindowedBolt.Count;
 import static org.apache.storm.topology.base.BaseWindowedBolt.Duration;
 
+import org.apache.storm.topology.base.BaseWindowedBolt;
+
 /**
  * A tumbling window specification. The window tumbles after the specified window length.
  *

File: storm-client/src/jvm/org/apache/storm/streams/windowing/Window.java
Patch:
@@ -12,10 +12,10 @@
 
 package org.apache.storm.streams.windowing;
 
-import java.io.Serializable;
-
 import static org.apache.storm.topology.base.BaseWindowedBolt.Duration;
 
+import java.io.Serializable;
+
 /**
  * The window specification within {@link org.apache.storm.streams.Stream}.
  *

File: storm-client/src/jvm/org/apache/storm/task/IMetricsContext.java
Patch:
@@ -26,18 +26,21 @@
 
 public interface IMetricsContext {
     /**
+     * Register metric.
      * @deprecated in favor of metrics v2 (the non-deprecated methods on this class)
      */
     @Deprecated
     <T extends IMetric> T registerMetric(String name, T metric, int timeBucketSizeInSecs);
 
     /**
+     * Register metric.
      * @deprecated in favor of metrics v2 (the non-deprecated methods on this class)
      */
     @Deprecated
     ReducedMetric registerMetric(String name, IReducer reducer, int timeBucketSizeInSecs);
 
     /**
+     * Register metric.
      * @deprecated in favor of metrics v2 (the non-deprecated methods on this class)
      */
     @Deprecated

File: storm-client/src/jvm/org/apache/storm/testing/AlternateRackDNSToSwitchMapping.java
Patch:
@@ -23,6 +23,7 @@
 /**
  * This class implements the {@link DNSToSwitchMapping} interface It alternates bewteen RACK1 and RACK2 for the hosts.
  */
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public final class AlternateRackDNSToSwitchMapping extends AbstractDNSToSwitchMapping {
 
     private Map<String, String> mappingCache = new ConcurrentHashMap<String, String>();

File: storm-client/src/jvm/org/apache/storm/testing/BoltTracker.java
Patch:
@@ -19,16 +19,16 @@
 
 
 public class BoltTracker extends NonRichBoltTracker implements IRichBolt {
-    IRichBolt _richDelegate;
+    IRichBolt richDelegate;
 
     public BoltTracker(IRichBolt delegate, String id) {
         super(delegate, id);
-        _richDelegate = delegate;
+        richDelegate = delegate;
     }
 
     @Override
     public void declareOutputFields(OutputFieldsDeclarer declarer) {
-        _richDelegate.declareOutputFields(declarer);
+        richDelegate.declareOutputFields(declarer);
     }
 
     @Override

File: storm-client/src/jvm/org/apache/storm/testing/IdentityBolt.java
Patch:
@@ -19,10 +19,10 @@
 import org.apache.storm.tuple.Tuple;
 
 public class IdentityBolt extends BaseBasicBolt {
-    Fields _fields;
+    Fields fields;
 
     public IdentityBolt(Fields fields) {
-        _fields = fields;
+        this.fields = fields;
     }
 
     @Override
@@ -32,6 +32,6 @@ public void execute(Tuple input, BasicOutputCollector collector) {
 
     @Override
     public void declareOutputFields(OutputFieldsDeclarer declarer) {
-        declarer.declare(_fields);
+        declarer.declare(fields);
     }
 }

File: storm-client/src/jvm/org/apache/storm/testing/PrepareBatchBolt.java
Patch:
@@ -23,15 +23,15 @@
 
 
 public class PrepareBatchBolt extends BaseBasicBolt {
-    Fields _outFields;
+    Fields outFields;
 
     public PrepareBatchBolt(Fields outFields) {
-        _outFields = outFields;
+        this.outFields = outFields;
     }
 
     @Override
     public void declareOutputFields(OutputFieldsDeclarer declarer) {
-        declarer.declare(_outFields);
+        declarer.declare(outFields);
     }
 
     @Override

File: storm-client/src/jvm/org/apache/storm/testing/PythonShellMetricsBolt.java
Patch:
@@ -41,8 +41,8 @@ public PythonShellMetricsBolt(String command, String file) {
     public void prepare(Map<String, Object> topoConf, TopologyContext context, OutputCollector collector) {
         super.prepare(topoConf, context, collector);
 
-        CountShellMetric cMetric = new CountShellMetric();
-        context.registerMetric("my-custom-shell-metric", cMetric, 5);
+        CountShellMetric countShellMetric = new CountShellMetric();
+        context.registerMetric("my-custom-shell-metric", countShellMetric, 5);
     }
 
     @Override

File: storm-client/src/jvm/org/apache/storm/testing/PythonShellMetricsSpout.java
Patch:
@@ -42,8 +42,8 @@ public PythonShellMetricsSpout(String command, String file) {
     public void open(Map<String, Object> conf, TopologyContext context, SpoutOutputCollector collector) {
         super.open(conf, context, collector);
 
-        CountShellMetric cMetric = new CountShellMetric();
-        context.registerMetric("my-custom-shellspout-metric", cMetric, 5);
+        CountShellMetric countShellMetric = new CountShellMetric();
+        context.registerMetric("my-custom-shellspout-metric", countShellMetric, 5);
     }
 
     @Override

File: storm-client/src/jvm/org/apache/storm/topology/IBasicBolt.java
Patch:
@@ -22,7 +22,7 @@ public interface IBasicBolt extends IComponent {
     /**
      * Process the input tuple and optionally emit new tuples based on the input tuple.
      *
-     * All acking is managed for you. Throw a FailedException if you want to fail the tuple.
+     * <p>All acking is managed for you. Throw a FailedException if you want to fail the tuple.
      */
     void execute(Tuple input, BasicOutputCollector collector);
 

File: storm-client/src/jvm/org/apache/storm/topology/IStatefulBolt.java
Patch:
@@ -26,16 +26,19 @@
  */
 public interface IStatefulBolt<T extends State> extends IStatefulComponent<T> {
     /**
+     * Analogue to bolt function.
      * @see org.apache.storm.task.IBolt#prepare(Map, TopologyContext, OutputCollector)
      */
     void prepare(Map<String, Object> topoConf, TopologyContext context, OutputCollector collector);
 
     /**
+     * Analogue to bolt function.
      * @see org.apache.storm.task.IBolt#execute(Tuple)
      */
     void execute(Tuple input);
 
     /**
+     * Analogue to bolt function.
      * @see org.apache.storm.task.IBolt#cleanup()
      */
     void cleanup();

File: storm-client/src/jvm/org/apache/storm/topology/PersistentWindowedBoltExecutor.java
Patch:
@@ -12,6 +12,8 @@
 
 package org.apache.storm.topology;
 
+import static org.apache.storm.windowing.persistence.WindowState.WindowPartition;
+
 import java.util.ArrayList;
 import java.util.Deque;
 import java.util.HashMap;
@@ -38,8 +40,6 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import static org.apache.storm.windowing.persistence.WindowState.WindowPartition;
-
 /**
  * Wraps a {@link IStatefulWindowedBolt} and handles the execution. Uses state and the underlying checkpointing mechanisms to save the
  * tuples in window to state. The tuples are also kept in-memory by transparently caching the window partitions and checkpointing them as

File: storm-client/src/jvm/org/apache/storm/topology/SharedOffHeapWithinNode.java
Patch:
@@ -16,13 +16,13 @@
 import org.apache.storm.utils.Utils;
 
 /**
- * A request for a shared memory region off heap between workers on a node
+ * A request for a shared memory region off heap between workers on a node.
  */
 public class SharedOffHeapWithinNode extends SharedMemory {
     private static final long serialVersionUID = 1L;
 
     /**
-     * Create a new request for a shared memory region off heap between workers on a node
+     * Create a new request for a shared memory region off heap between workers on a node.
      *
      * @param amount the number of MB to share
      * @param name   the name of the shared region (for tracking purposes)

File: storm-client/src/jvm/org/apache/storm/topology/SharedOffHeapWithinWorker.java
Patch:
@@ -20,7 +20,7 @@
  */
 public class SharedOffHeapWithinWorker extends SharedMemory {
     /**
-     * Create a new request for a shared memory region off heap within a worker
+     * Create a new request for a shared memory region off heap within a worker.
      *
      * @param amount the number of MB to share
      * @param name   the name of the shared region (for tracking purposes)

File: storm-client/src/jvm/org/apache/storm/topology/SharedOnHeap.java
Patch:
@@ -22,7 +22,7 @@ public class SharedOnHeap extends SharedMemory {
     private static final long serialVersionUID = 1L;
 
     /**
-     * Create a new request for a shared memory region on heap
+     * Create a new request for a shared memory region on heap.
      *
      * @param amount the number of MB to share on heap
      * @param name   the name of the shared region (for tracking purposes)

File: storm-client/src/jvm/org/apache/storm/topology/TupleFieldTimestampExtractor.java
Patch:
@@ -36,8 +36,8 @@ public long extractTimestamp(Tuple tuple) {
 
     @Override
     public String toString() {
-        return "TupleFieldTimestampExtractor{" +
-               "fieldName='" + fieldName + '\'' +
-               '}';
+        return "TupleFieldTimestampExtractor{"
+                + "fieldName='" + fieldName + '\''
+                + '}';
     }
 }

File: storm-client/src/jvm/org/apache/storm/topology/base/BaseTickTupleAwareRichBolt.java
Patch:
@@ -43,7 +43,7 @@ public void execute(final Tuple tuple) {
      * Process a single tick tuple of input. Tick tuple doesn't need to be acked. It provides default "DO NOTHING" implementation for
      * convenient. Override this method if needed.
      *
-     * More details on {@link org.apache.storm.task.IBolt#execute(Tuple)}.
+     * <p>More details on {@link org.apache.storm.task.IBolt#execute(Tuple)}.
      *
      * @param tuple The input tuple to be processed.
      */

File: examples/storm-redis-examples/src/main/java/org/apache/storm/redis/tools/Base64ToBinaryStateMigrationUtil.java
Patch:
@@ -38,7 +38,6 @@
 
 import redis.clients.util.SafeEncoder;
 
-
 public class Base64ToBinaryStateMigrationUtil {
     private static final Logger LOG = LoggerFactory.getLogger(Base64ToBinaryStateMigrationUtil.class);
     private static final String OPTION_REDIS_HOST_SHORT = "h";

File: examples/storm-redis-examples/src/main/java/org/apache/storm/redis/topology/PersistentWordCount.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.redis.topology;
 
 import org.apache.storm.Config;
@@ -36,8 +37,6 @@ public class PersistentWordCount {
     private static final int TEST_REDIS_PORT = 6379;
 
     public static void main(String[] args) throws Exception {
-        Config config = new Config();
-
         String host = TEST_REDIS_HOST;
         int port = TEST_REDIS_PORT;
 
@@ -68,6 +67,7 @@ public static void main(String[] args) throws Exception {
             System.out.println("Usage: PersistentWordCount <redis host> <redis port> (topology name)");
             return;
         }
+        Config config = new Config();
         StormSubmitter.submitTopology(topoName, config, builder.createTopology());
     }
 

File: examples/storm-redis-examples/src/main/java/org/apache/storm/redis/trident/WordCountStoreMapper.java
Patch:
@@ -15,11 +15,12 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.redis.trident;
 
-import org.apache.storm.tuple.ITuple;
 import org.apache.storm.redis.common.mapper.RedisDataTypeDescription;
 import org.apache.storm.redis.common.mapper.RedisStoreMapper;
+import org.apache.storm.tuple.ITuple;
 
 public class WordCountStoreMapper implements RedisStoreMapper {
     @Override

File: examples/storm-redis-examples/src/main/java/org/apache/storm/redis/trident/WordCountTridentRedis.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.redis.trident;
 
 import org.apache.storm.Config;
@@ -34,7 +35,8 @@
 import org.apache.storm.tuple.Values;
 
 public class WordCountTridentRedis {
-    public static StormTopology buildTopology(String redisHost, Integer redisPort){
+
+    public static StormTopology buildTopology(String redisHost, Integer redisPort) {
         Fields fields = new Fields("word", "count");
         FixedBatchSpout spout = new FixedBatchSpout(fields, 4,
                 new Values("storm", 1),

File: examples/storm-redis-examples/src/main/java/org/apache/storm/redis/trident/WordCountTridentRedisMap.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.redis.trident;
 
 import org.apache.storm.Config;
@@ -34,7 +35,8 @@
 import org.apache.storm.tuple.Values;
 
 public class WordCountTridentRedisMap {
-    public static StormTopology buildTopology(String redisHost, Integer redisPort){
+
+    public static StormTopology buildTopology(String redisHost, Integer redisPort) {
         Fields fields = new Fields("word", "count");
         FixedBatchSpout spout = new FixedBatchSpout(fields, 4,
                 new Values("storm", 1),

File: examples/storm-mongodb-examples/src/main/java/org/apache/storm/mongodb/topology/InsertWordCount.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.mongodb.topology;
 
 import org.apache.storm.Config;
@@ -35,8 +36,6 @@ public class InsertWordCount {
     
 
     public static void main(String[] args) throws Exception {
-        Config config = new Config();
-
         String url = TEST_MONGODB_URL;
         String collectionName = TEST_MONGODB_COLLECTION_NAME;
 
@@ -67,6 +66,7 @@ public static void main(String[] args) throws Exception {
             System.out.println("Usage: InsertWordCount <mongodb url> <mongodb collection> [topology name]");
             return;
         }
+        Config config = new Config();
         StormSubmitter.submitTopology(topoName, config, builder.createTopology());
     }
 }

File: examples/storm-mongodb-examples/src/main/java/org/apache/storm/mongodb/topology/LookupWordCount.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.mongodb.topology;
 
 import org.apache.storm.Config;
@@ -36,8 +37,6 @@ public class LookupWordCount {
     private static final String TEST_MONGODB_COLLECTION_NAME = "wordcount";
 
     public static void main(String[] args) throws Exception {
-        Config config = new Config();
-
         String url = TEST_MONGODB_URL;
         String collectionName = TEST_MONGODB_COLLECTION_NAME;
 
@@ -70,7 +69,8 @@ public static void main(String[] args) throws Exception {
             System.out.println("Usage: LookupWordCount <mongodb url> <mongodb collection> [topology name]");
             return;
         }
-        
+
+        Config config = new Config();
         StormSubmitter.submitTopology(topoName, config, builder.createTopology());
     }
 }

File: examples/storm-mongodb-examples/src/main/java/org/apache/storm/mongodb/topology/UpdateWordCount.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.mongodb.topology;
 
 import org.apache.storm.Config;
@@ -37,8 +38,6 @@ public class UpdateWordCount {
     
 
     public static void main(String[] args) throws Exception {
-        Config config = new Config();
-
         String url = TEST_MONGODB_URL;
         String collectionName = TEST_MONGODB_COLLECTION_NAME;
 
@@ -78,6 +77,7 @@ public static void main(String[] args) throws Exception {
             System.out.println("Usage: UpdateWordCount <mongodb url> <mongodb collection> [topology name]");
             return;
         }
+        Config config = new Config();
         StormSubmitter.submitTopology(topoName, config, builder.createTopology());
     }
 }

File: examples/storm-mongodb-examples/src/main/java/org/apache/storm/mongodb/trident/WordCountTrident.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.mongodb.trident;
 
 import org.apache.storm.Config;
@@ -36,7 +37,7 @@
 
 public class WordCountTrident {
 
-    public static StormTopology buildTopology(String url, String collectionName){
+    public static StormTopology buildTopology(String url, String collectionName) {
         Fields fields = new Fields("word", "count");
         FixedBatchSpout spout = new FixedBatchSpout(fields, 4,
                 new Values("storm", 1),

File: examples/storm-hdfs-examples/src/main/java/org/apache/storm/hdfs/bolt/HdfsFileTopology.java
Patch:
@@ -101,6 +101,7 @@ public static void waitForSeconds(int seconds) {
         try {
             Thread.sleep(seconds * 1000);
         } catch (InterruptedException e) {
+            //ignore
         }
     }
 

File: examples/storm-hdfs-examples/src/main/java/org/apache/storm/hdfs/bolt/SequenceFileTopology.java
Patch:
@@ -104,6 +104,7 @@ public static void waitForSeconds(int seconds) {
         try {
             Thread.sleep(seconds * 1000);
         } catch (InterruptedException e) {
+            //ignore
         }
     }
 

File: examples/storm-opentsdb-examples/src/main/java/org/apache/storm/opentsdb/SampleOpenTsdbBoltTopology.java
Patch:
@@ -16,6 +16,7 @@
  * specific language governing permissions and limitations
  * under the License.
  */
+
 package org.apache.storm.opentsdb;
 
 import java.util.Collections;
@@ -33,7 +34,7 @@
 public class SampleOpenTsdbBoltTopology {
 
     public static void main(String[] args) throws Exception {
-        if(args.length == 0) {
+        if (args.length == 0) {
             throw new IllegalArgumentException("There should be at least one argument. Run as `SampleOpenTsdbBoltTopology <tsdb-url>`");
         }
 

File: examples/storm-opentsdb-examples/src/main/java/org/apache/storm/opentsdb/SampleOpenTsdbTridentTopology.java
Patch:
@@ -16,6 +16,7 @@
  * specific language governing permissions and limitations
  * under the License.
  */
+
 package org.apache.storm.opentsdb;
 
 import java.util.Collections;
@@ -40,7 +41,7 @@ public class SampleOpenTsdbTridentTopology {
     private static final Logger LOG = LoggerFactory.getLogger(SampleOpenTsdbTridentTopology.class);
 
     public static void main(String[] args) throws Exception {
-        if(args.length == 0) {
+        if (args.length == 0) {
             throw new IllegalArgumentException("There should be at least one argument. Run as `SampleOpenTsdbTridentTopology <tsdb-url>`");
         }
 

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/avro/AbstractAvroSerializer.java
Patch:
@@ -57,7 +57,7 @@ public void write(Kryo kryo, Output output, GenericContainer record) {
     }
 
     @Override
-    public GenericContainer read(Kryo kryo, Input input, Class<GenericContainer> aClass) {
+    public GenericContainer read(Kryo kryo, Input input, Class<GenericContainer> someClass) {
         Schema theSchema = this.getSchema(input.readString());
         GenericDatumReader<GenericContainer> reader = new GenericDatumReader<>(theSchema);
         Decoder decoder = DecoderFactory

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/avro/ConfluentAvroSerializer.java
Patch:
@@ -27,7 +27,7 @@
  */
 public class ConfluentAvroSerializer extends AbstractAvroSerializer {
 
-    final private String url;
+    private final String url;
     private SchemaRegistryClient theClient;
 
     /**

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/avro/FixedAvroSerializer.java
Patch:
@@ -30,7 +30,7 @@
  */
 public class FixedAvroSerializer extends AbstractAvroSerializer {
 
-    private final static String FP_ALGO = "CRC-64-AVRO";
+    private static final String FP_ALGO = "CRC-64-AVRO";
     final Map<String, Schema> fingerprint2schemaMap = new HashMap<>();
     final Map<Schema, String> schema2fingerprintMap = new HashMap<>();
 

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/bolt/SequenceFileBolt.java
Patch:
@@ -114,7 +114,9 @@ public SequenceFileBolt withMaxOpenFiles(int maxOpenFiles) {
     @Override
     public void doPrepare(Map<String, Object> conf, TopologyContext topologyContext, OutputCollector collector) throws IOException {
         LOG.info("Preparing Sequence File Bolt...");
-        if (this.format == null) throw new IllegalStateException("SequenceFormat must be specified.");
+        if (this.format == null) {
+            throw new IllegalStateException("SequenceFormat must be specified.");
+        }
 
         this.fs = FileSystem.get(URI.create(this.fsUrl), hdfsConfig);
         this.codecFactory = new CompressionCodecFactory(hdfsConfig);

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/bolt/format/FileNameFormat.java
Patch:
@@ -28,7 +28,6 @@ public interface FileNameFormat extends Serializable {
      * Returns the filename the HdfsBolt will create.
      * @param rotation the current file rotation number (incremented on every rotation)
      * @param timeStamp current time in milliseconds when the rotation occurs
-     * @return
      */
     String getName(long rotation, long timeStamp);
 

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/bolt/format/SimpleFileNameFormat.java
Patch:
@@ -73,9 +73,7 @@ public SimpleFileNameFormat withPath(String path) {
      * $COMPONENT - component id<br/>
      * $TASK - task id<br/>
      *
-     * @param name
-     *            file name
-     * @return
+     * @param name file name
      */
     public SimpleFileNameFormat withName(String name) {
         this.name = name;

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/bolt/rotation/FileSizeRotationPolicy.java
Patch:
@@ -12,7 +12,6 @@
 
 package org.apache.storm.hdfs.bolt.rotation;
 
-
 import org.apache.storm.tuple.Tuple;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -21,7 +20,7 @@
  * File rotation policy that will rotate files when a certain
  * file size is reached.
  *
- * For example:
+ * <p>For example:
  * <pre>
  *     // rotate when files reach 5MB
  *     FileSizeRotationPolicy policy =
@@ -34,6 +33,7 @@ public class FileSizeRotationPolicy implements FileRotationPolicy {
     private long maxBytes;
     private long lastOffset = 0;
     private long currentBytesWritten = 0;
+
     public FileSizeRotationPolicy(float count, Units units) {
         this.maxBytes = (long) (count * units.getByteCount());
     }

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/bolt/sync/CountSyncPolicy.java
Patch:
@@ -12,7 +12,6 @@
 
 package org.apache.storm.hdfs.bolt.sync;
 
-
 import org.apache.storm.tuple.Tuple;
 
 /**

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/common/AvroGenericRecordHDFSWriter.java
Patch:
@@ -12,7 +12,6 @@
 
 package org.apache.storm.hdfs.common;
 
-
 import java.io.IOException;
 import java.util.EnumSet;
 import org.apache.avro.Schema;
@@ -28,6 +27,7 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public class AvroGenericRecordHDFSWriter extends AbstractHDFSWriter {
 
     private static final Logger LOG = LoggerFactory.getLogger(AvroGenericRecordHDFSWriter.class);

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/common/HDFSWriter.java
Patch:
@@ -23,6 +23,7 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public class HDFSWriter extends AbstractHDFSWriter {
 
     private static final Logger LOG = LoggerFactory.getLogger(HDFSWriter.class);

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/common/Partitioner.java
Patch:
@@ -21,10 +21,9 @@ public interface Partitioner extends Serializable {
      * Return a relative path that the tuple should be written to. For example, if an HdfsBolt were configured to write
      * to /common/output and a partitioner returned "/foo" then the bolt should open a file in "/common/output/foo"
      *
-     * A best practice is to use Path.SEPARATOR instead of a literal "/"
+     * <p>A best practice is to use Path.SEPARATOR instead of a literal "/"
      *
      * @param tuple The tuple for which the relative path is being calculated.
-     * @return
      */
     public String getPartitionPath(final Tuple tuple);
 }

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/common/rotation/RotationAction.java
Patch:
@@ -18,7 +18,6 @@
 
 package org.apache.storm.hdfs.common.rotation;
 
-
 import java.io.IOException;
 import java.io.Serializable;
 import org.apache.hadoop.fs.FileSystem;

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/spout/FileOffset.java
Patch:
@@ -24,7 +24,9 @@
  */
 
 interface FileOffset extends Comparable<FileOffset>, Cloneable {
-    /** tests if rhs == currOffset+1 */
+    /**
+     * tests if rhs == currOffset+1.
+     */
     boolean isNextOffset(FileOffset rhs);
 
     FileOffset clone();

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/spout/FileReader.java
Patch:
@@ -26,10 +26,9 @@ interface FileReader {
     FileOffset getFileOffset();
 
     /**
-     * Get the next tuple from the file
+     * Get the next tuple from the file.
      *
      * @return null if no more data
-     * @throws IOException
      */
     List<Object> next() throws IOException, ParseException;
 

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/trident/format/FileNameFormat.java
Patch:
@@ -17,7 +17,6 @@
 
 /**
  * Formatter interface for determining HDFS file names.
- *
  */
 public interface FileNameFormat extends Serializable {
 
@@ -27,7 +26,6 @@ public interface FileNameFormat extends Serializable {
      * Returns the filename the HdfsBolt will create.
      * @param rotation the current file rotation number (incremented on every rotation)
      * @param timeStamp current time in milliseconds when the rotation occurs
-     * @return
      */
     String getName(long rotation, long timeStamp);
 

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/trident/format/SimpleFileNameFormat.java
Patch:
@@ -68,9 +68,7 @@ public SimpleFileNameFormat withPath(String path) {
      * $HOST - local host name<br/>
      * $PARTITION - partition index<br/>
      *
-     * @param name
-     *            file name
-     * @return
+     * @param name file name
      */
     public SimpleFileNameFormat withName(String name) {
         this.name = name;

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/trident/rotation/FileRotationPolicy.java
Patch:
@@ -18,11 +18,11 @@
 /**
  * Used by the HdfsBolt to decide when to rotate files.
  *
- * The HdfsBolt will call the <code>mark()</code> method for every
+ * <p>The HdfsBolt will call the <code>mark()</code> method for every
  * tuple received. If the <code>mark()</code> method returns
  * <code>true</code> the HdfsBolt will perform a file rotation.
  *
- * After file rotation, the HdfsBolt will call the <code>reset()</code>
+ * <p>After file rotation, the HdfsBolt will call the <code>reset()</code>
  * method.
  */
 public interface FileRotationPolicy extends Serializable {

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/trident/rotation/FileSizeRotationPolicy.java
Patch:
@@ -12,7 +12,6 @@
 
 package org.apache.storm.hdfs.trident.rotation;
 
-
 import org.apache.storm.trident.tuple.TridentTuple;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -21,19 +20,19 @@
  * File rotation policy that will rotate files when a certain
  * file size is reached.
  *
- * For example:
+ * <p>For example:
  * <pre>
  *     // rotate when files reach 5MB
  *     FileSizeRotationPolicy policy =
  *          new FileSizeRotationPolicy(5.0, Units.MB);
  * </pre>
- *
  */
 public class FileSizeRotationPolicy implements FileRotationPolicy {
     private static final Logger LOG = LoggerFactory.getLogger(FileSizeRotationPolicy.class);
     private long maxBytes;
     private long lastOffset = 0;
     private long currentBytesWritten = 0;
+
     public FileSizeRotationPolicy(float count, Units units) {
         this.maxBytes = (long) (count * units.getByteCount());
     }

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/trident/rotation/TimedRotationPolicy.java
Patch:
@@ -23,12 +23,12 @@
 import java.util.concurrent.atomic.AtomicBoolean;
 import org.apache.storm.trident.tuple.TridentTuple;
 
-
 public class TimedRotationPolicy implements FileRotationPolicy {
 
     private long interval;
     private Timer rotationTimer;
     private AtomicBoolean rotationTimerTriggered = new AtomicBoolean();
+
     public TimedRotationPolicy(float count, TimeUnit units) {
         this.interval = (long) (count * units.getMilliSeconds());
     }

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/trident/sync/CountSyncPolicy.java
Patch:
@@ -12,7 +12,6 @@
 
 package org.apache.storm.hdfs.trident.sync;
 
-
 import org.apache.storm.trident.tuple.TridentTuple;
 
 /**

File: external/storm-hdfs/src/test/java/org/apache/storm/hdfs/spout/TestFileLock.java
Patch:
@@ -245,7 +245,7 @@ public void testStaleLockDetection_MultipleLocks() throws Exception {
 
             expired = FileLock.locateOldestExpiredLock(fs, locksDir, LOCK_EXPIRY_SEC);
             Assert.assertNotNull(expired);
-            Assert.assertEquals("spout3", expired.getValue().componentID);
+            Assert.assertEquals("spout3", expired.getValue().componentId);
         } finally {
             lock1.release();
             lock2.release();

File: external/storm-jdbc/src/main/java/org/apache/storm/jdbc/common/ConnectionProvider.java
Patch:
@@ -25,7 +25,7 @@ public interface ConnectionProvider extends Serializable {
     void prepare();
 
     /**
-     *
+     * Get connection.
      * @return a DB connection over which the queries can be executed.
      */
     Connection getConnection();

File: external/storm-jdbc/src/main/java/org/apache/storm/jdbc/common/HikariCPConnectionProvider.java
Patch:
@@ -21,14 +21,15 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public class HikariCPConnectionProvider implements ConnectionProvider {
     private static final Logger LOG = LoggerFactory.getLogger(HikariCPConnectionProvider.class);
 
     private Map<String, Object> configMap;
     private transient HikariDataSource dataSource;
 
-    public HikariCPConnectionProvider(Map<String, Object> hikariCPConfigMap) {
-        this.configMap = hikariCPConfigMap;
+    public HikariCPConnectionProvider(Map<String, Object> configMap) {
+        this.configMap = configMap;
     }
 
     @Override

File: external/storm-jdbc/src/main/java/org/apache/storm/jdbc/mapper/JdbcLookupMapper.java
Patch:
@@ -31,7 +31,6 @@ public interface JdbcLookupMapper extends JdbcMapper {
 
     /**
      * declare what are the fields that this code will output.
-     * @param declarer
      */
     void declareOutputFields(OutputFieldsDeclarer declarer);
 }

File: external/storm-jdbc/src/main/java/org/apache/storm/jdbc/mapper/JdbcMapper.java
Patch:
@@ -19,8 +19,7 @@
 
 public interface JdbcMapper extends Serializable {
     /**
-     *
-     * @param tuple
+     * Get columns.
      * @return list of columns that represents one row in a DB table.
      */
     List<Column> getColumns(ITuple tuple);

File: external/storm-jdbc/src/main/java/org/apache/storm/jdbc/mapper/SimpleJdbcLookupMapper.java
Patch:
@@ -12,7 +12,6 @@
 
 package org.apache.storm.jdbc.mapper;
 
-
 import java.util.ArrayList;
 import java.util.List;
 import org.apache.commons.lang.Validate;

File: external/storm-jdbc/src/main/java/org/apache/storm/jdbc/trident/state/JdbcStateFactory.java
Patch:
@@ -26,7 +26,7 @@ public JdbcStateFactory(JdbcState.Options options) {
     }
 
     @Override
-    public State makeState(Map<String, Object> map, IMetricsContext iMetricsContext, int partitionIndex, int numPartitions) {
+    public State makeState(Map<String, Object> map, IMetricsContext metricsContext, int partitionIndex, int numPartitions) {
         JdbcState state = new JdbcState(map, partitionIndex, numPartitions, options);
         state.prepare();
         return state;

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/AbstractExecutionResultHandler.java
Patch:
@@ -48,6 +48,8 @@ public void onThrowable(Throwable t, OutputCollector collector, Tuple i) {
 
     @Override
     public void onThrowable(Throwable t, OutputCollector collector, List<Tuple> tl) {
-        for (Tuple i : tl) onThrowable(t, collector, i);
+        for (Tuple i : tl) {
+            onThrowable(t, collector, i);
+        }
     }
 }

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/BaseExecutionResultHandler.java
Patch:
@@ -19,6 +19,7 @@
 import com.datastax.driver.core.exceptions.WriteTimeoutException;
 import org.apache.storm.task.OutputCollector;
 import org.apache.storm.tuple.Tuple;
+import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
 /**
@@ -29,7 +30,7 @@
  */
 public class BaseExecutionResultHandler extends AbstractExecutionResultHandler {
 
-    private final static org.slf4j.Logger LOG = LoggerFactory.getLogger(BaseExecutionResultHandler.class);
+    private static final Logger LOG = LoggerFactory.getLogger(BaseExecutionResultHandler.class);
 
     /**
      * {@inheritDoc}

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/CassandraContext.java
Patch:
@@ -24,9 +24,6 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-/**
- *
- */
 public class CassandraContext extends WorkerCtx implements SimpleClientProvider {
 
     /**

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/bolt/CassandraWriterBolt.java
Patch:
@@ -25,8 +25,6 @@ public class CassandraWriterBolt extends BaseCassandraBolt<Tuple> {
 
     /**
      * Creates a new {@link CassandraWriterBolt} instance.
-     *
-     * @param tupleMapper
      */
     public CassandraWriterBolt(CQLStatementTupleMapper tupleMapper) {
         super(tupleMapper);

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/bolt/PairStatementTuple.java
Patch:
@@ -26,8 +26,6 @@ public class PairStatementTuple {
 
     /**
      * Creates a new {@link PairStatementTuple} instance.
-     * @param tuple
-     * @param statement
      */
     public PairStatementTuple(Tuple tuple, Statement statement) {
         this.tuple = tuple;

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/client/CassandraConf.java
Patch:
@@ -62,7 +62,7 @@ public class CassandraConf implements Serializable {
      */
     private String username;
     /**
-     * The authorized cassandra password
+     * The authorized cassandra password.
      */
     private String password;
     /**

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/client/impl/DefaultClient.java
Patch:
@@ -30,7 +30,7 @@
  */
 public class DefaultClient implements SimpleClient, Closeable, Serializable {
 
-    private final static Logger LOG = LoggerFactory.getLogger(DefaultClient.class);
+    private static final Logger LOG = LoggerFactory.getLogger(DefaultClient.class);
 
     private String keyspace;
 
@@ -72,8 +72,9 @@ private String getExecutorName() {
     public synchronized Session connect() throws NoHostAvailableException {
         if (isDisconnected()) {
             LOG.info("Connected to cluster: {}", cluster.getClusterName());
-            for (Host host : getAllHosts())
+            for (Host host : getAllHosts()) {
                 LOG.info("Datacenter: {}; Host: {}; Rack: {}", host.getDatacenter(), host.getAddress(), host.getRack());
+            }
 
             LOG.info("Connect to cluster using keyspace %s", keyspace);
             session = cluster.connect(keyspace);

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/context/BaseBeanFactory.java
Patch:
@@ -36,7 +36,9 @@ public void setStormContext(WorkerCtx context) {
      */
     @Override
     public synchronized T get(Map<String, Object> topoConf) {
-        if (instance != null) return instance;
+        if (instance != null) {
+            return instance;
+        }
         return instance = make(topoConf);
     }
 

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/context/BeanFactory.java
Patch:
@@ -22,14 +22,12 @@ public interface BeanFactory<T> extends Serializable {
 
     /**
      * Sets the storm context.
-     * @param context
      */
     public void setStormContext(WorkerCtx context);
 
     /**
      * Return an instance, which may be shared or independent, of the specified type.
      * @param topoConf The storm configuration
-     * @return
      */
     T get(Map<String, Object> topoConf);
 

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/query/AyncCQLResultSetValuesMapper.java
Patch:
@@ -20,8 +20,9 @@
 import org.apache.storm.tuple.Values;
 
 /**
- * A resultset mapper that
+ * A resultset mapper that.
  */
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public interface AyncCQLResultSetValuesMapper extends Serializable {
 
     List<List<Values>> map(Session session, List<Statement> statements, List<ITuple> tuples);

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/query/BaseCQLStatementTupleMapper.java
Patch:
@@ -22,8 +22,8 @@
 
 /**
  * Default interface to map a {@link org.apache.storm.tuple.ITuple} to a CQL {@link com.datastax.driver.core.Statement}.
- *
  */
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public abstract class BaseCQLStatementTupleMapper implements CQLStatementTupleMapper, Serializable {
 
     /**

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/query/CQLResultSetValuesMapper.java
Patch:
@@ -19,9 +19,7 @@
 import org.apache.storm.tuple.ITuple;
 import org.apache.storm.tuple.Values;
 
-/**
- *
- */
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public interface CQLResultSetValuesMapper extends Serializable {
 
     List<List<Values>> map(Session session, Statement statement, ITuple tuple);

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/query/CQLStatementBuilder.java
Patch:
@@ -14,7 +14,7 @@
 
 import java.io.Serializable;
 
-
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public interface CQLStatementBuilder<T extends CQLStatementTupleMapper> extends Serializable {
 
     /**

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/query/CQLStatementTupleMapper.java
Patch:
@@ -23,6 +23,7 @@
 /**
  * Default interface to map a {@link org.apache.storm.tuple.ITuple} to a CQL {@link com.datastax.driver.core.Statement}.
  */
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public interface CQLStatementTupleMapper extends Serializable {
 
     /**
@@ -35,6 +36,7 @@ public interface CQLStatementTupleMapper extends Serializable {
      */
     List<Statement> map(Map<String, Object> conf, Session session, ITuple tuple);
 
+    @SuppressWarnings("checkstyle:AbbreviationAsWordInName")
     public static class DynamicCQLStatementTupleMapper implements CQLStatementTupleMapper {
         private List<CQLStatementBuilder> builders;
 

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/query/impl/ObjectMapperCqlStatementMapper.java
Patch:
@@ -34,8 +34,9 @@
 import org.apache.storm.tuple.ITuple;
 
 /**
- * Tuple mapper that is able to map objects annotated with {@link com.datastax.driver.mapping.annotations.Table} to CQL statements
+ * Tuple mapper that is able to map objects annotated with {@link com.datastax.driver.mapping.annotations.Table} to CQL statements.
  */
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public class ObjectMapperCqlStatementMapper implements CQLStatementTupleMapper {
     private static final Map<Session, MappingManager> mappingManagers = new WeakHashMap<>();
 
@@ -83,7 +84,7 @@ public List<Statement> map(Map<String, Object> map, Session session, ITuple tupl
                 options.add(Option.timestamp(((Number) timestampObject).longValue()));
             } else if (timestampObject instanceof Instant) {
                 Instant timestamp = (Instant) timestampObject;
-                options.add(Option.timestamp(timestamp.getEpochSecond() * 1000_0000l + timestamp.getNano() / 1000l));
+                options.add(Option.timestamp(timestamp.getEpochSecond() * 1000_0000L + timestamp.getNano() / 1000L));
             }
         }
 

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/query/impl/PreparedStatementBinder.java
Patch:
@@ -19,9 +19,6 @@
 import java.util.List;
 import org.apache.storm.cassandra.query.Column;
 
-/**
- *
- */
 public interface PreparedStatementBinder extends Serializable {
 
     public BoundStatement apply(PreparedStatement statement, List<Column> columns);
@@ -38,6 +35,7 @@ public BoundStatement apply(PreparedStatement statement, List<Column> columns) {
         }
     }
 
+    @SuppressWarnings("checkstyle:AbbreviationAsWordInName")
     public static final class CQL3NamedSettersBinder implements PreparedStatementBinder {
 
         /**

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/query/impl/RoutingKeyGenerator.java
Patch:
@@ -27,7 +27,6 @@ public class RoutingKeyGenerator implements Serializable {
 
     /**
      * Creates a new {@link RoutingKeyGenerator} instance.
-     * @param routingKeys
      */
     public RoutingKeyGenerator(List<String> routingKeys) {
         Preconditions.checkNotNull(routingKeys);

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/query/impl/SimpleCQLStatementMapper.java
Patch:
@@ -25,9 +25,7 @@
 import org.apache.storm.cassandra.query.CqlMapper;
 import org.apache.storm.tuple.ITuple;
 
-/**
- *
- */
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public class SimpleCQLStatementMapper implements CQLStatementTupleMapper {
 
     private final String queryString;

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/trident/state/CassandraMapStateFactory.java
Patch:
@@ -29,9 +29,8 @@
 /**
  * A StateFactory implementation that creates a MapState backed by CassandraBackingMap.
  *
- * The statefactory supports opaque, transactional and non-transactional configurations.
+ * <p>The statefactory supports opaque, transactional and non-transactional configurations.
  * Optionally, the backing map can be wrapped in a {@link CachedMap} by specifying {@link #withCache} (off by default).
- *
  */
 public class CassandraMapStateFactory implements StateFactory {
 

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/trident/state/CassandraQuery.java
Patch:
@@ -18,9 +18,6 @@
 import org.apache.storm.trident.tuple.TridentTuple;
 import org.apache.storm.tuple.Values;
 
-/**
- *
- */
 public class CassandraQuery extends BaseQueryFunction<CassandraState, List<Values>> {
 
     @Override

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/trident/state/CassandraState.java
Patch:
@@ -32,9 +32,6 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-/**
- *
- */
 public class CassandraState implements State {
 
     private static final Logger LOG = LoggerFactory.getLogger(CassandraState.class);
@@ -131,11 +128,13 @@ public Options(SimpleClientProvider clientProvider) {
             this.clientProvider = clientProvider;
         }
 
+        @SuppressWarnings("checkstyle:AbbreviationAsWordInName")
         public Options withCQLStatementTupleMapper(CQLStatementTupleMapper cqlStatementTupleMapper) {
             this.cqlStatementTupleMapper = cqlStatementTupleMapper;
             return this;
         }
 
+        @SuppressWarnings("checkstyle:AbbreviationAsWordInName")
         public Options withCQLResultSetValuesMapper(CQLResultSetValuesMapper cqlResultSetValuesMapper) {
             this.cqlResultSetValuesMapper = cqlResultSetValuesMapper;
             return this;

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/trident/state/CassandraStateFactory.java
Patch:
@@ -20,9 +20,6 @@
 import org.apache.storm.trident.state.State;
 import org.apache.storm.trident.state.StateFactory;
 
-/**
- *
- */
 public class CassandraStateFactory implements StateFactory {
     private final CassandraState.Options options;
 

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/trident/state/CassandraStateUpdater.java
Patch:
@@ -17,9 +17,6 @@
 import org.apache.storm.trident.state.BaseStateUpdater;
 import org.apache.storm.trident.tuple.TridentTuple;
 
-/**
- *
- */
 public class CassandraStateUpdater extends BaseStateUpdater<CassandraState> {
 
     @Override

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/trident/state/OpaqueTupleStateMapper.java
Patch:
@@ -80,6 +80,7 @@ public OpaqueValue<ITuple> fromValues(List<Values> valuesList) {
         }
         Values values = valuesList.get(0);
         int index = 0;
+        @SuppressWarnings("checkstyle:VariableDeclarationUsageDistance")
         Long currTx = (Long) values.get(index++);
 
         SimpleTuple curr = new SimpleTuple(tupleFields);

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/trident/state/TransactionalTupleStateMapper.java
Patch:
@@ -69,6 +69,7 @@ public TransactionalValue<ITuple> fromValues(List<Values> valuesList) {
         }
         Values values = valuesList.get(0);
         int index = 0;
+        @SuppressWarnings("checkstyle:VariableDeclarationUsageDistance")
         Long txId = (Long) values.get(index++);
 
         SimpleTuple curr = new SimpleTuple(tupleFields);

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/trident/state/TridentAyncCQLResultSetValuesMapper.java
Patch:
@@ -33,6 +33,7 @@
 /**
  * A result set mapper implementation which runs requests in parallel and waits for them all to finish.
  */
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public class TridentAyncCQLResultSetValuesMapper implements AyncCQLResultSetValuesMapper {
     private final Fields outputDeclaredFields;
     private final Semaphore throttle;

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/trident/state/TridentResultSetValuesMapper.java
Patch:
@@ -24,9 +24,6 @@
 import org.apache.storm.tuple.ITuple;
 import org.apache.storm.tuple.Values;
 
-/**
- *
- */
 public class TridentResultSetValuesMapper implements CQLResultSetValuesMapper {
     private Fields outputDeclaredFields;
 

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/avro/AbstractAvroSerializer.java
Patch:
@@ -57,7 +57,7 @@ public void write(Kryo kryo, Output output, GenericContainer record) {
     }
 
     @Override
-    public GenericContainer read(Kryo kryo, Input input, Class<GenericContainer> aClass) {
+    public GenericContainer read(Kryo kryo, Input input, Class<GenericContainer> someClass) {
         Schema theSchema = this.getSchema(input.readString());
         GenericDatumReader<GenericContainer> reader = new GenericDatumReader<>(theSchema);
         Decoder decoder = DecoderFactory

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/avro/ConfluentAvroSerializer.java
Patch:
@@ -27,7 +27,7 @@
  */
 public class ConfluentAvroSerializer extends AbstractAvroSerializer {
 
-    final private String url;
+    private final String url;
     private SchemaRegistryClient theClient;
 
     /**

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/avro/FixedAvroSerializer.java
Patch:
@@ -30,7 +30,7 @@
  */
 public class FixedAvroSerializer extends AbstractAvroSerializer {
 
-    private final static String FP_ALGO = "CRC-64-AVRO";
+    private static final String FP_ALGO = "CRC-64-AVRO";
     final Map<String, Schema> fingerprint2schemaMap = new HashMap<>();
     final Map<Schema, String> schema2fingerprintMap = new HashMap<>();
 

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/bolt/SequenceFileBolt.java
Patch:
@@ -114,7 +114,9 @@ public SequenceFileBolt withMaxOpenFiles(int maxOpenFiles) {
     @Override
     public void doPrepare(Map<String, Object> conf, TopologyContext topologyContext, OutputCollector collector) throws IOException {
         LOG.info("Preparing Sequence File Bolt...");
-        if (this.format == null) throw new IllegalStateException("SequenceFormat must be specified.");
+        if (this.format == null) {
+            throw new IllegalStateException("SequenceFormat must be specified.");
+        }
 
         this.fs = FileSystem.get(URI.create(this.fsUrl), hdfsConfig);
         this.codecFactory = new CompressionCodecFactory(hdfsConfig);

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/bolt/format/FileNameFormat.java
Patch:
@@ -28,7 +28,6 @@ public interface FileNameFormat extends Serializable {
      * Returns the filename the HdfsBolt will create.
      * @param rotation the current file rotation number (incremented on every rotation)
      * @param timeStamp current time in milliseconds when the rotation occurs
-     * @return
      */
     String getName(long rotation, long timeStamp);
 

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/bolt/format/SimpleFileNameFormat.java
Patch:
@@ -73,9 +73,7 @@ public SimpleFileNameFormat withPath(String path) {
      * $COMPONENT - component id<br/>
      * $TASK - task id<br/>
      *
-     * @param name
-     *            file name
-     * @return
+     * @param name file name
      */
     public SimpleFileNameFormat withName(String name) {
         this.name = name;

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/bolt/rotation/FileSizeRotationPolicy.java
Patch:
@@ -12,7 +12,6 @@
 
 package org.apache.storm.hdfs.bolt.rotation;
 
-
 import org.apache.storm.tuple.Tuple;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -21,7 +20,7 @@
  * File rotation policy that will rotate files when a certain
  * file size is reached.
  *
- * For example:
+ * <p>For example:
  * <pre>
  *     // rotate when files reach 5MB
  *     FileSizeRotationPolicy policy =
@@ -34,6 +33,7 @@ public class FileSizeRotationPolicy implements FileRotationPolicy {
     private long maxBytes;
     private long lastOffset = 0;
     private long currentBytesWritten = 0;
+
     public FileSizeRotationPolicy(float count, Units units) {
         this.maxBytes = (long) (count * units.getByteCount());
     }

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/bolt/sync/CountSyncPolicy.java
Patch:
@@ -12,7 +12,6 @@
 
 package org.apache.storm.hdfs.bolt.sync;
 
-
 import org.apache.storm.tuple.Tuple;
 
 /**

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/common/AvroGenericRecordHDFSWriter.java
Patch:
@@ -12,7 +12,6 @@
 
 package org.apache.storm.hdfs.common;
 
-
 import java.io.IOException;
 import java.util.EnumSet;
 import org.apache.avro.Schema;
@@ -28,6 +27,7 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public class AvroGenericRecordHDFSWriter extends AbstractHDFSWriter {
 
     private static final Logger LOG = LoggerFactory.getLogger(AvroGenericRecordHDFSWriter.class);

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/common/HDFSWriter.java
Patch:
@@ -23,6 +23,7 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public class HDFSWriter extends AbstractHDFSWriter {
 
     private static final Logger LOG = LoggerFactory.getLogger(HDFSWriter.class);

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/common/Partitioner.java
Patch:
@@ -21,10 +21,9 @@ public interface Partitioner extends Serializable {
      * Return a relative path that the tuple should be written to. For example, if an HdfsBolt were configured to write
      * to /common/output and a partitioner returned "/foo" then the bolt should open a file in "/common/output/foo"
      *
-     * A best practice is to use Path.SEPARATOR instead of a literal "/"
+     * <p>A best practice is to use Path.SEPARATOR instead of a literal "/"
      *
      * @param tuple The tuple for which the relative path is being calculated.
-     * @return
      */
     public String getPartitionPath(final Tuple tuple);
 }

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/common/rotation/RotationAction.java
Patch:
@@ -18,7 +18,6 @@
 
 package org.apache.storm.hdfs.common.rotation;
 
-
 import java.io.IOException;
 import java.io.Serializable;
 import org.apache.hadoop.fs.FileSystem;

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/spout/FileOffset.java
Patch:
@@ -24,7 +24,9 @@
  */
 
 interface FileOffset extends Comparable<FileOffset>, Cloneable {
-    /** tests if rhs == currOffset+1 */
+    /**
+     * tests if rhs == currOffset+1.
+     */
     boolean isNextOffset(FileOffset rhs);
 
     FileOffset clone();

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/spout/FileReader.java
Patch:
@@ -26,10 +26,9 @@ interface FileReader {
     FileOffset getFileOffset();
 
     /**
-     * Get the next tuple from the file
+     * Get the next tuple from the file.
      *
      * @return null if no more data
-     * @throws IOException
      */
     List<Object> next() throws IOException, ParseException;
 

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/trident/format/FileNameFormat.java
Patch:
@@ -17,7 +17,6 @@
 
 /**
  * Formatter interface for determining HDFS file names.
- *
  */
 public interface FileNameFormat extends Serializable {
 
@@ -27,7 +26,6 @@ public interface FileNameFormat extends Serializable {
      * Returns the filename the HdfsBolt will create.
      * @param rotation the current file rotation number (incremented on every rotation)
      * @param timeStamp current time in milliseconds when the rotation occurs
-     * @return
      */
     String getName(long rotation, long timeStamp);
 

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/trident/format/SimpleFileNameFormat.java
Patch:
@@ -68,9 +68,7 @@ public SimpleFileNameFormat withPath(String path) {
      * $HOST - local host name<br/>
      * $PARTITION - partition index<br/>
      *
-     * @param name
-     *            file name
-     * @return
+     * @param name file name
      */
     public SimpleFileNameFormat withName(String name) {
         this.name = name;

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/trident/rotation/FileRotationPolicy.java
Patch:
@@ -18,11 +18,11 @@
 /**
  * Used by the HdfsBolt to decide when to rotate files.
  *
- * The HdfsBolt will call the <code>mark()</code> method for every
+ * <p>The HdfsBolt will call the <code>mark()</code> method for every
  * tuple received. If the <code>mark()</code> method returns
  * <code>true</code> the HdfsBolt will perform a file rotation.
  *
- * After file rotation, the HdfsBolt will call the <code>reset()</code>
+ * <p>After file rotation, the HdfsBolt will call the <code>reset()</code>
  * method.
  */
 public interface FileRotationPolicy extends Serializable {

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/trident/rotation/FileSizeRotationPolicy.java
Patch:
@@ -12,7 +12,6 @@
 
 package org.apache.storm.hdfs.trident.rotation;
 
-
 import org.apache.storm.trident.tuple.TridentTuple;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -21,19 +20,19 @@
  * File rotation policy that will rotate files when a certain
  * file size is reached.
  *
- * For example:
+ * <p>For example:
  * <pre>
  *     // rotate when files reach 5MB
  *     FileSizeRotationPolicy policy =
  *          new FileSizeRotationPolicy(5.0, Units.MB);
  * </pre>
- *
  */
 public class FileSizeRotationPolicy implements FileRotationPolicy {
     private static final Logger LOG = LoggerFactory.getLogger(FileSizeRotationPolicy.class);
     private long maxBytes;
     private long lastOffset = 0;
     private long currentBytesWritten = 0;
+
     public FileSizeRotationPolicy(float count, Units units) {
         this.maxBytes = (long) (count * units.getByteCount());
     }

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/trident/rotation/TimedRotationPolicy.java
Patch:
@@ -23,12 +23,12 @@
 import java.util.concurrent.atomic.AtomicBoolean;
 import org.apache.storm.trident.tuple.TridentTuple;
 
-
 public class TimedRotationPolicy implements FileRotationPolicy {
 
     private long interval;
     private Timer rotationTimer;
     private AtomicBoolean rotationTimerTriggered = new AtomicBoolean();
+
     public TimedRotationPolicy(float count, TimeUnit units) {
         this.interval = (long) (count * units.getMilliSeconds());
     }

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/trident/sync/CountSyncPolicy.java
Patch:
@@ -12,7 +12,6 @@
 
 package org.apache.storm.hdfs.trident.sync;
 
-
 import org.apache.storm.trident.tuple.TridentTuple;
 
 /**

File: external/storm-hdfs/src/test/java/org/apache/storm/hdfs/spout/TestFileLock.java
Patch:
@@ -245,7 +245,7 @@ public void testStaleLockDetection_MultipleLocks() throws Exception {
 
             expired = FileLock.locateOldestExpiredLock(fs, locksDir, LOCK_EXPIRY_SEC);
             Assert.assertNotNull(expired);
-            Assert.assertEquals("spout3", expired.getValue().componentID);
+            Assert.assertEquals("spout3", expired.getValue().componentId);
         } finally {
             lock1.release();
             lock2.release();

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/AbstractExecutionResultHandler.java
Patch:
@@ -48,6 +48,8 @@ public void onThrowable(Throwable t, OutputCollector collector, Tuple i) {
 
     @Override
     public void onThrowable(Throwable t, OutputCollector collector, List<Tuple> tl) {
-        for (Tuple i : tl) onThrowable(t, collector, i);
+        for (Tuple i : tl) {
+            onThrowable(t, collector, i);
+        }
     }
 }

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/BaseExecutionResultHandler.java
Patch:
@@ -19,6 +19,7 @@
 import com.datastax.driver.core.exceptions.WriteTimeoutException;
 import org.apache.storm.task.OutputCollector;
 import org.apache.storm.tuple.Tuple;
+import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
 /**
@@ -29,7 +30,7 @@
  */
 public class BaseExecutionResultHandler extends AbstractExecutionResultHandler {
 
-    private final static org.slf4j.Logger LOG = LoggerFactory.getLogger(BaseExecutionResultHandler.class);
+    private static final Logger LOG = LoggerFactory.getLogger(BaseExecutionResultHandler.class);
 
     /**
      * {@inheritDoc}

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/CassandraContext.java
Patch:
@@ -24,9 +24,6 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-/**
- *
- */
 public class CassandraContext extends WorkerCtx implements SimpleClientProvider {
 
     /**

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/bolt/CassandraWriterBolt.java
Patch:
@@ -25,8 +25,6 @@ public class CassandraWriterBolt extends BaseCassandraBolt<Tuple> {
 
     /**
      * Creates a new {@link CassandraWriterBolt} instance.
-     *
-     * @param tupleMapper
      */
     public CassandraWriterBolt(CQLStatementTupleMapper tupleMapper) {
         super(tupleMapper);

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/bolt/PairStatementTuple.java
Patch:
@@ -26,8 +26,6 @@ public class PairStatementTuple {
 
     /**
      * Creates a new {@link PairStatementTuple} instance.
-     * @param tuple
-     * @param statement
      */
     public PairStatementTuple(Tuple tuple, Statement statement) {
         this.tuple = tuple;

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/client/CassandraConf.java
Patch:
@@ -62,7 +62,7 @@ public class CassandraConf implements Serializable {
      */
     private String username;
     /**
-     * The authorized cassandra password
+     * The authorized cassandra password.
      */
     private String password;
     /**

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/client/impl/DefaultClient.java
Patch:
@@ -30,7 +30,7 @@
  */
 public class DefaultClient implements SimpleClient, Closeable, Serializable {
 
-    private final static Logger LOG = LoggerFactory.getLogger(DefaultClient.class);
+    private static final Logger LOG = LoggerFactory.getLogger(DefaultClient.class);
 
     private String keyspace;
 
@@ -72,8 +72,9 @@ private String getExecutorName() {
     public synchronized Session connect() throws NoHostAvailableException {
         if (isDisconnected()) {
             LOG.info("Connected to cluster: {}", cluster.getClusterName());
-            for (Host host : getAllHosts())
+            for (Host host : getAllHosts()) {
                 LOG.info("Datacenter: {}; Host: {}; Rack: {}", host.getDatacenter(), host.getAddress(), host.getRack());
+            }
 
             LOG.info("Connect to cluster using keyspace %s", keyspace);
             session = cluster.connect(keyspace);

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/context/BaseBeanFactory.java
Patch:
@@ -36,7 +36,9 @@ public void setStormContext(WorkerCtx context) {
      */
     @Override
     public synchronized T get(Map<String, Object> topoConf) {
-        if (instance != null) return instance;
+        if (instance != null) {
+            return instance;
+        }
         return instance = make(topoConf);
     }
 

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/context/BeanFactory.java
Patch:
@@ -22,14 +22,12 @@ public interface BeanFactory<T> extends Serializable {
 
     /**
      * Sets the storm context.
-     * @param context
      */
     public void setStormContext(WorkerCtx context);
 
     /**
      * Return an instance, which may be shared or independent, of the specified type.
      * @param topoConf The storm configuration
-     * @return
      */
     T get(Map<String, Object> topoConf);
 

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/query/AyncCQLResultSetValuesMapper.java
Patch:
@@ -20,8 +20,9 @@
 import org.apache.storm.tuple.Values;
 
 /**
- * A resultset mapper that
+ * A resultset mapper that.
  */
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public interface AyncCQLResultSetValuesMapper extends Serializable {
 
     List<List<Values>> map(Session session, List<Statement> statements, List<ITuple> tuples);

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/query/BaseCQLStatementTupleMapper.java
Patch:
@@ -22,8 +22,8 @@
 
 /**
  * Default interface to map a {@link org.apache.storm.tuple.ITuple} to a CQL {@link com.datastax.driver.core.Statement}.
- *
  */
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public abstract class BaseCQLStatementTupleMapper implements CQLStatementTupleMapper, Serializable {
 
     /**

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/query/CQLResultSetValuesMapper.java
Patch:
@@ -19,9 +19,7 @@
 import org.apache.storm.tuple.ITuple;
 import org.apache.storm.tuple.Values;
 
-/**
- *
- */
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public interface CQLResultSetValuesMapper extends Serializable {
 
     List<List<Values>> map(Session session, Statement statement, ITuple tuple);

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/query/CQLStatementBuilder.java
Patch:
@@ -14,7 +14,7 @@
 
 import java.io.Serializable;
 
-
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public interface CQLStatementBuilder<T extends CQLStatementTupleMapper> extends Serializable {
 
     /**

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/query/CQLStatementTupleMapper.java
Patch:
@@ -23,6 +23,7 @@
 /**
  * Default interface to map a {@link org.apache.storm.tuple.ITuple} to a CQL {@link com.datastax.driver.core.Statement}.
  */
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public interface CQLStatementTupleMapper extends Serializable {
 
     /**
@@ -35,6 +36,7 @@ public interface CQLStatementTupleMapper extends Serializable {
      */
     List<Statement> map(Map<String, Object> conf, Session session, ITuple tuple);
 
+    @SuppressWarnings("checkstyle:AbbreviationAsWordInName")
     public static class DynamicCQLStatementTupleMapper implements CQLStatementTupleMapper {
         private List<CQLStatementBuilder> builders;
 

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/query/impl/ObjectMapperCqlStatementMapper.java
Patch:
@@ -34,8 +34,9 @@
 import org.apache.storm.tuple.ITuple;
 
 /**
- * Tuple mapper that is able to map objects annotated with {@link com.datastax.driver.mapping.annotations.Table} to CQL statements
+ * Tuple mapper that is able to map objects annotated with {@link com.datastax.driver.mapping.annotations.Table} to CQL statements.
  */
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public class ObjectMapperCqlStatementMapper implements CQLStatementTupleMapper {
     private static final Map<Session, MappingManager> mappingManagers = new WeakHashMap<>();
 
@@ -83,7 +84,7 @@ public List<Statement> map(Map<String, Object> map, Session session, ITuple tupl
                 options.add(Option.timestamp(((Number) timestampObject).longValue()));
             } else if (timestampObject instanceof Instant) {
                 Instant timestamp = (Instant) timestampObject;
-                options.add(Option.timestamp(timestamp.getEpochSecond() * 1000_0000l + timestamp.getNano() / 1000l));
+                options.add(Option.timestamp(timestamp.getEpochSecond() * 1000_0000L + timestamp.getNano() / 1000L));
             }
         }
 

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/query/impl/PreparedStatementBinder.java
Patch:
@@ -19,9 +19,6 @@
 import java.util.List;
 import org.apache.storm.cassandra.query.Column;
 
-/**
- *
- */
 public interface PreparedStatementBinder extends Serializable {
 
     public BoundStatement apply(PreparedStatement statement, List<Column> columns);
@@ -38,6 +35,7 @@ public BoundStatement apply(PreparedStatement statement, List<Column> columns) {
         }
     }
 
+    @SuppressWarnings("checkstyle:AbbreviationAsWordInName")
     public static final class CQL3NamedSettersBinder implements PreparedStatementBinder {
 
         /**

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/query/impl/RoutingKeyGenerator.java
Patch:
@@ -27,7 +27,6 @@ public class RoutingKeyGenerator implements Serializable {
 
     /**
      * Creates a new {@link RoutingKeyGenerator} instance.
-     * @param routingKeys
      */
     public RoutingKeyGenerator(List<String> routingKeys) {
         Preconditions.checkNotNull(routingKeys);

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/query/impl/SimpleCQLStatementMapper.java
Patch:
@@ -25,9 +25,7 @@
 import org.apache.storm.cassandra.query.CqlMapper;
 import org.apache.storm.tuple.ITuple;
 
-/**
- *
- */
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public class SimpleCQLStatementMapper implements CQLStatementTupleMapper {
 
     private final String queryString;

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/trident/state/CassandraMapStateFactory.java
Patch:
@@ -29,9 +29,8 @@
 /**
  * A StateFactory implementation that creates a MapState backed by CassandraBackingMap.
  *
- * The statefactory supports opaque, transactional and non-transactional configurations.
+ * <p>The statefactory supports opaque, transactional and non-transactional configurations.
  * Optionally, the backing map can be wrapped in a {@link CachedMap} by specifying {@link #withCache} (off by default).
- *
  */
 public class CassandraMapStateFactory implements StateFactory {
 

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/trident/state/CassandraQuery.java
Patch:
@@ -18,9 +18,6 @@
 import org.apache.storm.trident.tuple.TridentTuple;
 import org.apache.storm.tuple.Values;
 
-/**
- *
- */
 public class CassandraQuery extends BaseQueryFunction<CassandraState, List<Values>> {
 
     @Override

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/trident/state/CassandraState.java
Patch:
@@ -32,9 +32,6 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-/**
- *
- */
 public class CassandraState implements State {
 
     private static final Logger LOG = LoggerFactory.getLogger(CassandraState.class);
@@ -131,11 +128,13 @@ public Options(SimpleClientProvider clientProvider) {
             this.clientProvider = clientProvider;
         }
 
+        @SuppressWarnings("checkstyle:AbbreviationAsWordInName")
         public Options withCQLStatementTupleMapper(CQLStatementTupleMapper cqlStatementTupleMapper) {
             this.cqlStatementTupleMapper = cqlStatementTupleMapper;
             return this;
         }
 
+        @SuppressWarnings("checkstyle:AbbreviationAsWordInName")
         public Options withCQLResultSetValuesMapper(CQLResultSetValuesMapper cqlResultSetValuesMapper) {
             this.cqlResultSetValuesMapper = cqlResultSetValuesMapper;
             return this;

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/trident/state/CassandraStateFactory.java
Patch:
@@ -20,9 +20,6 @@
 import org.apache.storm.trident.state.State;
 import org.apache.storm.trident.state.StateFactory;
 
-/**
- *
- */
 public class CassandraStateFactory implements StateFactory {
     private final CassandraState.Options options;
 

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/trident/state/CassandraStateUpdater.java
Patch:
@@ -17,9 +17,6 @@
 import org.apache.storm.trident.state.BaseStateUpdater;
 import org.apache.storm.trident.tuple.TridentTuple;
 
-/**
- *
- */
 public class CassandraStateUpdater extends BaseStateUpdater<CassandraState> {
 
     @Override

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/trident/state/OpaqueTupleStateMapper.java
Patch:
@@ -80,6 +80,7 @@ public OpaqueValue<ITuple> fromValues(List<Values> valuesList) {
         }
         Values values = valuesList.get(0);
         int index = 0;
+        @SuppressWarnings("checkstyle:VariableDeclarationUsageDistance")
         Long currTx = (Long) values.get(index++);
 
         SimpleTuple curr = new SimpleTuple(tupleFields);

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/trident/state/TransactionalTupleStateMapper.java
Patch:
@@ -69,6 +69,7 @@ public TransactionalValue<ITuple> fromValues(List<Values> valuesList) {
         }
         Values values = valuesList.get(0);
         int index = 0;
+        @SuppressWarnings("checkstyle:VariableDeclarationUsageDistance")
         Long txId = (Long) values.get(index++);
 
         SimpleTuple curr = new SimpleTuple(tupleFields);

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/trident/state/TridentAyncCQLResultSetValuesMapper.java
Patch:
@@ -33,6 +33,7 @@
 /**
  * A result set mapper implementation which runs requests in parallel and waits for them all to finish.
  */
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public class TridentAyncCQLResultSetValuesMapper implements AyncCQLResultSetValuesMapper {
     private final Fields outputDeclaredFields;
     private final Semaphore throttle;

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/trident/state/TridentResultSetValuesMapper.java
Patch:
@@ -24,9 +24,6 @@
 import org.apache.storm.tuple.ITuple;
 import org.apache.storm.tuple.Values;
 
-/**
- *
- */
 public class TridentResultSetValuesMapper implements CQLResultSetValuesMapper {
     private Fields outputDeclaredFields;
 

File: external/storm-jdbc/src/main/java/org/apache/storm/jdbc/common/ConnectionProvider.java
Patch:
@@ -25,7 +25,7 @@ public interface ConnectionProvider extends Serializable {
     void prepare();
 
     /**
-     *
+     * Get connection.
      * @return a DB connection over which the queries can be executed.
      */
     Connection getConnection();

File: external/storm-jdbc/src/main/java/org/apache/storm/jdbc/common/HikariCPConnectionProvider.java
Patch:
@@ -21,14 +21,15 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public class HikariCPConnectionProvider implements ConnectionProvider {
     private static final Logger LOG = LoggerFactory.getLogger(HikariCPConnectionProvider.class);
 
     private Map<String, Object> configMap;
     private transient HikariDataSource dataSource;
 
-    public HikariCPConnectionProvider(Map<String, Object> hikariCPConfigMap) {
-        this.configMap = hikariCPConfigMap;
+    public HikariCPConnectionProvider(Map<String, Object> configMap) {
+        this.configMap = configMap;
     }
 
     @Override

File: external/storm-jdbc/src/main/java/org/apache/storm/jdbc/mapper/JdbcLookupMapper.java
Patch:
@@ -31,7 +31,6 @@ public interface JdbcLookupMapper extends JdbcMapper {
 
     /**
      * declare what are the fields that this code will output.
-     * @param declarer
      */
     void declareOutputFields(OutputFieldsDeclarer declarer);
 }

File: external/storm-jdbc/src/main/java/org/apache/storm/jdbc/mapper/JdbcMapper.java
Patch:
@@ -19,8 +19,7 @@
 
 public interface JdbcMapper extends Serializable {
     /**
-     *
-     * @param tuple
+     * Get columns.
      * @return list of columns that represents one row in a DB table.
      */
     List<Column> getColumns(ITuple tuple);

File: external/storm-jdbc/src/main/java/org/apache/storm/jdbc/mapper/SimpleJdbcLookupMapper.java
Patch:
@@ -12,7 +12,6 @@
 
 package org.apache.storm.jdbc.mapper;
 
-
 import java.util.ArrayList;
 import java.util.List;
 import org.apache.commons.lang.Validate;

File: external/storm-jdbc/src/main/java/org/apache/storm/jdbc/trident/state/JdbcStateFactory.java
Patch:
@@ -26,7 +26,7 @@ public JdbcStateFactory(JdbcState.Options options) {
     }
 
     @Override
-    public State makeState(Map<String, Object> map, IMetricsContext iMetricsContext, int partitionIndex, int numPartitions) {
+    public State makeState(Map<String, Object> map, IMetricsContext metricsContext, int partitionIndex, int numPartitions) {
         JdbcState state = new JdbcState(map, partitionIndex, numPartitions, options);
         state.prepare();
         return state;

File: examples/storm-starter/src/jvm/org/apache/storm/starter/BasicDRPCTopology.java
Patch:
@@ -29,6 +29,7 @@
  *
  * @see <a href="http://storm.apache.org/documentation/Distributed-RPC.html">Distributed RPC</a>
  */
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public class BasicDRPCTopology {
     public static void main(String[] args) throws Exception {
         Config conf = new Config();

File: examples/storm-starter/src/jvm/org/apache/storm/starter/ManualDRPC.java
Patch:
@@ -25,7 +25,9 @@
 import org.apache.storm.tuple.Values;
 import org.apache.storm.utils.DRPCClient;
 
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public class ManualDRPC {
+
     public static void main(String[] args) throws Exception {
         TopologyBuilder builder = new TopologyBuilder();
 

File: examples/storm-starter/src/jvm/org/apache/storm/starter/PersistentWindowingTopology.java
Patch:
@@ -18,6 +18,8 @@
 
 package org.apache.storm.starter;
 
+import static org.apache.storm.topology.base.BaseWindowedBolt.Duration;
+
 import java.util.Iterator;
 import java.util.Map;
 import java.util.concurrent.TimeUnit;
@@ -38,8 +40,6 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import static org.apache.storm.topology.base.BaseWindowedBolt.Duration;
-
 /**
  * An example that demonstrates the usage of {@link org.apache.storm.topology.IStatefulWindowedBolt} with window persistence.
  * <p>

File: examples/storm-starter/src/jvm/org/apache/storm/starter/SingleJoinExample.java
Patch:
@@ -21,10 +21,10 @@
 import org.apache.storm.tuple.Values;
 import org.apache.storm.utils.NimbusClient;
 
-/** Example of using a simple custom join bolt
- *  NOTE: Prefer to use the built-in JoinBolt wherever applicable
+/**
+ * Example of using a simple custom join bolt.
+ * NOTE: Prefer to use the built-in JoinBolt wherever applicable
  */
-
 public class SingleJoinExample {
     public static void main(String[] args) throws Exception {
         if (!NimbusClient.isLocalOverride()) {

File: examples/storm-starter/src/jvm/org/apache/storm/starter/bolt/SlidingWindowSumBolt.java
Patch:
@@ -26,7 +26,7 @@
 import org.slf4j.LoggerFactory;
 
 /**
- * Computes sliding window sum
+ * Computes sliding window sum.
  */
 public class SlidingWindowSumBolt extends BaseWindowedBolt {
     private static final Logger LOG = LoggerFactory.getLogger(SlidingWindowSumBolt.class);

File: examples/storm-starter/src/jvm/org/apache/storm/starter/streams/AggregateExample.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.storm.topology.base.BaseWindowedBolt;
 
 /**
- * An example that illustrates the global aggregate
+ * An example that illustrates the global aggregate.
  */
 public class AggregateExample {
     @SuppressWarnings("unchecked")

File: examples/storm-starter/src/jvm/org/apache/storm/starter/streams/GroupByKeyAndWindowExample.java
Patch:
@@ -34,7 +34,7 @@
 
 /**
  * An example that shows the usage of {@link PairStream#groupByKeyAndWindow(Window)}
- * and {@link PairStream#reduceByKeyAndWindow(Reducer, Window)}
+ * and {@link PairStream#reduceByKeyAndWindow(Reducer, Window)}.
  */
 public class GroupByKeyAndWindowExample {
     public static void main(String[] args) throws Exception {

File: examples/storm-starter/src/jvm/org/apache/storm/starter/streams/JoinExample.java
Patch:
@@ -70,7 +70,7 @@ public static void main(String[] args) throws Exception {
     private static class NumberSpout extends BaseRichSpout {
         private final Function<Integer, Integer> function;
         private SpoutOutputCollector collector;
-        private int i = 1;
+        private int count = 1;
 
         NumberSpout(Function<Integer, Integer> function) {
             this.function = function;
@@ -84,8 +84,8 @@ public void open(Map<String, Object> conf, TopologyContext context, SpoutOutputC
         @Override
         public void nextTuple() {
             Utils.sleep(990);
-            collector.emit(new Values(i, function.apply(i)));
-            i++;
+            collector.emit(new Values(count, function.apply(count)));
+            count++;
         }
 
         @Override

File: examples/storm-starter/src/jvm/org/apache/storm/starter/streams/WindowedWordCount.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.storm.topology.base.BaseWindowedBolt.Duration;
 
 /**
- * A windowed word count example
+ * A windowed word count example.
  */
 public class WindowedWordCount {
     public static void main(String[] args) throws Exception {

File: examples/storm-starter/src/jvm/org/apache/storm/starter/streams/WordCountToBolt.java
Patch:
@@ -27,7 +27,7 @@
 
 /**
  * An example that computes word counts and finally emits the results to an
- * external bolt (sink)
+ * external bolt (sink).
  */
 public class WordCountToBolt {
     public static void main(String[] args) throws Exception {

File: examples/storm-starter/src/jvm/org/apache/storm/starter/tools/RankableObjectWithFields.java
Patch:
@@ -74,6 +74,7 @@ public long getCount() {
     }
 
     /**
+     * Get fields.
      * @return an immutable list of any additional data fields of the object (may be empty but will never be null)
      */
     public List<Object> getFields() {
@@ -131,8 +132,6 @@ public String toString() {
     /**
      * Note: We do not defensively copy the wrapped object and any accompanying fields.  We do guarantee, however,
      * do return a defensive (shallow) copy of the List object that is wrapping any accompanying fields.
-     *
-     * @return
      */
     @Override
     public Rankable copy() {

File: examples/storm-starter/src/jvm/org/apache/storm/starter/tools/Rankings.java
Patch:
@@ -39,21 +39,22 @@ public Rankings(int topN) {
 
     /**
      * Copy constructor.
-     * @param other
      */
     public Rankings(Rankings other) {
         this(other.maxSize());
         updateWith(other);
     }
 
     /**
+     * Get max size.
      * @return the maximum possible number (size) of ranked objects this instance can hold
      */
     public int maxSize() {
         return maxSize;
     }
 
     /**
+     * Get size.
      * @return the number (size) of ranked objects this instance is currently holding
      */
     public int size() {

File: examples/storm-starter/src/jvm/org/apache/storm/starter/tools/SlidingWindowCounter.java
Patch:
@@ -95,6 +95,7 @@ public void incrementCount(T obj) {
      * @return The current (total) counts of all tracked objects.
      */
     public Map<T, Long> getCountsThenAdvanceWindow() {
+        @SuppressWarnings("checkstyle:VariableDeclarationUsageDistance")
         Map<T, Long> counts = objCounter.getCounts();
         objCounter.wipeZeros();
         objCounter.wipeSlot(tailSlot);

File: examples/storm-starter/src/jvm/org/apache/storm/starter/tools/SlotBasedCounter.java
Patch:
@@ -75,8 +75,6 @@ private long computeTotalCount(T obj) {
 
     /**
      * Reset the slot count of any tracked objects to zero for the given slot.
-     *
-     * @param slot
      */
     public void wipeSlot(int slot) {
         for (T obj : objToCounts.keySet()) {

File: examples/storm-starter/src/jvm/org/apache/storm/starter/trident/DebugMemoryMapState.java
Patch:
@@ -59,15 +59,15 @@ private void print(List<List<Object>> keys, List<ValueUpdater> updaters) {
     }
 
     public static class Factory implements StateFactory {
-        String _id;
+        String id;
 
         public Factory() {
-            _id = UUID.randomUUID().toString();
+            id = UUID.randomUUID().toString();
         }
 
         @Override
         public State makeState(Map<String, Object> conf, IMetricsContext metrics, int partitionIndex, int numPartitions) {
-            return new DebugMemoryMapState(_id + partitionIndex);
+            return new DebugMemoryMapState(id + partitionIndex);
         }
     }
 }

File: external/storm-solr/src/main/java/org/apache/storm/solr/config/CountBasedCommit.java
Patch:
@@ -21,7 +21,7 @@ public class CountBasedCommit implements SolrCommitStrategy {
     private int count;
 
     /**
-     * Initializes a count based commit strategy with the specified threshold
+     * Initializes a count based commit strategy with the specified threshold.
      *
      * @param threshold  The commit threshold, defining when SolrInputDocuments should be committed to Solr
      * */

File: external/storm-solr/src/main/java/org/apache/storm/solr/config/SolrConfig.java
Patch:
@@ -25,13 +25,15 @@ public class SolrConfig implements Serializable {
     private final boolean enableKerberos;
 
     /**
+     * Constructor.
      * @param zkHostString Zookeeper host string as defined in the {@link CloudSolrClient} constructor
      * */
     public SolrConfig(String zkHostString) {
         this(zkHostString, 0);
     }
 
     /**
+     * Constructor.
      * @param zkHostString Zookeeper host string as defined in the {@link CloudSolrClient} constructor
      * @param tickTupleInterval interval for tick tuples
      * */
@@ -40,6 +42,7 @@ public SolrConfig(String zkHostString, int tickTupleInterval) {
     }
 
     /**
+     * Constructor.
      * @param zkHostString Zookeeper host string as defined in the {@link CloudSolrClient} constructor
      * @param tickTupleInterval interval for tick tuples
      * @param enableKerberos true to enable kerberos else false

File: external/storm-solr/src/main/java/org/apache/storm/solr/mapper/SolrJsonMapper.java
Patch:
@@ -44,9 +44,7 @@ public String getCollection() {
     }
 
     /**
-     *
-     * @param tuples
-     * @return
+     * Maps to Solr request.
      * @throws SolrMapperException if the tuple does not contain the
      */
     @Override

File: external/storm-solr/src/main/java/org/apache/storm/solr/schema/builder/RestJsonSchemaBuilder.java
Patch:
@@ -32,7 +32,9 @@ public class RestJsonSchemaBuilder implements SchemaBuilder {
     private Schema schema;
 
 
-    /** Urls with the form http://localhost:8983/solr/gettingstarted/schema/ returns the schema in JSON format */
+    /**
+     * Urls with the form http://localhost:8983/solr/gettingstarted/schema/ returns the schema in JSON format.
+     */
     public RestJsonSchemaBuilder(String solrHost, String solrPort, String collection) throws IOException {
         this(new URL("http://" + solrHost + ":" + solrPort + "/solr/" + collection + "/schema/"));
     }

File: external/storm-solr/src/main/java/org/apache/storm/solr/schema/builder/RestJsonSchemaBuilderV2.java
Patch:
@@ -32,7 +32,7 @@
 import org.slf4j.LoggerFactory;
 
 /**
- * Class that builds the {@link Schema} object from the schema returned by the SchemaRequest
+ * Class that builds the {@link Schema} object from the schema returned by the SchemaRequest.
  */
 public class RestJsonSchemaBuilderV2 implements SchemaBuilder {
     private static final Logger logger = LoggerFactory.getLogger(RestJsonSchemaBuilderV2.class);

File: external/storm-solr/src/main/java/org/apache/storm/solr/trident/SolrState.java
Patch:
@@ -47,10 +47,10 @@ protected void prepare() {
     }
 
     @Override
-    public void beginCommit(Long aLong) { }
+    public void beginCommit(Long someLong) { }
 
     @Override
-    public void commit(Long aLong) { }
+    public void commit(Long someLong) { }
 
     public void updateState(List<TridentTuple> tuples) {
         try {

File: external/storm-solr/src/main/java/org/apache/storm/solr/trident/SolrStateFactory.java
Patch:
@@ -29,7 +29,7 @@ public SolrStateFactory(SolrConfig solrConfig, SolrMapper solrMapper) {
     }
 
     @Override
-    public State makeState(Map<String, Object> map, IMetricsContext iMetricsContext, int partitionIndex, int numPartitions) {
+    public State makeState(Map<String, Object> map, IMetricsContext metricsContext, int partitionIndex, int numPartitions) {
         SolrState state = new SolrState(solrConfig, solrMapper);
         state.prepare();
         return state;

File: examples/storm-hbase-examples/src/main/java/org/apache/storm/hbase/topology/TotalWordCounter.java
Patch:
@@ -12,6 +12,8 @@
 
 package org.apache.storm.hbase.topology;
 
+import static org.apache.storm.utils.Utils.tuple;
+
 import java.math.BigInteger;
 import java.util.Map;
 import java.util.Random;
@@ -24,8 +26,6 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import static org.apache.storm.utils.Utils.tuple;
-
 public class TotalWordCounter implements IBasicBolt {
 
     private static final Logger LOG = LoggerFactory.getLogger(TotalWordCounter.class);

File: examples/storm-hbase-examples/src/main/java/org/apache/storm/hbase/topology/WordCountClient.java
Patch:
@@ -24,7 +24,7 @@
 /**
  * Connects to the 'WordCount' table and prints counts for each word.
  *
- * Assumes you have run (or are running) <code>PersistentWordCount</code>
+ * <p>Assumes you have run (or are running) <code>PersistentWordCount</code>
  */
 public class WordCountClient {
 

File: examples/storm-hbase-examples/src/main/java/org/apache/storm/hbase/topology/WordCounter.java
Patch:
@@ -12,6 +12,8 @@
 
 package org.apache.storm.hbase.topology;
 
+import static org.apache.storm.utils.Utils.tuple;
+
 import java.util.Map;
 import org.apache.storm.task.TopologyContext;
 import org.apache.storm.topology.BasicOutputCollector;
@@ -20,8 +22,6 @@
 import org.apache.storm.tuple.Fields;
 import org.apache.storm.tuple.Tuple;
 
-import static org.apache.storm.utils.Utils.tuple;
-
 public class WordCounter implements IBasicBolt {
 
 

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/bolt/EventHubBolt.java
Patch:
@@ -18,7 +18,6 @@
 
 package org.apache.storm.eventhubs.bolt;
 
-
 import com.microsoft.azure.eventhubs.EventData;
 import com.microsoft.azure.eventhubs.EventHubClient;
 import com.microsoft.azure.eventhubs.PartitionSender;

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/bolt/IEventDataFormat.java
Patch:
@@ -22,7 +22,7 @@
 import org.apache.storm.tuple.Tuple;
 
 /**
- * Serialize a tuple to a byte array to be sent to EventHubs
+ * Serialize a tuple to a byte array to be sent to EventHubs.
  */
 public interface IEventDataFormat extends Serializable {
     public byte[] serialize(Tuple tuple);

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/spout/BinaryEventDataScheme.java
Patch:
@@ -31,7 +31,7 @@
 /**
  * An Event Data Scheme which deserializes message payload into the raw bytes.
  *
- * The resulting tuple would contain three items, the first being the message bytes, and the second a map of properties that include
+ * <p>The resulting tuple would contain three items, the first being the message bytes, and the second a map of properties that include
  * metadata, which can be used to determine who processes the message, and how it is processed.The third is the system properties which
  * exposes information like enqueue-time, offset and sequence number
  */

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/spout/EventDataWrap.java
Patch:
@@ -43,7 +43,6 @@ public MessageId getMessageId() {
 
     @Override
     public int compareTo(EventDataWrap ed) {
-        return messageId.getSequenceNumber().
-            compareTo(ed.getMessageId().getSequenceNumber());
+        return messageId.getSequenceNumber().compareTo(ed.getMessageId().getSequenceNumber());
     }
 }

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/spout/IEventHubReceiverFactory.java
Patch:
@@ -18,7 +18,7 @@
 import java.io.Serializable;
 
 /**
- * An abstract factory to generate EventHubReceiver
+ * An abstract factory to generate EventHubReceiver.
  */
 public interface IEventHubReceiverFactory extends Serializable {
     IEventHubReceiver create(EventHubSpoutConfig config, String partitionId);

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/spout/IPartitionManagerFactory.java
Patch:
@@ -18,7 +18,7 @@
 import java.io.Serializable;
 
 /**
- * An interface of factory method to create IPartitionManager
+ * An interface of factory method to create IPartitionManager.
  */
 public interface IPartitionManagerFactory extends Serializable {
     IPartitionManager create(EventHubSpoutConfig spoutConfig,

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/spout/SimplePartitionManager.java
Patch:
@@ -24,7 +24,7 @@
 import org.slf4j.LoggerFactory;
 
 /**
- * A simple partition manager that does not re-send failed messages
+ * A simple partition manager that does not re-send failed messages.
  */
 public class SimplePartitionManager implements IPartitionManager {
     private static final Logger logger = LoggerFactory.getLogger(SimplePartitionManager.class);

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/trident/ITridentPartitionManager.java
Patch:
@@ -27,7 +27,7 @@ public interface ITridentPartitionManager {
     void close();
 
     /**
-     * receive a batch of messages from EvenHub up to "count" messages
+     * receive a batch of messages from EvenHub up to "count" messages.
      *
      * @param offset the starting offset
      * @param count  max number of messages in this batch

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/trident/OpaqueTridentEventHubEmitter.java
Patch:
@@ -27,7 +27,7 @@
 import org.apache.storm.trident.topology.TransactionAttempt;
 
 /**
- * A thin wrapper of TransactionalTridentEventHubEmitter for OpaqueTridentEventHubSpout
+ * A thin wrapper of TransactionalTridentEventHubEmitter for OpaqueTridentEventHubSpout.
  */
 public class OpaqueTridentEventHubEmitter implements IOpaquePartitionedTridentSpout.Emitter<Partitions, Partition, Map> {
     private final TransactionalTridentEventHubEmitter transactionalEmitter;

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/trident/OpaqueTridentEventHubSpout.java
Patch:
@@ -26,7 +26,7 @@
 import org.apache.storm.tuple.Fields;
 
 /**
- * Opaque Trident EventHubs Spout
+ * Opaque Trident EventHubs Spout.
  */
 public class OpaqueTridentEventHubSpout implements IOpaquePartitionedTridentSpout<Partitions, Partition, Map> {
     private static final long serialVersionUID = 1L;

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/trident/Partition.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.storm.trident.spout.ISpoutPartition;
 
 /**
- * Represents an EventHub partition
+ * Represents an EventHub partition.
  */
 public class Partition implements ISpoutPartition, Serializable {
     private static final long serialVersionUID = 1L;

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/trident/TransactionalTridentEventHubEmitter.java
Patch:
@@ -85,8 +85,6 @@ public void close() {
 
     /**
      * Check if partition manager for a given partiton is created if not, create it.
-     *
-     * @param partition
      */
     private ITridentPartitionManager getOrCreatePartitionManager(Partition partition) {
         ITridentPartitionManager pm;

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/trident/TransactionalTridentEventHubSpout.java
Patch:
@@ -26,7 +26,7 @@
 import org.apache.storm.tuple.Fields;
 
 /**
- * Transactional Trident EventHub Spout
+ * Transactional Trident EventHub Spout.
  */
 public class TransactionalTridentEventHubSpout implements
                                                IPartitionedTridentSpout<Partitions, Partition, Map<String, Object>> {

File: external/storm-blobstore-migration/src/main/java/org/apache/storm/blobstore/ListLocalFs.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.blobstore;
 
 import java.util.Map;
@@ -30,7 +31,7 @@ public class ListLocalFs {
     
     public static void main(String[] args) throws Exception {
         
-        if(args.length != 1) {
+        if (args.length != 1) {
             System.out.println("Need 1 arguments, but have " + Integer.toString(args.length));
             System.out.println("listLocalFs <local_blobstore_dir>");
             System.out.println("Migrates blobs from LocalFsBlobStore to HdfsBlobStore");

File: external/storm-solr/src/main/java/org/apache/storm/solr/config/CountBasedCommit.java
Patch:
@@ -21,7 +21,7 @@ public class CountBasedCommit implements SolrCommitStrategy {
     private int count;
 
     /**
-     * Initializes a count based commit strategy with the specified threshold
+     * Initializes a count based commit strategy with the specified threshold.
      *
      * @param threshold  The commit threshold, defining when SolrInputDocuments should be committed to Solr
      * */

File: external/storm-solr/src/main/java/org/apache/storm/solr/config/SolrConfig.java
Patch:
@@ -25,13 +25,15 @@ public class SolrConfig implements Serializable {
     private final boolean enableKerberos;
 
     /**
+     * Constructor.
      * @param zkHostString Zookeeper host string as defined in the {@link CloudSolrClient} constructor
      * */
     public SolrConfig(String zkHostString) {
         this(zkHostString, 0);
     }
 
     /**
+     * Constructor.
      * @param zkHostString Zookeeper host string as defined in the {@link CloudSolrClient} constructor
      * @param tickTupleInterval interval for tick tuples
      * */
@@ -40,6 +42,7 @@ public SolrConfig(String zkHostString, int tickTupleInterval) {
     }
 
     /**
+     * Constructor.
      * @param zkHostString Zookeeper host string as defined in the {@link CloudSolrClient} constructor
      * @param tickTupleInterval interval for tick tuples
      * @param enableKerberos true to enable kerberos else false

File: external/storm-solr/src/main/java/org/apache/storm/solr/mapper/SolrJsonMapper.java
Patch:
@@ -44,9 +44,7 @@ public String getCollection() {
     }
 
     /**
-     *
-     * @param tuples
-     * @return
+     * Maps to Solr request.
      * @throws SolrMapperException if the tuple does not contain the
      */
     @Override

File: external/storm-solr/src/main/java/org/apache/storm/solr/schema/builder/RestJsonSchemaBuilder.java
Patch:
@@ -32,7 +32,9 @@ public class RestJsonSchemaBuilder implements SchemaBuilder {
     private Schema schema;
 
 
-    /** Urls with the form http://localhost:8983/solr/gettingstarted/schema/ returns the schema in JSON format */
+    /**
+     * Urls with the form http://localhost:8983/solr/gettingstarted/schema/ returns the schema in JSON format.
+     */
     public RestJsonSchemaBuilder(String solrHost, String solrPort, String collection) throws IOException {
         this(new URL("http://" + solrHost + ":" + solrPort + "/solr/" + collection + "/schema/"));
     }

File: external/storm-solr/src/main/java/org/apache/storm/solr/schema/builder/RestJsonSchemaBuilderV2.java
Patch:
@@ -32,7 +32,7 @@
 import org.slf4j.LoggerFactory;
 
 /**
- * Class that builds the {@link Schema} object from the schema returned by the SchemaRequest
+ * Class that builds the {@link Schema} object from the schema returned by the SchemaRequest.
  */
 public class RestJsonSchemaBuilderV2 implements SchemaBuilder {
     private static final Logger logger = LoggerFactory.getLogger(RestJsonSchemaBuilderV2.class);

File: external/storm-solr/src/main/java/org/apache/storm/solr/trident/SolrState.java
Patch:
@@ -47,10 +47,10 @@ protected void prepare() {
     }
 
     @Override
-    public void beginCommit(Long aLong) { }
+    public void beginCommit(Long someLong) { }
 
     @Override
-    public void commit(Long aLong) { }
+    public void commit(Long someLong) { }
 
     public void updateState(List<TridentTuple> tuples) {
         try {

File: external/storm-solr/src/main/java/org/apache/storm/solr/trident/SolrStateFactory.java
Patch:
@@ -29,7 +29,7 @@ public SolrStateFactory(SolrConfig solrConfig, SolrMapper solrMapper) {
     }
 
     @Override
-    public State makeState(Map<String, Object> map, IMetricsContext iMetricsContext, int partitionIndex, int numPartitions) {
+    public State makeState(Map<String, Object> map, IMetricsContext metricsContext, int partitionIndex, int numPartitions) {
         SolrState state = new SolrState(solrConfig, solrMapper);
         state.prepare();
         return state;

File: examples/storm-hbase-examples/src/main/java/org/apache/storm/hbase/topology/TotalWordCounter.java
Patch:
@@ -12,6 +12,8 @@
 
 package org.apache.storm.hbase.topology;
 
+import static org.apache.storm.utils.Utils.tuple;
+
 import java.math.BigInteger;
 import java.util.Map;
 import java.util.Random;
@@ -24,8 +26,6 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import static org.apache.storm.utils.Utils.tuple;
-
 public class TotalWordCounter implements IBasicBolt {
 
     private static final Logger LOG = LoggerFactory.getLogger(TotalWordCounter.class);

File: examples/storm-hbase-examples/src/main/java/org/apache/storm/hbase/topology/WordCountClient.java
Patch:
@@ -24,7 +24,7 @@
 /**
  * Connects to the 'WordCount' table and prints counts for each word.
  *
- * Assumes you have run (or are running) <code>PersistentWordCount</code>
+ * <p>Assumes you have run (or are running) <code>PersistentWordCount</code>
  */
 public class WordCountClient {
 

File: examples/storm-hbase-examples/src/main/java/org/apache/storm/hbase/topology/WordCounter.java
Patch:
@@ -12,6 +12,8 @@
 
 package org.apache.storm.hbase.topology;
 
+import static org.apache.storm.utils.Utils.tuple;
+
 import java.util.Map;
 import org.apache.storm.task.TopologyContext;
 import org.apache.storm.topology.BasicOutputCollector;
@@ -20,8 +22,6 @@
 import org.apache.storm.tuple.Fields;
 import org.apache.storm.tuple.Tuple;
 
-import static org.apache.storm.utils.Utils.tuple;
-
 public class WordCounter implements IBasicBolt {
 
 

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/bolt/EventHubBolt.java
Patch:
@@ -18,7 +18,6 @@
 
 package org.apache.storm.eventhubs.bolt;
 
-
 import com.microsoft.azure.eventhubs.EventData;
 import com.microsoft.azure.eventhubs.EventHubClient;
 import com.microsoft.azure.eventhubs.PartitionSender;

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/bolt/IEventDataFormat.java
Patch:
@@ -22,7 +22,7 @@
 import org.apache.storm.tuple.Tuple;
 
 /**
- * Serialize a tuple to a byte array to be sent to EventHubs
+ * Serialize a tuple to a byte array to be sent to EventHubs.
  */
 public interface IEventDataFormat extends Serializable {
     public byte[] serialize(Tuple tuple);

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/spout/BinaryEventDataScheme.java
Patch:
@@ -31,7 +31,7 @@
 /**
  * An Event Data Scheme which deserializes message payload into the raw bytes.
  *
- * The resulting tuple would contain three items, the first being the message bytes, and the second a map of properties that include
+ * <p>The resulting tuple would contain three items, the first being the message bytes, and the second a map of properties that include
  * metadata, which can be used to determine who processes the message, and how it is processed.The third is the system properties which
  * exposes information like enqueue-time, offset and sequence number
  */

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/spout/EventDataWrap.java
Patch:
@@ -43,7 +43,6 @@ public MessageId getMessageId() {
 
     @Override
     public int compareTo(EventDataWrap ed) {
-        return messageId.getSequenceNumber().
-            compareTo(ed.getMessageId().getSequenceNumber());
+        return messageId.getSequenceNumber().compareTo(ed.getMessageId().getSequenceNumber());
     }
 }

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/spout/IEventHubReceiverFactory.java
Patch:
@@ -18,7 +18,7 @@
 import java.io.Serializable;
 
 /**
- * An abstract factory to generate EventHubReceiver
+ * An abstract factory to generate EventHubReceiver.
  */
 public interface IEventHubReceiverFactory extends Serializable {
     IEventHubReceiver create(EventHubSpoutConfig config, String partitionId);

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/spout/IPartitionManagerFactory.java
Patch:
@@ -18,7 +18,7 @@
 import java.io.Serializable;
 
 /**
- * An interface of factory method to create IPartitionManager
+ * An interface of factory method to create IPartitionManager.
  */
 public interface IPartitionManagerFactory extends Serializable {
     IPartitionManager create(EventHubSpoutConfig spoutConfig,

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/spout/SimplePartitionManager.java
Patch:
@@ -24,7 +24,7 @@
 import org.slf4j.LoggerFactory;
 
 /**
- * A simple partition manager that does not re-send failed messages
+ * A simple partition manager that does not re-send failed messages.
  */
 public class SimplePartitionManager implements IPartitionManager {
     private static final Logger logger = LoggerFactory.getLogger(SimplePartitionManager.class);

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/trident/ITridentPartitionManager.java
Patch:
@@ -27,7 +27,7 @@ public interface ITridentPartitionManager {
     void close();
 
     /**
-     * receive a batch of messages from EvenHub up to "count" messages
+     * receive a batch of messages from EvenHub up to "count" messages.
      *
      * @param offset the starting offset
      * @param count  max number of messages in this batch

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/trident/OpaqueTridentEventHubEmitter.java
Patch:
@@ -27,7 +27,7 @@
 import org.apache.storm.trident.topology.TransactionAttempt;
 
 /**
- * A thin wrapper of TransactionalTridentEventHubEmitter for OpaqueTridentEventHubSpout
+ * A thin wrapper of TransactionalTridentEventHubEmitter for OpaqueTridentEventHubSpout.
  */
 public class OpaqueTridentEventHubEmitter implements IOpaquePartitionedTridentSpout.Emitter<Partitions, Partition, Map> {
     private final TransactionalTridentEventHubEmitter transactionalEmitter;

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/trident/OpaqueTridentEventHubSpout.java
Patch:
@@ -26,7 +26,7 @@
 import org.apache.storm.tuple.Fields;
 
 /**
- * Opaque Trident EventHubs Spout
+ * Opaque Trident EventHubs Spout.
  */
 public class OpaqueTridentEventHubSpout implements IOpaquePartitionedTridentSpout<Partitions, Partition, Map> {
     private static final long serialVersionUID = 1L;

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/trident/Partition.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.storm.trident.spout.ISpoutPartition;
 
 /**
- * Represents an EventHub partition
+ * Represents an EventHub partition.
  */
 public class Partition implements ISpoutPartition, Serializable {
     private static final long serialVersionUID = 1L;

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/trident/TransactionalTridentEventHubEmitter.java
Patch:
@@ -85,8 +85,6 @@ public void close() {
 
     /**
      * Check if partition manager for a given partiton is created if not, create it.
-     *
-     * @param partition
      */
     private ITridentPartitionManager getOrCreatePartitionManager(Partition partition) {
         ITridentPartitionManager pm;

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/trident/TransactionalTridentEventHubSpout.java
Patch:
@@ -26,7 +26,7 @@
 import org.apache.storm.tuple.Fields;
 
 /**
- * Transactional Trident EventHub Spout
+ * Transactional Trident EventHub Spout.
  */
 public class TransactionalTridentEventHubSpout implements
                                                IPartitionedTridentSpout<Partitions, Partition, Map<String, Object>> {

File: examples/storm-starter/src/jvm/org/apache/storm/starter/BasicDRPCTopology.java
Patch:
@@ -29,6 +29,7 @@
  *
  * @see <a href="http://storm.apache.org/documentation/Distributed-RPC.html">Distributed RPC</a>
  */
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public class BasicDRPCTopology {
     public static void main(String[] args) throws Exception {
         Config conf = new Config();

File: examples/storm-starter/src/jvm/org/apache/storm/starter/ManualDRPC.java
Patch:
@@ -25,7 +25,9 @@
 import org.apache.storm.tuple.Values;
 import org.apache.storm.utils.DRPCClient;
 
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public class ManualDRPC {
+
     public static void main(String[] args) throws Exception {
         TopologyBuilder builder = new TopologyBuilder();
 

File: examples/storm-starter/src/jvm/org/apache/storm/starter/PersistentWindowingTopology.java
Patch:
@@ -18,6 +18,8 @@
 
 package org.apache.storm.starter;
 
+import static org.apache.storm.topology.base.BaseWindowedBolt.Duration;
+
 import java.util.Iterator;
 import java.util.Map;
 import java.util.concurrent.TimeUnit;
@@ -38,8 +40,6 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import static org.apache.storm.topology.base.BaseWindowedBolt.Duration;
-
 /**
  * An example that demonstrates the usage of {@link org.apache.storm.topology.IStatefulWindowedBolt} with window persistence.
  * <p>

File: examples/storm-starter/src/jvm/org/apache/storm/starter/SingleJoinExample.java
Patch:
@@ -21,10 +21,10 @@
 import org.apache.storm.tuple.Values;
 import org.apache.storm.utils.NimbusClient;
 
-/** Example of using a simple custom join bolt
- *  NOTE: Prefer to use the built-in JoinBolt wherever applicable
+/**
+ * Example of using a simple custom join bolt.
+ * NOTE: Prefer to use the built-in JoinBolt wherever applicable
  */
-
 public class SingleJoinExample {
     public static void main(String[] args) throws Exception {
         if (!NimbusClient.isLocalOverride()) {

File: examples/storm-starter/src/jvm/org/apache/storm/starter/bolt/SlidingWindowSumBolt.java
Patch:
@@ -26,7 +26,7 @@
 import org.slf4j.LoggerFactory;
 
 /**
- * Computes sliding window sum
+ * Computes sliding window sum.
  */
 public class SlidingWindowSumBolt extends BaseWindowedBolt {
     private static final Logger LOG = LoggerFactory.getLogger(SlidingWindowSumBolt.class);

File: examples/storm-starter/src/jvm/org/apache/storm/starter/streams/AggregateExample.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.storm.topology.base.BaseWindowedBolt;
 
 /**
- * An example that illustrates the global aggregate
+ * An example that illustrates the global aggregate.
  */
 public class AggregateExample {
     @SuppressWarnings("unchecked")

File: examples/storm-starter/src/jvm/org/apache/storm/starter/streams/GroupByKeyAndWindowExample.java
Patch:
@@ -34,7 +34,7 @@
 
 /**
  * An example that shows the usage of {@link PairStream#groupByKeyAndWindow(Window)}
- * and {@link PairStream#reduceByKeyAndWindow(Reducer, Window)}
+ * and {@link PairStream#reduceByKeyAndWindow(Reducer, Window)}.
  */
 public class GroupByKeyAndWindowExample {
     public static void main(String[] args) throws Exception {

File: examples/storm-starter/src/jvm/org/apache/storm/starter/streams/JoinExample.java
Patch:
@@ -70,7 +70,7 @@ public static void main(String[] args) throws Exception {
     private static class NumberSpout extends BaseRichSpout {
         private final Function<Integer, Integer> function;
         private SpoutOutputCollector collector;
-        private int i = 1;
+        private int count = 1;
 
         NumberSpout(Function<Integer, Integer> function) {
             this.function = function;
@@ -84,8 +84,8 @@ public void open(Map<String, Object> conf, TopologyContext context, SpoutOutputC
         @Override
         public void nextTuple() {
             Utils.sleep(990);
-            collector.emit(new Values(i, function.apply(i)));
-            i++;
+            collector.emit(new Values(count, function.apply(count)));
+            count++;
         }
 
         @Override

File: examples/storm-starter/src/jvm/org/apache/storm/starter/streams/WindowedWordCount.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.storm.topology.base.BaseWindowedBolt.Duration;
 
 /**
- * A windowed word count example
+ * A windowed word count example.
  */
 public class WindowedWordCount {
     public static void main(String[] args) throws Exception {

File: examples/storm-starter/src/jvm/org/apache/storm/starter/streams/WordCountToBolt.java
Patch:
@@ -27,7 +27,7 @@
 
 /**
  * An example that computes word counts and finally emits the results to an
- * external bolt (sink)
+ * external bolt (sink).
  */
 public class WordCountToBolt {
     public static void main(String[] args) throws Exception {

File: examples/storm-starter/src/jvm/org/apache/storm/starter/tools/RankableObjectWithFields.java
Patch:
@@ -74,6 +74,7 @@ public long getCount() {
     }
 
     /**
+     * Get fields.
      * @return an immutable list of any additional data fields of the object (may be empty but will never be null)
      */
     public List<Object> getFields() {
@@ -131,8 +132,6 @@ public String toString() {
     /**
      * Note: We do not defensively copy the wrapped object and any accompanying fields.  We do guarantee, however,
      * do return a defensive (shallow) copy of the List object that is wrapping any accompanying fields.
-     *
-     * @return
      */
     @Override
     public Rankable copy() {

File: examples/storm-starter/src/jvm/org/apache/storm/starter/tools/Rankings.java
Patch:
@@ -39,21 +39,22 @@ public Rankings(int topN) {
 
     /**
      * Copy constructor.
-     * @param other
      */
     public Rankings(Rankings other) {
         this(other.maxSize());
         updateWith(other);
     }
 
     /**
+     * Get max size.
      * @return the maximum possible number (size) of ranked objects this instance can hold
      */
     public int maxSize() {
         return maxSize;
     }
 
     /**
+     * Get size.
      * @return the number (size) of ranked objects this instance is currently holding
      */
     public int size() {

File: examples/storm-starter/src/jvm/org/apache/storm/starter/tools/SlidingWindowCounter.java
Patch:
@@ -95,6 +95,7 @@ public void incrementCount(T obj) {
      * @return The current (total) counts of all tracked objects.
      */
     public Map<T, Long> getCountsThenAdvanceWindow() {
+        @SuppressWarnings("checkstyle:VariableDeclarationUsageDistance")
         Map<T, Long> counts = objCounter.getCounts();
         objCounter.wipeZeros();
         objCounter.wipeSlot(tailSlot);

File: examples/storm-starter/src/jvm/org/apache/storm/starter/tools/SlotBasedCounter.java
Patch:
@@ -75,8 +75,6 @@ private long computeTotalCount(T obj) {
 
     /**
      * Reset the slot count of any tracked objects to zero for the given slot.
-     *
-     * @param slot
      */
     public void wipeSlot(int slot) {
         for (T obj : objToCounts.keySet()) {

File: examples/storm-starter/src/jvm/org/apache/storm/starter/trident/DebugMemoryMapState.java
Patch:
@@ -59,15 +59,15 @@ private void print(List<List<Object>> keys, List<ValueUpdater> updaters) {
     }
 
     public static class Factory implements StateFactory {
-        String _id;
+        String id;
 
         public Factory() {
-            _id = UUID.randomUUID().toString();
+            id = UUID.randomUUID().toString();
         }
 
         @Override
         public State makeState(Map<String, Object> conf, IMetricsContext metrics, int partitionIndex, int numPartitions) {
-            return new DebugMemoryMapState(_id + partitionIndex);
+            return new DebugMemoryMapState(id + partitionIndex);
         }
     }
 }

File: external/storm-blobstore-migration/src/main/java/org/apache/storm/blobstore/ListLocalFs.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.blobstore;
 
 import java.util.Map;
@@ -30,7 +31,7 @@ public class ListLocalFs {
     
     public static void main(String[] args) throws Exception {
         
-        if(args.length != 1) {
+        if (args.length != 1) {
             System.out.println("Need 1 arguments, but have " + Integer.toString(args.length));
             System.out.println("listLocalFs <local_blobstore_dir>");
             System.out.println("Migrates blobs from LocalFsBlobStore to HdfsBlobStore");

File: external/storm-autocreds/src/main/java/org/apache/storm/hbase/security/AutoHBase.java
Patch:
@@ -18,11 +18,11 @@
 
 package org.apache.storm.hbase.security;
 
-import org.apache.storm.common.AbstractHadoopAutoCreds;
+import static org.apache.storm.hbase.security.HBaseSecurityUtil.HBASE_CREDENTIALS;
 
 import java.util.Map;
 
-import static org.apache.storm.hbase.security.HBaseSecurityUtil.HBASE_CREDENTIALS;
+import org.apache.storm.common.AbstractHadoopAutoCreds;
 
 /**
  * Auto credentials plugin for HBase implementation. This class provides a way to automatically

File: external/storm-autocreds/src/main/java/org/apache/storm/hdfs/security/AutoHDFS.java
Patch:
@@ -18,16 +18,17 @@
 
 package org.apache.storm.hdfs.security;
 
-import org.apache.storm.common.AbstractHadoopAutoCreds;
+import static org.apache.storm.hdfs.security.HdfsSecurityUtil.HDFS_CREDENTIALS;
 
 import java.util.Map;
 
-import static org.apache.storm.hdfs.security.HdfsSecurityUtil.HDFS_CREDENTIALS;
+import org.apache.storm.common.AbstractHadoopAutoCreds;
 
 /**
  * Auto credentials plugin for HDFS implementation. This class provides a way to automatically
  * push credentials to a topology and to retrieve them in the worker.
  */
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public class AutoHDFS extends AbstractHadoopAutoCreds {
     @Override
     public void doPrepare(Map<String, Object> conf) {

File: external/storm-autocreds/src/main/java/org/apache/storm/hive/security/AutoHive.java
Patch:
@@ -18,12 +18,12 @@
 
 package org.apache.storm.hive.security;
 
-import org.apache.storm.common.AbstractHadoopAutoCreds;
+import static org.apache.storm.hive.security.HiveSecurityUtil.HIVE_CREDENTIALS;
+import static org.apache.storm.hive.security.HiveSecurityUtil.HIVE_CREDENTIALS_CONFIG_KEYS;
 
 import java.util.Map;
 
-import static org.apache.storm.hive.security.HiveSecurityUtil.HIVE_CREDENTIALS;
-import static org.apache.storm.hive.security.HiveSecurityUtil.HIVE_CREDENTIALS_CONFIG_KEYS;
+import org.apache.storm.common.AbstractHadoopAutoCreds;
 
 /**
  * Auto credentials plugin for Hive implementation. This class provides a way to automatically

File: storm-client/src/jvm/org/apache/storm/blobstore/ClientBlobStore.java
Patch:
@@ -39,7 +39,6 @@
  * @see org.apache.storm.blobstore.NimbusBlobStore
  */
 public abstract class ClientBlobStore implements Shutdownable, AutoCloseable {
-    protected Map<String, Object> conf;
 
     public static void withConfiguredClient(WithBlobstore withBlobstore) throws Exception {
         Map<String, Object> conf = ConfigUtils.readStormConfig();

File: storm-client/test/jvm/org/apache/storm/blobstore/ClientBlobStoreTest.java
Patch:
@@ -111,7 +111,6 @@ public class TestClientBlobStore extends ClientBlobStore {
 
         @Override
         public void prepare(Map<String, Object> conf) {
-            this.conf = conf;
             allBlobs = new HashMap<String, SettableBlobMeta>();
         }
 

File: external/storm-jms/src/main/java/org/apache/storm/jms/spout/JmsSpout.java
Patch:
@@ -19,9 +19,9 @@
 package org.apache.storm.jms.spout;
 
 import java.io.Serializable;
+import java.util.Collections;
 import java.util.HashMap;
 import java.util.Map;
-import java.util.Collections;
 
 import javax.jms.Connection;
 import javax.jms.ConnectionFactory;

File: external/storm-jms/src/main/java/org/apache/storm/jms/trident/JmsState.java
Patch:
@@ -63,11 +63,11 @@ protected void prepare() {
     }
 
     @Override
-    public void beginCommit(Long aLong) {
+    public void beginCommit(Long someLong) {
     }
 
     @Override
-    public void commit(Long aLong) {
+    public void commit(Long someLong) {
         LOG.debug("Committing JMS transaction.");
         if (this.options.jmsTransactional) {
             try {

File: external/storm-jms/src/main/java/org/apache/storm/jms/trident/JmsStateFactory.java
Patch:
@@ -26,7 +26,7 @@ public JmsStateFactory(JmsState.Options options) {
     }
 
     @Override
-    public State makeState(Map<String, Object> map, IMetricsContext iMetricsContext, int partitionIndex, int numPartitions) {
+    public State makeState(Map<String, Object> map, IMetricsContext metricsContext, int partitionIndex, int numPartitions) {
         JmsState state = new JmsState(options);
         state.prepare();
         return state;

File: examples/storm-perf/src/main/java/org/apache/storm/perf/ConstSpoutIdBoltNullBoltTopo.java
Patch:
@@ -30,7 +30,7 @@
 
 /**
  * ConstSpout -> IdBolt -> DevNullBolt This topology measures speed of messaging between spouts->bolt  and  bolt->bolt ConstSpout :
- * Continuously emits a constant string IdBolt : clones and emits input tuples DevNullBolt : discards incoming tuples
+ * Continuously emits a constant string IdBolt : clones and emits input tuples DevNullBolt : discards incoming tuples.
  */
 public class ConstSpoutIdBoltNullBoltTopo {
 

File: examples/storm-perf/src/main/java/org/apache/storm/perf/FileReadWordCountTopo.java
Patch:
@@ -29,11 +29,11 @@
 import org.apache.storm.tuple.Fields;
 import org.apache.storm.utils.Utils;
 
-/***
+/**
  * This topo helps measure speed of word count.
- *  Spout loads a file into memory on initialization, then emits the lines in an endless loop.
+ *
+ * <p>Spout loads a file into memory on initialization, then emits the lines in an endless loop.
  */
-
 public class FileReadWordCountTopo {
     public static final String SPOUT_ID = "spout";
     public static final String COUNT_ID = "counter";

File: examples/storm-perf/src/main/java/org/apache/storm/perf/LowThroughputTopo.java
Patch:
@@ -18,7 +18,6 @@
 
 package org.apache.storm.perf;
 
-
 import java.util.Collections;
 import java.util.List;
 import java.util.Map;

File: examples/storm-perf/src/main/java/org/apache/storm/perf/ThroughputMeter.java
Patch:
@@ -31,6 +31,7 @@ public ThroughputMeter(String name) {
     }
 
     /**
+     * Calculate throughput.
      * @return events/sec
      */
     private static double calcThroughput(long count, long startTime, long endTime) {

File: examples/storm-perf/src/main/java/org/apache/storm/perf/bolt/DevNullBolt.java
Patch:
@@ -33,7 +33,7 @@ public class DevNullBolt extends BaseRichBolt {
     private static final org.slf4j.Logger LOG = LoggerFactory.getLogger(DevNullBolt.class);
     private OutputCollector collector;
     private Long sleepNanos;
-    private int eCount = 0;
+    private int count = 0;
 
     @Override
     public void prepare(Map<String, Object> topoConf, TopologyContext context, OutputCollector collector) {
@@ -47,7 +47,7 @@ public void execute(Tuple tuple) {
         if (sleepNanos > 0) {
             LockSupport.parkNanos(sleepNanos);
         }
-        ++eCount;
+        ++count;
     }
 
     @Override

File: examples/storm-perf/src/main/java/org/apache/storm/perf/spout/WordGenSpout.java
Patch:
@@ -63,8 +63,9 @@ public static ArrayList<String> readWords(String file) {
             try {
                 String line;
                 while ((line = reader.readLine()) != null) {
-                    for (String word : line.split("\\s+"))
+                    for (String word : line.split("\\s+")) {
                         lines.add(word);
+                    }
                 }
             } catch (IOException e) {
                 throw new RuntimeException("Reading file failed", e);

File: examples/storm-perf/src/main/java/org/apache/storm/perf/utils/Helper.java
Patch:
@@ -68,7 +68,7 @@ public static void collectMetricsAndKill(String topologyName, Integer pollInterv
     }
 
     /**
-     * Kill topo on Ctrl-C
+     * Kill topo on Ctrl-C.
      */
     public static void setupShutdownHook(final String topoName) {
         Map<String, Object> clusterConf = Utils.readStormConfig();

File: examples/storm-perf/src/main/java/org/apache/storm/perf/utils/MetricsSample.java
Patch:
@@ -82,8 +82,6 @@ private static MetricsSample getMetricsSample(TopologyInfo topInfo) {
         // number of spout executors
         int spoutExecCount = 0;
         double spoutLatencySum = 0.0;
-
-        long spoutEmitted = 0L;
         long spoutTransferred = 0L;
 
         // Executor summaries
@@ -156,6 +154,8 @@ private static MetricsSample getMetricsSample(TopologyInfo topInfo) {
         ret.totalAcked = totalAcked;
         ret.totalFailed = totalFailed;
         ret.totalLatency = spoutLatencySum / spoutExecCount;
+
+        long spoutEmitted = 0L;
         ret.spoutEmitted = spoutEmitted;
         ret.spoutTransferred = spoutTransferred;
         ret.sampleTime = System.currentTimeMillis();

File: storm-core/src/jvm/org/apache/storm/command/CLI.java
Patch:
@@ -23,6 +23,7 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public class CLI {
     /**
      * Parse function to return an Integer.
@@ -250,6 +251,7 @@ public Object process(Object current, String value) {
         }
     }
 
+    @SuppressWarnings("checkstyle:AbbreviationAsWordInName")
     public static class CLIBuilder {
         private final ArrayList<Opt> opts = new ArrayList<>();
         private final ArrayList<Arg> args = new ArrayList<>();

File: storm-core/src/jvm/org/apache/storm/command/ShellSubmission.java
Patch:
@@ -12,6 +12,9 @@
 
 package org.apache.storm.command;
 
+import java.util.Arrays;
+import java.util.Map;
+
 import org.apache.commons.lang.ArrayUtils;
 import org.apache.storm.StormSubmitter;
 import org.apache.storm.generated.NimbusSummary;
@@ -21,9 +24,6 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import java.util.Arrays;
-import java.util.Map;
-
 public class ShellSubmission {
     private static final Logger LOG = LoggerFactory.getLogger(ShellSubmission.class);
 

File: external/storm-autocreds/src/main/java/org/apache/storm/hbase/security/AutoHBase.java
Patch:
@@ -18,11 +18,11 @@
 
 package org.apache.storm.hbase.security;
 
-import org.apache.storm.common.AbstractHadoopAutoCreds;
+import static org.apache.storm.hbase.security.HBaseSecurityUtil.HBASE_CREDENTIALS;
 
 import java.util.Map;
 
-import static org.apache.storm.hbase.security.HBaseSecurityUtil.HBASE_CREDENTIALS;
+import org.apache.storm.common.AbstractHadoopAutoCreds;
 
 /**
  * Auto credentials plugin for HBase implementation. This class provides a way to automatically

File: external/storm-autocreds/src/main/java/org/apache/storm/hdfs/security/AutoHDFS.java
Patch:
@@ -18,16 +18,17 @@
 
 package org.apache.storm.hdfs.security;
 
-import org.apache.storm.common.AbstractHadoopAutoCreds;
+import static org.apache.storm.hdfs.security.HdfsSecurityUtil.HDFS_CREDENTIALS;
 
 import java.util.Map;
 
-import static org.apache.storm.hdfs.security.HdfsSecurityUtil.HDFS_CREDENTIALS;
+import org.apache.storm.common.AbstractHadoopAutoCreds;
 
 /**
  * Auto credentials plugin for HDFS implementation. This class provides a way to automatically
  * push credentials to a topology and to retrieve them in the worker.
  */
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public class AutoHDFS extends AbstractHadoopAutoCreds {
     @Override
     public void doPrepare(Map<String, Object> conf) {

File: external/storm-autocreds/src/main/java/org/apache/storm/hive/security/AutoHive.java
Patch:
@@ -18,12 +18,12 @@
 
 package org.apache.storm.hive.security;
 
-import org.apache.storm.common.AbstractHadoopAutoCreds;
+import static org.apache.storm.hive.security.HiveSecurityUtil.HIVE_CREDENTIALS;
+import static org.apache.storm.hive.security.HiveSecurityUtil.HIVE_CREDENTIALS_CONFIG_KEYS;
 
 import java.util.Map;
 
-import static org.apache.storm.hive.security.HiveSecurityUtil.HIVE_CREDENTIALS;
-import static org.apache.storm.hive.security.HiveSecurityUtil.HIVE_CREDENTIALS_CONFIG_KEYS;
+import org.apache.storm.common.AbstractHadoopAutoCreds;
 
 /**
  * Auto credentials plugin for Hive implementation. This class provides a way to automatically

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/bolt/AbstractHBaseBolt.java
Patch:
@@ -33,6 +33,7 @@ public abstract class AbstractHBaseBolt extends BaseRichBolt {
     private static final Logger LOG = LoggerFactory.getLogger(AbstractHBaseBolt.class);
 
     protected transient OutputCollector collector;
+    @SuppressWarnings("checkstyle:MemberName")
     protected transient HBaseClient hBaseClient;
     protected String tableName;
     protected HBaseMapper mapper;

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/bolt/HBaseLookupBolt.java
Patch:
@@ -36,7 +36,7 @@
 /**
  * Basic bolt for querying from HBase.
  *
- * Note: Each HBaseBolt defined in a topology is tied to a specific table.
+ * <p>Note: Each HBaseBolt defined in a topology is tied to a specific table.
  */
 public class HBaseLookupBolt extends AbstractHBaseBolt {
     private static final Logger LOG = LoggerFactory.getLogger(HBaseLookupBolt.class);
@@ -66,10 +66,10 @@ public HBaseLookupBolt withProjectionCriteria(HBaseProjectionCriteria projection
     public void prepare(Map<String, Object> config, TopologyContext topologyContext, OutputCollector collector) {
         super.prepare(config, topologyContext, collector);
         cacheEnabled = Boolean.parseBoolean(config.getOrDefault("hbase.cache.enable", "false").toString());
-        int cacheTTL = Integer.parseInt(config.getOrDefault("hbase.cache.ttl.seconds", "300").toString());
+        int cacheTtl = Integer.parseInt(config.getOrDefault("hbase.cache.ttl.seconds", "300").toString());
         int maxCacheSize = Integer.parseInt(config.getOrDefault("hbase.cache.size", "1000").toString());
         if (cacheEnabled) {
-            cache = Caffeine.newBuilder().maximumSize(maxCacheSize).expireAfterWrite(cacheTTL, TimeUnit.SECONDS)
+            cache = Caffeine.newBuilder().maximumSize(maxCacheSize).expireAfterWrite(cacheTtl, TimeUnit.SECONDS)
                             .build(new CacheLoader<byte[], Result>() {
 
                                 @Override

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/common/Utils.java
Patch:
@@ -64,8 +64,8 @@ public static Table getTable(UserProvider provider, Configuration config, String
 
                             foundHBaseAuthToken = true;
                         } else {
-                            LOG.warn("Found multiple HBASE_AUTH_TOKEN - will use already found token. " +
-                                     "Please enable DEBUG log level to track delegation tokens.");
+                            LOG.warn("Found multiple HBASE_AUTH_TOKEN - will use already found token. "
+                                    + "Please enable DEBUG log level to track delegation tokens.");
                         }
                     }
                 }

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/state/HBaseKeyValueStateIterator.java
Patch:
@@ -12,6 +12,8 @@
 
 package org.apache.storm.hbase.state;
 
+import static org.apache.storm.hbase.state.HBaseKeyValueState.STATE_QUALIFIER;
+
 import com.google.common.primitives.UnsignedBytes;
 import java.util.Arrays;
 import java.util.Iterator;
@@ -25,8 +27,6 @@
 import org.apache.storm.state.Serializer;
 import org.apache.storm.state.StateEncoder;
 
-import static org.apache.storm.hbase.state.HBaseKeyValueState.STATE_QUALIFIER;
-
 /**
  * An iterator over {@link HBaseKeyValueState}.
  */

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/trident/state/HBaseQuery.java
Patch:
@@ -21,6 +21,7 @@
 public class HBaseQuery extends BaseQueryFunction<HBaseState, List<Values>> {
 
     @Override
+    @SuppressWarnings("checkstyle:ParameterName")
     public List<List<Values>> batchRetrieve(HBaseState hBaseState, List<TridentTuple> tridentTuples) {
         return hBaseState.batchRetrieve(tridentTuples);
     }

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/trident/state/HBaseState.java
Patch:
@@ -43,6 +43,7 @@ public class HBaseState implements State {
     private static final Logger LOG = LoggerFactory.getLogger(HBaseState.class);
 
     private Options options;
+    @SuppressWarnings("checkstyle:MemberName")
     private HBaseClient hBaseClient;
     private Map<String, Object> map;
     private int numPartitions;
@@ -80,12 +81,12 @@ protected void prepare() {
     }
 
     @Override
-    public void beginCommit(Long aLong) {
+    public void beginCommit(Long someLong) {
         LOG.debug("beginCommit is noop.");
     }
 
     @Override
-    public void commit(Long aLong) {
+    public void commit(Long someLong) {
         LOG.debug("commit is noop.");
     }
 

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/trident/state/HBaseStateFactory.java
Patch:
@@ -26,7 +26,7 @@ public HBaseStateFactory(HBaseState.Options options) {
     }
 
     @Override
-    public State makeState(Map<String, Object> map, IMetricsContext iMetricsContext, int partitionIndex, int numPartitions) {
+    public State makeState(Map<String, Object> map, IMetricsContext metricsContext, int partitionIndex, int numPartitions) {
         HBaseState state = new HBaseState(map, partitionIndex, numPartitions, options);
         state.prepare();
         return state;

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/trident/state/HBaseUpdater.java
Patch:
@@ -20,6 +20,7 @@
 public class HBaseUpdater extends BaseStateUpdater<HBaseState> {
 
     @Override
+    @SuppressWarnings("checkstyle:ParameterName")
     public void updateState(HBaseState hBaseState, List<TridentTuple> tuples, TridentCollector collector) {
         hBaseState.updateState(tuples, collector);
     }

File: external/storm-jms/src/main/java/org/apache/storm/jms/spout/JmsSpout.java
Patch:
@@ -19,9 +19,9 @@
 package org.apache.storm.jms.spout;
 
 import java.io.Serializable;
+import java.util.Collections;
 import java.util.HashMap;
 import java.util.Map;
-import java.util.Collections;
 
 import javax.jms.Connection;
 import javax.jms.ConnectionFactory;

File: external/storm-jms/src/main/java/org/apache/storm/jms/trident/JmsState.java
Patch:
@@ -63,11 +63,11 @@ protected void prepare() {
     }
 
     @Override
-    public void beginCommit(Long aLong) {
+    public void beginCommit(Long someLong) {
     }
 
     @Override
-    public void commit(Long aLong) {
+    public void commit(Long someLong) {
         LOG.debug("Committing JMS transaction.");
         if (this.options.jmsTransactional) {
             try {

File: external/storm-jms/src/main/java/org/apache/storm/jms/trident/JmsStateFactory.java
Patch:
@@ -26,7 +26,7 @@ public JmsStateFactory(JmsState.Options options) {
     }
 
     @Override
-    public State makeState(Map<String, Object> map, IMetricsContext iMetricsContext, int partitionIndex, int numPartitions) {
+    public State makeState(Map<String, Object> map, IMetricsContext metricsContext, int partitionIndex, int numPartitions) {
         JmsState state = new JmsState(options);
         state.prepare();
         return state;

File: external/storm-redis/src/main/java/org/apache/storm/redis/common/commands/RedisCommands.java
Patch:
@@ -25,8 +25,8 @@
 /**
  * This interface represents Jedis methods exhaustively which are used on storm-redis.
  *
- * This is a workaround since Jedis and JedisCluster doesn't implement same interface for binary type of methods, and unify binary methods
- * and string methods into one interface.
+ * <p>This is a workaround since Jedis and JedisCluster doesn't implement same interface for binary type of methods, and
+ * unify binary methods and string methods into one interface.
  */
 public interface RedisCommands {
     // common

File: external/storm-redis/src/main/java/org/apache/storm/redis/common/container/JedisClusterContainer.java
Patch:
@@ -26,7 +26,7 @@ public class JedisClusterContainer implements JedisCommandsInstanceContainer {
     private JedisCluster jedisCluster;
 
     /**
-     * Constructor
+     * Constructor.
      * @param jedisCluster JedisCluster instance
      */
     public JedisClusterContainer(JedisCluster jedisCluster) {

File: external/storm-redis/src/main/java/org/apache/storm/redis/common/container/JedisCommandsInstanceContainer.java
Patch:
@@ -32,7 +32,7 @@ public interface JedisCommandsInstanceContainer extends Closeable {
     void returnInstance(JedisCommands jedisCommands);
 
     /**
-     * Release Container
+     * Release Container.
      */
     @Override
     public void close();

File: external/storm-redis/src/main/java/org/apache/storm/redis/common/container/JedisContainer.java
Patch:
@@ -28,7 +28,7 @@ public class JedisContainer implements JedisCommandsInstanceContainer {
     private JedisPool jedisPool;
 
     /**
-     * Constructor
+     * Constructor.
      * @param jedisPool JedisPool which actually manages Jedis instances
      */
     public JedisContainer(JedisPool jedisPool) {

File: external/storm-redis/src/main/java/org/apache/storm/redis/common/container/RedisClusterContainer.java
Patch:
@@ -32,7 +32,7 @@ public class RedisClusterContainer implements RedisCommandsInstanceContainer {
     private JedisCluster jedisCluster;
 
     /**
-     * Constructor
+     * Constructor.
      *
      * @param jedisCluster JedisCluster instance
      */

File: external/storm-redis/src/main/java/org/apache/storm/redis/common/container/RedisContainer.java
Patch:
@@ -32,7 +32,7 @@ public class RedisContainer implements RedisCommandsInstanceContainer {
     private JedisPool jedisPool;
 
     /**
-     * Constructor
+     * Constructor.
      *
      * @param jedisPool JedisPool which actually manages Jedis instances
      */

File: external/storm-redis/src/main/java/org/apache/storm/redis/state/RedisKeyValueState.java
Patch:
@@ -299,8 +299,8 @@ private void validatePrepareTxid(long txid) {
         Long committedTxid = lastCommittedTxid();
         if (committedTxid != null) {
             if (txid <= committedTxid) {
-                throw new RuntimeException("Invalid txid '" + txid + "' for prepare. Txid '" + committedTxid +
-                                           "' is already committed");
+                throw new RuntimeException("Invalid txid '" + txid + "' for prepare. Txid '" + committedTxid
+                        + "' is already committed");
             }
         }
     }

File: external/storm-redis/src/main/java/org/apache/storm/redis/trident/state/AbstractRedisStateQuerier.java
Patch:
@@ -36,7 +36,7 @@ public abstract class AbstractRedisStateQuerier<T extends State> extends BaseQue
     private final RedisLookupMapper lookupMapper;
 
     /**
-     * Constructor
+     * Constructor.
      *
      * @param lookupMapper mapper for querying
      */

File: external/storm-redis/src/main/java/org/apache/storm/redis/trident/state/KeyFactory.java
Patch:
@@ -27,7 +27,7 @@ public interface KeyFactory extends Serializable {
     String build(List<Object> key);
 
     /**
-     * Default Key Factory
+     * Default Key Factory.
      */
     class DefaultKeyFactory implements KeyFactory {
         /**

File: external/storm-redis/src/main/java/org/apache/storm/redis/trident/state/RedisClusterMapState.java
Patch:
@@ -46,7 +46,7 @@ public class RedisClusterMapState<T> extends AbstractRedisMapState<T> {
     private KeyFactory keyFactory;
 
     /**
-     * Constructor
+     * Constructor.
      *
      * @param jedisCluster JedisCluster
      * @param options options of State
@@ -287,7 +287,7 @@ protected static class Factory implements StateFactory {
         Options options;
 
         /**
-         * Constructor
+         * Constructor.
          *
          * @param jedisClusterConfig configuration for JedisCluster
          * @param type StateType

File: external/storm-redis/src/main/java/org/apache/storm/redis/trident/state/RedisClusterStateQuerier.java
Patch:
@@ -24,7 +24,7 @@
  */
 public class RedisClusterStateQuerier extends AbstractRedisStateQuerier<RedisClusterState> {
     /**
-     * Constructor
+     * Constructor.
      *
      * @param lookupMapper mapper for querying
      */

File: external/storm-redis/src/main/java/org/apache/storm/redis/trident/state/RedisClusterStateUpdater.java
Patch:
@@ -24,7 +24,7 @@
  */
 public class RedisClusterStateUpdater extends AbstractRedisStateUpdater<RedisClusterState> {
     /**
-     * Constructor
+     * Constructor.
      *
      * @param storeMapper mapper for storing
      */
@@ -74,8 +74,8 @@ protected void updateStatesToRedis(RedisClusterState redisClusterState, Map<Stri
 
             // send expire command for hash only once
             // it expires key itself entirely, so use it with caution
-            if (dataType == RedisDataTypeDescription.RedisDataType.HASH &&
-                this.expireIntervalSec > 0) {
+            if (dataType == RedisDataTypeDescription.RedisDataType.HASH
+                    && this.expireIntervalSec > 0) {
                 jedisCluster.expire(additionalKey, expireIntervalSec);
             }
         } finally {

File: external/storm-redis/src/main/java/org/apache/storm/redis/trident/state/RedisMapState.java
Patch:
@@ -47,7 +47,7 @@ public class RedisMapState<T> extends AbstractRedisMapState<T> {
     private KeyFactory keyFactory;
 
     /**
-     * Constructor
+     * Constructor.
      *
      * @param jedisPool JedisPool
      * @param options options of State
@@ -317,7 +317,7 @@ protected static class Factory implements StateFactory {
         Options options;
 
         /**
-         * Constructor
+         * Constructor.
          *
          * @param jedisPoolConfig configuration for JedisPool
          * @param type StateType

File: external/storm-redis/src/main/java/org/apache/storm/redis/trident/state/RedisStateQuerier.java
Patch:
@@ -23,7 +23,7 @@
  */
 public class RedisStateQuerier extends AbstractRedisStateQuerier<RedisState> {
     /**
-     * Constructor
+     * Constructor.
      *
      * @param lookupMapper mapper for querying
      */

File: external/storm-redis/src/main/java/org/apache/storm/redis/trident/state/RedisStateUpdater.java
Patch:
@@ -25,7 +25,7 @@
  */
 public class RedisStateUpdater extends AbstractRedisStateUpdater<RedisState> {
     /**
-     * Constructor
+     * Constructor.
      *
      * @param storeMapper mapper for storing
      */
@@ -76,8 +76,8 @@ protected void updateStatesToRedis(RedisState redisState, Map<String, String> ke
 
             // send expire command for hash only once
             // it expires key itself entirely, so use it with caution
-            if (dataType == RedisDataTypeDescription.RedisDataType.HASH &&
-                this.expireIntervalSec > 0) {
+            if (dataType == RedisDataTypeDescription.RedisDataType.HASH
+                    && this.expireIntervalSec > 0) {
                 pipeline.expire(additionalKey, expireIntervalSec);
             }
 

File: storm-client/src/jvm/org/apache/storm/blobstore/ClientBlobStore.java
Patch:
@@ -39,7 +39,6 @@
  * @see org.apache.storm.blobstore.NimbusBlobStore
  */
 public abstract class ClientBlobStore implements Shutdownable, AutoCloseable {
-    protected Map<String, Object> conf;
 
     public static void withConfiguredClient(WithBlobstore withBlobstore) throws Exception {
         Map<String, Object> conf = ConfigUtils.readStormConfig();

File: storm-client/test/jvm/org/apache/storm/blobstore/ClientBlobStoreTest.java
Patch:
@@ -111,7 +111,6 @@ public class TestClientBlobStore extends ClientBlobStore {
 
         @Override
         public void prepare(Map<String, Object> conf) {
-            this.conf = conf;
             allBlobs = new HashMap<String, SettableBlobMeta>();
         }
 

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/bolt/AbstractHBaseBolt.java
Patch:
@@ -33,6 +33,7 @@ public abstract class AbstractHBaseBolt extends BaseRichBolt {
     private static final Logger LOG = LoggerFactory.getLogger(AbstractHBaseBolt.class);
 
     protected transient OutputCollector collector;
+    @SuppressWarnings("checkstyle:MemberName")
     protected transient HBaseClient hBaseClient;
     protected String tableName;
     protected HBaseMapper mapper;

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/bolt/HBaseLookupBolt.java
Patch:
@@ -36,7 +36,7 @@
 /**
  * Basic bolt for querying from HBase.
  *
- * Note: Each HBaseBolt defined in a topology is tied to a specific table.
+ * <p>Note: Each HBaseBolt defined in a topology is tied to a specific table.
  */
 public class HBaseLookupBolt extends AbstractHBaseBolt {
     private static final Logger LOG = LoggerFactory.getLogger(HBaseLookupBolt.class);
@@ -66,10 +66,10 @@ public HBaseLookupBolt withProjectionCriteria(HBaseProjectionCriteria projection
     public void prepare(Map<String, Object> config, TopologyContext topologyContext, OutputCollector collector) {
         super.prepare(config, topologyContext, collector);
         cacheEnabled = Boolean.parseBoolean(config.getOrDefault("hbase.cache.enable", "false").toString());
-        int cacheTTL = Integer.parseInt(config.getOrDefault("hbase.cache.ttl.seconds", "300").toString());
+        int cacheTtl = Integer.parseInt(config.getOrDefault("hbase.cache.ttl.seconds", "300").toString());
         int maxCacheSize = Integer.parseInt(config.getOrDefault("hbase.cache.size", "1000").toString());
         if (cacheEnabled) {
-            cache = Caffeine.newBuilder().maximumSize(maxCacheSize).expireAfterWrite(cacheTTL, TimeUnit.SECONDS)
+            cache = Caffeine.newBuilder().maximumSize(maxCacheSize).expireAfterWrite(cacheTtl, TimeUnit.SECONDS)
                             .build(new CacheLoader<byte[], Result>() {
 
                                 @Override

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/common/Utils.java
Patch:
@@ -64,8 +64,8 @@ public static Table getTable(UserProvider provider, Configuration config, String
 
                             foundHBaseAuthToken = true;
                         } else {
-                            LOG.warn("Found multiple HBASE_AUTH_TOKEN - will use already found token. " +
-                                     "Please enable DEBUG log level to track delegation tokens.");
+                            LOG.warn("Found multiple HBASE_AUTH_TOKEN - will use already found token. "
+                                    + "Please enable DEBUG log level to track delegation tokens.");
                         }
                     }
                 }

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/state/HBaseKeyValueStateIterator.java
Patch:
@@ -12,6 +12,8 @@
 
 package org.apache.storm.hbase.state;
 
+import static org.apache.storm.hbase.state.HBaseKeyValueState.STATE_QUALIFIER;
+
 import com.google.common.primitives.UnsignedBytes;
 import java.util.Arrays;
 import java.util.Iterator;
@@ -25,8 +27,6 @@
 import org.apache.storm.state.Serializer;
 import org.apache.storm.state.StateEncoder;
 
-import static org.apache.storm.hbase.state.HBaseKeyValueState.STATE_QUALIFIER;
-
 /**
  * An iterator over {@link HBaseKeyValueState}.
  */

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/trident/state/HBaseQuery.java
Patch:
@@ -21,6 +21,7 @@
 public class HBaseQuery extends BaseQueryFunction<HBaseState, List<Values>> {
 
     @Override
+    @SuppressWarnings("checkstyle:ParameterName")
     public List<List<Values>> batchRetrieve(HBaseState hBaseState, List<TridentTuple> tridentTuples) {
         return hBaseState.batchRetrieve(tridentTuples);
     }

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/trident/state/HBaseState.java
Patch:
@@ -43,6 +43,7 @@ public class HBaseState implements State {
     private static final Logger LOG = LoggerFactory.getLogger(HBaseState.class);
 
     private Options options;
+    @SuppressWarnings("checkstyle:MemberName")
     private HBaseClient hBaseClient;
     private Map<String, Object> map;
     private int numPartitions;
@@ -80,12 +81,12 @@ protected void prepare() {
     }
 
     @Override
-    public void beginCommit(Long aLong) {
+    public void beginCommit(Long someLong) {
         LOG.debug("beginCommit is noop.");
     }
 
     @Override
-    public void commit(Long aLong) {
+    public void commit(Long someLong) {
         LOG.debug("commit is noop.");
     }
 

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/trident/state/HBaseStateFactory.java
Patch:
@@ -26,7 +26,7 @@ public HBaseStateFactory(HBaseState.Options options) {
     }
 
     @Override
-    public State makeState(Map<String, Object> map, IMetricsContext iMetricsContext, int partitionIndex, int numPartitions) {
+    public State makeState(Map<String, Object> map, IMetricsContext metricsContext, int partitionIndex, int numPartitions) {
         HBaseState state = new HBaseState(map, partitionIndex, numPartitions, options);
         state.prepare();
         return state;

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/trident/state/HBaseUpdater.java
Patch:
@@ -20,6 +20,7 @@
 public class HBaseUpdater extends BaseStateUpdater<HBaseState> {
 
     @Override
+    @SuppressWarnings("checkstyle:ParameterName")
     public void updateState(HBaseState hBaseState, List<TridentTuple> tuples, TridentCollector collector) {
         hBaseState.updateState(tuples, collector);
     }

File: examples/storm-perf/src/main/java/org/apache/storm/perf/ConstSpoutIdBoltNullBoltTopo.java
Patch:
@@ -30,7 +30,7 @@
 
 /**
  * ConstSpout -> IdBolt -> DevNullBolt This topology measures speed of messaging between spouts->bolt  and  bolt->bolt ConstSpout :
- * Continuously emits a constant string IdBolt : clones and emits input tuples DevNullBolt : discards incoming tuples
+ * Continuously emits a constant string IdBolt : clones and emits input tuples DevNullBolt : discards incoming tuples.
  */
 public class ConstSpoutIdBoltNullBoltTopo {
 

File: examples/storm-perf/src/main/java/org/apache/storm/perf/FileReadWordCountTopo.java
Patch:
@@ -29,11 +29,11 @@
 import org.apache.storm.tuple.Fields;
 import org.apache.storm.utils.Utils;
 
-/***
+/**
  * This topo helps measure speed of word count.
- *  Spout loads a file into memory on initialization, then emits the lines in an endless loop.
+ *
+ * <p>Spout loads a file into memory on initialization, then emits the lines in an endless loop.
  */
-
 public class FileReadWordCountTopo {
     public static final String SPOUT_ID = "spout";
     public static final String COUNT_ID = "counter";

File: examples/storm-perf/src/main/java/org/apache/storm/perf/LowThroughputTopo.java
Patch:
@@ -18,7 +18,6 @@
 
 package org.apache.storm.perf;
 
-
 import java.util.Collections;
 import java.util.List;
 import java.util.Map;

File: examples/storm-perf/src/main/java/org/apache/storm/perf/ThroughputMeter.java
Patch:
@@ -31,6 +31,7 @@ public ThroughputMeter(String name) {
     }
 
     /**
+     * Calculate throughput.
      * @return events/sec
      */
     private static double calcThroughput(long count, long startTime, long endTime) {

File: examples/storm-perf/src/main/java/org/apache/storm/perf/bolt/DevNullBolt.java
Patch:
@@ -33,7 +33,7 @@ public class DevNullBolt extends BaseRichBolt {
     private static final org.slf4j.Logger LOG = LoggerFactory.getLogger(DevNullBolt.class);
     private OutputCollector collector;
     private Long sleepNanos;
-    private int eCount = 0;
+    private int count = 0;
 
     @Override
     public void prepare(Map<String, Object> topoConf, TopologyContext context, OutputCollector collector) {
@@ -47,7 +47,7 @@ public void execute(Tuple tuple) {
         if (sleepNanos > 0) {
             LockSupport.parkNanos(sleepNanos);
         }
-        ++eCount;
+        ++count;
     }
 
     @Override

File: examples/storm-perf/src/main/java/org/apache/storm/perf/spout/WordGenSpout.java
Patch:
@@ -63,8 +63,9 @@ public static ArrayList<String> readWords(String file) {
             try {
                 String line;
                 while ((line = reader.readLine()) != null) {
-                    for (String word : line.split("\\s+"))
+                    for (String word : line.split("\\s+")) {
                         lines.add(word);
+                    }
                 }
             } catch (IOException e) {
                 throw new RuntimeException("Reading file failed", e);

File: examples/storm-perf/src/main/java/org/apache/storm/perf/utils/Helper.java
Patch:
@@ -68,7 +68,7 @@ public static void collectMetricsAndKill(String topologyName, Integer pollInterv
     }
 
     /**
-     * Kill topo on Ctrl-C
+     * Kill topo on Ctrl-C.
      */
     public static void setupShutdownHook(final String topoName) {
         Map<String, Object> clusterConf = Utils.readStormConfig();

File: examples/storm-perf/src/main/java/org/apache/storm/perf/utils/MetricsSample.java
Patch:
@@ -82,8 +82,6 @@ private static MetricsSample getMetricsSample(TopologyInfo topInfo) {
         // number of spout executors
         int spoutExecCount = 0;
         double spoutLatencySum = 0.0;
-
-        long spoutEmitted = 0L;
         long spoutTransferred = 0L;
 
         // Executor summaries
@@ -156,6 +154,8 @@ private static MetricsSample getMetricsSample(TopologyInfo topInfo) {
         ret.totalAcked = totalAcked;
         ret.totalFailed = totalFailed;
         ret.totalLatency = spoutLatencySum / spoutExecCount;
+
+        long spoutEmitted = 0L;
         ret.spoutEmitted = spoutEmitted;
         ret.spoutTransferred = spoutTransferred;
         ret.sampleTime = System.currentTimeMillis();

File: storm-core/src/jvm/org/apache/storm/command/CLI.java
Patch:
@@ -23,6 +23,7 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public class CLI {
     /**
      * Parse function to return an Integer.
@@ -250,6 +251,7 @@ public Object process(Object current, String value) {
         }
     }
 
+    @SuppressWarnings("checkstyle:AbbreviationAsWordInName")
     public static class CLIBuilder {
         private final ArrayList<Opt> opts = new ArrayList<>();
         private final ArrayList<Arg> args = new ArrayList<>();

File: storm-core/src/jvm/org/apache/storm/command/ShellSubmission.java
Patch:
@@ -12,6 +12,9 @@
 
 package org.apache.storm.command;
 
+import java.util.Arrays;
+import java.util.Map;
+
 import org.apache.commons.lang.ArrayUtils;
 import org.apache.storm.StormSubmitter;
 import org.apache.storm.generated.NimbusSummary;
@@ -21,9 +24,6 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import java.util.Arrays;
-import java.util.Map;
-
 public class ShellSubmission {
     private static final Logger LOG = LoggerFactory.getLogger(ShellSubmission.class);
 

File: external/storm-redis/src/main/java/org/apache/storm/redis/common/commands/RedisCommands.java
Patch:
@@ -25,8 +25,8 @@
 /**
  * This interface represents Jedis methods exhaustively which are used on storm-redis.
  *
- * This is a workaround since Jedis and JedisCluster doesn't implement same interface for binary type of methods, and unify binary methods
- * and string methods into one interface.
+ * <p>This is a workaround since Jedis and JedisCluster doesn't implement same interface for binary type of methods, and
+ * unify binary methods and string methods into one interface.
  */
 public interface RedisCommands {
     // common

File: external/storm-redis/src/main/java/org/apache/storm/redis/common/container/JedisClusterContainer.java
Patch:
@@ -26,7 +26,7 @@ public class JedisClusterContainer implements JedisCommandsInstanceContainer {
     private JedisCluster jedisCluster;
 
     /**
-     * Constructor
+     * Constructor.
      * @param jedisCluster JedisCluster instance
      */
     public JedisClusterContainer(JedisCluster jedisCluster) {

File: external/storm-redis/src/main/java/org/apache/storm/redis/common/container/JedisCommandsInstanceContainer.java
Patch:
@@ -32,7 +32,7 @@ public interface JedisCommandsInstanceContainer extends Closeable {
     void returnInstance(JedisCommands jedisCommands);
 
     /**
-     * Release Container
+     * Release Container.
      */
     @Override
     public void close();

File: external/storm-redis/src/main/java/org/apache/storm/redis/common/container/JedisContainer.java
Patch:
@@ -28,7 +28,7 @@ public class JedisContainer implements JedisCommandsInstanceContainer {
     private JedisPool jedisPool;
 
     /**
-     * Constructor
+     * Constructor.
      * @param jedisPool JedisPool which actually manages Jedis instances
      */
     public JedisContainer(JedisPool jedisPool) {

File: external/storm-redis/src/main/java/org/apache/storm/redis/common/container/RedisClusterContainer.java
Patch:
@@ -32,7 +32,7 @@ public class RedisClusterContainer implements RedisCommandsInstanceContainer {
     private JedisCluster jedisCluster;
 
     /**
-     * Constructor
+     * Constructor.
      *
      * @param jedisCluster JedisCluster instance
      */

File: external/storm-redis/src/main/java/org/apache/storm/redis/common/container/RedisContainer.java
Patch:
@@ -32,7 +32,7 @@ public class RedisContainer implements RedisCommandsInstanceContainer {
     private JedisPool jedisPool;
 
     /**
-     * Constructor
+     * Constructor.
      *
      * @param jedisPool JedisPool which actually manages Jedis instances
      */

File: external/storm-redis/src/main/java/org/apache/storm/redis/state/RedisKeyValueState.java
Patch:
@@ -299,8 +299,8 @@ private void validatePrepareTxid(long txid) {
         Long committedTxid = lastCommittedTxid();
         if (committedTxid != null) {
             if (txid <= committedTxid) {
-                throw new RuntimeException("Invalid txid '" + txid + "' for prepare. Txid '" + committedTxid +
-                                           "' is already committed");
+                throw new RuntimeException("Invalid txid '" + txid + "' for prepare. Txid '" + committedTxid
+                        + "' is already committed");
             }
         }
     }

File: external/storm-redis/src/main/java/org/apache/storm/redis/trident/state/AbstractRedisStateQuerier.java
Patch:
@@ -36,7 +36,7 @@ public abstract class AbstractRedisStateQuerier<T extends State> extends BaseQue
     private final RedisLookupMapper lookupMapper;
 
     /**
-     * Constructor
+     * Constructor.
      *
      * @param lookupMapper mapper for querying
      */

File: external/storm-redis/src/main/java/org/apache/storm/redis/trident/state/KeyFactory.java
Patch:
@@ -27,7 +27,7 @@ public interface KeyFactory extends Serializable {
     String build(List<Object> key);
 
     /**
-     * Default Key Factory
+     * Default Key Factory.
      */
     class DefaultKeyFactory implements KeyFactory {
         /**

File: external/storm-redis/src/main/java/org/apache/storm/redis/trident/state/RedisClusterMapState.java
Patch:
@@ -46,7 +46,7 @@ public class RedisClusterMapState<T> extends AbstractRedisMapState<T> {
     private KeyFactory keyFactory;
 
     /**
-     * Constructor
+     * Constructor.
      *
      * @param jedisCluster JedisCluster
      * @param options options of State
@@ -287,7 +287,7 @@ protected static class Factory implements StateFactory {
         Options options;
 
         /**
-         * Constructor
+         * Constructor.
          *
          * @param jedisClusterConfig configuration for JedisCluster
          * @param type StateType

File: external/storm-redis/src/main/java/org/apache/storm/redis/trident/state/RedisClusterStateQuerier.java
Patch:
@@ -24,7 +24,7 @@
  */
 public class RedisClusterStateQuerier extends AbstractRedisStateQuerier<RedisClusterState> {
     /**
-     * Constructor
+     * Constructor.
      *
      * @param lookupMapper mapper for querying
      */

File: external/storm-redis/src/main/java/org/apache/storm/redis/trident/state/RedisClusterStateUpdater.java
Patch:
@@ -24,7 +24,7 @@
  */
 public class RedisClusterStateUpdater extends AbstractRedisStateUpdater<RedisClusterState> {
     /**
-     * Constructor
+     * Constructor.
      *
      * @param storeMapper mapper for storing
      */
@@ -74,8 +74,8 @@ protected void updateStatesToRedis(RedisClusterState redisClusterState, Map<Stri
 
             // send expire command for hash only once
             // it expires key itself entirely, so use it with caution
-            if (dataType == RedisDataTypeDescription.RedisDataType.HASH &&
-                this.expireIntervalSec > 0) {
+            if (dataType == RedisDataTypeDescription.RedisDataType.HASH
+                    && this.expireIntervalSec > 0) {
                 jedisCluster.expire(additionalKey, expireIntervalSec);
             }
         } finally {

File: external/storm-redis/src/main/java/org/apache/storm/redis/trident/state/RedisMapState.java
Patch:
@@ -47,7 +47,7 @@ public class RedisMapState<T> extends AbstractRedisMapState<T> {
     private KeyFactory keyFactory;
 
     /**
-     * Constructor
+     * Constructor.
      *
      * @param jedisPool JedisPool
      * @param options options of State
@@ -317,7 +317,7 @@ protected static class Factory implements StateFactory {
         Options options;
 
         /**
-         * Constructor
+         * Constructor.
          *
          * @param jedisPoolConfig configuration for JedisPool
          * @param type StateType

File: external/storm-redis/src/main/java/org/apache/storm/redis/trident/state/RedisStateQuerier.java
Patch:
@@ -23,7 +23,7 @@
  */
 public class RedisStateQuerier extends AbstractRedisStateQuerier<RedisState> {
     /**
-     * Constructor
+     * Constructor.
      *
      * @param lookupMapper mapper for querying
      */

File: external/storm-redis/src/main/java/org/apache/storm/redis/trident/state/RedisStateUpdater.java
Patch:
@@ -25,7 +25,7 @@
  */
 public class RedisStateUpdater extends AbstractRedisStateUpdater<RedisState> {
     /**
-     * Constructor
+     * Constructor.
      *
      * @param storeMapper mapper for storing
      */
@@ -76,8 +76,8 @@ protected void updateStatesToRedis(RedisState redisState, Map<String, String> ke
 
             // send expire command for hash only once
             // it expires key itself entirely, so use it with caution
-            if (dataType == RedisDataTypeDescription.RedisDataType.HASH &&
-                this.expireIntervalSec > 0) {
+            if (dataType == RedisDataTypeDescription.RedisDataType.HASH
+                    && this.expireIntervalSec > 0) {
                 pipeline.expire(additionalKey, expireIntervalSec);
             }
 

File: storm-webapp/src/main/java/org/apache/storm/daemon/common/AuthorizationExceptionMapper.java
Patch:
@@ -18,6 +18,8 @@
 
 package org.apache.storm.daemon.common;
 
+import static org.apache.storm.daemon.ui.exceptionmappers.ExceptionMapperUtils.getResponse;
+
 import java.util.HashMap;
 import java.util.Map;
 
@@ -30,8 +32,6 @@
 import org.apache.storm.generated.AuthorizationException;
 import org.json.simple.JSONValue;
 
-import static org.apache.storm.daemon.ui.exceptionmappers.ExceptionMapperUtils.getResponse;
-
 @Provider
 public class AuthorizationExceptionMapper implements ExceptionMapper<AuthorizationException> {
 

File: storm-webapp/src/main/java/org/apache/storm/daemon/drpc/DRPCServer.java
Patch:
@@ -50,6 +50,7 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public class DRPCServer implements AutoCloseable {
     private static final Logger LOG = LoggerFactory.getLogger(DRPCServer.class);
     private final Meter meterShutdownCalls;

File: storm-webapp/src/main/java/org/apache/storm/daemon/drpc/webapp/DRPCApplication.java
Patch:
@@ -29,6 +29,7 @@
 import org.apache.storm.metric.StormMetricsRegistry;
 
 @ApplicationPath("")
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public class DRPCApplication extends Application {
     private static DRPC _drpc;
     private static StormMetricsRegistry metricsRegistry;

File: storm-webapp/src/main/java/org/apache/storm/daemon/drpc/webapp/DRPCExceptionMapper.java
Patch:
@@ -30,6 +30,7 @@
 import org.json.simple.JSONValue;
 
 @Provider
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public class DRPCExceptionMapper implements ExceptionMapper<DRPCExecutionException> {
 
     @Override

File: storm-webapp/src/main/java/org/apache/storm/daemon/drpc/webapp/DRPCResource.java
Patch:
@@ -32,6 +32,7 @@
 import org.apache.storm.metric.StormMetricsRegistry;
 
 @Path("/drpc/")
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public class DRPCResource {
     private final Meter meterHttpRequests;
     private final Timer responseDuration;

File: storm-webapp/src/main/java/org/apache/storm/daemon/ui/UIServer.java
Patch:
@@ -57,6 +57,7 @@
  * Main class.
  *
  */
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public class UIServer {
 
     public static final Logger LOG = LoggerFactory.getLogger(UIServer.class);
@@ -103,9 +104,6 @@ public static void main(String[] args) {
         UIHelpers.configSsl(jettyServer, httpsPort, httpsKsPath, httpsKsPassword, httpsKsType, httpsKeyPassword,
                 httpsTsPath, httpsTsPassword, httpsTsType, httpsNeedClientAuth, httpsWantClientAuth);
 
-
-        StormMetricsRegistry metricsRegistry = new StormMetricsRegistry();
-
         ServletContextHandler context = new ServletContextHandler(ServletContextHandler.SESSIONS);
         context.setContextPath("/");
         jettyServer.setHandler(context);
@@ -119,6 +117,7 @@ public static void main(String[] args) {
 
         UIHelpers.configFilters(context, filterConfigurationList);
 
+        StormMetricsRegistry metricsRegistry = new StormMetricsRegistry();
         ResourceConfig resourceConfig =
             new ResourceConfig()
                 .packages("org.apache.storm.daemon.ui.resources")

File: external/storm-pmml/src/main/java/org/apache/storm/pmml/runner/PmmlModelRunner.java
Patch:
@@ -21,14 +21,15 @@
 import org.apache.storm.tuple.Tuple;
 
 /**
- * Runner for models defined using PMML
+ * Runner for models defined using PMML.
  *
  * @param <I> type of the input source.  For Storm it is typically a {@link Tuple}
  * @param <R> the type of extracted raw input
  * @param <P> the type of preprocessed input
  * @param <S> the type of predicted scores
  */
 public interface PmmlModelRunner<I, R, P, S> extends ModelRunner {
+
     /**
      * Extracts from the tuple the raw inputs that are to be scored according to the predictive model.
      *
@@ -46,7 +47,7 @@ public interface PmmlModelRunner<I, R, P, S> extends ModelRunner {
     P preProcessInputs(R rawInputs);
 
     /**
-     * Compute the predicted scores from the pre-processed inputs in the step above
+     * Compute the predicted scores from the pre-processed inputs in the step above.
      *
      * @param preProcInputs that are to be preprocessed
      * @return predicted scores

File: examples/storm-solr-examples/src/main/java/org/apache/storm/solr/topology/SolrJsonTopology.java
Patch:
@@ -18,14 +18,14 @@
 
 package org.apache.storm.solr.topology;
 
+import java.io.IOException;
+
 import org.apache.storm.generated.StormTopology;
-import org.apache.storm.topology.TopologyBuilder;
 import org.apache.storm.solr.bolt.SolrUpdateBolt;
 import org.apache.storm.solr.mapper.SolrJsonMapper;
 import org.apache.storm.solr.mapper.SolrMapper;
 import org.apache.storm.solr.spout.SolrJsonSpout;
-
-import java.io.IOException;
+import org.apache.storm.topology.TopologyBuilder;
 
 public class SolrJsonTopology extends SolrTopology {
     public static void main(String[] args) throws Exception {

File: examples/storm-solr-examples/src/main/java/org/apache/storm/solr/trident/SolrFieldsTridentTopology.java
Patch:
@@ -18,16 +18,16 @@
 
 package org.apache.storm.solr.trident;
 
+import java.io.IOException;
+
 import org.apache.storm.generated.StormTopology;
 import org.apache.storm.solr.config.SolrConfig;
-import org.apache.storm.tuple.Fields;
 import org.apache.storm.solr.spout.SolrFieldsSpout;
 import org.apache.storm.solr.topology.SolrFieldsTopology;
 import org.apache.storm.trident.Stream;
 import org.apache.storm.trident.TridentTopology;
 import org.apache.storm.trident.state.StateFactory;
-
-import java.io.IOException;
+import org.apache.storm.tuple.Fields;
 
 public class SolrFieldsTridentTopology extends SolrFieldsTopology {
     public static void main(String[] args) throws Exception {

File: examples/storm-solr-examples/src/main/java/org/apache/storm/solr/trident/SolrJsonTridentTopology.java
Patch:
@@ -18,15 +18,15 @@
 
 package org.apache.storm.solr.trident;
 
+import java.io.IOException;
+
 import org.apache.storm.generated.StormTopology;
-import org.apache.storm.tuple.Fields;
 import org.apache.storm.solr.spout.SolrJsonSpout;
 import org.apache.storm.solr.topology.SolrJsonTopology;
 import org.apache.storm.trident.Stream;
 import org.apache.storm.trident.TridentTopology;
 import org.apache.storm.trident.state.StateFactory;
-
-import java.io.IOException;
+import org.apache.storm.tuple.Fields;
 
 public class SolrJsonTridentTopology extends SolrJsonTopology {
     public static void main(String[] args) throws Exception {

File: examples/storm-rocketmq-examples/src/main/java/org/apache/storm/rocketmq/topology/WordCountTopology.java
Patch:
@@ -41,16 +41,14 @@ public class WordCountTopology {
     private static final String CONSUMER_GROUP = "wordcount";
     private static final String CONSUMER_TOPIC = "source";
 
-    public static StormTopology buildTopology(String nameserverAddr, String topic){
+    public static StormTopology buildTopology(String nameserverAddr, String topic) {
         Properties properties = new Properties();
         properties.setProperty(SpoutConfig.NAME_SERVER_ADDR, nameserverAddr);
         properties.setProperty(SpoutConfig.CONSUMER_GROUP, CONSUMER_GROUP);
         properties.setProperty(SpoutConfig.CONSUMER_TOPIC, CONSUMER_TOPIC);
 
         RocketMqSpout spout = new RocketMqSpout(properties);
 
-        WordCounter bolt = new WordCounter();
-
         TupleToMessageMapper mapper = new FieldNameBasedTupleToMessageMapper("word", "count");
         TopicSelector selector = new DefaultTopicSelector(topic);
 
@@ -65,6 +63,7 @@ public static StormTopology buildTopology(String nameserverAddr, String topic){
         // wordSpout ==> countBolt ==> insertBolt
         TopologyBuilder builder = new TopologyBuilder();
 
+        WordCounter bolt = new WordCounter();
         builder.setSpout(WORD_SPOUT, spout, 1);
         builder.setBolt(COUNT_BOLT, bolt, 1).fieldsGrouping(WORD_SPOUT, new Fields("str"));
         builder.setBolt(INSERT_BOLT, insertBolt, 1).shuffleGrouping(COUNT_BOLT);

File: examples/storm-rocketmq-examples/src/main/java/org/apache/storm/rocketmq/trident/WordCountTrident.java
Patch:
@@ -39,7 +39,7 @@
 
 public class WordCountTrident {
 
-    public static StormTopology buildTopology(String nameserverAddr, String topic){
+    public static StormTopology buildTopology(String nameserverAddr, String topic) {
         Fields fields = new Fields("word", "count");
         FixedBatchSpout spout = new FixedBatchSpout(fields, 4,
                 new Values("storm", 1),

File: storm-clojure/src/main/java/org/apache/storm/clojure/ClojureSerializationRegister.java
Patch:
@@ -15,13 +15,14 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.clojure;
 
-import org.apache.storm.serialization.SerializationRegister;
+import carbonite.JavaBridge;
 
 import com.esotericsoftware.kryo.Kryo;
 
-import carbonite.JavaBridge;
+import org.apache.storm.serialization.SerializationRegister;
 
 public class ClojureSerializationRegister implements SerializationRegister {
 

File: storm-clojure/src/main/java/org/apache/storm/clojure/ClojureUtil.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.clojure;
 
 import clojure.lang.RT;

File: external/storm-mqtt/src/main/java/org/apache/storm/mqtt/MqttTupleMapper.java
Patch:
@@ -12,8 +12,8 @@
 
 package org.apache.storm.mqtt;
 
-
 import java.io.Serializable;
+
 import org.apache.storm.tuple.ITuple;
 
 /**
@@ -22,7 +22,7 @@
 public interface MqttTupleMapper extends Serializable {
 
     /**
-     * Converts a Tuple to a MqttMessage
+     * Converts a Tuple to a MqttMessage.
      * @param tuple the incoming tuple
      * @return the message to publish
      */

File: external/storm-mqtt/src/main/java/org/apache/storm/mqtt/common/MqttPublisher.java
Patch:
@@ -12,9 +12,9 @@
 
 package org.apache.storm.mqtt.common;
 
-
 import org.apache.storm.mqtt.MqttMessage;
 import org.apache.storm.mqtt.ssl.KeyStoreLoader;
+
 import org.fusesource.mqtt.client.BlockingConnection;
 import org.fusesource.mqtt.client.MQTT;
 import org.fusesource.mqtt.client.QoS;

File: external/storm-mqtt/src/main/java/org/apache/storm/mqtt/common/MqttUtils.java
Patch:
@@ -12,8 +12,8 @@
 
 package org.apache.storm.mqtt.common;
 
-
 import java.net.URI;
+
 import org.apache.storm.mqtt.MqttLogger;
 import org.apache.storm.mqtt.ssl.KeyStoreLoader;
 import org.fusesource.mqtt.client.MQTT;

File: external/storm-mqtt/src/main/java/org/apache/storm/mqtt/common/SslUtils.java
Patch:
@@ -12,7 +12,6 @@
 
 package org.apache.storm.mqtt.common;
 
-
 import java.net.URI;
 import java.security.KeyStore;
 import javax.net.ssl.KeyManagerFactory;
@@ -31,8 +30,8 @@ public static void checkSslConfig(String url, KeyStoreLoader loader) {
             throw new IllegalArgumentException("Unrecognized URI scheme: " + scheme);
         }
         if (!scheme.equalsIgnoreCase("tcp") && loader == null) {
-            throw new IllegalStateException("A TLS/SSL MQTT URL was specified, but no KeyStoreLoader configured. " +
-                                            "A KeyStoreLoader implementation is required when using TLS/SSL.");
+            throw new IllegalStateException("A TLS/SSL MQTT URL was specified, but no KeyStoreLoader configured. "
+                    + "A KeyStoreLoader implementation is required when using TLS/SSL.");
         }
     }
 

File: storm-client/src/jvm/org/apache/storm/executor/spout/SpoutExecutor.java
Patch:
@@ -337,7 +337,7 @@ public void ackSpoutMsg(SpoutExecutor executor, Task taskData, Long timeDelta, T
             ISpout spout = (ISpout) taskData.getTaskObject();
             int taskId = taskData.getTaskId();
             if (executor.getIsDebug()) {
-                LOG.info("SPOUT Acking message {} {}", tupleInfo.getId(), tupleInfo.getMessageId());
+                LOG.info("SPOUT Acking message {} {}", tupleInfo.getRootId(), tupleInfo.getMessageId());
             }
             spout.ack(tupleInfo.getMessageId());
             if (!taskData.getUserContext().getHooks().isEmpty()) { // avoid allocating SpoutAckInfo obj if not necessary
@@ -357,7 +357,7 @@ public void failSpoutMsg(SpoutExecutor executor, Task taskData, Long timeDelta,
             ISpout spout = (ISpout) taskData.getTaskObject();
             int taskId = taskData.getTaskId();
             if (executor.getIsDebug()) {
-                LOG.info("SPOUT Failing {} : {} REASON: {}", tupleInfo.getId(), tupleInfo, reason);
+                LOG.info("SPOUT Failing {} : {} REASON: {}", tupleInfo.getRootId(), tupleInfo, reason);
             }
             spout.fail(tupleInfo.getMessageId());
             new SpoutFailInfo(tupleInfo.getMessageId(), taskId, timeDelta).applyOn(taskData.getUserContext());

File: storm-client/src/jvm/org/apache/storm/executor/spout/SpoutOutputCollectorImpl.java
Patch:
@@ -149,6 +149,7 @@ private List<Integer> sendSpoutMsg(String stream, List<Object> values, Object me
             info.setTaskId(this.taskId);
             info.setStream(stream);
             info.setMessageId(messageId);
+            info.setRootId(rootId);
             if (isDebug) {
                 info.setValues(values);
             }
@@ -172,7 +173,7 @@ private List<Integer> sendSpoutMsg(String stream, List<Object> values, Object me
             globalTupleInfo.setValues(values);
             globalTupleInfo.setMessageId(messageId);
             globalTupleInfo.setTimestamp(0);
-            globalTupleInfo.setId("0:");
+            globalTupleInfo.setRootId(rootId);
             Long timeDelta = 0L;
             executor.ackSpoutMsg(executor, taskData, timeDelta, globalTupleInfo);
         }

File: storm-webapp/src/main/java/org/apache/storm/daemon/common/AuthorizationExceptionMapper.java
Patch:
@@ -18,6 +18,8 @@
 
 package org.apache.storm.daemon.common;
 
+import static org.apache.storm.daemon.ui.exceptionmappers.ExceptionMapperUtils.getResponse;
+
 import java.util.HashMap;
 import java.util.Map;
 
@@ -30,8 +32,6 @@
 import org.apache.storm.generated.AuthorizationException;
 import org.json.simple.JSONValue;
 
-import static org.apache.storm.daemon.ui.exceptionmappers.ExceptionMapperUtils.getResponse;
-
 @Provider
 public class AuthorizationExceptionMapper implements ExceptionMapper<AuthorizationException> {
 

File: storm-webapp/src/main/java/org/apache/storm/daemon/drpc/DRPCServer.java
Patch:
@@ -50,6 +50,7 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public class DRPCServer implements AutoCloseable {
     private static final Logger LOG = LoggerFactory.getLogger(DRPCServer.class);
     private final Meter meterShutdownCalls;

File: storm-webapp/src/main/java/org/apache/storm/daemon/drpc/webapp/DRPCApplication.java
Patch:
@@ -29,6 +29,7 @@
 import org.apache.storm.metric.StormMetricsRegistry;
 
 @ApplicationPath("")
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public class DRPCApplication extends Application {
     private static DRPC _drpc;
     private static StormMetricsRegistry metricsRegistry;

File: storm-webapp/src/main/java/org/apache/storm/daemon/drpc/webapp/DRPCExceptionMapper.java
Patch:
@@ -30,6 +30,7 @@
 import org.json.simple.JSONValue;
 
 @Provider
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public class DRPCExceptionMapper implements ExceptionMapper<DRPCExecutionException> {
 
     @Override

File: storm-webapp/src/main/java/org/apache/storm/daemon/drpc/webapp/DRPCResource.java
Patch:
@@ -32,6 +32,7 @@
 import org.apache.storm.metric.StormMetricsRegistry;
 
 @Path("/drpc/")
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public class DRPCResource {
     private final Meter meterHttpRequests;
     private final Timer responseDuration;

File: storm-webapp/src/main/java/org/apache/storm/daemon/ui/UIServer.java
Patch:
@@ -57,6 +57,7 @@
  * Main class.
  *
  */
+@SuppressWarnings("checkstyle:AbbreviationAsWordInName")
 public class UIServer {
 
     public static final Logger LOG = LoggerFactory.getLogger(UIServer.class);
@@ -103,9 +104,6 @@ public static void main(String[] args) {
         UIHelpers.configSsl(jettyServer, httpsPort, httpsKsPath, httpsKsPassword, httpsKsType, httpsKeyPassword,
                 httpsTsPath, httpsTsPassword, httpsTsType, httpsNeedClientAuth, httpsWantClientAuth);
 
-
-        StormMetricsRegistry metricsRegistry = new StormMetricsRegistry();
-
         ServletContextHandler context = new ServletContextHandler(ServletContextHandler.SESSIONS);
         context.setContextPath("/");
         jettyServer.setHandler(context);
@@ -119,6 +117,7 @@ public static void main(String[] args) {
 
         UIHelpers.configFilters(context, filterConfigurationList);
 
+        StormMetricsRegistry metricsRegistry = new StormMetricsRegistry();
         ResourceConfig resourceConfig =
             new ResourceConfig()
                 .packages("org.apache.storm.daemon.ui.resources")

File: storm-client/src/jvm/org/apache/storm/executor/spout/SpoutExecutor.java
Patch:
@@ -337,7 +337,7 @@ public void ackSpoutMsg(SpoutExecutor executor, Task taskData, Long timeDelta, T
             ISpout spout = (ISpout) taskData.getTaskObject();
             int taskId = taskData.getTaskId();
             if (executor.getIsDebug()) {
-                LOG.info("SPOUT Acking message {} {}", tupleInfo.getId(), tupleInfo.getMessageId());
+                LOG.info("SPOUT Acking message {} {}", tupleInfo.getRootId(), tupleInfo.getMessageId());
             }
             spout.ack(tupleInfo.getMessageId());
             if (!taskData.getUserContext().getHooks().isEmpty()) { // avoid allocating SpoutAckInfo obj if not necessary
@@ -357,7 +357,7 @@ public void failSpoutMsg(SpoutExecutor executor, Task taskData, Long timeDelta,
             ISpout spout = (ISpout) taskData.getTaskObject();
             int taskId = taskData.getTaskId();
             if (executor.getIsDebug()) {
-                LOG.info("SPOUT Failing {} : {} REASON: {}", tupleInfo.getId(), tupleInfo, reason);
+                LOG.info("SPOUT Failing {} : {} REASON: {}", tupleInfo.getRootId(), tupleInfo, reason);
             }
             spout.fail(tupleInfo.getMessageId());
             new SpoutFailInfo(tupleInfo.getMessageId(), taskId, timeDelta).applyOn(taskData.getUserContext());

File: storm-client/src/jvm/org/apache/storm/executor/spout/SpoutOutputCollectorImpl.java
Patch:
@@ -149,6 +149,7 @@ private List<Integer> sendSpoutMsg(String stream, List<Object> values, Object me
             info.setTaskId(this.taskId);
             info.setStream(stream);
             info.setMessageId(messageId);
+            info.setRootId(rootId);
             if (isDebug) {
                 info.setValues(values);
             }
@@ -172,7 +173,7 @@ private List<Integer> sendSpoutMsg(String stream, List<Object> values, Object me
             globalTupleInfo.setValues(values);
             globalTupleInfo.setMessageId(messageId);
             globalTupleInfo.setTimestamp(0);
-            globalTupleInfo.setId("0:");
+            globalTupleInfo.setRootId(rootId);
             Long timeDelta = 0L;
             executor.ackSpoutMsg(executor, taskData, timeDelta, globalTupleInfo);
         }

File: external/storm-pmml/src/main/java/org/apache/storm/pmml/runner/PmmlModelRunner.java
Patch:
@@ -21,14 +21,15 @@
 import org.apache.storm.tuple.Tuple;
 
 /**
- * Runner for models defined using PMML
+ * Runner for models defined using PMML.
  *
  * @param <I> type of the input source.  For Storm it is typically a {@link Tuple}
  * @param <R> the type of extracted raw input
  * @param <P> the type of preprocessed input
  * @param <S> the type of predicted scores
  */
 public interface PmmlModelRunner<I, R, P, S> extends ModelRunner {
+
     /**
      * Extracts from the tuple the raw inputs that are to be scored according to the predictive model.
      *
@@ -46,7 +47,7 @@ public interface PmmlModelRunner<I, R, P, S> extends ModelRunner {
     P preProcessInputs(R rawInputs);
 
     /**
-     * Compute the predicted scores from the pre-processed inputs in the step above
+     * Compute the predicted scores from the pre-processed inputs in the step above.
      *
      * @param preProcInputs that are to be preprocessed
      * @return predicted scores

File: examples/storm-solr-examples/src/main/java/org/apache/storm/solr/topology/SolrJsonTopology.java
Patch:
@@ -18,14 +18,14 @@
 
 package org.apache.storm.solr.topology;
 
+import java.io.IOException;
+
 import org.apache.storm.generated.StormTopology;
-import org.apache.storm.topology.TopologyBuilder;
 import org.apache.storm.solr.bolt.SolrUpdateBolt;
 import org.apache.storm.solr.mapper.SolrJsonMapper;
 import org.apache.storm.solr.mapper.SolrMapper;
 import org.apache.storm.solr.spout.SolrJsonSpout;
-
-import java.io.IOException;
+import org.apache.storm.topology.TopologyBuilder;
 
 public class SolrJsonTopology extends SolrTopology {
     public static void main(String[] args) throws Exception {

File: examples/storm-solr-examples/src/main/java/org/apache/storm/solr/trident/SolrFieldsTridentTopology.java
Patch:
@@ -18,16 +18,16 @@
 
 package org.apache.storm.solr.trident;
 
+import java.io.IOException;
+
 import org.apache.storm.generated.StormTopology;
 import org.apache.storm.solr.config.SolrConfig;
-import org.apache.storm.tuple.Fields;
 import org.apache.storm.solr.spout.SolrFieldsSpout;
 import org.apache.storm.solr.topology.SolrFieldsTopology;
 import org.apache.storm.trident.Stream;
 import org.apache.storm.trident.TridentTopology;
 import org.apache.storm.trident.state.StateFactory;
-
-import java.io.IOException;
+import org.apache.storm.tuple.Fields;
 
 public class SolrFieldsTridentTopology extends SolrFieldsTopology {
     public static void main(String[] args) throws Exception {

File: examples/storm-solr-examples/src/main/java/org/apache/storm/solr/trident/SolrJsonTridentTopology.java
Patch:
@@ -18,15 +18,15 @@
 
 package org.apache.storm.solr.trident;
 
+import java.io.IOException;
+
 import org.apache.storm.generated.StormTopology;
-import org.apache.storm.tuple.Fields;
 import org.apache.storm.solr.spout.SolrJsonSpout;
 import org.apache.storm.solr.topology.SolrJsonTopology;
 import org.apache.storm.trident.Stream;
 import org.apache.storm.trident.TridentTopology;
 import org.apache.storm.trident.state.StateFactory;
-
-import java.io.IOException;
+import org.apache.storm.tuple.Fields;
 
 public class SolrJsonTridentTopology extends SolrJsonTopology {
     public static void main(String[] args) throws Exception {

File: examples/storm-rocketmq-examples/src/main/java/org/apache/storm/rocketmq/topology/WordCountTopology.java
Patch:
@@ -41,16 +41,14 @@ public class WordCountTopology {
     private static final String CONSUMER_GROUP = "wordcount";
     private static final String CONSUMER_TOPIC = "source";
 
-    public static StormTopology buildTopology(String nameserverAddr, String topic){
+    public static StormTopology buildTopology(String nameserverAddr, String topic) {
         Properties properties = new Properties();
         properties.setProperty(SpoutConfig.NAME_SERVER_ADDR, nameserverAddr);
         properties.setProperty(SpoutConfig.CONSUMER_GROUP, CONSUMER_GROUP);
         properties.setProperty(SpoutConfig.CONSUMER_TOPIC, CONSUMER_TOPIC);
 
         RocketMqSpout spout = new RocketMqSpout(properties);
 
-        WordCounter bolt = new WordCounter();
-
         TupleToMessageMapper mapper = new FieldNameBasedTupleToMessageMapper("word", "count");
         TopicSelector selector = new DefaultTopicSelector(topic);
 
@@ -65,6 +63,7 @@ public static StormTopology buildTopology(String nameserverAddr, String topic){
         // wordSpout ==> countBolt ==> insertBolt
         TopologyBuilder builder = new TopologyBuilder();
 
+        WordCounter bolt = new WordCounter();
         builder.setSpout(WORD_SPOUT, spout, 1);
         builder.setBolt(COUNT_BOLT, bolt, 1).fieldsGrouping(WORD_SPOUT, new Fields("str"));
         builder.setBolt(INSERT_BOLT, insertBolt, 1).shuffleGrouping(COUNT_BOLT);

File: examples/storm-rocketmq-examples/src/main/java/org/apache/storm/rocketmq/trident/WordCountTrident.java
Patch:
@@ -39,7 +39,7 @@
 
 public class WordCountTrident {
 
-    public static StormTopology buildTopology(String nameserverAddr, String topic){
+    public static StormTopology buildTopology(String nameserverAddr, String topic) {
         Fields fields = new Fields("word", "count");
         FixedBatchSpout spout = new FixedBatchSpout(fields, 4,
                 new Values("storm", 1),

File: storm-clojure/src/main/java/org/apache/storm/clojure/ClojureSerializationRegister.java
Patch:
@@ -15,13 +15,14 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.clojure;
 
-import org.apache.storm.serialization.SerializationRegister;
+import carbonite.JavaBridge;
 
 import com.esotericsoftware.kryo.Kryo;
 
-import carbonite.JavaBridge;
+import org.apache.storm.serialization.SerializationRegister;
 
 public class ClojureSerializationRegister implements SerializationRegister {
 

File: storm-clojure/src/main/java/org/apache/storm/clojure/ClojureUtil.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.clojure;
 
 import clojure.lang.RT;

File: external/storm-mqtt/src/main/java/org/apache/storm/mqtt/MqttTupleMapper.java
Patch:
@@ -12,8 +12,8 @@
 
 package org.apache.storm.mqtt;
 
-
 import java.io.Serializable;
+
 import org.apache.storm.tuple.ITuple;
 
 /**
@@ -22,7 +22,7 @@
 public interface MqttTupleMapper extends Serializable {
 
     /**
-     * Converts a Tuple to a MqttMessage
+     * Converts a Tuple to a MqttMessage.
      * @param tuple the incoming tuple
      * @return the message to publish
      */

File: external/storm-mqtt/src/main/java/org/apache/storm/mqtt/common/MqttPublisher.java
Patch:
@@ -12,9 +12,9 @@
 
 package org.apache.storm.mqtt.common;
 
-
 import org.apache.storm.mqtt.MqttMessage;
 import org.apache.storm.mqtt.ssl.KeyStoreLoader;
+
 import org.fusesource.mqtt.client.BlockingConnection;
 import org.fusesource.mqtt.client.MQTT;
 import org.fusesource.mqtt.client.QoS;

File: external/storm-mqtt/src/main/java/org/apache/storm/mqtt/common/MqttUtils.java
Patch:
@@ -12,8 +12,8 @@
 
 package org.apache.storm.mqtt.common;
 
-
 import java.net.URI;
+
 import org.apache.storm.mqtt.MqttLogger;
 import org.apache.storm.mqtt.ssl.KeyStoreLoader;
 import org.fusesource.mqtt.client.MQTT;

File: external/storm-mqtt/src/main/java/org/apache/storm/mqtt/common/SslUtils.java
Patch:
@@ -12,7 +12,6 @@
 
 package org.apache.storm.mqtt.common;
 
-
 import java.net.URI;
 import java.security.KeyStore;
 import javax.net.ssl.KeyManagerFactory;
@@ -31,8 +30,8 @@ public static void checkSslConfig(String url, KeyStoreLoader loader) {
             throw new IllegalArgumentException("Unrecognized URI scheme: " + scheme);
         }
         if (!scheme.equalsIgnoreCase("tcp") && loader == null) {
-            throw new IllegalStateException("A TLS/SSL MQTT URL was specified, but no KeyStoreLoader configured. " +
-                                            "A KeyStoreLoader implementation is required when using TLS/SSL.");
+            throw new IllegalStateException("A TLS/SSL MQTT URL was specified, but no KeyStoreLoader configured. "
+                    + "A KeyStoreLoader implementation is required when using TLS/SSL.");
         }
     }
 

File: external/storm-opentsdb/src/main/java/org/apache/storm/opentsdb/trident/OpenTsdbStateUpdater.java
Patch:
@@ -16,14 +16,15 @@
  * specific language governing permissions and limitations
  * under the License.
  */
+
 package org.apache.storm.opentsdb.trident;
 
+import java.util.List;
+
 import org.apache.storm.trident.operation.TridentCollector;
 import org.apache.storm.trident.state.BaseStateUpdater;
 import org.apache.storm.trident.tuple.TridentTuple;
 
-import java.util.List;
-
 /**
  * Trident {@link org.apache.storm.trident.state.StateUpdater} implementation for OpenTSDB.
  */

File: integration-test/src/main/java/org/apache/storm/debug/DebugHelper.java
Patch:
@@ -17,13 +17,13 @@
 
 package org.apache.storm.debug;
 
+import java.net.URL;
+import java.net.URLClassLoader;
+
 import org.apache.commons.lang.StringUtils;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import java.net.URL;
-import java.net.URLClassLoader;
-
 public class DebugHelper {
     private static final Logger LOG = LoggerFactory.getLogger(DebugHelper.class);
 

File: integration-test/src/main/java/org/apache/storm/st/topology/window/IncrementingSpout.java
Patch:
@@ -52,8 +52,8 @@ public void open(Map<String, Object> conf, TopologyContext context, SpoutOutputC
     @Override
     public void nextTuple() {
         if (currentNum >= TestableTopology.MAX_SPOUT_EMITS) {
-          //Stop emitting at a certain point, because log rolling breaks the tests.
-          return;
+            //Stop emitting at a certain point, because log rolling breaks the tests.
+            return;
         }
         //Sleep a bit to avoid hogging the CPU.
         TimeUtil.sleepMilliSec(1);

File: integration-test/src/main/java/org/apache/storm/st/utils/TimeUtil.java
Patch:
@@ -17,13 +17,13 @@
 
 package org.apache.storm.st.utils;
 
+import java.util.concurrent.TimeUnit;
+
 import org.apache.commons.lang.exception.ExceptionUtils;
 import org.joda.time.DateTime;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import java.util.concurrent.TimeUnit;
-
 public class TimeUtil {
     private static Logger log = LoggerFactory.getLogger(TimeUtil.class);
 
@@ -34,6 +34,7 @@ public static void sleepSec(int sec) {
             log.warn("Caught exception: " + ExceptionUtils.getFullStackTrace(e));
         }
     }
+
     public static void sleepMilliSec(int milliSec) {
         try {
             TimeUnit.MILLISECONDS.sleep(milliSec);

File: storm-submit-tools/src/main/java/org/apache/storm/submit/dependency/Booter.java
Patch:
@@ -18,15 +18,15 @@
 
 package org.apache.storm.submit.dependency;
 
+import java.io.File;
+
 import org.apache.maven.repository.internal.MavenRepositorySystemUtils;
 import org.eclipse.aether.DefaultRepositorySystemSession;
 import org.eclipse.aether.RepositorySystem;
 import org.eclipse.aether.RepositorySystemSession;
 import org.eclipse.aether.repository.LocalRepository;
 import org.eclipse.aether.repository.RemoteRepository;
 
-import java.io.File;
-
 /**
  * Manage mvn repository.
  */

File: storm-submit-tools/src/main/java/org/apache/storm/submit/dependency/RepositorySystemFactory.java
Patch:
@@ -18,6 +18,8 @@
 
 package org.apache.storm.submit.dependency;
 
+import org.apache.maven.repository.internal.MavenRepositorySystemUtils;
+
 import org.eclipse.aether.RepositorySystem;
 import org.eclipse.aether.connector.basic.BasicRepositoryConnectorFactory;
 import org.eclipse.aether.impl.DefaultServiceLocator;
@@ -26,8 +28,6 @@
 import org.eclipse.aether.transport.file.FileTransporterFactory;
 import org.eclipse.aether.transport.http.HttpTransporterFactory;
 
-import org.apache.maven.repository.internal.MavenRepositorySystemUtils;
-
 /**
  * Get maven repository instance.
  */

File: external/storm-kinesis/src/main/java/org/apache/storm/kinesis/spout/CredentialsProviderChain.java
Patch:
@@ -26,10 +26,11 @@
 import com.amazonaws.auth.profile.ProfileCredentialsProvider;
 
 /**
- * Class representing chain of mechanisms that will be used in order to connect to kinesis
+ * Class representing chain of mechanisms that will be used in order to connect to kinesis.
  */
 public class CredentialsProviderChain extends AWSCredentialsProviderChain {
-    public CredentialsProviderChain () {
+
+    public CredentialsProviderChain() {
         super(new EnvironmentVariableCredentialsProvider(),
                 new SystemPropertiesCredentialsProvider(),
                 new ClasspathPropertiesFileCredentialsProvider(),

File: sql/storm-sql-core/src/jvm/org/apache/storm/sql/StormSql.java
Patch:
@@ -19,8 +19,8 @@
 /**
  * The StormSql class provides standalone, interactive interfaces to execute
  * SQL statements over streaming data.
- * <p>
- * The StormSql class is stateless. The user needs to submit the data
+ *
+ * <p>The StormSql class is stateless. The user needs to submit the data
  * definition language (DDL) statements and the query statements in the same
  * batch.
  */

File: sql/storm-sql-core/src/jvm/org/apache/storm/sql/StormSqlImpl.java
Patch:
@@ -78,14 +78,13 @@ public void submit(
     @Override
     public void explain(Iterable<String> statements) throws Exception {
         for (String sql : statements) {
-            StormParser parser = new StormParser(sql);
-            SqlNode node = parser.impl().parseSqlStmtEof();
-
             System.out.println("===========================================================");
             System.out.println("query>");
             System.out.println(sql);
             System.out.println("-----------------------------------------------------------");
 
+            StormParser parser = new StormParser(sql);
+            SqlNode node = parser.impl().parseSqlStmtEof();
             if (node instanceof SqlCreateTable) {
                 sqlContext.interpretCreateTable((SqlCreateTable) node);
                 System.out.println("No plan presented on DDL");

File: sql/storm-sql-core/src/jvm/org/apache/storm/sql/StormSqlRunner.java
Patch:
@@ -56,8 +56,8 @@ public static void main(String[] args) throws Exception {
             SubmitOptions submitOptions = new SubmitOptions(TopologyInitialStatus.ACTIVE);
             sql.submit(topoName, stmts, conf, submitOptions, null, null);
         } else {
-            printUsageAndExit(options, "Either " + OPTION_SQL_TOPOLOGY_NAME_LONG + " or " + OPTION_SQL_EXPLAIN_LONG +
-                                       " must be presented");
+            printUsageAndExit(options, "Either " + OPTION_SQL_TOPOLOGY_NAME_LONG
+                    + " or " + OPTION_SQL_EXPLAIN_LONG + " must be presented");
         }
     }
 

File: sql/storm-sql-core/src/jvm/org/apache/storm/sql/javac/CompilingClassLoader.java
Patch:
@@ -125,7 +125,8 @@ public Class<?> findClass(String name) throws ClassNotFoundException {
     }
 
     /**
-     * @return Whether compilation was successful.
+     * Compiles source code to byte code.
+     * @return indicates whether compilation was successful
      */
     private boolean compileSourceCodeToByteCode(
         String className, String sourceCode, DiagnosticListener<JavaFileObject> diagnosticListener) {
@@ -195,7 +196,7 @@ public CharSequence getCharContent(boolean ignoreEncodingErrors) throws IOExcept
     /**
      * Provides an in-memory representation of JavaFileManager abstraction, so we do not need to write any files to disk.
      *
-     * When files are written to, rather than putting the bytes on disk, they are appended to buffers in byteCodeForClasses.
+     * <p>When files are written to, rather than putting the bytes on disk, they are appended to buffers in byteCodeForClasses.
      *
      * @see javax.tools.JavaFileManager
      */

File: sql/storm-sql-core/src/jvm/org/apache/storm/sql/planner/streams/StreamsStormRuleSets.java
Patch:
@@ -19,10 +19,9 @@
 
 package org.apache.storm.sql.planner.streams;
 
-import java.util.Iterator;
-
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableSet;
+import java.util.Iterator;
 import org.apache.calcite.plan.RelOptRule;
 import org.apache.calcite.rel.rules.CalcMergeRule;
 import org.apache.calcite.rel.rules.FilterCalcMergeRule;

File: sql/storm-sql-core/src/jvm/org/apache/storm/sql/planner/streams/rel/StreamsStreamInsertRel.java
Patch:
@@ -19,10 +19,11 @@
 
 package org.apache.storm.sql.planner.streams.rel;
 
-import java.util.List;
-
 import com.google.common.base.Joiner;
 import com.google.common.base.Preconditions;
+
+import java.util.List;
+
 import org.apache.calcite.plan.RelOptCluster;
 import org.apache.calcite.plan.RelOptTable;
 import org.apache.calcite.plan.RelTraitSet;

File: sql/storm-sql-core/src/jvm/org/apache/storm/sql/planner/streams/rel/StreamsStreamScanRel.java
Patch:
@@ -19,10 +19,9 @@
 
 package org.apache.storm.sql.planner.streams.rel;
 
+import com.google.common.base.Joiner;
 import java.util.List;
 import java.util.Map;
-
-import com.google.common.base.Joiner;
 import org.apache.calcite.plan.RelOptCluster;
 import org.apache.calcite.plan.RelOptTable;
 import org.apache.calcite.plan.RelTraitSet;

File: external/storm-kinesis/src/main/java/org/apache/storm/kinesis/spout/CredentialsProviderChain.java
Patch:
@@ -26,10 +26,11 @@
 import com.amazonaws.auth.profile.ProfileCredentialsProvider;
 
 /**
- * Class representing chain of mechanisms that will be used in order to connect to kinesis
+ * Class representing chain of mechanisms that will be used in order to connect to kinesis.
  */
 public class CredentialsProviderChain extends AWSCredentialsProviderChain {
-    public CredentialsProviderChain () {
+
+    public CredentialsProviderChain() {
         super(new EnvironmentVariableCredentialsProvider(),
                 new SystemPropertiesCredentialsProvider(),
                 new ClasspathPropertiesFileCredentialsProvider(),

File: external/storm-opentsdb/src/main/java/org/apache/storm/opentsdb/trident/OpenTsdbStateUpdater.java
Patch:
@@ -16,14 +16,15 @@
  * specific language governing permissions and limitations
  * under the License.
  */
+
 package org.apache.storm.opentsdb.trident;
 
+import java.util.List;
+
 import org.apache.storm.trident.operation.TridentCollector;
 import org.apache.storm.trident.state.BaseStateUpdater;
 import org.apache.storm.trident.tuple.TridentTuple;
 
-import java.util.List;
-
 /**
  * Trident {@link org.apache.storm.trident.state.StateUpdater} implementation for OpenTSDB.
  */

File: storm-submit-tools/src/main/java/org/apache/storm/submit/dependency/Booter.java
Patch:
@@ -18,15 +18,15 @@
 
 package org.apache.storm.submit.dependency;
 
+import java.io.File;
+
 import org.apache.maven.repository.internal.MavenRepositorySystemUtils;
 import org.eclipse.aether.DefaultRepositorySystemSession;
 import org.eclipse.aether.RepositorySystem;
 import org.eclipse.aether.RepositorySystemSession;
 import org.eclipse.aether.repository.LocalRepository;
 import org.eclipse.aether.repository.RemoteRepository;
 
-import java.io.File;
-
 /**
  * Manage mvn repository.
  */

File: storm-submit-tools/src/main/java/org/apache/storm/submit/dependency/RepositorySystemFactory.java
Patch:
@@ -18,6 +18,8 @@
 
 package org.apache.storm.submit.dependency;
 
+import org.apache.maven.repository.internal.MavenRepositorySystemUtils;
+
 import org.eclipse.aether.RepositorySystem;
 import org.eclipse.aether.connector.basic.BasicRepositoryConnectorFactory;
 import org.eclipse.aether.impl.DefaultServiceLocator;
@@ -26,8 +28,6 @@
 import org.eclipse.aether.transport.file.FileTransporterFactory;
 import org.eclipse.aether.transport.http.HttpTransporterFactory;
 
-import org.apache.maven.repository.internal.MavenRepositorySystemUtils;
-
 /**
  * Get maven repository instance.
  */

File: integration-test/src/main/java/org/apache/storm/debug/DebugHelper.java
Patch:
@@ -17,13 +17,13 @@
 
 package org.apache.storm.debug;
 
+import java.net.URL;
+import java.net.URLClassLoader;
+
 import org.apache.commons.lang.StringUtils;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import java.net.URL;
-import java.net.URLClassLoader;
-
 public class DebugHelper {
     private static final Logger LOG = LoggerFactory.getLogger(DebugHelper.class);
 

File: integration-test/src/main/java/org/apache/storm/st/topology/window/IncrementingSpout.java
Patch:
@@ -52,8 +52,8 @@ public void open(Map<String, Object> conf, TopologyContext context, SpoutOutputC
     @Override
     public void nextTuple() {
         if (currentNum >= TestableTopology.MAX_SPOUT_EMITS) {
-          //Stop emitting at a certain point, because log rolling breaks the tests.
-          return;
+            //Stop emitting at a certain point, because log rolling breaks the tests.
+            return;
         }
         //Sleep a bit to avoid hogging the CPU.
         TimeUtil.sleepMilliSec(1);

File: integration-test/src/main/java/org/apache/storm/st/utils/TimeUtil.java
Patch:
@@ -17,13 +17,13 @@
 
 package org.apache.storm.st.utils;
 
+import java.util.concurrent.TimeUnit;
+
 import org.apache.commons.lang.exception.ExceptionUtils;
 import org.joda.time.DateTime;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import java.util.concurrent.TimeUnit;
-
 public class TimeUtil {
     private static Logger log = LoggerFactory.getLogger(TimeUtil.class);
 
@@ -34,6 +34,7 @@ public static void sleepSec(int sec) {
             log.warn("Caught exception: " + ExceptionUtils.getFullStackTrace(e));
         }
     }
+
     public static void sleepMilliSec(int milliSec) {
         try {
             TimeUnit.MILLISECONDS.sleep(milliSec);

File: sql/storm-sql-core/src/jvm/org/apache/storm/sql/StormSql.java
Patch:
@@ -19,8 +19,8 @@
 /**
  * The StormSql class provides standalone, interactive interfaces to execute
  * SQL statements over streaming data.
- * <p>
- * The StormSql class is stateless. The user needs to submit the data
+ *
+ * <p>The StormSql class is stateless. The user needs to submit the data
  * definition language (DDL) statements and the query statements in the same
  * batch.
  */

File: sql/storm-sql-core/src/jvm/org/apache/storm/sql/StormSqlImpl.java
Patch:
@@ -78,14 +78,13 @@ public void submit(
     @Override
     public void explain(Iterable<String> statements) throws Exception {
         for (String sql : statements) {
-            StormParser parser = new StormParser(sql);
-            SqlNode node = parser.impl().parseSqlStmtEof();
-
             System.out.println("===========================================================");
             System.out.println("query>");
             System.out.println(sql);
             System.out.println("-----------------------------------------------------------");
 
+            StormParser parser = new StormParser(sql);
+            SqlNode node = parser.impl().parseSqlStmtEof();
             if (node instanceof SqlCreateTable) {
                 sqlContext.interpretCreateTable((SqlCreateTable) node);
                 System.out.println("No plan presented on DDL");

File: sql/storm-sql-core/src/jvm/org/apache/storm/sql/StormSqlRunner.java
Patch:
@@ -56,8 +56,8 @@ public static void main(String[] args) throws Exception {
             SubmitOptions submitOptions = new SubmitOptions(TopologyInitialStatus.ACTIVE);
             sql.submit(topoName, stmts, conf, submitOptions, null, null);
         } else {
-            printUsageAndExit(options, "Either " + OPTION_SQL_TOPOLOGY_NAME_LONG + " or " + OPTION_SQL_EXPLAIN_LONG +
-                                       " must be presented");
+            printUsageAndExit(options, "Either " + OPTION_SQL_TOPOLOGY_NAME_LONG
+                    + " or " + OPTION_SQL_EXPLAIN_LONG + " must be presented");
         }
     }
 

File: sql/storm-sql-core/src/jvm/org/apache/storm/sql/javac/CompilingClassLoader.java
Patch:
@@ -125,7 +125,8 @@ public Class<?> findClass(String name) throws ClassNotFoundException {
     }
 
     /**
-     * @return Whether compilation was successful.
+     * Compiles source code to byte code.
+     * @return indicates whether compilation was successful
      */
     private boolean compileSourceCodeToByteCode(
         String className, String sourceCode, DiagnosticListener<JavaFileObject> diagnosticListener) {
@@ -195,7 +196,7 @@ public CharSequence getCharContent(boolean ignoreEncodingErrors) throws IOExcept
     /**
      * Provides an in-memory representation of JavaFileManager abstraction, so we do not need to write any files to disk.
      *
-     * When files are written to, rather than putting the bytes on disk, they are appended to buffers in byteCodeForClasses.
+     * <p>When files are written to, rather than putting the bytes on disk, they are appended to buffers in byteCodeForClasses.
      *
      * @see javax.tools.JavaFileManager
      */

File: sql/storm-sql-core/src/jvm/org/apache/storm/sql/planner/streams/StreamsStormRuleSets.java
Patch:
@@ -19,10 +19,9 @@
 
 package org.apache.storm.sql.planner.streams;
 
-import java.util.Iterator;
-
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableSet;
+import java.util.Iterator;
 import org.apache.calcite.plan.RelOptRule;
 import org.apache.calcite.rel.rules.CalcMergeRule;
 import org.apache.calcite.rel.rules.FilterCalcMergeRule;

File: sql/storm-sql-core/src/jvm/org/apache/storm/sql/planner/streams/rel/StreamsStreamInsertRel.java
Patch:
@@ -19,10 +19,11 @@
 
 package org.apache.storm.sql.planner.streams.rel;
 
-import java.util.List;
-
 import com.google.common.base.Joiner;
 import com.google.common.base.Preconditions;
+
+import java.util.List;
+
 import org.apache.calcite.plan.RelOptCluster;
 import org.apache.calcite.plan.RelOptTable;
 import org.apache.calcite.plan.RelTraitSet;

File: sql/storm-sql-core/src/jvm/org/apache/storm/sql/planner/streams/rel/StreamsStreamScanRel.java
Patch:
@@ -19,10 +19,9 @@
 
 package org.apache.storm.sql.planner.streams.rel;
 
+import com.google.common.base.Joiner;
 import java.util.List;
 import java.util.Map;
-
-import com.google.common.base.Joiner;
 import org.apache.calcite.plan.RelOptCluster;
 import org.apache.calcite.plan.RelOptTable;
 import org.apache.calcite.plan.RelTraitSet;

File: storm-webapp/src/main/java/org/apache/storm/daemon/ui/UIHelpers.java
Patch:
@@ -1119,6 +1119,8 @@ public static Map<String, Object> getTopologySummary(TopologyPageInfo topologyPa
         result.put("configuration", topologyConf);
         result.put("visualizationTable", new ArrayList());
         result.put("schedulerDisplayResource", config.get(DaemonConfig.SCHEDULER_DISPLAY_RESOURCE));
+        result.put("bugtracker-url", config.get(DaemonConfig.UI_PROJECT_BUGTRACKER_URL));
+        result.put("central-log-url", config.get(DaemonConfig.UI_CENTRAL_LOGGING_URL));
         return result;
     }
 

File: storm-webapp/src/main/java/org/apache/storm/daemon/ui/UIHelpers.java
Patch:
@@ -1119,6 +1119,8 @@ public static Map<String, Object> getTopologySummary(TopologyPageInfo topologyPa
         result.put("configuration", topologyConf);
         result.put("visualizationTable", new ArrayList());
         result.put("schedulerDisplayResource", config.get(DaemonConfig.SCHEDULER_DISPLAY_RESOURCE));
+        result.put("bugtracker-url", config.get(DaemonConfig.UI_PROJECT_BUGTRACKER_URL));
+        result.put("central-log-url", config.get(DaemonConfig.UI_CENTRAL_LOGGING_URL));
         return result;
     }
 

File: examples/storm-jms-examples/src/main/java/org/apache/storm/jms/example/JsonTupleProducer.java
Patch:
@@ -42,6 +42,7 @@
 @SuppressWarnings("serial")
 public class JsonTupleProducer implements JmsTupleProducer {
 
+    @Override
 	public Values toTuple(Message msg) throws JMSException {
 		if(msg instanceof TextMessage){
 			String json = ((TextMessage) msg).getText();
@@ -51,6 +52,7 @@ public Values toTuple(Message msg) throws JMSException {
 		}
 	}
 
+    @Override
 	public void declareOutputFields(OutputFieldsDeclarer declarer) {
 		declarer.declare(new Fields("json"));
 	}

File: examples/storm-jms-examples/src/main/java/org/apache/storm/jms/example/SpringJmsProvider.java
Patch:
@@ -63,10 +63,12 @@ public SpringJmsProvider(String appContextClasspathResource, String connectionFa
 		this.destination = (Destination)context.getBean(destinationBean);
 	}
 
+	@Override
 	public ConnectionFactory connectionFactory() throws Exception {
 		return this.connectionFactory;
 	}
 
+	@Override
 	public Destination destination() throws Exception {
 		return this.destination;
 	}

File: examples/storm-kafka-client-examples/src/test/java/org/apache/storm/kafka/spout/KafkaSpoutTopologyMainWildcardTopicsLocal.java
Patch:
@@ -22,6 +22,7 @@ public static void main(String[] args) throws Exception {
         new KafkaSpoutTopologyMainWildcardTopicsLocal().runExample();
     } 
     
+    @Override
     protected KafkaSpoutTopologyMainNamedTopics getTopology() {
         return new KafkaSpoutTopologyMainWildcardTopics();
     }

File: examples/storm-loadgen/src/main/java/org/apache/storm/loadgen/HttpForwardingMetricsServer.java
Patch:
@@ -54,6 +54,7 @@ protected KryoValuesDeserializer initialValue() {
     };
 
     private class MetricsCollectionServlet extends HttpServlet {
+        @Override
         protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {
             Input in = new Input(request.getInputStream());
             List<Object> metrics = des.get().deserializeFrom(in);

File: examples/storm-perf/src/main/java/org/apache/storm/perf/StrGenSpoutHdfsBoltTopo.java
Patch:
@@ -153,6 +153,7 @@ public LineWriter withLineDelimiter(String delimiter) {
             return this;
         }
 
+        @Override
         public byte[] format(Tuple tuple) {
             return (tuple.getValueByField(fieldName).toString() + this.lineDelimiter).getBytes();
         }

File: examples/storm-perf/src/main/java/org/apache/storm/perf/utils/Helper.java
Patch:
@@ -74,6 +74,7 @@ public static void setupShutdownHook(final String topoName) {
         Map<String, Object> clusterConf = Utils.readStormConfig();
         final Nimbus.Iface client = NimbusClient.getConfiguredClient(clusterConf).getClient();
         Runtime.getRuntime().addShutdownHook(new Thread() {
+            @Override
             public void run() {
                 try {
                     System.out.println("Killing...");

File: examples/storm-solr-examples/src/main/java/org/apache/storm/solr/topology/SolrFieldsTopology.java
Patch:
@@ -43,10 +43,12 @@ protected SolrMapper getSolrMapper(SolrConfig solrConfig) throws IOException {
                     .setMultiValueFieldToken("%").build();
     }
 
+    @Override
     protected SolrCommitStrategy getSolrCommitStgy() {
         return new CountBasedCommit(2);         // To Commit to Solr and Ack according to the commit strategy
     }
 
+    @Override
     protected StormTopology getTopology() throws IOException {
         TopologyBuilder builder = new TopologyBuilder();
         builder.setSpout("SolrFieldsSpout", new SolrFieldsSpout());

File: examples/storm-solr-examples/src/main/java/org/apache/storm/solr/topology/SolrJsonTopology.java
Patch:
@@ -38,6 +38,7 @@ protected SolrMapper getSolrMapper() throws IOException {
         return new SolrJsonMapper.Builder(COLLECTION, jsonTupleField).build();
     }
 
+    @Override
     protected StormTopology getTopology() throws IOException {
         TopologyBuilder builder = new TopologyBuilder();
         builder.setSpout("SolrJsonSpout", new SolrJsonSpout());

File: examples/storm-solr-examples/src/main/java/org/apache/storm/solr/trident/SolrFieldsTridentTopology.java
Patch:
@@ -35,6 +35,7 @@ public static void main(String[] args) throws Exception {
         solrFieldsTridentTopology.run(args);
     }
 
+    @Override
     protected StormTopology getTopology() throws IOException {
         final TridentTopology tridentTopology = new TridentTopology();
         final SolrFieldsSpout spout = new SolrFieldsSpout();

File: examples/storm-solr-examples/src/main/java/org/apache/storm/solr/trident/SolrJsonTridentTopology.java
Patch:
@@ -34,6 +34,7 @@ public static void main(String[] args) throws Exception {
         solrJsonTridentTopology.run(args);
     }
 
+    @Override
     protected StormTopology getTopology() throws IOException {
         final TridentTopology topology = new TridentTopology();
         final SolrJsonSpout spout = new SolrJsonSpout();

File: examples/storm-starter/src/jvm/org/apache/storm/starter/RollingTopWords.java
Patch:
@@ -61,6 +61,7 @@ public static void main(String[] args) throws Exception {
      *          locally ("-local") or remotely, i.e. on a real cluster.
      * @throws Exception
      */
+    @Override
     protected int run(String[] args) {
         String topologyName = "slidingWindowCounts";
         if (args.length >= 1) {

File: examples/storm-starter/src/jvm/org/apache/storm/starter/SkewedRollingTopWords.java
Patch:
@@ -64,6 +64,7 @@ public static void main(String[] args) throws Exception {
      *          locally ("-local") or remotely, i.e. on a real cluster.
      * @throws Exception
      */
+    @Override
     protected int run(String[] args) {
         String topologyName = "slidingWindowCounts";
         if (args.length >= 1) {

File: examples/storm-starter/src/jvm/org/apache/storm/starter/WordCountTopology.java
Patch:
@@ -35,6 +35,7 @@ public static void main(String[] args) throws Exception {
         ConfigurableTopology.start(new WordCountTopology(), args);
     }
 
+    @Override
     protected int run(String[] args) throws Exception {
 
         TopologyBuilder builder = new TopologyBuilder();

File: examples/storm-starter/src/jvm/org/apache/storm/starter/spout/RandomSentenceSpout.java
Patch:
@@ -82,6 +82,7 @@ public TimeStamped(String prefix) {
             this.prefix = prefix;
         }
 
+        @Override
         protected String sentence(String input) {
             return prefix + currentDate() + " " + input;
         }

File: examples/storm-starter/src/jvm/org/apache/storm/starter/tools/RankableObjectWithFields.java
Patch:
@@ -63,10 +63,12 @@ public static RankableObjectWithFields from(Tuple tuple) {
         return new RankableObjectWithFields(obj, count, otherFields.toArray());
     }
 
+    @Override
     public Object getObject() {
         return obj;
     }
 
+    @Override
     public long getCount() {
         return count;
     }
@@ -111,6 +113,7 @@ public int hashCode() {
         return result;
     }
 
+    @Override
     public String toString() {
         StringBuffer buf = new StringBuffer();
         buf.append("[");

File: examples/storm-starter/src/jvm/org/apache/storm/starter/tools/Rankings.java
Patch:
@@ -135,6 +135,7 @@ public void pruneZeroCounts() {
         }
     }
 
+    @Override
     public String toString() {
         return rankedItems.toString();
     }

File: examples/storm-starter/src/jvm/org/apache/storm/starter/trident/DebugMemoryMapState.java
Patch:
@@ -40,6 +40,7 @@ public DebugMemoryMapState(String id) {
         super(id);
     }
 
+    @Override
     public List<T> multiUpdate(List<List<Object>> keys, List<ValueUpdater> updaters) {
         print(keys, updaters);
         if ((updateCount++ % 5) == 0) {

File: examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java
Patch:
@@ -341,6 +341,7 @@ public void updatingWithNewRankablesShouldBeThreadSafe() throws InterruptedExcep
 
         // when
         blitzer.blitz(new Runnable() {
+            @Override
             public void run() {
                 for (Rankable r : entries) {
                     try {

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/executor/impl/BatchAsyncResultHandler.java
Patch:
@@ -41,6 +41,7 @@ public BatchAsyncResultHandler(ExecutionResultHandler handler) {
      *
      * The default method does no-operation.
      */
+    @Override
     public void failure(Throwable t, List<Tuple> input) {
         completed.offer(new ExecutionResultCollector.FailedCollector(input, t));
     }
@@ -50,6 +51,7 @@ public void failure(Throwable t, List<Tuple> input) {
      *
      * The default method does no-operation.
      */
+    @Override
     public void success(List<Tuple> input) {
         completed.offer(new ExecutionResultCollector.SucceedCollector(input));
     }

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/executor/impl/SingleAsyncResultHandler.java
Patch:
@@ -40,6 +40,7 @@ public SingleAsyncResultHandler(ExecutionResultHandler handler) {
      *
      * The default method does no-operation.
      */
+    @Override
     public void failure(Throwable t, Tuple input) {
         completed.offer(new ExecutionResultCollector.FailedCollector(input, t));
     }
@@ -49,6 +50,7 @@ public void failure(Throwable t, Tuple input) {
      *
      * The default method does no-operation.
      */
+    @Override
     public void success(Tuple input) {
         completed.offer(new ExecutionResultCollector.SucceedCollector(input));
     }

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/query/builder/ObjectMapperCqlStatementMapperBuilder.java
Patch:
@@ -47,6 +47,7 @@ public ObjectMapperCqlStatementMapperBuilder(String operationField, String value
     /**
      * Builds an ObjectMapperCqlStatementMapper.
      */
+    @Override
     public ObjectMapperCqlStatementMapper build() {
         List<TypeCodec<?>> codecs = codecProducers.stream().map(codecProducer -> {
             try {

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/trident/state/HBaseMapState.java
Patch:
@@ -234,6 +234,7 @@ public Factory(StateType stateType, Options options) {
         }
 
         @SuppressWarnings({ "rawtypes", "unchecked" })
+        @Override
         public State makeState(Map<String, Object> conf, IMetricsContext metrics, int partitionIndex, int numPartitions) {
             LOG.info("Preparing HBase State for partition {} of {}.", partitionIndex + 1, numPartitions);
             IBackingMap state = new HBaseMapState(options, conf, partitionIndex);

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/trident/windowing/HBaseWindowsStoreFactory.java
Patch:
@@ -34,6 +34,7 @@ public HBaseWindowsStoreFactory(Map<String, Object> config, String tableName, by
         this.qualifier = qualifier;
     }
 
+    @Override
     public WindowsStore create(Map<String, Object> topoConf) {
         Configuration configuration = HBaseConfiguration.create();
         for (Map.Entry<String, Object> entry : config.entrySet()) {

File: external/storm-hdfs-blobstore/src/test/java/org/apache/storm/hdfs/blobstore/HdfsBlobStoreImplTest.java
Patch:
@@ -64,6 +64,7 @@ public TestHdfsBlobStoreImpl(Path path, Map<String, Object> conf,
             super(path, conf, hconf);
         }
 
+        @Override
         protected Path getKeyDir(String key) {
             return new Path(new Path(blobDir, KEYDIR), key);
         }

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/bolt/AbstractHdfsBolt.java
Patch:
@@ -91,6 +91,7 @@ protected void rotateOutputFile(Writer writer) throws IOException {
      * @param topologyContext
      * @param collector
      */
+    @Override
     public final void prepare(Map<String, Object> conf, TopologyContext topologyContext, OutputCollector collector) {
         this.writeLock = new Object();
         if (this.syncPolicy == null) throw new IllegalStateException("SyncPolicy must be specified.");

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/bolt/format/DefaultFileNameFormat.java
Patch:
@@ -74,6 +74,7 @@ public String getName(long rotation, long timeStamp) {
         return this.prefix + this.componentId + "-" + this.taskId + "-" + rotation + "-" + timeStamp + this.extension;
     }
 
+    @Override
     public String getPath() {
         return this.path;
     }

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/spout/FileLock.java
Patch:
@@ -302,6 +302,7 @@ public static LogEntry deserialize(String line) {
             return new LogEntry(Long.parseLong(fields[0]), fields[1], fields[2]);
         }
 
+        @Override
         public String toString() {
             return eventTime + "," + componentID + "," + fileOffset;
         }

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/spout/HdfsSpout.java
Patch:
@@ -230,6 +230,7 @@ public SpoutOutputCollector getCollector() {
         return collector;
     }
 
+    @Override
     public void nextTuple() {
         LOG.trace("Next Tuple {}", spoutId);
         // 1) First re-emit any previously failed tuples (from retryList)
@@ -387,6 +388,7 @@ protected void emitData(List<Object> tuple, MessageId id) {
     }
 
     @SuppressWarnings("deprecation")
+    @Override
     public void open(Map<String, Object> conf, TopologyContext context, SpoutOutputCollector collector) {
         LOG.info("Opening HDFS Spout");
         this.conf = conf;

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/spout/SequenceFileReader.java
Patch:
@@ -69,6 +69,7 @@ private static <K> void skipToOffset(SequenceFile.Reader reader, Offset offset,
         }
     }
 
+    @Override
     public List<Object> next() throws IOException, ParseException {
         if (reader.next(key, value)) {
             ArrayList<Object> result = new ArrayList<Object>(2);
@@ -88,6 +89,7 @@ public void close() {
         }
     }
 
+    @Override
     public Offset getFileOffset() {
         return offset;
     }

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/spout/TextFileReader.java
Patch:
@@ -59,10 +59,12 @@ private TextFileReader(FileSystem fs, Path file, Map<String, Object> conf, TextF
 
     }
 
+    @Override
     public Offset getFileOffset() {
         return offset.clone();
     }
 
+    @Override
     public List<Object> next() throws IOException, ParseException {
         String line = readLineAndTrackOffset(reader);
         if (line != null) {

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/trident/format/DefaultFileNameFormat.java
Patch:
@@ -72,6 +72,7 @@ public String getName(long rotation, long timeStamp) {
         return this.prefix + "-" + this.partitionIndex + "-" + rotation + "-" + timeStamp + this.extension;
     }
 
+    @Override
     public String getPath() {
         return this.path;
     }

File: external/storm-hdfs/src/test/java/org/apache/storm/hdfs/spout/TestHdfsSpout.java
Patch:
@@ -776,6 +776,7 @@ public MockTopologyContext(int componentId, Map<String, Object> topoConf) {
             this.componentId = componentId;
         }
 
+        @Override
         public String getThisComponentId() {
             return Integer.toString(componentId);
         }

File: external/storm-jms/src/main/java/org/apache/storm/jms/spout/JmsSpout.java
Patch:
@@ -255,6 +255,7 @@ public void open(final Map<String, Object> conf,
      * <p>When overridden, should always call {@code super}
      * to finalize the active connections.
      */
+    @Override
     public void close() {
         try {
             LOG.debug("Closing JMS connection.");
@@ -272,6 +273,7 @@ public void close() {
      * <p>This method polls the queue that's being filled asynchronously by the
      * jms connection, every {@link #POLL_INTERVAL_MS} seconds.
      */
+    @Override
     public void nextTuple() {
         try {
             Message msg = consumer.receive(POLL_INTERVAL_MS);

File: external/storm-jms/src/test/java/org/apache/storm/jms/spout/MockJmsProvider.java
Patch:
@@ -46,6 +46,7 @@ public MockJmsProvider() throws NamingException {
      *
      * @throws Exception
      */
+    @Override
     public ConnectionFactory connectionFactory() throws Exception {
         return this.connectionFactory;
     }
@@ -58,6 +59,7 @@ public ConnectionFactory connectionFactory() throws Exception {
      *
      * @throws Exception
      */
+    @Override
     public Destination destination() throws Exception {
         return this.destination;
     }

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java
Patch:
@@ -293,6 +293,7 @@ private Builder<K, V> setKafkaPropsForProcessingGuarantee() {
             return this;
         }
 
+        @Override
         public KafkaSpoutConfig<K, V> build() {
             return new KafkaSpoutConfig<>(this);
         }

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/RecordTranslator.java
Patch:
@@ -39,6 +39,7 @@ public interface RecordTranslator<K, V> extends Serializable, Func<ConsumerRecor
      *     Return {@code null} to discard an invalid {@link ConsumerRecord}
      *     if {@link Builder#setEmitNullTuples(boolean)} is set to {@code false}.
      */
+    @Override
     List<Object> apply(ConsumerRecord<K,V> record);
     
     /**

File: external/storm-mongodb/src/main/java/org/apache/storm/mongodb/common/mapper/SimpleMongoUpdateMapper.java
Patch:
@@ -39,6 +39,7 @@ public Document toDocument(ITuple tuple) {
         return new Document("$set", document);
     }
 
+    @Override
     public SimpleMongoUpdateMapper withFields(String... fields) {
         this.fields = fields;
         return this;

File: external/storm-mongodb/src/main/java/org/apache/storm/mongodb/trident/state/MongoMapState.java
Patch:
@@ -147,6 +147,7 @@ public Factory(StateType stateType, Options options) {
         }
 
         @SuppressWarnings({"rawtypes", "unchecked"})
+        @Override
         public State makeState(Map<String, Object> conf, IMetricsContext metrics, int partitionIndex, int numPartitions) {
             IBackingMap state = new MongoMapState(conf, options);
 

File: external/storm-mqtt/src/main/java/org/apache/storm/mqtt/mappers/ByteArrayMessageMapper.java
Patch:
@@ -19,10 +19,12 @@
 
 
 public class ByteArrayMessageMapper implements MqttMessageMapper {
+    @Override
     public Values toValues(MqttMessage message) {
         return new Values(message.getTopic(), message.getMessage());
     }
 
+    @Override
     public Fields outputFields() {
         return new Fields("topic", "message");
     }

File: external/storm-mqtt/src/main/java/org/apache/storm/mqtt/mappers/StringMessageMapper.java
Patch:
@@ -22,10 +22,12 @@
  * "topic" and "message", both of which are Strings.
  */
 public class StringMessageMapper implements MqttMessageMapper {
+    @Override
     public Values toValues(MqttMessage message) {
         return new Values(message.getTopic(), new String(message.getMessage()));
     }
 
+    @Override
     public Fields outputFields() {
         return new Fields("topic", "message");
     }

File: external/storm-opentsdb/src/main/java/org/apache/storm/opentsdb/bolt/TupleOpenTsdbDatapointMapper.java
Patch:
@@ -86,6 +86,7 @@ public String getTagsField() {
         return tagsField;
     }
 
+    @Override
     public boolean equals(Object o) {
         if (this == o) return true;
         if (!(o instanceof TupleOpenTsdbDatapointMapper)) return false;

File: sql/storm-sql-core/src/jvm/org/apache/storm/sql/compiler/RexNodeToJavaCodeCompiler.java
Patch:
@@ -158,6 +158,7 @@ private BlockBuilder compileToBlock(final RexProgram program, ParameterExpressio
                                         JavaRowFormat.ARRAY, false))));
         final Function1<String, RexToLixTranslator.InputGetter> correlates =
             new Function1<String, RexToLixTranslator.InputGetter>() {
+                @Override
                 public RexToLixTranslator.InputGetter apply(String a0) {
                     throw new UnsupportedOperationException();
                 }

File: sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/datasource/socket/spout/SocketSpout.java
Patch:
@@ -157,6 +157,7 @@ public Map<String, Object> getComponentConfiguration() {
     }
 
     private class SocketReaderRunnable implements Runnable {
+        @Override
         public void run() {
             while (running) {
                 try {

File: storm-client/src/jvm/org/apache/storm/ILocalDRPC.java
Patch:
@@ -27,5 +27,6 @@ public interface ILocalDRPC extends DistributedRPC.Iface, DistributedRPCInvocati
      * @deprecated use {@link #close()} instead
      */
     @Deprecated
+    @Override
     public void shutdown();
 }

File: storm-client/src/jvm/org/apache/storm/LogWriter.java
Patch:
@@ -56,6 +56,7 @@ public static void main(String[] args) throws Exception {
         System.exit(ret);
     }
 
+    @Override
     public void run() {
         Logger logger = this.logger;
         BufferedReader in = this.in;

File: storm-client/src/jvm/org/apache/storm/assignments/ILocalAssignmentsBackend.java
Patch:
@@ -116,5 +116,6 @@ public interface ILocalAssignmentsBackend extends AutoCloseable {
     /**
      * Function to release resource.
      */
+    @Override
     void close();
 }

File: storm-client/src/jvm/org/apache/storm/blobstore/ClientBlobStore.java
Patch:
@@ -171,6 +171,7 @@ protected abstract AtomicOutputStream createBlobToExtend(String key, SettableBlo
      */
     public abstract void createStateInZookeeper(String key);
 
+    @Override
     public abstract void close();
 
     /**

File: storm-client/src/jvm/org/apache/storm/daemon/supervisor/ClientSupervisorUtils.java
Patch:
@@ -139,6 +139,7 @@ public static Process launchProcess(List<String> command,
         }
         if (logPrefix != null || exitCodeCallback != null) {
             Utils.asyncLoop(new Callable<Long>() {
+                @Override
                 public Long call() {
                     if (logPrefix != null) {
                         Utils.readAndLogStream(logPrefix,

File: storm-client/src/jvm/org/apache/storm/drpc/DRPCInvocationsClient.java
Patch:
@@ -57,6 +57,7 @@ public boolean isConnected() {
         return client.get() != null;
     }
 
+    @Override
     public void result(String id, String result) throws TException, AuthorizationException {
         DistributedRPCInvocations.Client c = client.get();
         try {
@@ -72,6 +73,7 @@ public void result(String id, String result) throws TException, AuthorizationExc
         }
     }
 
+    @Override
     public DRPCRequest fetchRequest(String func) throws TException, AuthorizationException {
         DistributedRPCInvocations.Client c = client.get();
         try {
@@ -87,6 +89,7 @@ public DRPCRequest fetchRequest(String func) throws TException, AuthorizationExc
         }
     }
 
+    @Override
     public void failRequest(String id) throws TException, AuthorizationException {
         DistributedRPCInvocations.Client c = client.get();
         try {

File: storm-client/src/jvm/org/apache/storm/drpc/JoinResult.java
Patch:
@@ -39,10 +39,12 @@ public JoinResult(String returnComponent) {
         this.returnComponent = returnComponent;
     }
 
+    @Override
     public void prepare(Map<String, Object> map, TopologyContext context, OutputCollector collector) {
         _collector = collector;
     }
 
+    @Override
     public void execute(Tuple tuple) {
         Object requestId = tuple.getValue(0);
         if (tuple.getSourceComponent().equals(returnComponent)) {
@@ -64,6 +66,7 @@ public void execute(Tuple tuple) {
         }
     }
 
+    @Override
     public void declareOutputFields(OutputFieldsDeclarer declarer) {
         declarer.declare(new Fields("result", "return-info"));
     }

File: storm-client/src/jvm/org/apache/storm/drpc/PrepareRequest.java
Patch:
@@ -46,6 +46,7 @@ public void execute(Tuple tuple, BasicOutputCollector collector) {
         collector.emit(ID_STREAM, new Values(requestId));
     }
 
+    @Override
     public void declareOutputFields(OutputFieldsDeclarer declarer) {
         declarer.declareStream(ARGS_STREAM, new Fields("request", "args"));
         declarer.declareStream(RETURN_STREAM, new Fields("request", "return"));

File: storm-client/src/jvm/org/apache/storm/drpc/ReturnResults.java
Patch:
@@ -126,6 +126,7 @@ public void cleanup() {
         }
     }
 
+    @Override
     public void declareOutputFields(OutputFieldsDeclarer declarer) {
     }
 }

File: storm-client/src/jvm/org/apache/storm/executor/ExecutorShutdown.java
Patch:
@@ -72,6 +72,7 @@ public void credentialsChanged(Credentials credentials) {
         }
     }
 
+    @Override
     public void loadChanged(LoadMapping loadMapping) {
         executor.reflectNewLoadMapping(loadMapping);
     }

File: storm-client/src/jvm/org/apache/storm/executor/bolt/BoltOutputCollectorImpl.java
Patch:
@@ -59,6 +59,7 @@ public BoltOutputCollectorImpl(BoltExecutor executor, Task taskData, Random rand
         this.xsfer = executor.getExecutorTransfer();
     }
 
+    @Override
     public List<Integer> emit(String streamId, Collection<Tuple> anchors, List<Object> tuple) {
         try {
             return boltEmit(streamId, anchors, tuple, null);

File: storm-client/src/jvm/org/apache/storm/generated/NumErrorsChoice.java
Patch:
@@ -39,6 +39,7 @@ private NumErrorsChoice(int value) {
   /**
    * Get the integer value of this enum value, as defined in the Thrift IDL.
    */
+  @Override
   public int getValue() {
     return value;
   }

File: storm-client/src/jvm/org/apache/storm/generated/TopologyInitialStatus.java
Patch:
@@ -38,6 +38,7 @@ private TopologyInitialStatus(int value) {
   /**
    * Get the integer value of this enum value, as defined in the Thrift IDL.
    */
+  @Override
   public int getValue() {
     return value;
   }

File: storm-client/src/jvm/org/apache/storm/grouping/Load.java
Patch:
@@ -64,6 +64,7 @@ public double getLoad() {
         return connectionLoad > boltLoad ? connectionLoad : boltLoad;
     }
 
+    @Override
     public String toString() {
         return "[:load " + boltLoad + " " + connectionLoad + "]";
     }

File: storm-client/src/jvm/org/apache/storm/grouping/PartialKeyGrouping.java
Patch:
@@ -156,6 +156,7 @@ public static class RandomTwoTaskAssignmentCreator implements AssignmentCreator
         /**
          * Creates a two task assignment by selecting random tasks.
          */
+        @Override
         public int[] createAssignment(List<Integer> tasks, byte[] key) {
             // It is necessary that this produce a deterministic assignment based on the key, so seed the Random from the key
             final long seedForRandom = Arrays.hashCode(key);
@@ -178,6 +179,7 @@ public static class BalancedTargetSelector implements TargetSelector {
         /**
          * Chooses one of the incoming tasks and selects the one that has been selected the fewest times so far.
          */
+        @Override
         public Integer chooseTask(int[] assignedTasks) {
             Integer taskIdWithMinLoad = null;
             Long minTaskLoad = Long.MAX_VALUE;

File: storm-client/src/jvm/org/apache/storm/messaging/netty/Client.java
Patch:
@@ -177,6 +177,7 @@ private void launchChannelAliveThread() {
         // netty TimerTask is already defined and hence a fully
         // qualified name
         TIMER.schedule(new java.util.TimerTask() {
+            @Override
             public void run() {
                 try {
                     LOG.debug("running timer task, address {}", dstAddress);

File: storm-client/src/jvm/org/apache/storm/messaging/netty/KerberosSaslNettyClient.java
Patch:
@@ -107,6 +107,7 @@ public KerberosSaslNettyClient(Map<String, Object> topoConf, String jaas_section
             final CallbackHandler fch = ch;
             LOG.debug("Kerberos Client with principal: {}, host: {}", fPrincipalName, fHost);
             saslClient = Subject.doAs(subject, new PrivilegedExceptionAction<SaslClient>() {
+                @Override
                 public SaslClient run() {
                     try {
                         Map<String, String> props = new TreeMap<String, String>();
@@ -146,6 +147,7 @@ public byte[] saslResponse(SaslMessageToken saslTokenMessage) {
         try {
             final SaslMessageToken fSaslTokenMessage = saslTokenMessage;
             byte[] retval = Subject.doAs(subject, new PrivilegedExceptionAction<byte[]>() {
+                @Override
                 public byte[] run() {
                     try {
                         byte[] retval = saslClient.evaluateChallenge(fSaslTokenMessage
@@ -176,6 +178,7 @@ private static class SaslClientCallbackHandler implements CallbackHandler {
          * @param callbacks objects that indicate what credential information the server's SaslServer requires from the client.
          * @throws UnsupportedCallbackException
          */
+        @Override
         public void handle(Callback[] callbacks) throws UnsupportedCallbackException {
             for (Callback callback : callbacks) {
                 LOG.info("Kerberos Client Callback Handler got callback: {}", callback.getClass());

File: storm-client/src/jvm/org/apache/storm/messaging/netty/KerberosSaslNettyServer.java
Patch:
@@ -97,6 +97,7 @@ class KerberosSaslNettyServer {
             LOG.debug("Server with host: {}", fHost);
             saslServer =
                 Subject.doAs(subject, new PrivilegedExceptionAction<SaslServer>() {
+                    @Override
                     public SaslServer run() {
                         try {
                             Map<String, String> props = new TreeMap<String, String>();
@@ -136,6 +137,7 @@ public String getUserName() {
     public byte[] response(final byte[] token) {
         try {
             byte[] retval = Subject.doAs(subject, new PrivilegedExceptionAction<byte[]>() {
+                @Override
                 public byte[] run() {
                     try {
                         LOG.debug("response: Responding to input token of length: {}",

File: storm-client/src/jvm/org/apache/storm/messaging/netty/Login.java
Patch:
@@ -111,6 +111,7 @@ public Login(final String loginContextName, CallbackHandler callbackHandler)
         // you can decrease the interval of expiration of tickets (for example, to 3 minutes) by running :
         //  "modprinc -maxlife 3mins <principal>" in kadmin.
         t = new Thread(new Runnable() {
+            @Override
             public void run() {
                 LOG.info("TGT refresh thread started.");
                 while (true) {  // renewal thread's main loop. if it exits from here, thread will exit.

File: storm-client/src/jvm/org/apache/storm/messaging/netty/SaslNettyClient.java
Patch:
@@ -108,6 +108,7 @@ public SaslClientCallbackHandler(String topologyToken, byte[] token) {
          * @param callbacks objects that indicate what credential information the server's SaslServer requires from the client.
          * @throws UnsupportedCallbackException
          */
+        @Override
         public void handle(Callback[] callbacks)
             throws UnsupportedCallbackException {
             NameCallback nc = null;

File: storm-client/src/jvm/org/apache/storm/metric/api/AssignableMetric.java
Patch:
@@ -23,6 +23,7 @@ public void setValue(Object value) {
         _value = value;
     }
 
+    @Override
     public Object getValueAndReset() {
         return _value;
     }

File: storm-client/src/jvm/org/apache/storm/metric/api/CombinedMetric.java
Patch:
@@ -25,6 +25,7 @@ public void update(Object value) {
         _value = _combiner.combine(_value, value);
     }
 
+    @Override
     public Object getValueAndReset() {
         Object ret = _value;
         _value = _combiner.identity();

File: storm-client/src/jvm/org/apache/storm/metric/api/CountMetric.java
Patch:
@@ -26,6 +26,7 @@ public void incrBy(long incrementBy) {
         _value += incrementBy;
     }
 
+    @Override
     public Object getValueAndReset() {
         long ret = _value;
         _value = 0;

File: storm-client/src/jvm/org/apache/storm/metric/api/MeanReducer.java
Patch:
@@ -24,10 +24,12 @@ class MeanReducerState {
 }
 
 public class MeanReducer implements IReducer<MeanReducerState> {
+    @Override
     public MeanReducerState init() {
         return new MeanReducerState();
     }
 
+    @Override
     public MeanReducerState reduce(MeanReducerState acc, Object input) {
         acc.count++;
         if (input instanceof Double) {
@@ -44,6 +46,7 @@ public MeanReducerState reduce(MeanReducerState acc, Object input) {
         return acc;
     }
 
+    @Override
     public Object extractResult(MeanReducerState acc) {
         if (acc.count > 0) {
             return acc.sum / (double) acc.count;

File: storm-client/src/jvm/org/apache/storm/metric/api/MultiCountMetric.java
Patch:
@@ -29,6 +29,7 @@ public CountMetric scope(String key) {
         return val;
     }
 
+    @Override
     public Map<String, Object> getValueAndReset() {
         Map<String, Object> ret = new HashMap<>();
         for (Map.Entry<String, CountMetric> e : _value.entrySet()) {

File: storm-client/src/jvm/org/apache/storm/metric/api/MultiReducedMetric.java
Patch:
@@ -31,6 +31,7 @@ public ReducedMetric scope(String key) {
         return val;
     }
 
+    @Override
     public Map<String, Object> getValueAndReset() {
         Map<String, Object> ret = new HashMap<>();
         for (Map.Entry<String, ReducedMetric> e : _value.entrySet()) {

File: storm-client/src/jvm/org/apache/storm/metric/api/ReducedMetric.java
Patch:
@@ -25,6 +25,7 @@ public void update(Object value) {
         _accumulator = _reducer.reduce(_accumulator, value);
     }
 
+    @Override
     public Object getValueAndReset() {
         Object ret = _reducer.extractResult(_accumulator);
         _accumulator = _reducer.init();

File: storm-client/src/jvm/org/apache/storm/metric/api/rpc/AssignableShellMetric.java
Patch:
@@ -19,6 +19,7 @@ public AssignableShellMetric(Object value) {
         super(value);
     }
 
+    @Override
     public void updateMetricFromRPC(Object value) {
         setValue(value);
     }

File: storm-client/src/jvm/org/apache/storm/metric/api/rpc/CombinedShellMetric.java
Patch:
@@ -20,6 +20,7 @@ public CombinedShellMetric(ICombiner combiner) {
         super(combiner);
     }
 
+    @Override
     public void updateMetricFromRPC(Object value) {
         update(value);
     }

File: storm-client/src/jvm/org/apache/storm/metric/api/rpc/CountShellMetric.java
Patch:
@@ -20,6 +20,7 @@ public class CountShellMetric extends CountMetric implements IShellMetric {
      *  if value is null, it will call incr()
      *  if value is long, it will call incrBy((long)params)
      * */
+    @Override
     public void updateMetricFromRPC(Object value) {
         if (value == null) {
             incr();

File: storm-client/src/jvm/org/apache/storm/metric/api/rpc/ReducedShellMetric.java
Patch:
@@ -21,6 +21,7 @@ public ReducedShellMetric(IReducer reducer) {
         super(reducer);
     }
 
+    @Override
     public void updateMetricFromRPC(Object value) {
         update(value);
     }

File: storm-client/src/jvm/org/apache/storm/metric/internal/CountStatAndMetric.java
Patch:
@@ -187,6 +187,7 @@ public void close() {
     }
 
     private class Fresher extends TimerTask {
+        @Override
         public void run() {
             rotateSched(System.currentTimeMillis());
         }

File: storm-client/src/jvm/org/apache/storm/metric/internal/LatencyStatAndMetric.java
Patch:
@@ -245,6 +245,7 @@ public void close() {
     }
 
     private class Fresher extends TimerTask {
+        @Override
         public void run() {
             rotateSched(System.currentTimeMillis());
         }

File: storm-client/src/jvm/org/apache/storm/metric/internal/RateTracker.java
Patch:
@@ -130,6 +130,7 @@ private synchronized void rotateBuckets(long time) {
     }
 
     private class Fresher extends TimerTask {
+        @Override
         public void run() {
             rotateBuckets(System.currentTimeMillis());
         }

File: storm-client/src/jvm/org/apache/storm/pacemaker/PacemakerClient.java
Patch:
@@ -145,10 +145,12 @@ public synchronized void channelReady(Channel channel) {
         this.notifyAll();
     }
 
+    @Override
     public String name() {
         return client_name;
     }
 
+    @Override
     public String secretKey() {
         return secret;
     }
@@ -231,6 +233,7 @@ public void gotMessage(HBMessage m) {
     public void reconnect() {
         final PacemakerClient client = this;
         timer.schedule(new TimerTask() {
+            @Override
             public void run() {
                 client.doReconnect();
             }

File: storm-client/src/jvm/org/apache/storm/security/auth/AutoSSL.java
Patch:
@@ -83,6 +83,7 @@ public static void deserializeSSLFile(String credsKey, String directory,
         }
     }
 
+    @Override
     public void prepare(Map<String, Object> conf) {
         this.conf = conf;
         writeDir = getSSLWriteDirFromConf(this.conf);

File: storm-client/src/jvm/org/apache/storm/security/auth/DefaultPrincipalToLocal.java
Patch:
@@ -21,6 +21,7 @@ public class DefaultPrincipalToLocal implements IPrincipalToLocal {
     /**
      * Invoked once immediately after construction
      */
+    @Override
     public void prepare(Map<String, Object> topoConf) {
     }
 

File: storm-client/src/jvm/org/apache/storm/security/auth/KerberosPrincipalToLocal.java
Patch:
@@ -24,6 +24,7 @@ public class KerberosPrincipalToLocal implements IPrincipalToLocal {
      *
      * @param topoConf Storm configuration
      */
+    @Override
     public void prepare(Map<String, Object> topoConf) {
     }
 

File: storm-client/src/jvm/org/apache/storm/security/auth/SimpleTransportPlugin.java
Patch:
@@ -127,6 +127,7 @@ private class SimpleWrapProcessor implements TProcessor {
             this.wrapped = wrapped;
         }
 
+        @Override
         public boolean process(final TProtocol inProt, final TProtocol outProt) throws TException {
             //populating request context 
             ReqContext req_context = ReqContext.context();
@@ -152,10 +153,12 @@ public boolean process(final TProtocol inProt, final TProtocol outProt) throws T
                 if (user != null) {
                     HashSet<Principal> principals = new HashSet<>();
                     principals.add(new Principal() {
+                        @Override
                         public String getName() {
                             return user;
                         }
 
+                        @Override
                         public String toString() {
                             return user;
                         }

File: storm-client/src/jvm/org/apache/storm/security/auth/authorizer/DenyAuthorizer.java
Patch:
@@ -26,6 +26,7 @@ public class DenyAuthorizer implements IAuthorizer {
      *
      * @param conf Storm configuration
      */
+    @Override
     public void prepare(Map<String, Object> conf) {
     }
 
@@ -37,6 +38,7 @@ public void prepare(Map<String, Object> conf) {
      * @param topoConf  configuration of targeted topology
      * @return true if the request is authorized, false if reject
      */
+    @Override
     public boolean permit(ReqContext context, String operation, Map<String, Object> topoConf) {
         return false;
     }

File: storm-client/src/jvm/org/apache/storm/security/auth/authorizer/NoopAuthorizer.java
Patch:
@@ -26,6 +26,7 @@ public class NoopAuthorizer implements IAuthorizer {
      *
      * @param conf Storm configuration
      */
+    @Override
     public void prepare(Map<String, Object> conf) {
     }
 
@@ -37,6 +38,7 @@ public void prepare(Map<String, Object> conf) {
      * @param topoConf  configuration of targeted topology
      * @return true if the request is authorized, false if reject
      */
+    @Override
     public boolean permit(ReqContext context, String operation, Map<String, Object> topoConf) {
         return true;
     }

File: storm-client/src/jvm/org/apache/storm/security/auth/digest/DigestSaslTransportPlugin.java
Patch:
@@ -36,6 +36,7 @@ public class DigestSaslTransportPlugin extends SaslTransportPlugin {
     private static final Logger LOG = LoggerFactory.getLogger(DigestSaslTransportPlugin.class);
     private WorkerTokenAuthorizer workerTokenAuthorizer;
 
+    @Override
     protected TTransportFactory getServerTransportFactory(boolean impersonationAllowed) throws IOException {
         if (workerTokenAuthorizer == null) {
             workerTokenAuthorizer = new WorkerTokenAuthorizer(conf, type);

File: storm-client/src/jvm/org/apache/storm/security/auth/kerberos/ClientCallbackHandler.java
Patch:
@@ -57,6 +57,7 @@ public ClientCallbackHandler(Configuration configuration) throws IOException {
      *
      * @param callbacks a collection of challenge callbacks
      */
+    @Override
     public void handle(Callback[] callbacks) throws IOException, UnsupportedCallbackException {
         for (Callback c : callbacks) {
             if (c instanceof NameCallback) {

File: storm-client/src/jvm/org/apache/storm/security/auth/kerberos/KerberosSaslTransportPlugin.java
Patch:
@@ -205,6 +205,7 @@ private TTransport kerberosConnect(TTransport transport, String serverHost, Stri
         try {
             Subject.doAs(subject,
                          new PrivilegedExceptionAction<Void>() {
+                             @Override
                              public Void run() {
                                  try {
                                      LOG.debug("do as:" + principal);

File: storm-client/src/jvm/org/apache/storm/security/auth/kerberos/ServerCallbackHandler.java
Patch:
@@ -50,6 +50,7 @@ public ServerCallbackHandler(Configuration configuration, boolean impersonationA
         }
     }
 
+    @Override
     public void handle(Callback[] callbacks) throws UnsupportedCallbackException {
         NameCallback nc = null;
         PasswordCallback pc = null;

File: storm-client/src/jvm/org/apache/storm/security/auth/sasl/SaslTransportPlugin.java
Patch:
@@ -124,6 +124,7 @@ private static class TUGIWrapProcessor implements TProcessor {
             this.wrapped = wrapped;
         }
 
+        @Override
         public boolean process(final TProtocol inProt, final TProtocol outProt) throws TException {
             //populating request context
             ReqContext reqContext = ReqContext.context();
@@ -163,6 +164,7 @@ public User(String name) {
         /**
          * Get the full name of the user.
          */
+        @Override
         public String getName() {
             return name;
         }

File: storm-client/src/jvm/org/apache/storm/serialization/DefaultKryoFactory.java
Patch:
@@ -32,6 +32,7 @@ public Kryo getKryo(Map<String, Object> conf) {
     public void preRegister(Kryo k, Map<String, Object> conf) {
     }
 
+    @Override
     public void postRegister(Kryo k, Map<String, Object> conf) {
         ((KryoSerializableDefault) k).overrideDefault(true);
     }

File: storm-client/src/jvm/org/apache/storm/serialization/KryoTupleSerializer.java
Patch:
@@ -29,6 +29,7 @@ public KryoTupleSerializer(final Map<String, Object> conf, final GeneralTopology
         _ids = new SerializationFactory.IdDictionary(context.getRawTopology());
     }
 
+    @Override
     public byte[] serialize(Tuple tuple) {
         try {
 

File: storm-client/src/jvm/org/apache/storm/spout/RawScheme.java
Patch:
@@ -24,12 +24,14 @@
 import org.apache.storm.utils.Utils;
 
 public class RawScheme implements Scheme {
+    @Override
     public List<Object> deserialize(ByteBuffer ser) {
         // Maintain backward compatibility for 0.10
         byte[] b = Utils.toByteArray(ser);
         return Utils.tuple(new Object[]{ b });
     }
 
+    @Override
     public Fields getOutputFields() {
         return new Fields("bytes");
     }

File: storm-client/src/jvm/org/apache/storm/spout/SpoutOutputCollector.java
Patch:
@@ -38,6 +38,7 @@ public SpoutOutputCollector(ISpoutOutputCollector delegate) {
      *
      * @return the list of task ids that this tuple was sent to
      */
+    @Override
     public List<Integer> emit(String streamId, List<Object> tuple, Object messageId) {
         return _delegate.emit(streamId, tuple, messageId);
     }
@@ -77,6 +78,7 @@ public List<Integer> emit(String streamId, List<Object> tuple) {
      * functionality will only work if the messageId is serializable via Kryo or the Serializable interface. The emitted values must be
      * immutable.
      */
+    @Override
     public void emitDirect(int taskId, String streamId, List<Object> tuple, Object messageId) {
         _delegate.emitDirect(taskId, streamId, tuple, messageId);
     }

File: storm-client/src/jvm/org/apache/storm/streams/operations/Reducer.java
Patch:
@@ -25,5 +25,6 @@ public interface Reducer<T> extends BiFunction<T, T, T> {
      * @param arg2 the second argument
      * @return the result
      */
+    @Override
     T apply(T arg1, T arg2);
 }

File: storm-client/src/jvm/org/apache/storm/task/TopologyContext.java
Patch:
@@ -316,6 +316,7 @@ public String toJSONString() {
      * @return The IMetric argument unchanged.
      */
     @Deprecated
+    @Override
     public <T extends IMetric> T registerMetric(String name, T metric, int timeBucketSizeInSecs) {
         if (_openOrPrepareWasCalled.get()) {
             throw new RuntimeException("TopologyContext.registerMetric can only be called from within overridden " +
@@ -382,6 +383,7 @@ public IMetric getRegisteredMetricByName(String name) {
      * Convenience method for registering ReducedMetric.
      */
     @Deprecated
+    @Override
     public ReducedMetric registerMetric(String name, IReducer reducer, int timeBucketSizeInSecs) {
         return registerMetric(name, new ReducedMetric(reducer), timeBucketSizeInSecs);
     }
@@ -390,6 +392,7 @@ public ReducedMetric registerMetric(String name, IReducer reducer, int timeBucke
      * Convenience method for registering CombinedMetric.
      */
     @Deprecated
+    @Override
     public CombinedMetric registerMetric(String name, ICombiner combiner, int timeBucketSizeInSecs) {
         return registerMetric(name, new CombinedMetric(combiner), timeBucketSizeInSecs);
     }

File: storm-client/src/jvm/org/apache/storm/testing/BoltTracker.java
Patch:
@@ -26,6 +26,7 @@ public BoltTracker(IRichBolt delegate, String id) {
         _richDelegate = delegate;
     }
 
+    @Override
     public void declareOutputFields(OutputFieldsDeclarer declarer) {
         _richDelegate.declareOutputFields(declarer);
     }

File: storm-client/src/jvm/org/apache/storm/testing/NonRichBoltTracker.java
Patch:
@@ -30,16 +30,19 @@ public NonRichBoltTracker(IBolt delegate, String id) {
         _trackId = id;
     }
 
+    @Override
     public void prepare(Map<String, Object> topoConf, TopologyContext context, OutputCollector collector) {
         _delegate.prepare(topoConf, context, collector);
     }
 
+    @Override
     public void execute(Tuple input) {
         _delegate.execute(input);
         Map<String, Object> stats = (Map<String, Object>) RegisteredGlobalState.getState(_trackId);
         ((AtomicInteger) stats.get("processed")).incrementAndGet();
     }
 
+    @Override
     public void cleanup() {
         _delegate.cleanup();
     }

File: storm-client/src/jvm/org/apache/storm/testing/PythonShellMetricsBolt.java
Patch:
@@ -37,16 +37,19 @@ public PythonShellMetricsBolt(String command, String file) {
         super(command, file);
     }
 
+    @Override
     public void prepare(Map<String, Object> topoConf, TopologyContext context, OutputCollector collector) {
         super.prepare(topoConf, context, collector);
 
         CountShellMetric cMetric = new CountShellMetric();
         context.registerMetric("my-custom-shell-metric", cMetric, 5);
     }
 
+    @Override
     public void declareOutputFields(OutputFieldsDeclarer declarer) {
     }
 
+    @Override
     public Map<String, Object> getComponentConfiguration() {
         return null;
     }

File: storm-client/src/jvm/org/apache/storm/testing/PythonShellMetricsSpout.java
Patch:
@@ -46,10 +46,12 @@ public void open(Map<String, Object> conf, TopologyContext context, SpoutOutputC
         context.registerMetric("my-custom-shellspout-metric", cMetric, 5);
     }
 
+    @Override
     public void declareOutputFields(OutputFieldsDeclarer declarer) {
         declarer.declare(new Fields("field1"));
     }
 
+    @Override
     public Map<String, Object> getComponentConfiguration() {
         return null;
     }

File: storm-client/src/jvm/org/apache/storm/testing/SingleUserSimpleTransport.java
Patch:
@@ -23,10 +23,12 @@ public class SingleUserSimpleTransport extends SimpleTransportPlugin {
     protected Subject getDefaultSubject() {
         HashSet<Principal> principals = new HashSet<Principal>();
         principals.add(new Principal() {
+            @Override
             public String getName() {
                 return "user";
             }
 
+            @Override
             public String toString() {
                 return "user";
             }

File: storm-client/src/jvm/org/apache/storm/testing/TestKryoDecorator.java
Patch:
@@ -17,6 +17,7 @@
 
 public class TestKryoDecorator implements IKryoDecorator {
 
+    @Override
     public void decorate(Kryo k) {
         k.register(TestSerObject.class);
     }

File: storm-client/src/jvm/org/apache/storm/testing/TestPlannerBolt.java
Patch:
@@ -22,10 +22,12 @@
 
 
 public class TestPlannerBolt extends BaseRichBolt {
+    @Override
     public void prepare(Map<String, Object> topoConf, TopologyContext context, OutputCollector collector) {
 
     }
 
+    @Override
     public void execute(Tuple input) {
 
     }
@@ -34,6 +36,7 @@ public Fields getOutputFields() {
         return new Fields("field1", "field2");
     }
 
+    @Override
     public void declareOutputFields(OutputFieldsDeclarer declarer) {
         declarer.declare(getOutputFields());
     }

File: storm-client/src/jvm/org/apache/storm/testing/TupleCaptureBolt.java
Patch:
@@ -35,10 +35,12 @@ public TupleCaptureBolt() {
         emitted_tuples.put(_name, new HashMap<String, List<FixedTuple>>());
     }
 
+    @Override
     public void prepare(Map<String, Object> topoConf, TopologyContext context, OutputCollector collector) {
         _collector = collector;
     }
 
+    @Override
     public void execute(Tuple input) {
         String component = input.getSourceComponent();
         Map<String, List<FixedTuple>> captured = emitted_tuples.get(_name);
@@ -53,6 +55,7 @@ public Map<String, List<FixedTuple>> getResults() {
         return emitted_tuples.get(_name);
     }
 
+    @Override
     public void cleanup() {
     }
 

File: storm-client/src/jvm/org/apache/storm/topology/CheckpointTupleForwarder.java
Patch:
@@ -67,6 +67,7 @@ public Map<String, Object> getComponentConfiguration() {
      * @param action          the action (prepare, commit, rollback or initstate)
      * @param txid            the transaction id.
      */
+    @Override
     protected void handleCheckpoint(Tuple checkpointTuple, Action action, long txid) {
         collector.emit(CHECKPOINT_STREAM_ID, checkpointTuple, new Values(txid, action));
         collector.ack(checkpointTuple);
@@ -82,6 +83,7 @@ protected void handleCheckpoint(Tuple checkpointTuple, Action action, long txid)
      *
      * @param input the input tuple
      */
+    @Override
     protected void handleTuple(Tuple input) {
         bolt.execute(input);
     }

File: storm-client/src/jvm/org/apache/storm/trident/operation/TridentOperationContext.java
Patch:
@@ -53,14 +53,17 @@ public int getPartitionIndex() {
         return _topoContext.getThisTaskIndex();
     }
 
+    @Override
     public <T extends IMetric> T registerMetric(String name, T metric, int timeBucketSizeInSecs) {
         return _topoContext.registerMetric(name, metric, timeBucketSizeInSecs);
     }
 
+    @Override
     public ReducedMetric registerMetric(String name, IReducer reducer, int timeBucketSizeInSecs) {
         return _topoContext.registerMetric(name, new ReducedMetric(reducer), timeBucketSizeInSecs);
     }
 
+    @Override
     public CombinedMetric registerMetric(String name, ICombiner combiner, int timeBucketSizeInSecs) {
         return _topoContext.registerMetric(name, new CombinedMetric(combiner), timeBucketSizeInSecs);
     }

File: storm-client/src/jvm/org/apache/storm/trident/testing/FeederBatchSpout.java
Patch:
@@ -45,6 +45,7 @@ public void setWaitToEmit(boolean trueIfWait) {
         _waitToEmit = trueIfWait;
     }
 
+    @Override
     public void feed(Object tuples) {
         Semaphore sem = new Semaphore(0);
         ((List) RegisteredGlobalState.getState(_semaphoreId)).add(sem);

File: storm-client/src/jvm/org/apache/storm/trident/topology/TransactionAttempt.java
Patch:
@@ -34,10 +34,12 @@ public Long getTransactionId() {
         return _txid;
     }
 
+    @Override
     public Object getId() {
         return _txid;
     }
 
+    @Override
     public int getAttemptId() {
         return _attemptId;
     }

File: storm-client/src/jvm/org/apache/storm/trident/windowing/AbstractTridentWindowManager.java
Patch:
@@ -138,10 +138,12 @@ private void execAggregatorAndStoreResult(int currentTriggerId, List<T> tupleEve
      */
     protected abstract List<TridentTuple> getTridentTuples(List<T> tupleEvents);
 
+    @Override
     public Queue<TriggerResult> getPendingTriggers() {
         return pendingTriggers;
     }
 
+    @Override
     public void shutdown() {
         try {
             LOG.info("window manager [{}] is being shutdown", windowManager);

File: storm-client/src/jvm/org/apache/storm/trident/windowing/InMemoryTridentWindowManager.java
Patch:
@@ -47,6 +47,7 @@ public void onTuplesExpired(List<TridentTuple> expiredTuples) {
         LOG.debug("InMemoryTridentWindowManager.onTuplesExpired");
     }
 
+    @Override
     public void addTuplesBatch(Object batchId, List<TridentTuple> tuples) {
         LOG.debug("Adding tuples to window-manager for batch: [{}]", batchId);
         for (TridentTuple tridentTuple : tuples) {

File: storm-client/src/jvm/org/apache/storm/trident/windowing/config/BaseWindowConfig.java
Patch:
@@ -34,6 +34,7 @@ public int getSlidingLength() {
         return slideLength;
     }
 
+    @Override
     public void validate() {
         if (slideLength > windowLength) {
             throw new IllegalArgumentException(

File: storm-client/src/jvm/org/apache/storm/tuple/Fields.java
Patch:
@@ -81,6 +81,7 @@ public String get(int index) {
         return _fields.get(index);
     }
 
+    @Override
     public Iterator<String> iterator() {
         return _fields.iterator();
     }

File: storm-client/src/jvm/org/apache/storm/utils/DRPCClient.java
Patch:
@@ -119,6 +119,7 @@ public int getPort() {
         return port;
     }
 
+    @Override
     public String execute(String func, String args) throws TException, DRPCExecutionException, AuthorizationException {
         if (func == null) {
             throw new IllegalArgumentException("DRPC Function cannot be null");

File: storm-client/src/jvm/org/apache/storm/utils/DefaultShellLogHandler.java
Patch:
@@ -47,6 +47,7 @@ private Logger getLogger(final Class<?> ownerCls) {
      * @param context  - the current {@link TopologyContext}.
      * @see {@link ShellLogHandler#setUpContext}
      */
+    @Override
     public void setUpContext(final Class<?> ownerCls, final ShellProcess process,
                              final TopologyContext context) {
         this.log = getLogger(ownerCls);
@@ -61,6 +62,7 @@ public void setUpContext(final Class<?> ownerCls, final ShellProcess process,
      * @param shellMsg - the {@link ShellMsg} to log.
      * @see {@link ShellLogHandler#log}
      */
+    @Override
     public void log(final ShellMsg shellMsg) {
         if (shellMsg == null) {
             throw new IllegalArgumentException("shellMsg is required");

File: storm-client/src/jvm/org/apache/storm/utils/TimeCacheMap.java
Patch:
@@ -42,6 +42,7 @@ public TimeCacheMap(int expirationSecs, int numBuckets, ExpiredCallback<K, V> ca
         final long expirationMillis = expirationSecs * 1000L;
         final long sleepTime = expirationMillis / (numBuckets - 1);
         _cleaner = new Thread(new Runnable() {
+            @Override
             public void run() {
                 try {
                     while (true) {

File: storm-client/src/jvm/org/apache/storm/windowing/WatermarkCountEvictionPolicy.java
Patch:
@@ -32,6 +32,7 @@ public WatermarkCountEvictionPolicy(int count) {
         currentCount = new AtomicLong();
     }
 
+    @Override
     public Action evict(Event<T> event) {
         if (getContext() == null) {
             //It is possible to get asked about eviction before we have a context, due to WindowManager.compactWindow.

File: storm-client/test/jvm/org/apache/storm/PaceMakerStateStorageFactoryTest.java
Patch:
@@ -147,10 +147,12 @@ public PacemakerClient getWriteClient() {
             return clientMock;
         }
 
+        @Override
         public HBMessage send(HBMessage m) throws PacemakerConnectionException, InterruptedException {
             return clientMock.send(m);
         }
 
+        @Override
         public List<HBMessage> sendAll(HBMessage m) throws PacemakerConnectionException, InterruptedException {
             List<HBMessage> response = new ArrayList<>();
             response.add(clientMock.send(m));

File: storm-client/test/jvm/org/apache/storm/bolt/TestJoinBolt.java
Patch:
@@ -331,10 +331,12 @@ public MockContext(String[] fieldNames) {
             this.fields = new Fields(fieldNames);
         }
 
+        @Override
         public String getComponentId(int taskId) {
             return "component";
         }
 
+        @Override
         public Fields getComponentOutputFields(String componentId, String streamId) {
             return fields;
         }

File: storm-client/test/jvm/org/apache/storm/dependency/DependencyUploaderTest.java
Patch:
@@ -152,6 +152,7 @@ public void uploadFiles() throws Exception {
 
         final AtomicInteger counter = new AtomicInteger();
         final Answer incrementCounter = new Answer() {
+            @Override
             public Object answer(InvocationOnMock invocation) throws Throwable {
                 counter.addAndGet(1);
                 return null;
@@ -271,6 +272,7 @@ public void uploadArtifacts() throws Exception {
 
         final AtomicInteger counter = new AtomicInteger();
         final Answer incrementCounter = new Answer() {
+            @Override
             public Object answer(InvocationOnMock invocation) throws Throwable {
                 counter.addAndGet(1);
                 return null;

File: storm-server/src/main/java/org/apache/storm/localizer/LocalizedResourceRetentionSet.java
Patch:
@@ -135,6 +135,7 @@ public String toString() {
     }
 
     static class LRUComparator implements Comparator<LocallyCachedBlob> {
+        @Override
         public int compare(LocallyCachedBlob r1, LocallyCachedBlob r2) {
             long ret = r1.getLastUsed() - r2.getLastUsed();
             if (0 == ret) {

File: storm-server/src/main/java/org/apache/storm/logging/filters/AccessLoggingFilter.java
Patch:
@@ -28,10 +28,12 @@ public class AccessLoggingFilter implements Filter {
 
     private static final Logger LOG = LoggerFactory.getLogger(AccessLoggingFilter.class);
 
+    @Override
     public void init(FilterConfig config) throws ServletException {
         //NOOP
     }
 
+    @Override
     public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException {
         handle((HttpServletRequest) request, (HttpServletResponse) response, chain);
     }
@@ -44,6 +46,7 @@ public void handle(HttpServletRequest request, HttpServletResponse response, Fil
         chain.doFilter(request, response);
     }
 
+    @Override
     public void destroy() {
         //NOOP
     }

File: storm-server/src/main/java/org/apache/storm/metricstore/Metric.java
Patch:
@@ -83,6 +83,7 @@ public Metric(Metric o) {
     /**
      *  Check if a Metric matches another object.
      */
+    @Override
     public boolean equals(Object other) {
 
         if (!(other instanceof Metric)) {

File: storm-server/src/main/java/org/apache/storm/metricstore/MetricStore.java
Patch:
@@ -45,6 +45,7 @@ public interface MetricStore extends AutoCloseable {
     /**
      * Close the metric store.
      */
+    @Override
     void close();
 
     /**

File: storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/RocksDbStore.java
Patch:
@@ -170,6 +170,7 @@ private String getRocksDbAbsoluteDir(Map<String, Object> conf) throws MetricExce
      * @param metric  Metric to store
      * @throws MetricException  if database write fails
      */
+    @Override
     public void insert(Metric metric) throws MetricException {
         try {
             // don't bother blocking on a full queue, just drop metrics in case we can't keep up
@@ -341,6 +342,7 @@ public void close() {
      * @param scanCallback  callback for each Metric found
      * @throws MetricException  on error
      */
+    @Override
     public void scan(FilterOptions filter, ScanCallback scanCallback) throws MetricException {
         scanInternal(filter, scanCallback, null);
     }

File: storm-server/src/main/java/org/apache/storm/scheduler/resource/User.java
Patch:
@@ -207,6 +207,7 @@ public String toString() {
      */
     static class PQsortByPriorityAndSubmittionTime implements Comparator<TopologyDetails> {
 
+        @Override
         public int compare(TopologyDetails topo1, TopologyDetails topo2) {
             if (topo1.getTopologyPriority() > topo2.getTopologyPriority()) {
                 return 1;

File: storm-webapp/src/main/java/org/apache/storm/daemon/ui/exceptionmappers/NotAliveExceptionMapper.java
Patch:
@@ -34,6 +34,7 @@ public class NotAliveExceptionMapper implements ExceptionMapper<NotAliveExceptio
     @Inject
     public javax.inject.Provider<HttpServletRequest> request;
 
+    @Override
     public Response toResponse(NotAliveException ex) {
         return getResponse(ex, request);
     }

File: storm-client/src/jvm/org/apache/storm/security/auth/workertoken/WorkerTokenAuthorizer.java
Patch:
@@ -152,4 +152,4 @@ public void close() {
             state.disconnect();
         }
     }
-}
\ No newline at end of file
+}

File: storm-client/src/jvm/org/apache/storm/utils/SupervisorClient.java
Patch:
@@ -28,7 +28,7 @@
  * <li>nimbus -> supervisor: assign assignments for a node.</li>
  * </ul>
  */
-public class SupervisorClient extends ThriftClient {
+public class SupervisorClient extends ThriftClient implements SupervisorIfaceFactory {
     private static final Logger LOG = LoggerFactory.getLogger(SupervisorClient.class);
     private Supervisor.Client client;
 
@@ -76,7 +76,8 @@ public static SupervisorClient getConfiguredClientAs(Map conf, String host, int
         }
     }
 
-    public Supervisor.Client getClient() {
+    @Override
+    public Supervisor.Client getIface() {
         return client;
     }
 }

File: storm-server/src/main/java/org/apache/storm/daemon/supervisor/Container.java
Patch:
@@ -309,7 +309,7 @@ private boolean isPosixProcessAlive(long pid, String user) throws IOException {
         }
         return ret;
     }
-
+    
     @Override
     public boolean areAllProcessesDead() throws IOException {
         Set<Long> pids = getAllPids();
@@ -325,7 +325,7 @@ public boolean areAllProcessesDead() throws IOException {
                 break;
             }
         }
-
+        
         if (allDead && shutdownTimer != null) {
             shutdownTimer.stop();
             shutdownTimer = null;

File: storm-server/src/main/java/org/apache/storm/daemon/supervisor/ReadClusterState.java
Patch:
@@ -86,7 +86,8 @@ public ReadClusterState(Supervisor supervisor) throws Exception {
         this.slotMetrics = supervisor.getSlotMetrics();
 
         this.launcher = ContainerLauncher.make(superConf, assignmentId, supervisorPort,
-            supervisor.getSharedContext(), supervisor.getMetricsRegistry(), supervisor.getContainerMemoryTracker());
+            supervisor.getSharedContext(), supervisor.getMetricsRegistry(), supervisor.getContainerMemoryTracker(),
+            supervisor.getSupervisorThriftInterface());
 
         this.metricsProcessor = null;
         try {

File: storm-server/src/main/java/org/apache/storm/nimbus/AssignmentDistributionService.java
Patch:
@@ -288,7 +288,7 @@ private void sendAssignmentsToNode(NodeAssignments assignments) {
                 try (SupervisorClient client = SupervisorClient.getConfiguredClient(service.getConf(),
                                                                                     assignments.getHost(), assignments.getServerPort())) {
                     try {
-                        client.getClient().sendSupervisorAssignments(assignments.getAssignments());
+                        client.getIface().sendSupervisorAssignments(assignments.getAssignments());
                     } catch (Exception e) {
                         //just ignore the exception.
                         LOG.error("Exception when trying to send assignments to node {}: {}", assignments.getNode(), e.getMessage());

File: storm-server/src/main/java/org/apache/storm/DaemonConfig.java
Patch:
@@ -317,7 +317,7 @@ public class DaemonConfig implements Validated {
     public static final String UI_DISABLE_HTTP_BINDING = "ui.disable.http.binding";
 
     /**
-     * This controls whether Storm UI would not monitor Spout lag.
+     * This controls whether Storm UI displays spout lag for the Kafka spout.
      */
     @isBoolean
     public static final String UI_DISABLE_SPOUT_LAG_MONITORING = "ui.disable.spout.lag.monitoring";

File: storm-webapp/src/main/java/org/apache/storm/daemon/ui/UIHelpers.java
Patch:
@@ -1619,7 +1619,7 @@ public static Map<String, Object> getTopologyWorkers(TopologyInfo topologyInfo,
      * @return getTopologyLag.
      */
     public static Map<String, Map<String, Object>> getTopologyLag(StormTopology userTopology, Map<String,Object> config) {
-        Boolean disableLagMonitoring = ObjectReader.getBoolean(config.get(DaemonConfig.UI_DISABLE_SPOUT_LAG_MONITORING), false);
+        Boolean disableLagMonitoring = (Boolean)(config.get(DaemonConfig.UI_DISABLE_SPOUT_LAG_MONITORING));
         return disableLagMonitoring ? Collections.EMPTY_MAP : TopologySpoutLag.lag(userTopology, config);
     }
 

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java
Patch:
@@ -40,7 +40,6 @@
 public class KafkaSpoutConfig<K, V> extends CommonKafkaSpoutConfig<K, V> {
 
     private static final long serialVersionUID = 141902646130682494L;
-    // 30s
     public static final long DEFAULT_OFFSET_COMMIT_PERIOD_MS = 30_000;
     // Retry forever
     public static final int DEFAULT_MAX_RETRIES = Integer.MAX_VALUE;

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/CommonKafkaSpoutConfig.java
Patch:
@@ -43,9 +43,7 @@
 import org.slf4j.LoggerFactory;
 
 public abstract class CommonKafkaSpoutConfig<K, V> implements Serializable {
-    // 200ms
     public static final long DEFAULT_POLL_TIMEOUT_MS = 200;
-    // 2s
     public static final long DEFAULT_PARTITION_REFRESH_PERIOD_MS = 2_000;
 
     public static final FirstPollOffsetStrategy DEFAULT_FIRST_POLL_OFFSET_STRATEGY = FirstPollOffsetStrategy.UNCOMMITTED_EARLIEST;
@@ -150,7 +148,7 @@ public T setProp(Properties props) {
 
         //Spout Settings
         /**
-         * Specifies the time, in milliseconds, spent waiting in poll if data is not available. Default is 2s.
+         * Specifies the time, in milliseconds, spent waiting in poll if data is not available. Default is 200ms.
          *
          * @param pollTimeoutMs time in ms
          */

File: storm-client/src/jvm/org/apache/storm/messaging/netty/StormServerHandler.java
Patch:
@@ -26,7 +26,7 @@
 
 public class StormServerHandler extends ChannelInboundHandlerAdapter {
     private static final Logger LOG = LoggerFactory.getLogger(StormServerHandler.class);
-    private static final Set<Class> ALLOWED_EXCEPTIONS = new HashSet<>(Arrays.asList(new Class[]{ IOException.class }));
+    private static final Set<Class<?>> ALLOWED_EXCEPTIONS = new HashSet<>(Arrays.asList(new Class<?>[]{ IOException.class }));
     private final IServer server;
     private final AtomicInteger failure_count;
 

File: storm-core/src/jvm/org/apache/storm/testing/MockLeaderElector.java
Patch:
@@ -47,7 +47,7 @@ public void addToLeaderLockQueue() throws Exception {
     }
 
     @Override
-    public void removeFromLeaderLockQueue() throws Exception {
+    public void quitElectionFor(int delayMs) throws Exception {
         //NOOP
     }
 

File: storm-server/src/main/java/org/apache/storm/blobstore/LocalFsBlobStore.java
Patch:
@@ -171,7 +171,7 @@ private void blobSync() throws Exception {
                 sync.setZookeeperKeySet(zkKeys);
                 sync.setZkClient(zkClient);
                 sync.syncBlobs();
-            } //else not leader (NOOP)
+            } //else leader (NOOP)
         } //else local (NOOP)
     }
 

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java
Patch:
@@ -40,7 +40,6 @@
 public class KafkaSpoutConfig<K, V> extends CommonKafkaSpoutConfig<K, V> {
 
     private static final long serialVersionUID = 141902646130682494L;
-    // 30s
     public static final long DEFAULT_OFFSET_COMMIT_PERIOD_MS = 30_000;
     // Retry forever
     public static final int DEFAULT_MAX_RETRIES = Integer.MAX_VALUE;

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/CommonKafkaSpoutConfig.java
Patch:
@@ -43,9 +43,7 @@
 import org.slf4j.LoggerFactory;
 
 public abstract class CommonKafkaSpoutConfig<K, V> implements Serializable {
-    // 200ms
     public static final long DEFAULT_POLL_TIMEOUT_MS = 200;
-    // 2s
     public static final long DEFAULT_PARTITION_REFRESH_PERIOD_MS = 2_000;
 
     public static final FirstPollOffsetStrategy DEFAULT_FIRST_POLL_OFFSET_STRATEGY = FirstPollOffsetStrategy.UNCOMMITTED_EARLIEST;
@@ -150,7 +148,7 @@ public T setProp(Properties props) {
 
         //Spout Settings
         /**
-         * Specifies the time, in milliseconds, spent waiting in poll if data is not available. Default is 2s.
+         * Specifies the time, in milliseconds, spent waiting in poll if data is not available. Default is 200ms.
          *
          * @param pollTimeoutMs time in ms
          */

File: storm-server/src/main/java/org/apache/storm/localizer/LocalizedResource.java
Patch:
@@ -276,7 +276,7 @@ public long fetchUnzipToTemp(ClientBlobStore store) throws IOException, KeyNotFo
     }
 
     @Override
-    public void commitNewVersion(long version) throws IOException {
+    protected void commitNewVersion(long version) throws IOException {
         String key = getKey();
         LOG.info("Blob: {} updated to version {} from version {}", key, version, getLocalVersion());
         Path localVersionFile = versionFilePath;

File: storm-server/src/main/java/org/apache/storm/localizer/LocallyCachedTopologyBlob.java
Patch:
@@ -204,7 +204,7 @@ public boolean isFullyDownloaded() {
     }
 
     @Override
-    public void commitNewVersion(long newVersion) throws IOException {
+    protected void commitNewVersion(long newVersion) throws IOException {
         //This is not atomic (so if something bad happens in the middle we need to be able to recover
         Path tempLoc = topologyBasicBlobsRootDir.resolve(type.getTempFileName(newVersion));
         Path dest = topologyBasicBlobsRootDir.resolve(type.getFileName());

File: storm-client/src/jvm/org/apache/storm/executor/bolt/BoltExecutor.java
Patch:
@@ -100,7 +100,7 @@ public BoltExecutorStats getStats() {
     public void init(ArrayList<Task> idToTask, int idToTaskBase) {
         executorTransfer.initLocalRecvQueues();
         while (!stormActive.get()) {
-            Utils.sleep(100);
+            Utils.sleepNoSimulation(100);
         }
 
         if (!componentId.equals(StormCommon.SYSTEM_STREAM_ID)) { // System bolt doesn't call reportError()

File: storm-client/src/jvm/org/apache/storm/executor/spout/SpoutExecutor.java
Patch:
@@ -98,7 +98,7 @@ public void init(final ArrayList<Task> idToTask, int idToTaskBase) {
         this.threadId = Thread.currentThread().getId();
         executorTransfer.initLocalRecvQueues();
         while (!stormActive.get()) {
-            Utils.sleep(100);
+            Utils.sleepNoSimulation(100);
         }
 
         LOG.info("Opening spout {}:{}", componentId, taskIds);

File: storm-client/src/jvm/org/apache/storm/utils/InprocMessaging.java
Patch:
@@ -66,7 +66,7 @@ public static void waitForReader(int port) {
         long start = Time.currentTimeMillis();
         while (!ab.get()) {
             if (Time.isSimulating()) {
-                Time.advanceTime(100);
+                Time.advanceTime(10);
             }
             try {
                 Thread.sleep(10);

File: storm-client/src/jvm/org/apache/storm/zookeeper/ClientZookeeper.java
Patch:
@@ -314,7 +314,7 @@ public CuratorFramework mkClientImpl(Map<String, Object> conf, List<String> serv
                 watcher.execute(event.getState(), event.getType(), event.getPath());
             }
         });
-        LOG.info("Staring ZK Curator");
+        LOG.info("Starting ZK Curator");
         fk.start();
         return fk;
     }

File: storm-core/test/jvm/org/apache/storm/integration/TopologyIntegrationTest.java
Patch:
@@ -16,8 +16,8 @@
 
 package org.apache.storm.integration;
 
-import static org.apache.storm.integration.AssertLoop.assertAcked;
-import static org.apache.storm.integration.AssertLoop.assertFailed;
+import static org.apache.storm.AssertLoop.assertAcked;
+import static org.apache.storm.AssertLoop.assertFailed;
 import static org.hamcrest.CoreMatchers.is;
 import static org.hamcrest.Matchers.contains;
 import static org.hamcrest.Matchers.containsInAnyOrder;

File: storm-server/src/main/java/org/apache/storm/LocalCluster.java
Patch:
@@ -720,6 +720,7 @@ private boolean areAllSupervisorsWaiting() {
 
     /**
      * Wait for the cluster to be idle.  This is intended to be used with Simulated time and is for internal testing.
+     * Note that this does not wait for spout or bolt executors to be idle.
      *
      * @throws InterruptedException if interrupted while waiting.
      * @throws AssertionError       if the cluster did not come to an idle point with a timeout.
@@ -730,6 +731,7 @@ public void waitForIdle() throws InterruptedException {
 
     /**
      * Wait for the cluster to be idle.  This is intended to be used with Simulated time and is for internal testing.
+     * Note that this does not wait for spout or bolt executors to be idle.
      *
      * @param timeoutMs the number of ms to wait before throwing an error.
      * @throws InterruptedException if interrupted while waiting.

File: storm-server/src/test/java/org/apache/storm/AssertLoop.java
Patch:
@@ -14,7 +14,7 @@
  * limitations under the License.
  */
 
-package org.apache.storm.integration;
+package org.apache.storm;
 
 import static org.apache.storm.utils.PredicateMatcher.matchesPredicate;
 import static org.hamcrest.CoreMatchers.everyItem;
@@ -27,13 +27,13 @@
 import org.awaitility.Awaitility;
 import org.awaitility.core.ConditionTimeoutException;
 
-class AssertLoop {
+public class AssertLoop {
 
     public static void assertLoop(Predicate<Object> condition, Object... conditionParams) {
         try {
             Awaitility.with()
                 .pollInterval(1, TimeUnit.MILLISECONDS)
-                .atMost(10, TimeUnit.SECONDS)
+                .atMost(Testing.TEST_TIMEOUT_MS, TimeUnit.MILLISECONDS)
                 .untilAsserted(() -> assertThat(Arrays.asList(conditionParams), everyItem(matchesPredicate(condition))));
         } catch (ConditionTimeoutException e) {
             throw new AssertionError(e.getMessage());

File: storm-client/src/jvm/org/apache/storm/metric/cgroup/CGroupMetricsBase.java
Patch:
@@ -39,7 +39,7 @@ public CGroupMetricsBase(Map<String, Object> conf, SubSystemType type) {
         enabled = false;
         CgroupCenter center = CgroupCenter.getInstance();
         if (center == null) {
-            LOG.warn("{} is diabled. cgroups do not appear to be enabled on this system", simpleName);
+            LOG.warn("{} is disabled. cgroups do not appear to be enabled on this system", simpleName);
             return;
         }
         if (!center.isSubSystemEnabled(type)) {

File: storm-client/src/jvm/org/apache/storm/testing/FixedTupleSpout.java
Patch:
@@ -22,7 +22,6 @@
 import org.apache.storm.topology.IRichSpout;
 import org.apache.storm.topology.OutputFieldsDeclarer;
 import org.apache.storm.tuple.Fields;
-import org.apache.storm.utils.Utils;
 
 import static org.apache.storm.utils.Utils.get;
 
@@ -131,8 +130,6 @@ public void nextTuple() {
             String id = UUID.randomUUID().toString();
             _pending.put(id, ft);
             _collector.emit(ft.stream, ft.values, id);
-        } else {
-            Utils.sleep(100);
         }
     }
 

File: storm-server/src/main/java/org/apache/storm/daemon/supervisor/Supervisor.java
Patch:
@@ -30,6 +30,7 @@
 import java.util.concurrent.ExecutorService;
 import java.util.concurrent.Executors;
 import java.util.concurrent.atomic.AtomicReference;
+import java.util.function.BiConsumer;
 import org.apache.commons.io.FileUtils;
 import org.apache.storm.Config;
 import org.apache.storm.DaemonConfig;
@@ -529,7 +530,7 @@ void killWorkers(Collection<String> workerIds, ContainerLauncher launcher) throw
         }
     }
 
-    public void shutdownAllWorkers(UniFunc<Slot> onWarnTimeout, UniFunc<Slot> onErrorTimeout) {
+    public void shutdownAllWorkers(BiConsumer<Slot, Long> onWarnTimeout, UniFunc<Slot> onErrorTimeout) {
         if (readState != null) {
             readState.shutdownAllWorkers(onWarnTimeout, onErrorTimeout);
         } else {

File: storm-client/src/jvm/org/apache/storm/executor/bolt/BoltExecutor.java
Patch:
@@ -100,7 +100,7 @@ public BoltExecutorStats getStats() {
     public void init(ArrayList<Task> idToTask, int idToTaskBase) {
         executorTransfer.initLocalRecvQueues();
         while (!stormActive.get()) {
-            Utils.sleep(100);
+            Utils.sleepNoSimulation(100);
         }
 
         if (!componentId.equals(StormCommon.SYSTEM_STREAM_ID)) { // System bolt doesn't call reportError()

File: storm-client/src/jvm/org/apache/storm/executor/spout/SpoutExecutor.java
Patch:
@@ -98,7 +98,7 @@ public void init(final ArrayList<Task> idToTask, int idToTaskBase) {
         this.threadId = Thread.currentThread().getId();
         executorTransfer.initLocalRecvQueues();
         while (!stormActive.get()) {
-            Utils.sleep(100);
+            Utils.sleepNoSimulation(100);
         }
 
         LOG.info("Opening spout {}:{}", componentId, taskIds);

File: storm-client/src/jvm/org/apache/storm/utils/InprocMessaging.java
Patch:
@@ -66,7 +66,7 @@ public static void waitForReader(int port) {
         long start = Time.currentTimeMillis();
         while (!ab.get()) {
             if (Time.isSimulating()) {
-                Time.advanceTime(100);
+                Time.advanceTime(10);
             }
             try {
                 Thread.sleep(10);

File: storm-client/src/jvm/org/apache/storm/zookeeper/ClientZookeeper.java
Patch:
@@ -314,7 +314,7 @@ public CuratorFramework mkClientImpl(Map<String, Object> conf, List<String> serv
                 watcher.execute(event.getState(), event.getType(), event.getPath());
             }
         });
-        LOG.info("Staring ZK Curator");
+        LOG.info("Starting ZK Curator");
         fk.start();
         return fk;
     }

File: storm-core/test/jvm/org/apache/storm/integration/TopologyIntegrationTest.java
Patch:
@@ -16,8 +16,8 @@
 
 package org.apache.storm.integration;
 
-import static org.apache.storm.integration.AssertLoop.assertAcked;
-import static org.apache.storm.integration.AssertLoop.assertFailed;
+import static org.apache.storm.AssertLoop.assertAcked;
+import static org.apache.storm.AssertLoop.assertFailed;
 import static org.hamcrest.CoreMatchers.is;
 import static org.hamcrest.Matchers.contains;
 import static org.hamcrest.Matchers.containsInAnyOrder;

File: storm-server/src/main/java/org/apache/storm/LocalCluster.java
Patch:
@@ -720,6 +720,7 @@ private boolean areAllSupervisorsWaiting() {
 
     /**
      * Wait for the cluster to be idle.  This is intended to be used with Simulated time and is for internal testing.
+     * Note that this does not wait for spout or bolt executors to be idle.
      *
      * @throws InterruptedException if interrupted while waiting.
      * @throws AssertionError       if the cluster did not come to an idle point with a timeout.
@@ -730,6 +731,7 @@ public void waitForIdle() throws InterruptedException {
 
     /**
      * Wait for the cluster to be idle.  This is intended to be used with Simulated time and is for internal testing.
+     * Note that this does not wait for spout or bolt executors to be idle.
      *
      * @param timeoutMs the number of ms to wait before throwing an error.
      * @throws InterruptedException if interrupted while waiting.

File: storm-server/src/test/java/org/apache/storm/AssertLoop.java
Patch:
@@ -14,7 +14,7 @@
  * limitations under the License.
  */
 
-package org.apache.storm.integration;
+package org.apache.storm;
 
 import static org.apache.storm.utils.PredicateMatcher.matchesPredicate;
 import static org.hamcrest.CoreMatchers.everyItem;
@@ -27,13 +27,13 @@
 import org.awaitility.Awaitility;
 import org.awaitility.core.ConditionTimeoutException;
 
-class AssertLoop {
+public class AssertLoop {
 
     public static void assertLoop(Predicate<Object> condition, Object... conditionParams) {
         try {
             Awaitility.with()
                 .pollInterval(1, TimeUnit.MILLISECONDS)
-                .atMost(10, TimeUnit.SECONDS)
+                .atMost(Testing.TEST_TIMEOUT_MS, TimeUnit.MILLISECONDS)
                 .untilAsserted(() -> assertThat(Arrays.asList(conditionParams), everyItem(matchesPredicate(condition))));
         } catch (ConditionTimeoutException e) {
             throw new AssertionError(e.getMessage());

File: storm-server/src/main/java/org/apache/storm/nimbus/LeaderListenerCallback.java
Patch:
@@ -196,8 +196,7 @@ private Set<String> getTopologyDependencyKeys(Set<String> activeTopologyCodeKeys
         Subject subject = ReqContext.context().subject();
 
         for (String activeTopologyCodeKey : activeTopologyCodeKeys) {
-            try {
-                InputStreamWithMeta blob = blobStore.getBlob(activeTopologyCodeKey, subject);
+            try (InputStreamWithMeta blob = blobStore.getBlob(activeTopologyCodeKey, subject)) {
                 byte[] blobContent = IOUtils.readFully(blob, new Long(blob.getFileLength()).intValue());
                 StormTopology stormCode = Utils.deserialize(blobContent, StormTopology.class);
                 if (stormCode.is_set_dependency_jars()) {

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/OffsetManager.java
Patch:
@@ -129,6 +129,7 @@ public OffsetAndMetadata findNextCommitOffset(final String commitMetadata) {
                     if (nextEmittedOffset != null && currOffset == nextEmittedOffset) {
                         LOG.debug("Found committable offset: [{}] after missing offset: [{}], skipping to the committable offset",
                             currOffset, nextCommitOffset);
+                        found = true;
                         nextCommitOffset = currOffset + 1;
                     } else {
                         LOG.debug("Topic-partition [{}] has non-sequential offset [{}]."

File: external/storm-kafka-client/src/test/java/org/apache/storm/kafka/spout/KafkaSpoutAbstractTest.java
Patch:
@@ -36,7 +36,6 @@
 import org.apache.storm.kafka.spout.internal.ConsumerFactory;
 import org.apache.storm.kafka.spout.internal.ConsumerFactoryDefault;
 import org.apache.storm.kafka.spout.subscription.TopicAssigner;
-import org.apache.storm.kafka.spout.subscription.TopicAssigner;
 import org.apache.storm.spout.SpoutOutputCollector;
 import org.apache.storm.task.TopologyContext;
 import org.apache.storm.tuple.Values;

File: external/storm-kafka-client/src/test/java/org/apache/storm/kafka/spout/KafkaSpoutMessagingGuaranteeTest.java
Patch:
@@ -212,7 +212,7 @@ public void testAtMostOnceModeDoesNotCommitAckedTuples() throws Exception {
 
             spout.nextTuple();
 
-            verify(consumerMock, never()).commitSync(argThat(arg -> {
+            verify(consumerMock, never()).commitSync(argThat((Map<TopicPartition, OffsetAndMetadata> arg) -> {
                 return !arg.containsKey(partition);
             }));
         }

File: external/storm-hive/src/main/java/org/apache/storm/hive/common/HiveOptions.java
Patch:
@@ -19,7 +19,7 @@ public class HiveOptions implements Serializable {
     /**
      * Half of the default Config.TOPOLOGY_MESSAGE_TIMEOUT_SECS
      */
-    private static final int DEFAULT_TICK_TUPLE_INTERVAL_SECS = 15;
+    public static final int DEFAULT_TICK_TUPLE_INTERVAL_SECS = 15;
 
     protected HiveMapper mapper;
     protected String databaseName;

File: storm-server/src/main/java/org/apache/storm/localizer/LocallyCachedTopologyBlob.java
Patch:
@@ -189,14 +189,16 @@ protected void extractDirFromJar(String jarpath, String dir, Path dest) throws I
                 JarEntry entry = jarEnums.nextElement();
                 String name = entry.getName();
                 if (!entry.isDirectory() && name.startsWith(toRemove)) {
-                    String shortenedName = name.replace(toRemove, "");
+                    String shortenedName = name.substring(toRemove.length());
                     Path targetFile = dest.resolve(shortenedName);
                     LOG.debug("EXTRACTING {} SHORTENED to {} into {}", name, shortenedName, targetFile);
                     fsOps.forceMkdir(targetFile.getParent());
                     try (FileOutputStream out = new FileOutputStream(targetFile.toFile());
                          InputStream in = jarFile.getInputStream(entry)) {
                         IOUtils.copy(in, out);
                     }
+                } else {
+                    LOG.debug("Skipping {}", entry);
                 }
             }
         }

File: storm-client/src/jvm/org/apache/storm/topology/ConfigurableTopology.java
Patch:
@@ -86,7 +86,7 @@ public static Config loadConf(String resource, Config conf)
             if (ret.size() == 1) {
                 Object confNode = ret.get("config");
                 if (confNode != null && confNode instanceof Map) {
-                    ret = (Map<String, Object>) ret;
+                    ret = (Map<String, Object>) confNode;
                 }
             }
         }

File: storm-client/src/jvm/org/apache/storm/topology/ConfigurableTopology.java
Patch:
@@ -86,7 +86,7 @@ public static Config loadConf(String resource, Config conf)
             if (ret.size() == 1) {
                 Object confNode = ret.get("config");
                 if (confNode != null && confNode instanceof Map) {
-                    ret = (Map<String, Object>) ret;
+                    ret = (Map<String, Object>) confNode;
                 }
             }
         }

File: storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/ReadOnlyStringMetadataCache.java
Patch:
@@ -11,12 +11,9 @@
 
 package org.apache.storm.metricstore.rocksdb;
 
-import org.apache.http.annotation.ThreadSafe;
-
 /**
  * The read-only interface to a StringMetadataCache allowed to be used by any thread.
  */
-@ThreadSafe
 public interface ReadOnlyStringMetadataCache {
 
     /**

File: storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/RocksDbMetricsWriter.java
Patch:
@@ -20,7 +20,6 @@
 import java.util.TreeMap;
 import java.util.concurrent.BlockingQueue;
 import java.util.concurrent.ThreadLocalRandom;
-import org.apache.http.annotation.NotThreadSafe;
 import org.apache.storm.metricstore.AggLevel;
 import org.apache.storm.metricstore.Metric;
 import org.apache.storm.metricstore.MetricException;
@@ -34,7 +33,7 @@
 /**
  * Class designed to perform all metrics inserts into RocksDB.  Metrics are processed from a blocking queue.  Inserts
  * to RocksDB are done using a single thread to simplify design (such as looking up existing metric data for aggregation,
- * and fetching/evicting metadata from the cache).
+ * and fetching/evicting metadata from the cache).  This class is not thread safe.
  * </P>
  * A writable LRU StringMetadataCache is used to minimize looking up metadata string Ids.  As entries are added to the full cache, older
  * entries are evicted from the cache and need to be written to the database.  This happens as the handleEvictedMetadata()
@@ -51,7 +50,6 @@
  *     <li>Investigate performance of multiple threads inserting into RocksDB versus a single ordered insert.</li>
  * </ul>
  */
-@NotThreadSafe
 public class RocksDbMetricsWriter implements Runnable, AutoCloseable {
     private static final Logger LOG = LoggerFactory.getLogger(RocksDbMetricsWriter.class);
     private RocksDbStore store;

File: storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/WritableStringMetadataCache.java
Patch:
@@ -13,13 +13,12 @@
 
 import java.util.Map;
 import java.util.Set;
-import org.apache.http.annotation.NotThreadSafe;
 import org.apache.storm.metricstore.MetricException;
 
 /**
  * The writable interface to a StringMetadataCache intended to be used by a single RocksDBMetricwWriter instance.
+ * This class is not thread safe.
  */
-@NotThreadSafe
 public interface WritableStringMetadataCache extends ReadOnlyStringMetadataCache {
 
     /**

File: storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/ReadOnlyStringMetadataCache.java
Patch:
@@ -11,12 +11,9 @@
 
 package org.apache.storm.metricstore.rocksdb;
 
-import org.apache.http.annotation.ThreadSafe;
-
 /**
  * The read-only interface to a StringMetadataCache allowed to be used by any thread.
  */
-@ThreadSafe
 public interface ReadOnlyStringMetadataCache {
 
     /**

File: storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/RocksDbMetricsWriter.java
Patch:
@@ -20,7 +20,6 @@
 import java.util.TreeMap;
 import java.util.concurrent.BlockingQueue;
 import java.util.concurrent.ThreadLocalRandom;
-import org.apache.http.annotation.NotThreadSafe;
 import org.apache.storm.metricstore.AggLevel;
 import org.apache.storm.metricstore.Metric;
 import org.apache.storm.metricstore.MetricException;
@@ -34,7 +33,7 @@
 /**
  * Class designed to perform all metrics inserts into RocksDB.  Metrics are processed from a blocking queue.  Inserts
  * to RocksDB are done using a single thread to simplify design (such as looking up existing metric data for aggregation,
- * and fetching/evicting metadata from the cache).
+ * and fetching/evicting metadata from the cache).  This class is not thread safe.
  * </P>
  * A writable LRU StringMetadataCache is used to minimize looking up metadata string Ids.  As entries are added to the full cache, older
  * entries are evicted from the cache and need to be written to the database.  This happens as the handleEvictedMetadata()
@@ -51,7 +50,6 @@
  *     <li>Investigate performance of multiple threads inserting into RocksDB versus a single ordered insert.</li>
  * </ul>
  */
-@NotThreadSafe
 public class RocksDbMetricsWriter implements Runnable, AutoCloseable {
     private static final Logger LOG = LoggerFactory.getLogger(RocksDbMetricsWriter.class);
     private RocksDbStore store;

File: storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/WritableStringMetadataCache.java
Patch:
@@ -13,13 +13,12 @@
 
 import java.util.Map;
 import java.util.Set;
-import org.apache.http.annotation.NotThreadSafe;
 import org.apache.storm.metricstore.MetricException;
 
 /**
  * The writable interface to a StringMetadataCache intended to be used by a single RocksDBMetricwWriter instance.
+ * This class is not thread safe.
  */
-@NotThreadSafe
 public interface WritableStringMetadataCache extends ReadOnlyStringMetadataCache {
 
     /**

File: storm-server/src/main/java/org/apache/storm/blobstore/BlobStoreUtils.java
Patch:
@@ -12,6 +12,7 @@
 
 package org.apache.storm.blobstore;
 
+import java.io.FileNotFoundException;
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.HashSet;
@@ -191,6 +192,8 @@ public static boolean downloadUpdatedBlob(Map<String, Object> conf, BlobStore bl
                     out.close();
                 }
                 isSuccess = true;
+            } catch(FileNotFoundException fnf) {
+                LOG.warn("Blobstore file for key '{}' does not exist or got deleted before it could be downloaded.", key, fnf);
             } catch (IOException | AuthorizationException exception) {
                 throw new RuntimeException(exception);
             } catch (KeyNotFoundException knf) {

File: storm-server/src/main/java/org/apache/storm/blobstore/LocalFsBlobStore.java
Patch:
@@ -19,6 +19,7 @@
 import java.io.FileNotFoundException;
 import java.io.IOException;
 import java.io.InputStream;
+import java.nio.file.NoSuchFileException;
 import java.util.HashSet;
 import java.util.Iterator;
 import java.util.List;
@@ -197,7 +198,7 @@ public void run() {
                     throw new RuntimeException(e);
                 }
             }
-        }, 0, ObjectReader.getInt(conf.get(DaemonConfig.NIMBUS_CODE_SYNC_FREQ_SECS)));
+        }, 0, ObjectReader.getInt(conf.get(DaemonConfig.NIMBUS_CODE_SYNC_FREQ_SECS))*1000);
 
     }
 
@@ -374,7 +375,7 @@ private void deleteKeyIgnoringFileNotFound(String key) throws IOException {
         try {
             fbs.deleteKey(key);
         } catch (IOException e) {
-            if (e instanceof FileNotFoundException) {
+            if (e instanceof FileNotFoundException || e instanceof NoSuchFileException) {
                 LOG.debug("Ignoring FileNotFoundException since we're about to delete such key... key: {}", key);
             } else {
                 throw e;

File: storm-server/src/main/java/org/apache/storm/blobstore/BlobStoreUtils.java
Patch:
@@ -12,6 +12,7 @@
 
 package org.apache.storm.blobstore;
 
+import java.io.FileNotFoundException;
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.HashSet;
@@ -191,6 +192,8 @@ public static boolean downloadUpdatedBlob(Map<String, Object> conf, BlobStore bl
                     out.close();
                 }
                 isSuccess = true;
+            } catch(FileNotFoundException fnf) {
+                LOG.warn("FileNotFoundException", fnf);
             } catch (IOException | AuthorizationException exception) {
                 throw new RuntimeException(exception);
             } catch (KeyNotFoundException knf) {

File: storm-server/src/main/java/org/apache/storm/blobstore/LocalFsBlobStore.java
Patch:
@@ -19,6 +19,7 @@
 import java.io.FileNotFoundException;
 import java.io.IOException;
 import java.io.InputStream;
+import java.nio.file.NoSuchFileException;
 import java.util.HashSet;
 import java.util.Iterator;
 import java.util.List;
@@ -197,7 +198,7 @@ public void run() {
                     throw new RuntimeException(e);
                 }
             }
-        }, 0, ObjectReader.getInt(conf.get(DaemonConfig.NIMBUS_CODE_SYNC_FREQ_SECS)));
+        }, 0, ObjectReader.getInt(conf.get(DaemonConfig.NIMBUS_CODE_SYNC_FREQ_SECS))*1000);
 
     }
 
@@ -374,7 +375,7 @@ private void deleteKeyIgnoringFileNotFound(String key) throws IOException {
         try {
             fbs.deleteKey(key);
         } catch (IOException e) {
-            if (e instanceof FileNotFoundException) {
+            if (e instanceof FileNotFoundException || e instanceof NoSuchFileException) {
                 LOG.debug("Ignoring FileNotFoundException since we're about to delete such key... key: {}", key);
             } else {
                 throw e;

File: storm-webapp/src/main/java/org/apache/storm/daemon/ui/UIHelpers.java
Patch:
@@ -1226,7 +1226,7 @@ private static Map<String, Object> getBoltAggStatsMap(
      * @return nullToZero
      */
     private static Long nullToZero(Long value) {
-        return Objects.isNull(value) ? value : 0;
+        return !Objects.isNull(value) ? value : 0;
     }
 
     /**
@@ -1235,7 +1235,7 @@ private static Long nullToZero(Long value) {
      * @return nullToZero
      */
     private static Double nullToZero(Double value) {
-        return Objects.isNull(value) ? value : 0;
+        return !Objects.isNull(value) ? value : 0;
     }
 
     /**

File: storm-webapp/src/main/java/org/apache/storm/daemon/ui/UIHelpers.java
Patch:
@@ -1226,7 +1226,7 @@ private static Map<String, Object> getBoltAggStatsMap(
      * @return nullToZero
      */
     private static Long nullToZero(Long value) {
-        return Objects.isNull(value) ? value : 0;
+        return !Objects.isNull(value) ? value : 0;
     }
 
     /**
@@ -1235,7 +1235,7 @@ private static Long nullToZero(Long value) {
      * @return nullToZero
      */
     private static Double nullToZero(Double value) {
-        return Objects.isNull(value) ? value : 0;
+        return !Objects.isNull(value) ? value : 0;
     }
 
     /**

File: storm-server/src/main/java/org/apache/storm/localizer/PortAndAssignmentImpl.java
Patch:
@@ -27,7 +27,7 @@ public PortAndAssignmentImpl(int port, LocalAssignment assignment) {
     }
 
     /**
-     * All implementations of PortAndAssignment should implement the same equals() method
+     * All implementations of PortAndAssignment should implement the same equals() method.
      */
     @Override
     public boolean equals(Object other) {
@@ -49,7 +49,7 @@ public String getOwner() {
     }
 
     /**
-     * All implementations of PortAndAssignment should implement the same hashCode() method
+     * All implementations of PortAndAssignment should implement the same hashCode() method.
      */
     @Override
     public int hashCode() {

File: storm-server/src/main/java/org/apache/storm/localizer/TimePortAndAssignment.java
Patch:
@@ -55,15 +55,15 @@ public String toString() {
     }
 
     /**
-     * All implementations of PortAndAssignment should implement the same hashCode() method
+     * All implementations of PortAndAssignment should implement the same hashCode() method.
      */
     @Override
     public int hashCode() {
         return (17 * getPort()) + getAssignment().hashCode();
     }
 
     /**
-     * All implementations of PortAndAssignment should implement the same equals() method
+     * All implementations of PortAndAssignment should implement the same equals() method.
      */
     @Override
     public boolean equals(Object other) {

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java
Patch:
@@ -30,6 +30,7 @@
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.Iterator;
+import java.util.LinkedList;
 import java.util.List;
 import java.util.Map;
 import java.util.Map.Entry;
@@ -350,7 +351,7 @@ private boolean isWaitingToEmit() {
 
     private void setWaitingToEmit(ConsumerRecords<K, V> consumerRecords) {
         for (TopicPartition tp : consumerRecords.partitions()) {
-            waitingToEmit.put(tp, new ArrayList<>(consumerRecords.records(tp)));
+            waitingToEmit.put(tp, new LinkedList<>(consumerRecords.records(tp)));
         }
     }
 
@@ -536,7 +537,7 @@ private void commitOffsetsForAckedTuples() {
                         //Discard the pending records that are already committed
                         waitingToEmit.put(tp, waitingToEmitForTp.stream()
                             .filter(record -> record.offset() >= committedOffset)
-                            .collect(Collectors.toList()));
+                            .collect(Collectors.toCollection(LinkedList::new)));
                     }
                 }
 

File: storm-client/src/jvm/org/apache/storm/StormSubmitter.java
Patch:
@@ -308,10 +308,10 @@ private static void submitTopologyInDistributeMode(String name, StormTopology to
             }
             LOG.info("Finished submitting topology: {}", name);
         } catch (InvalidTopologyException e) {
-            LOG.warn("Topology submission exception: {}", e.get_msg());
+            LOG.error("Topology submission exception: {}", e.get_msg());
             throw e;
         } catch (AlreadyAliveException e) {
-            LOG.warn("Topology already alive exception", e);
+            LOG.error("Topology already alive exception", e);
             throw e;
         }
     }
@@ -512,7 +512,7 @@ public static String submitJar(Map<String, Object> conf, String localJar, Progre
 
     private static void validateConfs(Map<String, Object> topoConf, StormTopology topology) throws IllegalArgumentException,
         InvalidTopologyException, AuthorizationException {
-        ConfigValidation.validateFields(topoConf);
+        ConfigValidation.validateTopoConf(topoConf);
         Utils.validateTopologyBlobStoreMap(topoConf);
     }
 

File: storm-client/src/jvm/org/apache/storm/tuple/TupleImpl.java
Patch:
@@ -46,7 +46,7 @@ public TupleImpl(Tuple t) {
     }
 
     public TupleImpl(GeneralTopologyContext context, List<Object> values, String srcComponent, int taskId, String streamId, MessageId id) {
-        this.values = Collections.unmodifiableList(values);
+        this.values = context.doSanityCheck() ? Collections.unmodifiableList(values) : values;
         this.taskId = taskId;
         this.streamId = streamId;
         this.id = id;

File: storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java
Patch:
@@ -2968,7 +2968,7 @@ public void submitTopologyWithOpts(String topoName, String uploadedJarLocation,
             @SuppressWarnings("unchecked")
             Map<String, Object> topoConf = (Map<String, Object>) JSONValue.parse(jsonConf);
             try {
-                ConfigValidation.validateFields(topoConf);
+                ConfigValidation.validateTopoConf(topoConf);
             } catch (IllegalArgumentException ex) {
                 throw new WrappedInvalidTopologyException(ex.getMessage());
             }

File: storm-server/src/main/java/org/apache/storm/daemon/supervisor/Supervisor.java
Patch:
@@ -135,7 +135,7 @@ public Supervisor(Map<String, Object> conf, IContext sharedContext, ISupervisor
             (String) conf.get(DaemonConfig.SUPERVISOR_AUTHORIZER), conf);
         if (authorizationHandler == null && conf.get(DaemonConfig.NIMBUS_AUTHORIZER) != null) {
             throw new IllegalStateException("It looks like authorization is turned on for nimbus but not for the "
-                                            + "supervisor....");
+                + "supervisor. ( " + DaemonConfig.SUPERVISOR_AUTHORIZER + " is not set)");
         }
 
         iSupervisor.prepare(conf, ServerConfigUtils.supervisorIsupervisorDir(conf));

File: storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/BaseResourceAwareStrategy.java
Patch:
@@ -332,7 +332,7 @@ public LazyNodeSorting(TopologyDetails td, ExecutorDetails exec,
 
         private TreeSet<ObjectResources> getSortedNodesFor(String rackId) {
             return cachedNodes.computeIfAbsent(rackId,
-                (rid) -> sortNodes(rackIdToNodes.get(rid), exec, td, rid, perNodeScheduledCount));
+                (rid) -> sortNodes(rackIdToNodes.getOrDefault(rid, Collections.emptyList()), exec, td, rid, perNodeScheduledCount));
         }
 
         @Override

File: storm-webapp/src/main/java/org/apache/storm/daemon/drpc/DRPCServer.java
Patch:
@@ -102,10 +102,11 @@ private static Server mkHttpServer(Map<String, Object> conf, DRPC drpc) {
             final String httpsTsType = (String) (conf.get(DaemonConfig.DRPC_HTTPS_TRUSTSTORE_TYPE));
             final Boolean httpsWantClientAuth = (Boolean) (conf.get(DaemonConfig.DRPC_HTTPS_WANT_CLIENT_AUTH));
             final Boolean httpsNeedClientAuth = (Boolean) (conf.get(DaemonConfig.DRPC_HTTPS_NEED_CLIENT_AUTH));
+            final Boolean disableHttpBinding = (Boolean) (conf.get(DaemonConfig.DRPC_DISABLE_HTTP_BINDING));
 
             //TODO a better way to do this would be great.
             DRPCApplication.setup(drpc);
-            ret = UIHelpers.jettyCreateServer(drpcHttpPort, null, httpsPort);
+            ret = UIHelpers.jettyCreateServer(drpcHttpPort, null, httpsPort, disableHttpBinding);
             
             UIHelpers.configSsl(ret, httpsPort, httpsKsPath, httpsKsPassword, httpsKsType, httpsKeyPassword,
                     httpsTsPath, httpsTsPassword, httpsTsType, httpsNeedClientAuth, httpsWantClientAuth);

File: storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/LogviewerServer.java
Patch:
@@ -83,9 +83,10 @@ private static Server mkHttpServer(Map<String, Object> conf) {
             final String httpsTsType = (String) (conf.get(DaemonConfig.LOGVIEWER_HTTPS_TRUSTSTORE_TYPE));
             final Boolean httpsWantClientAuth = (Boolean) (conf.get(DaemonConfig.LOGVIEWER_HTTPS_WANT_CLIENT_AUTH));
             final Boolean httpsNeedClientAuth = (Boolean) (conf.get(DaemonConfig.LOGVIEWER_HTTPS_NEED_CLIENT_AUTH));
+            final Boolean disableHttpBinding = (Boolean) (conf.get(DaemonConfig.LOGVIEWER_DISABLE_HTTP_BINDING));
 
             LogviewerApplication.setup(conf);
-            ret = UIHelpers.jettyCreateServer(logviewerHttpPort, null, httpsPort);
+            ret = UIHelpers.jettyCreateServer(logviewerHttpPort, null, httpsPort, disableHttpBinding);
 
             UIHelpers.configSsl(ret, httpsPort, httpsKsPath, httpsKsPassword, httpsKsType, httpsKeyPassword,
                     httpsTsPath, httpsTsPassword, httpsTsType, httpsNeedClientAuth, httpsWantClientAuth);

File: storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/utils/LogFileDownloader.java
Patch:
@@ -30,7 +30,7 @@
 
 
 public class LogFileDownloader {
-    private static final Histogram fileDownloadSizeDistMB= StormMetricsRegistry.registerHistogram("logviewer:download-file-size-rounded-MB");
+    private static final Histogram fileDownloadSizeDistMB = StormMetricsRegistry.registerHistogram("logviewer:download-file-size-rounded-MB");
 
     private final String logRoot;
     private final String daemonLogRoot;

File: storm-webapp/src/main/java/org/apache/storm/daemon/ui/UIServer.java
Patch:
@@ -93,11 +93,11 @@ public static void main(String[] args) {
         final String httpsTsType = (String) (conf.get(DaemonConfig.UI_HTTPS_TRUSTSTORE_TYPE));
         final Boolean httpsWantClientAuth = (Boolean) (conf.get(DaemonConfig.UI_HTTPS_WANT_CLIENT_AUTH));
         final Boolean httpsNeedClientAuth = (Boolean) (conf.get(DaemonConfig.UI_HTTPS_NEED_CLIENT_AUTH));
+        final Boolean disableHttpBinding = (Boolean) (conf.get(DaemonConfig.UI_DISABLE_HTTP_BINDING));
 
         Server jettyServer =
                 UIHelpers.jettyCreateServer(
-                        (int) conf.get(DaemonConfig.UI_PORT), null, httpsPort, headerBufferSize
-                );
+                        (int) conf.get(DaemonConfig.UI_PORT), null, httpsPort, headerBufferSize, disableHttpBinding);
 
         UIHelpers.configSsl(jettyServer, httpsPort, httpsKsPath, httpsKsPassword, httpsKsType, httpsKeyPassword,
                 httpsTsPath, httpsTsPassword, httpsTsType, httpsNeedClientAuth, httpsWantClientAuth);

File: storm-webapp/src/main/java/org/apache/storm/daemon/ui/exceptionmappers/DefaultExceptionMapper.java
Patch:
@@ -34,7 +34,7 @@ public class DefaultExceptionMapper implements ExceptionMapper<Throwable> {
 
     /**
      * toResponse.
-     * @param throwable
+     * @param throwable throwable
      * @return response
      */
     @Override

File: storm-webapp/src/main/java/org/apache/storm/daemon/ui/filters/AuthorizedUserFilter.java
Patch:
@@ -27,6 +27,7 @@
 import javax.ws.rs.container.ContainerRequestFilter;
 import javax.ws.rs.container.ResourceInfo;
 import javax.ws.rs.core.Context;
+import javax.ws.rs.core.MediaType;
 import javax.ws.rs.core.Response;
 import javax.ws.rs.ext.Provider;
 import org.apache.commons.codec.Charsets;
@@ -79,7 +80,7 @@ public class AuthorizedUserFilter implements ContainerRequestFilter {
     public static Response makeResponse(Exception ex, ContainerRequestContext request, int statusCode) {
         String callback = null;
 
-        if (request.getMediaType().toString().contains("application/json")) {
+        if (request.getMediaType() != null && request.getMediaType().equals(MediaType.APPLICATION_JSON_TYPE)) {
             try {
                 String json = IOUtils.toString(request.getEntityStream(), Charsets.UTF_8);
                 InputStream in = IOUtils.toInputStream(json);

File: storm-server/src/main/java/org/apache/storm/daemon/supervisor/Supervisor.java
Patch:
@@ -135,7 +135,7 @@ public Supervisor(Map<String, Object> conf, IContext sharedContext, ISupervisor
             (String) conf.get(DaemonConfig.SUPERVISOR_AUTHORIZER), conf);
         if (authorizationHandler == null && conf.get(DaemonConfig.NIMBUS_AUTHORIZER) != null) {
             throw new IllegalStateException("It looks like authorization is turned on for nimbus but not for the "
-                                            + "supervisor....");
+                + "supervisor.... ( " + DaemonConfig.SUPERVISOR_AUTHORIZER + " is not set)");
         }
 
         iSupervisor.prepare(conf, ServerConfigUtils.supervisorIsupervisorDir(conf));

File: storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/BaseResourceAwareStrategy.java
Patch:
@@ -332,7 +332,7 @@ public LazyNodeSorting(TopologyDetails td, ExecutorDetails exec,
 
         private TreeSet<ObjectResources> getSortedNodesFor(String rackId) {
             return cachedNodes.computeIfAbsent(rackId,
-                (rid) -> sortNodes(rackIdToNodes.get(rid), exec, td, rid, perNodeScheduledCount));
+                (rid) -> sortNodes(rackIdToNodes.getOrDefault(rid, Collections.emptyList()), exec, td, rid, perNodeScheduledCount));
         }
 
         @Override

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java
Patch:
@@ -30,6 +30,7 @@
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.Iterator;
+import java.util.LinkedList;
 import java.util.List;
 import java.util.Map;
 import java.util.Map.Entry;
@@ -350,7 +351,7 @@ private boolean isWaitingToEmit() {
 
     private void setWaitingToEmit(ConsumerRecords<K, V> consumerRecords) {
         for (TopicPartition tp : consumerRecords.partitions()) {
-            waitingToEmit.put(tp, new ArrayList<>(consumerRecords.records(tp)));
+            waitingToEmit.put(tp, new LinkedList<>(consumerRecords.records(tp)));
         }
     }
 
@@ -536,7 +537,7 @@ private void commitOffsetsForAckedTuples() {
                         //Discard the pending records that are already committed
                         waitingToEmit.put(tp, waitingToEmitForTp.stream()
                             .filter(record -> record.offset() >= committedOffset)
-                            .collect(Collectors.toList()));
+                            .collect(Collectors.toCollection(LinkedList::new)));
                     }
                 }
 

File: storm-server/src/main/java/org/apache/storm/daemon/supervisor/Supervisor.java
Patch:
@@ -135,7 +135,7 @@ public Supervisor(Map<String, Object> conf, IContext sharedContext, ISupervisor
             (String) conf.get(DaemonConfig.SUPERVISOR_AUTHORIZER), conf);
         if (authorizationHandler == null && conf.get(DaemonConfig.NIMBUS_AUTHORIZER) != null) {
             throw new IllegalStateException("It looks like authorization is turned on for nimbus but not for the "
-                                            + "supervisor....");
+                + "supervisor.... ( " + DaemonConfig.SUPERVISOR_AUTHORIZER + " is not set)");
         }
 
         iSupervisor.prepare(conf, ServerConfigUtils.supervisorIsupervisorDir(conf));

File: storm-webapp/src/main/java/org/apache/storm/daemon/ui/filters/AuthorizedUserFilter.java
Patch:
@@ -27,6 +27,7 @@
 import javax.ws.rs.container.ContainerRequestFilter;
 import javax.ws.rs.container.ResourceInfo;
 import javax.ws.rs.core.Context;
+import javax.ws.rs.core.MediaType;
 import javax.ws.rs.core.Response;
 import javax.ws.rs.ext.Provider;
 import org.apache.commons.codec.Charsets;
@@ -79,7 +80,7 @@ public class AuthorizedUserFilter implements ContainerRequestFilter {
     public static Response makeResponse(Exception ex, ContainerRequestContext request, int statusCode) {
         String callback = null;
 
-        if (request.getMediaType().toString().contains("application/json")) {
+        if (request.getMediaType() != null && request.getMediaType().equals(MediaType.APPLICATION_JSON_TYPE)) {
             try {
                 String json = IOUtils.toString(request.getEntityStream(), Charsets.UTF_8);
                 InputStream in = IOUtils.toInputStream(json);

File: storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/utils/LogFileDownloader.java
Patch:
@@ -30,7 +30,7 @@
 
 
 public class LogFileDownloader {
-    private static final Histogram fileDownloadSizeDistMB= StormMetricsRegistry.registerHistogram("logviewer:download-file-size-rounded-MB");
+    private static final Histogram fileDownloadSizeDistMB = StormMetricsRegistry.registerHistogram("logviewer:download-file-size-rounded-MB");
 
     private final String logRoot;
     private final String daemonLogRoot;

File: storm-webapp/src/main/java/org/apache/storm/daemon/ui/exceptionmappers/DefaultExceptionMapper.java
Patch:
@@ -34,7 +34,7 @@ public class DefaultExceptionMapper implements ExceptionMapper<Throwable> {
 
     /**
      * toResponse.
-     * @param throwable
+     * @param throwable throwable
      * @return response
      */
     @Override

File: storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/BaseResourceAwareStrategy.java
Patch:
@@ -332,7 +332,7 @@ public LazyNodeSorting(TopologyDetails td, ExecutorDetails exec,
 
         private TreeSet<ObjectResources> getSortedNodesFor(String rackId) {
             return cachedNodes.computeIfAbsent(rackId,
-                (rid) -> sortNodes(rackIdToNodes.get(rid), exec, td, rid, perNodeScheduledCount));
+                (rid) -> sortNodes(rackIdToNodes.getOrDefault(rid, Collections.emptyList()), exec, td, rid, perNodeScheduledCount));
         }
 
         @Override

File: storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/utils/LogFileDownloader.java
Patch:
@@ -30,7 +30,7 @@
 
 
 public class LogFileDownloader {
-    private static final Histogram fileDownloadSizeDistMB= StormMetricsRegistry.registerHistogram("logviewer:download-file-size-rounded-MB");
+    private static final Histogram fileDownloadSizeDistMB = StormMetricsRegistry.registerHistogram("logviewer:download-file-size-rounded-MB");
 
     private final String logRoot;
     private final String daemonLogRoot;

File: storm-webapp/src/main/java/org/apache/storm/daemon/ui/exceptionmappers/DefaultExceptionMapper.java
Patch:
@@ -34,7 +34,7 @@ public class DefaultExceptionMapper implements ExceptionMapper<Throwable> {
 
     /**
      * toResponse.
-     * @param throwable
+     * @param throwable throwable
      * @return response
      */
     @Override

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java
Patch:
@@ -30,6 +30,7 @@
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.Iterator;
+import java.util.LinkedList;
 import java.util.List;
 import java.util.Map;
 import java.util.Map.Entry;
@@ -350,7 +351,7 @@ private boolean isWaitingToEmit() {
 
     private void setWaitingToEmit(ConsumerRecords<K, V> consumerRecords) {
         for (TopicPartition tp : consumerRecords.partitions()) {
-            waitingToEmit.put(tp, new ArrayList<>(consumerRecords.records(tp)));
+            waitingToEmit.put(tp, new LinkedList<>(consumerRecords.records(tp)));
         }
     }
 
@@ -536,7 +537,7 @@ private void commitOffsetsForAckedTuples() {
                         //Discard the pending records that are already committed
                         waitingToEmit.put(tp, waitingToEmitForTp.stream()
                             .filter(record -> record.offset() >= committedOffset)
-                            .collect(Collectors.toList()));
+                            .collect(Collectors.toCollection(LinkedList::new)));
                     }
                 }
 

File: storm-client/src/jvm/org/apache/storm/utils/NimbusClient.java
Patch:
@@ -109,7 +109,7 @@ public static boolean isLocalOverride() {
     }
 
     /**
-     * Execute cb with a configured nimbus client that will be closed one cb returns.
+     * Execute cb with a configured nimbus client that will be closed once cb returns.
      * @param cb the callback to send to nimbus.
      * @throws Exception on any kind of error.
      */
@@ -118,7 +118,7 @@ public static void withConfiguredClient(WithNimbus cb) throws Exception {
     }
 
     /**
-     * Execute cb with a configured nimbus client that will be closed one cb returns.
+     * Execute cb with a configured nimbus client that will be closed once cb returns.
      * @param cb the callback to send to nimbus.
      * @param conf the conf to use instead of reading the global storm conf.
      * @throws Exception on any kind of error.

File: storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java
Patch:
@@ -2772,8 +2772,9 @@ private CommonTopoInfo getCommonTopoInfo(String topoId, String operation) throws
         ret.topoConf = tryReadTopoConf(topoId, topoCache);
         ret.topoName = (String) ret.topoConf.get(Config.TOPOLOGY_NAME);
         checkAuthorization(ret.topoName, ret.topoConf, operation);
-        ret.topology = tryReadTopology(topoId, topoCache);
-        ret.taskToComponent = StormCommon.stormTaskInfo(ret.topology, ret.topoConf);
+        StormTopology topology = tryReadTopology(topoId, topoCache);
+        ret.topology = StormCommon.systemTopology(ret.topoConf, topology);
+        ret.taskToComponent = StormCommon.stormTaskInfo(topology, ret.topoConf);
         ret.base = state.stormBase(topoId, null);
         if (ret.base != null && ret.base.is_set_launch_time_secs()) {
             ret.launchTimeSecs = ret.base.get_launch_time_secs();

File: storm-webapp/src/main/java/org/apache/storm/daemon/ui/filters/AuthorizedUserFilter.java
Patch:
@@ -169,7 +169,7 @@ public void filter(ContainerRequestContext containerRequestContext) {
                 containerRequestContext.abortWith(
                         makeResponse(new AuthorizationException("UI request '" + op + "' for '"
                                         + user + "' user is not authorized"),
-                                containerRequestContext, 401)
+                                containerRequestContext, 403)
                 );
             }
         }

File: storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java
Patch:
@@ -2772,8 +2772,9 @@ private CommonTopoInfo getCommonTopoInfo(String topoId, String operation) throws
         ret.topoConf = tryReadTopoConf(topoId, topoCache);
         ret.topoName = (String) ret.topoConf.get(Config.TOPOLOGY_NAME);
         checkAuthorization(ret.topoName, ret.topoConf, operation);
-        ret.topology = tryReadTopology(topoId, topoCache);
-        ret.taskToComponent = StormCommon.stormTaskInfo(ret.topology, ret.topoConf);
+        StormTopology topology = tryReadTopology(topoId, topoCache);
+        ret.topology = StormCommon.systemTopology(ret.topoConf, topology);
+        ret.taskToComponent = StormCommon.stormTaskInfo(topology, ret.topoConf);
         ret.base = state.stormBase(topoId, null);
         if (ret.base != null && ret.base.is_set_launch_time_secs()) {
             ret.launchTimeSecs = ret.base.get_launch_time_secs();

File: storm-webapp/src/main/java/org/apache/storm/daemon/ui/filters/AuthorizedUserFilter.java
Patch:
@@ -160,7 +160,7 @@ public void filter(ContainerRequestContext containerRequestContext) {
         }
 
         if (uiAclHandler != null) {
-            if (!uiAclHandler.permit(reqContext, op, conf)) {
+            if (!uiAclHandler.permit(reqContext, op, topoConf)) {
                 Principal principal = reqContext.principal();
                 String user = "unknown";
                 if (principal != null) {

File: storm-webapp/src/main/java/org/apache/storm/daemon/ui/filters/AuthorizedUserFilter.java
Patch:
@@ -169,7 +169,7 @@ public void filter(ContainerRequestContext containerRequestContext) {
                 containerRequestContext.abortWith(
                         makeResponse(new AuthorizationException("UI request '" + op + "' for '"
                                         + user + "' user is not authorized"),
-                                containerRequestContext, 401)
+                                containerRequestContext, 403)
                 );
             }
         }

File: storm-webapp/src/main/java/org/apache/storm/daemon/ui/filters/AuthorizedUserFilter.java
Patch:
@@ -160,7 +160,7 @@ public void filter(ContainerRequestContext containerRequestContext) {
         }
 
         if (uiAclHandler != null) {
-            if (!uiAclHandler.permit(reqContext, op, conf)) {
+            if (!uiAclHandler.permit(reqContext, op, topoConf)) {
                 Principal principal = reqContext.principal();
                 String user = "unknown";
                 if (principal != null) {

File: storm-client/src/jvm/org/apache/storm/security/auth/kerberos/KerberosSaslTransportPlugin.java
Patch:
@@ -90,7 +90,7 @@ public TTransportFactory getServerTransportFactory(boolean impersonationAllowed)
         factory.addServerDefinition(KERBEROS, serviceName, hostName, props, server_callback_handler);
 
         //Also add in support for worker tokens
-        factory.addServerDefinition(DIGEST, ClientAuthUtils.SERVICE, "localhost", null,
+        factory.addServerDefinition(DIGEST, ClientAuthUtils.SERVICE, hostName, null,
                                     new SimpleSaslServerCallbackHandler(impersonationAllowed, new WorkerTokenAuthorizer(conf, type)));
 
         //create a wrap transport factory so that we could apply user credential during connections
@@ -119,7 +119,7 @@ private Login mkLogin() throws IOException {
     @Override
     public TTransport connect(TTransport transport, String serverHost, String asUser) throws IOException, TTransportException {
         WorkerToken token = WorkerTokenClientCallbackHandler.findWorkerTokenInSubject(type);
-        if (token != null && asUser != null) {
+        if (token != null) {
             CallbackHandler clientCallbackHandler = new WorkerTokenClientCallbackHandler(token);
             TSaslClientTransport wrapperTransport = new TSaslClientTransport(DIGEST,
                                                                              null,

File: storm-client/src/jvm/org/apache/storm/security/auth/kerberos/KerberosSaslTransportPlugin.java
Patch:
@@ -90,7 +90,7 @@ public TTransportFactory getServerTransportFactory(boolean impersonationAllowed)
         factory.addServerDefinition(KERBEROS, serviceName, hostName, props, server_callback_handler);
 
         //Also add in support for worker tokens
-        factory.addServerDefinition(DIGEST, ClientAuthUtils.SERVICE, "localhost", null,
+        factory.addServerDefinition(DIGEST, ClientAuthUtils.SERVICE, hostName, null,
                                     new SimpleSaslServerCallbackHandler(impersonationAllowed, new WorkerTokenAuthorizer(conf, type)));
 
         //create a wrap transport factory so that we could apply user credential during connections
@@ -119,7 +119,7 @@ private Login mkLogin() throws IOException {
     @Override
     public TTransport connect(TTransport transport, String serverHost, String asUser) throws IOException, TTransportException {
         WorkerToken token = WorkerTokenClientCallbackHandler.findWorkerTokenInSubject(type);
-        if (token != null && asUser != null) {
+        if (token != null) {
             CallbackHandler clientCallbackHandler = new WorkerTokenClientCallbackHandler(token);
             TSaslClientTransport wrapperTransport = new TSaslClientTransport(DIGEST,
                                                                              null,

File: storm-client/test/jvm/org/apache/storm/PaceMakerStateStorageFactoryTest.java
Patch:
@@ -65,19 +65,19 @@ public void createPaceMakerStateStorage(HBServerMessageType messageType, HBMessa
     @Test
     public void testSetWorkerHb() throws Exception {
         createPaceMakerStateStorage(HBServerMessageType.SEND_PULSE_RESPONSE, null);
-        stateStorage.set_worker_hb("/foo", Utils.javaSerialize("data"), null);
+        stateStorage.set_worker_hb("/foo", "data".getBytes("UTF-8"), null);
         verify(clientMock).send(hbMessageCaptor.capture());
         HBMessage sent = hbMessageCaptor.getValue();
         HBPulse pulse = sent.get_data().get_pulse();
         Assert.assertEquals(HBServerMessageType.SEND_PULSE, sent.get_type());
         Assert.assertEquals("/foo", pulse.get_id());
-        Assert.assertEquals("data", Utils.javaDeserialize(pulse.get_details(), String.class));
+        Assert.assertEquals("data", new String(pulse.get_details(), "UTF-8"));
     }
 
     @Test(expected = RuntimeException.class)
     public void testSetWorkerHbResponseType() throws Exception {
         createPaceMakerStateStorage(HBServerMessageType.SEND_PULSE, null);
-        stateStorage.set_worker_hb("/foo", Utils.javaSerialize("data"), null);
+        stateStorage.set_worker_hb("/foo", "data".getBytes("UTF-8"), null);
     }
 
     @Test

File: external/storm-hdfs-blobstore/src/main/java/org/apache/storm/hdfs/blobstore/HdfsBlobStore.java
Patch:
@@ -162,7 +162,7 @@ protected void prepareInternal(Map<String, Object> conf, String overrideBase, Co
                 localSubject = getHadoopUser();
             }
         } catch (IOException e) {
-            throw new RuntimeException("Error logging in from keytab!", e);
+            throw new RuntimeException("Error logging in from keytab: " + e.getMessage(), e);
         }
         aclHandler = new BlobStoreAclHandler(conf);
         Path baseDir = new Path(overrideBase, BASE_BLOBS_DIR_NAME);

File: external/storm-hdfs-blobstore/src/main/java/org/apache/storm/hdfs/blobstore/HdfsBlobStore.java
Patch:
@@ -162,7 +162,7 @@ protected void prepareInternal(Map<String, Object> conf, String overrideBase, Co
                 localSubject = getHadoopUser();
             }
         } catch (IOException e) {
-            throw new RuntimeException("Error logging in from keytab!", e);
+            throw new RuntimeException("Error logging in from keytab: " + e.getMessage(), e);
         }
         aclHandler = new BlobStoreAclHandler(conf);
         Path baseDir = new Path(overrideBase, BASE_BLOBS_DIR_NAME);

File: storm-server/src/main/java/org/apache/storm/localizer/LocallyCachedBlob.java
Patch:
@@ -52,8 +52,7 @@ public abstract class LocallyCachedBlob {
     private long lastUsed = Time.currentTimeMillis();
     private CompletableFuture<Void> doneUpdating = null;
 
-    private static final Histogram fetchingRate = StormMetricsRegistry.registerHistogram(
-            "supervisor:blob-fetching-rate-MB/s", new ExponentiallyDecayingReservoir());
+    private static final Histogram fetchingRate = StormMetricsRegistry.registerHistogram("supervisor:blob-fetching-rate-MB/s");
 
     /**
      * Create a new LocallyCachedBlob.

File: storm-webapp/src/test/java/org/apache/storm/daemon/logviewer/handler/LogviewerLogSearchHandlerTest.java
Patch:
@@ -627,6 +627,7 @@ public void testFindNMatches() {
 
     public static class TestDeepSearchLogs {
 
+        public static final int METRIC_SCANNED_FILES = 0;
         private List<File> logFiles;
         private String topoPath;
 
@@ -857,7 +858,7 @@ private LogviewerLogSearchHandler getStubbedSearchHandler() {
                 int fileOffset = (Integer) arguments[2];
                 String search = (String) arguments[4];
 
-                return new LogviewerLogSearchHandler.Matched(fileOffset, search, Collections.emptyList());
+                return new LogviewerLogSearchHandler.Matched(fileOffset, search, Collections.emptyList(), METRIC_SCANNED_FILES);
             }).when(handler).findNMatches(any(), anyInt(), anyInt(), anyInt(), any());
 
             return handler;

File: storm-webapp/src/main/java/org/apache/storm/daemon/ui/UIHelpers.java
Patch:
@@ -1148,7 +1148,7 @@ private static List<Map> getTopologyStatsMap(TopologyStats topologyStats) {
             temp.put("window", window);
             temp.put("emitted", emittedStatDisplayMap.get(window));
             temp.put("transferred", transferred.get(window));
-            temp.put("completeLatency", completeLatency.get(window));
+            temp.put("completeLatency", StatsUtil.floatStr(completeLatency.get(getWindowHint(window))));
             temp.put("acked", acked.get(window));
             temp.put("failed", failed.get(window));
 

File: storm-core/src/jvm/org/apache/storm/command/BasicDrpcClient.java
Patch:
@@ -31,7 +31,7 @@ private static void runAndPrint(DRPCClient drpc, String func, String arg) throws
     }
 
     /**
-     * Main entry point for the basic DRPC client
+     * Main entry point for the basic DRPC client.
      * @param args command line arguments to be parsed
      * @throws Exception on errors
      */

File: storm-core/src/jvm/org/apache/storm/command/Rebalance.java
Patch:
@@ -12,6 +12,7 @@
 
 package org.apache.storm.command;
 
+import static java.lang.String.format;
 import java.util.HashMap;
 import java.util.Map;
 import org.apache.storm.generated.Nimbus;
@@ -22,7 +23,6 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import static java.lang.String.format;
 
 public class Rebalance {
 
@@ -42,7 +42,6 @@ public static void main(String[] args) throws Exception {
         Integer numWorkers = (Integer) cl.get("n");
         Map<String, Integer> numExecutors = (Map<String, Integer>) cl.get("e");
         Map<String, Map<String, Double>> resourceOverrides = (Map<String, Map<String, Double>>) cl.get("r");
-        Map<String, Object> confOverrides = (Map<String, Object>) cl.get("t");
 
         if (null != wait) {
             rebalanceOptions.set_wait_secs(wait);
@@ -58,6 +57,8 @@ public static void main(String[] args) throws Exception {
             rebalanceOptions.set_topology_resources_overrides(resourceOverrides);
         }
 
+        Map<String, Object> confOverrides = (Map<String, Object>) cl.get("t");
+
         if (null != confOverrides) {
             rebalanceOptions.set_topology_conf_overrides(JSONValue.toJSONString(confOverrides));
         }

File: storm-core/src/jvm/org/apache/storm/command/SetLogLevel.java
Patch:
@@ -65,10 +65,8 @@ public void run(Nimbus.Iface nimbus) throws Exception {
 
     /**
      * Parses [logger name]=[level string]:[optional timeout],[logger name2]...
-     *
      * e.g. ROOT=DEBUG:30
      *     root logger, debug for 30 seconds
-     *
      *     org.apache.foo=WARN
      *     org.apache.foo set to WARN indefinitely
      */

File: storm-core/src/jvm/org/apache/storm/utils/Monitor.java
Patch:
@@ -1,7 +1,7 @@
 /**
- * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
- * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
+ * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership. The ASF licenses this file to you under the Apache License, Version
+ * 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at
  *
  * http://www.apache.org/licenses/LICENSE-2.0
  *

File: storm-webapp/src/main/java/org/apache/storm/daemon/common/JsonResponseBuilder.java
Patch:
@@ -22,7 +22,7 @@
 import java.util.Map;
 import javax.ws.rs.core.Response;
 
-import org.apache.storm.ui.UIHelpers;
+import org.apache.storm.daemon.ui.UIHelpers;
 
 /**
  * Response builder for JSON. It utilizes {@link UIHelpers} to construct JSON body and headers.

File: storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/LogviewerServer.java
Patch:
@@ -34,9 +34,9 @@
 import org.apache.storm.daemon.logviewer.utils.LogCleaner;
 import org.apache.storm.daemon.logviewer.utils.WorkerLogs;
 import org.apache.storm.daemon.logviewer.webapp.LogviewerApplication;
+import org.apache.storm.daemon.ui.FilterConfiguration;
+import org.apache.storm.daemon.ui.UIHelpers;
 import org.apache.storm.metric.StormMetricsRegistry;
-import org.apache.storm.ui.FilterConfiguration;
-import org.apache.storm.ui.UIHelpers;
 import org.apache.storm.utils.ConfigUtils;
 import org.apache.storm.utils.ObjectReader;
 import org.apache.storm.utils.Utils;

File: storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/handler/LogviewerLogSearchHandler.java
Patch:
@@ -59,9 +59,9 @@
 import org.apache.storm.daemon.logviewer.utils.LogviewerResponseBuilder;
 import org.apache.storm.daemon.logviewer.utils.ResourceAuthorizer;
 import org.apache.storm.daemon.logviewer.utils.WorkerLogs;
+import org.apache.storm.daemon.ui.InvalidRequestException;
 import org.apache.storm.daemon.utils.StreamUtil;
 import org.apache.storm.daemon.utils.UrlBuilder;
-import org.apache.storm.ui.InvalidRequestException;
 import org.apache.storm.utils.ObjectReader;
 import org.apache.storm.utils.ServerUtils;
 import org.apache.storm.utils.Utils;
@@ -117,7 +117,7 @@ public LogviewerLogSearchHandler(Map<String, Object> stormConf, String logRoot,
      * @param search search string
      * @param numMatchesStr the count of maximum matches
      * @param offsetStr start offset for log file
-     * @param callback callback for JSONP
+     * @param callback callbackParameterName for JSONP
      * @param origin origin
      * @return Response containing JSON content representing search result
      */
@@ -173,7 +173,7 @@ public Response searchLogFile(String fileName, String user, boolean isDaemon, St
      * @param fileOffsetStr index (offset) of the log files
      * @param offsetStr start offset for log file
      * @param searchArchived true if the request wants to search also archived files, false if not
-     * @param callback callback for JSONP
+     * @param callback callbackParameterName for JSONP
      * @param origin origin
      * @return Response containing JSON content representing search result
      */

File: storm-webapp/src/main/java/org/apache/storm/daemon/ui/IConfigurator.java
Patch:
@@ -10,7 +10,7 @@
  * and limitations under the License.
  */
 
-package org.apache.storm.ui;
+package org.apache.storm.daemon.ui;
 
 import org.eclipse.jetty.server.Server;
 

File: storm-webapp/src/main/java/org/apache/storm/daemon/ui/InvalidRequestException.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.storm.ui;
+package org.apache.storm.daemon.ui;
 
 public class InvalidRequestException extends Exception {
 

File: storm-webapp/src/test/java/org/apache/storm/daemon/logviewer/handler/LogviewerLogSearchHandlerTest.java
Patch:
@@ -52,7 +52,7 @@
 import org.apache.storm.DaemonConfig;
 import org.apache.storm.daemon.logviewer.LogviewerConstant;
 import org.apache.storm.daemon.logviewer.utils.ResourceAuthorizer;
-import org.apache.storm.ui.InvalidRequestException;
+import org.apache.storm.daemon.ui.InvalidRequestException;
 import org.apache.storm.utils.Utils;
 import org.jooq.lambda.Seq;
 import org.jooq.lambda.Unchecked;

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java
Patch:
@@ -156,7 +156,7 @@ public void open(Map<String, Object> conf, TopologyContext context, SpoutOutputC
 
     private void registerMetric() {
         LOG.info("Registering Spout Metrics");
-        kafkaOffsetMetric = new KafkaOffsetMetric(() -> Collections.unmodifiableMap(offsetManagers), () -> consumer);
+        kafkaOffsetMetric = new KafkaOffsetMetric<>(() -> Collections.unmodifiableMap(offsetManagers), () -> consumer);
         context.registerMetric("kafkaOffset", kafkaOffsetMetric, kafkaSpoutConfig.getMetricsTimeBucketSizeInSecs());
     }
 

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java
Patch:
@@ -156,7 +156,7 @@ public void open(Map<String, Object> conf, TopologyContext context, SpoutOutputC
 
     private void registerMetric() {
         LOG.info("Registering Spout Metrics");
-        kafkaOffsetMetric = new KafkaOffsetMetric(() -> Collections.unmodifiableMap(offsetManagers), () -> consumer);
+        kafkaOffsetMetric = new KafkaOffsetMetric<>(() -> Collections.unmodifiableMap(offsetManagers), () -> consumer);
         context.registerMetric("kafkaOffset", kafkaOffsetMetric, kafkaSpoutConfig.getMetricsTimeBucketSizeInSecs());
     }
 

File: sql/storm-sql-core/src/jvm/org/apache/storm/sql/calcite/ParallelStreamableTable.java
Patch:
@@ -20,7 +20,7 @@
  *
  * @see Delta
  */
-public interface ParallelStreamableTable extends StreamableTable {
+public interface ParallelStreamableTable extends StormStreamableTable {
 
     /**
      * Returns parallelism hint of this table. Returns null if don't know.

File: sql/storm-sql-core/src/jvm/org/apache/storm/sql/parser/ColumnConstraint.java
Patch:
@@ -27,7 +27,7 @@ public static class PrimaryKey extends ColumnConstraint {
         private final SqlMonotonicity monotonicity;
 
         public PrimaryKey(SqlMonotonicity monotonicity, SqlParserPos pos) {
-            super(SqlDDLKeywords.PRIMARY, SqlTypeName.SYMBOL, pos);
+            super(SqlDdlKeywords.PRIMARY, SqlTypeName.SYMBOL, pos);
             this.monotonicity = monotonicity;
         }
 

File: sql/storm-sql-core/src/jvm/org/apache/storm/sql/parser/SqlDdlKeywords.java
Patch:
@@ -15,8 +15,8 @@
 import org.apache.calcite.sql.SqlLiteral;
 
 /**
- * Define the keywords that can occur in a CREATE TABLE statement
+ * Define the keywords that can occur in a CREATE TABLE statement.
  */
-public enum SqlDDLKeywords implements SqlLiteral.SqlSymbol {
+public enum SqlDdlKeywords implements SqlLiteral.SqlSymbol {
     PRIMARY
 }

File: sql/storm-sql-core/src/jvm/org/apache/storm/sql/parser/StormParser.java
Patch:
@@ -21,6 +21,9 @@ public class StormParser {
     public static final int DEFAULT_IDENTIFIER_MAX_LENGTH = 128;
     private final StormParserImpl impl;
 
+    /**
+     * Constructor.
+     */
     public StormParser(String s) {
         this.impl = new StormParserImpl(new StringReader(s));
         this.impl.setTabSize(1);

File: sql/storm-sql-core/src/jvm/org/apache/storm/sql/planner/rel/StormStreamScanRelBase.java
Patch:
@@ -19,8 +19,6 @@
 
 public abstract class StormStreamScanRelBase extends TableScan implements StormRelNode {
 
-    // FIXME: define Table class and table.unwrap() to get it
-
     protected StormStreamScanRelBase(RelOptCluster cluster, RelTraitSet traitSet, RelOptTable table) {
         super(cluster, traitSet, table);
     }

File: storm-client/src/jvm/org/apache/storm/messaging/netty/StormClientPipelineFactory.java
Patch:
@@ -15,6 +15,7 @@
 import java.util.Map;
 import java.util.concurrent.atomic.AtomicBoolean;
 import org.apache.storm.Config;
+import org.apache.storm.serialization.KryoValuesDeserializer;
 import org.apache.storm.shade.io.netty.channel.Channel;
 import org.apache.storm.shade.io.netty.channel.ChannelInitializer;
 import org.apache.storm.shade.io.netty.channel.ChannelPipeline;
@@ -36,7 +37,7 @@ protected void initChannel(Channel ch) throws Exception {
         ChannelPipeline pipeline = ch.pipeline();
 
         // Decoder
-        pipeline.addLast("decoder", new MessageDecoder(client.deser));
+        pipeline.addLast("decoder", new MessageDecoder(new KryoValuesDeserializer(conf)));
         // Encoder
         pipeline.addLast("encoder", NettySerializableMessageEncoder.INSTANCE);
 

File: storm-client/src/jvm/org/apache/storm/messaging/netty/StormClientPipelineFactory.java
Patch:
@@ -15,6 +15,7 @@
 import java.util.Map;
 import java.util.concurrent.atomic.AtomicBoolean;
 import org.apache.storm.Config;
+import org.apache.storm.serialization.KryoValuesDeserializer;
 import org.apache.storm.shade.io.netty.channel.Channel;
 import org.apache.storm.shade.io.netty.channel.ChannelInitializer;
 import org.apache.storm.shade.io.netty.channel.ChannelPipeline;
@@ -36,7 +37,7 @@ protected void initChannel(Channel ch) throws Exception {
         ChannelPipeline pipeline = ch.pipeline();
 
         // Decoder
-        pipeline.addLast("decoder", new MessageDecoder(client.deser));
+        pipeline.addLast("decoder", new MessageDecoder(new KryoValuesDeserializer(conf)));
         // Encoder
         pipeline.addLast("encoder", NettySerializableMessageEncoder.INSTANCE);
 

File: integration-test/src/main/java/org/apache/storm/st/topology/TestableTopology.java
Patch:
@@ -19,11 +19,11 @@
 
 import org.apache.storm.generated.StormTopology;
 
-import java.util.List;
-
 public interface TestableTopology {
     String DUMMY_FIELD = "dummy";
-    List<String> getExpectedOutput();
+    //Some tests rely on reading the worker log. If emits are too close together and too much is logged, the log might roll, breaking the test.
+    int MIN_SLEEP_BETWEEN_EMITS_MS = 10;
+    int MAX_SLEEP_BETWEEN_EMITS_MS = 100;
     StormTopology newTopology();
     String getBoltName();
     String getSpoutName();

File: storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/handler/LogviewerLogSearchHandler.java
Patch:
@@ -426,7 +426,8 @@ Matched findNMatches(List<File> logs, int numMatches, int fileOffset, int offset
 
             int newCount = matchCount + ((List<?>)theseMatches.get("matches")).size();
 
-            if (theseMatches.isEmpty()) {
+            //theseMatches is never empty! As guaranteed by the #get().size() method above
+            if (newCount == matchCount) {
                 // matches and matchCount is not changed
                 logs = rest(logs);
                 offset = 0;

File: storm-client/src/jvm/org/apache/storm/daemon/worker/BackPressureTracker.java
Patch:
@@ -59,7 +59,7 @@ private void recordNoBackPressure(Integer taskId) {
      * @return true if an update was recorded, false if taskId is already under BP
      */
     public boolean recordBackPressure(Integer taskId) {
-        return tasks.get(taskId).backpressure.getAndSet(true);
+        return tasks.get(taskId).backpressure.getAndSet(true) == false;
     }
 
     // returns true if there was a change in the BP situation

File: external/storm-kafka-client/src/test/java/org/apache/storm/kafka/NullRecordTranslator.java
Patch:
@@ -16,7 +16,6 @@
 
 package org.apache.storm.kafka;
 
-import java.util.Collections;
 import java.util.List;
 import org.apache.kafka.clients.consumer.ConsumerRecord;
 import org.apache.storm.kafka.spout.RecordTranslator;

File: storm-client/src/jvm/org/apache/storm/testing/TmpPath.java
Patch:
@@ -48,7 +48,7 @@ public File getFile() {
     }
 
     @Override
-    public void close() throws Exception {
+    public void close() {
         if (path.exists()) {
             try {
                 FileUtils.forceDelete(path);

File: storm-server/src/main/java/org/apache/storm/daemon/supervisor/Slot.java
Patch:
@@ -1025,7 +1025,7 @@ public DynamicState(final LocalAssignment currentAssignment, Container container
                 state = MachineState.RUNNING;
             }
 
-            this.startTime = System.currentTimeMillis();
+            this.startTime = Time.currentTimeMillis();
             this.newAssignment = newAssignment;
             this.pendingLocalization = null;
             this.pendingDownload = null;

File: storm-server/src/main/java/org/apache/storm/daemon/supervisor/Slot.java
Patch:
@@ -1025,7 +1025,7 @@ public DynamicState(final LocalAssignment currentAssignment, Container container
                 state = MachineState.RUNNING;
             }
 
-            this.startTime = System.currentTimeMillis();
+            this.startTime = Time.currentTimeMillis();
             this.newAssignment = newAssignment;
             this.pendingLocalization = null;
             this.pendingDownload = null;

File: examples/storm-mqtt-examples/src/main/java/org/apache/storm/mqtt/examples/CustomMessageMapper.java
Patch:
@@ -72,8 +72,8 @@ public Fields outputFields() {
     }
 
     /**
-     * Utility constructor.
+     * Constructor.
      */
-    private CustomMessageMapper() {
+    public CustomMessageMapper() {
     }
 }

File: storm-client/src/jvm/org/apache/storm/pacemaker/codec/ThriftEncoder.java
Patch:
@@ -41,6 +41,7 @@ private HBMessage encodeNettySerializable(ByteBufAllocator alloc,
         byte[] messageBuffer = new byte[netty_message.encodeLength()];
         ByteBuf wrappedBuffer = Unpooled.wrappedBuffer(messageBuffer);
         try {
+            wrappedBuffer.resetWriterIndex();
             netty_message.write(wrappedBuffer);
             
             message_data.set_message_blob(messageBuffer);

File: storm-core/src/jvm/org/apache/storm/command/KillWorkers.java
Patch:
@@ -22,7 +22,6 @@
 public class KillWorkers {
     public static void main(String[] args) throws Exception {
         Map<String, Object> conf = Utils.readStormConfig();
-        conf.put(Config.STORM_LOCAL_DIR, new File((String) conf.get(Config.STORM_LOCAL_DIR)).getCanonicalPath());
         try (Supervisor supervisor = new Supervisor(conf, null, new StandaloneSupervisor())) {
             supervisor.shutdownAllWorkers(null, null);
         }

File: storm-client/src/jvm/org/apache/storm/messaging/netty/ISaslServer.java
Patch:
@@ -12,7 +12,7 @@
 
 package org.apache.storm.messaging.netty;
 
-import org.apache.storm.shade.org.jboss.netty.channel.Channel;
+import org.apache.storm.shade.io.netty.channel.Channel;
 
 public interface ISaslServer extends IServer {
     String name();

File: storm-server/src/main/java/org/apache/storm/pacemaker/IServerMessageHandler.java
Patch:
@@ -16,5 +16,5 @@
 
 public interface IServerMessageHandler {
 
-    public HBMessage handleMessage(HBMessage m, boolean authenticated);
+    HBMessage handleMessage(HBMessage m, boolean authenticated);
 }

File: storm-server/src/main/java/org/apache/storm/daemon/supervisor/Slot.java
Patch:
@@ -381,7 +381,7 @@ private static DynamicState informChangedBlobs(DynamicState dynamicState, LocalA
      * @return the updated dynamicState
      */
     private static DynamicState filterChangingBlobsFor(DynamicState dynamicState, final LocalAssignment assignment) {
-        if (!dynamicState.changingBlobs.isEmpty()) {
+        if (dynamicState.changingBlobs.isEmpty()) {
             return dynamicState;
         }
 

File: storm-server/src/main/java/org/apache/storm/daemon/supervisor/Slot.java
Patch:
@@ -381,7 +381,7 @@ private static DynamicState informChangedBlobs(DynamicState dynamicState, LocalA
      * @return the updated dynamicState
      */
     private static DynamicState filterChangingBlobsFor(DynamicState dynamicState, final LocalAssignment assignment) {
-        if (!dynamicState.changingBlobs.isEmpty()) {
+        if (dynamicState.changingBlobs.isEmpty()) {
             return dynamicState;
         }
 

File: storm-server/src/main/java/org/apache/storm/blobstore/FileBlobStoreImpl.java
Patch:
@@ -12,7 +12,6 @@
 
 package org.apache.storm.blobstore;
 
-import com.google.common.annotations.VisibleForTesting;
 import java.io.File;
 import java.io.FileNotFoundException;
 import java.io.IOException;
@@ -30,6 +29,7 @@
 import java.util.Timer;
 import java.util.TimerTask;
 import org.apache.storm.Config;
+import org.apache.storm.shade.com.google.common.annotations.VisibleForTesting;
 import org.apache.storm.utils.ObjectReader;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;

File: storm-server/src/main/java/org/apache/storm/blobstore/LocalFsBlobStore.java
Patch:
@@ -14,7 +14,6 @@
 
 package org.apache.storm.blobstore;
 
-import com.google.common.annotations.VisibleForTesting;
 import java.io.ByteArrayOutputStream;
 import java.io.File;
 import java.io.FileNotFoundException;
@@ -41,6 +40,7 @@
 import org.apache.storm.generated.SettableBlobMeta;
 import org.apache.storm.nimbus.ILeaderElector;
 import org.apache.storm.nimbus.NimbusInfo;
+import org.apache.storm.shade.com.google.common.annotations.VisibleForTesting;
 import org.apache.storm.shade.org.apache.curator.framework.CuratorFramework;
 import org.apache.storm.shade.org.apache.zookeeper.KeeperException;
 import org.apache.storm.utils.ConfigUtils;

File: storm-server/src/main/java/org/apache/storm/daemon/drpc/DRPC.java
Patch:
@@ -19,7 +19,6 @@
 package org.apache.storm.daemon.drpc;
 
 import com.codahale.metrics.Meter;
-import com.google.common.annotations.VisibleForTesting;
 import java.security.Principal;
 import java.util.HashMap;
 import java.util.Map;
@@ -40,6 +39,7 @@
 import org.apache.storm.security.auth.IAuthorizer;
 import org.apache.storm.security.auth.ReqContext;
 import org.apache.storm.security.auth.authorizer.DRPCAuthorizerBase;
+import org.apache.storm.shade.com.google.common.annotations.VisibleForTesting;
 import org.apache.storm.utils.ObjectReader;
 import org.apache.storm.utils.WrappedAuthorizationException;
 import org.apache.storm.utils.WrappedDRPCExecutionException;

File: storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java
Patch:
@@ -21,9 +21,6 @@
 import com.codahale.metrics.ExponentiallyDecayingReservoir;
 import com.codahale.metrics.Histogram;
 import com.codahale.metrics.Meter;
-import com.google.common.annotations.VisibleForTesting;
-import com.google.common.base.Strings;
-import com.google.common.collect.ImmutableMap;
 import java.io.File;
 import java.io.FileInputStream;
 import java.io.FileOutputStream;
@@ -180,6 +177,9 @@
 import org.apache.storm.security.auth.ThriftConnectionType;
 import org.apache.storm.security.auth.ThriftServer;
 import org.apache.storm.security.auth.workertoken.WorkerTokenManager;
+import org.apache.storm.shade.com.google.common.annotations.VisibleForTesting;
+import org.apache.storm.shade.com.google.common.base.Strings;
+import org.apache.storm.shade.com.google.common.collect.ImmutableMap;
 import org.apache.storm.shade.org.apache.curator.framework.CuratorFramework;
 import org.apache.storm.shade.org.apache.zookeeper.ZooDefs;
 import org.apache.storm.shade.org.apache.zookeeper.data.ACL;

File: storm-server/src/main/java/org/apache/storm/daemon/supervisor/BasicContainer.java
Patch:
@@ -18,8 +18,6 @@
 
 package org.apache.storm.daemon.supervisor;
 
-import com.google.common.base.Joiner;
-import com.google.common.collect.Lists;
 import java.io.File;
 import java.io.FilenameFilter;
 import java.io.IOException;
@@ -43,6 +41,8 @@
 import org.apache.storm.generated.ProfileRequest;
 import org.apache.storm.generated.StormTopology;
 import org.apache.storm.generated.WorkerResources;
+import org.apache.storm.shade.com.google.common.base.Joiner;
+import org.apache.storm.shade.com.google.common.collect.Lists;
 import org.apache.storm.utils.ConfigUtils;
 import org.apache.storm.utils.LocalState;
 import org.apache.storm.utils.ObjectReader;

File: storm-server/src/main/java/org/apache/storm/daemon/supervisor/Slot.java
Patch:
@@ -13,7 +13,6 @@
 package org.apache.storm.daemon.supervisor;
 
 import com.codahale.metrics.Meter;
-import com.google.common.annotations.VisibleForTesting;
 import java.io.IOException;
 import java.util.Collections;
 import java.util.HashMap;
@@ -45,6 +44,7 @@
 import org.apache.storm.metric.StormMetricsRegistry;
 import org.apache.storm.metricstore.WorkerMetricsProcessor;
 import org.apache.storm.scheduler.ISupervisor;
+import org.apache.storm.shade.com.google.common.annotations.VisibleForTesting;
 import org.apache.storm.utils.LocalState;
 import org.apache.storm.utils.ObjectReader;
 import org.apache.storm.utils.Time;

File: storm-server/src/main/java/org/apache/storm/daemon/supervisor/Supervisor.java
Patch:
@@ -18,7 +18,6 @@
 
 package org.apache.storm.daemon.supervisor;
 
-import com.google.common.annotations.VisibleForTesting;
 import java.io.File;
 import java.io.IOException;
 import java.net.BindException;
@@ -64,6 +63,7 @@
 import org.apache.storm.security.auth.ReqContext;
 import org.apache.storm.security.auth.ThriftConnectionType;
 import org.apache.storm.security.auth.ThriftServer;
+import org.apache.storm.shade.com.google.common.annotations.VisibleForTesting;
 import org.apache.storm.utils.ConfigUtils;
 import org.apache.storm.utils.LocalState;
 import org.apache.storm.utils.ObjectReader;

File: storm-server/src/main/java/org/apache/storm/localizer/AsyncLocalizer.java
Patch:
@@ -18,8 +18,6 @@
 
 package org.apache.storm.localizer;
 
-import com.google.common.annotations.VisibleForTesting;
-import com.google.common.util.concurrent.ThreadFactoryBuilder;
 import java.io.File;
 import java.io.IOException;
 import java.nio.file.DirectoryStream;
@@ -51,6 +49,8 @@
 import org.apache.storm.generated.KeyNotFoundException;
 import org.apache.storm.generated.LocalAssignment;
 import org.apache.storm.generated.StormTopology;
+import org.apache.storm.shade.com.google.common.annotations.VisibleForTesting;
+import org.apache.storm.shade.com.google.common.util.concurrent.ThreadFactoryBuilder;
 import org.apache.storm.utils.ConfigUtils;
 import org.apache.storm.utils.NimbusLeaderNotFoundException;
 import org.apache.storm.utils.ObjectReader;

File: storm-server/src/main/java/org/apache/storm/localizer/LocalizedResource.java
Patch:
@@ -18,7 +18,6 @@
 
 package org.apache.storm.localizer;
 
-import com.google.common.annotations.VisibleForTesting;
 import java.io.BufferedReader;
 import java.io.BufferedWriter;
 import java.io.FileOutputStream;
@@ -49,6 +48,7 @@
 import org.apache.storm.generated.AuthorizationException;
 import org.apache.storm.generated.KeyNotFoundException;
 import org.apache.storm.generated.ReadableBlobMeta;
+import org.apache.storm.shade.com.google.common.annotations.VisibleForTesting;
 import org.apache.storm.utils.ObjectReader;
 import org.apache.storm.utils.ServerUtils;
 import org.apache.storm.utils.ShellUtils;

File: storm-server/src/main/java/org/apache/storm/localizer/LocalizedResourceRetentionSet.java
Patch:
@@ -12,7 +12,6 @@
 
 package org.apache.storm.localizer;
 
-import com.google.common.annotations.VisibleForTesting;
 import java.util.Comparator;
 import java.util.Iterator;
 import java.util.Map;
@@ -21,7 +20,7 @@
 import java.util.concurrent.ConcurrentMap;
 import org.apache.storm.blobstore.ClientBlobStore;
 import org.apache.storm.generated.AuthorizationException;
-import org.apache.storm.generated.KeyNotFoundException;
+import org.apache.storm.shade.com.google.common.annotations.VisibleForTesting;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 

File: storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/RocksDbKey.java
Patch:
@@ -11,14 +11,14 @@
 
 package org.apache.storm.metricstore.rocksdb;
 
-import com.google.common.primitives.UnsignedBytes;
 import java.nio.ByteBuffer;
 import java.util.Collections;
 import java.util.EnumSet;
 import java.util.HashMap;
 import java.util.Map;
 import javax.xml.bind.DatatypeConverter;
 import org.apache.storm.metricstore.AggLevel;
+import org.apache.storm.shade.com.google.common.primitives.UnsignedBytes;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 

File: storm-server/src/main/java/org/apache/storm/nimbus/LeaderListenerCallback.java
Patch:
@@ -12,8 +12,6 @@
 
 package org.apache.storm.nimbus;
 
-import com.google.common.base.Joiner;
-import com.google.common.collect.Sets;
 import java.io.IOException;
 import java.util.HashSet;
 import java.util.List;
@@ -32,6 +30,8 @@
 import org.apache.storm.generated.KeyNotFoundException;
 import org.apache.storm.generated.StormTopology;
 import org.apache.storm.security.auth.ReqContext;
+import org.apache.storm.shade.com.google.common.base.Joiner;
+import org.apache.storm.shade.com.google.common.collect.Sets;
 import org.apache.storm.shade.org.apache.curator.framework.CuratorFramework;
 import org.apache.storm.shade.org.apache.curator.framework.recipes.leader.LeaderLatch;
 import org.apache.storm.shade.org.apache.zookeeper.CreateMode;

File: storm-server/src/main/java/org/apache/storm/nimbus/WorkerHeartbeatsRecoveryStrategyFactory.java
Patch:
@@ -12,9 +12,9 @@
 
 package org.apache.storm.nimbus;
 
-import com.google.common.base.Preconditions;
 import java.util.Map;
 import org.apache.storm.DaemonConfig;
+import org.apache.storm.shade.com.google.common.base.Preconditions;
 import org.apache.storm.utils.ReflectionUtils;
 
 /**

File: storm-server/src/main/java/org/apache/storm/scheduler/Cluster.java
Patch:
@@ -18,7 +18,6 @@
 
 package org.apache.storm.scheduler;
 
-import com.google.common.annotations.VisibleForTesting;
 import java.util.ArrayList;
 import java.util.Collection;
 import java.util.Collections;
@@ -40,6 +39,7 @@
 import org.apache.storm.scheduler.resource.normalization.NormalizedResourceOffer;
 import org.apache.storm.scheduler.resource.normalization.NormalizedResourceRequest;
 import org.apache.storm.scheduler.resource.normalization.NormalizedResources;
+import org.apache.storm.shade.com.google.common.annotations.VisibleForTesting;
 import org.apache.storm.utils.ConfigUtils;
 import org.apache.storm.utils.ObjectReader;
 import org.apache.storm.utils.ReflectionUtils;

File: storm-server/src/main/java/org/apache/storm/scheduler/EvenScheduler.java
Patch:
@@ -18,8 +18,6 @@
 
 package org.apache.storm.scheduler;
 
-import com.google.common.annotations.VisibleForTesting;
-import com.google.common.collect.Sets;
 import java.util.ArrayList;
 import java.util.Collections;
 import java.util.Comparator;
@@ -29,6 +27,8 @@
 import java.util.Map;
 import java.util.Set;
 import java.util.TreeMap;
+import org.apache.storm.shade.com.google.common.annotations.VisibleForTesting;
+import org.apache.storm.shade.com.google.common.collect.Sets;
 import org.apache.storm.utils.ServerUtils;
 import org.apache.storm.utils.Utils;
 import org.slf4j.Logger;

File: storm-server/src/main/java/org/apache/storm/scheduler/blacklist/BlacklistScheduler.java
Patch:
@@ -12,8 +12,6 @@
 
 package org.apache.storm.scheduler.blacklist;
 
-import com.google.common.collect.EvictingQueue;
-import com.google.common.collect.Sets;
 import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.HashSet;
@@ -31,6 +29,8 @@
 import org.apache.storm.scheduler.blacklist.reporters.LogReporter;
 import org.apache.storm.scheduler.blacklist.strategies.DefaultBlacklistStrategy;
 import org.apache.storm.scheduler.blacklist.strategies.IBlacklistStrategy;
+import org.apache.storm.shade.com.google.common.collect.EvictingQueue;
+import org.apache.storm.shade.com.google.common.collect.Sets;
 import org.apache.storm.utils.ObjectReader;
 import org.apache.storm.utils.ReflectionUtils;
 import org.slf4j.Logger;

File: storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceAwareScheduler.java
Patch:
@@ -12,7 +12,6 @@
 
 package org.apache.storm.scheduler.resource;
 
-import com.google.common.collect.ImmutableList;
 import java.util.ArrayList;
 import java.util.Collection;
 import java.util.HashMap;
@@ -32,6 +31,7 @@
 import org.apache.storm.scheduler.resource.strategies.scheduling.IStrategy;
 import org.apache.storm.scheduler.utils.ConfigLoaderFactoryService;
 import org.apache.storm.scheduler.utils.IConfigLoader;
+import org.apache.storm.shade.com.google.common.collect.ImmutableList;
 import org.apache.storm.utils.DisallowedStrategyException;
 import org.apache.storm.utils.ObjectReader;
 import org.apache.storm.utils.ReflectionUtils;

File: storm-server/src/main/java/org/apache/storm/scheduler/resource/normalization/NormalizedResources.java
Patch:
@@ -19,12 +19,12 @@
 package org.apache.storm.scheduler.resource.normalization;
 
 import com.codahale.metrics.Meter;
-import com.google.common.annotations.VisibleForTesting;
 import java.util.Arrays;
 import java.util.Map;
 import org.apache.storm.Constants;
 import org.apache.storm.generated.WorkerResources;
 import org.apache.storm.metric.StormMetricsRegistry;
+import org.apache.storm.shade.com.google.common.annotations.VisibleForTesting;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 

File: storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/BaseResourceAwareStrategy.java
Patch:
@@ -18,8 +18,6 @@
 
 package org.apache.storm.scheduler.resource.strategies.scheduling;
 
-import com.google.common.annotations.VisibleForTesting;
-import com.google.common.collect.Sets;
 import java.util.ArrayList;
 import java.util.Collection;
 import java.util.Collections;
@@ -47,6 +45,8 @@
 import org.apache.storm.scheduler.resource.SchedulingStatus;
 import org.apache.storm.scheduler.resource.normalization.NormalizedResourceOffer;
 import org.apache.storm.scheduler.resource.normalization.NormalizedResourceRequest;
+import org.apache.storm.shade.com.google.common.annotations.VisibleForTesting;
+import org.apache.storm.shade.com.google.common.collect.Sets;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 

File: storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/ConstraintSolverStrategy.java
Patch:
@@ -12,7 +12,6 @@
 
 package org.apache.storm.scheduler.resource.strategies.scheduling;
 
-import com.google.common.annotations.VisibleForTesting;
 import java.util.ArrayList;
 import java.util.Collection;
 import java.util.Comparator;
@@ -35,6 +34,7 @@
 import org.apache.storm.scheduler.resource.RAS_Nodes;
 import org.apache.storm.scheduler.resource.SchedulingResult;
 import org.apache.storm.scheduler.resource.SchedulingStatus;
+import org.apache.storm.shade.com.google.common.annotations.VisibleForTesting;
 import org.apache.storm.utils.ObjectReader;
 import org.apache.storm.utils.Time;
 import org.slf4j.Logger;

File: storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/ConstraintSolverStrategy.java
Patch:
@@ -99,6 +99,7 @@ && checkConstraintsSatisfied(cluster, td)
      */
     private static boolean checkConstraintsSatisfied(Cluster cluster, TopologyDetails topo) {
         LOG.info("Checking constraints...");
+        assert (cluster.getAssignmentById(topo.getId()) != null);
         Map<ExecutorDetails, WorkerSlot> result = cluster.getAssignmentById(topo.getId()).getExecutorToSlot();
         Map<ExecutorDetails, String> execToComp = topo.getExecutorToComponent();
         //get topology constraints
@@ -136,6 +137,7 @@ private static Map<WorkerSlot, RAS_Node> workerToNodes(Cluster cluster) {
 
     private static boolean checkSpreadSchedulingValid(Cluster cluster, TopologyDetails topo) {
         LOG.info("Checking for a valid scheduling...");
+        assert (cluster.getAssignmentById(topo.getId()) != null);
         Map<ExecutorDetails, WorkerSlot> result = cluster.getAssignmentById(topo.getId()).getExecutorToSlot();
         Map<ExecutorDetails, String> execToComp = topo.getExecutorToComponent();
         Map<WorkerSlot, HashSet<ExecutorDetails>> workerExecMap = new HashMap<>();
@@ -174,6 +176,7 @@ private static boolean checkSpreadSchedulingValid(Cluster cluster, TopologyDetai
      */
     private static boolean checkResourcesCorrect(Cluster cluster, TopologyDetails topo) {
         LOG.info("Checking Resources...");
+        assert (cluster.getAssignmentById(topo.getId()) != null);
         Map<ExecutorDetails, WorkerSlot> result = cluster.getAssignmentById(topo.getId()).getExecutorToSlot();
         Map<RAS_Node, Collection<ExecutorDetails>> nodeToExecs = new HashMap<>();
         Map<ExecutorDetails, WorkerSlot> mergedExecToWorker = new HashMap<>();

File: examples/storm-jms-examples/src/main/java/org/apache/storm/jms/example/ExampleJmsTopology.java
Patch:
@@ -62,7 +62,6 @@ public static void main(String[] args) throws Exception {
         queueSpout.setJmsProvider(jmsQueueProvider);
         queueSpout.setJmsTupleProducer(producer);
         queueSpout.setJmsAcknowledgeMode(Session.CLIENT_ACKNOWLEDGE);
-        queueSpout.setDistributed(true); // allow multiple instances
 
         TopologyBuilder builder = new TopologyBuilder();
 

File: external/storm-jms/src/test/java/org/apache/storm/jms/spout/JmsSpoutTest.java
Patch:
@@ -53,7 +53,6 @@ public void testFailure() throws JMSException, Exception {
             spout.setJmsProvider(new MockJmsProvider());
             spout.setJmsTupleProducer(new MockTupleProducer());
             spout.setJmsAcknowledgeMode(Session.CLIENT_ACKNOWLEDGE);
-            spout.setRecoveryPeriodMs(10); // Rapid recovery for testing.
             spout.open(new HashMap<>(), null, collector);
             ConnectionFactory connectionFactory = mockProvider.connectionFactory();
             Destination destination = mockProvider.destination();

File: storm-server/src/main/java/org/apache/storm/zookeeper/Zookeeper.java
Patch:
@@ -72,7 +72,7 @@ public static void resetInstance() {
         _instance = INSTANCE;
     }
 
-    public static List mkInprocessZookeeper(String localdir, Integer port) throws Exception {
+    public static NIOServerCnxnFactory mkInprocessZookeeper(String localdir, Integer port) throws Exception {
         File localfile = new File(localdir);
         ZooKeeperServer zk = new ZooKeeperServer(localfile, localfile, 2000);
         NIOServerCnxnFactory factory = null;
@@ -96,7 +96,7 @@ public static List mkInprocessZookeeper(String localdir, Integer port) throws Ex
         }
         LOG.info("Starting inprocess zookeeper at port {} and dir {}", report, localdir);
         factory.startup(zk);
-        return Arrays.asList((Object) new Long(report), (Object) factory);
+        return factory;
     }
 
     public static void shutdownInprocessZookeeper(NIOServerCnxnFactory handle) {

File: storm-client/src/jvm/org/apache/storm/metrics2/reporters/JmxStormReporter.java
Patch:
@@ -28,7 +28,7 @@ public class JmxStormReporter implements StormReporter {
     JmxReporter reporter = null;
 
     public static String getMetricsJmxDomain(Map reporterConf) {
-        return ObjectReader.getString(reporterConf, JMX_DOMAIN);
+        return ObjectReader.getString(reporterConf.get(JMX_DOMAIN), null);
     }
 
     @Override

File: storm-client/src/jvm/org/apache/storm/testing/PerformanceTest.java
Patch:
@@ -25,13 +25,11 @@
  * add the annotation @Category(PerformanceTest.class) to the class definition as well as to its hierarchy of superclasses.
  * For example:
  * <p/>
- *
- *
  * @ Category(PerformanceTest.class)<br/>
  * public class MyPerformanceTest {<br/>
  *  ...<br/>
  * }
- *
+ * <p/>
  *  In general performance tests should have a time limit on them, but the time limit should be liberal enough to account
  *  for running on CI systems like travis ci, or the apache jenkins build.
  */

File: storm-server/src/main/java/org/apache/storm/scheduler/resource/normalization/ResourceMapArrayBridge.java
Patch:
@@ -65,7 +65,7 @@ public double[] translateToResourceArray(Map<String, Double> normalizedResources
     }
 
     /**
-     * Create an array that has all values 0;
+     * Create an array that has all values 0.
      * @return the empty array.
      */
     public double[] empty() {

File: storm-server/src/main/java/org/apache/storm/DaemonConfig.java
Patch:
@@ -684,6 +684,7 @@ public class DaemonConfig implements Validated {
 
     /**
      * A number representing the maximum number of workers any single topology can acquire.
+     * This will be ignored if the Resource Aware Scheduler is used.
      */
     @isInteger
     @isPositiveNumber(includeZero = true)

File: examples/storm-perf/src/main/java/org/apache/storm/perf/ConstSpoutNullBoltTopo.java
Patch:
@@ -19,7 +19,6 @@
 package org.apache.storm.perf;
 
 import java.util.Map;
-
 import org.apache.storm.Config;
 import org.apache.storm.generated.StormTopology;
 import org.apache.storm.perf.bolt.DevNullBolt;
@@ -89,7 +88,8 @@ public static void main(String[] args) throws Exception {
         //  For reference : numbers taken on MacBook Pro mid 2015
         //    -- ACKer=0:  ~8 mill/sec (batchSz=2k & recvQsize=50k).  6.7 mill/sec (batchSz=1 & recvQsize=1k)
         //    -- ACKer=1:  ~1 mill/sec,   lat= ~1 microsec  (batchSz=1 & bolt.wait.strategy=Park bolt.wait.park.micros=0)
-        //    -- ACKer=1:  ~1.3 mill/sec, lat= ~11 micros   (batchSz=1 & receive.buffer.size=1k, bolt.wait & bp.wait = Progressive[defaults])
+        //    -- ACKer=1:  ~1.3 mill/sec, lat= ~11 micros   (batchSz=1 & receive.buffer.size=1k, bolt.wait & bp.wait =
+        // Progressive[defaults])
         //    -- ACKer=1:  ~1.6 mill/sec, lat= ~300 micros  (batchSz=500 & bolt.wait.strategy=Park bolt.wait.park.micros=0)
         topoConf.put(Config.TOPOLOGY_SPOUT_RECVQ_SKIPS, 8);
         topoConf.put(Config.TOPOLOGY_PRODUCER_BATCH_SIZE, 500);

File: examples/storm-perf/src/main/java/org/apache/storm/perf/HdfsSpoutNullBoltTopo.java
Patch:
@@ -19,7 +19,6 @@
 package org.apache.storm.perf;
 
 import java.util.Map;
-
 import org.apache.storm.Config;
 import org.apache.storm.generated.StormTopology;
 import org.apache.storm.hdfs.spout.HdfsSpout;
@@ -78,7 +77,7 @@ static StormTopology getTopology(Map<String, Object> config) {
         TopologyBuilder builder = new TopologyBuilder();
         builder.setSpout(SPOUT_ID, spout, spoutNum);
         builder.setBolt(BOLT_ID, bolt, boltNum)
-            .localOrShuffleGrouping(SPOUT_ID);
+               .localOrShuffleGrouping(SPOUT_ID);
 
         return builder.createTopology();
     }

File: examples/storm-perf/src/main/java/org/apache/storm/perf/SimplifiedWordCountTopo.java
Patch:
@@ -19,7 +19,6 @@
 package org.apache.storm.perf;
 
 import java.util.Map;
-
 import org.apache.storm.Config;
 import org.apache.storm.generated.StormTopology;
 import org.apache.storm.perf.bolt.CountBolt;

File: examples/storm-perf/src/main/java/org/apache/storm/perf/StrGenSpoutHdfsBoltTopo.java
Patch:
@@ -20,7 +20,6 @@
 package org.apache.storm.perf;
 
 import java.util.Map;
-
 import org.apache.storm.Config;
 import org.apache.storm.generated.StormTopology;
 import org.apache.storm.hdfs.bolt.HdfsBolt;
@@ -96,7 +95,7 @@ static StormTopology getTopology(Map<String, Object> topoConf) {
         TopologyBuilder builder = new TopologyBuilder();
         builder.setSpout(SPOUT_ID, spout, spoutNum);
         builder.setBolt(BOLT_ID, bolt, boltNum)
-            .localOrShuffleGrouping(SPOUT_ID);
+               .localOrShuffleGrouping(SPOUT_ID);
 
         return builder.createTopology();
     }

File: examples/storm-perf/src/main/java/org/apache/storm/perf/bolt/CountBolt.java
Patch:
@@ -20,7 +20,6 @@
 
 import java.util.HashMap;
 import java.util.Map;
-
 import org.apache.storm.task.TopologyContext;
 import org.apache.storm.topology.BasicOutputCollector;
 import org.apache.storm.topology.OutputFieldsDeclarer;

File: examples/storm-perf/src/main/java/org/apache/storm/perf/bolt/DevNullBolt.java
Patch:
@@ -20,7 +20,6 @@
 
 import java.util.Map;
 import java.util.concurrent.locks.LockSupport;
-
 import org.apache.storm.task.OutputCollector;
 import org.apache.storm.task.TopologyContext;
 import org.apache.storm.topology.OutputFieldsDeclarer;

File: examples/storm-perf/src/main/java/org/apache/storm/perf/bolt/IdBolt.java
Patch:
@@ -19,7 +19,6 @@
 package org.apache.storm.perf.bolt;
 
 import java.util.Map;
-
 import org.apache.storm.task.OutputCollector;
 import org.apache.storm.task.TopologyContext;
 import org.apache.storm.topology.OutputFieldsDeclarer;

File: examples/storm-perf/src/main/java/org/apache/storm/perf/bolt/SplitSentenceBolt.java
Patch:
@@ -19,7 +19,6 @@
 package org.apache.storm.perf.bolt;
 
 import java.util.Map;
-
 import org.apache.storm.task.TopologyContext;
 import org.apache.storm.topology.BasicOutputCollector;
 import org.apache.storm.topology.OutputFieldsDeclarer;

File: examples/storm-perf/src/main/java/org/apache/storm/perf/spout/ConstSpout.java
Patch:
@@ -20,7 +20,6 @@
 
 import java.util.ArrayList;
 import java.util.Map;
-
 import org.apache.storm.spout.SpoutOutputCollector;
 import org.apache.storm.task.TopologyContext;
 import org.apache.storm.topology.OutputFieldsDeclarer;

File: examples/storm-perf/src/main/java/org/apache/storm/perf/spout/FileReadSpout.java
Patch:
@@ -27,7 +27,6 @@
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
-
 import org.apache.storm.spout.SpoutOutputCollector;
 import org.apache.storm.task.TopologyContext;
 import org.apache.storm.topology.OutputFieldsDeclarer;

File: examples/storm-perf/src/main/java/org/apache/storm/perf/utils/IdentityBolt.java
Patch:
@@ -19,7 +19,6 @@
 package org.apache.storm.perf.utils;
 
 import java.util.Map;
-
 import org.apache.storm.task.OutputCollector;
 import org.apache.storm.task.TopologyContext;
 import org.apache.storm.topology.OutputFieldsDeclarer;
@@ -36,7 +35,7 @@ public void prepare(Map<String, Object> topoConf, TopologyContext context, Outpu
 
     @Override
     public void execute(Tuple tuple) {
-        collector.emit(tuple, tuple.getValues() );
+        collector.emit(tuple, tuple.getValues());
         collector.ack(tuple);
     }
 

File: examples/storm-perf/src/main/java/org/apache/storm/perf/utils/MetricsSample.java
Patch:
@@ -20,7 +20,6 @@
 
 import java.util.List;
 import java.util.Map;
-
 import org.apache.storm.generated.ClusterSummary;
 import org.apache.storm.generated.ExecutorSpecificStats;
 import org.apache.storm.generated.ExecutorStats;
@@ -160,7 +159,7 @@ private static MetricsSample getMetricsSample(TopologyInfo topInfo) {
         ret.spoutEmitted = spoutEmitted;
         ret.spoutTransferred = spoutTransferred;
         ret.sampleTime = System.currentTimeMillis();
-//        ret.numSupervisors = clusterSummary.get_supervisors_size();
+        //        ret.numSupervisors = clusterSummary.get_supervisors_size();
         ret.numWorkers = 0;
         ret.numExecutors = 0;
         ret.numTasks = 0;

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/bolt/IEventDataFormat.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  *******************************************************************************/
+
 package org.apache.storm.eventhubs.bolt;
 
 import java.io.Serializable;
@@ -24,5 +25,5 @@
  * Serialize a tuple to a byte array to be sent to EventHubs
  */
 public interface IEventDataFormat extends Serializable {
-  public byte[] serialize(Tuple tuple);
+    public byte[] serialize(Tuple tuple);
 }

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/spout/IPartitionCoordinator.java
Patch:
@@ -15,13 +15,14 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  *******************************************************************************/
+
 package org.apache.storm.eventhubs.spout;
 
 import java.util.List;
 
 public interface IPartitionCoordinator {
 
-  List<IPartitionManager> getMyPartitionManagers();
+    List<IPartitionManager> getMyPartitionManagers();
 
-  IPartitionManager getPartitionManager(String partitionId);
+    IPartitionManager getPartitionManager(String partitionId);
 }

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/trident/ITridentPartitionManagerFactory.java
Patch:
@@ -15,12 +15,12 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  *******************************************************************************/
+
 package org.apache.storm.eventhubs.trident;
 
 import java.io.Serializable;
-
 import org.apache.storm.eventhubs.spout.IEventHubReceiver;
 
 public interface ITridentPartitionManagerFactory extends Serializable {
-  ITridentPartitionManager create(IEventHubReceiver receiver);
+    ITridentPartitionManager create(IEventHubReceiver receiver);
 }

File: storm-client/src/jvm/org/apache/storm/cluster/PaceMakerStateStorage.java
Patch:
@@ -129,7 +129,7 @@ public void set_worker_hb(String path, byte[] data, List<ACL> acls) {
                 }
                 LOG.debug("Successful set_worker_hb");
                 break;
-            } catch (HBExecutionException|PacemakerConnectionException e) {
+            } catch (HBExecutionException | PacemakerConnectionException e) {
                 if (retry <= 0) {
                     throw new RuntimeException(e);
                 }

File: storm-client/src/jvm/org/apache/storm/pacemaker/PacemakerClientPool.java
Patch:
@@ -46,7 +46,7 @@ public PacemakerClientPool(Map<String, Object> config) {
             servers = new ConcurrentLinkedQueue<>();
         }
     }
-    
+
     public HBMessage send(HBMessage m) throws PacemakerConnectionException, InterruptedException {
         try {
             return getWriteClient().send(m);
@@ -59,7 +59,7 @@ public HBMessage send(HBMessage m) throws PacemakerConnectionException, Interrup
     public List<HBMessage> sendAll(HBMessage m) throws PacemakerConnectionException, InterruptedException {
         List<HBMessage> responses = new ArrayList<HBMessage>();
         LOG.debug("Using servers: {}", servers);
-        for(String s : servers) {
+        for (String s : servers) {
             try {
                 HBMessage response = getClientForServer(s).send(m);
                 responses.add(response);

File: storm-client/src/jvm/org/apache/storm/cluster/PaceMakerStateStorage.java
Patch:
@@ -129,7 +129,7 @@ public void set_worker_hb(String path, byte[] data, List<ACL> acls) {
                 }
                 LOG.debug("Successful set_worker_hb");
                 break;
-            } catch (HBExecutionException e) {
+            } catch (HBExecutionException|PacemakerConnectionException e) {
                 if (retry <= 0) {
                     throw new RuntimeException(e);
                 }

File: external/storm-jms/src/test/java/org/apache/storm/jms/spout/MockSpoutOutputCollector.java
Patch:
@@ -15,11 +15,11 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.jms.spout;
 
 import java.util.ArrayList;
 import java.util.List;
-
 import org.apache.storm.spout.ISpoutOutputCollector;
 
 public class MockSpoutOutputCollector implements ISpoutOutputCollector {
@@ -45,11 +45,11 @@ public void flush() {
     public void reportError(Throwable error) {
     }
 
-    public boolean emitted(){
+    public boolean emitted() {
         return this.emitted;
     }
 
-    public void reset(){
+    public void reset() {
         this.emitted = false;
     }
 

File: external/storm-jms/src/test/java/org/apache/storm/jms/spout/MockTupleProducer.java
Patch:
@@ -15,12 +15,12 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.jms.spout;
 
 import javax.jms.JMSException;
 import javax.jms.Message;
 import javax.jms.TextMessage;
-
 import org.apache.storm.jms.JmsTupleProducer;
 import org.apache.storm.topology.OutputFieldsDeclarer;
 import org.apache.storm.tuple.Fields;

File: examples/storm-perf/src/main/java/org/apache/storm/perf/ConstSpoutNullBoltTopo.java
Patch:
@@ -19,7 +19,6 @@
 package org.apache.storm.perf;
 
 import java.util.Map;
-
 import org.apache.storm.Config;
 import org.apache.storm.generated.StormTopology;
 import org.apache.storm.perf.bolt.DevNullBolt;
@@ -89,7 +88,8 @@ public static void main(String[] args) throws Exception {
         //  For reference : numbers taken on MacBook Pro mid 2015
         //    -- ACKer=0:  ~8 mill/sec (batchSz=2k & recvQsize=50k).  6.7 mill/sec (batchSz=1 & recvQsize=1k)
         //    -- ACKer=1:  ~1 mill/sec,   lat= ~1 microsec  (batchSz=1 & bolt.wait.strategy=Park bolt.wait.park.micros=0)
-        //    -- ACKer=1:  ~1.3 mill/sec, lat= ~11 micros   (batchSz=1 & receive.buffer.size=1k, bolt.wait & bp.wait = Progressive[defaults])
+        //    -- ACKer=1:  ~1.3 mill/sec, lat= ~11 micros   (batchSz=1 & receive.buffer.size=1k, bolt.wait & bp.wait =
+        // Progressive[defaults])
         //    -- ACKer=1:  ~1.6 mill/sec, lat= ~300 micros  (batchSz=500 & bolt.wait.strategy=Park bolt.wait.park.micros=0)
         topoConf.put(Config.TOPOLOGY_SPOUT_RECVQ_SKIPS, 8);
         topoConf.put(Config.TOPOLOGY_PRODUCER_BATCH_SIZE, 500);

File: examples/storm-perf/src/main/java/org/apache/storm/perf/HdfsSpoutNullBoltTopo.java
Patch:
@@ -19,7 +19,6 @@
 package org.apache.storm.perf;
 
 import java.util.Map;
-
 import org.apache.storm.Config;
 import org.apache.storm.generated.StormTopology;
 import org.apache.storm.hdfs.spout.HdfsSpout;
@@ -78,7 +77,7 @@ static StormTopology getTopology(Map<String, Object> config) {
         TopologyBuilder builder = new TopologyBuilder();
         builder.setSpout(SPOUT_ID, spout, spoutNum);
         builder.setBolt(BOLT_ID, bolt, boltNum)
-            .localOrShuffleGrouping(SPOUT_ID);
+               .localOrShuffleGrouping(SPOUT_ID);
 
         return builder.createTopology();
     }

File: examples/storm-perf/src/main/java/org/apache/storm/perf/SimplifiedWordCountTopo.java
Patch:
@@ -19,7 +19,6 @@
 package org.apache.storm.perf;
 
 import java.util.Map;
-
 import org.apache.storm.Config;
 import org.apache.storm.generated.StormTopology;
 import org.apache.storm.perf.bolt.CountBolt;

File: examples/storm-perf/src/main/java/org/apache/storm/perf/StrGenSpoutHdfsBoltTopo.java
Patch:
@@ -20,7 +20,6 @@
 package org.apache.storm.perf;
 
 import java.util.Map;
-
 import org.apache.storm.Config;
 import org.apache.storm.generated.StormTopology;
 import org.apache.storm.hdfs.bolt.HdfsBolt;
@@ -96,7 +95,7 @@ static StormTopology getTopology(Map<String, Object> topoConf) {
         TopologyBuilder builder = new TopologyBuilder();
         builder.setSpout(SPOUT_ID, spout, spoutNum);
         builder.setBolt(BOLT_ID, bolt, boltNum)
-            .localOrShuffleGrouping(SPOUT_ID);
+               .localOrShuffleGrouping(SPOUT_ID);
 
         return builder.createTopology();
     }

File: examples/storm-perf/src/main/java/org/apache/storm/perf/bolt/CountBolt.java
Patch:
@@ -20,7 +20,6 @@
 
 import java.util.HashMap;
 import java.util.Map;
-
 import org.apache.storm.task.TopologyContext;
 import org.apache.storm.topology.BasicOutputCollector;
 import org.apache.storm.topology.OutputFieldsDeclarer;

File: examples/storm-perf/src/main/java/org/apache/storm/perf/bolt/DevNullBolt.java
Patch:
@@ -20,7 +20,6 @@
 
 import java.util.Map;
 import java.util.concurrent.locks.LockSupport;
-
 import org.apache.storm.task.OutputCollector;
 import org.apache.storm.task.TopologyContext;
 import org.apache.storm.topology.OutputFieldsDeclarer;

File: examples/storm-perf/src/main/java/org/apache/storm/perf/bolt/IdBolt.java
Patch:
@@ -19,7 +19,6 @@
 package org.apache.storm.perf.bolt;
 
 import java.util.Map;
-
 import org.apache.storm.task.OutputCollector;
 import org.apache.storm.task.TopologyContext;
 import org.apache.storm.topology.OutputFieldsDeclarer;

File: examples/storm-perf/src/main/java/org/apache/storm/perf/bolt/SplitSentenceBolt.java
Patch:
@@ -19,7 +19,6 @@
 package org.apache.storm.perf.bolt;
 
 import java.util.Map;
-
 import org.apache.storm.task.TopologyContext;
 import org.apache.storm.topology.BasicOutputCollector;
 import org.apache.storm.topology.OutputFieldsDeclarer;

File: examples/storm-perf/src/main/java/org/apache/storm/perf/spout/ConstSpout.java
Patch:
@@ -20,7 +20,6 @@
 
 import java.util.ArrayList;
 import java.util.Map;
-
 import org.apache.storm.spout.SpoutOutputCollector;
 import org.apache.storm.task.TopologyContext;
 import org.apache.storm.topology.OutputFieldsDeclarer;

File: examples/storm-perf/src/main/java/org/apache/storm/perf/spout/FileReadSpout.java
Patch:
@@ -27,7 +27,6 @@
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
-
 import org.apache.storm.spout.SpoutOutputCollector;
 import org.apache.storm.task.TopologyContext;
 import org.apache.storm.topology.OutputFieldsDeclarer;

File: examples/storm-perf/src/main/java/org/apache/storm/perf/utils/IdentityBolt.java
Patch:
@@ -19,7 +19,6 @@
 package org.apache.storm.perf.utils;
 
 import java.util.Map;
-
 import org.apache.storm.task.OutputCollector;
 import org.apache.storm.task.TopologyContext;
 import org.apache.storm.topology.OutputFieldsDeclarer;
@@ -36,7 +35,7 @@ public void prepare(Map<String, Object> topoConf, TopologyContext context, Outpu
 
     @Override
     public void execute(Tuple tuple) {
-        collector.emit(tuple, tuple.getValues() );
+        collector.emit(tuple, tuple.getValues());
         collector.ack(tuple);
     }
 

File: examples/storm-perf/src/main/java/org/apache/storm/perf/utils/MetricsSample.java
Patch:
@@ -20,7 +20,6 @@
 
 import java.util.List;
 import java.util.Map;
-
 import org.apache.storm.generated.ClusterSummary;
 import org.apache.storm.generated.ExecutorSpecificStats;
 import org.apache.storm.generated.ExecutorStats;
@@ -160,7 +159,7 @@ private static MetricsSample getMetricsSample(TopologyInfo topInfo) {
         ret.spoutEmitted = spoutEmitted;
         ret.spoutTransferred = spoutTransferred;
         ret.sampleTime = System.currentTimeMillis();
-//        ret.numSupervisors = clusterSummary.get_supervisors_size();
+        //        ret.numSupervisors = clusterSummary.get_supervisors_size();
         ret.numWorkers = 0;
         ret.numExecutors = 0;
         ret.numTasks = 0;

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/trident/mapper/SimpleTridentHBaseMapMapper.java
Patch:
@@ -37,7 +37,7 @@ public byte[] rowKey(List<Object> keys) {
                 bos.write(String.valueOf(key).getBytes());
             }
             bos.close();
-        } catch (IOException e){
+        } catch (IOException e) {
             throw new RuntimeException("IOException creating HBase row key.", e);
         }
         return bos.toByteArray();

File: external/storm-kafka/src/jvm/org/apache/storm/kafka/TopicOffsetOutOfRangeException.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.kafka;
 
 public class TopicOffsetOutOfRangeException extends RuntimeException {

File: external/storm-redis/src/main/java/org/apache/storm/redis/common/container/RedisClusterContainer.java
Patch:
@@ -18,12 +18,11 @@
 
 package org.apache.storm.redis.common.container;
 
+import java.io.IOException;
 import org.apache.storm.redis.common.adapter.RedisCommandsAdapterJedisCluster;
 import org.apache.storm.redis.common.commands.RedisCommands;
 import redis.clients.jedis.JedisCluster;
 
-import java.io.IOException;
-
 /**
  * Container for managing JedisCluster.
  * <p/>
@@ -34,6 +33,7 @@ public class RedisClusterContainer implements RedisCommandsInstanceContainer {
 
     /**
      * Constructor
+     *
      * @param jedisCluster JedisCluster instance
      */
     public RedisClusterContainer(JedisCluster jedisCluster) {

File: external/storm-redis/src/main/java/org/apache/storm/redis/common/container/RedisContainer.java
Patch:
@@ -18,22 +18,22 @@
 
 package org.apache.storm.redis.common.container;
 
+import java.io.Closeable;
+import java.io.IOException;
 import org.apache.storm.redis.common.adapter.RedisCommandsAdapterJedis;
 import org.apache.storm.redis.common.commands.RedisCommands;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import redis.clients.jedis.JedisPool;
 
-import java.io.Closeable;
-import java.io.IOException;
-
 public class RedisContainer implements RedisCommandsInstanceContainer {
     private static final Logger LOG = LoggerFactory.getLogger(JedisContainer.class);
 
     private JedisPool jedisPool;
 
     /**
      * Constructor
+     *
      * @param jedisPool JedisPool which actually manages Jedis instances
      */
     public RedisContainer(JedisPool jedisPool) {

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/common/rotation/MoveFileAction.java
Patch:
@@ -15,21 +15,21 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.hdfs.common.rotation;
 
+import java.io.IOException;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import java.io.IOException;
-
 public class MoveFileAction implements RotationAction {
     private static final Logger LOG = LoggerFactory.getLogger(MoveFileAction.class);
 
     private String destination;
 
-    public MoveFileAction toDestination(String destDir){
+    public MoveFileAction toDestination(String destDir) {
         destination = destDir;
         return this;
     }

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/common/rotation/RotationAction.java
Patch:
@@ -15,14 +15,14 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.storm.hdfs.common.rotation;
 
+package org.apache.storm.hdfs.common.rotation;
 
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
 
 import java.io.IOException;
 import java.io.Serializable;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
 
 public interface RotationAction extends Serializable {
     void execute(FileSystem fileSystem, Path filePath) throws IOException;

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/trident/HdfsUpdater.java
Patch:
@@ -15,15 +15,15 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.hdfs.trident;
 
+import java.util.List;
 import org.apache.storm.trident.operation.TridentCollector;
 import org.apache.storm.trident.state.BaseStateUpdater;
 import org.apache.storm.trident.tuple.TridentTuple;
 
-import java.util.List;
-
-public class HdfsUpdater extends BaseStateUpdater<HdfsState>{
+public class HdfsUpdater extends BaseStateUpdater<HdfsState> {
     @Override
     public void updateState(HdfsState state, List<TridentTuple> tuples, TridentCollector collector) {
         state.updateState(tuples, collector);

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/bolt/IEventDataFormat.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  *******************************************************************************/
+
 package org.apache.storm.eventhubs.bolt;
 
 import java.io.Serializable;
@@ -24,5 +25,5 @@
  * Serialize a tuple to a byte array to be sent to EventHubs
  */
 public interface IEventDataFormat extends Serializable {
-  public byte[] serialize(Tuple tuple);
+    public byte[] serialize(Tuple tuple);
 }

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/spout/IPartitionCoordinator.java
Patch:
@@ -15,13 +15,14 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  *******************************************************************************/
+
 package org.apache.storm.eventhubs.spout;
 
 import java.util.List;
 
 public interface IPartitionCoordinator {
 
-  List<IPartitionManager> getMyPartitionManagers();
+    List<IPartitionManager> getMyPartitionManagers();
 
-  IPartitionManager getPartitionManager(String partitionId);
+    IPartitionManager getPartitionManager(String partitionId);
 }

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/trident/ITridentPartitionManagerFactory.java
Patch:
@@ -15,12 +15,12 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  *******************************************************************************/
+
 package org.apache.storm.eventhubs.trident;
 
 import java.io.Serializable;
-
 import org.apache.storm.eventhubs.spout.IEventHubReceiver;
 
 public interface ITridentPartitionManagerFactory extends Serializable {
-  ITridentPartitionManager create(IEventHubReceiver receiver);
+    ITridentPartitionManager create(IEventHubReceiver receiver);
 }

File: storm-core/src/jvm/org/apache/storm/testing/staticmocking/MockedClientZookeeper.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.testing.staticmocking;
 
 import org.apache.storm.zookeeper.ClientZookeeper;

File: storm-core/src/jvm/org/apache/storm/ui/InvalidRequestException.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.ui;
 
 public class InvalidRequestException extends Exception {

File: storm-core/test/jvm/org/apache/storm/utils/staticmocking/ConfigUtilsInstaller.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.utils.staticmocking;
 
 import org.apache.storm.utils.ConfigUtils;
@@ -35,7 +36,7 @@ public ConfigUtilsInstaller(ConfigUtils instance) {
     public void close() throws Exception {
         if (ConfigUtils.setInstance(_oldInstance) != _curInstance) {
             throw new IllegalStateException(
-                    "Instances of this resource must be closed in reverse order of opening.");
+                "Instances of this resource must be closed in reverse order of opening.");
         }
     }
 }

File: storm-core/test/jvm/org/apache/storm/utils/staticmocking/ReflectionUtilsInstaller.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.utils.staticmocking;
 
 import org.apache.storm.utils.ReflectionUtils;
@@ -33,7 +34,7 @@ public ReflectionUtilsInstaller(ReflectionUtils instance) {
     public void close() throws Exception {
         if (ReflectionUtils.setInstance(_oldInstance) != _curInstance) {
             throw new IllegalStateException(
-                    "Instances of this resource must be closed in reverse order of opening.");
+                "Instances of this resource must be closed in reverse order of opening.");
         }
     }
 }

File: storm-core/test/jvm/org/apache/storm/utils/staticmocking/ServerConfigUtilsInstaller.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.utils.staticmocking;
 
 import org.apache.storm.utils.ServerConfigUtils;
@@ -35,7 +36,7 @@ public ServerConfigUtilsInstaller(ServerConfigUtils instance) {
     public void close() throws Exception {
         if (ServerConfigUtils.setInstance(_oldInstance) != _curInstance) {
             throw new IllegalStateException(
-                    "Instances of this resource must be closed in reverse order of opening.");
+                "Instances of this resource must be closed in reverse order of opening.");
         }
     }
 }

File: storm-core/test/jvm/org/apache/storm/utils/staticmocking/UtilsInstaller.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.utils.staticmocking;
 
 import org.apache.storm.utils.Utils;
@@ -33,7 +34,7 @@ public UtilsInstaller(Utils instance) {
     public void close() throws Exception {
         if (Utils.setInstance(_oldInstance) != _curInstance) {
             throw new IllegalStateException(
-                    "Instances of this resource must be closed in reverse order of opening.");
+                "Instances of this resource must be closed in reverse order of opening.");
         }
     }
 }

File: storm-server/src/main/java/org/apache/storm/daemon/drpc/BlockingOutstandingRequest.java
Patch:
@@ -15,17 +15,17 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.daemon.drpc;
 
 import java.util.concurrent.Semaphore;
-
 import org.apache.storm.generated.DRPCExceptionType;
 import org.apache.storm.generated.DRPCExecutionException;
 import org.apache.storm.generated.DRPCRequest;
 
 public class BlockingOutstandingRequest extends OutstandingRequest {
     public static final RequestFactory<BlockingOutstandingRequest> FACTORY =
-            (function, request) -> new BlockingOutstandingRequest(function, request);
+        (function, request) -> new BlockingOutstandingRequest(function, request);
     private Semaphore _sem;
     private volatile String _result = null;
     private volatile DRPCExecutionException _e = null;
@@ -49,7 +49,7 @@ public String getResult() throws DRPCExecutionException {
         if (_e == null) {
             _e = new DRPCExecutionException("Internal Error: No Result and No Exception");
             _e.set_type(DRPCExceptionType.INTERNAL_ERROR);
-        } 
+        }
         throw _e;
     }
 

File: storm-server/src/main/java/org/apache/storm/daemon/drpc/DRPCThrift.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.daemon.drpc;
 
 import org.apache.storm.generated.AuthorizationException;
@@ -52,7 +53,7 @@ public void failRequestV2(String id, DRPCExecutionException e) throws Authorizat
 
     @Override
     public String execute(String functionName, String funcArgs)
-            throws DRPCExecutionException, AuthorizationException {
+        throws DRPCExecutionException, AuthorizationException {
         return _drpc.executeBlocking(functionName, funcArgs);
     }
 }

File: storm-server/src/main/java/org/apache/storm/daemon/drpc/OutstandingRequest.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.daemon.drpc;
 
 import org.apache.storm.generated.DRPCExecutionException;
@@ -42,7 +43,7 @@ public void fetched() {
     }
 
     public boolean wasFetched() {
-        return _fetched; 
+        return _fetched;
     }
 
     public String getFunction() {
@@ -54,5 +55,6 @@ public boolean isTimedOut(long timeoutMs) {
     }
 
     public abstract void returnResult(String result);
+
     public abstract void fail(DRPCExecutionException e);
 }

File: storm-server/src/main/java/org/apache/storm/daemon/drpc/RequestFactory.java
Patch:
@@ -15,10 +15,11 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.daemon.drpc;
 
 import org.apache.storm.generated.DRPCRequest;
 
 public interface RequestFactory<T extends OutstandingRequest> {
-    public T mkRequest(String function, DRPCRequest req); 
+    public T mkRequest(String function, DRPCRequest req);
 }

File: storm-server/src/main/java/org/apache/storm/daemon/metrics/MetricsUtils.java
Patch:
@@ -22,7 +22,6 @@
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
-
 import org.apache.storm.DaemonConfig;
 import org.apache.storm.daemon.metrics.reporters.JmxPreparableReporter;
 import org.apache.storm.daemon.metrics.reporters.PreparableReporter;

File: storm-server/src/main/java/org/apache/storm/daemon/supervisor/timer/SupervisorHealthCheck.java
Patch:
@@ -19,7 +19,6 @@
 package org.apache.storm.daemon.supervisor.timer;
 
 import java.util.Map;
-
 import org.apache.storm.daemon.supervisor.Supervisor;
 import org.apache.storm.healthcheck.HealthChecker;
 import org.slf4j.Logger;

File: storm-server/src/main/java/org/apache/storm/scheduler/EvenScheduler.java
Patch:
@@ -20,7 +20,6 @@
 
 import com.google.common.annotations.VisibleForTesting;
 import com.google.common.collect.Sets;
-
 import java.util.ArrayList;
 import java.util.Collections;
 import java.util.Comparator;
@@ -30,7 +29,6 @@
 import java.util.Map;
 import java.util.Set;
 import java.util.TreeMap;
-
 import org.apache.storm.utils.ServerUtils;
 import org.apache.storm.utils.Utils;
 import org.slf4j.Logger;
@@ -116,7 +114,7 @@ private static Map<ExecutorDetails, WorkerSlot> scheduleTopology(TopologyDetails
 
         //allow requesting slots number bigger than available slots
         int toIndex = (totalSlotsToUse - aliveAssigned.size())
-            > sortedList.size() ? sortedList.size() : (totalSlotsToUse - aliveAssigned.size());
+                      > sortedList.size() ? sortedList.size() : (totalSlotsToUse - aliveAssigned.size());
         List<WorkerSlot> reassignSlots = sortedList.subList(0, toIndex);
 
         Set<ExecutorDetails> aliveExecutors = new HashSet<ExecutorDetails>();

File: storm-server/src/main/java/org/apache/storm/scheduler/ExecutorDetails.java
Patch:
@@ -40,16 +40,16 @@ public boolean equals(Object other) {
         if (other == null || !(other instanceof ExecutorDetails)) {
             return false;
         }
-        
-        ExecutorDetails executor = (ExecutorDetails)other;
+
+        ExecutorDetails executor = (ExecutorDetails) other;
         return (this.startTask == executor.startTask) && (this.endTask == executor.endTask);
     }
 
     @Override
     public int hashCode() {
         return this.startTask + 13 * this.endTask;
     }
-    
+
     @Override
     public String toString() {
         return "[" + this.startTask + ", " + this.endTask + "]";

File: storm-server/src/main/java/org/apache/storm/scheduler/SingleTopologyCluster.java
Patch:
@@ -27,7 +27,7 @@ public class SingleTopologyCluster extends Cluster {
     /**
      * Create a new cluster that only allows modifications to a single topology.
      *
-     * @param other the current cluster to base this off of
+     * @param other      the current cluster to base this off of
      * @param topologyId the topology that is allowed to be modified.
      */
     public SingleTopologyCluster(Cluster other, String topologyId) {

File: storm-server/src/main/java/org/apache/storm/scheduler/SupervisorResources.java
Patch:
@@ -31,8 +31,8 @@ public class SupervisorResources {
      *
      * @param totalMem the total mem on the supervisor
      * @param totalCpu the total CPU on the supervisor
-     * @param usedMem the used mem on the supervisor
-     * @param usedCpu the used CPU on the supervisor
+     * @param usedMem  the used mem on the supervisor
+     * @param usedCpu  the used CPU on the supervisor
      */
     public SupervisorResources(double totalMem, double totalCpu, double usedMem, double usedCpu) {
         this.totalMem = totalMem;

File: storm-client/src/jvm/org/apache/storm/cluster/ZKStateStorageFactory.java
Patch:
@@ -18,11 +18,8 @@
 
 package org.apache.storm.cluster;
 
-import java.util.List;
 import java.util.Map;
-
 import org.apache.storm.utils.Utils;
-import org.apache.zookeeper.data.ACL;
 
 public class ZKStateStorageFactory implements StateStorageFactory {
 

File: storm-client/src/jvm/org/apache/storm/daemon/worker/WorkerTransfer.java
Patch:
@@ -62,11 +62,11 @@ public WorkerTransfer(WorkerState workerState, Map<String, Object> topologyConf,
         Integer xferBatchSz = ObjectReader.getInt(topologyConf.get(Config.TOPOLOGY_TRANSFER_BATCH_SIZE));
         if (xferBatchSz > xferQueueSz / 2) {
             throw new IllegalArgumentException(Config.TOPOLOGY_TRANSFER_BATCH_SIZE + ":" + xferBatchSz + " must be no more than half of "
-                + Config.TOPOLOGY_TRANSFER_BUFFER_SIZE + ":" + xferQueueSz);
+                                               + Config.TOPOLOGY_TRANSFER_BUFFER_SIZE + ":" + xferQueueSz);
         }
 
         this.transferQueue = new JCQueue("worker-transfer-queue", xferQueueSz, 0, xferBatchSz, backPressureWaitStrategy,
-                workerState.getTopologyId(), Constants.SYSTEM_COMPONENT_ID, -1, workerState.getPort());
+                                         workerState.getTopologyId(), Constants.SYSTEM_COMPONENT_ID, -1, workerState.getPort());
     }
 
     public JCQueue getTransferQueue() {
@@ -109,7 +109,7 @@ public void flush() throws InterruptedException {
 
     /* Not a Blocking call. If cannot emit, will add 'tuple' to 'pendingEmits' and return 'false'. 'pendingEmits' can be null */
     public boolean tryTransferRemote(AddressedTuple addressedTuple, Queue<AddressedTuple> pendingEmits, ITupleSerializer serializer) {
-        if (pendingEmits != null  &&  !pendingEmits.isEmpty()) {
+        if (pendingEmits != null && !pendingEmits.isEmpty()) {
             pendingEmits.add(addressedTuple);
             return false;
         }

File: storm-client/src/jvm/org/apache/storm/dependency/DependencyBlobStoreUtils.java
Patch:
@@ -18,9 +18,8 @@
 
 package org.apache.storm.dependency;
 
-import org.apache.commons.lang.StringUtils;
-
 import java.util.UUID;
+import org.apache.commons.lang.StringUtils;
 
 public class DependencyBlobStoreUtils {
 

File: storm-client/src/jvm/org/apache/storm/hooks/SubmitterHookException.java
Patch:
@@ -16,6 +16,7 @@
  * specific language governing permissions and limitations
  * under the License.
  */
+
 package org.apache.storm.hooks;
 
 /**

File: storm-server/src/test/java/org/apache/storm/localizer/AsyncLocalizerTest.java
Patch:
@@ -51,6 +51,7 @@
 import java.util.Map;
 import java.util.UUID;
 import java.util.concurrent.CompletableFuture;
+import java.util.concurrent.ConcurrentHashMap;
 import java.util.concurrent.ConcurrentMap;
 import java.util.concurrent.Future;
 import java.util.concurrent.TimeUnit;
@@ -298,11 +299,11 @@ void setTargetCacheSize(long size) {
         }
 
         // For testing, be careful as it doesn't clone
-        ConcurrentMap<String, ConcurrentMap<String, LocalizedResource>> getUserFiles() {
+        ConcurrentHashMap<String, ConcurrentHashMap<String, LocalizedResource>> getUserFiles() {
             return userFiles;
         }
 
-        ConcurrentMap<String, ConcurrentMap<String, LocalizedResource>> getUserArchives() {
+        ConcurrentHashMap<String, ConcurrentHashMap<String, LocalizedResource>> getUserArchives() {
             return userArchives;
         }
 

File: storm-server/src/main/java/org/apache/storm/localizer/LocallyCachedBlob.java
Patch:
@@ -30,7 +30,6 @@
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.TimeUnit;
 import java.util.function.Function;
-import org.apache.storm.blobstore.BlobStore;
 import org.apache.storm.blobstore.ClientBlobStore;
 import org.apache.storm.blobstore.InputStreamWithMeta;
 import org.apache.storm.daemon.supervisor.IAdvancedFSOps;
@@ -197,7 +196,7 @@ public synchronized boolean isUsed() {
      * @param pna the slot and assignment that are using this blob.
      * @param cb an optional callback indicating that they want to know/synchronize when a blob is updated.
      */
-    public void addReference(final PortAndAssignment pna, BlobChangingCallback cb) {
+    public synchronized void addReference(final PortAndAssignment pna, BlobChangingCallback cb) {
         if (cb == null) {
             cb = NOOP_CB;
         }
@@ -210,7 +209,7 @@ public void addReference(final PortAndAssignment pna, BlobChangingCallback cb) {
      * Removes a reservation for this blob from a given slot and assignemnt.
      * @param pna the slot + assignment that no longer needs this blob.
      */
-    public void removeReference(final PortAndAssignment pna) {
+    public synchronized void removeReference(final PortAndAssignment pna) {
         if (references.remove(pna) == null) {
             LOG.warn("{} had no reservation for {}", pna, blobDescription);
         }

File: integration-test/src/test/java/org/apache/storm/st/DemoTest.java
Patch:
@@ -54,6 +54,7 @@ public void testExclamationTopology() throws Exception {
         topo.submitSuccessfully();
         final int minExclaim2Emits = 500;
         final int minSpountEmits = 10000;
+        //Keep the check time to be gt Config.EXECUTOR_METRICS_FREQUENCY_SECS.
         for(int i = 0; i < 10; ++i) {
             TopologyInfo topologyInfo = topo.getInfo();
             log.info(topologyInfo.toString());

File: integration-test/src/test/java/org/apache/storm/st/DemoTest.java
Patch:
@@ -54,6 +54,7 @@ public void testExclamationTopology() throws Exception {
         topo.submitSuccessfully();
         final int minExclaim2Emits = 500;
         final int minSpountEmits = 10000;
+        //Keep the check time to be gt Config.EXECUTOR_METRICS_FREQUENCY_SECS.
         for(int i = 0; i < 10; ++i) {
             TopologyInfo topologyInfo = topo.getInfo();
             log.info(topologyInfo.toString());

File: storm-server/src/test/java/org/apache/storm/TickTupleTest.java
Patch:
@@ -55,8 +55,9 @@ public void testTickTupleWorksWithSystemBolt() throws Exception {
             try (ILocalTopology topo = cluster.submitTopology("test", topoConf,  topology)) {
                 //Give the topology some time to come up
                 long time = 0;
+                int timeout = Math.max(Testing.TEST_TIMEOUT_MS, 100_000);
                 while (tickTupleTimes.size() <= 0) {
-                    assert time <= 100_000 : "took over " + time + " ms of simulated time to get a message back...";
+                    assert time <= timeout : "took over " + time + " ms of simulated time to get a message back...";
                     cluster.advanceClusterTime(10);
                     time += 10_000;
                 }

File: storm-server/src/test/java/org/apache/storm/TickTupleTest.java
Patch:
@@ -55,8 +55,9 @@ public void testTickTupleWorksWithSystemBolt() throws Exception {
             try (ILocalTopology topo = cluster.submitTopology("test", topoConf,  topology)) {
                 //Give the topology some time to come up
                 long time = 0;
+                int timeout = Math.max(Testing.TEST_TIMEOUT_MS, 100_000);
                 while (tickTupleTimes.size() <= 0) {
-                    assert time <= 100_000 : "took over " + time + " ms of simulated time to get a message back...";
+                    assert time <= timeout : "took over " + time + " ms of simulated time to get a message back...";
                     cluster.advanceClusterTime(10);
                     time += 10_000;
                 }

File: examples/storm-hbase-examples/src/main/java/org/apache/storm/hbase/topology/TotalWordCounter.java
Patch:
@@ -37,7 +37,6 @@ public class TotalWordCounter implements IBasicBolt {
     private BigInteger total = BigInteger.ZERO;
     private static final Logger LOG = LoggerFactory.getLogger(TotalWordCounter.class);
     private static final Random RANDOM = new Random();
-    @SuppressWarnings("rawtypes")
     public void prepare(Map<String, Object> topoConf, TopologyContext context) {
     }
 

File: examples/storm-hbase-examples/src/main/java/org/apache/storm/hbase/topology/WordCounter.java
Patch:
@@ -31,7 +31,6 @@
 public class WordCounter implements IBasicBolt {
 
 
-    @SuppressWarnings("rawtypes")
     public void prepare(Map<String, Object> topoConf, TopologyContext context) {
     }
 

File: examples/storm-hbase-examples/src/main/java/org/apache/storm/hbase/topology/WordSpout.java
Patch:
@@ -45,7 +45,6 @@ public boolean isDistributed() {
         return this.isDistributed;
     }
 
-    @SuppressWarnings("rawtypes")
     public void open(Map<String, Object> conf, TopologyContext context, SpoutOutputCollector collector) {
         this.collector = collector;
     }

File: examples/storm-hdfs-examples/src/main/java/org/apache/storm/hdfs/spout/HdfsSpoutTopology.java
Patch:
@@ -132,7 +132,7 @@ public static void main(String[] args) throws Exception {
     builder.setBolt(BOLT_ID, bolt, 1).shuffleGrouping(SPOUT_ID);
 
     // 4 - submit topology, wait for a few min and terminate it
-    Map clusterConf = Utils.readStormConfig();
+    Map<String, Object> clusterConf = Utils.readStormConfig();
     StormSubmitter.submitTopologyWithProgressBar(topologyName, conf, builder.createTopology());
     Nimbus.Iface client = NimbusClient.getConfiguredClient(clusterConf).getClient();
 

File: examples/storm-hive-examples/src/main/java/org/apache/storm/hive/trident/TridentHiveTopology.java
Patch:
@@ -177,7 +177,7 @@ public void close() {
         }
 
         @Override
-        public Map getComponentConfiguration() {
+        public Map<String, Object> getComponentConfiguration() {
             Config conf = new Config();
             conf.setMaxTaskParallelism(1);
             return conf;

File: examples/storm-jdbc-examples/src/main/java/org/apache/storm/jdbc/spout/UserSpout.java
Patch:
@@ -47,7 +47,6 @@ public boolean isDistributed() {
         return this.isDistributed;
     }
 
-    @SuppressWarnings("rawtypes")
     public void open(Map<String, Object> conf, TopologyContext context, SpoutOutputCollector collector) {
         this.collector = collector;
     }

File: examples/storm-jdbc-examples/src/main/java/org/apache/storm/jdbc/topology/AbstractUserTopology.java
Patch:
@@ -71,7 +71,7 @@ public void execute(String[] args) throws Exception {
                     + "<user> <password> [topology name]");
             System.exit(-1);
         }
-        Map map = Maps.newHashMap();
+        Map<String, Object> map = Maps.newHashMap();
         map.put("dataSourceClassName", args[0]);//com.mysql.jdbc.jdbc2.optional.MysqlDataSource
         map.put("dataSource.url", args[1]);//jdbc:mysql://localhost/test
         map.put("dataSource.user", args[2]);//root

File: examples/storm-jms-examples/src/main/java/org/apache/storm/jms/example/GenericBolt.java
Patch:
@@ -67,7 +67,6 @@ public GenericBolt(String name, boolean autoAck, boolean autoAnchor) {
         this(name, autoAck, autoAnchor, null);
     }
 
-    @SuppressWarnings("rawtypes")
     public void prepare(Map<String, Object> topoConf, TopologyContext context,
                         OutputCollector collector) {
         this.collector = collector;

File: examples/storm-loadgen/src/main/java/org/apache/storm/loadgen/CaptureLoad.java
Patch:
@@ -435,7 +435,7 @@ static Map<String, Double> parseResources(String input) {
      * @param topologyConf topology configuration
      * @throws Exception on any error
      */
-    public static void checkInitialization(Map<String, Double> topologyResources, String componentId, Map topologyConf) {
+    public static void checkInitialization(Map<String, Double> topologyResources, String componentId, Map<String, Object> topologyConf) {
         StringBuilder msgBuilder = new StringBuilder();
 
         for (String resourceName : topologyResources.keySet()) {
@@ -450,7 +450,7 @@ public static void checkInitialization(Map<String, Double> topologyResources, St
         }
     }
 
-    private static String checkInitResource(Map<String, Double> topologyResources, Map topologyConf, String resourceName) {
+    private static String checkInitResource(Map<String, Double> topologyResources, Map<String, Object> topologyConf, String resourceName) {
         StringBuilder msgBuilder = new StringBuilder();
         if (topologyResources.containsKey(resourceName)) {
             Double resourceValue = (Double) topologyConf.getOrDefault(resourceName, null);

File: examples/storm-loadgen/src/main/java/org/apache/storm/loadgen/HttpForwardingMetricsServer.java
Patch:
@@ -41,7 +41,7 @@
  * A server that can listen for metrics from the HttpForwardingMetricsConsumer.
  */
 public abstract class HttpForwardingMetricsServer {
-    private Map conf;
+    private Map<String, Object> conf;
     private Server server = null;
     private int port = -1;
     private String url = null;

File: examples/storm-loadgen/src/main/java/org/apache/storm/loadgen/ThroughputVsLatency.java
Patch:
@@ -97,7 +97,8 @@ public SplitSentence(SlowExecutorPattern slowness) {
         }
 
         @Override
-        public void prepare(Map stormConf, TopologyContext context) {
+        public void prepare(Map<String, Object> stormConf,
+                TopologyContext context) {
             executorIndex = context.getThisTaskIndex();
             sleep.prepare();
         }

File: examples/storm-mongodb-examples/src/main/java/org/apache/storm/mongodb/topology/TotalWordCounter.java
Patch:
@@ -37,7 +37,6 @@ public class TotalWordCounter implements IBasicBolt {
     private BigInteger total = BigInteger.ZERO;
     private static final Logger LOG = LoggerFactory.getLogger(TotalWordCounter.class);
     private static final Random RANDOM = new Random();
-    @SuppressWarnings("rawtypes")
     public void prepare(Map<String, Object> topoConf, TopologyContext context) {
     }
 

File: examples/storm-mongodb-examples/src/main/java/org/apache/storm/mongodb/topology/WordSpout.java
Patch:
@@ -45,7 +45,6 @@ public boolean isDistributed() {
         return this.isDistributed;
     }
 
-    @SuppressWarnings("rawtypes")
     public void open(Map<String, Object> conf, TopologyContext context, SpoutOutputCollector collector) {
         this.collector = collector;
     }

File: examples/storm-perf/src/main/java/org/apache/storm/perf/SimplifiedWordCountTopo.java
Patch:
@@ -44,7 +44,7 @@ public class SimplifiedWordCountTopo {
     public static final int DEFAULT_COUNT_BOLT_NUM = 1;
 
 
-    static StormTopology getTopology(Map config) {
+    static StormTopology getTopology(Map<String, Object> config) {
 
         final int spoutNum = Helper.getInt(config, SPOUT_NUM, DEFAULT_SPOUT_NUM);
         final int cntBoltNum = Helper.getInt(config, BOLT_NUM, DEFAULT_COUNT_BOLT_NUM);

File: examples/storm-perf/src/main/java/org/apache/storm/perf/utils/Helper.java
Patch:
@@ -70,7 +70,7 @@ public static void collectMetricsAndKill(String topologyName, Integer pollInterv
 
     /** Kill topo on Ctrl-C */
     public static void setupShutdownHook(final String topoName) {
-        Map clusterConf = Utils.readStormConfig();
+        Map<String, Object> clusterConf = Utils.readStormConfig();
         final Nimbus.Iface client = NimbusClient.getConfiguredClient(clusterConf).getClient();
         Runtime.getRuntime().addShutdownHook(new Thread() {
             public void run() {

File: examples/storm-redis-examples/src/main/java/org/apache/storm/redis/topology/WordCounter.java
Patch:
@@ -31,7 +31,6 @@
 public class WordCounter implements IBasicBolt {
     private Map<String, Integer> wordCounter = Maps.newHashMap();
 
-    @SuppressWarnings("rawtypes")
     public void prepare(Map<String, Object> topoConf, TopologyContext context) {
     }
 

File: examples/storm-redis-examples/src/main/java/org/apache/storm/redis/topology/WordSpout.java
Patch:
@@ -45,7 +45,6 @@ public boolean isDistributed() {
         return this.isDistributed;
     }
 
-    @SuppressWarnings("rawtypes")
     public void open(Map<String, Object> conf, TopologyContext context, SpoutOutputCollector collector) {
         this.collector = collector;
     }

File: examples/storm-starter/src/jvm/org/apache/storm/starter/FastWordCountTopology.java
Patch:
@@ -183,7 +183,7 @@ public static void main(String[] args) throws Exception {
     conf.setNumWorkers(1);
     StormSubmitter.submitTopologyWithProgressBar(name, conf, builder.createTopology());
 
-    Map clusterConf = Utils.readStormConfig();
+    Map<String, Object> clusterConf = Utils.readStormConfig();
     clusterConf.putAll(Utils.readCommandLineOpts());
     Nimbus.Iface client = NimbusClient.getConfiguredClient(clusterConf).getClient();
 

File: examples/storm-starter/src/jvm/org/apache/storm/starter/InOrderDeliveryTest.java
Patch:
@@ -159,7 +159,7 @@ public static void main(String[] args) throws Exception {
     conf.setNumWorkers(1);
     StormSubmitter.submitTopologyWithProgressBar(name, conf, builder.createTopology());
 
-    Map clusterConf = Utils.readStormConfig();
+    Map<String, Object> clusterConf = Utils.readStormConfig();
     clusterConf.putAll(Utils.readCommandLineOpts());
     Nimbus.Iface client = NimbusClient.getConfiguredClient(clusterConf).getClient();
 

File: examples/storm-starter/src/jvm/org/apache/storm/starter/ResourceAwareExampleTopology.java
Patch:
@@ -64,7 +64,7 @@ protected static void addToCache(String key, String value) {
         }
 
         @Override
-        public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
+        public void prepare(Map<String, Object> conf, TopologyContext context, OutputCollector collector) {
             _collector = collector;
         }
 

File: examples/storm-starter/src/jvm/org/apache/storm/starter/bolt/RollingCountAggBolt.java
Patch:
@@ -44,7 +44,6 @@ public class RollingCountAggBolt extends BaseRichBolt {
   private OutputCollector collector;
 
 
-  @SuppressWarnings("rawtypes")
   @Override
   public void prepare(Map<String, Object> topoConf, TopologyContext context, OutputCollector collector) {
     this.collector = collector;

File: examples/storm-starter/src/jvm/org/apache/storm/starter/bolt/RollingCountBolt.java
Patch:
@@ -85,7 +85,6 @@ private int deriveNumWindowChunksFrom(int windowLengthInSeconds, int windowUpdat
     return windowLengthInSeconds / windowUpdateFrequencyInSeconds;
   }
 
-  @SuppressWarnings("rawtypes")
   @Override
   public void prepare(Map<String, Object> topoConf, TopologyContext context, OutputCollector collector) {
     this.collector = collector;

File: external/storm-autocreds/src/main/java/org/apache/storm/common/AbstractHadoopAutoCreds.java
Patch:
@@ -45,7 +45,7 @@ public abstract class AbstractHadoopAutoCreds implements IAutoCredentials, Crede
     private Set<String> configKeys = new HashSet<>();
 
     @Override
-    public void prepare(Map topoConf) {
+    public void prepare(Map<String, Object> topoConf) {
         doPrepare(topoConf);
         loadConfigKeys(topoConf);
     }
@@ -83,7 +83,7 @@ public Set<Pair<String, Credentials>> getCredentials(Map<String, String> credent
      *
      * @param topoConf the topology conf
      */
-    protected abstract void doPrepare(Map topoConf);
+    protected abstract void doPrepare(Map<String, Object> topoConf);
 
     /**
      * The lookup key for the config key string

File: external/storm-autocreds/src/main/java/org/apache/storm/hbase/security/AutoHBase.java
Patch:
@@ -30,7 +30,7 @@
  */
 public class AutoHBase extends AbstractHadoopAutoCreds {
     @Override
-    public void doPrepare(Map conf) {
+    public void doPrepare(Map<String, Object> conf) {
     }
 
     @Override

File: external/storm-autocreds/src/main/java/org/apache/storm/hbase/security/AutoHBaseCommand.java
Patch:
@@ -39,7 +39,7 @@ private AutoHBaseCommand() {
 
   @SuppressWarnings("unchecked")
   public static void main(String[] args) throws Exception {
-    Map conf = new HashMap();
+    Map<String, Object> conf = new HashMap<>();
     conf.put(HBASE_PRINCIPAL_KEY, args[1]); // hbase principal storm-hbase@WITZEN.COM
     conf.put(HBASE_KEYTAB_FILE_KEY,
         args[2]); // storm hbase keytab /etc/security/keytabs/storm-hbase.keytab

File: external/storm-autocreds/src/main/java/org/apache/storm/hdfs/security/AutoHDFS.java
Patch:
@@ -30,7 +30,7 @@
  */
 public class AutoHDFS extends AbstractHadoopAutoCreds {
     @Override
-    public void doPrepare(Map conf) {
+    public void doPrepare(Map<String, Object> conf) {
     }
 
     @Override

File: external/storm-autocreds/src/main/java/org/apache/storm/hdfs/security/AutoHDFSCommand.java
Patch:
@@ -39,7 +39,7 @@ private AutoHDFSCommand() {
 
   @SuppressWarnings("unchecked")
   public static void main(String[] args) throws Exception {
-    Map conf = new HashMap();
+    Map<String, Object> conf = new HashMap<>();
     conf.put(STORM_USER_NAME_KEY, args[1]); //with realm e.g. hdfs@WITZEND.COM
     conf.put(STORM_KEYTAB_FILE_KEY, args[2]);// /etc/security/keytabs/storm.keytab
 

File: external/storm-autocreds/src/main/java/org/apache/storm/hdfs/security/HdfsSecurityUtil.java
Patch:
@@ -49,7 +49,7 @@ public final class HdfsSecurityUtil {
     private HdfsSecurityUtil() {
     }
 
-    public static void login(Map conf, Configuration hdfsConfig) throws IOException {
+    public static void login(Map<String, Object> conf, Configuration hdfsConfig) throws IOException {
         //If AutoHDFS is specified, do not attempt to login using keytabs, only kept for backward compatibility.
         if(conf.get(TOPOLOGY_AUTO_CREDENTIALS) == null ||
                 (!(((List)conf.get(TOPOLOGY_AUTO_CREDENTIALS)).contains(AutoHDFS.class.getName())) &&

File: external/storm-autocreds/src/main/java/org/apache/storm/hive/security/AutoHive.java
Patch:
@@ -31,7 +31,7 @@
  */
 public class AutoHive extends AbstractHadoopAutoCreds {
     @Override
-    public void doPrepare(Map conf) {
+    public void doPrepare(Map<String, Object> conf) {
     }
 
     @Override

File: external/storm-autocreds/src/main/java/org/apache/storm/hive/security/AutoHiveCommand.java
Patch:
@@ -40,7 +40,7 @@ private AutoHiveCommand() {
 
   @SuppressWarnings("unchecked")
   public static void main(String[] args) throws Exception {
-    Map conf = new HashMap();
+    Map<String, Object> conf = new HashMap<>();
     conf.put(HIVE_PRINCIPAL_KEY, args[1]); // hive principal storm-hive@WITZEN.COM
     conf.put(HIVE_KEYTAB_FILE_KEY, args[2]); // storm hive keytab /etc/security/keytabs/storm-hive.keytab
     conf.put(HiveConf.ConfVars.METASTOREURIS.varname, args[3]); // hive.metastore.uris : "thrift://pm-eng1-cluster1.field.hortonworks.com:9083"

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/query/impl/ObjectMapperCqlStatementMapper.java
Patch:
@@ -67,7 +67,7 @@ public ObjectMapperCqlStatementMapper(String operationField, String valueField,
     }
 
     @Override
-    public List<Statement> map(Map map, Session session, ITuple tuple) {
+    public List<Statement> map(Map<String, Object> map, Session session, ITuple tuple) {
         final ObjectMapperOperation operation = (ObjectMapperOperation)tuple.getValueByField(operationField);
 
         Preconditions.checkNotNull(operation, "Operation must not be null");

File: external/storm-cassandra/src/test/java/org/apache/storm/cassandra/WeatherSpout.java
Patch:
@@ -56,7 +56,7 @@ public void declareOutputFields(OutputFieldsDeclarer outputFieldsDeclarer) {
     }
 
     @Override
-    public void open(Map map, TopologyContext topologyContext, SpoutOutputCollector spoutOutputCollector) {
+    public void open(Map<String, Object> map, TopologyContext topologyContext, SpoutOutputCollector spoutOutputCollector) {
         this.spoutOutputCollector = spoutOutputCollector;
     }
 

File: external/storm-elasticsearch/src/main/java/org/apache/storm/elasticsearch/bolt/AbstractEsBolt.java
Patch:
@@ -49,7 +49,7 @@ public AbstractEsBolt(EsConfig esConfig) {
     }
 
     @Override
-    public void prepare(Map map, TopologyContext topologyContext, OutputCollector outputCollector) {
+    public void prepare(Map<String, Object> map, TopologyContext topologyContext, OutputCollector outputCollector) {
         try {
             this.collector = outputCollector;
             synchronized (AbstractEsBolt.class) {

File: external/storm-elasticsearch/src/main/java/org/apache/storm/elasticsearch/bolt/EsIndexBolt.java
Patch:
@@ -56,7 +56,7 @@ public EsIndexBolt(EsConfig esConfig, EsTupleMapper tupleMapper) {
     }
 
     @Override
-    public void prepare(Map map, TopologyContext topologyContext, OutputCollector outputCollector) {
+    public void prepare(Map<String, Object> map, TopologyContext topologyContext, OutputCollector outputCollector) {
         super.prepare(map, topologyContext, outputCollector);
     }
 

File: external/storm-elasticsearch/src/main/java/org/apache/storm/elasticsearch/bolt/EsPercolateBolt.java
Patch:
@@ -61,7 +61,7 @@ public EsPercolateBolt(EsConfig esConfig, EsTupleMapper tupleMapper) {
     }
 
     @Override
-    public void prepare(Map map, TopologyContext topologyContext, OutputCollector outputCollector) {
+    public void prepare(Map<String, Object> map, TopologyContext topologyContext, OutputCollector outputCollector) {
         super.prepare(map, topologyContext, outputCollector);
     }
 

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/spout/BinaryEventDataScheme.java
Patch:
@@ -58,8 +58,8 @@ else if (eventData.getObject()!=null) {
 				throw new RuntimeException(e);
 			}
 		}
-		Map metaDataMap =  eventData.getProperties();
-		Map systemMetaDataMap = eventData.getSystemProperties();
+		Map<String, Object> metaDataMap =  eventData.getProperties();
+		Map<String, Object> systemMetaDataMap = eventData.getSystemProperties();
 		fieldContents.add(messageData);
 		fieldContents.add(metaDataMap);
 		fieldContents.add(systemMetaDataMap);

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/spout/EventDataScheme.java
Patch:
@@ -65,8 +65,8 @@ else if (eventData.getObject()!=null) {
 				throw e;
 			}
 		}
-		Map metaDataMap = eventData.getProperties();
-		fieldContents.add(messageData);
+		Map<String, Object> metaDataMap = eventData.getProperties();
+                fieldContents.add(messageData);
 		fieldContents.add(metaDataMap);
 		return fieldContents;
 	}

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/spout/EventHubReceiverImpl.java
Patch:
@@ -156,8 +156,8 @@ public EventDataWrap receive() {
   }
 
   @Override
-  public Map getMetricsData() {
-    Map ret = new HashMap();
+  public Map<String, Object> getMetricsData() {
+    Map<String, Object> ret = new HashMap<>();
     ret.put(partitionId + "/receiveApiLatencyMean", receiveApiLatencyMean.getValueAndReset());
     ret.put(partitionId + "/receiveApiCallCount", receiveApiCallCount.getValueAndReset());
     ret.put(partitionId + "/receiveMessageCount", receiveMessageCount.getValueAndReset());

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/spout/EventHubSpout.java
Patch:
@@ -159,7 +159,7 @@ public void open(Map<String, Object> config, TopologyContext context, SpoutOutpu
     context.registerMetric("EventHubReceiver", new IMetric() {
       @Override
       public Object getValueAndReset() {
-          Map concatMetricsDataMaps = new HashMap();
+          Map<String, Object> concatMetricsDataMaps = new HashMap<>();
           for (IPartitionManager partitionManager : 
             partitionCoordinator.getMyPartitionManagers()) {
             concatMetricsDataMaps.putAll(partitionManager.getMetricsData());

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/spout/IEventHubReceiver.java
Patch:
@@ -29,5 +29,5 @@ public interface IEventHubReceiver {
 
   EventDataWrap receive();
 
-  Map getMetricsData();
+  Map<String, Object> getMetricsData();
 }

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/spout/IPartitionManager.java
Patch:
@@ -33,5 +33,5 @@ public interface IPartitionManager {
 
   void fail(String offset);
   
-  Map getMetricsData();
+  Map<String, Object> getMetricsData();
 }

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/spout/SimplePartitionManager.java
Patch:
@@ -127,7 +127,7 @@ private String getPartitionStatePath() {
   }
   
   @Override
-  public Map getMetricsData() {
+  public Map<String, Object> getMetricsData() {
     return receiver.getMetricsData();
   }
 }

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/trident/TransactionalTridentEventHubSpout.java
Patch:
@@ -31,7 +31,7 @@
  * Transactional Trident EventHub Spout
  */
 public class TransactionalTridentEventHubSpout implements 
-  IPartitionedTridentSpout<Partitions, Partition, Map> {
+  IPartitionedTridentSpout<Partitions, Partition, Map<String, Object>> {
   private static final long serialVersionUID = 1L;
   private final IEventDataScheme scheme;
   private final EventHubSpoutConfig spoutConfig;
@@ -53,7 +53,7 @@ public IPartitionedTridentSpout.Coordinator<Partitions> getCoordinator(
   }
 
   @Override
-  public IPartitionedTridentSpout.Emitter<Partitions, Partition, Map> getEmitter(
+  public IPartitionedTridentSpout.Emitter<Partitions, Partition, Map<String, Object>> getEmitter(
       Map<String, Object> conf, TopologyContext context) {
     return new TransactionalTridentEventHubEmitter(spoutConfig);
   }

File: external/storm-eventhubs/src/test/java/org/apache/storm/eventhubs/samples/bolt/GlobalCountBolt.java
Patch:
@@ -55,7 +55,7 @@ public Object getValueAndReset() {
         long now = System.nanoTime();
         long millis = (now - lastMetricsTime) / 1000000;
         throughput = globalCountDiff / millis * 1000;
-        Map values = new HashMap();
+        Map<String, Object> values = new HashMap<>();
         values.put("global_count", globalCount);
         values.put("throughput", throughput);
         lastMetricsTime = now;

File: external/storm-eventhubs/src/test/java/org/apache/storm/eventhubs/spout/EventHubReceiverMock.java
Patch:
@@ -79,7 +79,7 @@ public EventDataWrap receive() {
   }
   
   @Override
-  public Map getMetricsData() {
+  public Map<String, Object> getMetricsData() {
     return null;
   }
 }

File: external/storm-eventhubs/src/test/java/org/apache/storm/eventhubs/trident/TestTransactionalTridentEmitter.java
Patch:
@@ -62,7 +62,7 @@ public void tearDown() throws Exception {
   @Test
   public void testEmitInSequence() {
     //test the happy path, emit batches in sequence
-    Map meta = emitter.emitPartitionBatchNew(null, collectorMock, partition, null);
+    Map<String, Object> meta = emitter.emitPartitionBatchNew(null, collectorMock, partition, null);
     String collected = collectorMock.getBuffer();
     assertTrue(collected.startsWith("message"+0));
     //System.out.println("collected: " + collected);
@@ -77,11 +77,11 @@ public void testEmitInSequence() {
   @Test
   public void testReEmit() {
     //test we can re-emit the second batch
-    Map meta = emitter.emitPartitionBatchNew(null, collectorMock, partition, null);
+    Map<String, Object> meta = emitter.emitPartitionBatchNew(null, collectorMock, partition, null);
     collectorMock.clear();
     
     //emit second batch
-    Map meta1 = emitter.emitPartitionBatchNew(null, collectorMock, partition, meta);
+    Map<String, Object> meta1 = emitter.emitPartitionBatchNew(null, collectorMock, partition, meta);
     String collected0 = collectorMock.getBuffer();
     collectorMock.clear();
     

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/bolt/HBaseBolt.java
Patch:
@@ -105,7 +105,7 @@ public void execute(Tuple tuple) {
     }
 
     @Override
-    public void prepare(Map map, TopologyContext topologyContext, OutputCollector collector) {
+    public void prepare(Map<String, Object> map, TopologyContext topologyContext, OutputCollector collector) {
         super.prepare(map, topologyContext, collector);
         this.batchHelper = new BatchHelper(batchSize, collector);
     }

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/bolt/HBaseLookupBolt.java
Patch:
@@ -71,7 +71,6 @@ public HBaseLookupBolt withProjectionCriteria(HBaseProjectionCriteria projection
           return this;
      }
 
-     @SuppressWarnings({ "unchecked", "rawtypes" })
      @Override
      public void prepare(Map<String, Object> config, TopologyContext topologyContext, OutputCollector collector) {
           super.prepare(config, topologyContext, collector);

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/state/HBaseKeyValueStateProvider.java
Patch:
@@ -45,7 +45,7 @@ public class HBaseKeyValueStateProvider implements StateProvider {
     private static final Logger LOG = LoggerFactory.getLogger(HBaseKeyValueStateProvider.class);
 
     @Override
-    public State newState(String namespace, Map stormConf, TopologyContext context) {
+    public State newState(String namespace, Map<String, Object> stormConf, TopologyContext context) {
         try {
             return getHBaseKeyValueState(namespace, stormConf, context, getStateConfig(stormConf));
         } catch (Exception ex) {
@@ -81,7 +81,7 @@ private HBaseKeyValueState getHBaseKeyValueState(String namespace, Map<String, O
 
         //heck for backward compatibility, we need to pass TOPOLOGY_AUTO_CREDENTIALS to hbase conf
         //the conf instance is instance of persistentMap so making a copy.
-        Map<String, Object> hbaseConfMap = new HashMap<String, Object>(conf);
+        Map<String, Object> hbaseConfMap = new HashMap<>(conf);
         hbaseConfMap.put(Config.TOPOLOGY_AUTO_CREDENTIALS, stormConf.get(Config.TOPOLOGY_AUTO_CREDENTIALS));
         HBaseClient hbaseClient = new HBaseClient(hbaseConfMap, hbConfig, config.tableName);
 

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/trident/state/HBaseMapState.java
Patch:
@@ -86,7 +86,7 @@ public class HBaseMapState<T> implements IBackingMap<T> {
      * @param map topology config map.
      * @param partitionNum the number of partition.
      */
-    public HBaseMapState(final Options<T> options, Map map, int partitionNum) {
+    public HBaseMapState(final Options<T> options, Map<String, Object> map, int partitionNum) {
         this.options = options;
         this.serializer = options.serializer;
         this.partitionNum = partitionNum;

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/trident/state/HBaseState.java
Patch:
@@ -46,11 +46,11 @@ public class HBaseState implements State {
 
     private Options options;
     private HBaseClient hBaseClient;
-    private Map map;
+    private Map<String, Object> map;
     private int numPartitions;
     private int partitionIndex;
 
-    protected HBaseState(Map map, int partitionIndex, int numPartitions, Options options) {
+    protected HBaseState(Map<String, Object> map, int partitionIndex, int numPartitions, Options options) {
         this.options = options;
         this.map = map;
         this.partitionIndex = partitionIndex;

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/trident/state/HBaseStateFactory.java
Patch:
@@ -32,7 +32,7 @@ public HBaseStateFactory(HBaseState.Options options) {
     }
 
     @Override
-    public State makeState(Map map, IMetricsContext iMetricsContext, int partitionIndex, int numPartitions) {
+    public State makeState(Map<String, Object> map, IMetricsContext iMetricsContext, int partitionIndex, int numPartitions) {
         HBaseState state = new HBaseState(map , partitionIndex, numPartitions, options);
         state.prepare();
         return state;

File: external/storm-hdfs/src/test/java/org/apache/storm/hdfs/spout/TestHdfsSpout.java
Patch:
@@ -588,8 +588,8 @@ private static List<String> readTextFile(FileSystem fs, String f) throws IOExcep
         return result;
     }
 
-    private Map getCommonConfigs() {
-        Map<String, Object> topoConf = new HashMap();
+    private Map<String, Object> getCommonConfigs() {
+        Map<String, Object> topoConf = new HashMap<>();
         topoConf.put(Config.TOPOLOGY_ACKER_EXECUTORS, "0");
         return topoConf;
     }

File: external/storm-jdbc/src/main/java/org/apache/storm/jdbc/bolt/AbstractJdbcBolt.java
Patch:
@@ -65,7 +65,7 @@ public abstract class AbstractJdbcBolt extends BaseTickTupleAwareRichBolt {
      * {@inheritDoc}
      */
     @Override
-    public void prepare(final Map map, final TopologyContext topologyContext,
+    public void prepare(final Map<String, Object> map, final TopologyContext topologyContext,
                         final OutputCollector outputCollector) {
         this.collector = outputCollector;
 

File: external/storm-jdbc/src/main/java/org/apache/storm/jdbc/bolt/JdbcInsertBolt.java
Patch:
@@ -74,7 +74,7 @@ public JdbcInsertBolt withQueryTimeoutSecs(int queryTimeoutSecs) {
     }
 
     @Override
-    public void prepare(Map map, TopologyContext topologyContext, OutputCollector collector) {
+    public void prepare(Map<String, Object> map, TopologyContext topologyContext, OutputCollector collector) {
         super.prepare(map, topologyContext, collector);
         if(StringUtils.isBlank(tableName) && StringUtils.isBlank(insertQuery)) {
             throw new IllegalArgumentException("You must supply either a tableName or an insert Query.");

File: external/storm-jdbc/src/main/java/org/apache/storm/jdbc/trident/state/JdbcState.java
Patch:
@@ -46,7 +46,7 @@ public class JdbcState implements State {
     private JdbcClient jdbcClient;
     private Map map;
 
-    protected JdbcState(Map map, int partitionIndex, int numPartitions, Options options) {
+    protected JdbcState(Map<String, Object> map, int partitionIndex, int numPartitions, Options options) {
         this.options = options;
         this.map = map;
     }

File: external/storm-jdbc/src/main/java/org/apache/storm/jdbc/trident/state/JdbcStateFactory.java
Patch:
@@ -32,7 +32,7 @@ public JdbcStateFactory(JdbcState.Options options) {
     }
 
     @Override
-    public State makeState(Map map, IMetricsContext iMetricsContext, int partitionIndex, int numPartitions) {
+    public State makeState(Map<String, Object> map, IMetricsContext iMetricsContext, int partitionIndex, int numPartitions) {
         JdbcState state = new JdbcState(map , partitionIndex, numPartitions, options);
         state.prepare();
         return state;

File: external/storm-jdbc/src/test/java/org/apache/storm/jdbc/common/JdbcClientTest.java
Patch:
@@ -42,7 +42,7 @@ public class JdbcClientTest {
     private static final String tableName = "user_details";
     @Before
     public void setup() {
-        Map map = Maps.newHashMap();
+        Map<String, Object> map = Maps.newHashMap();
         map.put("dataSourceClassName","org.hsqldb.jdbc.JDBCDataSource");//com.mysql.jdbc.jdbc2.optional.MysqlDataSource
         map.put("dataSource.url", "jdbc:hsqldb:mem:test");//jdbc:mysql://localhost/test
         map.put("dataSource.user","SA");//root

File: external/storm-jms/src/main/java/org/apache/storm/jms/trident/JmsStateFactory.java
Patch:
@@ -32,7 +32,7 @@ public JmsStateFactory(JmsState.Options options) {
     }
 
     @Override
-    public State makeState(Map map, IMetricsContext iMetricsContext, int partitionIndex, int numPartitions) {
+    public State makeState(Map<String, Object> map, IMetricsContext iMetricsContext, int partitionIndex, int numPartitions) {
         JmsState state = new JmsState(options);
         state.prepare();
         return state;

File: external/storm-jms/src/main/java/org/apache/storm/jms/trident/TridentJmsSpout.java
Patch:
@@ -163,12 +163,12 @@ private static final String toDeliveryModeString(int acknowledgeMode) {
     
     @Override
     public ITridentSpout.BatchCoordinator<JmsBatch> getCoordinator(
-            String txStateId, @SuppressWarnings("rawtypes") Map<String, Object> conf, TopologyContext context) {
+            String txStateId, Map<String, Object> conf, TopologyContext context) {
         return new JmsBatchCoordinator(name);
     }
 
     @Override
-    public Emitter<JmsBatch> getEmitter(String txStateId, @SuppressWarnings("rawtypes") Map<String, Object> conf, TopologyContext context) {
+    public Emitter<JmsBatch> getEmitter(String txStateId, Map<String, Object> conf, TopologyContext context) {
         return new JmsEmitter(name, jmsProvider, tupleProducer, jmsAcknowledgeMode, conf);
     }
 
@@ -210,7 +210,7 @@ private class JmsEmitter implements Emitter<JmsBatch>, MessageListener {
        
         private final Logger LOG = LoggerFactory.getLogger(JmsEmitter.class);
  
-        public JmsEmitter(String name, JmsProvider jmsProvider, JmsTupleProducer tupleProducer, int jmsAcknowledgeMode, @SuppressWarnings("rawtypes") Map<String, Object> conf) {
+        public JmsEmitter(String name, JmsProvider jmsProvider, JmsTupleProducer tupleProducer, int jmsAcknowledgeMode, Map<String, Object> conf) {
             if (jmsProvider == null) {
                 throw new IllegalStateException("JMS provider has not been set.");
             }

File: external/storm-kafka-client/src/test/java/org/apache/storm/kafka/spout/trident/KafkaTridentSpoutBatchMetadataTest.java
Patch:
@@ -43,7 +43,7 @@ public void testMetadataIsRoundTripSerializableWithJsonSimple() throws Exception
 
         KafkaTridentSpoutBatchMetadata metadata = new KafkaTridentSpoutBatchMetadata(startOffset, endOffset);
         Map<String, Object> map = metadata.toMap();
-        Map deserializedMap = (Map)JSONValue.parseWithException(JSONValue.toJSONString(map));
+        Map<String, Object> deserializedMap = (Map)JSONValue.parseWithException(JSONValue.toJSONString(map));
         KafkaTridentSpoutBatchMetadata deserializedMetadata = KafkaTridentSpoutBatchMetadata.fromMap(deserializedMap);
         assertThat(deserializedMetadata.getFirstOffset(), is(metadata.getFirstOffset()));
         assertThat(deserializedMetadata.getLastOffset(), is(metadata.getLastOffset()));

File: external/storm-kafka/src/jvm/org/apache/storm/kafka/KafkaSpout.java
Patch:
@@ -111,7 +111,7 @@ public Object getValueAndReset() {
             @Override
             public Object getValueAndReset() {
                 List<PartitionManager> pms = _coordinator.getMyManagedPartitions();
-                Map concatMetricsDataMaps = new HashMap();
+                Map<String, Object> concatMetricsDataMaps = new HashMap<>();
                 for (PartitionManager pm : pms) {
                     concatMetricsDataMaps.putAll(pm.getMetricsDataMap());
                 }

File: external/storm-kafka/src/jvm/org/apache/storm/kafka/PartitionManager.java
Patch:
@@ -165,7 +165,7 @@ public PartitionManager(
         _messageIneligibleForRetryCount = new CountMetric();
     }
 
-    public Map getMetricsDataMap() {
+    public Map<String, Object> getMetricsDataMap() {
         String metricPrefix = _partition.getId();
 
         Map<String, Object> ret = new HashMap<>();

File: external/storm-kafka/src/test/org/apache/storm/kafka/ZkCoordinatorTest.java
Patch:
@@ -63,8 +63,8 @@ public void setUp() throws Exception {
         when(dynamicPartitionConnections.register(any(Broker.class), any(String.class) ,anyInt())).thenReturn(simpleConsumer);
     }
 
-    private Map buildZookeeperConfig(TestingServer server) {
-        Map<String, Object> conf = new HashMap();
+    private Map<String, Object> buildZookeeperConfig(TestingServer server) {
+        Map<String, Object> conf = new HashMap<>();
         conf.put(Config.TRANSACTIONAL_ZOOKEEPER_PORT, server.getPort());
         conf.put(Config.TRANSACTIONAL_ZOOKEEPER_SERVERS, Arrays.asList("localhost"));
         conf.put(Config.STORM_ZOOKEEPER_SESSION_TIMEOUT, 20000);

File: external/storm-mongodb/src/main/java/org/apache/storm/mongodb/trident/state/MongoMapState.java
Patch:
@@ -68,9 +68,9 @@ public class MongoMapState<T> implements IBackingMap<T> {
     private Options<T> options;
     private Serializer<T> serializer;
     private MongoDbClient mongoClient;
-    private Map map;
+    private Map<String, Object> map;
 
-    protected MongoMapState(Map map, Options options) {
+    protected MongoMapState(Map<String, Object> map, Options options) {
         this.options = options;
         this.map = map;
         this.serializer = options.serializer;

File: external/storm-mongodb/src/main/java/org/apache/storm/mongodb/trident/state/MongoState.java
Patch:
@@ -45,9 +45,9 @@ public class MongoState implements State {
 
     private Options options;
     private MongoDbClient mongoClient;
-    private Map map;
+    private Map<String, Object> map;
 
-    protected MongoState(Map map, Options options) {
+    protected MongoState(Map<String, Object> map, Options options) {
         this.options = options;
         this.map = map;
     }

File: external/storm-redis/src/main/java/org/apache/storm/redis/bolt/AbstractRedisBolt.java
Patch:
@@ -82,7 +82,7 @@ public AbstractRedisBolt(JedisClusterConfig config) {
      * {@inheritDoc}
      */
     @Override
-    public void prepare(Map map, TopologyContext topologyContext, OutputCollector collector) {
+    public void prepare(Map<String, Object> map, TopologyContext topologyContext, OutputCollector collector) {
         // FIXME: stores map (topoConf), topologyContext and expose these to derived classes
         this.collector = collector;
 

File: external/storm-redis/src/main/java/org/apache/storm/redis/trident/state/RedisClusterMapState.java
Patch:
@@ -230,7 +230,7 @@ public Factory(JedisClusterConfig jedisClusterConfig, StateType type, Options op
          * {@inheritDoc}
          */
         @Override
-        public State makeState(@SuppressWarnings("rawtypes") Map<String, Object> conf, IMetricsContext metrics, int partitionIndex, int numPartitions) {
+        public State makeState(Map<String, Object> conf, IMetricsContext metrics, int partitionIndex, int numPartitions) {
             JedisCluster jedisCluster = new JedisCluster(jedisClusterConfig.getNodes(),
                     jedisClusterConfig.getTimeout(),
                     jedisClusterConfig.getTimeout(),

File: external/storm-redis/src/main/java/org/apache/storm/redis/trident/state/RedisClusterState.java
Patch:
@@ -69,7 +69,7 @@ public Factory(JedisClusterConfig config) {
          * {@inheritDoc}
          */
         @Override
-        public State makeState(@SuppressWarnings("rawtypes") Map<String, Object> conf, IMetricsContext metrics, int partitionIndex, int numPartitions) {
+        public State makeState(Map<String, Object> conf, IMetricsContext metrics, int partitionIndex, int numPartitions) {
             JedisCluster jedisCluster = new JedisCluster(jedisClusterConfig.getNodes(),
                                                     jedisClusterConfig.getTimeout(),
                                                     jedisClusterConfig.getTimeout(),

File: external/storm-redis/src/main/java/org/apache/storm/redis/trident/state/RedisMapState.java
Patch:
@@ -230,7 +230,7 @@ public Factory(JedisPoolConfig jedisPoolConfig, StateType type, Options options)
          * {@inheritDoc}
          */
         @Override
-        public State makeState(@SuppressWarnings("rawtypes") Map<String, Object> conf, IMetricsContext metrics, int partitionIndex, int numPartitions) {
+        public State makeState(Map<String, Object> conf, IMetricsContext metrics, int partitionIndex, int numPartitions) {
             JedisPool jedisPool = new JedisPool(DEFAULT_POOL_CONFIG,
                                                     jedisPoolConfig.getHost(),
                                                     jedisPoolConfig.getPort(),

File: external/storm-redis/src/main/java/org/apache/storm/redis/trident/state/RedisState.java
Patch:
@@ -69,7 +69,7 @@ public Factory(JedisPoolConfig config) {
          * {@inheritDoc}
          */
         @Override
-        public State makeState(@SuppressWarnings("rawtypes") Map<String, Object> conf, IMetricsContext metrics, int partitionIndex, int numPartitions) {
+        public State makeState(Map<String, Object> conf, IMetricsContext metrics, int partitionIndex, int numPartitions) {
             JedisPool jedisPool = new JedisPool(DEFAULT_POOL_CONFIG,
                                                 jedisPoolConfig.getHost(),
                                                 jedisPoolConfig.getPort(),

File: external/storm-rocketmq/src/main/java/org/apache/storm/rocketmq/trident/state/RocketMqState.java
Patch:
@@ -45,7 +45,7 @@ public class RocketMqState implements State {
     private Options options;
     private MQProducer producer;
 
-    protected RocketMqState(Map map, Options options) {
+    protected RocketMqState(Map<String, Object> map, Options options) {
         this.options = options;
     }
 

File: external/storm-solr/src/main/java/org/apache/storm/solr/trident/SolrStateFactory.java
Patch:
@@ -36,7 +36,7 @@ public SolrStateFactory(SolrConfig solrConfig, SolrMapper solrMapper) {
     }
 
     @Override
-    public State makeState(Map map, IMetricsContext iMetricsContext, int partitionIndex, int numPartitions) {
+    public State makeState(Map<String, Object> map, IMetricsContext iMetricsContext, int partitionIndex, int numPartitions) {
         SolrState state = new SolrState(solrConfig, solrMapper);
         state.prepare();
         return state;

File: flux/flux-core/src/main/java/org/apache/storm/flux/model/ConfigMethodDef.java
Patch:
@@ -49,7 +49,7 @@ public void setArgs(List<Object> args) {
         List<Object> newVal = new ArrayList<Object>();
         for (Object obj : args) {
             if (obj instanceof LinkedHashMap) {
-                Map map = (Map)obj;
+                Map<String, Object> map = (Map<String, Object>)obj;
                 if (map.containsKey("ref") && map.size() == 1) {
                     newVal.add(new BeanReference((String)map.get("ref")));
                     this.hasReferences = true;

File: flux/flux-core/src/main/java/org/apache/storm/flux/model/ObjectDef.java
Patch:
@@ -57,7 +57,7 @@ public void setConstructorArgs(List<Object> constructorArgs) {
         List<Object> newVal = new ArrayList<Object>();
         for (Object obj : constructorArgs) {
             if (obj instanceof LinkedHashMap) {
-                Map map = (Map)obj;
+                Map<String, Object> map = (Map<String, Object>)obj;
                 if (map.containsKey("ref") && map.size() == 1) {
                     newVal.add(new BeanReference((String) map.get("ref")));
                     this.hasReferences = true;

File: flux/flux-examples/src/main/java/org/apache/storm/flux/examples/StatefulWordCounter.java
Patch:
@@ -34,7 +34,6 @@ public class StatefulWordCounter extends BaseStatefulBolt<KeyValueState<String,
     private KeyValueState<String, Long> wordCounts;
     private OutputCollector collector;
 
-    @SuppressWarnings("rawtypes")
     @Override
     public void prepare(Map<String, Object> topoConf, TopologyContext context, OutputCollector collector) {
         this.collector = collector;

File: flux/flux-examples/src/main/java/org/apache/storm/flux/examples/WordCounter.java
Patch:
@@ -43,7 +43,6 @@ public class WordCounter extends BaseBasicBolt {
 
 
 
-    @SuppressWarnings("rawtypes")
     public void prepare(Map<String, Object> topoConf, TopologyContext context) {
     }
 

File: integration-test/src/test/java/org/apache/storm/st/wrapper/StormCluster.java
Patch:
@@ -49,7 +49,7 @@ public StormCluster() {
         this.client = NimbusClient.getConfiguredClient(conf).getClient();
     }
 
-    public static Map getConfig() {
+    public static Map<String, Object> getConfig() {
         return Utils.readStormConfig();
     }
 

File: storm-client/src/jvm/org/apache/storm/Config.java
Patch:
@@ -1853,11 +1853,11 @@ public void setClasspath(String cp) {
         setClasspath(this, cp);
     }
 
-    public static void setEnvironment(Map<String, Object> conf, Map env) {
+    public static void setEnvironment(Map<String, Object> conf, Map<String, Object> env) {
         conf.put(Config.TOPOLOGY_ENVIRONMENT, env);
     }
 
-    public void setEnvironment(Map env) {
+    public void setEnvironment(Map<String, Object> env) {
         setEnvironment(this, env);
     }
 
@@ -1955,7 +1955,7 @@ public static void registerEventLogger(Map<String, Object> conf, Class<? extends
     }
 
     public static void registerMetricsConsumer(Map<String, Object> conf, Class klass, Object argument, long parallelismHint) {
-        HashMap m = new HashMap();
+        HashMap<String, Object> m = new HashMap<>();
         m.put("class", klass.getCanonicalName());
         m.put("parallelism.hint", parallelismHint);
         m.put("argument", argument);

File: storm-client/src/jvm/org/apache/storm/StormSubmitter.java
Patch:
@@ -80,9 +80,8 @@ public static boolean validateZKDigestPayload(String payload) {
         return false;
     }
 
-    @SuppressWarnings("unchecked")
-    public static Map prepareZookeeperAuthentication(Map<String, Object> conf) {
-        Map toRet = new HashMap();
+    public static Map<String, Object> prepareZookeeperAuthentication(Map<String, Object> conf) {
+        Map<String, Object> toRet = new HashMap<>();
         String secretPayload = (String) conf.get(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_PAYLOAD);
         // Is the topology ZooKeeper authentication configuration unset?
         if (! conf.containsKey(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_PAYLOAD) ||

File: storm-client/src/jvm/org/apache/storm/Thrift.java
Patch:
@@ -94,7 +94,7 @@ public Integer getParallelism() {
             return parallelism;
         }
 
-        public Map getConf() {
+        public Map<String, Object> getConf() {
             return conf;
         }
     }
@@ -117,7 +117,7 @@ public Object getBolt() {
             return bolt;
         }
 
-        public Map getConf() {
+        public Map<String, Object> getConf() {
             return conf;
         }
 

File: storm-client/src/jvm/org/apache/storm/cluster/ClusterUtils.java
Patch:
@@ -256,12 +256,12 @@ public IStormClusterState mkStormClusterStateImpl(Object stateStorage, ClusterSt
         if (stateStorage instanceof IStateStorage) {
             return new StormClusterStateImpl((IStateStorage) stateStorage, context, false);
         } else {
-            IStateStorage Storage = _instance.mkStateStorageImpl((Map) stateStorage, (Map) stateStorage, context);
+            IStateStorage Storage = _instance.mkStateStorageImpl((Map<String, Object>) stateStorage, (Map<String, Object>) stateStorage, context);
             return new StormClusterStateImpl(Storage, context, true);
         }
     }
 
-    public IStateStorage mkStateStorageImpl(Map<String, Object> config, Map auth_conf, ClusterStateContext context) throws Exception {
+    public IStateStorage mkStateStorageImpl(Map<String, Object> config, Map<String, Object> auth_conf, ClusterStateContext context) throws Exception {
         String className = null;
         IStateStorage stateStorage = null;
         if (config.get(Config.STORM_CLUSTER_STATE_STORE) != null) {
@@ -275,7 +275,7 @@ public IStateStorage mkStateStorageImpl(Map<String, Object> config, Map auth_con
         return stateStorage;
     }
 
-    public static IStateStorage mkStateStorage(Map<String, Object> config, Map auth_conf, ClusterStateContext context) throws Exception {
+    public static IStateStorage mkStateStorage(Map<String, Object> config, Map<String, Object> auth_conf, ClusterStateContext context) throws Exception {
         return _instance.mkStateStorageImpl(config, auth_conf, context);
     }
 

File: storm-client/src/jvm/org/apache/storm/cluster/PaceMakerStateStorageFactory.java
Patch:
@@ -24,7 +24,7 @@
 
 public class PaceMakerStateStorageFactory implements StateStorageFactory {
     @Override
-    public IStateStorage mkStore(Map<String, Object> config, Map auth_conf, ClusterStateContext context) {
+    public IStateStorage mkStore(Map<String, Object> config, Map<String, Object> auth_conf, ClusterStateContext context) {
         try {
             ZKStateStorageFactory zkfact = new ZKStateStorageFactory();
             IStateStorage zkState = zkfact.mkStore(config, auth_conf, context);

File: storm-client/src/jvm/org/apache/storm/cluster/StateStorageFactory.java
Patch:
@@ -22,5 +22,5 @@
 
 public interface StateStorageFactory {
 
-    IStateStorage mkStore(Map<String, Object> config, Map auth_conf, ClusterStateContext context);
+    IStateStorage mkStore(Map<String, Object> config, Map<String, Object> auth_conf, ClusterStateContext context);
 }

File: storm-client/src/jvm/org/apache/storm/cluster/ZKStateStorage.java
Patch:
@@ -50,7 +50,7 @@ public class ZKStateStorage implements IStateStorage {
     private AtomicBoolean active;
 
     private boolean isNimbus;
-    private Map authConf;
+    private Map<String, Object> authConf;
     private Map<String, Object> conf;
 
     private class ZkWatcherCallBack implements WatcherCallBack{
@@ -73,7 +73,7 @@ public void execute(Watcher.Event.KeeperState state, Watcher.Event.EventType typ
         }
     }
 
-    public ZKStateStorage(Map<String, Object> conf, Map authConf, ClusterStateContext context) throws Exception {
+    public ZKStateStorage(Map<String, Object> conf, Map<String, Object> authConf, ClusterStateContext context) throws Exception {
         this.conf = conf;
         this.authConf = authConf;
         if (context.getDaemonType().equals(DaemonType.NIMBUS))

File: storm-client/src/jvm/org/apache/storm/cluster/ZKStateStorageFactory.java
Patch:
@@ -26,7 +26,7 @@
 public class ZKStateStorageFactory implements StateStorageFactory {
 
     @Override
-    public IStateStorage mkStore(Map<String, Object> config, Map auth_conf, ClusterStateContext context) {
+    public IStateStorage mkStore(Map<String, Object> config, Map<String, Object> auth_conf, ClusterStateContext context) {
         try {
             return new ZKStateStorage(config, auth_conf, context);
         } catch (Exception e) {

File: storm-client/src/jvm/org/apache/storm/daemon/StormCommon.java
Patch:
@@ -296,7 +296,7 @@ public static void addAcker(Map<String, Object> conf, StormTopology topology) {
 
         for (SpoutSpec spout : topology.get_spouts().values()) {
             ComponentCommon common = spout.get_common();
-            Map spoutConf = componentConf(spout);
+            Map<String, Object> spoutConf = componentConf(spout);
             spoutConf.put(Config.TOPOLOGY_TICK_TUPLE_FREQ_SECS,
                     ObjectReader.getInt(conf.get(Config.TOPOLOGY_MESSAGE_TIMEOUT_SECS)));
             common.set_json_conf(JSONValue.toJSONString(spoutConf));

File: storm-client/src/jvm/org/apache/storm/daemon/worker/WorkerState.java
Patch:
@@ -96,7 +96,7 @@ public class WorkerState {
     private final WorkerTransfer workerTransfer;
     private final BackPressureTracker bpTracker;
 
-    public Map getConf() {
+    public Map<String, Object> getConf() {
         return conf;
     }
 
@@ -140,7 +140,7 @@ public Map<Integer, JCQueue> getLocalReceiveQueues() {
         return localReceiveQueues;
     }
 
-    public Map getTopologyConf() {
+    public Map<String, Object> getTopologyConf() {
         return topologyConf;
     }
 

File: storm-client/src/jvm/org/apache/storm/drpc/DRPCSpout.java
Patch:
@@ -204,7 +204,7 @@ public void nextTuple() {
                 try {
                     DRPCRequest req = client.fetchRequest(_function);
                     if(req.get_request_id().length() > 0) {
-                        Map returnInfo = new HashMap();
+                        Map<String, Object> returnInfo = new HashMap<>();
                         returnInfo.put("id", req.get_request_id());
                         returnInfo.put("host", client.getHost());
                         returnInfo.put("port", client.getPort());
@@ -228,7 +228,7 @@ public void nextTuple() {
                 try {
                     DRPCRequest req = drpc.fetchRequest(_function);
                     if(req.get_request_id().length() > 0) {
-                        Map returnInfo = new HashMap();
+                        Map<String, Object> returnInfo = new HashMap<>();
                         returnInfo.put("id", req.get_request_id());
                         returnInfo.put("host", _local_drpc_id);
                         returnInfo.put("port", 0);

File: storm-client/src/jvm/org/apache/storm/drpc/JoinResult.java
Patch:
@@ -44,7 +44,7 @@ public JoinResult(String returnComponent) {
         this.returnComponent = returnComponent;
     }
  
-    public void prepare(Map map, TopologyContext context, OutputCollector collector) {
+    public void prepare(Map<String, Object> map, TopologyContext context, OutputCollector collector) {
         _collector = collector;
     }
 

File: storm-client/src/jvm/org/apache/storm/drpc/PrepareRequest.java
Patch:
@@ -38,7 +38,7 @@ public class PrepareRequest extends BaseBasicBolt {
     Random rand;
 
     @Override
-    public void prepare(Map map, TopologyContext context) {
+    public void prepare(Map<String, Object> map, TopologyContext context) {
         rand = new Random();
     }
 

File: storm-client/src/jvm/org/apache/storm/drpc/ReturnResults.java
Patch:
@@ -47,7 +47,7 @@ public class ReturnResults extends BaseRichBolt {
     public static final Logger LOG = LoggerFactory.getLogger(ReturnResults.class);
     OutputCollector _collector;
     boolean local;
-    Map _conf; 
+    Map<String, Object> _conf;
     Map<List, DRPCInvocationsClient> _clients = new HashMap<List, DRPCInvocationsClient>();
 
     @Override
@@ -62,9 +62,9 @@ public void execute(Tuple input) {
         String result = (String) input.getValue(0);
         String returnInfo = (String) input.getValue(1);
         if (returnInfo!=null) {
-            Map retMap = null;
+            Map<String, Object> retMap;
             try {
-                retMap = (Map) JSONValue.parseWithException(returnInfo);
+                retMap = (Map<String, Object>) JSONValue.parseWithException(returnInfo);
             } catch (ParseException e) {
                  LOG.error("Parseing returnInfo failed", e);
                  _collector.fail(input);

File: storm-client/src/jvm/org/apache/storm/messaging/DeserializingConnectionCallback.java
Patch:
@@ -39,7 +39,7 @@
  */
 public class DeserializingConnectionCallback implements IConnectionCallback, IMetric {
     private final WorkerState.ILocalTransferCallback cb;
-    private final Map conf;
+    private final Map<String, Object> conf;
     private final GeneralTopologyContext context;
 
     private final ThreadLocal<KryoTupleDeserializer> _des =
@@ -55,7 +55,7 @@ protected KryoTupleDeserializer initialValue() {
     private final ConcurrentHashMap<String, AtomicLong> byteCounts = new ConcurrentHashMap<>();
 
 
-    public DeserializingConnectionCallback(final Map conf, final GeneralTopologyContext context, WorkerState.ILocalTransferCallback callback) {
+    public DeserializingConnectionCallback(final Map<String, Object> conf, final GeneralTopologyContext context, WorkerState.ILocalTransferCallback callback) {
         this.conf = conf;
         this.context = context;
         cb = callback;

File: storm-client/src/jvm/org/apache/storm/messaging/local/Context.java
Patch:
@@ -233,7 +233,6 @@ private static LocalServer getLocalServer(String nodeId, int port) {
         return ret;
     }
         
-    @SuppressWarnings("rawtypes")
     @Override
     public void prepare(Map<String, Object> topoConf) {
         //NOOP
@@ -249,7 +248,6 @@ public IConnection connect(String storm_id, String host, int port, AtomicBoolean
         return new LocalClient(getLocalServer(storm_id, port));
     }
 
-    @SuppressWarnings("rawtypes")
     @Override
     public void term() {
         //NOOP

File: storm-client/src/jvm/org/apache/storm/messaging/netty/Client.java
Patch:
@@ -151,7 +151,6 @@ public class Client extends ConnectionWithStatus implements IStatefulObject, ISa
     // wait strategy when the netty channel is not writable
     private final IWaitStrategy waitStrategy;
 
-    @SuppressWarnings("rawtypes")
     Client(Map<String, Object> topoConf, AtomicBoolean[] remoteBpStatus, ChannelFactory factory, HashedWheelTimer scheduler, String host, int port, Context context) {
         this.topoConf = topoConf;
         closing = false;
@@ -549,7 +548,7 @@ public Object getState() {
         return ret;
     }
 
-    public Map getConfig() {
+    public Map<String, Object> getConfig() {
         return topoConf;
     }
 

File: storm-client/src/jvm/org/apache/storm/messaging/netty/Context.java
Patch:
@@ -31,7 +31,6 @@
 import org.apache.storm.messaging.IContext;
 
 public class Context implements IContext {
-    @SuppressWarnings("rawtypes")
     private Map<String, Object> topoConf;
     private Map<String, IConnection> connections;
     private NioClientSocketChannelFactory clientChannelFactory;
@@ -40,7 +39,6 @@ public class Context implements IContext {
     /**
      * initialization per Storm configuration 
      */
-    @SuppressWarnings("rawtypes")
     public void prepare(Map<String, Object> topoConf) {
         this.topoConf = topoConf;
         connections = new HashMap<>();

File: storm-client/src/jvm/org/apache/storm/messaging/netty/Server.java
Patch:
@@ -53,7 +53,6 @@
 class Server extends ConnectionWithStatus implements IStatefulObject, ISaslServer {
 
     private static final Logger LOG = LoggerFactory.getLogger(Server.class);
-    @SuppressWarnings("rawtypes")
     Map<String, Object> topoConf;
     int port;
     private final ConcurrentHashMap<String, AtomicInteger> messagesEnqueued = new ConcurrentHashMap<>();
@@ -70,7 +69,6 @@ class Server extends ConnectionWithStatus implements IStatefulObject, ISaslServe
     private Supplier<Object> newConnectionResponse;
     private final int boundPort;
     
-    @SuppressWarnings("rawtypes")
     Server(Map<String, Object> topoConf, int port) {
         this.topoConf = topoConf;
         this.port = port;

File: storm-client/src/jvm/org/apache/storm/metric/SystemBolt.java
Patch:
@@ -89,7 +89,7 @@ public Object getValueAndReset() {
         }
     }
 
-    @SuppressWarnings({ "unchecked", "rawtypes" })
+    @SuppressWarnings({ "unchecked" })
     @Override
     public void prepare(final Map<String, Object> topoConf, TopologyContext context, OutputCollector collector) {
         if(_prepareWasCalled && !"local".equals(topoConf.get(Config.STORM_CLUSTER_MODE))) {

File: storm-client/src/jvm/org/apache/storm/metric/api/MultiCountMetric.java
Patch:
@@ -35,8 +35,8 @@ public CountMetric scope(String key) {
         return val;
     }
 
-    public Object getValueAndReset() {
-        Map ret = new HashMap();
+    public Map<String, Object> getValueAndReset() {
+        Map<String, Object> ret = new HashMap<>();
         for(Map.Entry<String, CountMetric> e : _value.entrySet()) {
             ret.put(e.getKey(), e.getValue().getValueAndReset());
         }

File: storm-client/src/jvm/org/apache/storm/metric/api/MultiReducedMetric.java
Patch:
@@ -37,8 +37,8 @@ public ReducedMetric scope(String key) {
         return val;
     }
 
-    public Object getValueAndReset() {
-        Map ret = new HashMap();
+    public Map<String, Object> getValueAndReset() {
+        Map<String, Object> ret = new HashMap<>();
         for(Map.Entry<String, ReducedMetric> e : _value.entrySet()) {
             Object val = e.getValue().getValueAndReset();
             if(val != null) {

File: storm-client/src/jvm/org/apache/storm/security/auth/FixedGroupsMapping.java
Patch:
@@ -41,7 +41,7 @@ public class FixedGroupsMapping implements IGroupMappingServiceProvider {
      * @param storm_conf Storm configuration
      */
     @Override
-    public void prepare(Map storm_conf) {
+    public void prepare(Map<String, Object> storm_conf) {
         Map<?, ?> params = (Map<?, ?>) storm_conf.get(Config.STORM_GROUP_MAPPING_SERVICE_PARAMS);
         Map<String, Set<String>> mapping = (Map<String, Set<String>>) params.get(STORM_FIXED_GROUP_MAPPING);
         if (mapping != null) {

File: storm-client/src/jvm/org/apache/storm/security/auth/authorizer/DRPCAuthorizerBase.java
Patch:
@@ -36,9 +36,9 @@ public abstract class DRPCAuthorizerBase implements IAuthorizer {
     @Override
     public abstract void prepare(Map<String, Object> conf);
 
-    abstract protected boolean permitClientRequest(ReqContext context, String operation, Map params);
+    abstract protected boolean permitClientRequest(ReqContext context, String operation, Map<String, Object> params);
 
-    abstract protected boolean permitInvocationRequest(ReqContext context, String operation, Map params);
+    abstract protected boolean permitInvocationRequest(ReqContext context, String operation, Map<String, Object> params);
     
     /**
      * Authorizes request from to the DRPC server.
@@ -47,7 +47,7 @@ public abstract class DRPCAuthorizerBase implements IAuthorizer {
      * @param params a Map with any key-value entries of use to the authorization implementation
      */
     @Override
-    public boolean permit(ReqContext context, String operation, Map params) {
+    public boolean permit(ReqContext context, String operation, Map<String, Object> params) {
         if ("execute".equals(operation)) {
             return permitClientRequest(context, operation, params);
         } else if ("failRequest".equals(operation) || 

File: storm-client/src/jvm/org/apache/storm/security/auth/authorizer/DRPCSimpleACLAuthorizer.java
Patch:
@@ -125,7 +125,7 @@ private String getLocalUserFromContext(ReqContext context) {
         return null;
     }
 
-    protected boolean permitClientOrInvocationRequest(ReqContext context, Map params,
+    protected boolean permitClientOrInvocationRequest(ReqContext context, Map<String, Object> params,
             String fieldName) {
         Map<String,AclFunctionEntry> acl = readAclFromConfig();
         String function = (String) params.get(FUNCTION_KEY);
@@ -165,13 +165,13 @@ protected boolean permitClientOrInvocationRequest(ReqContext context, Map params
 
     @Override
     protected boolean permitClientRequest(ReqContext context, String operation,
-            Map params) {
+            Map<String, Object> params) {
         return permitClientOrInvocationRequest(context, params, "clientUsers");
     }
 
     @Override
     protected boolean permitInvocationRequest(ReqContext context, String operation,
-            Map params) {
+            Map<String, Object> params) {
         return permitClientOrInvocationRequest(context, params, "invocationUser");
     }
 }

File: storm-client/src/jvm/org/apache/storm/serialization/types/HashMapSerializer.java
Patch:
@@ -26,7 +26,7 @@
 
 public class HashMapSerializer extends MapSerializer {
     @Override
-    public Map create(Kryo kryo, Input input, Class<Map> type) {
-        return new HashMap();
+    public Map<String, Object> create(Kryo kryo, Input input, Class<Map> type) {
+        return new HashMap<>();
     }
 }

File: storm-client/src/jvm/org/apache/storm/stats/CommonStats.java
Patch:
@@ -61,11 +61,11 @@ public void cleanupStats() {
         transferredStats.close();
     }
 
-    protected Map valueStat(MultiCountStatAndMetric metric) {
+    protected Map<String,Map<String,Long>> valueStat(MultiCountStatAndMetric metric) {
         return metric.getTimeCounts();
     }
 
-    protected Map valueStat(MultiLatencyStatAndMetric metric) {
+    protected Map<String, Map<String, Double>> valueStat(MultiLatencyStatAndMetric metric) {
         return metric.getTimeLatAvg();
     }
 

File: storm-client/src/jvm/org/apache/storm/task/GeneralTopologyContext.java
Patch:
@@ -160,7 +160,7 @@ public Map<String, Map<String, Grouping>> getTargets(String componentId) {
 
     @Override
     public String toJSONString() {
-        Map obj = new HashMap();
+        Map<String, Object> obj = new HashMap<>();
         obj.put("task->component", _taskToComponent);
         // TODO: jsonify StormTopology
         // at the minimum should send source info

File: storm-client/src/jvm/org/apache/storm/testing/NonRichBoltTracker.java
Patch:
@@ -41,7 +41,7 @@ public void prepare(Map<String, Object> topoConf, TopologyContext context, Outpu
 
     public void execute(Tuple input) {
         _delegate.execute(input);
-        Map stats = (Map) RegisteredGlobalState.getState(_trackId);
+        Map<String, Object> stats = (Map<String, Object>) RegisteredGlobalState.getState(_trackId);
         ((AtomicInteger) stats.get("processed")).incrementAndGet();
     }
 

File: storm-client/src/jvm/org/apache/storm/testing/SpoutTracker.java
Patch:
@@ -45,7 +45,7 @@ public SpoutTrackOutputCollector(SpoutOutputCollector collector) {
         }
         
         private void recordSpoutEmit() {
-            Map stats = (Map) RegisteredGlobalState.getState(_trackId);
+            Map<String, Object> stats = (Map<String, Object>) RegisteredGlobalState.getState(_trackId);
             ((AtomicInteger) stats.get("spout-emitted")).incrementAndGet();
             
         }
@@ -99,13 +99,13 @@ public void nextTuple() {
 
     public void ack(Object msgId) {
         _delegate.ack(msgId);
-        Map stats = (Map) RegisteredGlobalState.getState(_trackId);
+        Map<String, Object> stats = (Map<String, Object>) RegisteredGlobalState.getState(_trackId);
         ((AtomicInteger) stats.get("processed")).incrementAndGet();
     }
 
     public void fail(Object msgId) {
         _delegate.fail(msgId);
-        Map stats = (Map) RegisteredGlobalState.getState(_trackId);
+        Map<String, Object> stats = (Map<String, Object>) RegisteredGlobalState.getState(_trackId);
         ((AtomicInteger) stats.get("processed")).incrementAndGet();        
     }
 

File: storm-client/src/jvm/org/apache/storm/topology/TopologyBuilder.java
Patch:
@@ -735,8 +735,8 @@ public BoltDeclarer grouping(GlobalStreamId id, Grouping grouping) {
         }        
     }
 
-    private static String mergeIntoJson(Map into, Map newMap) {
-        Map res = new HashMap<>(into);
+    private static String mergeIntoJson(Map<String, Object> into, Map<String, Object> newMap) {
+        Map<String, Object> res = new HashMap<>(into);
         res.putAll(newMap);
         return JSONValue.toJSONString(res);
     }

File: storm-client/src/jvm/org/apache/storm/transactional/state/TestTransactionalState.java
Patch:
@@ -35,7 +35,7 @@ public class TestTransactionalState extends TransactionalState {
      * Matching constructor in absence of a default constructor in the parent
      * class.
      */
-    protected TestTransactionalState(Map<String, Object> conf, String id, Map componentConf, String subroot) {
+    protected TestTransactionalState(Map<String, Object> conf, String id, Map<String, Object> componentConf, String subroot) {
         super(conf, id, componentConf, subroot);
     }
 

File: storm-client/src/jvm/org/apache/storm/trident/drpc/ReturnResultsReducer.java
Patch:
@@ -80,9 +80,9 @@ public void complete(ReturnResultsState state, TridentCollector collector) {
         // only one of the multireducers will receive the tuples
         if (state.returnInfo!=null) {
             String result = JSONValue.toJSONString(state.results);
-            Map retMap = null;
+            Map<String, Object> retMap;
             try {
-                retMap = (Map) JSONValue.parseWithException(state.returnInfo);
+                retMap = (Map<String, Object>) JSONValue.parseWithException(state.returnInfo);
             } catch (ParseException e) {
                 collector.reportError(e);
                 return;

File: storm-client/src/jvm/org/apache/storm/utils/ConfigUtils.java
Patch:
@@ -271,7 +271,7 @@ public static Map<String, Object> readSupervisorStormConfGivenPath(Map<String, O
         return ret;
     }
 
-    public static Map overrideLoginConfigWithSystemProperty(Map<String, Object> conf) { // note that we delete the return value
+    public static Map<String, Object> overrideLoginConfigWithSystemProperty(Map<String, Object> conf) { // note that we delete the return value
         String loginConfFile = System.getProperty("java.security.auth.login.config");
         if (loginConfFile != null) {
             conf.put("java.security.auth.login.config", loginConfFile);
@@ -322,7 +322,7 @@ public static Map<String, Object> readYamlConfig(String name, boolean mustExist)
         return conf;
     }
 
-    public static Map readYamlConfig(String name) {
+    public static Map<String, Object> readYamlConfig(String name) {
         return readYamlConfig(name, true);
     }
 

File: storm-client/src/jvm/org/apache/storm/utils/JCQueue.java
Patch:
@@ -29,6 +29,7 @@
 
 import java.util.ArrayList;
 import java.util.HashMap;
+import java.util.Map;
 import java.util.concurrent.atomic.AtomicLong;
 
 
@@ -189,7 +190,7 @@ public long capacity() {
         }
 
         public Object getState() {
-            HashMap state = new HashMap<String, Object>();
+            Map<String, Object> state = new HashMap<>();
 
             final double arrivalRateInSecs = arrivalsTracker.reportRate();
 

File: storm-client/src/jvm/org/apache/storm/utils/NimbusClient.java
Patch:
@@ -111,7 +111,7 @@ private static synchronized boolean shouldLogLeader(String leader) {
         return true;
     }
 
-    public static NimbusClient getConfiguredClientAs(Map conf, String asUser) {
+    public static NimbusClient getConfiguredClientAs(Map<String, Object> conf, String asUser) {
         return getConfiguredClientAs(conf, asUser, null);
     }
 

File: storm-client/test/jvm/org/apache/storm/messaging/DeserializingConnectionCallbackTest.java
Patch:
@@ -48,7 +48,7 @@ public void setUp() throws Exception {
 
     @Test
     public void testUpdateMetricsConfigOff() {
-        Map config = new HashMap();
+        Map<String, Object> config = new HashMap<>();
         config.put(Config.TOPOLOGY_SERIALIZED_MESSAGE_SIZE_METRICS, Boolean.FALSE);
         DeserializingConnectionCallback withoutMetrics = 
             new DeserializingConnectionCallback(config, mock(GeneralTopologyContext.class), mock(
@@ -64,7 +64,7 @@ public void testUpdateMetricsConfigOff() {
     
     @Test
     public void testUpdateMetricsConfigOn() {
-        Map config = new HashMap();
+        Map<String, Object> config = new HashMap<>();
         config.put(Config.TOPOLOGY_SERIALIZED_MESSAGE_SIZE_METRICS, Boolean.TRUE);
         DeserializingConnectionCallback withMetrics =
             new DeserializingConnectionCallback(config, mock(GeneralTopologyContext.class), mock(

File: storm-client/test/jvm/org/apache/storm/streams/ProcessorBoltTest.java
Patch:
@@ -40,6 +40,7 @@
 import java.util.HashMap;
 import java.util.Map;
 import java.util.Set;
+import org.apache.storm.generated.Grouping;
 
 import static org.junit.Assert.assertArrayEquals;
 import static org.junit.Assert.assertEquals;
@@ -140,7 +141,7 @@ private void setUpProcessorBolt(Processor<?> processor,
         node.setWindowed(isWindowed);
         Mockito.when(mockStreamToProcessors.get(Mockito.anyString())).thenReturn(Collections.singletonList(node));
         Mockito.when(mockStreamToProcessors.keySet()).thenReturn(Collections.singleton("inputstream"));
-        Map mockSources = Mockito.mock(Map.class);
+        Map<GlobalStreamId, Grouping> mockSources = Mockito.mock(Map.class);
         GlobalStreamId mockGlobalStreamId = Mockito.mock(GlobalStreamId.class);
         Mockito.when(mockTopologyContext.getThisSources()).thenReturn(mockSources);
         Mockito.when(mockSources.keySet()).thenReturn(Collections.singleton(mockGlobalStreamId));

File: storm-core/src/jvm/org/apache/storm/ui/UIHelpers.java
Patch:
@@ -109,7 +109,7 @@ public static String prettyExecutorInfo(ExecutorInfo e) {
         return "[" + e.get_task_start() + "-" + e.get_task_end() + "]";
     }
 
-    public static Map unauthorizedUserJson(String user) {
+    public static Map<String, Object> unauthorizedUserJson(String user) {
         return ImmutableMap.of(
                 "error", "No Authorization",
                 "errorMessage", String.format("User %s is not authorized.", user));
@@ -203,7 +203,7 @@ public static void configFilters(ServletContextHandler context, List<FilterConfi
         for (FilterConfiguration filterConf : filtersConfs) {
             String filterName = filterConf.getFilterName();
             String filterClass = filterConf.getFilterClass();
-            Map filterParams = filterConf.getFilterParams();
+            Map<String, String> filterParams = filterConf.getFilterParams();
             if (filterClass != null) {
                 FilterHolder filterHolder = new FilterHolder();
                 filterHolder.setClassName(filterClass);
@@ -215,7 +215,7 @@ public static void configFilters(ServletContextHandler context, List<FilterConfi
                 if (filterParams != null) {
                     filterHolder.setInitParameters(filterParams);
                 } else {
-                    filterHolder.setInitParameters(new HashMap<String, String>());
+                    filterHolder.setInitParameters(new HashMap<>());
                 }
                 context.addFilter(filterHolder, "/*", EnumSet.allOf(DispatcherType.class));
             }

File: storm-core/test/jvm/org/apache/storm/nimbus/InMemoryTopologyActionNotifier.java
Patch:
@@ -30,7 +30,7 @@ public class InMemoryTopologyActionNotifier implements  ITopologyActionNotifierP
 
 
     @Override
-    public void prepare(Map StormConf) {
+    public void prepare(Map<String, Object> StormConf) {
         //no-op
     }
 

File: storm-server/src/main/java/org/apache/storm/Testing.java
Patch:
@@ -622,7 +622,6 @@ public static <T> Map<T, Integer> multiset(Collection<T> c) {
         return ret;
     }
     
-    @SuppressWarnings("rawtypes")
     private static void printRec(Object o, String prefix) {
         if (o instanceof Collection) {
             LOG.info("{} {} ({}) [",prefix,o, o.getClass());

File: storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java
Patch:
@@ -406,7 +406,7 @@ public Map<K, V> apply(Map<K, V> t) {
     public static class StandaloneINimbus implements INimbus {
 
         @Override
-        public void prepare(@SuppressWarnings("rawtypes") Map<String, Object> topoConf, String schedulerLocalDir) {
+        public void prepare(Map<String, Object> topoConf, String schedulerLocalDir) {
             //NOOP
         }
 
@@ -3637,7 +3637,7 @@ public TopologyInfo getTopologyInfoWithOpts(String topoId, GetInfoOptions option
                     //heartbeats "stats"
                     Map<String, Object> hb = (Map<String, Object>)heartbeat.get("heartbeat");
                     if (hb != null) {
-                        Map ex = (Map) hb.get("stats");
+                        Map<String, Object> ex = (Map<String, Object>) hb.get("stats");
                         if (ex != null) {
                             ExecutorStats stats = StatsUtil.thriftifyExecutorStats(ex);
                             summ.set_stats(stats);
@@ -4101,7 +4101,7 @@ public List<OwnerResourceSummary> getOwnerResourceSummaries(String owner) throws
             IStormClusterState state = stormClusterState;
             Map<String, Assignment> topoIdToAssignments = state.topologyAssignments();
             Map<String, StormBase> topoIdToBases = state.topologyBases();
-            Map<String, Object> clusterSchedulerConfig = scheduler.config();
+            Map<String, Number> clusterSchedulerConfig = scheduler.config();
 
             //put [owner-> StormBase-list] mapping to ownerToBasesMap
             //if this owner (the input parameter) is null, add all the owners with stormbase and guarantees

File: storm-server/src/main/java/org/apache/storm/metricstore/MetricStore.java
Patch:
@@ -28,7 +28,7 @@ public interface MetricStore extends AutoCloseable {
      * @param config Storm config map
      * @throws MetricException on preparation error
      */
-    void prepare(Map config) throws MetricException;
+    void prepare(Map<String, Object> config) throws MetricException;
 
     /**
      * Stores a metric in the store.

File: storm-server/src/main/java/org/apache/storm/metricstore/MetricStoreConfig.java
Patch:
@@ -30,7 +30,7 @@ public class MetricStoreConfig {
      * @return MetricStore prepared store
      * @throws MetricException  on misconfiguration
      */
-    public static MetricStore configure(Map conf) throws MetricException {
+    public static MetricStore configure(Map<String, Object> conf) throws MetricException {
 
         try {
             String storeClass = (String)conf.get(DaemonConfig.STORM_METRIC_STORE_CLASS);

File: storm-server/src/main/java/org/apache/storm/metricstore/NimbusMetricProcessor.java
Patch:
@@ -37,5 +37,5 @@ public void processWorkerMetrics(Map<String, Object> conf, WorkerMetrics metrics
     }
 
     @Override
-    public void prepare(Map config) throws MetricException {}
+    public void prepare(Map<String, Object> config) throws MetricException {}
 }

File: storm-server/src/main/java/org/apache/storm/metricstore/WorkerMetricsProcessor.java
Patch:
@@ -36,5 +36,5 @@ public interface WorkerMetricsProcessor {
      * @param config Storm config map
      * @throws MetricException  on error
      */
-    void prepare(Map config) throws MetricException;
+    void prepare(Map<String, Object> config) throws MetricException;
 }

File: storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/RocksDbStore.java
Patch:
@@ -68,7 +68,7 @@ interface RocksDbScanCallback {
      * @param config Storm config map
      * @throws MetricException on preparation error
      */
-    public void prepare(Map config) throws MetricException {
+    public void prepare(Map<String, Object> config) throws MetricException {
         validateConfig(config);
 
         this.failureMeter = StormMetricsRegistry.registerMeter("RocksDB:metric-failures");
@@ -125,7 +125,7 @@ public void prepare(Map config) throws MetricException {
      * @throws MetricException if there is a missing required configuration or if the store does not exist but
      *                         the config specifies not to create the store
      */
-    private void validateConfig(Map config) throws MetricException {
+    private void validateConfig(Map<String, Object> config) throws MetricException {
         if (!(config.containsKey(DaemonConfig.STORM_ROCKSDB_LOCATION))) {
             throw new MetricException("Not a vaild RocksDB configuration - Missing store location " + DaemonConfig.STORM_ROCKSDB_LOCATION);
         }
@@ -156,7 +156,7 @@ private void validateConfig(Map config) throws MetricException {
         }
     }
 
-    private String getRocksDbAbsoluteDir(Map conf) throws MetricException {
+    private String getRocksDbAbsoluteDir(Map<String, Object> conf) throws MetricException {
         String storePath = (String)conf.get(DaemonConfig.STORM_ROCKSDB_LOCATION);
         if (storePath == null) {
             throw new MetricException("Not a vaild RocksDB configuration - Missing store location " + DaemonConfig.STORM_ROCKSDB_LOCATION);

File: storm-server/src/main/java/org/apache/storm/nimbus/DefaultTopologyValidator.java
Patch:
@@ -22,9 +22,11 @@
 import java.util.Map;
 
 public class DefaultTopologyValidator implements ITopologyValidator {
+
     @Override
-    public void prepare(Map StormConf){
+    public void prepare(Map<String, Object> StormConf){
     }
+
     @Override
     public void validate(String topologyName, Map<String, Object> topologyConf, StormTopology topology) throws InvalidTopologyException {        
     }    

File: storm-server/src/main/java/org/apache/storm/nimbus/ITopologyActionNotifierPlugin.java
Patch:
@@ -27,7 +27,7 @@ public interface ITopologyActionNotifierPlugin {
      * Called once during nimbus initialization.
      * @param StormConf
      */
-    void prepare(Map StormConf);
+    void prepare(Map<String, Object> StormConf);
 
     /**
      * When a new actions is executed for a topology, this method will be called.

File: storm-server/src/main/java/org/apache/storm/nimbus/ITopologyValidator.java
Patch:
@@ -22,7 +22,9 @@
 import java.util.Map;
 
 public interface ITopologyValidator {
-    void prepare(Map StormConf);
+
+    void prepare(Map<String, Object> StormConf);
+
     void validate(String topologyName, Map<String, Object> topologyConf, StormTopology topology)
             throws InvalidTopologyException;
 }

File: storm-server/src/main/java/org/apache/storm/scheduler/DefaultScheduler.java
Patch:
@@ -106,7 +106,7 @@ public void schedule(Topologies topologies, Cluster cluster) {
     }
 
     @Override
-    public Map<String, Object> config() {
+    public Map<String, Map<String, Double>> config() {
         return new HashMap<>();
     }
 }

File: storm-server/src/main/java/org/apache/storm/scheduler/EvenScheduler.java
Patch:
@@ -173,7 +173,7 @@ public void schedule(Topologies topologies, Cluster cluster) {
     }
 
     @Override
-    public Map<String, Object> config() {
+    public Map<String, Map<String, Double>> config() {
         return new HashMap<>();
     }
 

File: storm-server/src/main/java/org/apache/storm/scheduler/IScheduler.java
Patch:
@@ -43,5 +43,5 @@ public interface IScheduler {
      *
      * @return The scheduler's configuration.
      */
-    Map<String, Object> config();
+    Map config();
 }

File: storm-server/src/main/java/org/apache/storm/scheduler/IsolationScheduler.java
Patch:
@@ -57,7 +57,7 @@ public void prepare(Map<String, Object> conf) {
     }
 
     @Override
-    public Map<String, Object> config() {
+    public Map<String, Map<String, Double>> config() {
         return new HashMap<>();
     }
 

File: storm-server/src/main/java/org/apache/storm/scheduler/blacklist/strategies/DefaultBlacklistStrategy.java
Patch:
@@ -53,7 +53,7 @@ public class DefaultBlacklistStrategy implements IBlacklistStrategy {
     private TreeMap<String, Integer> blacklist;
 
     @Override
-    public void prepare(Map conf) {
+    public void prepare(Map<String, Object> conf) {
         toleranceCount = ObjectReader.getInt(conf.get(DaemonConfig.BLACKLIST_SCHEDULER_TOLERANCE_COUNT),
                 DEFAULT_BLACKLIST_SCHEDULER_TOLERANCE_COUNT);
         resumeTime = ObjectReader.getInt(conf.get(DaemonConfig.BLACKLIST_SCHEDULER_RESUME_TIME), DEFAULT_BLACKLIST_SCHEDULER_RESUME_TIME);

File: storm-server/src/main/java/org/apache/storm/scheduler/blacklist/strategies/IBlacklistStrategy.java
Patch:
@@ -27,7 +27,7 @@
 
 public interface IBlacklistStrategy {
 
-    void prepare(Map conf);
+    void prepare(Map<String, Object> conf);
 
     /**
      * Get blacklist by blacklist strategy.

File: storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceAwareScheduler.java
Patch:
@@ -63,7 +63,7 @@ public void prepare(Map<String, Object> conf) {
     }
 
     @Override
-    public Map<String, Object> config() {
+    public Map<String, Map<String, Double>> config() {
         return (Map) getUserResourcePools();
     }
 
@@ -282,7 +282,7 @@ private Map<String, Map<String, Double>> getUserResourcePools() {
         }
 
         // if no configs from loader, try to read from user-resource-pools.yaml
-        Map fromFile = Utils.findAndReadConfigFile("user-resource-pools.yaml", false);
+        Map<String, Object> fromFile = Utils.findAndReadConfigFile("user-resource-pools.yaml", false);
         raw = (Map<String, Map<String, Number>>) fromFile.get(DaemonConfig.RESOURCE_AWARE_SCHEDULER_USER_POOLS);
         if (raw != null) {
             return convertToDouble(raw);

File: storm-server/src/main/java/org/apache/storm/scheduler/utils/FileConfigLoader.java
Patch:
@@ -56,12 +56,12 @@ public FileConfigLoader(Map<String, Object> conf) {
      * @return The scheduler configuration if exists; null otherwise.
      */
     @Override
-    public Map load(String configKey) {
+    public Map<String, Object> load(String configKey) {
         if (targetFilePath != null) {
             try {
-                Map raw = (Map) Utils.readYamlFile(targetFilePath);
+                Map<String, Object> raw = (Map<String, Object>) Utils.readYamlFile(targetFilePath);
                 if (raw != null) {
-                    return (Map) raw.get(configKey);
+                    return (Map<String, Object>) raw.get(configKey);
                 }
             } catch (Exception e) {
                 LOG.error("Failed to load from file {}", targetFilePath);

File: storm-server/src/main/java/org/apache/storm/utils/ServerUtils.java
Patch:
@@ -132,7 +132,7 @@ public static BlobStore getNimbusBlobStore(Map<String, Object> conf, String base
             type = LocalFsBlobStore.class.getName();
         }
         BlobStore store = (BlobStore) ReflectionUtils.newInstance(type);
-        HashMap nconf = new HashMap(conf);
+        Map<String, Object> nconf = new HashMap<>(conf);
         // only enable cleanup of blobstore on nimbus
         nconf.put(Config.BLOBSTORE_CLEANUP_ENABLE, Boolean.TRUE);
 

File: storm-server/src/test/java/org/apache/storm/scheduler/blacklist/FaultGenerateUtils.java
Patch:
@@ -50,7 +50,7 @@ public static List<Map<String, SupervisorDetails>> getSupervisorsList(int superv
         return supervisorsList;
     }
 
-    public static Cluster nextCluster(Cluster cluster, Map<String, SupervisorDetails> supervisors, INimbus iNimbus, Map config,
+    public static Cluster nextCluster(Cluster cluster, Map<String, SupervisorDetails> supervisors, INimbus iNimbus, Map<String, Object> config,
                                       Topologies topologies) {
         Map<String, SchedulerAssignmentImpl> assignment;
         if (cluster == null) {

File: storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/webapp/LogviewerResource.java
Patch:
@@ -258,9 +258,9 @@ public Response deepSearch(@PathParam("topoId") String topologyId,
                 startFileOffset, startByteOffset, BooleanUtils.toBooleanObject(searchArchived), callback, origin);
     }
 
-    private int parseIntegerFromMap(Map map, String parameterKey) throws InvalidRequestException {
+    private int parseIntegerFromMap(Map<String, String[]> map, String parameterKey) throws InvalidRequestException {
         try {
-            return Integer.parseInt(((String[]) map.get(parameterKey))[0]);
+            return Integer.parseInt(map.get(parameterKey)[0]);
         } catch (NumberFormatException ex) {
             throw new InvalidRequestException("Could not make an integer out of the query parameter '"
                 + parameterKey + "'", ex);

File: examples/storm-hbase-examples/src/main/java/org/apache/storm/hbase/topology/TotalWordCounter.java
Patch:
@@ -37,7 +37,6 @@ public class TotalWordCounter implements IBasicBolt {
     private BigInteger total = BigInteger.ZERO;
     private static final Logger LOG = LoggerFactory.getLogger(TotalWordCounter.class);
     private static final Random RANDOM = new Random();
-    @SuppressWarnings("rawtypes")
     public void prepare(Map<String, Object> topoConf, TopologyContext context) {
     }
 

File: examples/storm-hbase-examples/src/main/java/org/apache/storm/hbase/topology/WordCounter.java
Patch:
@@ -31,7 +31,6 @@
 public class WordCounter implements IBasicBolt {
 
 
-    @SuppressWarnings("rawtypes")
     public void prepare(Map<String, Object> topoConf, TopologyContext context) {
     }
 

File: examples/storm-hbase-examples/src/main/java/org/apache/storm/hbase/topology/WordSpout.java
Patch:
@@ -45,7 +45,6 @@ public boolean isDistributed() {
         return this.isDistributed;
     }
 
-    @SuppressWarnings("rawtypes")
     public void open(Map<String, Object> conf, TopologyContext context, SpoutOutputCollector collector) {
         this.collector = collector;
     }

File: examples/storm-jdbc-examples/src/main/java/org/apache/storm/jdbc/spout/UserSpout.java
Patch:
@@ -47,7 +47,6 @@ public boolean isDistributed() {
         return this.isDistributed;
     }
 
-    @SuppressWarnings("rawtypes")
     public void open(Map<String, Object> conf, TopologyContext context, SpoutOutputCollector collector) {
         this.collector = collector;
     }

File: examples/storm-jms-examples/src/main/java/org/apache/storm/jms/example/GenericBolt.java
Patch:
@@ -67,7 +67,6 @@ public GenericBolt(String name, boolean autoAck, boolean autoAnchor) {
         this(name, autoAck, autoAnchor, null);
     }
 
-    @SuppressWarnings("rawtypes")
     public void prepare(Map<String, Object> topoConf, TopologyContext context,
                         OutputCollector collector) {
         this.collector = collector;

File: examples/storm-mongodb-examples/src/main/java/org/apache/storm/mongodb/topology/TotalWordCounter.java
Patch:
@@ -37,7 +37,6 @@ public class TotalWordCounter implements IBasicBolt {
     private BigInteger total = BigInteger.ZERO;
     private static final Logger LOG = LoggerFactory.getLogger(TotalWordCounter.class);
     private static final Random RANDOM = new Random();
-    @SuppressWarnings("rawtypes")
     public void prepare(Map<String, Object> topoConf, TopologyContext context) {
     }
 

File: examples/storm-mongodb-examples/src/main/java/org/apache/storm/mongodb/topology/WordSpout.java
Patch:
@@ -45,7 +45,6 @@ public boolean isDistributed() {
         return this.isDistributed;
     }
 
-    @SuppressWarnings("rawtypes")
     public void open(Map<String, Object> conf, TopologyContext context, SpoutOutputCollector collector) {
         this.collector = collector;
     }

File: examples/storm-redis-examples/src/main/java/org/apache/storm/redis/topology/WordCounter.java
Patch:
@@ -31,7 +31,6 @@
 public class WordCounter implements IBasicBolt {
     private Map<String, Integer> wordCounter = Maps.newHashMap();
 
-    @SuppressWarnings("rawtypes")
     public void prepare(Map<String, Object> topoConf, TopologyContext context) {
     }
 

File: examples/storm-redis-examples/src/main/java/org/apache/storm/redis/topology/WordSpout.java
Patch:
@@ -45,7 +45,6 @@ public boolean isDistributed() {
         return this.isDistributed;
     }
 
-    @SuppressWarnings("rawtypes")
     public void open(Map<String, Object> conf, TopologyContext context, SpoutOutputCollector collector) {
         this.collector = collector;
     }

File: examples/storm-starter/src/jvm/org/apache/storm/starter/bolt/RollingCountAggBolt.java
Patch:
@@ -44,7 +44,6 @@ public class RollingCountAggBolt extends BaseRichBolt {
   private OutputCollector collector;
 
 
-  @SuppressWarnings("rawtypes")
   @Override
   public void prepare(Map<String, Object> topoConf, TopologyContext context, OutputCollector collector) {
     this.collector = collector;

File: examples/storm-starter/src/jvm/org/apache/storm/starter/bolt/RollingCountBolt.java
Patch:
@@ -85,7 +85,6 @@ private int deriveNumWindowChunksFrom(int windowLengthInSeconds, int windowUpdat
     return windowLengthInSeconds / windowUpdateFrequencyInSeconds;
   }
 
-  @SuppressWarnings("rawtypes")
   @Override
   public void prepare(Map<String, Object> topoConf, TopologyContext context, OutputCollector collector) {
     this.collector = collector;

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/bolt/HBaseLookupBolt.java
Patch:
@@ -71,7 +71,6 @@ public HBaseLookupBolt withProjectionCriteria(HBaseProjectionCriteria projection
           return this;
      }
 
-     @SuppressWarnings({ "unchecked", "rawtypes" })
      @Override
      public void prepare(Map<String, Object> config, TopologyContext topologyContext, OutputCollector collector) {
           super.prepare(config, topologyContext, collector);

File: external/storm-jms/src/main/java/org/apache/storm/jms/trident/TridentJmsSpout.java
Patch:
@@ -163,12 +163,12 @@ private static final String toDeliveryModeString(int acknowledgeMode) {
     
     @Override
     public ITridentSpout.BatchCoordinator<JmsBatch> getCoordinator(
-            String txStateId, @SuppressWarnings("rawtypes") Map<String, Object> conf, TopologyContext context) {
+            String txStateId, Map<String, Object> conf, TopologyContext context) {
         return new JmsBatchCoordinator(name);
     }
 
     @Override
-    public Emitter<JmsBatch> getEmitter(String txStateId, @SuppressWarnings("rawtypes") Map<String, Object> conf, TopologyContext context) {
+    public Emitter<JmsBatch> getEmitter(String txStateId, Map<String, Object> conf, TopologyContext context) {
         return new JmsEmitter(name, jmsProvider, tupleProducer, jmsAcknowledgeMode, conf);
     }
 
@@ -210,7 +210,7 @@ private class JmsEmitter implements Emitter<JmsBatch>, MessageListener {
        
         private final Logger LOG = LoggerFactory.getLogger(JmsEmitter.class);
  
-        public JmsEmitter(String name, JmsProvider jmsProvider, JmsTupleProducer tupleProducer, int jmsAcknowledgeMode, @SuppressWarnings("rawtypes") Map<String, Object> conf) {
+        public JmsEmitter(String name, JmsProvider jmsProvider, JmsTupleProducer tupleProducer, int jmsAcknowledgeMode, Map<String, Object> conf) {
             if (jmsProvider == null) {
                 throw new IllegalStateException("JMS provider has not been set.");
             }

File: external/storm-redis/src/main/java/org/apache/storm/redis/trident/state/RedisClusterMapState.java
Patch:
@@ -230,7 +230,7 @@ public Factory(JedisClusterConfig jedisClusterConfig, StateType type, Options op
          * {@inheritDoc}
          */
         @Override
-        public State makeState(@SuppressWarnings("rawtypes") Map<String, Object> conf, IMetricsContext metrics, int partitionIndex, int numPartitions) {
+        public State makeState(Map<String, Object> conf, IMetricsContext metrics, int partitionIndex, int numPartitions) {
             JedisCluster jedisCluster = new JedisCluster(jedisClusterConfig.getNodes(),
                     jedisClusterConfig.getTimeout(),
                     jedisClusterConfig.getTimeout(),

File: external/storm-redis/src/main/java/org/apache/storm/redis/trident/state/RedisClusterState.java
Patch:
@@ -69,7 +69,7 @@ public Factory(JedisClusterConfig config) {
          * {@inheritDoc}
          */
         @Override
-        public State makeState(@SuppressWarnings("rawtypes") Map<String, Object> conf, IMetricsContext metrics, int partitionIndex, int numPartitions) {
+        public State makeState(Map<String, Object> conf, IMetricsContext metrics, int partitionIndex, int numPartitions) {
             JedisCluster jedisCluster = new JedisCluster(jedisClusterConfig.getNodes(),
                                                     jedisClusterConfig.getTimeout(),
                                                     jedisClusterConfig.getTimeout(),

File: external/storm-redis/src/main/java/org/apache/storm/redis/trident/state/RedisMapState.java
Patch:
@@ -230,7 +230,7 @@ public Factory(JedisPoolConfig jedisPoolConfig, StateType type, Options options)
          * {@inheritDoc}
          */
         @Override
-        public State makeState(@SuppressWarnings("rawtypes") Map<String, Object> conf, IMetricsContext metrics, int partitionIndex, int numPartitions) {
+        public State makeState(Map<String, Object> conf, IMetricsContext metrics, int partitionIndex, int numPartitions) {
             JedisPool jedisPool = new JedisPool(DEFAULT_POOL_CONFIG,
                                                     jedisPoolConfig.getHost(),
                                                     jedisPoolConfig.getPort(),

File: external/storm-redis/src/main/java/org/apache/storm/redis/trident/state/RedisState.java
Patch:
@@ -69,7 +69,7 @@ public Factory(JedisPoolConfig config) {
          * {@inheritDoc}
          */
         @Override
-        public State makeState(@SuppressWarnings("rawtypes") Map<String, Object> conf, IMetricsContext metrics, int partitionIndex, int numPartitions) {
+        public State makeState(Map<String, Object> conf, IMetricsContext metrics, int partitionIndex, int numPartitions) {
             JedisPool jedisPool = new JedisPool(DEFAULT_POOL_CONFIG,
                                                 jedisPoolConfig.getHost(),
                                                 jedisPoolConfig.getPort(),

File: flux/flux-examples/src/main/java/org/apache/storm/flux/examples/StatefulWordCounter.java
Patch:
@@ -34,7 +34,6 @@ public class StatefulWordCounter extends BaseStatefulBolt<KeyValueState<String,
     private KeyValueState<String, Long> wordCounts;
     private OutputCollector collector;
 
-    @SuppressWarnings("rawtypes")
     @Override
     public void prepare(Map<String, Object> topoConf, TopologyContext context, OutputCollector collector) {
         this.collector = collector;

File: flux/flux-examples/src/main/java/org/apache/storm/flux/examples/WordCounter.java
Patch:
@@ -43,7 +43,6 @@ public class WordCounter extends BaseBasicBolt {
 
 
 
-    @SuppressWarnings("rawtypes")
     public void prepare(Map<String, Object> topoConf, TopologyContext context) {
     }
 

File: storm-client/src/jvm/org/apache/storm/messaging/local/Context.java
Patch:
@@ -233,7 +233,6 @@ private static LocalServer getLocalServer(String nodeId, int port) {
         return ret;
     }
         
-    @SuppressWarnings("rawtypes")
     @Override
     public void prepare(Map<String, Object> topoConf) {
         //NOOP
@@ -249,7 +248,6 @@ public IConnection connect(String storm_id, String host, int port, AtomicBoolean
         return new LocalClient(getLocalServer(storm_id, port));
     }
 
-    @SuppressWarnings("rawtypes")
     @Override
     public void term() {
         //NOOP

File: storm-client/src/jvm/org/apache/storm/messaging/netty/Client.java
Patch:
@@ -151,7 +151,6 @@ public class Client extends ConnectionWithStatus implements IStatefulObject, ISa
     // wait strategy when the netty channel is not writable
     private final IWaitStrategy waitStrategy;
 
-    @SuppressWarnings("rawtypes")
     Client(Map<String, Object> topoConf, AtomicBoolean[] remoteBpStatus, ChannelFactory factory, HashedWheelTimer scheduler, String host, int port, Context context) {
         this.topoConf = topoConf;
         closing = false;

File: storm-client/src/jvm/org/apache/storm/messaging/netty/Context.java
Patch:
@@ -31,7 +31,6 @@
 import org.apache.storm.messaging.IContext;
 
 public class Context implements IContext {
-    @SuppressWarnings("rawtypes")
     private Map<String, Object> topoConf;
     private Map<String, IConnection> connections;
     private NioClientSocketChannelFactory clientChannelFactory;
@@ -40,7 +39,6 @@ public class Context implements IContext {
     /**
      * initialization per Storm configuration 
      */
-    @SuppressWarnings("rawtypes")
     public void prepare(Map<String, Object> topoConf) {
         this.topoConf = topoConf;
         connections = new HashMap<>();

File: storm-client/src/jvm/org/apache/storm/messaging/netty/Server.java
Patch:
@@ -53,7 +53,6 @@
 class Server extends ConnectionWithStatus implements IStatefulObject, ISaslServer {
 
     private static final Logger LOG = LoggerFactory.getLogger(Server.class);
-    @SuppressWarnings("rawtypes")
     Map<String, Object> topoConf;
     int port;
     private final ConcurrentHashMap<String, AtomicInteger> messagesEnqueued = new ConcurrentHashMap<>();
@@ -70,7 +69,6 @@ class Server extends ConnectionWithStatus implements IStatefulObject, ISaslServe
     private Supplier<Object> newConnectionResponse;
     private final int boundPort;
     
-    @SuppressWarnings("rawtypes")
     Server(Map<String, Object> topoConf, int port) {
         this.topoConf = topoConf;
         this.port = port;

File: storm-client/src/jvm/org/apache/storm/metric/SystemBolt.java
Patch:
@@ -89,7 +89,7 @@ public Object getValueAndReset() {
         }
     }
 
-    @SuppressWarnings({ "unchecked", "rawtypes" })
+    @SuppressWarnings({ "unchecked" })
     @Override
     public void prepare(final Map<String, Object> topoConf, TopologyContext context, OutputCollector collector) {
         if(_prepareWasCalled && !"local".equals(topoConf.get(Config.STORM_CLUSTER_MODE))) {

File: storm-server/src/main/java/org/apache/storm/Testing.java
Patch:
@@ -622,7 +622,6 @@ public static <T> Map<T, Integer> multiset(Collection<T> c) {
         return ret;
     }
     
-    @SuppressWarnings("rawtypes")
     private static void printRec(Object o, String prefix) {
         if (o instanceof Collection) {
             LOG.info("{} {} ({}) [",prefix,o, o.getClass());

File: storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java
Patch:
@@ -406,7 +406,7 @@ public Map<K, V> apply(Map<K, V> t) {
     public static class StandaloneINimbus implements INimbus {
 
         @Override
-        public void prepare(@SuppressWarnings("rawtypes") Map<String, Object> topoConf, String schedulerLocalDir) {
+        public void prepare(Map<String, Object> topoConf, String schedulerLocalDir) {
             //NOOP
         }
 

File: storm-server/src/main/java/org/apache/storm/scheduler/multitenant/MultitenantScheduler.java
Patch:
@@ -33,7 +33,6 @@
 
 public class MultitenantScheduler implements IScheduler {
   private static final Logger LOG = LoggerFactory.getLogger(MultitenantScheduler.class);
-  @SuppressWarnings("rawtypes")
   private Map<String, Object> conf;
   protected IConfigLoader configLoader;
   

File: examples/storm-hdfs-examples/src/main/java/org/apache/storm/hdfs/spout/HdfsSpoutTopology.java
Patch:
@@ -132,7 +132,7 @@ public static void main(String[] args) throws Exception {
     builder.setBolt(BOLT_ID, bolt, 1).shuffleGrouping(SPOUT_ID);
 
     // 4 - submit topology, wait for a few min and terminate it
-    Map clusterConf = Utils.readStormConfig();
+    Map<String, Object> clusterConf = Utils.readStormConfig();
     StormSubmitter.submitTopologyWithProgressBar(topologyName, conf, builder.createTopology());
     Nimbus.Iface client = NimbusClient.getConfiguredClient(clusterConf).getClient();
 

File: examples/storm-hive-examples/src/main/java/org/apache/storm/hive/trident/TridentHiveTopology.java
Patch:
@@ -177,7 +177,7 @@ public void close() {
         }
 
         @Override
-        public Map getComponentConfiguration() {
+        public Map<String, Object> getComponentConfiguration() {
             Config conf = new Config();
             conf.setMaxTaskParallelism(1);
             return conf;

File: examples/storm-jdbc-examples/src/main/java/org/apache/storm/jdbc/topology/AbstractUserTopology.java
Patch:
@@ -71,7 +71,7 @@ public void execute(String[] args) throws Exception {
                     + "<user> <password> [topology name]");
             System.exit(-1);
         }
-        Map map = Maps.newHashMap();
+        Map<String, Object> map = Maps.newHashMap();
         map.put("dataSourceClassName", args[0]);//com.mysql.jdbc.jdbc2.optional.MysqlDataSource
         map.put("dataSource.url", args[1]);//jdbc:mysql://localhost/test
         map.put("dataSource.user", args[2]);//root

File: examples/storm-loadgen/src/main/java/org/apache/storm/loadgen/CaptureLoad.java
Patch:
@@ -435,7 +435,7 @@ static Map<String, Double> parseResources(String input) {
      * @param topologyConf topology configuration
      * @throws Exception on any error
      */
-    public static void checkInitialization(Map<String, Double> topologyResources, String componentId, Map topologyConf) {
+    public static void checkInitialization(Map<String, Double> topologyResources, String componentId, Map<String, Object> topologyConf) {
         StringBuilder msgBuilder = new StringBuilder();
 
         for (String resourceName : topologyResources.keySet()) {
@@ -450,7 +450,7 @@ public static void checkInitialization(Map<String, Double> topologyResources, St
         }
     }
 
-    private static String checkInitResource(Map<String, Double> topologyResources, Map topologyConf, String resourceName) {
+    private static String checkInitResource(Map<String, Double> topologyResources, Map<String, Object> topologyConf, String resourceName) {
         StringBuilder msgBuilder = new StringBuilder();
         if (topologyResources.containsKey(resourceName)) {
             Double resourceValue = (Double) topologyConf.getOrDefault(resourceName, null);

File: examples/storm-loadgen/src/main/java/org/apache/storm/loadgen/HttpForwardingMetricsServer.java
Patch:
@@ -41,7 +41,7 @@
  * A server that can listen for metrics from the HttpForwardingMetricsConsumer.
  */
 public abstract class HttpForwardingMetricsServer {
-    private Map conf;
+    private Map<String, Object> conf;
     private Server server = null;
     private int port = -1;
     private String url = null;

File: examples/storm-loadgen/src/main/java/org/apache/storm/loadgen/ThroughputVsLatency.java
Patch:
@@ -97,7 +97,8 @@ public SplitSentence(SlowExecutorPattern slowness) {
         }
 
         @Override
-        public void prepare(Map stormConf, TopologyContext context) {
+        public void prepare(Map<String, Object> stormConf,
+                TopologyContext context) {
             executorIndex = context.getThisTaskIndex();
             sleep.prepare();
         }

File: examples/storm-perf/src/main/java/org/apache/storm/perf/SimplifiedWordCountTopo.java
Patch:
@@ -44,7 +44,7 @@ public class SimplifiedWordCountTopo {
     public static final int DEFAULT_COUNT_BOLT_NUM = 1;
 
 
-    static StormTopology getTopology(Map config) {
+    static StormTopology getTopology(Map<String, Object> config) {
 
         final int spoutNum = Helper.getInt(config, SPOUT_NUM, DEFAULT_SPOUT_NUM);
         final int cntBoltNum = Helper.getInt(config, BOLT_NUM, DEFAULT_COUNT_BOLT_NUM);

File: examples/storm-perf/src/main/java/org/apache/storm/perf/utils/Helper.java
Patch:
@@ -70,7 +70,7 @@ public static void collectMetricsAndKill(String topologyName, Integer pollInterv
 
     /** Kill topo on Ctrl-C */
     public static void setupShutdownHook(final String topoName) {
-        Map clusterConf = Utils.readStormConfig();
+        Map<String, Object> clusterConf = Utils.readStormConfig();
         final Nimbus.Iface client = NimbusClient.getConfiguredClient(clusterConf).getClient();
         Runtime.getRuntime().addShutdownHook(new Thread() {
             public void run() {

File: examples/storm-starter/src/jvm/org/apache/storm/starter/FastWordCountTopology.java
Patch:
@@ -183,7 +183,7 @@ public static void main(String[] args) throws Exception {
     conf.setNumWorkers(1);
     StormSubmitter.submitTopologyWithProgressBar(name, conf, builder.createTopology());
 
-    Map clusterConf = Utils.readStormConfig();
+    Map<String, Object> clusterConf = Utils.readStormConfig();
     clusterConf.putAll(Utils.readCommandLineOpts());
     Nimbus.Iface client = NimbusClient.getConfiguredClient(clusterConf).getClient();
 

File: examples/storm-starter/src/jvm/org/apache/storm/starter/InOrderDeliveryTest.java
Patch:
@@ -159,7 +159,7 @@ public static void main(String[] args) throws Exception {
     conf.setNumWorkers(1);
     StormSubmitter.submitTopologyWithProgressBar(name, conf, builder.createTopology());
 
-    Map clusterConf = Utils.readStormConfig();
+    Map<String, Object> clusterConf = Utils.readStormConfig();
     clusterConf.putAll(Utils.readCommandLineOpts());
     Nimbus.Iface client = NimbusClient.getConfiguredClient(clusterConf).getClient();
 

File: examples/storm-starter/src/jvm/org/apache/storm/starter/ResourceAwareExampleTopology.java
Patch:
@@ -64,7 +64,7 @@ protected static void addToCache(String key, String value) {
         }
 
         @Override
-        public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
+        public void prepare(Map<String, Object> conf, TopologyContext context, OutputCollector collector) {
             _collector = collector;
         }
 

File: external/storm-autocreds/src/main/java/org/apache/storm/common/AbstractHadoopAutoCreds.java
Patch:
@@ -45,7 +45,7 @@ public abstract class AbstractHadoopAutoCreds implements IAutoCredentials, Crede
     private Set<String> configKeys = new HashSet<>();
 
     @Override
-    public void prepare(Map topoConf) {
+    public void prepare(Map<String, Object> topoConf) {
         doPrepare(topoConf);
         loadConfigKeys(topoConf);
     }
@@ -83,7 +83,7 @@ public Set<Pair<String, Credentials>> getCredentials(Map<String, String> credent
      *
      * @param topoConf the topology conf
      */
-    protected abstract void doPrepare(Map topoConf);
+    protected abstract void doPrepare(Map<String, Object> topoConf);
 
     /**
      * The lookup key for the config key string

File: external/storm-autocreds/src/main/java/org/apache/storm/hbase/security/AutoHBase.java
Patch:
@@ -30,7 +30,7 @@
  */
 public class AutoHBase extends AbstractHadoopAutoCreds {
     @Override
-    public void doPrepare(Map conf) {
+    public void doPrepare(Map<String, Object> conf) {
     }
 
     @Override

File: external/storm-autocreds/src/main/java/org/apache/storm/hbase/security/AutoHBaseCommand.java
Patch:
@@ -39,7 +39,7 @@ private AutoHBaseCommand() {
 
   @SuppressWarnings("unchecked")
   public static void main(String[] args) throws Exception {
-    Map conf = new HashMap();
+    Map<String, Object> conf = new HashMap<>();
     conf.put(HBASE_PRINCIPAL_KEY, args[1]); // hbase principal storm-hbase@WITZEN.COM
     conf.put(HBASE_KEYTAB_FILE_KEY,
         args[2]); // storm hbase keytab /etc/security/keytabs/storm-hbase.keytab

File: external/storm-autocreds/src/main/java/org/apache/storm/hdfs/security/AutoHDFS.java
Patch:
@@ -30,7 +30,7 @@
  */
 public class AutoHDFS extends AbstractHadoopAutoCreds {
     @Override
-    public void doPrepare(Map conf) {
+    public void doPrepare(Map<String, Object> conf) {
     }
 
     @Override

File: external/storm-autocreds/src/main/java/org/apache/storm/hdfs/security/AutoHDFSCommand.java
Patch:
@@ -39,7 +39,7 @@ private AutoHDFSCommand() {
 
   @SuppressWarnings("unchecked")
   public static void main(String[] args) throws Exception {
-    Map conf = new HashMap();
+    Map<String, Object> conf = new HashMap<>();
     conf.put(STORM_USER_NAME_KEY, args[1]); //with realm e.g. hdfs@WITZEND.COM
     conf.put(STORM_KEYTAB_FILE_KEY, args[2]);// /etc/security/keytabs/storm.keytab
 

File: external/storm-autocreds/src/main/java/org/apache/storm/hdfs/security/HdfsSecurityUtil.java
Patch:
@@ -49,7 +49,7 @@ public final class HdfsSecurityUtil {
     private HdfsSecurityUtil() {
     }
 
-    public static void login(Map conf, Configuration hdfsConfig) throws IOException {
+    public static void login(Map<String, Object> conf, Configuration hdfsConfig) throws IOException {
         //If AutoHDFS is specified, do not attempt to login using keytabs, only kept for backward compatibility.
         if(conf.get(TOPOLOGY_AUTO_CREDENTIALS) == null ||
                 (!(((List)conf.get(TOPOLOGY_AUTO_CREDENTIALS)).contains(AutoHDFS.class.getName())) &&

File: external/storm-autocreds/src/main/java/org/apache/storm/hive/security/AutoHive.java
Patch:
@@ -31,7 +31,7 @@
  */
 public class AutoHive extends AbstractHadoopAutoCreds {
     @Override
-    public void doPrepare(Map conf) {
+    public void doPrepare(Map<String, Object> conf) {
     }
 
     @Override

File: external/storm-autocreds/src/main/java/org/apache/storm/hive/security/AutoHiveCommand.java
Patch:
@@ -40,7 +40,7 @@ private AutoHiveCommand() {
 
   @SuppressWarnings("unchecked")
   public static void main(String[] args) throws Exception {
-    Map conf = new HashMap();
+    Map<String, Object> conf = new HashMap<>();
     conf.put(HIVE_PRINCIPAL_KEY, args[1]); // hive principal storm-hive@WITZEN.COM
     conf.put(HIVE_KEYTAB_FILE_KEY, args[2]); // storm hive keytab /etc/security/keytabs/storm-hive.keytab
     conf.put(HiveConf.ConfVars.METASTOREURIS.varname, args[3]); // hive.metastore.uris : "thrift://pm-eng1-cluster1.field.hortonworks.com:9083"

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/query/impl/ObjectMapperCqlStatementMapper.java
Patch:
@@ -67,7 +67,7 @@ public ObjectMapperCqlStatementMapper(String operationField, String valueField,
     }
 
     @Override
-    public List<Statement> map(Map map, Session session, ITuple tuple) {
+    public List<Statement> map(Map<String, Object> map, Session session, ITuple tuple) {
         final ObjectMapperOperation operation = (ObjectMapperOperation)tuple.getValueByField(operationField);
 
         Preconditions.checkNotNull(operation, "Operation must not be null");

File: external/storm-cassandra/src/test/java/org/apache/storm/cassandra/WeatherSpout.java
Patch:
@@ -56,7 +56,7 @@ public void declareOutputFields(OutputFieldsDeclarer outputFieldsDeclarer) {
     }
 
     @Override
-    public void open(Map map, TopologyContext topologyContext, SpoutOutputCollector spoutOutputCollector) {
+    public void open(Map<String, Object> map, TopologyContext topologyContext, SpoutOutputCollector spoutOutputCollector) {
         this.spoutOutputCollector = spoutOutputCollector;
     }
 

File: external/storm-elasticsearch/src/main/java/org/apache/storm/elasticsearch/bolt/AbstractEsBolt.java
Patch:
@@ -49,7 +49,7 @@ public AbstractEsBolt(EsConfig esConfig) {
     }
 
     @Override
-    public void prepare(Map map, TopologyContext topologyContext, OutputCollector outputCollector) {
+    public void prepare(Map<String, Object> map, TopologyContext topologyContext, OutputCollector outputCollector) {
         try {
             this.collector = outputCollector;
             synchronized (AbstractEsBolt.class) {

File: external/storm-elasticsearch/src/main/java/org/apache/storm/elasticsearch/bolt/EsIndexBolt.java
Patch:
@@ -56,7 +56,7 @@ public EsIndexBolt(EsConfig esConfig, EsTupleMapper tupleMapper) {
     }
 
     @Override
-    public void prepare(Map map, TopologyContext topologyContext, OutputCollector outputCollector) {
+    public void prepare(Map<String, Object> map, TopologyContext topologyContext, OutputCollector outputCollector) {
         super.prepare(map, topologyContext, outputCollector);
     }
 

File: external/storm-elasticsearch/src/main/java/org/apache/storm/elasticsearch/bolt/EsPercolateBolt.java
Patch:
@@ -61,7 +61,7 @@ public EsPercolateBolt(EsConfig esConfig, EsTupleMapper tupleMapper) {
     }
 
     @Override
-    public void prepare(Map map, TopologyContext topologyContext, OutputCollector outputCollector) {
+    public void prepare(Map<String, Object> map, TopologyContext topologyContext, OutputCollector outputCollector) {
         super.prepare(map, topologyContext, outputCollector);
     }
 

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/spout/BinaryEventDataScheme.java
Patch:
@@ -58,8 +58,8 @@ else if (eventData.getObject()!=null) {
 				throw new RuntimeException(e);
 			}
 		}
-		Map metaDataMap =  eventData.getProperties();
-		Map systemMetaDataMap = eventData.getSystemProperties();
+		Map<String, Object> metaDataMap =  eventData.getProperties();
+		Map<String, Object> systemMetaDataMap = eventData.getSystemProperties();
 		fieldContents.add(messageData);
 		fieldContents.add(metaDataMap);
 		fieldContents.add(systemMetaDataMap);

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/spout/EventDataScheme.java
Patch:
@@ -65,8 +65,8 @@ else if (eventData.getObject()!=null) {
 				throw e;
 			}
 		}
-		Map metaDataMap = eventData.getProperties();
-		fieldContents.add(messageData);
+		Map<String, Object> metaDataMap = eventData.getProperties();
+                fieldContents.add(messageData);
 		fieldContents.add(metaDataMap);
 		return fieldContents;
 	}

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/spout/EventHubReceiverImpl.java
Patch:
@@ -156,8 +156,8 @@ public EventDataWrap receive() {
   }
 
   @Override
-  public Map getMetricsData() {
-    Map ret = new HashMap();
+  public Map<String, Object> getMetricsData() {
+    Map<String, Object> ret = new HashMap<>();
     ret.put(partitionId + "/receiveApiLatencyMean", receiveApiLatencyMean.getValueAndReset());
     ret.put(partitionId + "/receiveApiCallCount", receiveApiCallCount.getValueAndReset());
     ret.put(partitionId + "/receiveMessageCount", receiveMessageCount.getValueAndReset());

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/spout/EventHubSpout.java
Patch:
@@ -159,7 +159,7 @@ public void open(Map<String, Object> config, TopologyContext context, SpoutOutpu
     context.registerMetric("EventHubReceiver", new IMetric() {
       @Override
       public Object getValueAndReset() {
-          Map concatMetricsDataMaps = new HashMap();
+          Map<String, Object> concatMetricsDataMaps = new HashMap<>();
           for (IPartitionManager partitionManager : 
             partitionCoordinator.getMyPartitionManagers()) {
             concatMetricsDataMaps.putAll(partitionManager.getMetricsData());

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/spout/IEventHubReceiver.java
Patch:
@@ -29,5 +29,5 @@ public interface IEventHubReceiver {
 
   EventDataWrap receive();
 
-  Map getMetricsData();
+  Map<String, Object> getMetricsData();
 }

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/spout/IPartitionManager.java
Patch:
@@ -33,5 +33,5 @@ public interface IPartitionManager {
 
   void fail(String offset);
   
-  Map getMetricsData();
+  Map<String, Object> getMetricsData();
 }

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/spout/SimplePartitionManager.java
Patch:
@@ -127,7 +127,7 @@ private String getPartitionStatePath() {
   }
   
   @Override
-  public Map getMetricsData() {
+  public Map<String, Object> getMetricsData() {
     return receiver.getMetricsData();
   }
 }

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/trident/TransactionalTridentEventHubSpout.java
Patch:
@@ -31,7 +31,7 @@
  * Transactional Trident EventHub Spout
  */
 public class TransactionalTridentEventHubSpout implements 
-  IPartitionedTridentSpout<Partitions, Partition, Map> {
+  IPartitionedTridentSpout<Partitions, Partition, Map<String, Object>> {
   private static final long serialVersionUID = 1L;
   private final IEventDataScheme scheme;
   private final EventHubSpoutConfig spoutConfig;
@@ -53,7 +53,7 @@ public IPartitionedTridentSpout.Coordinator<Partitions> getCoordinator(
   }
 
   @Override
-  public IPartitionedTridentSpout.Emitter<Partitions, Partition, Map> getEmitter(
+  public IPartitionedTridentSpout.Emitter<Partitions, Partition, Map<String, Object>> getEmitter(
       Map<String, Object> conf, TopologyContext context) {
     return new TransactionalTridentEventHubEmitter(spoutConfig);
   }

File: external/storm-eventhubs/src/test/java/org/apache/storm/eventhubs/samples/bolt/GlobalCountBolt.java
Patch:
@@ -55,7 +55,7 @@ public Object getValueAndReset() {
         long now = System.nanoTime();
         long millis = (now - lastMetricsTime) / 1000000;
         throughput = globalCountDiff / millis * 1000;
-        Map values = new HashMap();
+        Map<String, Object> values = new HashMap<>();
         values.put("global_count", globalCount);
         values.put("throughput", throughput);
         lastMetricsTime = now;

File: external/storm-eventhubs/src/test/java/org/apache/storm/eventhubs/spout/EventHubReceiverMock.java
Patch:
@@ -79,7 +79,7 @@ public EventDataWrap receive() {
   }
   
   @Override
-  public Map getMetricsData() {
+  public Map<String, Object> getMetricsData() {
     return null;
   }
 }

File: external/storm-eventhubs/src/test/java/org/apache/storm/eventhubs/trident/TestTransactionalTridentEmitter.java
Patch:
@@ -62,7 +62,7 @@ public void tearDown() throws Exception {
   @Test
   public void testEmitInSequence() {
     //test the happy path, emit batches in sequence
-    Map meta = emitter.emitPartitionBatchNew(null, collectorMock, partition, null);
+    Map<String, Object> meta = emitter.emitPartitionBatchNew(null, collectorMock, partition, null);
     String collected = collectorMock.getBuffer();
     assertTrue(collected.startsWith("message"+0));
     //System.out.println("collected: " + collected);
@@ -77,11 +77,11 @@ public void testEmitInSequence() {
   @Test
   public void testReEmit() {
     //test we can re-emit the second batch
-    Map meta = emitter.emitPartitionBatchNew(null, collectorMock, partition, null);
+    Map<String, Object> meta = emitter.emitPartitionBatchNew(null, collectorMock, partition, null);
     collectorMock.clear();
     
     //emit second batch
-    Map meta1 = emitter.emitPartitionBatchNew(null, collectorMock, partition, meta);
+    Map<String, Object> meta1 = emitter.emitPartitionBatchNew(null, collectorMock, partition, meta);
     String collected0 = collectorMock.getBuffer();
     collectorMock.clear();
     

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/bolt/HBaseBolt.java
Patch:
@@ -105,7 +105,7 @@ public void execute(Tuple tuple) {
     }
 
     @Override
-    public void prepare(Map map, TopologyContext topologyContext, OutputCollector collector) {
+    public void prepare(Map<String, Object> map, TopologyContext topologyContext, OutputCollector collector) {
         super.prepare(map, topologyContext, collector);
         this.batchHelper = new BatchHelper(batchSize, collector);
     }

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/state/HBaseKeyValueStateProvider.java
Patch:
@@ -45,7 +45,7 @@ public class HBaseKeyValueStateProvider implements StateProvider {
     private static final Logger LOG = LoggerFactory.getLogger(HBaseKeyValueStateProvider.class);
 
     @Override
-    public State newState(String namespace, Map stormConf, TopologyContext context) {
+    public State newState(String namespace, Map<String, Object> stormConf, TopologyContext context) {
         try {
             return getHBaseKeyValueState(namespace, stormConf, context, getStateConfig(stormConf));
         } catch (Exception ex) {
@@ -81,7 +81,7 @@ private HBaseKeyValueState getHBaseKeyValueState(String namespace, Map<String, O
 
         //heck for backward compatibility, we need to pass TOPOLOGY_AUTO_CREDENTIALS to hbase conf
         //the conf instance is instance of persistentMap so making a copy.
-        Map<String, Object> hbaseConfMap = new HashMap<String, Object>(conf);
+        Map<String, Object> hbaseConfMap = new HashMap<>(conf);
         hbaseConfMap.put(Config.TOPOLOGY_AUTO_CREDENTIALS, stormConf.get(Config.TOPOLOGY_AUTO_CREDENTIALS));
         HBaseClient hbaseClient = new HBaseClient(hbaseConfMap, hbConfig, config.tableName);
 

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/trident/state/HBaseMapState.java
Patch:
@@ -86,7 +86,7 @@ public class HBaseMapState<T> implements IBackingMap<T> {
      * @param map topology config map.
      * @param partitionNum the number of partition.
      */
-    public HBaseMapState(final Options<T> options, Map map, int partitionNum) {
+    public HBaseMapState(final Options<T> options, Map<String, Object> map, int partitionNum) {
         this.options = options;
         this.serializer = options.serializer;
         this.partitionNum = partitionNum;

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/trident/state/HBaseState.java
Patch:
@@ -46,11 +46,11 @@ public class HBaseState implements State {
 
     private Options options;
     private HBaseClient hBaseClient;
-    private Map map;
+    private Map<String, Object> map;
     private int numPartitions;
     private int partitionIndex;
 
-    protected HBaseState(Map map, int partitionIndex, int numPartitions, Options options) {
+    protected HBaseState(Map<String, Object> map, int partitionIndex, int numPartitions, Options options) {
         this.options = options;
         this.map = map;
         this.partitionIndex = partitionIndex;

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/trident/state/HBaseStateFactory.java
Patch:
@@ -32,7 +32,7 @@ public HBaseStateFactory(HBaseState.Options options) {
     }
 
     @Override
-    public State makeState(Map map, IMetricsContext iMetricsContext, int partitionIndex, int numPartitions) {
+    public State makeState(Map<String, Object> map, IMetricsContext iMetricsContext, int partitionIndex, int numPartitions) {
         HBaseState state = new HBaseState(map , partitionIndex, numPartitions, options);
         state.prepare();
         return state;

File: external/storm-hdfs/src/test/java/org/apache/storm/hdfs/spout/TestHdfsSpout.java
Patch:
@@ -588,8 +588,8 @@ private static List<String> readTextFile(FileSystem fs, String f) throws IOExcep
         return result;
     }
 
-    private Map getCommonConfigs() {
-        Map<String, Object> topoConf = new HashMap();
+    private Map<String, Object> getCommonConfigs() {
+        Map<String, Object> topoConf = new HashMap<>();
         topoConf.put(Config.TOPOLOGY_ACKER_EXECUTORS, "0");
         return topoConf;
     }

File: external/storm-jdbc/src/main/java/org/apache/storm/jdbc/bolt/AbstractJdbcBolt.java
Patch:
@@ -65,7 +65,7 @@ public abstract class AbstractJdbcBolt extends BaseTickTupleAwareRichBolt {
      * {@inheritDoc}
      */
     @Override
-    public void prepare(final Map map, final TopologyContext topologyContext,
+    public void prepare(final Map<String, Object> map, final TopologyContext topologyContext,
                         final OutputCollector outputCollector) {
         this.collector = outputCollector;
 

File: external/storm-jdbc/src/main/java/org/apache/storm/jdbc/bolt/JdbcInsertBolt.java
Patch:
@@ -74,7 +74,7 @@ public JdbcInsertBolt withQueryTimeoutSecs(int queryTimeoutSecs) {
     }
 
     @Override
-    public void prepare(Map map, TopologyContext topologyContext, OutputCollector collector) {
+    public void prepare(Map<String, Object> map, TopologyContext topologyContext, OutputCollector collector) {
         super.prepare(map, topologyContext, collector);
         if(StringUtils.isBlank(tableName) && StringUtils.isBlank(insertQuery)) {
             throw new IllegalArgumentException("You must supply either a tableName or an insert Query.");

File: external/storm-jdbc/src/main/java/org/apache/storm/jdbc/trident/state/JdbcState.java
Patch:
@@ -46,7 +46,7 @@ public class JdbcState implements State {
     private JdbcClient jdbcClient;
     private Map map;
 
-    protected JdbcState(Map map, int partitionIndex, int numPartitions, Options options) {
+    protected JdbcState(Map<String, Object> map, int partitionIndex, int numPartitions, Options options) {
         this.options = options;
         this.map = map;
     }

File: external/storm-jdbc/src/main/java/org/apache/storm/jdbc/trident/state/JdbcStateFactory.java
Patch:
@@ -32,7 +32,7 @@ public JdbcStateFactory(JdbcState.Options options) {
     }
 
     @Override
-    public State makeState(Map map, IMetricsContext iMetricsContext, int partitionIndex, int numPartitions) {
+    public State makeState(Map<String, Object> map, IMetricsContext iMetricsContext, int partitionIndex, int numPartitions) {
         JdbcState state = new JdbcState(map , partitionIndex, numPartitions, options);
         state.prepare();
         return state;

File: external/storm-jdbc/src/test/java/org/apache/storm/jdbc/common/JdbcClientTest.java
Patch:
@@ -42,7 +42,7 @@ public class JdbcClientTest {
     private static final String tableName = "user_details";
     @Before
     public void setup() {
-        Map map = Maps.newHashMap();
+        Map<String, Object> map = Maps.newHashMap();
         map.put("dataSourceClassName","org.hsqldb.jdbc.JDBCDataSource");//com.mysql.jdbc.jdbc2.optional.MysqlDataSource
         map.put("dataSource.url", "jdbc:hsqldb:mem:test");//jdbc:mysql://localhost/test
         map.put("dataSource.user","SA");//root

File: external/storm-jms/src/main/java/org/apache/storm/jms/trident/JmsStateFactory.java
Patch:
@@ -32,7 +32,7 @@ public JmsStateFactory(JmsState.Options options) {
     }
 
     @Override
-    public State makeState(Map map, IMetricsContext iMetricsContext, int partitionIndex, int numPartitions) {
+    public State makeState(Map<String, Object> map, IMetricsContext iMetricsContext, int partitionIndex, int numPartitions) {
         JmsState state = new JmsState(options);
         state.prepare();
         return state;

File: external/storm-kafka-client/src/test/java/org/apache/storm/kafka/spout/trident/KafkaTridentSpoutBatchMetadataTest.java
Patch:
@@ -43,7 +43,7 @@ public void testMetadataIsRoundTripSerializableWithJsonSimple() throws Exception
 
         KafkaTridentSpoutBatchMetadata metadata = new KafkaTridentSpoutBatchMetadata(startOffset, endOffset);
         Map<String, Object> map = metadata.toMap();
-        Map deserializedMap = (Map)JSONValue.parseWithException(JSONValue.toJSONString(map));
+        Map<String, Object> deserializedMap = (Map)JSONValue.parseWithException(JSONValue.toJSONString(map));
         KafkaTridentSpoutBatchMetadata deserializedMetadata = KafkaTridentSpoutBatchMetadata.fromMap(deserializedMap);
         assertThat(deserializedMetadata.getFirstOffset(), is(metadata.getFirstOffset()));
         assertThat(deserializedMetadata.getLastOffset(), is(metadata.getLastOffset()));

File: external/storm-kafka/src/jvm/org/apache/storm/kafka/KafkaSpout.java
Patch:
@@ -111,7 +111,7 @@ public Object getValueAndReset() {
             @Override
             public Object getValueAndReset() {
                 List<PartitionManager> pms = _coordinator.getMyManagedPartitions();
-                Map concatMetricsDataMaps = new HashMap();
+                Map<String, Object> concatMetricsDataMaps = new HashMap<>();
                 for (PartitionManager pm : pms) {
                     concatMetricsDataMaps.putAll(pm.getMetricsDataMap());
                 }

File: external/storm-kafka/src/jvm/org/apache/storm/kafka/PartitionManager.java
Patch:
@@ -165,7 +165,7 @@ public PartitionManager(
         _messageIneligibleForRetryCount = new CountMetric();
     }
 
-    public Map getMetricsDataMap() {
+    public Map<String, Object> getMetricsDataMap() {
         String metricPrefix = _partition.getId();
 
         Map<String, Object> ret = new HashMap<>();

File: external/storm-kafka/src/test/org/apache/storm/kafka/ZkCoordinatorTest.java
Patch:
@@ -63,8 +63,8 @@ public void setUp() throws Exception {
         when(dynamicPartitionConnections.register(any(Broker.class), any(String.class) ,anyInt())).thenReturn(simpleConsumer);
     }
 
-    private Map buildZookeeperConfig(TestingServer server) {
-        Map<String, Object> conf = new HashMap();
+    private Map<String, Object> buildZookeeperConfig(TestingServer server) {
+        Map<String, Object> conf = new HashMap<>();
         conf.put(Config.TRANSACTIONAL_ZOOKEEPER_PORT, server.getPort());
         conf.put(Config.TRANSACTIONAL_ZOOKEEPER_SERVERS, Arrays.asList("localhost"));
         conf.put(Config.STORM_ZOOKEEPER_SESSION_TIMEOUT, 20000);

File: external/storm-mongodb/src/main/java/org/apache/storm/mongodb/trident/state/MongoMapState.java
Patch:
@@ -68,9 +68,9 @@ public class MongoMapState<T> implements IBackingMap<T> {
     private Options<T> options;
     private Serializer<T> serializer;
     private MongoDbClient mongoClient;
-    private Map map;
+    private Map<String, Object> map;
 
-    protected MongoMapState(Map map, Options options) {
+    protected MongoMapState(Map<String, Object> map, Options options) {
         this.options = options;
         this.map = map;
         this.serializer = options.serializer;

File: external/storm-mongodb/src/main/java/org/apache/storm/mongodb/trident/state/MongoState.java
Patch:
@@ -45,9 +45,9 @@ public class MongoState implements State {
 
     private Options options;
     private MongoDbClient mongoClient;
-    private Map map;
+    private Map<String, Object> map;
 
-    protected MongoState(Map map, Options options) {
+    protected MongoState(Map<String, Object> map, Options options) {
         this.options = options;
         this.map = map;
     }

File: external/storm-redis/src/main/java/org/apache/storm/redis/bolt/AbstractRedisBolt.java
Patch:
@@ -82,7 +82,7 @@ public AbstractRedisBolt(JedisClusterConfig config) {
      * {@inheritDoc}
      */
     @Override
-    public void prepare(Map map, TopologyContext topologyContext, OutputCollector collector) {
+    public void prepare(Map<String, Object> map, TopologyContext topologyContext, OutputCollector collector) {
         // FIXME: stores map (topoConf), topologyContext and expose these to derived classes
         this.collector = collector;
 

File: external/storm-rocketmq/src/main/java/org/apache/storm/rocketmq/trident/state/RocketMqState.java
Patch:
@@ -45,7 +45,7 @@ public class RocketMqState implements State {
     private Options options;
     private MQProducer producer;
 
-    protected RocketMqState(Map map, Options options) {
+    protected RocketMqState(Map<String, Object> map, Options options) {
         this.options = options;
     }
 

File: external/storm-solr/src/main/java/org/apache/storm/solr/trident/SolrStateFactory.java
Patch:
@@ -36,7 +36,7 @@ public SolrStateFactory(SolrConfig solrConfig, SolrMapper solrMapper) {
     }
 
     @Override
-    public State makeState(Map map, IMetricsContext iMetricsContext, int partitionIndex, int numPartitions) {
+    public State makeState(Map<String, Object> map, IMetricsContext iMetricsContext, int partitionIndex, int numPartitions) {
         SolrState state = new SolrState(solrConfig, solrMapper);
         state.prepare();
         return state;

File: flux/flux-core/src/main/java/org/apache/storm/flux/model/ConfigMethodDef.java
Patch:
@@ -49,7 +49,7 @@ public void setArgs(List<Object> args) {
         List<Object> newVal = new ArrayList<Object>();
         for (Object obj : args) {
             if (obj instanceof LinkedHashMap) {
-                Map map = (Map)obj;
+                Map<String, Object> map = (Map<String, Object>)obj;
                 if (map.containsKey("ref") && map.size() == 1) {
                     newVal.add(new BeanReference((String)map.get("ref")));
                     this.hasReferences = true;

File: flux/flux-core/src/main/java/org/apache/storm/flux/model/ObjectDef.java
Patch:
@@ -57,7 +57,7 @@ public void setConstructorArgs(List<Object> constructorArgs) {
         List<Object> newVal = new ArrayList<Object>();
         for (Object obj : constructorArgs) {
             if (obj instanceof LinkedHashMap) {
-                Map map = (Map)obj;
+                Map<String, Object> map = (Map<String, Object>)obj;
                 if (map.containsKey("ref") && map.size() == 1) {
                     newVal.add(new BeanReference((String) map.get("ref")));
                     this.hasReferences = true;

File: integration-test/src/test/java/org/apache/storm/st/wrapper/StormCluster.java
Patch:
@@ -49,7 +49,7 @@ public StormCluster() {
         this.client = NimbusClient.getConfiguredClient(conf).getClient();
     }
 
-    public static Map getConfig() {
+    public static Map<String, Object> getConfig() {
         return Utils.readStormConfig();
     }
 

File: storm-client/src/jvm/org/apache/storm/Config.java
Patch:
@@ -1853,11 +1853,11 @@ public void setClasspath(String cp) {
         setClasspath(this, cp);
     }
 
-    public static void setEnvironment(Map<String, Object> conf, Map env) {
+    public static void setEnvironment(Map<String, Object> conf, Map<String, Object> env) {
         conf.put(Config.TOPOLOGY_ENVIRONMENT, env);
     }
 
-    public void setEnvironment(Map env) {
+    public void setEnvironment(Map<String, Object> env) {
         setEnvironment(this, env);
     }
 
@@ -1955,7 +1955,7 @@ public static void registerEventLogger(Map<String, Object> conf, Class<? extends
     }
 
     public static void registerMetricsConsumer(Map<String, Object> conf, Class klass, Object argument, long parallelismHint) {
-        HashMap m = new HashMap();
+        HashMap<String, Object> m = new HashMap<>();
         m.put("class", klass.getCanonicalName());
         m.put("parallelism.hint", parallelismHint);
         m.put("argument", argument);

File: storm-client/src/jvm/org/apache/storm/StormSubmitter.java
Patch:
@@ -80,9 +80,8 @@ public static boolean validateZKDigestPayload(String payload) {
         return false;
     }
 
-    @SuppressWarnings("unchecked")
-    public static Map prepareZookeeperAuthentication(Map<String, Object> conf) {
-        Map toRet = new HashMap();
+    public static Map<String, Object> prepareZookeeperAuthentication(Map<String, Object> conf) {
+        Map<String, Object> toRet = new HashMap<>();
         String secretPayload = (String) conf.get(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_PAYLOAD);
         // Is the topology ZooKeeper authentication configuration unset?
         if (! conf.containsKey(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_PAYLOAD) ||

File: storm-client/src/jvm/org/apache/storm/Thrift.java
Patch:
@@ -94,7 +94,7 @@ public Integer getParallelism() {
             return parallelism;
         }
 
-        public Map getConf() {
+        public Map<String, Object> getConf() {
             return conf;
         }
     }
@@ -117,7 +117,7 @@ public Object getBolt() {
             return bolt;
         }
 
-        public Map getConf() {
+        public Map<String, Object> getConf() {
             return conf;
         }
 

File: storm-client/src/jvm/org/apache/storm/cluster/ClusterUtils.java
Patch:
@@ -256,12 +256,12 @@ public IStormClusterState mkStormClusterStateImpl(Object stateStorage, ClusterSt
         if (stateStorage instanceof IStateStorage) {
             return new StormClusterStateImpl((IStateStorage) stateStorage, context, false);
         } else {
-            IStateStorage Storage = _instance.mkStateStorageImpl((Map) stateStorage, (Map) stateStorage, context);
+            IStateStorage Storage = _instance.mkStateStorageImpl((Map<String, Object>) stateStorage, (Map<String, Object>) stateStorage, context);
             return new StormClusterStateImpl(Storage, context, true);
         }
     }
 
-    public IStateStorage mkStateStorageImpl(Map<String, Object> config, Map auth_conf, ClusterStateContext context) throws Exception {
+    public IStateStorage mkStateStorageImpl(Map<String, Object> config, Map<String, Object> auth_conf, ClusterStateContext context) throws Exception {
         String className = null;
         IStateStorage stateStorage = null;
         if (config.get(Config.STORM_CLUSTER_STATE_STORE) != null) {
@@ -275,7 +275,7 @@ public IStateStorage mkStateStorageImpl(Map<String, Object> config, Map auth_con
         return stateStorage;
     }
 
-    public static IStateStorage mkStateStorage(Map<String, Object> config, Map auth_conf, ClusterStateContext context) throws Exception {
+    public static IStateStorage mkStateStorage(Map<String, Object> config, Map<String, Object> auth_conf, ClusterStateContext context) throws Exception {
         return _instance.mkStateStorageImpl(config, auth_conf, context);
     }
 

File: storm-client/src/jvm/org/apache/storm/cluster/PaceMakerStateStorageFactory.java
Patch:
@@ -24,7 +24,7 @@
 
 public class PaceMakerStateStorageFactory implements StateStorageFactory {
     @Override
-    public IStateStorage mkStore(Map<String, Object> config, Map auth_conf, ClusterStateContext context) {
+    public IStateStorage mkStore(Map<String, Object> config, Map<String, Object> auth_conf, ClusterStateContext context) {
         try {
             ZKStateStorageFactory zkfact = new ZKStateStorageFactory();
             IStateStorage zkState = zkfact.mkStore(config, auth_conf, context);

File: storm-client/src/jvm/org/apache/storm/cluster/StateStorageFactory.java
Patch:
@@ -22,5 +22,5 @@
 
 public interface StateStorageFactory {
 
-    IStateStorage mkStore(Map<String, Object> config, Map auth_conf, ClusterStateContext context);
+    IStateStorage mkStore(Map<String, Object> config, Map<String, Object> auth_conf, ClusterStateContext context);
 }

File: storm-client/src/jvm/org/apache/storm/cluster/ZKStateStorage.java
Patch:
@@ -50,7 +50,7 @@ public class ZKStateStorage implements IStateStorage {
     private AtomicBoolean active;
 
     private boolean isNimbus;
-    private Map authConf;
+    private Map<String, Object> authConf;
     private Map<String, Object> conf;
 
     private class ZkWatcherCallBack implements WatcherCallBack{
@@ -73,7 +73,7 @@ public void execute(Watcher.Event.KeeperState state, Watcher.Event.EventType typ
         }
     }
 
-    public ZKStateStorage(Map<String, Object> conf, Map authConf, ClusterStateContext context) throws Exception {
+    public ZKStateStorage(Map<String, Object> conf, Map<String, Object> authConf, ClusterStateContext context) throws Exception {
         this.conf = conf;
         this.authConf = authConf;
         if (context.getDaemonType().equals(DaemonType.NIMBUS))

File: storm-client/src/jvm/org/apache/storm/cluster/ZKStateStorageFactory.java
Patch:
@@ -26,7 +26,7 @@
 public class ZKStateStorageFactory implements StateStorageFactory {
 
     @Override
-    public IStateStorage mkStore(Map<String, Object> config, Map auth_conf, ClusterStateContext context) {
+    public IStateStorage mkStore(Map<String, Object> config, Map<String, Object> auth_conf, ClusterStateContext context) {
         try {
             return new ZKStateStorage(config, auth_conf, context);
         } catch (Exception e) {

File: storm-client/src/jvm/org/apache/storm/daemon/StormCommon.java
Patch:
@@ -296,7 +296,7 @@ public static void addAcker(Map<String, Object> conf, StormTopology topology) {
 
         for (SpoutSpec spout : topology.get_spouts().values()) {
             ComponentCommon common = spout.get_common();
-            Map spoutConf = componentConf(spout);
+            Map<String, Object> spoutConf = componentConf(spout);
             spoutConf.put(Config.TOPOLOGY_TICK_TUPLE_FREQ_SECS,
                     ObjectReader.getInt(conf.get(Config.TOPOLOGY_MESSAGE_TIMEOUT_SECS)));
             common.set_json_conf(JSONValue.toJSONString(spoutConf));

File: storm-client/src/jvm/org/apache/storm/daemon/worker/WorkerState.java
Patch:
@@ -96,7 +96,7 @@ public class WorkerState {
     private final WorkerTransfer workerTransfer;
     private final BackPressureTracker bpTracker;
 
-    public Map getConf() {
+    public Map<String, Object> getConf() {
         return conf;
     }
 
@@ -140,7 +140,7 @@ public Map<Integer, JCQueue> getLocalReceiveQueues() {
         return localReceiveQueues;
     }
 
-    public Map getTopologyConf() {
+    public Map<String, Object> getTopologyConf() {
         return topologyConf;
     }
 

File: storm-client/src/jvm/org/apache/storm/drpc/DRPCSpout.java
Patch:
@@ -204,7 +204,7 @@ public void nextTuple() {
                 try {
                     DRPCRequest req = client.fetchRequest(_function);
                     if(req.get_request_id().length() > 0) {
-                        Map returnInfo = new HashMap();
+                        Map<String, Object> returnInfo = new HashMap<>();
                         returnInfo.put("id", req.get_request_id());
                         returnInfo.put("host", client.getHost());
                         returnInfo.put("port", client.getPort());
@@ -228,7 +228,7 @@ public void nextTuple() {
                 try {
                     DRPCRequest req = drpc.fetchRequest(_function);
                     if(req.get_request_id().length() > 0) {
-                        Map returnInfo = new HashMap();
+                        Map<String, Object> returnInfo = new HashMap<>();
                         returnInfo.put("id", req.get_request_id());
                         returnInfo.put("host", _local_drpc_id);
                         returnInfo.put("port", 0);

File: storm-client/src/jvm/org/apache/storm/drpc/JoinResult.java
Patch:
@@ -44,7 +44,7 @@ public JoinResult(String returnComponent) {
         this.returnComponent = returnComponent;
     }
  
-    public void prepare(Map map, TopologyContext context, OutputCollector collector) {
+    public void prepare(Map<String, Object> map, TopologyContext context, OutputCollector collector) {
         _collector = collector;
     }
 

File: storm-client/src/jvm/org/apache/storm/drpc/PrepareRequest.java
Patch:
@@ -38,7 +38,7 @@ public class PrepareRequest extends BaseBasicBolt {
     Random rand;
 
     @Override
-    public void prepare(Map map, TopologyContext context) {
+    public void prepare(Map<String, Object> map, TopologyContext context) {
         rand = new Random();
     }
 

File: storm-client/src/jvm/org/apache/storm/drpc/ReturnResults.java
Patch:
@@ -47,7 +47,7 @@ public class ReturnResults extends BaseRichBolt {
     public static final Logger LOG = LoggerFactory.getLogger(ReturnResults.class);
     OutputCollector _collector;
     boolean local;
-    Map _conf; 
+    Map<String, Object> _conf;
     Map<List, DRPCInvocationsClient> _clients = new HashMap<List, DRPCInvocationsClient>();
 
     @Override
@@ -62,9 +62,9 @@ public void execute(Tuple input) {
         String result = (String) input.getValue(0);
         String returnInfo = (String) input.getValue(1);
         if (returnInfo!=null) {
-            Map retMap = null;
+            Map<String, Object> retMap;
             try {
-                retMap = (Map) JSONValue.parseWithException(returnInfo);
+                retMap = (Map<String, Object>) JSONValue.parseWithException(returnInfo);
             } catch (ParseException e) {
                  LOG.error("Parseing returnInfo failed", e);
                  _collector.fail(input);

File: storm-client/src/jvm/org/apache/storm/messaging/DeserializingConnectionCallback.java
Patch:
@@ -39,7 +39,7 @@
  */
 public class DeserializingConnectionCallback implements IConnectionCallback, IMetric {
     private final WorkerState.ILocalTransferCallback cb;
-    private final Map conf;
+    private final Map<String, Object> conf;
     private final GeneralTopologyContext context;
 
     private final ThreadLocal<KryoTupleDeserializer> _des =
@@ -55,7 +55,7 @@ protected KryoTupleDeserializer initialValue() {
     private final ConcurrentHashMap<String, AtomicLong> byteCounts = new ConcurrentHashMap<>();
 
 
-    public DeserializingConnectionCallback(final Map conf, final GeneralTopologyContext context, WorkerState.ILocalTransferCallback callback) {
+    public DeserializingConnectionCallback(final Map<String, Object> conf, final GeneralTopologyContext context, WorkerState.ILocalTransferCallback callback) {
         this.conf = conf;
         this.context = context;
         cb = callback;

File: storm-client/src/jvm/org/apache/storm/messaging/netty/Client.java
Patch:
@@ -549,7 +549,7 @@ public Object getState() {
         return ret;
     }
 
-    public Map getConfig() {
+    public Map<String, Object> getConfig() {
         return topoConf;
     }
 

File: storm-client/src/jvm/org/apache/storm/metric/api/MultiCountMetric.java
Patch:
@@ -35,8 +35,8 @@ public CountMetric scope(String key) {
         return val;
     }
 
-    public Object getValueAndReset() {
-        Map ret = new HashMap();
+    public Map<String, Object> getValueAndReset() {
+        Map<String, Object> ret = new HashMap<>();
         for(Map.Entry<String, CountMetric> e : _value.entrySet()) {
             ret.put(e.getKey(), e.getValue().getValueAndReset());
         }

File: storm-client/src/jvm/org/apache/storm/metric/api/MultiReducedMetric.java
Patch:
@@ -37,8 +37,8 @@ public ReducedMetric scope(String key) {
         return val;
     }
 
-    public Object getValueAndReset() {
-        Map ret = new HashMap();
+    public Map<String, Object> getValueAndReset() {
+        Map<String, Object> ret = new HashMap<>();
         for(Map.Entry<String, ReducedMetric> e : _value.entrySet()) {
             Object val = e.getValue().getValueAndReset();
             if(val != null) {

File: storm-client/src/jvm/org/apache/storm/security/auth/FixedGroupsMapping.java
Patch:
@@ -41,7 +41,7 @@ public class FixedGroupsMapping implements IGroupMappingServiceProvider {
      * @param storm_conf Storm configuration
      */
     @Override
-    public void prepare(Map storm_conf) {
+    public void prepare(Map<String, Object> storm_conf) {
         Map<?, ?> params = (Map<?, ?>) storm_conf.get(Config.STORM_GROUP_MAPPING_SERVICE_PARAMS);
         Map<String, Set<String>> mapping = (Map<String, Set<String>>) params.get(STORM_FIXED_GROUP_MAPPING);
         if (mapping != null) {

File: storm-client/src/jvm/org/apache/storm/security/auth/authorizer/DRPCAuthorizerBase.java
Patch:
@@ -36,9 +36,9 @@ public abstract class DRPCAuthorizerBase implements IAuthorizer {
     @Override
     public abstract void prepare(Map<String, Object> conf);
 
-    abstract protected boolean permitClientRequest(ReqContext context, String operation, Map params);
+    abstract protected boolean permitClientRequest(ReqContext context, String operation, Map<String, Object> params);
 
-    abstract protected boolean permitInvocationRequest(ReqContext context, String operation, Map params);
+    abstract protected boolean permitInvocationRequest(ReqContext context, String operation, Map<String, Object> params);
     
     /**
      * Authorizes request from to the DRPC server.
@@ -47,7 +47,7 @@ public abstract class DRPCAuthorizerBase implements IAuthorizer {
      * @param params a Map with any key-value entries of use to the authorization implementation
      */
     @Override
-    public boolean permit(ReqContext context, String operation, Map params) {
+    public boolean permit(ReqContext context, String operation, Map<String, Object> params) {
         if ("execute".equals(operation)) {
             return permitClientRequest(context, operation, params);
         } else if ("failRequest".equals(operation) || 

File: storm-client/src/jvm/org/apache/storm/security/auth/authorizer/DRPCSimpleACLAuthorizer.java
Patch:
@@ -125,7 +125,7 @@ private String getLocalUserFromContext(ReqContext context) {
         return null;
     }
 
-    protected boolean permitClientOrInvocationRequest(ReqContext context, Map params,
+    protected boolean permitClientOrInvocationRequest(ReqContext context, Map<String, Object> params,
             String fieldName) {
         Map<String,AclFunctionEntry> acl = readAclFromConfig();
         String function = (String) params.get(FUNCTION_KEY);
@@ -165,13 +165,13 @@ protected boolean permitClientOrInvocationRequest(ReqContext context, Map params
 
     @Override
     protected boolean permitClientRequest(ReqContext context, String operation,
-            Map params) {
+            Map<String, Object> params) {
         return permitClientOrInvocationRequest(context, params, "clientUsers");
     }
 
     @Override
     protected boolean permitInvocationRequest(ReqContext context, String operation,
-            Map params) {
+            Map<String, Object> params) {
         return permitClientOrInvocationRequest(context, params, "invocationUser");
     }
 }

File: storm-client/src/jvm/org/apache/storm/serialization/types/HashMapSerializer.java
Patch:
@@ -26,7 +26,7 @@
 
 public class HashMapSerializer extends MapSerializer {
     @Override
-    public Map create(Kryo kryo, Input input, Class<Map> type) {
-        return new HashMap();
+    public Map<String, Object> create(Kryo kryo, Input input, Class<Map> type) {
+        return new HashMap<>();
     }
 }

File: storm-client/src/jvm/org/apache/storm/stats/CommonStats.java
Patch:
@@ -61,11 +61,11 @@ public void cleanupStats() {
         transferredStats.close();
     }
 
-    protected Map valueStat(MultiCountStatAndMetric metric) {
+    protected Map<String,Map<String,Long>> valueStat(MultiCountStatAndMetric metric) {
         return metric.getTimeCounts();
     }
 
-    protected Map valueStat(MultiLatencyStatAndMetric metric) {
+    protected Map<String, Map<String, Double>> valueStat(MultiLatencyStatAndMetric metric) {
         return metric.getTimeLatAvg();
     }
 

File: storm-client/src/jvm/org/apache/storm/task/GeneralTopologyContext.java
Patch:
@@ -160,7 +160,7 @@ public Map<String, Map<String, Grouping>> getTargets(String componentId) {
 
     @Override
     public String toJSONString() {
-        Map obj = new HashMap();
+        Map<String, Object> obj = new HashMap<>();
         obj.put("task->component", _taskToComponent);
         // TODO: jsonify StormTopology
         // at the minimum should send source info

File: storm-client/src/jvm/org/apache/storm/testing/NonRichBoltTracker.java
Patch:
@@ -41,7 +41,7 @@ public void prepare(Map<String, Object> topoConf, TopologyContext context, Outpu
 
     public void execute(Tuple input) {
         _delegate.execute(input);
-        Map stats = (Map) RegisteredGlobalState.getState(_trackId);
+        Map<String, Object> stats = (Map<String, Object>) RegisteredGlobalState.getState(_trackId);
         ((AtomicInteger) stats.get("processed")).incrementAndGet();
     }
 

File: storm-client/src/jvm/org/apache/storm/testing/SpoutTracker.java
Patch:
@@ -45,7 +45,7 @@ public SpoutTrackOutputCollector(SpoutOutputCollector collector) {
         }
         
         private void recordSpoutEmit() {
-            Map stats = (Map) RegisteredGlobalState.getState(_trackId);
+            Map<String, Object> stats = (Map<String, Object>) RegisteredGlobalState.getState(_trackId);
             ((AtomicInteger) stats.get("spout-emitted")).incrementAndGet();
             
         }
@@ -99,13 +99,13 @@ public void nextTuple() {
 
     public void ack(Object msgId) {
         _delegate.ack(msgId);
-        Map stats = (Map) RegisteredGlobalState.getState(_trackId);
+        Map<String, Object> stats = (Map<String, Object>) RegisteredGlobalState.getState(_trackId);
         ((AtomicInteger) stats.get("processed")).incrementAndGet();
     }
 
     public void fail(Object msgId) {
         _delegate.fail(msgId);
-        Map stats = (Map) RegisteredGlobalState.getState(_trackId);
+        Map<String, Object> stats = (Map<String, Object>) RegisteredGlobalState.getState(_trackId);
         ((AtomicInteger) stats.get("processed")).incrementAndGet();        
     }
 

File: storm-client/src/jvm/org/apache/storm/topology/TopologyBuilder.java
Patch:
@@ -735,8 +735,8 @@ public BoltDeclarer grouping(GlobalStreamId id, Grouping grouping) {
         }        
     }
 
-    private static String mergeIntoJson(Map into, Map newMap) {
-        Map res = new HashMap<>(into);
+    private static String mergeIntoJson(Map<String, Object> into, Map<String, Object> newMap) {
+        Map<String, Object> res = new HashMap<>(into);
         res.putAll(newMap);
         return JSONValue.toJSONString(res);
     }

File: storm-client/src/jvm/org/apache/storm/transactional/state/TestTransactionalState.java
Patch:
@@ -35,7 +35,7 @@ public class TestTransactionalState extends TransactionalState {
      * Matching constructor in absence of a default constructor in the parent
      * class.
      */
-    protected TestTransactionalState(Map<String, Object> conf, String id, Map componentConf, String subroot) {
+    protected TestTransactionalState(Map<String, Object> conf, String id, Map<String, Object> componentConf, String subroot) {
         super(conf, id, componentConf, subroot);
     }
 

File: storm-client/src/jvm/org/apache/storm/trident/drpc/ReturnResultsReducer.java
Patch:
@@ -80,9 +80,9 @@ public void complete(ReturnResultsState state, TridentCollector collector) {
         // only one of the multireducers will receive the tuples
         if (state.returnInfo!=null) {
             String result = JSONValue.toJSONString(state.results);
-            Map retMap = null;
+            Map<String, Object> retMap;
             try {
-                retMap = (Map) JSONValue.parseWithException(state.returnInfo);
+                retMap = (Map<String, Object>) JSONValue.parseWithException(state.returnInfo);
             } catch (ParseException e) {
                 collector.reportError(e);
                 return;

File: storm-client/src/jvm/org/apache/storm/utils/ConfigUtils.java
Patch:
@@ -271,7 +271,7 @@ public static Map<String, Object> readSupervisorStormConfGivenPath(Map<String, O
         return ret;
     }
 
-    public static Map overrideLoginConfigWithSystemProperty(Map<String, Object> conf) { // note that we delete the return value
+    public static Map<String, Object> overrideLoginConfigWithSystemProperty(Map<String, Object> conf) { // note that we delete the return value
         String loginConfFile = System.getProperty("java.security.auth.login.config");
         if (loginConfFile != null) {
             conf.put("java.security.auth.login.config", loginConfFile);
@@ -322,7 +322,7 @@ public static Map<String, Object> readYamlConfig(String name, boolean mustExist)
         return conf;
     }
 
-    public static Map readYamlConfig(String name) {
+    public static Map<String, Object> readYamlConfig(String name) {
         return readYamlConfig(name, true);
     }
 

File: storm-client/src/jvm/org/apache/storm/utils/JCQueue.java
Patch:
@@ -29,6 +29,7 @@
 
 import java.util.ArrayList;
 import java.util.HashMap;
+import java.util.Map;
 import java.util.concurrent.atomic.AtomicLong;
 
 
@@ -189,7 +190,7 @@ public long capacity() {
         }
 
         public Object getState() {
-            HashMap state = new HashMap<String, Object>();
+            Map<String, Object> state = new HashMap<>();
 
             final double arrivalRateInSecs = arrivalsTracker.reportRate();
 

File: storm-client/src/jvm/org/apache/storm/utils/NimbusClient.java
Patch:
@@ -111,7 +111,7 @@ private static synchronized boolean shouldLogLeader(String leader) {
         return true;
     }
 
-    public static NimbusClient getConfiguredClientAs(Map conf, String asUser) {
+    public static NimbusClient getConfiguredClientAs(Map<String, Object> conf, String asUser) {
         return getConfiguredClientAs(conf, asUser, null);
     }
 

File: storm-client/test/jvm/org/apache/storm/messaging/DeserializingConnectionCallbackTest.java
Patch:
@@ -48,7 +48,7 @@ public void setUp() throws Exception {
 
     @Test
     public void testUpdateMetricsConfigOff() {
-        Map config = new HashMap();
+        Map<String, Object> config = new HashMap<>();
         config.put(Config.TOPOLOGY_SERIALIZED_MESSAGE_SIZE_METRICS, Boolean.FALSE);
         DeserializingConnectionCallback withoutMetrics = 
             new DeserializingConnectionCallback(config, mock(GeneralTopologyContext.class), mock(
@@ -64,7 +64,7 @@ public void testUpdateMetricsConfigOff() {
     
     @Test
     public void testUpdateMetricsConfigOn() {
-        Map config = new HashMap();
+        Map<String, Object> config = new HashMap<>();
         config.put(Config.TOPOLOGY_SERIALIZED_MESSAGE_SIZE_METRICS, Boolean.TRUE);
         DeserializingConnectionCallback withMetrics =
             new DeserializingConnectionCallback(config, mock(GeneralTopologyContext.class), mock(

File: storm-client/test/jvm/org/apache/storm/streams/ProcessorBoltTest.java
Patch:
@@ -40,6 +40,7 @@
 import java.util.HashMap;
 import java.util.Map;
 import java.util.Set;
+import org.apache.storm.generated.Grouping;
 
 import static org.junit.Assert.assertArrayEquals;
 import static org.junit.Assert.assertEquals;
@@ -140,7 +141,7 @@ private void setUpProcessorBolt(Processor<?> processor,
         node.setWindowed(isWindowed);
         Mockito.when(mockStreamToProcessors.get(Mockito.anyString())).thenReturn(Collections.singletonList(node));
         Mockito.when(mockStreamToProcessors.keySet()).thenReturn(Collections.singleton("inputstream"));
-        Map mockSources = Mockito.mock(Map.class);
+        Map<GlobalStreamId, Grouping> mockSources = Mockito.mock(Map.class);
         GlobalStreamId mockGlobalStreamId = Mockito.mock(GlobalStreamId.class);
         Mockito.when(mockTopologyContext.getThisSources()).thenReturn(mockSources);
         Mockito.when(mockSources.keySet()).thenReturn(Collections.singleton(mockGlobalStreamId));

File: storm-core/src/jvm/org/apache/storm/ui/UIHelpers.java
Patch:
@@ -109,7 +109,7 @@ public static String prettyExecutorInfo(ExecutorInfo e) {
         return "[" + e.get_task_start() + "-" + e.get_task_end() + "]";
     }
 
-    public static Map unauthorizedUserJson(String user) {
+    public static Map<String, Object> unauthorizedUserJson(String user) {
         return ImmutableMap.of(
                 "error", "No Authorization",
                 "errorMessage", String.format("User %s is not authorized.", user));
@@ -203,7 +203,7 @@ public static void configFilters(ServletContextHandler context, List<FilterConfi
         for (FilterConfiguration filterConf : filtersConfs) {
             String filterName = filterConf.getFilterName();
             String filterClass = filterConf.getFilterClass();
-            Map filterParams = filterConf.getFilterParams();
+            Map<String, String> filterParams = filterConf.getFilterParams();
             if (filterClass != null) {
                 FilterHolder filterHolder = new FilterHolder();
                 filterHolder.setClassName(filterClass);
@@ -215,7 +215,7 @@ public static void configFilters(ServletContextHandler context, List<FilterConfi
                 if (filterParams != null) {
                     filterHolder.setInitParameters(filterParams);
                 } else {
-                    filterHolder.setInitParameters(new HashMap<String, String>());
+                    filterHolder.setInitParameters(new HashMap<>());
                 }
                 context.addFilter(filterHolder, "/*", EnumSet.allOf(DispatcherType.class));
             }

File: storm-core/test/jvm/org/apache/storm/nimbus/InMemoryTopologyActionNotifier.java
Patch:
@@ -30,7 +30,7 @@ public class InMemoryTopologyActionNotifier implements  ITopologyActionNotifierP
 
 
     @Override
-    public void prepare(Map StormConf) {
+    public void prepare(Map<String, Object> StormConf) {
         //no-op
     }
 

File: storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java
Patch:
@@ -3637,7 +3637,7 @@ public TopologyInfo getTopologyInfoWithOpts(String topoId, GetInfoOptions option
                     //heartbeats "stats"
                     Map<String, Object> hb = (Map<String, Object>)heartbeat.get("heartbeat");
                     if (hb != null) {
-                        Map ex = (Map) hb.get("stats");
+                        Map<String, Object> ex = (Map<String, Object>) hb.get("stats");
                         if (ex != null) {
                             ExecutorStats stats = StatsUtil.thriftifyExecutorStats(ex);
                             summ.set_stats(stats);
@@ -4101,7 +4101,7 @@ public List<OwnerResourceSummary> getOwnerResourceSummaries(String owner) throws
             IStormClusterState state = stormClusterState;
             Map<String, Assignment> topoIdToAssignments = state.topologyAssignments();
             Map<String, StormBase> topoIdToBases = state.topologyBases();
-            Map<String, Object> clusterSchedulerConfig = scheduler.config();
+            Map<String, Number> clusterSchedulerConfig = scheduler.config();
 
             //put [owner-> StormBase-list] mapping to ownerToBasesMap
             //if this owner (the input parameter) is null, add all the owners with stormbase and guarantees

File: storm-server/src/main/java/org/apache/storm/metricstore/MetricStore.java
Patch:
@@ -28,7 +28,7 @@ public interface MetricStore extends AutoCloseable {
      * @param config Storm config map
      * @throws MetricException on preparation error
      */
-    void prepare(Map config) throws MetricException;
+    void prepare(Map<String, Object> config) throws MetricException;
 
     /**
      * Stores a metric in the store.

File: storm-server/src/main/java/org/apache/storm/metricstore/MetricStoreConfig.java
Patch:
@@ -30,7 +30,7 @@ public class MetricStoreConfig {
      * @return MetricStore prepared store
      * @throws MetricException  on misconfiguration
      */
-    public static MetricStore configure(Map conf) throws MetricException {
+    public static MetricStore configure(Map<String, Object> conf) throws MetricException {
 
         try {
             String storeClass = (String)conf.get(DaemonConfig.STORM_METRIC_STORE_CLASS);

File: storm-server/src/main/java/org/apache/storm/metricstore/NimbusMetricProcessor.java
Patch:
@@ -37,5 +37,5 @@ public void processWorkerMetrics(Map<String, Object> conf, WorkerMetrics metrics
     }
 
     @Override
-    public void prepare(Map config) throws MetricException {}
+    public void prepare(Map<String, Object> config) throws MetricException {}
 }

File: storm-server/src/main/java/org/apache/storm/metricstore/WorkerMetricsProcessor.java
Patch:
@@ -36,5 +36,5 @@ public interface WorkerMetricsProcessor {
      * @param config Storm config map
      * @throws MetricException  on error
      */
-    void prepare(Map config) throws MetricException;
+    void prepare(Map<String, Object> config) throws MetricException;
 }

File: storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/RocksDbStore.java
Patch:
@@ -68,7 +68,7 @@ interface RocksDbScanCallback {
      * @param config Storm config map
      * @throws MetricException on preparation error
      */
-    public void prepare(Map config) throws MetricException {
+    public void prepare(Map<String, Object> config) throws MetricException {
         validateConfig(config);
 
         this.failureMeter = StormMetricsRegistry.registerMeter("RocksDB:metric-failures");
@@ -125,7 +125,7 @@ public void prepare(Map config) throws MetricException {
      * @throws MetricException if there is a missing required configuration or if the store does not exist but
      *                         the config specifies not to create the store
      */
-    private void validateConfig(Map config) throws MetricException {
+    private void validateConfig(Map<String, Object> config) throws MetricException {
         if (!(config.containsKey(DaemonConfig.STORM_ROCKSDB_LOCATION))) {
             throw new MetricException("Not a vaild RocksDB configuration - Missing store location " + DaemonConfig.STORM_ROCKSDB_LOCATION);
         }
@@ -156,7 +156,7 @@ private void validateConfig(Map config) throws MetricException {
         }
     }
 
-    private String getRocksDbAbsoluteDir(Map conf) throws MetricException {
+    private String getRocksDbAbsoluteDir(Map<String, Object> conf) throws MetricException {
         String storePath = (String)conf.get(DaemonConfig.STORM_ROCKSDB_LOCATION);
         if (storePath == null) {
             throw new MetricException("Not a vaild RocksDB configuration - Missing store location " + DaemonConfig.STORM_ROCKSDB_LOCATION);

File: storm-server/src/main/java/org/apache/storm/nimbus/DefaultTopologyValidator.java
Patch:
@@ -22,9 +22,11 @@
 import java.util.Map;
 
 public class DefaultTopologyValidator implements ITopologyValidator {
+
     @Override
-    public void prepare(Map StormConf){
+    public void prepare(Map<String, Object> StormConf){
     }
+
     @Override
     public void validate(String topologyName, Map<String, Object> topologyConf, StormTopology topology) throws InvalidTopologyException {        
     }    

File: storm-server/src/main/java/org/apache/storm/nimbus/ITopologyActionNotifierPlugin.java
Patch:
@@ -27,7 +27,7 @@ public interface ITopologyActionNotifierPlugin {
      * Called once during nimbus initialization.
      * @param StormConf
      */
-    void prepare(Map StormConf);
+    void prepare(Map<String, Object> StormConf);
 
     /**
      * When a new actions is executed for a topology, this method will be called.

File: storm-server/src/main/java/org/apache/storm/nimbus/ITopologyValidator.java
Patch:
@@ -22,7 +22,9 @@
 import java.util.Map;
 
 public interface ITopologyValidator {
-    void prepare(Map StormConf);
+
+    void prepare(Map<String, Object> StormConf);
+
     void validate(String topologyName, Map<String, Object> topologyConf, StormTopology topology)
             throws InvalidTopologyException;
 }

File: storm-server/src/main/java/org/apache/storm/scheduler/DefaultScheduler.java
Patch:
@@ -106,7 +106,7 @@ public void schedule(Topologies topologies, Cluster cluster) {
     }
 
     @Override
-    public Map<String, Object> config() {
+    public Map<String, Map<String, Double>> config() {
         return new HashMap<>();
     }
 }

File: storm-server/src/main/java/org/apache/storm/scheduler/EvenScheduler.java
Patch:
@@ -173,7 +173,7 @@ public void schedule(Topologies topologies, Cluster cluster) {
     }
 
     @Override
-    public Map<String, Object> config() {
+    public Map<String, Map<String, Double>> config() {
         return new HashMap<>();
     }
 

File: storm-server/src/main/java/org/apache/storm/scheduler/IScheduler.java
Patch:
@@ -43,5 +43,5 @@ public interface IScheduler {
      *
      * @return The scheduler's configuration.
      */
-    Map<String, Object> config();
+    Map config();
 }

File: storm-server/src/main/java/org/apache/storm/scheduler/IsolationScheduler.java
Patch:
@@ -57,7 +57,7 @@ public void prepare(Map<String, Object> conf) {
     }
 
     @Override
-    public Map<String, Object> config() {
+    public Map<String, Map<String, Double>> config() {
         return new HashMap<>();
     }
 

File: storm-server/src/main/java/org/apache/storm/scheduler/blacklist/strategies/DefaultBlacklistStrategy.java
Patch:
@@ -53,7 +53,7 @@ public class DefaultBlacklistStrategy implements IBlacklistStrategy {
     private TreeMap<String, Integer> blacklist;
 
     @Override
-    public void prepare(Map conf) {
+    public void prepare(Map<String, Object> conf) {
         toleranceCount = ObjectReader.getInt(conf.get(DaemonConfig.BLACKLIST_SCHEDULER_TOLERANCE_COUNT),
                 DEFAULT_BLACKLIST_SCHEDULER_TOLERANCE_COUNT);
         resumeTime = ObjectReader.getInt(conf.get(DaemonConfig.BLACKLIST_SCHEDULER_RESUME_TIME), DEFAULT_BLACKLIST_SCHEDULER_RESUME_TIME);

File: storm-server/src/main/java/org/apache/storm/scheduler/blacklist/strategies/IBlacklistStrategy.java
Patch:
@@ -27,7 +27,7 @@
 
 public interface IBlacklistStrategy {
 
-    void prepare(Map conf);
+    void prepare(Map<String, Object> conf);
 
     /**
      * Get blacklist by blacklist strategy.

File: storm-server/src/main/java/org/apache/storm/scheduler/multitenant/MultitenantScheduler.java
Patch:
@@ -34,7 +34,7 @@
 public class MultitenantScheduler implements IScheduler {
   private static final Logger LOG = LoggerFactory.getLogger(MultitenantScheduler.class);
   @SuppressWarnings("rawtypes")
-  private Map conf;
+  private Map<String, Object> conf;
   protected IConfigLoader configLoader;
   
   @Override
@@ -82,8 +82,8 @@ private Map<String, Number> getUserConf() {
   }
 
   @Override
-  public Map<String, Object> config() {
-    return (Map) getUserConf();
+  public Map config() {
+    return getUserConf();
   }
  
   @Override

File: storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceAwareScheduler.java
Patch:
@@ -63,7 +63,7 @@ public void prepare(Map<String, Object> conf) {
     }
 
     @Override
-    public Map<String, Object> config() {
+    public Map<String, Map<String, Double>> config() {
         return (Map) getUserResourcePools();
     }
 
@@ -282,7 +282,7 @@ private Map<String, Map<String, Double>> getUserResourcePools() {
         }
 
         // if no configs from loader, try to read from user-resource-pools.yaml
-        Map fromFile = Utils.findAndReadConfigFile("user-resource-pools.yaml", false);
+        Map<String, Object> fromFile = Utils.findAndReadConfigFile("user-resource-pools.yaml", false);
         raw = (Map<String, Map<String, Number>>) fromFile.get(DaemonConfig.RESOURCE_AWARE_SCHEDULER_USER_POOLS);
         if (raw != null) {
             return convertToDouble(raw);

File: storm-server/src/main/java/org/apache/storm/scheduler/utils/FileConfigLoader.java
Patch:
@@ -56,12 +56,12 @@ public FileConfigLoader(Map<String, Object> conf) {
      * @return The scheduler configuration if exists; null otherwise.
      */
     @Override
-    public Map load(String configKey) {
+    public Map<String, Object> load(String configKey) {
         if (targetFilePath != null) {
             try {
-                Map raw = (Map) Utils.readYamlFile(targetFilePath);
+                Map<String, Object> raw = (Map<String, Object>) Utils.readYamlFile(targetFilePath);
                 if (raw != null) {
-                    return (Map) raw.get(configKey);
+                    return (Map<String, Object>) raw.get(configKey);
                 }
             } catch (Exception e) {
                 LOG.error("Failed to load from file {}", targetFilePath);

File: storm-server/src/main/java/org/apache/storm/utils/ServerUtils.java
Patch:
@@ -132,7 +132,7 @@ public static BlobStore getNimbusBlobStore(Map<String, Object> conf, String base
             type = LocalFsBlobStore.class.getName();
         }
         BlobStore store = (BlobStore) ReflectionUtils.newInstance(type);
-        HashMap nconf = new HashMap(conf);
+        Map<String, Object> nconf = new HashMap<>(conf);
         // only enable cleanup of blobstore on nimbus
         nconf.put(Config.BLOBSTORE_CLEANUP_ENABLE, Boolean.TRUE);
 

File: storm-server/src/test/java/org/apache/storm/scheduler/blacklist/FaultGenerateUtils.java
Patch:
@@ -50,7 +50,7 @@ public static List<Map<String, SupervisorDetails>> getSupervisorsList(int superv
         return supervisorsList;
     }
 
-    public static Cluster nextCluster(Cluster cluster, Map<String, SupervisorDetails> supervisors, INimbus iNimbus, Map config,
+    public static Cluster nextCluster(Cluster cluster, Map<String, SupervisorDetails> supervisors, INimbus iNimbus, Map<String, Object> config,
                                       Topologies topologies) {
         Map<String, SchedulerAssignmentImpl> assignment;
         if (cluster == null) {

File: storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/webapp/LogviewerResource.java
Patch:
@@ -258,9 +258,9 @@ public Response deepSearch(@PathParam("topoId") String topologyId,
                 startFileOffset, startByteOffset, BooleanUtils.toBooleanObject(searchArchived), callback, origin);
     }
 
-    private int parseIntegerFromMap(Map map, String parameterKey) throws InvalidRequestException {
+    private int parseIntegerFromMap(Map<String, String[]> map, String parameterKey) throws InvalidRequestException {
         try {
-            return Integer.parseInt(((String[]) map.get(parameterKey))[0]);
+            return Integer.parseInt(map.get(parameterKey)[0]);
         } catch (NumberFormatException ex) {
             throw new InvalidRequestException("Could not make an integer out of the query parameter '"
                 + parameterKey + "'", ex);

File: flux/flux-core/src/test/java/org/apache/storm/flux/test/TridentTopologySource.java
Patch:
@@ -21,7 +21,6 @@
 import org.apache.storm.generated.StormTopology;
 import org.apache.storm.tuple.Fields;
 import org.apache.storm.tuple.Values;
-import org.apache.storm.kafka.StringScheme;
 import org.apache.storm.trident.TridentTopology;
 import org.apache.storm.trident.operation.BaseFunction;
 import org.apache.storm.trident.operation.TridentCollector;

File: storm-server/src/main/java/org/apache/storm/daemon/supervisor/Supervisor.java
Patch:
@@ -209,7 +209,7 @@ public void launch() throws Exception {
             eventTimer.scheduleRecurring(0, 10, new EventManagerPushCallback(readState, eventManager));
 
             // supervisor health check
-            eventTimer.scheduleRecurring(300, 300, new SupervisorHealthCheck(this));
+            eventTimer.scheduleRecurring(30, 30, new SupervisorHealthCheck(this));
         }
         LOG.info("Starting supervisor with id {} at host {}.", getId(), getHostName());
     }

File: storm-client/src/jvm/org/apache/storm/utils/SimpleVersion.java
Patch:
@@ -27,7 +27,7 @@ public class SimpleVersion implements Comparable <SimpleVersion> {
     private final int _major;
     private final int _minor;
     
-    private static final Pattern VERSION_PATTERN = Pattern.compile("(\\d+)[.-_]+(\\d+).*");
+    private static final Pattern VERSION_PATTERN = Pattern.compile("(\\d+)[\\.\\-\\_]+(\\d+).*");
     
     public SimpleVersion(String version) {
         Matcher m = VERSION_PATTERN.matcher(version);

File: storm-client/src/jvm/org/apache/storm/trident/topology/TridentTopologyBuilder.java
Patch:
@@ -779,7 +779,7 @@ public Map<String, Object> getComponentConfiguration() {
             return component.componentConf;
         }
 
-       @Override
+        @Override
         public BoltDeclarer addSharedMemory(SharedMemory request) {
             component.sharedMemory.add(request);
             return this;

File: storm-client/src/jvm/org/apache/storm/utils/TupleUtils.java
Patch:
@@ -41,7 +41,7 @@ public static boolean isTick(Tuple tuple) {
     }
 
     public static <T> int chooseTaskIndex(List<T> keys, int numTasks) {
-        return Math.abs(listHashCode(keys)) % numTasks;
+        return Math.floorMod(listHashCode(keys), numTasks);
     }
 
     private static <T> int listHashCode(List<T> alist) {

File: storm-server/src/main/java/org/apache/storm/DaemonConfig.java
Patch:
@@ -1035,7 +1035,9 @@ public class DaemonConfig implements Validated {
      * Class implementing MetricStore.
      */
     @NotNull
-    @isImplementationOfClass(implementsClass = MetricStore.class)
+    @isString
+    // Validating class implementation could fail on non-Nimbus Daemons.  Nimbus will catch the class not found on startup
+    // and log an error message, so just validating this as a String for now.
     public static final String STORM_METRIC_STORE_CLASS = "storm.metricstore.class";
 
     /**

File: storm-server/src/main/java/org/apache/storm/DaemonConfig.java
Patch:
@@ -1035,7 +1035,9 @@ public class DaemonConfig implements Validated {
      * Class implementing MetricStore.
      */
     @NotNull
-    @isImplementationOfClass(implementsClass = MetricStore.class)
+    @isString
+    // Validating class implementation could fail on non-Nimbus Daemons.  Nimbus will catch the class not found on startup
+    // and log an error message, so just validating this as a String for now.
     public static final String STORM_METRIC_STORE_CLASS = "storm.metricstore.class";
 
     /**

File: storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java
Patch:
@@ -2301,6 +2301,7 @@ private void blobSync() throws Exception {
                 sync.setNimbusInfo(nimbusInfo);
                 sync.setBlobStoreKeySet(allKeys);
                 sync.setZookeeperKeySet(zkKeys);
+                sync.setZkClient(zkClient);
                 sync.syncBlobs();
             } //else not leader (NOOP)
         } //else local (NOOP)

File: storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java
Patch:
@@ -2283,6 +2283,7 @@ private void blobSync() throws Exception {
                 sync.setNimbusInfo(nimbusInfo);
                 sync.setBlobStoreKeySet(allKeys);
                 sync.setZookeeperKeySet(zkKeys);
+                sync.setZkClient(zkClient);
                 sync.syncBlobs();
             } //else not leader (NOOP)
         } //else local (NOOP)

File: external/storm-kafka/src/test/org/apache/storm/kafka/KafkaUtilsTest.java
Patch:
@@ -126,7 +126,7 @@ public void getOffsetFromConfigAndDontForceFromStart() {
     }
 
     @Test
-    public void getOffsetFromConfigAndFroceFromStart() {
+    public void getOffsetFromConfigAndForceFromStart() {
         config.ignoreZkOffsets = true;
         config.startOffsetTime = OffsetRequest.EarliestTime();
         createTopicAndSendMessage();

File: flux/flux-core/src/test/java/org/apache/storm/flux/multilang/MultilangEnvironmentTest.java
Patch:
@@ -31,8 +31,8 @@
 /**
  * Sanity checks to make sure we can at least invoke the shells used.
  */
-public class MultilangEnvirontmentTest {
-    private static final Logger LOG = LoggerFactory.getLogger(MultilangEnvirontmentTest.class);
+public class MultilangEnvironmentTest {
+    private static final Logger LOG = LoggerFactory.getLogger(MultilangEnvironmentTest.class);
 
     @Test
     public void testInvokePython() throws Exception {

File: storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java
Patch:
@@ -52,7 +52,6 @@
 import java.util.function.UnaryOperator;
 import java.util.regex.Matcher;
 import java.util.regex.Pattern;
-import java.util.stream.Collectors;
 import javax.security.auth.Subject;
 import org.apache.storm.Config;
 import org.apache.storm.Constants;
@@ -136,22 +135,22 @@
 import org.apache.storm.nimbus.ITopologyValidator;
 import org.apache.storm.nimbus.NimbusInfo;
 import org.apache.storm.scheduler.Cluster;
-import org.apache.storm.scheduler.SupervisorResources;
 import org.apache.storm.scheduler.DefaultScheduler;
 import org.apache.storm.scheduler.ExecutorDetails;
 import org.apache.storm.scheduler.INimbus;
 import org.apache.storm.scheduler.IScheduler;
 import org.apache.storm.scheduler.SchedulerAssignment;
 import org.apache.storm.scheduler.SchedulerAssignmentImpl;
 import org.apache.storm.scheduler.SupervisorDetails;
+import org.apache.storm.scheduler.SupervisorResources;
 import org.apache.storm.scheduler.Topologies;
 import org.apache.storm.scheduler.TopologyDetails;
 import org.apache.storm.scheduler.WorkerSlot;
 import org.apache.storm.scheduler.blacklist.BlacklistScheduler;
 import org.apache.storm.scheduler.multitenant.MultitenantScheduler;
-import org.apache.storm.scheduler.resource.NormalizedResourceRequest;
 import org.apache.storm.scheduler.resource.ResourceAwareScheduler;
 import org.apache.storm.scheduler.resource.ResourceUtils;
+import org.apache.storm.scheduler.resource.normalization.NormalizedResourceRequest;
 import org.apache.storm.security.INimbusCredentialPlugin;
 import org.apache.storm.security.auth.AuthUtils;
 import org.apache.storm.security.auth.IAuthorizer;

File: storm-server/src/main/java/org/apache/storm/scheduler/ISchedulingState.java
Patch:
@@ -22,11 +22,10 @@
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
-
 import org.apache.storm.daemon.nimbus.TopologyResources;
 import org.apache.storm.generated.WorkerResources;
-import org.apache.storm.scheduler.resource.NormalizedResourceOffer;
-import org.apache.storm.scheduler.resource.NormalizedResourceRequest;
+import org.apache.storm.scheduler.resource.normalization.NormalizedResourceOffer;
+import org.apache.storm.scheduler.resource.normalization.NormalizedResourceRequest;
 
 /** An interface that provides access to the current scheduling state. */
 public interface ISchedulingState {

File: storm-server/src/main/java/org/apache/storm/scheduler/SupervisorDetails.java
Patch:
@@ -23,7 +23,7 @@
 import java.util.Map;
 import java.util.Set;
 import org.apache.storm.Constants;
-import org.apache.storm.scheduler.resource.NormalizedResourceOffer;
+import org.apache.storm.scheduler.resource.normalization.NormalizedResourceOffer;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 

File: storm-server/src/main/java/org/apache/storm/scheduler/TopologyDetails.java
Patch:
@@ -35,7 +35,7 @@
 import org.apache.storm.generated.SharedMemory;
 import org.apache.storm.generated.SpoutSpec;
 import org.apache.storm.generated.StormTopology;
-import org.apache.storm.scheduler.resource.NormalizedResourceRequest;
+import org.apache.storm.scheduler.resource.normalization.NormalizedResourceRequest;
 import org.apache.storm.utils.ObjectReader;
 import org.apache.storm.utils.Time;
 import org.apache.storm.utils.Utils;

File: storm-server/src/main/java/org/apache/storm/scheduler/resource/RAS_Node.java
Patch:
@@ -25,14 +25,12 @@
 import java.util.Map;
 import java.util.Map.Entry;
 import java.util.Set;
-
-import org.apache.storm.Constants;
 import org.apache.storm.scheduler.Cluster;
 import org.apache.storm.scheduler.ExecutorDetails;
 import org.apache.storm.scheduler.SupervisorDetails;
 import org.apache.storm.scheduler.TopologyDetails;
 import org.apache.storm.scheduler.WorkerSlot;
-import org.apache.storm.utils.ObjectReader;
+import org.apache.storm.scheduler.resource.normalization.NormalizedResourceOffer;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 

File: storm-server/src/main/java/org/apache/storm/scheduler/resource/RAS_Nodes.java
Patch:
@@ -22,7 +22,6 @@
 import java.util.HashMap;
 import java.util.LinkedList;
 import java.util.Map;
-
 import org.apache.storm.scheduler.Cluster;
 import org.apache.storm.scheduler.ExecutorDetails;
 import org.apache.storm.scheduler.SchedulerAssignment;

File: storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceAwareScheduler.java
Patch:
@@ -30,7 +30,6 @@
 import org.apache.storm.scheduler.Cluster;
 import org.apache.storm.scheduler.IScheduler;
 import org.apache.storm.scheduler.SchedulerAssignment;
-import org.apache.storm.scheduler.SchedulerAssignmentImpl;
 import org.apache.storm.scheduler.SingleTopologyCluster;
 import org.apache.storm.scheduler.Topologies;
 import org.apache.storm.scheduler.TopologyDetails;

File: storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/GenericResourceAwareStrategy.java
Patch:
@@ -21,16 +21,13 @@
 import java.util.ArrayList;
 import java.util.Collection;
 import java.util.HashSet;
-import java.util.LinkedList;
 import java.util.List;
-import java.util.Map;
 import java.util.TreeSet;
 import org.apache.storm.Config;
 import org.apache.storm.scheduler.Cluster;
 import org.apache.storm.scheduler.Component;
 import org.apache.storm.scheduler.ExecutorDetails;
 import org.apache.storm.scheduler.TopologyDetails;
-import org.apache.storm.scheduler.resource.ResourceUtils;
 import org.apache.storm.scheduler.resource.SchedulingResult;
 import org.apache.storm.scheduler.resource.SchedulingStatus;
 import org.slf4j.Logger;

File: storm-server/src/test/java/org/apache/storm/scheduler/resource/TestUtilsForResourceAwareScheduler.java
Patch:
@@ -18,6 +18,7 @@
 
 package org.apache.storm.scheduler.resource;
 
+import org.apache.storm.scheduler.resource.normalization.NormalizedResources;
 import org.apache.storm.Config;
 import org.apache.storm.DaemonConfig;
 import org.apache.storm.generated.Bolt;
@@ -65,8 +66,6 @@
 import java.util.regex.Matcher;
 import java.util.regex.Pattern;
 
-import static org.apache.storm.scheduler.resource.NormalizedResources.normalizedResourceMap;
-
 public class TestUtilsForResourceAwareScheduler {
     private static final Logger LOG = LoggerFactory.getLogger(TestUtilsForResourceAwareScheduler.class);
 
@@ -155,7 +154,7 @@ public static Map<String, SupervisorDetails> genSupervisors(int numSup, int numP
             for (int j = 0; j < numPorts; j++) {
                 ports.add(j);
             }
-            SupervisorDetails sup = new SupervisorDetails("sup-" + i, "host-" + i, null, ports, normalizedResourceMap(resourceMap));
+            SupervisorDetails sup = new SupervisorDetails("sup-" + i, "host-" + i, null, ports, NormalizedResources.RESOURCE_NAME_NORMALIZER.normalizedResourceMap(resourceMap));
             retList.put(sup.getId(), sup);
         }
         return retList;

File: storm-server/src/test/java/org/apache/storm/scheduler/resource/normalization/NormalizedResourcesRule.java
Patch:
@@ -16,9 +16,8 @@
  * limitations under the License.
  */
 
-package org.apache.storm.scheduler.resource.strategies.scheduling;
+package org.apache.storm.scheduler.resource.normalization;
 
-import org.apache.storm.scheduler.resource.NormalizedResources;
 import org.junit.rules.TestRule;
 import org.junit.runner.Description;
 import org.junit.runners.model.Statement;

File: storm-server/src/test/java/org/apache/storm/scheduler/resource/strategies/scheduling/TestGenericResourceAwareStrategy.java
Patch:
@@ -100,7 +100,7 @@ public void testGenericResourceAwareStrategySharedMemory() {
 
         Topologies topologies = new Topologies(topo);
 
-        Cluster cluster = new Cluster(iNimbus, supMap, new HashMap<String, SchedulerAssignmentImpl>(), topologies, conf);
+        Cluster cluster = new Cluster(iNimbus, supMap, new HashMap<>(), topologies, conf);
 
         ResourceAwareScheduler rs = new ResourceAwareScheduler();
 
@@ -188,7 +188,7 @@ public void testGenericResourceAwareStrategy() {
                 genExecsAndComps(stormToplogy), currentTime, "user");
 
         Topologies topologies = new Topologies(topo);
-        Cluster cluster = new Cluster(iNimbus, supMap, new HashMap<String, SchedulerAssignmentImpl>(), topologies, conf);
+        Cluster cluster = new Cluster(iNimbus, supMap, new HashMap<>(), topologies, conf);
 
         ResourceAwareScheduler rs = new ResourceAwareScheduler();
 

File: storm-server/src/main/java/org/apache/storm/blobstore/BlobStoreUtils.java
Patch:
@@ -196,10 +196,10 @@ public static boolean downloadUpdatedBlob(Map<String, Object> conf, BlobStore bl
                 // Catching and logging KeyNotFoundException because, if
                 // there is a subsequent update and delete, the non-leader
                 // nimbodes might throw an exception.
-                LOG.info("KeyNotFoundException {}", knf);
+                LOG.info("KeyNotFoundException", knf);
             } catch (Exception exp) {
                 // Logging an exception while client is connecting
-                LOG.error("Exception {}", exp);
+                LOG.error("Exception", exp);
             }
         }
 

File: storm-server/src/test/java/org/apache/storm/scheduler/resource/strategies/scheduling/TestGenericResourceAwareStrategy.java
Patch:
@@ -100,7 +100,7 @@ public void testGenericResourceAwareStrategySharedMemory() {
 
         Topologies topologies = new Topologies(topo);
 
-        Cluster cluster = new Cluster(iNimbus, supMap, new HashMap<String, SchedulerAssignmentImpl>(), topologies, conf);
+        Cluster cluster = new Cluster(iNimbus, supMap, new HashMap<>(), topologies, conf);
 
         ResourceAwareScheduler rs = new ResourceAwareScheduler();
 
@@ -188,7 +188,7 @@ public void testGenericResourceAwareStrategy() {
                 genExecsAndComps(stormToplogy), currentTime, "user");
 
         Topologies topologies = new Topologies(topo);
-        Cluster cluster = new Cluster(iNimbus, supMap, new HashMap<String, SchedulerAssignmentImpl>(), topologies, conf);
+        Cluster cluster = new Cluster(iNimbus, supMap, new HashMap<>(), topologies, conf);
 
         ResourceAwareScheduler rs = new ResourceAwareScheduler();
 

File: storm-server/src/main/java/org/apache/storm/scheduler/Cluster.java
Patch:
@@ -468,7 +468,7 @@ public boolean wouldFit(
         double maxHeap) {
 
         NormalizedResourceRequest requestedResources = td.getTotalResources(exec);
-        if (!resourcesAvailable.couldHoldIgnoringMemory(requestedResources)) {
+        if (!resourcesAvailable.couldHoldIgnoringSharedMemory(requestedResources)) {
             return false;
         }
 

File: storm-server/src/main/java/org/apache/storm/scheduler/Cluster.java
Patch:
@@ -468,7 +468,7 @@ public boolean wouldFit(
         double maxHeap) {
 
         NormalizedResourceRequest requestedResources = td.getTotalResources(exec);
-        if (!resourcesAvailable.couldHoldIgnoringMemory(requestedResources)) {
+        if (!resourcesAvailable.couldHoldIgnoringSharedMemory(requestedResources)) {
             return false;
         }
 

File: storm-server/src/main/java/org/apache/storm/daemon/supervisor/ReadClusterState.java
Patch:
@@ -137,6 +137,7 @@ public synchronized void run() {
                 }
             }
             HashSet<Integer> allPorts = new HashSet<>(assignedPorts);
+            iSuper.assigned(allPorts);
             allPorts.addAll(slots.keySet());
             
             Map<Integer, Set<TopoProfileAction>> filtered = new HashMap<>();

File: storm-server/src/main/java/org/apache/storm/daemon/supervisor/Supervisor.java
Patch:
@@ -211,7 +211,7 @@ public void launch() throws Exception {
     /**
      * start distribute supervisor
      */
-    private void launchDaemon() {
+    public void launchDaemon() {
         LOG.info("Starting supervisor for storm version '{}'.", VersionInfo.getVersion());
         try {
             Map<String, Object> conf = getConf();

File: storm-client/src/jvm/org/apache/storm/daemon/StormCommon.java
Patch:
@@ -94,6 +94,9 @@ public static StormCommon setInstance(StormCommon common) {
     public static final String TOPOLOGY_METRICS_CONSUMER_EXPAND_MAP_TYPE = "expandMapType";
     public static final String TOPOLOGY_METRICS_CONSUMER_METRIC_NAME_SEPARATOR = "metricNameSeparator";
 
+    public static final String TOPOLOGY_EVENT_LOGGER_CLASS = "class";
+    public static final String TOPOLOGY_EVENT_LOGGER_ARGUMENTS = "arguments";
+
     @Deprecated
     public static String getStormId(final IStormClusterState stormClusterState, final String topologyName) {
         return stormClusterState.getTopoId(topologyName).get();

File: flux/flux-core/src/main/java/org/apache/storm/flux/model/BeanDef.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.flux.model;
 
 /**

File: flux/flux-core/src/main/java/org/apache/storm/flux/model/BeanListReference.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.flux.model;
 
 import java.util.List;
@@ -25,9 +26,9 @@
 public class BeanListReference {
     public List<String> ids;
 
-    public BeanListReference(){}
+    public BeanListReference() {}
 
-    public BeanListReference(List<String> ids){
+    public BeanListReference(List<String> ids) {
         this.ids = ids;
     }
 

File: flux/flux-core/src/main/java/org/apache/storm/flux/model/BeanReference.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.flux.model;
 
 /**
@@ -23,9 +24,9 @@
 public class BeanReference {
     public String id;
 
-    public BeanReference(){}
+    public BeanReference() {}
 
-    public BeanReference(String id){
+    public BeanReference(String id) {
         this.id = id;
     }
 

File: flux/flux-core/src/main/java/org/apache/storm/flux/model/BoltDef.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.flux.model;
 
 /**

File: flux/flux-core/src/main/java/org/apache/storm/flux/model/GroupingDef.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.flux.model;
 
 import java.util.List;
@@ -25,7 +26,7 @@
 public class GroupingDef {
 
     /**
-     * Types of stream groupings Storm allows
+     * Types of stream groupings Storm allows.
      */
     public static enum Type {
         ALL,

File: flux/flux-core/src/main/java/org/apache/storm/flux/model/IncludeDef.java
Patch:
@@ -15,11 +15,12 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.flux.model;
 
 /**
  * Represents an include. Includes can be either a file or a classpath resource.
- *
+ *<p/>
  * If an include is marked as `override=true` then existing properties will be replaced.
  *
  */

File: flux/flux-core/src/main/java/org/apache/storm/flux/model/SpoutDef.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.flux.model;
 
 /**

File: flux/flux-core/src/main/java/org/apache/storm/flux/model/StreamDef.java
Patch:
@@ -15,11 +15,12 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.flux.model;
 
 /**
  * Represents a stream of tuples from one Storm component (Spout or Bolt) to another (an edge in the topology DAG).
- *
+ * <p/>
  * Required fields are `from` and `to`, which define the source and destination, and the stream `grouping`.
  *
  */

File: flux/flux-core/src/main/java/org/apache/storm/flux/model/TopologySourceDef.java
Patch:
@@ -15,14 +15,15 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.flux.model;
 
 public class TopologySourceDef extends ObjectDef {
     public static final String DEFAULT_METHOD_NAME = "getTopology";
 
     private String methodName;
 
-    public TopologySourceDef(){
+    public TopologySourceDef() {
         this.methodName = DEFAULT_METHOD_NAME;
     }
 

File: flux/flux-core/src/main/java/org/apache/storm/flux/model/VertexDef.java
Patch:
@@ -15,10 +15,11 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.flux.model;
 
 /**
- * Abstract parent class of component definitions
+ * Abstract parent class of component definitions.
  * (spouts/bolts)
  */
 public abstract class VertexDef extends BeanDef {

File: flux/flux-examples/src/main/java/org/apache/storm/flux/examples/TestPrintBolt.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.flux.examples;
 
 import org.apache.storm.topology.BasicOutputCollector;
@@ -23,7 +24,7 @@
 import org.apache.storm.tuple.Tuple;
 
 /**
- * Prints the tuples to stdout
+ * Prints the tuples to stdout.
  */
 public class TestPrintBolt extends BaseBasicBolt {
 

File: flux/flux-examples/src/main/java/org/apache/storm/flux/examples/TestWindowBolt.java
Patch:
@@ -15,8 +15,11 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.flux.examples;
 
+import java.util.Map;
+
 import org.apache.storm.task.OutputCollector;
 import org.apache.storm.task.TopologyContext;
 import org.apache.storm.topology.OutputFieldsDeclarer;
@@ -25,8 +28,6 @@
 import org.apache.storm.tuple.Values;
 import org.apache.storm.windowing.TupleWindow;
 
-import java.util.Map;
-
 public class TestWindowBolt extends BaseWindowedBolt {
     private OutputCollector collector;
 

File: flux/flux-wrappers/src/main/java/org/apache/storm/flux/wrappers/bolts/LogInfoBolt.java
Patch:
@@ -34,7 +34,7 @@ public class LogInfoBolt extends BaseBasicBolt {
 
     @Override
     public void execute(Tuple tuple, BasicOutputCollector basicOutputCollector) {
-       LOG.info("{}", tuple);
+        LOG.info("{}", tuple);
     }
 
     @Override

File: storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/LogviewerServer.java
Patch:
@@ -155,7 +155,7 @@ public synchronized void close() {
      */
     public static void main(String [] args) throws Exception {
         Utils.setupDefaultUncaughtExceptionHandler();
-        Map<String, Object> conf = Utils.readStormConfig();
+        Map<String, Object> conf = ConfigUtils.readStormConfig();
 
         String logRoot = ConfigUtils.workerArtifactsRoot(conf);
         File logRootFile = new File(logRoot);

File: storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/handler/LogviewerLogSearchHandler.java
Patch:
@@ -414,7 +414,7 @@ Matched findNMatches(List<File> logs, int numMatches, int fileOffset, int offset
             } catch (IOException e) {
                 throw new RuntimeException(e);
             }
-            currentFileMatch.put("port", truncatePathToLastElements(firstLogAbsPath, 2).getName(0));
+            currentFileMatch.put("port", truncatePathToLastElements(firstLogAbsPath, 2).getName(0).toString());
             newMatches.add(currentFileMatch);
 
             int newCount = matchCount + ((List<?>)theseMatches.get("matches")).size();

File: storm-server/src/main/java/org/apache/storm/scheduler/TopologyDetails.java
Patch:
@@ -500,8 +500,7 @@ private void addDefaultResforExec(ExecutorDetails exec) {
 
         Map<String,Double> topologyComponentResourcesMap = (
                 Map<String, Double>) this.topologyConf.getOrDefault(
-                    Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, new HashMap<>()
-        );
+                    Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, new HashMap<>());
 
         topologyComponentResourcesMap = normalizedResourceMap(topologyComponentResourcesMap);
 

File: storm-server/src/test/java/org/apache/storm/daemon/nimbus/NimbusTest.java
Patch:
@@ -48,7 +48,7 @@ public void testMemoryLoadLargerThanMaxHeapSize() throws Exception {
         config1.put(Config.TOPOLOGY_WORKER_MAX_HEAP_SIZE_MB, 128.0);
         config1.put(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB, 129.0);
         try {
-            Nimbus.validateTopologyWorkerMaxHeapSizeConfigs(config1, stormTopology1);
+            Nimbus.validateTopologyWorkerMaxHeapSizeConfigs(config1, stormTopology1, 768.0);
             fail("Expected exception not thrown");
         } catch (IllegalArgumentException e) {
             //Expected...

File: storm-server/src/test/java/org/apache/storm/daemon/nimbus/NimbusTest.java
Patch:
@@ -48,7 +48,7 @@ public void testMemoryLoadLargerThanMaxHeapSize() throws Exception {
         config1.put(Config.TOPOLOGY_WORKER_MAX_HEAP_SIZE_MB, 128.0);
         config1.put(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB, 129.0);
         try {
-            Nimbus.validateTopologyWorkerMaxHeapSizeConfigs(config1, stormTopology1);
+            Nimbus.validateTopologyWorkerMaxHeapSizeConfigs(config1, stormTopology1, 768.0);
             fail("Expected exception not thrown");
         } catch (IllegalArgumentException e) {
             //Expected...

File: external/storm-kafka-client/src/test/java/org/apache/storm/kafka/spout/KafkaSpoutRetryLimitTest.java
Patch:
@@ -23,7 +23,6 @@
 import static org.mockito.Mockito.verify;
 import static org.mockito.Mockito.when;
 
-import java.util.ArrayList;
 import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
@@ -79,7 +78,7 @@ public void setUp() {
     public void testFailingTupleCompletesAckAfterRetryLimitIsMet() {
         //Spout should ack failed messages after they hit the retry limit
         try (SimulatedTime simulatedTime = new SimulatedTime()) {
-            KafkaSpout<String, String> spout = SpoutWithMockedConsumerSetupHelper.setupSpout(spoutConfig, conf, contextMock, collectorMock, consumerMock, Collections.singleton(partition));
+            KafkaSpout<String, String> spout = SpoutWithMockedConsumerSetupHelper.setupSpout(spoutConfig, conf, contextMock, collectorMock, consumerMock, partition);
             Map<TopicPartition, List<ConsumerRecord<String, String>>> records = new HashMap<>();
             int lastOffset = 3;
             int numRecords = lastOffset + 1;

File: external/storm-kafka-client/src/test/java/org/apache/storm/kafka/spout/internal/OffsetManagerTest.java
Patch:
@@ -21,7 +21,6 @@
 import static org.junit.Assert.assertThat;
 
 import java.util.NoSuchElementException;
-import org.apache.kafka.clients.consumer.ConsumerRecord;
 import org.apache.kafka.clients.consumer.OffsetAndMetadata;
 import org.apache.kafka.common.TopicPartition;
 import org.apache.storm.kafka.spout.KafkaSpoutMessageId;

File: integration-test/src/main/java/org/apache/storm/st/utils/StringDecorator.java
Patch:
@@ -17,7 +17,6 @@
 
 package org.apache.storm.st.utils;
 
-import java.nio.charset.StandardCharsets;
 import org.apache.commons.lang.StringUtils;
 
 public class StringDecorator {

File: integration-test/src/test/java/org/apache/storm/st/DemoTest.java
Patch:
@@ -80,6 +80,7 @@ public void testExclamationTopology() throws Exception {
     public void cleanup() throws Exception {
         if (topo != null) {
             topo.killOrThrow();
+            topo = null;
         }
     }
 }

File: integration-test/src/test/java/org/apache/storm/st/tests/window/SlidingWindowTest.java
Patch:
@@ -188,6 +188,7 @@ static void runAndVerifyTime(int windowSec, int slideSec, TestableTopology testa
     public void cleanup() throws Exception {
         if (topo != null) {
             topo.killOrThrow();
+            topo = null;
         }
     }
 }

File: integration-test/src/test/java/org/apache/storm/st/tests/window/TumblingWindowTest.java
Patch:
@@ -30,7 +30,7 @@
 
 public final class TumblingWindowTest extends AbstractTest {
     private static Logger log = LoggerFactory.getLogger(TumblingWindowTest.class);
-    TopoWrap topo;
+    private TopoWrap topo;
 
     @DataProvider
     public static Object[][] generateWindows() {
@@ -94,6 +94,7 @@ public void testTumbleTime(int tumbleSec) throws Exception {
     public void cleanup() throws Exception {
         if (topo != null) {
             topo.killOrThrow();
+            topo = null;
         }
     }
 }

File: integration-test/src/test/java/org/apache/storm/st/wrapper/StormCluster.java
Patch:
@@ -93,7 +93,7 @@ public void killOrThrow(String topologyName) throws Exception {
                 client.killTopologyWithOpts(topologyName, killOptions);
                 log.info("Topology killed: " + topologyName);
                 return;
-            } catch (Throwable e) {
+            } catch (TException e) {
                 log.warn("Couldn't kill topology: " + topologyName + ", going to retry soon. Exception: " + ExceptionUtils.getFullStackTrace(e));
                 Thread.sleep(TimeUnit.SECONDS.toMillis(2));
             }

File: storm-client/src/jvm/org/apache/storm/cluster/IStormClusterState.java
Patch:
@@ -168,8 +168,8 @@ default Map<String, SupervisorInfo> allSupervisorInfo(Runnable callback) {
     default Optional<String> getTopoId(final String topologyName) {
         String ret = null;
         for (String topoId: activeStorms()) {
-            String name = stormBase(topoId, null).get_name();
-            if (topologyName.equals(name)) {
+            StormBase base = stormBase(topoId, null);
+            if(base != null && topologyName.equals(base.get_name())) {
                 ret = topoId;
                 break;
             }

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/bolt/AbstractHdfsBolt.java
Patch:
@@ -255,6 +255,7 @@ public void declareOutputFields(OutputFieldsDeclarer outputFieldsDeclarer) {
     @Override
     public void cleanup() {
         doRotationAndRemoveAllWriters();
+        this.rotationTimer.cancel();
     }
 
     private void doRotationAndRemoveAllWriters() {

File: storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java
Patch:
@@ -958,8 +958,8 @@ private static Map<IClusterMetricsConsumer.SupervisorInfo, List<DataPoint>> extr
             List<DataPoint> metrics = new ArrayList<>();
             metrics.add(new DataPoint("slotsTotal", sup.get_num_workers()));
             metrics.add(new DataPoint("slotsUsed", sup.get_num_used_workers()));
-            metrics.add(new DataPoint("totalMem", sup.get_total_resources().get(Config.SUPERVISOR_MEMORY_CAPACITY_MB)));
-            metrics.add(new DataPoint("totalCpu", sup.get_total_resources().get(Config.SUPERVISOR_CPU_CAPACITY)));
+            metrics.add(new DataPoint("totalMem", sup.get_total_resources().get(Constants.COMMON_TOTAL_MEMORY_RESOURCE_NAME)));
+            metrics.add(new DataPoint("totalCpu", sup.get_total_resources().get(Constants.COMMON_CPU_RESOURCE_NAME)));
             metrics.add(new DataPoint("usedMem", sup.get_used_mem()));
             metrics.add(new DataPoint("usedCpu", sup.get_used_cpu()));
             ret.put(info, metrics);

File: integration-test/src/main/java/org/apache/storm/st/topology/window/TumblingTimeCorrectness.java
Patch:
@@ -55,9 +55,9 @@ public class TumblingTimeCorrectness implements TestableTopology {
     private final String spoutName;
     private final String boltName;
 
-    public TumblingTimeCorrectness(int timbleSec) {
-        this.tumbleSec = timbleSec;
-        final String prefix = this.getClass().getSimpleName() + "-timbleSec" + timbleSec;
+    public TumblingTimeCorrectness(int tumbleSec) {
+        this.tumbleSec = tumbleSec;
+        final String prefix = this.getClass().getSimpleName() + "-tumbleSec" + tumbleSec;
         spoutName = prefix + "IncrementingSpout";
         boltName = prefix + "VerificationBolt";
     }

File: integration-test/src/main/java/org/apache/storm/st/topology/window/TumblingWindowCorrectness.java
Patch:
@@ -53,7 +53,7 @@ public class TumblingWindowCorrectness implements TestableTopology {
 
     public TumblingWindowCorrectness(final int tumbleSize) {
         this.tumbleSize = tumbleSize;
-        final String prefix = this.getClass().getSimpleName() + "-tubleSize" + tumbleSize;
+        final String prefix = this.getClass().getSimpleName() + "-tumbleSize" + tumbleSize;
         spoutName = prefix + "IncrementingSpout";
         boltName = prefix + "VerificationBolt";
     }

File: integration-test/src/test/java/org/apache/storm/st/DemoTest.java
Patch:
@@ -79,7 +79,7 @@ public void testExclamationTopology() throws Exception {
     @AfterMethod
     public void cleanup() throws Exception {
         if (topo != null) {
-            topo.killQuietly();
+            topo.killOrThrow();
         }
     }
 }

File: integration-test/src/test/java/org/apache/storm/st/tests/window/SlidingWindowTest.java
Patch:
@@ -67,7 +67,7 @@ public static Object[][] generateCountWindows() {
     @Test(dataProvider = "generateCountWindows")
     public void testWindowCount(int windowSize, int slideSize) throws Exception {
         final SlidingWindowCorrectness testable = new SlidingWindowCorrectness(windowSize, slideSize);
-        final String topologyName = this.getClass().getSimpleName() + "w" + windowSize + "s" + slideSize;
+        final String topologyName = this.getClass().getSimpleName() + "-window" + windowSize + "-slide" + slideSize;
         if (windowSize <= 0 || slideSize <= 0) {
             try {
                 testable.newTopology();
@@ -135,7 +135,7 @@ public static Object[][] generateTimeWindows() {
     @Test(dataProvider = "generateTimeWindows")
     public void testTimeWindow(int windowSec, int slideSec) throws Exception {
         final SlidingTimeCorrectness testable = new SlidingTimeCorrectness(windowSec, slideSec);
-        final String topologyName = this.getClass().getSimpleName() + "w" + windowSec + "s" + slideSec;
+        final String topologyName = this.getClass().getSimpleName() + "-window" + windowSec + "-slide" + slideSec;
         if (windowSec <= 0 || slideSec <= 0) {
             try {
                 testable.newTopology();
@@ -187,7 +187,7 @@ static void runAndVerifyTime(int windowSec, int slideSec, TestableTopology testa
     @AfterMethod
     public void cleanup() throws Exception {
         if (topo != null) {
-            topo.killQuietly();
+            topo.killOrThrow();
         }
     }
 }

File: integration-test/src/test/java/org/apache/storm/st/tests/window/TumblingWindowTest.java
Patch:
@@ -48,7 +48,7 @@ public static Object[][] generateWindows() {
     @Test(dataProvider = "generateWindows")
     public void testTumbleCount(int tumbleSize) throws Exception {
         final TumblingWindowCorrectness testable = new TumblingWindowCorrectness(tumbleSize);
-        final String topologyName = this.getClass().getSimpleName() + "t" + tumbleSize;
+        final String topologyName = this.getClass().getSimpleName() + "-size" + tumbleSize;
         if (tumbleSize <= 0) {
             try {
                 testable.newTopology();
@@ -77,7 +77,7 @@ public static Object[][] generateTumbleTimes() {
     @Test(dataProvider = "generateTumbleTimes")
     public void testTumbleTime(int tumbleSec) throws Exception {
         final TumblingTimeCorrectness testable = new TumblingTimeCorrectness(tumbleSec);
-        final String topologyName = this.getClass().getSimpleName() + "t" + tumbleSec;
+        final String topologyName = this.getClass().getSimpleName() + "-sec" + tumbleSec;
         if (tumbleSec <= 0) {
             try {
                 testable.newTopology();
@@ -93,7 +93,7 @@ public void testTumbleTime(int tumbleSec) throws Exception {
     @AfterMethod
     public void cleanup() throws Exception {
         if (topo != null) {
-            topo.killQuietly();
+            topo.killOrThrow();
         }
     }
 }

File: integration-test/src/test/java/org/apache/storm/st/wrapper/TopoWrap.java
Patch:
@@ -229,7 +229,7 @@ public void waitForProgress(int minEmits, String componentName, int maxWaitSec)
     public void assertProgress(int minEmits, String componentName, int maxWaitSec) throws TException {
         waitForProgress(minEmits, componentName, maxWaitSec);
         long emitCount = getAllTimeEmittedCount(componentName);
-        Assert.assertTrue(emitCount >= minEmits, "Count for component " + componentName + " is " + emitCount + " min is " + minEmits);
+        Assert.assertTrue(emitCount >= minEmits, "Emit count for component '" + componentName + "' is " + emitCount + ", min is " + minEmits);
     }
 
     public static class ExecutorURL {
@@ -373,7 +373,7 @@ private Number sum(Collection<? extends Number> nums) {
         return retVal;
     }
 
-    public void killQuietly() {
-        cluster.killSilently(name);
+    public void killOrThrow() throws Exception {
+        cluster.killOrThrow(name);
     }
 }

File: storm-client/src/jvm/org/apache/storm/utils/RotatingMap.java
Patch:
@@ -67,7 +67,7 @@ public RotatingMap(ExpiredCallback<K, V> callback) {
 
     public RotatingMap(int numBuckets) {
         this(numBuckets, null);
-    }   
+    }
     
     public Map<K, V> rotate() {
         Map<K, V> dead = _buckets.removeLast();

File: storm-core/src/jvm/org/apache/storm/utils/TopologySpoutLag.java
Patch:
@@ -162,7 +162,7 @@ private static Map<String, Object> getLagResultForKafka (String spoutId, SpoutSp
 
             // if commands contains one or more null value, spout is compiled with lower version of storm-kafka / storm-kafka-client
             if (!commands.contains(null)) {
-                String resultFromMonitor = ShellUtils.execCommand(commands.toArray(new String[0]));
+                String resultFromMonitor = new ShellCommandRunnerImpl().execCommand(commands.toArray(new String[0]));
 
                 try {
                     result = (Map<String, Object>) JSONValue.parseWithException(resultFromMonitor);

File: storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java
Patch:
@@ -4132,6 +4132,9 @@ public void shutdown() {
             stormClusterState.disconnect();
             downloaders.cleanup();
             uploaders.cleanup();
+            blobDownloaders.cleanup();
+            blobUploaders.cleanup();
+            blobListers.cleanup();
             blobStore.shutdown();
             leaderElector.close();
             ITopologyActionNotifierPlugin actionNotifier = nimbusTopologyActionNotifier;

File: storm-client/src/jvm/org/apache/storm/utils/RotatingMap.java
Patch:
@@ -67,7 +67,7 @@ public RotatingMap(ExpiredCallback<K, V> callback) {
 
     public RotatingMap(int numBuckets) {
         this(numBuckets, null);
-    }   
+    }
     
     public Map<K, V> rotate() {
         Map<K, V> dead = _buckets.removeLast();

File: storm-core/src/jvm/org/apache/storm/utils/TopologySpoutLag.java
Patch:
@@ -162,7 +162,7 @@ private static Map<String, Object> getLagResultForKafka (String spoutId, SpoutSp
 
             // if commands contains one or more null value, spout is compiled with lower version of storm-kafka / storm-kafka-client
             if (!commands.contains(null)) {
-                String resultFromMonitor = ShellUtils.execCommand(commands.toArray(new String[0]));
+                String resultFromMonitor = new ShellCommandRunnerImpl().execCommand(commands.toArray(new String[0]));
 
                 try {
                     result = (Map<String, Object>) JSONValue.parseWithException(resultFromMonitor);

File: storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java
Patch:
@@ -4132,6 +4132,9 @@ public void shutdown() {
             stormClusterState.disconnect();
             downloaders.cleanup();
             uploaders.cleanup();
+            blobDownloaders.cleanup();
+            blobUploaders.cleanup();
+            blobListers.cleanup();
             blobStore.shutdown();
             leaderElector.close();
             ITopologyActionNotifierPlugin actionNotifier = nimbusTopologyActionNotifier;

File: external/storm-kafka-client/src/test/java/org/apache/storm/kafka/spout/config/builder/SingleTopicKafkaSpoutConfiguration.java
Patch:
@@ -41,7 +41,7 @@ public static KafkaSpoutConfig.Builder<String, String> createKafkaSpoutConfigBui
         return setCommonSpoutConfig(new KafkaSpoutConfig.Builder<>("127.0.0.1:" + port, subscription));
     }
 
-    private static KafkaSpoutConfig.Builder<String, String> setCommonSpoutConfig(KafkaSpoutConfig.Builder<String, String> config) {
+    public static KafkaSpoutConfig.Builder<String, String> setCommonSpoutConfig(KafkaSpoutConfig.Builder<String, String> config) {
         return config.setRecordTranslator((r) -> new Values(r.topic(), r.key(), r.value()),
             new Fields("topic", "key", "value"), STREAM)
             .setProp(ConsumerConfig.GROUP_ID_CONFIG, "kafkaSpoutTestGroup")

File: storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java
Patch:
@@ -2673,7 +2673,7 @@ public void submitTopologyWithOpts(String topoName, String uploadedJarLocation,
             }
             Map<String, Object> otherConf = Utils.getConfigFromClasspath(cp, conf);
             Map<String, Object> totalConfToSave = Utils.merge(otherConf, topoConf);
-            Map<String, Object> totalConf = Utils.merge(totalConfToSave, conf);
+            Map<String, Object> totalConf = Utils.merge(conf, totalConfToSave);
             //When reading the conf in nimbus we want to fall back to our own settings
             // if the other config does not have it set.
             topology = normalizeTopology(totalConf, topology);

File: storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java
Patch:
@@ -2673,7 +2673,7 @@ public void submitTopologyWithOpts(String topoName, String uploadedJarLocation,
             }
             Map<String, Object> otherConf = Utils.getConfigFromClasspath(cp, conf);
             Map<String, Object> totalConfToSave = Utils.merge(otherConf, topoConf);
-            Map<String, Object> totalConf = Utils.merge(totalConfToSave, conf);
+            Map<String, Object> totalConf = Utils.merge(conf, totalConfToSave);
             //When reading the conf in nimbus we want to fall back to our own settings
             // if the other config does not have it set.
             topology = normalizeTopology(totalConf, topology);

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/OffsetManager.java
Patch:
@@ -97,7 +97,7 @@ public OffsetAndMetadata findNextCommitOffset() {
                         So, if the topic doesn't contain offset = nextCommitOffset (possible
                         if the topic is compacted or deleted), the consumer should jump to
                         the next logical point in the topic. Next logical offset should be the
-                        first element after nextCommitOffsset in the ascending ordered emitted set.
+                        first element after nextCommitOffset in the ascending ordered emitted set.
                      */
                     LOG.debug("Processed non-sequential offset."
                         + " The earliest uncommitted offset is no longer part of the topic."

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java
Patch:
@@ -197,7 +197,7 @@ private void initialize(Collection<TopicPartition> partitions) {
         }
 
         /**
-         * sets the cursor to the location dictated by the first poll strategy and returns the fetch offset.
+         * Sets the cursor to the location dictated by the first poll strategy and returns the fetch offset.
          */
         private long doSeek(TopicPartition tp, OffsetAndMetadata committedOffset) {
             if (committedOffset != null) {             // offset was committed for this TopicPartition
@@ -206,8 +206,8 @@ private long doSeek(TopicPartition tp, OffsetAndMetadata committedOffset) {
                 } else if (firstPollOffsetStrategy.equals(LATEST)) {
                     kafkaConsumer.seekToEnd(Collections.singleton(tp));
                 } else {
-                    // By default polling starts at the last committed offset. +1 to point fetch to the first uncommitted offset.
-                    kafkaConsumer.seek(tp, committedOffset.offset() + 1);
+                    // By default polling starts at the last committed offset, i.e. the first offset that was not marked as processed.
+                    kafkaConsumer.seek(tp, committedOffset.offset());
                 }
             } else {    // no commits have ever been done, so start at the beginning or end depending on the strategy
                 if (firstPollOffsetStrategy.equals(EARLIEST) || firstPollOffsetStrategy.equals(UNCOMMITTED_EARLIEST)) {

File: external/storm-kafka-client/src/test/java/org/apache/storm/kafka/spout/SingleTopicKafkaUnitSetupHelper.java
Patch:
@@ -16,7 +16,7 @@
 
 package org.apache.storm.kafka.spout;
 
-import static org.mockito.Matchers.any;
+import static org.mockito.ArgumentMatchers.any;
 import static org.mockito.Mockito.when;
 
 import java.util.Collections;

File: external/storm-kafka-client/src/test/java/org/apache/storm/kafka/spout/SpoutWithMockedConsumerSetupHelper.java
Patch:
@@ -16,7 +16,7 @@
 
 package org.apache.storm.kafka.spout;
 
-import static org.mockito.Matchers.any;
+import static org.mockito.ArgumentMatchers.any;
 import static org.mockito.Mockito.verify;
 import static org.mockito.Mockito.when;
 

File: storm-client/src/jvm/org/apache/storm/utils/Utils.java
Patch:
@@ -106,7 +106,6 @@ public class Utils {
     public static final Logger LOG = LoggerFactory.getLogger(Utils.class);
     public static final String DEFAULT_STREAM_ID = "default";
     private static final Set<Class> defaultAllowedExceptions = new HashSet<>();
-    public static final String FILE_PATH_SEPARATOR = System.getProperty("file.separator");
     private static final List<String> LOCALHOST_ADDRESSES = Lists.newArrayList("localhost", "127.0.0.1", "0:0:0:0:0:0:0:1");
 
     private static ThreadLocal<TSerializer> threadSer = new ThreadLocal<TSerializer>();

File: storm-server/src/main/java/org/apache/storm/localizer/AsyncLocalizer.java
Patch:
@@ -318,7 +318,7 @@ public Void get() {
                     }
                 }
                 boolean deleteAll = true;
-                String tmproot = ServerConfigUtils.supervisorTmpDir(conf) + Utils.FILE_PATH_SEPARATOR + Utils.uuid();
+                String tmproot = ServerConfigUtils.supervisorTmpDir(conf) + File.separator + Utils.uuid();
                 File tr = new File(tmproot);
                 try {
                     downloadBaseBlobs(tr);
@@ -379,7 +379,7 @@ protected void downloadBaseBlobs(File tmproot) throws Exception {
             String resourcesJar = resourcesJar();
             URL url = classloader.getResource(ServerConfigUtils.RESOURCES_SUBDIR);
 
-            String targetDir = tmproot + Utils.FILE_PATH_SEPARATOR;
+            String targetDir = tmproot + File.separator;
             if (resourcesJar != null) {
                 LOG.info("Extracting resources from jar at {} to {}", resourcesJar, targetDir);
                 ServerUtils.extractDirFromJar(resourcesJar, ServerConfigUtils.RESOURCES_SUBDIR, new File(targetDir));

File: storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/utils/LogFileDownloader.java
Patch:
@@ -43,7 +43,7 @@ public LogFileDownloader(String logRoot, String daemonLogRoot, ResourceAuthorize
     }
 
     /**
-     * Checks authorization for the log file and download
+     * Checks authorization for the log file and download.
      *
      * @param fileName file to download
      * @param user username

File: storm-client/src/jvm/org/apache/storm/daemon/worker/WorkerState.java
Patch:
@@ -485,7 +485,7 @@ public void activateWorkerWhenAllConnectionsReady() {
         refreshActiveTimer.schedule(delaySecs, new Runnable() {
             @Override public void run() {
                 if (areAllConnectionsReady()) {
-                    LOG.info("All connections are ready for worker {}:{} with id", assignmentId, port, workerId);
+                    LOG.info("All connections are ready for worker {}:{} with id {}", assignmentId, port, workerId);
                     isWorkerActive.set(Boolean.TRUE);
                 } else {
                     refreshActiveTimer.schedule(recurSecs, () -> activateWorkerWhenAllConnectionsReady(), false, 0);

File: storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/LogviewerServer.java
Patch:
@@ -55,7 +55,8 @@
 public class LogviewerServer implements AutoCloseable {
     private static final Logger LOG = LoggerFactory.getLogger(LogviewerServer.class);
     private static final Meter meterShutdownCalls = StormMetricsRegistry.registerMeter("logviewer:num-shutdown-calls");
-    public static final String STATIC_RESOURCE_DIRECTORY_PATH = "./public";
+    private static final String stormHome = System.getProperty("storm.home");
+    public static final String STATIC_RESOURCE_DIRECTORY_PATH = stormHome + "/public";
 
     private static Server mkHttpServer(Map<String, Object> conf) {
         Integer logviewerHttpPort = (Integer) conf.get(DaemonConfig.LOGVIEWER_PORT);

File: storm-server/src/main/java/org/apache/storm/scheduler/blacklist/reporters/IReporter.java
Patch:
@@ -15,14 +15,15 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.scheduler.blacklist.reporters;
 
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
 
 /**
- * report blacklist to alert system
+ * report blacklist to alert system.
  */
 public interface IReporter {
     void report(String message);

File: storm-client/src/jvm/org/apache/storm/Config.java
Patch:
@@ -1515,8 +1515,8 @@ public class Config extends HashMap<String, Object> {
     public static final String STORM_EXHIBITOR_PORT = "storm.exhibitor.port";
 
     /*
- * How often to poll Exhibitor cluster in millis.
- */
+     * How often to poll Exhibitor cluster in millis.
+     */
     @isString
     public static final String STORM_EXHIBITOR_URIPATH="storm.exhibitor.poll.uripath";
 
@@ -1532,7 +1532,7 @@ public class Config extends HashMap<String, Object> {
     @isInteger
     public static final String STORM_EXHIBITOR_RETRY_TIMES="storm.exhibitor.retry.times";
 
-    /**
+    /*
      * The interval between retries of an Exhibitor operation.
      */
     @isInteger

File: storm-server/src/main/java/org/apache/storm/DaemonConfig.java
Patch:
@@ -310,7 +310,7 @@ public class DaemonConfig implements Validated {
     public static final String LOGVIEWER_MAX_PER_WORKER_LOGS_SIZE_MB = "logviewer.max.per.worker.logs.size.mb";
 
     /**
-     * Storm Logviewer HTTPS port.
+     * Storm Logviewer HTTPS port. Logviewer must use HTTPS if Storm UI is using HTTPS.
      */
     @isInteger
     @isPositiveNumber
@@ -413,7 +413,7 @@ public class DaemonConfig implements Validated {
     public static final String UI_HEADER_BUFFER_BYTES = "ui.header.buffer.bytes";
 
     /**
-     * This port is used by Storm DRPC for receiving HTTPS (SSL) DPRC requests from clients.
+     * This port is used by Storm UI for receiving HTTPS (SSL) requests from clients.
      */
     @isInteger
     @isPositiveNumber

File: storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/LogviewerServer.java
Patch:
@@ -54,7 +54,7 @@
  */
 public class LogviewerServer implements AutoCloseable {
     private static final Logger LOG = LoggerFactory.getLogger(LogviewerServer.class);
-    private static final Meter meterShutdownCalls = StormMetricsRegistry.registerMeter("drpc:num-shutdown-calls");
+    private static final Meter meterShutdownCalls = StormMetricsRegistry.registerMeter("logviewer:num-shutdown-calls");
     public static final String STATIC_RESOURCE_DIRECTORY_PATH = "./public";
 
     private static Server mkHttpServer(Map<String, Object> conf) {

File: storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/LogviewerServer.java
Patch:
@@ -55,7 +55,8 @@
 public class LogviewerServer implements AutoCloseable {
     private static final Logger LOG = LoggerFactory.getLogger(LogviewerServer.class);
     private static final Meter meterShutdownCalls = StormMetricsRegistry.registerMeter("drpc:num-shutdown-calls");
-    public static final String STATIC_RESOURCE_DIRECTORY_PATH = "./public";
+    private static final String stormHome = System.getProperty("storm.home");
+    public static final String STATIC_RESOURCE_DIRECTORY_PATH = stormHome + "/public";
 
     private static Server mkHttpServer(Map<String, Object> conf) {
         Integer logviewerHttpPort = (Integer) conf.get(DaemonConfig.LOGVIEWER_PORT);

File: external/storm-autocreds/src/main/java/org/apache/storm/hbase/security/HBaseSecurityUtil.java
Patch:
@@ -45,7 +45,7 @@ public class HBaseSecurityUtil {
     public static final String HBASE_KEYTAB_FILE_KEY = "hbase.keytab.file";
     public static final String HBASE_PRINCIPAL_KEY = "hbase.kerberos.principal";
 
-    private static UserProvider legacyProvider = null;
+    private static volatile UserProvider legacyProvider = null;
 
     private HBaseSecurityUtil() {
     }

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/trident/state/CassandraState.java
Patch:
@@ -149,7 +149,7 @@ public List<List<Values>> batchRetrieve(List<TridentTuple> tridentTuples) {
                 }
             }
         } catch (Exception e) {
-            LOG.warn("Batch retrieve operation is failed.");
+            LOG.warn("Batch retrieve operation is failed", e);
             throw new FailedException(e);
         }
         return batchRetrieveResult;

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/trident/state/CassandraState.java
Patch:
@@ -149,7 +149,7 @@ public List<List<Values>> batchRetrieve(List<TridentTuple> tridentTuples) {
                 }
             }
         } catch (Exception e) {
-            LOG.warn("Batch retrieve operation is failed.");
+            LOG.warn("Batch retrieve operation is failed", e);
             throw new FailedException(e);
         }
         return batchRetrieveResult;

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/ByTopicRecordTranslator.java
Patch:
@@ -61,8 +61,9 @@ public ByTopicRecordTranslator(Func<ConsumerRecord<K, V>, List<Object>> func, Fi
     }
     
     /**
+     * Create a record translator with the given default translator.
      * @param defaultTranslator a translator that will be used for all topics not explicitly set
-     *     elsewhere.
+     *     with one of the variants of {@link #forTopic(java.lang.String, org.apache.storm.kafka.spout.RecordTranslator) }.
      */
     public ByTopicRecordTranslator(RecordTranslator<K,V> defaultTranslator) {
         this.defaultTranslator = defaultTranslator;

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java
Patch:
@@ -332,7 +332,7 @@ private void emit() {
     }
 
     /**
-     * Creates a tuple from the kafka record and emits it if it was not yet emitted
+     * Creates a tuple from the kafka record and emits it if it was not yet emitted.
      *
      * @param record to be emitted
      * @return true if tuple was emitted. False if tuple has been acked or has been emitted and is pending ack or fail

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutRetryService.java
Patch:
@@ -54,6 +54,7 @@ public interface KafkaSpoutRetryService extends Serializable {
     boolean retainAll(Collection<TopicPartition> topicPartitions);
 
     /**
+     * Gets the earliest retriable offsets.
      * @return The earliest retriable offset for each TopicPartition that has
      *     offsets ready to be retried, i.e. for which a tuple has failed
      *     and has retry time less than current time.

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/RecordTranslator.java
Patch:
@@ -32,7 +32,7 @@ public interface RecordTranslator<K, V> extends Serializable, Func<ConsumerRecor
     public static final List<String> DEFAULT_STREAM = Collections.singletonList("default");
     
     /**
-     * Translate the ConsumerRecord into a list of objects that can be emitted
+     * Translate the ConsumerRecord into a list of objects that can be emitted.
      * @param record the record to translate
      * @return the objects in the tuple.  Return a {@link KafkaTuple}
      *     if you want to route the tuple to a non-default stream.

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/subscription/ManualPartitioner.java
Patch:
@@ -32,7 +32,7 @@
 @FunctionalInterface
 public interface ManualPartitioner extends Serializable {
     /**
-     * Get the partitions for this assignment
+     * Get the partitions for this assignment.
      * @param allPartitions all of the partitions that the set of spouts want to subscribe to, in a strict ordering
      * @param context the context of the topology
      * @return the subset of the partitions that this spout should use.

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/subscription/Subscription.java
Patch:
@@ -40,6 +40,7 @@ public abstract class Subscription implements Serializable {
     public abstract <K, V> void subscribe(KafkaConsumer<K,V> consumer, ConsumerRebalanceListener listener, TopologyContext context);
     
     /**
+     * Get the topics string.
      * @return A human-readable string representing the subscribed topics.
      */
     public abstract String getTopicsString();

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/subscription/TopicFilter.java
Patch:
@@ -31,6 +31,7 @@ public interface TopicFilter extends Serializable {
     List<TopicPartition> getFilteredTopicPartitions(KafkaConsumer<?, ?> consumer);
     
     /**
+     * Get the topics string.
      * @return A human-readable string representing the topics that pass the filter.
      */
     String getTopicsString();

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/trident/TridentKafkaState.java
Patch:
@@ -79,7 +79,7 @@ public void prepare(Properties options) {
     /**
      * Write the given tuples to Kafka.
      * @param tuples The tuples to write.
-     * @param collector Tbe Trident collector.
+     * @param collector The Trident collector.
      */
     public void updateState(List<TridentTuple> tuples, TridentCollector collector) {
         String topic = null;

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/ByTopicRecordTranslator.java
Patch:
@@ -61,8 +61,9 @@ public ByTopicRecordTranslator(Func<ConsumerRecord<K, V>, List<Object>> func, Fi
     }
     
     /**
+     * Create a record translator with the given default translator.
      * @param defaultTranslator a translator that will be used for all topics not explicitly set
-     *     elsewhere.
+     *     with one of the variants of {@link #forTopic(java.lang.String, org.apache.storm.kafka.spout.RecordTranslator) }.
      */
     public ByTopicRecordTranslator(RecordTranslator<K,V> defaultTranslator) {
         this.defaultTranslator = defaultTranslator;

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java
Patch:
@@ -332,7 +332,7 @@ private void emit() {
     }
 
     /**
-     * Creates a tuple from the kafka record and emits it if it was not yet emitted
+     * Creates a tuple from the kafka record and emits it if it was not yet emitted.
      *
      * @param record to be emitted
      * @return true if tuple was emitted. False if tuple has been acked or has been emitted and is pending ack or fail

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutRetryService.java
Patch:
@@ -54,6 +54,7 @@ public interface KafkaSpoutRetryService extends Serializable {
     boolean retainAll(Collection<TopicPartition> topicPartitions);
 
     /**
+     * Gets the earliest retriable offsets.
      * @return The earliest retriable offset for each TopicPartition that has
      *     offsets ready to be retried, i.e. for which a tuple has failed
      *     and has retry time less than current time.

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/RecordTranslator.java
Patch:
@@ -32,7 +32,7 @@ public interface RecordTranslator<K, V> extends Serializable, Func<ConsumerRecor
     public static final List<String> DEFAULT_STREAM = Collections.singletonList("default");
     
     /**
-     * Translate the ConsumerRecord into a list of objects that can be emitted
+     * Translate the ConsumerRecord into a list of objects that can be emitted.
      * @param record the record to translate
      * @return the objects in the tuple.  Return a {@link KafkaTuple}
      *     if you want to route the tuple to a non-default stream.

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/subscription/ManualPartitioner.java
Patch:
@@ -32,7 +32,7 @@
 @FunctionalInterface
 public interface ManualPartitioner extends Serializable {
     /**
-     * Get the partitions for this assignment
+     * Get the partitions for this assignment.
      * @param allPartitions all of the partitions that the set of spouts want to subscribe to, in a strict ordering
      * @param context the context of the topology
      * @return the subset of the partitions that this spout should use.

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/subscription/Subscription.java
Patch:
@@ -40,6 +40,7 @@ public abstract class Subscription implements Serializable {
     public abstract <K, V> void subscribe(KafkaConsumer<K,V> consumer, ConsumerRebalanceListener listener, TopologyContext context);
     
     /**
+     * Get the topics string.
      * @return A human-readable string representing the subscribed topics.
      */
     public abstract String getTopicsString();

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/subscription/TopicFilter.java
Patch:
@@ -31,6 +31,7 @@ public interface TopicFilter extends Serializable {
     List<TopicPartition> getFilteredTopicPartitions(KafkaConsumer<?, ?> consumer);
     
     /**
+     * Get the topics string.
      * @return A human-readable string representing the topics that pass the filter.
      */
     String getTopicsString();

File: external/storm-kafka-client/src/test/java/org/apache/storm/kafka/spout/KafkaSpoutConfigTest.java
Patch:
@@ -44,7 +44,7 @@ public void testBasic() {
     }
 
     @Test
-    public void test_setEmitNullTuples_true_true() {
+    public void testSetEmitNullTuplesToTrue() {
         final KafkaSpoutConfig<String, String> conf = KafkaSpoutConfig.builder("localhost:1234", "topic")
                 .setEmitNullTuples(true)
                 .build();

File: external/storm-kafka-client/src/test/java/org/apache/storm/kafka/spout/KafkaSpoutConfigTest.java
Patch:
@@ -44,7 +44,7 @@ public void testBasic() {
     }
 
     @Test
-    public void test_setEmitNullTuples_true_true() {
+    public void testSetEmitNullTuplesToTrue() {
         final KafkaSpoutConfig<String, String> conf = KafkaSpoutConfig.builder("localhost:1234", "topic")
                 .setEmitNullTuples(true)
                 .build();

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/bolt/Writer.java
Patch:
@@ -15,13 +15,13 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.hdfs.bolt;
 
+import java.io.IOException;
 import org.apache.hadoop.fs.Path;
 import org.apache.storm.tuple.Tuple;
 
-import java.io.IOException;
-
 public interface Writer {
     long write(Tuple tuple) throws IOException;
 
@@ -32,4 +32,4 @@ public interface Writer {
     boolean needsRotation();
 
     Path getFilePath();
-}
\ No newline at end of file
+}

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/common/AbstractHDFSWriter.java
Patch:
@@ -15,15 +15,15 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.hdfs.common;
 
+import java.io.IOException;
 import org.apache.hadoop.fs.Path;
 import org.apache.storm.hdfs.bolt.Writer;
 import org.apache.storm.hdfs.bolt.rotation.FileRotationPolicy;
 import org.apache.storm.tuple.Tuple;
 
-import java.io.IOException;
-
 abstract public class AbstractHDFSWriter implements Writer {
     protected long lastUsedTime;
     protected long offset;

File: storm-client/src/jvm/org/apache/storm/executor/Executor.java
Patch:
@@ -387,7 +387,7 @@ private DisruptorQueue mkExecutorBatchQueue(Map<String, Object> topoConf, List<L
         int waitTimeOutMs = ObjectReader.getInt(topoConf.get(Config.TOPOLOGY_DISRUPTOR_WAIT_TIMEOUT_MILLIS));
         int batchSize = ObjectReader.getInt(topoConf.get(Config.TOPOLOGY_DISRUPTOR_BATCH_SIZE));
         int batchTimeOutMs = ObjectReader.getInt(topoConf.get(Config.TOPOLOGY_DISRUPTOR_BATCH_TIMEOUT_MILLIS));
-        return new DisruptorQueue("executor" + executorId + "-send-queue", ProducerType.SINGLE,
+        return new DisruptorQueue("executor" + executorId + "-send-queue", ProducerType.MULTI,
                 sendSize, waitTimeOutMs, batchSize, batchTimeOutMs);
     }
 

File: storm-client/src/jvm/org/apache/storm/streams/processors/CoGroupByKeyProcessor.java
Patch:
@@ -19,7 +19,6 @@
 package org.apache.storm.streams.processors;
 
 import com.google.common.collect.ArrayListMultimap;
-import com.google.common.collect.ImmutableMultimap;
 import com.google.common.collect.Multimap;
 import org.apache.storm.streams.Pair;
 

File: storm-client/test/jvm/org/apache/storm/streams/processors/CoGroupByKeyProcessorTest.java
Patch:
@@ -17,10 +17,7 @@
  */
 package org.apache.storm.streams.processors;
 
-import org.apache.curator.shaded.com.google.common.collect.ImmutableBiMap;
-import org.apache.curator.shaded.com.google.common.collect.ImmutableMultimap;
 import org.apache.storm.streams.Pair;
-import org.apache.storm.streams.operations.PairValueJoiner;
 import org.junit.Test;
 
 import java.util.ArrayList;

File: storm-client/src/jvm/org/apache/storm/utils/ObjectReader.java
Patch:
@@ -91,7 +91,7 @@ public static Double getDouble(Object o, Double defaultValue) {
         if (o instanceof Number) {
             return ((Number) o).doubleValue();
         } else {
-            throw new IllegalArgumentException("Don't know how to convert " + o + " + to double");
+            throw new IllegalArgumentException("Don't know how to convert " + o + " to double");
         }
     }
 
@@ -102,7 +102,7 @@ public static boolean getBoolean(Object o, boolean defaultValue) {
         if (o instanceof Boolean) {
             return (Boolean) o;
         } else {
-            throw new IllegalArgumentException("Don't know how to convert " + o + " + to boolean");
+            throw new IllegalArgumentException("Don't know how to convert " + o + " to boolean");
         }
     }
 
@@ -113,7 +113,7 @@ public static String getString(Object o, String defaultValue) {
         if (o instanceof String) {
             return (String) o;
         } else {
-            throw new IllegalArgumentException("Don't know how to convert " + o + " + to String");
+            throw new IllegalArgumentException("Don't know how to convert " + o + " to String");
         }
     }
 }

File: storm-client/src/jvm/org/apache/storm/utils/ObjectReader.java
Patch:
@@ -91,7 +91,7 @@ public static Double getDouble(Object o, Double defaultValue) {
         if (o instanceof Number) {
             return ((Number) o).doubleValue();
         } else {
-            throw new IllegalArgumentException("Don't know how to convert " + o + " + to double");
+            throw new IllegalArgumentException("Don't know how to convert " + o + " to double");
         }
     }
 
@@ -102,7 +102,7 @@ public static boolean getBoolean(Object o, boolean defaultValue) {
         if (o instanceof Boolean) {
             return (Boolean) o;
         } else {
-            throw new IllegalArgumentException("Don't know how to convert " + o + " + to boolean");
+            throw new IllegalArgumentException("Don't know how to convert " + o + " to boolean");
         }
     }
 
@@ -113,7 +113,7 @@ public static String getString(Object o, String defaultValue) {
         if (o instanceof String) {
             return (String) o;
         } else {
-            throw new IllegalArgumentException("Don't know how to convert " + o + " + to String");
+            throw new IllegalArgumentException("Don't know how to convert " + o + " to String");
         }
     }
 }

File: storm-server/src/main/java/org/apache/storm/daemon/supervisor/ReadClusterState.java
Patch:
@@ -64,7 +64,6 @@ public class ReadClusterState implements Runnable, AutoCloseable {
     private final ContainerLauncher launcher;
     private final String host;
     private final LocalState localState;
-    private final IStormClusterState clusterState;
     private final AtomicReference<Map<Long, LocalAssignment>> cachedAssignments;
     
     public ReadClusterState(Supervisor supervisor) throws Exception {
@@ -77,7 +76,6 @@ public ReadClusterState(Supervisor supervisor) throws Exception {
         this.localizer = supervisor.getAsyncLocalizer();
         this.host = supervisor.getHostName();
         this.localState = supervisor.getLocalState();
-        this.clusterState = supervisor.getStormClusterState();
         this.cachedAssignments = supervisor.getCurrAssignment();
         
         this.launcher = ContainerLauncher.make(superConf, assignmentId, supervisor.getSharedContext());
@@ -117,7 +115,7 @@ public ReadClusterState(Supervisor supervisor) throws Exception {
 
     private Slot mkSlot(int port) throws Exception {
         return new Slot(localizer, superConf, launcher, host, port,
-                localState, clusterState, iSuper, cachedAssignments);
+                localState, stormClusterState, iSuper, cachedAssignments);
     }
     
     @Override

File: storm-client/src/jvm/org/apache/storm/daemon/worker/Worker.java
Patch:
@@ -103,13 +103,13 @@ public class Worker implements Shutdownable, DaemonCommon {
 
     /**
      * TODO: should worker even take the topologyId as input? this should be
-     * deducable from cluster state (by searching through assignments)
+     * deducible from cluster state (by searching through assignments)
      * what about if there's inconsistency in assignments? -> but nimbus should guarantee this consistency
      *
      * @param conf         - Storm configuration
      * @param context      -
      * @param topologyId   - topology id
-     * @param assignmentId - assignement id
+     * @param assignmentId - assignment id
      * @param port         - port on which the worker runs
      * @param workerId     - worker id
      */
@@ -491,7 +491,7 @@ private WorkerBackpressureCallback mkBackpressureHandler() {
     }
 
     public static void main(String[] args) throws Exception {
-        Preconditions.checkArgument(args.length == 4, "Illegal number of arguemtns. Expected: 4, Actual: " + args.length);
+        Preconditions.checkArgument(args.length == 4, "Illegal number of arguments. Expected: 4, Actual: " + args.length);
         String stormId = args[0];
         String assignmentId = args[1];
         String portStr = args[2];

File: external/storm-hive/src/main/java/org/apache/storm/hive/bolt/mapper/JsonRecordHiveMapper.java
Patch:
@@ -38,7 +38,7 @@
 import java.io.IOException;
 
 public class JsonRecordHiveMapper implements HiveMapper {
-    private static final Logger LOG = LoggerFactory.getLogger(DelimitedRecordHiveMapper.class);
+    private static final Logger LOG = LoggerFactory.getLogger(JsonRecordHiveMapper.class);
     private Fields columnFields;
     private Fields partitionFields;
     private String timeFormat;

File: storm-client/src/jvm/org/apache/storm/executor/bolt/BoltExecutor.java
Patch:
@@ -58,6 +58,8 @@ public void init(Map<Integer, Task> idToTask) {
             Utils.sleep(100);
         }
 
+        this.errorReportingMetrics.registerAll(topoConf, idToTask.values().iterator().next().getUserContext());
+
         LOG.info("Preparing bolt {}:{}", componentId, idToTask.keySet());
         for (Map.Entry<Integer, Task> entry : idToTask.entrySet()) {
             Task taskData = entry.getValue();

File: storm-client/src/jvm/org/apache/storm/executor/bolt/BoltOutputCollectorImpl.java
Patch:
@@ -151,6 +151,7 @@ public void resetTimeout(Tuple input) {
 
     @Override
     public void reportError(Throwable error) {
+        executor.getErrorReportingMetrics().incrReportedErrorCount();
         executor.getReportError().report(error);
     }
 

File: storm-client/src/jvm/org/apache/storm/executor/spout/SpoutExecutor.java
Patch:
@@ -106,6 +106,7 @@ public void expire(Long key, TupleInfo tupleInfo) {
         });
 
         this.spoutThrottlingMetrics.registerAll(topoConf, idToTask.values().iterator().next().getUserContext());
+        this.errorReportingMetrics.registerAll(topoConf, idToTask.values().iterator().next().getUserContext());
         this.outputCollectors = new ArrayList<>();
         for (Map.Entry<Integer, Task> entry : idToTask.entrySet()) {
             Task taskData = entry.getValue();

File: storm-client/src/jvm/org/apache/storm/executor/spout/SpoutOutputCollectorImpl.java
Patch:
@@ -77,6 +77,7 @@ public long getPendingCount() {
 
     @Override
     public void reportError(Throwable error) {
+        executor.getErrorReportingMetrics().incrReportedErrorCount();
         executor.getReportError().report(error);
     }
 

File: storm-client/src/jvm/org/apache/storm/executor/bolt/BoltExecutor.java
Patch:
@@ -58,6 +58,8 @@ public void init(Map<Integer, Task> idToTask) {
             Utils.sleep(100);
         }
 
+        this.errorReportingMetrics.registerAll(topoConf, idToTask.values().iterator().next().getUserContext());
+
         LOG.info("Preparing bolt {}:{}", componentId, idToTask.keySet());
         for (Map.Entry<Integer, Task> entry : idToTask.entrySet()) {
             Task taskData = entry.getValue();

File: storm-client/src/jvm/org/apache/storm/executor/bolt/BoltOutputCollectorImpl.java
Patch:
@@ -151,6 +151,7 @@ public void resetTimeout(Tuple input) {
 
     @Override
     public void reportError(Throwable error) {
+        executor.getErrorReportingMetrics().incrReportedErrorCount();
         executor.getReportError().report(error);
     }
 

File: storm-client/src/jvm/org/apache/storm/executor/spout/SpoutExecutor.java
Patch:
@@ -106,6 +106,7 @@ public void expire(Long key, TupleInfo tupleInfo) {
         });
 
         this.spoutThrottlingMetrics.registerAll(topoConf, idToTask.values().iterator().next().getUserContext());
+        this.errorReportingMetrics.registerAll(topoConf, idToTask.values().iterator().next().getUserContext());
         this.outputCollectors = new ArrayList<>();
         for (Map.Entry<Integer, Task> entry : idToTask.entrySet()) {
             Task taskData = entry.getValue();

File: storm-client/src/jvm/org/apache/storm/executor/spout/SpoutOutputCollectorImpl.java
Patch:
@@ -77,6 +77,7 @@ public long getPendingCount() {
 
     @Override
     public void reportError(Throwable error) {
+        executor.getErrorReportingMetrics().incrReportedErrorCount();
         executor.getReportError().report(error);
     }
 

File: external/storm-hive/src/main/java/org/apache/storm/hive/bolt/mapper/JsonRecordHiveMapper.java
Patch:
@@ -38,7 +38,7 @@
 import java.io.IOException;
 
 public class JsonRecordHiveMapper implements HiveMapper {
-    private static final Logger LOG = LoggerFactory.getLogger(DelimitedRecordHiveMapper.class);
+    private static final Logger LOG = LoggerFactory.getLogger(JsonRecordHiveMapper.class);
     private Fields columnFields;
     private Fields partitionFields;
     private String timeFormat;

File: storm-client/src/jvm/org/apache/storm/security/auth/DefaultPrincipalToLocal.java
Patch:
@@ -36,7 +36,8 @@ public void prepare(Map<String, Object> topoConf) {}
      * @param principal the principal to convert
      * @return The local user name.
      */
-    public String toLocal(Principal principal) {
-      return principal == null ? null : principal.getName();
+    @Override
+    public String toLocal(String principal) {
+      return principal;
     }
 }

File: storm-client/src/jvm/org/apache/storm/security/auth/KerberosPrincipalToLocal.java
Patch:
@@ -37,9 +37,10 @@ public void prepare(Map<String, Object> topoConf) {}
      * @param principal the principal to convert
      * @return The local user name.
      */
-    public String toLocal(Principal principal) {
+    @Override
+    public String toLocal(String principal) {
       //This technically does not conform with rfc1964, but should work so
       // long as you don't have any really odd names in your KDC.
-      return principal == null ? null : principal.getName().split("[/@]")[0];
+      return principal == null ? null : principal.split("[/@]")[0];
     }
 }

File: storm-client/test/jvm/org/apache/storm/security/auth/AuthUtilsTestMock.java
Patch:
@@ -57,7 +57,7 @@ public String getUserName(HttpServletRequest req){
 
     // IPrincipalToLocal 
     @Override
-    public String toLocal(Principal principal) {
+    public String toLocal(String principal) {
         return null;
     }
 

File: integration-test/src/test/java/org/apache/storm/st/wrapper/TopoWrap.java
Patch:
@@ -261,8 +261,8 @@ public int hashCode() {
 
         public ExecutorURL(String componentId, String host, int logViewerPort, int executorPort, String topoId) throws MalformedURLException {
             String sep = "%2F"; //hex of "/"
-            String viewUrlStr = String.format("http://%s:%s/log?file=", host, logViewerPort);
-            String downloadUrlStr = String.format("http://%s:%s/download?file=%%2F", host, logViewerPort);
+            String viewUrlStr = String.format("http://%s:%s/api/v1/log?file=", host, logViewerPort);
+            String downloadUrlStr = String.format("http://%s:%s/api/v1/download?file=%%2F", host, logViewerPort);
             viewUrl = new URL(String.format("%s/%s%s%d%sworker.log", viewUrlStr, topoId, sep, executorPort, sep));
             downloadUrl = new URL(String.format("%s/%s%s%d%sworker.log", downloadUrlStr, topoId, sep, executorPort, sep));
             this.componentId = componentId;

File: storm-client/src/jvm/org/apache/storm/daemon/StormCommon.java
Patch:
@@ -52,10 +52,8 @@
 import org.apache.storm.task.WorkerTopologyContext;
 import org.apache.storm.tuple.Fields;
 import org.apache.storm.utils.ConfigUtils;
-import org.apache.storm.utils.ThriftTopologyUtils;
 import org.apache.storm.utils.Utils;
 import org.apache.storm.utils.ObjectReader;
-import org.apache.storm.utils.ThriftTopologyUtils;
 import org.json.simple.JSONValue;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;

File: storm-server/src/main/java/org/apache/storm/daemon/metrics/reporters/JmxPreparableReporter.java
Patch:
@@ -19,7 +19,6 @@
 
 import com.codahale.metrics.JmxReporter;
 import com.codahale.metrics.MetricRegistry;
-import org.apache.storm.Config;
 import org.apache.storm.DaemonConfig;
 import org.apache.storm.daemon.metrics.MetricsUtils;
 import org.apache.storm.utils.ObjectReader;

File: storm-server/src/main/java/org/apache/storm/daemon/supervisor/BasicContainer.java
Patch:
@@ -22,7 +22,6 @@
 import java.io.File;
 import java.io.FilenameFilter;
 import java.io.IOException;
-import java.nio.file.Paths;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collections;

File: storm-server/src/main/java/org/apache/storm/utils/ServerConfigUtils.java
Patch:
@@ -30,6 +30,7 @@
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
+import java.util.regex.Pattern;
 
 public class ServerConfigUtils {
     public static final String FILE_SEPARATOR = File.separator;
@@ -161,7 +162,7 @@ public static String absoluteHealthCheckDir(Map<String, Object> conf) {
     }
 
     public static File getLogMetaDataFile(String fname) {
-        String[] subStrings = fname.split(FILE_SEPARATOR); // TODO: does this work well on windows?
+        String[] subStrings = fname.split(Pattern.quote(FILE_SEPARATOR)); // TODO: does this work well on windows?
         String id = subStrings[0];
         Integer port = Integer.parseInt(subStrings[1]);
         return getLogMetaDataFile(Utils.readStormConfig(), id, port);

File: storm-webapp/src/main/java/org/apache/storm/daemon/common/AuthorizationExceptionMapper.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.storm.daemon.drpc.webapp;
+package org.apache.storm.daemon.common;
 
 import java.util.HashMap;
 import java.util.Map;

File: storm-webapp/src/main/java/org/apache/storm/daemon/drpc/webapp/DRPCApplication.java
Patch:
@@ -24,6 +24,7 @@
 import javax.ws.rs.ApplicationPath;
 import javax.ws.rs.core.Application;
 
+import org.apache.storm.daemon.common.AuthorizationExceptionMapper;
 import org.apache.storm.daemon.drpc.DRPC;
 
 @ApplicationPath("")

File: storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/handler/LogviewerLogPageHandler.java
Patch:
@@ -176,7 +176,7 @@ public Response listLogFiles(String user, Integer port, String topologyId, Strin
     public Response logPage(String fileName, Integer start, Integer length, String grep, String user)
             throws IOException, InvalidRequestException {
         String rootDir = logRoot;
-        if (resourceAuthorizer.isUserAllowedToAccessFile(fileName, user)) {
+        if (resourceAuthorizer.isUserAllowedToAccessFile(user, fileName)) {
             workerLogs.setLogFilePermission(fileName);
 
             File file = new File(rootDir, fileName).getCanonicalFile();

File: storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/handler/LogviewerLogSearchHandler.java
Patch:
@@ -119,7 +119,7 @@ public Response searchLogFile(String fileName, String user, boolean isDaemon, St
         File file = new File(rootDir, fileName).getCanonicalFile();
         Response response;
         if (file.exists()) {
-            if (isDaemon || resourceAuthorizer.isUserAllowedToAccessFile(fileName, user)) {
+            if (isDaemon || resourceAuthorizer.isUserAllowedToAccessFile(user, fileName)) {
                 Integer numMatchesInt = numMatchesStr != null ? tryParseIntParam("num-matches", numMatchesStr) : null;
                 Integer offsetInt = offsetStr != null ? tryParseIntParam("start-byte-offset", offsetStr) : null;
 

File: storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/utils/LogFileDownloader.java
Patch:
@@ -54,7 +54,7 @@ public Response downloadFile(String fileName, String user, boolean isDaemon) thr
         String rootDir = isDaemon ? daemonLogRoot : logRoot;
         File file = new File(rootDir, fileName).getCanonicalFile();
         if (file.exists()) {
-            if (isDaemon || resourceAuthorizer.isUserAllowedToAccessFile(fileName, user)) {
+            if (isDaemon || resourceAuthorizer.isUserAllowedToAccessFile(user, fileName)) {
                 return LogviewerResponseBuilder.buildDownloadFile(file);
             } else {
                 return LogviewerResponseBuilder.buildResponseUnautohrizedUser(user);

File: storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/utils/ResourceAuthorizer.java
Patch:
@@ -61,7 +61,7 @@ public ResourceAuthorizer(Map<String, Object> stormConf) {
      * @param fileName file name to access
      * @param user username
      */
-    public boolean isUserAllowedToAccessFile(String fileName, String user) {
+    public boolean isUserAllowedToAccessFile(String user, String fileName) {
         return isUiFilterNotSet() || isAuthorizedLogUser(user, fileName);
     }
 

File: storm-server/src/main/java/org/apache/storm/utils/ServerConfigUtils.java
Patch:
@@ -30,6 +30,7 @@
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
+import java.util.regex.Pattern;
 
 public class ServerConfigUtils {
     public static final String FILE_SEPARATOR = File.separator;
@@ -161,7 +162,7 @@ public static String absoluteHealthCheckDir(Map<String, Object> conf) {
     }
 
     public static File getLogMetaDataFile(String fname) {
-        String[] subStrings = fname.split(FILE_SEPARATOR); // TODO: does this work well on windows?
+        String[] subStrings = fname.split(Pattern.quote(FILE_SEPARATOR)); // TODO: does this work well on windows?
         String id = subStrings[0];
         Integer port = Integer.parseInt(subStrings[1]);
         return getLogMetaDataFile(Utils.readStormConfig(), id, port);

File: storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/handler/LogviewerLogSearchHandler.java
Patch:
@@ -119,7 +119,7 @@ public Response searchLogFile(String fileName, String user, boolean isDaemon, St
         File file = new File(rootDir, fileName).getCanonicalFile();
         Response response;
         if (file.exists()) {
-            if (isDaemon || resourceAuthorizer.isUserAllowedToAccessFile(user, fileName)) {
+            if (isDaemon || resourceAuthorizer.isUserAllowedToAccessFile(fileName, user)) {
                 Integer numMatchesInt = numMatchesStr != null ? tryParseIntParam("num-matches", numMatchesStr) : null;
                 Integer offsetInt = offsetStr != null ? tryParseIntParam("start-byte-offset", offsetStr) : null;
 

File: storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/webapp/LogviewerResource.java
Patch:
@@ -214,10 +214,11 @@ public Response downloadDaemonLogFile(@Context HttpServletRequest request) throw
      * Handles '/search' (searching from specific worker or daemon log file) request.
      */
     @GET
-    @Path("/search/{file}")
-    public Response search(@PathParam("file") String file, @Context HttpServletRequest request) throws IOException {
+    @Path("/search")
+    public Response search(@Context HttpServletRequest request) throws IOException {
         String user = httpCredsHandler.getUserName(request);
         boolean isDaemon = StringUtils.equals(request.getParameter("is-daemon"), "yes");
+        String file = request.getParameter("file");
         String decodedFileName = URLDecoder.decode(file);
         String searchString = request.getParameter("search-string");
         String numMatchesStr = request.getParameter("num-matches");

File: integration-test/src/test/java/org/apache/storm/st/wrapper/TopoWrap.java
Patch:
@@ -261,8 +261,8 @@ public int hashCode() {
 
         public ExecutorURL(String componentId, String host, int logViewerPort, int executorPort, String topoId) throws MalformedURLException {
             String sep = "%2F"; //hex of "/"
-            String viewUrlStr = String.format("http://%s:%s/log?file=", host, logViewerPort);
-            String downloadUrlStr = String.format("http://%s:%s/download?file=%%2F", host, logViewerPort);
+            String viewUrlStr = String.format("http://%s:%s/api/v1/log?file=", host, logViewerPort);
+            String downloadUrlStr = String.format("http://%s:%s/api/v1/download?file=%%2F", host, logViewerPort);
             viewUrl = new URL(String.format("%s/%s%s%d%sworker.log", viewUrlStr, topoId, sep, executorPort, sep));
             downloadUrl = new URL(String.format("%s/%s%s%d%sworker.log", downloadUrlStr, topoId, sep, executorPort, sep));
             this.componentId = componentId;

File: storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/handler/LogviewerLogPageHandler.java
Patch:
@@ -132,7 +132,7 @@ public Response listLogFiles(String user, Integer port, String topologyId, Strin
             if (port == null) {
                 fileResults = new ArrayList<>();
 
-                File topoDir = new File(logRoot + Utils.FILE_PATH_SEPARATOR + topologyId);
+                File topoDir = new File(logRoot, topologyId);
                 if (topoDir.exists()) {
                     File[] topoDirFiles = topoDir.listFiles();
                     if (topoDirFiles != null) {

File: storm-client/src/jvm/org/apache/storm/Config.java
Patch:
@@ -804,7 +804,7 @@ public class Config extends HashMap<String, Object> {
     public static final String PACEMAKER_AUTH_METHOD = "pacemaker.auth.method";
 
     /**
-     * Pacemaker Thrift Max Message Size (bytes)
+     * Pacemaker Thrift Max Message Size (bytes).
      */
     @isInteger
     @isPositiveNumber

File: storm-client/src/jvm/org/apache/storm/pacemaker/codec/ThriftNettyClientCodec.java
Patch:
@@ -48,22 +48,22 @@ public enum AuthMethod {
     private AuthMethod authMethod;
     private Map<String, Object> topoConf;
     private String host;
-    private int thriftMessageMaxSize;
+    private final int _thriftMessageMaxSize;
 
     public ThriftNettyClientCodec(PacemakerClient pacemaker_client, Map<String, Object> topoConf, AuthMethod authMethod, String host, int thriftMessageMaxSize) {
         client = pacemaker_client;
         this.authMethod = authMethod;
         this.topoConf = topoConf;
         this.host = host;
-        this.thriftMessageMaxSize = thriftMessageMaxSize;
+        _thriftMessageMaxSize = thriftMessageMaxSize;
     }
 
     public ChannelPipelineFactory pipelineFactory() {
         return new ChannelPipelineFactory() {
             public ChannelPipeline getPipeline() {
                 ChannelPipeline pipeline = Channels.pipeline();
                 pipeline.addLast("encoder", new ThriftEncoder());
-                pipeline.addLast("decoder", new ThriftDecoder(thriftMessageMaxSize));
+                pipeline.addLast("decoder", new ThriftDecoder(_thriftMessageMaxSize));
 
                 if (authMethod == AuthMethod.KERBEROS) {
                     try {

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/Subscription.java
Patch:
@@ -37,7 +37,7 @@ public abstract class Subscription implements Serializable {
     public abstract <K, V> void subscribe(KafkaConsumer<K,V> consumer, ConsumerRebalanceListener listener, TopologyContext context);
     
     /**
-     * @return a string representing the subscribed topics.
+     * @return A human-readable string representing the subscribed topics.
      */
     public abstract String getTopicsString();
     

File: storm-server/src/main/java/org/apache/storm/pacemaker/codec/ThriftNettyServerCodec.java
Patch:
@@ -47,6 +47,7 @@ public enum AuthMethod {
     private IServer server;
     private AuthMethod authMethod;
     private Map<String, Object> topoConf;
+    private int thriftMessageMaxSize;
     
     private static final Logger LOG = LoggerFactory
         .getLogger(ThriftNettyServerCodec.class);
@@ -55,6 +56,7 @@ public ThriftNettyServerCodec(IServer server, Map<String, Object> topoConf, Auth
         this.server = server;
         this.authMethod = authMethod;
         this.topoConf = topoConf;
+        this.thriftMessageMaxSize = thriftMessageMaxSize;
     }
 
     public ChannelPipelineFactory pipelineFactory() {

File: storm-webapp/src/main/java/org/apache/storm/daemon/drpc/webapp/DRPCApplication.java
Patch:
@@ -24,8 +24,8 @@
 import javax.ws.rs.ApplicationPath;
 import javax.ws.rs.core.Application;
 
-import org.apache.storm.daemon.drpc.DRPC;
 import org.apache.storm.daemon.common.AuthorizationExceptionMapper;
+import org.apache.storm.daemon.drpc.DRPC;
 
 @ApplicationPath("")
 public class DRPCApplication extends Application {

File: storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/LogviewerConstant.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.storm.daemon.wip.logviewer;
+package org.apache.storm.daemon.logviewer;
 
 public class LogviewerConstant {
     public static final int DEFAULT_BYTES_PER_PAGE = 51200;

File: storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/handler/LogviewerLogDownloadHandler.java
Patch:
@@ -16,10 +16,10 @@
  * limitations under the License.
  */
 
-package org.apache.storm.daemon.wip.logviewer.handler;
+package org.apache.storm.daemon.logviewer.handler;
 
-import org.apache.storm.daemon.wip.logviewer.utils.LogFileDownloader;
-import org.apache.storm.daemon.wip.logviewer.utils.ResourceAuthorizer;
+import org.apache.storm.daemon.logviewer.utils.LogFileDownloader;
+import org.apache.storm.daemon.logviewer.utils.ResourceAuthorizer;
 
 import javax.ws.rs.core.Response;
 import java.io.IOException;

File: storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/utils/LogFileDownloader.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.storm.daemon.wip.logviewer.utils;
+package org.apache.storm.daemon.logviewer.utils;
 
 import javax.ws.rs.core.Response;
 import java.io.File;

File: storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/utils/LogviewerResponseBuilder.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.storm.daemon.wip.logviewer.utils;
+package org.apache.storm.daemon.logviewer.utils;
 
 import com.google.common.io.ByteStreams;
 import org.apache.storm.daemon.common.JsonResponseBuilder;

File: storm-client/src/jvm/org/apache/storm/daemon/StormCommon.java
Patch:
@@ -52,10 +52,8 @@
 import org.apache.storm.task.WorkerTopologyContext;
 import org.apache.storm.tuple.Fields;
 import org.apache.storm.utils.ConfigUtils;
-import org.apache.storm.utils.ThriftTopologyUtils;
 import org.apache.storm.utils.Utils;
 import org.apache.storm.utils.ObjectReader;
-import org.apache.storm.utils.ThriftTopologyUtils;
 import org.json.simple.JSONValue;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;

File: storm-server/src/main/java/org/apache/storm/daemon/metrics/reporters/JmxPreparableReporter.java
Patch:
@@ -19,7 +19,6 @@
 
 import com.codahale.metrics.JmxReporter;
 import com.codahale.metrics.MetricRegistry;
-import org.apache.storm.Config;
 import org.apache.storm.DaemonConfig;
 import org.apache.storm.daemon.metrics.MetricsUtils;
 import org.apache.storm.utils.ObjectReader;

File: storm-server/src/main/java/org/apache/storm/daemon/supervisor/BasicContainer.java
Patch:
@@ -22,7 +22,6 @@
 import java.io.File;
 import java.io.FilenameFilter;
 import java.io.IOException;
-import java.nio.file.Paths;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collections;

File: storm-webapp/src/main/java/org/apache/storm/daemon/common/AuthorizationExceptionMapper.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.storm.daemon.drpc.webapp;
+package org.apache.storm.daemon.common;
 
 import java.util.HashMap;
 import java.util.Map;

File: storm-webapp/src/main/java/org/apache/storm/daemon/drpc/webapp/DRPCApplication.java
Patch:
@@ -25,6 +25,7 @@
 import javax.ws.rs.core.Application;
 
 import org.apache.storm.daemon.drpc.DRPC;
+import org.apache.storm.daemon.common.AuthorizationExceptionMapper;
 
 @ApplicationPath("")
 public class DRPCApplication extends Application {

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/AbstractExecutionResultHandler.java
Patch:
@@ -16,6 +16,7 @@
  * specific language governing permissions and limitations
  * under the License.
  */
+
 package org.apache.storm.cassandra;
 
 import org.apache.storm.task.OutputCollector;

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/BaseExecutionResultHandler.java
Patch:
@@ -16,6 +16,7 @@
  * specific language governing permissions and limitations
  * under the License.
  */
+
 package org.apache.storm.cassandra;
 
 import org.apache.storm.task.OutputCollector;

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/CassandraContext.java
Patch:
@@ -16,6 +16,7 @@
  * specific language governing permissions and limitations
  * under the License.
  */
+
 package org.apache.storm.cassandra;
 
 import com.datastax.driver.core.Cluster;

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/DynamicStatementBuilder.java
Patch:
@@ -16,6 +16,7 @@
  * specific language governing permissions and limitations
  * under the License.
  */
+
 package org.apache.storm.cassandra;
 
 import com.datastax.driver.core.BatchStatement;

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/ExecutionResultHandler.java
Patch:
@@ -16,6 +16,7 @@
  * specific language governing permissions and limitations
  * under the License.
  */
+
 package org.apache.storm.cassandra;
 
 import org.apache.storm.task.OutputCollector;

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/Murmur3StreamGrouping.java
Patch:
@@ -16,6 +16,7 @@
  * specific language governing permissions and limitations
  * under the License.
  */
+
 package org.apache.storm.cassandra;
 
 import org.apache.storm.generated.GlobalStreamId;

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/AbstractExecutionResultHandler.java
Patch:
@@ -16,6 +16,7 @@
  * specific language governing permissions and limitations
  * under the License.
  */
+
 package org.apache.storm.cassandra;
 
 import org.apache.storm.task.OutputCollector;

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/BaseExecutionResultHandler.java
Patch:
@@ -16,6 +16,7 @@
  * specific language governing permissions and limitations
  * under the License.
  */
+
 package org.apache.storm.cassandra;
 
 import org.apache.storm.task.OutputCollector;

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/CassandraContext.java
Patch:
@@ -16,6 +16,7 @@
  * specific language governing permissions and limitations
  * under the License.
  */
+
 package org.apache.storm.cassandra;
 
 import com.datastax.driver.core.Cluster;

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/DynamicStatementBuilder.java
Patch:
@@ -16,6 +16,7 @@
  * specific language governing permissions and limitations
  * under the License.
  */
+
 package org.apache.storm.cassandra;
 
 import com.datastax.driver.core.BatchStatement;

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/ExecutionResultHandler.java
Patch:
@@ -16,6 +16,7 @@
  * specific language governing permissions and limitations
  * under the License.
  */
+
 package org.apache.storm.cassandra;
 
 import org.apache.storm.task.OutputCollector;

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/Murmur3StreamGrouping.java
Patch:
@@ -16,6 +16,7 @@
  * specific language governing permissions and limitations
  * under the License.
  */
+
 package org.apache.storm.cassandra;
 
 import org.apache.storm.generated.GlobalStreamId;

File: examples/storm-starter/src/jvm/org/apache/storm/starter/ThroughputVsLatency.java
Patch:
@@ -371,6 +371,7 @@ public void handle(TaskInfo taskInfo, Collection<DataPoint> dataPoints) {
             }
         } finally {
             kill(client.getClient(), name);
+            System.exit(0);
         }
     }
 }

File: examples/storm-starter/src/jvm/org/apache/storm/starter/ThroughputVsLatency.java
Patch:
@@ -371,6 +371,7 @@ public void handle(TaskInfo taskInfo, Collection<DataPoint> dataPoints) {
             }
         } finally {
             kill(client.getClient(), name);
+            System.exit(0);
         }
     }
 }

File: external/storm-redis/src/main/java/org/apache/storm/redis/common/config/JedisPoolConfig.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.redis.common.config;
 
 import redis.clients.jedis.Protocol;
@@ -36,7 +37,7 @@ public class JedisPoolConfig implements Serializable {
     public JedisPoolConfig() {
     }
     /**
-     * Constructor
+     * Constructor.
      * <p/>
      * You can use JedisPoolConfig.Builder() for leaving some fields to apply default value.
      *

File: external/storm-redis/src/main/java/org/apache/storm/redis/common/container/JedisClusterContainer.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.redis.common.container;
 
 import redis.clients.jedis.JedisCluster;

File: external/storm-redis/src/main/java/org/apache/storm/redis/common/container/JedisCommandsContainerBuilder.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.redis.common.container;
 
 import org.apache.storm.redis.common.config.JedisClusterConfig;

File: external/storm-redis/src/main/java/org/apache/storm/redis/common/container/JedisCommandsInstanceContainer.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.redis.common.container;
 
 import redis.clients.jedis.JedisCommands;

File: external/storm-redis/src/main/java/org/apache/storm/redis/common/container/JedisContainer.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.redis.common.container;
 
 import org.slf4j.Logger;

File: external/storm-redis/src/main/java/org/apache/storm/redis/trident/state/RedisClusterState.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.redis.trident.state;
 
 import org.apache.storm.task.IMetricsContext;

File: external/storm-redis/src/test/java/org/apache/storm/redis/state/DefaultStateSerializerTest.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.redis.state;
 
 import org.apache.storm.spout.CheckPointState;

File: storm-server/src/main/java/org/apache/storm/daemon/supervisor/BasicContainer.java
Patch:
@@ -34,7 +34,6 @@
 import java.util.Map;
 import java.util.Map.Entry;
 import java.util.NavigableMap;
-import java.util.stream.Collectors;
 
 import org.apache.commons.lang.StringUtils;
 import org.apache.storm.Config;
@@ -344,7 +343,7 @@ protected String javaLibraryPath(String stormRoot, Map<String, Object> conf) {
      * @return the path with wildcard ("*") suffix
      */
     protected String getWildcardDir(File dir) {
-        return Paths.get(dir.toString(), "*").toString();
+        return dir.toString() + File.separator + "*";
     }
     
     protected List<String> frameworkClasspath(SimpleVersion topoVersion) {

File: storm-client/src/jvm/org/apache/storm/security/serialization/BlowfishTupleSerializer.java
Patch:
@@ -80,13 +80,13 @@ public ListDelegate read(Kryo kryo, Input input, Class<ListDelegate> type) {
     public static void main(String[] args) {
         try{
             KeyGenerator kgen = KeyGenerator.getInstance("Blowfish");
+            kgen.init(256);
             SecretKey skey = kgen.generateKey();
             byte[] raw = skey.getEncoded();
             String keyString = new String(Hex.encodeHex(raw));
             System.out.println("storm -c "+SECRET_KEY+"="+keyString+" -c "+Config.TOPOLOGY_TUPLE_SERIALIZER+"="+BlowfishTupleSerializer.class.getName() + " ..." );
         } catch (Exception ex) {
             LOG.error(ex.getMessage());
-            ex.printStackTrace();
         }
     }    
 }

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/trident/KafkaTridentSpoutOpaque.java
Patch:
@@ -46,7 +46,8 @@ public KafkaTridentSpoutOpaque(KafkaTridentSpoutManager<K, V> kafkaManager) {
     }
 
     @Override
-    public Emitter<List<TopicPartition>, KafkaTridentSpoutTopicPartition, KafkaTridentSpoutBatchMetadata<K,V>> getEmitter(Map<String, Object> conf, TopologyContext context) {
+    public Emitter<List<TopicPartition>, KafkaTridentSpoutTopicPartition, KafkaTridentSpoutBatchMetadata<K,V>> getEmitter(
+            Map<String, Object> conf, TopologyContext context) {
         return new KafkaTridentSpoutEmitter<>(kafkaManager, context);
     }
 

File: storm-webapp/src/main/java/org/apache/storm/daemon/drpc/DRPCServer.java
Patch:
@@ -132,7 +132,7 @@ private static Server mkHttpServer(Map<String, Object> conf, DRPC drpc) {
 
     /**
      * Constructor.
-     * @param conf Drpc conf dor the servers
+     * @param conf Drpc conf for the servers
      */
     public DRPCServer(Map<String, Object> conf) {
         drpc = new DRPC(conf);

File: storm-webapp/src/main/java/org/apache/storm/daemon/drpc/webapp/DRPCExceptionMapper.java
Patch:
@@ -30,7 +30,7 @@
 import org.json.simple.JSONValue;
 
 @Provider
-public class DrpcExceptionMapper implements ExceptionMapper<DRPCExecutionException> {
+public class DRPCExceptionMapper implements ExceptionMapper<DRPCExecutionException> {
 
     @Override
     public Response toResponse(DRPCExecutionException ex) {

File: storm-webapp/src/main/java/org/apache/storm/daemon/drpc/webapp/DRPCResource.java
Patch:
@@ -32,11 +32,11 @@
 import org.apache.thrift.TException;
 
 @Path("/drpc/")
-public class DrpcResource {
+public class DRPCResource {
     private static final Meter meterHttpRequests = StormMetricsRegistry.registerMeter("drpc:num-execute-http-requests");
     private final DRPC drpc;
 
-    public DrpcResource(DRPC drpc) {
+    public DRPCResource(DRPC drpc) {
         this.drpc = drpc;
     }
     

File: storm-client/src/jvm/org/apache/storm/topology/WindowedBoltExecutor.java
Patch:
@@ -273,7 +273,7 @@ public void prepare(Map<String, Object> topoConf, TopologyContext context, Outpu
         this.listener = newWindowLifecycleListener();
         this.windowManager = initWindowManager(listener, topoConf, context);
         start();
-        LOG.debug("Initialized window manager {} ", this.windowManager);
+        LOG.info("Initialized window manager {} ", windowManager);
     }
 
     @Override

File: storm-client/src/jvm/org/apache/storm/windowing/TimeTriggerPolicy.java
Patch:
@@ -109,12 +109,14 @@ private Runnable newTriggerTask() {
         return new Runnable() {
             @Override
             public void run() {
+                // do not process current timestamp since tuples might arrive while the trigger is executing
+                long now = System.currentTimeMillis() - 1;
                 try {
                     /*
                      * set the current timestamp as the reference time for the eviction policy
                      * to evict the events
                      */
-                    evictionPolicy.setContext(new DefaultEvictionContext(System.currentTimeMillis()));
+                    evictionPolicy.setContext(new DefaultEvictionContext(now, null, null, duration));
                     handler.onTrigger();
                 } catch (Throwable th) {
                     LOG.error("handler.onTrigger failed ", th);

File: integration-test/src/main/java/org/apache/storm/st/topology/window/SlidingTimeCorrectness.java
Patch:
@@ -114,7 +114,9 @@ public void open(Map<String, Object> conf, TopologyContext context, SpoutOutputC
 
         @Override
         public void nextTuple() {
-            TimeUtil.sleepMilliSec(rng.nextInt(800));
+            //Emitting too quickly can lead to spurious test failures because the worker log may roll right before we read it
+            //Sleep a bit between emits
+            TimeUtil.sleepMilliSec(rng.nextInt(100));
             currentNum++;
             TimeData data = TimeData.newData(currentNum);
             final Values tuple = data.getValues();

File: integration-test/src/main/java/org/apache/storm/st/topology/window/SlidingWindowCorrectness.java
Patch:
@@ -39,7 +39,6 @@
 import java.util.List;
 import java.util.Map;
 import java.util.Random;
-import java.util.concurrent.TimeUnit;
 
 /**
  * Computes sliding window sum
@@ -74,7 +73,6 @@ public StormTopology newTopology() {
         builder.setSpout(getSpoutName(), new IncrementingSpout(), 1);
         builder.setBolt(getBoltName(),
                 new VerificationBolt()
-                        .withLag(new BaseWindowedBolt.Duration(10, TimeUnit.SECONDS))
                         .withWindow(new BaseWindowedBolt.Count(windowSize), new BaseWindowedBolt.Count(slideSize)),
                 1)
                 .shuffleGrouping(getSpoutName());

File: integration-test/src/main/java/org/apache/storm/st/topology/window/TumblingTimeCorrectness.java
Patch:
@@ -111,7 +111,9 @@ public void open(Map<String, Object> conf, TopologyContext context, SpoutOutputC
 
         @Override
         public void nextTuple() {
-            TimeUtil.sleepMilliSec(rng.nextInt(800));
+            //Emitting too quickly can lead to spurious test failures because the worker log may roll right before we read it
+            //Sleep a bit between emits
+            TimeUtil.sleepMilliSec(rng.nextInt(100));
             currentNum++;
             TimeData data = TimeData.newData(currentNum);
             final Values tuple = data.getValues();

File: integration-test/src/main/java/org/apache/storm/st/topology/window/TumblingWindowCorrectness.java
Patch:
@@ -39,7 +39,6 @@
 import java.util.List;
 import java.util.Map;
 import java.util.Random;
-import java.util.concurrent.TimeUnit;
 
 /**
  * Computes sliding window sum
@@ -72,7 +71,6 @@ public StormTopology newTopology() {
         builder.setSpout(getSpoutName(), new IncrementingSpout(), 1);
         builder.setBolt(getBoltName(),
                 new VerificationBolt()
-                        .withLag(new BaseWindowedBolt.Duration(10, TimeUnit.SECONDS))
                         .withTumblingWindow(new BaseWindowedBolt.Count(tumbleSize)), 1)
                 .shuffleGrouping(getSpoutName());
         return builder.createTopology();

File: integration-test/src/main/java/org/apache/storm/st/utils/StringDecorator.java
Patch:
@@ -17,6 +17,7 @@
 
 package org.apache.storm.st.utils;
 
+import java.nio.charset.StandardCharsets;
 import org.apache.commons.lang.StringUtils;
 
 public class StringDecorator {

File: storm-client/src/jvm/org/apache/storm/windowing/EvictionContext.java
Patch:
@@ -38,7 +38,7 @@ public interface EvictionContext {
     Long getSlidingCount();
 
     /**
-     * Returns the current count of events in the queue up to the reference tim
+     * Returns the current count of events in the queue up to the reference time
      * based on which count based evictions can be performed.
      *
      * @return the current count

File: storm-client/src/jvm/org/apache/storm/windowing/EvictionPolicy.java
Patch:
@@ -27,7 +27,7 @@ public interface EvictionPolicy<T> {
     /**
      * The action to be taken when {@link EvictionPolicy#evict(Event)} is invoked.
      */
-    enum Action {
+    public enum Action {
         /**
          * expire the event and remove it from the queue
          */

File: storm-client/src/jvm/org/apache/storm/windowing/TimeEvictionPolicy.java
Patch:
@@ -38,7 +38,7 @@ public TimeEvictionPolicy(int windowLength) {
      * {@inheritDoc}
      */
     @Override
-    public Action evict(Event<T> event) {
+    public Action evict(Event<T> event) {      
         long now = evictionContext == null ? System.currentTimeMillis() : evictionContext.getReferenceTime();
         long diff = now - event.getTimestamp();
         if (diff >= windowLength) {

File: storm-client/src/jvm/org/apache/storm/windowing/TimeTriggerPolicy.java
Patch:
@@ -114,9 +114,7 @@ public void run() {
                      * set the current timestamp as the reference time for the eviction policy
                      * to evict the events
                      */
-                    if (evictionPolicy != null) {
-                        evictionPolicy.setContext(new DefaultEvictionContext(System.currentTimeMillis()));
-                    }
+                    evictionPolicy.setContext(new DefaultEvictionContext(System.currentTimeMillis()));
                     handler.onTrigger();
                 } catch (Throwable th) {
                     LOG.error("handler.onTrigger failed ", th);

File: storm-client/src/jvm/org/apache/storm/windowing/WaterMarkEventGenerator.java
Patch:
@@ -77,7 +77,7 @@ public void run() {
         try {
             long waterMarkTs = computeWaterMarkTs();
             if (waterMarkTs > lastWaterMarkTs) {
-                this.windowManager.add(new WaterMarkEvent<T>(waterMarkTs));
+                this.windowManager.add(new WaterMarkEvent<>(waterMarkTs));
                 lastWaterMarkTs = waterMarkTs;
             }
         } catch (Throwable th) {

File: storm-client/src/jvm/org/apache/storm/windowing/WindowManager.java
Patch:
@@ -46,6 +46,9 @@ public class WindowManager<T> implements TriggerHandler {
     /**
      * Expire old events every EXPIRE_EVENTS_THRESHOLD to
      * keep the window size in check.
+     * 
+     * Note that if the eviction policy is based on watermarks, events will not be evicted until a new
+     * watermark would cause them to be considered expired anyway, regardless of this limit
      */
     public static final int EXPIRE_EVENTS_THRESHOLD = 100;
 

File: storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java
Patch:
@@ -3524,7 +3524,7 @@ public TopologyPageInfo getTopologyPageInfo(String topoId, String window, boolea
             topoPageInfo.set_name(topoName);
             topoPageInfo.set_status(extractStatusStr(base));
             topoPageInfo.set_uptime_secs(Time.deltaSecs(launchTimeSecs));
-            topoPageInfo.set_topoConf(JSONValue.toJSONString(topoConf));
+            topoPageInfo.set_topology_conf(JSONValue.toJSONString(topoConf));
             topoPageInfo.set_replication_count(getBlobReplicationCount(ConfigUtils.masterStormCodeKey(topoId)));
             if (base.is_set_component_debug()) {
                 DebugOptions debug = base.get_component_debug().get(topoId);

File: examples/storm-starter/src/jvm/org/apache/storm/starter/TransactionalWords.java
Patch:
@@ -125,7 +125,7 @@ public void finishBatch() {
             for (String key : _counts.keySet()) {
                 CountValue val = COUNT_DATABASE.get(key);
                 CountValue newVal;
-                if (val == null || !val.txid.equals(_id)) {
+                if (val == null || !val.txid.equals(_id.getTransactionId())) {
                     newVal = new CountValue();
                     newVal.txid = _id.getTransactionId();
                     if (val != null) {

File: storm-client/src/jvm/org/apache/storm/scheduler/resource/strategies/scheduling/DefaultResourceAwareStrategy.java
Patch:
@@ -541,7 +541,7 @@ public int compare(Component o1, Component o2) {
                     connections2 += (componentMap.get(childId).execs.size() * o2.execs.size());
                 }
 
-                if (connections1 > connections1) {
+                if (connections1 > connections2) {
                     return -1;
                 } else if (connections1 < connections2) {
                     return 1;
@@ -567,9 +567,9 @@ private Set<Component> sortNeighbors(final Component thisComp, final Map<String,
             public int compare(Component o1, Component o2) {
                 int connections1 = o1.execs.size() * thisComp.execs.size();
                 int connections2 = o2.execs.size() * thisComp.execs.size();
-                if (connections1 > connections2) {
+                if (connections1 < connections2) {
                     return -1;
-                } else if (connections1 < connections2) {
+                } else if (connections1 > connections2) {
                     return 1;
                 } else {
                     return o1.id.compareTo(o2.id);

File: storm-client/src/jvm/org/apache/storm/trident/windowing/StoreBasedTridentWindowManager.java
Patch:
@@ -121,7 +121,7 @@ private String secondLastPart(String key) {
         }
         String trimKey = key.substring(0, lastSepIndex);
         int secondLastSepIndex = trimKey.lastIndexOf(WindowsStore.KEY_SEPARATOR);
-        if (lastSepIndex < 0) {
+        if (secondLastSepIndex < 0) {
             throw new IllegalArgumentException("key "+key+" does not have second key separator '" + WindowsStore.KEY_SEPARATOR + "'");
         }
 

File: storm-server/src/main/java/org/apache/storm/daemon/supervisor/ReadClusterState.java
Patch:
@@ -195,7 +195,7 @@ protected Map<String, VersionedData<Assignment>> getAssignmentsSnapshot(IStormCl
             }
             if (version == null) {
                 // ignore
-            } else if (version == recordedVersion) {
+            } else if (version.equals(recordedVersion)) {
                 updateAssignmentVersion.put(topoId, locAssignment);
             } else {
                 VersionedData<Assignment> assignmentVersion = stormClusterState.assignmentInfoWithVersion(topoId, callback);

File: storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java
Patch:
@@ -3478,7 +3478,7 @@ public TopologyPageInfo getTopologyPageInfo(String topoId, String window, boolea
             topoPageInfo.set_name(topoName);
             topoPageInfo.set_status(extractStatusStr(base));
             topoPageInfo.set_uptime_secs(Time.deltaSecs(launchTimeSecs));
-            topoPageInfo.set_topoConf(JSONValue.toJSONString(topoConf));
+            topoPageInfo.set_topology_conf(JSONValue.toJSONString(topoConf));
             topoPageInfo.set_replication_count(getBlobReplicationCount(ConfigUtils.masterStormCodeKey(topoId)));
             if (base.is_set_component_debug()) {
                 DebugOptions debug = base.get_component_debug().get(topoId);

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/bolt/mapper/FieldNameBasedTupleToKafkaMapper.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.kafka.bolt.mapper;
 
 import org.apache.storm.tuple.Tuple;

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/bolt/selector/DefaultTopicSelector.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.kafka.bolt.selector;
 
 import org.apache.storm.tuple.Tuple;

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/bolt/selector/FieldNameTopicSelector.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.kafka.bolt.selector;
 
 import org.apache.storm.tuple.Tuple;

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/bolt/selector/KafkaTopicSelector.java
Patch:
@@ -15,11 +15,11 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.storm.kafka.bolt.selector;
 
-import org.apache.storm.tuple.Tuple;
+package org.apache.storm.kafka.bolt.selector;
 
 import java.io.Serializable;
+import org.apache.storm.tuple.Tuple;
 
 public interface KafkaTopicSelector extends Serializable {
     String getTopic(Tuple tuple);

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/DefaultRecordTranslator.java
Patch:
@@ -15,17 +15,18 @@
  *   See the License for the specific language governing permissions and
  *   limitations under the License.
  */
+
 package org.apache.storm.kafka.spout;
 
 import java.util.List;
-
 import org.apache.kafka.clients.consumer.ConsumerRecord;
 import org.apache.storm.tuple.Fields;
 import org.apache.storm.tuple.Values;
 
 public class DefaultRecordTranslator<K, V> implements RecordTranslator<K, V> {
     private static final long serialVersionUID = -5782462870112305750L;
     public static final Fields FIELDS = new Fields("topic", "partition", "offset", "key", "value");
+    
     @Override
     public List<Object> apply(ConsumerRecord<K, V> record) {
         return new Values(record.topic(),

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/Func.java
Patch:
@@ -15,13 +15,14 @@
  *   See the License for the specific language governing permissions and
  *   limitations under the License.
  */
+
 package org.apache.storm.kafka.spout;
 
 import java.io.Serializable;
 
 /**
  * A simple interface to allow compatibility with non java 8
- * code bases 
+ * code bases. 
  */
 public interface Func<V, R> extends Serializable {
     R apply(V record);

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/ManualPartitionNamedSubscription.java
Patch:
@@ -15,6 +15,7 @@
  *   See the License for the specific language governing permissions and
  *   limitations under the License.
  */
+
 package org.apache.storm.kafka.spout;
 
 import java.util.ArrayList;
@@ -24,7 +25,6 @@
 import java.util.HashSet;
 import java.util.List;
 import java.util.Set;
-
 import org.apache.kafka.clients.consumer.ConsumerRebalanceListener;
 import org.apache.kafka.clients.consumer.KafkaConsumer;
 import org.apache.kafka.common.PartitionInfo;

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/ManualPartitionPatternSubscription.java
Patch:
@@ -15,6 +15,7 @@
  *   See the License for the specific language governing permissions and
  *   limitations under the License.
  */
+
 package org.apache.storm.kafka.spout;
 
 import java.util.ArrayList;
@@ -24,7 +25,6 @@
 import java.util.Map;
 import java.util.Set;
 import java.util.regex.Pattern;
-
 import org.apache.kafka.clients.consumer.ConsumerRebalanceListener;
 import org.apache.kafka.clients.consumer.KafkaConsumer;
 import org.apache.kafka.common.PartitionInfo;
@@ -55,7 +55,7 @@ public <K, V> void subscribe(KafkaConsumer<K, V> consumer, ConsumerRebalanceList
     @Override
     public void refreshAssignment() {
         List<TopicPartition> allPartitions = new ArrayList<>();
-        for(Map.Entry<String, List<PartitionInfo>> entry: consumer.listTopics().entrySet()) {
+        for (Map.Entry<String, List<PartitionInfo>> entry: consumer.listTopics().entrySet()) {
             if (pattern.matcher(entry.getKey()).matches()) {
                 for (PartitionInfo partitionInfo: entry.getValue()) {
                     allPartitions.add(new TopicPartition(partitionInfo.topic(), partitionInfo.partition()));

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/ManualPartitioner.java
Patch:
@@ -15,10 +15,10 @@
  *   See the License for the specific language governing permissions and
  *   limitations under the License.
  */
+
 package org.apache.storm.kafka.spout;
 
 import java.util.List;
-
 import org.apache.kafka.common.TopicPartition;
 import org.apache.storm.task.TopologyContext;
 

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/NamedSubscription.java
Patch:
@@ -15,21 +15,21 @@
  *   See the License for the specific language governing permissions and
  *   limitations under the License.
  */
+
 package org.apache.storm.kafka.spout;
 
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collection;
 import java.util.Collections;
-
 import org.apache.kafka.clients.consumer.ConsumerRebalanceListener;
 import org.apache.kafka.clients.consumer.KafkaConsumer;
 import org.apache.storm.task.TopologyContext;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
 /**
- * Subscribe to all topics that follow a given list of values
+ * Subscribe to all topics that follow a given list of values.
  */
 public class NamedSubscription extends Subscription {
     private static final Logger LOG = LoggerFactory.getLogger(NamedSubscription.class);

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/PatternSubscription.java
Patch:
@@ -15,18 +15,18 @@
  *   See the License for the specific language governing permissions and
  *   limitations under the License.
  */
+
 package org.apache.storm.kafka.spout;
 
 import java.util.regex.Pattern;
-
 import org.apache.kafka.clients.consumer.ConsumerRebalanceListener;
 import org.apache.kafka.clients.consumer.KafkaConsumer;
 import org.apache.storm.task.TopologyContext;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
 /**
- * Subscribe to all topics that match a given pattern
+ * Subscribe to all topics that match a given pattern.
  */
 public class PatternSubscription extends Subscription {
     private static final Logger LOG = LoggerFactory.getLogger(PatternSubscription.class);

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/RoundRobinManualPartitioner.java
Patch:
@@ -15,13 +15,13 @@
  *   See the License for the specific language governing permissions and
  *   limitations under the License.
  */
+
 package org.apache.storm.kafka.spout;
 
 import java.util.ArrayList;
 import java.util.HashSet;
 import java.util.List;
 import java.util.Set;
-
 import org.apache.kafka.common.TopicPartition;
 import org.apache.storm.task.TopologyContext;
 
@@ -30,7 +30,7 @@
  * not just the ones that are alive. Because the parallelism of 
  * the spouts does not typically change while running this makes
  * the assignments more stable in the face of crashing spouts.
- * 
+ * <p/>
  * Round Robin means that first spout of N spouts will get the first
  * partition, and the N+1th partition... The second spout will get the second partition and
  * N+2th partition etc.
@@ -41,7 +41,7 @@ public class RoundRobinManualPartitioner implements ManualPartitioner {
     public List<TopicPartition> partition(List<TopicPartition> allPartitions, TopologyContext context) {
         int thisTaskIndex = context.getThisTaskIndex();
         int totalTaskCount = context.getComponentTasks(context.getThisComponentId()).size();
-        Set<TopicPartition> myPartitions = new HashSet<>(allPartitions.size()/totalTaskCount+1);
+        Set<TopicPartition> myPartitions = new HashSet<>(allPartitions.size() / totalTaskCount + 1);
         for (int i = thisTaskIndex; i < allPartitions.size(); i += totalTaskCount) {
             myPartitions.add(allPartitions.get(i));
         }

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/SerializableDeserializer.java
Patch:
@@ -13,6 +13,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.kafka.spout;
 
 import java.io.Serializable;

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/Subscription.java
Patch:
@@ -15,10 +15,10 @@
  *   See the License for the specific language governing permissions and
  *   limitations under the License.
  */
+
 package org.apache.storm.kafka.spout;
 
 import java.io.Serializable;
-
 import org.apache.kafka.clients.consumer.ConsumerRebalanceListener;
 import org.apache.kafka.clients.consumer.KafkaConsumer;
 import org.apache.storm.task.TopologyContext;

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/TopicPartitionComparator.java
Patch:
@@ -15,24 +15,24 @@
  *   See the License for the specific language governing permissions and
  *   limitations under the License.
  */
+
 package org.apache.storm.kafka.spout;
 
 import java.util.Comparator;
-
 import org.apache.kafka.common.TopicPartition;
 
 /**
  * Singleton comparator of TopicPartitions.  Topics have precedence over partitions.
  * Topics are compared through String.compare and partitions are compared
  * numerically.
- * 
+ * <p/>
  * Use INSTANCE for all sorting.
  */
 public class TopicPartitionComparator implements Comparator<TopicPartition> {
     public static final TopicPartitionComparator INSTANCE = new TopicPartitionComparator();
     
     /**
-     * Private to make it a singleton
+     * Private to make it a singleton.
      */
     private TopicPartitionComparator() {
         //Empty

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/KafkaConsumerFactory.java
Patch:
@@ -13,14 +13,15 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.kafka.spout.internal;
 
 import java.io.Serializable;
 import org.apache.kafka.clients.consumer.KafkaConsumer;
 import org.apache.storm.kafka.spout.KafkaSpoutConfig;
 
 /**
- * This is here to enable testing
+ * This is here to enable testing.
  */
 public interface KafkaConsumerFactory<K, V> extends Serializable {
     public KafkaConsumer<K,V> createConsumer(KafkaSpoutConfig<K, V> kafkaSpoutConfig);

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/KafkaConsumerFactoryDefault.java
Patch:
@@ -13,6 +13,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.kafka.spout.internal;
 
 import org.apache.kafka.clients.consumer.KafkaConsumer;

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/Timer.java
Patch:
@@ -15,6 +15,7 @@
  *   See the License for the specific language governing permissions and
  *   limitations under the License.
  */
+
 package org.apache.storm.kafka.spout.internal;
 
 import java.util.concurrent.TimeUnit;
@@ -63,7 +64,7 @@ public TimeUnit getTimeUnit() {
      * (re-initiated) and a new cycle will start.
      *
      * @return true if the time elapsed since the last call returning true is greater than {@code period}. Returns false
-     * otherwise.
+     *     otherwise.
      */
     public boolean isExpiredResetOnTrue() {
         final boolean expired = Time.nanoTime() - start >= periodNanos;

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/trident/KafkaTridentSpoutTopicPartitionRegistry.java
Patch:
@@ -18,12 +18,11 @@
 
 package org.apache.storm.kafka.spout.trident;
 
-import org.apache.kafka.common.TopicPartition;
-
 import java.util.Collection;
 import java.util.Collections;
 import java.util.LinkedHashSet;
 import java.util.Set;
+import org.apache.kafka.common.TopicPartition;
 
 public enum KafkaTridentSpoutTopicPartitionRegistry {
     INSTANCE;

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/trident/KafkaTridentSpoutTransactional.java
Patch:
@@ -18,13 +18,12 @@
 
 package org.apache.storm.kafka.spout.trident;
 
+import java.util.Map;
 import org.apache.storm.task.TopologyContext;
 import org.apache.storm.trident.spout.IPartitionedTridentSpout;
 import org.apache.storm.trident.spout.ISpoutPartition;
 import org.apache.storm.tuple.Fields;
 
-import java.util.Map;
-
 // TODO
 public class KafkaTridentSpoutTransactional<Ps, P extends ISpoutPartition, T> implements IPartitionedTridentSpout<Ps, P, T> {
     @Override

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/trident/mapper/FieldNameBasedTupleToKafkaMapper.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.kafka.trident.mapper;
 
 import org.apache.storm.trident.tuple.TridentTuple;

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/trident/selector/DefaultTopicSelector.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.kafka.trident.selector;
 
 import org.apache.storm.trident.tuple.TridentTuple;

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/trident/selector/KafkaTopicSelector.java
Patch:
@@ -15,11 +15,11 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.storm.kafka.trident.selector;
 
-import org.apache.storm.trident.tuple.TridentTuple;
+package org.apache.storm.kafka.trident.selector;
 
 import java.io.Serializable;
+import org.apache.storm.trident.tuple.TridentTuple;
 
 public interface KafkaTopicSelector extends Serializable {
     String getTopic(TridentTuple tuple);

File: external/storm-kafka-client/src/test/java/org/apache/storm/kafka/KafkaUnitRule.java
Patch:
@@ -17,9 +17,8 @@
  */
 package org.apache.storm.kafka;
 
-import org.junit.rules.ExternalResource;
-
 import java.io.IOException;
+import org.junit.rules.ExternalResource;
 
 
 public class KafkaUnitRule extends ExternalResource {

File: external/storm-kafka-client/src/test/java/org/apache/storm/kafka/bolt/KafkaBoltTest.java
Patch:
@@ -27,7 +27,6 @@
 import java.util.HashMap;
 import java.util.Map;
 import java.util.Properties;
-
 import org.apache.kafka.clients.producer.Callback;
 import org.apache.kafka.clients.producer.KafkaProducer;
 import org.apache.kafka.clients.producer.ProducerRecord;

File: external/storm-kafka-client/src/test/java/org/apache/storm/kafka/spout/ByTopicRecordTranslatorTest.java
Patch:
@@ -21,7 +21,6 @@
 
 import java.util.Arrays;
 import java.util.HashSet;
-
 import org.apache.kafka.clients.consumer.ConsumerRecord;
 import org.apache.storm.tuple.Fields;
 import org.apache.storm.tuple.Values;

File: external/storm-kafka-client/src/test/java/org/apache/storm/kafka/spout/DefaultRecordTranslatorTest.java
Patch:
@@ -20,7 +20,6 @@
 import static org.junit.Assert.assertEquals;
 
 import java.util.Arrays;
-
 import org.apache.kafka.clients.consumer.ConsumerRecord;
 import org.apache.storm.tuple.Fields;
 import org.junit.Test;

File: external/storm-kafka-client/src/test/java/org/apache/storm/kafka/spout/KafkaSpoutRebalanceTest.java
Patch:
@@ -34,7 +34,6 @@
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
-
 import org.apache.kafka.clients.consumer.ConsumerRebalanceListener;
 import org.apache.kafka.clients.consumer.ConsumerRecord;
 import org.apache.kafka.clients.consumer.ConsumerRecords;

File: external/storm-kafka-client/src/test/java/org/apache/storm/kafka/spout/MaxUncommittedOffsetTest.java
Patch:
@@ -32,7 +32,6 @@
 import java.util.List;
 import java.util.Map;
 import java.util.stream.Collectors;
-import org.apache.kafka.clients.consumer.KafkaConsumer;
 import org.apache.kafka.clients.producer.ProducerRecord;
 import org.apache.storm.kafka.KafkaUnitRule;
 import org.apache.storm.kafka.spout.builders.SingleTopicKafkaSpoutConfiguration;

File: external/storm-kafka-client/src/test/java/org/apache/storm/kafka/spout/test/KafkaSpoutTestBolt.java
Patch:
@@ -18,6 +18,7 @@
 
 package org.apache.storm.kafka.spout.test;
 
+import java.util.Map;
 import org.apache.storm.task.OutputCollector;
 import org.apache.storm.task.TopologyContext;
 import org.apache.storm.topology.OutputFieldsDeclarer;
@@ -26,8 +27,6 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import java.util.Map;
-
 public class KafkaSpoutTestBolt extends BaseRichBolt {
     protected static final Logger LOG = LoggerFactory.getLogger(KafkaSpoutTestBolt.class);
     private OutputCollector collector;

File: external/storm-kafka-client/src/test/java/org/apache/storm/kafka/spout/test/KafkaSpoutTopologyMainNamedTopics.java
Patch:
@@ -23,7 +23,6 @@
 import java.io.BufferedReader;
 import java.io.IOException;
 import java.io.InputStreamReader;
-
 import org.apache.storm.Config;
 import org.apache.storm.LocalCluster;
 import org.apache.storm.StormSubmitter;

File: external/storm-kafka-client/src/test/java/org/apache/storm/kafka/spout/test/KafkaSpoutTopologyMainWildcardTopics.java
Patch:
@@ -21,7 +21,6 @@
 import static org.apache.storm.kafka.spout.KafkaSpoutConfig.FirstPollOffsetStrategy.EARLIEST;
 
 import java.util.regex.Pattern;
-
 import org.apache.storm.generated.StormTopology;
 import org.apache.storm.kafka.spout.KafkaSpout;
 import org.apache.storm.kafka.spout.KafkaSpoutConfig;

File: integration-test/src/main/java/org/apache/storm/st/topology/window/SlidingTimeCorrectness.java
Patch:
@@ -114,7 +114,9 @@ public void open(Map<String, Object> conf, TopologyContext context, SpoutOutputC
 
         @Override
         public void nextTuple() {
-            TimeUtil.sleepMilliSec(rng.nextInt(800));
+            //Emitting too quickly can lead to spurious test failures because the worker log may roll right before we read it
+            //Sleep a bit between emits
+            TimeUtil.sleepMilliSec(rng.nextInt(100));
             currentNum++;
             TimeData data = TimeData.newData(currentNum);
             final Values tuple = data.getValues();

File: integration-test/src/main/java/org/apache/storm/st/topology/window/SlidingWindowCorrectness.java
Patch:
@@ -39,7 +39,6 @@
 import java.util.List;
 import java.util.Map;
 import java.util.Random;
-import java.util.concurrent.TimeUnit;
 
 /**
  * Computes sliding window sum
@@ -74,7 +73,6 @@ public StormTopology newTopology() {
         builder.setSpout(getSpoutName(), new IncrementingSpout(), 1);
         builder.setBolt(getBoltName(),
                 new VerificationBolt()
-                        .withLag(new BaseWindowedBolt.Duration(10, TimeUnit.SECONDS))
                         .withWindow(new BaseWindowedBolt.Count(windowSize), new BaseWindowedBolt.Count(slideSize)),
                 1)
                 .shuffleGrouping(getSpoutName());

File: integration-test/src/main/java/org/apache/storm/st/topology/window/TumblingTimeCorrectness.java
Patch:
@@ -111,7 +111,9 @@ public void open(Map<String, Object> conf, TopologyContext context, SpoutOutputC
 
         @Override
         public void nextTuple() {
-            TimeUtil.sleepMilliSec(rng.nextInt(800));
+            //Emitting too quickly can lead to spurious test failures because the worker log may roll right before we read it
+            //Sleep a bit between emits
+            TimeUtil.sleepMilliSec(rng.nextInt(100));
             currentNum++;
             TimeData data = TimeData.newData(currentNum);
             final Values tuple = data.getValues();

File: integration-test/src/main/java/org/apache/storm/st/topology/window/TumblingWindowCorrectness.java
Patch:
@@ -39,7 +39,6 @@
 import java.util.List;
 import java.util.Map;
 import java.util.Random;
-import java.util.concurrent.TimeUnit;
 
 /**
  * Computes sliding window sum
@@ -72,7 +71,6 @@ public StormTopology newTopology() {
         builder.setSpout(getSpoutName(), new IncrementingSpout(), 1);
         builder.setBolt(getBoltName(),
                 new VerificationBolt()
-                        .withLag(new BaseWindowedBolt.Duration(10, TimeUnit.SECONDS))
                         .withTumblingWindow(new BaseWindowedBolt.Count(tumbleSize)), 1)
                 .shuffleGrouping(getSpoutName());
         return builder.createTopology();

File: integration-test/src/main/java/org/apache/storm/st/utils/StringDecorator.java
Patch:
@@ -17,6 +17,7 @@
 
 package org.apache.storm.st.utils;
 
+import java.nio.charset.StandardCharsets;
 import org.apache.commons.lang.StringUtils;
 
 public class StringDecorator {

File: storm-client/src/jvm/org/apache/storm/windowing/EvictionContext.java
Patch:
@@ -38,7 +38,7 @@ public interface EvictionContext {
     Long getSlidingCount();
 
     /**
-     * Returns the current count of events in the queue up to the reference tim
+     * Returns the current count of events in the queue up to the reference time
      * based on which count based evictions can be performed.
      *
      * @return the current count

File: storm-client/src/jvm/org/apache/storm/windowing/EvictionPolicy.java
Patch:
@@ -27,7 +27,7 @@ public interface EvictionPolicy<T> {
     /**
      * The action to be taken when {@link EvictionPolicy#evict(Event)} is invoked.
      */
-    enum Action {
+    public enum Action {
         /**
          * expire the event and remove it from the queue
          */

File: storm-client/src/jvm/org/apache/storm/windowing/TimeEvictionPolicy.java
Patch:
@@ -38,7 +38,7 @@ public TimeEvictionPolicy(int windowLength) {
      * {@inheritDoc}
      */
     @Override
-    public Action evict(Event<T> event) {
+    public Action evict(Event<T> event) {      
         long now = evictionContext == null ? System.currentTimeMillis() : evictionContext.getReferenceTime();
         long diff = now - event.getTimestamp();
         if (diff >= windowLength) {

File: storm-client/src/jvm/org/apache/storm/windowing/TimeTriggerPolicy.java
Patch:
@@ -114,9 +114,7 @@ public void run() {
                      * set the current timestamp as the reference time for the eviction policy
                      * to evict the events
                      */
-                    if (evictionPolicy != null) {
-                        evictionPolicy.setContext(new DefaultEvictionContext(System.currentTimeMillis()));
-                    }
+                    evictionPolicy.setContext(new DefaultEvictionContext(System.currentTimeMillis()));
                     handler.onTrigger();
                 } catch (Throwable th) {
                     LOG.error("handler.onTrigger failed ", th);

File: storm-client/src/jvm/org/apache/storm/windowing/WaterMarkEventGenerator.java
Patch:
@@ -77,7 +77,7 @@ public void run() {
         try {
             long waterMarkTs = computeWaterMarkTs();
             if (waterMarkTs > lastWaterMarkTs) {
-                this.windowManager.add(new WaterMarkEvent<T>(waterMarkTs));
+                this.windowManager.add(new WaterMarkEvent<>(waterMarkTs));
                 lastWaterMarkTs = waterMarkTs;
             }
         } catch (Throwable th) {

File: storm-client/src/jvm/org/apache/storm/windowing/WindowManager.java
Patch:
@@ -46,6 +46,9 @@ public class WindowManager<T> implements TriggerHandler {
     /**
      * Expire old events every EXPIRE_EVENTS_THRESHOLD to
      * keep the window size in check.
+     * 
+     * Note that if the eviction policy is based on watermarks, events will not be evicted until a new
+     * watermark would cause them to be considered expired anyway, regardless of this limit
      */
     public static final int EXPIRE_EVENTS_THRESHOLD = 100;
 

File: examples/storm-starter/src/jvm/org/apache/storm/starter/ReachTopology.java
Patch:
@@ -109,7 +109,7 @@ public void declareOutputFields(OutputFieldsDeclarer declarer) {
         }
     }
 
-    public static class PartialUniquer extends BaseBatchBolt {
+    public static class PartialUniquer extends BaseBatchBolt<Object> {
         BatchOutputCollector _collector;
         Object _id;
         Set<String> _followers = new HashSet<String>();
@@ -136,7 +136,7 @@ public void declareOutputFields(OutputFieldsDeclarer declarer) {
         }
     }
 
-    public static class CountAggregator extends BaseBatchBolt {
+    public static class CountAggregator extends BaseBatchBolt<Object> {
         BatchOutputCollector _collector;
         Object _id;
         int _count = 0;

File: examples/storm-starter/src/jvm/org/apache/storm/starter/TransactionalGlobalCount.java
Patch:
@@ -83,7 +83,7 @@ public static class Value {
     public static Map<String, Value> DATABASE = new HashMap<String, Value>();
     public static final String GLOBAL_COUNT_KEY = "GLOBAL-COUNT";
 
-    public static class BatchCount extends BaseBatchBolt {
+    public static class BatchCount extends BaseBatchBolt<Object> {
         Object _id;
         BatchOutputCollector _collector;
 

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/bolt/AbstractHBaseBolt.java
Patch:
@@ -51,11 +51,11 @@ public AbstractHBaseBolt(String tableName, HBaseMapper mapper) {
     }
 
     @Override
-    public void prepare(Map map, TopologyContext topologyContext, OutputCollector collector) {
+    public void prepare(Map topoConf, TopologyContext topologyContext, OutputCollector collector) {
         this.collector = collector;
         final Configuration hbConfig = HBaseConfiguration.create();
 
-        Map<String, Object> conf = (Map<String, Object>)map.get(this.configKey);
+        Map<String, Object> conf = (Map<String, Object>)topoConf.get(this.configKey);
         if(conf == null) {
             throw new IllegalArgumentException("HBase configuration not found using key '" + this.configKey + "'");
         }
@@ -70,7 +70,7 @@ public void prepare(Map map, TopologyContext topologyContext, OutputCollector co
         //heck for backward compatibility, we need to pass TOPOLOGY_AUTO_CREDENTIALS to hbase conf
         //the conf instance is instance of persistentMap so making a copy.
         Map<String, Object> hbaseConfMap = new HashMap<String, Object>(conf);
-        hbaseConfMap.put(Config.TOPOLOGY_AUTO_CREDENTIALS, map.get(Config.TOPOLOGY_AUTO_CREDENTIALS));
+        hbaseConfMap.put(Config.TOPOLOGY_AUTO_CREDENTIALS, topoConf.get(Config.TOPOLOGY_AUTO_CREDENTIALS));
         this.hBaseClient = new HBaseClient(hbaseConfMap, hbConfig, tableName);
     }
 

File: external/storm-jms/src/test/java/org/apache/storm/jms/spout/JmsSpoutTest.java
Patch:
@@ -51,7 +51,7 @@ public void testFailure() throws JMSException, Exception{
         spout.setJmsTupleProducer(new MockTupleProducer());
         spout.setJmsAcknowledgeMode(Session.CLIENT_ACKNOWLEDGE);
         spout.setRecoveryPeriod(10); // Rapid recovery for testing.
-        spout.open(new HashMap<String,String>(), null, collector);
+        spout.open(new HashMap<>(), null, collector);
         Message msg = this.sendMessage(mockProvider.connectionFactory(), mockProvider.destination());
         Thread.sleep(100);
         spout.nextTuple(); // Pretend to be storm.

File: storm-client/src/jvm/org/apache/storm/stats/StatsUtil.java
Patch:
@@ -2377,11 +2377,11 @@ public static Object getByKey(Map map, String key) {
         return map.get(key);
     }
 
-    public static Map getMapByKey(Map map, String key) {
+    public static <K, V> Map<K,V> getMapByKey(Map map, String key) {
         if (map == null) {
             return null;
         }
-        return (Map) map.get(key);
+        return (Map<K,V>) map.get(key);
     }
 
     private static <K, V extends Number> long sumValues(Map<K, V> m) {

File: storm-client/src/jvm/org/apache/storm/testing/BatchNumberList.java
Patch:
@@ -29,7 +29,7 @@
 import java.util.List;
 import java.util.Map;
 
-public class BatchNumberList extends BaseBatchBolt {
+public class BatchNumberList extends BaseBatchBolt<Object> {
 
     @Override
     public void declareOutputFields(OutputFieldsDeclarer declarer) {

File: storm-client/src/jvm/org/apache/storm/testing/CountingBatchBolt.java
Patch:
@@ -26,7 +26,7 @@
 import org.apache.storm.tuple.Values;
 import java.util.Map;
 
-public class CountingBatchBolt extends BaseBatchBolt {
+public class CountingBatchBolt extends BaseBatchBolt<Object> {
     BatchOutputCollector _collector;
     Object _id;
     int _count = 0;

File: storm-client/src/jvm/org/apache/storm/testing/KeyedCountingBatchBolt.java
Patch:
@@ -29,7 +29,7 @@
 import java.util.HashMap;
 import java.util.Map;
 
-public class KeyedCountingBatchBolt extends BaseBatchBolt {
+public class KeyedCountingBatchBolt extends BaseBatchBolt<Object> {
     BatchOutputCollector _collector;
     Object _id;
     Map<Object, Integer> _counts = new HashMap<Object, Integer>();

File: storm-client/src/jvm/org/apache/storm/testing/KeyedSummingBatchBolt.java
Patch:
@@ -28,7 +28,7 @@
 import java.util.HashMap;
 import java.util.Map;
 
-public class KeyedSummingBatchBolt extends BaseBatchBolt {
+public class KeyedSummingBatchBolt extends BaseBatchBolt<Object> {
     BatchOutputCollector _collector;
     Object _id;
     Map<Object, Number> _sums = new HashMap<Object, Number>();

File: storm-client/src/jvm/org/apache/storm/trident/testing/FeederBatchSpout.java
Patch:
@@ -32,7 +32,7 @@
 import org.apache.storm.trident.topology.TransactionAttempt;
 import org.apache.storm.trident.topology.TridentTopologyBuilder;
 
-public class FeederBatchSpout implements ITridentSpout, IFeeder {
+public class FeederBatchSpout implements ITridentSpout<Map<Integer, List<List<Object>>>>, IFeeder {
 
     String _id;
     String _semaphoreId;
@@ -167,7 +167,7 @@ public Fields getOutputFields() {
     }
 
     @Override
-    public BatchCoordinator getCoordinator(String txStateId, Map conf, TopologyContext context) {
+    public BatchCoordinator<Map<Integer, List<List<Object>>>> getCoordinator(String txStateId, Map conf, TopologyContext context) {
         int numTasks = context.getComponentTasks(
                             TridentTopologyBuilder.spoutIdFromCoordinatorId(
                                 context.getThisComponentId()))
@@ -176,7 +176,7 @@ public BatchCoordinator getCoordinator(String txStateId, Map conf, TopologyConte
     }
 
     @Override
-    public Emitter getEmitter(String txStateId, Map conf, TopologyContext context) {
+    public Emitter<Map<Integer, List<List<Object>>>> getEmitter(String txStateId, Map conf, TopologyContext context) {
         return new FeederEmitter(context.getThisTaskIndex());
     }
     

File: storm-client/src/jvm/org/apache/storm/trident/testing/FeederCommitterBatchSpout.java
Patch:
@@ -27,7 +27,7 @@
 import org.apache.storm.trident.topology.TransactionAttempt;
 
 
-public class FeederCommitterBatchSpout implements ICommitterTridentSpout, IFeeder {
+public class FeederCommitterBatchSpout implements ICommitterTridentSpout<Map<Integer, List<List<Object>>>>, IFeeder {
 
     FeederBatchSpout _spout;
     
@@ -74,7 +74,7 @@ public Emitter getEmitter(String txStateId, Map conf, TopologyContext context) {
     }
 
     @Override
-    public BatchCoordinator getCoordinator(String txStateId, Map conf, TopologyContext context) {
+    public BatchCoordinator<Map<Integer, List<List<Object>>>> getCoordinator(String txStateId, Map conf, TopologyContext context) {
         return _spout.getCoordinator(txStateId, conf, context);
     }
 

File: storm-client/test/jvm/org/apache/storm/TestConfigValidate.java
Patch:
@@ -86,7 +86,7 @@ public void invalidConfigTest() throws InvocationTargetException, NoSuchMethodEx
 
     @Test(expected = InvalidTopologyException.class)
     public void testValidateTopologyBlobStoreMap() throws InvalidTopologyException {
-        Map<String,Map> stormConf = new HashMap<>();
+        Map<String, Object> stormConf = new HashMap<>();
         Map<String,Map> topologyMap = new HashMap<>();
         topologyMap.put("key1", new HashMap<String,String>());
         topologyMap.put("key2", new HashMap<String,String>());

File: storm-server/src/main/java/org/apache/storm/daemon/drpc/DRPC.java
Patch:
@@ -76,7 +76,7 @@ static void checkAuthorization(ReqContext reqContext, IAuthorizer auth, String o
             ThriftAccessLogger.logAccessFunction(reqContext.requestID(), reqContext.remoteAddress(), reqContext.principal(), operation, function);
         }
         if (auth != null) {
-            Map<String, String> map = new HashMap<>();
+            Map<String, Object> map = new HashMap<>();
             map.put(DRPCAuthorizerBase.FUNCTION_NAME, function);
             if (!auth.permit(reqContext, operation, map)) {
                 Principal principal = reqContext.principal();

File: examples/storm-starter/src/jvm/org/apache/storm/starter/TransactionalWords.java
Patch:
@@ -125,7 +125,7 @@ public void finishBatch() {
             for (String key : _counts.keySet()) {
                 CountValue val = COUNT_DATABASE.get(key);
                 CountValue newVal;
-                if (val == null || !val.txid.equals(_id)) {
+                if (val == null || !val.txid.equals(_id.getTransactionId())) {
                     newVal = new CountValue();
                     newVal.txid = _id.getTransactionId();
                     if (val != null) {

File: storm-client/src/jvm/org/apache/storm/scheduler/resource/strategies/scheduling/DefaultResourceAwareStrategy.java
Patch:
@@ -541,7 +541,7 @@ public int compare(Component o1, Component o2) {
                     connections2 += (componentMap.get(childId).execs.size() * o2.execs.size());
                 }
 
-                if (connections1 > connections1) {
+                if (connections1 > connections2) {
                     return -1;
                 } else if (connections1 < connections2) {
                     return 1;
@@ -567,9 +567,9 @@ private Set<Component> sortNeighbors(final Component thisComp, final Map<String,
             public int compare(Component o1, Component o2) {
                 int connections1 = o1.execs.size() * thisComp.execs.size();
                 int connections2 = o2.execs.size() * thisComp.execs.size();
-                if (connections1 > connections2) {
+                if (connections1 < connections2) {
                     return -1;
-                } else if (connections1 < connections2) {
+                } else if (connections1 > connections2) {
                     return 1;
                 } else {
                     return o1.id.compareTo(o2.id);

File: storm-client/src/jvm/org/apache/storm/trident/windowing/StoreBasedTridentWindowManager.java
Patch:
@@ -121,7 +121,7 @@ private String secondLastPart(String key) {
         }
         String trimKey = key.substring(0, lastSepIndex);
         int secondLastSepIndex = trimKey.lastIndexOf(WindowsStore.KEY_SEPARATOR);
-        if (lastSepIndex < 0) {
+        if (secondLastSepIndex < 0) {
             throw new IllegalArgumentException("key "+key+" does not have second key separator '" + WindowsStore.KEY_SEPARATOR + "'");
         }
 

File: storm-server/src/main/java/org/apache/storm/daemon/supervisor/ReadClusterState.java
Patch:
@@ -195,7 +195,7 @@ protected Map<String, VersionedData<Assignment>> getAssignmentsSnapshot(IStormCl
             }
             if (version == null) {
                 // ignore
-            } else if (version == recordedVersion) {
+            } else if (version.equals(recordedVersion)) {
                 updateAssignmentVersion.put(topoId, locAssignment);
             } else {
                 VersionedData<Assignment> assignmentVersion = stormClusterState.assignmentInfoWithVersion(topoId, callback);

File: examples/storm-starter/src/jvm/org/apache/storm/starter/ReachTopology.java
Patch:
@@ -109,7 +109,7 @@ public void declareOutputFields(OutputFieldsDeclarer declarer) {
         }
     }
 
-    public static class PartialUniquer extends BaseBatchBolt {
+    public static class PartialUniquer extends BaseBatchBolt<Object> {
         BatchOutputCollector _collector;
         Object _id;
         Set<String> _followers = new HashSet<String>();
@@ -136,7 +136,7 @@ public void declareOutputFields(OutputFieldsDeclarer declarer) {
         }
     }
 
-    public static class CountAggregator extends BaseBatchBolt {
+    public static class CountAggregator extends BaseBatchBolt<Object> {
         BatchOutputCollector _collector;
         Object _id;
         int _count = 0;

File: examples/storm-starter/src/jvm/org/apache/storm/starter/TransactionalGlobalCount.java
Patch:
@@ -83,7 +83,7 @@ public static class Value {
     public static Map<String, Value> DATABASE = new HashMap<String, Value>();
     public static final String GLOBAL_COUNT_KEY = "GLOBAL-COUNT";
 
-    public static class BatchCount extends BaseBatchBolt {
+    public static class BatchCount extends BaseBatchBolt<Object> {
         Object _id;
         BatchOutputCollector _collector;
 

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/bolt/AbstractHBaseBolt.java
Patch:
@@ -51,11 +51,11 @@ public AbstractHBaseBolt(String tableName, HBaseMapper mapper) {
     }
 
     @Override
-    public void prepare(Map map, TopologyContext topologyContext, OutputCollector collector) {
+    public void prepare(Map topoConf, TopologyContext topologyContext, OutputCollector collector) {
         this.collector = collector;
         final Configuration hbConfig = HBaseConfiguration.create();
 
-        Map<String, Object> conf = (Map<String, Object>)map.get(this.configKey);
+        Map<String, Object> conf = (Map<String, Object>)topoConf.get(this.configKey);
         if(conf == null) {
             throw new IllegalArgumentException("HBase configuration not found using key '" + this.configKey + "'");
         }
@@ -70,7 +70,7 @@ public void prepare(Map map, TopologyContext topologyContext, OutputCollector co
         //heck for backward compatibility, we need to pass TOPOLOGY_AUTO_CREDENTIALS to hbase conf
         //the conf instance is instance of persistentMap so making a copy.
         Map<String, Object> hbaseConfMap = new HashMap<String, Object>(conf);
-        hbaseConfMap.put(Config.TOPOLOGY_AUTO_CREDENTIALS, map.get(Config.TOPOLOGY_AUTO_CREDENTIALS));
+        hbaseConfMap.put(Config.TOPOLOGY_AUTO_CREDENTIALS, topoConf.get(Config.TOPOLOGY_AUTO_CREDENTIALS));
         this.hBaseClient = new HBaseClient(hbaseConfMap, hbConfig, tableName);
     }
 

File: external/storm-jms/src/test/java/org/apache/storm/jms/spout/JmsSpoutTest.java
Patch:
@@ -51,7 +51,7 @@ public void testFailure() throws JMSException, Exception{
         spout.setJmsTupleProducer(new MockTupleProducer());
         spout.setJmsAcknowledgeMode(Session.CLIENT_ACKNOWLEDGE);
         spout.setRecoveryPeriod(10); // Rapid recovery for testing.
-        spout.open(new HashMap<String,String>(), null, collector);
+        spout.open(new HashMap<>(), null, collector);
         Message msg = this.sendMessage(mockProvider.connectionFactory(), mockProvider.destination());
         Thread.sleep(100);
         spout.nextTuple(); // Pretend to be storm.

File: storm-client/src/jvm/org/apache/storm/stats/StatsUtil.java
Patch:
@@ -2377,11 +2377,11 @@ public static Object getByKey(Map map, String key) {
         return map.get(key);
     }
 
-    public static Map getMapByKey(Map map, String key) {
+    public static <K, V> Map<K,V> getMapByKey(Map map, String key) {
         if (map == null) {
             return null;
         }
-        return (Map) map.get(key);
+        return (Map<K,V>) map.get(key);
     }
 
     private static <K, V extends Number> long sumValues(Map<K, V> m) {

File: storm-client/src/jvm/org/apache/storm/testing/BatchNumberList.java
Patch:
@@ -29,7 +29,7 @@
 import java.util.List;
 import java.util.Map;
 
-public class BatchNumberList extends BaseBatchBolt {
+public class BatchNumberList extends BaseBatchBolt<Object> {
 
     @Override
     public void declareOutputFields(OutputFieldsDeclarer declarer) {

File: storm-client/src/jvm/org/apache/storm/testing/CountingBatchBolt.java
Patch:
@@ -26,7 +26,7 @@
 import org.apache.storm.tuple.Values;
 import java.util.Map;
 
-public class CountingBatchBolt extends BaseBatchBolt {
+public class CountingBatchBolt extends BaseBatchBolt<Object> {
     BatchOutputCollector _collector;
     Object _id;
     int _count = 0;

File: storm-client/src/jvm/org/apache/storm/testing/KeyedCountingBatchBolt.java
Patch:
@@ -29,7 +29,7 @@
 import java.util.HashMap;
 import java.util.Map;
 
-public class KeyedCountingBatchBolt extends BaseBatchBolt {
+public class KeyedCountingBatchBolt extends BaseBatchBolt<Object> {
     BatchOutputCollector _collector;
     Object _id;
     Map<Object, Integer> _counts = new HashMap<Object, Integer>();

File: storm-client/src/jvm/org/apache/storm/testing/KeyedSummingBatchBolt.java
Patch:
@@ -28,7 +28,7 @@
 import java.util.HashMap;
 import java.util.Map;
 
-public class KeyedSummingBatchBolt extends BaseBatchBolt {
+public class KeyedSummingBatchBolt extends BaseBatchBolt<Object> {
     BatchOutputCollector _collector;
     Object _id;
     Map<Object, Number> _sums = new HashMap<Object, Number>();

File: storm-client/src/jvm/org/apache/storm/trident/testing/FeederBatchSpout.java
Patch:
@@ -32,7 +32,7 @@
 import org.apache.storm.trident.topology.TransactionAttempt;
 import org.apache.storm.trident.topology.TridentTopologyBuilder;
 
-public class FeederBatchSpout implements ITridentSpout, IFeeder {
+public class FeederBatchSpout implements ITridentSpout<Map<Integer, List<List<Object>>>>, IFeeder {
 
     String _id;
     String _semaphoreId;
@@ -167,7 +167,7 @@ public Fields getOutputFields() {
     }
 
     @Override
-    public BatchCoordinator getCoordinator(String txStateId, Map conf, TopologyContext context) {
+    public BatchCoordinator<Map<Integer, List<List<Object>>>> getCoordinator(String txStateId, Map conf, TopologyContext context) {
         int numTasks = context.getComponentTasks(
                             TridentTopologyBuilder.spoutIdFromCoordinatorId(
                                 context.getThisComponentId()))
@@ -176,7 +176,7 @@ public BatchCoordinator getCoordinator(String txStateId, Map conf, TopologyConte
     }
 
     @Override
-    public Emitter getEmitter(String txStateId, Map conf, TopologyContext context) {
+    public Emitter<Map<Integer, List<List<Object>>>> getEmitter(String txStateId, Map conf, TopologyContext context) {
         return new FeederEmitter(context.getThisTaskIndex());
     }
     

File: storm-client/src/jvm/org/apache/storm/trident/testing/FeederCommitterBatchSpout.java
Patch:
@@ -27,7 +27,7 @@
 import org.apache.storm.trident.topology.TransactionAttempt;
 
 
-public class FeederCommitterBatchSpout implements ICommitterTridentSpout, IFeeder {
+public class FeederCommitterBatchSpout implements ICommitterTridentSpout<Map<Integer, List<List<Object>>>>, IFeeder {
 
     FeederBatchSpout _spout;
     
@@ -74,7 +74,7 @@ public Emitter getEmitter(String txStateId, Map conf, TopologyContext context) {
     }
 
     @Override
-    public BatchCoordinator getCoordinator(String txStateId, Map conf, TopologyContext context) {
+    public BatchCoordinator<Map<Integer, List<List<Object>>>> getCoordinator(String txStateId, Map conf, TopologyContext context) {
         return _spout.getCoordinator(txStateId, conf, context);
     }
 

File: storm-client/test/jvm/org/apache/storm/TestConfigValidate.java
Patch:
@@ -86,7 +86,7 @@ public void invalidConfigTest() throws InvocationTargetException, NoSuchMethodEx
 
     @Test(expected = InvalidTopologyException.class)
     public void testValidateTopologyBlobStoreMap() throws InvalidTopologyException {
-        Map<String,Map> stormConf = new HashMap<>();
+        Map<String, Object> stormConf = new HashMap<>();
         Map<String,Map> topologyMap = new HashMap<>();
         topologyMap.put("key1", new HashMap<String,String>());
         topologyMap.put("key2", new HashMap<String,String>());

File: storm-server/src/main/java/org/apache/storm/daemon/drpc/DRPC.java
Patch:
@@ -76,7 +76,7 @@ static void checkAuthorization(ReqContext reqContext, IAuthorizer auth, String o
             ThriftAccessLogger.logAccessFunction(reqContext.requestID(), reqContext.remoteAddress(), reqContext.principal(), operation, function);
         }
         if (auth != null) {
-            Map<String, String> map = new HashMap<>();
+            Map<String, Object> map = new HashMap<>();
             map.put(DRPCAuthorizerBase.FUNCTION_NAME, function);
             if (!auth.permit(reqContext, operation, map)) {
                 Principal principal = reqContext.principal();

File: storm-server/src/main/java/org/apache/storm/logging/filters/AccessLoggingFilter.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.logging.filters;
 import java.io.IOException;
 import javax.servlet.Filter;

File: external/storm-autocreds/src/main/java/org/apache/storm/hdfs/security/HdfsSecurityUtil.java
Patch:
@@ -17,11 +17,10 @@
  */
 package org.apache.storm.hdfs.security;
 
-import org.apache.storm.security.auth.kerberos.AutoTGT;
-
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.security.SecurityUtil;
 import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.storm.security.auth.kerberos.AutoTGT;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/bolt/BaseCassandraBolt.java
Patch:
@@ -102,6 +102,7 @@ public void prepare(Map stormConfig, TopologyContext topologyContext, OutputColl
             session = client.connect();
         } catch (NoHostAvailableException e) {
             outputCollector.reportError(e);
+            throw e;
         }
     }
 

File: storm-client/src/jvm/org/apache/storm/nimbus/NimbusInfo.java
Patch:
@@ -39,7 +39,7 @@ public class NimbusInfo implements Serializable {
 
     public NimbusInfo(String host, int port, boolean isLeader) {
         if (host == null) throw new NullPointerException("Host cannot be null");
-        if (port <= 0) throw new IllegalArgumentException("Port must be positive");
+        if (port < 0) throw new IllegalArgumentException("Port cannot be negative");
         this.host = host;
         this.port = port;
         this.isLeader = isLeader;

File: external/storm-kafka/src/jvm/org/apache/storm/kafka/StringScheme.java
Patch:
@@ -38,7 +38,7 @@ public List<Object> deserialize(ByteBuffer bytes) {
     public static String deserializeString(ByteBuffer string) {
         if (string.hasArray()) {
             int base = string.arrayOffset();
-            return new String(string.array(), base + string.position(), string.remaining());
+            return new String(string.array(), base + string.position(), string.remaining(), UTF8_CHARSET);
         } else {
             return new String(Utils.toByteArray(string), UTF8_CHARSET);
         }

File: storm-server/src/test/java/org/apache/storm/daemon/drpc/DRPCTest.java
Patch:
@@ -31,7 +31,6 @@
 import javax.security.auth.Subject;
 
 import org.apache.storm.Config;
-import org.apache.storm.daemon.drpc.DRPC;
 import org.apache.storm.generated.AuthorizationException;
 import org.apache.storm.generated.DRPCExceptionType;
 import org.apache.storm.generated.DRPCExecutionException;
@@ -118,7 +117,7 @@ public void testFailedBlocking() throws Exception {
     
     @Test
     public void testDequeueAfterTimeout() throws Exception {
-        long timeout = 2;
+        long timeout = 1000;
         try (DRPC server = new DRPC(null, timeout)) {
             long start = Time.currentTimeMillis();
             try {

File: external/storm-kafka/src/jvm/org/apache/storm/kafka/StringScheme.java
Patch:
@@ -38,7 +38,7 @@ public List<Object> deserialize(ByteBuffer bytes) {
     public static String deserializeString(ByteBuffer string) {
         if (string.hasArray()) {
             int base = string.arrayOffset();
-            return new String(string.array(), base + string.position(), string.remaining());
+            return new String(string.array(), base + string.position(), string.remaining(), UTF8_CHARSET);
         } else {
             return new String(Utils.toByteArray(string), UTF8_CHARSET);
         }

File: storm-server/src/test/java/org/apache/storm/daemon/drpc/DRPCTest.java
Patch:
@@ -31,7 +31,6 @@
 import javax.security.auth.Subject;
 
 import org.apache.storm.Config;
-import org.apache.storm.daemon.drpc.DRPC;
 import org.apache.storm.generated.AuthorizationException;
 import org.apache.storm.generated.DRPCExceptionType;
 import org.apache.storm.generated.DRPCExecutionException;
@@ -118,7 +117,7 @@ public void testFailedBlocking() throws Exception {
     
     @Test
     public void testDequeueAfterTimeout() throws Exception {
-        long timeout = 2;
+        long timeout = 2000;
         try (DRPC server = new DRPC(null, timeout)) {
             long start = Time.currentTimeMillis();
             try {

File: integration-test/src/test/java/org/apache/storm/st/wrapper/StormCluster.java
Patch:
@@ -41,7 +41,7 @@
 
 public class StormCluster {
     private static Logger log = LoggerFactory.getLogger(StormCluster.class);
-    private final Nimbus.Client client;
+    private final Nimbus.Iface client;
 
     public StormCluster() {
         Map conf = getConfig();
@@ -103,7 +103,7 @@ public TopologyInfo getInfo(TopologySummary topologySummary) throws TException {
         return client.getTopologyInfo(topologySummary.get_id());
     }
 
-    public Nimbus.Client getNimbusClient() {
+    public Nimbus.Iface getNimbusClient() {
         return client;
     }
 

File: examples/storm-hdfs-examples/src/main/java/org/apache/storm/hdfs/spout/HdfsSpoutTopology.java
Patch:
@@ -29,8 +29,8 @@
 import org.apache.storm.generated.ClusterSummary;
 import org.apache.storm.metric.LoggingMetricsConsumer;
 import org.apache.storm.topology.TopologyBuilder;
-import org.apache.storm.utils.NimbusClient;
 import org.apache.storm.utils.Utils;
+import org.apache.storm.utils.NimbusClient;
 import org.apache.storm.topology.base.BaseRichBolt;
 import org.apache.storm.topology.OutputFieldsDeclarer;
 import org.apache.storm.tuple.Fields;

File: examples/storm-kafka-examples/src/main/java/org/apache/storm/kafka/trident/DrpcResultsPrinter.java
Patch:
@@ -20,9 +20,9 @@
 
 import org.apache.storm.LocalDRPC;
 import org.apache.storm.generated.DistributedRPC;
-import org.apache.storm.thrift.transport.TTransportException;
-import org.apache.storm.utils.DRPCClient;
+import org.apache.thrift.transport.TTransportException;
 import org.apache.storm.utils.Utils;
+import org.apache.storm.utils.DRPCClient;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 

File: examples/storm-mongodb-examples/src/main/java/org/apache/storm/mongodb/topology/WordCounter.java
Patch:
@@ -28,8 +28,6 @@
 
 import java.util.Map;
 
-import static org.apache.storm.utils.Utils.tuple;
-
 public class WordCounter implements IBasicBolt {
     private Map<String, Integer> wordCounter = Maps.newHashMap();
 

File: examples/storm-perf/src/main/java/org/apache/storm/perf/KafkaHdfsTopo.java
Patch:
@@ -36,6 +36,7 @@
 import org.apache.storm.topology.TopologyBuilder;
 import org.apache.storm.tuple.Tuple;
 import org.apache.storm.utils.Utils;
+import org.apache.storm.utils.ObjectReader;
 
 import java.util.Map;
 import java.util.UUID;
@@ -117,7 +118,7 @@ public static StormTopology getTopology(Map config) {
 
 
   public static int getInt(Map map, Object key, int def) {
-    return Utils.getInt(Utils.get(map, key, def));
+    return ObjectReader.getInt(Utils.get(map, key, def));
   }
 
   public static String getStr(Map map, Object key) {

File: examples/storm-perf/src/main/java/org/apache/storm/perf/KafkaSpoutNullBoltTopo.java
Patch:
@@ -28,6 +28,7 @@
 import org.apache.storm.perf.utils.Helper;
 import org.apache.storm.topology.TopologyBuilder;
 import org.apache.storm.utils.Utils;
+import org.apache.storm.utils.ObjectReader;
 
 import java.util.Map;
 import java.util.UUID;
@@ -89,7 +90,7 @@ public static StormTopology getTopology(Map config) {
 
 
     public static int getInt(Map map, Object key, int def) {
-        return Utils.getInt(Utils.get(map, key, def));
+        return ObjectReader.getInt(Utils.get(map, key, def));
     }
 
     public static String getStr(Map map, Object key) {

File: examples/storm-perf/src/main/java/org/apache/storm/perf/utils/BasicMetricsCollector.java
Patch:
@@ -20,7 +20,6 @@
 
 import org.apache.storm.LocalCluster;
 import org.apache.storm.generated.Nimbus;
-import org.apache.storm.utils.NimbusClient;
 import org.apache.storm.utils.Utils;
 import org.apache.log4j.Logger;
 

File: examples/storm-perf/src/main/java/org/apache/storm/perf/utils/Helper.java
Patch:
@@ -24,8 +24,9 @@
 import org.apache.storm.generated.KillOptions;
 import org.apache.storm.generated.Nimbus;
 import org.apache.storm.generated.StormTopology;
-import org.apache.storm.utils.NimbusClient;
 import org.apache.storm.utils.Utils;
+import org.apache.storm.utils.ObjectReader;
+import org.apache.storm.utils.NimbusClient;
 
 import java.util.Map;
 
@@ -53,7 +54,7 @@ public static LocalCluster runOnLocalCluster(String topoName, StormTopology topo
     }
 
     public static int getInt(Map map, Object key, int def) {
-        return Utils.getInt(Utils.get(map, key, def));
+        return ObjectReader.getInt(Utils.get(map, key, def));
     }
 
     public static String getStr(Map map, Object key) {

File: examples/storm-redis-examples/src/main/java/org/apache/storm/redis/topology/WordCounter.java
Patch:
@@ -28,8 +28,6 @@
 
 import java.util.Map;
 
-import static org.apache.storm.utils.Utils.tuple;
-
 public class WordCounter implements IBasicBolt {
     private Map<String, Integer> wordCounter = Maps.newHashMap();
 

File: examples/storm-starter/src/jvm/org/apache/storm/starter/BlobStoreAPIWordCountTopology.java
Patch:
@@ -21,11 +21,8 @@
 import org.apache.storm.StormSubmitter;
 import org.apache.storm.blobstore.AtomicOutputStream;
 import org.apache.storm.blobstore.ClientBlobStore;
-import org.apache.storm.blobstore.InputStreamWithMeta;
-import org.apache.storm.blobstore.NimbusBlobStore;
 
 import org.apache.storm.generated.AccessControl;
-import org.apache.storm.generated.AccessControlType;
 import org.apache.storm.generated.AlreadyAliveException;
 import org.apache.storm.generated.AuthorizationException;
 import org.apache.storm.generated.InvalidTopologyException;

File: examples/storm-starter/src/jvm/org/apache/storm/starter/FastWordCountTopology.java
Patch:
@@ -23,16 +23,15 @@
 import org.apache.storm.spout.SpoutOutputCollector;
 import org.apache.storm.task.TopologyContext;
 import org.apache.storm.topology.BasicOutputCollector;
-import org.apache.storm.topology.IRichBolt;
 import org.apache.storm.topology.OutputFieldsDeclarer;
 import org.apache.storm.topology.TopologyBuilder;
 import org.apache.storm.topology.base.BaseBasicBolt;
 import org.apache.storm.topology.base.BaseRichSpout;
 import org.apache.storm.tuple.Fields;
 import org.apache.storm.tuple.Tuple;
 import org.apache.storm.tuple.Values;
-import org.apache.storm.utils.NimbusClient;
 import org.apache.storm.utils.Utils;
+import org.apache.storm.utils.NimbusClient;
 
 import java.util.HashMap;
 import java.util.Map;

File: examples/storm-starter/src/jvm/org/apache/storm/starter/InOrderDeliveryTest.java
Patch:
@@ -24,20 +24,18 @@
 import org.apache.storm.task.TopologyContext;
 import org.apache.storm.topology.BasicOutputCollector;
 import org.apache.storm.topology.FailedException;
-import org.apache.storm.topology.IRichBolt;
 import org.apache.storm.topology.OutputFieldsDeclarer;
 import org.apache.storm.topology.TopologyBuilder;
 import org.apache.storm.topology.base.BaseBasicBolt;
 import org.apache.storm.topology.base.BaseRichSpout;
 import org.apache.storm.tuple.Fields;
 import org.apache.storm.tuple.Tuple;
 import org.apache.storm.tuple.Values;
-import org.apache.storm.utils.NimbusClient;
 import org.apache.storm.utils.Utils;
+import org.apache.storm.utils.NimbusClient;
 
 import java.util.HashMap;
 import java.util.Map;
-import java.util.Random;
 
 public class InOrderDeliveryTest {
   public static class InOrderSpout extends BaseRichSpout {

File: examples/storm-starter/src/jvm/org/apache/storm/starter/MultipleLoggerTopology.java
Patch:
@@ -35,7 +35,6 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import java.util.List;
 import java.util.Map;
 
 /**

File: examples/storm-starter/src/jvm/org/apache/storm/starter/StatefulTopology.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information
@@ -15,13 +15,12 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package storm.starter;
+package org.apache.storm.starter;
 
 import org.apache.storm.Config;
 import org.apache.storm.LocalCluster;
 import org.apache.storm.LocalCluster.LocalTopology;
 import org.apache.storm.StormSubmitter;
-import org.apache.storm.generated.StormTopology;
 import org.apache.storm.starter.spout.RandomIntegerSpout;
 import org.apache.storm.state.KeyValueState;
 import org.apache.storm.task.OutputCollector;

File: examples/storm-starter/src/jvm/org/apache/storm/starter/StatefulWindowingTopology.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information
@@ -15,13 +15,12 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package storm.starter;
+package org.apache.storm.starter;
 
 import org.apache.storm.Config;
 import org.apache.storm.LocalCluster;
 import org.apache.storm.LocalCluster.LocalTopology;
 import org.apache.storm.StormSubmitter;
-import org.apache.storm.generated.StormTopology;
 import org.apache.storm.starter.bolt.PrinterBolt;
 import org.apache.storm.starter.spout.RandomIntegerSpout;
 import org.apache.storm.state.KeyValueState;

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/bolt/BaseCassandraBolt.java
Patch:
@@ -35,11 +35,9 @@
 import org.apache.storm.task.OutputCollector;
 import org.apache.storm.task.TopologyContext;
 import org.apache.storm.topology.OutputFieldsDeclarer;
-import org.apache.storm.topology.base.BaseRichBolt;
 import org.apache.storm.topology.base.BaseTickTupleAwareRichBolt;
 import org.apache.storm.tuple.Fields;
 import org.apache.storm.tuple.Tuple;
-import org.apache.storm.utils.TupleUtils;
 import org.apache.storm.utils.Utils;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/blobstore/HdfsBlobStoreImpl.java
Patch:
@@ -19,7 +19,7 @@
 
 import org.apache.storm.Config;
 import org.apache.storm.blobstore.BlobStoreFile;
-import org.apache.storm.utils.Utils;
+import org.apache.storm.utils.ObjectReader;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
@@ -139,7 +139,7 @@ public HdfsBlobStoreImpl(Path path, Map<String, Object> conf,
         }
 
         Object shouldCleanup = conf.get(Config.BLOBSTORE_CLEANUP_ENABLE);
-        if (Utils.getBoolean(shouldCleanup, false)) {
+        if (ObjectReader.getBoolean(shouldCleanup, false)) {
             LOG.debug("Starting hdfs blobstore cleaner");
             _cleanup = new TimerTask() {
                 @Override

File: external/storm-jms/src/main/java/org/apache/storm/jms/spout/JmsSpout.java
Patch:
@@ -31,6 +31,7 @@
 import javax.jms.Session;
 
 import org.apache.storm.topology.base.BaseRichSpout;
+import org.apache.storm.utils.Utils;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -40,7 +41,6 @@
 import org.apache.storm.task.TopologyContext;
 import org.apache.storm.topology.OutputFieldsDeclarer;
 import org.apache.storm.tuple.Values;
-import org.apache.storm.utils.Utils;
 
 /**
  * A Storm <code>Spout</code> implementation that listens to a JMS topic or queue

File: external/storm-jms/src/main/java/org/apache/storm/jms/trident/TridentJmsSpout.java
Patch:
@@ -33,6 +33,7 @@
 
 import org.apache.storm.jms.JmsProvider;
 import org.apache.storm.jms.JmsTupleProducer;
+import org.apache.storm.utils.Utils;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -46,7 +47,6 @@
 import org.apache.storm.tuple.Fields;
 import org.apache.storm.tuple.Values;
 import org.apache.storm.utils.RotatingMap;
-import org.apache.storm.utils.Utils;
 
 /**
  * Trident implementation of the JmsSpout

File: external/storm-kafka/src/test/org/apache/storm/kafka/KafkaUtilsTest.java
Patch:
@@ -33,6 +33,7 @@
 
 import org.apache.kafka.clients.producer.KafkaProducer;
 import org.apache.kafka.clients.producer.ProducerRecord;
+import org.apache.storm.utils.Utils;
 import org.junit.After;
 import org.junit.Assert;
 import org.junit.Before;
@@ -43,7 +44,6 @@
 
 import org.apache.storm.kafka.trident.GlobalPartitionInformation;
 import org.apache.storm.spout.SchemeAsMultiScheme;
-import org.apache.storm.utils.Utils;
 
 import com.google.common.collect.ImmutableMap;
 public class KafkaUtilsTest {

File: external/storm-kafka/src/test/org/apache/storm/kafka/TestUtils.java
Patch:
@@ -17,14 +17,12 @@
  */
 package org.apache.storm.kafka;
 
-import org.apache.storm.Config;
 import org.apache.storm.utils.Utils;
 import kafka.api.OffsetRequest;
 import kafka.javaapi.consumer.SimpleConsumer;
 import kafka.javaapi.message.ByteBufferMessageSet;
 import kafka.message.Message;
 import kafka.message.MessageAndOffset;
-import org.apache.storm.kafka.bolt.KafkaBolt;
 import org.apache.storm.kafka.trident.GlobalPartitionInformation;
 
 import java.nio.ByteBuffer;

File: external/storm-kafka/src/test/org/apache/storm/kafka/bolt/KafkaBoltTest.java
Patch:
@@ -27,8 +27,8 @@
 import org.apache.storm.tuple.Tuple;
 import org.apache.storm.tuple.TupleImpl;
 import org.apache.storm.tuple.Values;
-import org.apache.storm.utils.TupleUtils;
 import org.apache.storm.utils.Utils;
+import org.apache.storm.utils.TupleUtils;
 import com.google.common.collect.ImmutableList;
 import kafka.api.OffsetRequest;
 import kafka.api.FetchRequest;

File: flux/flux-examples/src/main/java/org/apache/storm/flux/examples/WordCounter.java
Patch:
@@ -19,7 +19,6 @@
 
 import org.apache.storm.task.TopologyContext;
 import org.apache.storm.topology.BasicOutputCollector;
-import org.apache.storm.topology.IBasicBolt;
 import org.apache.storm.topology.OutputFieldsDeclarer;
 import org.apache.storm.topology.base.BaseBasicBolt;
 import org.apache.storm.tuple.Fields;

File: integration-test/src/test/java/org/apache/storm/st/tests/window/SlidingWindowTest.java
Patch:
@@ -20,7 +20,7 @@
 import org.apache.storm.st.helper.AbstractTest;
 import org.apache.storm.st.wrapper.LogData;
 import org.apache.storm.st.wrapper.TopoWrap;
-import org.apache.storm.thrift.TException;
+import org.apache.thrift.TException;
 import org.apache.storm.st.topology.TestableTopology;
 import org.apache.storm.st.topology.window.SlidingTimeCorrectness;
 import org.apache.storm.st.topology.window.SlidingWindowCorrectness;

File: integration-test/src/test/java/org/apache/storm/st/wrapper/StormCluster.java
Patch:
@@ -26,7 +26,7 @@
 import org.apache.storm.generated.TopologySummary;
 import org.apache.storm.st.utils.AssertUtil;
 import org.apache.commons.lang.exception.ExceptionUtils;
-import org.apache.storm.thrift.TException;
+import org.apache.thrift.TException;
 import org.apache.storm.utils.NimbusClient;
 import org.apache.storm.utils.Utils;
 import org.slf4j.Logger;

File: integration-test/src/test/java/org/apache/storm/st/wrapper/TopoWrap.java
Patch:
@@ -39,7 +39,7 @@
 import org.apache.commons.lang.StringUtils;
 import org.apache.commons.lang.exception.ExceptionUtils;
 import org.apache.storm.StormSubmitter;
-import org.apache.storm.thrift.TException;
+import org.apache.thrift.TException;
 import org.apache.storm.st.topology.window.data.FromJson;
 import org.apache.storm.st.utils.StringDecorator;
 import org.apache.storm.st.utils.TimeUtil;

File: sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/utils/SerdeUtils.java
Patch:
@@ -32,7 +32,7 @@
 import org.apache.storm.sql.runtime.serde.json.JsonSerializer;
 import org.apache.storm.sql.runtime.serde.tsv.TsvScheme;
 import org.apache.storm.sql.runtime.serde.tsv.TsvSerializer;
-import org.apache.storm.utils.Utils;
+import org.apache.storm.utils.ReflectionUtils;
 
 import java.util.ArrayList;
 import java.util.HashMap;
@@ -56,7 +56,7 @@ public static Scheme getScheme(String inputFormatClass, Properties properties, L
                 Preconditions.checkArgument(isNotEmpty(schemaString), "input.avro.schema can not be empty");
                 scheme = new AvroScheme(schemaString, fieldNames);
             } else {
-                scheme = Utils.newInstance(inputFormatClass);
+                scheme = ReflectionUtils.newInstance(inputFormatClass);
             }
         } else {
             //use JsonScheme as the default scheme
@@ -80,7 +80,7 @@ public static IOutputSerializer getSerializer(String outputFormatClass, Properti
                 Preconditions.checkArgument(isNotEmpty(schemaString), "output.avro.schema can not be empty");
                 serializer = new AvroSerializer(schemaString, fieldNames);
             } else {
-                serializer = Utils.newInstance(outputFormatClass);
+                serializer = ReflectionUtils.newInstance(outputFormatClass);
             }
         } else {
             //use JsonSerializer as the default serializer

File: storm-client-misc/src/main/java/org/apache/storm/misc/metric/HttpForwardingMetricsConsumer.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information
@@ -15,7 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.storm.metric;
+package org.apache.storm.misc.metric;
 
 import java.util.Arrays;
 import java.util.Collection;

File: storm-client-misc/src/main/java/org/apache/storm/misc/metric/HttpForwardingMetricsServer.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information
@@ -15,7 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.storm.metric;
+package org.apache.storm.misc.metric;
 
 import java.io.IOException;
 import java.util.Collection;

File: storm-client/src/jvm/org/apache/storm/Thrift.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information
@@ -48,12 +48,11 @@
 import org.apache.storm.topology.IBasicBolt;
 import org.apache.storm.topology.IRichSpout;
 import org.apache.storm.topology.SpoutDeclarer;
+import org.apache.storm.utils.Utils;
 import org.json.simple.JSONValue;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import org.apache.storm.utils.Utils;
-import org.apache.storm.grouping.CustomStreamGrouping;
 import org.apache.storm.topology.TopologyBuilder;
 
 public class Thrift {

File: storm-client/src/jvm/org/apache/storm/blobstore/ClientBlobStore.java
Patch:
@@ -24,8 +24,8 @@
 import org.apache.storm.generated.KeyAlreadyExistsException;
 import org.apache.storm.generated.KeyNotFoundException;
 import org.apache.storm.utils.ConfigUtils;
-import org.apache.storm.utils.NimbusClient;
 import org.apache.storm.utils.Utils;
+import org.apache.storm.utils.NimbusClient;
 
 import java.util.Iterator;
 import java.util.Map;

File: storm-client/src/jvm/org/apache/storm/blobstore/NimbusBlobStore.java
Patch:
@@ -25,8 +25,8 @@
 import org.apache.storm.generated.SettableBlobMeta;
 import org.apache.storm.generated.KeyAlreadyExistsException;
 import org.apache.storm.generated.KeyNotFoundException;
+import org.apache.storm.utils.ObjectReader;
 import org.apache.storm.utils.NimbusClient;
-import org.apache.storm.utils.Utils;
 import org.apache.thrift.TException;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -267,7 +267,7 @@ public void close() throws IOException {
     public void prepare(Map conf) {
         this.client = NimbusClient.getConfiguredClient(conf);
         if (conf != null) {
-            this.bufferSize = Utils.getInt(conf.get(Config.STORM_BLOBSTORE_INPUTSTREAM_BUFFER_SIZE_BYTES), bufferSize);
+            this.bufferSize = ObjectReader.getInt(conf.get(Config.STORM_BLOBSTORE_INPUTSTREAM_BUFFER_SIZE_BYTES), bufferSize);
         }
     }
 
@@ -403,7 +403,7 @@ public boolean setClient(Map conf, NimbusClient client) {
         }
         this.client = client;
         if (conf != null) {
-            this.bufferSize = Utils.getInt(conf.get(Config.STORM_BLOBSTORE_INPUTSTREAM_BUFFER_SIZE_BYTES), bufferSize);
+            this.bufferSize = ObjectReader.getInt(conf.get(Config.STORM_BLOBSTORE_INPUTSTREAM_BUFFER_SIZE_BYTES), bufferSize);
         }
         return true;
     }

File: storm-client/src/jvm/org/apache/storm/clojure/ClojureBolt.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information
@@ -26,12 +26,13 @@
 import org.apache.storm.topology.OutputFieldsDeclarer;
 import org.apache.storm.tuple.Fields;
 import org.apache.storm.tuple.Tuple;
-import org.apache.storm.utils.Utils;
 import clojure.lang.IFn;
 import clojure.lang.PersistentArrayMap;
 import clojure.lang.Keyword;
 import clojure.lang.Symbol;
 import clojure.lang.RT;
+import org.apache.storm.utils.Utils;
+
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;

File: storm-client/src/jvm/org/apache/storm/clojure/ClojureSpout.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: storm-client/src/jvm/org/apache/storm/cluster/ClusterUtils.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: storm-client/src/jvm/org/apache/storm/cluster/PaceMakerStateStorage.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information
@@ -20,7 +20,6 @@
 import java.util.ArrayList;
 import java.util.HashSet;
 import java.util.List;
-import java.util.Map;
 import org.apache.curator.framework.state.ConnectionStateListener;
 import org.apache.storm.callback.ZKStateChangedCallback;
 import org.apache.storm.generated.*;

File: storm-client/src/jvm/org/apache/storm/cluster/PaceMakerStateStorageFactory.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: storm-client/src/jvm/org/apache/storm/cluster/ZKStateStorageFactory.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: storm-client/src/jvm/org/apache/storm/coordination/BatchBoltExecutor.java
Patch:
@@ -26,6 +26,7 @@
 import org.apache.storm.topology.OutputFieldsDeclarer;
 import org.apache.storm.tuple.Tuple;
 import org.apache.storm.utils.Utils;
+
 import java.util.HashMap;
 import java.util.Map;
 import org.slf4j.Logger;

File: storm-client/src/jvm/org/apache/storm/coordination/BatchOutputCollector.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information

File: storm-client/src/jvm/org/apache/storm/daemon/Acker.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information
@@ -22,13 +22,12 @@
 import org.apache.storm.task.TopologyContext;
 import org.apache.storm.tuple.Tuple;
 import org.apache.storm.tuple.Values;
+import org.apache.storm.utils.Utils;
 import org.apache.storm.utils.RotatingMap;
 import org.apache.storm.utils.TupleUtils;
-import org.apache.storm.utils.Utils;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import java.util.List;
 import java.util.Map;
 
 public class Acker implements IBolt {

File: storm-client/src/jvm/org/apache/storm/daemon/GrouperFactory.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information
@@ -31,8 +31,8 @@
 import org.apache.storm.grouping.ShuffleGrouping;
 import org.apache.storm.task.WorkerTopologyContext;
 import org.apache.storm.tuple.Fields;
-import org.apache.storm.utils.TupleUtils;
 import org.apache.storm.utils.Utils;
+import org.apache.storm.utils.TupleUtils;
 
 import java.util.ArrayList;
 import java.util.Collections;
@@ -107,7 +107,6 @@ public static LoadAwareCustomStreamGrouping mkGrouper(WorkerTopologyContext cont
         }
     }
 
-
     /**
      * A bridge between CustomStreamGrouping and LoadAwareCustomStreamGrouping
      */

File: storm-client/src/jvm/org/apache/storm/daemon/Task.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information
@@ -188,7 +188,7 @@ private TopologyContext mkTopologyContext(StormTopology topology) throws IOExcep
             workerData.getTopologyId(),
             ConfigUtils.supervisorStormResourcesPath(
                     ConfigUtils.supervisorStormDistRoot(conf, workerData.getTopologyId())),
-            ConfigUtils.workerPidsRoot(conf, workerData.getWorkerId()),
+                    ConfigUtils.workerPidsRoot(conf, workerData.getWorkerId()),
             taskId,
             workerData.getPort(), workerData.getTaskIds(),
             workerData.getDefaultSharedResources(),

File: storm-client/src/jvm/org/apache/storm/daemon/worker/LogConfigManager.java
Patch:
@@ -1,14 +1,14 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information
  * regarding copyright ownership.  The ASF licenses this file
  * to you under the Apache License, Version 2.0 (the
  * "License"); you may not use this file except in compliance
  * with the License.  You may obtain a copy of the License at
- * <p>
+ *
  * http://www.apache.org/licenses/LICENSE-2.0
- * <p>
+ *
  * Unless required by applicable law or agreed to in writing, software
  * distributed under the License is distributed on an "AS IS" BASIS,
  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.

File: storm-client/src/jvm/org/apache/storm/drpc/PrepareRequest.java
Patch:
@@ -26,6 +26,7 @@
 import org.apache.storm.tuple.Values;
 import java.util.Map;
 import java.util.Random;
+
 import org.apache.storm.utils.Utils;
 
 

File: storm-client/src/jvm/org/apache/storm/drpc/ReturnResults.java
Patch:
@@ -25,8 +25,9 @@
 import org.apache.storm.topology.OutputFieldsDeclarer;
 import org.apache.storm.topology.base.BaseRichBolt;
 import org.apache.storm.tuple.Tuple;
+import org.apache.storm.utils.ObjectReader;
 import org.apache.storm.utils.ServiceRegistry;
-import org.apache.storm.utils.Utils;
+
 import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.List;
@@ -70,7 +71,7 @@ public void execute(Tuple input) {
                  return;
             }
             final String host = (String) retMap.get("host");
-            final int port = Utils.getInt(retMap.get("port"));
+            final int port = ObjectReader.getInt(retMap.get("port"));
             String id = (String) retMap.get("id");
             DistributedRPCInvocations.Iface client;
             if (local) {

File: storm-client/src/jvm/org/apache/storm/executor/ExecutorTransfer.java
Patch:
@@ -26,7 +26,7 @@
 import org.apache.storm.tuple.Tuple;
 import org.apache.storm.utils.DisruptorQueue;
 import org.apache.storm.utils.MutableObject;
-import org.apache.storm.utils.Utils;
+import org.apache.storm.utils.ObjectReader;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -50,7 +50,7 @@ public ExecutorTransfer(WorkerState workerData, DisruptorQueue batchTransferQueu
         this.stormConf = stormConf;
         this.serializer = new KryoTupleSerializer(stormConf, workerData.getWorkerTopologyContext());
         this.cachedEmit = new MutableObject(new ArrayList<>());
-        this.isDebug = Utils.getBoolean(stormConf.get(Config.TOPOLOGY_DEBUG), false);
+        this.isDebug = ObjectReader.getBoolean(stormConf.get(Config.TOPOLOGY_DEBUG), false);
     }
 
     public void transfer(int task, Tuple tuple) {

File: storm-client/src/jvm/org/apache/storm/executor/bolt/BoltExecutor.java
Patch:
@@ -33,9 +33,9 @@
 import org.apache.storm.task.TopologyContext;
 import org.apache.storm.tuple.TupleImpl;
 import org.apache.storm.utils.ConfigUtils;
+import org.apache.storm.utils.Utils;
 import org.apache.storm.utils.DisruptorQueue;
 import org.apache.storm.utils.Time;
-import org.apache.storm.utils.Utils;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 

File: storm-client/src/jvm/org/apache/storm/executor/bolt/BoltOutputCollectorImpl.java
Patch:
@@ -33,8 +33,8 @@
 import org.apache.storm.tuple.Tuple;
 import org.apache.storm.tuple.TupleImpl;
 import org.apache.storm.tuple.Values;
-import org.apache.storm.utils.Time;
 import org.apache.storm.utils.Utils;
+import org.apache.storm.utils.Time;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 

File: storm-client/src/jvm/org/apache/storm/executor/spout/SpoutOutputCollectorImpl.java
Patch:
@@ -25,9 +25,9 @@
 import org.apache.storm.tuple.MessageId;
 import org.apache.storm.tuple.TupleImpl;
 import org.apache.storm.tuple.Values;
+import org.apache.storm.utils.Utils;
 import org.apache.storm.utils.MutableLong;
 import org.apache.storm.utils.RotatingMap;
-import org.apache.storm.utils.Utils;
 
 import java.util.ArrayList;
 import java.util.List;

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java
Patch:
@@ -497,7 +497,7 @@ public long getOffsetsCommitPeriodMs() {
     }
 
     public boolean isConsumerAutoCommitMode() {
-        return kafkaProps.get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG) == null     // default is true
+        return kafkaProps.get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG) == null     // default is false
                 || Boolean.valueOf((String)kafkaProps.get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG));
     }
 

File: examples/storm-starter/src/jvm/org/apache/storm/starter/JoinBoltExample.java
Patch:
@@ -53,7 +53,6 @@ public static void main(String[] args) throws Exception {
         builder.setBolt("printer", new PrinterBolt() ).shuffleGrouping("joiner");
 
         Config conf = new Config();
-        conf.setDebug(true);
 
         LocalCluster cluster = new LocalCluster();
         cluster.submitTopology("join-example", conf, builder.createTopology());

File: storm-core/test/jvm/org/apache/storm/bolt/TestJoinBolt.java
Patch:
@@ -31,6 +31,7 @@
 import java.util.Arrays;
 import java.util.HashMap;
 import java.util.List;
+import java.util.Collection;
 
 public class TestJoinBolt {
     String[] userFields = {"userId", "name", "city"};
@@ -328,7 +329,7 @@ public MockCollector() {
         }
 
         @Override
-        public List<Integer> emit(List<Object> tuple) {
+        public List<Integer> emit(Collection<Tuple> anchors, List<Object> tuple) {
             actualResults.add(tuple);
             return null;
         }

File: storm-core/src/jvm/org/apache/storm/blobstore/BlobStoreUtils.java
Patch:
@@ -128,6 +128,7 @@ public static boolean downloadMissingBlob(Map<String, Object> conf, BlobStore bl
             if(isSuccess) {
                 break;
             }
+            LOG.debug("Download blob key: {}, NimbusInfo {}", key, nimbusInfo);
             try(NimbusClient client = new NimbusClient(conf, nimbusInfo.getHost(), nimbusInfo.getPort(), null)) {
                 rbm = client.getClient().getBlobMeta(key);
                 remoteBlobStore = new NimbusBlobStore();

File: storm-core/src/jvm/org/apache/storm/blobstore/BlobSynchronizer.java
Patch:
@@ -82,6 +82,7 @@ public synchronized void syncBlobs() {
             for (String key : keySetToDownload) {
                 try {
                     Set<NimbusInfo> nimbusInfoSet = BlobStoreUtils.getNimbodesWithLatestSequenceNumberOfBlob(zkClient, key);
+                    LOG.debug("syncBlobs, key: {}, nimbusInfoSet: {}", key, nimbusInfoSet);
                     if (BlobStoreUtils.downloadMissingBlob(conf, blobStore, key, nimbusInfoSet)) {
                         BlobStoreUtils.createStateInZookeeper(conf, key, nimbusInfo);
                     }

File: storm-core/src/jvm/org/apache/storm/daemon/nimbus/Nimbus.java
Patch:
@@ -1296,7 +1296,9 @@ private void waitForDesiredCodeReplication(Map<String, Object> topoConf, String
                             minReplicationCount, maxWaitTime, confCount, codeCount, jarCount);
                     return;
                 }
-                LOG.info("WAITING... {} <? {} {} {}", minReplicationCount, jarCount, codeCount, confCount);
+                LOG.debug("Checking if I am still the leader");
+                assertIsLeader();
+                LOG.info("WAITING... storm-id {}, {} <? {} {} {}", topoId, minReplicationCount, jarCount, codeCount, confCount);
                 LOG.info("WAITING... {} <? {}", totalWaitTime, maxWaitTime);
                 Time.sleepSecs(1);
                 totalWaitTime++;

File: external/storm-kafka/src/test/org/apache/storm/kafka/bolt/KafkaBoltTest.java
Patch:
@@ -97,15 +97,15 @@ private void setupKafkaConsumer() {
     }
 
     @Test
-    public void shouldAcknowledgeTickTuples() throws Exception {
+    public void shouldNotAcknowledgeTickTuples() throws Exception {
         // Given
         Tuple tickTuple = mockTickTuple();
 
         // When
         bolt.execute(tickTuple);
 
         // Then
-        verify(collector).ack(tickTuple);
+        verify(collector, never()).ack(tickTuple);
     }
 
     @Test

File: storm-core/src/jvm/org/apache/storm/daemon/supervisor/ReadClusterState.java
Patch:
@@ -155,7 +155,7 @@ public synchronized void run() {
                         NodeInfo ni = req.get_nodeInfo();
                         if (host.equals(ni.get_node())) {
                             Long port = ni.get_port().iterator().next();
-                            Set<TopoProfileAction> actions = filtered.get(port);
+                            Set<TopoProfileAction> actions = filtered.get(port.intValue());
                             if (actions == null) {
                                 actions = new HashSet<>();
                                 filtered.put(port.intValue(), actions);

File: storm-core/src/jvm/org/apache/storm/utils/Utils.java
Patch:
@@ -356,6 +356,7 @@ public static void sleep(long millis) {
         try {
             Time.sleep(millis);
         } catch(InterruptedException e) {
+            Thread.currentThread().interrupt();
             throw new RuntimeException(e);
         }
     }

File: examples/storm-starter/src/jvm/org/apache/storm/starter/JoinBoltExample.java
Patch:
@@ -31,7 +31,7 @@
 import java.util.concurrent.TimeUnit;
 
 public class JoinBoltExample {
-    public static void main(String[] args) {
+    public static void main(String[] args) throws Exception {
 
         FeederSpout genderSpout = new FeederSpout(new Fields("id", "gender"));
         FeederSpout ageSpout = new FeederSpout(new Fields("id", "age"));

File: storm-core/src/jvm/org/apache/storm/pacemaker/Pacemaker.java
Patch:
@@ -190,8 +190,8 @@ private HBMessage getPulse(String path, boolean authenticated) {
     private HBMessage deletePath(String path) {
         String prefix = path.endsWith("/") ? path : (path + "/");
         for (String key : heartbeats.keySet()) {
-            key = key + "/";
-            if (key.indexOf(prefix) == 0)
+            String checkKey = key + "/";
+            if (checkKey.indexOf(prefix) == 0)
                 deletePulseId(key);
         }
         return new HBMessage(HBServerMessageType.DELETE_PATH_RESPONSE, null);

File: storm-core/src/jvm/org/apache/storm/pacemaker/Pacemaker.java
Patch:
@@ -190,6 +190,7 @@ private HBMessage getPulse(String path, boolean authenticated) {
     private HBMessage deletePath(String path) {
         String prefix = path.endsWith("/") ? path : (path + "/");
         for (String key : heartbeats.keySet()) {
+            key = key + "/";
             if (key.indexOf(prefix) == 0)
                 deletePulseId(key);
         }

File: storm-core/src/jvm/org/apache/storm/executor/error/ReportErrorAndDie.java
Patch:
@@ -39,7 +39,8 @@ public void uncaughtException(Thread t, Throwable e) {
             LOG.error("Error while reporting error to cluster, proceeding with shutdown", ex);
         }
         if (Utils.exceptionCauseIsInstanceOf(InterruptedException.class, e)
-                || Utils.exceptionCauseIsInstanceOf(java.io.InterruptedIOException.class, e)) {
+                || (Utils.exceptionCauseIsInstanceOf(java.io.InterruptedIOException.class, e)
+                    && !Utils.exceptionCauseIsInstanceOf(java.net.SocketTimeoutException.class, e))) {
             LOG.info("Got interrupted exception shutting thread down...");
         } else {
             suicideFn.run();

File: storm-core/test/jvm/org/apache/storm/daemon/supervisor/ContainerTest.java
Patch:
@@ -216,7 +216,7 @@ public void testSetup() throws Exception {
         verify(ops).createSymlink(new File(workerRoot, "artifacts"), workerArtifacts);
         
         //Create links to blobs 
-        verify(ops).createSymlink(new File(workerRoot, "resources"), new File(distRoot, "resources"));
+        verify(ops, never()).createSymlink(new File(workerRoot, "resources"), new File(distRoot, "resources"));
     }
     
     @Test

File: external/sql/storm-sql-core/src/jvm/org/apache/storm/sql/compiler/RexNodeToBlockStatementCompiler.java
Patch:
@@ -72,7 +72,7 @@ public BlockStatement compile(final RexProgram program) {
     final ParameterExpression outputValues_ =
         Expressions.parameter(Object[].class, "outputValues");
     final JavaTypeFactoryImpl javaTypeFactory =
-        new JavaTypeFactoryImpl(rexBuilder.getTypeFactory().getTypeSystem());
+        new StormSqlTypeFactoryImpl(rexBuilder.getTypeFactory().getTypeSystem());
 
     final RexToLixTranslator.InputGetter inputGetter =
             new RexToLixTranslator.InputGetterImpl(

File: external/sql/storm-sql-core/src/jvm/org/apache/storm/sql/StormSqlImpl.java
Patch:
@@ -253,6 +253,9 @@ private List<FieldInfo> updateSchema(SqlCreateTable n) {
       fields.add(new FieldInfo(col.name(), javaType, isPrimary));
     }
 
+    if (n.parallelism() != null) {
+      builder.parallelismHint(n.parallelism());
+    }
     Table table = builder.build();
     schema.add(n.tableName(), table);
     return fields;

File: external/sql/storm-sql-core/src/jvm/org/apache/storm/sql/compiler/CompilerUtil.java
Patch:
@@ -131,7 +131,7 @@ public Statistic getStatistic() {
 
         @Override
         public Schema.TableType getJdbcTableType() {
-          return Schema.TableType.TABLE;
+          return Schema.TableType.STREAM;
         }
       };
 
@@ -153,7 +153,7 @@ public Statistic getStatistic() {
 
         @Override
         public Schema.TableType getJdbcTableType() {
-          return Schema.TableType.TABLE;
+          return Schema.TableType.STREAM;
         }
       };
     }

File: external/sql/storm-sql-core/src/jvm/org/apache/storm/sql/compiler/backends/standalone/PostOrderRelNodeVisitor.java
Patch:
@@ -15,7 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.storm.sql.compiler;
+package org.apache.storm.sql.compiler.backends.standalone;
 
 import org.apache.calcite.rel.RelNode;
 import org.apache.calcite.rel.core.*;

File: external/sql/storm-sql-core/src/jvm/org/apache/storm/sql/compiler/backends/standalone/RelNodeCompiler.java
Patch:
@@ -18,7 +18,6 @@
 package org.apache.storm.sql.compiler.backends.standalone;
 
 import com.google.common.base.Joiner;
-import com.google.common.primitives.Primitives;
 import org.apache.calcite.adapter.java.JavaTypeFactory;
 import org.apache.calcite.linq4j.tree.BlockStatement;
 import org.apache.calcite.plan.RelOptUtil;
@@ -38,9 +37,7 @@
 import org.apache.calcite.schema.impl.AggregateFunctionImpl;
 import org.apache.calcite.sql.SqlAggFunction;
 import org.apache.calcite.sql.validate.SqlUserDefinedAggFunction;
-import org.apache.storm.sql.compiler.PostOrderRelNodeVisitor;
 import org.apache.storm.sql.compiler.RexNodeToBlockStatementCompiler;
-import org.apache.storm.tuple.Values;
 
 import java.io.PrintWriter;
 import java.io.StringWriter;

File: external/sql/storm-sql-core/src/test/org/apache/storm/sql/compiler/TestExprSemantic.java
Patch:
@@ -18,7 +18,7 @@
 package org.apache.storm.sql.compiler;
 
 import com.google.common.base.Function;
-import org.apache.calcite.avatica.util.ByteString;
+import org.apache.storm.sql.compiler.backends.standalone.TestCompilerUtils;
 import org.apache.storm.tuple.Values;
 import com.google.common.base.Joiner;
 import com.google.common.collect.Lists;
@@ -398,7 +398,7 @@ private Values testExpr(List<String> exprs) throws Exception {
         " WHERE ID > 0 AND ID < 2";
     TestCompilerUtils.CalciteState state = TestCompilerUtils.sqlOverDummyTable(sql);
     PlanCompiler compiler = new PlanCompiler(typeFactory);
-    AbstractValuesProcessor proc = compiler.compile(state.tree);
+    AbstractValuesProcessor proc = compiler.compile(state.tree());
     Map<String, DataSource> data = new HashMap<>();
     data.put("FOO", new TestUtils.MockDataSource());
     List<Values> values = new ArrayList<>();

File: external/sql/storm-sql-core/src/test/org/apache/storm/sql/compiler/backends/standalone/TestPlanCompiler.java
Patch:
@@ -23,7 +23,6 @@
 import org.apache.calcite.jdbc.JavaTypeFactoryImpl;
 import org.apache.calcite.rel.type.RelDataTypeSystem;
 import org.apache.storm.sql.TestUtils;
-import org.apache.storm.sql.compiler.TestCompilerUtils;
 import org.apache.storm.sql.runtime.AbstractValuesProcessor;
 import org.apache.storm.sql.runtime.ChannelHandler;
 import org.apache.storm.sql.runtime.DataSource;

File: external/sql/storm-sql-core/src/test/org/apache/storm/sql/compiler/backends/standalone/TestRelNodeCompiler.java
Patch:
@@ -22,7 +22,6 @@
 import org.apache.calcite.rel.logical.LogicalFilter;
 import org.apache.calcite.rel.logical.LogicalProject;
 import org.apache.calcite.rel.type.RelDataTypeSystem;
-import org.apache.storm.sql.compiler.TestCompilerUtils;
 import org.junit.Assert;
 import org.junit.Test;
 

File: external/storm-opentsdb/src/main/java/org/apache/storm/opentsdb/bolt/OpenTsdbBolt.java
Patch:
@@ -59,7 +59,7 @@ public class OpenTsdbBolt extends BaseRichBolt {
     private static final Logger LOG = LoggerFactory.getLogger(OpenTsdbBolt.class);
 
     private final OpenTsdbClient.Builder openTsdbClientBuilder;
-    private final List<ITupleOpenTsdbDatapointMapper> tupleOpenTsdbDatapointMappers;
+    private final List<? extends ITupleOpenTsdbDatapointMapper> tupleOpenTsdbDatapointMappers;
     private int batchSize;
     private int flushIntervalInSeconds;
     private boolean failTupleForFailedMetrics;
@@ -74,7 +74,7 @@ public OpenTsdbBolt(OpenTsdbClient.Builder openTsdbClientBuilder, ITupleOpenTsdb
         this.tupleOpenTsdbDatapointMappers = Collections.singletonList(tupleOpenTsdbDatapointMapper);
     }
 
-    public OpenTsdbBolt(OpenTsdbClient.Builder openTsdbClientBuilder, List<ITupleOpenTsdbDatapointMapper> tupleOpenTsdbDatapointMappers) {
+    public OpenTsdbBolt(OpenTsdbClient.Builder openTsdbClientBuilder, List<? extends ITupleOpenTsdbDatapointMapper> tupleOpenTsdbDatapointMappers) {
         this.openTsdbClientBuilder = openTsdbClientBuilder;
         this.tupleOpenTsdbDatapointMappers = tupleOpenTsdbDatapointMappers;
     }

File: external/storm-opentsdb/src/main/java/org/apache/storm/opentsdb/trident/OpenTsdbStateFactory.java
Patch:
@@ -18,6 +18,7 @@
  */
 package org.apache.storm.opentsdb.trident;
 
+import org.apache.storm.opentsdb.bolt.ITupleOpenTsdbDatapointMapper;
 import org.apache.storm.opentsdb.bolt.TupleOpenTsdbDatapointMapper;
 import org.apache.storm.opentsdb.client.OpenTsdbClient;
 import org.apache.storm.task.IMetricsContext;
@@ -33,9 +34,9 @@
 public class OpenTsdbStateFactory implements StateFactory {
 
     private OpenTsdbClient.Builder builder;
-    private final List<TupleOpenTsdbDatapointMapper> tridentTupleOpenTsdbDatapointMappers;
+    private final List<? extends ITupleOpenTsdbDatapointMapper> tridentTupleOpenTsdbDatapointMappers;
 
-    public OpenTsdbStateFactory(OpenTsdbClient.Builder builder, List<TupleOpenTsdbDatapointMapper> tridentTupleOpenTsdbDatapointMappers) {
+    public OpenTsdbStateFactory(OpenTsdbClient.Builder builder, List<? extends ITupleOpenTsdbDatapointMapper> tridentTupleOpenTsdbDatapointMappers) {
         this.builder = builder;
         this.tridentTupleOpenTsdbDatapointMappers = tridentTupleOpenTsdbDatapointMappers;
     }

File: external/storm-opentsdb/src/main/java/org/apache/storm/opentsdb/client/OpenTsdbClient.java
Patch:
@@ -120,7 +120,7 @@ public static class Builder implements Serializable {
         private boolean enableChunkedEncoding;
         private ResponseType responseType = ResponseType.None;
 
-        protected Builder(String url) {
+        public Builder(String url) {
             this.url = url;
         }
 

File: examples/storm-starter/src/jvm/org/apache/storm/starter/streams/StateQueryExample.java
Patch:
@@ -25,7 +25,6 @@
 import org.apache.storm.streams.Stream;
 import org.apache.storm.streams.StreamBuilder;
 import org.apache.storm.streams.StreamState;
-import org.apache.storm.streams.operations.aggregators.Count;
 import org.apache.storm.streams.operations.mappers.ValueMapper;
 import org.apache.storm.task.TopologyContext;
 import org.apache.storm.testing.TestWordSpout;
@@ -56,8 +55,7 @@ public static void main(String[] args) throws Exception {
         StreamBuilder builder = new StreamBuilder();
         StreamState<String, Long> ss = builder.newStream(new TestWordSpout(), new ValueMapper<String>(0))
                 .mapToPair(w -> Pair.of(w, 1))
-                .groupByKey()
-                .updateStateByKey(new Count<>());
+                .updateStateByKey(0L, (count, val) -> count + 1);
 
         /*
          * A stream of words emitted by the QuerySpout is used as

File: storm-core/src/jvm/org/apache/storm/streams/WindowNode.java
Patch:
@@ -29,6 +29,7 @@ public class WindowNode extends Node {
 
     WindowNode(Window<?, ?> windowParams, String outputStream, Fields outputFields) {
         super(outputStream, outputFields);
+        setWindowed(true);
         this.windowParams = windowParams;
     }
 

File: storm-core/src/jvm/org/apache/storm/streams/operations/Reducer.java
Patch:
@@ -23,7 +23,7 @@
  *
  * @param <T> the type of the arguments and the result
  */
-public interface Reducer<T> extends Operation {
+public interface Reducer<T> extends BiFunction<T, T, T> {
     /**
      * Applies this function to the given arguments.
      *

File: storm-core/src/jvm/org/apache/storm/streams/processors/BranchProcessor.java
Patch:
@@ -23,15 +23,15 @@
 import java.util.Map;
 
 public class BranchProcessor<T> extends BaseProcessor<T> {
-    private final Map<Predicate<T>, String> predicateToStream = new HashMap<>();
+    private final Map<Predicate<? super T>, String> predicateToStream = new HashMap<>();
 
-    public void addPredicate(Predicate<T> predicate, String stream) {
+    public void addPredicate(Predicate<? super T> predicate, String stream) {
         predicateToStream.put(predicate, stream);
     }
 
     @Override
     public void execute(T input) {
-        for (Map.Entry<Predicate<T>, String> entry : predicateToStream.entrySet()) {
+        for (Map.Entry<Predicate<? super T>, String> entry : predicateToStream.entrySet()) {
             if (entry.getKey().test(input)) {
                 context.forward(input, entry.getValue());
                 break;

File: storm-core/test/jvm/org/apache/storm/streams/WindowedProcessorBoltTest.java
Patch:
@@ -77,7 +77,7 @@ public void testEmit() throws Exception {
         Mockito.verify(mockOutputCollector, Mockito.times(2)).emit(os.capture(), values.capture());
         assertEquals("outputstream", os.getAllValues().get(0));
         assertEquals(new Values(3L), values.getAllValues().get(0));
-        assertEquals("outputstream", os.getAllValues().get(1));
+        assertEquals("outputstream__punctuation", os.getAllValues().get(1));
         assertEquals(new Values(WindowNode.PUNCTUATION), values.getAllValues().get(1));
     }
 

File: storm-core/src/jvm/org/apache/storm/topology/StatefulWindowedBoltExecutor.java
Patch:
@@ -207,14 +207,14 @@ public void onExpiry(List<Tuple> events) {
             }
 
             @Override
-            public void onActivation(List<Tuple> events, List<Tuple> newEvents, List<Tuple> expired) {
+            public void onActivation(List<Tuple> events, List<Tuple> newEvents, List<Tuple> expired, Long timestamp) {
                 if (isRecovering()) {
                     String msg = String.format("Unexpected activation with events %s, newEvents %s, expired %s in recovering state. " +
                                                        "recoveryStates %s ", events, newEvents, expired, recoveryStates);
                     LOG.error(msg);
                     throw new IllegalStateException(msg);
                 } else {
-                    parentListener.onActivation(events, newEvents, expired);
+                    parentListener.onActivation(events, newEvents, expired, timestamp);
                     updateWindowState(expired, newEvents);
                 }
             }

File: storm-core/src/jvm/org/apache/storm/topology/WindowedBoltExecutor.java
Patch:
@@ -327,9 +327,9 @@ public void onExpiry(List<Tuple> tuples) {
             }
 
             @Override
-            public void onActivation(List<Tuple> tuples, List<Tuple> newTuples, List<Tuple> expiredTuples) {
+            public void onActivation(List<Tuple> tuples, List<Tuple> newTuples, List<Tuple> expiredTuples, Long timestamp) {
                 windowedOutputCollector.setContext(tuples);
-                bolt.execute(new TupleWindowImpl(tuples, newTuples, expiredTuples));
+                bolt.execute(new TupleWindowImpl(tuples, newTuples, expiredTuples, timestamp));
             }
         };
     }

File: storm-core/src/jvm/org/apache/storm/trident/windowing/AbstractTridentWindowManager.java
Patch:
@@ -121,7 +121,7 @@ public void onExpiry(List<T> expiredEvents) {
         }
 
         @Override
-        public void onActivation(List<T> events, List<T> newEvents, List<T> expired) {
+        public void onActivation(List<T> events, List<T> newEvents, List<T> expired, Long timestamp) {
             LOG.debug("onActivation is invoked with events size: [{}]", events.size());
             // trigger occurred, create an aggregation and keep them in store
             int currentTriggerId = triggerId.incrementAndGet();

File: storm-core/src/jvm/org/apache/storm/windowing/WatermarkTimeEvictionPolicy.java
Patch:
@@ -45,7 +45,6 @@ public WatermarkTimeEvictionPolicy(int windowLength) {
      */
     public WatermarkTimeEvictionPolicy(int windowLength, int lag) {
         super(windowLength);
-        referenceTime = 0L;
         this.lag = lag;
     }
 
@@ -58,7 +57,8 @@ public WatermarkTimeEvictionPolicy(int windowLength, int lag) {
      */
     @Override
     public Action evict(Event<T> event) {
-        long diff = referenceTime - event.getTimestamp();
+        long referenceTime = evictionContext.getReferenceTime() != null ? evictionContext.getReferenceTime() : 0L;
+        long diff =  referenceTime - event.getTimestamp();
         if (diff < -lag) {
             return Action.STOP;
         } else if (diff < 0) {

File: storm-core/src/jvm/org/apache/storm/windowing/WindowLifecycleListener.java
Patch:
@@ -37,6 +37,7 @@ public interface WindowLifecycleListener<T> {
      * @param events the list of current events in the window.
      * @param newEvents the newly added events since last activation.
      * @param expired the expired events since last activation.
+     * @param referenceTime the reference (event or processing) time that resulted in activation
      */
-    void onActivation(List<T> events, List<T> newEvents, List<T> expired);
+    void onActivation(List<T> events, List<T> newEvents, List<T> expired, Long referenceTime);
 }

File: storm-core/src/jvm/org/apache/storm/windowing/WindowManager.java
Patch:
@@ -142,7 +142,7 @@ public boolean onTrigger() {
         if (!events.isEmpty()) {
             prevWindowEvents.addAll(windowEvents);
             LOG.debug("invoking windowLifecycleListener onActivation, [{}] events in window.", events.size());
-            windowLifecycleListener.onActivation(events, newEvents, expired);
+            windowLifecycleListener.onActivation(events, newEvents, expired, evictionPolicy.getContext().getReferenceTime());
         } else {
             LOG.debug("No events in the window, skipping onActivation");
         }

File: storm-core/test/jvm/org/apache/storm/windowing/WindowManagerTest.java
Patch:
@@ -60,7 +60,7 @@ public void onExpiry(List<Integer> events) {
         }
 
         @Override
-        public void onActivation(List<Integer> events, List<Integer> newEvents, List<Integer> expired) {
+        public void onActivation(List<Integer> events, List<Integer> newEvents, List<Integer> expired, Long timestamp) {
             onActivationEvents = events;
             allOnActivationEvents.add(events);
             onActivationNewEvents = newEvents;

File: integration-test/src/test/java/org/apache/storm/st/wrapper/TopoWrap.java
Patch:
@@ -256,7 +256,7 @@ public int hashCode() {
         public ExecutorURL(String componentId, String host, int logViewerPort, int executorPort, String topoId) throws MalformedURLException {
             String sep = "%2F"; //hex of "/"
             String viewUrlStr = String.format("http://%s:%s/log?file=", host, logViewerPort);
-            String downloadUrlStr = String.format("http://%s:%s/download", host, logViewerPort);
+            String downloadUrlStr = String.format("http://%s:%s/download?file=%%2F", host, logViewerPort);
             viewUrl = new URL(String.format("%s/%s%s%d%sworker.log", viewUrlStr, topoId, sep, executorPort, sep));
             downloadUrl = new URL(String.format("%s/%s%s%d%sworker.log", downloadUrlStr, topoId, sep, executorPort, sep));
             this.componentId = componentId;

File: storm-core/src/jvm/org/apache/storm/messaging/netty/StormClientHandler.java
Patch:
@@ -60,7 +60,6 @@ public void messageReceived(ChannelHandlerContext ctx, MessageEvent event) {
                 //This should be the metrics, and there should only be one of them
                 List<TaskMessage> list = (List<TaskMessage>)message;
                 if (list.size() < 1) throw new RuntimeException("Didn't see enough load metrics ("+client.getDstAddress()+") "+list);
-                if (list.size() != 1) LOG.debug("Messages are not being delivered fast enough, got "+list.size()+" metrics messages at once("+client.getDstAddress()+")");
                 TaskMessage tm = ((List<TaskMessage>)message).get(list.size() - 1);
                 if (tm.task() != -1) throw new RuntimeException("Metrics messages are sent to the system task ("+client.getDstAddress()+") "+tm);
                 List metrics = _des.deserialize(tm.message());

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java
Patch:
@@ -429,6 +429,7 @@ public Map<String, Object> getComponentConfiguration () {
 
         configuration.put(configKeyPrefix + "groupid", kafkaSpoutConfig.getConsumerGroupId());
         configuration.put(configKeyPrefix + "bootstrap.servers", kafkaSpoutConfig.getKafkaProps().get("bootstrap.servers"));
+        configuration.put(configKeyPrefix + "security.protocol", kafkaSpoutConfig.getKafkaProps().get("security.protocol"));
         return configuration;
     }
 

File: external/storm-cassandra/src/main/java/org/apache/storm/cassandra/bolt/BatchCassandraWriterBolt.java
Patch:
@@ -123,11 +123,11 @@ public void prepareAndExecuteStatement() {
 
                 checkTimeElapsedSinceLastExec(sinceLastModified);
 
-                GroupingBatchBuilder batchBuilder = new GroupingBatchBuilder(cassandraConfConfig.getBatchSizeRows(), psl);
+                GroupingBatchBuilder batchBuilder = new GroupingBatchBuilder(cassandraConf.getBatchSizeRows(), psl);
 
                 int batchSize = 0;
                 for (PairBatchStatementTuples batch : batchBuilder) {
-                    LOG.debug(logPrefix() + "Writing data to {} in batches of {} rows.", cassandraConfConfig.getKeyspace(), batch.getInputs().size());
+                    LOG.debug(logPrefix() + "Writing data to {} in batches of {} rows.", cassandraConf.getKeyspace(), batch.getInputs().size());
                     getAsyncExecutor().execAsync(batch.getStatement(), batch.getInputs());
                     batchSize++;
                 }

File: external/storm-redis/src/test/java/org/apache/storm/redis/state/RedisKeyValueStateTest.java
Patch:
@@ -105,7 +105,7 @@ public Long answer(InvocationOnMock invocation) throws Throwable {
                 });
 
         keyValueState = new RedisKeyValueState<String, String>("test", mockContainer, new DefaultStateSerializer<String>(),
-                                                               new DefaultStateSerializer<Optional<String>>());
+                                                               new DefaultStateSerializer<String>());
     }
 
 

File: storm-core/src/jvm/org/apache/storm/daemon/DrpcServer.java
Patch:
@@ -118,7 +118,7 @@ private void initHttp() throws Exception {
         if (drpcHttpPort != null && drpcHttpPort > 0) {
             String filterClass = (String) (conf.get(Config.DRPC_HTTP_FILTER));
             Map<String, String> filterParams = (Map<String, String>) (conf.get(Config.DRPC_HTTP_FILTER_PARAMS));
-            FilterConfiguration filterConfiguration = new FilterConfiguration(filterParams, filterClass);
+            FilterConfiguration filterConfiguration = new FilterConfiguration(filterClass, filterParams);
             final List<FilterConfiguration> filterConfigurations = Arrays.asList(filterConfiguration);
             final Integer httpsPort = Utils.getInt(conf.get(Config.DRPC_HTTPS_PORT), 0);
             final String httpsKsPath = (String) (conf.get(Config.DRPC_HTTPS_KEYSTORE_PATH));

File: storm-core/src/jvm/org/apache/storm/ui/FilterConfiguration.java
Patch:
@@ -25,7 +25,7 @@ public class FilterConfiguration {
     private Map filterParams;
 
 
-    public FilterConfiguration(Map filterParams, String filterClass) {
+    public FilterConfiguration(String filterClass, Map filterParams) {
         this.filterParams = filterParams;
         this.filterClass = filterClass;
         this.filterName = null;

File: external/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/utils/SerdeUtils.java
Patch:
@@ -47,7 +47,7 @@ public static Scheme getScheme(String inputFormatClass, Properties properties, L
             if (JsonScheme.class.getName().equals(inputFormatClass)) {
                 scheme = new JsonScheme(fieldNames);
             } else if (TsvScheme.class.getName().equals(inputFormatClass)) {
-                String delimiter = properties.getProperty("tsv.delimiter", "\t");
+                String delimiter = properties.getProperty("input.tsv.delimiter", "\t");
                 scheme = new TsvScheme(fieldNames, delimiter.charAt(0));
             } else if (CsvScheme.class.getName().equals(inputFormatClass)) {
                 scheme = new CsvScheme(fieldNames);
@@ -71,7 +71,7 @@ public static IOutputSerializer getSerializer(String outputFormatClass, Properti
             if (JsonSerializer.class.getName().equals(outputFormatClass)) {
                 serializer = new JsonSerializer(fieldNames);
             } else if (TsvSerializer.class.getName().equals(outputFormatClass)) {
-                String delimiter = properties.getProperty("tsv.delimiter", "\t");
+                String delimiter = properties.getProperty("output.tsv.delimiter", "\t");
                 serializer = new TsvSerializer(fieldNames, delimiter.charAt(0));
             } else if (CsvSerializer.class.getName().equals(outputFormatClass)) {
                 serializer = new CsvSerializer(fieldNames);

File: storm-core/src/jvm/org/apache/storm/scheduler/resource/RAS_Node.java
Patch:
@@ -497,7 +497,7 @@ public Double getTotalCpuResources() {
     public Double consumeCPU(Double amount) {
         if (amount > _availCPU) {
             LOG.error("Attempting to consume more CPU than available! Needed: {}, we only have: {}", amount, _availCPU);
-            throw new IllegalStateException("Attempting to consume more memory than available");
+            throw new IllegalStateException("Attempting to consume more CPU than available");
         }
         _availCPU = _availCPU - amount;
         return _availCPU;

File: storm-core/src/jvm/org/apache/storm/scheduler/resource/RAS_Node.java
Patch:
@@ -250,7 +250,7 @@ private void freeMemory(double amount) {
     private void freeCPU(double amount) {
         LOG.debug("freeing {} CPU on node...avail CPU: {}", amount, getHostname(), _availCPU);
         if ((_availCPU + amount) > getTotalCpuResources()) {
-            LOG.warn("Freeing more CPU than there exists! CPU trying to free: {} Total CPU on Node: {}", (_availMemory + amount), getTotalCpuResources());
+            LOG.warn("Freeing more CPU than there exists! CPU trying to free: {} Total CPU on Node: {}", (_availCPU + amount), getTotalCpuResources());
             return;
         }
         _availCPU += amount;

File: external/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/serde/avro/AvroSerializer.java
Patch:
@@ -33,6 +33,9 @@
 import java.nio.ByteBuffer;
 import java.util.List;
 
+/**
+ * AvroSerializer uses generic(without code generation) instead of specific(with code generation) writers.
+ */
 public class AvroSerializer implements IOutputSerializer, Serializable {
   private final String schemaString;
   private final List<String> fieldNames;

File: storm-core/src/jvm/org/apache/storm/blobstore/BlobStoreUtils.java
Patch:
@@ -150,7 +150,7 @@ public static boolean downloadMissingBlob(Map conf, BlobStore blobStore, String
         }
 
         if (!isSuccess) {
-            LOG.error("Could not download blob with key" + key);
+            LOG.error("Could not download the blob with key: {}", key);
         }
         return isSuccess;
     }
@@ -273,4 +273,4 @@ public static String applyUUIDToFileName(String fileName) {
     }
 
 
-}
\ No newline at end of file
+}

File: external/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/serde/avro/AvroSerializer.java
Patch:
@@ -19,7 +19,6 @@
 
 import com.google.common.base.Preconditions;
 import org.apache.avro.Schema;
-import org.apache.avro.generic.GenericContainer;
 import org.apache.avro.generic.GenericData;
 import org.apache.avro.generic.GenericDatumWriter;
 import org.apache.avro.generic.GenericRecord;
@@ -56,7 +55,7 @@ public ByteBuffer write(List<Object> data, ByteBuffer buffer) {
       }
 
       ByteArrayOutputStream out = new ByteArrayOutputStream();
-      DatumWriter<GenericContainer> writer = new GenericDatumWriter<>(record.getSchema());
+      DatumWriter<GenericRecord> writer = new GenericDatumWriter<>(record.getSchema());
       Encoder encoder = EncoderFactory.get().directBinaryEncoder(out, null);
       writer.write(record, encoder);
       encoder.flush();

File: storm-core/src/jvm/org/apache/storm/blobstore/BlobStoreUtils.java
Patch:
@@ -150,7 +150,7 @@ public static boolean downloadMissingBlob(Map conf, BlobStore blobStore, String
         }
 
         if (!isSuccess) {
-            LOG.error("Could not download blob with key" + key);
+            LOG.error("Could not download the blob with key: {}", key);
         }
         return isSuccess;
     }
@@ -273,4 +273,4 @@ public static String applyUUIDToFileName(String fileName) {
     }
 
 
-}
\ No newline at end of file
+}

File: storm-core/src/jvm/org/apache/storm/security/auth/SimpleTransportPlugin.java
Patch:
@@ -76,6 +76,8 @@ public TServer getServer(TProcessor processor) throws IOException, TTransportExc
                 maxWorkerThreads(numWorkerThreads).
                 protocolFactory(new TBinaryProtocol.Factory(false, true, maxBufferSize, -1));
 
+        server_args.maxReadBufferBytes = maxBufferSize;
+
         if (queueSize != null) {
             server_args.executorService(new ThreadPoolExecutor(numWorkerThreads, numWorkerThreads, 
                                    60, TimeUnit.SECONDS, new ArrayBlockingQueue(queueSize)));

File: storm-core/src/jvm/org/apache/storm/security/auth/SimpleTransportPlugin.java
Patch:
@@ -76,6 +76,8 @@ public TServer getServer(TProcessor processor) throws IOException, TTransportExc
                 maxWorkerThreads(numWorkerThreads).
                 protocolFactory(new TBinaryProtocol.Factory(false, true, maxBufferSize, -1));
 
+        server_args.maxReadBufferBytes = maxBufferSize;
+
         if (queueSize != null) {
             server_args.executorService(new ThreadPoolExecutor(numWorkerThreads, numWorkerThreads, 
                                    60, TimeUnit.SECONDS, new ArrayBlockingQueue(queueSize)));

File: external/sql/storm-sql-core/src/test/org/apache/storm/sql/compiler/backends/standalone/TestPlanCompiler.java
Patch:
@@ -71,9 +71,9 @@ public void testLogicalExpr() throws Exception {
 
   @Test
   public void testNested() throws Exception {
-    String sql = "SELECT ID, MAPFIELD, NESTEDMAPFIELD, ARRAYFIELD " +
+    String sql = "SELECT ID, MAPFIELD['c'], NESTEDMAPFIELD, ARRAYFIELD " +
             "FROM FOO " +
-            "WHERE NESTEDMAPFIELD['a']['b'] = 2 AND ARRAYFIELD[1] = 200";
+            "WHERE NESTEDMAPFIELD['a']['b'] = 2 AND ARRAYFIELD[2] = 200";
     TestCompilerUtils.CalciteState state = TestCompilerUtils.sqlOverNestedTable(sql);
     PlanCompiler compiler = new PlanCompiler(typeFactory);
     AbstractValuesProcessor proc = compiler.compile(state.tree());
@@ -84,7 +84,7 @@ public void testNested() throws Exception {
     proc.initialize(data, h);
     Map<String, Integer> map = ImmutableMap.of("b", 2, "c", 4);
     Map<String, Map<String, Integer>> nestedMap = ImmutableMap.of("a", map);
-    Assert.assertEquals(new Values(2, map, nestedMap, Arrays.asList(100, 200, 300)), values.get(0));
+    Assert.assertEquals(new Values(2, 4, nestedMap, Arrays.asList(100, 200, 300)), values.get(0));
   }
 
   @Test

File: external/sql/storm-sql-core/src/test/org/apache/storm/sql/compiler/backends/standalone/TestRelNodeCompiler.java
Patch:
@@ -59,7 +59,7 @@ public void testFilter() throws Exception {
       // standalone mode doesn't use inputstreams argument
       compiler.visitProject(project, Collections.EMPTY_LIST);
       pw.flush();
-      Assert.assertThat(sw.toString(), containsString("plus("));
+      Assert.assertThat(sw.toString(), containsString(" + 1"));
     }
   }
 }

File: external/sql/storm-sql-core/src/jvm/org/apache/storm/sql/compiler/backends/standalone/PlanCompiler.java
Patch:
@@ -40,6 +40,7 @@ public class PlanCompiler {
       "// GENERATED CODE", "package " + PACKAGE_NAME + ";", "",
       "import java.util.Iterator;", "import java.util.Map;", "import java.util.HashMap;",
       "import java.util.List;", "import java.util.ArrayList;",
+      "import java.util.LinkedHashMap;",
       "import org.apache.storm.tuple.Values;",
       "import org.apache.storm.sql.runtime.AbstractChannelHandler;",
       "import org.apache.storm.sql.runtime.Channels;",

File: storm-core/src/jvm/org/apache/storm/executor/error/ReportErrorAndDie.java
Patch:
@@ -41,6 +41,7 @@ public void uncaughtException(Thread t, Throwable e) {
         if (Utils.exceptionCauseIsInstanceOf(InterruptedException.class, e)
                 || Utils.exceptionCauseIsInstanceOf(java.io.InterruptedIOException.class, e)) {
             LOG.info("Got interrupted exception shutting thread down...");
+        } else {
             suicideFn.run();
         }
     }

File: external/storm-jms/examples/src/main/java/org/apache/storm/jms/example/JsonTupleProducer.java
Patch:
@@ -22,9 +22,9 @@
 import javax.jms.TextMessage;
 
 import org.apache.storm.jms.JmsTupleProducer;
-import backtype.storm.topology.OutputFieldsDeclarer;
-import backtype.storm.tuple.Fields;
-import backtype.storm.tuple.Values;
+import org.apache.storm.topology.OutputFieldsDeclarer;
+import org.apache.storm.tuple.Fields;
+import org.apache.storm.tuple.Values;
 
 /**
  * A simple <code>JmsTupleProducer</code> that expects to receive

File: storm-core/src/jvm/org/apache/storm/daemon/supervisor/Container.java
Patch:
@@ -316,7 +316,7 @@ protected void setup() throws IOException {
         File workerArtifacts = new File(ConfigUtils.workerArtifactsRoot(_conf, _topologyId, _port));
         if (!_ops.fileExists(workerArtifacts)) {
             _ops.forceMkdir(workerArtifacts);
-            _ops.setupStormCodeDir(_topoConf, workerArtifacts);
+            _ops.setupWorkerArtifactsDir(_topoConf, workerArtifacts);
         }
     
         String user = getWorkerUser();

File: storm-core/src/jvm/org/apache/storm/cluster/PaceMakerStateStorage.java
Patch:
@@ -146,11 +146,13 @@ public byte[] get_worker_hb(String path, boolean watch) {
                 List<HBMessage> responses = pacemakerClientPool.sendAll(message);
                 for(HBMessage response : responses) {
                     if (response.get_type() != HBServerMessageType.GET_PULSE_RESPONSE) {
-                        //throw new HBExecutionException("Invalid Response Type");
                         LOG.error("get_worker_hb: Invalid Response Type");
                         continue;
                     }
                     byte[] details = response.get_data().get_pulse().get_details();
+                    if(details == null) {
+                        continue;
+                    }
                     ClusterWorkerHeartbeat cwh = Utils.deserialize(details, ClusterWorkerHeartbeat.class);
                     if(cwh != null && cwh.get_time_secs() > latest_time_secs) {
                         latest_time_secs = cwh.get_time_secs();

File: storm-core/src/jvm/org/apache/storm/pacemaker/PacemakerClientHandler.java
Patch:
@@ -69,7 +69,7 @@ else if(evm instanceof HBMessage) {
 
     @Override
     public void exceptionCaught(ChannelHandlerContext ctx, ExceptionEvent event) {
-        LOG.error("Connection to pacemaker failed", event.getCause());
+        LOG.error("Connection to pacemaker failed. Trying to reconnect {}", event.getCause().getMessage());
         client.reconnect();
     }
 }

File: external/storm-kafka/src/jvm/org/apache/storm/kafka/PartitionManager.java
Patch:
@@ -164,7 +164,7 @@ public EmitState next(SpoutOutputCollector collector) {
             if ((tups != null) && tups.iterator().hasNext()) {
                if (!Strings.isNullOrEmpty(_spoutConfig.outputStreamId)) {
                     for (List<Object> tup : tups) {
-                        collector.emit(_spoutConfig.topic, tup, new KafkaMessageId(_partition, toEmit.offset()));
+                        collector.emit(_spoutConfig.outputStreamId, tup, new KafkaMessageId(_partition, toEmit.offset()));
                     }
                 } else {
                     for (List<Object> tup : tups) {

File: external/storm-submit-tools/src/main/java/org/apache/storm/submit/command/DependencyResolverMain.java
Patch:
@@ -6,9 +6,9 @@
  * to you under the Apache License, Version 2.0 (the
  * "License"); you may not use this file except in compliance
  * with the License.  You may obtain a copy of the License at
- * <p>
+ *
  * http://www.apache.org/licenses/LICENSE-2.0
- * <p>
+ *
  * Unless required by applicable law or agreed to in writing, software
  * distributed under the License is distributed on an "AS IS" BASIS,
  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java
Patch:
@@ -32,10 +32,10 @@
  * KafkaSpoutConfig defines the required configuration to connect a consumer to a consumer group, as well as the subscribing topics
  */
 public class KafkaSpoutConfig<K, V> implements Serializable {
-    public static final long DEFAULT_POLL_TIMEOUT_MS = 2_000;            // 2s
-    public static final long DEFAULT_OFFSET_COMMIT_PERIOD_MS = 15_000;   // 15s
+    public static final long DEFAULT_POLL_TIMEOUT_MS = 200;            // 200ms
+    public static final long DEFAULT_OFFSET_COMMIT_PERIOD_MS = 30_000;   // 30s
     public static final int DEFAULT_MAX_RETRIES = Integer.MAX_VALUE;     // Retry forever
-    public static final int DEFAULT_MAX_UNCOMMITTED_OFFSETS = 10_000;    // 10,000 records
+    public static final int DEFAULT_MAX_UNCOMMITTED_OFFSETS = 10_000_000;    // 10,000,000 records => 80MBs of memory footprint in the worst case
 
     // Kafka property names
     public interface Consumer {

File: storm-core/src/jvm/org/apache/storm/daemon/StormCommon.java
Patch:
@@ -132,7 +132,7 @@ private static void validateIds(StormTopology topology) throws InvalidTopologyEx
         List<String> componentIds = new ArrayList<>();
 
         for (StormTopology._Fields field : Thrift.getTopologyFields()) {
-            if (!ThriftTopologyUtils.isWorkerHook(field)) {
+            if (!ThriftTopologyUtils.isWorkerHook(field) && !ThriftTopologyUtils.isDependencies(field)) {
                 Object value = topology.getFieldValue(field);
                 Map<String, Object> componentMap = (Map<String, Object>) value;
                 componentIds.addAll(componentMap.keySet());
@@ -173,7 +173,7 @@ public static Map<String, Object> allComponents(StormTopology topology) {
         Map<String, Object> components = new HashMap<>();
         List<StormTopology._Fields> topologyFields = Arrays.asList(Thrift.getTopologyFields());
         for (StormTopology._Fields field : topologyFields) {
-            if (!ThriftTopologyUtils.isWorkerHook(field)) {
+            if (!ThriftTopologyUtils.isWorkerHook(field) && !ThriftTopologyUtils.isDependencies(field)) {
                 components.putAll(((Map) topology.getFieldValue(field)));
             }
         }

File: storm-core/src/jvm/org/apache/storm/utils/Utils.java
Patch:
@@ -631,7 +631,7 @@ public static boolean checkFileExists(String path) {
     }
 
     public static boolean checkFileExists(String dir, String file) {
-        return checkFileExists(dir + "/" + file);
+        return checkFileExists(dir + FILE_PATH_SEPARATOR + file);
     }
 
     public static boolean CheckDirExists(String dir) {
@@ -1993,12 +1993,12 @@ protected void forceDeleteImpl(String path) throws IOException {
      * Creates a symbolic link to the target
      * @param dir the parent directory of the link
      * @param targetDir the parent directory of the link's target
-     * @param targetFilename the file name of the links target
      * @param filename the file name of the link
+     * @param targetFilename the file name of the links target
      * @throws IOException
      */
     public static void createSymlink(String dir, String targetDir,
-            String targetFilename, String filename) throws IOException {
+            String filename, String targetFilename) throws IOException {
         Path path = Paths.get(dir, filename).toAbsolutePath();
         Path target = Paths.get(targetDir, targetFilename).toAbsolutePath();
         LOG.debug("Creating symlink [{}] to [{}]", path, target);

File: storm-core/test/jvm/org/apache/storm/utils/staticmocking/ConfigUtilsInstaller.java
Patch:
@@ -26,6 +26,8 @@ public class ConfigUtilsInstaller implements AutoCloseable {
     public ConfigUtilsInstaller(ConfigUtils instance) {
         _oldInstance = ConfigUtils.setInstance(instance);
         _curInstance = instance;
+
+        System.out.println("DEBUG: instance changed: " + _curInstance);
     }
 
     @Override

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutRetryExponentialBackoff.java
Patch:
@@ -117,7 +117,7 @@ public static TimeInterval milliSeconds(long length) {
         }
 
         public static TimeInterval microSeconds(long length) {
-            return new TimeInterval(length, TimeUnit.MILLISECONDS);
+            return new TimeInterval(length, TimeUnit.MICROSECONDS);
         }
 
         public long lengthNanos() {

File: storm-core/src/jvm/org/apache/storm/executor/spout/SpoutExecutor.java
Patch:
@@ -202,7 +202,7 @@ public void tupleActionFn(int taskId, TupleImpl tuple) throws Exception {
             Long id = (Long) tuple.getValue(0);
             Long timeDeltaMs = (Long) tuple.getValue(1);
             TupleInfo tupleInfo = (TupleInfo) pending.remove(id);
-            if (tupleInfo.getMessageId() != null) {
+            if (tupleInfo != null && tupleInfo.getMessageId() != null) {
                 if (taskId != tupleInfo.getTaskId()) {
                     throw new RuntimeException("Fatal error, mismatched task ids: " + taskId + " " + tupleInfo.getTaskId());
                 }

File: storm-core/src/jvm/org/apache/storm/security/auth/authorizer/SimpleACLAuthorizer.java
Patch:
@@ -65,6 +65,7 @@ public class SimpleACLAuthorizer implements IAuthorizer {
             "dumpProfile",
             "dumpJstack",
             "dumpHeap",
+            "debug",
             "getLogConfig"));
 
     protected Set<String> _admins;

File: storm-core/src/jvm/org/apache/storm/messaging/netty/StormClientHandler.java
Patch:
@@ -60,7 +60,7 @@ public void messageReceived(ChannelHandlerContext ctx, MessageEvent event) {
                 //This should be the metrics, and there should only be one of them
                 List<TaskMessage> list = (List<TaskMessage>)message;
                 if (list.size() < 1) throw new RuntimeException("Didn't see enough load metrics ("+client.getDstAddress()+") "+list);
-                if (list.size() != 1) LOG.warn("Messages are not being delivered fast enough, got "+list.size()+" metrics messages at once("+client.getDstAddress()+")");
+                if (list.size() != 1) LOG.debug("Messages are not being delivered fast enough, got "+list.size()+" metrics messages at once("+client.getDstAddress()+")");
                 TaskMessage tm = ((List<TaskMessage>)message).get(list.size() - 1);
                 if (tm.task() != -1) throw new RuntimeException("Metrics messages are sent to the system task ("+client.getDstAddress()+") "+tm);
                 List metrics = _des.deserialize(tm.message());

File: storm-core/src/jvm/org/apache/storm/executor/Executor.java
Patch:
@@ -204,8 +204,8 @@ public static Executor mkExecutor(Map workerData, List<Long> executorId, Map<Str
                 throw Utils.wrapInRuntime(ex);
             }
         }
-        executor.init(idToTask);
 
+        executor.idToTask = idToTask;
         return executor;
     }
 
@@ -243,8 +243,6 @@ public ExecutorShutdown execute() throws Exception {
 
     public abstract void tupleActionFn(int taskId, TupleImpl tuple) throws Exception;
 
-    public abstract void init(Map<Integer, Task> idToTask);
-
     @SuppressWarnings("unchecked")
     @Override
     public void onEvent(Object event, long seq, boolean endOfBatch) throws Exception {

File: storm-core/src/jvm/org/apache/storm/stats/BoltExecutorStats.java
Patch:
@@ -84,6 +84,7 @@ public void boltFailedTuple(String component, String stream, long latencyMs) {
 
     }
 
+    @Override
     public ExecutorStats renderStats() {
         ExecutorStats ret = new ExecutorStats();
         // common stats

File: storm-core/src/jvm/org/apache/storm/stats/SpoutExecutorStats.java
Patch:
@@ -58,6 +58,7 @@ public void spoutFailedTuple(String stream, long latencyMs) {
         this.getFailed().incBy(stream, this.rate);
     }
 
+    @Override
     public ExecutorStats renderStats() {
         ExecutorStats ret = new ExecutorStats();
         // common fields

File: external/storm-hdfs/src/test/java/org/apache/storm/hdfs/spout/TestHdfsSpout.java
Patch:
@@ -57,7 +57,6 @@
 import java.util.Map;
 
 import org.apache.storm.hdfs.common.HdfsUtils.Pair;
-import sun.reflect.generics.reflectiveObjects.NotImplementedException;
 
 
 public class TestHdfsSpout {
@@ -704,12 +703,12 @@ public List<Integer> emit(String streamId, List<Object> tuple, Object messageId)
 
     @Override
     public void emitDirect(int arg0, String arg1, List<Object> arg2, Object arg3) {
-      throw new NotImplementedException();
+      throw new UnsupportedOperationException("NOT Implemented");
     }
 
     @Override
     public void reportError(Throwable arg0) {
-      throw new NotImplementedException();
+        throw new UnsupportedOperationException("NOT Implemented");
     }
 
     @Override

File: storm-core/test/jvm/org/apache/storm/tuple/FieldsTest.java
Patch:
@@ -118,7 +118,7 @@ public void selectTest() {
         Assert.assertTrue(pickSecondAndFirst.equals(secondAndFirst));
     }
 
-    @Test(expected = NullPointerException.class)
+    @Test(expected = IllegalArgumentException.class)
     public void selectingUnknownFieldThrowsTest() {
         Fields fields = new Fields("foo", "bar");
         fields.select(new Fields("bar", "baz"), Arrays.asList(new Object[]{"a", "b", "c"}));

File: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutRetryExponentialBackoff.java
Patch:
@@ -264,8 +264,8 @@ public void schedule(KafkaSpoutMessageId msgId) {
     private long nextTime(KafkaSpoutMessageId msgId) {
         final long currentTimeNanos = System.nanoTime();
         final long nextTimeNanos = msgId.numFails() == 1                // numFails = 1, 2, 3, ...
-                ? currentTimeNanos + initialDelay.lengthNanos()
-                : (long) (currentTimeNanos + Math.pow(delayPeriod.lengthNanos, msgId.numFails() - 1));
+                ? currentTimeNanos + initialDelay.lengthNanos
+                : (currentTimeNanos + delayPeriod.timeUnit.toNanos((long) Math.pow(delayPeriod.length, msgId.numFails() - 1)));
         return Math.min(nextTimeNanos, currentTimeNanos + maxDelay.lengthNanos);
     }
 

File: examples/storm-starter/src/jvm/org/apache/storm/starter/trident/TridentKafkaWordCount.java
Patch:
@@ -213,7 +213,7 @@ public static void main(String[] args) throws Exception {
             System.exit(1);
         } else if (args.length == 1) {
             zkUrl = args[0];
-        } else if (args.length == 2) {
+        } else {
             zkUrl = args[0];
             brokerUrl = args[1];
         }

File: external/storm-hive/src/main/java/org/apache/storm/hive/bolt/mapper/HiveMapper.java
Patch:
@@ -38,7 +38,7 @@ public interface HiveMapper extends Serializable {
     /**
      * Given a endPoint, returns a RecordWriter with columnNames.
      *
-     * @param tuple
+     * @param endPoint
      * @return
      */
 
@@ -66,7 +66,7 @@ void write(TransactionBatch txnBatch, Tuple tuple)
     /**
      * Given a TridetnTuple, return a hive partition values list.
      *
-     * @param TridentTuple
+     * @param tuple
      * @return List<String>
      */
     List<String> mapPartitions(TridentTuple tuple);

File: external/storm-hive/src/main/java/org/apache/storm/hive/common/HiveWriter.java
Patch:
@@ -324,7 +324,7 @@ public Void call() throws StreamingException, InterruptedException {
     /**
      * if there are remainingTransactions in current txnBatch, begins nextTransactions
      * otherwise creates new txnBatch.
-     * @param boolean rollToNext
+     * @param rollToNext
      */
     private void nextTxn(boolean rollToNext) throws StreamingException, InterruptedException, TxnBatchFailure {
         if(txnBatch.remainingTransactions() == 0) {

File: storm-core/src/jvm/org/apache/storm/Config.java
Patch:
@@ -1838,13 +1838,13 @@ public class Config extends HashMap<String, Object> {
      * Topology-specific classpath for the worker child process. This will be *prepended* to
      * the usual classpath, meaning it can override the Storm classpath. This is for debugging
      * purposes, and is disabled by default. To allow topologies to be submitted with user-first
-     * classpaths, set the user.classpath.first.enabled config to true.
+     * classpaths, set the storm.topology.classpath.beginning.enabled config to true.
      */
     @isStringOrStringList
     public static final String TOPOLOGY_CLASSPATH_BEGINNING="topology.classpath.beginning";
 
     /**
-     * Enables user-first classpath. See topology.classpath.first
+     * Enables user-first classpath. See topology.classpath.beginning
      */
     @isBoolean
     public static final String STORM_TOPOLOGY_CLASSPATH_BEGINNING_ENABLED="storm.topology.classpath.beginning.enabled";

File: external/storm-hive/src/main/java/org/apache/storm/hive/bolt/mapper/HiveMapper.java
Patch:
@@ -38,7 +38,7 @@ public interface HiveMapper extends Serializable {
     /**
      * Given a endPoint, returns a RecordWriter with columnNames.
      *
-     * @param tuple
+     * @param endPoint
      * @return
      */
 
@@ -66,7 +66,7 @@ void write(TransactionBatch txnBatch, Tuple tuple)
     /**
      * Given a TridetnTuple, return a hive partition values list.
      *
-     * @param TridentTuple
+     * @param tuple
      * @return List<String>
      */
     List<String> mapPartitions(TridentTuple tuple);

File: external/storm-hive/src/main/java/org/apache/storm/hive/common/HiveWriter.java
Patch:
@@ -324,7 +324,7 @@ public Void call() throws StreamingException, InterruptedException {
     /**
      * if there are remainingTransactions in current txnBatch, begins nextTransactions
      * otherwise creates new txnBatch.
-     * @param boolean rollToNext
+     * @param rollToNext
      */
     private void nextTxn(boolean rollToNext) throws StreamingException, InterruptedException, TxnBatchFailure {
         if(txnBatch.remainingTransactions() == 0) {

File: storm-core/src/jvm/org/apache/storm/trident/operation/DefaultResourceDeclarer.java
Patch:
@@ -30,7 +30,7 @@
 public class DefaultResourceDeclarer<T extends DefaultResourceDeclarer> implements ResourceDeclarer<T>, ITridentResource {
 
     private Map<String, Number> resources = new HashMap<>();
-    private Map conf = Utils.readStormConfig();
+    private static Map conf = Utils.readStormConfig();
 
     @Override
     public T setMemoryLoad(Number onHeap) {

File: storm-core/src/jvm/org/apache/storm/daemon/supervisor/workermanager/DefaultWorkerManager.java
Patch:
@@ -352,7 +352,7 @@ protected String getWorkerClassPath(String stormJar, Map stormConf) {
         Object topologyClasspathFirst = stormConf.get(Config.TOPOLOGY_CLASSPATH_FIRST);
         List<String> firstClasspathList = new ArrayList<>();
         if(topologyClasspathFirst instanceof List) {
-           firstClasspathList.addAll((List<String>)topologyClasspathFirst);
+            firstClasspathList.addAll((List<String>)topologyClasspathFirst);
         } else if (topologyClasspathFirst instanceof String) {
             firstClasspathList.add((String) topologyClasspathFirst);
         }

File: storm-core/src/jvm/org/apache/storm/utils/Utils.java
Patch:
@@ -2116,7 +2116,7 @@ public String addToClasspathImpl(Collection<String> classpaths,
             allPaths.addAll(classpaths);
         }
         if(paths != null) {
-            allpaths.addAll(classpaths);
+            allPaths.addAll(paths);
         }
         return StringUtils.join(allPaths, CLASS_PATH_SEPARATOR);
     }

File: external/storm-hdfs/src/test/java/org/apache/storm/hdfs/bolt/HdfsFileTopology.java
Patch:
@@ -106,9 +106,9 @@ public static void main(String[] args) throws Exception {
             cluster.shutdown();
             System.exit(0);
         } else if (args.length == 3) {
-            StormSubmitter.submitTopology(args[0], config, builder.createTopology());
+            StormSubmitter.submitTopology(args[2], config, builder.createTopology());
         } else{
-            System.out.println("Usage: HdfsFileTopology [topology name] <yaml config file>");
+            System.out.println("Usage: HdfsFileTopology [hdfs url] [hdfs yaml config file] <topology name>");
         }
     }
 

File: external/storm-kafka/src/jvm/org/apache/storm/kafka/ExponentialBackoffMsgRetryManager.java
Patch:
@@ -106,7 +106,6 @@ public boolean retryFurther(Long offset) {
         MessageRetryRecord record = this.records.get(offset);
         return ! (record != null &&
                this.retryLimit > 0 &&
-               this.waiting.contains(record) &&
                this.retryLimit <= record.retryNum);
     }
 

File: external/storm-hive/src/main/java/org/apache/storm/hive/trident/HiveState.java
Patch:
@@ -243,7 +243,7 @@ private void retireEldestWriter() {
             LOG.warn("Interrupted when attempting to close writer for end point: " + eldest, e);
             Thread.currentThread().interrupt();
         } catch (Exception e) {
-            LOG.warn("Interrupted when attempting to close writer for end point: " + ep, e);
+            LOG.warn("Interrupted when attempting to close writer for end point: " + eldest, e);
         }
     }
 

File: storm-core/src/jvm/org/apache/storm/generated/HBMessageData.java
Patch:
@@ -139,13 +139,13 @@ public String getFieldName() {
     tmpMap.put(_Fields.PATH, new org.apache.thrift.meta_data.FieldMetaData("path", org.apache.thrift.TFieldRequirementType.DEFAULT, 
         new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.STRING)));
     tmpMap.put(_Fields.PULSE, new org.apache.thrift.meta_data.FieldMetaData("pulse", org.apache.thrift.TFieldRequirementType.DEFAULT, 
-        new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.STRUCT        , "HBPulse")));
+        new org.apache.thrift.meta_data.StructMetaData(org.apache.thrift.protocol.TType.STRUCT, HBPulse.class)));
     tmpMap.put(_Fields.BOOLVAL, new org.apache.thrift.meta_data.FieldMetaData("boolval", org.apache.thrift.TFieldRequirementType.DEFAULT, 
         new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.BOOL)));
     tmpMap.put(_Fields.RECORDS, new org.apache.thrift.meta_data.FieldMetaData("records", org.apache.thrift.TFieldRequirementType.DEFAULT, 
-        new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.STRUCT        , "HBRecords")));
+        new org.apache.thrift.meta_data.StructMetaData(org.apache.thrift.protocol.TType.STRUCT, HBRecords.class)));
     tmpMap.put(_Fields.NODES, new org.apache.thrift.meta_data.FieldMetaData("nodes", org.apache.thrift.TFieldRequirementType.DEFAULT, 
-        new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.STRUCT        , "HBNodes")));
+        new org.apache.thrift.meta_data.StructMetaData(org.apache.thrift.protocol.TType.STRUCT, HBNodes.class)));
     tmpMap.put(_Fields.MESSAGE_BLOB, new org.apache.thrift.meta_data.FieldMetaData("message_blob", org.apache.thrift.TFieldRequirementType.OPTIONAL, 
         new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.STRING        , true)));
     metaDataMap = Collections.unmodifiableMap(tmpMap);

File: external/storm-kafka/src/jvm/org/apache/storm/kafka/KafkaConfig.java
Patch:
@@ -33,7 +33,8 @@ public class KafkaConfig implements Serializable {
 
     public int fetchSizeBytes = 1024 * 1024;
     public int socketTimeoutMs = 10000;
-    public int fetchMaxWait = 10000;
+    public int fetchMaxWait = 100;
+    public int fetchMinBytes = 1;
     public int bufferSizeBytes = 1024 * 1024;
     public MultiScheme scheme = new RawMultiScheme();
     public boolean ignoreZkOffsets = false;

File: external/flux/flux-wrappers/src/main/java/org/apache/storm/flux/wrappers/bolts/FluxShellBolt.java
Patch:
@@ -23,6 +23,8 @@
 import org.apache.storm.tuple.Fields;
 
 import java.util.Map;
+import java.util.HashMap;
+import java.util.Iterator;
 
 /**
  * A generic `ShellBolt` implementation that allows you specify output fields

File: examples/storm-starter/src/jvm/org/apache/storm/starter/SlidingWindowTopology.java
Patch:
@@ -87,9 +87,9 @@ public void declareOutputFields(OutputFieldsDeclarer declarer) {
     public static void main(String[] args) throws Exception {
         TopologyBuilder builder = new TopologyBuilder();
         builder.setSpout("integer", new RandomIntegerSpout(), 1);
-        builder.setBolt("slidingsum", new SlidingWindowSumBolt().withWindow(new Count(30), new Count(10)), 1)
+        builder.setBolt("slidingsum", new SlidingWindowSumBolt().withWindow(Count.of(30), Count.of(10)), 1)
                 .shuffleGrouping("integer");
-        builder.setBolt("tumblingavg", new TumblingWindowAvgBolt().withTumblingWindow(new Count(3)), 1)
+        builder.setBolt("tumblingavg", new TumblingWindowAvgBolt().withTumblingWindow(Count.of(3)), 1)
                 .shuffleGrouping("slidingsum");
         builder.setBolt("printer", new PrinterBolt(), 1).shuffleGrouping("tumblingavg");
         Config conf = new Config();

File: storm-core/test/jvm/org/apache/storm/scheduler/resource/TestUtilsForResourceAwareScheduler.java
Patch:
@@ -103,7 +103,7 @@ public static Map<String, SupervisorDetails> genSupervisors(int numSup, int numP
             for (int j = 0; j < numPorts; j++) {
                 ports.add(j);
             }
-            SupervisorDetails sup = new SupervisorDetails("sup-" + i, "host-" + i, null, ports, new HashMap<>(resourceMap));
+            SupervisorDetails sup = new SupervisorDetails("sup-" + i, "host-" + i, null, ports, new HashMap<String, Double>(resourceMap));
             retList.put(sup.getId(), sup);
         }
         return retList;

File: external/storm-redis/src/main/java/org/apache/storm/redis/common/config/JedisPoolConfig.java
Patch:
@@ -25,7 +25,6 @@
  * Configuration for JedisPool.
  */
 public class JedisPoolConfig implements Serializable {
-    public static final String DEFAULT_HOST = "127.0.0.1";
 
     private String host;
     private int port;
@@ -99,7 +98,7 @@ public String getPassword() {
      * Builder for initializing JedisPoolConfig.
      */
     public static class Builder {
-        private String host = DEFAULT_HOST;
+        private String host = Protocol.DEFAULT_HOST;
         private int port = Protocol.DEFAULT_PORT;
         private int timeout = Protocol.DEFAULT_TIMEOUT;
         private int database = Protocol.DEFAULT_DATABASE;

File: storm-core/src/jvm/org/apache/storm/metric/StormMetricsRegistry.java
Patch:
@@ -37,7 +37,7 @@ public static Meter registerMeter(String name) {
         return register(name, meter);
     }
 
-    // TODO: should replace fn to Gauge<Integer> when nimbus.clj is translated to java
+    // TODO: should replace Callable to Gauge<Integer> when nimbus.clj is translated to java
     public static Gauge<Integer> registerGauge(final String name, final Callable fn) {
         Gauge<Integer> gauge = new Gauge<Integer>() {
             @Override

File: storm-core/src/jvm/org/apache/storm/utils/Utils.java
Patch:
@@ -216,7 +216,7 @@ public static <T> T thriftDeserialize(Class c, byte[] b, int offset, int length)
         try {
             T ret = (T) c.newInstance();
             TDeserializer des = getDes();
-            des.deserialize((TBase)ret, b, offset, length);
+            des.deserialize((TBase) ret, b, offset, length);
             return ret;
         } catch (Exception e) {
             throw new RuntimeException(e);
@@ -1717,7 +1717,7 @@ public static <T, U> T findOne (IPredicate<T> pred, Map<U, T> map) {
         if(map == null) {
             return null;
         }
-        return findOne(pred, (Set<T>)map.entrySet());
+        return findOne(pred, (Set<T>) map.entrySet());
     }
 
     public static String localHostname () throws UnknownHostException {

File: external/storm-mongodb/src/main/java/org/apache/storm/mongodb/bolt/MongoUpdateBolt.java
Patch:
@@ -63,8 +63,9 @@ public void execute(Tuple tuple) {
         }
     }
 
-    public void withUpsert(boolean upsert) {
+    public MongoUpdateBolt withUpsert(boolean upsert) {
         this.upsert = upsert;
+        return this;
     }
 
     @Override

File: external/storm-mongodb/src/main/java/org/apache/storm/mongodb/trident/state/MongoState.java
Patch:
@@ -91,7 +91,7 @@ public void updateState(List<TridentTuple> tuples, TridentCollector collector) {
             Document document = options.mapper.toDocument(tuple);
             documents.add(document);
         }
-        this.mongoClient.insert(documents);
+        this.mongoClient.insert(documents, true);
     }
 
 }

File: external/storm-hive/src/main/java/org/apache/storm/hive/trident/HiveStateFactory.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.hive.trident;
 
 import org.apache.storm.task.IMetricsContext;

File: external/storm-hive/src/main/java/org/apache/storm/hive/trident/HiveUpdater.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.hive.trident;
 
 import org.apache.storm.trident.operation.TridentCollector;

File: external/storm-hive/src/test/java/org/apache/storm/hive/trident/TridentHiveTopology.java
Patch:
@@ -179,7 +179,7 @@ public void close() {
         }
 
         @Override
-        public Map<String, Object> getComponentConfiguration() {
+        public Map getComponentConfiguration() {
             Config conf = new Config();
             conf.setMaxTaskParallelism(1);
             return conf;

File: storm-core/src/jvm/org/apache/storm/StormTimer.java
Patch:
@@ -230,6 +230,7 @@ public void close() throws Exception {
         checkActive();
         this.task.setActive(false);
         this.task.interrupt();
+        this.task.join();
     }
 
     /**

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/bolt/AbstractHBaseBolt.java
Patch:
@@ -43,7 +43,6 @@ public abstract class AbstractHBaseBolt extends BaseRichBolt {
     protected HBaseMapper mapper;
     protected String configKey;
     protected int batchSize = 15000;
-    protected int flushIntervalSecs = 0;
 
     public AbstractHBaseBolt(String tableName, HBaseMapper mapper) {
         Validate.notEmpty(tableName, "Table name can not be blank or null");

File: storm-core/src/jvm/org/apache/storm/scheduler/TopologyDetails.java
Patch:
@@ -22,6 +22,7 @@
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
+import java.util.Set;
 
 import org.apache.storm.Config;
 import org.apache.storm.generated.Bolt;
@@ -117,7 +118,7 @@ public Map<ExecutorDetails, String> selectExecutorToComponent(Collection<Executo
         return ret;
     }
 
-    public Collection<ExecutorDetails> getExecutors() {
+    public Set<ExecutorDetails> getExecutors() {
         return this.executorToComponent.keySet();
     }
 

File: storm-core/test/jvm/org/apache/storm/security/auth/ThriftClientTest.java
Patch:
@@ -62,7 +62,7 @@ public void testConstructorThrowsIfHostNull() {
 
     @Test
     public void testConstructorThrowsIfHostEmpty() {
-        expectedException.expectCause(ThrowableNestedCauseMatcher.isCausedBy(TTransportException.class));
+        expectedException.expect(ThrowableNestedCauseMatcher.isCausedBy(TTransportException.class));
         new ThriftClient(conf, ThriftConnectionType.DRPC, "", 4242, NIMBUS_TIMEOUT);
     }
 }

File: storm-core/test/jvm/org/apache/storm/scheduler/resource/TestUtilsForResourceAwareScheduler.java
Patch:
@@ -161,6 +161,7 @@ public static StormTopology buildTopology(int numSpout, int numBolt,
             }
             BoltDeclarer b1 = builder.setBolt("bolt-" + i, new TestBolt(),
                     boltParallelism).shuffleGrouping("spout-" + j);
+            j++;
         }
 
         return builder.createTopology();

File: storm-core/src/jvm/org/apache/storm/cluster/PaceMakerStateStorageFactory.java
Patch:
@@ -55,7 +55,8 @@ public static PacemakerClient initMakeClient(Map config) {
     }
 
     public IStateStorage initZKstateImpl(Map config, Map auth_conf, List<ACL> acls, ClusterStateContext context) throws Exception {
-        return ClusterUtils.mkStateStorage(config, auth_conf, acls, context);
+        ZKStateStorageFactory zkStateStorageFactory = new ZKStateStorageFactory();
+        return zkStateStorageFactory.mkStore(config, auth_conf, acls, context);
     }
 
     public PacemakerClient initMakeClientImpl(Map config) {

File: storm-core/src/jvm/org/apache/storm/topology/CheckpointTupleForwarder.java
Patch:
@@ -116,7 +116,6 @@ protected void handleCheckpoint(Tuple checkpointTuple, Action action, long txid)
     protected void handleTuple(Tuple input) {
         collector.setContext(input);
         bolt.execute(input);
-        collector.ack(input);
     }
 
     /**

File: storm-core/src/jvm/org/apache/storm/topology/BasicOutputCollector.java
Patch:
@@ -56,7 +56,7 @@ public void emitDirect(int taskId, List<Object> tuple) {
     * Resets the message timeout for any tuple trees to which the given tuple belongs.
     * The timeout is reset to Config.TOPOLOGY_MESSAGE_TIMEOUT_SECS.
     * Note that this is an expensive operation, and should be used sparingly.
-    * @param input the tuple to reset timeout for
+    * @param tuple the tuple to reset timeout for
     */
     public void resetTimeout(Tuple tuple){
         out.resetTimeout(tuple);

File: storm-core/src/jvm/org/apache/storm/task/OutputCollector.java
Patch:
@@ -221,6 +221,7 @@ public void fail(Tuple input) {
     /**
     * Resets the message timeout for any tuple trees to which the given tuple belongs.
     * The timeout is reset to Config.TOPOLOGY_MESSAGE_TIMEOUT_SECS.
+    * Note that this is an expensive operation, and should be used sparingly.
     * @param input the tuple to reset timeout for
     */
     @Override

File: storm-core/src/jvm/org/apache/storm/blobstore/LocalFsBlobStore.java
Patch:
@@ -40,7 +40,7 @@
 import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
-import java.util.Set;;
+import java.util.Set;
 
 import static org.apache.storm.blobstore.BlobStoreAclHandler.ADMIN;
 import static org.apache.storm.blobstore.BlobStoreAclHandler.READ;

File: storm-core/src/jvm/org/apache/storm/utils/ConfigUtils.java
Patch:
@@ -44,7 +44,7 @@ public class ConfigUtils {
 
     // A singleton instance allows us to mock delegated static methods in our
     // tests by subclassing.
-    private static ConfigUtils _instance = new ConfigUtils();;
+    private static ConfigUtils _instance = new ConfigUtils();
 
     /**
      * Provide an instance of this class for delegates to use.  To mock out

File: storm-core/src/jvm/org/apache/storm/stats/StatsUtil.java
Patch:
@@ -1156,6 +1156,9 @@ public static List extractDataFromHb(Map executor2hostPort, Map task2component,
     public static List extractDataFromHb(Map executor2hostPort, Map task2component, Map beats,
                                          boolean includeSys, StormTopology topology, String compId) {
         List ret = new ArrayList();
+        if (executor2hostPort == null) {
+            return ret;
+        }
         for (Object o : executor2hostPort.entrySet()) {
             Map.Entry entry = (Map.Entry) o;
             List key = (List) entry.getKey();

File: storm-core/test/jvm/org/apache/storm/utils/staticmocking/package-info.java
Patch:
@@ -92,4 +92,4 @@
  * This class should be removed when troublesome static methods have been
  * replaced in the code.
  */
-package org.apache.storm.testing.staticmocking;
+package org.apache.storm.utils.staticmocking;

File: storm-core/src/jvm/org/apache/storm/cluster/ClusterStateContext.java
Patch:
@@ -19,7 +19,7 @@
 package org.apache.storm.cluster;
 
 /**
- * This class is intended to provide runtime-context to ClusterStateFactory
+ * This class is intended to provide runtime-context to StateStorageFactory
  * implementors, giving information such as what daemon is creating it.
  */
 public class ClusterStateContext {

File: storm-core/src/jvm/org/apache/storm/cluster/IStormClusterState.java
Patch:
@@ -45,9 +45,9 @@ public interface IStormClusterState {
 
     public ClusterWorkerHeartbeat getWorkerHeartbeat(String stormId, String node, Long port);
 
-    public List<ProfileRequest> getWorkerProfileRequests(String stormId, NodeInfo nodeInfo, boolean isThrift);
+    public List<ProfileRequest> getWorkerProfileRequests(String stormId, NodeInfo nodeInfo);
 
-    public List<ProfileRequest> getTopologyProfileRequests(String stormId, boolean isThrift);
+    public List<ProfileRequest> getTopologyProfileRequests(String stormId);
 
     public void setWorkerProfileRequest(String stormId, ProfileRequest profileRequest);
 

File: storm-core/test/jvm/org/apache/storm/command/TestCLI.java
Patch:
@@ -32,13 +32,15 @@ public void testSimple() throws Exception {
            .opt("b", "bb", 1, CLI.AS_INT)
            .opt("c", "cc", 1, CLI.AS_INT, CLI.FIRST_WINS)
            .opt("d", "dd", null, CLI.AS_STRING, CLI.INTO_LIST)
+           .opt("e", "ee", null, CLI.AS_INT)
            .arg("A")
            .arg("B", CLI.AS_INT)
            .parse("-a100", "--aa", "200", "-c2", "-b", "50", "--cc", "100", "A-VALUE", "1", "2", "3", "-b40", "-d1", "-d2", "-d3");
-        assertEquals(6, values.size());
+        assertEquals(7, values.size());
         assertEquals("200", (String)values.get("a"));
         assertEquals((Integer)40, (Integer)values.get("b"));
         assertEquals((Integer)2, (Integer)values.get("c"));
+        assertEquals(null, values.get("e"));
 
         List<String> d = (List<String>)values.get("d");
         assertEquals(3, d.size());

File: storm-core/src/jvm/org/apache/storm/cluster/ClusterUtils.java
Patch:
@@ -94,7 +94,7 @@ public static void resetInstance() {
     public static List<ACL> mkTopoOnlyAcls(Map topoConf) throws NoSuchAlgorithmException {
         List<ACL> aclList = null;
         String payload = (String) topoConf.get(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_PAYLOAD);
-        if (Utils.isZkAuthenticationConfiguredStormServer(topoConf)) {
+        if (Utils.isZkAuthenticationConfiguredTopology(topoConf)) {
             aclList = new ArrayList<>();
             ACL acl1 = ZooDefs.Ids.CREATOR_ALL_ACL.get(0);
             aclList.add(acl1);

File: storm-core/test/jvm/org/apache/storm/utils/staticmocking/package-info.java
Patch:
@@ -92,4 +92,4 @@
  * This class should be removed when troublesome static methods have been
  * replaced in the code.
  */
-package org.apache.storm.testing.staticmocking;
+package org.apache.storm.utils.staticmocking;

File: storm-core/src/jvm/org/apache/storm/scheduler/DefaultScheduler.java
Patch:
@@ -38,7 +38,7 @@ private static Set<WorkerSlot> badSlots(Map<WorkerSlot, List<ExecutorDetails>> e
                 Integer workerCount = distribution.get(executorCount);
                 if (workerCount != null && workerCount > 0) {
                     slots.add(entry.getKey());
-                    executorCount--;
+                    workerCount--;
                     distribution.put(executorCount, workerCount);
                 }
             }

File: storm-core/src/jvm/org/apache/storm/Config.java
Patch:
@@ -2260,7 +2260,7 @@ public class Config extends HashMap<String, Object> {
      * The amount of memory a worker can exceed its allocation before cgroup will kill it
      */
     @isPositiveNumber
-    public static String STORM_CGROUP_MEMORY_MB_LIMIT_TOLERANCE_MARGIN = "storm.cgroup.memory.mb.limit.tolerance.margin";
+    public static String STORM_CGROUP_MEMORY_LIMIT_TOLERANCE_MARGIN_MB = "storm.cgroup.memory.limit.tolerance.margin.mb";
 
     public static void setClasspath(Map conf, String cp) {
         conf.put(Config.TOPOLOGY_CLASSPATH, cp);

File: storm-core/src/jvm/org/apache/storm/serialization/SerializationFactory.java
Patch:
@@ -24,6 +24,7 @@
 import org.apache.storm.serialization.types.HashMapSerializer;
 import org.apache.storm.serialization.types.HashSetSerializer;
 import org.apache.storm.transactional.TransactionAttempt;
+import org.apache.storm.trident.tuple.ConsList;
 import org.apache.storm.tuple.Values;
 import org.apache.storm.utils.ListDelegate;
 import org.apache.storm.utils.Utils;
@@ -68,6 +69,7 @@ public static Kryo getKryo(Map conf) {
         k.register(Values.class);
         k.register(org.apache.storm.metric.api.IMetricsConsumer.DataPoint.class);
         k.register(org.apache.storm.metric.api.IMetricsConsumer.TaskInfo.class);
+        k.register(ConsList.class);
         try {
             JavaBridge.registerPrimitives(k);
             JavaBridge.registerCollections(k);

File: storm-core/src/jvm/org/apache/storm/utils/Utils.java
Patch:
@@ -1557,11 +1557,11 @@ public static Object getConfiguredClass(Map conf, Object configKey) {
         return null;
     }
 
-    public static String logsFilename(String stormId, int port) {
+    public static String logsFilename(String stormId, String port) {
         return stormId + FILE_PATH_SEPARATOR + port + FILE_PATH_SEPARATOR + "worker.log";
     }
 
-    public static String eventLogsFilename(String stormId, int port) {
+    public static String eventLogsFilename(String stormId, String port) {
         return stormId + FILE_PATH_SEPARATOR + port + FILE_PATH_SEPARATOR + "events.log";
     }
 

File: storm-core/src/jvm/org/apache/storm/utils/Utils.java
Patch:
@@ -2353,8 +2353,8 @@ public void uncaughtException(Thread t, Throwable e) {
      * @return the newly created thread
      * @see java.lang.Thread
      */
-    public static SmartThread asyncLoop(final Callable afn, String threadName) {
-        return asyncLoop(afn, false, null, Thread.NORM_PRIORITY, false, true,
+    public static SmartThread asyncLoop(final Callable afn, String threadName, final Thread.UncaughtExceptionHandler eh) {
+        return asyncLoop(afn, false, eh, Thread.NORM_PRIORITY, false, true,
                 threadName);
     }
 

File: storm-core/src/jvm/org/apache/storm/daemon/metrics/MetricsUtils.java
Patch:
@@ -86,7 +86,7 @@ private static TimeUnit getTimeUnitForCofig(Map stormConf, String configName) {
     public static File getCsvLogDir(Map stormConf) {
         String csvMetricsLogDirectory = Utils.getString(stormConf.get(Config.STORM_DAEMON_METRICS_REPORTER_CSV_LOG_DIR), null);
         if (csvMetricsLogDirectory == null) {
-            csvMetricsLogDirectory = ConfigUtils.absoluteHealthCheckDir(stormConf);
+            csvMetricsLogDirectory = ConfigUtils.absoluteStormLocalDir(stormConf);
             csvMetricsLogDirectory = csvMetricsLogDirectory + ConfigUtils.FILE_SEPARATOR + "csvmetrics";
         }
         File csvMetricsDir = new File(csvMetricsLogDirectory);

File: storm-core/src/jvm/org/apache/storm/Config.java
Patch:
@@ -140,7 +140,7 @@ public class Config extends HashMap<String, Object> {
     public static final String STORM_META_SERIALIZATION_DELEGATE = "storm.meta.serialization.delegate";
 
     /**
-     * A list of daemon metrics  reporter plugin class names. The classes should implement
+     * A list of daemon metrics  reporter plugin class names.
      * These plugins must implement {@link org.apache.storm.daemon.metrics.reporters.PreparableReporter} interface.
      */
     @isStringList

File: storm-core/src/jvm/org/apache/storm/nimbus/ILeaderElector.java
Patch:
@@ -37,14 +37,14 @@ public interface ILeaderElector extends Closeable {
      * check isLeader() to perform any leadership action. This method can be called
      * multiple times so it needs to be idempotent.
      */
-    void addToLeaderLockQueue();
+    void addToLeaderLockQueue() throws Exception;
 
     /**
      * Removes the caller from the leader lock queue. If the caller is leader
      * also releases the lock. This method can be called multiple times so it needs
      * to be idempotent.
      */
-    void removeFromLeaderLockQueue();
+    void removeFromLeaderLockQueue() throws Exception;
 
     /**
      *
@@ -62,7 +62,7 @@ public interface ILeaderElector extends Closeable {
      *
      * @return list of current nimbus addresses, includes leader.
      */
-    List<NimbusInfo> getAllNimbuses();
+    List<NimbusInfo> getAllNimbuses()throws Exception;
 
     /**
      * Method called to allow for cleanup. once close this object can not be reused.

File: storm-core/test/jvm/org/apache/storm/blobstore/BlobStoreTest.java
Patch:
@@ -1,3 +1,4 @@
+
 /**
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file

File: storm-core/src/jvm/org/apache/storm/zookeeper/Zookeeper.java
Patch:
@@ -52,6 +52,7 @@
 import java.net.InetAddress;
 import java.net.InetSocketAddress;
 import java.net.UnknownHostException;
+import java.util.Arrays;
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
@@ -282,7 +283,7 @@ public static boolean exists(CuratorFramework zk, String path, boolean watch){
         return existsNode(zk, path, watch);
     }
 
-    public static NIOServerCnxnFactory mkInprocessZookeeper(String localdir, Integer port) throws Exception {
+    public static List mkInprocessZookeeper(String localdir, Integer port) throws Exception {
         LOG.info("Starting inprocess zookeeper at port {} and dir {}", port, localdir);
         File localfile = new File(localdir);
         ZooKeeperServer zk = new ZooKeeperServer(localfile, localfile, 2000);
@@ -306,7 +307,7 @@ public static NIOServerCnxnFactory mkInprocessZookeeper(String localdir, Integer
             }
         }
         factory.startup(zk);
-        return factory;
+        return Arrays.asList((Object)new Long(report), (Object)factory);
     }
 
     public static void shutdownInprocessZookeeper(Factory handle) {

File: storm-core/src/jvm/org/apache/storm/nimbus/ILeaderElector.java
Patch:
@@ -37,14 +37,14 @@ public interface ILeaderElector extends Closeable {
      * check isLeader() to perform any leadership action. This method can be called
      * multiple times so it needs to be idempotent.
      */
-    void addToLeaderLockQueue();
+    void addToLeaderLockQueue() throws Exception;
 
     /**
      * Removes the caller from the leader lock queue. If the caller is leader
      * also releases the lock. This method can be called multiple times so it needs
      * to be idempotent.
      */
-    void removeFromLeaderLockQueue();
+    void removeFromLeaderLockQueue() throws Exception;
 
     /**
      *
@@ -62,7 +62,7 @@ public interface ILeaderElector extends Closeable {
      *
      * @return list of current nimbus addresses, includes leader.
      */
-    List<NimbusInfo> getAllNimbuses();
+    List<NimbusInfo> getAllNimbuses()throws Exception;
 
     /**
      * Method called to allow for cleanup. once close this object can not be reused.

File: storm-core/test/jvm/org/apache/storm/blobstore/BlobStoreTest.java
Patch:
@@ -1,3 +1,4 @@
+
 /**
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file

File: external/storm-kafka/src/jvm/org/apache/storm/kafka/PartitionManager.java
Patch:
@@ -320,7 +320,7 @@ public KafkaMessageId(Partition partition, long offset) {
         }
     }
 
-    static class OffsetData {
+    public static class OffsetData {
         public long latestEmittedOffset;
         public long latestCompletedOffset;
 

File: storm-core/src/jvm/org/apache/storm/messaging/netty/KerberosSaslNettyClient.java
Patch:
@@ -35,7 +35,6 @@
 import javax.security.sasl.Sasl;
 import javax.security.sasl.SaslClient;
 import javax.security.sasl.SaslException;
-import org.apache.zookeeper.Login;
 import org.apache.zookeeper.server.auth.KerberosName;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;

File: storm-core/src/jvm/org/apache/storm/messaging/netty/KerberosSaslNettyServer.java
Patch:
@@ -39,7 +39,6 @@
 import javax.security.sasl.Sasl;
 import javax.security.sasl.SaslException;
 import javax.security.sasl.SaslServer;
-import org.apache.zookeeper.Login;
 import org.apache.zookeeper.server.auth.KerberosName;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;

File: storm-core/src/jvm/org/apache/storm/security/auth/ThriftClient.java
Patch:
@@ -36,6 +36,7 @@ public class ThriftClient {
     private Map _conf;
     private ThriftConnectionType _type;
     private String _asUser;
+    protected boolean _retryForever = false;
 
     public ThriftClient(Map storm_conf, ThriftConnectionType type, String host) {
         this(storm_conf, type, host, null, null, null);
@@ -93,7 +94,8 @@ public synchronized void reconnect() {
                 = new TBackoffConnect(
                                       Utils.getInt(_conf.get(Config.STORM_NIMBUS_RETRY_TIMES)),
                                       Utils.getInt(_conf.get(Config.STORM_NIMBUS_RETRY_INTERVAL)),
-                                      Utils.getInt(_conf.get(Config.STORM_NIMBUS_RETRY_INTERVAL_CEILING)));
+                                      Utils.getInt(_conf.get(Config.STORM_NIMBUS_RETRY_INTERVAL_CEILING)),
+                                      _retryForever);
             _transport = connectionRetry.doConnectWithRetry(transportPlugin, socket, _host, _asUser);
         } catch (IOException ex) {
             throw new RuntimeException(ex);

File: storm-core/src/jvm/org/apache/storm/security/auth/kerberos/KerberosSaslTransportPlugin.java
Patch:
@@ -33,13 +33,13 @@
 import javax.security.auth.login.LoginException;
 import javax.security.sasl.Sasl;
 
+import org.apache.storm.messaging.netty.Login;
 import org.apache.commons.lang.StringUtils;
 import org.apache.thrift.transport.TSaslClientTransport;
 import org.apache.thrift.transport.TSaslServerTransport;
 import org.apache.thrift.transport.TTransport;
 import org.apache.thrift.transport.TTransportException;
 import org.apache.thrift.transport.TTransportFactory;
-import org.apache.zookeeper.Login;
 import org.apache.zookeeper.server.auth.KerberosName;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;

File: storm-core/src/jvm/org/apache/storm/utils/DRPCClient.java
Patch:
@@ -37,13 +37,15 @@ public class DRPCClient extends ThriftClient implements DistributedRPC.Iface {
 
     public DRPCClient(Map conf, String host, int port) throws TTransportException {
         this(conf, host, port, null);
+        _retryForever = true;
     }
 
     public DRPCClient(Map conf, String host, int port, Integer timeout) throws TTransportException {
         super(conf, ThriftConnectionType.DRPC, host, port, timeout, null);
         this.host = host;
         this.port = port;
         this.client = new DistributedRPC.Client(_protocol);
+        _retryForever = true;
     }
         
     public String getHost() {

File: storm-core/src/jvm/org/apache/storm/Config.java
Patch:
@@ -1374,7 +1374,9 @@ public class Config extends HashMap<String, Object> {
     public static final String WORKER_PROFILER_CHILDOPTS = "worker.profiler.childopts";
 
     /**
-     * This configuration would enable or disable component page profiing and debugging for workers.
+     * Enable profiling of worker JVMs using Oracle's Java Flight Recorder.
+     * Unlocking commercial features requires a special license from Oracle.
+     * See http://www.oracle.com/technetwork/java/javase/terms/products/index.html
      */
     @isBoolean
     public static final String WORKER_PROFILER_ENABLED = "worker.profiler.enabled";

File: storm-core/src/jvm/org/apache/storm/messaging/netty/Login.java
Patch:
@@ -47,7 +47,7 @@ public class Login {
     Logger LOG = Logger.getLogger(Login.class);
     public CallbackHandler callbackHandler;
 
-    // LoginThread will sleep until 80% of time from last refresh to
+    // Login will sleep until 80% of time from last refresh to
     // ticket's expiry has been reached, at which time it will wake
     // and try to renew the ticket.
     private static final float TICKET_RENEW_WINDOW = 0.80f;
@@ -79,7 +79,7 @@ public class Login {
     private long lastLogin = 0;
 
     /**
-     * LoginThread constructor. The constructor starts the thread used
+     * Login constructor. The constructor starts the thread used
      * to periodically re-login to the Kerberos Ticket Granting Server.
      * @param loginContextName
      *               name of section in JAAS file that will be use to login.

File: storm-core/src/jvm/org/apache/storm/scheduler/SchedulerAssignment.java
Patch:
@@ -17,6 +17,7 @@
  */
 package org.apache.storm.scheduler;
 
+import java.util.Collection;
 import java.util.Map;
 import java.util.Set;
 
@@ -55,4 +56,6 @@ public interface SchedulerAssignment {
     public Set<ExecutorDetails> getExecutors();
     
     public Set<WorkerSlot> getSlots();
+
+    public Map<WorkerSlot, Collection<ExecutorDetails>> getSlotToExecutors();
 }

File: storm-core/src/jvm/org/apache/storm/scheduler/TopologyDetails.java
Patch:
@@ -420,7 +420,7 @@ public void addResourcesForExec(ExecutorDetails exec, Map<String, Double> resour
     /**
      * Add default resource requirements for a executor
      */
-    public void addDefaultResforExec(ExecutorDetails exec) {
+    private void addDefaultResforExec(ExecutorDetails exec) {
         Double topologyComponentCpuPcorePercent = Utils.getDouble(this.topologyConf.get(Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT), null);
         Double topologyComponentResourcesOffheapMemoryMb = Utils.getDouble(this.topologyConf.get(Config.TOPOLOGY_COMPONENT_RESOURCES_OFFHEAP_MEMORY_MB), null);
         Double topologyComponentResourcesOnheapMemoryMb = Utils.getDouble(this.topologyConf.get(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB), null);

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/bolt/AbstractHdfsBolt.java
Patch:
@@ -216,7 +216,7 @@ public final void execute(Tuple tuple) {
 
     @Override
     public Map<String, Object> getComponentConfiguration() {
-        return TupleUtils.putTickFreqencyIntoComponentConfig(super.getComponentConfiguration(), tickTupleInterval);
+        return TupleUtils.putTickFrequencyIntoComponentConfig(super.getComponentConfiguration(), tickTupleInterval);
     }
 
     @Override

File: external/storm-hive/src/main/java/org/apache/storm/hive/bolt/HiveBolt.java
Patch:
@@ -188,7 +188,7 @@ public void cleanup() {
 
     @Override
     public Map<String, Object> getComponentConfiguration() {
-        return TupleUtils.putTickFreqencyIntoComponentConfig(super.getComponentConfiguration(),
+        return TupleUtils.putTickFrequencyIntoComponentConfig(super.getComponentConfiguration(),
                 options.getTickTupleInterval());
     }
 

File: external/storm-solr/src/main/java/org/apache/storm/solr/bolt/SolrUpdateBolt.java
Patch:
@@ -153,7 +153,7 @@ private List<Tuple> getQueuedTuples() {
 
     @Override
     public Map<String, Object> getComponentConfiguration() {
-        return TupleUtils.putTickFreqencyIntoComponentConfig(super.getComponentConfiguration(), tickTupleInterval);
+        return TupleUtils.putTickFrequencyIntoComponentConfig(super.getComponentConfiguration(), tickTupleInterval);
     }
 
     @Override

File: storm-core/src/jvm/org/apache/storm/utils/TupleUtils.java
Patch:
@@ -48,7 +48,7 @@ public static <T> int listHashCode(List<T> alist) {
       }
     }
 
-    public static Map<String, Object> putTickFreqencyIntoComponentConfig(Map<String, Object> conf, int tickFreqSecs) {
+    public static Map<String, Object> putTickFrequencyIntoComponentConfig(Map<String, Object> conf, int tickFreqSecs) {
       if (conf == null) {
           conf = new Config();
       }

File: storm-core/src/jvm/org/apache/storm/scheduler/SchedulerAssignment.java
Patch:
@@ -17,6 +17,7 @@
  */
 package org.apache.storm.scheduler;
 
+import java.util.Collection;
 import java.util.Map;
 import java.util.Set;
 
@@ -55,4 +56,6 @@ public interface SchedulerAssignment {
     public Set<ExecutorDetails> getExecutors();
     
     public Set<WorkerSlot> getSlots();
+
+    public Map<WorkerSlot, Collection<ExecutorDetails>> getSlotToExecutors();
 }

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/spout/FileReader.java
Patch:
@@ -18,7 +18,6 @@
 
 package org.apache.storm.hdfs.spout;
 
-import backtype.storm.tuple.Fields;
 import org.apache.hadoop.fs.Path;
 
 import java.io.IOException;

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/spout/Configs.java
Patch:
@@ -23,6 +23,7 @@ public class Configs {
   public static final String TEXT = "text";
   public static final String SEQ = "seq";
 
+  public static final String HDFS_URI = "hdfsspout.hdfs";                   // Required - HDFS name node
   public static final String SOURCE_DIR = "hdfsspout.source.dir";           // Required - dir from which to read files
   public static final String ARCHIVE_DIR = "hdfsspout.archive.dir";         // Required - completed files will be moved here
   public static final String BAD_DIR = "hdfsspout.badfiles.dir";            // Required - unparsable files will be moved here

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/spout/SequenceFileReader.java
Patch:
@@ -37,6 +37,7 @@ public class SequenceFileReader<Key extends Writable,Value extends Writable>
         extends AbstractFileReader {
   private static final Logger log = LoggerFactory
           .getLogger(SequenceFileReader.class);
+  public static final String[] defaultFields = {"key", "value"};
   private static final int DEFAULT_BUFF_SIZE = 4096;
   public static final String BUFFER_SIZE = "hdfsspout.reader.buffer.bytes";
 

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/spout/TextFileReader.java
Patch:
@@ -34,6 +34,7 @@
 
 // Todo: Track file offsets instead of line number
 class TextFileReader extends AbstractFileReader {
+  public static final String[] defaultFields = {"line"};
   public static final String CHARSET = "hdfsspout.reader.charset";
   public static final String BUFFER_SIZE = "hdfsspout.reader.buffer.bytes";
 

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/spout/FileOffset.java
Patch:
@@ -32,5 +32,5 @@
 interface FileOffset extends Comparable<FileOffset>, Cloneable {
   /** tests if rhs == currOffset+1 */
   boolean isNextOffset(FileOffset rhs);
-  public FileOffset clone();
+  FileOffset clone();
 }
\ No newline at end of file

File: external/storm-hdfs/src/test/java/org/apache/storm/hdfs/spout/TestHdfsSpout.java
Patch:
@@ -565,7 +565,7 @@ private Map getDefaultConfig() {
 
 
   private static HdfsSpout makeSpout(int spoutId, Map conf, String readerType) {
-    HdfsSpout spout = new HdfsSpout();
+    HdfsSpout spout = new HdfsSpout().withOutputFields("line");
     MockCollector collector = new MockCollector();
     conf.put(Configs.READER_TYPE, readerType);
     spout.open(conf, new MockTopologyContext(spoutId), collector);

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/spout/HdfsSpout.java
Patch:
@@ -130,10 +130,11 @@ public void nextTuple() {
         // 3) Select a new file if one is not open already
         if (reader == null) {
           reader = pickNextFile();
-          fileReadCompletely=false;
           if (reader == null) {
             LOG.debug("Currently no new files to process under : " + sourceDirPath);
             return;
+          } else {
+            fileReadCompletely=false;
           }
         }
         if( fileReadCompletely ) { // wait for more ACKs before proceeding

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/common/HdfsUtils.java
Patch:
@@ -24,6 +24,7 @@
 import org.apache.hadoop.fs.LocatedFileStatus;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.fs.RemoteIterator;
+import org.apache.hadoop.hdfs.DistributedFileSystem;
 import org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException;
 import org.apache.hadoop.ipc.RemoteException;
 

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/spout/SequenceFileReader.java
Patch:
@@ -150,6 +150,8 @@ public Offset(long lastSyncPoint, long recordsSinceLastSync, long currentRecord
 
     public Offset(String offset) {
       try {
+        if(offset==null)
+          throw new IllegalArgumentException("offset cannot be null");
         String[] parts = offset.split(",");
         this.lastSyncPoint = Long.parseLong(parts[0].split("=")[1]);
         this.recordsSinceLastSync = Long.parseLong(parts[1].split("=")[1]);
@@ -169,7 +171,7 @@ public String toString() {
               "sync=" + lastSyncPoint +
               ":afterSync=" + recordsSinceLastSync +
               ":record=" + currentRecord +
-              '}';
+              ":}";
     }
 
     @Override

File: external/storm-hdfs/src/test/java/org/apache/storm/hdfs/spout/TestFileLock.java
Patch:
@@ -314,7 +314,7 @@ public void testLockRecovery() throws Exception {
     }
   }
 
-  private void closeUnderlyingLockFile(FileLock lock) throws ReflectiveOperationException {
+  public static void closeUnderlyingLockFile(FileLock lock) throws ReflectiveOperationException {
     Method m = FileLock.class.getDeclaredMethod("forceCloseLockFile");
     m.setAccessible(true);
     m.invoke(lock);

File: external/storm-hdfs/src/test/java/org/apache/storm/hdfs/spout/TestProgressTracker.java
Patch:
@@ -60,7 +60,7 @@ public void testBasic() throws Exception {
 
     TextFileReader.Offset currOffset = reader.getFileOffset();
     Assert.assertNotNull(currOffset);
-    Assert.assertEquals(0, currOffset.byteOffset);
+    Assert.assertEquals(0, currOffset.charOffset);
 
     // read 1st line and ack
     Assert.assertNotNull(reader.next());

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/common/HdfsUtils.java
Patch:
@@ -49,7 +49,7 @@ public static ArrayList<Path> listFilesByModificationTime(FileSystem fs, Path di
         fstats.add(fileStatus);
       }
     }
-    Collections.sort(fstats, new CmpFilesByModificationTime() );
+    Collections.sort(fstats, new ModifTimeComparator() );
 
     ArrayList<Path> result = new ArrayList<>(fstats.size());
     for (LocatedFileStatus fstat : fstats) {
@@ -59,7 +59,7 @@ public static ArrayList<Path> listFilesByModificationTime(FileSystem fs, Path di
   }
 
   /**
-   * Returns true if succeeded. False if file already exists. throws if there was unexpected problem
+   * Returns null if file already exists. throws if there was unexpected problem
    */
   public static FSDataOutputStream tryCreateFile(FileSystem fs, Path file) throws IOException {
     try {

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/common/ModifTimeComparator.java
Patch:
@@ -23,7 +23,7 @@
 import java.util.Comparator;
 
 
-public class CmpFilesByModificationTime
+public class ModifTimeComparator
         implements Comparator<FileStatus> {
    @Override
     public int compare(FileStatus o1, FileStatus o2) {

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/spout/AbstractFileReader.java
Patch:
@@ -26,13 +26,11 @@
 abstract class AbstractFileReader implements FileReader {
 
   private final Path file;
-  private final FileSystem fs;
   private Fields fields;
 
   public AbstractFileReader(FileSystem fs, Path file, Fields fieldNames) {
     if (fs == null || file == null)
       throw new IllegalArgumentException("file and filesystem args cannot be null");
-    this.fs = fs;
     this.file = file;
     this.fields = fieldNames;
   }

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/common/HdfsUtils.java
Patch:
@@ -26,7 +26,6 @@
 import org.apache.hadoop.fs.RemoteIterator;
 import org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException;
 import org.apache.hadoop.ipc.RemoteException;
-import org.apache.storm.hdfs.spout.DirLock;
 
 import java.io.IOException;
 import java.util.ArrayList;

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/spout/HdfsSpout.java
Patch:
@@ -504,7 +504,7 @@ private FileLock getOldestExpiredLock() throws IOException {
       // 3 - if clocks are not in sync ..
       if( lastExpiredLock == null ) {
         // just make a note of the oldest expired lock now and check if its still unmodified after lockTimeoutSec
-        lastExpiredLock = FileLock.locateOldestExpiredLock(hdfs, lockDirPath, lockTimeoutSec, spoutId);
+        lastExpiredLock = FileLock.locateOldestExpiredLock(hdfs, lockDirPath, lockTimeoutSec);
         lastExpiredLockTime = System.currentTimeMillis();
         return null;
       }

File: external/storm-hdfs/src/test/java/org/apache/storm/hdfs/spout/TestDirLock.java
Patch:
@@ -76,7 +76,7 @@ public void tearDown() throws Exception {
     fs.delete(lockDir, true);
   }
 
-  @Test
+//  @Test
   public void testConcurrentLocking() throws Exception {
 //    -Dlog4j.configuration=config
     Logger.getRootLogger().setLevel(Level.ERROR);

File: storm-core/src/jvm/org/apache/storm/scheduler/Cluster.java
Patch:
@@ -32,11 +32,13 @@
 public class Cluster {
 
     /**
-     * key: supervisor id, value: supervisor details
+     * key: supervisor id,
+     * value: supervisor's total and used resources, i.e. {totalMem, totalCpu, usedMem, usedCpu}
      */
     private Map<String, SupervisorDetails> supervisors;
     /**
      * key: supervisor id, value: supervisor's total and used resources
+     * value: {requestedMemOnHeap, requestedMemOffHeap, requestedCpu, assignedMemOnHeap, assignedMemOffHeap, assignedCpu}
      */
     private Map<String, Double[]> supervisorsResources;
 

File: storm-core/src/jvm/org/apache/storm/topology/IStatefulWindowedBolt.java
Patch:
@@ -15,9 +15,9 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package backtype.storm.topology;
+package org.apache.storm.topology;
 
-import backtype.storm.state.State;
+import org.apache.storm.state.State;
 
 /**
  * A windowed bolt abstraction for supporting windowing operation with state

File: storm-core/src/jvm/org/apache/storm/topology/TopologyBuilder.java
Patch:
@@ -252,7 +252,7 @@ public <T extends State> BoltDeclarer setBolt(String id, IStatefulBolt<T> bolt,
      * @param id the id of this component. This id is referenced by other components that want to consume this bolt's outputs.
      * @param bolt the stateful windowed bolt
      * @param parallelism_hint the number of tasks that should be assigned to execute this bolt. Each task will run on a thread in a process somwehere around the cluster.
-     * @param <T> the type of the state (e.g. {@link backtype.storm.state.KeyValueState})
+     * @param <T> the type of the state (e.g. {@link org.apache.storm.state.KeyValueState})
      * @return use the returned object to declare the inputs to this component
      * @throws IllegalArgumentException if {@code parallelism_hint} is not positive
      */

File: storm-core/src/jvm/org/apache/storm/topology/base/BaseWindowedBolt.java
Patch:
@@ -22,7 +22,6 @@
 import org.apache.storm.task.TopologyContext;
 import org.apache.storm.topology.IWindowedBolt;
 import org.apache.storm.topology.OutputFieldsDeclarer;
-import org.apache.storm.windowing.TupleWindow;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 

File: external/storm-redis/src/main/java/org/apache/storm/redis/state/RedisKeyValueState.java
Patch:
@@ -17,9 +17,9 @@
  */
 package org.apache.storm.redis.state;
 
-import backtype.storm.state.DefaultStateSerializer;
-import backtype.storm.state.KeyValueState;
-import backtype.storm.state.Serializer;
+import org.apache.storm.state.DefaultStateSerializer;
+import org.apache.storm.state.KeyValueState;
+import org.apache.storm.state.Serializer;
 import org.apache.storm.redis.common.config.JedisPoolConfig;
 import org.apache.storm.redis.common.container.JedisCommandsContainerBuilder;
 import org.apache.storm.redis.common.container.JedisCommandsInstanceContainer;

File: external/storm-redis/src/test/java/org/apache/storm/redis/state/DefaultStateSerializerTest.java
Patch:
@@ -17,9 +17,9 @@
  */
 package org.apache.storm.redis.state;
 
-import backtype.storm.spout.CheckPointState;
-import backtype.storm.state.DefaultStateSerializer;
-import backtype.storm.state.Serializer;
+import org.apache.storm.spout.CheckPointState;
+import org.apache.storm.state.DefaultStateSerializer;
+import org.apache.storm.state.Serializer;
 import org.junit.Test;
 
 import java.util.ArrayList;

File: external/storm-redis/src/test/java/org/apache/storm/redis/state/RedisKeyValueStateProviderTest.java
Patch:
@@ -17,8 +17,8 @@
  */
 package org.apache.storm.redis.state;
 
-import backtype.storm.Config;
-import backtype.storm.state.State;
+import org.apache.storm.Config;
+import org.apache.storm.state.State;
 import org.junit.Assert;
 import org.junit.Test;
 

File: external/storm-redis/src/test/java/org/apache/storm/redis/state/RedisKeyValueStateTest.java
Patch:
@@ -17,7 +17,7 @@
  */
 package org.apache.storm.redis.state;
 
-import backtype.storm.state.DefaultStateSerializer;
+import org.apache.storm.state.DefaultStateSerializer;
 import org.apache.storm.redis.common.container.JedisCommandsInstanceContainer;
 import org.junit.Before;
 import org.junit.Test;

File: storm-core/src/jvm/org/apache/storm/state/DefaultStateSerializer.java
Patch:
@@ -15,7 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package backtype.storm.state;
+package org.apache.storm.state;
 
 import com.esotericsoftware.kryo.Kryo;
 import com.esotericsoftware.kryo.io.Input;

File: storm-core/src/jvm/org/apache/storm/state/InMemoryKeyValueState.java
Patch:
@@ -15,7 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package backtype.storm.state;
+package org.apache.storm.state;
 
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;

File: storm-core/src/jvm/org/apache/storm/state/InMemoryKeyValueStateProvider.java
Patch:
@@ -15,9 +15,9 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package backtype.storm.state;
+package org.apache.storm.state;
 
-import backtype.storm.task.TopologyContext;
+import org.apache.storm.task.TopologyContext;
 
 import java.util.Map;
 import java.util.concurrent.ConcurrentHashMap;

File: storm-core/src/jvm/org/apache/storm/state/KeyValueState.java
Patch:
@@ -15,7 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package backtype.storm.state;
+package org.apache.storm.state;
 
 /**
  * A state that supports key-value mappings.

File: storm-core/src/jvm/org/apache/storm/state/Serializer.java
Patch:
@@ -15,7 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package backtype.storm.state;
+package org.apache.storm.state;
 
 import java.io.Serializable;
 

File: storm-core/src/jvm/org/apache/storm/state/StateProvider.java
Patch:
@@ -15,9 +15,9 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package backtype.storm.state;
+package org.apache.storm.state;
 
-import backtype.storm.task.TopologyContext;
+import org.apache.storm.task.TopologyContext;
 
 import java.util.Map;
 

File: storm-core/src/jvm/org/apache/storm/topology/IStatefulBolt.java
Patch:
@@ -15,9 +15,9 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package backtype.storm.topology;
+package org.apache.storm.topology;
 
-import backtype.storm.state.State;
+import org.apache.storm.state.State;
 
 /**
  * A bolt abstraction for supporting stateful computation.

File: storm-core/src/jvm/org/apache/storm/topology/IStatefulComponent.java
Patch:
@@ -15,9 +15,9 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package backtype.storm.topology;
+package org.apache.storm.topology;
 
-import backtype.storm.state.State;
+import org.apache.storm.state.State;
 
 /**
  * <p>

File: storm-core/test/jvm/org/apache/storm/state/InMemoryKeyValueStateTest.java
Patch:
@@ -15,8 +15,10 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package backtype.storm.state;
+package org.apache.storm.state;
 
+import org.apache.storm.state.InMemoryKeyValueState;
+import org.apache.storm.state.KeyValueState;
 import org.junit.Before;
 import org.junit.Test;
 

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/bolt/HBaseBolt.java
Patch:
@@ -95,7 +95,7 @@ public void execute(Tuple tuple) {
         boolean flush = false;
         try {
             if (TupleUtils.isTick(tuple)) {
-                LOG.debug("TICK received! current batch status [" + tupleBatch.size() + "/" + batchSize + "]");
+                LOG.debug("TICK received! current batch status [{}/{}]", tupleBatch.size(), batchSize);
                 collector.ack(tuple);
                 flush = true;
             } else {

File: external/storm-hive/src/main/java/org/apache/storm/hive/bolt/HiveBolt.java
Patch:
@@ -112,7 +112,7 @@ public void execute(Tuple tuple) {
         try {
             boolean forceFlush = false;
             if (TupleUtils.isTick(tuple)) {
-                LOG.debug("TICK received! current batch status [" + tupleBatch.size() + "/" + options.getBatchSize() + "]");
+                LOG.debug("TICK received! current batch status [{}/{}]", tupleBatch.size(), options.getBatchSize());
                 collector.ack(tuple);
                 forceFlush = true;
             }

File: external/storm-kafka/src/jvm/storm/kafka/ZkState.java
Patch:
@@ -66,7 +66,7 @@ public ZkState(Map stateConf) {
     }
 
     public void writeJSON(String path, Map<Object, Object> data) {
-        LOG.debug("Writing " + path + " the data " + data.toString());
+        LOG.debug("Writing {} the data {}", path, data.toString());
         writeBytes(path, JSONValue.toJSONString(data).getBytes(Charset.forName("UTF-8")));
     }
 

File: storm-core/src/jvm/backtype/storm/spout/RawMultiScheme.java
Patch:
@@ -21,6 +21,7 @@
 import java.util.List;
 
 import backtype.storm.tuple.Fields;
+import backtype.storm.utils.Utils;
 
 
 import static backtype.storm.utils.Utils.tuple;
@@ -29,7 +30,7 @@
 public class RawMultiScheme implements MultiScheme {
   @Override
   public Iterable<List<Object>> deserialize(ByteBuffer ser) {
-    return asList(tuple(ser));
+    return asList(tuple(Utils.toByteArray(ser)));
   }
 
   @Override

File: storm-core/src/jvm/backtype/storm/security/auth/DefaultHttpCredentialsPlugin.java
Patch:
@@ -79,6 +79,8 @@ public ReqContext populateContext(ReqContext context,
         if(doAsUser != null) {
             context.setRealPrincipal(new SingleUserPrincipal(userName));
             userName = doAsUser;
+        } else {
+            context.setRealPrincipal(null);
         }
 
         Set<Principal> principals = new HashSet<>();

File: storm-core/src/jvm/backtype/storm/security/auth/digest/ServerCallbackHandler.java
Patch:
@@ -123,6 +123,8 @@ private void handleAuthorizeCallback(AuthorizeCallback ac) {
         if(!authenticationID.equals(ac.getAuthorizationID())) {
             LOG.info("Impersonation attempt  authenticationID = " + ac.getAuthenticationID() + " authorizationID = " + ac.getAuthorizationID());
             ReqContext.context().setRealPrincipal(new SaslTransportPlugin.User(ac.getAuthenticationID()));
+        } else {
+            ReqContext.context().setRealPrincipal(null);
         }
 
         ac.setAuthorized(true);

File: storm-core/src/jvm/backtype/storm/utils/NimbusClient.java
Patch:
@@ -63,7 +63,7 @@ public static NimbusClient getConfiguredClientAs(Map conf, String asUser) {
             int port = Integer.parseInt(conf.get(Config.NIMBUS_THRIFT_PORT).toString());
             ClusterSummary clusterInfo;
             try {
-                NimbusClient client = new NimbusClient(conf, host, port);
+                NimbusClient client = new NimbusClient(conf, host, port, null, asUser);
                 clusterInfo = client.getClient().getClusterInfo();
             } catch (Exception e) {
                 LOG.warn("Ignoring exception while trying to get leader nimbus info from " + host

File: storm-core/src/jvm/backtype/storm/utils/Utils.java
Patch:
@@ -596,7 +596,7 @@ public static void restrictPermissions(String baseDir) {
     }
 
 
-    public static IFn loadClojureFn(String namespace, String name) {
+    public static synchronized IFn loadClojureFn(String namespace, String name) {
         try {
             clojure.lang.Compiler.eval(RT.readString("(require '" + namespace + ")"));
         } catch (Exception e) {

File: storm-rename-hack/src/main/java/org/apache/storm/hack/resource/ResourceTransformer.java
Patch:
@@ -39,4 +39,6 @@ public interface ResourceTransformer
 
     void processResource( String resource, InputStream is, List<Relocator> relocators )
         throws IOException;
+
+    public void modifyOutputStream(JarOutputStream jarOut) throws IOException;
 }

File: storm-core/src/jvm/backtype/storm/spout/RawMultiScheme.java
Patch:
@@ -21,6 +21,7 @@
 import java.util.List;
 
 import backtype.storm.tuple.Fields;
+import backtype.storm.utils.Utils;
 
 
 import static backtype.storm.utils.Utils.tuple;
@@ -29,7 +30,7 @@
 public class RawMultiScheme implements MultiScheme {
   @Override
   public Iterable<List<Object>> deserialize(ByteBuffer ser) {
-    return asList(tuple(ser));
+    return asList(tuple(Utils.toByteArray(ser)));
   }
 
   @Override

File: external/storm-mqtt/core/src/main/java/org/apache/storm/mqtt/bolt/MqttBolt.java
Patch:
@@ -89,11 +89,12 @@ public void execute(Tuple input) {
         if(!TupleUtils.isTick(input)){
             MqttMessage message = this.mapper.toMessage(input);
             try {
-                this.publisher.publish(message, this.retain);
+                this.publisher.publish(message);
                 this.collector.ack(input);
             } catch (Exception e) {
                 LOG.warn("Error publishing MQTT message. Failing tuple.", e);
                 // should we fail the tuple or kill the worker?
+                collector.reportError(e);
                 collector.fail(input);
             }
         } else {

File: external/storm-mqtt/core/src/main/java/org/apache/storm/mqtt/common/MqttOptions.java
Patch:
@@ -77,7 +77,7 @@ public boolean isCleanConnection() {
 
     /**
      * Set to false if you want the MQTT server to persist topic subscriptions and ack positions across client sessions.
-     * Defaults to true.
+     * Defaults to false.
      *
      * @param cleanConnection
      */

File: external/storm-mqtt/core/src/main/java/org/apache/storm/mqtt/trident/MqttPublishFunction.java
Patch:
@@ -74,7 +74,7 @@ public void prepare(Map conf, TridentOperationContext context) {
     public void execute(TridentTuple tuple, TridentCollector collector) {
         MqttMessage message = this.mapper.toMessage(tuple);
         try {
-            this.publisher.publish(message, this.retain);
+            this.publisher.publish(message);
         } catch (Exception e) {
             LOG.warn("Error publishing MQTT message. Failing tuple.", e);
             // should we fail the batch or kill the worker?

File: external/storm-kafka/src/test/storm/kafka/ExponentialBackoffMsgRetryManagerTest.java
Patch:
@@ -50,14 +50,14 @@ public void testImmediateRetry() throws Exception {
 
     @Test
     public void testSingleDelay() throws Exception {
-        ExponentialBackoffMsgRetryManager manager = new ExponentialBackoffMsgRetryManager(10, 1d, 100);
+        ExponentialBackoffMsgRetryManager manager = new ExponentialBackoffMsgRetryManager(100, 1d, 1000);
         manager.failed(TEST_OFFSET);
         Thread.sleep(5);
         Long next = manager.nextFailedMessageToRetry();
         assertNull("expect no message ready for retry yet", next);
         assertFalse("message should not be ready for retry yet", manager.shouldRetryMsg(TEST_OFFSET));
 
-        Thread.sleep(10);
+        Thread.sleep(100);
         next = manager.nextFailedMessageToRetry();
         assertEquals("expect test offset next available for retry", TEST_OFFSET, next);
         assertTrue("message should be ready for retry", manager.shouldRetryMsg(TEST_OFFSET));

File: external/storm-kafka/src/test/storm/kafka/ExponentialBackoffMsgRetryManagerTest.java
Patch:
@@ -163,9 +163,9 @@ public void testFailRetryRetry() throws Exception {
 
     @Test
     public void testMaxBackoff() throws Exception {
-        final long initial = 10;
+        final long initial = 100;
         final double mult = 2d;
-        final long max = 20;
+        final long max = 2000;
         ExponentialBackoffMsgRetryManager manager = new ExponentialBackoffMsgRetryManager(initial, mult, max);
 
         long expectedWaitTime = initial;

File: external/storm-hive/src/test/java/org/apache/storm/hive/bolt/TestHiveBolt.java
Patch:
@@ -380,10 +380,10 @@ public void testNoTickEmptyBatches() throws Exception
         bolt = new HiveBolt(hiveOptions);
         bolt.prepare(config, null, new OutputCollector(collector));
 
-        //The tick should NOT cause any acks since the batch was empty
+        //The tick should NOT cause any acks since the batch was empty except for acking itself
         Tuple mockTick = MockTupleHelpers.mockTickTuple();
         bolt.execute(mockTick);
-        verifyZeroInteractions(collector);
+        verify(collector).ack(mockTick);
 
         bolt.cleanup();
     }

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/bolt/HBaseBolt.java
Patch:
@@ -95,7 +95,7 @@ public void execute(Tuple tuple) {
         boolean flush = false;
         try {
             if (TupleUtils.isTick(tuple)) {
-                LOG.debug("TICK received! current batch status [" + tupleBatch.size() + "/" + batchSize + "]");
+                LOG.debug("TICK received! current batch status [{}/{}]", tupleBatch.size(), batchSize);
                 flush = true;
             } else {
                 byte[] rowKey = this.mapper.rowKey(tuple);

File: external/storm-hive/src/main/java/org/apache/storm/hive/bolt/HiveBolt.java
Patch:
@@ -112,7 +112,7 @@ public void execute(Tuple tuple) {
         try {
             boolean forceFlush = false;
             if (TupleUtils.isTick(tuple)) {
-                LOG.debug("TICK received! current batch status [" + tupleBatch.size() + "/" + options.getBatchSize() + "]");
+                LOG.debug("TICK received! current batch status [{}/{}]", tupleBatch.size(), options.getBatchSize());
                 forceFlush = true;
             }
             else {

File: external/storm-kafka/src/jvm/storm/kafka/ZkState.java
Patch:
@@ -66,7 +66,7 @@ public ZkState(Map stateConf) {
     }
 
     public void writeJSON(String path, Map<Object, Object> data) {
-        LOG.debug("Writing " + path + " the data " + data.toString());
+        LOG.debug("Writing {} the data {}", path, data.toString());
         writeBytes(path, JSONValue.toJSONString(data).getBytes(Charset.forName("UTF-8")));
     }
 

File: storm-core/src/jvm/backtype/storm/security/auth/DefaultHttpCredentialsPlugin.java
Patch:
@@ -79,6 +79,8 @@ public ReqContext populateContext(ReqContext context,
         if(doAsUser != null) {
             context.setRealPrincipal(new SingleUserPrincipal(userName));
             userName = doAsUser;
+        } else {
+            context.setRealPrincipal(null);
         }
 
         Set<Principal> principals = new HashSet<>();

File: storm-core/src/jvm/backtype/storm/security/auth/digest/ServerCallbackHandler.java
Patch:
@@ -123,6 +123,8 @@ private void handleAuthorizeCallback(AuthorizeCallback ac) {
         if(!authenticationID.equals(ac.getAuthorizationID())) {
             LOG.info("Impersonation attempt  authenticationID = " + ac.getAuthenticationID() + " authorizationID = " + ac.getAuthorizationID());
             ReqContext.context().setRealPrincipal(new SaslTransportPlugin.User(ac.getAuthenticationID()));
+        } else {
+            ReqContext.context().setRealPrincipal(null);
         }
 
         ac.setAuthorized(true);

File: storm-core/src/jvm/backtype/storm/utils/NimbusClient.java
Patch:
@@ -63,7 +63,7 @@ public static NimbusClient getConfiguredClientAs(Map conf, String asUser) {
             int port = Integer.parseInt(conf.get(Config.NIMBUS_THRIFT_PORT).toString());
             ClusterSummary clusterInfo;
             try {
-                NimbusClient client = new NimbusClient(conf, host, port);
+                NimbusClient client = new NimbusClient(conf, host, port, null, asUser);
                 clusterInfo = client.getClient().getClusterInfo();
             } catch (Exception e) {
                 LOG.warn("Ignoring exception while trying to get leader nimbus info from " + host

File: storm-core/src/jvm/backtype/storm/windowing/EventImpl.java
Patch:
@@ -17,7 +17,7 @@
  */
 package backtype.storm.windowing;
 
-class EventImpl<T> implements Event<T> {
+public class EventImpl<T> implements Event<T> {
     private final T event;
     private long ts;
 

File: storm-core/src/jvm/backtype/storm/windowing/TimeTriggerPolicy.java
Patch:
@@ -43,7 +43,7 @@ public TimeTriggerPolicy(long millis, TriggerHandler handler) {
         this(millis, handler, null);
     }
 
-    public TimeTriggerPolicy(long millis, TriggerHandler handler, EvictionPolicy evictionPolicy) {
+    public TimeTriggerPolicy(long millis, TriggerHandler handler, EvictionPolicy<T> evictionPolicy) {
         this.duration = millis;
         this.handler = handler;
         this.executor = Executors.newSingleThreadScheduledExecutor();

File: storm-core/src/jvm/backtype/storm/windowing/WatermarkTimeTriggerPolicy.java
Patch:
@@ -32,7 +32,7 @@ public class WatermarkTimeTriggerPolicy<T> implements TriggerPolicy<T> {
     private final WindowManager<T> windowManager;
     private long nextWindowEndTs = 0;
 
-    public WatermarkTimeTriggerPolicy(long slidingIntervalMs, TriggerHandler handler, EvictionPolicy evictionPolicy,
+    public WatermarkTimeTriggerPolicy(long slidingIntervalMs, TriggerHandler handler, EvictionPolicy<T> evictionPolicy,
                                       WindowManager<T> windowManager) {
         this.slidingIntervalMs = slidingIntervalMs;
         this.handler = handler;

File: storm-core/src/jvm/backtype/storm/blobstore/BlobStoreUtils.java
Patch:
@@ -43,7 +43,7 @@
 
 public class BlobStoreUtils {
     private static final String BLOBSTORE_SUBTREE="/blobstore";
-    private static final Logger LOG = LoggerFactory.getLogger(Utils.class);
+    private static final Logger LOG = LoggerFactory.getLogger(BlobStoreUtils.class);
 
     public static CuratorFramework createZKClient(Map conf) {
         List<String> zkServers = (List<String>) conf.get(Config.STORM_ZOOKEEPER_SERVERS);

File: storm-core/src/jvm/backtype/storm/blobstore/KeySequenceNumber.java
Patch:
@@ -117,7 +117,7 @@
  *  synchronization happens appropriately and all nimbodes have the same blob.
  */
 public class KeySequenceNumber {
-    private static final Logger LOG = LoggerFactory.getLogger(Utils.class);
+    private static final Logger LOG = LoggerFactory.getLogger(KeySequenceNumber.class);
     private final String BLOBSTORE_SUBTREE="/blobstore";
     private final String BLOBSTORE_MAX_KEY_SEQUENCE_SUBTREE="/blobstoremaxkeysequencenumber";
     private final String key;

File: storm-core/src/jvm/backtype/storm/blobstore/KeySequenceNumber.java
Patch:
@@ -117,7 +117,7 @@
  *  synchronization happens appropriately and all nimbodes have the same blob.
  */
 public class KeySequenceNumber {
-    private static final Logger LOG = LoggerFactory.getLogger(Utils.class);
+    private static final Logger LOG = LoggerFactory.getLogger(KeySequenceNumber.class);
     private final String BLOBSTORE_SUBTREE="/blobstore";
     private final String BLOBSTORE_MAX_KEY_SEQUENCE_SUBTREE="/blobstoremaxkeysequencenumber";
     private final String key;

File: storm-core/src/jvm/backtype/storm/blobstore/BlobStoreUtils.java
Patch:
@@ -43,7 +43,7 @@
 
 public class BlobStoreUtils {
     private static final String BLOBSTORE_SUBTREE="/blobstore";
-    private static final Logger LOG = LoggerFactory.getLogger(Utils.class);
+    private static final Logger LOG = LoggerFactory.getLogger(BlobStoreUtils.class);
 
     public static CuratorFramework createZKClient(Map conf) {
         List<String> zkServers = (List<String>) conf.get(Config.STORM_ZOOKEEPER_SERVERS);

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/bolt/AbstractHdfsBolt.java
Patch:
@@ -49,10 +49,9 @@ public abstract class AbstractHdfsBolt extends BaseRichBolt {
     private static final Logger LOG = LoggerFactory.getLogger(AbstractHdfsBolt.class);
     private static final Integer DEFAULT_RETRY_COUNT = 3;
     /**
-     * A low default value so that tuples are flushed and acked as soon as possible to avoid
-     * replay and spouts can continue emitting without hitting TOPOLOGY_MAX_SPOUT_PENDING.
+     * Half of the default Config.TOPOLOGY_MESSAGE_TIMEOUT_SECS
      */
-    private static final int DEFAULT_TICK_TUPLE_INTERVAL_SECS = 1;
+    private static final int DEFAULT_TICK_TUPLE_INTERVAL_SECS = 15;
 
     protected ArrayList<RotationAction> rotationActions = new ArrayList<RotationAction>();
     private Path currentFile;

File: external/storm-hive/src/main/java/org/apache/storm/hive/common/HiveOptions.java
Patch:
@@ -27,10 +27,9 @@
 
 public class HiveOptions implements Serializable {
     /**
-     * A low default value so that tuples are flushed and acked as soon as possible to avoid
-     * replay and spouts can continue emitting without hitting TOPOLOGY_MAX_SPOUT_PENDING.
+     * Half of the default Config.TOPOLOGY_MESSAGE_TIMEOUT_SECS
      */
-    private static final int DEFAULT_TICK_TUPLE_INTERVAL_SECS = 1;
+    private static final int DEFAULT_TICK_TUPLE_INTERVAL_SECS = 15;
 
     protected HiveMapper mapper;
     protected String databaseName;

File: external/storm-cassandra/src/test/java/org/apache/storm/cassandra/trident/TridentTopologyTest.java
Patch:
@@ -45,7 +45,6 @@
 
 import static org.apache.storm.cassandra.DynamicStatementBuilder.boundQuery;
 import static org.apache.storm.cassandra.DynamicStatementBuilder.field;
-import static org.apache.storm.cassandra.DynamicStatementBuilder.with;
 
 /**
  *
@@ -109,15 +108,16 @@ private CassandraStateFactory getInsertTemperatureStateFactory() {
         CassandraState.Options options = new CassandraState.Options(new CassandraContext());
         CQLStatementTupleMapper insertTemperatureValues = boundQuery(
                 "INSERT INTO weather.temperature(weather_station_id, weather_station_name, event_time, temperature) VALUES(?, ?, ?, ?)")
-                .bind(with(field("weather_station_id"), field("name").as("weather_station_name"), field("event_time").now(), field("temperature")));
+                .bind(field("weather_station_id"), field("name").as("weather_station_name"), field("event_time").now(), field("temperature"))
+                .build();
         options.withCQLStatementTupleMapper(insertTemperatureValues);
         return new CassandraStateFactory(options);
     }
 
     public CassandraStateFactory getSelectWeatherStationStateFactory() {
         CassandraState.Options options = new CassandraState.Options(new CassandraContext());
         CQLStatementTupleMapper insertTemperatureValues = boundQuery("SELECT name FROM weather.station WHERE id = ?")
-                .bind(with(field("weather_station_id").as("id")));
+                .bind(field("weather_station_id").as("id")).build();
         options.withCQLStatementTupleMapper(insertTemperatureValues);
         options.withCQLResultSetValuesMapper(new TridentResultSetValuesMapper(new Fields("name")));
         return new CassandraStateFactory(options);

File: storm-core/src/jvm/backtype/storm/scheduler/multitenant/Node.java
Patch:
@@ -334,7 +334,7 @@ public static Map<String, Node> getAllNodesFrom(Cluster cluster) {
   public static final Comparator<Node> FREE_NODE_COMPARATOR_DEC = new Comparator<Node>() {
     @Override
     public int compare(Node o1, Node o2) {
-      return o2.totalSlotsFree() - o1.totalSlotsFree();
+      return o1.totalSlotsUsed() - o2.totalSlotsUsed();
     }
   };
 }

File: storm-core/src/jvm/backtype/storm/topology/WindowedBoltExecutor.java
Patch:
@@ -59,7 +59,8 @@ public class WindowedBoltExecutor implements IRichBolt {
     private transient WindowManager<Tuple> windowManager;
     private transient int maxLagMs;
     private transient String tupleTsFieldName;
-    private transient WaterMarkEventGenerator<Tuple> waterMarkEventGenerator;
+    // package level for unit tests
+    transient WaterMarkEventGenerator<Tuple> waterMarkEventGenerator;
 
     public WindowedBoltExecutor(IWindowedBolt bolt) {
         this.bolt = bolt;

File: examples/storm-starter/src/jvm/storm/starter/ResourceAwareExampleTopology.java
Patch:
@@ -54,8 +54,6 @@ public void execute(Tuple tuple) {
     public void declareOutputFields(OutputFieldsDeclarer declarer) {
       declarer.declare(new Fields("word"));
     }
-
-
   }
 
   public static void main(String[] args) throws Exception {
@@ -87,7 +85,7 @@ public static void main(String[] args) throws Exception {
     // Set topology priority 0-30 with 0 being the highest priority and 30 being the lowest priority.
     conf.setTopologyPriority(30);
 
-    //Set strategy to schedule topology. If not specified, default to backtype.storm.scheduler.resource.strategies.scheduling.DefaultResourceAwareStrategy
+    // Set strategy to schedule topology. If not specified, default to backtype.storm.scheduler.resource.strategies.scheduling.DefaultResourceAwareStrategy
     conf.setTopologyStrategy(backtype.storm.scheduler.resource.strategies.scheduling.DefaultResourceAwareStrategy.class);
 
     if (args != null && args.length > 0) {

File: storm-core/src/jvm/backtype/storm/Config.java
Patch:
@@ -2229,6 +2229,7 @@ public void setTopologyPriority(int priority) {
 
     /**
      * Takes as input the strategy class name. Strategy must implement the IStrategy interface
+     * @param clazz class of the strategy to use
      */
     public void setTopologyStrategy(Class<? extends IStrategy> clazz) {
         if (clazz != null) {

File: storm-core/src/jvm/backtype/storm/scheduler/Cluster.java
Patch:
@@ -104,9 +104,7 @@ public Cluster getCopy() {
         Map newConf = new HashMap<String, Object>();
         newConf.putAll(this.conf);
         Cluster copy = new Cluster(this.inimbus, this.supervisors, newAssignments, newConf);
-        for (Map.Entry<String, String> entry : this.status.entrySet()) {
-            copy.setStatus(entry.getKey(), entry.getValue());
-        }
+        copy.status = new HashMap<>(this.status);
         return copy;
     }
     

File: storm-core/src/jvm/backtype/storm/scheduler/resource/RAS_Node.java
Patch:
@@ -144,7 +144,7 @@ private void validateSlot(WorkerSlot ws) {
         }
     }
 
-     void addOrphanedSlot(WorkerSlot ws) {
+    void addOrphanedSlot(WorkerSlot ws) {
         if (_isAlive) {
             throw new IllegalArgumentException("Orphaned Slots " +
                     "only are allowed on dead nodes.");
@@ -241,15 +241,15 @@ public void freeTopology(String topId) {
         _topIdToUsedSlots.remove(topId);
     }
 
-    public void freeMemory(double amount) {
+    private void freeMemory(double amount) {
         _availMemory += amount;
         LOG.debug("freeing {} memory on node {}...avail mem: {}", amount, this.getHostname(), _availMemory);
         if (_availMemory > this.getTotalMemoryResources()) {
             LOG.warn("Freeing more memory than there exists!");
         }
     }
 
-    public void freeCPU(double amount) {
+    private void freeCPU(double amount) {
         _availCPU += amount;
         LOG.debug("freeing {} CPU on node...avail CPU: {}", amount, this.getHostname(), _availCPU);
         if (_availCPU > this.getAvailableCpuResources()) {

File: storm-core/src/jvm/backtype/storm/scheduler/resource/strategies/eviction/DefaultEvictionStrategy.java
Patch:
@@ -23,7 +23,6 @@
 import backtype.storm.scheduler.TopologyDetails;
 import backtype.storm.scheduler.WorkerSlot;
 import backtype.storm.scheduler.resource.RAS_Nodes;
-import backtype.storm.scheduler.resource.ResourceUtils;
 import backtype.storm.scheduler.resource.User;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -58,7 +57,7 @@ public boolean makeSpaceForTopo(TopologyDetails td) {
         double cpuNeeded = td.getTotalRequestedCpu() / submitter.getCPUResourceGuaranteed();
         double memoryNeeded = (td.getTotalRequestedMemOffHeap() + td.getTotalRequestedMemOnHeap()) / submitter.getMemoryResourceGuaranteed();
 
-        User evictUser = this.findUserWithMostResourcesAboveGuarantee();
+        User evictUser = this.findUserWithHighestAverageResourceUtilAboveGuarantee();
         //user has enough resource under his or her resource guarantee to schedule topology
         if ((1.0 - submitter.getCPUResourcePoolUtilization()) >= cpuNeeded && (1.0 - submitter.getMemoryResourcePoolUtilization()) >= memoryNeeded) {
             if (evictUser != null) {
@@ -96,7 +95,7 @@ private void evictTopology(TopologyDetails topologyEvict) {
         submitter.moveTopoFromRunningToPending(topologyEvict, this.cluster);
     }
 
-    private User findUserWithMostResourcesAboveGuarantee() {
+    private User findUserWithHighestAverageResourceUtilAboveGuarantee() {
         double most = 0.0;
         User mostOverUser = null;
         for (User user : this.userMap.values()) {

File: storm-core/src/jvm/backtype/storm/scheduler/multitenant/Node.java
Patch:
@@ -334,7 +334,7 @@ public static Map<String, Node> getAllNodesFrom(Cluster cluster) {
   public static final Comparator<Node> FREE_NODE_COMPARATOR_DEC = new Comparator<Node>() {
     @Override
     public int compare(Node o1, Node o2) {
-      return o2.totalSlotsFree() - o1.totalSlotsFree();
+      return o1.totalSlotsUsed() - o2.totalSlotsUsed();
     }
   };
 }

File: storm-core/test/jvm/backtype/storm/utils/ShellBoltMessageQueueTest.java
Patch:
@@ -50,9 +50,10 @@ public void testPollWhileThereAreNoDataAvailable() throws InterruptedException {
         long start = System.currentTimeMillis();
         Object msg = queue.poll(1, TimeUnit.SECONDS);
         long finish = System.currentTimeMillis();
+        long waitDuration = finish - start;
 
         assertNull(msg);
-        assertTrue(finish - start > 1000);
+        assertTrue("wait duration should be equal or greater than 1000, current: " + waitDuration, waitDuration >= 1000);
     }
 
     @Test

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/blobstore/HdfsBlobStore.java
Patch:
@@ -65,7 +65,7 @@
  * 1. The USER interacts with nimbus to upload and access blobs through NimbusBlobStore Client API. Here, unlike
  * local blob store which stores the blobs locally, the nimbus talks to HDFS to upload the blobs.
  * 2. The USER sets the ACLs, and the blob access is validated against these ACLs.
- * 3. The SUPERVISOR interacts with nimbus thorugh HdfsClientBlobStore to download the blobs. Here, unlike local
+ * 3. The SUPERVISOR interacts with nimbus through HdfsClientBlobStore to download the blobs. Here, unlike local
  * blob store the supervisor interacts with HDFS directly to download the blobs. The call to HdfsBlobStore is made as a "null"
  * subject. The blobstore gets the hadoop user and validates permissions for the supervisor.
  */

File: examples/storm-starter/src/jvm/storm/starter/ResourceAwareExampleTopology.java
Patch:
@@ -82,7 +82,7 @@ public static void main(String[] args) throws Exception {
      * Use to limit the maximum amount of memory (in MB) allocated to one worker process.
      * Can be used to spread executors to to multiple workers
      */
-    conf.setTopologyWorkerMaxHeapSize(512.0);
+    conf.setTopologyWorkerMaxHeapSize(1024.0);
 
     if (args != null && args.length > 0) {
       conf.setNumWorkers(3);

File: storm-core/src/jvm/backtype/storm/scheduler/resource/ResourceAwareScheduler.java
Patch:
@@ -58,7 +58,7 @@ public void schedule(Topologies topologies, Cluster cluster) {
         for (TopologyDetails td : topologies.getTopologies()) {
             String topId = td.getId();
             Map<WorkerSlot, Collection<ExecutorDetails>> schedulerAssignmentMap;
-            if (cluster.needsScheduling(td) && cluster.getUnassignedExecutors(td).size() > 0) {
+            if (cluster.getUnassignedExecutors(td).size() > 0) {
                 LOG.debug("/********Scheduling topology {} ************/", topId);
 
                 schedulerAssignmentMap = RAStrategy.schedule(td);

File: storm-core/src/jvm/backtype/storm/scheduler/resource/strategies/ResourceAwareStrategy.java
Patch:
@@ -263,7 +263,7 @@ private Double getTotalClusterRes(List<String> cluster) {
     private Double distToNode(RAS_Node src, RAS_Node dest) {
         if (src.getId().equals(dest.getId())) {
             return 0.0;
-        } else if (this.NodeToCluster(src) == this.NodeToCluster(dest)) {
+        } else if (this.NodeToCluster(src).equals(this.NodeToCluster(dest))) {
             return 0.5;
         } else {
             return 1.0;

File: storm-core/src/jvm/backtype/storm/Config.java
Patch:
@@ -1652,6 +1652,7 @@ public class Config extends HashMap<String, Object> {
     /**
      * Max pending tuples in one ShellBolt
      */
+    @NotNull
     @isInteger
     @isPositiveNumber
     public static final String TOPOLOGY_SHELLBOLT_MAX_PENDING="topology.shellbolt.max.pending";

File: storm-core/src/jvm/backtype/storm/cluster/ClusterState.java
Patch:
@@ -87,9 +87,8 @@ public interface ClusterState {
      * @param path The path where a node will be created.
      * @param data The data to be written at the node.
      * @param acls The acls to apply to the path. May be null.
-     * @return path
      */
-    String set_ephemeral_node(String path, byte[] data, List<ACL> acls);
+    void set_ephemeral_node(String path, byte[] data, List<ACL> acls);
 
     /**
      * Gets the 'version' of the node at a path. Optionally sets a watch

File: storm-core/src/jvm/backtype/storm/utils/Utils.java
Patch:
@@ -777,6 +777,7 @@ public static long zipFileSize(File myFile) throws IOException{
 
     public static double zeroIfNaNOrInf(double x) {
         return (Double.isNaN(x) || Double.isInfinite(x)) ? 0.0 : x;
+    }
 
     /**
      * parses the arguments to extract jvm heap memory size.
@@ -785,14 +786,14 @@ public static double zeroIfNaNOrInf(double x) {
      * @return the value of the JVM heap memory setting in a java command.
      */
     public static Double parseWorkerChildOpts(String input, Double defaultValue) {
-        if(input != null) {
+        if (input != null) {
             Pattern optsPattern = Pattern.compile("Xmx[0-9]+m");
             Matcher m = optsPattern.matcher(input);
             String memoryOpts = null;
             while (m.find()) {
                 memoryOpts = m.group();
             }
-            if(memoryOpts!=null) {
+            if(memoryOpts != null) {
                 memoryOpts = memoryOpts.replaceAll("[a-zA-Z]", "");
                 return Double.parseDouble(memoryOpts);
             } else {

File: storm-core/src/jvm/backtype/storm/scheduler/resource/ResourceAwareScheduler.java
Patch:
@@ -58,7 +58,7 @@ public void schedule(Topologies topologies, Cluster cluster) {
         for (TopologyDetails td : topologies.getTopologies()) {
             String topId = td.getId();
             Map<WorkerSlot, Collection<ExecutorDetails>> schedulerAssignmentMap;
-            if (cluster.needsScheduling(td) && cluster.getUnassignedExecutors(td).size() > 0) {
+            if (cluster.getUnassignedExecutors(td).size() > 0) {
                 LOG.debug("/********Scheduling topology {} ************/", topId);
 
                 schedulerAssignmentMap = RAStrategy.schedule(td);

File: storm-core/src/jvm/backtype/storm/scheduler/resource/strategies/ResourceAwareStrategy.java
Patch:
@@ -263,7 +263,7 @@ private Double getTotalClusterRes(List<String> cluster) {
     private Double distToNode(RAS_Node src, RAS_Node dest) {
         if (src.getId().equals(dest.getId())) {
             return 0.0;
-        } else if (this.NodeToCluster(src) == this.NodeToCluster(dest)) {
+        } else if (this.NodeToCluster(src).equals(this.NodeToCluster(dest))) {
             return 0.5;
         } else {
             return 1.0;

File: examples/storm-starter/src/jvm/storm/starter/BasicDRPCTopology.java
Patch:
@@ -32,9 +32,8 @@
 /**
  * This topology is a basic example of doing distributed RPC on top of Storm. It implements a function that appends a
  * "!" to any string you send the DRPC function.
- * <p/>
- * See https://github.com/nathanmarz/storm/wiki/Distributed-RPC for more information on doing distributed RPC on top of
- * Storm.
+ *
+ * @see <a href="http://storm.apache.org/documentation/Distributed-RPC.html">Distributed RPC</a>
  */
 public class BasicDRPCTopology {
   public static class ExclaimBolt extends BaseBasicBolt {

File: examples/storm-starter/src/jvm/storm/starter/ReachTopology.java
Patch:
@@ -47,8 +47,8 @@
  * minutes on a single machine into one that takes just a couple seconds.
  * <p/>
  * For the purposes of demonstration, this topology replaces the use of actual DBs with in-memory hashmaps.
- * <p/>
- * See https://github.com/nathanmarz/storm/wiki/Distributed-RPC for more information on Distributed RPC.
+ *
+ * @see <a href="http://storm.apache.org/documentation/Distributed-RPC.html">Distributed RPC</a>
  */
 public class ReachTopology {
   public static Map<String, List<String>> TWEETERS_DB = new HashMap<String, List<String>>() {{

File: examples/storm-starter/src/jvm/storm/starter/TransactionalGlobalCount.java
Patch:
@@ -40,8 +40,9 @@
 
 /**
  * This is a basic example of a transactional topology. It keeps a count of the number of tuples seen so far in a
- * database. The source of data and the databases are mocked out as in memory maps for demonstration purposes. This
- * class is defined in depth on the wiki at https://github.com/nathanmarz/storm/wiki/Transactional-topologies
+ * database. The source of data and the databases are mocked out as in memory maps for demonstration purposes.
+ *
+ * @see <a href="http://storm.apache.org/documentation/Transactional-topologies.html">Transactional topologies</a>
  */
 public class TransactionalGlobalCount {
   public static final int PARTITION_TAKE_PER_BATCH = 3;

File: examples/storm-starter/test/jvm/storm/starter/bolt/IntermediateRankingsBoltTest.java
Patch:
@@ -23,10 +23,10 @@
 import backtype.storm.tuple.Fields;
 import backtype.storm.tuple.Tuple;
 import backtype.storm.tuple.Values;
+import backtype.storm.utils.MockTupleHelpers;
 import com.google.common.collect.Lists;
 import org.testng.annotations.DataProvider;
 import org.testng.annotations.Test;
-import storm.starter.tools.MockTupleHelpers;
 
 import java.util.Map;
 

File: examples/storm-starter/test/jvm/storm/starter/bolt/RollingCountBoltTest.java
Patch:
@@ -24,8 +24,8 @@
 import backtype.storm.tuple.Fields;
 import backtype.storm.tuple.Tuple;
 import backtype.storm.tuple.Values;
+import backtype.storm.utils.MockTupleHelpers;
 import org.testng.annotations.Test;
-import storm.starter.tools.MockTupleHelpers;
 
 import java.util.Map;
 

File: examples/storm-starter/test/jvm/storm/starter/bolt/TotalRankingsBoltTest.java
Patch:
@@ -23,9 +23,9 @@
 import backtype.storm.tuple.Fields;
 import backtype.storm.tuple.Tuple;
 import backtype.storm.tuple.Values;
+import backtype.storm.utils.MockTupleHelpers;
 import org.testng.annotations.DataProvider;
 import org.testng.annotations.Test;
-import storm.starter.tools.MockTupleHelpers;
 import storm.starter.tools.Rankings;
 
 import java.util.Map;

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/spout/IEventHubReceiver.java
Patch:
@@ -19,11 +19,12 @@
 
 import java.util.Map;
 
-import org.apache.storm.eventhubs.client.EventHubException;
+import com.microsoft.eventhubs.client.EventHubException;
+import com.microsoft.eventhubs.client.IEventHubFilter;
 
 public interface IEventHubReceiver {
 
-  void open(IEventHubReceiverFilter filter) throws EventHubException;
+  void open(IEventHubFilter filter) throws EventHubException;
 
   void close();
   

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/spout/StaticPartitionCoordinator.java
Patch:
@@ -25,7 +25,7 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import org.apache.storm.eventhubs.client.Constants;
+import com.microsoft.eventhubs.client.Constants;
 
 public class StaticPartitionCoordinator implements IPartitionCoordinator {
 

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/trident/OpaqueTridentEventHubSpout.java
Patch:
@@ -40,7 +40,7 @@ public OpaqueTridentEventHubSpout(EventHubSpoutConfig config) {
   }
 
   @Override
-  public Map getComponentConfiguration() {
+  public Map<String, Object> getComponentConfiguration() {
     return null;
   }
 

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/trident/TransactionalTridentEventHubEmitter.java
Patch:
@@ -29,7 +29,7 @@
 import org.apache.storm.eventhubs.spout.EventHubSpoutConfig;
 import org.apache.storm.eventhubs.spout.IEventHubReceiver;
 import org.apache.storm.eventhubs.spout.IEventHubReceiverFactory;
-import org.apache.storm.eventhubs.client.Constants;
+import com.microsoft.eventhubs.client.Constants;
 
 import storm.trident.operation.TridentCollector;
 import storm.trident.spout.IOpaquePartitionedTridentSpout;

File: external/storm-eventhubs/src/main/java/org/apache/storm/eventhubs/trident/TransactionalTridentEventHubSpout.java
Patch:
@@ -42,7 +42,7 @@ public TransactionalTridentEventHubSpout(EventHubSpoutConfig config) {
   }
   
   @Override
-  public Map getComponentConfiguration() {
+  public Map<String, Object> getComponentConfiguration() {
     return null;
   }
 

File: external/storm-eventhubs/src/test/java/org/apache/storm/eventhubs/spout/TestEventHubSpout.java
Patch:
@@ -37,7 +37,9 @@ public void tearDown() throws Exception {
   @Test
   public void testSpoutConfig() {
     EventHubSpoutConfig conf = new EventHubSpoutConfig("username", "pas\\s+w/ord",
-        "namespace", "entityname", 16, "zookeeper");
+        "namespace", "entityname", 16);
+    conf.setZkConnectionString("zookeeper");
+    conf.setCheckpointIntervalInSeconds(1);
     assertEquals(conf.getConnectionString(), "amqps://username:pas%5Cs%2Bw%2Ford@namespace.servicebus.windows.net");
   }
 

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/bolt/AbstractHBaseBolt.java
Patch:
@@ -42,6 +42,8 @@ public abstract class AbstractHBaseBolt extends BaseRichBolt {
     protected String tableName;
     protected HBaseMapper mapper;
     protected String configKey;
+    protected int batchSize = 15000;
+    protected int flushIntervalSecs = 0;
 
     public AbstractHBaseBolt(String tableName, HBaseMapper mapper) {
         Validate.notEmpty(tableName, "Table name can not be blank or null");

File: external/storm-hdfs/src/test/java/org/apache/storm/hdfs/trident/FixedBatchSpout.java
Patch:
@@ -84,7 +84,7 @@ public void close() {
     }
 
     @Override
-    public Map getComponentConfiguration() {
+    public Map<String, Object> getComponentConfiguration() {
         Config conf = new Config();
         conf.setMaxTaskParallelism(1);
         return conf;

File: storm-core/src/jvm/backtype/storm/Config.java
Patch:
@@ -338,15 +338,15 @@ public class Config extends HashMap<String, Object> {
     /**
      * The directory where storm's health scripts go.
      */
+    @isString
     public static final String STORM_HEALTH_CHECK_DIR = "storm.health.check.dir";
-    public static final Object STORM_HEALTH_CHECK_DIR_SCHEMA = String.class;
 
     /**
      * The time to allow any given healthcheck script to run before it
      * is marked failed due to timeout
      */
+    @isNumber
     public static final String STORM_HEALTH_CHECK_TIMEOUT_MS = "storm.health.check.timeout.ms";
-    public static final Object STORM_HEALTH_CHECK_TIMEOUT_MS_SCHEMA = Number.class;
 
     /**
      * The number of times to retry a Nimbus operation.

File: storm-core/test/jvm/backtype/storm/utils/DisruptorQueueTest.java
Patch:
@@ -55,6 +55,7 @@ public void onEvent(Object obj, long sequence, boolean endOfBatch)
         });
 
         run(producer, consumer);
+        queue.haltWithInterrupt();
         Assert.assertEquals("We expect to receive first published message first, but received " + result.get(),
                 "FIRST", result.get());
       }
@@ -80,6 +81,7 @@ public void onEvent(Object obj, long sequence, boolean endOfBatch)
         });
 
         run(producer, consumer, 1000, 1);
+        queue.haltWithInterrupt();
         Assert.assertTrue("Messages delivered out of order",
                 allInOrder.get());
     }
@@ -104,6 +106,7 @@ public void onEvent(Object obj, long sequence, boolean endOfBatch)
         });
 
         run(producer, consumer, 1000, 1);
+        queue.haltWithInterrupt();
         Assert.assertTrue("Messages delivered out of order",
                 allInOrder.get());
     }

File: storm-core/src/jvm/backtype/storm/utils/DisruptorQueue.java
Patch:
@@ -65,7 +65,7 @@ public class DisruptorQueue implements IStatefulObject {
 
     private static class FlusherPool { 
         private Timer _timer = new Timer("disruptor-flush-trigger", true);
-        private ThreadPoolExecutor _exec = new ThreadPoolExecutor(1, 100, 10, TimeUnit.SECONDS, new ArrayBlockingQueue<Runnable>(1024));
+        private ThreadPoolExecutor _exec = new ThreadPoolExecutor(1, 100, 10, TimeUnit.SECONDS, new ArrayBlockingQueue<Runnable>(1024), new ThreadPoolExecutor.DiscardPolicy());
         private HashMap<Long, ArrayList<Flusher>> _pendingFlush = new HashMap<>();
         private HashMap<Long, TimerTask> _tt = new HashMap<>();
 
@@ -92,8 +92,8 @@ private synchronized void invokeAll(long flushInterval) {
                 if (tasks != null) {
                     _exec.invokeAll(tasks);
                 }
-            } catch (Exception e) {
-               LOG.error("Could not invoke all ", e); 
+            } catch (InterruptedException e) {
+               //Ignored
             }
         }
 

File: storm-core/src/jvm/backtype/storm/messaging/netty/KerberosSaslClientHandler.java
Patch:
@@ -145,8 +145,7 @@ else if (event.getMessage() instanceof SaslMessageToken) {
             // server.
             SaslMessageToken saslResponse = new SaslMessageToken(responseToServer);
             channel.write(saslResponse);
-        }
-        else {
+        } else {
             LOG.error("Unexpected message from server: {}", event.getMessage());
         }
     }

File: storm-core/src/jvm/backtype/storm/messaging/netty/KerberosSaslNettyServerState.java
Patch:
@@ -26,5 +26,5 @@ final class KerberosSaslNettyServerState {
             protected KerberosSaslNettyServer initialValue(Channel channel) {
                 return null;
             }
-	};
+        };
 }

File: external/flux/flux-core/src/main/java/org/apache/storm/flux/Flux.java
Patch:
@@ -253,7 +253,8 @@ private static void printSplash() throws IOException {
         // banner
         InputStream is = Flux.class.getResourceAsStream("/splash.txt");
         if(is != null){
-            BufferedReader br = new BufferedReader(new InputStreamReader(is));
+            InputStreamReader isr = new InputStreamReader(is, "UTF-8");
+            BufferedReader br = new BufferedReader(isr);
             String line = null;
             while((line = br.readLine()) != null){
                 System.out.println(line);

File: external/storm-jdbc/src/main/java/org/apache/storm/jdbc/bolt/AbstractJdbcBolt.java
Patch:
@@ -21,6 +21,7 @@
 import backtype.storm.task.OutputCollector;
 import backtype.storm.task.TopologyContext;
 import backtype.storm.topology.base.BaseRichBolt;
+import org.apache.commons.lang.Validate;
 import org.apache.storm.jdbc.common.ConnectionProvider;
 import org.apache.storm.jdbc.common.JdbcClient;
 import org.slf4j.Logger;
@@ -52,6 +53,7 @@ public void prepare(Map map, TopologyContext topologyContext, OutputCollector co
     }
 
     public AbstractJdbcBolt(ConnectionProvider connectionProvider) {
+        Validate.notNull(connectionProvider);
         this.connectionProvider = connectionProvider;
     }
 

File: external/storm-jdbc/src/main/java/org/apache/storm/jdbc/mapper/SimpleJdbcLookupMapper.java
Patch:
@@ -22,6 +22,7 @@
 import backtype.storm.tuple.Fields;
 import backtype.storm.tuple.ITuple;
 import backtype.storm.tuple.Values;
+import org.apache.commons.lang.Validate;
 import org.apache.storm.jdbc.common.Column;
 
 import java.util.ArrayList;
@@ -33,6 +34,8 @@ public class SimpleJdbcLookupMapper extends SimpleJdbcMapper implements JdbcLook
 
     public SimpleJdbcLookupMapper(Fields outputFields, List<Column> queryColumns) {
         super(queryColumns);
+
+        Validate.notEmpty(outputFields.toList());
         this.outputFields = outputFields;
     }
 

File: external/storm-kafka/src/jvm/storm/kafka/KafkaSpout.java
Patch:
@@ -98,7 +98,7 @@ public void open(Map conf, final TopologyContext context, final SpoutOutputColle
         }
 
         context.registerMetric("kafkaOffset", new IMetric() {
-            KafkaUtils.KafkaOffsetMetric _kafkaOffsetMetric = new KafkaUtils.KafkaOffsetMetric(_spoutConfig.topic, _connections);
+            KafkaUtils.KafkaOffsetMetric _kafkaOffsetMetric = new KafkaUtils.KafkaOffsetMetric(_connections);
 
             @Override
             public Object getValueAndReset() {

File: external/storm-kafka/src/jvm/storm/kafka/ZkCoordinator.java
Patch:
@@ -76,7 +76,7 @@ public List<PartitionManager> getMyManagedPartitions() {
     public void refresh() {
         try {
             LOG.info(taskId(_taskIndex, _totalTasks) + "Refreshing partition manager connections");
-            GlobalPartitionInformation brokerInfo = _reader.getBrokerInfo();
+            List<GlobalPartitionInformation> brokerInfo = _reader.getBrokerInfo();
             List<Partition> mine = KafkaUtils.calculatePartitionsForTask(brokerInfo, _totalTasks, _taskIndex);
 
             Set<Partition> curr = _managers.keySet();

File: external/storm-kafka/src/jvm/storm/kafka/trident/OpaqueTridentKafkaSpout.java
Patch:
@@ -22,11 +22,12 @@
 import storm.kafka.Partition;
 import storm.trident.spout.IOpaquePartitionedTridentSpout;
 
+import java.util.List;
 import java.util.Map;
 import java.util.UUID;
 
 
-public class OpaqueTridentKafkaSpout implements IOpaquePartitionedTridentSpout<GlobalPartitionInformation, Partition, Map> {
+public class OpaqueTridentKafkaSpout implements IOpaquePartitionedTridentSpout<List<GlobalPartitionInformation>, Partition, Map> {
 
 
     TridentKafkaConfig _config;
@@ -36,7 +37,7 @@ public OpaqueTridentKafkaSpout(TridentKafkaConfig config) {
     }
 
     @Override
-    public IOpaquePartitionedTridentSpout.Emitter<GlobalPartitionInformation, Partition, Map> getEmitter(Map conf, TopologyContext context) {
+    public IOpaquePartitionedTridentSpout.Emitter<List<GlobalPartitionInformation>, Partition, Map> getEmitter(Map conf, TopologyContext context) {
         return new TridentKafkaEmitter(conf, context, _config, context
                 .getStormId()).asOpaqueEmitter();
     }

File: external/storm-kafka/src/test/storm/kafka/bolt/KafkaBoltTest.java
Patch:
@@ -79,7 +79,7 @@ public void shutdown() {
     }
 
     private void setupKafkaConsumer() {
-        GlobalPartitionInformation globalPartitionInformation = new GlobalPartitionInformation();
+        GlobalPartitionInformation globalPartitionInformation = new GlobalPartitionInformation(TEST_TOPIC);
         globalPartitionInformation.addPartition(0, Broker.fromString(broker.getBrokerConnectionString()));
         BrokerHosts brokerHosts = new StaticHosts(globalPartitionInformation);
         kafkaConfig = new KafkaConfig(brokerHosts, TEST_TOPIC);
@@ -249,7 +249,7 @@ public void executeWithBrokerDown() throws Exception {
     private boolean verifyMessage(String key, String message) {
         long lastMessageOffset = KafkaUtils.getOffset(simpleConsumer, kafkaConfig.topic, 0, OffsetRequest.LatestTime()) - 1;
         ByteBufferMessageSet messageAndOffsets = KafkaUtils.fetchMessages(kafkaConfig, simpleConsumer,
-                new Partition(Broker.fromString(broker.getBrokerConnectionString()), 0), lastMessageOffset);
+                new Partition(Broker.fromString(broker.getBrokerConnectionString()),kafkaConfig.topic, 0), lastMessageOffset);
         MessageAndOffset messageAndOffset = messageAndOffsets.iterator().next();
         Message kafkaMessage = messageAndOffset.message();
         ByteBuffer messageKeyBuffer = kafkaMessage.key();

File: storm-core/src/jvm/backtype/storm/coordination/BatchBoltExecutor.java
Patch:
@@ -32,7 +32,7 @@
 import org.slf4j.LoggerFactory;
 
 public class BatchBoltExecutor implements IRichBolt, FinishedCallback, TimeoutCallback {
-    public static Logger LOG = LoggerFactory.getLogger(BatchBoltExecutor.class);    
+    public static final Logger LOG = LoggerFactory.getLogger(BatchBoltExecutor.class);
 
     byte[] _boltSer;
     Map<Object, IBatchBolt> _openTransactions;
@@ -49,7 +49,7 @@ public void prepare(Map conf, TopologyContext context, OutputCollector collector
         _conf = conf;
         _context = context;
         _collector = new BatchOutputCollectorImpl(collector);
-        _openTransactions = new HashMap<Object, IBatchBolt>();
+        _openTransactions = new HashMap<>();
     }
 
     @Override

File: storm-core/src/jvm/backtype/storm/scheduler/Topologies.java
Patch:
@@ -34,9 +34,9 @@ public Topologies(Map<String, TopologyDetails> topologies) {
         this.topologies.putAll(topologies);
         this.nameToId = new HashMap<String, String>(topologies.size());
         
-        for (String topologyId : topologies.keySet()) {
-            TopologyDetails topology = topologies.get(topologyId);
-            this.nameToId.put(topology.getName(), topologyId);
+        for (Map.Entry<String, TopologyDetails> entry : topologies.entrySet()) {
+            TopologyDetails topology = entry.getValue();
+            this.nameToId.put(topology.getName(), entry.getKey());
         }
     }
     

File: storm-core/src/jvm/backtype/storm/security/auth/authorizer/DRPCAuthorizerBase.java
Patch:
@@ -26,7 +26,7 @@
 import org.slf4j.LoggerFactory;
 
 public abstract class DRPCAuthorizerBase implements IAuthorizer {
-    public static Logger LOG = LoggerFactory.getLogger(DRPCAuthorizerBase.class);
+    public static final Logger LOG = LoggerFactory.getLogger(DRPCAuthorizerBase.class);
 
     /**
      * A key name for the function requested to be executed by a user.

File: storm-core/src/jvm/backtype/storm/security/serialization/BlowfishTupleSerializer.java
Patch:
@@ -36,19 +36,19 @@
 import backtype.storm.Config;
 
 /**
- * Apply Blowfish encrption for tuple communication to bolts
+ * Apply Blowfish encryption for tuple communication to bolts
  */
 public class BlowfishTupleSerializer extends Serializer<ListDelegate> {
     /**
      * The secret key (if any) for data encryption by blowfish payload serialization factory (BlowfishSerializationFactory). 
      * You should use in via "storm -c topology.tuple.serializer.blowfish.key=YOURKEY -c topology.tuple.serializer=backtype.storm.security.serialization.BlowfishTupleSerializer jar ...".
      */
-    public static String SECRET_KEY = "topology.tuple.serializer.blowfish.key";
+    public static final String SECRET_KEY = "topology.tuple.serializer.blowfish.key";
     private static final Logger LOG = LoggerFactory.getLogger(BlowfishTupleSerializer.class);
     private BlowfishSerializer _serializer;
 
     public BlowfishTupleSerializer(Kryo kryo, Map storm_conf) {
-        String encryption_key = null;
+        String encryption_key;
         try {
             encryption_key = (String)storm_conf.get(SECRET_KEY);
             LOG.debug("Blowfish serializer being constructed ...");

File: storm-core/src/jvm/backtype/storm/serialization/SerializationFactory.java
Patch:
@@ -81,10 +81,10 @@ public static Kryo getKryo(Map conf) {
         kryoFactory.preRegister(k, conf);
 
         boolean skipMissing = (Boolean) conf.get(Config.TOPOLOGY_SKIP_MISSING_KRYO_REGISTRATIONS);
-        for(String klassName: registrations.keySet()) {
-            String serializerClassName = registrations.get(klassName);
+        for(Map.Entry<String, String> entry: registrations.entrySet()) {
+            String serializerClassName = entry.getValue();
             try {
-                Class klass = Class.forName(klassName);
+                Class klass = Class.forName(entry.getKey());
                 Class serializerClass = null;
                 if(serializerClassName!=null)
                     serializerClass = Class.forName(serializerClassName);

File: storm-core/src/jvm/backtype/storm/spout/ShellSpout.java
Patch:
@@ -41,11 +41,11 @@
 
 
 public class ShellSpout implements ISpout {
-    public static Logger LOG = LoggerFactory.getLogger(ShellSpout.class);
+    public static final Logger LOG = LoggerFactory.getLogger(ShellSpout.class);
 
     private SpoutOutputCollector _collector;
     private String[] _command;
-    private Map<String, String> env = new HashMap<String, String>();
+    private Map<String, String> env = new HashMap<>();
     private ShellProcess _process;
     
     private TopologyContext _context;

File: storm-core/src/jvm/backtype/storm/task/GeneralTopologyContext.java
Patch:
@@ -141,11 +141,12 @@ public Map<String, Map<String, Grouping>> getTargets(String componentId) {
         Map<String, Map<String, Grouping>> ret = new HashMap<String, Map<String, Grouping>>();
         for(String otherComponentId: getComponentIds()) {
             Map<GlobalStreamId, Grouping> inputs = getComponentCommon(otherComponentId).get_inputs();
-            for(GlobalStreamId id: inputs.keySet()) {
+            for(Map.Entry<GlobalStreamId, Grouping> entry: inputs.entrySet()) {
+                GlobalStreamId id = entry.getKey();
                 if(id.get_componentId().equals(componentId)) {
                     Map<String, Grouping> curr = ret.get(id.get_streamId());
                     if(curr==null) curr = new HashMap<String, Grouping>();
-                    curr.put(otherComponentId, inputs.get(id));
+                    curr.put(otherComponentId, entry.getValue());
                     ret.put(id.get_streamId(), curr);
                 }
             }

File: storm-core/src/jvm/backtype/storm/testing/TupleCaptureBolt.java
Patch:
@@ -30,7 +30,7 @@
 
 
 public class TupleCaptureBolt implements IRichBolt {
-    public static transient Map<String, Map<String, List<FixedTuple>>> emitted_tuples = new HashMap<String, Map<String, List<FixedTuple>>>();
+    public static final transient Map<String, Map<String, List<FixedTuple>>> emitted_tuples = new HashMap<>();
 
     private String _name;
     private OutputCollector _collector;
@@ -66,7 +66,7 @@ public Map<String, List<FixedTuple>> getAndRemoveResults() {
     }
 
     public Map<String, List<FixedTuple>> getAndClearResults() {
-        Map<String, List<FixedTuple>> ret = new HashMap<String, List<FixedTuple>>(emitted_tuples.get(_name));
+        Map<String, List<FixedTuple>> ret = new HashMap<>(emitted_tuples.get(_name));
         emitted_tuples.get(_name).clear();
         return ret;
     }

File: storm-core/src/jvm/backtype/storm/transactional/TransactionalSpoutBatchExecutor.java
Patch:
@@ -31,13 +31,13 @@
 import org.slf4j.LoggerFactory;
 
 public class TransactionalSpoutBatchExecutor implements IRichBolt {
-    public static Logger LOG = LoggerFactory.getLogger(TransactionalSpoutBatchExecutor.class);    
+    public static final Logger LOG = LoggerFactory.getLogger(TransactionalSpoutBatchExecutor.class);
 
     BatchOutputCollectorImpl _collector;
     ITransactionalSpout _spout;
     ITransactionalSpout.Emitter _emitter;
     
-    TreeMap<BigInteger, TransactionAttempt> _activeTransactions = new TreeMap<BigInteger, TransactionAttempt>();
+    TreeMap<BigInteger, TransactionAttempt> _activeTransactions = new TreeMap<>();
 
     public TransactionalSpoutBatchExecutor(ITransactionalSpout spout) {
         _spout = spout;

File: storm-core/src/jvm/backtype/storm/transactional/partitioned/OpaquePartitionedTransactionalSpoutExecutor.java
Patch:
@@ -120,8 +120,9 @@ public void cleanupBefore(BigInteger txid) {
         public void commit(TransactionAttempt attempt) {
             BigInteger txid = attempt.getTransactionId();
             Map<Integer, Object> metas = _cachedMetas.remove(txid);
-            for(Integer partition: metas.keySet()) {
-                Object meta = metas.get(partition);
+            for(Entry<Integer, Object> entry: metas.entrySet()) {
+                Integer partition = entry.getKey();
+                Object meta = entry.getValue();
                 _partitionStates.get(partition).overrideState(txid, meta);
             }
         }

File: storm-core/src/jvm/backtype/storm/tuple/TupleImpl.java
Patch:
@@ -40,7 +40,7 @@ public class TupleImpl extends IndifferentAccessMap implements Seqable, Indexed,
     private String streamId;
     private GeneralTopologyContext context;
     private MessageId id;
-    private IPersistentMap _meta = null;
+    private IPersistentMap _meta;
     
     public TupleImpl(GeneralTopologyContext context, List<Object> values, int taskId, String streamId, MessageId id) {
         this.values = values;
@@ -63,8 +63,8 @@ public TupleImpl(GeneralTopologyContext context, List<Object> values, int taskId
         this(context, values, taskId, streamId, MessageId.makeUnanchored());
     }    
     
-    Long _processSampleStartTime = null;
-    Long _executeSampleStartTime = null;
+    Long _processSampleStartTime;
+    Long _executeSampleStartTime;
     
     public void setProcessSampleStartTime(long ms) {
         _processSampleStartTime = ms;

File: storm-core/src/jvm/backtype/storm/utils/ShellProcess.java
Patch:
@@ -36,12 +36,12 @@
 import org.slf4j.LoggerFactory;
 
 public class ShellProcess implements Serializable {
-    public static Logger LOG = LoggerFactory.getLogger(ShellProcess.class);
+    public static final Logger LOG = LoggerFactory.getLogger(ShellProcess.class);
     public static Logger ShellLogger;
     private Process      _subprocess;
     private InputStream  processErrorStream;
     private String[]     command;
-    private Map<String, String> env = new HashMap<String, String>();
+    private Map<String, String> env = new HashMap<>();
     public ISerializer   serializer;
     public Number pid;
     public String componentName;
@@ -98,7 +98,7 @@ private ISerializer getSerializer(Map conf) {
         String serializer_className = (String)conf.get(Config.TOPOLOGY_MULTILANG_SERIALIZER);
         LOG.info("Storm multilang serializer: " + serializer_className);
 
-        ISerializer serializer = null;
+        ISerializer serializer;
         try {
             //create a factory class
             Class klass = Class.forName(serializer_className);

File: storm-core/src/jvm/backtype/storm/utils/ShellUtils.java
Patch:
@@ -34,7 +34,7 @@
 
 
 abstract public class ShellUtils {
-    public static Logger LOG = LoggerFactory.getLogger(ShellUtils.class);
+    public static final Logger LOG = LoggerFactory.getLogger(ShellUtils.class);
 
     // OSType detection
     public enum OSType {

File: storm-core/src/jvm/backtype/storm/utils/Time.java
Patch:
@@ -26,7 +26,7 @@
 
 
 public class Time {
-    public static Logger LOG = LoggerFactory.getLogger(Time.class);    
+    public static final Logger LOG = LoggerFactory.getLogger(Time.class);
     
     private static AtomicBoolean simulating = new AtomicBoolean(false);
     //TODO: should probably use weak references here or something
@@ -39,7 +39,7 @@ public static void startSimulating() {
         synchronized(sleepTimesLock) {
             simulating.set(true);
             simulatedCurrTimeMs = new AtomicLong(0);
-            threadSleepTimes = new ConcurrentHashMap<Thread, AtomicLong>();
+            threadSleepTimes = new ConcurrentHashMap<>();
         }
     }
     

File: storm-core/src/jvm/storm/trident/spout/TridentSpoutExecutor.java
Patch:
@@ -37,7 +37,7 @@
 import storm.trident.tuple.ConsList;
 
 public class TridentSpoutExecutor implements ITridentBatchBolt {
-    public static String ID_FIELD = "$tx";
+    public static final String ID_FIELD = "$tx";
     
     public static Logger LOG = LoggerFactory.getLogger(TridentSpoutExecutor.class);    
 

File: storm-core/src/jvm/storm/trident/topology/TridentBoltExecutor.java
Patch:
@@ -47,7 +47,7 @@
 import storm.trident.spout.IBatchID;
 
 public class TridentBoltExecutor implements IRichBolt {
-    public static String COORD_STREAM_PREFIX = "$coord-";
+    public static final String COORD_STREAM_PREFIX = "$coord-";
     
     public static String COORD_STREAM(String batch) {
         return COORD_STREAM_PREFIX + batch;
@@ -81,7 +81,7 @@ public String toString() {
     
     public static class CoordSpec implements Serializable {
         public GlobalStreamId commitStream = null;
-        public Map<String, CoordType> coords = new HashMap<String, CoordType>();
+        public Map<String, CoordType> coords = new HashMap<>();
         
         public CoordSpec() {
         }
@@ -317,7 +317,7 @@ public void execute(Tuple tuple) {
         }
         IBatchID id = (IBatchID) tuple.getValue(0);
         //get transaction id
-        //if it already exissts and attempt id is greater than the attempt there
+        //if it already exists and attempt id is greater than the attempt there
         
         
         TrackedBatch tracked = (TrackedBatch) _batches.get(id.getId());

File: storm-core/test/jvm/backtype/storm/utils/DisruptorQueueTest.java
Patch:
@@ -126,7 +126,7 @@ private static class IncProducer implements Runnable {
 
         @Override
         public void run() {
-            for (long i = 0; i < _max; i++) {
+            for (long i = 0; i < _max && !(Thread.currentThread().isInterrupted()); i++) {
                 queue.publish(i);
             }
         }

File: external/storm-kafka/src/jvm/storm/kafka/KafkaSpout.java
Patch:
@@ -98,7 +98,7 @@ public void open(Map conf, final TopologyContext context, final SpoutOutputColle
         }
 
         context.registerMetric("kafkaOffset", new IMetric() {
-            KafkaUtils.KafkaOffsetMetric _kafkaOffsetMetric = new KafkaUtils.KafkaOffsetMetric(_spoutConfig.topic, _connections);
+            KafkaUtils.KafkaOffsetMetric _kafkaOffsetMetric = new KafkaUtils.KafkaOffsetMetric(_connections);
 
             @Override
             public Object getValueAndReset() {

File: external/storm-kafka/src/jvm/storm/kafka/ZkCoordinator.java
Patch:
@@ -76,7 +76,7 @@ public List<PartitionManager> getMyManagedPartitions() {
     public void refresh() {
         try {
             LOG.info(taskId(_taskIndex, _totalTasks) + "Refreshing partition manager connections");
-            GlobalPartitionInformation brokerInfo = _reader.getBrokerInfo();
+            List<GlobalPartitionInformation> brokerInfo = _reader.getBrokerInfo();
             List<Partition> mine = KafkaUtils.calculatePartitionsForTask(brokerInfo, _totalTasks, _taskIndex);
 
             Set<Partition> curr = _managers.keySet();

File: external/storm-kafka/src/jvm/storm/kafka/trident/OpaqueTridentKafkaSpout.java
Patch:
@@ -22,11 +22,12 @@
 import storm.kafka.Partition;
 import storm.trident.spout.IOpaquePartitionedTridentSpout;
 
+import java.util.List;
 import java.util.Map;
 import java.util.UUID;
 
 
-public class OpaqueTridentKafkaSpout implements IOpaquePartitionedTridentSpout<GlobalPartitionInformation, Partition, Map> {
+public class OpaqueTridentKafkaSpout implements IOpaquePartitionedTridentSpout<List<GlobalPartitionInformation>, Partition, Map> {
 
 
     TridentKafkaConfig _config;
@@ -36,7 +37,7 @@ public OpaqueTridentKafkaSpout(TridentKafkaConfig config) {
     }
 
     @Override
-    public IOpaquePartitionedTridentSpout.Emitter<GlobalPartitionInformation, Partition, Map> getEmitter(Map conf, TopologyContext context) {
+    public IOpaquePartitionedTridentSpout.Emitter<List<GlobalPartitionInformation>, Partition, Map> getEmitter(Map conf, TopologyContext context) {
         return new TridentKafkaEmitter(conf, context, _config, context
                 .getStormId()).asOpaqueEmitter();
     }

File: external/storm-kafka/src/test/storm/kafka/bolt/KafkaBoltTest.java
Patch:
@@ -79,7 +79,7 @@ public void shutdown() {
     }
 
     private void setupKafkaConsumer() {
-        GlobalPartitionInformation globalPartitionInformation = new GlobalPartitionInformation();
+        GlobalPartitionInformation globalPartitionInformation = new GlobalPartitionInformation(TEST_TOPIC);
         globalPartitionInformation.addPartition(0, Broker.fromString(broker.getBrokerConnectionString()));
         BrokerHosts brokerHosts = new StaticHosts(globalPartitionInformation);
         kafkaConfig = new KafkaConfig(brokerHosts, TEST_TOPIC);
@@ -249,7 +249,7 @@ public void executeWithBrokerDown() throws Exception {
     private boolean verifyMessage(String key, String message) {
         long lastMessageOffset = KafkaUtils.getOffset(simpleConsumer, kafkaConfig.topic, 0, OffsetRequest.LatestTime()) - 1;
         ByteBufferMessageSet messageAndOffsets = KafkaUtils.fetchMessages(kafkaConfig, simpleConsumer,
-                new Partition(Broker.fromString(broker.getBrokerConnectionString()), 0), lastMessageOffset);
+                new Partition(Broker.fromString(broker.getBrokerConnectionString()),kafkaConfig.topic, 0), lastMessageOffset);
         MessageAndOffset messageAndOffset = messageAndOffsets.iterator().next();
         Message kafkaMessage = messageAndOffset.message();
         ByteBuffer messageKeyBuffer = kafkaMessage.key();

File: storm-core/src/jvm/backtype/storm/coordination/BatchBoltExecutor.java
Patch:
@@ -32,7 +32,7 @@
 import org.slf4j.LoggerFactory;
 
 public class BatchBoltExecutor implements IRichBolt, FinishedCallback, TimeoutCallback {
-    public static Logger LOG = LoggerFactory.getLogger(BatchBoltExecutor.class);    
+    public static final Logger LOG = LoggerFactory.getLogger(BatchBoltExecutor.class);
 
     byte[] _boltSer;
     Map<Object, IBatchBolt> _openTransactions;
@@ -49,7 +49,7 @@ public void prepare(Map conf, TopologyContext context, OutputCollector collector
         _conf = conf;
         _context = context;
         _collector = new BatchOutputCollectorImpl(collector);
-        _openTransactions = new HashMap<Object, IBatchBolt>();
+        _openTransactions = new HashMap<>();
     }
 
     @Override

File: storm-core/src/jvm/backtype/storm/security/auth/authorizer/DRPCAuthorizerBase.java
Patch:
@@ -26,7 +26,7 @@
 import org.slf4j.LoggerFactory;
 
 public abstract class DRPCAuthorizerBase implements IAuthorizer {
-    public static Logger LOG = LoggerFactory.getLogger(DRPCAuthorizerBase.class);
+    public static final Logger LOG = LoggerFactory.getLogger(DRPCAuthorizerBase.class);
 
     /**
      * A key name for the function requested to be executed by a user.

File: storm-core/src/jvm/backtype/storm/security/serialization/BlowfishTupleSerializer.java
Patch:
@@ -36,19 +36,19 @@
 import backtype.storm.Config;
 
 /**
- * Apply Blowfish encrption for tuple communication to bolts
+ * Apply Blowfish encryption for tuple communication to bolts
  */
 public class BlowfishTupleSerializer extends Serializer<ListDelegate> {
     /**
      * The secret key (if any) for data encryption by blowfish payload serialization factory (BlowfishSerializationFactory). 
      * You should use in via "storm -c topology.tuple.serializer.blowfish.key=YOURKEY -c topology.tuple.serializer=backtype.storm.security.serialization.BlowfishTupleSerializer jar ...".
      */
-    public static String SECRET_KEY = "topology.tuple.serializer.blowfish.key";
+    public static final String SECRET_KEY = "topology.tuple.serializer.blowfish.key";
     private static final Logger LOG = LoggerFactory.getLogger(BlowfishTupleSerializer.class);
     private BlowfishSerializer _serializer;
 
     public BlowfishTupleSerializer(Kryo kryo, Map storm_conf) {
-        String encryption_key = null;
+        String encryption_key;
         try {
             encryption_key = (String)storm_conf.get(SECRET_KEY);
             LOG.debug("Blowfish serializer being constructed ...");

File: storm-core/src/jvm/backtype/storm/spout/ShellSpout.java
Patch:
@@ -41,11 +41,11 @@
 
 
 public class ShellSpout implements ISpout {
-    public static Logger LOG = LoggerFactory.getLogger(ShellSpout.class);
+    public static final Logger LOG = LoggerFactory.getLogger(ShellSpout.class);
 
     private SpoutOutputCollector _collector;
     private String[] _command;
-    private Map<String, String> env = new HashMap<String, String>();
+    private Map<String, String> env = new HashMap<>();
     private ShellProcess _process;
     
     private TopologyContext _context;

File: storm-core/src/jvm/backtype/storm/testing/TupleCaptureBolt.java
Patch:
@@ -30,7 +30,7 @@
 
 
 public class TupleCaptureBolt implements IRichBolt {
-    public static transient Map<String, Map<String, List<FixedTuple>>> emitted_tuples = new HashMap<String, Map<String, List<FixedTuple>>>();
+    public static final transient Map<String, Map<String, List<FixedTuple>>> emitted_tuples = new HashMap<>();
 
     private String _name;
     private OutputCollector _collector;
@@ -66,7 +66,7 @@ public Map<String, List<FixedTuple>> getAndRemoveResults() {
     }
 
     public Map<String, List<FixedTuple>> getAndClearResults() {
-        Map<String, List<FixedTuple>> ret = new HashMap<String, List<FixedTuple>>(emitted_tuples.get(_name));
+        Map<String, List<FixedTuple>> ret = new HashMap<>(emitted_tuples.get(_name));
         emitted_tuples.get(_name).clear();
         return ret;
     }

File: storm-core/src/jvm/backtype/storm/transactional/TransactionalSpoutBatchExecutor.java
Patch:
@@ -31,13 +31,13 @@
 import org.slf4j.LoggerFactory;
 
 public class TransactionalSpoutBatchExecutor implements IRichBolt {
-    public static Logger LOG = LoggerFactory.getLogger(TransactionalSpoutBatchExecutor.class);    
+    public static final Logger LOG = LoggerFactory.getLogger(TransactionalSpoutBatchExecutor.class);
 
     BatchOutputCollectorImpl _collector;
     ITransactionalSpout _spout;
     ITransactionalSpout.Emitter _emitter;
     
-    TreeMap<BigInteger, TransactionAttempt> _activeTransactions = new TreeMap<BigInteger, TransactionAttempt>();
+    TreeMap<BigInteger, TransactionAttempt> _activeTransactions = new TreeMap<>();
 
     public TransactionalSpoutBatchExecutor(ITransactionalSpout spout) {
         _spout = spout;

File: storm-core/src/jvm/backtype/storm/tuple/TupleImpl.java
Patch:
@@ -40,7 +40,7 @@ public class TupleImpl extends IndifferentAccessMap implements Seqable, Indexed,
     private String streamId;
     private GeneralTopologyContext context;
     private MessageId id;
-    private IPersistentMap _meta = null;
+    private IPersistentMap _meta;
     
     public TupleImpl(GeneralTopologyContext context, List<Object> values, int taskId, String streamId, MessageId id) {
         this.values = values;
@@ -63,8 +63,8 @@ public TupleImpl(GeneralTopologyContext context, List<Object> values, int taskId
         this(context, values, taskId, streamId, MessageId.makeUnanchored());
     }    
     
-    Long _processSampleStartTime = null;
-    Long _executeSampleStartTime = null;
+    Long _processSampleStartTime;
+    Long _executeSampleStartTime;
     
     public void setProcessSampleStartTime(long ms) {
         _processSampleStartTime = ms;

File: storm-core/src/jvm/backtype/storm/utils/ShellProcess.java
Patch:
@@ -36,12 +36,12 @@
 import org.slf4j.LoggerFactory;
 
 public class ShellProcess implements Serializable {
-    public static Logger LOG = LoggerFactory.getLogger(ShellProcess.class);
+    public static final Logger LOG = LoggerFactory.getLogger(ShellProcess.class);
     public static Logger ShellLogger;
     private Process      _subprocess;
     private InputStream  processErrorStream;
     private String[]     command;
-    private Map<String, String> env = new HashMap<String, String>();
+    private Map<String, String> env = new HashMap<>();
     public ISerializer   serializer;
     public Number pid;
     public String componentName;
@@ -98,7 +98,7 @@ private ISerializer getSerializer(Map conf) {
         String serializer_className = (String)conf.get(Config.TOPOLOGY_MULTILANG_SERIALIZER);
         LOG.info("Storm multilang serializer: " + serializer_className);
 
-        ISerializer serializer = null;
+        ISerializer serializer;
         try {
             //create a factory class
             Class klass = Class.forName(serializer_className);

File: storm-core/src/jvm/backtype/storm/utils/ShellUtils.java
Patch:
@@ -34,7 +34,7 @@
 
 
 abstract public class ShellUtils {
-    public static Logger LOG = LoggerFactory.getLogger(ShellUtils.class);
+    public static final Logger LOG = LoggerFactory.getLogger(ShellUtils.class);
 
     // OSType detection
     public enum OSType {

File: storm-core/src/jvm/backtype/storm/utils/Time.java
Patch:
@@ -26,7 +26,7 @@
 
 
 public class Time {
-    public static Logger LOG = LoggerFactory.getLogger(Time.class);    
+    public static final Logger LOG = LoggerFactory.getLogger(Time.class);
     
     private static AtomicBoolean simulating = new AtomicBoolean(false);
     //TODO: should probably use weak references here or something
@@ -39,7 +39,7 @@ public static void startSimulating() {
         synchronized(sleepTimesLock) {
             simulating.set(true);
             simulatedCurrTimeMs = new AtomicLong(0);
-            threadSleepTimes = new ConcurrentHashMap<Thread, AtomicLong>();
+            threadSleepTimes = new ConcurrentHashMap<>();
         }
     }
     

File: storm-core/src/jvm/storm/trident/spout/TridentSpoutExecutor.java
Patch:
@@ -37,7 +37,7 @@
 import storm.trident.tuple.ConsList;
 
 public class TridentSpoutExecutor implements ITridentBatchBolt {
-    public static String ID_FIELD = "$tx";
+    public static final String ID_FIELD = "$tx";
     
     public static Logger LOG = LoggerFactory.getLogger(TridentSpoutExecutor.class);    
 

File: storm-core/src/jvm/storm/trident/topology/TridentBoltExecutor.java
Patch:
@@ -47,7 +47,7 @@
 import storm.trident.spout.IBatchID;
 
 public class TridentBoltExecutor implements IRichBolt {
-    public static String COORD_STREAM_PREFIX = "$coord-";
+    public static final String COORD_STREAM_PREFIX = "$coord-";
     
     public static String COORD_STREAM(String batch) {
         return COORD_STREAM_PREFIX + batch;
@@ -81,7 +81,7 @@ public String toString() {
     
     public static class CoordSpec implements Serializable {
         public GlobalStreamId commitStream = null;
-        public Map<String, CoordType> coords = new HashMap<String, CoordType>();
+        public Map<String, CoordType> coords = new HashMap<>();
         
         public CoordSpec() {
         }
@@ -317,7 +317,7 @@ public void execute(Tuple tuple) {
         }
         IBatchID id = (IBatchID) tuple.getValue(0);
         //get transaction id
-        //if it already exissts and attempt id is greater than the attempt there
+        //if it already exists and attempt id is greater than the attempt there
         
         
         TrackedBatch tracked = (TrackedBatch) _batches.get(id.getId());

File: storm-core/test/jvm/backtype/storm/utils/DisruptorQueueTest.java
Patch:
@@ -126,7 +126,7 @@ private static class IncProducer implements Runnable {
 
         @Override
         public void run() {
-            for (long i = 0; i < _max; i++) {
+            for (long i = 0; i < _max && !(Thread.currentThread().isInterrupted()); i++) {
                 queue.publish(i);
             }
         }

File: storm-core/src/jvm/backtype/storm/StormSubmitter.java
Patch:
@@ -18,7 +18,6 @@
 package backtype.storm;
 
 import java.io.File;
-import java.lang.reflect.InvocationTargetException;
 import java.nio.ByteBuffer;
 import java.util.regex.Matcher;
 import java.util.regex.Pattern;
@@ -273,7 +272,7 @@ public static void submitTopologyAs(String name, Map stormConf, StormTopology to
      */
     @SuppressWarnings("unchecked")
     public static void submitTopology(String name, Map stormConf, StormTopology topology, SubmitOptions opts,
-             ProgressListener progressListener) throws AlreadyAliveException, InvalidTopologyException, AuthorizationException{
+             ProgressListener progressListener) throws AlreadyAliveException, InvalidTopologyException, AuthorizationException {
         submitTopologyAs(name, stormConf, topology, opts, progressListener, null);
     }
 

File: storm-core/src/jvm/backtype/storm/generated/DebugOptions.java
Patch:
@@ -51,7 +51,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-9")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-29")
 public class DebugOptions implements org.apache.thrift.TBase<DebugOptions, DebugOptions._Fields>, java.io.Serializable, Cloneable, Comparable<DebugOptions> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("DebugOptions");
 

File: storm-core/src/jvm/backtype/storm/generated/NimbusSummary.java
Patch:
@@ -51,7 +51,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-9")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-10-29")
 public class NimbusSummary implements org.apache.thrift.TBase<NimbusSummary, NimbusSummary._Fields>, java.io.Serializable, Cloneable, Comparable<NimbusSummary> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("NimbusSummary");
 

File: storm-core/src/jvm/backtype/storm/Config.java
Patch:
@@ -1206,7 +1206,7 @@ public class Config extends HashMap<String, Object> {
      * guaranteeing that the same value goes to the same task).
      */
     @isInteger
-    @isPositiveNumber
+    @isPositiveNumber(includeZero = true)
     public static final String TOPOLOGY_TASKS = "topology.tasks";
 
     /**
@@ -1245,7 +1245,7 @@ public class Config extends HashMap<String, Object> {
      * then Storm will immediately ack tuples as soon as they come off the spout, effectively disabling reliability.</p>
      */
     @isInteger
-    @isPositiveNumber
+    @isPositiveNumber(includeZero = true)
     public static final String TOPOLOGY_ACKER_EXECUTORS = "topology.acker.executors";
 
     /**

File: external/storm-kafka/src/jvm/storm/kafka/trident/TridentKafkaState.java
Patch:
@@ -94,5 +94,4 @@ public void updateState(List<TridentTuple> tuples, TridentCollector collector) {
             }
         }
     }
-    }
 }

File: storm-core/src/jvm/backtype/storm/Config.java
Patch:
@@ -1206,7 +1206,7 @@ public class Config extends HashMap<String, Object> {
      * guaranteeing that the same value goes to the same task).
      */
     @isInteger
-    @isPositiveNumber
+    @isPositiveNumber(includeZero = true)
     public static final String TOPOLOGY_TASKS = "topology.tasks";
 
     /**
@@ -1245,7 +1245,7 @@ public class Config extends HashMap<String, Object> {
      * then Storm will immediately ack tuples as soon as they come off the spout, effectively disabling reliability.</p>
      */
     @isInteger
-    @isPositiveNumber
+    @isPositiveNumber(includeZero = true)
     public static final String TOPOLOGY_ACKER_EXECUTORS = "topology.acker.executors";
 
     /**
@@ -1256,7 +1256,7 @@ public class Config extends HashMap<String, Object> {
      * event logging will be disabled.</p>
      */
     @isInteger
-    @isPositiveNumber
+    @isPositiveNumber(includeZero = true)
     public static final String TOPOLOGY_EVENTLOGGER_EXECUTORS = "topology.eventlogger.executors";
 
     /**

File: storm-core/src/jvm/backtype/storm/Config.java
Patch:
@@ -1256,7 +1256,7 @@ public class Config extends HashMap<String, Object> {
      * event logging will be disabled.</p>
      */
     @isInteger
-    @isPositiveNumber
+    @isPositiveNumber(includeZero = true)
     public static final String TOPOLOGY_EVENTLOGGER_EXECUTORS = "topology.eventlogger.executors";
 
     /**

File: storm-core/src/jvm/backtype/storm/utils/MutableObject.java
Patch:
@@ -18,7 +18,7 @@
 package backtype.storm.utils;
 
 public class MutableObject {
-    Object o = null;
+    private Object o = null;
     
     public MutableObject() {
         
@@ -28,11 +28,11 @@ public MutableObject(Object o) {
         this.o = o;
     }
     
-    public void setObject(Object o) {
+    public synchronized void setObject(Object o) {
         this.o = o;
     }
     
-    public Object getObject() {
+    public synchronized Object getObject() {
         return o;
     }
 }

File: storm-core/src/jvm/storm/trident/spout/ITridentSpout.java
Patch:
@@ -102,6 +102,7 @@ interface Emitter<X> {
      * @param conf Storm config map
      * @param context topology context
      * @return spout coordinator instance
+     */
     BatchCoordinator<T> getCoordinator(String txStateId, Map conf, TopologyContext context);
 
     /**

File: storm-core/src/jvm/backtype/storm/validation/ConfigValidation.java
Patch:
@@ -467,7 +467,7 @@ public void validateField(String name, Object o) throws IllegalAccessException {
             }
 
             SimpleTypeValidator.validateField(name, String.class, ((Map) o).get("class"));
-            SimpleTypeValidator.validateField(name, long.class, ((Map) o).get("parallelism.hint"));
+            SimpleTypeValidator.validateField(name, Long.class, ((Map) o).get("parallelism.hint"));
         }
     }
 

File: storm-core/src/jvm/backtype/storm/validation/ConfigValidationAnnotations.java
Patch:
@@ -45,6 +45,7 @@ public static class ValidatorParams {
         static final String KEY_TYPE = "keyType";
         static final String VALUE_TYPE = "valueType";
         static final String INCLUDE_ZERO = "includeZero";
+        static final String ACCEPTED_VALUES = "acceptedValues";
     }
 
     /**
@@ -86,6 +87,7 @@ public static class ValidatorParams {
     @Target(ElementType.FIELD)
     public @interface isString {
         Class validatorClass() default ConfigValidation.StringValidator.class;
+        String[] acceptedValues() default "";
     }
 
     @Retention(RetentionPolicy.RUNTIME)

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/bolt/HBaseBolt.java
Patch:
@@ -78,13 +78,13 @@ public Map<String, Object> getComponentConfiguration() {
             conf = new Config();
         }
 
-        if (conf.containsKey("topology.message.timeout.secs") && tickTupleInterval == 0) {
+        if (conf.containsKey("topology.message.timeout.secs") && flushIntervalSecs == 0) {
             Integer topologyTimeout = Integer.parseInt(conf.get("topology.message.timeout.secs").toString());
             flushIntervalSecs = (int)(Math.floor(topologyTimeout / 2));
-            LOG.debug("Setting flush interval to [" + flushIntervalSecs + "] based on topology.message.timeout.secs");
+            LOG.debug("Setting flush interval to [{}] based on topology.message.timeout.secs", flushIntervalSecs);
         }
 
-        LOG.info("Enabling tick tuple with interval [" + flushIntervalSecs + "]");
+        LOG.info("Enabling tick tuple with interval [{}]", flushIntervalSecs);
         conf.put(Config.TOPOLOGY_TICK_TUPLE_FREQ_SECS, flushIntervalSecs);
         return conf;
     }

File: storm-core/src/jvm/backtype/storm/utils/Utils.java
Patch:
@@ -250,7 +250,7 @@ private static InputStream getConfigFileInputStream(String configFilePath)
                             + " resources. You're probably bundling the Storm jars with your topology jar. "
                             + resources);
         } else {
-            LOG.info("Using "+configFilePath+" from resources");
+            LOG.debug("Using "+configFilePath+" from resources");
             URL resource = resources.iterator().next();
             return resource.openStream();
         }

File: storm-core/src/jvm/backtype/storm/security/auth/kerberos/ServerCallbackHandler.java
Patch:
@@ -87,6 +87,8 @@ private void handleAuthorizeCallback(AuthorizeCallback ac) {
         //add the authNid as the real user in reqContext's subject which will be used during authorization.
         if(!ac.getAuthenticationID().equals(ac.getAuthorizationID())) {
             ReqContext.context().setRealPrincipal(new SaslTransportPlugin.User(ac.getAuthenticationID()));
+        } else {
+            ReqContext.context().setRealPrincipal(null);
         }
 
         ac.setAuthorized(true);

File: storm-core/src/jvm/backtype/storm/security/auth/kerberos/ServerCallbackHandler.java
Patch:
@@ -87,6 +87,8 @@ private void handleAuthorizeCallback(AuthorizeCallback ac) {
         //add the authNid as the real user in reqContext's subject which will be used during authorization.
         if(!ac.getAuthenticationID().equals(ac.getAuthorizationID())) {
             ReqContext.context().setRealPrincipal(new SaslTransportPlugin.User(ac.getAuthenticationID()));
+        } else {
+            ReqContext.context().setRealPrincipal(null);
         }
 
         ac.setAuthorized(true);

File: external/storm-kafka/src/test/storm/kafka/KafkaUtilsTest.java
Patch:
@@ -195,8 +195,7 @@ private void createTopicAndSendMessage(String value) {
 
     private void createTopicAndSendMessage(String key, String value) {
         Properties p = new Properties();
-        p.put("request.required.acks", "1");
-        p.put("serializer.class", "kafka.serializer.StringEncoder");
+        p.put("acks", "1");
         p.put("bootstrap.servers", broker.getBrokerConnectionString());
         p.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
         p.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

File: external/storm-kafka/src/jvm/storm/kafka/PartitionManager.java
Patch:
@@ -136,7 +136,7 @@ public EmitState next(SpoutOutputCollector collector) {
                 return EmitState.NO_EMITTED;
             }
             Iterable<List<Object>> tups = KafkaUtils.generateTuples(_spoutConfig, toEmit.msg);
-            if ((tups != null) && tups.iterator.hasNext()) {
+            if ((tups != null) && tups.iterator().hasNext()) {
                 if(_spoutConfig.topicAsStreamId) {
                     for (List<Object> tup : tups) {
                         collector.emit(_spoutConfig.topic, tup, new KafkaMessageId(_partition, toEmit.offset));

File: external/storm-kafka/src/test/storm/kafka/KafkaUtilsTest.java
Patch:
@@ -195,8 +195,7 @@ private void createTopicAndSendMessage(String value) {
 
     private void createTopicAndSendMessage(String key, String value) {
         Properties p = new Properties();
-        p.put("request.required.acks", "1");
-        p.put("serializer.class", "kafka.serializer.StringEncoder");
+        p.put("acks", "1");
         p.put("bootstrap.servers", broker.getBrokerConnectionString());
         p.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
         p.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

File: external/storm-kafka/src/jvm/storm/kafka/trident/OpaqueTridentKafkaSpout.java
Patch:
@@ -37,7 +37,7 @@ public OpaqueTridentKafkaSpout(TridentKafkaConfig config) {
     }
 
     @Override
-    public IOpaquePartitionedTridentSpout.Emitter<GlobalPartitionInformation, Partition, Map> getEmitter(Map conf, TopologyContext context) {
+    public IOpaquePartitionedTridentSpout.Emitter<List<GlobalPartitionInformation>, Partition, Map> getEmitter(Map conf, TopologyContext context) {
         return new TridentKafkaEmitter(conf, context, _config, context
                 .getStormId()).asOpaqueEmitter();
     }

File: external/storm-kafka/src/jvm/storm/kafka/PartitionManager.java
Patch:
@@ -136,7 +136,7 @@ public EmitState next(SpoutOutputCollector collector) {
                 return EmitState.NO_EMITTED;
             }
             Iterable<List<Object>> tups = KafkaUtils.generateTuples(_spoutConfig, toEmit.msg);
-            if ((tups != null) && (tups.size() > 0)) {
+            if ((tups != null) && tups.iterator.hasNext()) {
                 if(_spoutConfig.topicAsStreamId) {
                     for (List<Object> tup : tups) {
                         collector.emit(_spoutConfig.topic, tup, new KafkaMessageId(_partition, toEmit.offset));

File: storm-core/src/jvm/backtype/storm/messaging/netty/Client.java
Patch:
@@ -317,7 +317,7 @@ private int iteratorSize(Iterator<TaskMessage> msgs) {
      * If the write operation fails, then we will close the channel and trigger a reconnect.
      */
     private void flushMessages(Channel channel, final MessageBatch batch) {
-        if(batch.isEmpty()){
+        if(null == batch || batch.isEmpty()){
             return;
         }
 

File: external/flux/flux-core/src/main/java/org/apache/storm/flux/model/ObjectDef.java
Patch:
@@ -56,6 +56,8 @@ public void setConstructorArgs(List<Object> constructorArgs) {
                 if(map.containsKey("ref") && map.size() == 1){
                     newVal.add(new BeanReference((String)map.get("ref")));
                     this.hasReferences = true;
+                } else {
+                    newVal.add(obj);
                 }
             } else {
                 newVal.add(obj);

File: storm-core/src/jvm/backtype/storm/generated/Nimbus.java
Patch:
@@ -74,7 +74,7 @@ public interface Iface {
      * Enable/disable logging the tuples generated in topology via an internal EventLogger bolt. The component name is optional
      * and if null or empty, the debug flag will apply to the entire topology.
      * 
-     * If 'samplingPercentage' is specified, it will limit loggging to a percentage of generated tuples. The default is to log all (100 pct).
+     * The 'samplingPercentage' will limit loggging to a percentage of generated tuples.
      * 
      * 
      * @param name

File: storm-core/src/jvm/backtype/storm/metric/EventLoggerBolt.java
Patch:
@@ -28,6 +28,7 @@ public class EventLoggerBolt implements IBolt {
     public static final String FIELD_TS = "ts";
     public static final String FIELD_VALUES = "values";
     public static final String FIELD_COMPONENT_ID = "component-id";
+    public static final String FIELD_MESSAGE_ID = "message-id";
 
     private IEventLogger eventLogger;
 
@@ -42,8 +43,9 @@ public void prepare(Map stormConf, TopologyContext context, OutputCollector coll
     public void execute(Tuple input) {
         LOG.debug("** EventLoggerBolt got tuple from sourceComponent {}, with values {}", input.getSourceComponent(), input.getValues());
 
+        Object msgId = input.getValueByField(FIELD_MESSAGE_ID);
         EventInfo eventInfo = new EventInfo(input.getValueByField(FIELD_TS).toString(), input.getSourceComponent(),
-                                            String.valueOf(input.getSourceTask()), input.getMessageId().toString(),
+                                            String.valueOf(input.getSourceTask()), msgId == null ? "" : msgId.toString(),
                                             input.getValueByField(FIELD_VALUES).toString());
 
         eventLogger.log(eventInfo);

File: external/storm-elasticsearch/src/main/java/org/apache/storm/elasticsearch/common/TransportAddresses.java
Patch:
@@ -15,7 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.storm.elasticsearch.bolt;
+package org.apache.storm.elasticsearch.common;
 
 import java.util.Iterator;
 import java.util.LinkedList;

File: external/storm-elasticsearch/src/main/java/org/apache/storm/elasticsearch/trident/EsUpdater.java
Patch:
@@ -26,6 +26,6 @@
 public class EsUpdater extends BaseStateUpdater<EsState> {
     @Override
     public void updateState(EsState state, List<TridentTuple> tuples, TridentCollector collector) {
-        state.updateState(tuples, collector);
+        state.updateState(tuples);
     }
 }

File: external/storm-elasticsearch/src/test/java/org/apache/storm/elasticsearch/bolt/EsIndexTopology.java
Patch:
@@ -46,9 +46,7 @@ public static void main(String[] args) throws Exception {
         TopologyBuilder builder = new TopologyBuilder();
         UserDataSpout spout = new UserDataSpout();
         builder.setSpout(SPOUT_ID, spout, 1);
-        EsConfig esConfig = new EsConfig();
-        esConfig.setClusterName(EsConstants.clusterName);
-        esConfig.setNodes(new String[]{"localhost:9300"});
+        EsConfig esConfig = new EsConfig(EsConstants.clusterName, new String[]{"localhost:9300"});
         builder.setBolt(BOLT_ID, new EsIndexBolt(esConfig), 1).shuffleGrouping(SPOUT_ID);
 
         EsTestUtil.startEsNode();

File: external/storm-elasticsearch/src/test/java/org/apache/storm/elasticsearch/common/TransportAddressesTest.java
Patch:
@@ -15,7 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.storm.elasticsearch.bolt;
+package org.apache.storm.elasticsearch.common;
 
 import org.elasticsearch.common.transport.InetSocketTransportAddress;
 import org.junit.Test;

File: external/storm-elasticsearch/src/test/java/org/apache/storm/elasticsearch/trident/TridentEsTopology.java
Patch:
@@ -45,9 +45,7 @@ public static void main(String[] args) {
 
         TridentTopology topology = new TridentTopology();
         Stream stream = topology.newStream("spout", spout);
-        EsConfig esConfig = new EsConfig();
-        esConfig.setClusterName(EsConstants.clusterName);
-        esConfig.setNodes(new String[]{"localhost:9300"});
+        EsConfig esConfig = new EsConfig(EsConstants.clusterName, new String[]{"localhost:9300"});
         Fields esFields = new Fields("index", "type", "source");
         StateFactory factory = new EsStateFactory(esConfig);
         TridentState state = stream.partitionPersist(factory, esFields, new EsUpdater(), new Fields());

File: external/storm-solr/src/main/java/org/apache/storm/solr/trident/SolrState.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.solr.trident;
 
 import backtype.storm.topology.FailedException;

File: external/storm-solr/src/main/java/org/apache/storm/solr/trident/SolrStateFactory.java
Patch:
@@ -15,10 +15,10 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.solr.trident;
 
 import backtype.storm.task.IMetricsContext;
-import org.apache.storm.solr.config.SolrCommitStrategy;
 import org.apache.storm.solr.config.SolrConfig;
 import org.apache.storm.solr.mapper.SolrMapper;
 import storm.trident.state.State;

File: external/storm-solr/src/main/java/org/apache/storm/solr/trident/SolrUpdater.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.solr.trident;
 
 import storm.trident.operation.TridentCollector;

File: storm-core/src/jvm/backtype/storm/security/auth/kerberos/KerberosSaslTransportPlugin.java
Patch:
@@ -195,8 +195,9 @@ public TTransport run() {
                             return wrapped.getTransport(trans);
                         }
                         catch (Exception e) {
-                            LOG.error("Storm server failed to open transport to interact with a client during session initiation: " + e, e);
-                            return null;
+                            LOG.debug("Storm server failed to open transport " +
+                                    "to interact with a client during session initiation: " + e, e);
+                            return new NoOpTTrasport(null);
                         }
                     }
                 });

File: external/storm-elasticsearch/src/test/java/org/apache/storm/elasticsearch/bolt/EsIndexTopology.java
Patch:
@@ -29,6 +29,7 @@
 import org.apache.storm.elasticsearch.common.EsConfig;
 import org.apache.storm.elasticsearch.common.EsConstants;
 import org.apache.storm.elasticsearch.common.EsTestUtil;
+import org.apache.storm.elasticsearch.common.EsTupleMapper;
 
 import java.util.Map;
 import java.util.UUID;
@@ -49,7 +50,8 @@ public static void main(String[] args) throws Exception {
         EsConfig esConfig = new EsConfig();
         esConfig.setClusterName(EsConstants.clusterName);
         esConfig.setNodes(new String[]{"localhost:9300"});
-        builder.setBolt(BOLT_ID, new EsIndexBolt(esConfig), 1).shuffleGrouping(SPOUT_ID);
+        EsTupleMapper tupleMapper = EsTestUtil.generateDefaultTupleMapper();
+        builder.setBolt(BOLT_ID, new EsIndexBolt(esConfig, tupleMapper), 1).shuffleGrouping(SPOUT_ID);
 
         EsTestUtil.startEsNode();
         EsTestUtil.waitForSeconds(5);

File: external/storm-elasticsearch/src/test/java/org/apache/storm/elasticsearch/bolt/EsPercolateBoltTest.java
Patch:
@@ -21,6 +21,7 @@
 import backtype.storm.tuple.Values;
 import org.apache.storm.elasticsearch.common.EsConfig;
 import org.apache.storm.elasticsearch.common.EsTestUtil;
+import org.apache.storm.elasticsearch.common.EsTupleMapper;
 import org.elasticsearch.action.count.CountResponse;
 import org.elasticsearch.action.percolate.PercolateResponse;
 import org.elasticsearch.index.query.TermQueryBuilder;
@@ -42,7 +43,8 @@ public void testEsPercolateBolt()
         EsConfig esConfig = new EsConfig();
         esConfig.setClusterName("test-cluster");
         esConfig.setNodes(new String[]{"localhost:9300"});
-        bolt = new EsPercolateBolt(esConfig);
+        EsTupleMapper tupleMapper = EsTestUtil.generateDefaultTupleMapper();
+        bolt = new EsPercolateBolt(esConfig, tupleMapper);
         bolt.prepare(config, null, collector);
 
         String source = "{\"user\":\"user1\"}";

File: storm-core/src/jvm/backtype/storm/ConfigValidation.java
Patch:
@@ -28,6 +28,7 @@ public class ConfigValidation {
     /**
      * Declares methods for validating configuration values.
      */
+    public static interface FieldValidator {
         /**
          * Validates the given field.
          * @param name the name of the field.

File: storm-core/src/jvm/backtype/storm/Config.java
Patch:
@@ -1194,7 +1194,7 @@ public class Config extends HashMap<String, Object> {
      * The percentage of tuples to sample to produce stats for a task.
      */
     public static final String TOPOLOGY_STATS_SAMPLE_RATE="topology.stats.sample.rate";
-    public static final Object TOPOLOGY_STATS_SAMPLE_RATE_SCHEMA = ConfigValidation.DoubleValidator;
+    public static final Object TOPOLOGY_STATS_SAMPLE_RATE_SCHEMA = Number.class;
 
     /**
      * The time period that builtin metrics data in bucketed into.

File: external/storm-kafka/src/jvm/storm/kafka/FailedMsgRetryManager.java
Patch:
@@ -17,10 +17,13 @@
  */
 package storm.kafka;
 
+import java.util.Set;
+
 public interface FailedMsgRetryManager {
     public void failed(Long offset);
     public void acked(Long offset);
     public void retryStarted(Long offset);
     public Long nextFailedMessageToRetry();
     public boolean shouldRetryMsg(Long offset);
+    public Set<Long> clearInvalidMessages(Long kafkaOffset);
 }

File: storm-core/src/jvm/backtype/storm/spout/ISpoutOutputCollector.java
Patch:
@@ -27,5 +27,6 @@ public interface ISpoutOutputCollector extends IErrorReporter{
     */
     List<Integer> emit(String streamId, List<Object> tuple, Object messageId);
     void emitDirect(int taskId, String streamId, List<Object> tuple, Object messageId);
+    long getPendingCount();
 }
 

File: storm-core/test/jvm/backtype/storm/utils/DisruptorQueueTest.java
Patch:
@@ -154,6 +154,6 @@ public void run() {
 
     private static DisruptorQueue createQueue(String name, int queueSize) {
         return new DisruptorQueue(name, new MultiThreadedClaimStrategy(
-                queueSize), new BlockingWaitStrategy());
+                queueSize), new BlockingWaitStrategy(), 10L);
     }
 }

File: storm-core/src/jvm/backtype/storm/metric/MetricsConsumerBolt.java
Patch:
@@ -46,7 +46,7 @@ public void prepare(Map stormConf, TopologyContext context, OutputCollector coll
             throw new RuntimeException("Could not instantiate a class listed in config under section " +
                 Config.TOPOLOGY_METRICS_CONSUMER_REGISTER + " with fully qualified name " + _consumerClassName, e);
         }
-        _metricsConsumer.prepare(stormConf, _registrationArgument, context, (IErrorReporter)collector);
+        _metricsConsumer.prepare(stormConf, _registrationArgument, context, collector);
         _collector = collector;
     }
     

File: storm-core/src/jvm/backtype/storm/spout/ISpoutOutputCollector.java
Patch:
@@ -17,14 +17,15 @@
  */
 package backtype.storm.spout;
 
+import backtype.storm.task.IErrorReporter;
+
 import java.util.List;
 
-public interface ISpoutOutputCollector {
+public interface ISpoutOutputCollector extends IErrorReporter{
     /**
         Returns the task ids that received the tuples.
     */
     List<Integer> emit(String streamId, List<Object> tuple, Object messageId);
     void emitDirect(int taskId, String streamId, List<Object> tuple, Object messageId);
-    void reportError(Throwable error);
 }
 

File: storm-core/src/jvm/backtype/storm/topology/IBasicOutputCollector.java
Patch:
@@ -17,10 +17,11 @@
  */
 package backtype.storm.topology;
 
+import backtype.storm.task.IErrorReporter;
+
 import java.util.List;
 
-public interface IBasicOutputCollector {
+public interface IBasicOutputCollector extends IErrorReporter{
     List<Integer> emit(String streamId, List<Object> tuple);
     void emitDirect(int taskId, String streamId, List<Object> tuple);
-    void reportError(Throwable t);
 }

File: examples/storm-starter/src/jvm/storm/starter/BasicDRPCTopology.java
Patch:
@@ -32,9 +32,8 @@
 /**
  * This topology is a basic example of doing distributed RPC on top of Storm. It implements a function that appends a
  * "!" to any string you send the DRPC function.
- * <p/>
- * See https://github.com/nathanmarz/storm/wiki/Distributed-RPC for more information on doing distributed RPC on top of
- * Storm.
+ *
+ * @see <a href="http://storm.apache.org/documentation/Distributed-RPC.html">Distributed RPC</a>
  */
 public class BasicDRPCTopology {
   public static class ExclaimBolt extends BaseBasicBolt {

File: examples/storm-starter/src/jvm/storm/starter/ReachTopology.java
Patch:
@@ -47,8 +47,8 @@
  * minutes on a single machine into one that takes just a couple seconds.
  * <p/>
  * For the purposes of demonstration, this topology replaces the use of actual DBs with in-memory hashmaps.
- * <p/>
- * See https://github.com/nathanmarz/storm/wiki/Distributed-RPC for more information on Distributed RPC.
+ *
+ * @see <a href="http://storm.apache.org/documentation/Distributed-RPC.html">Distributed RPC</a>
  */
 public class ReachTopology {
   public static Map<String, List<String>> TWEETERS_DB = new HashMap<String, List<String>>() {{

File: examples/storm-starter/src/jvm/storm/starter/TransactionalGlobalCount.java
Patch:
@@ -40,8 +40,9 @@
 
 /**
  * This is a basic example of a transactional topology. It keeps a count of the number of tuples seen so far in a
- * database. The source of data and the databases are mocked out as in memory maps for demonstration purposes. This
- * class is defined in depth on the wiki at https://github.com/nathanmarz/storm/wiki/Transactional-topologies
+ * database. The source of data and the databases are mocked out as in memory maps for demonstration purposes.
+ *
+ * @see <a href="http://storm.apache.org/documentation/Transactional-topologies.html">Transactional topologies</a>
  */
 public class TransactionalGlobalCount {
   public static final int PARTITION_TAKE_PER_BATCH = 3;

File: storm-core/src/jvm/backtype/storm/tuple/Tuple.java
Patch:
@@ -29,7 +29,8 @@
  * Storm needs to know how to serialize all the values in a tuple. By default, Storm 
  * knows how to serialize the primitive types, strings, and byte arrays. If you want to 
  * use another type, you'll need to implement and register a serializer for that type.
- * See {@link http://github.com/nathanmarz/storm/wiki/Serialization} for more info.
+ *
+ * @see <a href="http://storm.apache.org/documentation/Serialization.html">Serialization</a>
  */
 public interface Tuple extends ITuple{
 

File: storm-core/src/jvm/backtype/storm/utils/StormBoundedExponentialBackoffRetry.java
Patch:
@@ -44,8 +44,8 @@ public StormBoundedExponentialBackoffRetry(int baseSleepTimeMs, int maxSleepTime
         expRetriesThreshold = 1;
         while ((1 << (expRetriesThreshold + 1)) < ((maxSleepTimeMs - baseSleepTimeMs) / 2))
             expRetriesThreshold++;
-        LOG.info("The baseSleepTimeMs [" + baseSleepTimeMs + "] the maxSleepTimeMs [" + maxSleepTimeMs + "] " +
-                "the maxRetries [" + maxRetries + "]");
+        LOG.debug("The baseSleepTimeMs [{}] the maxSleepTimeMs [{}] the maxRetries [{}]", 
+				baseSleepTimeMs, maxSleepTimeMs, maxRetries);
         if (baseSleepTimeMs > maxSleepTimeMs) {
             LOG.warn("Misconfiguration: the baseSleepTimeMs [" + baseSleepTimeMs + "] can't be greater than " +
                     "the maxSleepTimeMs [" + maxSleepTimeMs + "].");

File: storm-core/src/jvm/backtype/storm/utils/StormBoundedExponentialBackoffRetry.java
Patch:
@@ -44,7 +44,7 @@ public StormBoundedExponentialBackoffRetry(int baseSleepTimeMs, int maxSleepTime
         expRetriesThreshold = 1;
         while ((1 << (expRetriesThreshold + 1)) < ((maxSleepTimeMs - baseSleepTimeMs) / 2))
             expRetriesThreshold++;
-        LOG.info("The baseSleepTimeMs [" + baseSleepTimeMs + "] the maxSleepTimeMs [" + maxSleepTimeMs + "] " +
+        LOG.debug("The baseSleepTimeMs [" + baseSleepTimeMs + "] the maxSleepTimeMs [" + maxSleepTimeMs + "] " +
                 "the maxRetries [" + maxRetries + "]");
         if (baseSleepTimeMs > maxSleepTimeMs) {
             LOG.warn("Misconfiguration: the baseSleepTimeMs [" + baseSleepTimeMs + "] can't be greater than " +

File: examples/storm-starter/src/jvm/storm/starter/BasicDRPCTopology.java
Patch:
@@ -32,9 +32,8 @@
 /**
  * This topology is a basic example of doing distributed RPC on top of Storm. It implements a function that appends a
  * "!" to any string you send the DRPC function.
- * <p/>
- * See https://github.com/nathanmarz/storm/wiki/Distributed-RPC for more information on doing distributed RPC on top of
- * Storm.
+ *
+ * @see <a href="http://storm.apache.org/documentation/Distributed-RPC.html">Distributed RPC</a>
  */
 public class BasicDRPCTopology {
   public static class ExclaimBolt extends BaseBasicBolt {

File: examples/storm-starter/src/jvm/storm/starter/ReachTopology.java
Patch:
@@ -47,8 +47,8 @@
  * minutes on a single machine into one that takes just a couple seconds.
  * <p/>
  * For the purposes of demonstration, this topology replaces the use of actual DBs with in-memory hashmaps.
- * <p/>
- * See https://github.com/nathanmarz/storm/wiki/Distributed-RPC for more information on Distributed RPC.
+ *
+ * @see <a href="http://storm.apache.org/documentation/Distributed-RPC.html">Distributed RPC</a>
  */
 public class ReachTopology {
   public static Map<String, List<String>> TWEETERS_DB = new HashMap<String, List<String>>() {{

File: examples/storm-starter/src/jvm/storm/starter/TransactionalGlobalCount.java
Patch:
@@ -40,8 +40,9 @@
 
 /**
  * This is a basic example of a transactional topology. It keeps a count of the number of tuples seen so far in a
- * database. The source of data and the databases are mocked out as in memory maps for demonstration purposes. This
- * class is defined in depth on the wiki at https://github.com/nathanmarz/storm/wiki/Transactional-topologies
+ * database. The source of data and the databases are mocked out as in memory maps for demonstration purposes.
+ *
+ * @see <a href="http://storm.apache.org/documentation/Transactional-topologies.html">Transactional topologies</a>
  */
 public class TransactionalGlobalCount {
   public static final int PARTITION_TAKE_PER_BATCH = 3;

File: storm-core/src/jvm/backtype/storm/tuple/Tuple.java
Patch:
@@ -29,7 +29,8 @@
  * Storm needs to know how to serialize all the values in a tuple. By default, Storm 
  * knows how to serialize the primitive types, strings, and byte arrays. If you want to 
  * use another type, you'll need to implement and register a serializer for that type.
- * See {@link http://github.com/nathanmarz/storm/wiki/Serialization} for more info.
+ *
+ * @see <a href="http://storm.apache.org/documentation/Serialization.html">Serialization</a>
  */
 public interface Tuple extends ITuple{
 

File: storm-core/src/jvm/backtype/storm/utils/DisruptorQueue.java
Patch:
@@ -94,7 +94,7 @@ public void haltWithInterrupt() {
     public void consumeBatchWhenAvailable(EventHandler<Object> handler) {
         try {
             final long nextSequence = _consumer.get() + 1;
-            final long availableSequence = _barrier.waitFor(nextSequence, 10, TimeUnit.MILLISECONDS);
+            final long availableSequence = _barrier.waitFor(nextSequence, 1000, TimeUnit.MILLISECONDS);
             if(availableSequence >= nextSequence) {
                 consumeBatchToCursor(availableSequence, handler);
             }

File: storm-core/src/jvm/backtype/storm/spout/ShellSpout.java
Patch:
@@ -26,6 +26,7 @@
 import backtype.storm.task.TopologyContext;
 import backtype.storm.utils.ShellProcess;
 import java.util.Map;
+import java.util.HashMap;
 import java.util.List;
 import java.util.TimerTask;
 import java.util.concurrent.ScheduledExecutorService;

File: storm-core/src/jvm/backtype/storm/utils/ShellProcess.java
Patch:
@@ -28,6 +28,7 @@
 import java.io.IOException;
 import java.io.InputStream;
 import java.io.Serializable;
+import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 
@@ -63,6 +64,7 @@ private void modifyEnvironment(Map<String, String> buildEnv) {
         }
     }
 
+
     public Number launch(Map conf, TopologyContext context) {
         ProcessBuilder builder = new ProcessBuilder(command);
         if (!env.isEmpty()) {

File: storm-core/src/jvm/backtype/storm/messaging/netty/SaslStormClientHandler.java
Patch:
@@ -146,8 +146,8 @@ public void messageReceived(ChannelHandlerContext ctx, MessageEvent event)
     }
 
     private void getSASLCredentials() throws IOException {
-        topologyName = (String) this.client.stormConf.get(Config.TOPOLOGY_NAME);
-        String secretKey = SaslUtils.getSecretKey(this.client.stormConf);
+        topologyName = (String) this.client.getStormConf().get(Config.TOPOLOGY_NAME);
+        String secretKey = SaslUtils.getSecretKey(this.client.getStormConf());
         if (secretKey != null) {
             token = secretKey.getBytes();
         }

File: storm-core/src/jvm/backtype/storm/messaging/netty/StormClientPipelineFactory.java
Patch:
@@ -39,14 +39,14 @@ public ChannelPipeline getPipeline() throws Exception {
         // Encoder
         pipeline.addLast("encoder", new MessageEncoder());
 
-        boolean isNettyAuth = (Boolean) this.client.stormConf.get(Config.STORM_MESSAGING_NETTY_AUTHENTICATION);
+        boolean isNettyAuth = (Boolean) this.client.getStormConf().get(Config.STORM_MESSAGING_NETTY_AUTHENTICATION);
         if (isNettyAuth) {
             // Authenticate: Removed after authentication completes
             pipeline.addLast("saslClientHandler", new SaslStormClientHandler(
                     client));
         }
         // business logic.
-        pipeline.addLast("handler", new StormClientErrorHandler(client.dstAddressPrefixedName));
+        pipeline.addLast("handler", new StormClientHandler(client));
 
         return pipeline;
     }

File: storm-core/src/jvm/backtype/storm/metric/MetricsConsumerBolt.java
Patch:
@@ -46,7 +46,7 @@ public void prepare(Map stormConf, TopologyContext context, OutputCollector coll
             throw new RuntimeException("Could not instantiate a class listed in config under section " +
                 Config.TOPOLOGY_METRICS_CONSUMER_REGISTER + " with fully qualified name " + _consumerClassName, e);
         }
-        _metricsConsumer.prepare(stormConf, _registrationArgument, context, (IErrorReporter)collector);
+        _metricsConsumer.prepare(stormConf, _registrationArgument, context, collector);
         _collector = collector;
     }
     

File: storm-core/src/jvm/backtype/storm/spout/ISpoutOutputCollector.java
Patch:
@@ -17,14 +17,15 @@
  */
 package backtype.storm.spout;
 
+import backtype.storm.task.IErrorReporter;
+
 import java.util.List;
 
-public interface ISpoutOutputCollector {
+public interface ISpoutOutputCollector extends IErrorReporter{
     /**
         Returns the task ids that received the tuples.
     */
     List<Integer> emit(String streamId, List<Object> tuple, Object messageId);
     void emitDirect(int taskId, String streamId, List<Object> tuple, Object messageId);
-    void reportError(Throwable error);
 }
 

File: storm-core/src/jvm/backtype/storm/topology/IBasicOutputCollector.java
Patch:
@@ -17,10 +17,11 @@
  */
 package backtype.storm.topology;
 
+import backtype.storm.task.IErrorReporter;
+
 import java.util.List;
 
-public interface IBasicOutputCollector {
+public interface IBasicOutputCollector extends IErrorReporter{
     List<Integer> emit(String streamId, List<Object> tuple);
     void emitDirect(int taskId, String streamId, List<Object> tuple);
-    void reportError(Throwable t);
 }

File: external/storm-eventhubs/src/test/java/org/apache/storm/eventhubs/spout/TestEventHubSpout.java
Patch:
@@ -37,7 +37,9 @@ public void tearDown() throws Exception {
   @Test
   public void testSpoutConfig() {
     EventHubSpoutConfig conf = new EventHubSpoutConfig("username", "pas\\s+w/ord",
-        "namespace", "entityname", 16, "zookeeper");
+        "namespace", "entityname", 16);
+    conf.setZkConnectionString("zookeeper");
+    conf.setCheckpointIntervalInSeconds(1);
     assertEquals(conf.getConnectionString(), "amqps://username:pas%5Cs%2Bw%2Ford@namespace.servicebus.windows.net");
   }
 

File: external/storm-kafka/src/test/storm/kafka/DynamicBrokersReaderTest.java
Patch:
@@ -171,7 +171,7 @@ public void testSwitchHostForPartition() throws Exception {
         assertEquals(newHost, brokerInfo.getBrokerFor(partition).host);
     }
 
-    @Test(expected = RuntimeException.class)
+    @Test(expected = NullPointerException.class)
     public void testErrorLogsWhenConfigIsMissing() throws Exception {
         String connectionString = server.getConnectString();
         Map conf = new HashMap();

File: external/storm-kafka/src/test/storm/kafka/ZkCoordinatorTest.java
Patch:
@@ -69,6 +69,7 @@ private Map buildZookeeperConfig(TestingServer server) {
         conf.put(Config.TRANSACTIONAL_ZOOKEEPER_PORT, server.getPort());
         conf.put(Config.TRANSACTIONAL_ZOOKEEPER_SERVERS, Arrays.asList("localhost"));
         conf.put(Config.STORM_ZOOKEEPER_SESSION_TIMEOUT, 20000);
+        conf.put(Config.STORM_ZOOKEEPER_CONNECTION_TIMEOUT, 20000);
         conf.put(Config.STORM_ZOOKEEPER_RETRY_TIMES, 3);
         conf.put(Config.STORM_ZOOKEEPER_RETRY_INTERVAL, 30);
         return conf;

File: storm-core/src/jvm/backtype/storm/task/IBolt.java
Patch:
@@ -34,7 +34,7 @@
  * Nimbus then launches workers which deserialize the object, call prepare on it, and then
  * start processing tuples.</p>
  *
- * <p>If you want to parameterize an IBolt, you should set the parameter's through its
+ * <p>If you want to parameterize an IBolt, you should set the parameters through its
  * constructor and save the parameterization state as instance variables (which will
  * then get serialized and shipped to every task executing this bolt across the cluster).</p>
  *
@@ -81,4 +81,4 @@ public interface IBolt extends Serializable {
      * is killed when running Storm in local mode.</p>
      */
     void cleanup();
-}
\ No newline at end of file
+}

File: storm-core/src/jvm/backtype/storm/task/IBolt.java
Patch:
@@ -34,7 +34,7 @@
  * Nimbus then launches workers which deserialize the object, call prepare on it, and then
  * start processing tuples.</p>
  *
- * <p>If you want to parameterize an IBolt, you should set the parameter's through its
+ * <p>If you want to parameterize an IBolt, you should set the parameters through its
  * constructor and save the parameterization state as instance variables (which will
  * then get serialized and shipped to every task executing this bolt across the cluster).</p>
  *
@@ -81,4 +81,4 @@ public interface IBolt extends Serializable {
      * is killed when running Storm in local mode.</p>
      */
     void cleanup();
-}
\ No newline at end of file
+}

File: storm-core/src/jvm/backtype/storm/generated/SupervisorInfo.java
Patch:
@@ -51,7 +51,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-4-17")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-4-20")
 public class SupervisorInfo implements org.apache.thrift.TBase<SupervisorInfo, SupervisorInfo._Fields>, java.io.Serializable, Cloneable, Comparable<SupervisorInfo> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("SupervisorInfo");
 

File: external/storm-kafka/src/jvm/storm/kafka/MessageMetadataSchemeAsMultiScheme.java
Patch:
@@ -3,13 +3,12 @@
 import java.util.Arrays;
 import java.util.List;
 
-import backtype.storm.spout.Scheme;
 import backtype.storm.spout.SchemeAsMultiScheme;
 
 public class MessageMetadataSchemeAsMultiScheme extends SchemeAsMultiScheme {
     private static final long serialVersionUID = -7172403703813625116L;
 
-    public MessageMetadataSchemeAsMultiScheme(Scheme scheme) {
+    public MessageMetadataSchemeAsMultiScheme(MessageMetadataScheme scheme) {
         super(scheme);
     }
 

File: flux-core/src/main/java/org/apache/storm/flux/model/VertexDef.java
Patch:
@@ -23,7 +23,8 @@
  */
 public abstract class VertexDef extends BeanDef {
 
-    private int parallelism;
+    // default parallelism to 1 so if it's ommitted, the topology will still function.
+    private int parallelism = 1;
 
     public int getParallelism() {
         return parallelism;

File: storm-core/src/jvm/backtype/storm/Config.java
Patch:
@@ -42,7 +42,7 @@
  * Spouts.</p>
  */
 public class Config extends HashMap<String, Object> {
-    //DO NOT CHANGE UNLESS WE ADD IN STATE NOTE STORED IN THE PARENT CLASS
+    //DO NOT CHANGE UNLESS WE ADD IN STATE NOT STORED IN THE PARENT CLASS
     private static final long serialVersionUID = -1550278723792864455L;
 
     /**

File: external/storm-redis/src/test/java/org/apache/storm/redis/trident/WordCountLookupMapper.java
Patch:
@@ -25,7 +25,7 @@ public void declareOutputFields(OutputFieldsDeclarer declarer) {
 
     @Override
     public RedisDataTypeDescription getDataTypeDescription() {
-        return new RedisDataTypeDescription(RedisDataTypeDescription.RedisDataType.STRING);
+        return new RedisDataTypeDescription(RedisDataTypeDescription.RedisDataType.HASH, "test");
     }
 
     @Override

File: external/storm-redis/src/test/java/org/apache/storm/redis/trident/WordCountStoreMapper.java
Patch:
@@ -7,7 +7,7 @@
 public class WordCountStoreMapper implements RedisStoreMapper {
     @Override
     public RedisDataTypeDescription getDataTypeDescription() {
-        return new RedisDataTypeDescription(RedisDataTypeDescription.RedisDataType.STRING);
+        return new RedisDataTypeDescription(RedisDataTypeDescription.RedisDataType.HASH, "test");
     }
 
     @Override

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/bolt/HBaseBolt.java
Patch:
@@ -62,7 +62,7 @@ public void execute(Tuple tuple) {
         try {
             this.hBaseClient.batchMutate(mutations);
         } catch(Exception e){
-            LOG.warn("Failing tuple. Error writing rowKey " + rowKey, e);
+            this.collector.reportError(e);
             this.collector.fail(tuple);
             return;
         }

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/bolt/HBaseLookupBolt.java
Patch:
@@ -67,11 +67,11 @@ public void execute(Tuple tuple) {
         try {
             Result result = hBaseClient.batchGet(Lists.newArrayList(get))[0];
             for(Values values : rowToTupleMapper.toValues(tuple, result)) {
-                this.collector.emit(values);
+                this.collector.emit(tuple, values);
             }
             this.collector.ack(tuple);
         } catch (Exception e) {
-            LOG.warn("Could not perform Lookup for rowKey =" + rowKey + " from Hbase.", e);
+            this.collector.reportError(e);
             this.collector.fail(tuple);
         }
     }

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/trident/state/HBaseMapState.java
Patch:
@@ -231,6 +231,8 @@ public void multiPut(List<List<Object>> keys, List<T> values) {
             throw new FailedException("Interrupted while writing to HBase", e);
         } catch (RetriesExhaustedWithDetailsException e) {
             throw new FailedException("Retries exhaused while writing to HBase", e);
+        } catch (IOException e) {
+            throw new FailedException("IOException while writing to HBase", e);
         }
     }
 

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/trident/state/HBaseState.java
Patch:
@@ -141,7 +141,7 @@ public void updateState(List<TridentTuple> tuples, TridentCollector collector) {
         try {
             hBaseClient.batchMutate(mutations);
         } catch (Exception e) {
-            LOG.warn("Batch write failed but some requests might have succeeded. Triggering replay.", e);
+            collector.reportError(e);
             throw new FailedException(e);
         }
     }

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/bolt/HdfsBolt.java
Patch:
@@ -112,7 +112,7 @@ public void execute(Tuple tuple) {
                 this.rotationPolicy.reset();
             }
         } catch (IOException e) {
-            LOG.warn("write/sync failed.", e);
+            this.collector.reportError(e);
             this.collector.fail(tuple);
         }
     }

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/bolt/SequenceFileBolt.java
Patch:
@@ -123,7 +123,7 @@ public void execute(Tuple tuple) {
                 this.rotationPolicy.reset();
             }
         } catch (IOException e) {
-            LOG.warn("write/sync failed.", e);
+            this.collector.reportError(e);
             this.collector.fail(tuple);
         }
 

File: external/storm-redis/src/main/java/org/apache/storm/redis/common/config/JedisClusterConfig.java
Patch:
@@ -15,7 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.storm.redis.util.config;
+package org.apache.storm.redis.common.config;
 
 import com.google.common.base.Preconditions;
 import redis.clients.jedis.HostAndPort;

File: external/storm-redis/src/main/java/org/apache/storm/redis/common/config/JedisPoolConfig.java
Patch:
@@ -15,7 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.storm.redis.util.config;
+package org.apache.storm.redis.common.config;
 
 import redis.clients.jedis.Protocol;
 

File: external/storm-redis/src/main/java/org/apache/storm/redis/common/container/JedisClusterContainer.java
Patch:
@@ -15,7 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.storm.redis.util.container;
+package org.apache.storm.redis.common.container;
 
 import redis.clients.jedis.JedisCluster;
 import redis.clients.jedis.JedisCommands;

File: external/storm-redis/src/main/java/org/apache/storm/redis/common/container/JedisCommandsContainerBuilder.java
Patch:
@@ -15,10 +15,10 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.storm.redis.util.container;
+package org.apache.storm.redis.common.container;
 
-import org.apache.storm.redis.util.config.JedisClusterConfig;
-import org.apache.storm.redis.util.config.JedisPoolConfig;
+import org.apache.storm.redis.common.config.JedisClusterConfig;
+import org.apache.storm.redis.common.config.JedisPoolConfig;
 import redis.clients.jedis.JedisCluster;
 import redis.clients.jedis.JedisPool;
 

File: external/storm-redis/src/main/java/org/apache/storm/redis/common/container/JedisCommandsInstanceContainer.java
Patch:
@@ -15,7 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.storm.redis.util.container;
+package org.apache.storm.redis.common.container;
 
 import redis.clients.jedis.JedisCommands;
 

File: external/storm-redis/src/main/java/org/apache/storm/redis/common/container/JedisContainer.java
Patch:
@@ -15,7 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.storm.redis.util.container;
+package org.apache.storm.redis.common.container;
 
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;

File: external/storm-redis/src/main/java/org/apache/storm/redis/trident/state/RedisClusterMapState.java
Patch:
@@ -23,7 +23,7 @@
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.Lists;
 import com.google.common.collect.Maps;
-import org.apache.storm.redis.util.config.JedisClusterConfig;
+import org.apache.storm.redis.common.config.JedisClusterConfig;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import redis.clients.jedis.JedisCluster;

File: external/storm-redis/src/main/java/org/apache/storm/redis/trident/state/RedisClusterState.java
Patch:
@@ -18,7 +18,7 @@
 package org.apache.storm.redis.trident.state;
 
 import backtype.storm.task.IMetricsContext;
-import org.apache.storm.redis.util.config.JedisClusterConfig;
+import org.apache.storm.redis.common.config.JedisClusterConfig;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import redis.clients.jedis.JedisCluster;

File: external/storm-redis/src/main/java/org/apache/storm/redis/trident/state/RedisState.java
Patch:
@@ -18,7 +18,7 @@
 package org.apache.storm.redis.trident.state;
 
 import backtype.storm.task.IMetricsContext;
-import org.apache.storm.redis.util.config.JedisPoolConfig;
+import org.apache.storm.redis.common.config.JedisPoolConfig;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import redis.clients.jedis.Jedis;

File: external/storm-redis/src/test/java/org/apache/storm/redis/trident/WordCountTridentRedisCluster.java
Patch:
@@ -23,11 +23,11 @@
 import backtype.storm.generated.StormTopology;
 import backtype.storm.tuple.Fields;
 import backtype.storm.tuple.Values;
-import org.apache.storm.redis.trident.mapper.TridentTupleMapper;
+import org.apache.storm.redis.common.mapper.TupleMapper;
 import org.apache.storm.redis.trident.state.RedisClusterState;
 import org.apache.storm.redis.trident.state.RedisClusterStateQuerier;
 import org.apache.storm.redis.trident.state.RedisClusterStateUpdater;
-import org.apache.storm.redis.util.config.JedisClusterConfig;
+import org.apache.storm.redis.common.config.JedisClusterConfig;
 import storm.trident.Stream;
 import storm.trident.TridentState;
 import storm.trident.TridentTopology;
@@ -55,7 +55,7 @@ public static StormTopology buildTopology(String redisHostPort){
         }
         JedisClusterConfig clusterConfig = new JedisClusterConfig.Builder().setNodes(nodes)
                                         .build();
-        TridentTupleMapper tupleMapper = new WordCountTupleMapper();
+        TupleMapper tupleMapper = new WordCountTupleMapper();
         RedisClusterState.Factory factory = new RedisClusterState.Factory(clusterConfig);
 
         TridentTopology topology = new TridentTopology();

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/bolt/HBaseBolt.java
Patch:
@@ -62,7 +62,7 @@ public void execute(Tuple tuple) {
         try {
             this.hBaseClient.batchMutate(mutations);
         } catch(Exception e){
-            LOG.warn("Failing tuple. Error writing rowKey " + rowKey, e);
+            this.collector.reportError(e);
             this.collector.fail(tuple);
             return;
         }

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/bolt/HBaseLookupBolt.java
Patch:
@@ -67,11 +67,11 @@ public void execute(Tuple tuple) {
         try {
             Result result = hBaseClient.batchGet(Lists.newArrayList(get))[0];
             for(Values values : rowToTupleMapper.toValues(tuple, result)) {
-                this.collector.emit(values);
+                this.collector.emit(tuple, values);
             }
             this.collector.ack(tuple);
         } catch (Exception e) {
-            LOG.warn("Could not perform Lookup for rowKey =" + rowKey + " from Hbase.", e);
+            this.collector.reportError(e);
             this.collector.fail(tuple);
         }
     }

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/trident/state/HBaseMapState.java
Patch:
@@ -231,6 +231,8 @@ public void multiPut(List<List<Object>> keys, List<T> values) {
             throw new FailedException("Interrupted while writing to HBase", e);
         } catch (RetriesExhaustedWithDetailsException e) {
             throw new FailedException("Retries exhaused while writing to HBase", e);
+        } catch (IOException e) {
+            throw new FailedException("IOException while writing to HBase", e);
         }
     }
 

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/trident/state/HBaseState.java
Patch:
@@ -141,7 +141,7 @@ public void updateState(List<TridentTuple> tuples, TridentCollector collector) {
         try {
             hBaseClient.batchMutate(mutations);
         } catch (Exception e) {
-            LOG.warn("Batch write failed but some requests might have succeeded. Triggering replay.", e);
+            collector.reportError(e);
             throw new FailedException(e);
         }
     }

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/bolt/HdfsBolt.java
Patch:
@@ -112,7 +112,7 @@ public void execute(Tuple tuple) {
                 this.rotationPolicy.reset();
             }
         } catch (IOException e) {
-            LOG.warn("write/sync failed.", e);
+            this.collector.reportError(e);
             this.collector.fail(tuple);
         }
     }

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/bolt/SequenceFileBolt.java
Patch:
@@ -123,7 +123,7 @@ public void execute(Tuple tuple) {
                 this.rotationPolicy.reset();
             }
         } catch (IOException e) {
-            LOG.warn("write/sync failed.", e);
+            this.collector.reportError(e);
             this.collector.fail(tuple);
         }
 

File: external/storm-redis/src/main/java/org/apache/storm/redis/common/config/JedisClusterConfig.java
Patch:
@@ -15,7 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.storm.redis.util.config;
+package org.apache.storm.redis.common.config;
 
 import com.google.common.base.Preconditions;
 import redis.clients.jedis.HostAndPort;

File: external/storm-redis/src/main/java/org/apache/storm/redis/common/config/JedisPoolConfig.java
Patch:
@@ -15,7 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.storm.redis.util.config;
+package org.apache.storm.redis.common.config;
 
 import redis.clients.jedis.Protocol;
 

File: external/storm-redis/src/main/java/org/apache/storm/redis/common/container/JedisClusterContainer.java
Patch:
@@ -15,7 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.storm.redis.util.container;
+package org.apache.storm.redis.common.container;
 
 import redis.clients.jedis.JedisCluster;
 import redis.clients.jedis.JedisCommands;

File: external/storm-redis/src/main/java/org/apache/storm/redis/common/container/JedisCommandsContainerBuilder.java
Patch:
@@ -15,10 +15,10 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.storm.redis.util.container;
+package org.apache.storm.redis.common.container;
 
-import org.apache.storm.redis.util.config.JedisClusterConfig;
-import org.apache.storm.redis.util.config.JedisPoolConfig;
+import org.apache.storm.redis.common.config.JedisClusterConfig;
+import org.apache.storm.redis.common.config.JedisPoolConfig;
 import redis.clients.jedis.JedisCluster;
 import redis.clients.jedis.JedisPool;
 

File: external/storm-redis/src/main/java/org/apache/storm/redis/common/container/JedisCommandsInstanceContainer.java
Patch:
@@ -15,7 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.storm.redis.util.container;
+package org.apache.storm.redis.common.container;
 
 import redis.clients.jedis.JedisCommands;
 

File: external/storm-redis/src/main/java/org/apache/storm/redis/common/container/JedisContainer.java
Patch:
@@ -15,7 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.storm.redis.util.container;
+package org.apache.storm.redis.common.container;
 
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;

File: external/storm-redis/src/main/java/org/apache/storm/redis/trident/state/RedisClusterMapState.java
Patch:
@@ -23,7 +23,7 @@
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.Lists;
 import com.google.common.collect.Maps;
-import org.apache.storm.redis.util.config.JedisClusterConfig;
+import org.apache.storm.redis.common.config.JedisClusterConfig;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import redis.clients.jedis.JedisCluster;

File: external/storm-redis/src/main/java/org/apache/storm/redis/trident/state/RedisClusterState.java
Patch:
@@ -18,7 +18,7 @@
 package org.apache.storm.redis.trident.state;
 
 import backtype.storm.task.IMetricsContext;
-import org.apache.storm.redis.util.config.JedisClusterConfig;
+import org.apache.storm.redis.common.config.JedisClusterConfig;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import redis.clients.jedis.JedisCluster;

File: external/storm-redis/src/main/java/org/apache/storm/redis/trident/state/RedisState.java
Patch:
@@ -18,7 +18,7 @@
 package org.apache.storm.redis.trident.state;
 
 import backtype.storm.task.IMetricsContext;
-import org.apache.storm.redis.util.config.JedisPoolConfig;
+import org.apache.storm.redis.common.config.JedisPoolConfig;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import redis.clients.jedis.Jedis;

File: external/storm-redis/src/test/java/org/apache/storm/redis/trident/WordCountTridentRedisCluster.java
Patch:
@@ -23,11 +23,11 @@
 import backtype.storm.generated.StormTopology;
 import backtype.storm.tuple.Fields;
 import backtype.storm.tuple.Values;
-import org.apache.storm.redis.trident.mapper.TridentTupleMapper;
+import org.apache.storm.redis.common.mapper.TupleMapper;
 import org.apache.storm.redis.trident.state.RedisClusterState;
 import org.apache.storm.redis.trident.state.RedisClusterStateQuerier;
 import org.apache.storm.redis.trident.state.RedisClusterStateUpdater;
-import org.apache.storm.redis.util.config.JedisClusterConfig;
+import org.apache.storm.redis.common.config.JedisClusterConfig;
 import storm.trident.Stream;
 import storm.trident.TridentState;
 import storm.trident.TridentTopology;
@@ -55,7 +55,7 @@ public static StormTopology buildTopology(String redisHostPort){
         }
         JedisClusterConfig clusterConfig = new JedisClusterConfig.Builder().setNodes(nodes)
                                         .build();
-        TridentTupleMapper tupleMapper = new WordCountTupleMapper();
+        TupleMapper tupleMapper = new WordCountTupleMapper();
         RedisClusterState.Factory factory = new RedisClusterState.Factory(clusterConfig);
 
         TridentTopology topology = new TridentTopology();

File: flux-core/src/main/java/org/apache/storm/flux/FluxBuilder.java
Patch:
@@ -161,6 +161,7 @@ private static CustomStreamGrouping buildCustomStreamGrouping(ObjectDef def, Exe
         } else {
             grouping = (CustomStreamGrouping) clazz.newInstance();
         }
+        applyProperties(def, grouping, context);
         return grouping;
     }
 
@@ -324,6 +325,7 @@ private static IRichSpout buildSpout(SpoutDef def, ExecutionContext context) thr
         } else {
             spout = (IRichSpout) clazz.newInstance();
         }
+        applyProperties(def, spout, context);
         return spout;
     }
 
@@ -361,6 +363,7 @@ private static void buildBolts(ExecutionContext context) throws ClassNotFoundExc
                 bolt = clazz.newInstance();
             }
             context.addBolt(def.getId(), bolt);
+            applyProperties(def, bolt, context);
         }
     }
 

File: src/main/java/org/apache/storm/flux/Flux.java
Patch:
@@ -141,7 +141,7 @@ private static void runCli(CommandLine cmd)throws Exception {
                     }
                     StormSubmitter.submitTopology(topologyName, conf, topology, submitOptions);
                 } catch (Exception e) {
-                    LOG.warn("Unable to deploy topology tp remote cluster.", e);
+                    LOG.warn("Unable to deploy topology to remote cluster.", e);
                 }
             } else {
                 LOG.info("Running in local mode...");

File: external/storm-jdbc/src/main/java/org/apache/storm/jdbc/common/Column.java
Patch:
@@ -42,6 +42,7 @@
  * rows.add(row2)
  *
  * </pre>
+ *
  * @param <T>
  */
 public class Column<T> implements Serializable {
@@ -50,7 +51,7 @@ public class Column<T> implements Serializable {
     private T val;
 
     /**
-     * The sql type(e.g. varchar, date, int) Idealy we would have an enum but java's jdbc API uses integer.
+     * The sql type(e.g. varchar, date, int) Ideally we would have an enum but java's jdbc API uses integer.
      * See {@link java.sql.Types}
      */
     private int sqlType;

File: external/storm-jdbc/src/main/java/org/apache/storm/jdbc/mapper/SimpleJdbcLookupMapper.java
Patch:
@@ -28,7 +28,7 @@ public List<Values> toTuple(ITuple input, List<Column> columns) {
                 values.add(input.getValueByField(field));
             } else {
                 for(Column column : columns) {
-                    if(column.getColumnName().equals(field)) {
+                    if(column.getColumnName().equalsIgnoreCase(field)) {
                         values.add(column.getVal());
                     }
                 }

File: external/storm-jdbc/src/test/java/org/apache/storm/jdbc/topology/UserPersistanceTopology.java
Patch:
@@ -35,7 +35,7 @@ public static void main(String[] args) throws Exception {
     @Override
     public StormTopology getTopology() {
         JdbcLookupBolt departmentLookupBolt = new JdbcLookupBolt(JDBC_CONF, SELECT_QUERY, this.jdbcLookupMapper);
-        JdbcInsertBolt userPersistanceBolt = new JdbcInsertBolt(JDBC_CONF, TABLE_NAME, this.jdbcMapper);
+        JdbcInsertBolt userPersistanceBolt = new JdbcInsertBolt(JDBC_CONF, this.jdbcMapper).withInsertQuery("insert into user (create_date, dept_name, user_id, user_name) values (?,?,?,?)");
 
         // userSpout ==> jdbcBolt
         TopologyBuilder builder = new TopologyBuilder();

File: external/storm-kafka/src/jvm/storm/kafka/ExponentialBackoffMsgRetryManager.java
Patch:
@@ -156,7 +156,7 @@ class RetryTimeComparator implements Comparator<MessageRetryRecord> {
 
         @Override
         public int compare(MessageRetryRecord record1, MessageRetryRecord record2) {
-            return Long.compare(record1.retryTimeUTC, record2.retryTimeUTC);
+            return Long.valueOf(record1.retryTimeUTC).compareTo(Long.valueOf(record2.retryTimeUTC));
         }
 
         @Override

File: external/storm-kafka/src/test/storm/kafka/bolt/KafkaBoltTest.java
Patch:
@@ -143,7 +143,7 @@ public void executeWithBrokerDown() throws Exception {
         String message = "value-234";
         Tuple tuple = generateTestTuple(message);
         bolt.execute(tuple);
-        verify(collector).ack(tuple);
+        verify(collector).fail(tuple);
     }
 
 

File: external/storm-redis/src/main/java/org/apache/storm/redis/trident/state/RedisState.java
Patch:
@@ -77,7 +77,7 @@ public Jedis getJedis() {
      * The state updater and querier return the Jedis instance
      * */
     public void returnJedis(Jedis jedis) {
-        this.jedisPool.returnResource(jedis);
+        jedis.close();
     }
 
 }

File: storm-core/src/jvm/backtype/storm/coordination/BatchBoltExecutor.java
Patch:
@@ -41,7 +41,7 @@ public class BatchBoltExecutor implements IRichBolt, FinishedCallback, TimeoutCa
     BatchOutputCollectorImpl _collector;
     
     public BatchBoltExecutor(IBatchBolt bolt) {
-        _boltSer = Utils.serialize(bolt);
+        _boltSer = Utils.javaSerialize(bolt);
     }
     
     @Override
@@ -103,6 +103,6 @@ private IBatchBolt getBatchBolt(Object id) {
     }
     
     private IBatchBolt newTransactionalBolt() {
-        return (IBatchBolt) Utils.deserialize(_boltSer);
+        return Utils.javaDeserialize(_boltSer, IBatchBolt.class);
     }
 }

File: storm-core/src/jvm/backtype/storm/security/auth/ITransportPlugin.java
Patch:
@@ -54,6 +54,8 @@ public interface ITransportPlugin {
      * Connect to the specified server via framed transport 
      * @param transport The underlying Thrift transport.
      * @param serverHost server host
+     * @param asUser the user as which the connection should be established, and all the subsequent actions should be executed.
+     *               Only applicable when using secure storm cluster. A null/blank value here will just indicate to use the logged in user.
      */
-    public TTransport connect(TTransport transport, String serverHost) throws IOException, TTransportException;
+    public TTransport connect(TTransport transport, String serverHost, String asUser) throws IOException, TTransportException;
 }

File: storm-core/src/jvm/backtype/storm/security/auth/SaslTransportPlugin.java
Patch:
@@ -116,7 +116,6 @@ public boolean process(final TProtocol inProt, final TProtocol outProt) throws T
             TTransport trans = inProt.getTransport();
             //Sasl transport
             TSaslServerTransport saslTrans = (TSaslServerTransport)trans;
-
             //remote address
             TSocket tsocket = (TSocket)saslTrans.getUnderlyingTransport();
             Socket socket = tsocket.getSocket();
@@ -128,7 +127,7 @@ public boolean process(final TProtocol inProt, final TProtocol outProt) throws T
             Subject remoteUser = new Subject();
             remoteUser.getPrincipals().add(new User(authId));
             req_context.setSubject(remoteUser);
-            
+
             //invoke service handler
             return wrapped.process(inProt, outProt);
         }

File: storm-core/src/jvm/backtype/storm/security/auth/TBackoffConnect.java
Patch:
@@ -40,12 +40,12 @@ public TBackoffConnect(int retryTimes, int retryInterval, int retryIntervalCeili
                                                               retryTimes);
     }
 
-    public TTransport doConnectWithRetry(ITransportPlugin transportPlugin, TTransport underlyingTransport, String host) throws IOException {
+    public TTransport doConnectWithRetry(ITransportPlugin transportPlugin, TTransport underlyingTransport, String host, String asUser) throws IOException {
         boolean connected = false;
         TTransport transportResult = null;
         while(!connected) {
             try {
-                transportResult = transportPlugin.connect(underlyingTransport, host);
+                transportResult = transportPlugin.connect(underlyingTransport, host, asUser);
                 connected = true;
             } catch (TTransportException ex) {
                 retryNext(ex);

File: storm-core/src/jvm/backtype/storm/security/auth/digest/DigestSaslTransportPlugin.java
Patch:
@@ -51,10 +51,10 @@ protected TTransportFactory getServerTransportFactory() throws IOException {
     }
 
     @Override
-    public TTransport connect(TTransport transport, String serverHost) throws TTransportException, IOException {
+    public TTransport connect(TTransport transport, String serverHost, String asUser) throws TTransportException, IOException {
         ClientCallbackHandler client_callback_handler = new ClientCallbackHandler(login_conf);
-        TSaslClientTransport wrapper_transport = new TSaslClientTransport(DIGEST, 
-                null, 
+        TSaslClientTransport wrapper_transport = new TSaslClientTransport(DIGEST,
+                null,
                 AuthUtils.SERVICE, 
                 serverHost,
                 null,

File: storm-core/src/jvm/backtype/storm/serialization/SerializationDelegate.java
Patch:
@@ -31,5 +31,5 @@ public interface SerializationDelegate {
 
     byte[] serialize(Object object);
 
-    Object deserialize(byte[] bytes);
+    <T> T deserialize(byte[] bytes, Class<T> clazz);
 }

File: storm-core/src/jvm/backtype/storm/topology/TopologyBuilder.java
Patch:
@@ -104,12 +104,12 @@ public StormTopology createTopology() {
         for(String boltId: _bolts.keySet()) {
             IRichBolt bolt = _bolts.get(boltId);
             ComponentCommon common = getComponentCommon(boltId, bolt);
-            boltSpecs.put(boltId, new Bolt(ComponentObject.serialized_java(Utils.serialize(bolt)), common));
+            boltSpecs.put(boltId, new Bolt(ComponentObject.serialized_java(Utils.javaSerialize(bolt)), common));
         }
         for(String spoutId: _spouts.keySet()) {
             IRichSpout spout = _spouts.get(spoutId);
             ComponentCommon common = getComponentCommon(spoutId, spout);
-            spoutSpecs.put(spoutId, new SpoutSpec(ComponentObject.serialized_java(Utils.serialize(spout)), common));
+            spoutSpecs.put(spoutId, new SpoutSpec(ComponentObject.serialized_java(Utils.javaSerialize(spout)), common));
             
         }
         return new StormTopology(spoutSpecs,
@@ -348,7 +348,7 @@ public BoltDeclarer customGrouping(String componentId, CustomStreamGrouping grou
 
         @Override
         public BoltDeclarer customGrouping(String componentId, String streamId, CustomStreamGrouping grouping) {
-            return grouping(componentId, streamId, Grouping.custom_serialized(Utils.serialize(grouping)));
+            return grouping(componentId, streamId, Grouping.custom_serialized(Utils.javaSerialize(grouping)));
         }
 
         @Override

File: storm-core/src/jvm/backtype/storm/utils/DRPCClient.java
Patch:
@@ -41,7 +41,7 @@ public DRPCClient(Map conf, String host, int port) throws TTransportException {
     }
 
     public DRPCClient(Map conf, String host, int port, Integer timeout) throws TTransportException {
-        super(conf, ThriftConnectionType.DRPC, host, port, timeout);
+        super(conf, ThriftConnectionType.DRPC, host, port, timeout, null);
         this.host = host;
         this.port = port;
         this.client = new DistributedRPC.Client(_protocol);

File: storm-core/src/jvm/backtype/storm/utils/LocalState.java
Patch:
@@ -64,7 +64,7 @@ private Map<Object, Object> deserializeLatestVersion() throws IOException {
             if (serialized.length == 0) {
                 LOG.warn("LocalState file '{}' contained no data, resetting state", latestPath);
             } else {
-                result = (Map<Object, Object>) Utils.deserialize(serialized);
+                result = Utils.javaDeserialize(serialized, Map.class);
             }
         }
         return result;
@@ -99,7 +99,7 @@ public synchronized void cleanup(int keepVersions) throws IOException {
     }
     
     private void persist(Map<Object, Object> val, boolean cleanup) throws IOException {
-        byte[] toWrite = Utils.serialize(val);
+        byte[] toWrite = Utils.javaSerialize(val);
         String newPath = _vs.createVersion();
         File file = new File(newPath);
         FileUtils.writeByteArrayToFile(file, toWrite);

File: storm-core/src/jvm/storm/trident/Stream.java
Patch:
@@ -94,7 +94,7 @@ public Stream partitionBy(Fields fields) {
     }
     
     public Stream partition(CustomStreamGrouping partitioner) {
-        return partition(Grouping.custom_serialized(Utils.serialize(partitioner)));
+        return partition(Grouping.custom_serialized(Utils.javaSerialize(partitioner)));
     }
     
     public Stream shuffle() {

File: storm-core/test/jvm/backtype/storm/serialization/GzipBridgeSerializationDelegateTest.java
Patch:
@@ -41,7 +41,7 @@ public void testDeserialize_readingFromGzip() throws Exception {
 
         byte[] serialized = new GzipSerializationDelegate().serialize(pojo);
 
-        TestPojo pojo2 = (TestPojo)testDelegate.deserialize(serialized);
+        TestPojo pojo2 = (TestPojo)testDelegate.deserialize(serialized, TestPojo.class);
 
         assertEquals(pojo2.name, pojo.name);
         assertEquals(pojo2.age, pojo.age);
@@ -55,7 +55,7 @@ public void testDeserialize_readingFromGzipBridge() throws Exception {
 
         byte[] serialized = new GzipBridgeSerializationDelegate().serialize(pojo);
 
-        TestPojo pojo2 = (TestPojo)testDelegate.deserialize(serialized);
+        TestPojo pojo2 = (TestPojo)testDelegate.deserialize(serialized, TestPojo.class);
 
         assertEquals(pojo2.name, pojo.name);
         assertEquals(pojo2.age, pojo.age);
@@ -69,7 +69,7 @@ public void testDeserialize_readingFromDefault() throws Exception {
 
         byte[] serialized = new DefaultSerializationDelegate().serialize(pojo);
 
-        TestPojo pojo2 = (TestPojo)testDelegate.deserialize(serialized);
+        TestPojo pojo2 = (TestPojo)testDelegate.deserialize(serialized, TestPojo.class);
 
         assertEquals(pojo2.name, pojo.name);
         assertEquals(pojo2.age, pojo.age);

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/bolt/HBaseBolt.java
Patch:
@@ -62,7 +62,7 @@ public void execute(Tuple tuple) {
         try {
             this.hBaseClient.batchMutate(mutations);
         } catch(Exception e){
-            LOG.warn("Failing tuple. Error writing rowKey " + rowKey, e);
+            this.collector.reportError(e);
             this.collector.fail(tuple);
             return;
         }

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/bolt/HBaseLookupBolt.java
Patch:
@@ -67,11 +67,11 @@ public void execute(Tuple tuple) {
         try {
             Result result = hBaseClient.batchGet(Lists.newArrayList(get))[0];
             for(Values values : rowToTupleMapper.toValues(tuple, result)) {
-                this.collector.emit(values);
+                this.collector.emit(tuple, values);
             }
             this.collector.ack(tuple);
         } catch (Exception e) {
-            LOG.warn("Could not perform Lookup for rowKey =" + rowKey + " from Hbase.", e);
+            this.collector.reportError(e);
             this.collector.fail(tuple);
         }
     }

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/trident/state/HBaseState.java
Patch:
@@ -141,7 +141,7 @@ public void updateState(List<TridentTuple> tuples, TridentCollector collector) {
         try {
             hBaseClient.batchMutate(mutations);
         } catch (Exception e) {
-            LOG.warn("Batch write failed but some requests might have succeeded. Triggering replay.", e);
+            collector.reportError(e);
             throw new FailedException(e);
         }
     }

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/bolt/HdfsBolt.java
Patch:
@@ -112,7 +112,7 @@ public void execute(Tuple tuple) {
                 this.rotationPolicy.reset();
             }
         } catch (IOException e) {
-            LOG.warn("write/sync failed.", e);
+            this.collector.reportError(e);
             this.collector.fail(tuple);
         }
     }

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/bolt/SequenceFileBolt.java
Patch:
@@ -123,7 +123,7 @@ public void execute(Tuple tuple) {
                 this.rotationPolicy.reset();
             }
         } catch (IOException e) {
-            LOG.warn("write/sync failed.", e);
+            this.collector.reportError(e);
             this.collector.fail(tuple);
         }
 

File: storm-core/src/jvm/backtype/storm/coordination/BatchBoltExecutor.java
Patch:
@@ -103,6 +103,6 @@ private IBatchBolt getBatchBolt(Object id) {
     }
     
     private IBatchBolt newTransactionalBolt() {
-        return Utils.deserialize(_boltSer, IBatchBolt.class);
+        return Utils.javaDeserialize(_boltSer, IBatchBolt.class);
     }
 }

File: storm-core/src/jvm/backtype/storm/utils/ExtendedThreadPoolExecutor.java
Patch:
@@ -56,7 +56,7 @@ protected void afterExecute(Runnable r, Throwable t) {
       } catch (ExecutionException ee) {
         t = ee.getCause();
       } catch (InterruptedException ie) {
-        // If future got interrupted exception, we want to main parent thread itself.
+        // If future got interrupted exception, we want to interrupt parent thread itself.
         Thread.currentThread().interrupt();
       }
     }

File: storm-core/src/jvm/backtype/storm/security/auth/SaslTransportPlugin.java
Patch:
@@ -84,12 +84,11 @@ public TServer getServer(TProcessor processor) throws IOException, TTransportExc
         }
         BlockingQueue workQueue = new SynchronousQueue();
         if (queueSize != null) {
-          workQueue = new ArrayBlockingQueue(queueSize);
+            workQueue = new ArrayBlockingQueue(queueSize);
         }
         ThreadPoolExecutor executorService = new ExtendedThreadPoolExecutor(numWorkerThreads, numWorkerThreads,
-        60, TimeUnit.SECONDS, workQueue);
+            60, TimeUnit.SECONDS, workQueue);
         server_args.executorService(executorService);
-
         return new TThreadPoolServer(server_args);
     }
 

File: storm-core/src/jvm/backtype/storm/utils/Utils.java
Patch:
@@ -543,5 +543,4 @@ public static void handleUncaughtException(Throwable t) {
       }
       Runtime.getRuntime().halt(-1);
     }
-  }
 }

File: src/main/java/org/apache/storm/flux/model/GroupingDef.java
Patch:
@@ -41,7 +41,7 @@ public static enum Type {
     private Type type;
     private String streamId;
     private List<String> args;
-    private String customClass;
+    private ObjectDef customClass;
 
     public List<String> getArgs() {
         return args;
@@ -67,11 +67,11 @@ public void setStreamId(String streamId) {
         this.streamId = streamId;
     }
 
-    public String getCustomClass() {
+    public ObjectDef getCustomClass() {
         return customClass;
     }
 
-    public void setCustomClass(String customClass) {
+    public void setCustomClass(ObjectDef customClass) {
         this.customClass = customClass;
     }
 }

File: storm-core/src/jvm/backtype/storm/coordination/BatchBoltExecutor.java
Patch:
@@ -41,7 +41,7 @@ public class BatchBoltExecutor implements IRichBolt, FinishedCallback, TimeoutCa
     BatchOutputCollectorImpl _collector;
     
     public BatchBoltExecutor(IBatchBolt bolt) {
-        _boltSer = Utils.serialize(bolt);
+        _boltSer = Utils.javaSerialize(bolt);
     }
     
     @Override

File: storm-core/src/jvm/backtype/storm/serialization/DefaultSerializationDelegate.java
Patch:
@@ -20,6 +20,7 @@
 import java.io.*;
 import java.util.Map;
 
+@Deprecated
 public class DefaultSerializationDelegate implements SerializationDelegate {
 
     @Override

File: storm-core/src/jvm/backtype/storm/serialization/GzipBridgeSerializationDelegate.java
Patch:
@@ -25,6 +25,7 @@
  * {@link backtype.storm.serialization.DefaultSerializationDelegate} to deserialize. Any logic needing to be enabled
  * via {@link #prepare(java.util.Map)} is passed through to both delegates.
  */
+@Deprecated
 public class GzipBridgeSerializationDelegate implements SerializationDelegate {
 
     private DefaultSerializationDelegate defaultDelegate = new DefaultSerializationDelegate();

File: storm-core/src/jvm/backtype/storm/topology/TopologyBuilder.java
Patch:
@@ -348,7 +348,7 @@ public BoltDeclarer customGrouping(String componentId, CustomStreamGrouping grou
 
         @Override
         public BoltDeclarer customGrouping(String componentId, String streamId, CustomStreamGrouping grouping) {
-            return grouping(componentId, streamId, Grouping.custom_serialized(Utils.serialize(grouping)));
+            return grouping(componentId, streamId, Grouping.custom_serialized(Utils.javaSerialize(grouping)));
         }
 
         @Override

File: storm-core/src/jvm/backtype/storm/generated/TopologyStatus.java
Patch:
@@ -16,9 +16,10 @@
  * limitations under the License.
  */
 /**
- * Autogenerated by Thrift Compiler (0.7.0)
+ * Autogenerated by Thrift Compiler (0.9.2)
  *
  * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
+ *  @generated
  */
 package backtype.storm.generated;
 

File: examples/storm-starter/src/jvm/storm/starter/BasicDRPCTopology.java
Patch:
@@ -67,8 +67,9 @@ public static void main(String[] args) throws Exception {
         System.out.println("Result for \"" + word + "\": " + drpc.execute("exclamation", word));
       }
 
-      cluster.shutdown();
+      Thread.sleep(10000);
       drpc.shutdown();
+      cluster.shutdown();
     }
     else {
       conf.setNumWorkers(3);

File: external/storm-hdfs/src/test/java/org/apache/storm/hdfs/bolt/SequenceFileTopology.java
Patch:
@@ -63,7 +63,7 @@ public static void main(String[] args) throws Exception {
         FileRotationPolicy rotationPolicy = new FileSizeRotationPolicy(5.0f, Units.MB);
 
         FileNameFormat fileNameFormat = new DefaultFileNameFormat()
-                .withPath("/source/")
+                .withPath("/tmp/source/")
                 .withExtension(".seq");
 
         // create sequence format instance.
@@ -77,7 +77,7 @@ public static void main(String[] args) throws Exception {
                 .withSyncPolicy(syncPolicy)
                 .withCompressionType(SequenceFile.CompressionType.RECORD)
                 .withCompressionCodec("deflate")
-                .addRotationAction(new MoveFileAction().toDestination("/dest/"));
+                .addRotationAction(new MoveFileAction().toDestination("/tmp/dest/"));
 
 
 

File: external/storm-hdfs/src/test/java/org/apache/storm/hdfs/trident/TridentFileTopology.java
Patch:
@@ -49,7 +49,7 @@ public static StormTopology buildTopology(String hdfsUrl){
         Fields hdfsFields = new Fields("sentence", "key");
 
         FileNameFormat fileNameFormat = new DefaultFileNameFormat()
-                .withPath("/trident")
+                .withPath("/tmp/trident")
                 .withPrefix("trident")
                 .withExtension(".txt");
 

File: external/storm-hdfs/src/test/java/org/apache/storm/hdfs/trident/TridentSequenceTopology.java
Patch:
@@ -49,7 +49,7 @@ public static StormTopology buildTopology(String hdfsUrl){
         Fields hdfsFields = new Fields("sentence", "key");
 
         FileNameFormat fileNameFormat = new DefaultFileNameFormat()
-                .withPath("/trident")
+                .withPath("/tmp/trident")
                 .withPrefix("trident")
                 .withExtension(".seq");
 
@@ -60,8 +60,8 @@ public static StormTopology buildTopology(String hdfsUrl){
                 .withSequenceFormat(new DefaultSequenceFormat("key", "sentence"))
                 .withRotationPolicy(rotationPolicy)
                 .withFsUrl(hdfsUrl)
-                .addRotationAction(new MoveFileAction().toDestination("/dest2/"));
-
+                .withConfigKey("hdfs.config")
+                .addRotationAction(new MoveFileAction().toDestination("/tmp/dest2/"));
         StateFactory factory = new HdfsStateFactory().withOptions(seqOpts);
 
         TridentState state = stream

File: external/storm-kafka/src/jvm/storm/kafka/PartitionManager.java
Patch:
@@ -101,9 +101,10 @@ public PartitionManager(DynamicPartitionConnections connections, String topology
 
         if (currentOffset - _committedTo > spoutConfig.maxOffsetBehind || _committedTo <= 0) {
             LOG.info("Last commit offset from zookeeper: " + _committedTo);
+            Long lastCommittedOffset = _committedTo;
             _committedTo = currentOffset;
-            LOG.info("Commit offset " + _committedTo + " is more than " +
-                    spoutConfig.maxOffsetBehind + " behind, resetting to startOffsetTime=" + spoutConfig.startOffsetTime);
+            LOG.info("Commit offset " + lastCommittedOffset + " is more than " +
+                    spoutConfig.maxOffsetBehind + " behind latest offset " + currentOffset + ", resetting to startOffsetTime=" + spoutConfig.startOffsetTime);
         }
 
         LOG.info("Starting Kafka " + _consumer.host() + ":" + id.partition + " from offset " + _committedTo);

File: external/storm-kafka/src/jvm/storm/kafka/KafkaUtils.java
Patch:
@@ -211,7 +211,7 @@ public static Iterable<List<Object>> generateTuples(KafkaConfig kafkaConfig, Mes
         return tups;
     }
     
-    public static Iterable<List<Object>> generateTuples(KafkaConfig kafkaConfig, Message msg, Partition partition, int offset) {
+    public static Iterable<List<Object>> generateTuples(KafkaConfig kafkaConfig, Message msg, Partition partition, long offset) {
         Iterable<List<Object>> tups;
         ByteBuffer payload = msg.payload();
         if (payload == null) {

File: external/storm-kafka/src/jvm/storm/kafka/MessageMetadataScheme.java
Patch:
@@ -21,5 +21,5 @@
  * limitations under the License.
  */
 public interface MessageMetadataScheme extends Scheme {
-    public List<Object> deserializeMessageWithMetadata(byte[] message, Partition partition, int offset);
+    public List<Object> deserializeMessageWithMetadata(byte[] message, Partition partition, long offset);
 }

File: external/storm-kafka/src/jvm/storm/kafka/MessageMetadataSchemeAsMultiScheme.java
Patch:
@@ -14,7 +14,7 @@ public MessageMetadataSchemeAsMultiScheme(Scheme scheme) {
     }
 
     @SuppressWarnings("unchecked")
-    public Iterable<List<Object>> deserializeMessageWithMetadata(byte[] message, Partition partition, int offset) {
+    public Iterable<List<Object>> deserializeMessageWithMetadata(byte[] message, Partition partition, long offset) {
         List<Object> o = ((MessageMetadataScheme) scheme).deserializeMessageWithMetadata(message, partition, offset);
         if (o == null) {
             return null;

File: external/storm-kafka/src/jvm/storm/kafka/StringMessageAndMetadataScheme.java
Patch:
@@ -12,7 +12,7 @@ public class StringMessageAndMetadataScheme extends StringScheme implements Mess
     public static final String STRING_SCHEME_OFFSET = "offset";
 
     @Override
-    public List<Object> deserializeMessageWithMetadata(byte[] message, Partition partition, int offset) {
+    public List<Object> deserializeMessageWithMetadata(byte[] message, Partition partition, long offset) {
         String stringMessage = StringScheme.deserializeString(message);
         return new Values(stringMessage, partition.partition, offset);
     }

File: external/storm-kafka/src/test/storm/kafka/bolt/KafkaBoltTest.java
Patch:
@@ -143,7 +143,7 @@ public void executeWithBrokerDown() throws Exception {
         String message = "value-234";
         Tuple tuple = generateTestTuple(message);
         bolt.execute(tuple);
-        verify(collector).ack(tuple);
+        verify(collector).fail(tuple);
     }
 
 

File: examples/storm-starter/src/jvm/storm/starter/BasicDRPCTopology.java
Patch:
@@ -67,8 +67,9 @@ public static void main(String[] args) throws Exception {
         System.out.println("Result for \"" + word + "\": " + drpc.execute("exclamation", word));
       }
 
-      cluster.shutdown();
+      Thread.sleep(10000);
       drpc.shutdown();
+      cluster.shutdown();
     }
     else {
       conf.setNumWorkers(3);

File: external/storm-hdfs/src/test/java/org/apache/storm/hdfs/bolt/SequenceFileTopology.java
Patch:
@@ -63,7 +63,7 @@ public static void main(String[] args) throws Exception {
         FileRotationPolicy rotationPolicy = new FileSizeRotationPolicy(5.0f, Units.MB);
 
         FileNameFormat fileNameFormat = new DefaultFileNameFormat()
-                .withPath("/source/")
+                .withPath("/tmp/source/")
                 .withExtension(".seq");
 
         // create sequence format instance.
@@ -77,7 +77,7 @@ public static void main(String[] args) throws Exception {
                 .withSyncPolicy(syncPolicy)
                 .withCompressionType(SequenceFile.CompressionType.RECORD)
                 .withCompressionCodec("deflate")
-                .addRotationAction(new MoveFileAction().toDestination("/dest/"));
+                .addRotationAction(new MoveFileAction().toDestination("/tmp/dest/"));
 
 
 

File: external/storm-hdfs/src/test/java/org/apache/storm/hdfs/trident/TridentFileTopology.java
Patch:
@@ -49,7 +49,7 @@ public static StormTopology buildTopology(String hdfsUrl){
         Fields hdfsFields = new Fields("sentence", "key");
 
         FileNameFormat fileNameFormat = new DefaultFileNameFormat()
-                .withPath("/trident")
+                .withPath("/tmp/trident")
                 .withPrefix("trident")
                 .withExtension(".txt");
 

File: external/storm-hdfs/src/test/java/org/apache/storm/hdfs/trident/TridentSequenceTopology.java
Patch:
@@ -49,7 +49,7 @@ public static StormTopology buildTopology(String hdfsUrl){
         Fields hdfsFields = new Fields("sentence", "key");
 
         FileNameFormat fileNameFormat = new DefaultFileNameFormat()
-                .withPath("/trident")
+                .withPath("/tmp/trident")
                 .withPrefix("trident")
                 .withExtension(".seq");
 
@@ -60,8 +60,8 @@ public static StormTopology buildTopology(String hdfsUrl){
                 .withSequenceFormat(new DefaultSequenceFormat("key", "sentence"))
                 .withRotationPolicy(rotationPolicy)
                 .withFsUrl(hdfsUrl)
-                .addRotationAction(new MoveFileAction().toDestination("/dest2/"));
-
+                .withConfigKey("hdfs.config")
+                .addRotationAction(new MoveFileAction().toDestination("/tmp/dest2/"));
         StateFactory factory = new HdfsStateFactory().withOptions(seqOpts);
 
         TridentState state = stream

File: external/storm-kafka/src/jvm/storm/kafka/PartitionManager.java
Patch:
@@ -101,9 +101,10 @@ public PartitionManager(DynamicPartitionConnections connections, String topology
 
         if (currentOffset - _committedTo > spoutConfig.maxOffsetBehind || _committedTo <= 0) {
             LOG.info("Last commit offset from zookeeper: " + _committedTo);
+            Long lastCommittedOffset = _committedTo;
             _committedTo = currentOffset;
-            LOG.info("Commit offset " + _committedTo + " is more than " +
-                    spoutConfig.maxOffsetBehind + " behind, resetting to startOffsetTime=" + spoutConfig.startOffsetTime);
+            LOG.info("Commit offset " + lastCommittedOffset + " is more than " +
+                    spoutConfig.maxOffsetBehind + " behind latest offset " + currentOffset + ", resetting to startOffsetTime=" + spoutConfig.startOffsetTime);
         }
 
         LOG.info("Starting Kafka " + _consumer.host() + ":" + id.partition + " from offset " + _committedTo);

File: external/storm-redis/src/main/java/org/apache/storm/redis/common/config/JedisClusterConfig.java
Patch:
@@ -15,7 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.storm.redis.util.config;
+package org.apache.storm.redis.common.config;
 
 import com.google.common.base.Preconditions;
 import redis.clients.jedis.HostAndPort;

File: external/storm-redis/src/main/java/org/apache/storm/redis/common/config/JedisPoolConfig.java
Patch:
@@ -15,7 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.storm.redis.util.config;
+package org.apache.storm.redis.common.config;
 
 import redis.clients.jedis.Protocol;
 

File: external/storm-redis/src/main/java/org/apache/storm/redis/common/container/JedisClusterContainer.java
Patch:
@@ -15,7 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.storm.redis.util.container;
+package org.apache.storm.redis.common.container;
 
 import redis.clients.jedis.JedisCluster;
 import redis.clients.jedis.JedisCommands;

File: external/storm-redis/src/main/java/org/apache/storm/redis/common/container/JedisCommandsContainerBuilder.java
Patch:
@@ -15,10 +15,10 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.storm.redis.util.container;
+package org.apache.storm.redis.common.container;
 
-import org.apache.storm.redis.util.config.JedisClusterConfig;
-import org.apache.storm.redis.util.config.JedisPoolConfig;
+import org.apache.storm.redis.common.config.JedisClusterConfig;
+import org.apache.storm.redis.common.config.JedisPoolConfig;
 import redis.clients.jedis.JedisCluster;
 import redis.clients.jedis.JedisPool;
 

File: external/storm-redis/src/main/java/org/apache/storm/redis/common/container/JedisCommandsInstanceContainer.java
Patch:
@@ -15,7 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.storm.redis.util.container;
+package org.apache.storm.redis.common.container;
 
 import redis.clients.jedis.JedisCommands;
 

File: external/storm-redis/src/main/java/org/apache/storm/redis/common/container/JedisContainer.java
Patch:
@@ -15,7 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.storm.redis.util.container;
+package org.apache.storm.redis.common.container;
 
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;

File: external/storm-redis/src/main/java/org/apache/storm/redis/trident/state/RedisClusterMapState.java
Patch:
@@ -23,7 +23,7 @@
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.Lists;
 import com.google.common.collect.Maps;
-import org.apache.storm.redis.util.config.JedisClusterConfig;
+import org.apache.storm.redis.common.config.JedisClusterConfig;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import redis.clients.jedis.JedisCluster;

File: external/storm-redis/src/main/java/org/apache/storm/redis/trident/state/RedisClusterState.java
Patch:
@@ -18,7 +18,7 @@
 package org.apache.storm.redis.trident.state;
 
 import backtype.storm.task.IMetricsContext;
-import org.apache.storm.redis.util.config.JedisClusterConfig;
+import org.apache.storm.redis.common.config.JedisClusterConfig;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import redis.clients.jedis.JedisCluster;

File: external/storm-redis/src/main/java/org/apache/storm/redis/trident/state/RedisMapState.java
Patch:
@@ -22,7 +22,7 @@
 import com.google.common.base.Strings;
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.Maps;
-import org.apache.storm.redis.util.config.JedisPoolConfig;
+import org.apache.storm.redis.common.config.JedisPoolConfig;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import redis.clients.jedis.Jedis;

File: external/storm-redis/src/main/java/org/apache/storm/redis/trident/state/RedisState.java
Patch:
@@ -18,7 +18,7 @@
 package org.apache.storm.redis.trident.state;
 
 import backtype.storm.task.IMetricsContext;
-import org.apache.storm.redis.util.config.JedisPoolConfig;
+import org.apache.storm.redis.common.config.JedisPoolConfig;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import redis.clients.jedis.Jedis;

File: external/storm-redis/src/test/java/org/apache/storm/redis/trident/WordCountTridentRedisCluster.java
Patch:
@@ -23,11 +23,11 @@
 import backtype.storm.generated.StormTopology;
 import backtype.storm.tuple.Fields;
 import backtype.storm.tuple.Values;
-import org.apache.storm.redis.trident.mapper.TridentTupleMapper;
+import org.apache.storm.redis.common.mapper.TupleMapper;
 import org.apache.storm.redis.trident.state.RedisClusterState;
 import org.apache.storm.redis.trident.state.RedisClusterStateQuerier;
 import org.apache.storm.redis.trident.state.RedisClusterStateUpdater;
-import org.apache.storm.redis.util.config.JedisClusterConfig;
+import org.apache.storm.redis.common.config.JedisClusterConfig;
 import storm.trident.Stream;
 import storm.trident.TridentState;
 import storm.trident.TridentTopology;
@@ -55,7 +55,7 @@ public static StormTopology buildTopology(String redisHostPort){
         }
         JedisClusterConfig clusterConfig = new JedisClusterConfig.Builder().setNodes(nodes)
                                         .build();
-        TridentTupleMapper tupleMapper = new WordCountTupleMapper();
+        TupleMapper tupleMapper = new WordCountTupleMapper();
         RedisClusterState.Factory factory = new RedisClusterState.Factory(clusterConfig);
 
         TridentTopology topology = new TridentTopology();

File: storm-core/src/jvm/backtype/storm/messaging/netty/SaslStormClientHandler.java
Patch:
@@ -146,9 +146,8 @@ public void messageReceived(ChannelHandlerContext ctx, MessageEvent event)
     }
 
     private void getSASLCredentials() throws IOException {
-        topologyName = (String) this.client.storm_conf
-                .get(Config.TOPOLOGY_NAME);
-        String secretKey = SaslUtils.getSecretKey(this.client.storm_conf);
+        topologyName = (String) this.client.stormConf.get(Config.TOPOLOGY_NAME);
+        String secretKey = SaslUtils.getSecretKey(this.client.stormConf);
         if (secretKey != null) {
             token = secretKey.getBytes();
         }

File: storm-core/src/jvm/backtype/storm/messaging/netty/StormClientPipelineFactory.java
Patch:
@@ -39,15 +39,14 @@ public ChannelPipeline getPipeline() throws Exception {
         // Encoder
         pipeline.addLast("encoder", new MessageEncoder());
 
-        boolean isNettyAuth = (Boolean) this.client.storm_conf
-                .get(Config.STORM_MESSAGING_NETTY_AUTHENTICATION);
+        boolean isNettyAuth = (Boolean) this.client.stormConf.get(Config.STORM_MESSAGING_NETTY_AUTHENTICATION);
         if (isNettyAuth) {
             // Authenticate: Removed after authentication completes
             pipeline.addLast("saslClientHandler", new SaslStormClientHandler(
                     client));
         }
         // business logic.
-        pipeline.addLast("handler", new StormClientErrorHandler(client.name()));
+        pipeline.addLast("handler", new StormClientErrorHandler(client.dstAddressPrefixedName));
 
         return pipeline;
     }

File: external/storm-kafka/src/jvm/storm/kafka/PartitionManager.java
Patch:
@@ -101,9 +101,10 @@ public PartitionManager(DynamicPartitionConnections connections, String topology
 
         if (currentOffset - _committedTo > spoutConfig.maxOffsetBehind || _committedTo <= 0) {
             LOG.info("Last commit offset from zookeeper: " + _committedTo);
+            Long lastCommittedOffset = _committedTo;
             _committedTo = currentOffset;
-            LOG.info("Commit offset " + _committedTo + " is more than " +
-                    spoutConfig.maxOffsetBehind + " behind, resetting to startOffsetTime=" + spoutConfig.startOffsetTime);
+            LOG.info("Commit offset " + lastCommittedOffset + " is more than " +
+                    spoutConfig.maxOffsetBehind + " behind latest offset " + currentOffset + ", resetting to startOffsetTime=" + spoutConfig.startOffsetTime);
         }
 
         LOG.info("Starting Kafka " + _consumer.host() + ":" + id.partition + " from offset " + _committedTo);

File: storm-core/src/jvm/storm/trident/spout/RichSpoutBatchExecutor.java
Patch:
@@ -140,6 +140,7 @@ private void fail(long batchId) {
         
         @Override
         public void close() {
+            _spout.close();
         }
         
     }

File: storm-core/src/jvm/backtype/storm/messaging/netty/Client.java
Patch:
@@ -358,7 +358,7 @@ public void send(int taskId, byte[] payload) {
     public synchronized void send(Iterator<TaskMessage> msgs) {
         if (closing) {
             int numMessages = iteratorSize(msgs);
-            LOG.warn("discarding {} messages because the Netty client to {} is being closed", numMessages,
+            LOG.error("discarding {} messages because the Netty client to {} is being closed", numMessages,
                 dstAddressPrefixedName);
             return;
         }
@@ -475,7 +475,7 @@ public void operationComplete(ChannelFuture future) throws Exception {
                     messagesSent.getAndAdd(batch.size());
                 }
                 else {
-                    LOG.warn("failed to send {} messages to {}: {}", numMessages, dstAddressPrefixedName,
+                    LOG.error("failed to send {} messages to {}: {}", numMessages, dstAddressPrefixedName,
                         future.getCause());
                     closeChannelAndReconnect(future.getChannel());
                     messagesLost.getAndAdd(numMessages);

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/trident/state/HBaseMapState.java
Patch:
@@ -75,7 +75,7 @@ public HBaseMapState(final Options<T> options, Map map, int partitionNum) {
                 LOG.warn("No 'hbase.rootdir' value found in configuration! Using HBase defaults.");
             }
             for (String key : conf.keySet()) {
-                hbConfig.set(key, String.valueOf(map.get(key)));
+                hbConfig.set(key, String.valueOf(conf.get(key)));
             }
         }
 

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/trident/state/HBaseState.java
Patch:
@@ -107,7 +107,7 @@ protected void prepare() {
                 LOG.warn("No 'hbase.rootdir' value found in configuration! Using HBase defaults.");
             }
             for (String key : conf.keySet()) {
-                hbConfig.set(key, String.valueOf(map.get(key)));
+                hbConfig.set(key, String.valueOf(conf.get(key)));
             }
         }
 

File: storm-core/src/jvm/backtype/storm/messaging/netty/Server.java
Patch:
@@ -89,6 +89,7 @@ class Server implements IConnection, IStatefulObject {
         
         // Configure the server.
         int buffer_size = Utils.getInt(storm_conf.get(Config.STORM_MESSAGING_NETTY_BUFFER_SIZE));
+        int backlog = Utils.getInt(storm_conf.get(Config.STORM_MESSAGING_NETTY_SOCKET_BACKLOG), 500);
         int maxWorkers = Utils.getInt(storm_conf.get(Config.STORM_MESSAGING_NETTY_SERVER_WORKER_THREADS));
 
         ThreadFactory bossFactory = new NettyRenameThreadFactory(name() + "-boss");
@@ -108,6 +109,7 @@ class Server implements IConnection, IStatefulObject {
         bootstrap.setOption("child.tcpNoDelay", true);
         bootstrap.setOption("child.receiveBufferSize", buffer_size);
         bootstrap.setOption("child.keepAlive", true);
+        bootstrap.setOption("backlog", backlog);
 
         // Set up the pipeline factory.
         bootstrap.setPipelineFactory(new StormServerPipelineFactory(this));

File: storm-core/src/jvm/backtype/storm/topology/InputDeclarer.java
Patch:
@@ -45,6 +45,9 @@ public interface InputDeclarer<T extends InputDeclarer> {
     public T directGrouping(String componentId);
     public T directGrouping(String componentId, String streamId);
 
+    public T partialKeyGrouping(String componentId, Fields fields);
+    public T partialKeyGrouping(String componentId, String streamId, Fields fields);
+
     public T customGrouping(String componentId, CustomStreamGrouping grouping);
     public T customGrouping(String componentId, String streamId, CustomStreamGrouping grouping);
     

File: external/storm-hive/src/main/java/org/apache/storm/hive/bolt/HiveBolt.java
Patch:
@@ -111,9 +111,9 @@ public void execute(Tuple tuple) {
             }
             collector.ack(tuple);
         } catch(Exception e) {
+            this.collector.reportError(e);
             collector.fail(tuple);
             flushAndCloseWriters();
-            LOG.warn("hive streaming failed. ",e);
         }
     }
 

File: storm-core/src/jvm/backtype/storm/messaging/netty/SaslStormClientHandler.java
Patch:
@@ -146,9 +146,8 @@ public void messageReceived(ChannelHandlerContext ctx, MessageEvent event)
     }
 
     private void getSASLCredentials() throws IOException {
-        topologyName = (String) this.client.storm_conf
-                .get(Config.TOPOLOGY_NAME);
-        String secretKey = SaslUtils.getSecretKey(this.client.storm_conf);
+        topologyName = (String) this.client.stormConf.get(Config.TOPOLOGY_NAME);
+        String secretKey = SaslUtils.getSecretKey(this.client.stormConf);
         if (secretKey != null) {
             token = secretKey.getBytes();
         }

File: storm-core/src/jvm/backtype/storm/messaging/netty/StormClientPipelineFactory.java
Patch:
@@ -39,15 +39,14 @@ public ChannelPipeline getPipeline() throws Exception {
         // Encoder
         pipeline.addLast("encoder", new MessageEncoder());
 
-        boolean isNettyAuth = (Boolean) this.client.storm_conf
-                .get(Config.STORM_MESSAGING_NETTY_AUTHENTICATION);
+        boolean isNettyAuth = (Boolean) this.client.stormConf.get(Config.STORM_MESSAGING_NETTY_AUTHENTICATION);
         if (isNettyAuth) {
             // Authenticate: Removed after authentication completes
             pipeline.addLast("saslClientHandler", new SaslStormClientHandler(
                     client));
         }
         // business logic.
-        pipeline.addLast("handler", new StormClientErrorHandler(client.name()));
+        pipeline.addLast("handler", new StormClientErrorHandler(client.dstAddressPrefixedName));
 
         return pipeline;
     }

File: storm-core/src/jvm/backtype/storm/topology/InputDeclarer.java
Patch:
@@ -45,6 +45,9 @@ public interface InputDeclarer<T extends InputDeclarer> {
     public T directGrouping(String componentId);
     public T directGrouping(String componentId, String streamId);
 
+    public T partialKeyGrouping(String componentId, Fields fields);
+    public T partialKeyGrouping(String componentId, String streamId, Fields fields);
+
     public T customGrouping(String componentId, CustomStreamGrouping grouping);
     public T customGrouping(String componentId, String streamId, CustomStreamGrouping grouping);
     

File: storm-core/src/jvm/backtype/storm/generated/AlreadyAliveException.java
Patch:
@@ -51,7 +51,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-2")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-6")
 public class AlreadyAliveException extends TException implements org.apache.thrift.TBase<AlreadyAliveException, AlreadyAliveException._Fields>, java.io.Serializable, Cloneable, Comparable<AlreadyAliveException> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("AlreadyAliveException");
 

File: storm-core/src/jvm/backtype/storm/generated/AuthorizationException.java
Patch:
@@ -51,7 +51,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-2")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-6")
 public class AuthorizationException extends TException implements org.apache.thrift.TBase<AuthorizationException, AuthorizationException._Fields>, java.io.Serializable, Cloneable, Comparable<AuthorizationException> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("AuthorizationException");
 

File: storm-core/src/jvm/backtype/storm/generated/Bolt.java
Patch:
@@ -51,7 +51,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-2")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-6")
 public class Bolt implements org.apache.thrift.TBase<Bolt, Bolt._Fields>, java.io.Serializable, Cloneable, Comparable<Bolt> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("Bolt");
 

File: storm-core/src/jvm/backtype/storm/generated/BoltStats.java
Patch:
@@ -51,7 +51,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-2")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-6")
 public class BoltStats implements org.apache.thrift.TBase<BoltStats, BoltStats._Fields>, java.io.Serializable, Cloneable, Comparable<BoltStats> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("BoltStats");
 

File: storm-core/src/jvm/backtype/storm/generated/ClusterSummary.java
Patch:
@@ -51,7 +51,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-2")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-6")
 public class ClusterSummary implements org.apache.thrift.TBase<ClusterSummary, ClusterSummary._Fields>, java.io.Serializable, Cloneable, Comparable<ClusterSummary> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("ClusterSummary");
 

File: storm-core/src/jvm/backtype/storm/generated/ComponentCommon.java
Patch:
@@ -51,7 +51,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-2")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-6")
 public class ComponentCommon implements org.apache.thrift.TBase<ComponentCommon, ComponentCommon._Fields>, java.io.Serializable, Cloneable, Comparable<ComponentCommon> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("ComponentCommon");
 

File: storm-core/src/jvm/backtype/storm/generated/Credentials.java
Patch:
@@ -51,7 +51,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-2")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-6")
 public class Credentials implements org.apache.thrift.TBase<Credentials, Credentials._Fields>, java.io.Serializable, Cloneable, Comparable<Credentials> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("Credentials");
 

File: storm-core/src/jvm/backtype/storm/generated/DRPCExecutionException.java
Patch:
@@ -51,7 +51,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-2")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-6")
 public class DRPCExecutionException extends TException implements org.apache.thrift.TBase<DRPCExecutionException, DRPCExecutionException._Fields>, java.io.Serializable, Cloneable, Comparable<DRPCExecutionException> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("DRPCExecutionException");
 

File: storm-core/src/jvm/backtype/storm/generated/DRPCRequest.java
Patch:
@@ -51,7 +51,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-2")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-6")
 public class DRPCRequest implements org.apache.thrift.TBase<DRPCRequest, DRPCRequest._Fields>, java.io.Serializable, Cloneable, Comparable<DRPCRequest> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("DRPCRequest");
 

File: storm-core/src/jvm/backtype/storm/generated/DistributedRPC.java
Patch:
@@ -51,7 +51,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-2")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-6")
 public class DistributedRPC {
 
   public interface Iface {

File: storm-core/src/jvm/backtype/storm/generated/DistributedRPCInvocations.java
Patch:
@@ -51,7 +51,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-2")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-6")
 public class DistributedRPCInvocations {
 
   public interface Iface {

File: storm-core/src/jvm/backtype/storm/generated/ErrorInfo.java
Patch:
@@ -51,7 +51,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-2")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-6")
 public class ErrorInfo implements org.apache.thrift.TBase<ErrorInfo, ErrorInfo._Fields>, java.io.Serializable, Cloneable, Comparable<ErrorInfo> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("ErrorInfo");
 

File: storm-core/src/jvm/backtype/storm/generated/ExecutorInfo.java
Patch:
@@ -51,7 +51,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-2")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-6")
 public class ExecutorInfo implements org.apache.thrift.TBase<ExecutorInfo, ExecutorInfo._Fields>, java.io.Serializable, Cloneable, Comparable<ExecutorInfo> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("ExecutorInfo");
 

File: storm-core/src/jvm/backtype/storm/generated/ExecutorStats.java
Patch:
@@ -51,7 +51,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-2")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-6")
 public class ExecutorStats implements org.apache.thrift.TBase<ExecutorStats, ExecutorStats._Fields>, java.io.Serializable, Cloneable, Comparable<ExecutorStats> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("ExecutorStats");
 

File: storm-core/src/jvm/backtype/storm/generated/ExecutorSummary.java
Patch:
@@ -51,7 +51,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-2")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-6")
 public class ExecutorSummary implements org.apache.thrift.TBase<ExecutorSummary, ExecutorSummary._Fields>, java.io.Serializable, Cloneable, Comparable<ExecutorSummary> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("ExecutorSummary");
 

File: storm-core/src/jvm/backtype/storm/generated/GlobalStreamId.java
Patch:
@@ -51,7 +51,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-2")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-6")
 public class GlobalStreamId implements org.apache.thrift.TBase<GlobalStreamId, GlobalStreamId._Fields>, java.io.Serializable, Cloneable, Comparable<GlobalStreamId> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("GlobalStreamId");
 

File: storm-core/src/jvm/backtype/storm/generated/InvalidTopologyException.java
Patch:
@@ -51,7 +51,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-2")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-6")
 public class InvalidTopologyException extends TException implements org.apache.thrift.TBase<InvalidTopologyException, InvalidTopologyException._Fields>, java.io.Serializable, Cloneable, Comparable<InvalidTopologyException> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("InvalidTopologyException");
 

File: storm-core/src/jvm/backtype/storm/generated/JavaObject.java
Patch:
@@ -51,7 +51,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-2")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-6")
 public class JavaObject implements org.apache.thrift.TBase<JavaObject, JavaObject._Fields>, java.io.Serializable, Cloneable, Comparable<JavaObject> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("JavaObject");
 

File: storm-core/src/jvm/backtype/storm/generated/KillOptions.java
Patch:
@@ -51,7 +51,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-2")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-6")
 public class KillOptions implements org.apache.thrift.TBase<KillOptions, KillOptions._Fields>, java.io.Serializable, Cloneable, Comparable<KillOptions> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("KillOptions");
 

File: storm-core/src/jvm/backtype/storm/generated/NotAliveException.java
Patch:
@@ -51,7 +51,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-2")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-6")
 public class NotAliveException extends TException implements org.apache.thrift.TBase<NotAliveException, NotAliveException._Fields>, java.io.Serializable, Cloneable, Comparable<NotAliveException> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("NotAliveException");
 

File: storm-core/src/jvm/backtype/storm/generated/NullStruct.java
Patch:
@@ -51,7 +51,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-2")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-6")
 public class NullStruct implements org.apache.thrift.TBase<NullStruct, NullStruct._Fields>, java.io.Serializable, Cloneable, Comparable<NullStruct> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("NullStruct");
 

File: storm-core/src/jvm/backtype/storm/generated/RebalanceOptions.java
Patch:
@@ -51,7 +51,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-2")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-6")
 public class RebalanceOptions implements org.apache.thrift.TBase<RebalanceOptions, RebalanceOptions._Fields>, java.io.Serializable, Cloneable, Comparable<RebalanceOptions> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("RebalanceOptions");
 

File: storm-core/src/jvm/backtype/storm/generated/ShellComponent.java
Patch:
@@ -51,7 +51,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-2")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-6")
 public class ShellComponent implements org.apache.thrift.TBase<ShellComponent, ShellComponent._Fields>, java.io.Serializable, Cloneable, Comparable<ShellComponent> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("ShellComponent");
 

File: storm-core/src/jvm/backtype/storm/generated/SpoutSpec.java
Patch:
@@ -51,7 +51,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-2")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-6")
 public class SpoutSpec implements org.apache.thrift.TBase<SpoutSpec, SpoutSpec._Fields>, java.io.Serializable, Cloneable, Comparable<SpoutSpec> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("SpoutSpec");
 

File: storm-core/src/jvm/backtype/storm/generated/SpoutStats.java
Patch:
@@ -51,7 +51,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-2")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-6")
 public class SpoutStats implements org.apache.thrift.TBase<SpoutStats, SpoutStats._Fields>, java.io.Serializable, Cloneable, Comparable<SpoutStats> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("SpoutStats");
 

File: storm-core/src/jvm/backtype/storm/generated/StateSpoutSpec.java
Patch:
@@ -51,7 +51,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-2")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-6")
 public class StateSpoutSpec implements org.apache.thrift.TBase<StateSpoutSpec, StateSpoutSpec._Fields>, java.io.Serializable, Cloneable, Comparable<StateSpoutSpec> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("StateSpoutSpec");
 

File: storm-core/src/jvm/backtype/storm/generated/StormTopology.java
Patch:
@@ -51,7 +51,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-2")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-6")
 public class StormTopology implements org.apache.thrift.TBase<StormTopology, StormTopology._Fields>, java.io.Serializable, Cloneable, Comparable<StormTopology> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("StormTopology");
 

File: storm-core/src/jvm/backtype/storm/generated/StreamInfo.java
Patch:
@@ -51,7 +51,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-2")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-6")
 public class StreamInfo implements org.apache.thrift.TBase<StreamInfo, StreamInfo._Fields>, java.io.Serializable, Cloneable, Comparable<StreamInfo> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("StreamInfo");
 

File: storm-core/src/jvm/backtype/storm/generated/SubmitOptions.java
Patch:
@@ -51,7 +51,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-2")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-6")
 public class SubmitOptions implements org.apache.thrift.TBase<SubmitOptions, SubmitOptions._Fields>, java.io.Serializable, Cloneable, Comparable<SubmitOptions> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("SubmitOptions");
 

File: storm-core/src/jvm/backtype/storm/generated/SupervisorSummary.java
Patch:
@@ -51,7 +51,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-2")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-6")
 public class SupervisorSummary implements org.apache.thrift.TBase<SupervisorSummary, SupervisorSummary._Fields>, java.io.Serializable, Cloneable, Comparable<SupervisorSummary> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("SupervisorSummary");
 

File: storm-core/src/jvm/backtype/storm/generated/TopologyInfo.java
Patch:
@@ -51,7 +51,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-2")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-6")
 public class TopologyInfo implements org.apache.thrift.TBase<TopologyInfo, TopologyInfo._Fields>, java.io.Serializable, Cloneable, Comparable<TopologyInfo> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TopologyInfo");
 

File: storm-core/src/jvm/backtype/storm/generated/TopologySummary.java
Patch:
@@ -51,7 +51,7 @@
 import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
-@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-2")
+@Generated(value = "Autogenerated by Thrift Compiler (0.9.2)", date = "2015-2-6")
 public class TopologySummary implements org.apache.thrift.TBase<TopologySummary, TopologySummary._Fields>, java.io.Serializable, Cloneable, Comparable<TopologySummary> {
   private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TopologySummary");
 

File: examples/storm-starter/src/jvm/storm/starter/PrintSampleStream.java
Patch:
@@ -39,10 +39,10 @@ public static void main(String[] args) {
         
         TopologyBuilder builder = new TopologyBuilder();
         
-        builder.setSpout("spoutId", new TwitterSampleSpout(consumerKey, consumerSecret,
+        builder.setSpout("twitter", new TwitterSampleSpout(consumerKey, consumerSecret,
                                 accessToken, accessTokenSecret, keyWords));
         builder.setBolt("print", new PrinterBolt())
-                .shuffleGrouping("spout");
+                .shuffleGrouping("twitter");
                 
                 
         Config conf = new Config();

File: examples/storm-starter/src/jvm/storm/starter/util/StormRunner.java
Patch:
@@ -21,6 +21,7 @@
 import backtype.storm.LocalCluster;
 import backtype.storm.StormSubmitter;
 import backtype.storm.generated.AlreadyAliveException;
+import backtype.storm.generated.AuthorizationException;
 import backtype.storm.generated.InvalidTopologyException;
 import backtype.storm.generated.StormTopology;
 
@@ -41,7 +42,7 @@ public static void runTopologyLocally(StormTopology topology, String topologyNam
   }
 
   public static void runTopologyRemotely(StormTopology topology, String topologyName, Config conf)
-      throws AlreadyAliveException, InvalidTopologyException {
+      throws AlreadyAliveException, InvalidTopologyException, AuthorizationException {
     StormSubmitter.submitTopology(topologyName, conf, topology);
   }
 }

File: external/storm-kafka/src/jvm/storm/kafka/DynamicBrokersReader.java
Patch:
@@ -47,7 +47,7 @@ public DynamicBrokersReader(Map conf, String zkStr, String zkPath, String topic)
             _curator = CuratorFrameworkFactory.newClient(
                     zkStr,
                     Utils.getInt(conf.get(Config.STORM_ZOOKEEPER_SESSION_TIMEOUT)),
-                    15000,
+                    Utils.getInt(conf.get(Config.STORM_ZOOKEEPER_CONNECTION_TIMEOUT)),
                     new RetryNTimes(Utils.getInt(conf.get(Config.STORM_ZOOKEEPER_RETRY_TIMES)),
                             Utils.getInt(conf.get(Config.STORM_ZOOKEEPER_RETRY_INTERVAL))));
             _curator.start();

File: external/storm-kafka/src/jvm/storm/kafka/bolt/mapper/FieldNameBasedTupleToKafkaMapper.java
Patch:
@@ -19,7 +19,7 @@
 
 import backtype.storm.tuple.Tuple;
 
-public class FieldNameBasedTupleToKafkaMapper<K,V> implements TupleToKafkaMapper {
+public class FieldNameBasedTupleToKafkaMapper<K,V> implements TupleToKafkaMapper<K, V> {
 
     public static final String BOLT_KEY = "key";
     public static final String BOLT_MESSAGE = "message";

File: external/storm-kafka/src/test/storm/kafka/DynamicBrokersReaderTest.java
Patch:
@@ -50,6 +50,7 @@ public void setUp() throws Exception {
         String connectionString = server.getConnectString();
         Map conf = new HashMap();
         conf.put(Config.STORM_ZOOKEEPER_SESSION_TIMEOUT, 1000);
+        conf.put(Config.STORM_ZOOKEEPER_CONNECTION_TIMEOUT, 1000);
         conf.put(Config.STORM_ZOOKEEPER_RETRY_TIMES, 4);
         conf.put(Config.STORM_ZOOKEEPER_RETRY_INTERVAL, 5);
         ExponentialBackoffRetry retryPolicy = new ExponentialBackoffRetry(1000, 3);

File: storm-core/src/jvm/backtype/storm/coordination/BatchBoltExecutor.java
Patch:
@@ -103,6 +103,6 @@ private IBatchBolt getBatchBolt(Object id) {
     }
     
     private IBatchBolt newTransactionalBolt() {
-        return (IBatchBolt) Utils.deserialize(_boltSer);
+        return Utils.deserialize(_boltSer, IBatchBolt.class);
     }
 }

File: storm-core/src/jvm/backtype/storm/serialization/GzipBridgeSerializationDelegate.java
Patch:
@@ -42,11 +42,11 @@ public byte[] serialize(Object object) {
     }
 
     @Override
-    public Object deserialize(byte[] bytes) {
+    public <T> T deserialize(byte[] bytes, Class<T> clazz) {
         if (isGzipped(bytes)) {
-            return gzipDelegate.deserialize(bytes);
+            return gzipDelegate.deserialize(bytes, clazz);
         } else {
-            return defaultDelegate.deserialize(bytes);
+            return defaultDelegate.deserialize(bytes,clazz);
         }
     }
 

File: storm-core/src/jvm/backtype/storm/serialization/SerializationDelegate.java
Patch:
@@ -31,5 +31,5 @@ public interface SerializationDelegate {
 
     byte[] serialize(Object object);
 
-    Object deserialize(byte[] bytes);
+    <T> T deserialize(byte[] bytes, Class<T> clazz);
 }

File: storm-core/src/jvm/backtype/storm/utils/LocalState.java
Patch:
@@ -64,7 +64,7 @@ private Map<Object, Object> deserializeLatestVersion() throws IOException {
             if (serialized.length == 0) {
                 LOG.warn("LocalState file '{}' contained no data, resetting state", latestPath);
             } else {
-                result = (Map<Object, Object>) Utils.deserialize(serialized);
+                result = Utils.deserialize(serialized, Map.class);
             }
         }
         return result;

File: storm-core/src/jvm/storm/trident/TridentTopology.java
Patch:
@@ -28,6 +28,8 @@
 import backtype.storm.topology.IRichSpout;
 import backtype.storm.tuple.Fields;
 import backtype.storm.utils.Utils;
+
+import java.io.Serializable;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collection;
@@ -664,7 +666,7 @@ private static Integer getFixedParallelism(Set<Group> groups) {
     private static boolean isIdentityPartition(PartitionNode n) {
         Grouping g = n.thriftGrouping;
         if(g.is_set_custom_serialized()) {
-            CustomStreamGrouping csg = (CustomStreamGrouping) Utils.deserialize(g.get_custom_serialized());
+            CustomStreamGrouping csg = (CustomStreamGrouping) Utils.deserialize(g.get_custom_serialized(), Serializable.class);
             return csg instanceof IdentityGrouping;
         }
         return false;

File: storm-core/test/jvm/backtype/storm/serialization/GzipBridgeSerializationDelegateTest.java
Patch:
@@ -41,7 +41,7 @@ public void testDeserialize_readingFromGzip() throws Exception {
 
         byte[] serialized = new GzipSerializationDelegate().serialize(pojo);
 
-        TestPojo pojo2 = (TestPojo)testDelegate.deserialize(serialized);
+        TestPojo pojo2 = (TestPojo)testDelegate.deserialize(serialized, TestPojo.class);
 
         assertEquals(pojo2.name, pojo.name);
         assertEquals(pojo2.age, pojo.age);
@@ -55,7 +55,7 @@ public void testDeserialize_readingFromGzipBridge() throws Exception {
 
         byte[] serialized = new GzipBridgeSerializationDelegate().serialize(pojo);
 
-        TestPojo pojo2 = (TestPojo)testDelegate.deserialize(serialized);
+        TestPojo pojo2 = (TestPojo)testDelegate.deserialize(serialized, TestPojo.class);
 
         assertEquals(pojo2.name, pojo.name);
         assertEquals(pojo2.age, pojo.age);
@@ -69,7 +69,7 @@ public void testDeserialize_readingFromDefault() throws Exception {
 
         byte[] serialized = new DefaultSerializationDelegate().serialize(pojo);
 
-        TestPojo pojo2 = (TestPojo)testDelegate.deserialize(serialized);
+        TestPojo pojo2 = (TestPojo)testDelegate.deserialize(serialized, TestPojo.class);
 
         assertEquals(pojo2.name, pojo.name);
         assertEquals(pojo2.age, pojo.age);

File: storm-core/src/jvm/storm/trident/spout/RichSpoutBatchExecutor.java
Patch:
@@ -140,6 +140,7 @@ private void fail(long batchId) {
         
         @Override
         public void close() {
+            _spout.close();
         }
         
     }

File: external/storm-jdbc/src/main/java/org/apache/storm/jdbc/bolt/JdbcBolt.java
Patch:
@@ -61,6 +61,7 @@ public void execute(Tuple tuple) {
         } catch (Exception e) {
             LOG.warn("Failing tuple.", e);
             this.collector.fail(tuple);
+            this.collector.reportError(e);
             return;
         }
 

File: external/storm-jdbc/src/main/java/org/apache/storm/jdbc/bolt/JdbcLookupBolt.java
Patch:
@@ -68,8 +68,9 @@ public void execute(Tuple tuple) {
             }
             this.collector.ack(tuple);
         } catch (Exception e) {
-            LOG.info("Failed to execute a select query {} on tuple {} ", this.selectQuery, tuple);
+            LOG.warn("Failed to execute a select query {} on tuple {} ", this.selectQuery, tuple);
             this.collector.fail(tuple);
+            this.collector.reportError(e);
         }
     }
 

File: external/storm-jdbc/src/main/java/org/apache/storm/jdbc/common/JDBCClient.java
Patch:
@@ -31,12 +31,12 @@
 import java.sql.Date;
 import java.util.*;
 
-public class JDBCClient {
-    private static final Logger LOG = LoggerFactory.getLogger(JDBCClient.class);
+public class JdbcClient {
+    private static final Logger LOG = LoggerFactory.getLogger(JdbcClient.class);
 
     private HikariDataSource dataSource;
 
-    public JDBCClient(Map<String, Object> map) {
+    public JdbcClient(Map<String, Object> map) {
         Properties properties = new Properties();
         properties.putAll(map);
         HikariConfig config = new HikariConfig(properties);

File: external/storm-jdbc/src/main/java/org/apache/storm/jdbc/mapper/SimpleJdbcMapper.java
Patch:
@@ -19,7 +19,7 @@
 
 import backtype.storm.tuple.ITuple;
 import org.apache.storm.jdbc.common.Column;
-import org.apache.storm.jdbc.common.JDBCClient;
+import org.apache.storm.jdbc.common.JdbcClient;
 import org.apache.storm.jdbc.common.Util;
 
 import java.sql.Date;
@@ -34,7 +34,7 @@ public class SimpleJdbcMapper implements JdbcMapper {
     private List<Column> schemaColumns;
 
     public SimpleJdbcMapper(String tableName, Map map) {
-        JDBCClient client = new JDBCClient(map);
+        JdbcClient client = new JdbcClient(map);
         this.schemaColumns = client.getColumnSchema(tableName);
     }
 

File: external/storm-jdbc/src/main/java/org/apache/storm/jdbc/trident/state/JdbcState.java
Patch:
@@ -22,7 +22,7 @@
 import com.google.common.collect.Lists;
 import org.apache.commons.lang.Validate;
 import org.apache.storm.jdbc.common.Column;
-import org.apache.storm.jdbc.common.JDBCClient;
+import org.apache.storm.jdbc.common.JdbcClient;
 import org.apache.storm.jdbc.mapper.JdbcMapper;
 import org.apache.storm.jdbc.mapper.JdbcLookupMapper;
 import org.slf4j.Logger;
@@ -41,7 +41,7 @@ public class JdbcState implements State {
     private static final Logger LOG = LoggerFactory.getLogger(JdbcState.class);
 
     private Options options;
-    private JDBCClient jdbcClient;
+    private JdbcClient jdbcClient;
     private Map map;
 
     protected JdbcState(Map map, int partitionIndex, int numPartitions, Options options) {
@@ -86,7 +86,7 @@ protected void prepare() {
         Map<String, Object> conf = (Map<String, Object>) map.get(options.configKey);
         Validate.notEmpty(conf, "Hikari configuration not found using key '" + options.configKey + "'");
 
-        this.jdbcClient = new JDBCClient(conf);
+        this.jdbcClient = new JdbcClient(conf);
     }
 
     @Override

File: external/storm-jdbc/src/test/java/org/apache/storm/jdbc/common/JdbcClientTest.java
Patch:
@@ -32,7 +32,7 @@
 
 public class JdbcClientTest {
 
-    private JDBCClient client;
+    private JdbcClient client;
 
     private static final String tableName = "user_details";
     @Before
@@ -43,7 +43,7 @@ public void setup() {
         map.put("dataSource.user","SA");//root
         map.put("dataSource.password","");//password
 
-        this.client = new JDBCClient(map);
+        this.client = new JdbcClient(map);
         client.executeSql("create table user_details (id integer, user_name varchar(100), create_date date)");
     }
 

File: external/storm-jdbc/src/test/java/org/apache/storm/jdbc/topology/AbstractUserTopology.java
Patch:
@@ -24,7 +24,7 @@
 import com.google.common.collect.Lists;
 import com.google.common.collect.Maps;
 import org.apache.storm.jdbc.common.Column;
-import org.apache.storm.jdbc.common.JDBCClient;
+import org.apache.storm.jdbc.common.JdbcClient;
 import org.apache.storm.jdbc.mapper.JdbcMapper;
 import org.apache.storm.jdbc.mapper.JdbcLookupMapper;
 import org.apache.storm.jdbc.mapper.SimpleJdbcMapper;
@@ -74,7 +74,7 @@ public void execute(String[] args) throws Exception {
         Config config = new Config();
         config.put(JDBC_CONF, map);
 
-        JDBCClient jdbcClient = new JDBCClient(map);
+        JdbcClient jdbcClient = new JdbcClient(map);
         for (String sql : setupSqls) {
             jdbcClient.executeSql(sql);
         }

File: external/storm-jdbc/src/test/java/org/apache/storm/jdbc/topology/AbstractUserTopology.java
Patch:
@@ -93,7 +93,7 @@ public void execute(String[] args) throws Exception {
             cluster.shutdown();
             System.exit(0);
         } else {
-            StormSubmitter.submitTopology(args[5], config, getTopology());
+            StormSubmitter.submitTopology(args[4], config, getTopology());
         }
     }
 

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/bolt/format/SequenceFormat.java
Patch:
@@ -18,7 +18,6 @@
 package org.apache.storm.hdfs.bolt.format;
 
 import backtype.storm.tuple.Tuple;
-import org.apache.hadoop.io.Writable;
 
 import java.io.Serializable;
 
@@ -46,12 +45,12 @@ public interface SequenceFormat extends Serializable {
      * @param tuple
      * @return
      */
-    Writable key(Tuple tuple);
+    Object key(Tuple tuple);
 
     /**
      * Given a tuple, return the value that should be written to the sequence file.
      * @param tuple
      * @return
      */
-    Writable value(Tuple tuple);
+    Object value(Tuple tuple);
 }

File: external/storm-kafka/src/jvm/storm/kafka/bolt/mapper/FieldNameBasedTupleToKafkaMapper.java
Patch:
@@ -19,7 +19,7 @@
 
 import backtype.storm.tuple.Tuple;
 
-public class FieldNameBasedTupleToKafkaMapper<K,V> implements TupleToKafkaMapper {
+public class FieldNameBasedTupleToKafkaMapper<K,V> implements TupleToKafkaMapper<K, V> {
 
     public static final String BOLT_KEY = "key";
     public static final String BOLT_MESSAGE = "message";

File: external/storm-kafka/src/test/storm/kafka/KafkaUtilsTest.java
Patch:
@@ -99,7 +99,7 @@ public void fetchMessagesWithInvalidOffsetAndDefaultHandlingDisabled() throws Ex
                 new Partition(Broker.fromString(broker.getBrokerConnectionString()), 0), -99);
     }
 
-    @Test(expected = UpdateOffsetException.class)
+    @Test(expected = TopicOffsetOutOfRangeException.class)
     public void fetchMessagesWithInvalidOffsetAndDefaultHandlingEnabled() throws Exception {
         config = new KafkaConfig(brokerHosts, "newTopic");
         String value = "test";

File: external/storm-kafka/src/jvm/storm/kafka/KafkaUtils.java
Patch:
@@ -115,7 +115,7 @@ public Object getValueAndReset() {
                         }
                         long latestTimeOffset = getOffset(consumer, _topic, partition.partition, kafka.api.OffsetRequest.LatestTime());
                         long earliestTimeOffset = getOffset(consumer, _topic, partition.partition, kafka.api.OffsetRequest.EarliestTime());
-                        if (latestTimeOffset == 0) {
+                        if (latestTimeOffset == KafkaUtils.NO_OFFSET) {
                             LOG.warn("No data found in Kafka Partition " + partition.getId());
                             return null;
                         }

File: external/storm-kafka/src/jvm/storm/kafka/KafkaUtils.java
Patch:
@@ -115,7 +115,7 @@ public Object getValueAndReset() {
                         }
                         long latestTimeOffset = getOffset(consumer, _topic, partition.partition, kafka.api.OffsetRequest.LatestTime());
                         long earliestTimeOffset = getOffset(consumer, _topic, partition.partition, kafka.api.OffsetRequest.EarliestTime());
-                        if (latestTimeOffset == 0) {
+                        if (latestTimeOffset == KafkaUtils.NO_OFFSET) {
                             LOG.warn("No data found in Kafka Partition " + partition.getId());
                             return null;
                         }

File: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/ha/codedistributor/HDFSCodeDistributor.java
Patch:
@@ -1,6 +1,6 @@
 package org.apache.storm.hdfs.ha.codedistributor;
 
-import backtype.storm.nimbus.ICodeDistributor;
+import backtype.storm.codedistributor.ICodeDistributor;
 import com.google.common.collect.Lists;
 import org.apache.commons.io.IOUtils;
 import org.apache.commons.lang.Validate;

File: storm-core/src/jvm/backtype/storm/Config.java
Patch:
@@ -1257,7 +1257,7 @@ public class Config extends HashMap<String, Object> {
     public static final Object TOPOLOGY_HDFS_URI_SCHEMA = String.class;
 
     /**
-     * Which implementation of {@link backtype.storm.nimbus.ICodeDistributor} should be used by storm for code
+     * Which implementation of {@link backtype.storm.codedistributor.ICodeDistributor} should be used by storm for code
      * distribution.
      */
     public static final String STORM_CODE_DISTRIBUTOR_CLASS = "storm.codedistributor.class";

File: storm-core/src/jvm/backtype/storm/codedistributor/BitTorrentCodeDistributor.java
Patch:
@@ -1,7 +1,6 @@
-package backtype.storm.torrent;
+package backtype.storm.codedistributor;
 
 import backtype.storm.Config;
-import backtype.storm.nimbus.ICodeDistributor;
 import com.google.common.collect.Lists;
 import com.google.common.primitives.Shorts;
 import com.turn.ttorrent.client.Client;
@@ -78,7 +77,8 @@ public File upload(String dirPath, String topologyId) throws Exception {
         LOG.info("Seeding torrent...");
 
         /**
-         * Every time on prepare we need to call tracker.announce for all torrents that
+         *
+         * TODO: Every time on prepare we need to call tracker.announce for all torrents that
          * exists in the file system, other wise the tracker will reject any peer request
          * with unknown torrents. You need to bootstrap trackers.
          */

File: storm-core/src/jvm/backtype/storm/codedistributor/ICodeDistributor.java
Patch:
@@ -1,4 +1,4 @@
-package backtype.storm.nimbus;
+package backtype.storm.codedistributor;
 
 
 import java.io.File;

File: storm-core/src/jvm/backtype/storm/codedistributor/LocalFileSystemCodeDistributor.java
Patch:
@@ -1,6 +1,5 @@
-package backtype.storm.nimbus;
+package backtype.storm.codedistributor;
 
-import backtype.storm.Config;
 import backtype.storm.utils.ZookeeperAuthInfo;
 import com.google.common.collect.Lists;
 import org.apache.commons.io.FileUtils;

File: storm-core/src/jvm/backtype/storm/torrent/BitTorrentCodeDistributor.java
Patch:
@@ -138,7 +138,7 @@ private InetAddress getInetAddress() throws UnknownHostException {
             }
         }
 
-        throw new RuntimeException("No valid InetAddress could be obtained, something really wrong with network configuration.")
+        throw new RuntimeException("No valid InetAddress could be obtained, something really wrong with network configuration.");
     }
 
     @Override

File: storm-core/src/jvm/backtype/storm/utils/Utils.java
Patch:
@@ -260,13 +260,13 @@ public static void downloadFromMaster(Map conf, String file, String localFile) t
         download(client, file, localFile);
     }
 
-    public static void downloadFromHost(Map conf, String file, String localFile, String host, int port) throws IOException, TException {
+    public static void downloadFromHost(Map conf, String file, String localFile, String host, int port) throws IOException, TException, AuthorizationException {
         //TODO : instead of null as last arg we probably need some real timeout, check what is the default and if its ok to reuse.
         NimbusClient client = new NimbusClient (conf, host, port, null);
         download(client, file, localFile);
     }
 
-    private static void download(NimbusClient client, String file, String localFile) throws IOException, TException {
+    private static void download(NimbusClient client, String file, String localFile) throws IOException, TException, AuthorizationException {
         String id = client.getClient().beginFileDownload(file);
         WritableByteChannel out = Channels.newChannel(new FileOutputStream(localFile));
         while(true) {

File: examples/storm-starter/src/jvm/storm/starter/util/StormRunner.java
Patch:
@@ -21,6 +21,7 @@
 import backtype.storm.LocalCluster;
 import backtype.storm.StormSubmitter;
 import backtype.storm.generated.AlreadyAliveException;
+import backtype.storm.generated.AuthorizationException;
 import backtype.storm.generated.InvalidTopologyException;
 import backtype.storm.generated.StormTopology;
 
@@ -41,7 +42,7 @@ public static void runTopologyLocally(StormTopology topology, String topologyNam
   }
 
   public static void runTopologyRemotely(StormTopology topology, String topologyName, Config conf)
-      throws AlreadyAliveException, InvalidTopologyException {
+      throws AlreadyAliveException, InvalidTopologyException, AuthorizationException {
     StormSubmitter.submitTopology(topologyName, conf, topology);
   }
 }

File: storm-core/src/jvm/backtype/storm/security/auth/authorizer/SimpleACLAuthorizer.java
Patch:
@@ -110,7 +110,7 @@ public boolean permit(ReqContext context, String operation, Map topology_conf) {
             }
 
             Set<String> topoGroups = new HashSet<String>();
-            if (topology_conf.containsKey(Config.TOPOLOGY_GROUPS)) {
+            if (topology_conf.containsKey(Config.TOPOLOGY_GROUPS) && topology_conf.get(Config.TOPOLOGY_GROUPS) != null) {
                 topoGroups.addAll((Collection<String>)topology_conf.get(Config.TOPOLOGY_GROUPS));
             }
 

File: external/storm-kafka/src/jvm/storm/kafka/SpoutConfig.java
Patch:
@@ -32,9 +32,6 @@ public class SpoutConfig extends KafkaConfig implements Serializable {
 
     // Exponential back-off retry settings.  These are used when retrying messages after a bolt
     // calls OutputCollector.fail().
-    //
-    // Note: be sure to set backtype.storm.Config.MESSAGE_TIMEOUT_SECS appropriately to prevent
-    // resubmitting the message while still retrying.
     public long retryInitialDelayMs = 0;
     public double retryDelayMultiplier = 1.0;
     public long retryDelayMaxMs = 60 * 1000;

File: external/storm-kafka/src/test/storm/kafka/KafkaUtilsTest.java
Patch:
@@ -137,6 +137,7 @@ public void generateTuplesWithoutKeyAndKeyValueScheme() {
     @Test
     public void generateTuplesWithKeyAndKeyValueScheme() {
         config.scheme = new KeyValueSchemeAsMultiScheme(new StringKeyValueScheme());
+        config.useStartOffsetTimeIfOffsetOutOfRange = false;
         String value = "value";
         String key = "key";
         createTopicAndSendMessage(key, value);

File: storm-core/src/jvm/backtype/storm/messaging/netty/Client.java
Patch:
@@ -147,6 +147,8 @@ private synchronized void connect() {
             }
 
             int tried = 0;
+            //setting channel to null to make sure we throw an exception when reconnection fails
+            channel = null;
             while (tried <= max_retries) {
 
                 LOG.info("Reconnect started for {}... [{}]", name(), tried);

File: storm-core/src/jvm/backtype/storm/messaging/netty/Server.java
Patch:
@@ -79,6 +79,7 @@ class Server implements IConnection {
         
         // Configure the server.
         int buffer_size = Utils.getInt(storm_conf.get(Config.STORM_MESSAGING_NETTY_BUFFER_SIZE));
+        int backlog = Utils.getInt(storm_conf.get(Config.STORM_MESSAGING_NETTY_BACKLOG), 500);
         int maxWorkers = Utils.getInt(storm_conf.get(Config.STORM_MESSAGING_NETTY_SERVER_WORKER_THREADS));
 
         ThreadFactory bossFactory = new NettyRenameThreadFactory(name() + "-boss");
@@ -98,6 +99,7 @@ class Server implements IConnection {
         bootstrap.setOption("child.tcpNoDelay", true);
         bootstrap.setOption("child.receiveBufferSize", buffer_size);
         bootstrap.setOption("child.keepAlive", true);
+        bootstrap.setOption("backlog", backlog);
 
         // Set up the pipeline factory.
         bootstrap.setPipelineFactory(new StormServerPipelineFactory(this));

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/bolt/HBaseLookupBolt.java
Patch:
@@ -66,7 +66,7 @@ public void execute(Tuple tuple) {
 
         try {
             Result result = hBaseClient.batchGet(Lists.newArrayList(get))[0];
-            for(Values values : rowToTupleMapper.toValues(result)) {
+            for(Values values : rowToTupleMapper.toValues(tuple, result)) {
                 this.collector.emit(values);
             }
             this.collector.ack(tuple);

File: external/storm-hbase/src/main/java/org/apache/storm/hbase/bolt/mapper/HBaseValueMapper.java
Patch:
@@ -18,6 +18,7 @@
 package org.apache.storm.hbase.bolt.mapper;
 
 import backtype.storm.topology.OutputFieldsDeclarer;
+import backtype.storm.tuple.ITuple;
 import backtype.storm.tuple.Values;
 import org.apache.hadoop.hbase.client.Result;
 
@@ -27,11 +28,12 @@
 public interface HBaseValueMapper extends Serializable {
     /**
      *
+     * @param input tuple.
      * @param result HBase lookup result instance.
      * @return list of values that should be emitted by the lookup bolt.
      * @throws Exception
      */
-    public List<Values> toValues(Result result) throws Exception;
+    public List<Values> toValues(ITuple input, Result result) throws Exception;
 
     /**
      * declares the output fields for the lookup bolt.

File: external/storm-hbase/src/test/java/org/apache/storm/hbase/topology/WordCountValueMapper.java
Patch:
@@ -20,6 +20,7 @@
 
 import backtype.storm.topology.OutputFieldsDeclarer;
 import backtype.storm.tuple.Fields;
+import backtype.storm.tuple.ITuple;
 import backtype.storm.tuple.Values;
 import org.apache.hadoop.hbase.Cell;
 import org.apache.hadoop.hbase.CellUtil;
@@ -51,7 +52,7 @@
 public class WordCountValueMapper implements HBaseValueMapper {
 
     @Override
-    public List<Values> toValues(Result result) throws Exception {
+    public List<Values> toValues(ITuple tuple, Result result) throws Exception {
         List<Values> values = new ArrayList<Values>();
         Cell[] cells = result.rawCells();
         for(Cell cell : cells) {

File: src/main/java/backtype/storm/contrib/jms/spout/JmsSpout.java
Patch:
@@ -214,8 +214,7 @@ public void nextTuple() {
 				// ack if we're not in AUTO_ACKNOWLEDGE mode, or the message requests ACKNOWLEDGE
 				LOG.debug("Requested deliveryMode: " + toDeliveryModeString(msg.getJMSDeliveryMode()));
 				LOG.debug("Our deliveryMode: " + toDeliveryModeString(this.jmsAcknowledgeMode));
-				if (this.isDurableSubscription()
-						|| (msg.getJMSDeliveryMode() != Session.AUTO_ACKNOWLEDGE)) {
+				if (this.isDurableSubscription()) {
 					LOG.debug("Requesting acks.");
                     JmsMessageID messageId = new JmsMessageID(this.messageSequence++, msg.getJMSMessageID());
 					this.collector.emit(vals, messageId);

File: src/main/java/backtype/storm/contrib/jms/trident/TridentJmsSpout.java
Patch:
@@ -321,7 +321,7 @@ public void emitBatch(TransactionAttempt tx, JmsBatch coordinatorMeta,
                 }
                 
                 try {
-                    if (msg.getJMSDeliveryMode() != Session.AUTO_ACKNOWLEDGE) {
+                    if (TridentJmsSpout.this.jmsAcknowledgeMode != Session.AUTO_ACKNOWLEDGE) {
                         batchMessages.add(msg);
                     }
                     Values tuple = tupleProducer.toTuple(msg);

File: storm-core/src/jvm/storm/trident/tuple/ValuePointer.java
Patch:
@@ -37,7 +37,6 @@ public static ValuePointer[] buildIndex(Fields fieldsOrder, Map<String, ValuePoi
             throw new IllegalArgumentException("Fields order must be same length as pointers map");
         }
         ValuePointer[] ret = new ValuePointer[pointers.size()];
-        List<String> flist = fieldsOrder.toList();
         for(int i=0; i<fieldsOrder.size(); i++) {
             ret[i] = pointers.get(fieldsOrder.get(i));
         }

File: external/storm-kafka/src/jvm/storm/kafka/PartitionManager.java
Patch:
@@ -161,7 +161,7 @@ private void fill() {
         try {
             msgs = KafkaUtils.fetchMessages(_spoutConfig, _consumer, _partition, offset);
         } catch (UpdateOffsetException e) {
-            offset = e.startOffset;
+            _emittedToOffset = e.startOffset;
         }
         long end = System.nanoTime();
         long millis = (end - start) / 1000000;

File: external/storm-kafka/src/jvm/storm/kafka/DynamicBrokersReader.java
Patch:
@@ -47,7 +47,7 @@ public DynamicBrokersReader(Map conf, String zkStr, String zkPath, String topic)
             _curator = CuratorFrameworkFactory.newClient(
                     zkStr,
                     Utils.getInt(conf.get(Config.STORM_ZOOKEEPER_SESSION_TIMEOUT)),
-                    15000,
+                    Utils.getInt(conf.get(Config.STORM_ZOOKEEPER_CONNECTION_TIMEOUT)),
                     new RetryNTimes(Utils.getInt(conf.get(Config.STORM_ZOOKEEPER_RETRY_TIMES)),
                             Utils.getInt(conf.get(Config.STORM_ZOOKEEPER_RETRY_INTERVAL))));
             _curator.start();

File: external/storm-kafka/src/test/storm/kafka/DynamicBrokersReaderTest.java
Patch:
@@ -50,6 +50,7 @@ public void setUp() throws Exception {
         String connectionString = server.getConnectString();
         Map conf = new HashMap();
         conf.put(Config.STORM_ZOOKEEPER_SESSION_TIMEOUT, 1000);
+        conf.put(Config.STORM_ZOOKEEPER_CONNECTION_TIMEOUT, 1000);
         conf.put(Config.STORM_ZOOKEEPER_RETRY_TIMES, 4);
         conf.put(Config.STORM_ZOOKEEPER_RETRY_INTERVAL, 5);
         ExponentialBackoffRetry retryPolicy = new ExponentialBackoffRetry(1000, 3);

File: storm-core/src/jvm/storm/trident/tuple/ValuePointer.java
Patch:
@@ -37,7 +37,6 @@ public static ValuePointer[] buildIndex(Fields fieldsOrder, Map<String, ValuePoi
             throw new IllegalArgumentException("Fields order must be same length as pointers map");
         }
         ValuePointer[] ret = new ValuePointer[pointers.size()];
-        List<String> flist = fieldsOrder.toList();
         for(int i=0; i<fieldsOrder.size(); i++) {
             ret[i] = pointers.get(fieldsOrder.get(i));
         }

File: storm-core/src/jvm/storm/trident/tuple/ValuePointer.java
Patch:
@@ -37,7 +37,6 @@ public static ValuePointer[] buildIndex(Fields fieldsOrder, Map<String, ValuePoi
             throw new IllegalArgumentException("Fields order must be same length as pointers map");
         }
         ValuePointer[] ret = new ValuePointer[pointers.size()];
-        List<String> flist = fieldsOrder.toList();
         for(int i=0; i<fieldsOrder.size(); i++) {
             ret[i] = pointers.get(fieldsOrder.get(i));
         }

File: external/storm-kafka/src/jvm/storm/kafka/PartitionManager.java
Patch:
@@ -188,10 +188,9 @@ private void fill() {
     public void ack(Long offset) {
         if (!_pending.isEmpty() && _pending.first() < offset - _spoutConfig.maxOffsetBehind) {
             // Too many things pending!
-            _pending.headSet(offset).clear();
-        } else {
-            _pending.remove(offset);
+            _pending.headSet(offset - _spoutConfig.maxOffsetBehind).clear();
         }
+        _pending.remove(offset);
         numberAcked++;
     }
 

File: external/storm-kafka/src/jvm/storm/kafka/PartitionManager.java
Patch:
@@ -307,7 +307,7 @@ public MessageRetryRecord() {
 
         private MessageRetryRecord(int retryNum) {
             this.retryNum = retryNum;
-            this.retryTimeUTC = new Date().getTime() + calculateRetryDelay();
+            this.retryTimeUTC = System.currentTimeMillis() + calculateRetryDelay();
         }
 
         public MessageRetryRecord createNextRetryRecord() {
@@ -321,7 +321,7 @@ private long calculateRetryDelay() {
         }
 
         public boolean isReadyForRetry() {
-            return new Date().getTime() > this.retryTimeUTC;
+            return System.currentTimeMillis() > this.retryTimeUTC;
         }
     }
 }

File: external/storm-kafka/src/jvm/storm/kafka/PartitionManager.java
Patch:
@@ -307,15 +307,15 @@ public MessageRetryRecord() {
 
         private MessageRetryRecord(int retryNum) {
             this.retryNum = retryNum;
-            this.retryTimeUTC = new Date().getTime() + calculateRetryDelay(this.retryNum);
+            this.retryTimeUTC = new Date().getTime() + calculateRetryDelay();
         }
 
         public MessageRetryRecord createNextRetryRecord() {
             return new MessageRetryRecord(this.retryNum + 1);
         }
 
-        private long calculateRetryDelay(int retryNum) {
-            double delayMultiplier = Math.pow(_spoutConfig.retryDelayMultiplier, retryNum - 1);
+        private long calculateRetryDelay() {
+            double delayMultiplier = Math.pow(_spoutConfig.retryDelayMultiplier, this.retryNum - 1);
             long delayThisRetryMs = (long) (_spoutConfig.retryInitialDelayMs * delayMultiplier);
             return Math.min(delayThisRetryMs, _spoutConfig.retryDelayMaxMs);
         }

File: external/storm-kafka/src/jvm/storm/kafka/SpoutConfig.java
Patch:
@@ -30,9 +30,9 @@ public class SpoutConfig extends KafkaConfig implements Serializable {
 
     // Exponential back-off retry settings - note: be sure to set backtype.storm.Config.MESSAGE_TIMEOUT_SECS
     // appropriately to prevent resubmitting the message while still retrying
-    public long retryInitialDelayMs = 1000;
+    public long retryInitialDelayMs = 0;
     public double retryDelayMultiplier = 1.0;
-    public long retryMaxDelayMs = 1000;
+    public long retryMaxDelayMs = 60 * 1000;
 
     public SpoutConfig(BrokerHosts hosts, String topic, String zkRoot, String id) {
         super(hosts, topic);

File: external/storm-kafka/src/jvm/storm/kafka/KafkaUtils.java
Patch:
@@ -115,7 +115,7 @@ public Object getValueAndReset() {
                         }
                         long latestTimeOffset = getOffset(consumer, _topic, partition.partition, kafka.api.OffsetRequest.LatestTime());
                         long earliestTimeOffset = getOffset(consumer, _topic, partition.partition, kafka.api.OffsetRequest.EarliestTime());
-                        if (latestTimeOffset == 0 || earliestTimeOffset == 0) {
+                        if (latestTimeOffset == 0) {
                             LOG.warn("No data found in Kafka Partition " + partition.getId());
                             return null;
                         }

File: examples/storm-starter/src/jvm/storm/starter/PrintSampleStream.java
Patch:
@@ -39,10 +39,10 @@ public static void main(String[] args) {
         
         TopologyBuilder builder = new TopologyBuilder();
         
-        builder.setSpout("spoutId", new TwitterSampleSpout(consumerKey, consumerSecret,
+        builder.setSpout("twitter", new TwitterSampleSpout(consumerKey, consumerSecret,
                                 accessToken, accessTokenSecret, keyWords));
         builder.setBolt("print", new PrinterBolt())
-                .shuffleGrouping("spout");
+                .shuffleGrouping("twitter");
                 
                 
         Config conf = new Config();

File: storm-core/src/jvm/backtype/storm/utils/Utils.java
Patch:
@@ -361,8 +361,8 @@ public static CuratorFramework newCurator(Map conf, List<String> servers, Object
                 .sessionTimeoutMs(Utils.getInt(conf.get(Config.STORM_ZOOKEEPER_SESSION_TIMEOUT)))
                 .retryPolicy(new StormBoundedExponentialBackoffRetry(
                             Utils.getInt(conf.get(Config.STORM_ZOOKEEPER_RETRY_INTERVAL)),
-                            Utils.getInt(conf.get(Config.STORM_ZOOKEEPER_RETRY_TIMES)),
-                            Utils.getInt(conf.get(Config.STORM_ZOOKEEPER_RETRY_INTERVAL_CEILING))));
+                            Utils.getInt(conf.get(Config.STORM_ZOOKEEPER_RETRY_INTERVAL_CEILING)),
+                            Utils.getInt(conf.get(Config.STORM_ZOOKEEPER_RETRY_TIMES))));
         if(auth!=null && auth.scheme!=null) {
             builder = builder.authorization(auth.scheme, auth.payload);
         }

File: storm-core/src/jvm/backtype/storm/spout/ShellSpout.java
Patch:
@@ -128,6 +128,9 @@ private void querySubprocess() {
             while (true) {
                 ShellMsg shellMsg = _process.readShellMsg();
                 String command = shellMsg.getCommand();
+                if (command == null) {
+                    throw new IllegalArgumentException("Command not found in spout message: " + shellMsg);
+                }
                 if (command.equals("sync")) {
                     return;
                 } else if (command.equals("log")) {

File: external/storm-kafka/src/jvm/storm/kafka/PartitionManager.java
Patch:
@@ -188,10 +188,9 @@ private void fill() {
     public void ack(Long offset) {
         if (!_pending.isEmpty() && _pending.first() < offset - _spoutConfig.maxOffsetBehind) {
             // Too many things pending!
-            _pending.headSet(offset).clear();
-        } else {
-            _pending.remove(offset);
+            _pending.headSet(offset - _spoutConfig.maxOffsetBehind).clear();
         }
+        _pending.remove(offset);
         numberAcked++;
     }
 

File: storm-core/src/jvm/backtype/storm/task/ShellBolt.java
Patch:
@@ -21,6 +21,7 @@
 import backtype.storm.generated.ShellComponent;
 import backtype.storm.metric.api.IMetric;
 import backtype.storm.metric.api.rpc.IShellMetric;
+import backtype.storm.topology.ReportedFailedException;
 import backtype.storm.tuple.MessageId;
 import backtype.storm.tuple.Tuple;
 import backtype.storm.utils.ShellProcess;
@@ -257,6 +258,7 @@ private void handleLog(ShellMsg shellMsg) {
                 break;
             case ERROR:
                 LOG.error(msg);
+                _collector.reportError(new ReportedFailedException(msg));
                 break;
             default:
                 LOG.info(msg);

File: examples/storm-starter/src/jvm/storm/starter/PrintSampleStream.java
Patch:
@@ -39,10 +39,10 @@ public static void main(String[] args) {
         
         TopologyBuilder builder = new TopologyBuilder();
         
-        builder.setSpout("spoutId", new TwitterSampleSpout(consumerKey, consumerSecret,
+        builder.setSpout("twitter", new TwitterSampleSpout(consumerKey, consumerSecret,
                                 accessToken, accessTokenSecret, keyWords));
         builder.setBolt("print", new PrinterBolt())
-                .shuffleGrouping("spout");
+                .shuffleGrouping("twitter");
                 
                 
         Config conf = new Config();

File: storm-core/src/jvm/backtype/storm/task/ShellBolt.java
Patch:
@@ -21,6 +21,7 @@
 import backtype.storm.generated.ShellComponent;
 import backtype.storm.metric.api.IMetric;
 import backtype.storm.metric.api.rpc.IShellMetric;
+import backtype.storm.topology.ReportedFailedException;
 import backtype.storm.tuple.MessageId;
 import backtype.storm.tuple.Tuple;
 import backtype.storm.utils.ShellProcess;
@@ -257,6 +258,7 @@ private void handleLog(ShellMsg shellMsg) {
                 break;
             case ERROR:
                 LOG.error(msg);
+                _collector.reportError(new ReportedFailedException(msg));
                 break;
             default:
                 LOG.info(msg);

File: examples/storm-starter/src/jvm/storm/starter/PrintSampleStream.java
Patch:
@@ -39,10 +39,10 @@ public static void main(String[] args) {
         
         TopologyBuilder builder = new TopologyBuilder();
         
-        builder.setSpout("spoutId", new TwitterSampleSpout(consumerKey, consumerSecret,
+        builder.setSpout("twitter", new TwitterSampleSpout(consumerKey, consumerSecret,
                                 accessToken, accessTokenSecret, keyWords));
         builder.setBolt("print", new PrinterBolt())
-                .shuffleGrouping("spout");
+                .shuffleGrouping("twitter");
                 
                 
         Config conf = new Config();

File: storm-core/src/jvm/backtype/storm/spout/ShellSpout.java
Patch:
@@ -128,6 +128,9 @@ private void querySubprocess() {
             while (true) {
                 ShellMsg shellMsg = _process.readShellMsg();
                 String command = shellMsg.getCommand();
+                if (command == null) {
+                    throw new IllegalArgumentException("Command not found in spout message: " + shellMsg);
+                }
                 if (command.equals("sync")) {
                     return;
                 } else if (command.equals("log")) {

File: storm-core/src/jvm/backtype/storm/task/ShellBolt.java
Patch:
@@ -115,6 +115,9 @@ public void run() {
                         ShellMsg shellMsg = _process.readShellMsg();
 
                         String command = shellMsg.getCommand();
+                        if (command == null) {
+                            throw new IllegalArgumentException("Command not found in bolt message: " + shellMsg);
+                        }
                         if(command.equals("ack")) {
                             handleAck(shellMsg.getId());
                         } else if (command.equals("fail")) {

File: storm-core/src/jvm/backtype/storm/security/auth/ICredentialsRenewer.java
Patch:
@@ -35,6 +35,7 @@ public interface ICredentialsRenewer {
     /**
      * Renew any credentials that need to be renewed. (Update the credentials if needed)
      * @param credentials the credentials that may have something to renew.
+     * @param topologyConf topology configuration.
      */ 
-    public void renew(Map<String, String> credentials);
+    public void renew(Map<String, String> credentials, Map topologyConf);
 }

File: storm-core/src/jvm/backtype/storm/security/auth/kerberos/AutoTGT.java
Patch:
@@ -236,7 +236,7 @@ private long getRefreshTime(KerberosTicket tgt) {
     }
 
     @Override
-    public void renew(Map<String,String> credentials) {
+    public void renew(Map<String,String> credentials, Map topologyConf) {
         KerberosTicket tgt = getTGT(credentials);
         if (tgt != null) {
             long refreshTime = getRefreshTime(tgt);

File: external/storm-kafka/src/jvm/storm/kafka/KafkaConfig.java
Patch:
@@ -35,7 +35,7 @@ public class KafkaConfig implements Serializable {
     public MultiScheme scheme = new RawMultiScheme();
     public boolean forceFromStart = false;
     public long startOffsetTime = kafka.api.OffsetRequest.EarliestTime();
-    public long maxOffsetBehind = 100000;
+    public long maxOffsetBehind = Long.MAX_VALUE;
     public boolean useStartOffsetTimeIfOffsetOutOfRange = true;
     public int metricsTimeBucketSizeInSecs = 60;
 

File: storm-core/src/jvm/backtype/storm/spout/ISpout.java
Patch:
@@ -28,7 +28,7 @@
  * based on a tuple emitted by the spout. When Storm detects that every tuple in
  * that DAG has been successfully processed, it will send an ack message to the Spout.
  *
- * <p>If a tuple fails to be fully process within the configured timeout for the
+ * <p>If a tuple fails to be fully processed within the configured timeout for the
  * topology (see {@link backtype.storm.Config}), Storm will send a fail message to the spout
  * for the message.</p>
  *

File: storm-core/src/jvm/backtype/storm/utils/ShellProcess.java
Patch:
@@ -173,7 +173,7 @@ public int getExitCode() {
     }
 
     public String getProcessInfoString() {
-        return String.format(" pid:%s, name:%s ", pid, componentName);
+        return String.format("pid:%s, name:%s", pid, componentName);
     }
 
     public String getProcessTerminationInfoString() {

File: external/storm-kafka/src/jvm/storm/kafka/KafkaConfig.java
Patch:
@@ -35,7 +35,7 @@ public class KafkaConfig implements Serializable {
     public MultiScheme scheme = new RawMultiScheme();
     public boolean forceFromStart = false;
     public long startOffsetTime = kafka.api.OffsetRequest.EarliestTime();
-    public long maxOffsetBehind = 100000;
+    public long maxOffsetBehind = Long.MAX_VALUE;
     public boolean useStartOffsetTimeIfOffsetOutOfRange = true;
     public int metricsTimeBucketSizeInSecs = 60;
 

File: storm-core/src/jvm/backtype/storm/spout/ISpout.java
Patch:
@@ -28,7 +28,7 @@
  * based on a tuple emitted by the spout. When Storm detects that every tuple in
  * that DAG has been successfully processed, it will send an ack message to the Spout.
  *
- * <p>If a tuple fails to be fully process within the configured timeout for the
+ * <p>If a tuple fails to be fully processed within the configured timeout for the
  * topology (see {@link backtype.storm.Config}), Storm will send a fail message to the spout
  * for the message.</p>
  *

File: external/storm-kafka/src/jvm/storm/kafka/KafkaConfig.java
Patch:
@@ -35,7 +35,7 @@ public class KafkaConfig implements Serializable {
     public MultiScheme scheme = new RawMultiScheme();
     public boolean forceFromStart = false;
     public long startOffsetTime = kafka.api.OffsetRequest.EarliestTime();
-    public long maxOffsetBehind = 100000;
+    public long maxOffsetBehind = Long.MAX_VALUE;
     public boolean useStartOffsetTimeIfOffsetOutOfRange = true;
     public int metricsTimeBucketSizeInSecs = 60;
 

File: storm-core/src/jvm/backtype/storm/spout/ISpout.java
Patch:
@@ -28,7 +28,7 @@
  * based on a tuple emitted by the spout. When Storm detects that every tuple in
  * that DAG has been successfully processed, it will send an ack message to the Spout.
  *
- * <p>If a tuple fails to be fully process within the configured timeout for the
+ * <p>If a tuple fails to be fully processed within the configured timeout for the
  * topology (see {@link backtype.storm.Config}), Storm will send a fail message to the spout
  * for the message.</p>
  *

File: src/main/java/org/apache/storm/hbase/bolt/HBaseBolt.java
Patch:
@@ -86,7 +86,7 @@ public void prepare(Map map, TopologyContext topologyContext, OutputCollector co
             LOG.warn("No 'hbase.rootdir' value found in configuration! Using HBase defaults.");
         }
         for(String key : conf.keySet()){
-            hbConfig.set(key, String.valueOf(map.get(key)));
+            hbConfig.set(key, String.valueOf(conf.get(key)));
         }
 
         try{

File: src/main/java/org/apache/storm/hbase/trident/state/HBaseMapState.java
Patch:
@@ -86,7 +86,6 @@ public static class Options<T> implements Serializable {
         public String tableName;
         public String columnFamily;
         public String qualifier;
-
     }
 
 

File: storm-core/src/jvm/backtype/storm/Config.java
Patch:
@@ -98,11 +98,11 @@ public class Config extends HashMap<String, Object> {
     public static final Object STORM_NETTY_FLUSH_CHECK_INTERVAL_MS_SCHEMA = Number.class;
     
     /**
-     * The type of compression, if any, that should be used for serialized objects stored in zookeeper and on disk.
+     * The delegate for serializing metadata, should be used for serialized objects stored in zookeeper and on disk.
      * This is NOT used for compressing serialized tuples sent between topologies.
      */
-    public static final String STORM_META_SERIALIZATION_COMPRESSION_TYPE = "storm.meta.serialization.compression.type";
-    public static final Object STORM_META_SERIALIZATION_COMPRESSION_TYPE_SCHEMA = String.class;
+    public static final String STORM_META_SERIALIZATION_DELEGATE = "storm.meta.serialization.delegate";
+    public static final Object STORM_META_SERIALIZATION_DELEGATE_SCHEMA = String.class;
     
     /**
      * A list of hosts of ZooKeeper servers used to manage the cluster.

File: storm-core/src/jvm/backtype/storm/Config.java
Patch:
@@ -98,11 +98,11 @@ public class Config extends HashMap<String, Object> {
     public static final Object STORM_NETTY_FLUSH_CHECK_INTERVAL_MS_SCHEMA = Number.class;
     
     /**
-     * The type of compression, if any, that should be used for serialized objects stored in zookeeper and on disk.
+     * The delegate for serializing metadata, should be used for serialized objects stored in zookeeper and on disk.
      * This is NOT used for compressing serialized tuples sent between topologies.
      */
-    public static final String STORM_META_SERIALIZATION_COMPRESSION_TYPE = "storm.meta.serialization.compression.type";
-    public static final Object STORM_META_SERIALIZATION_COMPRESSION_TYPE_SCHEMA = String.class;
+    public static final String STORM_META_SERIALIZATION_DELEGATE = "storm.meta.serialization.delegate";
+    public static final Object STORM_META_SERIALIZATION_DELEGATE_SCHEMA = String.class;
     
     /**
      * A list of hosts of ZooKeeper servers used to manage the cluster.

File: external/storm-kafka/src/jvm/storm/kafka/KafkaConfig.java
Patch:
@@ -35,7 +35,7 @@ public class KafkaConfig implements Serializable {
     public MultiScheme scheme = new RawMultiScheme();
     public boolean forceFromStart = false;
     public long startOffsetTime = kafka.api.OffsetRequest.EarliestTime();
-    public long maxOffsetBehind = 100000;
+    public long maxOffsetBehind = Long.MAX_VALUE;
     public boolean useStartOffsetTimeIfOffsetOutOfRange = true;
     public int metricsTimeBucketSizeInSecs = 60;
 

File: storm-core/src/jvm/backtype/storm/Config.java
Patch:
@@ -468,8 +468,7 @@ public class Config extends HashMap<String, Object> {
 
     /**
      * The jvm opts provided to workers launched by this supervisor. All "%ID%", "%WORKER-ID%", "%STORM-ID%"
-     * and "%WORKER-PORT%" substrings are replaced with an identifier for this worker. Each of these ids are
-     * replaced with:
+     * and "%WORKER-PORT%" substrings are replaced with:
      * %ID%          -> port (for backward compatibility),
      * %WORKER-ID%   -> worker-id, 
      * %STORM-ID%    -> topology-id,

File: storm-core/src/jvm/backtype/storm/spout/ISpout.java
Patch:
@@ -28,7 +28,7 @@
  * based on a tuple emitted by the spout. When Storm detects that every tuple in
  * that DAG has been successfully processed, it will send an ack message to the Spout.
  *
- * <p>If a tuple fails to be fully process within the configured timeout for the
+ * <p>If a tuple fails to be fully processed within the configured timeout for the
  * topology (see {@link backtype.storm.Config}), Storm will send a fail message to the spout
  * for the message.</p>
  *

File: external/storm-kafka/src/jvm/storm/kafka/KafkaUtils.java
Patch:
@@ -115,7 +115,8 @@ public Object getValueAndReset() {
                         }
                         long latestTimeOffset = getOffset(consumer, _topic, partition.partition, kafka.api.OffsetRequest.LatestTime());
                         long earliestTimeOffset = getOffset(consumer, _topic, partition.partition, kafka.api.OffsetRequest.EarliestTime());
-                        if (latestTimeOffset == 0 || earliestTimeOffset == 0) {
+                        //if (latestTimeOffset == 0 || earliestTimeOffset == 0) {
+                        if (latestTimeOffset == 0) {
                             LOG.warn("No data found in Kafka Partition " + partition.getId());
                             return null;
                         }

File: storm-core/src/jvm/backtype/storm/task/ShellBolt.java
Patch:
@@ -241,8 +241,8 @@ private void handleEmit(Map action) throws InterruptedException {
 
     private void die(Throwable exception) {
         _exception = exception;
-        LOG.info("Halting process: ShellBolt died.", exception);
+        LOG.error("Halting process: ShellBolt died.", exception);
         _collector.reportError(exception);
-        Runtime.getRuntime().halt(11);
+        System.exit(11);
     }
 }

File: storm-core/src/jvm/backtype/storm/security/auth/SaslTransportPlugin.java
Patch:
@@ -67,14 +67,13 @@ public TServer getServer(TProcessor processor) throws IOException, TTransportExc
         TTransportFactory serverTransportFactory = getServerTransportFactory();
         TServerSocket serverTransport = new TServerSocket(port);
         int numWorkerThreads = type.getNumThreads(storm_conf);
-        int maxBufferSize = type.getMaxBufferSize(storm_conf);
         Integer queueSize = type.getQueueSize(storm_conf);
 
         TThreadPoolServer.Args server_args = new TThreadPoolServer.Args(serverTransport).
                 processor(new TUGIWrapProcessor(processor)).
                 minWorkerThreads(numWorkerThreads).
                 maxWorkerThreads(numWorkerThreads).
-                protocolFactory(new TBinaryProtocol.Factory(false, true, maxBufferSize));
+                protocolFactory(new TBinaryProtocol.Factory(false, true));
 
         if (serverTransportFactory != null) {
             server_args.transportFactory(serverTransportFactory);

File: storm-core/src/jvm/backtype/storm/spout/ShellSpout.java
Patch:
@@ -129,7 +129,7 @@ private void querySubprocess() {
                 ShellMsg shellMsg = _process.readShellMsg();
                 String command = shellMsg.getCommand();
                 if (command == null) {
-                    throw new UnsupportedOperationException("Command not found in spout message: " + shellMsg);
+                    throw new IllegalArgumentException("Command not found in spout message: " + shellMsg);
                 }
                 if (command.equals("sync")) {
                     return;

File: storm-core/src/jvm/backtype/storm/task/ShellBolt.java
Patch:
@@ -116,7 +116,7 @@ public void run() {
 
                         String command = shellMsg.getCommand();
                         if (command == null) {
-                            throw new UnsupportedOperationException("Command not found in bolt message: " + shellMsg);
+                            throw new IllegalArgumentException("Command not found in bolt message: " + shellMsg);
                         }
                         if(command.equals("ack")) {
                             handleAck(shellMsg.getId());

File: external/storm-kafka/src/jvm/storm/kafka/KafkaConfig.java
Patch:
@@ -35,7 +35,7 @@ public class KafkaConfig implements Serializable {
     public MultiScheme scheme = new RawMultiScheme();
     public boolean forceFromStart = false;
     public long startOffsetTime = kafka.api.OffsetRequest.EarliestTime();
-    public long maxOffsetBehind = 100000;
+    public long maxOffsetBehind = Long.MAX_VALUE;
     public boolean useStartOffsetTimeIfOffsetOutOfRange = true;
     public int metricsTimeBucketSizeInSecs = 60;
 

File: storm-core/src/jvm/backtype/storm/utils/DisruptorQueue.java
Patch:
@@ -29,7 +29,6 @@
 import com.lmax.disruptor.dsl.ProducerType;
 
 import java.util.concurrent.ConcurrentLinkedQueue;
-import java.util.concurrent.TimeUnit;
 import java.util.concurrent.locks.Lock;
 import java.util.concurrent.locks.ReentrantReadWriteLock;
 import java.util.HashMap;

File: external/storm-kafka/src/jvm/storm/kafka/PartitionManager.java
Patch:
@@ -214,7 +214,7 @@ public void fail(Long offset) {
 
     public void commit() {
         long lastCompletedOffset = lastCompletedOffset();
-        if (lastCompletedOffset != lastCompletedOffset) {
+        if (_committedTo != lastCompletedOffset) {
             LOG.debug("Writing last completed offset (" + lastCompletedOffset + ") to ZK for " + _partition + " for topology: " + _topologyInstanceId);
             Map<Object, Object> data = (Map<Object, Object>) ImmutableMap.builder()
                     .put("topology", ImmutableMap.of("id", _topologyInstanceId,

File: storm-core/src/jvm/backtype/storm/messaging/netty/NettyRenameThreadFactory.java
Patch:
@@ -49,4 +49,4 @@ public Thread newThread(Runnable r) {
             t.setPriority(Thread.NORM_PRIORITY);
         return t;
     }
-}
\ No newline at end of file
+}

File: storm-core/test/jvm/backtype/storm/utils/DisruptorQueueTest.java
Patch:
@@ -55,10 +55,10 @@ public void onEvent(Object obj, long sequence, boolean endOfBatch)
                 }
             }
         });
-        
-        Assert.assertEquals("We expect to receive first published message first, but received " + result[0].toString(), 
-                "1", result[0]);
+
         run(producer, consumer);
+        Assert.assertEquals("We expect to receive first published message first, but received " + result[0],
+                "1", result[0]);
     }
     
     @Test 

File: storm-core/src/jvm/backtype/storm/messaging/netty/NettyRenameThreadFactory.java
Patch:
@@ -49,4 +49,4 @@ public Thread newThread(Runnable r) {
             t.setPriority(Thread.NORM_PRIORITY);
         return t;
     }
-}
\ No newline at end of file
+}

File: storm-core/src/jvm/backtype/storm/messaging/netty/StormClientPipelineFactory.java
Patch:
@@ -37,7 +37,7 @@ public ChannelPipeline getPipeline() throws Exception {
         // Encoder
         pipeline.addLast("encoder", new MessageEncoder());
         // business logic.
-        pipeline.addLast("handler", new StormClientHandler(client));
+        pipeline.addLast("handler", new StormClientErrorHandler(client.name()));
 
         return pipeline;
     }

File: storm-core/src/jvm/backtype/storm/messaging/netty/StormClientPipelineFactory.java
Patch:
@@ -37,7 +37,7 @@ public ChannelPipeline getPipeline() throws Exception {
         // Encoder
         pipeline.addLast("encoder", new MessageEncoder());
         // business logic.
-        pipeline.addLast("handler", new StormClientHandler(client));
+        pipeline.addLast("handler", new StormClientErrorHandler(client.name()));
 
         return pipeline;
     }

File: storm-core/src/jvm/backtype/storm/utils/TransferDrainer.java
Patch:
@@ -50,7 +50,7 @@ public void send(HashMap<String, IConnection> connections) {
       if (null != connection) { 
         ArrayList<ArrayList<TaskMessage>> bundle = bundles.get(hostPort);
         Iterator<TaskMessage> iter = getBundleIterator(bundle);
-        if (null != iter) {
+        if (null != iter && iter.hasNext()) {
           connection.send(iter);
         }
       }

File: storm-core/src/jvm/backtype/storm/messaging/netty/Client.java
Patch:
@@ -109,7 +109,7 @@ public void run() {
             @Override
             public void run() {
 
-                while(!closing) {
+                if(!closing) {
                     long flushCheckTime = flushCheckTimer.get();
                     long now = System.currentTimeMillis();
                     if (now > flushCheckTime) {

File: storm-core/src/jvm/backtype/storm/messaging/netty/Context.java
Patch:
@@ -63,7 +63,7 @@ public void prepare(Map storm_conf) {
                     Executors.newCachedThreadPool(workerFactory));
         }
         
-        int otherWorkers = Utils.getInt(storm_conf.get(Config.TOPOLOGY_WORKERS)) - 1;
+        int otherWorkers = Utils.getInt(storm_conf.get(Config.TOPOLOGY_WORKERS), 1) - 1;
         int poolSize = Math.min(Math.max(1, otherWorkers), MAX_CLIENT_SCHEDULER_THREAD_POOL_SIZE);
         clientScheduleService = Executors.newScheduledThreadPool(poolSize, new NettyRenameThreadFactory("client-schedule-service"));
     }

File: storm-core/src/jvm/backtype/storm/messaging/netty/StormClientPipelineFactory.java
Patch:
@@ -37,7 +37,7 @@ public ChannelPipeline getPipeline() throws Exception {
         // Encoder
         pipeline.addLast("encoder", new MessageEncoder());
         // business logic.
-        pipeline.addLast("handler", new StormClientHandler(client));
+        pipeline.addLast("handler", new StormClientErrorHandler(client.name()));
 
         return pipeline;
     }

File: storm-core/src/jvm/backtype/storm/messaging/netty/Client.java
Patch:
@@ -52,7 +52,7 @@ public class Client implements IConnection {
     private final int buffer_size;
     private boolean closing;
 
-    private Integer messageBatchSize;
+    private int messageBatchSize;
     
     private AtomicLong pendings;
 

File: storm-core/src/jvm/backtype/storm/messaging/netty/StormServerHandler.java
Patch:
@@ -60,7 +60,7 @@ public void messageReceived(ChannelHandlerContext ctx, MessageEvent e) {
 
     @Override
     public void exceptionCaught(ChannelHandlerContext ctx, ExceptionEvent e) {
-        e.getCause().printStackTrace();
+        LOG.error("server errors in handling the request", e.getCause());
         server.closeChannel(e.getChannel());
     }
 }

File: storm-core/src/jvm/backtype/storm/messaging/netty/Server.java
Patch:
@@ -73,7 +73,7 @@ class Server implements IConnection {
         taskToQueueId = new HashMap<Integer, Integer>();
     
         message_queue = new LinkedBlockingQueue[queueCount];
-		    for (int i = 0; i < queueCount; i++) {
+        for (int i = 0; i < queueCount; i++) {
             message_queue[i] = new LinkedBlockingQueue<ArrayList<TaskMessage>>();
         }
         
@@ -247,7 +247,7 @@ public void send(Iterator<TaskMessage> msgs) {
       throw new RuntimeException("Server connection should not send any messages");
     }
 	
-	 public String name() {
+    public String name() {
       return "Netty-server-localhost-" + port;
     }
 }

File: storm-core/src/jvm/backtype/storm/StormSubmitter.java
Patch:
@@ -214,7 +214,7 @@ private static void submitJar(Map conf, ProgressListener listener) {
      * Submit jar file
      * @param conf the topology-specific configuration. See {@link Config}.
      * @param localJar file path of the jar file to submit
-     * @return
+     * @return the remote location of the submitted jar
      */
     public static String submitJar(Map conf, String localJar) {
         return submitJar(conf, localJar, null);
@@ -225,7 +225,7 @@ public static String submitJar(Map conf, String localJar) {
      * @param conf the topology-specific configuration. See {@link Config}.
      * @param localJar file path of the jar file to submit
      * @param listener progress listener to track the jar file upload
-     * @return
+     * @return the remote location of the submitted jar
      */
     public static String submitJar(Map conf, String localJar, ProgressListener listener) {
         if (localJar == null) {

File: examples/storm-starter/src/jvm/storm/starter/BasicDRPCTopology.java
Patch:
@@ -72,7 +72,7 @@ public static void main(String[] args) throws Exception {
     }
     else {
       conf.setNumWorkers(3);
-        StormSubmitter.submitTopologyWithProgressBar(args[0], conf, builder.createRemoteTopology());
+      StormSubmitter.submitTopologyWithProgressBar(args[0], conf, builder.createRemoteTopology());
     }
   }
 }

File: storm-core/src/jvm/backtype/storm/Config.java
Patch:
@@ -88,19 +88,19 @@ public class Config extends HashMap<String, Object> {
     /**
      * If the Netty messaging layer is busy, the Netty client will try to batch message as more as possible up to the size of STORM_NETTY_MESSAGE_BATCH_SIZE bytes
      */
-    public static final String STORM_NETTY_MESSAGE_BATCH_SIZE = "netty.transfer.batch.size";
+    public static final String STORM_NETTY_MESSAGE_BATCH_SIZE = "storm.messaging.netty.transfer.batch.size";
     public static final Object STORM_NETTY_MESSAGE_BATCH_SIZE_SCHEMA = Number.class;
 
     /**
      * This control whether we do Netty message transfer in a synchronized way or async way. 
      */
-    public static final String STORM_NETTY_BLOCKING = "netty.blocking";
+    public static final String STORM_NETTY_BLOCKING = "storm.messaging.netty.blocking";
     public static final Object STORM_NETTY_BLOCKING_SCHEMA = Boolean.class;
     
     /**
      * We check with this interval that whether the Netty channel is writable and try to write pending messages
      */
-    public static final String STORM_NETTY_FLUSH_CHECK_INTERVAL_MS = "netty.flush.check.interval.ms";
+    public static final String STORM_NETTY_FLUSH_CHECK_INTERVAL_MS = "storm.messaging.netty.flush.check.interval.ms";
     public static final Object STORM_NETTY_FLUSH_CHECK_INTERVAL_MS_SCHEMA = Number.class;
     
     

File: storm-core/src/jvm/backtype/storm/Config.java
Patch:
@@ -459,8 +459,7 @@ public class Config extends HashMap<String, Object> {
      * with an identifier for this worker.
      */
     public static final String WORKER_CHILDOPTS = "worker.childopts";
-    public static final Object WORKER_CHILDOPTS_SCHEMA = String.class;
-
+    public static final Object WORKER_CHILDOPTS_SCHEMA = ConfigValidation.StringOrStringListValidator;
 
     /**
      * How often this worker should heartbeat to the supervisor.
@@ -662,7 +661,7 @@ public class Config extends HashMap<String, Object> {
      * Topology-specific options for the worker child process. This is used in addition to WORKER_CHILDOPTS.
      */
     public static final String TOPOLOGY_WORKER_CHILDOPTS="topology.worker.childopts";
-    public static final Object TOPOLOGY_WORKER_CHILDOPTS_SCHEMA = String.class;
+    public static final Object TOPOLOGY_WORKER_CHILDOPTS_SCHEMA = ConfigValidation.StringOrStringListValidator;
 
     /**
      * This config is available for TransactionalSpouts, and contains the id ( a String) for

File: storm-core/src/jvm/backtype/storm/messaging/netty/Server.java
Patch:
@@ -123,7 +123,7 @@ protected void closeChannel(Channel channel) {
      * close all channels, and release resources
      */
     public synchronized void close() {
-        if (allChannels != null) {  
+        if (allChannels != null) {
             allChannels.close().awaitUninterruptibly();
             factory.releaseExternalResources();
             allChannels = null;

File: storm-core/src/jvm/backtype/storm/transactional/state/TransactionalState.java
Patch:
@@ -21,7 +21,7 @@
 import backtype.storm.serialization.KryoValuesDeserializer;
 import backtype.storm.serialization.KryoValuesSerializer;
 import backtype.storm.utils.Utils;
-import com.netflix.curator.framework.CuratorFramework;
+import org.apache.curator.framework.CuratorFramework;
 import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.List;

File: storm-core/src/jvm/storm/trident/spout/RichSpoutBatchTriggerer.java
Patch:
@@ -158,6 +158,7 @@ public List<Integer> emit(String ignore, List<Object> values, Object msgId) {
                 long r = _rand.nextLong();
                 _collector.emitDirect(t, _coordStream, new Values(batchId, count), r);
                 finish.vals.add(r);
+                _msgIdToBatchId.put(r, batchIdVal);
             }
             _finishConditions.put(batchIdVal, finish);
             return tasks;

File: storm-core/src/jvm/storm/trident/topology/state/TransactionalState.java
Patch:
@@ -20,7 +20,7 @@
 
 import backtype.storm.Config;
 import backtype.storm.utils.Utils;
-import com.netflix.curator.framework.CuratorFramework;
+import org.apache.curator.framework.CuratorFramework;
 import java.io.UnsupportedEncodingException;
 import java.util.ArrayList;
 import java.util.HashMap;

File: external/storm-kafka/src/test/storm/kafka/ZkCoordinatorTest.java
Patch:
@@ -61,7 +61,7 @@ private Map buildZookeeperConfig(TestingServer server) {
     public void shutdown() throws Exception {
         simpleConsumer.close();
         broker.shutdown();
-        server.stop();
+        server.close();
     }
 
     @Test

File: storm-core/src/jvm/backtype/storm/utils/Utils.java
Patch:
@@ -47,6 +47,7 @@
 import org.apache.thrift.TException;
 import org.json.simple.JSONValue;
 import org.yaml.snakeyaml.Yaml;
+import org.yaml.snakeyaml.constructor.SafeConstructor;
 
 import backtype.storm.Config;
 import backtype.storm.generated.ComponentCommon;
@@ -138,7 +139,7 @@ public static Map findAndReadConfigFile(String name, boolean mustExist) {
                   + resources);
             }
             URL resource = resources.iterator().next();
-            Yaml yaml = new Yaml();
+            Yaml yaml = new Yaml(new SafeConstructor());
             Map ret = null;
             InputStream input = resource.openStream();
             try {

File: external/storm-kafka/src/test/storm/kafka/KafkaTestBroker.java
Patch:
@@ -25,12 +25,14 @@ public KafkaTestBroker() {
             server = new TestingServer();
             zookeeperConnectionString = server.getConnectString();
             ExponentialBackoffRetry retryPolicy = new ExponentialBackoffRetry(1000, 3);
+            String tempDir = System.getProperty("java.io.tmpdir");
             CuratorFramework zookeeper = CuratorFrameworkFactory.newClient(zookeeperConnectionString, retryPolicy);
             zookeeper.start();
             Properties p = new Properties();
             p.setProperty("zookeeper.connect", zookeeperConnectionString);
             p.setProperty("broker.id", "0");
             p.setProperty("port", "" + port);
+            p.setProperty("log.dirs",tempDir+"kafka_tmp_logs");
             kafka.server.KafkaConfig config = new kafka.server.KafkaConfig(p);
             kafka = new KafkaServerStartable(config);
             kafka.startup();

File: storm-core/src/jvm/backtype/storm/utils/Utils.java
Patch:
@@ -26,6 +26,7 @@
 import java.io.ObjectInputStream;
 import java.io.ObjectOutputStream;
 import java.net.URL;
+import java.net.URLDecoder;
 import java.nio.ByteBuffer;
 import java.nio.channels.Channels;
 import java.nio.channels.WritableByteChannel;
@@ -168,9 +169,9 @@ public static Map readCommandLineOpts() {
         Map ret = new HashMap();
         String commandOptions = System.getProperty("storm.options");
         if(commandOptions != null) {
-            commandOptions = commandOptions.replaceAll("%%%%", " ");
             String[] configs = commandOptions.split(",");
             for (String config : configs) {
+                config = URLDecoder.decode(config);
                 String[] options = config.split("=", 2);
                 if (options.length == 2) {
                     Object val = JSONValue.parse(options[1]);

File: storm-core/src/jvm/backtype/storm/Config.java
Patch:
@@ -459,8 +459,7 @@ public class Config extends HashMap<String, Object> {
      * with an identifier for this worker.
      */
     public static final String WORKER_CHILDOPTS = "worker.childopts";
-    public static final Object WORKER_CHILDOPTS_SCHEMA = String.class;
-
+    public static final Object WORKER_CHILDOPTS_SCHEMA = ConfigValidation.StringOrStringListValidator;
 
     /**
      * How often this worker should heartbeat to the supervisor.
@@ -662,7 +661,7 @@ public class Config extends HashMap<String, Object> {
      * Topology-specific options for the worker child process. This is used in addition to WORKER_CHILDOPTS.
      */
     public static final String TOPOLOGY_WORKER_CHILDOPTS="topology.worker.childopts";
-    public static final Object TOPOLOGY_WORKER_CHILDOPTS_SCHEMA = String.class;
+    public static final Object TOPOLOGY_WORKER_CHILDOPTS_SCHEMA = ConfigValidation.StringOrStringListValidator;
 
     /**
      * This config is available for TransactionalSpouts, and contains the id ( a String) for

File: storm-core/src/jvm/backtype/storm/messaging/netty/Server.java
Patch:
@@ -123,7 +123,7 @@ protected void closeChannel(Channel channel) {
      * close all channels, and release resources
      */
     public synchronized void close() {
-        if (allChannels != null) {  
+        if (allChannels != null) {
             allChannels.close().awaitUninterruptibly();
             factory.releaseExternalResources();
             allChannels = null;

File: storm-core/src/jvm/storm/trident/spout/RichSpoutBatchTriggerer.java
Patch:
@@ -158,6 +158,7 @@ public List<Integer> emit(String ignore, List<Object> values, Object msgId) {
                 long r = _rand.nextLong();
                 _collector.emitDirect(t, _coordStream, new Values(batchId, count), r);
                 finish.vals.add(r);
+                _msgIdToBatchId.put(r, batchIdVal);
             }
             _finishConditions.put(batchIdVal, finish);
             return tasks;

File: storm-core/src/jvm/storm/trident/spout/RichSpoutBatchTriggerer.java
Patch:
@@ -158,6 +158,7 @@ public List<Integer> emit(String ignore, List<Object> values, Object msgId) {
                 long r = _rand.nextLong();
                 _collector.emitDirect(t, _coordStream, new Values(batchId, count), r);
                 finish.vals.add(r);
+                _msgIdToBatchId.put(r, batchIdVal);
             }
             _finishConditions.put(batchIdVal, finish);
             return tasks;

File: storm-core/src/jvm/backtype/storm/drpc/DRPCSpout.java
Patch:
@@ -35,7 +35,7 @@
 import java.util.Map;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
-import org.apache.thrift7.TException;
+import org.apache.thrift.TException;
 import org.json.simple.JSONValue;
 
 public class DRPCSpout extends BaseRichSpout {
@@ -114,7 +114,7 @@ public void nextTuple() {
                         _collector.emit(new Values(req.get_func_args(), JSONValue.toJSONString(returnInfo)), new DRPCMessageId(req.get_request_id(), i));
                         break;
                     }
-                } catch (TException e) {
+                } catch (Exception e) {
                     LOG.error("Failed to fetch DRPC result from DRPC server", e);
                 }
             }

File: storm-core/src/jvm/backtype/storm/drpc/ReturnResults.java
Patch:
@@ -32,7 +32,7 @@
 import java.util.Map;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
-import org.apache.thrift7.TException;
+import org.apache.thrift.TException;
 import org.json.simple.JSONValue;
 
 

File: storm-core/src/jvm/backtype/storm/generated/TopologyInitialStatus.java
Patch:
@@ -25,9 +25,9 @@
 
 import java.util.Map;
 import java.util.HashMap;
-import org.apache.thrift7.TEnum;
+import org.apache.thrift.TEnum;
 
-public enum TopologyInitialStatus implements org.apache.thrift7.TEnum {
+public enum TopologyInitialStatus implements org.apache.thrift.TEnum {
   ACTIVE(1),
   INACTIVE(2);
 

File: storm-core/src/jvm/backtype/storm/messaging/netty/Server.java
Patch:
@@ -123,7 +123,7 @@ protected void closeChannel(Channel channel) {
      * close all channels, and release resources
      */
     public synchronized void close() {
-        if (allChannels != null) {  
+        if (allChannels != null) {
             allChannels.close().awaitUninterruptibly();
             factory.releaseExternalResources();
             allChannels = null;

File: storm-core/src/jvm/backtype/storm/security/auth/ThriftServer.java
Patch:
@@ -19,8 +19,8 @@
 
 import java.util.Map;
 import javax.security.auth.login.Configuration;
-import org.apache.thrift7.TProcessor;
-import org.apache.thrift7.server.TServer;
+import org.apache.thrift.TProcessor;
+import org.apache.thrift.server.TServer;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import backtype.storm.utils.Utils;

File: storm-core/src/jvm/backtype/storm/utils/NimbusClient.java
Patch:
@@ -21,7 +21,7 @@
 import backtype.storm.security.auth.ThriftClient;
 import backtype.storm.generated.Nimbus;
 import java.util.Map;
-import org.apache.thrift7.transport.TTransportException;
+import org.apache.thrift.transport.TTransportException;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 

File: storm-core/src/jvm/storm/trident/drpc/ReturnResultsReducer.java
Patch:
@@ -27,7 +27,7 @@
 import java.util.List;
 import java.util.Map;
 import org.apache.commons.lang.builder.ToStringBuilder;
-import org.apache.thrift7.TException;
+import org.apache.thrift.TException;
 import org.json.simple.JSONValue;
 import storm.trident.drpc.ReturnResultsReducer.ReturnResultsState;
 import storm.trident.operation.MultiReducer;

File: src/main/java/org/apache/storm/hdfs/bolt/format/DelimitedRecordFormat.java
Patch:
@@ -77,7 +77,7 @@ public byte[] format(Tuple tuple) {
         Fields fields = this.fields == null ? tuple.getFields() : this.fields;
         int size = fields.size();
         for(int i = 0; i < size; i++){
-            sb.append(tuple.getValue(i));
+            sb.append(tuple.getValueByField(fields.get(i)));
             if(i != size - 1){
                 sb.append(this.fieldDelimiter);
             }

File: src/test/java/org/apache/storm/hdfs/trident/TridentSequenceTopology.java
Patch:
@@ -40,7 +40,7 @@ public static StormTopology buildTopology(String hdfsUrl){
 
         HdfsState.Options seqOpts = new HdfsState.SequenceFileOptions()
                 .withFileNameFormat(fileNameFormat)
-                .withSequenceFormat(new DefaultSequenceFormat("key", "data"))
+                .withSequenceFormat(new DefaultSequenceFormat("key", "sentence"))
                 .withRotationPolicy(rotationPolicy)
                 .withFsUrl(hdfsUrl)
                 .addRotationAction(new MoveFileAction().toDestination("/dest2/"));

File: src/jvm/storm/kafka/KafkaConfig.java
Patch:
@@ -18,6 +18,7 @@ public class KafkaConfig implements Serializable {
     public boolean forceFromStart = false;
     public long startOffsetTime = kafka.api.OffsetRequest.EarliestTime();
     public boolean useStartOffsetTimeIfOffsetOutOfRange = true;
+    public int metricsTimeBucketSizeInSecs = 60;
 
     public KafkaConfig(BrokerHosts hosts, String topic) {
         this(hosts, topic, kafka.api.OffsetRequest.DefaultClientId());

File: src/jvm/storm/kafka/trident/TridentKafkaEmitter.java
Patch:
@@ -49,9 +49,9 @@ public TridentKafkaEmitter(Map conf, TopologyContext context, TridentKafkaConfig
         _connections = new DynamicPartitionConnections(_config, KafkaUtils.makeBrokerReader(conf, _config));
         _topologyName = (String) conf.get(Config.TOPOLOGY_NAME);
         _kafkaOffsetMetric = new KafkaUtils.KafkaOffsetMetric(_config.topic, _connections);
-        context.registerMetric("kafkaOffset", _kafkaOffsetMetric, 60);
-        _kafkaMeanFetchLatencyMetric = context.registerMetric("kafkaFetchAvg", new MeanReducer(), 60);
-        _kafkaMaxFetchLatencyMetric = context.registerMetric("kafkaFetchMax", new MaxMetric(), 60);
+        context.registerMetric("kafkaOffset", _kafkaOffsetMetric, _config.metricsTimeBucketSizeInSecs);
+        _kafkaMeanFetchLatencyMetric = context.registerMetric("kafkaFetchAvg", new MeanReducer(), _config.metricsTimeBucketSizeInSecs);
+        _kafkaMaxFetchLatencyMetric = context.registerMetric("kafkaFetchMax", new MaxMetric(), _config.metricsTimeBucketSizeInSecs);
     }
 
 

File: src/jvm/storm/kafka/KafkaConfig.java
Patch:
@@ -18,6 +18,7 @@ public class KafkaConfig implements Serializable {
     public boolean forceFromStart = false;
     public long startOffsetTime = kafka.api.OffsetRequest.EarliestTime();
     public boolean useStartOffsetTimeIfOffsetOutOfRange = true;
+    public int metricsTimeBucketSizeInSecs = 60;
 
     public KafkaConfig(BrokerHosts hosts, String topic) {
         this(hosts, topic, kafka.api.OffsetRequest.DefaultClientId());

File: src/jvm/storm/kafka/trident/TridentKafkaEmitter.java
Patch:
@@ -49,9 +49,9 @@ public TridentKafkaEmitter(Map conf, TopologyContext context, TridentKafkaConfig
         _connections = new DynamicPartitionConnections(_config, KafkaUtils.makeBrokerReader(conf, _config));
         _topologyName = (String) conf.get(Config.TOPOLOGY_NAME);
         _kafkaOffsetMetric = new KafkaUtils.KafkaOffsetMetric(_config.topic, _connections);
-        context.registerMetric("kafkaOffset", _kafkaOffsetMetric, 60);
-        _kafkaMeanFetchLatencyMetric = context.registerMetric("kafkaFetchAvg", new MeanReducer(), 60);
-        _kafkaMaxFetchLatencyMetric = context.registerMetric("kafkaFetchMax", new MaxMetric(), 60);
+        context.registerMetric("kafkaOffset", _kafkaOffsetMetric, _config.metricsTimeBucketSizeInSecs);
+        _kafkaMeanFetchLatencyMetric = context.registerMetric("kafkaFetchAvg", new MeanReducer(), _config.metricsTimeBucketSizeInSecs);
+        _kafkaMaxFetchLatencyMetric = context.registerMetric("kafkaFetchMax", new MaxMetric(), _config.metricsTimeBucketSizeInSecs);
     }
 
 

File: src/jvm/storm/kafka/KafkaError.java
Patch:
@@ -17,10 +17,11 @@ public enum KafkaError {
     REPLICA_NOT_AVAILABLE,
     MESSAGE_SIZE_TOO_LARGE,
     STALE_CONTROLLER_EPOCH,
+    OFFSET_METADATA_TOO_LARGE,
     UNKNOWN;
 
-    public static KafkaError getError(short errorCode) {
-        if (errorCode < 0) {
+    public static KafkaError getError(int errorCode) {
+        if (errorCode < 0 || errorCode >= UNKNOWN.ordinal()) {
             return UNKNOWN;
         } else {
             return values()[errorCode];

File: src/jvm/storm/kafka/trident/Coordinator.java
Patch:
@@ -1,5 +1,6 @@
 package storm.kafka.trident;
 
+import storm.kafka.KafkaUtils;
 import storm.trident.spout.IOpaquePartitionedTridentSpout;
 import storm.trident.spout.IPartitionedTridentSpout;
 

File: storm-core/src/jvm/backtype/storm/Config.java
Patch:
@@ -556,7 +556,7 @@ public class Config extends HashMap<String, Object> {
      * See Kryo's documentation for more information about writing custom serializers.
      */
     public static final String TOPOLOGY_KRYO_REGISTER = "topology.kryo.register";
-    public static final Object TOPOLOGY_KRYO_REGISTER_SCHEMA = ConfigValidation.StringsValidator;
+    public static final Object TOPOLOGY_KRYO_REGISTER_SCHEMA = ConfigValidation.KryoRegValidator;
 
     /**
      * A list of classes that customize storm's kryo instance during start-up.

File: storm-core/src/jvm/backtype/storm/messaging/netty/Context.java
Patch:
@@ -17,12 +17,12 @@
  */
 package backtype.storm.messaging.netty;
 
-import java.util.Map;
-import java.util.Vector;
-
 import backtype.storm.messaging.IConnection;
 import backtype.storm.messaging.IContext;
 
+import java.util.Map;
+import java.util.Vector;
+
 public class Context implements IContext {
     @SuppressWarnings("rawtypes")
     private Map storm_conf;

File: storm-core/src/jvm/backtype/storm/messaging/netty/ControlMessage.java
Patch:
@@ -20,7 +20,6 @@
 import org.jboss.netty.buffer.ChannelBuffer;
 import org.jboss.netty.buffer.ChannelBufferOutputStream;
 import org.jboss.netty.buffer.ChannelBuffers;
-import org.jboss.netty.channel.Channel;
 
 enum ControlMessage {
     CLOSE_MESSAGE((short)-100),

File: storm-core/src/jvm/backtype/storm/messaging/netty/MessageDecoder.java
Patch:
@@ -17,11 +17,11 @@
  */
 package backtype.storm.messaging.netty;
 
+import backtype.storm.messaging.TaskMessage;
 import org.jboss.netty.buffer.ChannelBuffer;
 import org.jboss.netty.channel.Channel;
 import org.jboss.netty.channel.ChannelHandlerContext;
 import org.jboss.netty.handler.codec.frame.FrameDecoder;
-import backtype.storm.messaging.TaskMessage;
 
 public class MessageDecoder extends FrameDecoder {    
     /*

File: storm-core/src/jvm/backtype/storm/spout/ShellSpout.java
Patch:
@@ -94,6 +94,8 @@ private void querySubprocess() {
                         _collector.emitDirect((int) task.longValue(), stream,
                                 tuple, messageId);
                     }
+                } else {
+                    throw new RuntimeException("Unknown command received: " + command);
                 }
             }
         } catch (IOException e) {

File: storm-core/src/jvm/backtype/storm/clojure/RichShellBolt.java
Patch:
@@ -1,7 +1,7 @@
 package backtype.storm.clojure;
 
 import backtype.storm.generated.StreamInfo;
-import backtype.storm.task.ShellBolt;
+import backtype.storm.multilang.ShellBolt;
 import backtype.storm.topology.IRichBolt;
 import backtype.storm.topology.OutputFieldsDeclarer;
 import backtype.storm.tuple.Fields;

File: storm-core/src/jvm/backtype/storm/clojure/RichShellSpout.java
Patch:
@@ -1,7 +1,7 @@
 package backtype.storm.clojure;
 
 import backtype.storm.generated.StreamInfo;
-import backtype.storm.spout.ShellSpout;
+import backtype.storm.multilang.ShellSpout;
 import backtype.storm.topology.IRichSpout;
 import backtype.storm.topology.OutputFieldsDeclarer;
 import backtype.storm.tuple.Fields;

File: storm-core/src/jvm/backtype/storm/metric/SystemBolt.java
Patch:
@@ -74,7 +74,7 @@ public Object getValueAndReset() {
 
     @Override
     public void prepare(final Map stormConf, TopologyContext context, OutputCollector collector) {
-        if(_prepareWasCalled && stormConf.get(Config.STORM_CLUSTER_MODE) != "local") {
+        if(_prepareWasCalled && !"local".equals(stormConf.get(Config.STORM_CLUSTER_MODE))) {
             throw new RuntimeException("A single worker should have 1 SystemBolt instance.");
         }
         _prepareWasCalled = true;

File: src/jvm/storm/starter/bolt/RollingCountBolt.java
Patch:
@@ -80,7 +80,7 @@ public void prepare(Map stormConf, TopologyContext context, OutputCollector coll
     @Override
     public void execute(Tuple tuple) {
         if (TupleHelpers.isTickTuple(tuple)) {
-            LOG.info("Received tick tuple, triggering emit of current window counts");
+            LOG.debug("Received tick tuple, triggering emit of current window counts");
             emitCurrentWindowCounts();
         }
         else {

File: src/jvm/storm/starter/bolt/AbstractRankerBolt.java
Patch:
@@ -64,7 +64,7 @@ protected Rankings getRankings() {
     @Override
     public final void execute(Tuple tuple, BasicOutputCollector collector) {
         if (TupleHelpers.isTickTuple(tuple)) {
-            getLogger().info("Received tick tuple, triggering emit of current rankings");
+            getLogger().debug("Received tick tuple, triggering emit of current rankings");
             emitRankings(collector);
         }
         else {
@@ -76,7 +76,7 @@ public final void execute(Tuple tuple, BasicOutputCollector collector) {
 
     private void emitRankings(BasicOutputCollector collector) {
         collector.emit(new Values(rankings));
-        getLogger().info("Rankings: " + rankings);
+        getLogger().debug("Rankings: " + rankings);
     }
 
     @Override

File: src/jvm/storm/starter/tools/RankableObjectWithFields.java
Patch:
@@ -1,5 +1,6 @@
 package storm.starter.tools;
 
+import java.io.Serializable;
 import java.util.List;
 
 import backtype.storm.tuple.Tuple;
@@ -13,8 +14,9 @@
  * This class can be used, for instance, to track the number of occurrences of an object in a Storm topology.
  * 
  */
-public class RankableObjectWithFields implements Rankable {
+public class RankableObjectWithFields implements Rankable, Serializable {
 
+    private static final long serialVersionUID = -9102878650001058090L;
     private static final String toStringSeparator = "|";
 
     private final Object obj;

File: src/jvm/storm/starter/tools/RankableObjectWithFields.java
Patch:
@@ -82,7 +82,7 @@ else if (delta < 0) {
 
     @Override
     public boolean equals(Object o) {
-        if (this == obj) {
+        if (this == o) {
             return true;
         }
         if (!(o instanceof RankableObjectWithFields)) {

File: storm-core/src/jvm/backtype/storm/utils/Utils.java
Patch:
@@ -23,6 +23,7 @@
 import java.util.ArrayList;
 import java.util.Enumeration;
 import java.util.HashMap;
+import java.util.HashSet;
 import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
@@ -106,7 +107,7 @@ public static List<URL> findResources(String name) {
 
     public static Map findAndReadConfigFile(String name, boolean mustExist) {
         try {
-            List<URL> resources = findResources(name);
+            HashSet<URL> resources = new HashSet<URL>(findResources(name));
             if(resources.isEmpty()) {
                 if(mustExist) throw new RuntimeException("Could not find config file on classpath " + name);
                 else return new HashMap();
@@ -115,7 +116,7 @@ public static Map findAndReadConfigFile(String name, boolean mustExist) {
                 throw new RuntimeException("Found multiple " + name + " resources. You're probably bundling the Storm jars with your topology jar. "
                   + resources);
             }
-            URL resource = resources.get(0);
+            URL resource = resources.iterator().next();
             Yaml yaml = new Yaml();
             Map ret = (Map) yaml.load(new InputStreamReader(resource.openStream()));
             if(ret==null) ret = new HashMap();

File: storm-core/src/jvm/backtype/storm/utils/Utils.java
Patch:
@@ -23,6 +23,7 @@
 import java.util.ArrayList;
 import java.util.Enumeration;
 import java.util.HashMap;
+import java.util.HashSet;
 import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
@@ -106,7 +107,7 @@ public static List<URL> findResources(String name) {
 
     public static Map findAndReadConfigFile(String name, boolean mustExist) {
         try {
-            List<URL> resources = findResources(name);
+            HashSet<URL> resources = new HashSet<URL>(findResources(name));
             if(resources.isEmpty()) {
                 if(mustExist) throw new RuntimeException("Could not find config file on classpath " + name);
                 else return new HashMap();
@@ -115,7 +116,7 @@ public static Map findAndReadConfigFile(String name, boolean mustExist) {
                 throw new RuntimeException("Found multiple " + name + " resources. You're probably bundling the Storm jars with your topology jar. "
                   + resources);
             }
-            URL resource = resources.get(0);
+            URL resource = resources.iterator().next();
             Yaml yaml = new Yaml();
             Map ret = (Map) yaml.load(new InputStreamReader(resource.openStream()));
             if(ret==null) ret = new HashMap();

File: storm-core/src/jvm/backtype/storm/Config.java
Patch:
@@ -777,7 +777,7 @@ public class Config extends HashMap<String, Object> {
      * to backtype.storm.scheduler.IsolationScheduler to make use of the isolation scheduler.
      */
     public static final String ISOLATION_SCHEDULER_MACHINES = "isolation.scheduler.machines";
-    public static final Object ISOLATION_SCHEDULER_MACHINES_SCHEMA = Number.class;
+    public static final Object ISOLATION_SCHEDULER_MACHINES_SCHEMA = Map.class;
 
     public static void setDebug(Map conf, boolean isOn) {
         conf.put(Config.TOPOLOGY_DEBUG, isOn);

File: storm-core/src/jvm/backtype/storm/Config.java
Patch:
@@ -777,7 +777,7 @@ public class Config extends HashMap<String, Object> {
      * to backtype.storm.scheduler.IsolationScheduler to make use of the isolation scheduler.
      */
     public static final String ISOLATION_SCHEDULER_MACHINES = "isolation.scheduler.machines";
-    public static final Object ISOLATION_SCHEDULER_MACHINES_SCHEMA = Number.class;
+    public static final Object ISOLATION_SCHEDULER_MACHINES_SCHEMA = Map.class;
 
     public static void setDebug(Map conf, boolean isOn) {
         conf.put(Config.TOPOLOGY_DEBUG, isOn);

File: src/jvm/storm/kafka/PartitionManager.java
Patch:
@@ -151,7 +151,7 @@ private void fill() {
         for(MessageAndOffset msg: msgs) {
             _pending.add(_emittedToOffset);
             _waitingToEmit.add(new MessageAndRealOffset(msg.message(), _emittedToOffset));
-            _emittedToOffset = msg.offset();
+            _emittedToOffset = msg.nextOffset();
         }
         if(numMessages>0) {
           LOG.info("Added " + numMessages + " messages from Kafka: " + _consumer.host() + ":" + _partition.partition + " to internal buffers");

File: storm-core/src/jvm/backtype/storm/nimbus/DefaultTopologyValidator.java
Patch:
@@ -9,6 +9,6 @@ public class DefaultTopologyValidator implements ITopologyValidator {
     public void prepare(Map StormConf){
     }
     @Override
-    public void validate(String topologyName, Map topologyConf, StormTopology topology, Map NimbusConf) throws InvalidTopologyException {        
+    public void validate(String topologyName, Map topologyConf, StormTopology topology) throws InvalidTopologyException {        
     }    
 }

File: storm-core/src/jvm/backtype/storm/nimbus/DefaultTopologyValidator.java
Patch:
@@ -5,6 +5,9 @@
 import java.util.Map;
 
 public class DefaultTopologyValidator implements ITopologyValidator {
+    @Override
+    public void prepare(Map StormConf){
+    }
     @Override
     public void validate(String topologyName, Map topologyConf, StormTopology topology, Map NimbusConf) throws InvalidTopologyException {        
     }    

File: storm-core/src/jvm/backtype/storm/Config.java
Patch:
@@ -554,7 +554,7 @@ public class Config extends HashMap<String, Object> {
      * Each listed class maps 1:1 to a system bolt named __metrics_ClassName#N, and it's parallelism is configurable.
      */
     public static final String TOPOLOGY_METRICS_CONSUMER_REGISTER = "topology.metrics.consumer.register";
-    public static final Object TOPOLOGY_METRICS_CONSUMER_REGISTER_SCHEMA = ConfigValidation.StringsValidator;
+    public static final Object TOPOLOGY_METRICS_CONSUMER_REGISTER_SCHEMA = ConfigValidation.MapsValidator;
 
 
     /**

File: storm-core/src/jvm/backtype/storm/Config.java
Patch:
@@ -554,7 +554,7 @@ public class Config extends HashMap<String, Object> {
      * Each listed class maps 1:1 to a system bolt named __metrics_ClassName#N, and it's parallelism is configurable.
      */
     public static final String TOPOLOGY_METRICS_CONSUMER_REGISTER = "topology.metrics.consumer.register";
-    public static final Object TOPOLOGY_METRICS_CONSUMER_REGISTER_SCHEMA = ConfigValidation.StringsValidator;
+    public static final Object TOPOLOGY_METRICS_CONSUMER_REGISTER_SCHEMA = ConfigValidation.MapsValidator;
 
 
     /**

File: storm-core/src/jvm/backtype/storm/nimbus/DefaultTopologyValidator.java
Patch:
@@ -6,6 +6,6 @@
 
 public class DefaultTopologyValidator implements ITopologyValidator {
     @Override
-    public void validate(String topologyName, Map topologyConf, StormTopology topology) throws InvalidTopologyException {        
+    public void validate(String topologyName, Map topologyConf, StormTopology topology, Map NimbusConf) throws InvalidTopologyException {        
     }    
 }

File: storm-core/src/jvm/storm/trident/testing/LRUMemoryMapState.java
Patch:
@@ -72,7 +72,7 @@ public Factory(int maxSize) {
 
         @Override
         public State makeState(Map conf, IMetricsContext metrics, int partitionIndex, int numPartitions) {
-            return new LRUMemoryMapState(_maxSize, _id);
+            return new LRUMemoryMapState(_maxSize, _id + partitionIndex);
         }
     }
 

File: storm-core/src/jvm/storm/trident/testing/MemoryMapState.java
Patch:
@@ -69,7 +69,7 @@ public Factory() {
 
         @Override
         public State makeState(Map conf, IMetricsContext metrics, int partitionIndex, int numPartitions) {
-            return new MemoryMapState(_id);
+            return new MemoryMapState(_id + partitionIndex);
         }
     }
 

File: storm-netty/src/jvm/backtype/storm/messaging/netty/ControlMessage.java
Patch:
@@ -31,11 +31,10 @@ static ControlMessage mkMessage(short encoded) {
 
     /**
      * encode the current Control Message into a channel buffer
-     * @param bout
      * @throws Exception
      */
     ChannelBuffer buffer() throws Exception {
-        ChannelBufferOutputStream bout = new ChannelBufferOutputStream(ChannelBuffers.dynamicBuffer());      
+        ChannelBufferOutputStream bout = new ChannelBufferOutputStream(ChannelBuffers.buffer(2));      
         write(bout);
         bout.close();
         return bout.buffer();

File: storm-netty/src/jvm/backtype/storm/messaging/netty/Client.java
Patch:
@@ -84,7 +84,7 @@ void reconnect() {
                 close();
             }
         } catch (InterruptedException e) {
-            LOG.info("connection failed", e);
+            LOG.warn("connection failed", e);
         } 
     }
 
@@ -157,7 +157,7 @@ public void close() {
             @Override
             public void run() {
                 if (ready_to_release_resource.get()) {
-                    LOG.info("client resource released");
+                    LOG.debug("client resource released");
                     factory.releaseExternalResources();
                     timer.cancel();
                 }

File: storm-netty/src/jvm/backtype/storm/messaging/netty/MessageDecoder.java
Patch:
@@ -9,8 +9,6 @@
 import backtype.storm.messaging.TaskMessage;
 
 public class MessageDecoder extends FrameDecoder {    
-    private static final Logger LOG = LoggerFactory.getLogger(MessageDecoder.class);
-
     /*
      * Each ControlMessage is encoded as:
      *  code (<0) ... short(2)

File: src/jvm/storm/kafka/DynamicPartitionConnections.java
Patch:
@@ -30,7 +30,7 @@ public SimpleConsumer register(GlobalPartitionId id) {
     
     public SimpleConsumer register(HostPort host, int partition) {
         if(!_connections.containsKey(host)) {
-            _connections.put(host, new ConnectionInfo(new SimpleConsumer(host.host, host.port, _config.socketTimeoutMs, _config.bufferSizeBytes)));
+            _connections.put(host, new ConnectionInfo(new SimpleConsumer(host.host, host.port, _config.socketTimeoutMs, _config.bufferSizeBytes, kafka.api.OffsetRequest.DefaultClientId())));
         }
         ConnectionInfo info = _connections.get(host);
         info.partitions.add(partition);

File: src/jvm/storm/kafka/StaticPartitionConnections.java
Patch:
@@ -23,7 +23,7 @@ public SimpleConsumer getConsumer(int partition) {
         int hostIndex = partition / hosts.partitionsPerHost;
         if(!_kafka.containsKey(hostIndex)) {
             HostPort hp = hosts.hosts.get(hostIndex);
-            _kafka.put(hostIndex, new SimpleConsumer(hp.host, hp.port, _config.socketTimeoutMs, _config.bufferSizeBytes));
+            _kafka.put(hostIndex, new SimpleConsumer(hp.host, hp.port, _config.socketTimeoutMs, _config.bufferSizeBytes, kafka.api.OffsetRequest.DefaultClientId()));
 
         }
         return _kafka.get(hostIndex);

File: src/jvm/storm/kafka/trident/TransactionalTridentKafkaSpout.java
Patch:
@@ -11,6 +11,7 @@
 import java.util.*;
 
 import kafka.api.FetchRequest;
+import kafka.api.FetchRequestBuilder;
 import kafka.api.OffsetRequest;
 import kafka.javaapi.consumer.SimpleConsumer;
 import kafka.javaapi.message.ByteBufferMessageSet;
@@ -91,7 +92,7 @@ public void emitPartitionBatch(TransactionAttempt attempt, TridentCollector coll
                 long offset = (Long) meta.get("offset");
                 long nextOffset = (Long) meta.get("nextOffset");
                 long start = System.nanoTime();
-                ByteBufferMessageSet msgs = consumer.fetch(new FetchRequest(_config.topic, partition.partition, offset, _config.fetchSizeBytes));
+                ByteBufferMessageSet msgs = consumer.fetch(new FetchRequestBuilder().addFetch(_config.topic, partition.partition, offset, _config.fetchSizeBytes).build()).messageSet(_config.topic, partition.partition);
                 long end = System.nanoTime();
                 long millis = (end - start) / 1000000;
                 _kafkaMeanFetchLatencyMetric.update(millis);
@@ -103,7 +104,7 @@ public void emitPartitionBatch(TransactionAttempt attempt, TridentCollector coll
                         throw new RuntimeException("Error when re-emitting batch. overshot the end offset");
                     }
                     KafkaUtils.emit(_config, collector, msg.message());
-                    offset = msg.offset();
+                    offset = msg.nextOffset();
                 }        
             }
         }

File: storm-netty/src/jvm/backtype/storm/messaging/netty/StormClientPipelineFactory.java
Patch:
@@ -5,7 +5,7 @@
 import org.jboss.netty.channel.ChannelPipelineFactory;
 import org.jboss.netty.channel.Channels;
 
-public class StormClientPipelineFactory implements ChannelPipelineFactory {
+class StormClientPipelineFactory implements ChannelPipelineFactory {
     private Client client;
     @SuppressWarnings("rawtypes")
     private Map conf;
@@ -16,7 +16,6 @@ public class StormClientPipelineFactory implements ChannelPipelineFactory {
         this.conf = conf;
     }
 
-    @Override
     public ChannelPipeline getPipeline() throws Exception {
         // Create a default pipeline implementation.
         ChannelPipeline pipeline = Channels.pipeline();

File: storm-netty/src/jvm/backtype/storm/messaging/netty/StormServerPipelineFactory.java
Patch:
@@ -18,7 +18,6 @@ class StormServerPipelineFactory implements  ChannelPipelineFactory {
         this.conf = conf;
     }
     
-    @Override
     public ChannelPipeline getPipeline() throws Exception {
         // Create a default pipeline implementation.
         ChannelPipeline pipeline = Channels.pipeline();

File: storm-netty/src/jvm/backtype/storm/messaging/netty/TaskMessageDecoder.java
Patch:
@@ -7,7 +7,7 @@
 
 import backtype.storm.messaging.TaskMessage;
 
-class TaskMessageDecoder extends FrameDecoder {    
+public class TaskMessageDecoder extends FrameDecoder {    
     /*
      * Each TaskMessage is encoded as:
      *  task ... short(2)

File: storm-netty/src/jvm/backtype/storm/messaging/netty/TaskMessageEncoder.java
Patch:
@@ -13,7 +13,7 @@
 import backtype.storm.messaging.TaskMessage;
 import backtype.storm.utils.Utils;
 
-class TaskMessageEncoder extends OneToOneEncoder {
+public class TaskMessageEncoder extends OneToOneEncoder {
     int estimated_buffer_size;
     
     @SuppressWarnings("rawtypes")

File: storm-netty/src/jvm/backtype/storm/messaging/netty/Util.java
Patch:
@@ -2,7 +2,7 @@
 
 import backtype.storm.messaging.TaskMessage;
 
-public class Util {
+class Util {
     static final int OK = -200; //HTTP status: 200
     static final int FAILURE = -400; //HTTP status: 400 BAD REQUEST
     static final int CLOSE = -410; //HTTP status: 410 GONE

File: src/jvm/backtype/storm/messaging/zmq/TransportPlugin.java
Patch:
@@ -12,8 +12,8 @@
 import org.zeromq.ZMQ.Context;
 import org.zeromq.ZMQ.Socket;
 
-public class TranportPlugin implements IContext {
-    public static final Logger LOG = LoggerFactory.getLogger(TranportPlugin.class);
+public class TransportPlugin implements IContext {
+    public static final Logger LOG = LoggerFactory.getLogger(TransportPlugin.class);
 
     private Context context; 
     private long linger_ms, hwm;

File: src/jvm/backtype/storm/utils/ShellProcess.java
Patch:
@@ -113,6 +113,8 @@ private String readString() throws IOException {
                     else {
                         errorMessage.append(" Currently read output: " + line.toString() + "\n");
                     }
+                    errorMessage.append("Shell Process Exception:\n");
+                    errorMessage.append(getErrorsString() + "\n");
                     throw new RuntimeException(errorMessage.toString());
                 }
                 if(subline.equals("end")) {

File: src/jvm/backtype/storm/task/ShellBolt.java
Patch:
@@ -124,6 +124,8 @@ public void run() {
                         if (write != null) {
                             _process.writeMessage(write);
                         }
+                        // drain the error stream to avoid dead lock because of full error stream buffer
+                        _process.drainErrorStream();
                     } catch (InterruptedException e) {
                     } catch (Throwable t) {
                         die(t);

File: src/jvm/backtype/storm/Constants.java
Patch:
@@ -1,11 +1,14 @@
 package backtype.storm;
 
 import backtype.storm.coordination.CoordinatedBolt;
+import clojure.lang.RT;
 
 
 public class Constants {
     public static final String COORDINATED_STREAM_ID = CoordinatedBolt.class.getName() + "/coord-stream"; 
 
+    public static final long SYSTEM_TASK_ID = -1;
+    public static final Object SYSTEM_EXECUTOR_ID = RT.readString("[-1 -1]");
     public static final String SYSTEM_COMPONENT_ID = "__system";
     public static final String SYSTEM_TICK_STREAM_ID = "__tick";
     public static final String METRICS_COMPONENT_ID_PREFIX = "__metrics";

File: src/jvm/backtype/storm/metric/api/CountMetric.java
Patch:
@@ -2,7 +2,7 @@
 
 import backtype.storm.metric.api.IMetric;
 
-public class CountMetric implements IMetric, java.io.Serializable {
+public class CountMetric implements IMetric {
     long _value = 0;
 
     public CountMetric() {

File: src/jvm/backtype/storm/metric/api/MultiCountMetric.java
Patch:
@@ -4,7 +4,7 @@
 import java.util.HashMap;
 import java.util.Map;
 
-public class MultiCountMetric implements IMetric, java.io.Serializable {
+public class MultiCountMetric implements IMetric {
     Map<String, CountMetric> _value = new HashMap();
 
     public MultiCountMetric() {

File: src/jvm/backtype/storm/task/GeneralTopologyContext.java
Patch:
@@ -63,7 +63,7 @@ public StormTopology getRawTopology() {
      * @return the component id for the input task id
      */
     public String getComponentId(int taskId) {
-        if(taskId==-1) {
+        if(taskId==Constants.SYSTEM_TASK_ID) {
             return Constants.SYSTEM_COMPONENT_ID;
         } else {
             return _taskToComponent.get(taskId);

File: src/jvm/backtype/storm/utils/Utils.java
Patch:
@@ -112,7 +112,7 @@ public static Map findAndReadConfigFile(String name, boolean mustExist) {
                 else return new HashMap();
             }
             if(resources.size() > 1) {
-                throw new RuntimeException("Found multiple " + name + " resources. You're probably bundling the Storm jars with your topology jar. ");
+                throw new RuntimeException("Found multiple " + name + " resources. You're probably bundling the Storm jars with your topology jar.");
             }
             URL resource = resources.get(0);
             Yaml yaml = new Yaml();

File: src/jvm/backtype/storm/utils/Utils.java
Patch:
@@ -112,8 +112,7 @@ public static Map findAndReadConfigFile(String name, boolean mustExist) {
                 else return new HashMap();
             }
             if(resources.size() > 1) {
-                throw new RuntimeException("Found multiple " + name + " resources. You're probably bundling the Storm jars with your topology jar. "
-                  + resources);
+                throw new RuntimeException("Found multiple " + name + " resources. You're probably bundling the Storm jars with your topology jar. ");
             }
             URL resource = resources.get(0);
             Yaml yaml = new Yaml();

File: src/jvm/backtype/storm/security/auth/ITransportPlugin.java
Patch:
@@ -1,7 +1,6 @@
 package backtype.storm.security.auth;
 
 import java.io.IOException;
-
 import org.apache.thrift7.TProcessor;
 import org.apache.thrift7.server.TServer;
 import org.apache.thrift7.transport.TTransport;
@@ -15,13 +14,13 @@
  */
 public interface ITransportPlugin {
     /**
-     * Create a server for server to use
+     * Create a server associated with a given port and service handler
      * @param port listening port
      * @param processor service handler
      * @return server to be binded
      */
     public TServer getServer(int port, TProcessor processor) throws IOException, TTransportException;
-    
+
     /**
      * Connect to the specified server via framed transport 
      * @param transport The underlying Thrift transport.

File: test/jvm/storm/starter/tools/SlotBasedCounterTest.java
Patch:
@@ -142,7 +142,7 @@ public void wipeSlotShouldSetAllCountsInSlotToZero() {
     }
 
     @Test
-    public void wipeSlotShouldRemoveAnyObjectsWithZeroTotalCount() {
+    public void wipeZerosShouldRemoveAnyObjectsWithZeroTotalCount() {
         // given
         SlotBasedCounter<Object> counter = new SlotBasedCounter<Object>(2);
         int wipeSlot = 0;
@@ -155,6 +155,7 @@ public void wipeSlotShouldRemoveAnyObjectsWithZeroTotalCount() {
 
         // when
         counter.wipeSlot(wipeSlot);
+        counter.wipeZeros();
 
         // then
         assertThat(counter.getCounts()).doesNotContainKey(willBeRemoved);

File: src/jvm/storm/kafka/trident/MaxMetric.java
Patch:
@@ -12,6 +12,8 @@ public Long identity() {
 
     @Override
     public Long combine(Long l1, Long l2) {
+        if(l1 == null) return l2;
+        if(l2 == null) return l1;
         return Math.max(l1, l2);
     }
 

File: src/jvm/backtype/storm/utils/Utils.java
Patch:
@@ -112,7 +112,8 @@ public static Map findAndReadConfigFile(String name, boolean mustExist) {
                 else return new HashMap();
             }
             if(resources.size() > 1) {
-                throw new RuntimeException("Found multiple " + name + " resources. You're probably bundling the Storm jars with your topology jar.");
+                throw new RuntimeException("Found multiple " + name + " resources. You're probably bundling the Storm jars with your topology jar. "
+                  + resources);
             }
             URL resource = resources.get(0);
             Yaml yaml = new Yaml();

File: src/jvm/storm/kafka/trident/KafkaUtils.java
Patch:
@@ -102,13 +102,12 @@ public static void emit(TridentKafkaConfig config, TridentCollector collector, M
 
 
     public static class KafkaOffsetMetric implements IMetric {
-        Map<GlobalPartitionId, Long> _partitionToOffset;
+        Map<GlobalPartitionId, Long> _partitionToOffset = new HashMap<GlobalPartitionId, Long>();
         Set<GlobalPartitionId> _partitions;
         String _topic;
         DynamicPartitionConnections _connections;
 
         public KafkaOffsetMetric(String topic, DynamicPartitionConnections connections) {
-            _partitionToOffset = new HashMap<GlobalPartitionId, Long>();
             _topic = topic;
             _connections = connections;
         }
@@ -124,7 +123,7 @@ public Object getValueAndReset() {
                 long totalLatestTimeOffset = 0;
                 long totalLatestEmittedOffset = 0;
                 HashMap ret = new HashMap();
-                if(_partitions.size() == _partitionToOffset.size()) {
+                if(_partitions != null && _partitions.size() == _partitionToOffset.size()) {
                     for(Map.Entry<GlobalPartitionId, Long> e : _partitionToOffset.entrySet()) {
                         GlobalPartitionId partition = e.getKey();
                         SimpleConsumer consumer = _connections.getConnection(partition);

File: test/jvm/storm/starter/tools/SlotBasedCounterTest.java
Patch:
@@ -142,7 +142,7 @@ public void wipeSlotShouldSetAllCountsInSlotToZero() {
     }
 
     @Test
-    public void wipeSlotShouldRemoveAnyObjectsWithZeroTotalCount() {
+    public void wipeZerosShouldRemoveAnyObjectsWithZeroTotalCount() {
         // given
         SlotBasedCounter<Object> counter = new SlotBasedCounter<Object>(2);
         int wipeSlot = 0;
@@ -155,6 +155,7 @@ public void wipeSlotShouldRemoveAnyObjectsWithZeroTotalCount() {
 
         // when
         counter.wipeSlot(wipeSlot);
+        counter.wipeZeros();
 
         // then
         assertThat(counter.getCounts()).doesNotContainKey(willBeRemoved);

File: src/jvm/storm/kafka/trident/KafkaUtils.java
Patch:
@@ -161,8 +161,9 @@ public Object getValueAndReset() {
 
        public void refreshPartitions(Set<GlobalPartitionId> partitions) {
            _partitions = partitions;
-           for(GlobalPartitionId p : _partitionToOffset.keySet()) {
-              if(!partitions.contains(p)) _partitionToOffset.remove(p);
+           Iterator<GlobalPartitionId> it = _partitionToOffset.keySet().iterator();
+           while(it.hasNext()) {
+               if(!partitions.contains(it.next())) it.remove();
            }
        }
     };

File: src/jvm/storm/kafka/trident/OpaqueTridentKafkaSpout.java
Patch:
@@ -86,8 +86,9 @@ public Emitter(Map conf, TopologyContext context) {
         public Map emitPartitionBatch(TransactionAttempt attempt, TridentCollector collector, GlobalPartitionId partition, Map lastMeta) {
             try {
                 SimpleConsumer consumer = _connections.register(partition);
-                _kafkaOffsetMetric.setLatestEmittedOffset(partition, (Long)lastMeta.get("offset"));
-                return KafkaUtils.emitPartitionBatchNew(_config, consumer, partition, collector, lastMeta, _topologyInstanceId, _topologyName);
+                Map ret = KafkaUtils.emitPartitionBatchNew(_config, consumer, partition, collector, lastMeta, _topologyInstanceId, _topologyName);
+                _kafkaOffsetMetric.setLatestEmittedOffset(partition, (Long)ret.get("offset"));
+                return ret;
             } catch(FailedFetchException e) {
                 LOG.warn("Failed to fetch from partition " + partition);
                 if(lastMeta==null) {

File: src/jvm/storm/kafka/trident/TransactionalTridentKafkaSpout.java
Patch:
@@ -71,8 +71,9 @@ public Emitter(Map conf, TopologyContext context) {
         @Override
         public Map emitPartitionBatchNew(TransactionAttempt attempt, TridentCollector collector, GlobalPartitionId partition, Map lastMeta) {
             SimpleConsumer consumer = _connections.register(partition);
-            _kafkaOffsetMetric.setLatestEmittedOffset(partition, (Long)lastMeta.get("offset"));
-            return KafkaUtils.emitPartitionBatchNew(_config, consumer, partition, collector, lastMeta, _topologyInstanceId, _topologyName);
+            Map ret = KafkaUtils.emitPartitionBatchNew(_config, consumer, partition, collector, lastMeta, _topologyInstanceId, _topologyName);
+            _kafkaOffsetMetric.setLatestEmittedOffset(partition, (Long)ret.get("offset"));
+            return ret;
         }
 
         @Override

File: src/jvm/backtype/storm/spout/SchemeAsMultiScheme.java
Patch:
@@ -13,7 +13,9 @@ public SchemeAsMultiScheme(Scheme scheme) {
   }
 
   @Override public Iterable<List<Object>> deserialize(final byte[] ser) {
-    return Arrays.asList(scheme.deserialize(ser));
+    List<Object> o = scheme.deserialize(ser);
+    if(o == null) return null;
+    else return Arrays.asList(o);
   }
 
   @Override public Fields getOutputFields() {

File: src/jvm/backtype/storm/spout/SchemeAsMultiScheme.java
Patch:
@@ -13,7 +13,9 @@ public SchemeAsMultiScheme(Scheme scheme) {
   }
 
   @Override public Iterable<List<Object>> deserialize(final byte[] ser) {
-    return Arrays.asList(scheme.deserialize(ser));
+    List<Object> o = scheme.deserialize(ser);
+    if(o == null) return null;
+    else return Arrays.asList(o);
   }
 
   @Override public Fields getOutputFields() {

File: src/jvm/backtype/storm/task/ShellBolt.java
Patch:
@@ -123,6 +123,8 @@ public void run() {
                         if (write != null) {
                             _process.writeMessage(write);
                         }
+                        // drain the error stream to avoid dead lock because of full error stream buffer
+                        _process.drainErrorStream();
                     } catch (InterruptedException e) {
                     } catch (Throwable t) {
                         die(t);

File: src/jvm/storm/trident/operation/impl/GroupedAggregator.java
Patch:
@@ -56,6 +56,7 @@ public void aggregate(Object[] arr, TridentTuple tuple, TridentCollector collect
         } else {
             curr = val.get(group);
         }
+        groupColl.currGroup = group;
         _agg.aggregate(curr, input, groupColl);
         
     }

File: src/jvm/storm/starter/trident/TridentWordCount.java
Patch:
@@ -16,6 +16,7 @@
 import storm.trident.operation.builtin.FilterNull;
 import storm.trident.operation.builtin.MapGet;
 import storm.trident.operation.builtin.Sum;
+import storm.trident.planner.processor.StateQueryProcessor;
 import storm.trident.testing.MemoryMapState;
 import storm.trident.tuple.TridentTuple;
 
@@ -57,7 +58,6 @@ public static StormTopology buildTopology(LocalDRPC drpc) {
                 .each(new Fields("count"), new FilterNull())
                 .aggregate(new Fields("count"), new Sum(), new Fields("sum"))
                 ;
-        
         return topology.build();
     }
     

File: src/jvm/storm/kafka/KafkaConfig.java
Patch:
@@ -4,8 +4,8 @@
 import java.util.ArrayList;
 import java.util.List;
 
-import backtype.storm.spout.RawScheme;
-import backtype.storm.spout.Scheme;
+import backtype.storm.spout.MultiScheme;
+import backtype.storm.spout.RawMultiScheme;
 
 public class KafkaConfig implements Serializable {
     public static interface BrokerHosts extends Serializable {

File: src/jvm/storm/kafka/trident/KafkaUtils.java
Patch:
@@ -90,7 +90,8 @@ public static Map emitPartitionBatchNew(TridentKafkaConfig config, SimpleConsume
      }
 
      public static void emit(TridentKafkaConfig config, TridentCollector collector, Message msg) {
-         Iterator<List<Object>> values = config.scheme.deserialize(Utils.toByteArray(msg.payload()));
+         Iterable<List<Object>> values =
+             config.scheme.deserialize(Utils.toByteArray(msg.payload()));
          if(values!=null) {
              for(List<Object> value: values)
                  collector.emit(value);

File: src/jvm/backtype/storm/task/TopologyContext.java
Patch:
@@ -208,7 +208,7 @@ public Collection<ITaskHook> getHooks() {
      * You must call this during IBolt::prepare or ISpout::open.
      * @return The IMetric argument unchanged.
      */
-    public IMetric registerMetric(String name, IMetric metric, int timeBucketSizeInSecs) {
+    public <T extends IMetric> T registerMetric(String name, T metric, int timeBucketSizeInSecs) {
         if((Boolean)_openOrPrepareWasCalled.deref() == true) {
             throw new RuntimeException("TopologyContext.registerMetric can only be called from within overridden " + 
                                        "IBolt::prepare() or ISpout::open() method.");
@@ -237,13 +237,13 @@ public IMetric registerMetric(String name, IMetric metric, int timeBucketSizeInS
     /*
      * Convinience method for registering ReducedMetric.
      */
-    public IMetric registerMetric(String name, IReducer reducer, int timeBucketSizeInSecs) {
+    public ReducedMetric registerMetric(String name, IReducer reducer, int timeBucketSizeInSecs) {
         return registerMetric(name, new ReducedMetric(reducer), timeBucketSizeInSecs);
     }
     /*
      * Convinience method for registering CombinedMetric.
      */
-    public IMetric registerMetric(String name, ICombiner combiner, int timeBucketSizeInSecs) {
+    public CombinedMetric registerMetric(String name, ICombiner combiner, int timeBucketSizeInSecs) {
         return registerMetric(name, new CombinedMetric(combiner), timeBucketSizeInSecs);
     }
 }
\ No newline at end of file

File: src/jvm/storm/trident/operation/TridentOperationContext.java
Patch:
@@ -31,13 +31,13 @@ public int getPartitionIndex() {
         return _topoContext.getThisTaskIndex();
     }
 
-    public IMetric registerMetric(String name, IMetric metric, int timeBucketSizeInSecs) {
+    public <T extends IMetric> T registerMetric(String name, T metric, int timeBucketSizeInSecs) {
         return _topoContext.registerMetric(name, metric, timeBucketSizeInSecs);
     }
-    public IMetric registerMetric(String name, IReducer reducer, int timeBucketSizeInSecs) {
+    public ReducedMetric registerMetric(String name, IReducer reducer, int timeBucketSizeInSecs) {
         return _topoContext.registerMetric(name, new ReducedMetric(reducer), timeBucketSizeInSecs);
     }
-    public IMetric registerMetric(String name, ICombiner combiner, int timeBucketSizeInSecs) {
+    public CombinedMetric registerMetric(String name, ICombiner combiner, int timeBucketSizeInSecs) {
         return _topoContext.registerMetric(name, new CombinedMetric(combiner), timeBucketSizeInSecs);
     }
 }

File: src/jvm/backtype/storm/metric/api/CountMetric.java
Patch:
@@ -8,11 +8,11 @@ public class CountMetric implements IMetric {
     public CountMetric() {
     }
     
-    public void inc() {
+    public void incr() {
         _value++;
     }
 
-    public void inc(long incrementBy) {
+    public void incrBy(long incrementBy) {
         _value += incrementBy;
     }
 

File: src/jvm/backtype/storm/metric/api/IMetricsConsumer.java
Patch:
@@ -7,6 +7,7 @@
 
 public interface IMetricsConsumer {
     public static class TaskInfo {
+        public TaskInfo() {}
         public TaskInfo(String srcWorkerHost, int srcWorkerPort, String srcComponentId, int srcTaskId, long timestamp, int updateIntervalSecs) {
             this.srcWorkerHost = srcWorkerHost;
             this.srcWorkerPort = srcWorkerPort;
@@ -23,6 +24,7 @@ public TaskInfo(String srcWorkerHost, int srcWorkerPort, String srcComponentId,
         public int updateIntervalSecs; 
     }
     public static class DataPoint {
+        public DataPoint() {}
         public DataPoint(String name, Object value) {
             this.name = name;
             this.value = value;

File: src/jvm/backtype/storm/serialization/SerializationFactory.java
Patch:
@@ -39,6 +39,8 @@ public static Kryo getKryo(Map conf) {
         k.register(BigInteger.class, new BigIntegerSerializer());
         k.register(TransactionAttempt.class);
         k.register(Values.class);
+        k.register(backtype.storm.metric.api.IMetricsConsumer.DataPoint.class);
+        k.register(backtype.storm.metric.api.IMetricsConsumer.TaskInfo.class);
         try {
             JavaBridge.registerPrimitives(k);
             JavaBridge.registerCollections(k);

File: src/jvm/backtype/storm/metric/api/IMetricsConsumer.java
Patch:
@@ -7,6 +7,7 @@
 
 public interface IMetricsConsumer {
     public static class TaskInfo {
+        public TaskInfo() {}
         public TaskInfo(String srcWorkerHost, int srcWorkerPort, String srcComponentId, int srcTaskId, long timestamp, int updateIntervalSecs) {
             this.srcWorkerHost = srcWorkerHost;
             this.srcWorkerPort = srcWorkerPort;
@@ -23,6 +24,7 @@ public TaskInfo(String srcWorkerHost, int srcWorkerPort, String srcComponentId,
         public int updateIntervalSecs; 
     }
     public static class DataPoint {
+        public DataPoint() {}
         public DataPoint(String name, Object value) {
             this.name = name;
             this.value = value;

File: src/jvm/backtype/storm/serialization/SerializationFactory.java
Patch:
@@ -38,6 +38,8 @@ public static Kryo getKryo(Map conf) {
         k.register(BigInteger.class, new BigIntegerSerializer());
         k.register(TransactionAttempt.class);
         k.register(Values.class);
+        k.register(backtype.storm.metric.api.IMetricsConsumer.DataPoint.class);
+        k.register(backtype.storm.metric.api.IMetricsConsumer.TaskInfo.class);
         try {
             JavaBridge.registerPrimitives(k);
             JavaBridge.registerCollections(k);

File: src/jvm/backtype/storm/metric/api/IMetricsConsumer.java
Patch:
@@ -7,6 +7,7 @@
 
 public interface IMetricsConsumer {
     public static class TaskInfo {
+        public TaskInfo() {}
         public TaskInfo(String srcWorkerHost, int srcWorkerPort, String srcComponentId, int srcTaskId, long timestamp, int updateIntervalSecs) {
             this.srcWorkerHost = srcWorkerHost;
             this.srcWorkerPort = srcWorkerPort;
@@ -23,6 +24,7 @@ public TaskInfo(String srcWorkerHost, int srcWorkerPort, String srcComponentId,
         public int updateIntervalSecs; 
     }
     public static class DataPoint {
+        public DataPoint() {}
         public DataPoint(String name, Object value) {
             this.name = name;
             this.value = value;

File: src/jvm/backtype/storm/serialization/SerializationFactory.java
Patch:
@@ -38,6 +38,8 @@ public static Kryo getKryo(Map conf) {
         k.register(BigInteger.class, new BigIntegerSerializer());
         k.register(TransactionAttempt.class);
         k.register(Values.class);
+        k.register(backtype.storm.metric.api.IMetricsConsumer.DataPoint.class);
+        k.register(backtype.storm.metric.api.IMetricsConsumer.TaskInfo.class);
         try {
             JavaBridge.registerPrimitives(k);
             JavaBridge.registerCollections(k);

File: src/jvm/backtype/storm/metric/MetricsConsumerBolt.java
Patch:
@@ -1,8 +1,9 @@
 package backtype.storm.metric;
 
-import backtype.storm.metric.api.IMetricsConsumer;
 import backtype.storm.Config;
+import backtype.storm.metric.api.IMetricsConsumer;
 import backtype.storm.task.IBolt;
+import backtype.storm.task.IErrorReporter;
 import backtype.storm.task.OutputCollector;
 import backtype.storm.task.TopologyContext;
 import backtype.storm.tuple.Tuple;
@@ -28,7 +29,7 @@ public void prepare(Map stormConf, TopologyContext context, OutputCollector coll
             throw new RuntimeException("Could not instantiate a class listed in config under section " +
                 Config.TOPOLOGY_METRICS_CONSUMER_REGISTER + " with fully qualified name " + _consumerClassName, e);
         }
-        _metricsConsumer.prepare(stormConf, _registrationArgument, context, collector);
+        _metricsConsumer.prepare(stormConf, _registrationArgument, context, (IErrorReporter)collector);
         _collector = collector;
     }
     

File: src/jvm/backtype/storm/metric/api/IMetricsConsumer.java
Patch:
@@ -1,6 +1,6 @@
 package backtype.storm.metric.api;
 
-import backtype.storm.task.OutputCollector;
+import backtype.storm.task.IErrorReporter;
 import backtype.storm.task.TopologyContext;
 import java.util.Collection;
 import java.util.Map;
@@ -35,7 +35,7 @@ public String toString() {
         public Object value;
     }
 
-    void prepare(Map stormConf, Object registrationArgument, TopologyContext context, OutputCollector outputCollector);
+    void prepare(Map stormConf, Object registrationArgument, TopologyContext context, IErrorReporter errorReporter);
     void handleDataPoints(TaskInfo taskInfo, Collection<DataPoint> dataPoints);
     void cleanup();
 }
\ No newline at end of file

File: src/jvm/backtype/storm/task/IOutputCollector.java
Patch:
@@ -4,13 +4,12 @@
 import java.util.Collection;
 import java.util.List;
 
-public interface IOutputCollector {
+public interface IOutputCollector extends IErrorReporter {
     /**
      *  Returns the task ids that received the tuples.
      */
     List<Integer> emit(String streamId, Collection<Tuple> anchors, List<Object> tuple);
     void emitDirect(int taskId, String streamId, Collection<Tuple> anchors, List<Object> tuple);
     void ack(Tuple input);
     void fail(Tuple input);
-    void reportError(Throwable error);
 }

File: src/jvm/backtype/storm/metric/MetricsConsumerBolt.java
Patch:
@@ -28,7 +28,7 @@ public void prepare(Map stormConf, TopologyContext context, OutputCollector coll
             throw new RuntimeException("Could not instantiate a class listed in config under section " +
                 Config.TOPOLOGY_METRICS_CONSUMER_REGISTER + " with fully qualified name " + _consumerClassName, e);
         }
-        _metricsConsumer.prepare(stormConf, _registrationArgument, context);
+        _metricsConsumer.prepare(stormConf, _registrationArgument, context, collector);
         _collector = collector;
     }
     

File: src/jvm/backtype/storm/metric/api/IMetricsConsumer.java
Patch:
@@ -1,5 +1,6 @@
 package backtype.storm.metric.api;
 
+import backtype.storm.task.OutputCollector;
 import backtype.storm.task.TopologyContext;
 import java.util.Collection;
 import java.util.Map;
@@ -34,7 +35,7 @@ public String toString() {
         public Object value;
     }
 
-    void prepare(Map stormConf, Object registrationArgument, TopologyContext context);
+    void prepare(Map stormConf, Object registrationArgument, TopologyContext context, OutputCollector outputCollector);
     void handleDataPoints(TaskInfo taskInfo, Collection<DataPoint> dataPoints);
     void cleanup();
 }
\ No newline at end of file

File: src/jvm/backtype/storm/task/TopologyContext.java
Patch:
@@ -238,7 +238,7 @@ public IMetric registerMetric(String name, IReducer reducer, int timeBucketSizeI
         return registerMetric(name, new ReducedMetric(reducer), timeBucketSizeInSecs);
     }
     /*
-     * Convinience method for registering ReducedMetric.
+     * Convinience method for registering CombinedMetric.
      */
     public IMetric registerMetric(String name, ICombiner combiner, int timeBucketSizeInSecs) {
         return registerMetric(name, new CombinedMetric(combiner), timeBucketSizeInSecs);

File: src/jvm/backtype/storm/metric/api/ReducedMetric.java
Patch:
@@ -1,7 +1,7 @@
 package backtype.storm.metric.api;
 
 public class ReducedMetric implements IMetric {
-    private IReducer _reducer;
+    private final IReducer _reducer;
     private Object _accumulator;
 
     public ReducedMetric(IReducer reducer) {

File: src/jvm/storm/trident/Stream.java
Patch:
@@ -350,7 +350,7 @@ private void projectionValidation(Fields projFields) {
         Fields allFields = this.getOutputFields();
         for (String field : projFields) {
             if (!allFields.contains(field)) {
-                throw new IllegalArgumentException("Trying to select non-existent field: '" + field + "' from all fields: " + allFields + "!");
+                throw new IllegalArgumentException("Trying to select non-existent field: '" + field + "' from stream containing fields fields: <" + allFields + ">");
             }
         }
     }

File: src/jvm/backtype/storm/hooks/ITaskHook.java
Patch:
@@ -1,6 +1,7 @@
 package backtype.storm.hooks;
 
 import backtype.storm.hooks.info.BoltAckInfo;
+import backtype.storm.hooks.info.BoltExecuteInfo;
 import backtype.storm.hooks.info.SpoutFailInfo;
 import backtype.storm.hooks.info.SpoutAckInfo;
 import backtype.storm.hooks.info.EmitInfo;
@@ -14,6 +15,7 @@ public interface ITaskHook {
     void emit(EmitInfo info);
     void spoutAck(SpoutAckInfo info);
     void spoutFail(SpoutFailInfo info);
+    void boltExecute(BoltExecuteInfo info);
     void boltAck(BoltAckInfo info);
     void boltFail(BoltFailInfo info);
 }

File: src/jvm/backtype/storm/topology/BasicBoltExecutor.java
Patch:
@@ -33,7 +33,9 @@ public void execute(Tuple input) {
             _bolt.execute(input, _collector);
             _collector.getOutputter().ack(input);
         } catch(FailedException e) {
-            LOG.warn("Failed to process tuple", e);
+            if(e instanceof ReportedFailedException) {
+                _collector.reportError(e);
+            }
             _collector.getOutputter().fail(input);
         }
     }

File: src/jvm/backtype/storm/task/ShellBolt.java
Patch:
@@ -181,9 +181,7 @@ private void handleFail(Map action) {
 
     private void handleError(Map action) {
         String msg = (String) action.get("msg");
-        if (msg != null) {
-            _collector.reportError(new Exception("Shell Process Exception: " + msg));
-        }
+        _collector.reportError(new Exception("Shell Process Exception: " + msg));
     }
 
     private void handleEmit(Map action) throws InterruptedException {

File: src/jvm/backtype/storm/scheduler/SchedulerAssignment.java
Patch:
@@ -35,5 +35,7 @@ public interface SchedulerAssignment {
      * Return the executors covered by this assignments
      * @return
      */
-    public Set<ExecutorDetails> getExecutors();
+    public Set<ExecutorDetails> getExecutors();
+    
+    public Set<WorkerSlot> getSlots();
 }
\ No newline at end of file

File: src/jvm/backtype/storm/scheduler/WorkerSlot.java
Patch:
@@ -4,9 +4,9 @@ public class WorkerSlot {
     String nodeId;
     int port;
     
-    public WorkerSlot(String nodeId, int port) {
+    public WorkerSlot(String nodeId, Number port) {
         this.nodeId = nodeId;
-        this.port = port;
+        this.port = port.intValue();
     }
     
     public String getNodeId() {

File: src/jvm/backtype/storm/testing/TestJob.java
Patch:
@@ -20,5 +20,5 @@ public interface TestJob {
 	 * @param cluster the cluster which created by <code>Testing.withSimulatedTimeLocalCluster</code>
 	 *        and <code>Testing.withTrackedCluster</code>.
 	 */
-    public void run(ILocalCluster cluster);
+    public void run(ILocalCluster cluster) throws Exception;
 }

File: src/jvm/backtype/storm/scheduler/Cluster.java
Patch:
@@ -33,7 +33,7 @@ public Cluster(Map<String, SupervisorDetails> supervisors, Map<String, Scheduler
         for (String nodeId : supervisors.keySet()) {
             SupervisorDetails supervisor = supervisors.get(nodeId);
             String host = supervisor.getHost();
-            if (!this.supervisors.containsKey(host)) {
+            if (!this.hostToId.containsKey(host)) {
                 this.hostToId.put(host, new ArrayList<String>());
             }
             this.hostToId.get(host).add(nodeId);

File: src/jvm/backtype/storm/scheduler/SchedulerAssignment.java
Patch:
@@ -35,5 +35,7 @@ public interface SchedulerAssignment {
      * Return the executors covered by this assignments
      * @return
      */
-    public Set<ExecutorDetails> getExecutors();
+    public Set<ExecutorDetails> getExecutors();
+    
+    public Set<WorkerSlot> getSlots();
 }
\ No newline at end of file

File: src/jvm/backtype/storm/scheduler/WorkerSlot.java
Patch:
@@ -4,9 +4,9 @@ public class WorkerSlot {
     String nodeId;
     int port;
     
-    public WorkerSlot(String nodeId, int port) {
+    public WorkerSlot(String nodeId, Number port) {
         this.nodeId = nodeId;
-        this.port = port;
+        this.port = port.intValue();
     }
     
     public String getNodeId() {

File: src/jvm/backtype/storm/testing/TestJob.java
Patch:
@@ -20,5 +20,5 @@ public interface TestJob {
 	 * @param cluster the cluster which created by <code>Testing.withSimulatedTimeLocalCluster</code>
 	 *        and <code>Testing.withTrackedCluster</code>.
 	 */
-    public void run(ILocalCluster cluster);
+    public void run(ILocalCluster cluster) throws Exception;
 }

File: src/jvm/storm/kafka/DynamicPartitionConnections.java
Patch:
@@ -47,7 +47,7 @@ public void unregister(HostPort port, int partition) {
     }
     
     public void unregister(GlobalPartitionId id) {
-        return unregister(id.host, id.partition);
+        unregister(id.host, id.partition);
     }
     
     public void clear() {

File: src/jvm/backtype/storm/ILocalCluster.java
Patch:
@@ -4,6 +4,7 @@
 import backtype.storm.generated.ClusterSummary;
 import backtype.storm.generated.InvalidTopologyException;
 import backtype.storm.generated.KillOptions;
+import backtype.storm.generated.SubmitOptions;
 import backtype.storm.generated.NotAliveException;
 import backtype.storm.generated.RebalanceOptions;
 import backtype.storm.generated.StormTopology;
@@ -14,6 +15,7 @@
 
 public interface ILocalCluster {
     void submitTopology(String topologyName, Map conf, StormTopology topology) throws AlreadyAliveException, InvalidTopologyException;
+    void submitTopologyWithOpts(String topologyName, Map conf, StormTopology topology, SubmitOptions submitOpts) throws AlreadyAliveException, InvalidTopologyException;
     void killTopology(String topologyName) throws NotAliveException;
     void killTopologyWithOpts(String name, KillOptions options) throws NotAliveException;
     void activate(String topologyName) throws NotAliveException;

File: src/jvm/storm/trident/TridentState.java
Patch:
@@ -13,7 +13,7 @@ protected TridentState(TridentTopology topology, Node node) {
     }
     
     public Stream newValuesStream() {
-        return new Stream(_topology, _node);
+        return new Stream(_topology, _node.name, _node);
     }
     
     public TridentState parallelismHint(int parallelism) {

File: src/jvm/storm/trident/planner/Node.java
Patch:
@@ -13,16 +13,18 @@ public class Node implements Serializable {
     
     private String nodeId;
     
+    public String name = null;
     public Fields allOutputFields;
     public String streamId;
     public Integer parallelismHint = null;
     public NodeStateInfo stateInfo = null;
     public int creationIndex;
     
-    public Node(String streamId, Fields allOutputFields) {
+    public Node(String streamId, String name, Fields allOutputFields) {
         this.nodeId = UUID.randomUUID().toString();
         this.allOutputFields = allOutputFields;
         this.streamId = streamId;
+        this.name = name;
         this.creationIndex = INDEX.incrementAndGet();
     }
 

File: src/jvm/storm/trident/planner/PartitionNode.java
Patch:
@@ -14,8 +14,8 @@ public class PartitionNode extends Node {
     public transient Grouping thriftGrouping;
     
     //has the streamid/outputFields of the node it's doing the partitioning on
-    public PartitionNode(String streamId, Fields allOutputFields, Grouping grouping) {
-        super(streamId, allOutputFields);
+    public PartitionNode(String streamId, String name, Fields allOutputFields, Grouping grouping) {
+        super(streamId, name, allOutputFields);
         this.thriftGrouping = grouping;
     }
     

File: src/jvm/storm/trident/planner/ProcessorNode.java
Patch:
@@ -8,8 +8,8 @@ public class ProcessorNode extends Node {
     public TridentProcessor processor;
     public Fields selfOutFields;
     
-    public ProcessorNode(String streamId, Fields allOutputFields, Fields selfOutFields, TridentProcessor processor) {
-        super(streamId, allOutputFields);
+    public ProcessorNode(String streamId, String name, Fields allOutputFields, Fields selfOutFields, TridentProcessor processor) {
+        super(streamId, name, allOutputFields);
         this.processor = processor;
         this.selfOutFields = selfOutFields;
     }

File: src/jvm/storm/trident/planner/SpoutNode.java
Patch:
@@ -14,7 +14,7 @@ public static enum SpoutType {
     public SpoutType type;
     
     public SpoutNode(String streamId, Fields allOutputFields, String txid, Object spout, SpoutType type) {
-        super(streamId, allOutputFields);
+        super(streamId, null, allOutputFields);
         this.txId = txid;
         this.spout = spout;
         this.type = type;

File: src/jvm/storm/trident/state/map/SnapshottableMap.java
Patch:
@@ -9,12 +9,12 @@
 public class SnapshottableMap<T> implements MapState<T>, Snapshottable<T> {
     MapState<T> _delegate;
     List<List<Object>> _keys;
-    
+
     public SnapshottableMap(MapState<T> delegate, List<Object> snapshotKey) {
         _delegate = delegate;
         _keys = Arrays.asList(snapshotKey);
     }
-    
+
     @Override
     public List<T> multiGet(List<List<Object>> keys) {
         return _delegate.multiGet(keys);

File: src/jvm/backtype/storm/ILocalCluster.java
Patch:
@@ -4,6 +4,7 @@
 import backtype.storm.generated.ClusterSummary;
 import backtype.storm.generated.InvalidTopologyException;
 import backtype.storm.generated.KillOptions;
+import backtype.storm.generated.SubmitOptions;
 import backtype.storm.generated.NotAliveException;
 import backtype.storm.generated.RebalanceOptions;
 import backtype.storm.generated.StormTopology;
@@ -14,6 +15,7 @@
 
 public interface ILocalCluster {
     void submitTopology(String topologyName, Map conf, StormTopology topology) throws AlreadyAliveException, InvalidTopologyException;
+    void submitTopologyWithOpts(String topologyName, Map conf, StormTopology topology, SubmitOptions submitOpts) throws AlreadyAliveException, InvalidTopologyException;
     void killTopology(String topologyName) throws NotAliveException;
     void killTopologyWithOpts(String name, KillOptions options) throws NotAliveException;
     void activate(String topologyName) throws NotAliveException;

File: src/jvm/storm/trident/TridentState.java
Patch:
@@ -13,7 +13,7 @@ protected TridentState(TridentTopology topology, Node node) {
     }
     
     public Stream newValuesStream() {
-        return new Stream(_topology, _node);
+        return new Stream(_topology, _node.name, _node);
     }
     
     public TridentState parallelismHint(int parallelism) {

File: src/jvm/storm/trident/planner/Node.java
Patch:
@@ -13,16 +13,18 @@ public class Node implements Serializable {
     
     private String nodeId;
     
+    public String name = null;
     public Fields allOutputFields;
     public String streamId;
     public Integer parallelismHint = null;
     public NodeStateInfo stateInfo = null;
     public int creationIndex;
     
-    public Node(String streamId, Fields allOutputFields) {
+    public Node(String streamId, String name, Fields allOutputFields) {
         this.nodeId = UUID.randomUUID().toString();
         this.allOutputFields = allOutputFields;
         this.streamId = streamId;
+        this.name = name;
         this.creationIndex = INDEX.incrementAndGet();
     }
 

File: src/jvm/storm/trident/planner/PartitionNode.java
Patch:
@@ -14,8 +14,8 @@ public class PartitionNode extends Node {
     public transient Grouping thriftGrouping;
     
     //has the streamid/outputFields of the node it's doing the partitioning on
-    public PartitionNode(String streamId, Fields allOutputFields, Grouping grouping) {
-        super(streamId, allOutputFields);
+    public PartitionNode(String streamId, String name, Fields allOutputFields, Grouping grouping) {
+        super(streamId, name, allOutputFields);
         this.thriftGrouping = grouping;
     }
     

File: src/jvm/storm/trident/planner/ProcessorNode.java
Patch:
@@ -8,8 +8,8 @@ public class ProcessorNode extends Node {
     public TridentProcessor processor;
     public Fields selfOutFields;
     
-    public ProcessorNode(String streamId, Fields allOutputFields, Fields selfOutFields, TridentProcessor processor) {
-        super(streamId, allOutputFields);
+    public ProcessorNode(String streamId, String name, Fields allOutputFields, Fields selfOutFields, TridentProcessor processor) {
+        super(streamId, name, allOutputFields);
         this.processor = processor;
         this.selfOutFields = selfOutFields;
     }

File: src/jvm/storm/trident/planner/SpoutNode.java
Patch:
@@ -14,7 +14,7 @@ public static enum SpoutType {
     public SpoutType type;
     
     public SpoutNode(String streamId, Fields allOutputFields, String txid, Object spout, SpoutType type) {
-        super(streamId, allOutputFields);
+        super(streamId, null, allOutputFields);
         this.txId = txid;
         this.spout = spout;
         this.type = type;

File: src/jvm/storm/trident/state/map/SnapshottableMap.java
Patch:
@@ -9,12 +9,12 @@
 public class SnapshottableMap<T> implements MapState<T>, Snapshottable<T> {
     MapState<T> _delegate;
     List<List<Object>> _keys;
-    
+
     public SnapshottableMap(MapState<T> delegate, List<Object> snapshotKey) {
         _delegate = delegate;
         _keys = Arrays.asList(snapshotKey);
     }
-    
+
     @Override
     public List<T> multiGet(List<List<Object>> keys) {
         return _delegate.multiGet(keys);

File: src/jvm/storm/trident/state/ITupleCollection.java
Patch:
@@ -1,4 +1,4 @@
-package backtype.storm.state;
+package storm.trident.state;
 
 import java.util.Iterator;
 import java.util.List;

File: src/jvm/storm/trident/testing/LRUMemoryMapState.java
Patch:
@@ -1,6 +1,6 @@
 package storm.trident.testing;
 
-import backtype.storm.state.ITupleCollection;
+import storm.trident.state.ITupleCollection;
 import backtype.storm.tuple.Values;
 import java.util.*;
 import java.util.Map.Entry;

File: src/jvm/storm/trident/testing/MemoryMapState.java
Patch:
@@ -1,6 +1,6 @@
 package storm.trident.testing;
 
-import backtype.storm.state.ITupleCollection;
+import storm.trident.state.ITupleCollection;
 import backtype.storm.tuple.Values;
 import java.util.*;
 import java.util.Map.Entry;

File: src/jvm/storm/trident/testing/LRUMemoryMapState.java
Patch:
@@ -13,7 +13,7 @@
 import storm.trident.state.snapshot.Snapshottable;
 import storm.trident.util.LRUMap;
 
-public class LRUMemoryMapState<T> implements Snapshottable<T>, ITupleCollection {
+public class LRUMemoryMapState<T> implements Snapshottable<T>, ITupleCollection, MapState<T> {
 
     LRUMemoryMapStateBacking<OpaqueValue> _backing;
     SnapshottableMap<T> _delegate;

File: src/jvm/storm/trident/spout/RichSpoutBatchExecutor.java
Patch:
@@ -73,6 +73,7 @@ public void emitBatch(TransactionAttempt tx, Object coordinatorMeta, TridentColl
             if(now - lastRotate > rotateTime) {
                 Map<Long, List<Object>> failed = idsMap.rotate();
                 for(Long id: failed.keySet()) {
+                    //TODO: this isn't right... it's not in the map anymore
                     fail(id);
                 }
                 lastRotate = now;

File: src/jvm/backtype/storm/StormSubmitter.java
Patch:
@@ -7,7 +7,8 @@
 import java.nio.ByteBuffer;
 import java.util.HashMap;
 import java.util.Map;
-import org.apache.log4j.Logger;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 import org.apache.thrift7.TException;
 import org.json.simple.JSONValue;
 
@@ -17,7 +18,7 @@
  * submit your topologies.
  */
 public class StormSubmitter {
-    public static Logger LOG = Logger.getLogger(StormSubmitter.class);    
+    public static Logger LOG = LoggerFactory.getLogger(StormSubmitter.class);    
 
     private static Nimbus.Iface localNimbus = null;
 

File: src/jvm/backtype/storm/coordination/BatchBoltExecutor.java
Patch:
@@ -11,10 +11,11 @@
 import backtype.storm.utils.Utils;
 import java.util.HashMap;
 import java.util.Map;
-import org.apache.log4j.Logger;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 public class BatchBoltExecutor implements IRichBolt, FinishedCallback, TimeoutCallback {
-    public static Logger LOG = Logger.getLogger(BatchBoltExecutor.class);    
+    public static Logger LOG = LoggerFactory.getLogger(BatchBoltExecutor.class);    
 
     byte[] _boltSer;
     Map<Object, IBatchBolt> _openTransactions;

File: src/jvm/backtype/storm/coordination/CoordinatedBolt.java
Patch:
@@ -23,15 +23,16 @@
 import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
-import org.apache.log4j.Logger;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 import static backtype.storm.utils.Utils.get;
 
 /**
  * Coordination requires the request ids to be globally unique for awhile. This is so it doesn't get confused
  * in the case of retries.
  */
 public class CoordinatedBolt implements IRichBolt {
-    public static Logger LOG = Logger.getLogger(CoordinatedBolt.class);
+    public static Logger LOG = LoggerFactory.getLogger(CoordinatedBolt.class);
 
     public static interface FinishedCallback {
         void finishedId(Object id);

File: src/jvm/backtype/storm/drpc/DRPCSpout.java
Patch:
@@ -16,12 +16,13 @@
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
-import org.apache.log4j.Logger;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 import org.apache.thrift7.TException;
 import org.json.simple.JSONValue;
 
 public class DRPCSpout extends BaseRichSpout {
-    public static Logger LOG = Logger.getLogger(DRPCSpout.class);
+    public static Logger LOG = LoggerFactory.getLogger(DRPCSpout.class);
     
     SpoutOutputCollector _collector;
     List<DRPCInvocationsClient> _clients = new ArrayList<DRPCInvocationsClient>();

File: src/jvm/backtype/storm/drpc/JoinResult.java
Patch:
@@ -11,11 +11,12 @@
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
-import org.apache.log4j.Logger;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 
 public class JoinResult extends BaseRichBolt {
-    public static Logger LOG = Logger.getLogger(JoinResult.class);
+    public static Logger LOG = LoggerFactory.getLogger(JoinResult.class);
 
     String returnComponent;
     Map<Object, Tuple> returns = new HashMap<Object, Tuple>();

File: src/jvm/backtype/storm/drpc/ReturnResults.java
Patch:
@@ -13,13 +13,14 @@
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
-import org.apache.log4j.Logger;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 import org.apache.thrift7.TException;
 import org.json.simple.JSONValue;
 
 
 public class ReturnResults extends BaseRichBolt {
-    public static final Logger LOG = Logger.getLogger(ReturnResults.class);
+    public static final Logger LOG = LoggerFactory.getLogger(ReturnResults.class);
     OutputCollector _collector;
     boolean local;
 

File: src/jvm/backtype/storm/serialization/SerializationFactory.java
Patch:
@@ -22,10 +22,11 @@
 import java.util.List;
 import java.util.Map;
 import java.util.TreeMap;
-import org.apache.log4j.Logger;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 public class SerializationFactory {
-    public static final Logger LOG = Logger.getLogger(SerializationFactory.class);
+    public static final Logger LOG = LoggerFactory.getLogger(SerializationFactory.class);
     
     public static class KryoSerializableDefault extends Kryo {
         boolean _override = false;

File: src/jvm/backtype/storm/spout/ShellSpout.java
Patch:
@@ -7,12 +7,13 @@
 import java.util.Map;
 import java.util.List;
 import java.io.IOException;
-import org.apache.log4j.Logger;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 import org.json.simple.JSONObject;
 
 
 public class ShellSpout implements ISpout {
-    public static Logger LOG = Logger.getLogger(ShellSpout.class);
+    public static Logger LOG = LoggerFactory.getLogger(ShellSpout.class);
 
     private SpoutOutputCollector _collector;
     private String[] _command;

File: src/jvm/backtype/storm/task/ShellBolt.java
Patch:
@@ -14,7 +14,8 @@
 import java.util.List;
 import java.util.Map;
 import java.util.Random;
-import org.apache.log4j.Logger;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 import org.json.simple.JSONObject;
 
 /**
@@ -45,7 +46,7 @@
  * </pre>
  */
 public class ShellBolt implements IBolt {
-    public static Logger LOG = Logger.getLogger(ShellBolt.class);
+    public static Logger LOG = LoggerFactory.getLogger(ShellBolt.class);
     Process _subprocess;
     OutputCollector _collector;
     Map<String, Tuple> _inputs = new ConcurrentHashMap<String, Tuple>();

File: src/jvm/backtype/storm/testing/TestAggregatesCounter.java
Patch:
@@ -8,12 +8,13 @@
 import java.util.Map;
 import backtype.storm.task.TopologyContext;
 import java.util.HashMap;
-import org.apache.log4j.Logger;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 import static backtype.storm.utils.Utils.tuple;
 
 
 public class TestAggregatesCounter extends BaseRichBolt {
-    public static Logger LOG = Logger.getLogger(TestWordCounter.class);
+    public static Logger LOG = LoggerFactory.getLogger(TestWordCounter.class);
 
     Map<String, Integer> _counts;
     OutputCollector _collector;

File: src/jvm/backtype/storm/testing/TestGlobalCount.java
Patch:
@@ -8,11 +8,12 @@
 import java.util.Map;
 import backtype.storm.task.TopologyContext;
 import backtype.storm.tuple.Values;
-import org.apache.log4j.Logger;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 
 public class TestGlobalCount extends BaseRichBolt {
-    public static Logger LOG = Logger.getLogger(TestWordCounter.class);
+    public static Logger LOG = LoggerFactory.getLogger(TestWordCounter.class);
 
     private int _count;
     OutputCollector _collector;

File: src/jvm/backtype/storm/testing/TestWordCounter.java
Patch:
@@ -8,12 +8,13 @@
 import backtype.storm.task.TopologyContext;
 import backtype.storm.topology.BasicOutputCollector;
 import java.util.HashMap;
-import org.apache.log4j.Logger;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 import static backtype.storm.utils.Utils.tuple;
 
 
 public class TestWordCounter extends BaseBasicBolt {
-    public static Logger LOG = Logger.getLogger(TestWordCounter.class);
+    public static Logger LOG = LoggerFactory.getLogger(TestWordCounter.class);
 
     Map<String, Integer> _counts;
     

File: src/jvm/backtype/storm/testing/TestWordSpout.java
Patch:
@@ -11,11 +11,12 @@
 import backtype.storm.utils.Utils;
 import java.util.HashMap;
 import java.util.Random;
-import org.apache.log4j.Logger;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 
 public class TestWordSpout extends BaseRichSpout {
-    public static Logger LOG = Logger.getLogger(TestWordSpout.class);
+    public static Logger LOG = LoggerFactory.getLogger(TestWordSpout.class);
     boolean _isDistributed;
     SpoutOutputCollector _collector;
 

File: src/jvm/backtype/storm/topology/BasicBoltExecutor.java
Patch:
@@ -4,10 +4,11 @@
 import backtype.storm.task.TopologyContext;
 import backtype.storm.tuple.Tuple;
 import java.util.Map;
-import org.apache.log4j.Logger;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 public class BasicBoltExecutor implements IRichBolt {
-    public static Logger LOG = Logger.getLogger(BasicBoltExecutor.class);    
+    public static Logger LOG = LoggerFactory.getLogger(BasicBoltExecutor.class);    
     
     private IBasicBolt _bolt;
     private transient BasicOutputCollector _collector;

File: src/jvm/backtype/storm/transactional/TransactionalSpoutBatchExecutor.java
Patch:
@@ -10,10 +10,11 @@
 import java.math.BigInteger;
 import java.util.Map;
 import java.util.TreeMap;
-import org.apache.log4j.Logger;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 public class TransactionalSpoutBatchExecutor implements IRichBolt {
-    public static Logger LOG = Logger.getLogger(TransactionalSpoutBatchExecutor.class);    
+    public static Logger LOG = LoggerFactory.getLogger(TransactionalSpoutBatchExecutor.class);    
 
     BatchOutputCollectorImpl _collector;
     ITransactionalSpout _spout;

File: src/jvm/backtype/storm/transactional/TransactionalSpoutCoordinator.java
Patch:
@@ -15,10 +15,11 @@
 import java.util.Map;
 import java.util.TreeMap;
 import java.util.Random;
-import org.apache.log4j.Logger;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 public class TransactionalSpoutCoordinator extends BaseRichSpout { 
-    public static final Logger LOG = Logger.getLogger(TransactionalSpoutCoordinator.class);
+    public static final Logger LOG = LoggerFactory.getLogger(TransactionalSpoutCoordinator.class);
     
     public static final BigInteger INIT_TXID = BigInteger.ONE;
     

File: src/jvm/backtype/storm/utils/Time.java
Patch:
@@ -4,11 +4,12 @@
 import java.util.Map;
 import java.util.concurrent.ConcurrentHashMap;
 import java.util.concurrent.atomic.AtomicBoolean;
-import org.apache.log4j.Logger;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 
 public class Time {
-    public static Logger LOG = Logger.getLogger(Time.class);    
+    public static Logger LOG = LoggerFactory.getLogger(Time.class);    
     
     private static AtomicBoolean simulating = new AtomicBoolean(false);
     //TODO: should probably use weak references here or something

File: src/jvm/storm/trident/spout/TridentSpoutCoordinator.java
Patch:
@@ -10,14 +10,15 @@
 import backtype.storm.tuple.Tuple;
 import backtype.storm.tuple.Values;
 import java.util.Map;
-import org.apache.log4j.Logger;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 import storm.trident.topology.MasterBatchCoordinator;
 import storm.trident.topology.state.RotatingTransactionalState;
 import storm.trident.topology.state.TransactionalState;
 
 
 public class TridentSpoutCoordinator implements IBasicBolt {
-    public static final Logger LOG = Logger.getLogger(TridentSpoutCoordinator.class);
+    public static final Logger LOG = LoggerFactory.getLogger(TridentSpoutCoordinator.class);
     private static final String META_DIR = "meta";
 
     ITridentSpout _spout;

File: src/jvm/storm/trident/spout/TridentSpoutExecutor.java
Patch:
@@ -11,7 +11,8 @@
 import java.util.List;
 import java.util.Map;
 import java.util.TreeMap;
-import org.apache.log4j.Logger;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 import storm.trident.operation.TridentCollector;
 import storm.trident.topology.BatchInfo;
 import storm.trident.topology.ITridentBatchBolt;
@@ -21,7 +22,7 @@
 public class TridentSpoutExecutor implements ITridentBatchBolt {
     public static String ID_FIELD = "$tx";
     
-    public static Logger LOG = Logger.getLogger(TridentSpoutExecutor.class);    
+    public static Logger LOG = LoggerFactory.getLogger(TridentSpoutExecutor.class);    
 
     AddIdCollector _collector;
     ITridentSpout _spout;

File: src/jvm/storm/trident/topology/MasterBatchCoordinator.java
Patch:
@@ -13,12 +13,13 @@
 import java.util.Map;
 import java.util.TreeMap;
 import java.util.Random;
-import org.apache.log4j.Logger;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 import storm.trident.spout.ITridentSpout;
 import storm.trident.topology.state.TransactionalState;
 
 public class MasterBatchCoordinator extends BaseRichSpout { 
-    public static final Logger LOG = Logger.getLogger(MasterBatchCoordinator.class);
+    public static final Logger LOG = LoggerFactory.getLogger(MasterBatchCoordinator.class);
     
     public static final long INIT_TXID = 1L;
     

File: src/jvm/storm/trident/operation/builtin/TupleCollectionGet.java
Patch:
@@ -14,16 +14,15 @@ public class TupleCollectionGet extends BaseQueryFunction<State, Object> {
     @Override
     public List<Object> batchRetrieve(State state, List<TridentTuple> args) {
         List<Object> ret = new ArrayList<Object>(args.size());
-        Iterator<List<Object>> tuplesIterator = ((ITupleCollection)state).getTuples();
         for(int i=0; i<args.size(); i++) {
-            ret.add(tuplesIterator);
+            ret.add(((ITupleCollection)state).getTuples());
         }
         return ret;
     }
 
     @Override
     public void execute(TridentTuple tuple, Object result, TridentCollector collector) {
-        Iterator<List<Object>> tuplesIterator = (Iterator<List<Object>>) result;
+        Iterator<List<Object>> tuplesIterator = (Iterator<List<Object>>)result;
         while(tuplesIterator.hasNext()) {
             collector.emit(tuplesIterator.next());
         }

File: src/jvm/storm/starter/spout/TwitterSampleSpout.java
Patch:
@@ -1,3 +1,5 @@
+/*
+
 package storm.starter.spout;
 
 import backtype.storm.Config;
@@ -100,3 +102,4 @@ public void declareOutputFields(OutputFieldsDeclarer declarer) {
     }
     
 }
+*/
\ No newline at end of file

File: src/jvm/backtype/storm/ILocalCluster.java
Patch:
@@ -24,4 +24,5 @@ public interface ILocalCluster {
     StormTopology getTopology(String id);
     ClusterSummary getClusterInfo();
     TopologyInfo getTopologyInfo(String id);
+    Map getState();
 }

File: src/jvm/backtype/storm/serialization/SerializationFactory.java
Patch:
@@ -98,7 +98,6 @@ public static Kryo getKryo(Map conf) {
                 Class klass = Class.forName(klassName);
                 IKryoDecorator decorator = (IKryoDecorator)klass.newInstance();
                 decorator.decorate(k);
-                System.out.println(klassName + ".. executed decorate.");
             } catch(ClassNotFoundException e) {
                 if(skipMissing) {
                     LOG.info("Could not find kryo decorator named " + klassName + ". Skipping registration...");

File: src/jvm/storm/trident/spout/TridentSpoutExecutor.java
Patch:
@@ -48,7 +48,6 @@ public void execute(BatchInfo info, Tuple input) {
         // there won't be a BatchInfo for the success stream
         TransactionAttempt attempt = (TransactionAttempt) input.getValue(0);
         if(input.getSourceStreamId().equals(MasterBatchCoordinator.COMMIT_STREAM_ID)) {
-            _collector.setBatch(info.batchId);
             if(attempt.equals(_activeBatches.get(attempt.getTransactionId()))) {
                 ((ICommitterTridentSpout.Emitter) _emitter).commit(attempt);
                 _activeBatches.remove(attempt.getTransactionId());

File: src/jvm/storm/trident/tuple/TridentTupleView.java
Patch:
@@ -96,7 +96,7 @@ public static class OperationOutputFactory implements Factory {
 
         public OperationOutputFactory(Factory parent, Fields selfFields) {
             _parent = parent;
-            _fieldIndex = parent.getFieldIndex();
+            _fieldIndex = new HashMap(parent.getFieldIndex());
             int myIndex = parent.numDelegates();
             for(int i=0; i<selfFields.size(); i++) {
                 String field = selfFields.get(i);

File: src/jvm/storm/trident/testing/Split.java
Patch:
@@ -10,7 +10,9 @@ public class Split extends BaseFunction {
     @Override
     public void execute(TridentTuple tuple, TridentCollector collector) {
         for(String word: tuple.getString(0).split(" ")) {
-            collector.emit(new Values(word));
+            if(word.length() > 0) {
+                collector.emit(new Values(word));
+            }
         }
     }
     

File: src/jvm/storm/trident/state/Serializer.java
Patch:
@@ -5,5 +5,5 @@
 
 public interface Serializer<T> extends Serializable {
     byte[] serialize(T obj);
-    Object deserialize(byte[] b);
+    T deserialize(byte[] b);
 }

File: src/jvm/backtype/storm/testing/MockedSources.java
Patch:
@@ -33,7 +33,7 @@ public void addMockData(String spoutId, String streamId, Values... valueses) {
         }
     }
     
-    public void addMockedData(String spoutId, Values... valueses) {
+    public void addMockData(String spoutId, Values... valueses) {
         this.addMockData(spoutId, Utils.DEFAULT_STREAM_ID, valueses);
     }
     

File: src/jvm/storm/trident/state/OpaqueValue.java
Patch:
@@ -17,14 +17,14 @@ public OpaqueValue(Long currTxid, T val) {
         this(currTxid, val, null);
     }
     
-    public OpaqueValue update(Long batchTxid, T newVal) {
+    public OpaqueValue<T> update(Long batchTxid, T newVal) {
         T prev;
         if(batchTxid!=null && batchTxid.equals(this.currTxid)) {
             prev = this.prev;
         } else {
             prev = this.curr;
         }
-        return new OpaqueValue(batchTxid, newVal, prev);
+        return new OpaqueValue<T>(batchTxid, newVal, prev);
     }
     
     public T get(Long batchTxid) {

File: src/jvm/storm/trident/testing/LRUMemoryMapState.java
Patch:
@@ -50,7 +50,7 @@ public LRUMemoryMapState(int cacheSize, String id) {
 
     @Override
     public List<T> multiGet(List<List<Object>> keys) {
-        List<T> ret = new ArrayList();
+        List<T> ret = new ArrayList<T>();
         for(List<Object> key: keys) {
             ret.add(db.get(key));
         }

File: src/jvm/storm/trident/state/map/NonTransactionalMap.java
Patch:
@@ -2,13 +2,12 @@
 
 import java.util.ArrayList;
 import java.util.List;
-import storm.trident.state.TransactionalValue;
 import storm.trident.state.ValueUpdater;
 
 
 public class NonTransactionalMap<T> implements MapState<T> {
-    public static MapState build(IBackingMap<TransactionalValue> backing) {
-        return new NonTransactionalMap(backing);
+    public static <T> MapState<T> build(IBackingMap<T> backing) {
+        return new NonTransactionalMap<T>(backing);
     }
     
     IBackingMap<T> _backing;

File: src/jvm/storm/trident/operation/GroupedMultiReducer.java
Patch:
@@ -7,7 +7,7 @@
 
 public interface GroupedMultiReducer<T> extends Serializable {
     void prepare(Map conf, TridentMultiReducerContext context);
-    T init(TridentCollector collector);
+    T init(TridentCollector collector, TridentTuple group);
     void execute(T state, int streamIndex, TridentTuple group, TridentTuple input, TridentCollector collector);
     void complete(T state, TridentTuple group, TridentCollector collector);
     void cleanup();

File: src/jvm/storm/trident/operation/impl/GroupedMultiReducerExecutor.java
Patch:
@@ -26,6 +26,7 @@ public GroupedMultiReducerExecutor(GroupedMultiReducer reducer, List<Fields> gro
         }
         _groupFields = groupFields;
         _inputFields = inputFields;
+        _reducer = reducer;
     }
     
     @Override
@@ -48,11 +49,11 @@ public void execute(Map<TridentTuple, Object> state, int streamIndex, TridentTup
         ProjectionFactory inputFactory = _inputFactories.get(streamIndex);
         
         TridentTuple group = groupFactory.create(full);
-        TridentTuple input = groupFactory.create(full);
+        TridentTuple input = inputFactory.create(full);
         
         Object curr;
         if(!state.containsKey(group)) {
-            curr = _reducer.init(collector);
+            curr = _reducer.init(collector, group);
             state.put(group, curr);
         } else {
             curr = state.get(group);

File: src/jvm/backtype/storm/utils/RotatingMap.java
Patch:
@@ -48,14 +48,15 @@ public RotatingMap(int numBuckets) {
         this(numBuckets, null);
     }   
     
-    public void rotate() {
+    public Map<K, V> rotate() {
         Map<K, V> dead = _buckets.removeLast();
         _buckets.addFirst(new HashMap<K, V>());
         if(_callback!=null) {
             for(Entry<K, V> entry: dead.entrySet()) {
                 _callback.expire(entry.getKey(), entry.getValue());
             }
         }
+        return dead;
     }
 
     public boolean containsKey(K key) {

File: src/jvm/storm/kafka/ZkCoordinator.java
Patch:
@@ -126,7 +126,7 @@ public PartitionManager getManager(GlobalPartitionId id) {
     }
     
     private boolean myOwnership(GlobalPartitionId id) {
-        int val = id.host.hashCode() + 23 * id.partition;        
+        int val = Math.abs(id.host.hashCode() + 23 * id.partition);        
         return val % _totalTasks == _taskIndex;
     }
     

File: src/jvm/backtype/storm/spout/ShellSpout.java
Patch:
@@ -35,7 +35,7 @@ public void open(Map stormConf, TopologyContext context,
             Number subpid = _process.launch(stormConf, context);
             LOG.info("Launched subprocess with pid " + subpid);
         } catch (IOException e) {
-            throw new RuntimeException("Error when launching multilang subprocess", e);
+            throw new RuntimeException("Error when launching multilang subprocess\n" + _process.getErrorsString(), e);
         }
     }
 

File: src/jvm/backtype/storm/task/ShellBolt.java
Patch:
@@ -79,7 +79,7 @@ public void prepare(Map stormConf, TopologyContext context,
             Number subpid = _process.launch(stormConf, context);
             LOG.info("Launched subprocess with pid " + subpid);
         } catch (IOException e) {
-            throw new RuntimeException("Error when launching multilang subprocess", e);
+            throw new RuntimeException("Error when launching multilang subprocess\n" + _process.getErrorsString(), e);
         }
 
         // reader

File: src/jvm/storm/kafka/ZkCoordinator.java
Patch:
@@ -129,7 +129,7 @@ public PartitionManager getManager(GlobalPartitionId id) {
     }
     
     private boolean myOwnership(GlobalPartitionId id) {
-        int val = id.host.hashCode() + 23 * id.partition;        
+        int val = Math.abs(id.host.hashCode() + 23 * id.partition);        
         return val % _totalTasks == _taskIndex;
     }
     

File: src/jvm/backtype/storm/serialization/KryoValuesSerializer.java
Patch:
@@ -3,9 +3,7 @@
 import backtype.storm.utils.ListDelegate;
 import com.esotericsoftware.kryo.Kryo;
 import com.esotericsoftware.kryo.io.Output;
-import java.io.ByteArrayOutputStream;
 import java.io.IOException;
-import java.io.OutputStream;
 import java.util.List;
 import java.util.Map;
 

File: src/jvm/backtype/storm/serialization/SerializationFactory.java
Patch:
@@ -14,7 +14,6 @@
 import com.esotericsoftware.kryo.Kryo;
 import com.esotericsoftware.kryo.Serializer;
 import com.esotericsoftware.kryo.serializers.DefaultSerializers.BigIntegerSerializer;
-import com.esotericsoftware.kryo.serializers.JavaSerializer;
 import java.math.BigInteger;
 import java.util.ArrayList;
 import java.util.Collections;
@@ -38,7 +37,7 @@ public void overrideDefault(boolean value) {
         @Override
         public Serializer getDefaultSerializer(Class type) {
             if(_override) {
-                return new JavaSerializer();
+                return new SerializableSerializer();
             } else {
                 return super.getDefaultSerializer(type);
             }

File: src/jvm/backtype/storm/testing/BatchNumberList.java
Patch:
@@ -33,12 +33,10 @@ public BatchNumberList(String wordComponent) {
     @Override
     public void prepare(Map conf, TopologyContext context, BatchOutputCollector collector, Object id) {
         _collector = collector;
-        System.out.println("STARTED: " + id);
     }
 
     @Override
     public void execute(Tuple tuple) {
-        System.out.println("RECEIVED: " + tuple.toString());
         if(tuple.getSourceComponent().equals(_wordComponent)) {
             this.word = tuple.getString(1);
         } else {

File: src/jvm/backtype/storm/scheduler/Cluster.java
Patch:
@@ -69,7 +69,7 @@ public List<TopologyDetails> needsSchedulingTopologies(Topologies topologies) {
      * </ul>
      */
     public boolean needsScheduling(TopologyDetails topology) {
-        int desiredNumWorkers = ((Number) topology.getConf().get(Config.TOPOLOGY_WORKERS)).intValue();
+        int desiredNumWorkers = topology.getNumWorkers();
         int assignedNumWorkers = this.getAssignedNumWorkers(topology);
 
         if (desiredNumWorkers > assignedNumWorkers) {

File: src/jvm/backtype/storm/scheduler/INimbus.java
Patch:
@@ -10,7 +10,7 @@ public interface INimbus {
     //   1. if some slots are used, return as much as it currently has available
     //   2. otherwise return nothing until it has enough slots, or enough time has passed
     // sets the node id as {normalized hostname (invalid chars removed}-{topologyid}
-    Collection<WorkerSlot> availableSlots(Collection<SupervisorDetails> existingSupervisors, Collection<WorkerSlot> usedSlots, TopologyDetails topology);
+    Collection<WorkerSlot> availableSlots(Collection<SupervisorDetails> existingSupervisors, Collection<WorkerSlot> usedSlots, Collection<TopologyDetails> topologies);
     // mesos should call launchTasks on an executor for this topology... 
     // gives it the executor with:
     //   - name: the node id

File: src/jvm/storm/kafka/HostPort.java
Patch:
@@ -1,6 +1,8 @@
 package storm.kafka;
 
-public class HostPort {
+import java.io.Serializable;
+
+public class HostPort implements Serializable {
     public String host;
     public int port;
     

File: src/jvm/storm/kafka/KafkaConfig.java
Patch:
@@ -44,6 +44,7 @@ public static List<HostPort> convertHosts(List<String> hosts) {
             } else {
                 throw new IllegalArgumentException("Invalid host specification: " + s);
             }
+            ret.add(hp);
         }
         return ret;
     }

File: src/jvm/backtype/storm/tuple/TupleImpl.java
Patch:
@@ -1,7 +1,6 @@
 package backtype.storm.tuple;
 
 import backtype.storm.task.GeneralTopologyContext;
-import backtype.storm.task.TopologyContext;
 import java.util.List;
 
 public class TupleImpl extends Tuple {

File: src/jvm/backtype/storm/StormSubmitter.java
Patch:
@@ -81,6 +81,9 @@ private static void submitJar(Map conf) {
     }
     
     public static String submitJar(Map conf, String localJar) {
+        if(localJar==null) {
+            throw new RuntimeException("Must submit topologies using the 'storm' client script so that StormSubmitter knows which jar to upload.");
+        }
         NimbusClient client = NimbusClient.getConfiguredClient(conf);
         try {
             String uploadLocation = client.getClient().beginFileUpload();

File: src/jvm/backtype/storm/coordination/CoordinatedBolt.java
Patch:
@@ -278,7 +278,6 @@ public void execute(Tuple tuple) {
             } else {
                 _collector.ack(tuple);
             }
-            
         } else if(!_sourceArgs.isEmpty()
                 && tuple.getSourceStreamId().equals(Constants.COORDINATED_STREAM_ID)) {
             int count = (Integer) tuple.getValue(1);
@@ -299,8 +298,10 @@ public void execute(Tuple tuple) {
         }
     }
 
+    @Override
     public void cleanup() {
         _delegate.cleanup();
+        _tracked.cleanup();
     }
 
     public void declareOutputFields(OutputFieldsDeclarer declarer) {

File: src/jvm/backtype/storm/grouping/CustomStreamGrouping.java
Patch:
@@ -1,10 +1,9 @@
 package backtype.storm.grouping;
 
-import backtype.storm.task.TopologyContext;
+import backtype.storm.task.WorkerTopologyContext;
 import backtype.storm.tuple.Fields;
 import java.io.Serializable;
 import java.util.List;
-import java.util.Map;
 
 public interface CustomStreamGrouping extends Serializable {
     
@@ -14,7 +13,7 @@ public interface CustomStreamGrouping extends Serializable {
      * 
      * It also tells the grouping the metadata on the stream this grouping will be used on.
      */
-   void prepare(TopologyContext context, Fields outFields, List<Integer> targetTasks);
+   void prepare(WorkerTopologyContext context, Fields outFields, List<Integer> targetTasks);
     
    /**
      * This function implements a custom stream grouping. It takes in as input

File: src/jvm/backtype/storm/hooks/ITaskHook.java
Patch:
@@ -16,5 +16,4 @@ public interface ITaskHook {
     void spoutFail(SpoutFailInfo info);
     void boltAck(BoltAckInfo info);
     void boltFail(BoltFailInfo info);
-    void error(Throwable error);
 }

File: src/jvm/backtype/storm/hooks/info/BoltAckInfo.java
Patch:
@@ -4,9 +4,9 @@
 
 public class BoltAckInfo {
     public Tuple tuple;
-    public long processLatencyMs;
+    public Long processLatencyMs; // null if it wasn't sampled
     
-    public BoltAckInfo(Tuple tuple, long processLatencyMs) {
+    public BoltAckInfo(Tuple tuple, Long processLatencyMs) {
         this.tuple = tuple;
         this.processLatencyMs = processLatencyMs;
     }

File: src/jvm/backtype/storm/hooks/info/BoltFailInfo.java
Patch:
@@ -4,9 +4,9 @@
 
 public class BoltFailInfo {
     public Tuple tuple;
-    public long failLatencyMs;
+    public Long failLatencyMs; // null if it wasn't sampled
     
-    public BoltFailInfo(Tuple tuple, long failLatencyMs) {
+    public BoltFailInfo(Tuple tuple, Long failLatencyMs) {
         this.tuple = tuple;
         this.failLatencyMs = failLatencyMs;
     }

File: src/jvm/backtype/storm/hooks/info/SpoutAckInfo.java
Patch:
@@ -2,9 +2,9 @@
 
 public class SpoutAckInfo {
     public Object messageId;
-    public long completeLatencyMs;
+    public Long completeLatencyMs; // null if it wasn't sampled
     
-    public SpoutAckInfo(Object messageId, long completeLatencyMs) {
+    public SpoutAckInfo(Object messageId, Long completeLatencyMs) {
         this.messageId = messageId;
         this.completeLatencyMs = completeLatencyMs;
     }

File: src/jvm/backtype/storm/hooks/info/SpoutFailInfo.java
Patch:
@@ -2,9 +2,9 @@
 
 public class SpoutFailInfo {
     public Object messageId;
-    public long failLatencyMs;
+    public Long failLatencyMs; // null if it wasn't sampled
     
-    public SpoutFailInfo(Object messageId, long failLatencyMs) {
+    public SpoutFailInfo(Object messageId, Long failLatencyMs) {
         this.messageId = messageId;
         this.failLatencyMs = failLatencyMs;
     }

File: src/jvm/backtype/storm/serialization/SerializationFactory.java
Patch:
@@ -4,6 +4,7 @@
 import backtype.storm.generated.ComponentCommon;
 import backtype.storm.generated.StormTopology;
 import backtype.storm.transactional.TransactionAttempt;
+import backtype.storm.tuple.Values;
 import backtype.storm.utils.ListDelegate;
 import backtype.storm.utils.Utils;
 import carbonite.JavaBridge;
@@ -52,6 +53,7 @@ public static ObjectBuffer getKryo(Map conf) {
         k.register(HashSet.class);
         k.register(BigInteger.class, new BigIntegerSerializer());
         k.register(TransactionAttempt.class);
+        k.register(Values.class);
         JavaBridge clojureSerializersBridge = new JavaBridge();
         clojureSerializersBridge.registerClojureCollections(k);
         clojureSerializersBridge.registerClojurePrimitives(k);

File: src/jvm/backtype/storm/testing/NGrouping.java
Patch:
@@ -1,7 +1,7 @@
 package backtype.storm.testing;
 
 import backtype.storm.grouping.CustomStreamGrouping;
-import backtype.storm.task.TopologyContext;
+import backtype.storm.task.WorkerTopologyContext;
 import backtype.storm.tuple.Fields;
 import java.util.ArrayList;
 import java.util.Collections;
@@ -16,7 +16,7 @@ public NGrouping(int n) {
     }
     
     @Override
-    public void prepare(TopologyContext context, Fields outFields, List<Integer> targetTasks) {
+    public void prepare(WorkerTopologyContext context, Fields outFields, List<Integer> targetTasks) {
         targetTasks = new ArrayList<Integer>(targetTasks);
         Collections.sort(targetTasks);
         _outTasks = new ArrayList<Integer>();

File: src/jvm/backtype/storm/topology/ComponentConfigurationDeclarer.java
Patch:
@@ -7,5 +7,6 @@ public interface ComponentConfigurationDeclarer<T extends ComponentConfiguration
     T addConfiguration(String config, Object value);
     T setDebug(boolean debug);
     T setMaxTaskParallelism(Number val);
-    T setMaxSpoutPending(Number val);    
+    T setMaxSpoutPending(Number val);
+    T setNumTasks(Number val);
 }

File: src/jvm/backtype/storm/coordination/CoordinatedBolt.java
Patch:
@@ -311,6 +311,7 @@ public void execute(Tuple tuple) {
 
     public void cleanup() {
         _delegate.cleanup();
+        _tracked.cleanup();
     }
 
     public void declareOutputFields(OutputFieldsDeclarer declarer) {

File: src/jvm/backtype/storm/testing/TestGlobalCount.java
Patch:
@@ -7,8 +7,8 @@
 import backtype.storm.tuple.Fields;
 import java.util.Map;
 import backtype.storm.task.TopologyContext;
+import backtype.storm.tuple.Values;
 import org.apache.log4j.Logger;
-import static backtype.storm.utils.Utils.tuple;
 
 
 public class TestGlobalCount extends BaseRichBolt {
@@ -24,7 +24,7 @@ public void prepare(Map stormConf, TopologyContext context, OutputCollector coll
 
     public void execute(Tuple input) {
         _count++;
-        _collector.emit(tuple(_count));
+        _collector.emit(input, new Values(_count));
         _collector.ack(input);
     }
 

File: src/jvm/backtype/storm/transactional/TransactionalTopologyBuilder.java
Patch:
@@ -52,7 +52,7 @@ public TransactionalTopologyBuilder(String id, String spoutId, ITransactionalSpo
         _id = id;
         _spoutId = spoutId;
         _spout = spout;
-        _spoutParallelism = spoutParallelism.intValue();
+        _spoutParallelism = (spoutParallelism == null) ? null : spoutParallelism.intValue();
     }
     
     public TransactionalTopologyBuilder(String id, String spoutId, ITransactionalSpout spout) {

File: src/jvm/backtype/storm/transactional/TransactionalTopologyBuilder.java
Patch:
@@ -52,7 +52,7 @@ public TransactionalTopologyBuilder(String id, String spoutId, ITransactionalSpo
         _id = id;
         _spoutId = spoutId;
         _spout = spout;
-        _spoutParallelism = spoutParallelism.intValue();
+        _spoutParallelism = (spoutParallelism == null) ? null : spoutParallelism.intValue();
     }
     
     public TransactionalTopologyBuilder(String id, String spoutId, ITransactionalSpout spout) {

File: src/jvm/backtype/storm/coordination/CoordinatedBolt.java
Patch:
@@ -299,8 +299,10 @@ public void execute(Tuple tuple) {
         }
     }
 
+    @Override
     public void cleanup() {
         _delegate.cleanup();
+        _tracked.cleanup();
     }
 
     public void declareOutputFields(OutputFieldsDeclarer declarer) {

File: src/jvm/backtype/storm/task/TopologyContext.java
Patch:
@@ -34,7 +34,7 @@ public class TopologyContext extends WorkerTopologyContext {
     public TopologyContext(StormTopology topology, Map stormConf,
             Map<Integer, String> taskToComponent, String stormId,
             String codeDir, String pidDir, Integer taskId,
-            Integer workerPort, List<Integer> workerTasks) {
+            Integer workerPort, List<Number> workerTasks) {
         super(topology, stormConf, taskToComponent, stormId, codeDir, pidDir, workerPort, workerTasks);
         _taskId = taskId;
     }

File: src/jvm/backtype/storm/topology/ComponentConfigurationDeclarer.java
Patch:
@@ -7,5 +7,6 @@ public interface ComponentConfigurationDeclarer<T extends ComponentConfiguration
     T addConfiguration(String config, Object value);
     T setDebug(boolean debug);
     T setMaxTaskParallelism(Number val);
-    T setMaxSpoutPending(Number val);    
+    T setMaxSpoutPending(Number val);
+    T setNumTasks(Number val);
 }

File: src/jvm/backtype/storm/serialization/SerializationFactory.java
Patch:
@@ -4,6 +4,7 @@
 import backtype.storm.generated.ComponentCommon;
 import backtype.storm.generated.StormTopology;
 import backtype.storm.transactional.TransactionAttempt;
+import backtype.storm.tuple.Values;
 import backtype.storm.utils.ListDelegate;
 import backtype.storm.utils.Utils;
 import carbonite.JavaBridge;
@@ -52,6 +53,7 @@ public static ObjectBuffer getKryo(Map conf) {
         k.register(HashSet.class);
         k.register(BigInteger.class, new BigIntegerSerializer());
         k.register(TransactionAttempt.class);
+        k.register(Values.class);
         JavaBridge clojureSerializersBridge = new JavaBridge();
         clojureSerializersBridge.registerClojureCollections(k);
         clojureSerializersBridge.registerClojurePrimitives(k);

File: src/jvm/backtype/storm/StormSubmitter.java
Patch:
@@ -81,6 +81,9 @@ private static void submitJar(Map conf) {
     }
     
     public static String submitJar(Map conf, String localJar) {
+        if(localJar==null) {
+            throw new RuntimeException("Must submit topologies using the 'storm' client script so that StormSubmitter knows which jar to upload.");
+        }
         NimbusClient client = NimbusClient.getConfiguredClient(conf);
         try {
             String uploadLocation = client.getClient().beginFileUpload();

File: src/jvm/backtype/storm/utils/Utils.java
Patch:
@@ -103,7 +103,7 @@ public static Map findAndReadConfigFile(String name, boolean mustExist) {
                 else return new HashMap();
             }
             if(resources.size() > 1) {
-                throw new RuntimeException("Found multiple " + name + " resources");
+                throw new RuntimeException("Found multiple " + name + " resources. You're probably bundling the Storm jars with your topology jar.");
             }
             URL resource = resources.get(0);
             Yaml yaml = new Yaml();

File: src/jvm/storm/kafka/KafkaSpout.java
Patch:
@@ -197,10 +197,8 @@ public void open(Map conf, TopologyContext context, SpoutOutputCollector collect
 
     @Override
     public void nextTuple() {
-        //TODO: change behavior to stick with one partition until it's empty
         for(int i=0; i<_managedPartitions.size(); i++) {
             int partition = _managedPartitions.get(_currPartitionIndex);
-            _currPartitionIndex = (_currPartitionIndex + 1) % _managedPartitions.size();
             EmitState state = _managers.get(partition).next();
             if(state!=EmitState.EMITTED_MORE_LEFT) {
                 _currPartitionIndex = (_currPartitionIndex + 1) % _managedPartitions.size();                

File: src/main/java/backtype/storm/contrib/jms/spout/JmsSpout.java
Patch:
@@ -1,5 +1,6 @@
 package backtype.storm.contrib.jms.spout;
 
+import java.io.Serializable;
 import java.util.Map;
 import java.util.Timer;
 import java.util.TimerTask;
@@ -67,7 +68,7 @@ public class JmsSpout implements IRichSpout, MessageListener {
 	private transient Session session;
 	
 	private boolean hasFailures = false;
-	private Object recoveryMutex = new Object();
+	public final Serializable recoveryMutex = "RECOVERY_MUTEX";
 	private Timer recoveryTimer = null;
 	private long recoveryPeriod = -1; // default to disabled
 	

File: src/main/java/backtype/storm/contrib/jms/spout/JmsSpout.java
Patch:
@@ -1,5 +1,6 @@
 package backtype.storm.contrib.jms.spout;
 
+import java.io.Serializable;
 import java.util.Map;
 import java.util.Timer;
 import java.util.concurrent.ConcurrentHashMap;
@@ -66,7 +67,7 @@ public class JmsSpout implements IRichSpout, MessageListener {
 	private transient Session session;
 	
 	private boolean hasFailures = false;
-	public Object recoveryMutex = new Object();
+	public Serializable recoveryMutex = "RECOVERY_MUTEX";
 	private Timer recoveryTimer = null;
 	private long recoveryPeriod = 30*1000;  // Default to 30 seconds
 	

File: src/jvm/storm/kafka/KafkaUtils.java
Patch:
@@ -11,7 +11,6 @@
 import kafka.javaapi.message.ByteBufferMessageSet;
 import kafka.message.Message;
 import kafka.message.MessageAndOffset;
-import kafka.message.MessageSet;
 
 public class KafkaUtils {
     

File: src/jvm/backtype/storm/topology/ComponentConfigurationDeclarer.java
Patch:
@@ -6,6 +6,6 @@ public interface ComponentConfigurationDeclarer<T extends ComponentConfiguration
     T addConfigurations(Map conf);
     T addConfiguration(String config, Object value);
     T setDebug(boolean debug);
-    T setMaxTaskParallelism(Integer val);
-    T setMaxSpoutPending(Integer val);    
+    T setMaxTaskParallelism(Number val);
+    T setMaxSpoutPending(Number val);    
 }

File: src/jvm/backtype/storm/utils/IndifferentAccessMap.java
Patch:
@@ -80,13 +80,13 @@ public IPersistentMap assoc(Object k, Object v) {
         return new IndifferentAccessMap(getMap().assoc(k, v));
     }
 
-    public IPersistentMap assocEx(Object k, Object v) throws Exception {
+    public IPersistentMap assocEx(Object k, Object v) {
         if(k instanceof Keyword) return assocEx(((Keyword) k).getName(), v);
 
         return new IndifferentAccessMap(getMap().assocEx(k, v));
     }
 
-    public IPersistentMap without(Object k) throws Exception {
+    public IPersistentMap without(Object k) {
         if(k instanceof Keyword) return without(((Keyword) k).getName());
 
         return new IndifferentAccessMap(getMap().without(k));

File: src/jvm/backtype/storm/topology/ComponentConfigurationDeclarer.java
Patch:
@@ -6,6 +6,6 @@ public interface ComponentConfigurationDeclarer<T extends ComponentConfiguration
     T addConfigurations(Map conf);
     T addConfiguration(String config, Object value);
     T setDebug(boolean debug);
-    T setMaxTaskParallelism(Integer val);
-    T setMaxSpoutPending(Integer val);    
+    T setMaxTaskParallelism(Number val);
+    T setMaxSpoutPending(Number val);    
 }

File: src/jvm/backtype/storm/coordination/BatchOutputCollector.java
Patch:
@@ -23,5 +23,7 @@ public void emitDirect(int taskId, List<Object> tuple) {
         emitDirect(taskId, Utils.DEFAULT_STREAM_ID, tuple);
     }
     
-    public abstract void emitDirect(int taskId, String streamId, List<Object> tuple);    
+    public abstract void emitDirect(int taskId, String streamId, List<Object> tuple); 
+    
+    public abstract void reportError(Throwable error);
 }

File: src/jvm/backtype/storm/topology/IBasicOutputCollector.java
Patch:
@@ -5,4 +5,5 @@
 public interface IBasicOutputCollector {
     List<Integer> emit(String streamId, List<Object> tuple);
     void emitDirect(int taskId, String streamId, List<Object> tuple);
+    void reportError(Throwable t);
 }

File: src/jvm/backtype/storm/transactional/partitioned/IOpaquePartitionedTransactionalSpout.java
Patch:
@@ -19,6 +19,7 @@ public interface Coordinator {
          * repeatedly in a loop).
          */
         boolean isReady();
+        void close();
     }
     
     public interface Emitter<X> {

File: src/jvm/backtype/storm/transactional/partitioned/IOpaquePartitionedTransactionalSpout.java
Patch:
@@ -19,6 +19,7 @@ public interface Coordinator {
          * repeatedly in a loop).
          */
         boolean isReady();
+        void close();
     }
     
     public interface Emitter<X> {

File: src/jvm/storm/starter/TransactionalWords.java
Patch:
@@ -114,7 +114,7 @@ public void finishBatch() {
                         newVal.count = val.count;
                     }
                     newVal.count = newVal.count + _counts.get(key);
-                    COUNT_DATABASE.put(key, val);
+                    COUNT_DATABASE.put(key, newVal);
                 } else {
                     newVal = val;
                 }

File: src/jvm/storm/starter/TransactionalWords.java
Patch:
@@ -114,7 +114,7 @@ public void finishBatch() {
                         newVal.count = val.count;
                     }
                     newVal.count = newVal.count + _counts.get(key);
-                    COUNT_DATABASE.put(key, val);
+                    COUNT_DATABASE.put(key, newVal);
                 } else {
                     newVal = val;
                 }

File: src/jvm/storm/kafka/OpaqueTransactionalKafkaSpout.java
Patch:
@@ -31,7 +31,7 @@ public IOpaquePartitionedTransactionalSpout.Emitter<BatchMeta> getEmitter(Map co
     
     @Override
     public IOpaquePartitionedTransactionalSpout.Coordinator getCoordinator(Map map, TopologyContext tc) {
-        throw new UnsupportedOperationException("Not supported yet.");
+        return new Coordinator();
     }
 
     @Override

File: src/jvm/storm/kafka/OpaqueTransactionalKafkaSpout.java
Patch:
@@ -31,7 +31,7 @@ public IOpaquePartitionedTransactionalSpout.Emitter<BatchMeta> getEmitter(Map co
     
     @Override
     public IOpaquePartitionedTransactionalSpout.Coordinator getCoordinator(Map map, TopologyContext tc) {
-        throw new UnsupportedOperationException("Not supported yet.");
+        return new Coordinator();
     }
 
     @Override

File: src/jvm/backtype/storm/coordination/BatchOutputCollector.java
Patch:
@@ -23,5 +23,7 @@ public void emitDirect(int taskId, List<Object> tuple) {
         emitDirect(taskId, Utils.DEFAULT_STREAM_ID, tuple);
     }
     
-    public abstract void emitDirect(int taskId, String streamId, List<Object> tuple);    
+    public abstract void emitDirect(int taskId, String streamId, List<Object> tuple); 
+    
+    public abstract void reportError(Throwable error);
 }

File: src/jvm/backtype/storm/topology/IBasicOutputCollector.java
Patch:
@@ -5,4 +5,5 @@
 public interface IBasicOutputCollector {
     List<Integer> emit(String streamId, List<Object> tuple);
     void emitDirect(int taskId, String streamId, List<Object> tuple);
+    void reportError(Throwable t);
 }

File: src/jvm/backtype/storm/task/IBolt.java
Patch:
@@ -33,7 +33,7 @@ public interface IBolt extends Serializable {
      * 
      * @param stormConf The Storm configuration for this bolt. This is the configuration provided to the topology merged in with cluster configuration on this machine.
      * @param context This object can be used to get information about this task's place within the topology, including the task id and component id of this task, input and output information, etc.
-     * @param collector The collector is used to emit tuples from this bolt. Tuples can be emitted at any time, including the prepare and cleanup methods. The collector is not thread-safe and should be saved as an instance variable of this bolt object.
+     * @param collector The collector is used to emit tuples from this bolt. Tuples can be emitted at any time, including the prepare and cleanup methods. The collector is thread-safe and should be saved as an instance variable of this bolt object.
      */
     void prepare(Map stormConf, TopologyContext context, OutputCollector collector);
 

File: src/jvm/backtype/storm/grouping/CustomStreamGrouping.java
Patch:
@@ -9,8 +9,8 @@
 public interface CustomStreamGrouping extends Serializable {
     
    /**
-     * Tells the stream grouping at runtime the number of tasks in the target bolt.
-     * This information should be used in taskIndicies to determine the target tasks.
+     * Tells the stream grouping at runtime the tasks in the target bolt.
+     * This information should be used in chooseTasks to determine the target tasks.
      * 
      * It also tells the grouping the metadata on the stream this grouping will be used on.
      */

File: examples/src/main/java/backtype/storm/contrib/jms/example/ExampleJmsTopology.java
Patch:
@@ -64,7 +64,7 @@ public static void main(String[] args) throws Exception {
 		builder.setBolt(FINAL_BOLT, new GenericBolt("FINAL_BOLT", true, true), 3).shuffleGrouping(
 				INTERMEDIATE_BOLT);
 		
-		// bolt that subscribes to the intermeidate bold, and publishes to a JMS Topic		
+		// bolt that subscribes to the intermediate bolt, and publishes to a JMS Topic		
 		JmsBolt jmsBolt = new JmsBolt();
 		jmsBolt.setJmsProvider(jmsTopicProvider);
 		

File: examples/src/main/java/backtype/storm/contrib/jms/example/ExampleJmsTopology.java
Patch:
@@ -64,7 +64,7 @@ public static void main(String[] args) throws Exception {
 		builder.setBolt(FINAL_BOLT, new GenericBolt("FINAL_BOLT", true, true), 3).shuffleGrouping(
 				INTERMEDIATE_BOLT);
 		
-		// bolt that subscribes to the intermeidate bold, and publishes to a JMS Topic		
+		// bolt that subscribes to the intermediate bolt, and publishes to a JMS Topic		
 		JmsBolt jmsBolt = new JmsBolt();
 		jmsBolt.setJmsProvider(jmsTopicProvider);
 		

File: src/jvm/storm/starter/bolt/SingleJoinBolt.java
Patch:
@@ -32,8 +32,8 @@ public SingleJoinBolt(Fields outFields) {
     public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
         _fieldLocations = new HashMap<String, GlobalStreamId>();
         _collector = collector;
-        long timeout = (Long) conf.get(Config.TOPOLOGY_MESSAGE_TIMEOUT_SECS);
-        _pending = new TimeCacheMap<List<Object>, Map<GlobalStreamId, Tuple>>((int) timeout, new ExpireCallback());
+        int timeout = ((Number) conf.get(Config.TOPOLOGY_MESSAGE_TIMEOUT_SECS)).intValue();
+        _pending = new TimeCacheMap<List<Object>, Map<GlobalStreamId, Tuple>>(timeout, new ExpireCallback());
         _numSources = context.getThisSources().size();
         Set<String> idFields = null;
         for(GlobalStreamId source: context.getThisSources().keySet()) {

File: src/jvm/backtype/storm/spout/ISpoutOutputCollector.java
Patch:
@@ -8,5 +8,6 @@ public interface ISpoutOutputCollector {
     */
     List<Integer> emit(String streamId, List<Object> tuple, Object messageId);
     void emitDirect(int taskId, String streamId, List<Object> tuple, Object messageId);
+    void reportError(Throwable error);
 }
 

File: src/jvm/backtype/storm/hooks/info/BoltAckInfo.java
Patch:
@@ -4,10 +4,10 @@
 
 public class BoltAckInfo {
     public Tuple tuple;
-    public long completeLatencyMs;
+    public long processLatencyMs;
     
-    public BoltAckInfo(Tuple tuple, long completeLatencyMs) {
+    public BoltAckInfo(Tuple tuple, long processLatencyMs) {
         this.tuple = tuple;
-        this.completeLatencyMs = completeLatencyMs;
+        this.processLatencyMs = processLatencyMs;
     }
 }

File: src/jvm/backtype/storm/StormSubmitter.java
Patch:
@@ -40,6 +40,9 @@ public static void setLocalNimbus(Nimbus.Iface localNimbusHandler) {
      * @throws InvalidTopologyException if an invalid topology was submitted
      */
     public static void submitTopology(String name, Map stormConf, StormTopology topology) throws AlreadyAliveException, InvalidTopologyException {
+        if(!Utils.isValidConf(stormConf)) {
+            throw new IllegalArgumentException("Storm conf is not valid. Must be json-serializable");
+        }
         stormConf = new HashMap(stormConf);
         stormConf.putAll(Utils.readCommandLineOpts());
         Map conf = Utils.readStormConfig();

File: src/jvm/backtype/storm/drpc/DRPCSpout.java
Patch:
@@ -149,6 +149,6 @@ public void declareOutputFields(OutputFieldsDeclarer declarer) {
 
     @Override
     public Map<String, Object> getComponentConfiguration() {
-        return new HashMap<String, Object>();
+        return null;
     }
 }

File: src/jvm/backtype/storm/topology/base/BaseComponent.java
Patch:
@@ -1,12 +1,11 @@
 package backtype.storm.topology.base;
 
 import backtype.storm.topology.IComponent;
-import java.util.HashMap;
 import java.util.Map;
 
 public abstract class BaseComponent implements IComponent {
     @Override
     public Map<String, Object> getComponentConfiguration() {
-        return new HashMap<String, Object>();
+        return null;
     }    
 }

File: src/jvm/backtype/storm/transactional/partitioned/OpaquePartitionedTransactionalSpoutExecutor.java
Patch:
@@ -43,7 +43,7 @@ public Emitter(Map conf, TopologyContext context) {
             _index = context.getThisTaskIndex();
             _numTasks = context.getComponentTasks(context.getThisComponentId()).size();
             _state = TransactionalState.newUserState(conf, (String) conf.get(Config.TOPOLOGY_TRANSACTIONAL_ID), getComponentConfiguration()); 
-            List<String> existingPartitions = _state.list("/");
+            List<String> existingPartitions = _state.list("");
             for(String p: existingPartitions) {
                 int partition = Integer.parseInt(p);
                 if((partition - _index) % _numTasks == 0) {

File: src/jvm/backtype/storm/transactional/TransactionalSpoutCoordinator.java
Patch:
@@ -80,7 +80,7 @@ public void nextTuple() {
     public void ack(Object msgId) {
         TransactionAttempt tx = (TransactionAttempt) msgId;
         TransactionStatus status = _activeTx.get(tx.getTransactionId());
-        if(tx.equals(status.attempt)) {
+        if(status!=null && tx.equals(status.attempt)) {
             if(status.status==AttemptStatus.PROCESSING) {
                 status.status = AttemptStatus.PROCESSED;
             } else if(status.status==AttemptStatus.COMMITTING) {

File: src/jvm/backtype/storm/coordination/CoordinatedBolt.java
Patch:
@@ -64,7 +64,7 @@ public String toString() {
         }
     }
 
-    public class CoordinatedOutputCollector extends OutputCollector {
+    public class CoordinatedOutputCollector implements IOutputCollector {
         IOutputCollector _delegate;
 
         public CoordinatedOutputCollector(IOutputCollector delegate) {
@@ -185,7 +185,7 @@ public void prepare(Map config, TopologyContext context, OutputCollector collect
         }
         _tracked = new TimeCacheMap<Object, TrackingInfo>(Utils.getInt(config.get(Config.TOPOLOGY_MESSAGE_TIMEOUT_SECS)), callback);
         _collector = collector;
-        _delegate.prepare(config, context, new CoordinatedOutputCollector(collector));
+        _delegate.prepare(config, context, new OutputCollector(new CoordinatedOutputCollector(collector)));
         for(String component: Utils.get(context.getThisTargets(),
                                         Constants.COORDINATED_STREAM_ID,
                                         new HashMap<String, Grouping>())

File: src/jvm/backtype/storm/task/OutputCollectorImpl.java
Patch:
@@ -27,6 +27,7 @@ public class OutputCollectorImpl extends OutputCollector {
     private Map<Tuple, List<Long>> _pendingAcks = new ConcurrentHashMap<Tuple, List<Long>>();
     
     public OutputCollectorImpl(TopologyContext context, IInternalOutputCollector collector) {
+        super(null); // TODO: remove
         _context = context;
         _collector = collector;
     }

File: src/jvm/backtype/storm/testing/NGrouping.java
Patch:
@@ -1,7 +1,7 @@
 package backtype.storm.testing;
 
 import backtype.storm.grouping.CustomStreamGrouping;
-import backtype.storm.tuple.Tuple;
+import backtype.storm.tuple.Fields;
 import java.util.ArrayList;
 import java.util.List;
 
@@ -13,11 +13,11 @@ public NGrouping(int n) {
     }
     
     @Override
-    public void prepare(int numTasks) {
+    public void prepare(Fields outFields, int numTasks) {
     }
 
     @Override
-    public List<Integer> taskIndices(Tuple tuple) {
+    public List<Integer> taskIndices(List<Object> values) {
         List<Integer> ret = new ArrayList<Integer>();
         for(int i=0; i<_n; i++) {
             ret.add(i);

File: src/jvm/backtype/storm/tuple/MessageId.java
Patch:
@@ -25,9 +25,9 @@ public static MessageId makeId(Map<Long, Long> anchorsToIds) {
         return new MessageId(anchorsToIds);
     }
         
-    public static MessageId makeRootId(long id) {
+    public static MessageId makeRootId(long id, long val) {
         Map<Long, Long> anchorsToIds = new HashMap<Long, Long>();
-        anchorsToIds.put(id, id);
+        anchorsToIds.put(id, val);
         return new MessageId(anchorsToIds);
     }
     

File: src/jvm/backtype/storm/transactional/TransactionalSpoutBatchExecutor.java
Patch:
@@ -34,13 +34,13 @@ public void execute(Tuple input) {
         try {
             _emitter.emitBatch(attempt, input.getValue(1), _collector);
             _collector.ack(input);
+            // this is valid here because the batch has been successfully emitted, 
+            // so we can safely delete metadata for prior transactions
+            _emitter.cleanupBefore((BigInteger) input.getValue(2));
         } catch(FailedException e) {
             LOG.warn("Failed to emit batch for transaction", e);
             _collector.fail(input);
         }
-        // this is valid here because the batch has been successfully emitted, 
-        // so we can safely delete metadata for prior transactions
-        _emitter.cleanupBefore((BigInteger) input.getValue(2));
     }
 
     @Override

File: src/jvm/backtype/storm/drpc/DRPCSpout.java
Patch:
@@ -57,7 +57,7 @@ public void open(Map conf, TopologyContext context, SpoutOutputCollector collect
 
             int port = Utils.getInt(conf.get(Config.DRPC_INVOCATIONS_PORT));
             List<String> servers = (List<String>) conf.get(Config.DRPC_SERVERS);
-            if(servers.isEmpty()) {
+            if(servers == null || servers.isEmpty()) {
                 throw new RuntimeException("No DRPC servers configured for topology");   
             }
             if(numTasks < servers.size()) {

File: src/jvm/backtype/storm/Constants.java
Patch:
@@ -1,8 +1,8 @@
 package backtype.storm;
 
-import backtype.storm.drpc.CoordinatedBolt;
+import backtype.storm.coordination.CoordinatedBolt;
 
 
 public class Constants {
-    public static final String COORDINATED_STREAM_ID = CoordinatedBolt.class.getName() + "/coord-stream";
+    public static final String COORDINATED_STREAM_ID = CoordinatedBolt.class.getName() + "/coord-stream";    
 }

File: src/jvm/backtype/storm/drpc/LinearDRPCInputDeclarer.java
Patch:
@@ -1,8 +1,9 @@
 package backtype.storm.drpc;
 
+import backtype.storm.topology.ComponentConfigurationDeclarer;
 import backtype.storm.tuple.Fields;
 
-public interface LinearDRPCInputDeclarer {
+public interface LinearDRPCInputDeclarer extends ComponentConfigurationDeclarer<LinearDRPCInputDeclarer> {
     public LinearDRPCInputDeclarer fieldsGrouping(Fields fields);
     public LinearDRPCInputDeclarer fieldsGrouping(String streamId, Fields fields);
 

File: src/jvm/backtype/storm/spout/ISpoutOutputCollector.java
Patch:
@@ -1,7 +1,6 @@
 package backtype.storm.spout;
 
 import java.util.List;
-import backtype.storm.tuple.Tuple;
 
 public interface ISpoutOutputCollector {
     /**

File: src/jvm/backtype/storm/task/IOutputCollector.java
Patch:
@@ -1,13 +1,13 @@
 package backtype.storm.task;
 
-import java.util.List;
 import backtype.storm.tuple.Tuple;
 import java.util.Collection;
+import java.util.List;
 
 public interface IOutputCollector {
     /**
-        Returns the task ids that received the tuples.
-    */
+     *  Returns the task ids that received the tuples.
+     */
     List<Integer> emit(String streamId, Collection<Tuple> anchors, List<Object> tuple);
     void emitDirect(int taskId, String streamId, Collection<Tuple> anchors, List<Object> tuple);
     void ack(Tuple input);

File: src/jvm/backtype/storm/task/OutputCollector.java
Patch:
@@ -12,7 +12,6 @@
  * form of stream processing, see IBasicBolt and BasicOutputCollector.
  */
 public abstract class OutputCollector implements IOutputCollector {
-
     /**
      * Emits a new tuple to a specific stream with a single anchor.
      *
@@ -26,7 +25,7 @@ public List<Integer> emit(String streamId, Tuple anchor, List<Object> tuple) {
     }
 
     /**
-     * Emits a new unanchored tuple to the specified stream. Beacuse it's unanchored,
+     * Emits a new unanchored tuple to the specified stream. Because it's unanchored,
      * if a failure happens downstream, this new tuple won't affect whether any
      * spout tuples are considered failed or not.
      * 

File: src/jvm/backtype/storm/task/ShellBolt.java
Patch:
@@ -14,7 +14,6 @@
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
-import java.util.UUID;
 import org.apache.log4j.Logger;
 import org.json.simple.JSONObject;
 import org.json.simple.JSONValue;

File: src/jvm/backtype/storm/testing/TestAggregatesCounter.java
Patch:
@@ -1,18 +1,18 @@
 package backtype.storm.testing;
 
+import backtype.storm.topology.base.BaseRichBolt;
 import backtype.storm.task.OutputCollector;
 import backtype.storm.topology.OutputFieldsDeclarer;
 import backtype.storm.tuple.Tuple;
 import backtype.storm.tuple.Fields;
 import java.util.Map;
 import backtype.storm.task.TopologyContext;
-import backtype.storm.topology.IRichBolt;
 import java.util.HashMap;
 import org.apache.log4j.Logger;
 import static backtype.storm.utils.Utils.tuple;
 
 
-public class TestAggregatesCounter implements IRichBolt {
+public class TestAggregatesCounter extends BaseRichBolt {
     public static Logger LOG = Logger.getLogger(TestWordCounter.class);
 
     Map<String, Integer> _counts;

File: src/jvm/backtype/storm/testing/TestGlobalCount.java
Patch:
@@ -1,17 +1,17 @@
 package backtype.storm.testing;
 
+import backtype.storm.topology.base.BaseRichBolt;
 import backtype.storm.task.OutputCollector;
 import backtype.storm.topology.OutputFieldsDeclarer;
 import backtype.storm.tuple.Tuple;
 import backtype.storm.tuple.Fields;
 import java.util.Map;
 import backtype.storm.task.TopologyContext;
-import backtype.storm.topology.IRichBolt;
 import org.apache.log4j.Logger;
 import static backtype.storm.utils.Utils.tuple;
 
 
-public class TestGlobalCount implements IRichBolt {
+public class TestGlobalCount extends BaseRichBolt {
     public static Logger LOG = Logger.getLogger(TestWordCounter.class);
 
     private int _count;

File: src/jvm/backtype/storm/testing/TestWordCounter.java
Patch:
@@ -1,18 +1,18 @@
 package backtype.storm.testing;
 
+import backtype.storm.topology.base.BaseBasicBolt;
 import backtype.storm.topology.OutputFieldsDeclarer;
 import backtype.storm.tuple.Tuple;
 import backtype.storm.tuple.Fields;
 import java.util.Map;
 import backtype.storm.task.TopologyContext;
 import backtype.storm.topology.BasicOutputCollector;
-import backtype.storm.topology.IBasicBolt;
 import java.util.HashMap;
 import org.apache.log4j.Logger;
 import static backtype.storm.utils.Utils.tuple;
 
 
-public class TestWordCounter implements IBasicBolt {
+public class TestWordCounter extends BaseBasicBolt {
     public static Logger LOG = Logger.getLogger(TestWordCounter.class);
 
     Map<String, Integer> _counts;

File: src/jvm/backtype/storm/tuple/MessageId.java
Patch:
@@ -1,5 +1,6 @@
 package backtype.storm.tuple;
 
+import backtype.storm.utils.Utils;
 import backtype.storm.utils.WritableUtils;
 import java.io.DataInputStream;
 import java.io.DataOutputStream;
@@ -8,13 +9,12 @@
 import java.util.Map;
 import java.util.Map.Entry;
 import java.util.Set;
-import java.util.UUID;
 
 public class MessageId {
     private Map<Long, Long> _anchorsToIds;
     
     public static long generateId() {
-        return UUID.randomUUID().getLeastSignificantBits();
+        return Utils.randomLong();
     }
 
     public static MessageId makeUnanchored() {

File: src/jvm/backtype/storm/tuple/Tuple.java
Patch:
@@ -1,9 +1,7 @@
 package backtype.storm.tuple;
 
 import backtype.storm.generated.GlobalStreamId;
-import backtype.storm.generated.Grouping;
 import backtype.storm.task.TopologyContext;
-import backtype.storm.utils.Utils;
 import clojure.lang.ILookup;
 import clojure.lang.Seqable;
 import clojure.lang.Indexed;
@@ -73,7 +71,7 @@ public Tuple(TopologyContext context, List<Object> values, int taskId, String st
     public Tuple(TopologyContext context, List<Object> values, int taskId, String streamId) {
         this(context, values, taskId, streamId, MessageId.makeUnanchored());
     }
-
+    
     public Tuple copyWithNewId(long id) {
         Map<Long, Long> newIds = new HashMap<Long, Long>();
         for(Long anchor: this.id.getAnchorsToIds().keySet()) {

File: src/jvm/backtype/storm/drpc/DRPCSpout.java
Patch:
@@ -63,7 +63,7 @@ public void open(Map conf, TopologyContext context, SpoutOutputCollector collect
 
             int port = Utils.getInt(conf.get(Config.DRPC_INVOCATIONS_PORT));
             List<String> servers = (List<String>) conf.get(Config.DRPC_SERVERS);
-            if(servers.isEmpty()) {
+            if(servers == null || servers.isEmpty()) {
                 throw new RuntimeException("No DRPC servers configured for topology");   
             }
             if(numTasks < servers.size()) {

File: src/jvm/backtype/storm/transactional/TransactionalSpoutCoordinator.java
Patch:
@@ -57,7 +57,7 @@ public void open(Map conf, TopologyContext context, SpoutOutputCollector collect
         _coordinator = _spout.getCoordinator(conf, context);
         _currTransaction = getStoredCurrTransaction(_state);
         if(!conf.containsKey(Config.TOPOLOGY_MAX_SPOUT_PENDING)) {
-            _maxTransactionActive = 0;
+            _maxTransactionActive = 1;
         } else {
             _maxTransactionActive = Utils.getInt(conf.get(Config.TOPOLOGY_MAX_SPOUT_PENDING));
         }

File: src/jvm/backtype/storm/transactional/TransactionalSpoutBatchExecutor.java
Patch:
@@ -38,6 +38,8 @@ public void execute(Tuple input) {
             LOG.warn("Failed to emit batch for transaction", e);
             _collector.fail(input);
         }
+        // this is valid here because the batch has been successfully emitted, 
+        // so we can safely delete metadata for prior transactions
         _emitter.cleanupBefore((BigInteger) input.getValue(2));
     }
 

File: src/jvm/backtype/storm/transactional/TransactionalTopologyBuilder.java
Patch:
@@ -38,7 +38,7 @@ public class TransactionalTopologyBuilder {
     String _id;
     String _spoutId;
     ITransactionalSpout _spout;
-    Map<String, Component> _bolts;
+    Map<String, Component> _bolts = new HashMap<String, Component>();
     Integer _spoutParallelism;
     List<Map> _spoutConfs = new ArrayList();
     
@@ -91,8 +91,9 @@ public StormTopology buildTopology() {
         declarer.addConfiguration(Config.TOPOLOGY_TRANSACTIONAL_ID, _id);
 
         builder.setBolt(_spoutId,
+                // TODO: receiving the commit stream should not make it send out coordinated tuples
+                // to consumers... ***********************************
                         new CoordinatedBolt(new TransactionalSpoutBatchExecutor(_spout),
-                                             coordinator,
                                              null,
                                              null),
                         _spoutParallelism)

File: src/jvm/backtype/storm/transactional/TransactionalSpoutCoordinator.java
Patch:
@@ -43,7 +43,7 @@ public TransactionalSpoutCoordinator(ITransactionalSpout spout) {
     
     @Override
     public void open(Map conf, TopologyContext context, SpoutOutputCollector collector) {
-        _state = TransactionalState.newCoordinatorState(conf, (String) conf.get(Config.TOPOLOGY_TRANSACTIONAL_ID), _spout);
+        _state = TransactionalState.newCoordinatorState(conf, (String) conf.get(Config.TOPOLOGY_TRANSACTIONAL_ID), _spout.getComponentConfiguration());
         _coordinatorState = new RotatingTransactionalState(_state, META_DIR, true);
         _collector = collector;
         _coordinator = _spout.getCoordinator(conf, context);

File: src/jvm/backtype/storm/transactional/partitioned/PartitionedTransactionalSpoutExecutor.java
Patch:
@@ -47,7 +47,7 @@ class Emitter implements ITransactionalSpout.Emitter<Integer> {
         
         public Emitter(Map conf, TopologyContext context) {
             _emitter = _spout.getEmitter(conf, context);
-            _state = TransactionalState.newUserState(conf, (String) conf.get(Config.TOPOLOGY_TRANSACTIONAL_ID), PartitionedTransactionalSpoutExecutor.this); 
+            _state = TransactionalState.newUserState(conf, (String) conf.get(Config.TOPOLOGY_TRANSACTIONAL_ID), getComponentConfiguration()); 
             _index = context.getThisTaskIndex();
             _numTasks = context.getComponentTasks(context.getThisComponentId()).size();
         }

File: src/jvm/backtype/storm/transactional/TransactionalTopologyBuilder.java
Patch:
@@ -31,7 +31,7 @@
  * essentially want to implement a file lock on top of zk (use ephemeral nodes?)
  * or just use the topology name?
  * 
- * /
+ */
 
 public class TransactionalTopologyBuilder {
     String _id;

File: src/jvm/backtype/storm/spout/ISpoutOutputCollector.java
Patch:
@@ -1,7 +1,6 @@
 package backtype.storm.spout;
 
 import java.util.List;
-import backtype.storm.tuple.Tuple;
 
 public interface ISpoutOutputCollector {
     /**

File: src/jvm/backtype/storm/transactional/TransactionAttempt.java
Patch:
@@ -32,6 +32,6 @@ public boolean equals(Object o) {
 
     @Override
     public String toString() {
-        return "" + _txid + ": " + _attemptId;
+        return "" + _txid + ":" + _attemptId;
     }    
 }

File: src/jvm/backtype/storm/transactional/TransactionalSpoutCoordinator.java
Patch:
@@ -101,6 +101,8 @@ public void declareOutputFields(OutputFieldsDeclarer declarer) {
     }
     
     private void sync() {
+        // TODO: this code might be redundant. can just find the next transaction that needs a batch or commit tuple
+        // and emit that, instead of iterating through (MAX_SPOUT_PENDING should take care of things)
         TransactionStatus maybeCommit = _activeTx.get(_currTransaction);
         if(maybeCommit!=null && maybeCommit.status == AttemptStatus.PROCESSED) {
             maybeCommit.status = AttemptStatus.COMMITTING;

File: src/jvm/backtype/storm/transactional/TransactionalTopologyBuilder.java
Patch:
@@ -29,6 +29,7 @@
 /**
  * TODO: check to see if there are two topologies active with the same transactional id 
  * essentially want to implement a file lock on top of zk (use ephemeral nodes?)
+ * or just use the topology name?
  * 
  * Testing TODO:
  * 
@@ -45,6 +46,7 @@
  * 8. Test that it picks up where it left off when restarting the topology
  *       - run topology and restart it
  * 9. Test that coordinator and partitioned state are cleaned up properly (and not too early) - test rotatingtransactionalstate
+ * 10. Test that it repeats the meta on a fail instead of recmoputing (for both partition state and coordinator state)
  */
 public class TransactionalTopologyBuilder {
     String _id;

File: src/jvm/backtype/storm/utils/Utils.java
Patch:
@@ -209,7 +209,7 @@ public static long randomLong() {
     public static CuratorFramework newCurator(Map conf, String root) {
         List<String> serverPorts = new ArrayList<String>();
         for(String zkServer: (List<String>) conf.get(Config.STORM_ZOOKEEPER_SERVERS)) {
-            serverPorts.add(zkServer + Utils.getInt(conf.get(Config.STORM_ZOOKEEPER_PORT)));
+            serverPorts.add(zkServer + ":" + Utils.getInt(conf.get(Config.STORM_ZOOKEEPER_PORT)));
         }
         String zkStr = StringUtils.join(serverPorts, ",") + root; 
         try {

File: src/jvm/backtype/storm/transactional/partitioned/PartitionedTransactionalSpoutExecutor.java
Patch:
@@ -54,7 +54,7 @@ public Emitter(Map conf, TopologyContext context) {
         @Override
         public void emitBatch(final TransactionAttempt tx, final Object coordinatorMeta,
                 final TransactionalOutputCollector collector) {
-            int partitions = (int) coordinatorMeta;
+            int partitions = (Integer) coordinatorMeta;
             for(int i=_index; i < partitions; i+=_numTasks) {
                 if(!_partitionStates.containsKey(i)) {
                     _partitionStates.put(i, new RotatingTransactionalState(_state, "" + i));

File: src/jvm/backtype/storm/transactional/ITransactionalSpout.java
Patch:
@@ -16,7 +16,7 @@ public interface Emitter {
         // must always emit same batch for same transaction id
         // must emit attempt as first field in output tuple (any way to enforce this?)
         // for kafka: get up to X tuples, emit, store number of tuples for that partition in zk
-        Object emitBatch(TransactionAttempt tx, TransactionalOutputCollector collector);
+        void emitBatch(TransactionAttempt tx, Object coordinatorMeta, TransactionalOutputCollector collector);
         //can do things like cleanup user state in zk
         void cleanupBefore(BigInteger txid);
         void close();

File: src/jvm/backtype/storm/transactional/TransactionalSpoutBatchExecutor.java
Patch:
@@ -31,7 +31,7 @@ public void execute(Tuple input) {
             _emitter.cleanupBefore(attempt.getTransactionId());
         } else {
             _collector.setAnchor(input);
-            _emitter.emitBatch((TransactionAttempt) input.getValue(0), _collector);
+            _emitter.emitBatch(attempt, input.getValue(1), _collector);
             _collector.ack(input);
         }
     }

File: src/jvm/backtype/storm/transactional/state/TransactionalState.java
Patch:
@@ -10,8 +10,6 @@
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
-import java.util.logging.Level;
-import java.util.logging.Logger;
 import org.apache.zookeeper.CreateMode;
 
 public class TransactionalState {

File: src/jvm/backtype/storm/serialization/SerializationFactory.java
Patch:
@@ -10,7 +10,9 @@
 import com.esotericsoftware.kryo.Kryo;
 import com.esotericsoftware.kryo.ObjectBuffer;
 import com.esotericsoftware.kryo.Serializer;
+import com.esotericsoftware.kryo.serialize.BigIntegerSerializer;
 import com.esotericsoftware.kryo.serialize.SerializableSerializer;
+import java.math.BigInteger;
 import java.util.ArrayList;
 import java.util.Collections;
 import java.util.HashMap;
@@ -49,6 +51,7 @@ public static ObjectBuffer getKryo(Map conf) {
         k.register(HashMap.class);
         k.register(HashSet.class);
         k.register(TransactionAttempt.class);
+        k.register(BigInteger.class, new BigIntegerSerializer());
         JavaBridge clojureSerializersBridge = new JavaBridge();
         clojureSerializersBridge.registerClojureCollections(k);
         clojureSerializersBridge.registerClojurePrimitives(k);

File: src/jvm/backtype/storm/utils/Time.java
Patch:
@@ -76,6 +76,6 @@ public static boolean isThreadWaiting(Thread t) {
         synchronized(sleepTimesLock) {
             time = threadSleepTimes.get(t);
         }
-        return time!=null && currentTimeMillis() < time.longValue();
+        return !t.isAlive() || time!=null && currentTimeMillis() < time.longValue();
     }    
 }

File: src/jvm/backtype/storm/tuple/Tuple.java
Patch:
@@ -382,8 +382,8 @@ private PersistentArrayMap toMap() {
         Object array[] = new Object[values.size()*2];
         List<String> fields = getFields().toList();
         for(int i=0; i < values.size(); i++) {
-            array[i] = fields.get(i);
-            array[i+1] = values.get(i);
+            array[i*2] = fields.get(i);
+            array[(i*2)+1] = values.get(i);
         }
         return new PersistentArrayMap(array);
     }

File: src/jvm/backtype/storm/topology/TopologyBuilder.java
Patch:
@@ -213,9 +213,10 @@ private ComponentCommon getComponentCommon(String id, IComponent component) {
     
     private void initCommon(String id, IComponent component, Integer parallelism) {
         ComponentCommon common = new ComponentCommon();
-        common.set_parallelism_hint(parallelism);
+        common.set_inputs(new HashMap<GlobalStreamId, Grouping>());
+        if(parallelism!=null) common.set_parallelism_hint(parallelism);
         Map conf = component.getComponentConfiguration();
-        common.set_json_conf(JSONValue.toJSONString(conf));
+        if(conf!=null) common.set_json_conf(JSONValue.toJSONString(conf));
         _commons.put(id, common);
     }
 

File: src/jvm/backtype/storm/drpc/DRPCInvocationsClient.java
Patch:
@@ -1,4 +1,4 @@
-package backtype.storm.utils;
+package backtype.storm.drpc;
 
 import backtype.storm.generated.DRPCRequest;
 import backtype.storm.generated.DistributedRPCInvocations;

File: src/jvm/backtype/storm/drpc/DRPCSpout.java
Patch:
@@ -3,15 +3,13 @@
 import backtype.storm.Config;
 import backtype.storm.ILocalDRPC;
 import backtype.storm.generated.DRPCRequest;
-import backtype.storm.generated.DistributedRPC;
 import backtype.storm.generated.DistributedRPCInvocations;
 import backtype.storm.spout.SpoutOutputCollector;
 import backtype.storm.task.TopologyContext;
 import backtype.storm.topology.IRichSpout;
 import backtype.storm.topology.OutputFieldsDeclarer;
 import backtype.storm.tuple.Fields;
 import backtype.storm.tuple.Values;
-import backtype.storm.utils.DRPCInvocationsClient;
 import backtype.storm.utils.ServiceRegistry;
 import backtype.storm.utils.Utils;
 import java.util.ArrayList;

File: src/jvm/backtype/storm/drpc/ReturnResults.java
Patch:
@@ -7,7 +7,6 @@
 import backtype.storm.topology.IRichBolt;
 import backtype.storm.topology.OutputFieldsDeclarer;
 import backtype.storm.tuple.Tuple;
-import backtype.storm.utils.DRPCInvocationsClient;
 import backtype.storm.utils.ServiceRegistry;
 import backtype.storm.utils.Utils;
 import java.util.ArrayList;

File: src/jvm/backtype/storm/drpc/DRPCInvocationsClient.java
Patch:
@@ -1,4 +1,4 @@
-package backtype.storm.utils;
+package backtype.storm.drpc;
 
 import backtype.storm.generated.DRPCRequest;
 import backtype.storm.generated.DistributedRPCInvocations;

File: src/jvm/backtype/storm/drpc/DRPCSpout.java
Patch:
@@ -3,15 +3,13 @@
 import backtype.storm.Config;
 import backtype.storm.ILocalDRPC;
 import backtype.storm.generated.DRPCRequest;
-import backtype.storm.generated.DistributedRPC;
 import backtype.storm.generated.DistributedRPCInvocations;
 import backtype.storm.spout.SpoutOutputCollector;
 import backtype.storm.task.TopologyContext;
 import backtype.storm.topology.IRichSpout;
 import backtype.storm.topology.OutputFieldsDeclarer;
 import backtype.storm.tuple.Fields;
 import backtype.storm.tuple.Values;
-import backtype.storm.utils.DRPCInvocationsClient;
 import backtype.storm.utils.ServiceRegistry;
 import backtype.storm.utils.Utils;
 import java.util.ArrayList;

File: src/jvm/backtype/storm/drpc/ReturnResults.java
Patch:
@@ -7,7 +7,6 @@
 import backtype.storm.topology.IRichBolt;
 import backtype.storm.topology.OutputFieldsDeclarer;
 import backtype.storm.tuple.Tuple;
-import backtype.storm.utils.DRPCInvocationsClient;
 import backtype.storm.utils.ServiceRegistry;
 import backtype.storm.utils.Utils;
 import java.util.ArrayList;

File: src/jvm/backtype/storm/ILocalDRPC.java
Patch:
@@ -2,8 +2,9 @@
 
 import backtype.storm.daemon.Shutdownable;
 import backtype.storm.generated.DistributedRPC;
+import backtype.storm.generated.DistributedRPCInvocations;
 
 
-public interface ILocalDRPC extends DistributedRPC.Iface, Shutdownable {
+public interface ILocalDRPC extends DistributedRPC.Iface, DistributedRPCInvocations.Iface, Shutdownable {
     public String getServiceId();    
 }

File: src/jvm/backtype/storm/drpc/DRPCSpout.java
Patch:
@@ -62,7 +62,7 @@ public void open(Map conf, TopologyContext context, SpoutOutputCollector collect
             int numTasks = context.getComponentTasks(context.getThisComponentId()).size();
             int index = context.getThisTaskIndex();
 
-            int port = ((Long) conf.get(Config.DRPC_PORT)).intValue();
+            int port = Utils.getInt(conf.get(Config.DRPC_PORT));
             List<String> servers = (List<String>) conf.get(Config.DRPC_SERVERS);
             if(servers.isEmpty()) {
                 throw new RuntimeException("No DRPC servers configured for topology");   

File: src/jvm/backtype/storm/drpc/ReturnResults.java
Patch:
@@ -9,6 +9,7 @@
 import backtype.storm.tuple.Tuple;
 import backtype.storm.utils.DRPCClient;
 import backtype.storm.utils.ServiceRegistry;
+import backtype.storm.utils.Utils;
 import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.List;
@@ -36,7 +37,7 @@ public void execute(Tuple input) {
         if(returnInfo!=null) {
             Map retMap = (Map) JSONValue.parse(returnInfo);
             final String host = (String) retMap.get("host");
-            final int port = (int) ((Long) retMap.get("port")).longValue();                                   
+            final int port = Utils.getInt(retMap.get("port"));
             String id = (String) retMap.get("id");
             DistributedRPC.Iface client;
             if(local) {

File: src/jvm/backtype/storm/utils/NimbusClient.java
Patch:
@@ -13,7 +13,7 @@
 public class NimbusClient {
     public static NimbusClient getConfiguredClient(Map conf) {
         String nimbusHost = (String) conf.get(Config.NIMBUS_HOST);
-        int nimbusPort = ((Long) conf.get(Config.NIMBUS_THRIFT_PORT)).intValue();
+        int nimbusPort = Utils.getInt(conf.get(Config.NIMBUS_THRIFT_PORT));
         return new NimbusClient(nimbusHost, nimbusPort);
     }
 

File: src/jvm/backtype/storm/drpc/ReturnResults.java
Patch:
@@ -13,11 +13,13 @@
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
+import org.apache.log4j.Logger;
 import org.apache.thrift7.TException;
 import org.json.simple.JSONValue;
 
 
 public class ReturnResults implements IRichBolt {
+    public static final Logger LOG = Logger.getLogger(ReturnResults.class);
     OutputCollector _collector;
     boolean local;
 
@@ -55,6 +57,7 @@ public void execute(Tuple input) {
                 client.result(id, result);
                 _collector.ack(input);
             } catch(TException e) {
+                LOG.error("Failed to return results to DRPC server", e);
                 _collector.fail(input);
             }
         }

File: src/jvm/backtype/storm/serialization/SerializationFactory.java
Patch:
@@ -41,6 +41,7 @@ protected Serializer newDefaultSerializer(Class type) {
     public static ObjectBuffer getKryo(Map conf) {
         KryoSerializableDefault k = new KryoSerializableDefault();
         k.setRegistrationOptional((Boolean) conf.get(Config.TOPOLOGY_FALL_BACK_ON_JAVA_SERIALIZATION));
+        k.register(byte[].class);
         k.register(ListDelegate.class);
         k.register(ArrayList.class);
         k.register(HashMap.class);

File: src/jvm/storm/starter/ReachTopology.java
Patch:
@@ -209,7 +209,6 @@ public static void main(String[] args) throws Exception {
         
         
         Config conf = new Config();
-        conf.setDebug(true);
         
         if(args==null || args.length==0) {
             conf.setMaxTaskParallelism(3);

File: src/jvm/backtype/storm/drpc/PrepareRequest.java
Patch:
@@ -9,10 +9,11 @@
 import backtype.storm.tuple.Values;
 import java.util.Map;
 import java.util.Random;
+import backtype.storm.utils.Utils;
 
 
 public class PrepareRequest implements IBasicBolt {
-    public static final String ARGS_STREAM = "args";
+    public static final String ARGS_STREAM = Utils.DEFAULT_STREAM_ID;
     public static final String RETURN_STREAM = "ret";
     public static final String ID_STREAM = "id";
 

File: src/main/java/backtype/storm/contrib/jms/JmsMessageProducer.java
Patch:
@@ -15,7 +15,7 @@
  * <p/>
  * 
  * 
- * @author tgoetz
+ * @author P. Taylor Goetz
  *
  */
 public interface JmsMessageProducer extends Serializable{

File: src/main/java/backtype/storm/contrib/jms/JmsProvider.java
Patch:
@@ -9,7 +9,7 @@
  * and <code>Destination</code> JMS objects the <code>JmsSpout</code> needs to manage
  * a topic/queue connection over the course of it's lifecycle.
  * 
- * @author tgoetz
+ * @author P. Taylor Goetz
  *
  */
 public interface JmsProvider extends Serializable{

File: src/main/java/backtype/storm/contrib/jms/JmsTupleProducer.java
Patch:
@@ -10,7 +10,7 @@
 
 /**
  * Interface to define classes that can produce a Storm <code>Values</code> objects
- * from a <code>javax.jms.Message</object>.
+ * from a <code>javax.jms.Message</code> object>.
  * <p/>
  * Implementations are also responsible for declaring the output
  * fields they produce.
@@ -21,7 +21,7 @@
  * return <code>null</code> to indicate to the <code>JmsSpout</code> that
  * the message could not be processed.
  * 
- * @author tgoetz
+ * @author P. Taylor Goetz
  *
  */
 public interface JmsTupleProducer extends Serializable{

File: src/jvm/backtype/storm/drpc/PrepareRequest.java
Patch:
@@ -16,6 +16,7 @@
 public class PrepareRequest implements IBasicBolt {
     public static final int ARGS_STREAM = 1;
     public static final int RETURN_STREAM = 2;
+    public static final int ID_STREAM = 3;
 
     Random rand;
 
@@ -29,6 +30,7 @@ public void execute(Tuple tuple, BasicOutputCollector collector) {
         long requestId = rand.nextLong();
         collector.emit(ARGS_STREAM, new Values(requestId, args));
         collector.emit(RETURN_STREAM, new Values(requestId, returnInfo));
+        collector.emit(ID_STREAM, new Values(requestId));
     }
 
     public void cleanup() {
@@ -37,7 +39,6 @@ public void cleanup() {
     public void declareOutputFields(OutputFieldsDeclarer declarer) {
         declarer.declareStream(ARGS_STREAM, new Fields("request", "args"));
         declarer.declareStream(RETURN_STREAM, new Fields("request", "return"));
+        declarer.declareStream(ID_STREAM, new Fields("request"));
     }
-
-
 }

File: src/jvm/storm/starter/PrintSampleStream.java
Patch:
@@ -5,6 +5,7 @@
 import backtype.storm.LocalCluster;
 import backtype.storm.topology.TopologyBuilder;
 import backtype.storm.utils.Utils;
+import storm.starter.bolt.PrinterBolt;
 
 
 public class PrintSampleStream {        
@@ -14,10 +15,11 @@ public static void main(String[] args) {
         TopologyBuilder builder = new TopologyBuilder();
         
         builder.setSpout(1, new TwitterSampleSpout(username, pwd));
+        builder.setBolt(2, new PrinterBolt())
+                .shuffleGrouping(1);
                 
         
         Config conf = new Config();
-        conf.setDebug(true);
         
         
         LocalCluster cluster = new LocalCluster();

File: src/jvm/backtype/storm/utils/DRPCClient.java
Patch:
@@ -9,7 +9,7 @@
 import org.apache.thrift7.transport.TSocket;
 import org.apache.thrift7.transport.TTransport;
 
-
+//TODO: needs to auto-reconnect
 public class DRPCClient implements DistributedRPC.Iface {
     private TTransport conn;
     private DistributedRPC.Client client;

File: src/jvm/backtype/storm/drpc/DRPCSpout.java
Patch:
@@ -17,7 +17,6 @@
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
-import java.util.logging.Level;
 import org.apache.log4j.Logger;
 import org.apache.thrift7.TException;
 import org.json.simple.JSONValue;

File: src/jvm/backtype/storm/tuple/Tuple.java
Patch:
@@ -271,6 +271,8 @@ private static final Keyword makeKeyword(String name) {
     
     @Override
     public Object valAt(Object o) {
+        // should change this to get by field name, and push metadata stuff like this
+        // into metadata
         if(o.equals(STREAM_KEYWORD)) {
             return getSourceStreamId();
         } else if(o.equals(COMPONENT_KEYWORD)) {

File: src/jvm/storm/starter/spout/TwitterSampleSpout.java
Patch:
@@ -15,6 +15,7 @@
 import twitter4j.Status;
 import twitter4j.StatusDeletionNotice;
 import twitter4j.StatusListener;
+import twitter4j.json.DataObjectFactory;
 
 public class TwitterSampleSpout implements IRichSpout {
     SpoutOutputCollector _collector;
@@ -41,8 +42,8 @@ public void open(Map conf, TopologyContext context, SpoutOutputCollector collect
         StatusListener listener = new StatusListener() {
 
             @Override
-            public void onStatus(Status status) {
-                queue.offer(status.toString());
+            public void onStatus(Status status) {                
+                queue.offer(DataObjectFactory.getRawJSON(status));
             }
 
             @Override

File: src/jvm/backtype/storm/task/ShellBolt.java
Patch:
@@ -92,7 +92,7 @@ public void prepare(Map stormConf, TopologyContext context, OutputCollector coll
             sendToSubprocess(JSONValue.toJSONString(stormConf));
             sendToSubprocess(context.toJSONString());
         } catch (IOException e) {
-            throw new RuntimeException(e);
+            throw new RuntimeException("Error when launching multilang subprocess", e);
         }
     }
 
@@ -173,7 +173,7 @@ public void execute(Tuple input) {
               }
             }
         } catch(IOException e) {
-            throw new RuntimeException(e);
+            throw new RuntimeException("Error during multilang processing", e);
         }
     }
 

File: src/jvm/backtype/storm/Config.java
Patch:
@@ -343,7 +343,7 @@ public void setMessageTimeoutSecs(int secs) {
         put(Config.TOPOLOGY_MESSAGE_TIMEOUT_SECS, secs);
     }
     
-    public void addSerialization(int token, Class<ISerialization> serialization) {
+    public void addSerialization(int token, Class<? extends ISerialization> serialization) {
         if(!containsKey(Config.TOPOLOGY_SERIALIZATIONS)) {
             put(Config.TOPOLOGY_SERIALIZATIONS, new HashMap());
         }

File: src/jvm/backtype/storm/Config.java
Patch:
@@ -343,7 +343,7 @@ public void setMessageTimeoutSecs(int secs) {
         put(Config.TOPOLOGY_MESSAGE_TIMEOUT_SECS, secs);
     }
     
-    public void addSerialization(int token, Class<ISerialization> serialization) {
+    public void addSerialization(int token, Class<? extends ISerialization> serialization) {
         if(!containsKey(Config.TOPOLOGY_SERIALIZATIONS)) {
             put(Config.TOPOLOGY_SERIALIZATIONS, new HashMap());
         }

File: src/jvm/backtype/storm/serialization/SerializationFactory.java
Patch:
@@ -10,6 +10,7 @@
 
 
 public class SerializationFactory {
+    public static final int SERIALIZATION_TOKEN_BOUNDARY = 32;
     public static Logger LOG = Logger.getLogger(SerializationFactory.class);
     private static byte[] EMPTY_BYTE_ARRAY = new byte[0];
 
@@ -151,7 +152,7 @@ public SerializationFactory(Map conf) {
         for(Object tokenObj: customSerializations.keySet()) {
             String serializationClassName = customSerializations.get(tokenObj);
             int token = toToken(tokenObj);
-            if(token<=32) {
+            if(token<=SERIALIZATION_TOKEN_BOUNDARY) {
                 throw new RuntimeException("Illegal token " + token + " for " + serializationClassName);
             }
             try {

