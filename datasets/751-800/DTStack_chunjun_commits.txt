File: chunjun-connectors/chunjun-connector-kingbase/src/main/java/com/dtstack/chunjun/connector/kingbase/dialect/KingbaseDialect.java
Patch:
@@ -118,7 +118,7 @@ public String getInsertIntoStatement(String schema, String tableName, String[] f
                 Arrays.stream(fieldNames).map(f -> ":" + f).collect(Collectors.joining(", "));
         return "INSERT INTO "
                 + buildTableInfoWithSchema(schema, tableName)
-                + " t1 "
+                + " as t1 "
                 + "("
                 + columns
                 + ")"

File: chunjun-connectors/chunjun-connector-s3/src/main/java/com/dtstack/chunjun/connector/s3/sink/S3DynamicTableSink.java
Patch:
@@ -84,6 +84,6 @@ public DynamicTableSink copy() {
 
     @Override
     public String asSummaryString() {
-        return "StreamDynamicTableSink";
+        return S3DynamicTableSink.class.getName();
     }
 }

File: chunjun-connectors/chunjun-connector-jdbc-base/src/main/java/com/dtstack/chunjun/connector/jdbc/source/JdbcSourceFactory.java
Patch:
@@ -89,7 +89,7 @@ public JdbcSourceFactory(
         if (StringUtils.isBlank(jdbcConfig.getIncreColumn())) jdbcConfig.setPolling(false);
         jdbcConfig.setColumn(syncConfig.getReader().getFieldList());
 
-        Properties properties = syncConfig.getWriter().getProperties("properties", null);
+        Properties properties = syncConfig.getReader().getProperties("properties", null);
         jdbcConfig.setProperties(properties);
 
         setDefaultSplitStrategy(jdbcConfig);

File: chunjun-connectors/chunjun-connector-starrocks/src/main/java/com/dtstack/chunjun/connector/starrocks/options/ConstantValue.java
Patch:
@@ -36,6 +36,8 @@ public class ConstantValue {
     public static final Integer HTTP_CHECK_TIMEOUT_DEFAULT = 10 * 1000;
     public static final Integer QUEUE_OFFER_TIMEOUT_DEFAULT = 60 * 1000;
     public static final Integer QUEUE_POLL_TIMEOUT_DEFAULT = 60 * 1000;
-    public static final Long SINK_BATCH_MAX_BYTES_DEFAULT = 2 * 1024 * 1024 * 1024L;
+    // 50mb, If you need to set a larger value, you need to set a larger taskmanager memory,
+    // otherwise OOM may occur.
+    public static final Long SINK_BATCH_MAX_BYTES_DEFAULT = 50 * 1024 * 1024L;
     public static final Long SINK_BATCH_MAX_ROWS_DEFAULT = 2048 * 100L;
 }

File: chunjun-core/src/main/java/com/dtstack/chunjun/sink/format/BaseRichOutputFormat.java
Patch:
@@ -132,8 +132,10 @@ public abstract class BaseRichOutputFormat extends RichOutputFormat<RowData>
     /** 最新读取的数据 */
     protected RowData lastRow = null;
 
-    /** 存储用于批量写入的数据 */
+    /** 存储用于批量写入的数据行数 */
     protected transient List<RowData> rows;
+    /** 存储用于批量写入的数据字节数 */
+    protected transient long batchMaxByteSize;
     /** 数据类型转换器 */
     protected AbstractRowConverter rowConverter;
     /** 是否需要初始化脏数据和累加器，目前只有hive插件该参数设置为false */

File: chunjun-connectors/chunjun-connector-hbase-base/src/test/java/com/dtstack/chunjun/connector/hbase/converter/HBaseSerdeTest.java
Patch:
@@ -43,7 +43,9 @@ public void setUp() {
         tableSchema = new HBaseTableSchema();
         tableSchema.setRowKey("stu:id", Integer.class);
         tableSchema.addColumn("stu", "name", String.class);
-        serde = new HBaseSerde(tableSchema, new HBaseConfig());
+        HBaseConfig hBaseConfig = new HBaseConfig();
+        hBaseConfig.setNullStringLiteral("null");
+        serde = new HBaseSerde(tableSchema, hBaseConfig);
     }
 
     @Test

File: chunjun-connectors/chunjun-connector-jdbc-base/src/test/java/com/dtstack/chunjun/connector/jdbc/sink/SinkFactoryTest.java
Patch:
@@ -33,6 +33,7 @@
 import org.junit.BeforeClass;
 import org.junit.Test;
 import org.junit.runner.RunWith;
+import org.powermock.core.classloader.annotations.PowerMockIgnore;
 import org.powermock.core.classloader.annotations.PrepareForTest;
 import org.powermock.modules.junit4.PowerMockRunner;
 
@@ -47,6 +48,7 @@
 import static org.powermock.api.mockito.PowerMockito.mockStatic;
 import static org.powermock.api.mockito.PowerMockito.when;
 
+@PowerMockIgnore("javax.management.*")
 @RunWith(PowerMockRunner.class)
 @PrepareForTest({JdbcUtil.class, Connection.class})
 public class SinkFactoryTest {

File: chunjun-connectors/chunjun-connector-jdbc-base/src/test/java/com/dtstack/chunjun/connector/jdbc/source/JdbcInputFormatBuilderTest.java
Patch:
@@ -25,12 +25,14 @@
 import org.junit.BeforeClass;
 import org.junit.Test;
 import org.junit.runner.RunWith;
+import org.powermock.core.classloader.annotations.PowerMockIgnore;
 import org.powermock.core.classloader.annotations.PrepareForTest;
 import org.powermock.modules.junit4.PowerMockRunner;
 
 import static org.powermock.api.mockito.PowerMockito.mock;
 import static org.powermock.api.mockito.PowerMockito.when;
 
+@PowerMockIgnore("javax.management.*")
 @RunWith(PowerMockRunner.class)
 @PrepareForTest({JdbcInputFormat.class, JdbcConfig.class, JdbcDialect.class})
 public class JdbcInputFormatBuilderTest {

File: chunjun-connectors/chunjun-connector-jdbc-base/src/test/java/com/dtstack/chunjun/connector/jdbc/source/JdbcInputFormatTest.java
Patch:
@@ -42,6 +42,7 @@
 import org.junit.Test;
 import org.junit.runner.RunWith;
 import org.powermock.api.mockito.PowerMockito;
+import org.powermock.core.classloader.annotations.PowerMockIgnore;
 import org.powermock.core.classloader.annotations.PrepareForTest;
 import org.powermock.modules.junit4.PowerMockRunner;
 
@@ -72,6 +73,7 @@
 import static org.powermock.api.mockito.PowerMockito.when;
 import static org.powermock.reflect.Whitebox.setInternalState;
 
+@PowerMockIgnore("javax.management.*")
 @RunWith(PowerMockRunner.class)
 @PrepareForTest({
     JdbcInputFormat.class,

File: chunjun-connectors/chunjun-connector-jdbc-base/src/test/java/com/dtstack/chunjun/connector/jdbc/source/SourceFactoryTest.java
Patch:
@@ -31,6 +31,7 @@
 import org.junit.Before;
 import org.junit.Test;
 import org.junit.runner.RunWith;
+import org.powermock.core.classloader.annotations.PowerMockIgnore;
 import org.powermock.core.classloader.annotations.PrepareForTest;
 import org.powermock.modules.junit4.PowerMockRunner;
 
@@ -45,6 +46,7 @@
 import static org.powermock.api.mockito.PowerMockito.mockStatic;
 import static org.powermock.api.mockito.PowerMockito.when;
 
+@PowerMockIgnore("javax.management.*")
 @RunWith(PowerMockRunner.class)
 @PrepareForTest({JdbcUtil.class, Connection.class})
 public class SourceFactoryTest {

File: chunjun-connectors/chunjun-connector-jdbc-base/src/test/java/com/dtstack/chunjun/connector/jdbc/source/distribute/DistributeJdbcInputFormatTest.java
Patch:
@@ -44,6 +44,7 @@
 import org.junit.Test;
 import org.junit.runner.RunWith;
 import org.powermock.api.mockito.PowerMockito;
+import org.powermock.core.classloader.annotations.PowerMockIgnore;
 import org.powermock.core.classloader.annotations.PrepareForTest;
 import org.powermock.modules.junit4.PowerMockRunner;
 import org.slf4j.Logger;
@@ -68,6 +69,7 @@
 import static org.powermock.api.mockito.PowerMockito.when;
 import static org.powermock.reflect.Whitebox.setInternalState;
 
+@PowerMockIgnore("javax.management.*")
 @RunWith(PowerMockRunner.class)
 @PrepareForTest({
     DistributedJdbcInputFormat.class,

File: chunjun-core/src/main/java/com/dtstack/chunjun/Main.java
Patch:
@@ -70,6 +70,7 @@
 import org.apache.flink.table.api.TableResult;
 import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;
 import org.apache.flink.table.data.RowData;
+import org.apache.flink.table.factories.FactoryUtil;
 import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.types.DataType;
 import org.apache.flink.table.types.logical.RowType;
@@ -330,6 +331,7 @@ private static void configStreamExecutionEnvironment(
                     Thread.currentThread().getContextClassLoader(),
                     ConstantValue.DIRTY_DATA_DIR_NAME);
             // TODO sql 支持restore.
+            FactoryUtil.setFactoryHelper(factoryHelper);
         }
         PluginUtil.registerShipfileToCachedFile(options.getAddShipfile(), env);
     }

File: chunjun-core/src/test/java/com/dtstack/chunjun/config/BaseFileConfigTest.java
Patch:
@@ -39,7 +39,7 @@ public void toStringShouldReturnAStringWithAllTheFields() {
         baseFileConf.setNextCheckRows(5000L);
 
         String expected =
-                "BaseFileConfig(fromLine=1, path=path, fileName=fileName, writeMode=writeMode, compress=compress, encoding=UTF-8, maxFileSize=1024, nextCheckRows=5000, suffix=null)";
+                "BaseFileConfig(fromLine=1, path=path, fileName=fileName, writeMode=writeMode, compress=compress, encoding=UTF-8, maxFileSize=1024, nextCheckRows=5000, suffix=null, jobIdentifier=)";
 
         assertEquals(expected, baseFileConf.toString());
     }

File: chunjun-connectors/chunjun-connector-jdbc-base/src/main/java/com/dtstack/chunjun/connector/jdbc/table/JdbcDynamicTableFactory.java
Patch:
@@ -157,9 +157,8 @@ protected JdbcConfig getSinkConnectionConfig(
         JdbcConfig jdbcConfig = new JdbcConfig();
         SinkConnectionConfig conf = new SinkConnectionConfig();
         jdbcConfig.setConnection(Collections.singletonList(conf));
-
         conf.setJdbcUrl(readableConfig.get(URL));
-        conf.setTable(Collections.singletonList(readableConfig.get(TABLE_NAME)));
+        conf.setTable(Arrays.asList(readableConfig.get(TABLE_NAME)));
         conf.setSchema(readableConfig.get(SCHEMA));
         conf.setAllReplace(readableConfig.get(SINK_ALL_REPLACE));
 

File: chunjun-core/src/main/java/com/dtstack/chunjun/util/JsonUtil.java
Patch:
@@ -110,7 +110,7 @@ public static String toPrintJson(Object obj) {
             Map<String, Object> result =
                     objectMapper.readValue(objectMapper.writeValueAsString(obj), HashMap.class);
             MapUtil.replaceAllElement(
-                    result, Lists.newArrayList("pwd", "password", "druid.password"), "******");
+                    result, Lists.newArrayList("pwd", "password", "druid.password", "secretKey"), "******");
             return objectMapper.writerWithDefaultPrettyPrinter().writeValueAsString(result);
         } catch (Exception e) {
             throw new RuntimeException("error parse [" + obj + "] to json", e);

File: chunjun-connectors/chunjun-connector-jdbc-base/src/main/java/com/dtstack/chunjun/connector/jdbc/source/JdbcDynamicTableSource.java
Patch:
@@ -122,9 +122,10 @@ public LookupRuntimeProvider getLookupRuntimeProvider(LookupContext context) {
     public ScanRuntimeProvider getScanRuntimeProvider(ScanContext runtimeProviderContext) {
         final RowType rowType = (RowType) physicalRowDataType.getLogicalType();
         TypeInformation<RowData> typeInformation = InternalTypeInfo.of(rowType);
-
+        ResolvedSchema projectionSchema =
+                ResolvedSchema.physical(rowType.getFieldNames(), physicalRowDataType.getChildren());
         JdbcInputFormatBuilder builder = this.builder;
-        List<Column> columns = resolvedSchema.getColumns();
+        List<Column> columns = projectionSchema.getColumns();
         List<FieldConfig> columnList = new ArrayList<>(columns.size());
         for (Column column : columns) {
             FieldConfig field = new FieldConfig();

File: chunjun-connectors/chunjun-connector-jdbc-base/src/main/java/com/dtstack/chunjun/connector/jdbc/config/JdbcConfig.java
Patch:
@@ -25,6 +25,7 @@
 import org.apache.commons.lang3.StringUtils;
 
 import java.io.Serializable;
+import java.util.ArrayList;
 import java.util.List;
 import java.util.Properties;
 
@@ -99,7 +100,7 @@ public class JdbcConfig extends CommonConfig implements Serializable {
 
     protected List<String> preSql;
     protected List<String> postSql;
-    protected List<String> uniqueKey;
+    protected List<String> uniqueKey = new ArrayList<>();
 
     /** upsert 写数据库时，是否null覆盖原来的值 */
     protected boolean allReplace = false;

File: chunjun-core/src/main/java/com/dtstack/chunjun/util/JsonUtil.java
Patch:
@@ -109,7 +109,8 @@ public static String toPrintJson(Object obj) {
         try {
             Map<String, Object> result =
                     objectMapper.readValue(objectMapper.writeValueAsString(obj), HashMap.class);
-            MapUtil.replaceAllElement(result, Lists.newArrayList("pwd", "password"), "******");
+            MapUtil.replaceAllElement(
+                    result, Lists.newArrayList("pwd", "password", "druid.password"), "******");
             return objectMapper.writerWithDefaultPrettyPrinter().writeValueAsString(result);
         } catch (Exception e) {
             throw new RuntimeException("error parse [" + obj + "] to json", e);

File: chunjun-connectors/chunjun-connector-hdfs/src/main/java/com/dtstack/chunjun/connector/hdfs/table/HdfsDynamicTableFactory.java
Patch:
@@ -135,6 +135,7 @@ private HdfsConfig getHdfsConfig(ReadableConfig config) {
         hdfsConfig.setEncoding(config.get(BaseFileOptions.ENCODING));
         hdfsConfig.setMaxFileSize(config.get(BaseFileOptions.MAX_FILE_SIZE));
         hdfsConfig.setNextCheckRows(config.get(BaseFileOptions.NEXT_CHECK_ROWS));
+        hdfsConfig.setJobIdentifier(config.get(BaseFileOptions.JOB_IDENTIFIER));
 
         hdfsConfig.setDefaultFS(config.get(HdfsOptions.DEFAULT_FS));
         hdfsConfig.setFileType(config.get(HdfsOptions.FILE_TYPE));

File: chunjun-core/src/main/java/com/dtstack/chunjun/config/BaseFileConfig.java
Patch:
@@ -50,4 +50,6 @@ public class BaseFileConfig extends CommonConfig {
     private long nextCheckRows = 5000;
 
     private String suffix;
+
+    private String jobIdentifier = "";
 }

File: chunjun-core/src/main/java/com/dtstack/chunjun/dirty/manager/DirtyManager.java
Patch:
@@ -131,8 +131,8 @@ public void collect(Object data, Throwable cause, String field, long globalError
         entity.setFieldName(field);
         entity.setErrorMessage(ExceptionUtil.getErrorMessage(cause));
 
-        consumer.offer(entity, globalErrors);
         errorCounter.add(1L);
+        consumer.offer(entity, globalErrors);
     }
 
     public String toString(Object data) {

File: chunjun-core/src/main/java/com/dtstack/chunjun/converter/AbstractRowConverter.java
Patch:
@@ -21,6 +21,7 @@
 import com.dtstack.chunjun.config.CommonConfig;
 import com.dtstack.chunjun.config.FieldConfig;
 import com.dtstack.chunjun.element.AbstractBaseColumn;
+import com.dtstack.chunjun.element.column.NullColumn;
 import com.dtstack.chunjun.element.column.StringColumn;
 import com.dtstack.chunjun.enums.ColumnType;
 import com.dtstack.chunjun.throwable.ChunJunException;
@@ -93,7 +94,7 @@ public AbstractRowConverter(int converterSize, CommonConfig commonConfig) {
     protected IDeserializationConverter wrapIntoNullableInternalConverter(
             IDeserializationConverter IDeserializationConverter) {
         return val -> {
-            if (val == null) {
+            if (val == null || val instanceof NullColumn) {
                 return null;
             } else {
                 try {

File: chunjun-connectors/chunjun-connector-kudu/src/main/java/com/dtstack/chunjun/connector/kudu/table/KuduDynamicTableFactory.java
Patch:
@@ -44,6 +44,7 @@
 import static com.dtstack.chunjun.connector.kudu.table.KuduOptions.SCANNER_BATCH_SIZE_BYTES;
 import static com.dtstack.chunjun.connector.kudu.table.KuduOptions.TABLE_NAME;
 import static com.dtstack.chunjun.connector.kudu.table.KuduOptions.WORKER_COUNT;
+import static com.dtstack.chunjun.connector.kudu.table.KuduOptions.WRITE_MODE;
 import static com.dtstack.chunjun.lookup.options.LookupOptions.LOOKUP_ASYNC_TIMEOUT;
 import static com.dtstack.chunjun.lookup.options.LookupOptions.LOOKUP_CACHE_MAX_ROWS;
 import static com.dtstack.chunjun.lookup.options.LookupOptions.LOOKUP_CACHE_PERIOD;
@@ -146,6 +147,7 @@ public Set<ConfigOption<?>> optionalOptions() {
         optionalOptions.add(SINK_BUFFER_FLUSH_INTERVAL);
         optionalOptions.add(SINK_MAX_RETRIES);
         optionalOptions.add(SINK_PARALLELISM);
+        optionalOptions.add(WRITE_MODE);
 
         // kerberos
         optionalOptions.add(PRINCIPAL);

File: chunjun-connectors/chunjun-connector-oracle/src/main/java/com/dtstack/chunjun/connector/oracle/converter/OracleSyncConverter.java
Patch:
@@ -35,6 +35,7 @@
 import com.dtstack.chunjun.element.column.FloatColumn;
 import com.dtstack.chunjun.element.column.IntColumn;
 import com.dtstack.chunjun.element.column.LongColumn;
+import com.dtstack.chunjun.element.column.NullColumn;
 import com.dtstack.chunjun.element.column.ShortColumn;
 import com.dtstack.chunjun.element.column.StringColumn;
 import com.dtstack.chunjun.element.column.TimestampColumn;
@@ -275,7 +276,8 @@ protected IDeserializationConverter createInternalConverter(LogicalType type) {
             wrapIntoNullableExternalConverter(
                     ISerializationConverter serializationConverter, LogicalType type) {
         return (val, index, statement) -> {
-            if (((ColumnRowData) val).getField(index) == null) {
+            if (((ColumnRowData) val).getField(index) == null
+                    || ((ColumnRowData) val).getField(index) instanceof NullColumn) {
                 try {
                     final int sqlType =
                             JdbcTypeUtil.typeInformationToSqlType(

File: chunjun-connectors/chunjun-connector-oracle/src/main/java/com/dtstack/chunjun/connector/oracle/converter/OracleSqlConverter.java
Patch:
@@ -67,6 +67,7 @@ public OracleSqlConverter(RowType rowType) {
                     wrapIntoNullableInternalConverter(
                             createAsyncInternalConverter(rowType.getTypeAt(i))));
         }
+        toInternalConverters = toAsyncInternalConverters;
     }
 
     protected IDeserializationConverter createAsyncInternalConverter(LogicalType type) {

File: chunjun-connectors/chunjun-connector-jdbc-base/src/main/java/com/dtstack/chunjun/connector/jdbc/table/JdbcDynamicTableFactory.java
Patch:
@@ -180,7 +180,9 @@ protected JdbcConfig getSinkConnectionConfig(
         }
 
         List<String> keyFields = new ArrayList<>();
-        schema.getPrimaryKey().ifPresent(item -> keyFields.add(item.getName()));
+        if (schema.getPrimaryKey().isPresent()) {
+            keyFields = schema.getPrimaryKey().get().getColumns();
+        }
 
         jdbcConfig.setUniqueKey(keyFields);
         resetTableInfo(jdbcConfig);

File: chunjun-connectors/chunjun-connector-hdfs/src/main/java/com/dtstack/chunjun/connector/hdfs/converter/HdfsParquetSyncConverter.java
Patch:
@@ -111,7 +111,7 @@ public RowData toInternal(RowData input) throws Exception {
     @Override
     @SuppressWarnings("unchecked")
     public Group toExternal(RowData rowData, Group group) throws Exception {
-        for (int index = 0; index < fieldTypes.length; index++) {
+        for (int index = 0; index < columnNameList.size(); index++) {
             toExternalConverters.get(index).serialize(rowData, index, group);
         }
         return group;

File: chunjun-connectors/chunjun-connector-kafka/src/main/java/com/dtstack/chunjun/connector/kafka/serialization/RowSerializationSchema.java
Patch:
@@ -21,6 +21,7 @@
 import com.dtstack.chunjun.connector.kafka.conf.KafkaConfig;
 import com.dtstack.chunjun.connector.kafka.sink.DynamicKafkaSerializationSchema;
 import com.dtstack.chunjun.constants.CDCConstantValue;
+import com.dtstack.chunjun.constants.Metrics;
 import com.dtstack.chunjun.converter.AbstractRowConverter;
 import com.dtstack.chunjun.element.AbstractBaseColumn;
 import com.dtstack.chunjun.element.ColumnRowData;
@@ -99,7 +100,8 @@ public ProducerRecord<byte[], byte[]> serialize(RowData element, @Nullable Long
                     null,
                     valueSerialized);
         } catch (Exception e) {
-            dirtyManager.collect(element, e, null);
+            long globalErrors = accumulatorCollector.getAccumulatorValue(Metrics.NUM_ERRORS, false);
+            dirtyManager.collect(element, e, null, globalErrors);
         }
         return null;
     }

File: chunjun-connectors/chunjun-connector-kafka/src/main/java/com/dtstack/chunjun/connector/kafka/sink/DynamicKafkaSerializationSchema.java
Patch:
@@ -265,7 +265,8 @@ public ProducerRecord<byte[], byte[]> serialize(RowData consumedRow, @Nullable L
                     valueSerialized,
                     readMetadata(consumedRow, KafkaDynamicSink.WritableMetadata.HEADERS));
         } catch (Exception e) {
-            dirtyManager.collect(consumedRow, e, null);
+            long globalErrors = accumulatorCollector.getAccumulatorValue(Metrics.NUM_ERRORS, false);
+            dirtyManager.collect(consumedRow, e, null, globalErrors);
         }
         return null;
     }

File: chunjun-connectors/chunjun-connector-kafka/src/main/java/com/dtstack/chunjun/connector/kafka/source/DynamicKafkaDeserializationSchema.java
Patch:
@@ -231,7 +231,9 @@ public void deserialize(ConsumerRecord<byte[], byte[]> record, Collector<RowData
             if (record.value() != null) {
                 data = new String(record.value(), StandardCharsets.UTF_8);
             }
-            dirtyManager.collect(data, e, null);
+            long globalErrors = accumulatorCollector.getAccumulatorValue(Metrics.NUM_ERRORS, false);
+
+            dirtyManager.collect(data, e, null, globalErrors);
         }
     }
 

File: chunjun-connectors/chunjun-connector-rocketmq/src/main/java/com/dtstack/chunjun/connector/rocketmq/source/deserialization/RowKeyValueDeserializationSchema.java
Patch:
@@ -212,7 +212,8 @@ private RowData deserializeValue(byte[] value) {
         try {
             return converter.toInternal(value);
         } catch (Exception e) {
-            dirtyManager.collect(value, e, null);
+            long globalErrors = accumulatorCollector.getAccumulatorValue(Metrics.NUM_ERRORS, false);
+            dirtyManager.collect(value, e, null, globalErrors);
         }
         return null;
     }

File: chunjun-core/src/main/java/com/dtstack/chunjun/dirty/manager/DirtyManager.java
Patch:
@@ -116,7 +116,7 @@ public LongCounter getFailedConsumedMetric() {
         return consumer.getFailedConsumed();
     }
 
-    public void collect(Object data, Throwable cause, String field) {
+    public void collect(Object data, Throwable cause, String field, long globalErrors) {
         if (executor == null) {
             execute();
         }
@@ -131,7 +131,7 @@ public void collect(Object data, Throwable cause, String field) {
         entity.setFieldName(field);
         entity.setErrorMessage(ExceptionUtil.getErrorMessage(cause));
 
-        consumer.offer(entity);
+        consumer.offer(entity, globalErrors);
         errorCounter.add(1L);
     }
 

File: chunjun-core/src/main/java/com/dtstack/chunjun/sink/format/BaseRichOutputFormat.java
Patch:
@@ -488,7 +488,9 @@ protected void writeSingleRecord(RowData rowData, LongCounter numWriteCounter) {
             writeSingleRecordInternal(rowData);
             numWriteCounter.add(1L);
         } catch (WriteRecordException e) {
-            dirtyManager.collect(e.getRowData(), e, null);
+            long globalErrors = accumulatorCollector.getAccumulatorValue(Metrics.NUM_ERRORS, false);
+
+            dirtyManager.collect(e.getRowData(), e, null, globalErrors);
             if (log.isTraceEnabled()) {
                 log.trace(
                         "write error rowData, rowData = {}, e = {}",

File: chunjun-core/src/main/java/com/dtstack/chunjun/source/format/BaseRichInputFormat.java
Patch:
@@ -198,7 +198,9 @@ public RowData nextRecord(RowData rowData) {
         try {
             internalRow = nextRecordInternal(rowData);
         } catch (ReadRecordException e) {
-            dirtyManager.collect(e.getRowData(), e, null);
+            // 脏数据总数应是所有slot的脏数据总数，而不是单个的
+            long globalErrors = accumulatorCollector.getAccumulatorValue(Metrics.NUM_ERRORS, false);
+            dirtyManager.collect(e.getRowData(), e, null, globalErrors);
         }
         if (internalRow != null) {
             updateDuration();

File: chunjun-connectors/chunjun-connector-oceanbasecdc/src/main/java/com/dtstack/chunjun/connector/oceanbasecdc/listener/OceanBaseCdcListener.java
Patch:
@@ -145,5 +145,6 @@ private void flushBuffer() {
                 throw new ChunJunRuntimeException("flush log message buffer failed", e);
             }
         }
+        logMessageBuffer.clear();
     }
 }

File: chunjun-connectors/chunjun-connector-mysql/src/main/java/com/dtstack/chunjun/connector/mysql/dialect/MysqlDialect.java
Patch:
@@ -145,8 +145,6 @@ public Function<Tuple3<String, Integer, Integer>, TypeConfig> typeBuilder() {
                     && precision > 10) {
                 // "." 还占一个字符长度，需要去掉.
                 scale = precision - 10 - 1;
-            } else if (columnTypeName.toUpperCase(Locale.ENGLISH).contains("DECIMAL")) {
-                precision = precision - scale;
             }
             TypeConfig typeConfig = TypeConfig.fromString(columnTypeName);
             typeConfig.setPrecision(precision);

File: chunjun-connectors/chunjun-connector-hbase-base/src/main/java/com/dtstack/chunjun/connector/hbase/sink/HBaseOutputFormat.java
Patch:
@@ -87,7 +87,8 @@ public void openConnection() {
         Validate.isTrue(hbaseConfig != null && hbaseConfig.size() != 0, "hbaseConfig不能为空Map结构!");
 
         try {
-            connection = HBaseHelper.getHbaseConnection(hbaseConfig);
+            connection =
+                    HBaseHelper.getHbaseConnection(hbaseConfig, jobId, String.valueOf(taskNumber));
             org.apache.hadoop.conf.Configuration hConfiguration =
                     HBaseHelper.getConfig(hbaseConfig);
             try (Admin admin = this.connection.getAdmin()) {

File: chunjun-connectors/chunjun-connector-hbase-base/src/main/java/com/dtstack/chunjun/connector/hbase/table/lookup/HBaseLruTableFunction.java
Patch:
@@ -91,7 +91,7 @@ public void open(FunctionContext context) throws Exception {
                         new LinkedBlockingQueue<>(),
                         new ChunJunThreadFactory("hbase-async"));
 
-        this.connection = HBaseHelper.getHbaseConnection(hBaseConfig);
+        this.connection = HBaseHelper.getHbaseConnection(hBaseConfig, null, null);
         this.table = connection.getTable(TableName.valueOf(hbaseTableSchema.getTableName()));
     }
 

File: chunjun-connectors/chunjun-connector-hdfs/src/main/java/com/dtstack/chunjun/connector/hdfs/sink/BaseHdfsOutputFormat.java
Patch:
@@ -156,7 +156,9 @@ protected void openSource() {
                     FileSystemUtil.getFileSystem(
                             hdfsConfig.getHadoopConfig(),
                             hdfsConfig.getDefaultFS(),
-                            distributedCache);
+                            distributedCache,
+                            jobId,
+                            String.valueOf(taskNumber));
         } catch (Exception e) {
             throw new ChunJunRuntimeException("can't init fileSystem", e);
         }

File: chunjun-connectors/chunjun-connector-hdfs/src/main/java/com/dtstack/chunjun/connector/hdfs/sink/HdfsParquetOutputFormat.java
Patch:
@@ -129,7 +129,9 @@ protected void nextBlock() {
                         FileSystemUtil.getUGI(
                                 hdfsConfig.getHadoopConfig(),
                                 hdfsConfig.getDefaultFS(),
-                                getRuntimeContext().getDistributedCache());
+                                getRuntimeContext().getDistributedCache(),
+                                jobId,
+                                String.valueOf(taskNumber));
                 ugi.doAs(
                         (PrivilegedAction<Object>)
                                 () -> {

File: chunjun-connectors/chunjun-connector-hdfs/src/main/java/com/dtstack/chunjun/connector/hdfs/source/HdfsParquetInputFormat.java
Patch:
@@ -103,7 +103,9 @@ public InputSplit[] createHdfsSplit(int minNumSplits) {
                 FileSystemUtil.getFileSystem(
                         hdfsConfig.getHadoopConfig(),
                         hdfsConfig.getDefaultFS(),
-                        PluginUtil.createDistributedCacheFromContextClassLoader())) {
+                        PluginUtil.createDistributedCacheFromContextClassLoader(),
+                        jobId,
+                        String.valueOf(indexOfSubTask))) {
             allFilePaths = getAllPartitionPath(hdfsConfig.getPath(), fs, pathFilter);
         } catch (Exception e) {
             throw new ChunJunRuntimeException(e);

File: chunjun-connectors/chunjun-connector-hive3/src/main/java/com/dtstack/chunjun/connector/hive3/sink/BaseHdfsOutputFormat.java
Patch:
@@ -130,7 +130,9 @@ protected void openSource() {
                     Hive3Util.getFileSystem(
                             hdfsConfig.getHadoopConfig(),
                             hdfsConfig.getDefaultFS(),
-                            distributedCache);
+                            distributedCache,
+                            jobId,
+                            String.valueOf(taskNumber));
         } catch (Exception e) {
             throw new ChunJunRuntimeException("can't init fileSystem", e);
         }

File: chunjun-connectors/chunjun-connector-hive3/src/main/java/com/dtstack/chunjun/connector/hive3/sink/HdfsParquetOutputFormat.java
Patch:
@@ -115,7 +115,9 @@ protected void nextBlock() {
                         Hive3Util.getUGI(
                                 hdfsConfig.getHadoopConfig(),
                                 hdfsConfig.getDefaultFS(),
-                                getRuntimeContext().getDistributedCache());
+                                getRuntimeContext().getDistributedCache(),
+                                jobId,
+                                String.valueOf(taskNumber));
                 ugi.doAs(
                         (PrivilegedAction<Object>)
                                 () -> {

File: chunjun-connectors/chunjun-connector-solr/src/main/java/com/dtstack/chunjun/connector/solr/sink/SolrOutputFormat.java
Patch:
@@ -73,7 +73,7 @@ protected void openInternal(int taskNumber, int numTasks) {
         solrClientWrapper =
                 new CloudSolrClientKerberosWrapper(
                         solrConfig, getRuntimeContext().getDistributedCache());
-        solrClientWrapper.init();
+        solrClientWrapper.init(jobId, String.valueOf(taskNumber));
     }
 
     @Override

File: chunjun-connectors/chunjun-connector-solr/src/main/java/com/dtstack/chunjun/connector/solr/source/SolrInputFormat.java
Patch:
@@ -68,7 +68,7 @@ protected void openInternal(InputSplit inputSplit) {
         solrClientWrapper =
                 new CloudSolrClientKerberosWrapper(
                         solrConfig, getRuntimeContext().getDistributedCache());
-        solrClientWrapper.init();
+        solrClientWrapper.init(jobId, String.valueOf(indexOfSubTask));
 
         GenericInputSplit genericInputSplit = (GenericInputSplit) inputSplit;
         solrQuery = new SolrQuery();

File: chunjun-connectors/chunjun-connector-starrocks/src/main/java/com/dtstack/chunjun/connector/starrocks/sink/MappedWriteProcessor.java
Patch:
@@ -72,7 +72,7 @@ public void write(List<RowData> rowDataList) throws Exception {
             valueList.add(value);
         }
         for (Map.Entry<String, List<Map<String, Object>>> entry : identifyXValueMap.entrySet()) {
-            streamLoadManager.write(entry.getKey(), null, entry.getValue());
+            streamLoadManager.write(entry.getKey(), null, entry.getValue(), true);
         }
     }
 }

File: chunjun-connectors/chunjun-connector-starrocks/src/main/java/com/dtstack/chunjun/connector/starrocks/sink/StarRocksDynamicTableSink.java
Patch:
@@ -70,6 +70,7 @@ public SinkRuntimeProvider getSinkRuntimeProvider(Context context) {
                         Arrays.asList(resolvedSchema.getColumnNames().toArray(new String[0]))));
         builder.setStarRocksConf(sinkConfig);
 
+        builder.checkFormat();
         return SinkFunctionProvider.of(
                 new DtOutputFormatSinkFunction<>(builder.finish()), sinkConfig.getParallelism());
     }

File: chunjun-connectors/chunjun-connector-starrocks/src/main/java/com/dtstack/chunjun/connector/starrocks/sink/StarRocksOutputFormatBuilder.java
Patch:
@@ -46,6 +46,9 @@ protected void checkFormat() {
                     "starRocks database is required when nameMapped is not enable");
             Preconditions.checkNotNull(
                     conf.getTable(), "starRocks table is required when nameMapped is not enable");
+            // cache is not necessary because we only have one table
+            conf.setCacheTableStruct(false);
+            conf.setCheckStructFirstTime(false);
         }
         Preconditions.checkNotNull(conf.getUsername(), "starRocks username is required");
         Preconditions.checkNotNull(conf.getPassword(), "starRocks password is required");

File: chunjun-connectors/chunjun-connector-kudu/src/main/java/com/dtstack/chunjun/connector/kudu/config/KuduSinkConfig.java
Patch:
@@ -32,6 +32,7 @@
 import static com.dtstack.chunjun.connector.kudu.table.KuduOptions.WRITE_MODE;
 import static com.dtstack.chunjun.table.options.SinkOptions.SINK_BUFFER_FLUSH_INTERVAL;
 import static com.dtstack.chunjun.table.options.SinkOptions.SINK_BUFFER_FLUSH_MAX_ROWS;
+import static com.dtstack.chunjun.table.options.SinkOptions.SINK_PARALLELISM;
 
 @EqualsAndHashCode(callSuper = true)
 @Data
@@ -58,6 +59,7 @@ public static KuduSinkConfig from(ReadableConfig readableConfig) {
         config.setMaxBufferSize(readableConfig.get(MUTATION_BUFFER_SPACE));
         config.setFlushMode(readableConfig.get(FLUSH_MODE));
         config.setFlushInterval(readableConfig.get(SINK_BUFFER_FLUSH_INTERVAL));
+        config.setParallelism(readableConfig.get(SINK_PARALLELISM));
 
         return config;
     }

File: chunjun-connectors/chunjun-connector-saphana/src/main/java/com/dtstack/chunjun/connector/saphana/dialect/SaphanaDialect.java
Patch:
@@ -136,11 +136,11 @@ public String buildDualQueryStatement(String[] column) {
         return sb.toString();
     }
 
-    /** build sql part e.g: T1.`A` = T2.`A`, T1.`B` = T2.`B` */
+    /** build sql part e.g: T1.`A` = T2.`A` and T1.`B` = T2.`B` */
     private String buildEqualConditions(String[] uniqueKeyFields) {
         return Arrays.stream(uniqueKeyFields)
                 .map(col -> "T1." + quoteIdentifier(col) + " = T2." + quoteIdentifier(col))
-                .collect(Collectors.joining(", "));
+                .collect(Collectors.joining(" and "));
     }
 
     /** build T1."A"=T2."A" or T1."A"=nvl(T2."A",T1."A") */

File: chunjun-connectors/chunjun-connector-mongodb/src/main/java/com/dtstack/chunjun/connector/mongodb/source/MongodbSourceFactory.java
Patch:
@@ -46,6 +46,7 @@ public MongodbSourceFactory(SyncConfig syncConfig, StreamExecutionEnvironment en
                 gson.fromJson(
                         gson.toJson(syncConfig.getReader().getParameter()),
                         MongodbDataSyncConfig.class);
+        mongodbDataSyncConfig.setColumn(syncConfig.getReader().getFieldList());
     }
 
     @Override

File: chunjun-connectors/chunjun-connector-ftp/src/main/java/com/dtstack/chunjun/connector/ftp/converter/FtpSqlConverter.java
Patch:
@@ -43,11 +43,13 @@ public FtpSqlConverter(SerializationSchema<RowData> valueSerialization) {
 
     @Override
     public RowData toInternal(String input) throws Exception {
+        valueDeserialization.open(new DummyInitializationContext());
         return valueDeserialization.deserialize(input.getBytes());
     }
 
     @Override
     public String toExternal(RowData rowData, String output) throws Exception {
+        valueSerialization.open(new DummyInitializationContext());
         byte[] serialize = valueSerialization.serialize(rowData);
         return new String(serialize);
     }

File: chunjun-connectors/chunjun-connector-ftp/src/main/java/com/dtstack/chunjun/connector/ftp/spliter/ConcurrentZipCompressSplit.java
Patch:
@@ -54,7 +54,7 @@ private List<FtpFileSplit> analyseSingleFile(
             while ((zipEntry = zipInputStream.getNextEntry()) != null) {
                 fileSplits.add(
                         new FtpFileSplit(
-                                0, zipEntry.getSize(), filePath, zipEntry.getName(), compressType));
+                                0, zipEntry.getSize(), zipEntry.getName(), filePath, compressType));
             }
             return fileSplits;
         } catch (Exception e) {

File: chunjun-connectors/chunjun-connector-ftp/src/main/java/com/dtstack/chunjun/connector/ftp/table/FtpDynamicTableFactory.java
Patch:
@@ -68,6 +68,7 @@ private static FtpConfig getFtpConfByOptions(ReadableConfig config) {
         ftpConfig.setMaxFileSize(config.get(BaseFileOptions.MAX_FILE_SIZE));
         ftpConfig.setCompressType(config.get(FtpOptions.COMPRESS_TYPE));
         ftpConfig.setFileType(config.get(FtpOptions.FILE_TYPE));
+        ftpConfig.setNextCheckRows(config.get(BaseFileOptions.NEXT_CHECK_ROWS));
 
         if (!ConfigConstants.DEFAULT_FIELD_DELIMITER.equals(
                 config.get(FtpOptions.FIELD_DELIMITER))) {
@@ -179,6 +180,7 @@ public Set<ConfigOption<?>> optionalOptions() {
         options.add(FtpOptions.FIRST_LINE_HEADER);
         options.add(FtpOptions.FIELD_DELIMITER);
         options.add(FtpOptions.COMPRESS_TYPE);
+        options.add(BaseFileOptions.NEXT_CHECK_ROWS);
         return options;
     }
 }

File: chunjun-connectors/chunjun-connector-hbase-base/src/test/java/com/dtstack/chunjun/connector/hbase/config/HBaseConfigTest.java
Patch:
@@ -19,6 +19,7 @@
 package com.dtstack.chunjun.connector.hbase.config;
 
 import com.dtstack.chunjun.config.FieldConfig;
+import com.dtstack.chunjun.config.TypeConfig;
 
 import com.google.common.collect.Lists;
 import com.google.common.collect.Maps;
@@ -39,11 +40,11 @@ public void testGetHBaseConfig() {
 
         FieldConfig id = new FieldConfig();
         id.setName("stu.id");
-        id.setType("int");
+        id.setType(TypeConfig.fromString("int"));
 
         FieldConfig address = new FieldConfig();
         address.setName("msg.address");
-        address.setType("string");
+        address.setType(TypeConfig.fromString("string"));
 
         confList.add(id);
         confList.add(address);

File: chunjun-connectors/chunjun-connector-hbase-base/src/test/java/com/dtstack/chunjun/connector/hbase/source/HBaseInputFormatTest.java
Patch:
@@ -19,6 +19,7 @@
 package com.dtstack.chunjun.connector.hbase.source;
 
 import com.dtstack.chunjun.config.FieldConfig;
+import com.dtstack.chunjun.config.TypeConfig;
 import com.dtstack.chunjun.connector.hbase.HBaseTableSchema;
 import com.dtstack.chunjun.connector.hbase.util.ScanBuilder;
 
@@ -50,11 +51,11 @@ public void setUp() {
 
         FieldConfig id = new FieldConfig();
         id.setName("stu.id");
-        id.setType("int");
+        id.setType(TypeConfig.fromString("int"));
 
         FieldConfig address = new FieldConfig();
         address.setName("msg.address");
-        address.setType("string");
+        address.setType(TypeConfig.fromString("string"));
 
         confList.add(id);
         confList.add(address);

File: chunjun-connectors/chunjun-connector-hbase-base/src/test/java/com/dtstack/chunjun/connector/hbase/util/ScanBuilderTest.java
Patch:
@@ -19,6 +19,7 @@
 package com.dtstack.chunjun.connector.hbase.util;
 
 import com.dtstack.chunjun.config.FieldConfig;
+import com.dtstack.chunjun.config.TypeConfig;
 import com.dtstack.chunjun.connector.hbase.HBaseTableSchema;
 
 import org.apache.flink.table.api.DataTypes;
@@ -43,11 +44,11 @@ public void setUp() {
 
         FieldConfig id = new FieldConfig();
         id.setName("stu.id");
-        id.setType("int");
+        id.setType(TypeConfig.fromString("int"));
 
         FieldConfig address = new FieldConfig();
         address.setName("msg.address");
-        address.setType("string");
+        address.setType(TypeConfig.fromString("string"));
 
         confList.add(id);
         confList.add(address);

File: chunjun-core/src/main/java/com/dtstack/chunjun/typeutil/serializer/base/TimeColumnSerializer.java
Patch:
@@ -53,7 +53,7 @@ public AbstractBaseColumn createInstance() {
 
     @Override
     public AbstractBaseColumn copy(AbstractBaseColumn from) {
-        if (from instanceof NullColumn) {
+        if (from instanceof NullColumn || from == null) {
             return new NullColumn();
         }
         return TimeColumn.from(from.asTime());

File: chunjun-connectors/chunjun-connector-hive/src/main/java/com/dtstack/chunjun/connector/hive/source/HiveInputFormat.java
Patch:
@@ -49,6 +49,9 @@ public void openInternal(InputSplit inputSplit) {
         connectionInfo.setJdbcUrl(jdbcConfig.getJdbcUrl());
         connectionInfo.setUsername(jdbcConfig.getUsername());
         connectionInfo.setPassword(jdbcConfig.getPassword());
+        if (jdbcConfig.getQueryTimeOut() > 0) {
+            connectionInfo.setTimeout(jdbcConfig.getQueryTimeOut());
+        }
         this.currentJdbcInputSplit = (JdbcInputSplit) inputSplit;
         initMetric(currentJdbcInputSplit);
         if (!canReadData(currentJdbcInputSplit)) {

File: chunjun-connectors/chunjun-connector-hive/src/main/java/com/dtstack/chunjun/connector/hive/table/HiveDynamicTableFactory.java
Patch:
@@ -190,6 +190,7 @@ public Set<ConfigOption<?>> optionalOptions() {
         options.add(HiveOptions.PASSWORD);
         options.add(HiveOptions.PARTITION_TYPE);
         options.add(HiveOptions.PARTITION);
+        options.add(SCAN_QUERY_TIMEOUT);
 
         options.add(SCAN_CUSTOM_SQL);
 

File: chunjun-connectors/chunjun-connector-binlog/src/main/java/com/dtstack/chunjun/connector/binlog/converter/BinlogSqlConverter.java
Patch:
@@ -19,7 +19,7 @@
 
 import com.dtstack.chunjun.connector.binlog.format.TimestampFormat;
 import com.dtstack.chunjun.connector.binlog.listener.BinlogEventRow;
-import com.dtstack.chunjun.converter.AbstractCDCRowConverter;
+import com.dtstack.chunjun.converter.AbstractCDCRawTypeMapper;
 import com.dtstack.chunjun.converter.IDeserializationConverter;
 
 import org.apache.flink.table.api.TableException;
@@ -49,13 +49,13 @@
 import java.util.List;
 import java.util.Map;
 
-public class BinlogRowConverter extends AbstractCDCRowConverter<BinlogEventRow, LogicalType> {
+public class BinlogSqlConverter extends AbstractCDCRawTypeMapper<BinlogEventRow, LogicalType> {
 
     private static final long serialVersionUID = 5788793678947004381L;
 
     private final TimestampFormat timestampFormat;
 
-    public BinlogRowConverter(RowType rowType, TimestampFormat timestampFormat) {
+    public BinlogSqlConverter(RowType rowType, TimestampFormat timestampFormat) {
         super.fieldNameList = rowType.getFieldNames();
         this.timestampFormat = timestampFormat;
         super.converters = new ArrayList<>();

File: chunjun-connectors/chunjun-connector-binlog/src/main/java/com/dtstack/chunjun/connector/binlog/converter/BinlogSyncConverter.java
Patch:
@@ -23,7 +23,7 @@
 import com.dtstack.chunjun.connector.binlog.listener.BinlogEventRow;
 import com.dtstack.chunjun.constants.CDCConstantValue;
 import com.dtstack.chunjun.constants.ConstantValue;
-import com.dtstack.chunjun.converter.AbstractCDCRowConverter;
+import com.dtstack.chunjun.converter.AbstractCDCRawTypeMapper;
 import com.dtstack.chunjun.converter.IDeserializationConverter;
 import com.dtstack.chunjun.element.AbstractBaseColumn;
 import com.dtstack.chunjun.element.ColumnRowData;
@@ -69,11 +69,11 @@
 import static com.dtstack.chunjun.constants.CDCConstantValue.TS;
 import static com.dtstack.chunjun.constants.CDCConstantValue.TYPE;
 
-public class BinlogColumnConverter extends AbstractCDCRowConverter<BinlogEventRow, String> {
+public class BinlogSyncConverter extends AbstractCDCRawTypeMapper<BinlogEventRow, String> {
 
     private static final long serialVersionUID = -4387050357951282347L;
 
-    public BinlogColumnConverter(boolean pavingData, boolean splitUpdate) {
+    public BinlogSyncConverter(boolean pavingData, boolean splitUpdate) {
         super.pavingData = pavingData;
         super.split = splitUpdate;
     }

File: chunjun-connectors/chunjun-connector-binlog/src/main/java/com/dtstack/chunjun/connector/binlog/inputformat/BinlogInputFormat.java
Patch:
@@ -26,7 +26,7 @@
 import com.dtstack.chunjun.connector.binlog.listener.HeartBeatController;
 import com.dtstack.chunjun.connector.binlog.util.BinlogUtil;
 import com.dtstack.chunjun.constants.ConstantValue;
-import com.dtstack.chunjun.converter.AbstractCDCRowConverter;
+import com.dtstack.chunjun.converter.AbstractCDCRawTypeMapper;
 import com.dtstack.chunjun.restore.FormatState;
 import com.dtstack.chunjun.source.format.BaseRichInputFormat;
 import com.dtstack.chunjun.util.ClassUtil;
@@ -67,7 +67,7 @@ public class BinlogInputFormat extends BaseRichInputFormat {
     protected BinlogConfig binlogConfig;
     protected volatile EntryPosition entryPosition;
     protected List<String> categories = new ArrayList<>();
-    protected AbstractCDCRowConverter cdcRowConverter;
+    protected AbstractCDCRawTypeMapper cdcRowConverter;
 
     protected transient MysqlEventParser controller;
     protected transient BinlogEventSink binlogEventSink;

File: chunjun-connectors/chunjun-connector-binlog/src/main/java/com/dtstack/chunjun/connector/binlog/inputformat/BinlogInputFormatBuilder.java
Patch:
@@ -20,7 +20,7 @@
 import com.dtstack.chunjun.cdc.EventType;
 import com.dtstack.chunjun.connector.binlog.config.BinlogConfig;
 import com.dtstack.chunjun.connector.binlog.util.BinlogUtil;
-import com.dtstack.chunjun.converter.AbstractCDCRowConverter;
+import com.dtstack.chunjun.converter.AbstractCDCRawTypeMapper;
 import com.dtstack.chunjun.source.format.BaseRichInputFormat;
 import com.dtstack.chunjun.source.format.BaseRichInputFormatBuilder;
 import com.dtstack.chunjun.util.ClassUtil;
@@ -53,11 +53,11 @@ public void setBinlogConf(BinlogConfig binlogConfig) {
         this.format.setBinlogConfig(binlogConfig);
     }
 
-    public void setRowConverter(AbstractCDCRowConverter rowConverter) {
+    public void setRowConverter(AbstractCDCRawTypeMapper rowConverter) {
         setRowConverter(rowConverter, false);
     }
 
-    public void setRowConverter(AbstractCDCRowConverter rowConverter, boolean useAbstractColumn) {
+    public void setRowConverter(AbstractCDCRawTypeMapper rowConverter, boolean useAbstractColumn) {
         this.format.setCdcRowConverter(rowConverter);
         format.setUseAbstractColumn(useAbstractColumn);
     }

File: chunjun-connectors/chunjun-connector-binlog/src/main/java/com/dtstack/chunjun/connector/binlog/listener/BinlogEventSink.java
Patch:
@@ -22,7 +22,7 @@
 import com.dtstack.chunjun.cdc.EventType;
 import com.dtstack.chunjun.connector.binlog.config.BinlogConfig;
 import com.dtstack.chunjun.connector.binlog.inputformat.BinlogInputFormat;
-import com.dtstack.chunjun.converter.AbstractCDCRowConverter;
+import com.dtstack.chunjun.converter.AbstractCDCRawTypeMapper;
 import com.dtstack.chunjun.element.ErrorMsgRowData;
 import com.dtstack.chunjun.throwable.WriteRecordException;
 import com.dtstack.chunjun.util.ClassUtil;
@@ -62,7 +62,7 @@ public class BinlogEventSink extends AbstractCanalLifeCycle
 
     private final BinlogInputFormat format;
     private final LinkedBlockingDeque<RowData> queue;
-    private final AbstractCDCRowConverter rowConverter;
+    private final AbstractCDCRawTypeMapper rowConverter;
 
     private final String OFFSET_LENGTH;
 

File: chunjun-connectors/chunjun-connector-binlog/src/main/java/com/dtstack/chunjun/connector/binlog/source/BinlogDynamicTableSource.java
Patch:
@@ -19,7 +19,7 @@
 package com.dtstack.chunjun.connector.binlog.source;
 
 import com.dtstack.chunjun.connector.binlog.config.BinlogConfig;
-import com.dtstack.chunjun.connector.binlog.converter.BinlogRowConverter;
+import com.dtstack.chunjun.connector.binlog.converter.BinlogSqlConverter;
 import com.dtstack.chunjun.connector.binlog.format.TimestampFormat;
 import com.dtstack.chunjun.connector.binlog.inputformat.BinlogInputFormatBuilder;
 import com.dtstack.chunjun.source.DtInputFormatSourceFunction;
@@ -59,7 +59,7 @@ public ScanRuntimeProvider getScanRuntimeProvider(ScanContext runtimeProviderCon
         BinlogInputFormatBuilder builder = new BinlogInputFormatBuilder();
         builder.setBinlogConf(binlogConfig);
         builder.setRowConverter(
-                new BinlogRowConverter(
+                new BinlogSqlConverter(
                         InternalTypeInfo.of(dataType.getLogicalType()).toRowType(),
                         this.timestampFormat));
 

File: chunjun-connectors/chunjun-connector-clickhouse/src/main/java/com/dtstack/chunjun/connector/clickhouse/source/ClickhouseInputFormatBuilder.java
Patch:
@@ -61,7 +61,7 @@ protected void checkFormat() {
                         FieldConfig.getSameNameMetaColumn(config.getColumn(), config.getSplitPk());
                 if (field == null) {
                     sb.append("split column must in columns;\n");
-                } else if (!ColumnType.isNumberType(field.getType())) {
+                } else if (!ColumnType.isNumberType(field.getType().getType())) {
                     sb.append("split column's type must be number type;\n");
                 }
             }

File: chunjun-connectors/chunjun-connector-db2/src/main/java/com/dtstack/chunjun/connector/db2/dialect/Db2Dialect.java
Patch:
@@ -25,7 +25,7 @@
 import com.dtstack.chunjun.connector.jdbc.dialect.JdbcDialect;
 import com.dtstack.chunjun.connector.jdbc.statement.FieldNamedPreparedStatement;
 import com.dtstack.chunjun.converter.AbstractRowConverter;
-import com.dtstack.chunjun.converter.RawTypeConverter;
+import com.dtstack.chunjun.converter.RawTypeMapper;
 
 import org.apache.flink.table.types.logical.LogicalType;
 import org.apache.flink.table.types.logical.RowType;
@@ -59,7 +59,7 @@ public boolean canHandle(String url) {
     }
 
     @Override
-    public RawTypeConverter getRawTypeConverter() {
+    public RawTypeMapper getRawTypeConverter() {
         return Db2RawTypeConverter::apply;
     }
 

File: chunjun-connectors/chunjun-connector-dm/src/main/java/com/dtstack/chunjun/connector/dm/dialect/DmDialect.java
Patch:
@@ -25,7 +25,7 @@
 import com.dtstack.chunjun.connector.jdbc.dialect.JdbcDialect;
 import com.dtstack.chunjun.connector.jdbc.statement.FieldNamedPreparedStatement;
 import com.dtstack.chunjun.converter.AbstractRowConverter;
-import com.dtstack.chunjun.converter.RawTypeConverter;
+import com.dtstack.chunjun.converter.RawTypeMapper;
 import com.dtstack.chunjun.throwable.ChunJunRuntimeException;
 
 import org.apache.flink.table.types.logical.LogicalType;
@@ -55,7 +55,7 @@ public boolean canHandle(String url) {
     }
 
     @Override
-    public RawTypeConverter getRawTypeConverter() {
+    public RawTypeMapper getRawTypeConverter() {
         return DmRawTypeConverter::apply;
     }
 

File: chunjun-connectors/chunjun-connector-doris/src/main/java/com/dtstack/chunjun/connector/doris/converter/DorisHttpSqlConverter.java
Patch:
@@ -27,14 +27,14 @@
 import org.apache.flink.table.types.logical.LogicalTypeRoot;
 import org.apache.flink.table.types.logical.RowType;
 
-public class DorisHttpRowConverter
+public class DorisHttpSqlConverter
         extends AbstractRowConverter<RowData, RowData, String[], LogicalType> {
 
     private static final String NULL_VALUE = "\\N";
 
     private static final long serialVersionUID = -2636292632781799617L;
 
-    public DorisHttpRowConverter(RowType rowType) {
+    public DorisHttpSqlConverter(RowType rowType) {
         super(rowType);
         for (int i = 0; i < rowType.getFieldCount(); i++) {
             toInternalConverters.add(

File: chunjun-connectors/chunjun-connector-elasticsearch-base/src/main/java/com/dtstack/chunjun/connector/elasticsearch/ElasticsearchSqlConverter.java
Patch:
@@ -58,14 +58,14 @@
 import java.util.stream.Collectors;
 
 @Slf4j
-public class ElasticsearchRowConverter
+public class ElasticsearchSqlConverter
         extends AbstractRowConverter<
                 Map<String, Object>, Map<String, Object>, Map<String, Object>, LogicalType> {
 
     private static final long serialVersionUID = -1093338072559909026L;
     private final List<Tuple2<String, Integer>> typeIndexList = new ArrayList<>();
 
-    public ElasticsearchRowConverter(RowType rowType) {
+    public ElasticsearchSqlConverter(RowType rowType) {
         super(rowType);
         List<RowType.RowField> fields = rowType.getFields();
 

File: chunjun-connectors/chunjun-connector-elasticsearch6/src/main/java/com/dtstack/chunjun/connector/elasticsearch6/table/Elasticsearch6DynamicTableSink.java
Patch:
@@ -18,7 +18,7 @@
 
 package com.dtstack.chunjun.connector.elasticsearch6.table;
 
-import com.dtstack.chunjun.connector.elasticsearch.ElasticsearchRowConverter;
+import com.dtstack.chunjun.connector.elasticsearch.ElasticsearchSqlConverter;
 import com.dtstack.chunjun.connector.elasticsearch6.Elasticsearch6Config;
 import com.dtstack.chunjun.connector.elasticsearch6.sink.Elasticsearch6OutputFormatBuilder;
 import com.dtstack.chunjun.sink.DtOutputFormatSinkFunction;
@@ -59,7 +59,7 @@ public SinkRuntimeProvider getSinkRuntimeProvider(Context context) {
 
         Elasticsearch6OutputFormatBuilder builder = new Elasticsearch6OutputFormatBuilder();
         builder.setRowConverter(
-                new ElasticsearchRowConverter(InternalTypeInfo.of(logicalType).toRowType()));
+                new ElasticsearchSqlConverter(InternalTypeInfo.of(logicalType).toRowType()));
         builder.setEsConf(elasticsearchConfig);
 
         return SinkFunctionProvider.of(

File: chunjun-connectors/chunjun-connector-elasticsearch7/src/main/java/com/dtstack/chunjun/connector/elasticsearch7/table/ElasticsearchDynamicTableSink.java
Patch:
@@ -18,7 +18,7 @@
 
 package com.dtstack.chunjun.connector.elasticsearch7.table;
 
-import com.dtstack.chunjun.connector.elasticsearch.ElasticsearchRowConverter;
+import com.dtstack.chunjun.connector.elasticsearch.ElasticsearchSqlConverter;
 import com.dtstack.chunjun.connector.elasticsearch7.ElasticsearchConfig;
 import com.dtstack.chunjun.connector.elasticsearch7.sink.ElasticsearchOutputFormatBuilder;
 import com.dtstack.chunjun.sink.DtOutputFormatSinkFunction;
@@ -60,7 +60,7 @@ public SinkRuntimeProvider getSinkRuntimeProvider(Context context) {
         ElasticsearchOutputFormatBuilder builder =
                 new ElasticsearchOutputFormatBuilder(elasticsearchConfig, physicalSchema);
         builder.setRowConverter(
-                new ElasticsearchRowConverter(InternalTypeInfo.of(logicalType).toRowType()));
+                new ElasticsearchSqlConverter(InternalTypeInfo.of(logicalType).toRowType()));
 
         return SinkFunctionProvider.of(
                 new DtOutputFormatSinkFunction<>(builder.finish()),

File: chunjun-connectors/chunjun-connector-emqx/src/main/java/com/dtstack/chunjun/connector/emqx/converter/EmqxSyncConverter.java
Patch:
@@ -40,7 +40,7 @@
 
 import static com.dtstack.chunjun.connector.emqx.options.EmqxOptions.DEFAULT_CODEC;
 
-public class EmqxColumnConverter
+public class EmqxSyncConverter
         extends AbstractRowConverter<String, Object, MqttMessage, LogicalType> {
 
     private static final long serialVersionUID = -2424508439125862995L;
@@ -52,7 +52,7 @@ public class EmqxColumnConverter
     /** emqx Conf */
     private final EmqxConfig emqxConfig;
 
-    public EmqxColumnConverter(EmqxConfig emqxConfig) {
+    public EmqxSyncConverter(EmqxConfig emqxConfig) {
         this.emqxConfig = emqxConfig;
 
         if (DEFAULT_CODEC.defaultValue().equals(emqxConfig.getCodec())) {

File: chunjun-connectors/chunjun-connector-emqx/src/main/java/com/dtstack/chunjun/connector/emqx/sink/EmqxDynamicTableSink.java
Patch:
@@ -19,7 +19,7 @@
 package com.dtstack.chunjun.connector.emqx.sink;
 
 import com.dtstack.chunjun.connector.emqx.config.EmqxConfig;
-import com.dtstack.chunjun.connector.emqx.converter.EmqxRowConverter;
+import com.dtstack.chunjun.connector.emqx.converter.EmqxSqlConverter;
 import com.dtstack.chunjun.sink.DtOutputFormatSinkFunction;
 
 import org.apache.flink.api.common.serialization.SerializationSchema;
@@ -59,7 +59,7 @@ public SinkRuntimeProvider getSinkRuntimeProvider(Context runtimeProviderContext
         EmqxOutputFormatBuilder builder = new EmqxOutputFormatBuilder();
         builder.setEmqxConf(emqxConfig);
         builder.setRowConverter(
-                new EmqxRowConverter(
+                new EmqxSqlConverter(
                         valueEncodingFormat.createRuntimeEncoder(
                                 runtimeProviderContext, physicalSchema.toPhysicalRowDataType())));
 

File: chunjun-connectors/chunjun-connector-emqx/src/main/java/com/dtstack/chunjun/connector/emqx/source/EmqxDynamicTableSource.java
Patch:
@@ -19,7 +19,7 @@
 package com.dtstack.chunjun.connector.emqx.source;
 
 import com.dtstack.chunjun.connector.emqx.config.EmqxConfig;
-import com.dtstack.chunjun.connector.emqx.converter.EmqxRowConverter;
+import com.dtstack.chunjun.connector.emqx.converter.EmqxSqlConverter;
 import com.dtstack.chunjun.source.DtInputFormatSourceFunction;
 import com.dtstack.chunjun.table.connector.source.ParallelSourceFunctionProvider;
 
@@ -65,7 +65,7 @@ public ScanRuntimeProvider getScanRuntimeProvider(ScanContext runtimeProviderCon
         EmqxInputFormatBuilder builder = new EmqxInputFormatBuilder();
         builder.setEmqxConf(emqxConfig);
         builder.setRowConverter(
-                new EmqxRowConverter(
+                new EmqxSqlConverter(
                         valueDecodingFormat.createRuntimeDecoder(
                                 runtimeProviderContext, physicalSchema.toPhysicalRowDataType())));
 

File: chunjun-connectors/chunjun-connector-file/src/main/java/com/dtstack/chunjun/connector/file/converter/FileSqlConverter.java
Patch:
@@ -25,13 +25,13 @@
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.types.logical.LogicalType;
 
-public class FileRowConverter extends AbstractRowConverter<String, String, String, LogicalType> {
+public class FileSqlConverter extends AbstractRowConverter<String, String, String, LogicalType> {
 
     private static final long serialVersionUID = -3575052921856513308L;
 
     private final DeserializationSchema<RowData> valueDeserialization;
 
-    public FileRowConverter(DeserializationSchema<RowData> valueDeserialization) {
+    public FileSqlConverter(DeserializationSchema<RowData> valueDeserialization) {
         this.valueDeserialization = valueDeserialization;
     }
 

File: chunjun-connectors/chunjun-connector-file/src/main/java/com/dtstack/chunjun/connector/file/table/FileDynamicTableSource.java
Patch:
@@ -19,7 +19,7 @@
 package com.dtstack.chunjun.connector.file.table;
 
 import com.dtstack.chunjun.config.BaseFileConfig;
-import com.dtstack.chunjun.connector.file.converter.FileRowConverter;
+import com.dtstack.chunjun.connector.file.converter.FileSqlConverter;
 import com.dtstack.chunjun.connector.file.source.FileInputFormatBuilder;
 import com.dtstack.chunjun.source.DtInputFormatSourceFunction;
 import com.dtstack.chunjun.table.connector.source.ParallelSourceFunctionProvider;
@@ -65,7 +65,7 @@ public ScanRuntimeProvider getScanRuntimeProvider(ScanContext runtimeProviderCon
         FileInputFormatBuilder builder = new FileInputFormatBuilder();
         builder.setFileConf(fileConfig);
         builder.setRowConverter(
-                new FileRowConverter(
+                new FileSqlConverter(
                         decodingFormat.createRuntimeDecoder(runtimeProviderContext, dataType)));
 
         return ParallelSourceFunctionProvider.of(

File: chunjun-connectors/chunjun-connector-ftp/src/main/java/com/dtstack/chunjun/connector/ftp/converter/FtpSqlConverter.java
Patch:
@@ -25,19 +25,19 @@
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.types.logical.LogicalType;
 
-public class FtpRowConverter extends AbstractRowConverter<String, String, String, LogicalType> {
+public class FtpSqlConverter extends AbstractRowConverter<String, String, String, LogicalType> {
 
     private static final long serialVersionUID = 4127516611259169686L;
 
     private DeserializationSchema<RowData> valueDeserialization;
 
     private SerializationSchema<RowData> valueSerialization;
 
-    public FtpRowConverter(DeserializationSchema<RowData> valueDeserialization) {
+    public FtpSqlConverter(DeserializationSchema<RowData> valueDeserialization) {
         this.valueDeserialization = valueDeserialization;
     }
 
-    public FtpRowConverter(SerializationSchema<RowData> valueSerialization) {
+    public FtpSqlConverter(SerializationSchema<RowData> valueSerialization) {
         this.valueSerialization = valueSerialization;
     }
 

File: chunjun-connectors/chunjun-connector-ftp/src/main/java/com/dtstack/chunjun/connector/ftp/sink/FtpDynamicTableSink.java
Patch:
@@ -19,7 +19,7 @@
 package com.dtstack.chunjun.connector.ftp.sink;
 
 import com.dtstack.chunjun.connector.ftp.config.FtpConfig;
-import com.dtstack.chunjun.connector.ftp.converter.FtpRowConverter;
+import com.dtstack.chunjun.connector.ftp.converter.FtpSqlConverter;
 import com.dtstack.chunjun.sink.DtOutputFormatSinkFunction;
 
 import org.apache.flink.api.common.serialization.SerializationSchema;
@@ -55,7 +55,7 @@ public SinkRuntimeProvider getSinkRuntimeProvider(Context context) {
         FtpOutputFormatBuilder builder = new FtpOutputFormatBuilder();
         builder.setFtpConfig(ftpConfig);
         builder.setRowConverter(
-                new FtpRowConverter(
+                new FtpSqlConverter(
                         valueEncodingFormat.createRuntimeEncoder(
                                 context, resolvedSchema.toPhysicalRowDataType())));
 

File: chunjun-connectors/chunjun-connector-ftp/src/main/java/com/dtstack/chunjun/connector/ftp/source/FtpDynamicTableSource.java
Patch:
@@ -19,7 +19,7 @@
 package com.dtstack.chunjun.connector.ftp.source;
 
 import com.dtstack.chunjun.connector.ftp.config.FtpConfig;
-import com.dtstack.chunjun.connector.ftp.converter.FtpRowConverter;
+import com.dtstack.chunjun.connector.ftp.converter.FtpSqlConverter;
 import com.dtstack.chunjun.source.DtInputFormatSourceFunction;
 import com.dtstack.chunjun.table.connector.source.ParallelSourceFunctionProvider;
 
@@ -62,7 +62,7 @@ public ScanRuntimeProvider getScanRuntimeProvider(ScanContext runtimeProviderCon
         FtpInputFormatBuilder builder = new FtpInputFormatBuilder();
         builder.setFtpConfig(ftpConfig);
         builder.setRowConverter(
-                new FtpRowConverter(
+                new FtpSqlConverter(
                         decodingFormat.createRuntimeDecoder(runtimeProviderContext, dataType)));
 
         return ParallelSourceFunctionProvider.of(

File: chunjun-connectors/chunjun-connector-ftp/src/main/java/com/dtstack/chunjun/connector/ftp/table/FtpDynamicTableFactory.java
Patch:
@@ -19,6 +19,7 @@
 package com.dtstack.chunjun.connector.ftp.table;
 
 import com.dtstack.chunjun.config.FieldConfig;
+import com.dtstack.chunjun.config.TypeConfig;
 import com.dtstack.chunjun.connector.ftp.config.FtpConfig;
 import com.dtstack.chunjun.connector.ftp.options.FtpOptions;
 import com.dtstack.chunjun.connector.ftp.sink.FtpDynamicTableSink;
@@ -101,7 +102,8 @@ public DynamicTableSource createDynamicTableSource(Context context) {
         for (Column column : columns) {
             FieldConfig field = new FieldConfig();
             field.setName(column.getName());
-            field.setType(column.getDataType().getLogicalType().asSummaryString());
+            field.setType(
+                    TypeConfig.fromString(column.getDataType().getLogicalType().asSummaryString()));
             field.setIndex(columns.indexOf(column));
             columnList.add(field);
         }

File: chunjun-connectors/chunjun-connector-hbase-base/src/main/java/com/dtstack/chunjun/connector/hbase/converter/HBaseSqlConverter.java
Patch:
@@ -38,7 +38,7 @@
 
 import static org.apache.flink.table.types.logical.utils.LogicalTypeChecks.getPrecision;
 
-public class HBaseRowConverter
+public class HBaseSqlConverter
         extends AbstractRowConverter<Result, RowData, Mutation, LogicalType> {
     private static final long serialVersionUID = -8935215591844851238L;
 
@@ -50,7 +50,7 @@ public class HBaseRowConverter
     private final String nullStringLiteral;
     private transient HBaseSerde serde;
 
-    public HBaseRowConverter(HBaseTableSchema schema, String nullStringLiteral) {
+    public HBaseSqlConverter(HBaseTableSchema schema, String nullStringLiteral) {
         this.schema = schema;
         this.nullStringLiteral = nullStringLiteral;
     }

File: chunjun-connectors/chunjun-connector-hbase-base/src/main/java/com/dtstack/chunjun/connector/hbase/source/HBaseInputFormat.java
Patch:
@@ -18,6 +18,7 @@
 
 package com.dtstack.chunjun.connector.hbase.source;
 
+import com.dtstack.chunjun.config.TypeConfig;
 import com.dtstack.chunjun.connector.hbase.util.HBaseHelper;
 import com.dtstack.chunjun.connector.hbase.util.ScanBuilder;
 import com.dtstack.chunjun.source.format.BaseRichInputFormat;
@@ -54,7 +55,7 @@ public class HBaseInputFormat extends BaseRichInputFormat {
     protected List<String> columnNames;
     protected List<String> columnValues;
     protected List<String> columnFormats;
-    protected List<String> columnTypes;
+    protected List<TypeConfig> columnTypes;
     protected boolean isBinaryRowkey;
     /** 客户端每次 rpc fetch 的行数 */
     protected int scanCacheSize = 1000;

File: chunjun-connectors/chunjun-connector-hbase-base/src/main/java/com/dtstack/chunjun/connector/hbase/table/BaseHBaseDynamicTableSource.java
Patch:
@@ -19,6 +19,7 @@
 package com.dtstack.chunjun.connector.hbase.table;
 
 import com.dtstack.chunjun.config.FieldConfig;
+import com.dtstack.chunjun.config.TypeConfig;
 import com.dtstack.chunjun.connector.hbase.HBaseTableSchema;
 import com.dtstack.chunjun.connector.hbase.config.HBaseConfig;
 import com.dtstack.chunjun.connector.hbase.util.HBaseConfigUtils;
@@ -75,7 +76,7 @@ public ScanRuntimeProvider getScanRuntimeProvider(ScanContext runtimeProviderCon
         for (int i = 0; i < fieldNames.length; i++) {
             FieldConfig field = new FieldConfig();
             field.setName(fieldNames[i]);
-            field.setType(rowType.getTypeAt(i).asSummaryString());
+            field.setType(TypeConfig.fromString(rowType.getTypeAt(i).asSummaryString()));
             field.setIndex(i);
             columnList.add(field);
         }

File: chunjun-connectors/chunjun-connector-hbase-base/src/main/java/com/dtstack/chunjun/connector/hbase/table/HBaseDynamicTableSink.java
Patch:
@@ -19,7 +19,7 @@
 
 import com.dtstack.chunjun.connector.hbase.HBaseTableSchema;
 import com.dtstack.chunjun.connector.hbase.config.HBaseConfig;
-import com.dtstack.chunjun.connector.hbase.converter.HBaseRowConverter;
+import com.dtstack.chunjun.connector.hbase.converter.HBaseSqlConverter;
 import com.dtstack.chunjun.connector.hbase.sink.HBaseOutputFormatBuilder;
 import com.dtstack.chunjun.sink.DtOutputFormatSinkFunction;
 
@@ -58,8 +58,8 @@ public SinkFunctionProvider getSinkRuntimeProvider(Context context) {
         builder.setHbaseConfig(config.getHbaseConfig());
         builder.setTableName(config.getTable());
         builder.setWriteBufferSize(config.getWriteBufferSize());
-        HBaseRowConverter hbaseRowConverter = new HBaseRowConverter(hbaseSchema, nullStringLiteral);
-        builder.setRowConverter(hbaseRowConverter);
+        HBaseSqlConverter hbaseSqlConverter = new HBaseSqlConverter(hbaseSchema, nullStringLiteral);
+        builder.setRowConverter(hbaseSqlConverter);
         builder.setConfig(config);
         return SinkFunctionProvider.of(
                 new DtOutputFormatSinkFunction<>(builder.finish()), config.getParallelism());

File: chunjun-connectors/chunjun-connector-hbase-base/src/main/java/com/dtstack/chunjun/connector/hbase/table/HBaseDynamicTableSource.java
Patch:
@@ -19,7 +19,7 @@
 
 import com.dtstack.chunjun.connector.hbase.HBaseTableSchema;
 import com.dtstack.chunjun.connector.hbase.config.HBaseConfig;
-import com.dtstack.chunjun.connector.hbase.converter.HBaseRowConverter;
+import com.dtstack.chunjun.connector.hbase.converter.HBaseSqlConverter;
 import com.dtstack.chunjun.connector.hbase.source.HBaseInputFormatBuilder;
 import com.dtstack.chunjun.connector.hbase.table.lookup.HBaseAllTableFunction;
 import com.dtstack.chunjun.connector.hbase.table.lookup.HBaseLruTableFunction;
@@ -76,14 +76,14 @@ protected BaseRichInputFormatBuilder<?> getBaseRichInputFormatBuilder() {
         builder.setConfig(hBaseConfig);
         builder.setHbaseConfig(hBaseConfig.getHbaseConfig());
         // 投影下推后, hbaseSchema 会被过滤无用的字段，而 tableSchema 不变, 后面根据 hbaseSchema 生成 hbase scan
-        AbstractRowConverter rowConverter = new HBaseRowConverter(hbaseSchema, nullStringLiteral);
+        AbstractRowConverter rowConverter = new HBaseSqlConverter(hbaseSchema, nullStringLiteral);
         builder.setRowConverter(rowConverter);
         return builder;
     }
 
     @Override
     protected AbstractLruTableFunction getAbstractLruTableFunction() {
-        AbstractRowConverter rowConverter = new HBaseRowConverter(hbaseSchema, nullStringLiteral);
+        AbstractRowConverter rowConverter = new HBaseSqlConverter(hbaseSchema, nullStringLiteral);
         return new HBaseLruTableFunction(lookupConfig, hbaseSchema, hBaseConfig, rowConverter);
     }
 

File: chunjun-connectors/chunjun-connector-hdfs/src/main/java/com/dtstack/chunjun/connector/hdfs/converter/HdfsOrcSqlConverter.java
Patch:
@@ -43,12 +43,12 @@
 import java.sql.Timestamp;
 import java.time.LocalDate;
 
-public class HdfsOrcRowConverter
+public class HdfsOrcSqlConverter
         extends AbstractRowConverter<RowData, RowData, Object[], LogicalType> {
 
     private static final long serialVersionUID = 6632938157518455020L;
 
-    public HdfsOrcRowConverter(RowType rowType) {
+    public HdfsOrcSqlConverter(RowType rowType) {
         super(rowType);
         for (int i = 0; i < rowType.getFieldCount(); i++) {
             toInternalConverters.add(

File: chunjun-connectors/chunjun-connector-hdfs/src/main/java/com/dtstack/chunjun/connector/hdfs/converter/HdfsParquetSqlConverter.java
Patch:
@@ -47,14 +47,14 @@
 import java.time.LocalDate;
 import java.util.List;
 
-public class HdfsParquetRowConverter
+public class HdfsParquetSqlConverter
         extends AbstractRowConverter<RowData, RowData, Group, LogicalType> {
 
     private static final long serialVersionUID = -4819153961537643326L;
 
     private List<String> columnNameList;
 
-    public HdfsParquetRowConverter(RowType rowType) {
+    public HdfsParquetSqlConverter(RowType rowType) {
         super(rowType);
         for (int i = 0; i < rowType.getFieldCount(); i++) {
             toInternalConverters.add(

File: chunjun-connectors/chunjun-connector-hdfs/src/main/java/com/dtstack/chunjun/connector/hdfs/converter/HdfsTextSqlConverter.java
Patch:
@@ -44,12 +44,12 @@
 import java.time.temporal.TemporalQueries;
 import java.util.Arrays;
 
-public class HdfsTextRowConverter
+public class HdfsTextSqlConverter
         extends AbstractRowConverter<RowData, RowData, String[], LogicalType> {
 
     private static final long serialVersionUID = -165352676945714752L;
 
-    public HdfsTextRowConverter(RowType rowType) {
+    public HdfsTextSqlConverter(RowType rowType) {
         super(rowType);
         for (int i = 0; i < rowType.getFieldCount(); i++) {
             toInternalConverters.add(

File: chunjun-connectors/chunjun-connector-hdfs/src/main/java/com/dtstack/chunjun/connector/hdfs/sink/BaseHdfsOutputFormat.java
Patch:
@@ -86,11 +86,11 @@ protected void initVariableFields() {
         }
 
         if (CollectionUtils.isNotEmpty(hdfsConfig.getFullColumnType())) {
-            fullColumnTypeList = hdfsConfig.getFullColumnType();
+            List<String> fullColumnType = hdfsConfig.getFullColumnType();
         } else {
             fullColumnTypeList =
                     hdfsConfig.getColumn().stream()
-                            .map(FieldConfig::getType)
+                            .map(fieldConfig -> fieldConfig.getType().getType())
                             .collect(Collectors.toList());
             hdfsConfig.setFullColumnType(fullColumnTypeList);
         }

File: chunjun-connectors/chunjun-connector-hive/src/main/java/com/dtstack/chunjun/connector/hive/converter/HiveJdbcSyncConverter.java
Patch:
@@ -22,9 +22,9 @@
 
 import org.apache.flink.table.types.logical.RowType;
 
-public class HiveJdbcColumnConverter extends JdbcSyncConverter {
+public class HiveJdbcSyncConverter extends JdbcSyncConverter {
 
-    public HiveJdbcColumnConverter(RowType rowType, CommonConfig commonConfig) {
+    public HiveJdbcSyncConverter(RowType rowType, CommonConfig commonConfig) {
         super(rowType, commonConfig);
     }
 }

File: chunjun-connectors/chunjun-connector-hive3/src/main/java/com/dtstack/chunjun/connector/hive3/converter/HdfsOrcSqlConverter.java
Patch:
@@ -44,11 +44,11 @@
 import java.sql.Timestamp;
 import java.util.List;
 
-public class HdfsOrcRowConverter
+public class HdfsOrcSqlConverter
         extends AbstractRowConverter<RowData, RowData, List<Object>, LogicalType> {
     private static final long serialVersionUID = -3115772125863778621L;
 
-    public HdfsOrcRowConverter(RowType rowType, HdfsConfig hdfsConfig) {
+    public HdfsOrcSqlConverter(RowType rowType, HdfsConfig hdfsConfig) {
         super(rowType, hdfsConfig);
         for (int i = 0; i < rowType.getFieldCount(); i++) {
             toInternalConverters.add(

File: chunjun-connectors/chunjun-connector-hive3/src/main/java/com/dtstack/chunjun/connector/hive3/converter/HdfsParquetSqlConverter.java
Patch:
@@ -52,15 +52,15 @@
 import java.util.List;
 import java.util.stream.Collectors;
 
-public class HdfsParquetRowConverter
+public class HdfsParquetSqlConverter
         extends AbstractRowConverter<RowData, RowData, Group, LogicalType> {
 
     private static final long serialVersionUID = 4755990143355610314L;
 
     HdfsConfig hdfsConfig;
     private final List<String> columnNameList;
 
-    public HdfsParquetRowConverter(RowType rowType, HdfsConfig hdfsConfig) {
+    public HdfsParquetSqlConverter(RowType rowType, HdfsConfig hdfsConfig) {
         super(rowType, hdfsConfig);
         this.hdfsConfig = hdfsConfig;
         List<FieldConfig> fieldConfList = hdfsConfig.getColumn();

File: chunjun-connectors/chunjun-connector-hive3/src/main/java/com/dtstack/chunjun/connector/hive3/converter/HdfsTextSqlConverter.java
Patch:
@@ -47,14 +47,14 @@
 import java.util.Arrays;
 import java.util.List;
 
-public class HdfsTextRowConverter
+public class HdfsTextSqlConverter
         extends AbstractRowConverter<RowData, RowData, List<String>, LogicalType> {
 
     private static final long serialVersionUID = -2860279052347532462L;
 
     HdfsConfig hdfsConfig;
 
-    public HdfsTextRowConverter(RowType rowType, HdfsConfig hdfsConfig) {
+    public HdfsTextSqlConverter(RowType rowType, HdfsConfig hdfsConfig) {
         super(rowType, hdfsConfig);
         this.hdfsConfig = hdfsConfig;
         for (int i = 0; i < rowType.getFieldCount(); i++) {

File: chunjun-connectors/chunjun-connector-http/src/main/java/com/dtstack/chunjun/connector/http/converter/HttpSqlConverter.java
Patch:
@@ -43,14 +43,14 @@
 import java.util.Map;
 
 /** Base class for all converters that convert between restapi body and Flink internal object. */
-public class HttpRowConverter
+public class HttpSqlConverter
         extends AbstractRowConverter<Map<String, Object>, RowData, RowData, LogicalType> {
 
     private static final long serialVersionUID = -9145005567073875082L;
 
     private HttpRestConfig httpRestConfig;
 
-    public HttpRowConverter(RowType rowType, HttpRestConfig httpRestConfig) {
+    public HttpSqlConverter(RowType rowType, HttpRestConfig httpRestConfig) {
         super(rowType);
         for (int i = 0; i < rowType.getFieldCount(); i++) {
             toInternalConverters.add(
@@ -60,7 +60,7 @@ public HttpRowConverter(RowType rowType, HttpRestConfig httpRestConfig) {
         this.httpRestConfig = httpRestConfig;
     }
 
-    public HttpRowConverter(RowType rowType) {
+    public HttpSqlConverter(RowType rowType) {
         super(rowType);
         for (int i = 0; i < rowType.getFieldCount(); i++) {
             toExternalConverters.add(

File: chunjun-connectors/chunjun-connector-http/src/main/java/com/dtstack/chunjun/connector/http/converter/HttpSyncConverter.java
Patch:
@@ -44,15 +44,15 @@
 import java.util.Locale;
 import java.util.Map;
 
-public class HttpColumnConverter
+public class HttpSyncConverter
         extends AbstractRowConverter<Map<String, Object>, Object, Map<String, Object>, String> {
 
     private static final long serialVersionUID = -7759259374957914968L;
 
     /** restapi Config */
     private final HttpRestConfig httpRestConfig;
 
-    public HttpColumnConverter(HttpRestConfig httpRestConfig) {
+    public HttpSyncConverter(HttpRestConfig httpRestConfig) {
         this.httpRestConfig = httpRestConfig;
         this.commonConfig = httpRestConfig;
 

File: chunjun-connectors/chunjun-connector-http/src/main/java/com/dtstack/chunjun/connector/http/sink/HttpSinkFactory.java
Patch:
@@ -21,7 +21,7 @@
 import com.dtstack.chunjun.config.SyncConfig;
 import com.dtstack.chunjun.connector.http.common.HttpWriterConfig;
 import com.dtstack.chunjun.connector.http.outputformat.HttpOutputFormatBuilder;
-import com.dtstack.chunjun.converter.RawTypeConverter;
+import com.dtstack.chunjun.converter.RawTypeMapper;
 import com.dtstack.chunjun.sink.SinkFactory;
 import com.dtstack.chunjun.util.JsonUtil;
 
@@ -34,7 +34,7 @@ public class HttpSinkFactory extends SinkFactory {
     protected HttpWriterConfig httpWriterConfig;
 
     @Override
-    public RawTypeConverter getRawTypeConverter() {
+    public RawTypeMapper getRawTypeMapper() {
         return null;
     }
 

File: chunjun-connectors/chunjun-connector-http/src/main/java/com/dtstack/chunjun/connector/http/table/HttpDynamicTableSink.java
Patch:
@@ -19,7 +19,7 @@
 
 import com.dtstack.chunjun.config.FieldConfig;
 import com.dtstack.chunjun.connector.http.common.HttpWriterConfig;
-import com.dtstack.chunjun.connector.http.converter.HttpRowConverter;
+import com.dtstack.chunjun.connector.http.converter.HttpSqlConverter;
 import com.dtstack.chunjun.connector.http.outputformat.HttpOutputFormatBuilder;
 import com.dtstack.chunjun.sink.DtOutputFormatSinkFunction;
 
@@ -72,7 +72,7 @@ public SinkRuntimeProvider getSinkRuntimeProvider(Context context) {
         }
         restapiWriterConf.setColumn(fieldList);
         HttpOutputFormatBuilder builder = new HttpOutputFormatBuilder();
-        builder.setRowConverter(new HttpRowConverter(rowType));
+        builder.setRowConverter(new HttpSqlConverter(rowType));
         builder.setConfig(restapiWriterConf);
 
         return SinkFunctionProvider.of(new DtOutputFormatSinkFunction<>(builder.finish()), 1);

File: chunjun-connectors/chunjun-connector-http/src/main/java/com/dtstack/chunjun/connector/http/table/HttpDynamicTableSource.java
Patch:
@@ -23,7 +23,7 @@
 import com.dtstack.chunjun.connector.http.common.HttpRestConfig;
 import com.dtstack.chunjun.connector.http.common.MetaParam;
 import com.dtstack.chunjun.connector.http.common.ParamType;
-import com.dtstack.chunjun.connector.http.converter.HttpRowConverter;
+import com.dtstack.chunjun.connector.http.converter.HttpSqlConverter;
 import com.dtstack.chunjun.connector.http.inputformat.HttpInputFormatBuilder;
 import com.dtstack.chunjun.source.DtInputFormatSourceFunction;
 import com.dtstack.chunjun.table.connector.source.ParallelSourceFunctionProvider;
@@ -65,7 +65,7 @@ public ScanRuntimeProvider getScanRuntimeProvider(ScanContext runtimeProviderCon
         HttpInputFormatBuilder builder = new HttpInputFormatBuilder();
         builder.setHttpRestConfig(httpRestConfig);
         builder.setRowConverter(
-                new HttpRowConverter(
+                new HttpSqlConverter(
                         (RowType) this.schema.toRowDataType().getLogicalType(), httpRestConfig));
         builder.setMetaHeaders(httpRestConfig.getHeader());
         builder.setMetaParams(

File: chunjun-connectors/chunjun-connector-hudi/src/main/java/com/dtstack/chunjun/connector/hudi/adaptor/HudiOnChunjunAdaptor.java
Patch:
@@ -19,6 +19,7 @@
 
 import com.dtstack.chunjun.config.FieldConfig;
 import com.dtstack.chunjun.config.SyncConfig;
+
 import org.apache.flink.api.java.typeutils.TypeExtractionException;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.streaming.api.datastream.DataStream;

File: chunjun-connectors/chunjun-connector-hudi/src/main/java/com/dtstack/chunjun/connector/hudi/sink/HudiSinkFactory.java
Patch:
@@ -21,7 +21,7 @@
 import com.dtstack.chunjun.config.SyncConfig;
 import com.dtstack.chunjun.connector.hudi.adaptor.HudiOnChunjunAdaptor;
 import com.dtstack.chunjun.connector.hudi.converter.HudiRowDataMapping;
-import com.dtstack.chunjun.converter.RawTypeConverter;
+import com.dtstack.chunjun.converter.RawTypeMapper;
 import com.dtstack.chunjun.sink.SinkFactory;
 import com.dtstack.chunjun.util.TableUtil;
 
@@ -55,15 +55,15 @@ public HudiSinkFactory(SyncConfig syncConf) {
     }
 
     @Override
-    public RawTypeConverter getRawTypeConverter() {
+    public RawTypeMapper getRawTypeMapper() {
         return HudiRowDataMapping::apply;
     }
 
     @Override
     public DataStreamSink<RowData> createSink(DataStream<RowData> dataSet) {
         //        RowType rowType = null;
         List<FieldConfig> fieldList = syncConfig.getWriter().getFieldList();
-        ResolvedSchema schema = TableUtil.createTableSchema(fieldList, getRawTypeConverter());
+        ResolvedSchema schema = TableUtil.createTableSchema(fieldList, getRawTypeMapper());
 
         try {
             return adaptor.createHudiSinkDataStream(dataSet, schema);

File: chunjun-connectors/chunjun-connector-iceberg/src/main/java/com/dtstack/chunjun/connector/iceberg/sink/ChunjunRowDataConvertMap.java
Patch:
@@ -55,7 +55,7 @@ public RowData map(RowData row) {
                 } else {
                     index = column.getIndex();
                 }
-                String type = column.getType();
+                String type = column.getType().getType();
                 Object value = getRowDataByType(row, type, index);
                 convertedData.setField(column.getIndex(), value);
             }

File: chunjun-connectors/chunjun-connector-iceberg/src/main/java/com/dtstack/chunjun/connector/iceberg/sink/IcebergSinkFactory.java
Patch:
@@ -21,7 +21,7 @@
 import com.dtstack.chunjun.config.FieldConfig;
 import com.dtstack.chunjun.config.SyncConfig;
 import com.dtstack.chunjun.connector.iceberg.config.IcebergConfig;
-import com.dtstack.chunjun.converter.RawTypeConverter;
+import com.dtstack.chunjun.converter.RawTypeMapper;
 import com.dtstack.chunjun.sink.SinkFactory;
 import com.dtstack.chunjun.sink.WriteMode;
 import com.dtstack.chunjun.util.GsonUtil;
@@ -54,7 +54,7 @@ public IcebergSinkFactory(SyncConfig config) {
     }
 
     @Override
-    public RawTypeConverter getRawTypeConverter() {
+    public RawTypeMapper getRawTypeMapper() {
         return null;
     }
 

File: chunjun-connectors/chunjun-connector-jdbc-base/src/main/java/com/dtstack/chunjun/connector/jdbc/sink/JdbcOutputFormat.java
Patch:
@@ -21,6 +21,7 @@
 import com.dtstack.chunjun.cdc.EventType;
 import com.dtstack.chunjun.cdc.ddl.DdlRowDataConvented;
 import com.dtstack.chunjun.cdc.ddl.definition.TableIdentifier;
+import com.dtstack.chunjun.config.TypeConfig;
 import com.dtstack.chunjun.connector.jdbc.config.JdbcConfig;
 import com.dtstack.chunjun.connector.jdbc.dialect.JdbcDialect;
 import com.dtstack.chunjun.connector.jdbc.sink.wrapper.InsertOrUpdateStatementWrapper;
@@ -551,7 +552,7 @@ public void setColumnNameList(List<String> columnNameList) {
         this.columnNameList = columnNameList;
     }
 
-    public void setColumnTypeList(List<String> columnTypeList) {
+    public void setColumnTypeList(List<TypeConfig> columnTypeList) {
         this.columnTypeList = columnTypeList;
     }
 

File: chunjun-connectors/chunjun-connector-jdbc-base/src/main/java/com/dtstack/chunjun/connector/jdbc/sink/JdbcOutputFormatBuilder.java
Patch:
@@ -17,6 +17,7 @@
  */
 package com.dtstack.chunjun.connector.jdbc.sink;
 
+import com.dtstack.chunjun.config.TypeConfig;
 import com.dtstack.chunjun.connector.jdbc.config.JdbcConfig;
 import com.dtstack.chunjun.connector.jdbc.dialect.JdbcDialect;
 import com.dtstack.chunjun.converter.AbstractRowConverter;
@@ -61,7 +62,7 @@ public void setColumnNameList(List<String> columnNameList) {
         format.setColumnNameList(columnNameList);
     }
 
-    public void setColumnTypeList(List<String> columnTypeList) {
+    public void setColumnTypeList(List<TypeConfig> columnTypeList) {
         format.setColumnTypeList(columnTypeList);
     }
 

File: chunjun-connectors/chunjun-connector-jdbc-base/src/main/java/com/dtstack/chunjun/connector/jdbc/sink/wrapper/proxy/DynamicSimpleDeleteWrapper.java
Patch:
@@ -17,6 +17,7 @@
  */
 package com.dtstack.chunjun.connector.jdbc.sink.wrapper.proxy;
 
+import com.dtstack.chunjun.config.TypeConfig;
 import com.dtstack.chunjun.connector.jdbc.config.JdbcConfig;
 import com.dtstack.chunjun.connector.jdbc.dialect.JdbcDialect;
 import com.dtstack.chunjun.connector.jdbc.sink.wrapper.SimpleStatementWrapper;
@@ -41,7 +42,7 @@ public class DynamicSimpleDeleteWrapper extends SimpleStatementWrapper {
 
     protected List<String> columnNameList = new ArrayList<>();
 
-    protected List<String> columnTypeList = new ArrayList<>();
+    protected List<TypeConfig> columnTypeList = new ArrayList<>();
     protected List<String> nullColumnNameList = new ArrayList<>();
 
     protected JdbcConfig jdbcConfig;
@@ -54,7 +55,7 @@ public DynamicSimpleDeleteWrapper(
 
     public static DynamicSimpleDeleteWrapper buildExecutor(
             List<String> columnNameList,
-            List<String> columnTypeList,
+            List<TypeConfig> columnTypeList,
             List<String> nullColumnNameList,
             String schemaName,
             String tableName,

File: chunjun-connectors/chunjun-connector-jdbc-base/src/main/java/com/dtstack/chunjun/connector/jdbc/sink/wrapper/proxy/NoKeyDeleteWrapperProxy.java
Patch:
@@ -17,6 +17,7 @@
  */
 package com.dtstack.chunjun.connector.jdbc.sink.wrapper.proxy;
 
+import com.dtstack.chunjun.config.TypeConfig;
 import com.dtstack.chunjun.connector.jdbc.dialect.JdbcDialect;
 import com.dtstack.chunjun.element.ColumnRowData;
 import com.dtstack.chunjun.element.column.NullColumn;
@@ -34,7 +35,7 @@ public class NoKeyDeleteWrapperProxy extends CachedWrapperProxy {
     private final String table;
     private final JdbcDialect jdbcDialect;
     private final List<String> columnNameList;
-    private final List<String> columnTypeList;
+    private final List<TypeConfig> columnTypeList;
 
     private List<String> nullColumnNameList = new ArrayList<>();
 
@@ -44,7 +45,7 @@ public NoKeyDeleteWrapperProxy(
             String table,
             JdbcDialect jdbcDialect,
             List<String> columnNameList,
-            List<String> columnTypeList) {
+            List<TypeConfig> columnTypeList) {
         super(connection, 100, 10, true);
         this.schema = schema;
         this.table = table;

File: chunjun-connectors/chunjun-connector-jdbc-base/src/main/java/com/dtstack/chunjun/connector/jdbc/source/JdbcInputFormatBuilder.java
Patch:
@@ -77,7 +77,7 @@ protected void checkFormat() {
                         FieldConfig.getSameNameMetaColumn(conf.getColumn(), conf.getSplitPk());
                 if (field == null) {
                     sb.append("split column must in columns;\n");
-                } else if (!ColumnType.isNumberType(field.getType())) {
+                } else if (!ColumnType.isNumberType(field.getType().getType())) {
                     sb.append("split column's type must be number type;\n");
                 }
             }

File: chunjun-connectors/chunjun-connector-jdbc-base/src/main/java/com/dtstack/chunjun/connector/jdbc/source/distribute/DistributedJdbcSourceFactory.java
Patch:
@@ -19,6 +19,7 @@
 
 import com.dtstack.chunjun.config.FieldConfig;
 import com.dtstack.chunjun.config.SyncConfig;
+import com.dtstack.chunjun.config.TypeConfig;
 import com.dtstack.chunjun.connector.jdbc.config.ConnectionConfig;
 import com.dtstack.chunjun.connector.jdbc.config.DataSourceConfig;
 import com.dtstack.chunjun.connector.jdbc.dialect.JdbcDialect;
@@ -80,7 +81,7 @@ protected void initColumnInfo() {
             this.columnNameList.add(fieldConfig.getName());
             this.columnTypeList.add(fieldConfig.getType());
         }
-        Pair<List<String>, List<String>> columnPair =
+        Pair<List<String>, List<TypeConfig>> columnPair =
                 ColumnBuildUtil.handleColumnList(
                         jdbcConfig.getColumn(), this.columnNameList, this.columnTypeList);
         this.columnNameList = columnPair.getLeft();

File: chunjun-connectors/chunjun-connector-jdbc-base/src/test/java/com/dtstack/chunjun/connector/jdbc/source/distribute/SourceFactoryTest.java
Patch:
@@ -21,7 +21,7 @@
 import com.dtstack.chunjun.config.SyncConfig;
 import com.dtstack.chunjun.connector.jdbc.converter.JdbcRawTypeConverterTest;
 import com.dtstack.chunjun.connector.jdbc.dialect.JdbcDialect;
-import com.dtstack.chunjun.converter.RawTypeConverter;
+import com.dtstack.chunjun.converter.RawTypeMapper;
 
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
 
@@ -63,7 +63,7 @@ public boolean canHandle(String url) {
                             }
 
                             @Override
-                            public RawTypeConverter getRawTypeConverter() {
+                            public RawTypeMapper getRawTypeConverter() {
                                 return JdbcRawTypeConverterTest::apply;
                             }
                         });

File: chunjun-connectors/chunjun-connector-kafka/src/main/java/com/dtstack/chunjun/connector/kafka/sink/KafkaSinkFactory.java
Patch:
@@ -25,7 +25,7 @@
 import com.dtstack.chunjun.connector.kafka.enums.StartupMode;
 import com.dtstack.chunjun.connector.kafka.partitioner.CustomerFlinkPartition;
 import com.dtstack.chunjun.connector.kafka.serialization.RowSerializationSchema;
-import com.dtstack.chunjun.converter.RawTypeConverter;
+import com.dtstack.chunjun.converter.RawTypeMapper;
 import com.dtstack.chunjun.sink.SinkFactory;
 import com.dtstack.chunjun.util.GsonUtil;
 
@@ -139,7 +139,7 @@ public DataStreamSink<RowData> createSink(DataStream<RowData> dataSet) {
     }
 
     @Override
-    public RawTypeConverter getRawTypeConverter() {
+    public RawTypeMapper getRawTypeMapper() {
         return KafkaRawTypeMapping::apply;
     }
 }

File: chunjun-connectors/chunjun-connector-kafka/src/main/java/com/dtstack/chunjun/connector/kafka/source/KafkaSourceFactory.java
Patch:
@@ -28,7 +28,7 @@
 import com.dtstack.chunjun.connector.kafka.serialization.RowDeserializationSchema;
 import com.dtstack.chunjun.connector.kafka.util.FormatNameConvertUtil;
 import com.dtstack.chunjun.connector.kafka.util.KafkaUtil;
-import com.dtstack.chunjun.converter.RawTypeConverter;
+import com.dtstack.chunjun.converter.RawTypeMapper;
 import com.dtstack.chunjun.source.SourceFactory;
 import com.dtstack.chunjun.util.GsonUtil;
 import com.dtstack.chunjun.util.PluginUtil;
@@ -161,7 +161,7 @@ public Set<URL> getExtraUrl() {
     }
 
     @Override
-    public RawTypeConverter getRawTypeConverter() {
+    public RawTypeMapper getRawTypeMapper() {
         return KafkaRawTypeMapping::apply;
     }
 }

File: chunjun-connectors/chunjun-connector-kingbase/src/main/java/com/dtstack/chunjun/connector/kingbase/dialect/KingbaseDialect.java
Patch:
@@ -20,7 +20,7 @@
 import com.dtstack.chunjun.connector.jdbc.dialect.JdbcDialect;
 import com.dtstack.chunjun.connector.kingbase.converter.KingbaseRawTypeMapper;
 import com.dtstack.chunjun.connector.kingbase.util.KingbaseConstants;
-import com.dtstack.chunjun.converter.RawTypeConverter;
+import com.dtstack.chunjun.converter.RawTypeMapper;
 
 import java.util.Arrays;
 import java.util.Optional;
@@ -44,7 +44,7 @@ public boolean canHandle(String url) {
     }
 
     @Override
-    public RawTypeConverter getRawTypeConverter() {
+    public RawTypeMapper getRawTypeConverter() {
         return KingbaseRawTypeMapper::apply;
     }
 

File: chunjun-connectors/chunjun-connector-kudu/src/main/java/com/dtstack/chunjun/connector/kudu/converter/KuduSqlConverter.java
Patch:
@@ -44,14 +44,14 @@
 import java.time.LocalDate;
 import java.util.List;
 
-public class KuduRowConverter
+public class KuduSqlConverter
         extends AbstractRowConverter<RowResult, RowResult, Operation, LogicalType> {
 
     private static final long serialVersionUID = -142881666054444897L;
 
     private final List<String> columnName;
 
-    public KuduRowConverter(RowType rowType, List<String> columnName) {
+    public KuduSqlConverter(RowType rowType, List<String> columnName) {
         super(rowType);
         this.columnName = columnName;
         for (int i = 0; i < rowType.getFieldCount(); i++) {

File: chunjun-connectors/chunjun-connector-kudu/src/main/java/com/dtstack/chunjun/connector/kudu/table/KuduDynamicTableSink.java
Patch:
@@ -19,7 +19,7 @@
 package com.dtstack.chunjun.connector.kudu.table;
 
 import com.dtstack.chunjun.connector.kudu.config.KuduSinkConfig;
-import com.dtstack.chunjun.connector.kudu.converter.KuduRowConverter;
+import com.dtstack.chunjun.connector.kudu.converter.KuduSqlConverter;
 import com.dtstack.chunjun.connector.kudu.sink.KuduOutputFormatBuilder;
 import com.dtstack.chunjun.sink.DtOutputFormatSinkFunction;
 
@@ -57,7 +57,7 @@ public SinkRuntimeProvider getSinkRuntimeProvider(Context context) {
 
         builder.setSinkConfig(sinkConfig);
         builder.setRowConverter(
-                new KuduRowConverter((RowType) logicalType, tableSchema.getColumnNames()));
+                new KuduSqlConverter((RowType) logicalType, tableSchema.getColumnNames()));
 
         return SinkFunctionProvider.of(
                 new DtOutputFormatSinkFunction<>(builder.finish()), sinkConfig.getParallelism());

File: chunjun-connectors/chunjun-connector-kudu/src/main/java/com/dtstack/chunjun/connector/kudu/util/KuduUtil.java
Patch:
@@ -163,7 +163,7 @@ private static void addPredicates(
         columns.forEach(
                 item -> {
                     String name = item.getName();
-                    String type = item.getType();
+                    String type = item.getType().getType();
                     nameTypeMap.put(name, getType(type));
                 });
 

File: chunjun-connectors/chunjun-connector-mongodb/src/main/java/com/dtstack/chunjun/connector/mongodb/converter/MongodbSqlConverter.java
Patch:
@@ -44,7 +44,7 @@
 import java.util.ArrayList;
 import java.util.List;
 
-public final class MongodbRowConverter
+public final class MongodbSqlConverter
         extends AbstractRowConverter<Document, Document, Document, LogicalType> {
 
     private static final long serialVersionUID = -2683857781343809969L;
@@ -53,7 +53,7 @@ public final class MongodbRowConverter
     private final List<MongoSerializationConverter> toExternalConverters;
     private final String[] fieldNames;
 
-    public MongodbRowConverter(RowType rowType, String[] fieldNames) {
+    public MongodbSqlConverter(RowType rowType, String[] fieldNames) {
         super(rowType);
         this.fieldNames = fieldNames;
         toInternalConverters = new ArrayList<>();

File: chunjun-connectors/chunjun-connector-mongodb/src/main/java/com/dtstack/chunjun/connector/mongodb/table/MongodbDynamicTableSink.java
Patch:
@@ -21,7 +21,7 @@
 import com.dtstack.chunjun.config.CommonConfig;
 import com.dtstack.chunjun.connector.mongodb.config.MongoClientConfig;
 import com.dtstack.chunjun.connector.mongodb.config.MongoWriteConfig;
-import com.dtstack.chunjun.connector.mongodb.converter.MongodbRowConverter;
+import com.dtstack.chunjun.connector.mongodb.converter.MongodbSqlConverter;
 import com.dtstack.chunjun.connector.mongodb.sink.MongodbOutputFormat;
 import com.dtstack.chunjun.connector.mongodb.sink.MongodbOutputFormatBuilder;
 import com.dtstack.chunjun.sink.DtOutputFormatSinkFunction;
@@ -69,7 +69,7 @@ public SinkRuntimeProvider getSinkRuntimeProvider(Context context) {
         commonConfig.setFlushIntervalMills(mongoWriteConfig.getFlushInterval());
         builder.setConfig(commonConfig);
 
-        builder.setRowConverter(new MongodbRowConverter(rowType, fieldNames));
+        builder.setRowConverter(new MongodbSqlConverter(rowType, fieldNames));
         return SinkFunctionProvider.of(
                 new DtOutputFormatSinkFunction<>(builder.finish()),
                 mongoWriteConfig.getParallelism());

File: chunjun-connectors/chunjun-connector-mongodb/src/main/java/com/dtstack/chunjun/connector/mongodb/table/lookup/MongoAllTableFunction.java
Patch:
@@ -20,7 +20,7 @@
 
 import com.dtstack.chunjun.connector.mongodb.MongoClientFactory;
 import com.dtstack.chunjun.connector.mongodb.config.MongoClientConfig;
-import com.dtstack.chunjun.connector.mongodb.converter.MongodbRowConverter;
+import com.dtstack.chunjun.connector.mongodb.converter.MongodbSqlConverter;
 import com.dtstack.chunjun.lookup.AbstractAllTableFunction;
 import com.dtstack.chunjun.lookup.config.LookupConfig;
 
@@ -49,7 +49,7 @@ public MongoAllTableFunction(
             RowType rowType,
             String[] keyNames,
             String[] fieldNames) {
-        super(fieldNames, keyNames, lookupConfig, new MongodbRowConverter(rowType, fieldNames));
+        super(fieldNames, keyNames, lookupConfig, new MongodbSqlConverter(rowType, fieldNames));
         this.mongoClientConfig = mongoClientConfig;
         this.fetchSize = lookupConfig.getFetchSize();
     }
@@ -69,7 +69,7 @@ protected void loadData(Object cacheRef) {
         for (Document doc : findIterable) {
             Map<String, Object> row = Maps.newHashMap();
             GenericRowData rowData =
-                    (GenericRowData) ((MongodbRowConverter) rowConverter).toInternal(doc);
+                    (GenericRowData) ((MongodbSqlConverter) rowConverter).toInternal(doc);
             for (int i = 0; i < fieldsName.length; i++) {
                 Object object = rowData.getField(i);
                 row.put(fieldsName[i].trim(), object);

File: chunjun-connectors/chunjun-connector-mongodb/src/main/java/com/dtstack/chunjun/connector/mongodb/table/lookup/MongoLruTableFunction.java
Patch:
@@ -19,7 +19,7 @@
 package com.dtstack.chunjun.connector.mongodb.table.lookup;
 
 import com.dtstack.chunjun.connector.mongodb.config.MongoClientConfig;
-import com.dtstack.chunjun.connector.mongodb.converter.MongodbRowConverter;
+import com.dtstack.chunjun.connector.mongodb.converter.MongodbSqlConverter;
 import com.dtstack.chunjun.lookup.AbstractLruTableFunction;
 import com.dtstack.chunjun.lookup.config.LookupConfig;
 
@@ -60,7 +60,7 @@ public MongoLruTableFunction(
             RowType rowType,
             String[] keyNames,
             String[] fieldNames) {
-        super(lookupConfig, new MongodbRowConverter(rowType, fieldNames));
+        super(lookupConfig, new MongodbSqlConverter(rowType, fieldNames));
         this.mongoClientConfig = mongoClientConfig;
         this.keyNames = keyNames;
     }
@@ -88,7 +88,7 @@ public void handleAsyncInvoke(CompletableFuture<Collection<RowData>> future, Obj
                 (document) -> {
                     RowData row = null;
                     try {
-                        row = ((MongodbRowConverter) rowConverter).toInternalLookup(document);
+                        row = ((MongodbSqlConverter) rowConverter).toInternalLookup(document);
                     } catch (Exception e) {
                         log.error("", e);
                     }

File: chunjun-connectors/chunjun-connector-nebula/src/main/java/com/dtstack/chunjun/connector/nebula/converter/NebulaSqlConverter.java
Patch:
@@ -45,12 +45,12 @@
 import java.time.LocalTime;
 import java.util.List;
 
-public class NebulaRowConverter
+public class NebulaSqlConverter
         extends AbstractRowConverter<BaseTableRow, BaseTableRow, NebulaRows, RowType.RowField> {
 
     private static final long serialVersionUID = -2353660561978026681L;
 
-    public NebulaRowConverter(RowType rowType) {
+    public NebulaSqlConverter(RowType rowType) {
         super(rowType);
         List<RowType.RowField> fields = rowType.getFields();
         for (RowType.RowField field : fields) {

File: chunjun-connectors/chunjun-connector-nebula/src/main/java/com/dtstack/chunjun/connector/nebula/sink/NebulaDynamicTableSink.java
Patch:
@@ -19,7 +19,7 @@
 package com.dtstack.chunjun.connector.nebula.sink;
 
 import com.dtstack.chunjun.connector.nebula.config.NebulaConfig;
-import com.dtstack.chunjun.connector.nebula.converter.NebulaRowConverter;
+import com.dtstack.chunjun.connector.nebula.converter.NebulaSqlConverter;
 import com.dtstack.chunjun.sink.DtOutputFormatSinkFunction;
 
 import org.apache.flink.table.catalog.ResolvedSchema;
@@ -61,7 +61,7 @@ public SinkRuntimeProvider getSinkRuntimeProvider(Context context) {
                 InternalTypeInfo.of(tableSchema.toPhysicalRowDataType().getLogicalType())
                         .toRowType();
         builder.setNebulaConfig(nebulaConfig);
-        builder.setRowConverter(new NebulaRowConverter(rowType));
+        builder.setRowConverter(new NebulaSqlConverter(rowType));
         return SinkFunctionProvider.of(
                 new DtOutputFormatSinkFunction<>(builder.finish()), nebulaConfig.getWriteTasks());
     }

File: chunjun-connectors/chunjun-connector-oceanbasecdc/src/main/java/com/dtstack/chunjun/connector/oceanbasecdc/inputformat/OceanBaseCdcInputFormatBuilder.java
Patch:
@@ -19,7 +19,7 @@
 package com.dtstack.chunjun.connector.oceanbasecdc.inputformat;
 
 import com.dtstack.chunjun.connector.oceanbasecdc.config.OceanBaseCdcConfig;
-import com.dtstack.chunjun.converter.AbstractCDCRowConverter;
+import com.dtstack.chunjun.converter.AbstractCDCRawTypeMapper;
 import com.dtstack.chunjun.source.format.BaseRichInputFormatBuilder;
 
 import com.oceanbase.clogproxy.client.config.ObReaderConfig;
@@ -37,11 +37,11 @@ public void setOceanBaseCdcConf(OceanBaseCdcConfig cdcConf) {
         this.format.setCdcConf(cdcConf);
     }
 
-    public void setRowConverter(AbstractCDCRowConverter rowConverter) {
+    public void setRowConverter(AbstractCDCRawTypeMapper rowConverter) {
         this.setRowConverter(rowConverter, false);
     }
 
-    public void setRowConverter(AbstractCDCRowConverter rowConverter, boolean useAbstractColumn) {
+    public void setRowConverter(AbstractCDCRawTypeMapper rowConverter, boolean useAbstractColumn) {
         this.format.setRowConverter(rowConverter);
         format.setUseAbstractColumn(useAbstractColumn);
     }

File: chunjun-connectors/chunjun-connector-oceanbasecdc/src/main/java/com/dtstack/chunjun/connector/oceanbasecdc/listener/OceanBaseCdcListener.java
Patch:
@@ -22,7 +22,7 @@
 import com.dtstack.chunjun.connector.oceanbasecdc.entity.OceanBaseCdcEventRow;
 import com.dtstack.chunjun.connector.oceanbasecdc.inputformat.OceanBaseCdcInputFormat;
 import com.dtstack.chunjun.constants.ConstantValue;
-import com.dtstack.chunjun.converter.AbstractCDCRowConverter;
+import com.dtstack.chunjun.converter.AbstractCDCRawTypeMapper;
 import com.dtstack.chunjun.throwable.ChunJunRuntimeException;
 
 import org.apache.flink.table.data.RowData;
@@ -47,7 +47,7 @@ public class OceanBaseCdcListener implements Runnable {
 
     private final OceanBaseCdcInputFormat format;
     private final OceanBaseCdcConfig cdcConf;
-    private final AbstractCDCRowConverter rowConverter;
+    private final AbstractCDCRawTypeMapper rowConverter;
     private final List<DataMessage.Record.Type> categories;
 
     private final List<LogMessage> logMessageBuffer = new LinkedList<>();

File: chunjun-connectors/chunjun-connector-oceanbasecdc/src/main/java/com/dtstack/chunjun/connector/oceanbasecdc/source/OceanBaseCdcDynamicTableSource.java
Patch:
@@ -19,7 +19,7 @@
 package com.dtstack.chunjun.connector.oceanbasecdc.source;
 
 import com.dtstack.chunjun.connector.oceanbasecdc.config.OceanBaseCdcConfig;
-import com.dtstack.chunjun.connector.oceanbasecdc.converter.OceanBaseCdcRowConverter;
+import com.dtstack.chunjun.connector.oceanbasecdc.converter.OceanBaseCdcSqlConverter;
 import com.dtstack.chunjun.connector.oceanbasecdc.format.TimestampFormat;
 import com.dtstack.chunjun.connector.oceanbasecdc.inputformat.OceanBaseCdcInputFormatBuilder;
 import com.dtstack.chunjun.source.DtInputFormatSourceFunction;
@@ -65,7 +65,7 @@ public ScanRuntimeProvider getScanRuntimeProvider(ScanContext runtimeProviderCon
         OceanBaseCdcInputFormatBuilder builder = new OceanBaseCdcInputFormatBuilder();
         builder.setOceanBaseCdcConf(config);
         builder.setRowConverter(
-                new OceanBaseCdcRowConverter(
+                new OceanBaseCdcSqlConverter(
                         InternalTypeInfo.of(schema.toPhysicalRowDataType().getLogicalType())
                                 .toRowType(),
                         this.timestampFormat));

File: chunjun-connectors/chunjun-connector-oracle/src/test/java/OracleRawTypeMapperTest.java
Patch:
@@ -23,7 +23,7 @@
 import java.util.regex.Matcher;
 import java.util.regex.Pattern;
 
-public class OracleRawTypeConverterTest {
+public class OracleRawTypeMapperTest {
 
     @Test
     public void testRegex() {

File: chunjun-connectors/chunjun-connector-oraclelogminer/src/main/java/com/dtstack/chunjun/connector/oraclelogminer/converter/LogMinerRawTypeMapper.java
Patch:
@@ -20,7 +20,7 @@
 import com.dtstack.chunjun.connector.oraclelogminer.entity.EventRow;
 import com.dtstack.chunjun.connector.oraclelogminer.entity.EventRowData;
 import com.dtstack.chunjun.connector.oraclelogminer.listener.LogParser;
-import com.dtstack.chunjun.converter.AbstractCDCRowConverter;
+import com.dtstack.chunjun.converter.AbstractCDCRawTypeMapper;
 import com.dtstack.chunjun.converter.IDeserializationConverter;
 import com.dtstack.chunjun.util.DateUtil;
 
@@ -47,11 +47,11 @@
 import java.util.List;
 import java.util.Map;
 
-public class LogMinerRowConverter extends AbstractCDCRowConverter<EventRow, LogicalType> {
+public class LogMinerRawTypeMapper extends AbstractCDCRawTypeMapper<EventRow, LogicalType> {
 
     private static final long serialVersionUID = -7611385227503313499L;
 
-    public LogMinerRowConverter(RowType rowType) {
+    public LogMinerRawTypeMapper(RowType rowType) {
         super.fieldNameList = rowType.getFieldNames();
         super.converters = new ArrayList<>();
         for (int i = 0; i < rowType.getFieldCount(); i++) {

File: chunjun-connectors/chunjun-connector-oraclelogminer/src/main/java/com/dtstack/chunjun/connector/oraclelogminer/entity/TableMetaData.java
Patch:
@@ -18,6 +18,8 @@
 
 package com.dtstack.chunjun.connector.oraclelogminer.entity;
 
+import com.dtstack.chunjun.config.TypeConfig;
+
 import lombok.AllArgsConstructor;
 import lombok.Getter;
 import lombok.ToString;
@@ -33,5 +35,5 @@ public class TableMetaData {
     private final String tableName;
     private final List<String> fieldList;
     /** field type * */
-    private final List<String> typeList;
+    private final List<TypeConfig> typeList;
 }

File: chunjun-connectors/chunjun-connector-oraclelogminer/src/main/java/com/dtstack/chunjun/connector/oraclelogminer/inputformat/OracleLogMinerInputFormat.java
Patch:
@@ -21,7 +21,7 @@
 import com.dtstack.chunjun.connector.oraclelogminer.config.LogMinerConfig;
 import com.dtstack.chunjun.connector.oraclelogminer.listener.LogMinerListener;
 import com.dtstack.chunjun.connector.oraclelogminer.listener.PositionManager;
-import com.dtstack.chunjun.converter.AbstractCDCRowConverter;
+import com.dtstack.chunjun.converter.AbstractCDCRawTypeMapper;
 import com.dtstack.chunjun.restore.FormatState;
 import com.dtstack.chunjun.source.format.BaseRichInputFormat;
 
@@ -42,7 +42,7 @@ public class OracleLogMinerInputFormat extends BaseRichInputFormat {
 
     private transient PositionManager positionManager;
 
-    private AbstractCDCRowConverter rowConverter;
+    private AbstractCDCRawTypeMapper rowConverter;
 
     @Override
     protected InputSplit[] createInputSplitsInternal(int i) {
@@ -105,7 +105,7 @@ protected void closeInternal() throws IOException {
         }
     }
 
-    public void setRowConverter(AbstractCDCRowConverter rowConverter) {
+    public void setRowConverter(AbstractCDCRawTypeMapper rowConverter) {
         this.rowConverter = rowConverter;
     }
 }

File: chunjun-connectors/chunjun-connector-oraclelogminer/src/main/java/com/dtstack/chunjun/connector/oraclelogminer/inputformat/OracleLogMinerInputFormatBuilder.java
Patch:
@@ -22,7 +22,7 @@
 import com.dtstack.chunjun.connector.oraclelogminer.listener.LogMinerConnection;
 import com.dtstack.chunjun.connector.oraclelogminer.util.SqlUtil;
 import com.dtstack.chunjun.constants.ConstantValue;
-import com.dtstack.chunjun.converter.AbstractCDCRowConverter;
+import com.dtstack.chunjun.converter.AbstractCDCRawTypeMapper;
 import com.dtstack.chunjun.source.format.BaseRichInputFormatBuilder;
 import com.dtstack.chunjun.util.ClassUtil;
 import com.dtstack.chunjun.util.ExceptionUtil;
@@ -60,7 +60,7 @@ public void setLogMinerConfig(LogMinerConfig logMinerConfig) {
         format.logMinerConfig = logMinerConfig;
     }
 
-    public void setRowConverter(AbstractCDCRowConverter rowConverter) {
+    public void setRowConverter(AbstractCDCRawTypeMapper rowConverter) {
         this.format.setRowConverter(rowConverter);
     }
 

File: chunjun-connectors/chunjun-connector-oraclelogminer/src/main/java/com/dtstack/chunjun/connector/oraclelogminer/listener/LogMinerListener.java
Patch:
@@ -27,7 +27,7 @@
 import com.dtstack.chunjun.connector.oraclelogminer.entity.QueueData;
 import com.dtstack.chunjun.connector.oraclelogminer.util.OraUtil;
 import com.dtstack.chunjun.connector.oraclelogminer.util.SqlUtil;
-import com.dtstack.chunjun.converter.AbstractCDCRowConverter;
+import com.dtstack.chunjun.converter.AbstractCDCRawTypeMapper;
 import com.dtstack.chunjun.element.ColumnRowData;
 import com.dtstack.chunjun.element.ErrorMsgRowData;
 import com.dtstack.chunjun.throwable.ChunJunRuntimeException;
@@ -78,7 +78,7 @@ public class LogMinerListener implements Runnable {
 
     private final LogMinerConfig logMinerConfig;
     private final PositionManager positionManager;
-    private final AbstractCDCRowConverter rowConverter;
+    private final AbstractCDCRawTypeMapper rowConverter;
     private final LogMinerHelper logMinerHelper;
     private BlockingQueue<QueueData> queue;
     private ExecutorService executor;
@@ -91,7 +91,7 @@ public class LogMinerListener implements Runnable {
     public LogMinerListener(
             LogMinerConfig logMinerConfig,
             PositionManager positionManager,
-            AbstractCDCRowConverter rowConverter) {
+            AbstractCDCRawTypeMapper rowConverter) {
         this.positionManager = positionManager;
         this.logMinerConfig = logMinerConfig;
         this.listener = this;

File: chunjun-connectors/chunjun-connector-oraclelogminer/src/main/java/com/dtstack/chunjun/connector/oraclelogminer/listener/LogParser.java
Patch:
@@ -21,7 +21,7 @@
 import com.dtstack.chunjun.connector.oraclelogminer.entity.EventRow;
 import com.dtstack.chunjun.connector.oraclelogminer.entity.EventRowData;
 import com.dtstack.chunjun.connector.oraclelogminer.entity.QueueData;
-import com.dtstack.chunjun.converter.AbstractCDCRowConverter;
+import com.dtstack.chunjun.converter.AbstractCDCRawTypeMapper;
 import com.dtstack.chunjun.element.ColumnRowData;
 import com.dtstack.chunjun.util.SnowflakeIdWorker;
 
@@ -254,7 +254,7 @@ public static String parseString(String value) {
         return value;
     }
 
-    public LinkedList<RowData> parse(QueueData pair, AbstractCDCRowConverter rowConverter)
+    public LinkedList<RowData> parse(QueueData pair, AbstractCDCRawTypeMapper rowConverter)
             throws Exception {
         ColumnRowData logData = (ColumnRowData) pair.getData();
 

File: chunjun-connectors/chunjun-connector-oraclelogminer/src/main/java/com/dtstack/chunjun/connector/oraclelogminer/source/OraclelogminerDynamicTableSource.java
Patch:
@@ -19,7 +19,7 @@
 package com.dtstack.chunjun.connector.oraclelogminer.source;
 
 import com.dtstack.chunjun.connector.oraclelogminer.config.LogMinerConfig;
-import com.dtstack.chunjun.connector.oraclelogminer.converter.LogMinerRowConverter;
+import com.dtstack.chunjun.connector.oraclelogminer.converter.LogMinerRawTypeMapper;
 import com.dtstack.chunjun.connector.oraclelogminer.format.TimestampFormat;
 import com.dtstack.chunjun.connector.oraclelogminer.inputformat.OracleLogMinerInputFormatBuilder;
 import com.dtstack.chunjun.source.DtInputFormatSourceFunction;
@@ -54,7 +54,7 @@ public ScanRuntimeProvider getScanRuntimeProvider(ScanContext runtimeProviderCon
         OracleLogMinerInputFormatBuilder builder = new OracleLogMinerInputFormatBuilder();
         builder.setLogMinerConfig(logMinerConfig);
         builder.setRowConverter(
-                new LogMinerRowConverter(
+                new LogMinerRawTypeMapper(
                         InternalTypeInfo.of(schema.toPhysicalRowDataType().getLogicalType())
                                 .toRowType()));
 

File: chunjun-connectors/chunjun-connector-postgresql/src/main/java/com/dtstack/chunjun/connector/postgresql/sink/PostgresOutputFormat.java
Patch:
@@ -93,7 +93,6 @@ protected void openInternal(int taskNumber, int numTasks) {
                 if (jdbcDialect.dialectName().equals("PostgresSQL")) {
                     ((PostgresqlSyncConverter) rowConverter).setConnection((BaseConnection) dbConn);
                 }
-                ((PostgresqlSyncConverter) rowConverter).setFieldTypeList(columnTypeList);
             }
         } catch (SQLException sqe) {
             throw new IllegalArgumentException("checkUpsert() failed.", sqe);

File: chunjun-connectors/chunjun-connector-redis/src/main/java/com/dtstack/chunjun/connector/redis/converter/RedisSqlConverter.java
Patch:
@@ -50,7 +50,7 @@
 import java.util.Map;
 import java.util.stream.Collectors;
 
-public class RedisRowConverter
+public class RedisSqlConverter
         extends AbstractRowConverter<
                 Map<String, String>, Map<String, String>, JedisCommands, LogicalType> {
 
@@ -60,7 +60,7 @@ public class RedisRowConverter
 
     private final List<Triplet<String, Integer, LogicalType>> typeIndexList = new ArrayList<>();
 
-    public RedisRowConverter(RowType rowType) {
+    public RedisSqlConverter(RowType rowType) {
         super(rowType);
         List<String> fieldNames = rowType.getFieldNames();
         for (int i = 0; i < rowType.getFieldCount(); i++) {
@@ -71,7 +71,7 @@ public RedisRowConverter(RowType rowType) {
         }
     }
 
-    public RedisRowConverter(RowType rowType, RedisConfig redisConfig) {
+    public RedisSqlConverter(RowType rowType, RedisConfig redisConfig) {
         super(rowType);
         for (int i = 0; i < rowType.getFieldCount(); i++) {
             toExternalConverters.add(

File: chunjun-connectors/chunjun-connector-redis/src/main/java/com/dtstack/chunjun/connector/redis/converter/RedisSyncConverter.java
Patch:
@@ -46,7 +46,7 @@
 import static com.dtstack.chunjun.connector.redis.options.RedisOptions.REDIS_CRITICAL_TIME;
 import static com.dtstack.chunjun.connector.redis.options.RedisOptions.REDIS_KEY_VALUE_SIZE;
 
-public class RedisColumnConverter extends AbstractRowConverter<Object, Object, Jedis, LogicalType> {
+public class RedisSyncConverter extends AbstractRowConverter<Object, Object, Jedis, LogicalType> {
 
     private static final long serialVersionUID = 3573774552923872927L;
 
@@ -58,7 +58,7 @@ public class RedisColumnConverter extends AbstractRowConverter<Object, Object, J
     /** The index of column can be used as a field when mode is hash and Column is not empty */
     private List<Integer> fieldIndex = new ArrayList<>(2);
 
-    public RedisColumnConverter(RedisConfig redisConfig) {
+    public RedisSyncConverter(RedisConfig redisConfig) {
         this.redisConfig = redisConfig;
         if (StringUtils.isNotBlank(redisConfig.getDateFormat())) {
             sdf = new SimpleDateFormat(redisConfig.getDateFormat());

File: chunjun-connectors/chunjun-connector-redis/src/main/java/com/dtstack/chunjun/connector/redis/sink/RedisDynamicTableSink.java
Patch:
@@ -19,7 +19,7 @@
 package com.dtstack.chunjun.connector.redis.sink;
 
 import com.dtstack.chunjun.connector.redis.config.RedisConfig;
-import com.dtstack.chunjun.connector.redis.converter.RedisRowConverter;
+import com.dtstack.chunjun.connector.redis.converter.RedisSqlConverter;
 import com.dtstack.chunjun.sink.DtOutputFormatSinkFunction;
 
 import org.apache.flink.table.catalog.ResolvedSchema;
@@ -59,7 +59,7 @@ public SinkRuntimeProvider getSinkRuntimeProvider(Context context) {
         redisConfig.setKeyIndexes(Lists.newArrayList(1));
         builder.setRedisConf(redisConfig);
         builder.setRowConverter(
-                new RedisRowConverter(InternalTypeInfo.of(logicalType).toRowType(), redisConfig));
+                new RedisSqlConverter(InternalTypeInfo.of(logicalType).toRowType(), redisConfig));
 
         return SinkFunctionProvider.of(
                 new DtOutputFormatSinkFunction<>(builder.finish()), redisConfig.getParallelism());

File: chunjun-connectors/chunjun-connector-rocketmq/src/main/java/com/dtstack/chunjun/connector/rocketmq/converter/RocketMQSqlConverter.java
Patch:
@@ -41,15 +41,15 @@
 import java.util.Map;
 import java.util.Objects;
 
-public class RocketMQRowConverter
+public class RocketMQSqlConverter
         extends AbstractRowConverter<byte[], byte[], Message, LogicalType> {
 
     private static final long serialVersionUID = 7501509569174874833L;
 
     private final String encoding;
     private final String[] filedNames;
 
-    public RocketMQRowConverter(RowType rowType, String encoding, String[] filedNames) {
+    public RocketMQSqlConverter(RowType rowType, String encoding, String[] filedNames) {
         super(rowType);
         for (int i = 0; i < rowType.getFieldCount(); i++) {
             toInternalConverters.add(

File: chunjun-connectors/chunjun-connector-rocketmq/src/main/java/com/dtstack/chunjun/connector/rocketmq/source/RocketMQScanTableSource.java
Patch:
@@ -18,7 +18,7 @@
 package com.dtstack.chunjun.connector.rocketmq.source;
 
 import com.dtstack.chunjun.connector.rocketmq.config.RocketMQConfig;
-import com.dtstack.chunjun.connector.rocketmq.converter.RocketMQRowConverter;
+import com.dtstack.chunjun.connector.rocketmq.converter.RocketMQSqlConverter;
 import com.dtstack.chunjun.connector.rocketmq.source.deserialization.KeyValueDeserializationSchema;
 import com.dtstack.chunjun.connector.rocketmq.source.deserialization.RowKeyValueDeserializationSchema;
 import com.dtstack.chunjun.converter.AbstractRowConverter;
@@ -66,7 +66,7 @@ public ScanRuntimeProvider getScanRuntimeProvider(ScanContext scanContext) {
 
         String[] fieldNames = schema.getColumnNames().toArray(new String[0]);
         converter =
-                new RocketMQRowConverter(
+                new RocketMQSqlConverter(
                         InternalTypeInfo.of(logicalType).toRowType(),
                         rocketMQConfig.getEncoding(),
                         fieldNames);

File: chunjun-connectors/chunjun-connector-s3/src/main/java/com/dtstack/chunjun/connector/s3/converter/S3SqlConverter.java
Patch:
@@ -35,11 +35,11 @@
 import java.time.LocalDate;
 import java.time.LocalTime;
 
-public class S3RowConverter extends AbstractRowConverter<String[], RowData, String[], LogicalType> {
+public class S3SqlConverter extends AbstractRowConverter<String[], RowData, String[], LogicalType> {
 
     private static final long serialVersionUID = 4835129977890244317L;
 
-    public S3RowConverter(RowType rowType, CommonConfig conf) {
+    public S3SqlConverter(RowType rowType, CommonConfig conf) {
         super(rowType, conf);
         for (int i = 0; i < rowType.getFieldCount(); i++) {
             toInternalConverters.add(

File: chunjun-connectors/chunjun-connector-s3/src/main/java/com/dtstack/chunjun/connector/s3/converter/S3SyncConverter.java
Patch:
@@ -42,12 +42,12 @@
 import java.sql.Time;
 import java.util.List;
 
-public class S3ColumnConverter
+public class S3SyncConverter
         extends AbstractRowConverter<String[], RowData, String[], LogicalType> {
 
     private static final long serialVersionUID = -3778159110420581423L;
 
-    public S3ColumnConverter(RowType rowType, CommonConfig config) {
+    public S3SyncConverter(RowType rowType, CommonConfig config) {
         super(rowType, config);
         super.commonConfig = config;
         for (LogicalType type : fieldTypes) {

File: chunjun-connectors/chunjun-connector-socket/src/main/java/com/dtstack/chunjun/connector/socket/source/SocketSourceFactory.java
Patch:
@@ -21,7 +21,7 @@
 import com.dtstack.chunjun.config.SyncConfig;
 import com.dtstack.chunjun.connector.socket.entity.SocketConfig;
 import com.dtstack.chunjun.connector.socket.inputformat.SocketInputFormatBuilder;
-import com.dtstack.chunjun.converter.RawTypeConverter;
+import com.dtstack.chunjun.converter.RawTypeMapper;
 import com.dtstack.chunjun.source.SourceFactory;
 import com.dtstack.chunjun.util.JsonUtil;
 
@@ -49,7 +49,7 @@ public DataStream<RowData> createSource() {
     }
 
     @Override
-    public RawTypeConverter getRawTypeConverter() {
+    public RawTypeMapper getRawTypeMapper() {
         return null;
     }
 }

File: chunjun-connectors/chunjun-connector-solr/src/main/java/com/dtstack/chunjun/connector/solr/converter/SolrSqlConverter.java
Patch:
@@ -46,14 +46,14 @@
 
 import static org.apache.flink.util.Preconditions.checkNotNull;
 
-public class SolrRowConverter
+public class SolrSqlConverter
         extends AbstractRowConverter<SolrDocument, SolrDocument, SolrInputDocument, LogicalType> {
 
     private static final long serialVersionUID = -1488018940395139854L;
     protected List<SolrSerializationConverter> toExternalConverters;
     protected String[] fieldNames;
 
-    public SolrRowConverter(RowType rowType, String[] fieldNames) {
+    public SolrSqlConverter(RowType rowType, String[] fieldNames) {
         this.rowType = checkNotNull(rowType);
         this.fieldNames = fieldNames;
         this.fieldTypes =

File: chunjun-connectors/chunjun-connector-solr/src/main/java/com/dtstack/chunjun/connector/solr/table/SolrDynamicTableSink.java
Patch:
@@ -19,7 +19,7 @@
 package com.dtstack.chunjun.connector.solr.table;
 
 import com.dtstack.chunjun.connector.solr.SolrConfig;
-import com.dtstack.chunjun.connector.solr.converter.SolrRowConverter;
+import com.dtstack.chunjun.connector.solr.converter.SolrSqlConverter;
 import com.dtstack.chunjun.connector.solr.sink.SolrOutputFormatBuilder;
 import com.dtstack.chunjun.sink.DtOutputFormatSinkFunction;
 
@@ -55,7 +55,7 @@ public SinkRuntimeProvider getSinkRuntimeProvider(Context context) {
         String[] fieldNames = resolvedSchema.getColumnNames().toArray(new String[0]);
 
         SolrOutputFormatBuilder builder = new SolrOutputFormatBuilder(solrConfig);
-        builder.setRowConverter(new SolrRowConverter(rowType, fieldNames));
+        builder.setRowConverter(new SolrSqlConverter(rowType, fieldNames));
 
         return SinkFunctionProvider.of(
                 new DtOutputFormatSinkFunction<>(builder.finish()), solrConfig.getParallelism());

File: chunjun-connectors/chunjun-connector-sqlservercdc/src/main/java/com/dtstack/chunjun/connector/sqlservercdc/inputFormat/SqlServerCdcInputFormatBuilder.java
Patch:
@@ -22,7 +22,7 @@
 import com.dtstack.chunjun.connector.sqlservercdc.entity.SqlServerCdcEnum;
 import com.dtstack.chunjun.connector.sqlservercdc.util.SqlServerCdcUtil;
 import com.dtstack.chunjun.constants.ConstantValue;
-import com.dtstack.chunjun.converter.AbstractCDCRowConverter;
+import com.dtstack.chunjun.converter.AbstractCDCRawTypeMapper;
 import com.dtstack.chunjun.source.format.BaseRichInputFormatBuilder;
 import com.dtstack.chunjun.util.ClassUtil;
 import com.dtstack.chunjun.util.ExceptionUtil;
@@ -59,12 +59,12 @@ public void setSqlServerCdcConf(SqlServerCdcConfig sqlServerCdcConfig) {
         this.format.setSqlServerCdcConf(sqlServerCdcConfig);
     }
 
-    public void setRowConverter(AbstractCDCRowConverter rowConverter) {
+    public void setRowConverter(AbstractCDCRawTypeMapper rowConverter) {
         setRowConverter(rowConverter, false);
     }
 
     public void setRowConverter(
-            AbstractCDCRowConverter rowConverter, boolean useAbstractBaseColumn) {
+            AbstractCDCRawTypeMapper rowConverter, boolean useAbstractBaseColumn) {
         this.format.setRowConverter(rowConverter);
         this.format.setUseAbstractColumn(useAbstractBaseColumn);
     }

File: chunjun-connectors/chunjun-connector-sqlservercdc/src/main/java/com/dtstack/chunjun/connector/sqlservercdc/listener/SqlServerCdcListener.java
Patch:
@@ -27,7 +27,7 @@
 import com.dtstack.chunjun.connector.sqlservercdc.inputFormat.SqlServerCdcInputFormat;
 import com.dtstack.chunjun.connector.sqlservercdc.util.SqlServerCdcUtil;
 import com.dtstack.chunjun.constants.ConstantValue;
-import com.dtstack.chunjun.converter.AbstractCDCRowConverter;
+import com.dtstack.chunjun.converter.AbstractCDCRawTypeMapper;
 import com.dtstack.chunjun.throwable.ChunJunRuntimeException;
 import com.dtstack.chunjun.throwable.WriteRecordException;
 import com.dtstack.chunjun.util.Clock;
@@ -58,7 +58,7 @@ public class SqlServerCdcListener implements Runnable {
     private final Set<Integer> cat;
     private final Duration pollInterval;
     private final SnowflakeIdWorker idWorker;
-    private final AbstractCDCRowConverter rowConverter;
+    private final AbstractCDCRawTypeMapper rowConverter;
 
     public SqlServerCdcListener(SqlServerCdcInputFormat format) throws SQLException {
         this.format = format;

File: chunjun-connectors/chunjun-connector-sqlservercdc/src/main/java/com/dtstack/chunjun/connector/sqlservercdc/source/SqlServerCdcDynamicTableSource.java
Patch:
@@ -19,7 +19,7 @@
 package com.dtstack.chunjun.connector.sqlservercdc.source;
 
 import com.dtstack.chunjun.connector.sqlservercdc.config.SqlServerCdcConfig;
-import com.dtstack.chunjun.connector.sqlservercdc.convert.SqlServerCdcRowConverter;
+import com.dtstack.chunjun.connector.sqlservercdc.convert.SqlServerCdcSqlConverter;
 import com.dtstack.chunjun.connector.sqlservercdc.format.TimestampFormat;
 import com.dtstack.chunjun.connector.sqlservercdc.inputFormat.SqlServerCdcInputFormatBuilder;
 import com.dtstack.chunjun.source.DtInputFormatSourceFunction;
@@ -57,7 +57,7 @@ public ScanRuntimeProvider getScanRuntimeProvider(ScanContext runtimeProviderCon
         SqlServerCdcInputFormatBuilder builder = new SqlServerCdcInputFormatBuilder();
         builder.setSqlServerCdcConf(sqlserverCdcConfig);
         builder.setRowConverter(
-                new SqlServerCdcRowConverter(
+                new SqlServerCdcSqlConverter(
                         InternalTypeInfo.of(logicalType).toRowType(), this.timestampFormat));
 
         return ParallelSourceFunctionProvider.of(

File: chunjun-connectors/chunjun-connector-starrocks/src/main/java/com/dtstack/chunjun/connector/starrocks/converter/StarRocksSqlConverter.java
Patch:
@@ -44,15 +44,15 @@
 
 import static com.dtstack.chunjun.connector.starrocks.util.StarRocksUtil.addStrForNum;
 
-public class StarRocksRowConverter
+public class StarRocksSqlConverter
         extends AbstractRowConverter<Object[], Object[], Map<String, Object>, LogicalType> {
 
     private static final long serialVersionUID = -176225284276566894L;
 
     private final List<String> columnList;
     public static final String DATETIME_FORMAT_SHORT = "yyyy-MM-dd HH:mm:ss";
 
-    public StarRocksRowConverter(RowType rowType, List<String> columnList) {
+    public StarRocksSqlConverter(RowType rowType, List<String> columnList) {
         super(rowType);
         this.columnList = columnList;
         for (int i = 0; i < rowType.getFieldCount(); i++) {

File: chunjun-connectors/chunjun-connector-starrocks/src/main/java/com/dtstack/chunjun/connector/starrocks/source/be/StarRocksArrowReader.java
Patch:
@@ -18,7 +18,8 @@
 
 package com.dtstack.chunjun.connector.starrocks.source.be;
 
-import com.dtstack.chunjun.connector.starrocks.converter.StarRocksRawTypeConverter;
+import com.dtstack.chunjun.config.TypeConfig;
+import com.dtstack.chunjun.connector.starrocks.converter.StarRocksRawTypeMapper;
 import com.dtstack.chunjun.connector.starrocks.source.be.entity.ColumnInfo;
 
 import com.starrocks.thrift.TScanBatchResult;
@@ -147,7 +148,7 @@ public StarRocksToJavaTrans getStarRocksToJavaTrans(ColumnInfo columnInfo) {
                 "Type corresponding error,Column[%s]'s StarRocksType should be %s;LogicalTypeRoot except %s but is %s",
                 columnInfo.getFieldName(),
                 columnInfo.getStarRocksType(),
-                StarRocksRawTypeConverter.apply(columnInfo.getStarRocksType())
+                StarRocksRawTypeMapper.apply(TypeConfig.fromString(columnInfo.getStarRocksType()))
                         .getLogicalType()
                         .getTypeRoot(),
                 columnInfo.getLogicalTypeRoot());

File: chunjun-connectors/chunjun-connector-stream/src/main/java/com/dtstack/chunjun/connector/stream/converter/StreamSqlConverter.java
Patch:
@@ -56,14 +56,14 @@
 
 import static java.time.temporal.ChronoField.MILLI_OF_DAY;
 
-public class StreamRowConverter
+public class StreamSqlConverter
         extends AbstractRowConverter<RowData, RowData, RowData, LogicalType> {
 
     private static final long serialVersionUID = 6652637680662065910L;
 
     private final Random random = new Random();
 
-    public StreamRowConverter(RowType rowType) {
+    public StreamSqlConverter(RowType rowType) {
         super(rowType);
         List<RowType.RowField> fields = rowType.getFields();
         for (RowType.RowField field : fields) {

File: chunjun-connectors/chunjun-connector-starrocks/src/main/java/com/dtstack/chunjun/connector/starrocks/source/be/StarRocksQueryPlanVisitor.java
Patch:
@@ -86,7 +86,7 @@ private static void allocateTabletsEqually(
         int tabletCount = Integer.MAX_VALUE;
         String currentBeNode = "";
         // Allocate tablets equally for all BeNodes
-        for (String beNode : tablet.getRouting()) {
+        for (String beNode : tablet.getRoutings()) {
             if (!beXTablets.containsKey(beNode)) {
                 beXTablets.put(beNode, new HashSet<>());
                 currentBeNode = beNode;

File: chunjun-connectors/chunjun-connector-starrocks/src/main/java/com/dtstack/chunjun/connector/starrocks/source/be/entity/Tablet.java
Patch:
@@ -28,7 +28,7 @@ public class Tablet implements Serializable {
 
     private static final long serialVersionUID = 8313899328024578523L;
 
-    private List<String> routing;
+    private List<String> routings;
     private int version;
     private long versionHash;
     private long schemaHash;

File: chunjun-connectors/chunjun-connector-kingbase/src/main/java/com/dtstack/chunjun/connector/kingbase/dialect/KingbaseDialect.java
Patch:
@@ -75,7 +75,6 @@ public Optional<String> getUpsertStatement(
      *
      * @param fieldNames
      * @param allReplace
-     *
      * @return
      */
     private String buildUpdateClause(String[] fieldNames, boolean allReplace) {
@@ -107,7 +106,6 @@ private String buildUpdateClause(String[] fieldNames, boolean allReplace) {
      * @param schema
      * @param tableName
      * @param fieldNames
-     *
      * @return
      */
     @Override

File: chunjun-clients/src/main/java/com/dtstack/chunjun/client/Launcher.java
Patch:
@@ -85,7 +85,7 @@ public static void main(String[] args) throws Exception {
         List<URL> jarUrlList = ExecuteProcessHelper.getExternalJarUrls(launcherOptions.getAddjar());
         ClassLoaderManager.loadExtraJar(jarUrlList, urlClassLoader);
         try (ClusterClient<?> client = clusterClientHelper.submit(jobDeployer)) {
-            if(null != client){
+            if (null != client) {
                 log.info(client.getClusterId() + " submit successfully.");
             }
         }

File: chunjun-clients/src/main/java/com/dtstack/chunjun/client/Launcher.java
Patch:
@@ -85,7 +85,9 @@ public static void main(String[] args) throws Exception {
         List<URL> jarUrlList = ExecuteProcessHelper.getExternalJarUrls(launcherOptions.getAddjar());
         ClassLoaderManager.loadExtraJar(jarUrlList, urlClassLoader);
         try (ClusterClient<?> client = clusterClientHelper.submit(jobDeployer)) {
-            log.info(client.getClusterId() + " submit successfully.");
+            if(null != client){
+                log.info(client.getClusterId() + " submit successfully.");
+            }
         }
     }
 

File: chunjun-core/src/main/java/com/dtstack/chunjun/environment/EnvFactory.java
Patch:
@@ -43,6 +43,9 @@ public class EnvFactory {
     public static StreamExecutionEnvironment createStreamExecutionEnvironment(Options options) {
         Configuration flinkConf = new Configuration();
         Configuration cfg = Configuration.fromMap(PropertiesUtil.confToMap(options.getConfProp()));
+        if (options.getSqlSetConfiguration() != null) {
+            cfg.addAll(options.getSqlSetConfiguration());
+        }
         if (StringUtils.isNotEmpty(options.getFlinkConfDir())) {
             flinkConf = GlobalConfiguration.loadConfiguration(options.getFlinkConfDir());
         }

File: chunjun-local-test/src/main/java/com/dtstack/chunjun/local/test/LocalTest.java
Patch:
@@ -44,7 +44,7 @@ public static void main(String[] args) throws Exception {
         String userDir = System.getProperty("user.dir");
 
         String jobPath =
-                "/Users/wtz/job_place/chunjun/1.16/_01_SQL/_06_Kafka/_01_sourceKafka_dimMysql_sinkMysql.sql";
+                "/Users/wtz/job_place/chunjun/1.16/_02_SYNC/_05_Oceanbase/oceanbase_stream.json";
         String chunjunDistDir = userDir + "/chunjun-dist";
         String s = "";
 

File: chunjun-core/src/main/java/com/dtstack/chunjun/util/PrintUtil.java
Patch:
@@ -28,7 +28,7 @@
 @Slf4j
 public class PrintUtil {
 
-    public static void printResult(Map<String, Object> result) {
+    public static String printResult(Map<String, Object> result) {
         List<String> names = Lists.newArrayList();
         List<String> values = Lists.newArrayList();
         result.forEach(
@@ -58,5 +58,6 @@ public static void printResult(Map<String, Object> result) {
         }
         builder.append("\n*********************************************\n");
         log.info(builder.toString());
+        return builder.toString();
     }
 }

File: chunjun-connectors/chunjun-connector-clickhouse/src/main/java/com/dtstack/chunjun/connector/clickhouse/sink/ClickhouseOutputFormat.java
Patch:
@@ -30,6 +30,7 @@ public class ClickhouseOutputFormat extends JdbcOutputFormat {
 
     @Override
     protected Connection getConnection() throws SQLException {
+        jdbcConfig.setAutoCommit(true);
         return ClickhouseUtil.getConnection(
                 jdbcConfig.getJdbcUrl(), jdbcConfig.getUsername(), jdbcConfig.getPassword());
     }

File: chunjun-connectors/chunjun-connector-doris/src/main/java/com/dtstack/chunjun/connector/doris/rest/DorisStreamLoad.java
Patch:
@@ -182,6 +182,7 @@ public void load(Carrier carrier) throws IOException {
         } else {
             RespContent respContent = OM.readValue(loadResponse.respContent, RespContent.class);
             if (!DORIS_SUCCESS_STATUS.contains(respContent.getStatus())) {
+                log.error("stream load error url: " + respContent.getErrorURL());
                 throw new IOException("stream load error: " + getDetailErrorLog(respContent));
             }
         }

File: chunjun-connectors/chunjun-connector-jdbc-base/src/test/java/com/dtstack/chunjun/connector/jdbc/source/JdbcInputFormatTest.java
Patch:
@@ -353,7 +353,8 @@ public void buildQuerySqlTest() {
                 .thenCallRealMethod();
         when(SqlUtil.buildQuerySqlBySplit(any(), any(), anyList(), anyList(), any()))
                 .thenAnswer(invocation -> "select id from table where id > 10");
-        when(SqlUtil.buildOrderSql(jdbcConfig, jdbcDialect, "ASC"))
+        when(SqlUtil.buildOrderSql(
+                        jdbcInputFormat.buildQuerySql(inputSplit), jdbcConfig, jdbcDialect, "ASC"))
                 .thenAnswer(invocation -> " order by id ASC");
         String except = "select id from table where id > 10 order by id ASC";
         Assert.assertEquals(except, jdbcInputFormat.buildQuerySql(inputSplit));

File: chunjun-connectors/chunjun-connector-postgresql/src/main/java/com/dtstack/chunjun/connector/postgresql/source/PostgresqlInputFormat.java
Patch:
@@ -39,7 +39,7 @@ protected void queryPollingWithOutStartLocation() throws SQLException {
         // , the query will report an error after the method
         // #setFetchDirection(ResultSet.FETCH_REVERSE) is called.
         String querySql =
-                jdbcConfig.getQuerySql() + SqlUtil.buildOrderSql(jdbcConfig, jdbcDialect, "ASC");
+                SqlUtil.buildOrderSql(jdbcConfig.getQuerySql(), jdbcConfig, jdbcDialect, "ASC");
         ps =
                 dbConn.prepareStatement(
                         querySql, ResultSet.TYPE_SCROLL_INSENSITIVE, resultSetConcurrency);

File: chunjun-connectors/chunjun-connector-hdfs/src/main/java/com/dtstack/chunjun/connector/hdfs/source/HdfsPathFilter.java
Patch:
@@ -37,6 +37,8 @@ public class HdfsPathFilter implements PathFilter, JobConfigurable {
     private static Pattern PATTERN;
     private String regex;
 
+    public HdfsPathFilter() {}
+
     public HdfsPathFilter(String regex) {
         this.regex = regex;
         compileRegex();

File: chunjun-connectors/chunjun-connector-sybase/src/main/java/com/dtstack/chunjun/connector/sybase/converter/SybaseRawTypeConverter.java
Patch:
@@ -76,6 +76,7 @@ public static DataType apply(String type) {
             case "UNITEXT":
             case "LONGSYSNAME":
             case "STRING":
+            case "SYSNAME":
                 return DataTypes.STRING();
             case "BINARY":
             case "TIMESTAMP":

File: chunjun-connectors/chunjun-connector-sybase/src/main/java/com/dtstack/chunjun/connector/sybase/converter/SybaseRawTypeConverter.java
Patch:
@@ -50,6 +50,7 @@ public static DataType apply(String type) {
             case "REAL":
                 return DataTypes.FLOAT();
             case "DOUBLE":
+            case "DOUBLE PRECISION":
                 return DataTypes.DOUBLE();
             case "SMALLMONEY":
                 return DataTypes.DECIMAL(10, 4);

File: chunjun-connectors/chunjun-connector-ftp/src/main/java/com/dtstack/chunjun/connector/ftp/sink/FtpOutputFormat.java
Patch:
@@ -176,6 +176,7 @@ public void closeInternal() throws IOException {
     protected void closeSource() {
         try {
             if (writer != null) {
+                writer.flush();
                 writer.close();
                 writer = null;
             }

File: chunjun-local-test/src/main/java/com/dtstack/chunjun/local/test/LocalTest.java
Patch:
@@ -43,7 +43,8 @@ public static void main(String[] args) throws Exception {
         //        confProperties.setProperty("state.checkpoints.dir", "file:///ck");
         String userDir = System.getProperty("user.dir");
 
-        String jobPath = "/Users/wtz/job_place/chunjun/1.16/_01_SQL/_06_Kafka/_01_sourceKafka_dimMysql_sinkMysql.sql";
+        String jobPath =
+                "/Users/wtz/job_place/chunjun/1.16/_01_SQL/_06_Kafka/_01_sourceKafka_dimMysql_sinkMysql.sql";
         String chunjunDistDir = userDir + "/chunjun-dist";
         String s = "";
 

File: chunjun-connectors/chunjun-connector-jdbc-base/src/main/java/com/dtstack/chunjun/connector/jdbc/util/SqlUtil.java
Patch:
@@ -41,7 +41,7 @@ public static String buildQuerySplitRangeSql(JdbcConfig jdbcConfig, JdbcDialect
         if (StringUtils.isNotEmpty(jdbcConfig.getCustomSql())) {
             querySplitRangeSql =
                     String.format(
-                            "SELECT min(%s.%s) as min_value,max(%s.%s) as max_value, FROM ( %s ) %s %s",
+                            "SELECT min(%s.%s) as min_value,max(%s.%s) as max_value FROM ( %s ) %s %s",
                             JdbcUtil.TEMPORARY_TABLE_NAME,
                             jdbcDialect.quoteIdentifier(jdbcConfig.getSplitPk()),
                             JdbcUtil.TEMPORARY_TABLE_NAME,

File: chunjun-connectors/chunjun-connector-jdbc-base/src/main/java/com/dtstack/chunjun/connector/jdbc/source/JdbcInputFormat.java
Patch:
@@ -270,7 +270,8 @@ protected void initMetric(InputSplit inputSplit) {
         if (StringUtils.isNotBlank(startLocation)) {
             startLocationAccumulator.add(new BigInteger(startLocation));
             // 防止数据库无增量数据时下次从prometheus获取到的startLocation为空
-            if (endLocationAccumulator.getLocalValue().longValue() == Long.MIN_VALUE) {
+            if (endLocationAccumulator.getLocalValue().intValue()
+                    == BigIntegerAccumulator.MIN_VAL) {
                 endLocationAccumulator.add(new BigInteger(startLocation));
             }
         }

File: chunjun-core/src/main/java/com/dtstack/chunjun/metrics/BigIntegerAccumulator.java
Patch:
@@ -29,7 +29,9 @@ public class BigIntegerAccumulator implements SimpleAccumulator<BigInteger> {
 
     private static final long serialVersionUID = 1L;
 
-    private BigInteger max = BigInteger.valueOf(Integer.MIN_VALUE);
+    public static Integer MIN_VAL = Integer.MIN_VALUE;
+
+    private BigInteger max = BigInteger.valueOf(MIN_VAL);
 
     @Override
     public void add(BigInteger value) {

File: chunjun-connectors/chunjun-connector-http/src/main/java/com/dtstack/chunjun/connector/http/converter/HttpColumnConverter.java
Patch:
@@ -52,7 +52,7 @@ public class HttpColumnConverter
 
     public HttpColumnConverter(HttpRestConfig httpRestConfig) {
         this.httpRestConfig = httpRestConfig;
-        this.commonConf = httpRestConfig;
+        this.commonConfig = httpRestConfig;
 
         // Only json need to extract the fields
         if (StringUtils.isNotBlank((httpRestConfig.getFields()))) {

File: chunjun-connectors/chunjun-connector-http/src/main/java/com/dtstack/chunjun/connector/http/converter/HttpRowConverter.java
Patch:
@@ -74,9 +74,7 @@ public RowData toInternal(Map<String, Object> result) throws Exception {
         GenericRowData genericRowData = new GenericRowData(rowType.getFieldCount());
         List<String> columns = rowType.getFieldNames();
         for (int pos = 0; pos < columns.size(); pos++) {
-            Object value =
-                    MapUtil.getValueByKey(
-                            result, columns.get(pos), httpRestConfig.getFieldDelimiter());
+            Object value = MapUtil.getValueByKey(result, columns.get(pos), "");
             if (value instanceof LinkedTreeMap) {
                 value = value.toString();
             }

File: chunjun-connectors/chunjun-connector-http/src/main/java/com/dtstack/chunjun/connector/http/converter/HttpColumnConverter.java
Patch:
@@ -55,6 +55,7 @@ public class HttpColumnConverter
 
     public HttpColumnConverter(HttpRestConfig httpRestConfig) {
         this.httpRestConfig = httpRestConfig;
+        this.commonConf = httpRestConfig;
 
         // Only json need to extract the fields
         if (StringUtils.isNotBlank((httpRestConfig.getFields()))) {

File: chunjun-connectors/chunjun-connector-hbase-base/src/main/java/com/dtstack/chunjun/connector/hbase/converter/HBaseColumnConverter.java
Patch:
@@ -213,7 +213,8 @@ public Mutation toExternal(RowData rowData, Mutation output) throws Exception {
         }
 
         put.setTTL(
-                Optional.ofNullable(hBaseConf.getTtl()).orElseGet(() -> (long) Integer.MAX_VALUE));
+                Optional.ofNullable(hBaseConf.getTtl())
+                        .orElseGet(() -> (long) Integer.MAX_VALUE * 1000));
 
         for (int i = 0; i < rowData.getArity(); i++) {
             if (rowKeyIndex == i || columnConfigIndex.contains(i)) {

File: chunjun-connectors/chunjun-connector-postgresql/src/main/java/com/dtstack/chunjun/connector/postgresql/sink/PostgresOutputFormat.java
Patch:
@@ -90,7 +90,8 @@ protected void openInternal(int taskNumber, int numTasks) {
             checkUpsert();
             if (rowConverter instanceof JdbcColumnConverter) {
                 if (jdbcDialect.dialectName().equals("PostgresSQL")) {
-                    ((PostgresqlColumnConverter) rowConverter).setConnection((BaseConnection) dbConn);
+                    ((PostgresqlColumnConverter) rowConverter)
+                            .setConnection((BaseConnection) dbConn);
                 }
                 ((PostgresqlColumnConverter) rowConverter).setFieldTypeList(columnTypeList);
             }

File: chunjun-connectors/chunjun-connector-hdfs/src/main/java/com/dtstack/chunjun/connector/hdfs/util/HdfsUtil.java
Patch:
@@ -159,13 +159,13 @@ public static ObjectInspector columnTypeToObjectInspetor(ColumnType columnType)
             case TIMESTAMP:
                 objectInspector =
                         ObjectInspectorFactory.getReflectionObjectInspector(
-                                java.sql.Timestamp.class,
+                                org.apache.hadoop.hive.common.type.Timestamp.class,
                                 ObjectInspectorFactory.ObjectInspectorOptions.JAVA);
                 break;
             case DATE:
                 objectInspector =
                         ObjectInspectorFactory.getReflectionObjectInspector(
-                                java.sql.Date.class,
+                                org.apache.hadoop.hive.common.type.Date.class,
                                 ObjectInspectorFactory.ObjectInspectorOptions.JAVA);
                 break;
             case STRING:

File: chunjun-connectors/chunjun-connector-hdfs/src/main/java/com/dtstack/chunjun/connector/hdfs/converter/HdfsTextColumnConverter.java
Patch:
@@ -130,7 +130,7 @@ protected ISerializationConverter<String[]> wrapIntoNullableExternalConverter(
             ISerializationConverter serializationConverter, String type) {
         return (rowData, index, data) -> {
             if (rowData == null || rowData.isNullAt(index)) {
-                data[index] = null;
+                data[index] = "\\N";
             } else {
                 serializationConverter.serialize(rowData, index, data);
             }

File: chunjun-connectors/chunjun-connector-hdfs/src/main/java/com/dtstack/chunjun/connector/hdfs/source/HdfsTextInputFormat.java
Patch:
@@ -123,7 +123,8 @@ public RowData nextRecordInternal(RowData rowData) throws ReadRecordException {
                             ((Text) value).getLength(),
                             hdfsConf.getEncoding());
             String[] fields =
-                    StringUtils.splitPreserveAllTokens(line, hdfsConf.getFieldDelimiter());
+                    StringUtils.splitByWholeSeparatorPreserveAllTokens(
+                            line, hdfsConf.getFieldDelimiter());
 
             List<FieldConf> fieldConfList = hdfsConf.getColumn();
             GenericRowData genericRowData;

File: chunjun-connectors/chunjun-connector-oraclelogminer/src/main/java/com/dtstack/chunjun/connector/oraclelogminer/listener/LogMinerListener.java
Patch:
@@ -524,7 +524,9 @@ private BigInteger getLockTableScn(Connection conn, String tbnWithSchema) {
             }
 
             /* generate create table ddl */
-            initialTableStruct(conn);
+            if (logMinerConf.isInitialTableStructure()) {
+                initialTableStruct(conn);
+            }
 
             /* release lock */
             stmt.execute(SqlUtil.releaseTableLock());

File: chunjun-connectors/chunjun-connector-saphana/src/main/java/com/dtstack/chunjun/connector/saphana/dialect/SaphanaDialect.java
Patch:
@@ -110,7 +110,7 @@ public Optional<String> getUpsertStatement(
 
     @Override
     public String quoteIdentifier(String identifier) {
-        return identifier;
+        return "\"" + identifier + "\"";
     }
 
     @Override

File: chunjun-connectors/chunjun-connector-jdbc-base/src/main/java/com/dtstack/chunjun/connector/jdbc/util/JdbcUtil.java
Patch:
@@ -166,7 +166,7 @@ public static Pair<List<String>, List<String>> getTableMetaData(
                 }
                 querySql = String.format("select * from %s where 1=2", tableInfo);
             } else {
-                querySql = String.format("select * from ((%s) custom) where 1=2", querySql);
+                querySql = String.format("select * from (%s) custom where 1=2", querySql);
             }
 
             Statement statement = dbConn.createStatement();

File: chunjun-connectors/chunjun-connector-emqx/src/main/java/com/dtstack/chunjun/connector/emqx/converter/EmqxColumnConverter.java
Patch:
@@ -105,7 +105,7 @@ public MqttMessage toExternal(RowData rowData, MqttMessage output) throws Except
             map = Collections.singletonMap("message", row.getString());
         }
 
-        output.setPayload(MapUtil.writeValueAsString(map).getBytes());
+        output.setPayload(MapUtil.writeValueAsBytes(map));
         return output;
     }
 }

File: chunjun-connectors/chunjun-connector-jdbc-base/src/main/java/com/dtstack/chunjun/connector/jdbc/statement/FieldNamedPreparedStatement.java
Patch:
@@ -266,4 +266,7 @@ static FieldNamedPreparedStatement prepareStatement(
 
     /** *Reopen the Statement */
     void reOpen(Connection connection) throws SQLException;
+
+    /** get the connection */
+    Connection getConnection() throws SQLException;
 }

File: chunjun-connectors/chunjun-connector-elasticsearch-base/src/main/java/com/dtstack/chunjun/connector/elasticsearch/ElasticsearchRawTypeMapper.java
Patch:
@@ -54,9 +54,10 @@ public static DataType apply(String type) {
             case "DOUBLE":
                 return DataTypes.DOUBLE();
             case "TEXT":
-                return DataTypes.STRING();
+            case "STRING":
             case "BINARY":
-                return DataTypes.BYTES();
+            case "KEYWORD":
+                return DataTypes.STRING();
             case "DATE":
                 return DataTypes.TIMESTAMP();
             case "OBJECT":

File: chunjun-core/src/main/java/com/dtstack/chunjun/Main.java
Patch:
@@ -187,7 +187,9 @@ private static void exeSyncJob(
 
         dataStreamSource = addMappingOperator(config, dataStreamSource);
 
-        if (null != config.getCdcConf()) {
+        if (null != config.getCdcConf()
+                && (null != config.getCdcConf().getDdl()
+                        && null != config.getCdcConf().getCache())) {
             CdcConf cdcConf = config.getCdcConf();
             DDLHandler ddlHandler = DataSyncFactoryUtil.discoverDdlHandler(cdcConf, config);
 

File: chunjun-connectors/chunjun-connector-hdfs/src/main/java/com/dtstack/chunjun/connector/hdfs/sink/HdfsOrcOutputFormat.java
Patch:
@@ -51,7 +51,6 @@
 import org.apache.hadoop.mapred.RecordWriter;
 import org.apache.hadoop.mapred.Reporter;
 
-import java.io.File;
 import java.io.IOException;
 import java.lang.reflect.Field;
 import java.util.ArrayList;
@@ -163,7 +162,7 @@ protected void nextBlock() {
         }
 
         try {
-            String currentBlockTmpPath = tmpPath + File.separatorChar + currentFileName;
+            String currentBlockTmpPath = tmpPath + getHdfsPathChar() + currentFileName;
             recordWriter =
                     outputFormat.getRecordWriter(null, jobConf, currentBlockTmpPath, Reporter.NULL);
             currentFileIndex++;

File: chunjun-connectors/chunjun-connector-hdfs/src/main/java/com/dtstack/chunjun/connector/hdfs/sink/HdfsParquetOutputFormat.java
Patch:
@@ -48,7 +48,6 @@
 import org.apache.parquet.schema.PrimitiveType;
 import org.apache.parquet.schema.Types;
 
-import java.io.File;
 import java.io.IOException;
 import java.security.PrivilegedAction;
 import java.util.HashMap;
@@ -95,7 +94,7 @@ protected void nextBlock() {
         }
 
         try {
-            String currentBlockTmpPath = tmpPath + File.separatorChar + currentFileName;
+            String currentBlockTmpPath = tmpPath + getHdfsPathChar() + currentFileName;
             Path writePath = new Path(currentBlockTmpPath);
 
             // Compatible with old code

File: chunjun-connectors/chunjun-connector-hdfs/src/main/java/com/dtstack/chunjun/connector/hdfs/sink/HdfsTextOutputFormat.java
Patch:
@@ -32,7 +32,6 @@
 import org.apache.commons.compress.compressors.gzip.GzipCompressorOutputStream;
 import org.apache.hadoop.fs.Path;
 
-import java.io.File;
 import java.io.IOException;
 import java.io.OutputStream;
 
@@ -55,7 +54,7 @@ protected void nextBlock() {
         }
 
         try {
-            String currentBlockTmpPath = tmpPath + File.separatorChar + currentFileName;
+            String currentBlockTmpPath = tmpPath + getHdfsPathChar() + currentFileName;
             Path p = new Path(currentBlockTmpPath);
 
             if (CompressType.TEXT_NONE.equals(compressType)) {

File: chunjun-connectors/chunjun-connector-ftp/src/main/java/com/dtstack/chunjun/connector/ftp/sink/FtpOutputFormat.java
Patch:
@@ -135,7 +135,7 @@ protected void closeSource() {
                 os = null;
             }
             // avoid Failure of FtpClient operating
-            this.ftpHandler.completePendingCommand();
+            this.ftpHandler.logoutFtpServer();
         } catch (Exception e) {
             throw new ChunJunRuntimeException("can't close source.", e);
         }

File: chunjun-connectors/chunjun-connector-ftp/src/main/java/com/dtstack/chunjun/connector/ftp/sink/FtpOutputFormatBuilder.java
Patch:
@@ -31,9 +31,7 @@
  *
  * @author huyifan.zju@163.com
  */
-public class FtpOutputFormatBuilder extends FileOutputFormatBuilder {
-
-    private FtpOutputFormat format;
+public class FtpOutputFormatBuilder extends FileOutputFormatBuilder<FtpOutputFormat> {
 
     public FtpOutputFormatBuilder() {
         super(new FtpOutputFormat());

File: chunjun-core/src/main/java/com/dtstack/chunjun/sink/format/FileOutputFormatBuilder.java
Patch:
@@ -24,10 +24,10 @@
  *
  * @author tudou
  */
-public abstract class FileOutputFormatBuilder
-        extends BaseRichOutputFormatBuilder<BaseFileOutputFormat> {
+public abstract class FileOutputFormatBuilder<T extends BaseFileOutputFormat>
+        extends BaseRichOutputFormatBuilder<T> {
 
-    public FileOutputFormatBuilder(BaseFileOutputFormat format) {
+    public FileOutputFormatBuilder(T format) {
         super(format);
     }
 

File: chunjun-connectors/chunjun-connector-hdfs/src/main/java/com/dtstack/chunjun/connector/hdfs/InputSplit/HdfsOrcInputSplit.java
Patch:
@@ -51,7 +51,9 @@ public HdfsOrcInputSplit(OrcSplit orcSplit, int splitNumber) throws IOException
     public OrcSplit getOrcSplit() throws IOException {
         ByteArrayInputStream bais = new ByteArrayInputStream(orcSplitData);
         DataInputStream dis = new DataInputStream(bais);
-        OrcSplit orcSplit = new OrcSplit(null, 0, 0, null, null, false, false, new ArrayList<>());
+        OrcSplit orcSplit =
+                new OrcSplit(
+                        null, null, 0, 0, null, null, false, false, new ArrayList<>(), 0, 0, null);
         orcSplit.readFields(dis);
         bais.close();
         dis.close();

File: chunjun-connectors/chunjun-connector-hdfs/src/main/java/com/dtstack/chunjun/connector/hdfs/sink/BaseHdfsOutputFormat.java
Patch:
@@ -83,6 +83,7 @@ protected void initVariableFields() {
                     hdfsConf.getColumn().stream()
                             .map(FieldConf::getName)
                             .collect(Collectors.toList());
+            hdfsConf.setFullColumnName(fullColumnNameList);
         }
 
         if (CollectionUtils.isNotEmpty(hdfsConf.getFullColumnType())) {
@@ -92,6 +93,7 @@ protected void initVariableFields() {
                     hdfsConf.getColumn().stream()
                             .map(FieldConf::getType)
                             .collect(Collectors.toList());
+            hdfsConf.setFullColumnType(fullColumnTypeList);
         }
         compressType = getCompressType();
         super.initVariableFields();

File: chunjun-core/src/main/java/com/dtstack/chunjun/Main.java
Patch:
@@ -211,6 +211,7 @@ private static void exeSyncJob(
         if (transformer) {
             dataStream = syncStreamToTable(tableEnv, config, dataStreamSource);
         } else {
+            env.disableOperatorChaining();
             dataStream = dataStreamSource;
         }
 
@@ -223,7 +224,7 @@ private static void exeSyncJob(
         if (speed.getWriterChannel() > 0) {
             dataStreamSink.setParallelism(speed.getWriterChannel());
         }
-        // env.disableOperatorChaining();
+
         JobExecutionResult result = env.execute(options.getJobName());
         if (env instanceof MyLocalStreamEnvironment) {
             PrintUtil.printResult(result.getAllAccumulatorResults());

File: chunjun-connectors/chunjun-connector-jdbc-base/src/main/java/com/dtstack/chunjun/connector/jdbc/sink/JdbcOutputFormat.java
Patch:
@@ -98,7 +98,7 @@ protected void openInternal(int taskNumber, int numTasks) {
                 List<String> updateKey = jdbcConf.getUniqueKey();
                 if (CollectionUtils.isEmpty(updateKey)) {
                     List<String> tableIndex =
-                            JdbcUtil.getTableIndex(
+                            JdbcUtil.getTableUniqueIndex(
                                     jdbcConf.getSchema(), jdbcConf.getTable(), dbConn);
                     jdbcConf.setUniqueKey(tableIndex);
                     LOG.info("updateKey = {}", JsonUtil.toJson(tableIndex));

File: chunjun-connectors/chunjun-connector-oraclelogminer/src/main/java/com/dtstack/chunjun/connector/oraclelogminer/converter/OracleRawTypeConverter.java
Patch:
@@ -52,14 +52,15 @@ public static DataType apply(String type) {
             case "NCHAR":
             case "NVARCHAR2":
             case "LONG":
-            case "RAW":
-            case "LONG RAW":
             case "BLOB":
             case "CLOB":
             case "NCLOB":
             case "INTERVAL YEAR":
             case "INTERVAL DAY":
                 return DataTypes.STRING();
+            case "RAW":
+            case "LONG RAW":
+                return DataTypes.BYTES();
             case "INT":
             case "INTEGER":
                 return DataTypes.INT();

File: chunjun-connectors/chunjun-connector-kudu/src/main/java/com/dtstack/chunjun/connector/kudu/source/KuduInputFormatBuilder.java
Patch:
@@ -50,7 +50,9 @@ protected void checkFormat() {
         StringBuilder sb = new StringBuilder(256);
 
         if (columns == null || columns.size() == 0) {
-            sb.append("Columns can not be empty.\n");
+            if (format.getRowConverter() == null) {
+                sb.append("At least one of the Column and rowConverter is not empty.\n");
+            }
         }
 
         if (sourceConf.getBatchSizeBytes() > ConstantValue.STORE_SIZE_G) {

File: chunjun-connectors/chunjun-connector-elasticsearch-base/src/main/java/com/dtstack/chunjun/connector/elasticsearch/ElasticsearchColumnConverter.java
Patch:
@@ -258,7 +258,8 @@ protected ISerializationConverter<Map<String, Object>> createExternalConverter(
             case TIMESTAMP_WITHOUT_TIME_ZONE:
                 return (val, index, output) -> {
                     AbstractBaseColumn field = ((ColumnRowData) val).getField(index);
-                    if (field instanceof StringColumn && ((StringColumn) field).isCustomFormat()) {
+                    if (field instanceof StringColumn && ((StringColumn) field).isCustomFormat()
+                            || null == dateFormatMap.get(index)) {
                         output.put(typeIndexList.get(index)._1(), field.asTimestampStr());
                     } else {
                         output.put(

File: chunjun-connectors/chunjun-connector-ftp/src/main/java/com/dtstack/chunjun/connector/ftp/handler/IFtpHandler.java
Patch:
@@ -32,7 +32,7 @@
  *
  * @author huyifan.zju@163.com
  */
-public interface IFtpHandler {
+public interface IFtpHandler extends AutoCloseable {
 
     /**
      * 登录服务器

File: chunjun-connectors/chunjun-connector-sqlservercdc/src/main/java/com/dtstack/chunjun/connector/sqlservercdc/inputFormat/SqlServerCdcInputFormatBuilder.java
Patch:
@@ -55,8 +55,6 @@ public class SqlServerCdcInputFormatBuilder
 
     protected String tableFormat = "%s.%s";
 
-    protected SqlServerCdcInputFormat format;
-
     public SqlServerCdcInputFormatBuilder() {
         super(new SqlServerCdcInputFormat());
     }

File: chunjun-connectors/chunjun-connector-emqx/src/main/java/com/dtstack/chunjun/connector/emqx/table/EmqxDynamicTableFactory.java
Patch:
@@ -53,6 +53,7 @@
 import static com.dtstack.chunjun.connector.emqx.options.EmqxOptions.QOS;
 import static com.dtstack.chunjun.connector.emqx.options.EmqxOptions.TOPIC;
 import static com.dtstack.chunjun.connector.emqx.options.EmqxOptions.USERNAME;
+import static com.dtstack.chunjun.connector.emqx.options.EmqxOptions.connectRetryTimes;
 
 /**
  * @author chuixue
@@ -127,6 +128,7 @@ public Set<ConfigOption<?>> optionalOptions() {
         optionalOptions.add(USERNAME);
         optionalOptions.add(PASSWORD);
         optionalOptions.add(FORMAT);
+        optionalOptions.add(connectRetryTimes);
         return optionalOptions;
     }
 

File: chunjun-connectors/chunjun-connector-emqx/src/main/java/com/dtstack/chunjun/connector/emqx/util/MqttConnectUtil.java
Patch:
@@ -49,7 +49,7 @@ public class MqttConnectUtil {
      */
     public static MqttClient getMqttClient(EmqxConf emqxConf, String clientId) {
         MqttClient client = null;
-        for (int i = 0; i <= 2; i++) {
+        for (int i = 0; i <= emqxConf.getConnectRetryTimes(); i++) {
             try {
                 client = new MqttClient(emqxConf.getBroker(), clientId);
                 MqttConnectOptions options = new MqttConnectOptions();
@@ -73,7 +73,7 @@ public static MqttClient getMqttClient(EmqxConf emqxConf, String clientId) {
                 } catch (InterruptedException interruptedException) {
                     throw new RuntimeException(interruptedException);
                 }
-                if (i == 2) {
+                if (i == emqxConf.getConnectRetryTimes()) {
                     throw new RuntimeException(e);
                 }
             }

File: chunjun-core/src/main/java/com/dtstack/chunjun/converter/AbstractRowConverter.java
Patch:
@@ -21,7 +21,6 @@
 import com.dtstack.chunjun.conf.ChunJunCommonConf;
 import com.dtstack.chunjun.conf.FieldConf;
 import com.dtstack.chunjun.element.AbstractBaseColumn;
-import com.dtstack.chunjun.element.column.NullColumn;
 import com.dtstack.chunjun.element.column.StringColumn;
 import com.dtstack.chunjun.enums.ColumnType;
 import com.dtstack.chunjun.util.DateUtil;
@@ -92,7 +91,7 @@ protected IDeserializationConverter wrapIntoNullableInternalConverter(
             IDeserializationConverter IDeserializationConverter) {
         return val -> {
             if (val == null) {
-                return new NullColumn();
+                return null;
             } else {
                 try {
                     return IDeserializationConverter.deserialize(val);

File: chunjun-connectors/chunjun-connector-hbase-base/src/main/java/com/dtstack/chunjun/connector/hbase/HBaseTableSchema.java
Patch:
@@ -251,7 +251,7 @@ public DataType[] getQualifierDataTypes(String family) {
      *     are returned.
      * @return The names and types of all registered column qualifiers of a specific column family.
      */
-    private Map<String, DataType> getFamilyInfo(String family) {
+    public Map<String, DataType> getFamilyInfo(String family) {
         return familyMap.get(family);
     }
 

File: chunjun-connectors/chunjun-connector-hbase-base/src/main/java/com/dtstack/chunjun/connector/hbase/table/HBaseOptions.java
Patch:
@@ -15,7 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package com.dtstack.chunjun.connector.hbase14.table;
+package com.dtstack.chunjun.connector.hbase.table;
 
 import com.dtstack.chunjun.table.options.BaseFileOptions;
 

File: chunjun-core/src/main/java/com/dtstack/chunjun/Main.java
Patch:
@@ -223,7 +223,7 @@ private static void exeSyncJob(
         if (speed.getWriterChannel() > 0) {
             dataStreamSink.setParallelism(speed.getWriterChannel());
         }
-        env.disableOperatorChaining();
+        // env.disableOperatorChaining();
         JobExecutionResult result = env.execute(options.getJobName());
         if (env instanceof MyLocalStreamEnvironment) {
             PrintUtil.printResult(result.getAllAccumulatorResults());

File: chunjun-connectors/chunjun-connector-greenplum/src/main/java/com/dtstack/chunjun/connector/greenplum/sink/GreenplumSinkFactory.java
Patch:
@@ -19,6 +19,7 @@
 package com.dtstack.chunjun.connector.greenplum.sink;
 
 import com.dtstack.chunjun.conf.SyncConf;
+import com.dtstack.chunjun.connector.greenplum.dialect.GreenplumDialect;
 import com.dtstack.chunjun.connector.postgresql.sink.PostgresqlSinkFactory;
 
 /**
@@ -29,6 +30,6 @@
 public class GreenplumSinkFactory extends PostgresqlSinkFactory {
 
     public GreenplumSinkFactory(SyncConf syncConf) {
-        super(syncConf);
+        super(syncConf, new GreenplumDialect());
     }
 }

File: chunjun-connectors/chunjun-connector-postgresql/src/main/java/com/dtstack/chunjun/connector/postgresql/sink/PostgresOutputFormat.java
Patch:
@@ -88,10 +88,10 @@ protected void openInternal(int taskNumber, int numTasks) {
                 LOG.info("write sql:{}", copySql);
             }
             checkUpsert();
-            if (rowConverter instanceof PostgresqlColumnConverter) {
+            if (jdbcDialect.dialectName().equals("PostgreSQL")) {
                 ((PostgresqlColumnConverter) rowConverter).setConnection((BaseConnection) dbConn);
-                ((PostgresqlColumnConverter) rowConverter).setFieldTypeList(columnTypeList);
             }
+            ((PostgresqlColumnConverter) rowConverter).setFieldTypeList(columnTypeList);
         } catch (SQLException sqe) {
             throw new IllegalArgumentException("checkUpsert() failed.", sqe);
         }

File: chunjun-connectors/chunjun-connector-hive/src/main/java/com/dtstack/chunjun/connector/hive/sink/HiveOutputFormat.java
Patch:
@@ -246,8 +246,8 @@ public void closeInternal() {
                 outputFormatMap.entrySet()) {
             try {
                 BaseHdfsOutputFormat format = entry.getValue().getRight();
-                format.finalizeGlobal(numTasks);
                 format.close();
+                format.finalizeGlobal(numTasks);
             } catch (IOException e) {
                 LOG.warn("close {} outputFormat error", entry.getKey(), e);
             }

File: chunjun-connectors/chunjun-connector-jdbc-base/src/main/java/com/dtstack/chunjun/connector/jdbc/converter/JdbcColumnConverter.java
Patch:
@@ -77,7 +77,7 @@ public JdbcColumnConverter(RowType rowType, ChunJunCommonConf commonConf) {
             wrapIntoNullableExternalConverter(
                     ISerializationConverter serializationConverter, LogicalType type) {
         return (val, index, statement) -> {
-            if (((ColumnRowData) val).getField(index) == null) {
+            if(val == null || val.isNullAt(index)) {
                 statement.setObject(index, null);
             } else {
                 serializationConverter.serialize(val, index, statement);

File: chunjun-core/src/main/java/com/dtstack/chunjun/converter/AbstractRowConverter.java
Patch:
@@ -21,6 +21,7 @@
 import com.dtstack.chunjun.conf.ChunJunCommonConf;
 import com.dtstack.chunjun.conf.FieldConf;
 import com.dtstack.chunjun.element.AbstractBaseColumn;
+import com.dtstack.chunjun.element.column.NullColumn;
 import com.dtstack.chunjun.element.column.StringColumn;
 import com.dtstack.chunjun.enums.ColumnType;
 import com.dtstack.chunjun.util.DateUtil;
@@ -91,7 +92,7 @@ protected IDeserializationConverter wrapIntoNullableInternalConverter(
             IDeserializationConverter IDeserializationConverter) {
         return val -> {
             if (val == null) {
-                return null;
+                return new NullColumn();
             } else {
                 try {
                     return IDeserializationConverter.deserialize(val);

File: chunjun-connectors/chunjun-connector-jdbc-base/src/main/java/com/dtstack/chunjun/connector/jdbc/statement/FieldNamedPreparedStatement.java
Patch:
@@ -263,4 +263,7 @@ static FieldNamedPreparedStatement prepareStatement(
      * @see PreparedStatement#close()
      */
     void close() throws SQLException;
+
+    /** *Reopen the Statement */
+    void reOpen(Connection connection) throws SQLException;
 }

File: chunjun-core/src/main/java/com/dtstack/chunjun/source/SourceFactory.java
Patch:
@@ -127,7 +127,7 @@ protected DataStream<RowData> createInput(
     protected DataStream<RowData> createInput(
             RichParallelSourceFunction<RowData> function, String sourceName) {
         Preconditions.checkNotNull(sourceName);
-        return env.addSource(function, sourceName, getTypeInformation());
+        return env.addSource(function, sourceName);
     }
 
     protected DataStream<RowData> createInput(InputFormat<RowData, InputSplit> inputFormat) {

File: chunjun-connectors/chunjun-connector-oracle/src/main/java/com/dtstack/chunjun/connector/oracle/converter/OracleRawTypeConverter.java
Patch:
@@ -82,9 +82,9 @@ public static DataType apply(String type) {
                 return DataTypes.DECIMAL(decimalInfo.getPrecision(), decimalInfo.getScale());
             case "DATE":
                 return DataTypes.DATE();
-            case "RAW":
             case "TIMESTAMP":
                 return DataTypes.TIMESTAMP();
+            case "RAW":
             case "LONG RAW":
                 return DataTypes.BYTES();
             case "BLOB":

File: chunjun-connectors/chunjun-connector-redis/src/main/java/com/dtstack/chunjun/connector/redis/converter/RedisColumnConverter.java
Patch:
@@ -166,7 +166,7 @@ private String[] getValues(ColumnRowData row) {
             }
         }
 
-        return values.toArray(new String[values.size()]);
+        return values.toArray(new String[0]);
     }
 
     private String concatValues(ColumnRowData row) {

File: chunjun-connectors/chunjun-connector-redis/src/main/java/com/dtstack/chunjun/connector/redis/enums/RedisDataType.java
Patch:
@@ -36,7 +36,7 @@ public enum RedisDataType {
 
     HASH("hash");
 
-    public String type;
+    public final String type;
 
     RedisDataType(String type) {
         this.type = type;

File: chunjun-connectors/chunjun-connector-kudu/src/main/java/com/dtstack/chunjun/connector/kudu/conf/KuduSourceConf.java
Patch:
@@ -43,7 +43,7 @@ public class KuduSourceConf extends KuduCommonConf {
      */
     private String readMode;
 
-    private int batchSizeBytes;
+    private int batchSizeBytes = 1024;
 
     private String filter;
 

File: chunjun-connectors/chunjun-connector-kudu/src/main/java/com/dtstack/chunjun/connector/kudu/source/KuduSourceFactory.java
Patch:
@@ -21,8 +21,8 @@
 import com.dtstack.chunjun.conf.FieldConf;
 import com.dtstack.chunjun.conf.SyncConf;
 import com.dtstack.chunjun.connector.kudu.conf.KuduSourceConf;
+import com.dtstack.chunjun.connector.kudu.converter.KuduColumnConverter;
 import com.dtstack.chunjun.connector.kudu.converter.KuduRawTypeConverter;
-import com.dtstack.chunjun.connector.kudu.converter.KuduRowConverter;
 import com.dtstack.chunjun.converter.RawTypeConverter;
 import com.dtstack.chunjun.source.SourceFactory;
 import com.dtstack.chunjun.util.JsonUtil;
@@ -74,7 +74,7 @@ public DataStream<RowData> createSource() {
         fieldConfList.forEach(fieldConf -> columnNameList.add(fieldConf.getName()));
 
         builder.setRowConverter(
-                new KuduRowConverter(rowType, columnNameList), useAbstractBaseColumn);
+                new KuduColumnConverter(rowType, columnNameList), useAbstractBaseColumn);
 
         return createInput(builder.finish());
     }

File: chunjun-connectors/chunjun-connector-hive/src/main/java/com/dtstack/chunjun/connector/hive/util/HiveUtil.java
Patch:
@@ -175,7 +175,8 @@ private static void fillTableInfo(Connection connection, TableInfo tableInfo) {
             metadataParser.fillTableInfo(tableInfo, result);
         } catch (Exception e) {
             if (e.getMessage().contains(NO_SUCH_TABLE_EXCEPTION)) {
-                throw new ChunJunRuntimeException(String.format("表%s不存在", tableInfo.getTablePath()));
+                throw new ChunJunRuntimeException(
+                        String.format("表%s不存在", tableInfo.getTablePath()));
             } else {
                 throw e;
             }

File: chunjun-connectors/chunjun-connector-binlog/src/main/java/com/dtstack/chunjun/connector/binlog/listener/BinlogEventSink.java
Patch:
@@ -83,8 +83,8 @@ public boolean sink(
                 processRowChange(rowChange, schema, table, executeTime);
             } catch (WriteRecordException e) {
                 // todo 脏数据记录
-                if (LOG.isTraceEnabled()) {
-                    LOG.trace(
+                if (LOG.isDebugEnabled()) {
+                    LOG.debug(
                             "write error rowData, rowData = {}, e = {}",
                             e.getRowData().toString(),
                             ExceptionUtil.getErrorMessage(e));

File: chunjun-connectors/chunjun-connector-hive/src/main/java/com/dtstack/chunjun/connector/hive/util/HiveDbUtil.java
Patch:
@@ -127,9 +127,7 @@ private static Connection getConnectionWithKerberos(
 
         UserGroupInformation ugi;
         try {
-            ugi =
-                    KerberosUtil.loginAndReturnUgi(
-                            conf.get(KerberosUtil.KEY_PRINCIPAL_FILE), principal, keytabFileName);
+            ugi = KerberosUtil.loginAndReturnUgi(conf, principal, keytabFileName);
         } catch (Exception e) {
             throw new RuntimeException("Login kerberos error:", e);
         }

File: chunjun-connectors/chunjun-connector-mongodb/src/main/java/com/dtstack/chunjun/connector/mongodb/sink/MongodbOutputFormatBuilder.java
Patch:
@@ -36,9 +36,11 @@
  */
 public class MongodbOutputFormatBuilder extends BaseRichOutputFormatBuilder {
     MongodbDataSyncConf mongodbDataSyncConf;
+    String upsertKey;
 
     public MongodbOutputFormatBuilder(MongodbDataSyncConf mongodbDataSyncConf) {
         this.mongodbDataSyncConf = mongodbDataSyncConf;
+        this.upsertKey = mongodbDataSyncConf.getReplaceKey();
         MongoClientConf mongoClientConf =
                 MongoClientConfFactory.createMongoClientConf(mongodbDataSyncConf);
         MongodbOutputFormat.WriteMode writeMode =
@@ -50,12 +52,12 @@ public MongodbOutputFormatBuilder(MongodbDataSyncConf mongodbDataSyncConf) {
 
     public MongodbOutputFormatBuilder(
             MongoClientConf mongoClientConf, String key, MongodbOutputFormat.WriteMode writeMode) {
+        this.upsertKey = key;
         this.format = new MongodbOutputFormat(mongoClientConf, key, writeMode);
     }
 
     @Override
     protected void checkFormat() {
-        String upsertKey = mongodbDataSyncConf.getReplaceKey();
         if (!StringUtils.isBlank(upsertKey)) {
             List<FieldConf> fields = mongodbDataSyncConf.getColumn();
             boolean flag = false;

File: chunjun-connectors/chunjun-connector-mongodb/src/main/java/com/dtstack/chunjun/connector/mongodb/table/lookup/MongoAllTableFunction.java
Patch:
@@ -44,7 +44,7 @@
  */
 public class MongoAllTableFunction extends AbstractAllTableFunction {
 
-    private static final int FETCH_SIZE = 1000;
+    private final int fetchSize;
     private final MongoClientConf mongoClientConf;
     private transient MongoClient mongoClient;
     private transient MongoCollection collection;
@@ -57,6 +57,7 @@ public MongoAllTableFunction(
             String[] fieldNames) {
         super(fieldNames, keyNames, lookupConf, new MongodbRowConverter(rowType, fieldNames));
         this.mongoClientConf = mongoClientConf;
+        this.fetchSize = lookupConf.getFetchSize();
     }
 
     @Override
@@ -70,7 +71,7 @@ protected void loadData(Object cacheRef) {
         Map<String, List<Map<String, Object>>> tmpCache =
                 (Map<String, List<Map<String, Object>>>) cacheRef;
 
-        FindIterable<Document> findIterable = collection.find().limit(FETCH_SIZE);
+        FindIterable<Document> findIterable = collection.find().limit(fetchSize);
         MongoCursor<Document> mongoCursor = findIterable.iterator();
         while (mongoCursor.hasNext()) {
             Document doc = mongoCursor.next();

File: chunjun-connectors/chunjun-connector-oraclelogminer/src/main/java/com/dtstack/chunjun/connector/oraclelogminer/listener/LogMinerListener.java
Patch:
@@ -144,7 +144,7 @@ public void run() {
                 }
             } catch (Exception e) {
                 sendException(e, log);
-                logMinerHelper.restart();
+                logMinerHelper.restart(e);
             }
         }
     }
@@ -221,6 +221,7 @@ public RowData getData() {
                         sb.append("\nerror msg is : ").append(errorMsg);
                         throw new RuntimeException(sb.toString());
                     }
+                    rowData = null;
                 } else {
                     positionManager.updatePosition(poll.getScn());
                     failedTimes = 0;

File: chunjun-core/src/main/java/com/dtstack/chunjun/constants/ConstantValue.java
Patch:
@@ -41,6 +41,7 @@ public class ConstantValue {
     public static final String RIGHT_PARENTHESIS_SYMBOL = ")";
 
     public static final String DATA_TYPE_UNSIGNED = "UNSIGNED";
+    public static final String DATA_TYPE_UNSIGNED_LOWER = "unsigned";
 
     public static final String KEY_HTTP = "http";
 

File: chunjun-connectors/chunjun-connector-hbase-1.4/src/main/java/com/dtstack/chunjun/connector/hbase14/sink/HBase14SinkFactory.java
Patch:
@@ -95,7 +95,7 @@ public DataStreamSink<RowData> createSink(DataStream<RowData> dataSet) {
             rowConverter = new HbaseRowConverter(hbaseSchema, nullStringLiteral);
         }
 
-        builder.setRowConverter(rowConverter,useAbstractBaseColumn);
+        builder.setRowConverter(rowConverter, useAbstractBaseColumn);
         return createOutput(dataSet, builder.finish());
     }
 

File: chunjun-connectors/chunjun-connector-hbase-1.4/src/main/java/com/dtstack/chunjun/connector/hbase14/source/HBase14SourceFactory.java
Patch:
@@ -97,7 +97,7 @@ public DataStream<RowData> createSource() {
             rowConverter = new HbaseRowConverter(hbaseSchema, nullStringLiteral);
         }
 
-        builder.setRowConverter(rowConverter,useAbstractBaseColumn);
+        builder.setRowConverter(rowConverter, useAbstractBaseColumn);
         return createInput(builder.finish());
     }
 }

File: chunjun-core/src/main/java/com/dtstack/chunjun/element/column/ArrayColumn.java
Patch:
@@ -42,8 +42,7 @@ public class ArrayColumn extends AbstractBaseColumn {
     protected Array data;
 
     public ArrayColumn(final Array data) {
-        super(data);
-        this.data = data;
+        super(data, data.toString().length());
     }
 
     @Override

File: chunjun-core/src/main/java/com/dtstack/chunjun/element/column/ArrayColumn.java
Patch:
@@ -42,7 +42,8 @@ public class ArrayColumn extends AbstractBaseColumn {
     protected Array data;
 
     public ArrayColumn(final Array data) {
-        super(data, data.toString().length());
+        super(data);
+        this.data = data;
     }
 
     @Override

File: chunjun-connectors/chunjun-connector-binlog/src/main/java/com/dtstack/chunjun/connector/binlog/listener/BinlogEventSink.java
Patch:
@@ -83,8 +83,8 @@ public boolean sink(
                 processRowChange(rowChange, schema, table, executeTime);
             } catch (WriteRecordException e) {
                 // todo 脏数据记录
-                if (LOG.isTraceEnabled()) {
-                    LOG.trace(
+                if (LOG.isDebugEnabled()) {
+                    LOG.debug(
                             "write error rowData, rowData = {}, e = {}",
                             e.getRowData().toString(),
                             ExceptionUtil.getErrorMessage(e));

File: chunjun-connectors/chunjun-connector-hive/src/main/java/com/dtstack/chunjun/connector/hive/util/HiveDbUtil.java
Patch:
@@ -127,9 +127,7 @@ private static Connection getConnectionWithKerberos(
 
         UserGroupInformation ugi;
         try {
-            ugi =
-                    KerberosUtil.loginAndReturnUgi(
-                            conf.get(KerberosUtil.KEY_PRINCIPAL_FILE), principal, keytabFileName);
+            ugi = KerberosUtil.loginAndReturnUgi(conf, principal, keytabFileName);
         } catch (Exception e) {
             throw new RuntimeException("Login kerberos error:", e);
         }

File: chunjun-connectors/chunjun-connector-mongodb/src/main/java/com/dtstack/chunjun/connector/mongodb/sink/MongodbOutputFormatBuilder.java
Patch:
@@ -36,9 +36,11 @@
  */
 public class MongodbOutputFormatBuilder extends BaseRichOutputFormatBuilder {
     MongodbDataSyncConf mongodbDataSyncConf;
+    String upsertKey;
 
     public MongodbOutputFormatBuilder(MongodbDataSyncConf mongodbDataSyncConf) {
         this.mongodbDataSyncConf = mongodbDataSyncConf;
+        this.upsertKey = mongodbDataSyncConf.getReplaceKey();
         MongoClientConf mongoClientConf =
                 MongoClientConfFactory.createMongoClientConf(mongodbDataSyncConf);
         MongodbOutputFormat.WriteMode writeMode =
@@ -50,12 +52,12 @@ public MongodbOutputFormatBuilder(MongodbDataSyncConf mongodbDataSyncConf) {
 
     public MongodbOutputFormatBuilder(
             MongoClientConf mongoClientConf, String key, MongodbOutputFormat.WriteMode writeMode) {
+        this.upsertKey = key;
         this.format = new MongodbOutputFormat(mongoClientConf, key, writeMode);
     }
 
     @Override
     protected void checkFormat() {
-        String upsertKey = mongodbDataSyncConf.getReplaceKey();
         if (!StringUtils.isBlank(upsertKey)) {
             List<FieldConf> fields = mongodbDataSyncConf.getColumn();
             boolean flag = false;

File: chunjun-connectors/chunjun-connector-mongodb/src/main/java/com/dtstack/chunjun/connector/mongodb/table/lookup/MongoAllTableFunction.java
Patch:
@@ -44,7 +44,7 @@
  */
 public class MongoAllTableFunction extends AbstractAllTableFunction {
 
-    private static final int FETCH_SIZE = 1000;
+    private final int fetchSize;
     private final MongoClientConf mongoClientConf;
     private transient MongoClient mongoClient;
     private transient MongoCollection collection;
@@ -57,6 +57,7 @@ public MongoAllTableFunction(
             String[] fieldNames) {
         super(fieldNames, keyNames, lookupConf, new MongodbRowConverter(rowType, fieldNames));
         this.mongoClientConf = mongoClientConf;
+        this.fetchSize = lookupConf.getFetchSize();
     }
 
     @Override
@@ -70,7 +71,7 @@ protected void loadData(Object cacheRef) {
         Map<String, List<Map<String, Object>>> tmpCache =
                 (Map<String, List<Map<String, Object>>>) cacheRef;
 
-        FindIterable<Document> findIterable = collection.find().limit(FETCH_SIZE);
+        FindIterable<Document> findIterable = collection.find().limit(fetchSize);
         MongoCursor<Document> mongoCursor = findIterable.iterator();
         while (mongoCursor.hasNext()) {
             Document doc = mongoCursor.next();

File: chunjun-connectors/chunjun-connector-oraclelogminer/src/main/java/com/dtstack/chunjun/connector/oraclelogminer/listener/LogMinerListener.java
Patch:
@@ -144,7 +144,7 @@ public void run() {
                 }
             } catch (Exception e) {
                 sendException(e, log);
-                logMinerHelper.restart();
+                logMinerHelper.restart(e);
             }
         }
     }
@@ -221,6 +221,7 @@ public RowData getData() {
                         sb.append("\nerror msg is : ").append(errorMsg);
                         throw new RuntimeException(sb.toString());
                     }
+                    rowData = null;
                 } else {
                     positionManager.updatePosition(poll.getScn());
                     failedTimes = 0;

File: chunjun-core/src/main/java/com/dtstack/chunjun/constants/ConstantValue.java
Patch:
@@ -41,6 +41,7 @@ public class ConstantValue {
     public static final String RIGHT_PARENTHESIS_SYMBOL = ")";
 
     public static final String DATA_TYPE_UNSIGNED = "UNSIGNED";
+    public static final String DATA_TYPE_UNSIGNED_LOWER = "unsigned";
 
     public static final String KEY_HTTP = "http";
 

File: chunjun-connectors/chunjun-connector-binlog/src/main/java/com/dtstack/chunjun/connector/binlog/inputformat/BinlogInputFormat.java
Patch:
@@ -227,7 +227,7 @@ protected EntryPosition findStartPosition() {
             checkBinlogFile(startPosition.getJournalName());
         } else if (MapUtils.isNotEmpty(binlogConf.getStart())) {
             startPosition = new EntryPosition();
-            String journalName = (String) binlogConf.getStart().get("journalName");
+            String journalName = (String) binlogConf.getStart().get("journal-name");
             checkBinlogFile(journalName);
 
             if (StringUtils.isNotEmpty(journalName)) {

File: chunjun-core/src/main/java/com/dtstack/chunjun/conf/SyncConf.java
Patch:
@@ -101,7 +101,7 @@ private static void checkJob(SyncConf config) {
         Preconditions.checkNotNull(
                 writerName,
                 "[name] under [writer] in the task script is empty, please check the configuration of the task script.");
-        Map<String, Object> writerParameter = reader.getParameter();
+        Map<String, Object> writerParameter = writer.getParameter();
         Preconditions.checkNotNull(
                 writerParameter,
                 "[parameter] under [writer] in the task script is empty, please check the configuration of the task script.");

File: chunjun-connectors/chunjun-connector-jdbc-base/src/main/java/com/dtstack/chunjun/connector/jdbc/util/SqlUtil.java
Patch:
@@ -216,7 +216,7 @@ public static String buildOrderSql(
             JdbcConf jdbcConf, JdbcDialect jdbcDialect, String sortRule) {
         String column;
         // 增量任务
-        if (jdbcConf.isIncrement() && !jdbcConf.isPolling()) {
+        if (jdbcConf.isIncrement()) {
             column = jdbcConf.getIncreColumn();
         } else {
             column = jdbcConf.getOrderByColumn();

File: chunjun-connectors/chunjun-connector-hbase-1.4/src/main/java/com/dtstack/chunjun/connector/hbase14/sink/HBase14SinkFactory.java
Patch:
@@ -95,7 +95,7 @@ public DataStreamSink<RowData> createSink(DataStream<RowData> dataSet) {
             rowConverter = new HbaseRowConverter(hbaseSchema, nullStringLiteral);
         }
 
-        builder.setRowConverter(rowConverter,useAbstractBaseColumn);
+        builder.setRowConverter(rowConverter, useAbstractBaseColumn);
         return createOutput(dataSet, builder.finish());
     }
 

File: chunjun-connectors/chunjun-connector-hbase-1.4/src/main/java/com/dtstack/chunjun/connector/hbase14/source/HBase14SourceFactory.java
Patch:
@@ -97,7 +97,7 @@ public DataStream<RowData> createSource() {
             rowConverter = new HbaseRowConverter(hbaseSchema, nullStringLiteral);
         }
 
-        builder.setRowConverter(rowConverter,useAbstractBaseColumn);
+        builder.setRowConverter(rowConverter, useAbstractBaseColumn);
         return createInput(builder.finish());
     }
 }

File: chunjun-connectors/chunjun-connector-jdbc-base/src/main/java/com/dtstack/chunjun/connector/jdbc/util/SqlUtil.java
Patch:
@@ -57,7 +57,7 @@ public static String buildQuerySplitRangeSql(JdbcConf jdbcConf, JdbcDialect jdbc
 
         } else {
             // rowNum字段作为splitKey
-            if (addRowNumColumn(jdbcConf.getSplitPk())) {
+            if (isRowNumSplitKey(jdbcConf.getSplitPk())) {
                 StringBuilder customTableBuilder =
                         new StringBuilder(128)
                                 .append("SELECT ")
@@ -98,7 +98,7 @@ public static String buildQuerySqlBySplit(
         // customSql为空 且 splitPk是ROW_NUMBER()
         boolean flag =
                 StringUtils.isBlank(jdbcConf.getCustomSql())
-                        && SqlUtil.addRowNumColumn(jdbcConf.getSplitPk());
+                        && SqlUtil.isRowNumSplitKey(jdbcConf.getSplitPk());
 
         String splitFilter = null;
         if (jdbcInputSplit.getTotalNumberOfSplits() > 1) {
@@ -227,7 +227,7 @@ public static String buildOrderSql(
     }
 
     /* 是否添加自定义函数column 作为分片key ***/
-    public static boolean addRowNumColumn(String splitKey) {
+    public static boolean isRowNumSplitKey(String splitKey) {
         return StringUtils.isNotBlank(splitKey)
                 && splitKey.contains(ConstantValue.LEFT_PARENTHESIS_SYMBOL);
     }

File: chunjun-core/src/main/java/com/dtstack/chunjun/element/column/ArrayColumn.java
Patch:
@@ -42,8 +42,7 @@ public class ArrayColumn extends AbstractBaseColumn {
     protected Array data;
 
     public ArrayColumn(final Array data) {
-        super(data);
-        this.data = data;
+        super(data, data.toString().length());
     }
 
     @Override

File: chunjun-connectors/chunjun-connector-kudu/src/main/java/com/dtstack/chunjun/connector/kudu/converter/KuduColumnConverter.java
Patch:
@@ -134,6 +134,7 @@ protected IDeserializationConverter createInternalConverter(String type) {
             case "DATE":
                 return val -> new SqlDateColumn(Date.valueOf(String.valueOf(val)));
             case "TIMESTAMP":
+            case "TIMESTAMP_WITHOUT_TIME_ZONE":
                 return val -> new TimestampColumn((Timestamp) val);
             case "BINARY":
                 return val -> new BytesColumn((byte[]) val);
@@ -197,6 +198,7 @@ protected ISerializationConverter<Operation> createExternalConverter(String type
                                         ((ColumnRowData) val).getField(index).asSqlDate());
 
             case "TIMESTAMP":
+            case "TIMESTAMP_WITHOUT_TIME_ZONE":
                 return (val, index, operation) ->
                         operation
                                 .getRow()

File: chunjun-connectors/chunjun-connector-kudu/src/main/java/com/dtstack/chunjun/connector/kudu/converter/KuduRawTypeConverter.java
Patch:
@@ -53,7 +53,6 @@ public static DataType apply(String type) {
             case "INT64":
             case "BIGINT":
             case "LONG":
-            case "UNIXTIME_MICROS":
                 return DataTypes.BIGINT();
             case "BOOL":
             case "BOOLEAN":
@@ -70,6 +69,7 @@ public static DataType apply(String type) {
             case "DATE":
                 return DataTypes.DATE();
             case "TIMESTAMP":
+            case "UNIXTIME_MICROS":
                 return DataTypes.TIMESTAMP();
             default:
                 throw new UnsupportedTypeException(type);

File: chunjun-connectors/chunjun-connector-kudu/src/main/java/com/dtstack/chunjun/connector/kudu/converter/KuduRowConverter.java
Patch:
@@ -138,6 +138,7 @@ protected IDeserializationConverter createInternalConverter(LogicalType type) {
             case "DATE":
                 return val -> (int) Date.valueOf(String.valueOf(val)).toLocalDate().toEpochDay();
             case "TIMESTAMP":
+            case "TIMESTAMP_WITHOUT_TIME_ZONE":
                 return val -> TimestampData.fromEpochMillis(((java.util.Date) val).getTime());
             case "BINARY":
                 return val -> (byte[]) val;
@@ -205,6 +206,7 @@ protected ISerializationConverter<Operation> createExternalConverter(LogicalType
                                                                 .asInt())));
 
             case "TIMESTAMP":
+            case "TIMESTAMP_WITHOUT_TIME_ZONE":
                 return (val, index, operation) ->
                         operation
                                 .getRow()

File: chunjun-connectors/chunjun-connector-jdbc-base/src/main/java/com/dtstack/chunjun/connector/jdbc/sink/JdbcDynamicTableSink.java
Patch:
@@ -70,6 +70,7 @@ public ChangelogMode getChangelogMode(ChangelogMode requestedMode) {
                 .addContainedKind(RowKind.INSERT)
                 .addContainedKind(RowKind.DELETE)
                 .addContainedKind(RowKind.UPDATE_AFTER)
+                .addContainedKind(RowKind.UPDATE_BEFORE)
                 .build();
     }
 

File: chunjun-connectors/chunjun-connector-jdbc-base/src/main/java/com/dtstack/chunjun/connector/jdbc/sink/PreparedStmtProxy.java
Patch:
@@ -170,7 +170,7 @@ public void getOrCreateFieldNamedPstmt(RowData row) throws ExecutionException {
             currentRowConverter = fieldNamedPreparedStatement.getRowConverter();
         } else {
             String key =
-                    getPstmtCacheKey(jdbcConf.getSchema(), jdbcConf.getTable(), RowKind.INSERT);
+                    getPstmtCacheKey(jdbcConf.getSchema(), jdbcConf.getTable(), row.getRowKind());
             DynamicPreparedStmt fieldNamedPreparedStatement =
                     pstmtCache.get(
                             key,

File: chunjun-clients/src/main/java/com/dtstack/chunjun/client/yarn/YarnSessionClusterClientHelper.java
Patch:
@@ -104,6 +104,9 @@ public ClusterClient submit(JobDeployer jobDeployer) throws Exception {
                     JobGraph jobGraph =
                             JobGraphUtil.buildJobGraph(
                                     launcherOptions, programArgs.toArray(new String[0]));
+                    jobGraph.getClasspaths().clear();
+                    jobGraph.getUserJars().clear();
+                    jobGraph.getUserArtifacts().clear();
                     JobID jobID = (JobID) clusterClient.submitJob(jobGraph).get();
                     LOG.info("submit job successfully, jobID = {}", jobID);
                     return clusterClient;

File: chunjun-connectors/chunjun-connector-binlog/src/main/java/com/dtstack/chunjun/connector/binlog/inputformat/BinlogInputFormat.java
Patch:
@@ -227,7 +227,7 @@ protected EntryPosition findStartPosition() {
             checkBinlogFile(startPosition.getJournalName());
         } else if (MapUtils.isNotEmpty(binlogConf.getStart())) {
             startPosition = new EntryPosition();
-            String journalName = (String) binlogConf.getStart().get("journalName");
+            String journalName = (String) binlogConf.getStart().get("journal-name");
             checkBinlogFile(journalName);
 
             if (StringUtils.isNotEmpty(journalName)) {

File: chunjun-core/src/main/java/com/dtstack/chunjun/conf/SyncConf.java
Patch:
@@ -101,7 +101,7 @@ private static void checkJob(SyncConf config) {
         Preconditions.checkNotNull(
                 writerName,
                 "[name] under [writer] in the task script is empty, please check the configuration of the task script.");
-        Map<String, Object> writerParameter = reader.getParameter();
+        Map<String, Object> writerParameter = writer.getParameter();
         Preconditions.checkNotNull(
                 writerParameter,
                 "[parameter] under [writer] in the task script is empty, please check the configuration of the task script.");

File: chunjun-connectors/chunjun-connector-jdbc-base/src/main/java/com/dtstack/chunjun/connector/jdbc/util/SqlUtil.java
Patch:
@@ -216,7 +216,7 @@ public static String buildOrderSql(
             JdbcConf jdbcConf, JdbcDialect jdbcDialect, String sortRule) {
         String column;
         // 增量任务
-        if (jdbcConf.isIncrement() && !jdbcConf.isPolling()) {
+        if (jdbcConf.isIncrement()) {
             column = jdbcConf.getIncreColumn();
         } else {
             column = jdbcConf.getOrderByColumn();

File: chunjun-connectors/chunjun-connector-hbase-1.4/src/main/java/com/dtstack/chunjun/connector/hbase14/sink/HBase14SinkFactory.java
Patch:
@@ -95,7 +95,7 @@ public DataStreamSink<RowData> createSink(DataStream<RowData> dataSet) {
             rowConverter = new HbaseRowConverter(hbaseSchema, nullStringLiteral);
         }
 
-        builder.setRowConverter(rowConverter,useAbstractBaseColumn);
+        builder.setRowConverter(rowConverter, useAbstractBaseColumn);
         return createOutput(dataSet, builder.finish());
     }
 

File: chunjun-connectors/chunjun-connector-hbase-1.4/src/main/java/com/dtstack/chunjun/connector/hbase14/source/HBase14SourceFactory.java
Patch:
@@ -97,7 +97,7 @@ public DataStream<RowData> createSource() {
             rowConverter = new HbaseRowConverter(hbaseSchema, nullStringLiteral);
         }
 
-        builder.setRowConverter(rowConverter,useAbstractBaseColumn);
+        builder.setRowConverter(rowConverter, useAbstractBaseColumn);
         return createInput(builder.finish());
     }
 }

File: chunjun-connectors/chunjun-connector-jdbc-base/src/main/java/com/dtstack/chunjun/connector/jdbc/util/SqlUtil.java
Patch:
@@ -57,7 +57,7 @@ public static String buildQuerySplitRangeSql(JdbcConf jdbcConf, JdbcDialect jdbc
 
         } else {
             // rowNum字段作为splitKey
-            if (addRowNumColumn(jdbcConf.getSplitPk())) {
+            if (isRowNumSplitKey(jdbcConf.getSplitPk())) {
                 StringBuilder customTableBuilder =
                         new StringBuilder(128)
                                 .append("SELECT ")
@@ -98,7 +98,7 @@ public static String buildQuerySqlBySplit(
         // customSql为空 且 splitPk是ROW_NUMBER()
         boolean flag =
                 StringUtils.isBlank(jdbcConf.getCustomSql())
-                        && SqlUtil.addRowNumColumn(jdbcConf.getSplitPk());
+                        && SqlUtil.isRowNumSplitKey(jdbcConf.getSplitPk());
 
         String splitFilter = null;
         if (jdbcInputSplit.getTotalNumberOfSplits() > 1) {
@@ -227,7 +227,7 @@ public static String buildOrderSql(
     }
 
     /* 是否添加自定义函数column 作为分片key ***/
-    public static boolean addRowNumColumn(String splitKey) {
+    public static boolean isRowNumSplitKey(String splitKey) {
         return StringUtils.isNotBlank(splitKey)
                 && splitKey.contains(ConstantValue.LEFT_PARENTHESIS_SYMBOL);
     }

File: chunjun-core/src/main/java/com/dtstack/chunjun/element/column/ArrayColumn.java
Patch:
@@ -42,8 +42,7 @@ public class ArrayColumn extends AbstractBaseColumn {
     protected Array data;
 
     public ArrayColumn(final Array data) {
-        super(data);
-        this.data = data;
+        super(data, data.toString().length());
     }
 
     @Override

File: chunjun-connectors/chunjun-connector-doris/src/main/java/com/dtstack/chunjun/connector/doris/sink/DorisHttpOutputFormat.java
Patch:
@@ -88,7 +88,7 @@ protected void writeSingleRecordInternal(RowData rowData) throws WriteRecordExce
         try {
             client.process(rowData, columns, rowConverter);
         } catch (Exception e) {
-            throw new WriteRecordException("", e);
+            throw new WriteRecordException("", e, 0, rowData);
         }
     }
 

File: chunjun-connectors/chunjun-connector-ftp/src/main/java/com/dtstack/chunjun/connector/ftp/sink/FtpOutputFormat.java
Patch:
@@ -122,7 +122,7 @@ public void writeSingleRecordToFile(RowData rowData) throws WriteRecordException
             rowsOfCurrentBlock++;
             lastRow = rowData;
         } catch (Exception ex) {
-            throw new WriteRecordException(ex.getMessage(), ex);
+            throw new WriteRecordException(ex.getMessage(), ex, 0, rowData);
         }
     }
 

File: chunjun-connectors/chunjun-connector-doris/src/main/java/com/dtstack/chunjun/connector/doris/sink/DorisHttpOutputFormat.java
Patch:
@@ -88,7 +88,7 @@ protected void writeSingleRecordInternal(RowData rowData) throws WriteRecordExce
         try {
             client.process(rowData, columns, rowConverter);
         } catch (Exception e) {
-            throw new WriteRecordException("", e);
+            throw new WriteRecordException("", e, 0, rowData);
         }
     }
 

File: chunjun-connectors/chunjun-connector-ftp/src/main/java/com/dtstack/chunjun/connector/ftp/sink/FtpOutputFormat.java
Patch:
@@ -122,7 +122,7 @@ public void writeSingleRecordToFile(RowData rowData) throws WriteRecordException
             rowsOfCurrentBlock++;
             lastRow = rowData;
         } catch (Exception ex) {
-            throw new WriteRecordException(ex.getMessage(), ex);
+            throw new WriteRecordException(ex.getMessage(), ex, 0, rowData);
         }
     }
 

File: chunjun-connectors/chunjun-connector-doris/src/main/java/com/dtstack/chunjun/connector/doris/options/DorisKeys.java
Patch:
@@ -36,6 +36,8 @@ public final class DorisKeys {
 
     public static final String FLUSH_INTERNAL_MS_KEY = "flushIntervalMills";
 
+    public static final String WAITRETRIES_MS_KEY = "waitRetryMills";
+
     public static final String MAX_RETRIES_KEY = "maxRetries";
 
     public static final String DATABASE_KEY = "database";

File: chunjun-connectors/chunjun-connector-doris/src/main/java/com/dtstack/chunjun/connector/doris/sink/DorisHttpOutputFormat.java
Patch:
@@ -70,7 +70,8 @@ private String getBackend() throws IOException {
     @Override
     public void open(int taskNumber, int numTasks) throws IOException {
         DorisStreamLoad dorisStreamLoad = new DorisStreamLoad(options);
-        client = new DorisLoadClient(dorisStreamLoad, options, getBackend());
+        dorisStreamLoad.replaceBackend();
+        client = new DorisLoadClient(dorisStreamLoad, options);
         super.open(taskNumber, numTasks);
     }
 

File: chunjun-connectors/chunjun-connector-jdbc-base/src/main/java/com/dtstack/chunjun/connector/jdbc/lookup/provider/DruidDataSourceProvider.java
Patch:
@@ -44,7 +44,8 @@ public DataSource getDataSource(JsonObject config) {
             String key = entry.getKey();
             if (!"provider_class".equals(key)) {
                 String formattedName = CaseFormat.LOWER_HYPHEN.to(CaseFormat.LOWER_CAMEL, key);
-                props.setProperty(formattedName, entry.getValue().toString());
+                props.setProperty(
+                        formattedName, entry.getValue() == null ? "" : entry.getValue().toString());
             }
         }
         dataSource.configFromPropety(props);

File: chunjun-connectors/chunjun-connector-doris/src/main/java/com/dtstack/chunjun/connector/doris/sink/DorisSinkFactory.java
Patch:
@@ -137,7 +137,9 @@ public DorisSinkFactory(SyncConf syncConf) {
                         .setLoadProperties(
                                 parameter.getProperties(LOAD_PROPERTIES_KEY, new Properties()))
                         .setPassword(parameter.getStringVal(PASSWORD_KEY, ""))
-                        .setNameMapped(syncConf.getNameMappingConf() != null)
+                        .setNameMapped(
+                                syncConf.getNameMappingConf() != null
+                                        && !syncConf.getNameMappingConf().isEmpty())
                         .setWriteMode(
                                 parameter.getStringVal(WRITE_MODE_KEY, DORIS_WRITE_MODE_DEFAULT))
                         .setUsername(parameter.getStringVal(USER_NAME_KEY))

File: chunjun-connectors/chunjun-connector-stream/src/main/java/com/dtstack/chunjun/connector/stream/sink/StreamOutputFormat.java
Patch:
@@ -55,7 +55,8 @@ protected void writeSingleRecordInternal(RowData rowData) throws WriteRecordExce
             RowData row =
                     (RowData)
                             rowConverter.toExternal(
-                                    rowData, new GenericRowData(rowData.getArity()));
+                                    rowData,
+                                    new GenericRowData(rowData.getRowKind(), rowData.getArity()));
             if (streamConf.getPrint()) {
                 TablePrintUtil.printTable(row, getFieldNames(rowData));
             }

File: chunjun-core/src/main/java/com/dtstack/chunjun/enums/ColumnType.java
Patch:
@@ -99,7 +99,8 @@ public enum ColumnType {
     TIME,
     YEAR,
     BIT,
-    OBJECT;
+    OBJECT,
+    TIMESTAMPTZ;
 
     public static List<ColumnType> TIME_TYPE = Arrays.asList(DATE, DATETIME, TIME, TIMESTAMP);
 

File: flinkx-connectors/flinkx-connector-mysql/src/main/java/com/dtstack/flinkx/connector/mysql/converter/MysqlRawTypeConverter.java
Patch:
@@ -43,6 +43,7 @@ public static DataType apply(String type) {
             case "FLOAT UNSIGNED":
                 return DataTypes.FLOAT();
             case "DECIMAL":
+            case "DECIMAL128":
             case "DECIMAL UNSIGNED":
             case "NUMERIC":
                 return DataTypes.DECIMAL(38, 18);

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/PropertiesUtil.java
Patch:
@@ -97,6 +97,8 @@ public static void initFlinkxCommonConf(FlinkxCommonConf flinkxCommonConf, SyncC
                             : syncConf.getRemotePluginPath());
             flinkxCommonConf.setMetricPluginName(syncConf.getMetricPluginConf().getPluginName());
             flinkxCommonConf.setMetricProps(syncConf.getMetricPluginConf().getPluginProp());
+            flinkxCommonConf.setRowSizeCalculatorType(
+                    syncConf.getMetricPluginConf().getRowSizeCalculatorType());
         }
     }
 }

File: flinkx-connectors/flinkx-connector-kafka/src/main/java/com/dtstack/flinkx/connector/kafka/converter/KafkaColumnConverter.java
Patch:
@@ -183,7 +183,7 @@ public byte[] toExternal(RowData rowData, byte[] output) throws Exception {
             } else {
                 List<String> values = new ArrayList<>(row.getArity());
                 for (int i = 0; i < row.getArity(); i++) {
-                    values.add(row.getField(i).asString());
+                    values.add(row.getField(i) == null ? "" : row.getField(i).asString());
                 }
                 map = decode.decode(String.join(",", values));
             }

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/source/JdbcInputFormat.java
Patch:
@@ -255,7 +255,6 @@ public RowData nextRecordInternal(RowData rowData) throws ReadRecordException {
                 switch (type) {
                     case DATETIME:
                     case TIMESTAMP:
-                    case DATETIME:
                     case TIMESTAMPTZ:
                     case DATE:
                         obj = resultSet.getTimestamp(jdbcConf.getIncreColumn()).getTime();

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/source/JdbcInputFormat.java
Patch:
@@ -253,6 +253,7 @@ public RowData nextRecordInternal(RowData rowData) throws ReadRecordException {
             if (isUpdateLocation) {
                 Object obj;
                 switch (type) {
+                    case DATETIME:
                     case TIMESTAMP:
                     case DATETIME:
                     case TIMESTAMPTZ:

File: flinkx-core/src/main/java/com/dtstack/flinkx/enums/ColumnType.java
Patch:
@@ -63,6 +63,7 @@ public enum ColumnType {
     SHORT,
     INTEGER,
     NUMBER,
+    NUMERIC,
 
     /** double type */
     DOUBLE,

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/source/JdbcSourceFactory.java
Patch:
@@ -93,7 +93,9 @@ ConnectionConf.class, new ConnectionAdapter("SourceConnectionConf"))
         }
         initIncrementConfig(jdbcConf);
         super.initFlinkxCommonConf(jdbcConf);
-        rebuildJdbcConf();
+        if(StringUtils.isBlank(jdbcConf.getCustomSql())){
+            rebuildJdbcConf();
+        }
     }
 
     protected Class<? extends JdbcConf> getConfClass() {

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/sink/JdbcOutputFormatBuilder.java
Patch:
@@ -54,9 +54,7 @@ protected void checkFormat() {
         if (StringUtils.isBlank(jdbcConf.getUsername())) {
             sb.append("No username supplied;\n");
         }
-        if (StringUtils.isBlank(jdbcConf.getPassword())) {
-            sb.append("No password supplied;\n");
-        }
+
         if (StringUtils.isBlank(jdbcConf.getJdbcUrl())) {
             sb.append("No jdbc url supplied;\n");
         }

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/source/JdbcInputFormatBuilder.java
Patch:
@@ -61,9 +61,7 @@ protected void checkFormat() {
         if (StringUtils.isBlank(conf.getUsername())) {
             sb.append("No username supplied;\n");
         }
-        if (StringUtils.isBlank(conf.getPassword())) {
-            sb.append("No password supplied;\n");
-        }
+
         if (StringUtils.isBlank(conf.getJdbcUrl())) {
             sb.append("No jdbc url supplied;\n");
         }

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/table/JdbcDynamicTableFactory.java
Patch:
@@ -301,9 +301,9 @@ public Set<ConfigOption<?>> optionalOptions() {
     protected void validateConfigOptions(ReadableConfig config) {
         String jdbcUrl = config.get(URL);
         final Optional<JdbcDialect> dialect = Optional.of(getDialect());
-        checkState(true, "Cannot handle such jdbc url: " + jdbcUrl);
+        checkState(dialect.get().canHandle(jdbcUrl), "Cannot handle such jdbc url: " + jdbcUrl);
 
-        checkAllOrNone(config, new ConfigOption[] {USERNAME, PASSWORD});
+        checkAllOrNone(config, new ConfigOption[] {USERNAME});
 
         if (config.getOptional(SCAN_POLLING_INTERVAL).isPresent()
                 && config.getOptional(SCAN_POLLING_INTERVAL).get() > 0) {
@@ -342,7 +342,7 @@ protected void validateConfigOptions(ReadableConfig config) {
 
     protected void checkAllOrNone(ReadableConfig config, ConfigOption<?>[] configOptions) {
         int presentCount = 0;
-        for (ConfigOption configOption : configOptions) {
+        for (ConfigOption<?> configOption : configOptions) {
             if (config.getOptional(configOption).isPresent()) {
                 presentCount++;
             }

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/sink/JdbcOutputFormatBuilder.java
Patch:
@@ -54,9 +54,7 @@ protected void checkFormat() {
         if (StringUtils.isBlank(jdbcConf.getUsername())) {
             sb.append("No username supplied;\n");
         }
-        if (StringUtils.isBlank(jdbcConf.getPassword())) {
-            sb.append("No password supplied;\n");
-        }
+
         if (StringUtils.isBlank(jdbcConf.getJdbcUrl())) {
             sb.append("No jdbc url supplied;\n");
         }

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/source/JdbcInputFormatBuilder.java
Patch:
@@ -61,9 +61,7 @@ protected void checkFormat() {
         if (StringUtils.isBlank(conf.getUsername())) {
             sb.append("No username supplied;\n");
         }
-        if (StringUtils.isBlank(conf.getPassword())) {
-            sb.append("No password supplied;\n");
-        }
+
         if (StringUtils.isBlank(conf.getJdbcUrl())) {
             sb.append("No jdbc url supplied;\n");
         }

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/table/JdbcDynamicTableFactory.java
Patch:
@@ -301,9 +301,9 @@ public Set<ConfigOption<?>> optionalOptions() {
     protected void validateConfigOptions(ReadableConfig config) {
         String jdbcUrl = config.get(URL);
         final Optional<JdbcDialect> dialect = Optional.of(getDialect());
-        checkState(true, "Cannot handle such jdbc url: " + jdbcUrl);
+        checkState(dialect.get().canHandle(jdbcUrl), "Cannot handle such jdbc url: " + jdbcUrl);
 
-        checkAllOrNone(config, new ConfigOption[] {USERNAME, PASSWORD});
+        checkAllOrNone(config, new ConfigOption[] {USERNAME});
 
         if (config.getOptional(SCAN_POLLING_INTERVAL).isPresent()
                 && config.getOptional(SCAN_POLLING_INTERVAL).get() > 0) {
@@ -342,7 +342,7 @@ protected void validateConfigOptions(ReadableConfig config) {
 
     protected void checkAllOrNone(ReadableConfig config, ConfigOption<?>[] configOptions) {
         int presentCount = 0;
-        for (ConfigOption configOption : configOptions) {
+        for (ConfigOption<?> configOption : configOptions) {
             if (config.getOptional(configOption).isPresent()) {
                 presentCount++;
             }

File: flinkx-connectors/flinkx-connector-influxdb/src/main/java/com/dtstack/flinkx/connector/influxdb/sink/InfluxdbSinkFactory.java
Patch:
@@ -27,6 +27,8 @@ public InfluxdbSinkFactory(SyncConf syncConf) {
         Gson gson = new GsonBuilder().create();
         GsonUtil.setTypeAdapter(gson);
         this.influxdbConfig = gson.fromJson(gson.toJson(parameter), InfluxdbSinkConfig.class);
+        Object writeMode = parameter.get("writeMode");
+        influxdbConfig.setWriteMode(writeMode == null ? null : writeMode.toString());
         influxdbConfig.setColumn(syncConf.getWriter().getFieldList());
         super.initFlinkxCommonConf(influxdbConfig);
     }

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/PluginUtil.java
Patch:
@@ -346,7 +346,7 @@ public static void registerPluginUrlToCachedFile(
                     e);
         }
 
-        config.setSyncJarList(setPipelineOptionsToEnvConfig(env, urlList, options));
+        config.setSyncJarList(setPipelineOptionsToEnvConfig(env, urlList, options.getMode()));
     }
 
     /**
@@ -358,7 +358,7 @@ public static void registerPluginUrlToCachedFile(
      */
     @SuppressWarnings("all")
     public static List<String> setPipelineOptionsToEnvConfig(
-            StreamExecutionEnvironment env, List<String> urlList, Options options) {
+            StreamExecutionEnvironment env, List<String> urlList, String executionMode) {
         try {
             Configuration configuration =
                     (Configuration)
@@ -370,7 +370,6 @@ public static List<String> setPipelineOptionsToEnvConfig(
             jarList.addAll(urlList);
 
             List<String> pipelineJars = new ArrayList();
-            String executionMode = options.getMode();
             LOG.info("Flinkx executionMode: " + executionMode);
             if (ClusterMode.getByName(executionMode) == ClusterMode.kubernetesApplication) {
                 for (String jarUrl : jarList) {

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/PluginUtil.java
Patch:
@@ -346,7 +346,7 @@ public static void registerPluginUrlToCachedFile(
                     e);
         }
 
-        config.setSyncJarList(setPipelineOptionsToEnvConfig(env, urlList, options.getMode()));
+        config.setSyncJarList(setPipelineOptionsToEnvConfig(env, urlList, options));
     }
 
     /**
@@ -358,7 +358,7 @@ public static void registerPluginUrlToCachedFile(
      */
     @SuppressWarnings("all")
     public static List<String> setPipelineOptionsToEnvConfig(
-            StreamExecutionEnvironment env, List<String> urlList, String executionMode) {
+            StreamExecutionEnvironment env, List<String> urlList, Options options) {
         try {
             Configuration configuration =
                     (Configuration)
@@ -370,6 +370,7 @@ public static List<String> setPipelineOptionsToEnvConfig(
             jarList.addAll(urlList);
 
             List<String> pipelineJars = new ArrayList();
+            String executionMode = options.getMode();
             LOG.info("Flinkx executionMode: " + executionMode);
             if (ClusterMode.getByName(executionMode) == ClusterMode.kubernetesApplication) {
                 for (String jarUrl : jarList) {

File: flinkx-connectors/flinkx-connector-gbase/src/main/java/com/dtstack/flinkx/connector/gBase/converter/GBaseRawTypeConverter.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.gbase.converter;
+package com.dtstack.flinkx.connector.gBase.converter;
 
 import com.dtstack.flinkx.throwable.UnsupportedTypeException;
 

File: flinkx-connectors/flinkx-connector-gbase/src/main/java/com/dtstack/flinkx/connector/gBase/sink/GBaseSinkFactory.java
Patch:
@@ -16,10 +16,10 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.gbase.sink;
+package com.dtstack.flinkx.connector.gBase.sink;
 
 import com.dtstack.flinkx.conf.SyncConf;
-import com.dtstack.flinkx.connector.gbase.dialect.GBaseDialect;
+import com.dtstack.flinkx.connector.gBase.dialect.GBaseDialect;
 import com.dtstack.flinkx.connector.jdbc.sink.JdbcSinkFactory;
 
 /**

File: flinkx-connectors/flinkx-connector-gbase/src/main/java/com/dtstack/flinkx/connector/gBase/source/GBaseSourceFactory.java
Patch:
@@ -16,10 +16,10 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.gbase.source;
+package com.dtstack.flinkx.connector.gBase.source;
 
 import com.dtstack.flinkx.conf.SyncConf;
-import com.dtstack.flinkx.connector.gbase.dialect.GBaseDialect;
+import com.dtstack.flinkx.connector.gBase.dialect.GBaseDialect;
 import com.dtstack.flinkx.connector.jdbc.source.JdbcSourceFactory;
 
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;

File: flinkx-connectors/flinkx-connector-gbase/src/main/java/com/dtstack/flinkx/connector/gBase/table/GBaseDynamicTableFactory.java
Patch:
@@ -16,9 +16,9 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.gbase.table;
+package com.dtstack.flinkx.connector.gBase.table;
 
-import com.dtstack.flinkx.connector.gbase.dialect.GBaseDialect;
+import com.dtstack.flinkx.connector.gBase.dialect.GBaseDialect;
 import com.dtstack.flinkx.connector.jdbc.dialect.JdbcDialect;
 import com.dtstack.flinkx.connector.jdbc.table.JdbcDynamicTableFactory;
 

File: flinkx-connectors/flinkx-connector-gbase/src/main/java/com/dtstack/flinkx/connector/gbase/dialect/GBaseDialect.java
Patch:
@@ -16,9 +16,9 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.gbase.dialect;
+package com.dtstack.flinkx.connector.gBase.dialect;
 
-import com.dtstack.flinkx.connector.gbase.converter.GBaseRawTypeConverter;
+import com.dtstack.flinkx.connector.gBase.converter.GBaseRawTypeConverter;
 import com.dtstack.flinkx.connector.jdbc.dialect.JdbcDialect;
 import com.dtstack.flinkx.converter.RawTypeConverter;
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/ConnectorNameConvertUtil.java
Patch:
@@ -39,7 +39,7 @@ public class ConnectorNameConvertUtil {
         connectorNameMap.put("restapi", new Tuple2<>("http", "http"));
         connectorNameMap.put("adbpostgresql", new Tuple2<>("postgresql", "postgresql"));
         connectorNameMap.put("dorisbatch", new Tuple2<>("doris", "doris"));
-        connectorNameMap.put("gbase", new Tuple2<>("gbase", "gBase"));
+        connectorNameMap.put("gbase", new Tuple2<>("gBase", "gBase"));
     }
 
     public static String convertClassPrefix(String originName) {

File: flinkx-connectors/flinkx-connector-hive/src/main/java/com/dtstack/flinkx/connector/hive/sink/HiveOutputFormat.java
Patch:
@@ -332,7 +332,6 @@ private BaseHdfsOutputFormat createHdfsOutputFormat(
             BaseHdfsOutputFormat outputFormat = (BaseHdfsOutputFormat) builder.finish();
             outputFormat.setFormatId(hiveTablePath);
             outputFormat.setDirtyDataManager(dirtyDataManager);
-            outputFormat.setErrorLimiter(errorLimiter);
             outputFormat.setRuntimeContext(getRuntimeContext());
             outputFormat.setRestoreState(formatStateMap.get(hiveTablePath));
             outputFormat.configure(parameters);

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/PropertiesUtil.java
Patch:
@@ -89,8 +89,6 @@ public static Properties propertiesTrim(Properties confProperties) {
      */
     public static void initFlinkxCommonConf(FlinkxCommonConf flinkxCommonConf, SyncConf syncConf) {
         flinkxCommonConf.setSpeedBytes(syncConf.getSpeed().getBytes());
-        flinkxCommonConf.setErrorRecord(syncConf.getErrorLimit().getRecord());
-        flinkxCommonConf.setErrorPercentage(syncConf.getErrorLimit().getPercentage());
         flinkxCommonConf.setSavePointPath(syncConf.getSavePointPath());
         if (syncConf.getMetricPluginConf() != null) {
             flinkxCommonConf.setMetricPluginRoot(

File: flinkx-core/src/test/java/com/dtstack/flinkx/util/DateUtilTest.java
Patch:
@@ -93,7 +93,7 @@ public void testColumnToTimestamp() {
         try {
             DateUtil.columnToTimestamp(true, null);
         } catch (Exception e) {
-            Assert.assertTrue(e instanceof IllegalArgumentException);
+            Assert.assertTrue(e instanceof UnsupportedOperationException);
         }
     }
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/sql/FunctionManager.java
Patch:
@@ -70,7 +70,7 @@ public static void registerScalarUDF(
                     Class.forName(classPath, false, classLoader)
                             .asSubclass(ScalarFunction.class)
                             .newInstance();
-            tableEnv.registerFunction(funcName, udfFunc);
+            tableEnv.createTemporaryFunction(funcName, udfFunc);
             logger.info("register scalar function:{} success.", funcName);
         } catch (Exception e) {
             logger.error("", e);
@@ -94,7 +94,7 @@ public static void registerTableUDF(
                             .asSubclass(TableFunction.class)
                             .newInstance();
 
-            ((StreamTableEnvironment) tableEnv).registerFunction(funcName, udtf);
+            tableEnv.createTemporaryFunction(funcName, udtf);
             logger.info("register table function:{} success.", funcName);
         } catch (Exception e) {
             logger.error("", e);
@@ -125,7 +125,7 @@ public static void registerAggregateUDF(
                     Class.forName(classPath, false, classLoader)
                             .asSubclass(AggregateFunction.class)
                             .newInstance();
-            ((StreamTableEnvironment) tableEnv).registerFunction(funcName, udaf);
+            tableEnv.createTemporaryFunction(funcName, udaf);
             logger.info("register Aggregate function:{} success.", funcName);
         } catch (Exception e) {
             logger.error("", e);

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/sink/JdbcOutputFormat.java
Patch:
@@ -217,7 +217,6 @@ protected void writeMultipleRecordsInternal() throws Exception {
             // 开启了cp，但是并没有使用2pc方式让下游数据可见
             if (Semantic.EXACTLY_ONCE == semantic) {
                 rowsOfCurrentTransaction += rows.size();
-                JdbcUtil.commit(dbConn);
             }
         } catch (Exception e) {
             LOG.warn(

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/sink/JdbcOutputFormat.java
Patch:
@@ -217,6 +217,7 @@ protected void writeMultipleRecordsInternal() throws Exception {
             // 开启了cp，但是并没有使用2pc方式让下游数据可见
             if (Semantic.EXACTLY_ONCE == semantic) {
                 rowsOfCurrentTransaction += rows.size();
+                JdbcUtil.commit(dbConn);
             }
         } catch (Exception e) {
             LOG.warn(

File: flinkx-connectors/flinkx-connector-sqlservercdc/src/main/java/com/dtstack/flinkx/connector/sqlservercdc/inputFormat/SqlServerCdcInputFormat.java
Patch:
@@ -37,7 +37,6 @@
 import org.apache.commons.lang.StringUtils;
 
 import java.sql.Connection;
-import java.sql.SQLException;
 import java.util.concurrent.BlockingQueue;
 import java.util.concurrent.ExecutorService;
 import java.util.concurrent.LinkedBlockingDeque;
@@ -192,5 +191,4 @@ public BlockingQueue<RowData> getQueue() {
     public void setSqlServerCdcConf(SqlServerCdcConf sqlserverCdcConf) {
         this.sqlserverCdcConf = sqlserverCdcConf;
     }
-
 }

File: flinkx-connectors/flinkx-connector-sqlservercdc/src/main/java/com/dtstack/flinkx/connector/sqlservercdc/table/SqlservercdcDynamicTableFactory.java
Patch:
@@ -105,7 +105,8 @@ private SqlServerCdcConf getSqlServerCdcConf(ReadableConfig config) {
         sqlServerCdcConf.setDatabaseName(config.get(SqlServerCdcOptions.DATABASE));
         sqlServerCdcConf.setTableList(Arrays.asList(config.get(SqlServerCdcOptions.TABLE)));
         sqlServerCdcConf.setAutoCommit(config.get(SqlServerCdcOptions.AUTO_COMMIT));
-        sqlServerCdcConf.setAutoResetConnection(config.get(SqlServerCdcOptions.AUTO_RESET_CONNECTION));
+        sqlServerCdcConf.setAutoResetConnection(
+                config.get(SqlServerCdcOptions.AUTO_RESET_CONNECTION));
 
         return sqlServerCdcConf;
     }

File: flinkx-connectors/flinkx-connector-sqlservercdc/src/main/java/com/dtstack/flinkx/connector/sqlservercdc/convert/SqlServerCdcColumnConverter.java
Patch:
@@ -29,6 +29,7 @@
 import com.dtstack.flinkx.element.column.BooleanColumn;
 import com.dtstack.flinkx.element.column.BytesColumn;
 import com.dtstack.flinkx.element.column.MapColumn;
+import com.dtstack.flinkx.element.column.NullColumn;
 import com.dtstack.flinkx.element.column.SqlDateColumn;
 import com.dtstack.flinkx.element.column.StringColumn;
 import com.dtstack.flinkx.element.column.TimeColumn;
@@ -250,7 +251,7 @@ private void parseColumnList(
                         (AbstractBaseColumn) converters.get(i).deserialize(data[i]);
                 columnList.add(column);
             } else {
-                columnList.add(null);
+                columnList.add(new NullColumn());
             }
         }
     }

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/source/JdbcInputFormat.java
Patch:
@@ -247,6 +247,7 @@ public RowData nextRecordInternal(RowData rowData) throws ReadRecordException {
                 switch (type) {
                     case TIMESTAMP:
                     case DATETIME:
+                    case TIMESTAMPTZ:
                     case DATE:
                         obj = resultSet.getTimestamp(jdbcConf.getIncreColumn()).getTime();
                         break;

File: flinkx-core/src/main/java/com/dtstack/flinkx/enums/ColumnType.java
Patch:
@@ -76,7 +76,9 @@ public enum ColumnType {
     DECIMAL,
     YEAR,
     BIT,
-    OBJECT;
+    OBJECT,
+    /** for postgresql */
+    TIMESTAMPTZ;
 
     public static List<ColumnType> TIME_TYPE = Arrays.asList(DATE, DATETIME, TIME, TIMESTAMP);
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/StringUtil.java
Patch:
@@ -358,6 +358,7 @@ public static String stringToTimestampStr(String location, ColumnType type) {
             switch (type) {
                 case TIMESTAMP:
                 case DATETIME:
+                case TIMESTAMPTZ:
                     return String.valueOf(Timestamp.valueOf(location).getTime());
                 case DATE:
                     return String.valueOf(

File: flinkx-connectors/flinkx-connector-sqlservercdc/src/main/java/com/dtstack/flinkx/connector/sqlservercdc/inputFormat/SqlServerCdcInputFormat.java
Patch:
@@ -37,6 +37,7 @@
 import org.apache.commons.lang.StringUtils;
 
 import java.sql.Connection;
+import java.sql.SQLException;
 import java.util.concurrent.BlockingQueue;
 import java.util.concurrent.ExecutorService;
 import java.util.concurrent.LinkedBlockingDeque;
@@ -95,7 +96,7 @@ protected void openInternal(InputSplit inputSplit) {
                             sqlserverCdcConf.getUrl(),
                             sqlserverCdcConf.getUsername(),
                             sqlserverCdcConf.getPassword());
-            conn.setAutoCommit(false);
+            conn.setAutoCommit(sqlserverCdcConf.isAutoCommit());
             SqlServerCdcUtil.changeDatabase(conn, sqlserverCdcConf.getDatabaseName());
 
             if (StringUtils.isNotBlank(sqlserverCdcConf.getLsn())) {
@@ -191,4 +192,5 @@ public BlockingQueue<RowData> getQueue() {
     public void setSqlServerCdcConf(SqlServerCdcConf sqlserverCdcConf) {
         this.sqlserverCdcConf = sqlserverCdcConf;
     }
+
 }

File: flinkx-connectors/flinkx-connector-sqlservercdc/src/main/java/com/dtstack/flinkx/connector/sqlservercdc/table/SqlservercdcDynamicTableFactory.java
Patch:
@@ -104,6 +104,8 @@ private SqlServerCdcConf getSqlServerCdcConf(ReadableConfig config) {
         sqlServerCdcConf.setPavingData(true);
         sqlServerCdcConf.setDatabaseName(config.get(SqlServerCdcOptions.DATABASE));
         sqlServerCdcConf.setTableList(Arrays.asList(config.get(SqlServerCdcOptions.TABLE)));
+        sqlServerCdcConf.setAutoCommit(config.get(SqlServerCdcOptions.AUTO_COMMIT));
+        sqlServerCdcConf.setAutoResetConnection(config.get(SqlServerCdcOptions.AUTO_RESET_CONNECTION));
 
         return sqlServerCdcConf;
     }

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/sink/JdbcOutputFormat.java
Patch:
@@ -289,8 +289,8 @@ protected void executeBatch(List<String> sqlList) {
                     String[] strings = sql.split(";");
                     for (String s : strings) {
                         if (StringUtils.isNotBlank(s)) {
-                            LOG.info("add sql to batch, sql = {}", sql);
-                            stmt.addBatch(sql);
+                            LOG.info("add sql to batch, sql = {}", s);
+                            stmt.addBatch(s);
                         }
                     }
                 }

File: flinkx-connectors/flinkx-connector-hdfs/src/main/java/com/dtstack/flinkx/connector/hdfs/sink/HdfsParquetOutputFormat.java
Patch:
@@ -130,7 +130,8 @@ protected void nextBlock() {
                         FileSystemUtil.getUGI(
                                 hdfsConf.getHadoopConfig(),
                                 hdfsConf.getDefaultFS(),
-                                getRuntimeContext().getDistributedCache());
+                                getRuntimeContext().getDistributedCache(),
+                                jobId);
                 ugi.doAs(
                         (PrivilegedAction<Object>)
                                 () -> {

File: flinkx-connectors/flinkx-connector-hdfs/src/main/java/com/dtstack/flinkx/connector/hdfs/source/HdfsParquetInputFormat.java
Patch:
@@ -104,7 +104,8 @@ public InputSplit[] createHdfsSplit(int minNumSplits) {
                 FileSystemUtil.getFileSystem(
                         hdfsConf.getHadoopConfig(),
                         hdfsConf.getDefaultFS(),
-                        PluginUtil.createDistributedCacheFromContextClassLoader())) {
+                        PluginUtil.createDistributedCacheFromContextClassLoader(),
+                        jobId)) {
             allFilePaths = getAllPartitionPath(hdfsConf.getPath(), fs, pathFilter);
         } catch (Exception e) {
             throw new FlinkxRuntimeException(e);

File: flinkx-core/src/main/java/com/dtstack/flinkx/sink/DirtyDataManager.java
Patch:
@@ -128,7 +128,7 @@ private String retrieveCategory(WriteRecordException ex) {
 
     public void open() {
         try {
-            FileSystem fs = FileSystemUtil.getFileSystem(config, null, distributedCache);
+            FileSystem fs = FileSystemUtil.getFileSystem(config, null, distributedCache, jobId);
             Path path = new Path(location);
             stream = fs.create(path, true);
         } catch (Exception e) {

File: flinkx-connectors/flinkx-connector-hive/src/main/java/com/dtstack/flinkx/connector/hive/sink/HiveOutputFormat.java
Patch:
@@ -272,7 +272,8 @@ private Pair<BaseHdfsOutputFormat, TableInfo> getHdfsOutputFormat(
                     tableInfo,
                     partitionPath,
                     connectionInfo,
-                    getRuntimeContext().getDistributedCache());
+                    getRuntimeContext().getDistributedCache(),
+                    jobId);
             String path = tableInfo.getPath() + File.separatorChar + partitionPath;
 
             outputFormat =
@@ -381,7 +382,7 @@ private TableInfo checkCreateTable(
             }
             tableInfo.setTablePath(tablePath);
             HiveUtil.createHiveTableWithTableInfo(
-                    tableInfo, connectionInfo, getRuntimeContext().getDistributedCache());
+                    tableInfo, connectionInfo, getRuntimeContext().getDistributedCache(), jobId);
             tableCacheMap.put(tablePath, tableInfo);
         }
         return tableInfo;

File: flinkx-connectors/flinkx-connector-solr/src/main/java/com/dtstack/flinkx/connector/solr/sink/SolrOutputFormat.java
Patch:
@@ -77,7 +77,7 @@ protected void writeMultipleRecordsInternal() throws WriteRecordException {
     protected void openInternal(int taskNumber, int numTasks) throws IOException {
         solrClientWrapper =
                 new CloudSolrClientKerberosWrapper(
-                        solrConf, getRuntimeContext().getDistributedCache());
+                        solrConf, getRuntimeContext().getDistributedCache(), jobId);
         solrClientWrapper.init();
     }
 

File: flinkx-connectors/flinkx-connector-solr/src/main/java/com/dtstack/flinkx/connector/solr/source/SolrInputFormat.java
Patch:
@@ -72,7 +72,7 @@ protected InputSplit[] createInputSplitsInternal(int splitNum) throws Exception
     protected void openInternal(InputSplit inputSplit) {
         solrClientWrapper =
                 new CloudSolrClientKerberosWrapper(
-                        solrConf, getRuntimeContext().getDistributedCache());
+                        solrConf, getRuntimeContext().getDistributedCache(), jobId);
         solrClientWrapper.init();
 
         GenericInputSplit genericInputSplit = (GenericInputSplit) inputSplit;

File: flinkx-connectors/flinkx-connector-inceptor/src/main/java/com/dtstack/flinkx/connector/inceptor/util/InceptorDbUtil.java
Patch:
@@ -92,7 +92,7 @@ private static Connection getConnectionWithKerberos(
 
         String principal =
                 KerberosUtil.getPrincipal(connectionInfo.getHadoopConfig(), keytabFileName);
-        KerberosUtil.loadKrb5Conf(connectionInfo.getHadoopConfig(), distributedCache);
+        KerberosUtil.loadKrb5Conf(connectionInfo.getHadoopConfig(), distributedCache, jobId);
 
         Configuration conf =
                 FileSystemUtil.getConfiguration(connectionInfo.getHadoopConfig(), null);

File: flinkx-connectors/flinkx-connector-hbase-1.4/src/main/java/com/dtstack/flinkx/connector/hbase14/sink/HBaseOutputFormat.java
Patch:
@@ -108,7 +108,9 @@ protected void writeSingleRecordInternal(RowData rawRecord) throws WriteRecordEx
     public void openInternal(int taskNumber, int numTasks) throws IOException {
         boolean openKerberos = HBaseConfigUtils.isEnableKerberos(hbaseConfig);
         if (openKerberos) {
-            UserGroupInformation ugi = HBaseHelper.getUgi(hbaseConfig);
+            UserGroupInformation ugi =
+                    HBaseHelper.getUgi(
+                            hbaseConfig, getRuntimeContext().getDistributedCache(), jobId);
             ugi.doAs(
                     (PrivilegedAction<Object>)
                             () -> {

File: flinkx-connectors/flinkx-connector-greenplum/src/main/java/com/dtstack/flinkx/connector/greenplum/sink/GreenplumSinkFactory.java
Patch:
@@ -21,15 +21,16 @@
 import com.dtstack.flinkx.conf.SyncConf;
 import com.dtstack.flinkx.connector.greenplum.dialect.GreenplumDialect;
 import com.dtstack.flinkx.connector.jdbc.sink.JdbcSinkFactory;
+import com.dtstack.flinkx.connector.postgresql.sink.PostgresqlSinkFactory;
 
 /**
  * company www.dtstack.com
  *
  * @author jier
  */
-public class GreenplumSinkFactory extends JdbcSinkFactory {
+public class GreenplumSinkFactory extends PostgresqlSinkFactory {
 
     public GreenplumSinkFactory(SyncConf syncConf) {
-        super(syncConf, new GreenplumDialect());
+        super(syncConf);
     }
 }

File: flinkx-connectors/flinkx-connector-binlog/src/main/java/com/dtstack/flinkx/connector/binlog/inputformat/BinlogInputFormat.java
Patch:
@@ -161,6 +161,7 @@ protected MysqlEventParser getController(
         controller.setIsGTIDMode(binlogConf.isGTIDMode());
 
         controller.setAlarmHandler(new BinlogAlarmHandler());
+        controller.setTransactionSize(binlogConf.getTransactionSize());
 
         controller.setEventSink(binlogEventSink);
 

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/source/JdbcInputFormat.java
Patch:
@@ -244,6 +244,7 @@ public RowData nextRecordInternal(RowData rowData) throws ReadRecordException {
                 Object obj;
                 switch (type) {
                     case TIMESTAMP:
+                    case DATETIME:
                     case DATE:
                         obj = resultSet.getTimestamp(jdbcConf.getIncreColumn()).getTime();
                         break;
@@ -609,6 +610,7 @@ protected void queryForPolling(String startLocation) throws SQLException {
         boolean isNumber = StringUtils.isNumeric(startLocation);
         switch (type) {
             case TIMESTAMP:
+            case DATETIME:
                 Timestamp ts =
                         isNumber
                                 ? new Timestamp(Long.parseLong(startLocation))

File: flinkx-connectors/flinkx-connector-inceptor/src/main/java/com/dtstack/flinkx/connector/inceptor/table/InceptorDynamicTableFactory.java
Patch:
@@ -177,7 +177,7 @@ protected InceptorConf getSinkConnectionConf(
         List<String> keyFields =
                 schema.getPrimaryKey().map(UniqueConstraint::getColumns).orElse(null);
         inceptorConf.setUniqueKey(keyFields);
-        resetTableInfo(inceptorConf);
+        rebuildJdbcConf(inceptorConf);
         JdbcUtil.putExtParam(inceptorConf);
         return inceptorConf;
     }
@@ -228,7 +228,7 @@ protected InceptorConf getSourceConnectionConf(ReadableConfig readableConfig) {
                             : readableConfig.get(SCAN_FETCH_SIZE));
         }
 
-        resetTableInfo(jdbcConf);
+        rebuildJdbcConf(jdbcConf);
         return jdbcConf;
     }
 

File: flinkx-connectors/flinkx-connector-sqlserver/src/main/java/com/dtstack/flinkx/connector/sqlserver/source/SqlserverSourceFactory.java
Patch:
@@ -51,13 +51,13 @@ protected JdbcInputFormatBuilder getBuilder() {
 
     /** table字段有可能是[schema].[table]格式 需要转换为对应的schema 和 table 字段* */
     @Override
-    protected void resetTableInfo() {
+    protected void rebuildJdbcConf() {
         if (jdbcConf.getTable().startsWith("[")
                 && jdbcConf.getTable().endsWith("]")
                 && StringUtils.isBlank(jdbcConf.getSchema())) {
             JdbcUtil.resetSchemaAndTable(jdbcConf, "\\[", "\\]");
         } else {
-            super.resetTableInfo();
+            super.rebuildJdbcConf();
         }
     }
 }

File: flinkx-connectors/flinkx-connector-sqlserver/src/main/java/com/dtstack/flinkx/connector/sqlserver/table/SqlserverDynamicTableFactory.java
Patch:
@@ -75,7 +75,7 @@ public DynamicTableSource createDynamicTableSource(Context context) {
 
     /** table字段有可能是[schema].[table]格式 需要转换为对应的schema 和 table 字段* */
     @Override
-    protected void resetTableInfo(JdbcConf jdbcConf) {
+    protected void rebuildJdbcConf(JdbcConf jdbcConf) {
         if (jdbcConf.getTable().startsWith("[")
                 && jdbcConf.getTable().endsWith("]")
                 && StringUtils.isBlank(jdbcConf.getSchema())) {

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/sink/PreparedStmtProxy.java
Patch:
@@ -164,7 +164,7 @@ public void getOrCreateFieldNamedPstmt(RowData row) throws ExecutionException {
             }
         } else {
             String key =
-                    getPstmtCacheKey(jdbcConf.getSchema(), jdbcConf.getTable(), row.getRowKind());
+                    getPstmtCacheKey(jdbcConf.getSchema(), jdbcConf.getTable(), RowKind.INSERT);
             DynamicPreparedStmt fieldNamedPreparedStatement =
                     pstmtCache.get(
                             key,

File: flinkx-local-test/src/main/java/com/dtstack/flinkx/local/test/LocalTest.java
Patch:
@@ -29,6 +29,7 @@
 import java.lang.reflect.Method;
 import java.net.URL;
 import java.net.URLClassLoader;
+import java.net.URLEncoder;
 import java.nio.charset.StandardCharsets;
 import java.nio.file.Files;
 import java.nio.file.Paths;

File: flinkx-connectors/flinkx-connector-influxdb/src/main/java/com/dtstack/flinkx/connector/influxdb/sink/InfluxdbSinkFactory.java
Patch:
@@ -27,6 +27,8 @@ public InfluxdbSinkFactory(SyncConf syncConf) {
         Gson gson = new GsonBuilder().create();
         GsonUtil.setTypeAdapter(gson);
         this.influxdbConfig = gson.fromJson(gson.toJson(parameter), InfluxdbSinkConfig.class);
+        Object writeMode = parameter.get("writeMode");
+        influxdbConfig.setWriteMode(writeMode == null ? null : writeMode.toString());
         influxdbConfig.setColumn(syncConf.getWriter().getFieldList());
         super.initFlinkxCommonConf(influxdbConfig);
     }

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/PluginUtil.java
Patch:
@@ -346,7 +346,7 @@ public static void registerPluginUrlToCachedFile(
                     e);
         }
 
-        config.setSyncJarList(setPipelineOptionsToEnvConfig(env, urlList, options));
+        config.setSyncJarList(setPipelineOptionsToEnvConfig(env, urlList, options.getMode()));
     }
 
     /**
@@ -358,7 +358,7 @@ public static void registerPluginUrlToCachedFile(
      */
     @SuppressWarnings("all")
     public static List<String> setPipelineOptionsToEnvConfig(
-            StreamExecutionEnvironment env, List<String> urlList, Options options) {
+            StreamExecutionEnvironment env, List<String> urlList, String executionMode) {
         try {
             Configuration configuration =
                     (Configuration)
@@ -370,7 +370,6 @@ public static List<String> setPipelineOptionsToEnvConfig(
             jarList.addAll(urlList);
 
             List<String> pipelineJars = new ArrayList();
-            String executionMode = options.getMode();
             LOG.info("Flinkx executionMode: " + executionMode);
             if (ClusterMode.getByName(executionMode) == ClusterMode.kubernetesApplication) {
                 for (String jarUrl : jarList) {

File: flinkx-local-test/src/main/java/com/dtstack/flinkx/local/test/LocalTest.java
Patch:
@@ -67,7 +67,7 @@ public static void main(String[] args) throws Exception {
             argsList.add("-jobType");
             argsList.add("sync");
             argsList.add("-job");
-            argsList.add(content);
+            argsList.add(URLEncoder.encode(content, StandardCharsets.UTF_8.name()));
 //            argsList.add("-flinkConfDir");
 //            argsList.add("/opt/dtstack/flink-1.12.2/conf/");
 //            argsList.add("-confProp");
@@ -101,7 +101,7 @@ public static void main(String[] args) throws Exception {
             argsList.add("-jobType");
             argsList.add("sql");
             argsList.add("-job");
-            argsList.add(content);
+            argsList.add(URLEncoder.encode(content, StandardCharsets.UTF_8.name()));
 //            argsList.add("-flinkConfDir");
 //            argsList.add("/opt/dtstack/flink-1.12.2/conf/");
             argsList.add("-jobName");

File: flinkx-core/src/main/java/com/dtstack/flinkx/Main.java
Patch:
@@ -313,6 +313,7 @@ private static void configStreamExecutionEnvironment(
             factoryHelper.setRemotePluginPath(options.getRemoteFlinkxDistDir());
             factoryHelper.setPluginLoadMode(options.getPluginLoadMode());
             factoryHelper.setEnv(env);
+            factoryHelper.setOptions(options);
 
             DirtyConf dirtyConf = DirtyConfUtil.parse(options);
             factoryHelper.registerCachedFile(

File: flinkx-connectors/flinkx-connector-hive/src/main/java/com/dtstack/flinkx/connector/hive/util/HiveDbUtil.java
Patch:
@@ -277,7 +277,8 @@ private static Connection getHiveConnection(String url, Properties prop) throws
             return connection;
         }
 
-        throw new RuntimeException("jdbcUrl is irregular");
+        throw new RuntimeException(
+                "jdbcUrl is irregular，the correct format is jdbc:hive2://ip:port/db");
     }
 
     public static String parseIpAndPort(String url) {
@@ -287,7 +288,7 @@ public static String parseIpAndPort(String url) {
             addr = matcher.group(HOST_KEY) + ":" + matcher.group(PORT_KEY);
         } else {
             addr = url.substring(url.indexOf("//") + 2);
-            addr = addr.substring(0, addr.indexOf("/"));
+            addr = addr.substring(0, addr.contains("/") ? addr.indexOf("/") : addr.length());
         }
         return addr;
     }

File: flinkx-connectors/flinkx-connector-redis/src/main/java/com/dtstack/flinkx/connector/redis/sink/RedisOutputFormatBuilder.java
Patch:
@@ -76,10 +76,10 @@ protected void checkFormat() {
                             .append("\n");
                 }
             }
+        }
 
-            if (sb.length() > 0) {
-                throw new IllegalArgumentException("\n" + sb.toString());
-            }
+        if (sb.length() > 0) {
+            throw new IllegalArgumentException("\n" + sb);
         }
     }
 }

File: flinkx-connectors/flinkx-connector-redis/src/main/java/com/dtstack/flinkx/connector/redis/sink/RedisOutputFormatBuilder.java
Patch:
@@ -49,7 +49,7 @@ public void setRedisConf(RedisConf redisConf) {
     protected void checkFormat() {
         RedisConf redisConf = format.getRedisConf();
         StringBuilder sb = new StringBuilder(1024);
-        if (redisConf.getHostPort() == null) {
+        if (StringUtils.isBlank(redisConf.getHostPort())) {
             sb.append("No host and port supplied").append("\n");
         }
         if (redisConf.getType() == null) {

File: flinkx-connectors/flinkx-connector-redis/src/main/java/com/dtstack/flinkx/connector/redis/sink/RedisSinkFactory.java
Patch:
@@ -55,6 +55,7 @@ public RedisSinkFactory(SyncConf syncConf) {
         GsonUtil.setTypeAdapter(gson);
         redisConf =
                 gson.fromJson(gson.toJson(syncConf.getWriter().getParameter()), RedisConf.class);
+        redisConf.setColumn(syncConf.getWriter().getFieldList());
         super.initFlinkxCommonConf(redisConf);
     }
 

File: flinkx-connectors/flinkx-connector-gbase/src/main/java/com/dtstack/flinkx/connector/gbase/converter/GBaseRawTypeConverter.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.gBase.converter;
+package com.dtstack.flinkx.connector.gbase.converter;
 
 import com.dtstack.flinkx.throwable.UnsupportedTypeException;
 

File: flinkx-connectors/flinkx-connector-gbase/src/main/java/com/dtstack/flinkx/connector/gbase/dialect/GBaseDialect.java
Patch:
@@ -16,9 +16,9 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.gBase.dialect;
+package com.dtstack.flinkx.connector.gbase.dialect;
 
-import com.dtstack.flinkx.connector.gBase.converter.GBaseRawTypeConverter;
+import com.dtstack.flinkx.connector.gbase.converter.GBaseRawTypeConverter;
 import com.dtstack.flinkx.connector.jdbc.dialect.JdbcDialect;
 import com.dtstack.flinkx.converter.RawTypeConverter;
 

File: flinkx-connectors/flinkx-connector-gbase/src/main/java/com/dtstack/flinkx/connector/gbase/sink/GBaseSinkFactory.java
Patch:
@@ -16,10 +16,10 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.gBase.sink;
+package com.dtstack.flinkx.connector.gbase.sink;
 
 import com.dtstack.flinkx.conf.SyncConf;
-import com.dtstack.flinkx.connector.gBase.dialect.GBaseDialect;
+import com.dtstack.flinkx.connector.gbase.dialect.GBaseDialect;
 import com.dtstack.flinkx.connector.jdbc.sink.JdbcSinkFactory;
 
 /**

File: flinkx-connectors/flinkx-connector-gbase/src/main/java/com/dtstack/flinkx/connector/gbase/source/GBaseSourceFactory.java
Patch:
@@ -16,10 +16,10 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.gBase.source;
+package com.dtstack.flinkx.connector.gbase.source;
 
 import com.dtstack.flinkx.conf.SyncConf;
-import com.dtstack.flinkx.connector.gBase.dialect.GBaseDialect;
+import com.dtstack.flinkx.connector.gbase.dialect.GBaseDialect;
 import com.dtstack.flinkx.connector.jdbc.source.JdbcSourceFactory;
 
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;

File: flinkx-connectors/flinkx-connector-gbase/src/main/java/com/dtstack/flinkx/connector/gbase/table/GBaseDynamicTableFactory.java
Patch:
@@ -16,9 +16,9 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.gBase.table;
+package com.dtstack.flinkx.connector.gbase.table;
 
-import com.dtstack.flinkx.connector.gBase.dialect.GBaseDialect;
+import com.dtstack.flinkx.connector.gbase.dialect.GBaseDialect;
 import com.dtstack.flinkx.connector.jdbc.dialect.JdbcDialect;
 import com.dtstack.flinkx.connector.jdbc.table.JdbcDynamicTableFactory;
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/ConnectorNameConvertUtil.java
Patch:
@@ -39,7 +39,7 @@ public class ConnectorNameConvertUtil {
         connectorNameMap.put("restapi", new Tuple2<>("http", "http"));
         connectorNameMap.put("adbpostgresql", new Tuple2<>("postgresql", "postgresql"));
         connectorNameMap.put("dorisbatch", new Tuple2<>("doris", "doris"));
-        connectorNameMap.put("gbase", new Tuple2<>("gBase", "gBase"));
+        connectorNameMap.put("gbase", new Tuple2<>("gbase", "gBase"));
     }
 
     public static String convertClassPrefix(String originName) {

File: flinkx-connectors/flinkx-connector-gbase/src/main/java/com/dtstack/flinkx/connector/gBase/converter/GBaseRawTypeConverter.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.gbase.converter;
+package com.dtstack.flinkx.connector.gBase.converter;
 
 import com.dtstack.flinkx.throwable.UnsupportedTypeException;
 

File: flinkx-connectors/flinkx-connector-gbase/src/main/java/com/dtstack/flinkx/connector/gBase/sink/GBaseSinkFactory.java
Patch:
@@ -16,10 +16,10 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.gbase.sink;
+package com.dtstack.flinkx.connector.gBase.sink;
 
 import com.dtstack.flinkx.conf.SyncConf;
-import com.dtstack.flinkx.connector.gbase.dialect.GBaseDialect;
+import com.dtstack.flinkx.connector.gBase.dialect.GBaseDialect;
 import com.dtstack.flinkx.connector.jdbc.sink.JdbcSinkFactory;
 
 /**

File: flinkx-connectors/flinkx-connector-gbase/src/main/java/com/dtstack/flinkx/connector/gBase/source/GBaseSourceFactory.java
Patch:
@@ -16,10 +16,10 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.gbase.source;
+package com.dtstack.flinkx.connector.gBase.source;
 
 import com.dtstack.flinkx.conf.SyncConf;
-import com.dtstack.flinkx.connector.gbase.dialect.GBaseDialect;
+import com.dtstack.flinkx.connector.gBase.dialect.GBaseDialect;
 import com.dtstack.flinkx.connector.jdbc.source.JdbcSourceFactory;
 
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;

File: flinkx-connectors/flinkx-connector-gbase/src/main/java/com/dtstack/flinkx/connector/gBase/table/GBaseDynamicTableFactory.java
Patch:
@@ -16,9 +16,9 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.gbase.table;
+package com.dtstack.flinkx.connector.gBase.table;
 
-import com.dtstack.flinkx.connector.gbase.dialect.GBaseDialect;
+import com.dtstack.flinkx.connector.gBase.dialect.GBaseDialect;
 import com.dtstack.flinkx.connector.jdbc.dialect.JdbcDialect;
 import com.dtstack.flinkx.connector.jdbc.table.JdbcDynamicTableFactory;
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/ConnectorNameConvertUtil.java
Patch:
@@ -39,6 +39,7 @@ public class ConnectorNameConvertUtil {
         connectorNameMap.put("restapi", new Tuple2<>("http", "http"));
         connectorNameMap.put("adbpostgresql", new Tuple2<>("postgresql", "postgresql"));
         connectorNameMap.put("dorisbatch", new Tuple2<>("doris", "doris"));
+        connectorNameMap.put("gbase", new Tuple2<>("gBase", "gBase"));
     }
 
     public static String convertClassPrefix(String originName) {

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/sink/JdbcSinkFactory.java
Patch:
@@ -23,6 +23,7 @@
 import com.dtstack.flinkx.connector.jdbc.conf.ConnectionConf;
 import com.dtstack.flinkx.connector.jdbc.conf.JdbcConf;
 import com.dtstack.flinkx.connector.jdbc.dialect.JdbcDialect;
+import com.dtstack.flinkx.connector.jdbc.exclusion.FieldNameExclusionStrategy;
 import com.dtstack.flinkx.connector.jdbc.util.JdbcUtil;
 import com.dtstack.flinkx.converter.AbstractRowConverter;
 import com.dtstack.flinkx.converter.RawTypeConverter;
@@ -63,6 +64,8 @@ public JdbcSinkFactory(SyncConf syncConf, JdbcDialect jdbcDialect) {
                 new GsonBuilder()
                         .registerTypeAdapter(
                                 ConnectionConf.class, new ConnectionAdapter("SinkConnectionConf"))
+                        .addDeserializationExclusionStrategy(
+                                new FieldNameExclusionStrategy("column"))
                         .create();
         GsonUtil.setTypeAdapter(gson);
         jdbcConf = gson.fromJson(gson.toJson(syncConf.getWriter().getParameter()), JdbcConf.class);

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/source/JdbcSourceFactory.java
Patch:
@@ -24,6 +24,7 @@
 import com.dtstack.flinkx.connector.jdbc.conf.ConnectionConf;
 import com.dtstack.flinkx.connector.jdbc.conf.JdbcConf;
 import com.dtstack.flinkx.connector.jdbc.dialect.JdbcDialect;
+import com.dtstack.flinkx.connector.jdbc.exclusion.FieldNameExclusionStrategy;
 import com.dtstack.flinkx.connector.jdbc.util.JdbcUtil;
 import com.dtstack.flinkx.converter.AbstractRowConverter;
 import com.dtstack.flinkx.converter.RawTypeConverter;
@@ -68,6 +69,8 @@ public JdbcSourceFactory(
                 new GsonBuilder()
                         .registerTypeAdapter(
                                 ConnectionConf.class, new ConnectionAdapter("SourceConnectionConf"))
+                        .addDeserializationExclusionStrategy(
+                                new FieldNameExclusionStrategy("column"))
                         .create();
         GsonUtil.setTypeAdapter(gson);
         jdbcConf = gson.fromJson(gson.toJson(syncConf.getReader().getParameter()), getConfClass());

File: flinkx-connectors/flinkx-connector-clickhouse/src/main/java/com/dtstack/flinkx/connector/clickhouse/sink/ClickhouseSinkFactory.java
Patch:
@@ -36,6 +36,6 @@ public ClickhouseSinkFactory(SyncConf syncConf) {
 
     @Override
     protected JdbcOutputFormatBuilder getBuilder() {
-        return new JdbcOutputFormatBuilder(new ClickhouseOutputFormat());
+        return new ClickhouseOutputFormatBuilder(new ClickhouseOutputFormat());
     }
 }

File: flinkx-connectors/flinkx-connector-clickhouse/src/main/java/com/dtstack/flinkx/connector/clickhouse/source/ClickhouseSourceFactory.java
Patch:
@@ -46,6 +46,6 @@ public ClickhouseSourceFactory(SyncConf syncConf, StreamExecutionEnvironment env
 
     @Override
     protected JdbcInputFormatBuilder getBuilder() {
-        return new JdbcInputFormatBuilder(new ClickhouseInputFormat());
+        return new ClickhouseInputFormatBuilder(new ClickhouseInputFormat());
     }
 }

File: flinkx-connectors/flinkx-connector-kafka/src/main/java/com/dtstack/flinkx/connector/kafka/serialization/RowSerializationSchema.java
Patch:
@@ -88,8 +88,7 @@ public ProducerRecord<byte[], byte[]> serialize(RowData element, @Nullable Long
                     null,
                     valueSerialized);
         } catch (Exception e) {
-            // todo kafka比较特殊，这里直接记录脏数据。
-            LOG.error(e.getMessage());
+            dirtyManager.collect(element, e, null);
         }
         return null;
     }

File: flinkx-connectors/flinkx-connector-kafka/src/main/java/com/dtstack/flinkx/connector/kafka/sink/DynamicKafkaSerializationSchema.java
Patch:
@@ -254,7 +254,7 @@ public ProducerRecord<byte[], byte[]> serialize(RowData consumedRow, @Nullable L
                     valueSerialized,
                     readMetadata(consumedRow, KafkaDynamicSink.WritableMetadata.HEADERS));
         } catch (Exception e) {
-            dirtyManager.collect(consumedRow, e, null, runtimeContext);
+            dirtyManager.collect(consumedRow, e, null);
         }
         return null;
     }

File: flinkx-connectors/flinkx-connector-kafka/src/main/java/com/dtstack/flinkx/connector/kafka/source/DynamicKafkaDeserializationSchema.java
Patch:
@@ -215,8 +215,7 @@ public void deserialize(ConsumerRecord<byte[], byte[]> record, Collector<RowData
             }
             keyCollector.buffer.clear();
         } catch (Exception e) {
-            dirtyManager.collect(
-                    new String(record.value(), StandardCharsets.UTF_8), e, null, runtimeContext);
+            dirtyManager.collect(new String(record.value(), StandardCharsets.UTF_8), e, null);
         }
     }
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/dirty/manager/DirtyManager.java
Patch:
@@ -122,7 +122,7 @@ public LongCounter getFailedConsumedMetric() {
         return consumer.getFailedConsumed();
     }
 
-    public void collect(Object data, Throwable cause, String field, RuntimeContext runtimeContext) {
+    public void collect(Object data, Throwable cause, String field) {
         if (executor == null) {
             execute();
         }

File: flinkx-core/src/main/java/com/dtstack/flinkx/sink/format/BaseRichOutputFormat.java
Patch:
@@ -443,7 +443,7 @@ protected void writeSingleRecord(RowData rowData, LongCounter numWriteCounter) {
             writeSingleRecordInternal(rowData);
             numWriteCounter.add(1L);
         } catch (WriteRecordException e) {
-            dirtyManager.collect(e.getRowData(), e, null, getRuntimeContext());
+            dirtyManager.collect(e.getRowData(), e, null);
             if (LOG.isTraceEnabled()) {
                 LOG.trace(
                         "write error rowData, rowData = {}, e = {}",

File: flinkx-core/src/main/java/com/dtstack/flinkx/source/format/BaseRichInputFormat.java
Patch:
@@ -191,7 +191,7 @@ public RowData nextRecord(RowData rowData) {
         try {
             internalRow = nextRecordInternal(rowData);
         } catch (ReadRecordException e) {
-            dirtyManager.collect(e.getRowData(), e, null, getRuntimeContext());
+            dirtyManager.collect(e.getRowData(), e, null);
         }
         if (internalRow != null) {
             updateDuration();

File: flinkx-connectors/flinkx-connector-postgresql/src/main/java/com/dtstack/flinkx/connector/postgresql/dialect/PostgresqlDialect.java
Patch:
@@ -78,6 +78,7 @@ public Optional<String> getUpsertStatement(
                         .collect(Collectors.joining(", "));
         updateClause =
                 Arrays.stream(fieldNames)
+                        .filter(f -> !Arrays.asList(uniqueKeyFields).contains(f))
                         .map(f -> quoteIdentifier(f) + "=EXCLUDED." + quoteIdentifier(f))
                         .collect(Collectors.joining(", "));
 

File: flinkx-connectors/flinkx-connector-hive/src/main/java/com/dtstack/flinkx/connector/hive/util/HiveDbUtil.java
Patch:
@@ -277,7 +277,8 @@ private static Connection getHiveConnection(String url, Properties prop) throws
             return connection;
         }
 
-        throw new RuntimeException("jdbcUrl is irregular");
+        throw new RuntimeException(
+                "jdbcUrl is irregular，the correct format is jdbc:hive2://ip:port/db");
     }
 
     public static String parseIpAndPort(String url) {
@@ -287,7 +288,7 @@ public static String parseIpAndPort(String url) {
             addr = matcher.group(HOST_KEY) + ":" + matcher.group(PORT_KEY);
         } else {
             addr = url.substring(url.indexOf("//") + 2);
-            addr = addr.substring(0, addr.indexOf("/"));
+            addr = addr.substring(0, addr.contains("/") ? addr.indexOf("/") : addr.length());
         }
         return addr;
     }

File: flinkx-connectors/flinkx-connector-redis/src/main/java/com/dtstack/flinkx/connector/redis/sink/RedisSinkFactory.java
Patch:
@@ -55,6 +55,7 @@ public RedisSinkFactory(SyncConf syncConf) {
         GsonUtil.setTypeAdapter(gson);
         redisConf =
                 gson.fromJson(gson.toJson(syncConf.getWriter().getParameter()), RedisConf.class);
+        redisConf.setColumn(syncConf.getWriter().getFieldList());
         super.initFlinkxCommonConf(redisConf);
     }
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/ConnectorNameConvertUtil.java
Patch:
@@ -39,6 +39,7 @@ public class ConnectorNameConvertUtil {
         connectorNameMap.put("restapi", new Tuple2<>("http", "http"));
         connectorNameMap.put("adbpostgresql", new Tuple2<>("postgresql", "postgresql"));
         connectorNameMap.put("dorisbatch", new Tuple2<>("doris", "doris"));
+        connectorNameMap.put("gbase", new Tuple2<>("gbase", "gBase"));
     }
 
     public static String convertClassPrefix(String originName) {

File: flinkx-connectors/flinkx-connector-influxdb/src/main/java/com/dtstack/flinkx/connector/influxdb/source/InfluxdbInputFormat.java
Patch:
@@ -321,7 +321,7 @@ private void judgeSplitPkCompliant(
             int index = columnNames.indexOf(key.get());
             if (!StringUtils.equalsIgnoreCase("integer", columnTypes.get(index))) {
                 String errorMessage =
-                        "splitPk must spiltPk must be of type integer, but is actually "
+                        "spiltPk must be of type integer, but is actually "
                                 + columnTypes.get(index);
                 throw new IllegalArgumentException(errorMessage);
             }

File: flinkx-local-test/src/main/java/com/dtstack/flinkx/local/test/LocalTest.java
Patch:
@@ -67,7 +67,7 @@ public static void main(String[] args) throws Exception {
             argsList.add("-jobType");
             argsList.add("sync");
             argsList.add("-job");
-            argsList.add(content);
+            argsList.add(URLEncoder.encode(content, StandardCharsets.UTF_8.name()));
 //            argsList.add("-flinkConfDir");
 //            argsList.add("/opt/dtstack/flink-1.12.2/conf/");
 //            argsList.add("-confProp");
@@ -101,7 +101,7 @@ public static void main(String[] args) throws Exception {
             argsList.add("-jobType");
             argsList.add("sql");
             argsList.add("-job");
-            argsList.add(content);
+            argsList.add(URLEncoder.encode(content, StandardCharsets.UTF_8.name()));
 //            argsList.add("-flinkConfDir");
 //            argsList.add("/opt/dtstack/flink-1.12.2/conf/");
             argsList.add("-jobName");

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/PluginUtil.java
Patch:
@@ -346,7 +346,7 @@ public static void registerPluginUrlToCachedFile(
                     e);
         }
 
-        config.setSyncJarList(setPipelineOptionsToEnvConfig(env, urlList, options));
+        config.setSyncJarList(setPipelineOptionsToEnvConfig(env, urlList, options.getMode()));
     }
 
     /**
@@ -358,7 +358,7 @@ public static void registerPluginUrlToCachedFile(
      */
     @SuppressWarnings("all")
     public static List<String> setPipelineOptionsToEnvConfig(
-            StreamExecutionEnvironment env, List<String> urlList, Options options) {
+            StreamExecutionEnvironment env, List<String> urlList, String executionMode) {
         try {
             Configuration configuration =
                     (Configuration)
@@ -370,7 +370,6 @@ public static List<String> setPipelineOptionsToEnvConfig(
             jarList.addAll(urlList);
 
             List<String> pipelineJars = new ArrayList();
-            String executionMode = options.getMode();
             LOG.info("Flinkx executionMode: " + executionMode);
             if (ClusterMode.getByName(executionMode) == ClusterMode.kubernetesApplication) {
                 for (String jarUrl : jarList) {

File: flinkx-local-test/src/main/java/com/dtstack/flinkx/local/test/LocalTest.java
Patch:
@@ -67,7 +67,7 @@ public static void main(String[] args) throws Exception {
             argsList.add("-jobType");
             argsList.add("sync");
             argsList.add("-job");
-            argsList.add(content);
+            argsList.add(URLEncoder.encode(content, StandardCharsets.UTF_8.name()));
 //            argsList.add("-flinkConfDir");
 //            argsList.add("/opt/dtstack/flink-1.12.2/conf/");
 //            argsList.add("-confProp");
@@ -101,7 +101,7 @@ public static void main(String[] args) throws Exception {
             argsList.add("-jobType");
             argsList.add("sql");
             argsList.add("-job");
-            argsList.add(content);
+            argsList.add(URLEncoder.encode(content, StandardCharsets.UTF_8.name()));
 //            argsList.add("-flinkConfDir");
 //            argsList.add("/opt/dtstack/flink-1.12.2/conf/");
             argsList.add("-jobName");

File: flinkx-connectors/flinkx-connector-hive/src/main/java/com/dtstack/flinkx/connector/hive/util/HiveDbUtil.java
Patch:
@@ -277,7 +277,8 @@ private static Connection getHiveConnection(String url, Properties prop) throws
             return connection;
         }
 
-        throw new RuntimeException("jdbcUrl is irregular");
+        throw new RuntimeException(
+                "jdbcUrl is irregular，the correct format is jdbc:hive2://ip:port/db");
     }
 
     public static String parseIpAndPort(String url) {
@@ -287,7 +288,7 @@ public static String parseIpAndPort(String url) {
             addr = matcher.group(HOST_KEY) + ":" + matcher.group(PORT_KEY);
         } else {
             addr = url.substring(url.indexOf("//") + 2);
-            addr = addr.substring(0, addr.indexOf("/"));
+            addr = addr.substring(0, addr.contains("/") ? addr.indexOf("/") : addr.length());
         }
         return addr;
     }

File: flinkx-connectors/flinkx-connector-redis/src/main/java/com/dtstack/flinkx/connector/redis/sink/RedisOutputFormatBuilder.java
Patch:
@@ -76,10 +76,10 @@ protected void checkFormat() {
                             .append("\n");
                 }
             }
+        }
 
-            if (sb.length() > 0) {
-                throw new IllegalArgumentException("\n" + sb.toString());
-            }
+        if (sb.length() > 0) {
+            throw new IllegalArgumentException("\n" + sb);
         }
     }
 }

File: flinkx-connectors/flinkx-connector-redis/src/main/java/com/dtstack/flinkx/connector/redis/sink/RedisOutputFormatBuilder.java
Patch:
@@ -49,7 +49,7 @@ public void setRedisConf(RedisConf redisConf) {
     protected void checkFormat() {
         RedisConf redisConf = format.getRedisConf();
         StringBuilder sb = new StringBuilder(1024);
-        if (redisConf.getHostPort() == null) {
+        if (StringUtils.isBlank(redisConf.getHostPort())) {
             sb.append("No host and port supplied").append("\n");
         }
         if (redisConf.getType() == null) {

File: flinkx-connectors/flinkx-connector-hive/src/main/java/com/dtstack/flinkx/connector/hive/util/HiveDbUtil.java
Patch:
@@ -277,7 +277,8 @@ private static Connection getHiveConnection(String url, Properties prop) throws
             return connection;
         }
 
-        throw new RuntimeException("jdbcUrl is irregular");
+        throw new RuntimeException(
+                "jdbcUrl is irregular，the correct format is jdbc:hive2://ip:port/db");
     }
 
     public static String parseIpAndPort(String url) {
@@ -287,7 +288,7 @@ public static String parseIpAndPort(String url) {
             addr = matcher.group(HOST_KEY) + ":" + matcher.group(PORT_KEY);
         } else {
             addr = url.substring(url.indexOf("//") + 2);
-            addr = addr.substring(0, addr.indexOf("/"));
+            addr = addr.substring(0, addr.contains("/") ? addr.indexOf("/") : addr.length());
         }
         return addr;
     }

File: flinkx-connectors/flinkx-connector-redis/src/main/java/com/dtstack/flinkx/connector/redis/sink/RedisSinkFactory.java
Patch:
@@ -55,6 +55,7 @@ public RedisSinkFactory(SyncConf syncConf) {
         GsonUtil.setTypeAdapter(gson);
         redisConf =
                 gson.fromJson(gson.toJson(syncConf.getWriter().getParameter()), RedisConf.class);
+        redisConf.setColumn(syncConf.getWriter().getFieldList());
         super.initFlinkxCommonConf(redisConf);
     }
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/Main.java
Patch:
@@ -313,6 +313,7 @@ private static void configStreamExecutionEnvironment(
             factoryHelper.setRemotePluginPath(options.getRemoteFlinkxDistDir());
             factoryHelper.setPluginLoadMode(options.getPluginLoadMode());
             factoryHelper.setEnv(env);
+            factoryHelper.setOptions(options);
 
             DirtyConf dirtyConf = DirtyConfUtil.parse(options);
             factoryHelper.registerCachedFile(

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/ConnectorNameConvertUtil.java
Patch:
@@ -39,6 +39,7 @@ public class ConnectorNameConvertUtil {
         connectorNameMap.put("restapi", new Tuple2<>("http", "http"));
         connectorNameMap.put("adbpostgresql", new Tuple2<>("postgresql", "postgresql"));
         connectorNameMap.put("dorisbatch", new Tuple2<>("doris", "doris"));
+        connectorNameMap.put("gbase", new Tuple2<>("gbase", "gBase"));
     }
 
     public static String convertClassPrefix(String originName) {

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/sink/JdbcSinkFactory.java
Patch:
@@ -23,6 +23,7 @@
 import com.dtstack.flinkx.connector.jdbc.conf.ConnectionConf;
 import com.dtstack.flinkx.connector.jdbc.conf.JdbcConf;
 import com.dtstack.flinkx.connector.jdbc.dialect.JdbcDialect;
+import com.dtstack.flinkx.connector.jdbc.exclusion.FieldNameExclusionStrategy;
 import com.dtstack.flinkx.connector.jdbc.util.JdbcUtil;
 import com.dtstack.flinkx.converter.AbstractRowConverter;
 import com.dtstack.flinkx.converter.RawTypeConverter;
@@ -63,6 +64,8 @@ public JdbcSinkFactory(SyncConf syncConf, JdbcDialect jdbcDialect) {
                 new GsonBuilder()
                         .registerTypeAdapter(
                                 ConnectionConf.class, new ConnectionAdapter("SinkConnectionConf"))
+                        .addDeserializationExclusionStrategy(
+                                new FieldNameExclusionStrategy("column"))
                         .create();
         GsonUtil.setTypeAdapter(gson);
         jdbcConf = gson.fromJson(gson.toJson(syncConf.getWriter().getParameter()), JdbcConf.class);

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/source/JdbcSourceFactory.java
Patch:
@@ -24,6 +24,7 @@
 import com.dtstack.flinkx.connector.jdbc.conf.ConnectionConf;
 import com.dtstack.flinkx.connector.jdbc.conf.JdbcConf;
 import com.dtstack.flinkx.connector.jdbc.dialect.JdbcDialect;
+import com.dtstack.flinkx.connector.jdbc.exclusion.FieldNameExclusionStrategy;
 import com.dtstack.flinkx.connector.jdbc.util.JdbcUtil;
 import com.dtstack.flinkx.converter.AbstractRowConverter;
 import com.dtstack.flinkx.converter.RawTypeConverter;
@@ -68,6 +69,8 @@ public JdbcSourceFactory(
                 new GsonBuilder()
                         .registerTypeAdapter(
                                 ConnectionConf.class, new ConnectionAdapter("SourceConnectionConf"))
+                        .addDeserializationExclusionStrategy(
+                                new FieldNameExclusionStrategy("column"))
                         .create();
         GsonUtil.setTypeAdapter(gson);
         jdbcConf = gson.fromJson(gson.toJson(syncConf.getReader().getParameter()), getConfClass());

File: flinkx-connectors/flinkx-connector-gbase/src/main/java/com/dtstack/flinkx/connector/gbase/converter/GBaseRawTypeConverter.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.gBase.converter;
+package com.dtstack.flinkx.connector.gbase.converter;
 
 import com.dtstack.flinkx.throwable.UnsupportedTypeException;
 

File: flinkx-connectors/flinkx-connector-gbase/src/main/java/com/dtstack/flinkx/connector/gbase/dialect/GBaseDialect.java
Patch:
@@ -16,9 +16,9 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.gBase.dialect;
+package com.dtstack.flinkx.connector.gbase.dialect;
 
-import com.dtstack.flinkx.connector.gBase.converter.GBaseRawTypeConverter;
+import com.dtstack.flinkx.connector.gbase.converter.GBaseRawTypeConverter;
 import com.dtstack.flinkx.connector.jdbc.dialect.JdbcDialect;
 import com.dtstack.flinkx.converter.RawTypeConverter;
 

File: flinkx-connectors/flinkx-connector-gbase/src/main/java/com/dtstack/flinkx/connector/gbase/sink/GBaseSinkFactory.java
Patch:
@@ -16,10 +16,10 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.gBase.sink;
+package com.dtstack.flinkx.connector.gbase.sink;
 
 import com.dtstack.flinkx.conf.SyncConf;
-import com.dtstack.flinkx.connector.gBase.dialect.GBaseDialect;
+import com.dtstack.flinkx.connector.gbase.dialect.GBaseDialect;
 import com.dtstack.flinkx.connector.jdbc.sink.JdbcSinkFactory;
 
 /**

File: flinkx-connectors/flinkx-connector-gbase/src/main/java/com/dtstack/flinkx/connector/gbase/source/GBaseSourceFactory.java
Patch:
@@ -16,10 +16,10 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.gBase.source;
+package com.dtstack.flinkx.connector.gbase.source;
 
 import com.dtstack.flinkx.conf.SyncConf;
-import com.dtstack.flinkx.connector.gBase.dialect.GBaseDialect;
+import com.dtstack.flinkx.connector.gbase.dialect.GBaseDialect;
 import com.dtstack.flinkx.connector.jdbc.source.JdbcSourceFactory;
 
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;

File: flinkx-connectors/flinkx-connector-gbase/src/main/java/com/dtstack/flinkx/connector/gbase/table/GBaseDynamicTableFactory.java
Patch:
@@ -16,9 +16,9 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.gBase.table;
+package com.dtstack.flinkx.connector.gbase.table;
 
-import com.dtstack.flinkx.connector.gBase.dialect.GBaseDialect;
+import com.dtstack.flinkx.connector.gbase.dialect.GBaseDialect;
 import com.dtstack.flinkx.connector.jdbc.dialect.JdbcDialect;
 import com.dtstack.flinkx.connector.jdbc.table.JdbcDynamicTableFactory;
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/ConnectorNameConvertUtil.java
Patch:
@@ -39,7 +39,7 @@ public class ConnectorNameConvertUtil {
         connectorNameMap.put("restapi", new Tuple2<>("http", "http"));
         connectorNameMap.put("adbpostgresql", new Tuple2<>("postgresql", "postgresql"));
         connectorNameMap.put("dorisbatch", new Tuple2<>("doris", "doris"));
-        connectorNameMap.put("gbase", new Tuple2<>("gBase", "gBase"));
+        connectorNameMap.put("gbase", new Tuple2<>("gbase", "gBase"));
     }
 
     public static String convertClassPrefix(String originName) {

File: flinkx-connectors/flinkx-connector-gbase/src/main/java/com/dtstack/flinkx/connector/gBase/converter/GBaseRawTypeConverter.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.gbase.converter;
+package com.dtstack.flinkx.connector.gBase.converter;
 
 import com.dtstack.flinkx.throwable.UnsupportedTypeException;
 

File: flinkx-connectors/flinkx-connector-gbase/src/main/java/com/dtstack/flinkx/connector/gBase/sink/GBaseSinkFactory.java
Patch:
@@ -16,10 +16,10 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.gbase.sink;
+package com.dtstack.flinkx.connector.gBase.sink;
 
 import com.dtstack.flinkx.conf.SyncConf;
-import com.dtstack.flinkx.connector.gbase.dialect.GBaseDialect;
+import com.dtstack.flinkx.connector.gBase.dialect.GBaseDialect;
 import com.dtstack.flinkx.connector.jdbc.sink.JdbcSinkFactory;
 
 /**

File: flinkx-connectors/flinkx-connector-gbase/src/main/java/com/dtstack/flinkx/connector/gBase/source/GBaseSourceFactory.java
Patch:
@@ -16,10 +16,10 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.gbase.source;
+package com.dtstack.flinkx.connector.gBase.source;
 
 import com.dtstack.flinkx.conf.SyncConf;
-import com.dtstack.flinkx.connector.gbase.dialect.GBaseDialect;
+import com.dtstack.flinkx.connector.gBase.dialect.GBaseDialect;
 import com.dtstack.flinkx.connector.jdbc.source.JdbcSourceFactory;
 
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;

File: flinkx-connectors/flinkx-connector-gbase/src/main/java/com/dtstack/flinkx/connector/gBase/table/GBaseDynamicTableFactory.java
Patch:
@@ -16,9 +16,9 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.gbase.table;
+package com.dtstack.flinkx.connector.gBase.table;
 
-import com.dtstack.flinkx.connector.gbase.dialect.GBaseDialect;
+import com.dtstack.flinkx.connector.gBase.dialect.GBaseDialect;
 import com.dtstack.flinkx.connector.jdbc.dialect.JdbcDialect;
 import com.dtstack.flinkx.connector.jdbc.table.JdbcDynamicTableFactory;
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/ConnectorNameConvertUtil.java
Patch:
@@ -39,6 +39,7 @@ public class ConnectorNameConvertUtil {
         connectorNameMap.put("restapi", new Tuple2<>("http", "http"));
         connectorNameMap.put("adbpostgresql", new Tuple2<>("postgresql", "postgresql"));
         connectorNameMap.put("dorisbatch", new Tuple2<>("doris", "doris"));
+        connectorNameMap.put("gbase", new Tuple2<>("gBase", "gBase"));
     }
 
     public static String convertClassPrefix(String originName) {

File: flinkx-core/src/main/java/com/dtstack/flinkx/Main.java
Patch:
@@ -98,7 +98,9 @@ public static void main(String[] args) throws Exception {
         StreamExecutionEnvironment env = EnvFactory.createStreamExecutionEnvironment(options);
         StreamTableEnvironment tEnv =
                 EnvFactory.createStreamTableEnvironment(env, confProperties, options.getJobName());
-
+        LOG.info(
+                "Register to table configuration:{}",
+                tEnv.getConfig().getConfiguration().toString());
         switch (EJobType.getByName(options.getJobType())) {
             case SQL:
                 exeSqlJob(env, tEnv, job, options);

File: flinkx-connectors/flinkx-connector-clickhouse/src/main/java/com/dtstack/flinkx/connector/clickhouse/sink/ClickhouseSinkFactory.java
Patch:
@@ -36,6 +36,6 @@ public ClickhouseSinkFactory(SyncConf syncConf) {
 
     @Override
     protected JdbcOutputFormatBuilder getBuilder() {
-        return new JdbcOutputFormatBuilder(new ClickhouseOutputFormat());
+        return new ClickhouseOutputFormatBuilder(new ClickhouseOutputFormat());
     }
 }

File: flinkx-connectors/flinkx-connector-clickhouse/src/main/java/com/dtstack/flinkx/connector/clickhouse/source/ClickhouseSourceFactory.java
Patch:
@@ -46,6 +46,6 @@ public ClickhouseSourceFactory(SyncConf syncConf, StreamExecutionEnvironment env
 
     @Override
     protected JdbcInputFormatBuilder getBuilder() {
-        return new JdbcInputFormatBuilder(new ClickhouseInputFormat());
+        return new ClickhouseInputFormatBuilder(new ClickhouseInputFormat());
     }
 }

File: flinkx-connectors/flinkx-connector-kafka/src/main/java/com/dtstack/flinkx/connector/kafka/serialization/RowSerializationSchema.java
Patch:
@@ -88,8 +88,7 @@ public ProducerRecord<byte[], byte[]> serialize(RowData element, @Nullable Long
                     null,
                     valueSerialized);
         } catch (Exception e) {
-            // todo kafka比较特殊，这里直接记录脏数据。
-            LOG.error(e.getMessage());
+            dirtyManager.collect(element, e, null);
         }
         return null;
     }

File: flinkx-connectors/flinkx-connector-kafka/src/main/java/com/dtstack/flinkx/connector/kafka/sink/DynamicKafkaSerializationSchema.java
Patch:
@@ -254,7 +254,7 @@ public ProducerRecord<byte[], byte[]> serialize(RowData consumedRow, @Nullable L
                     valueSerialized,
                     readMetadata(consumedRow, KafkaDynamicSink.WritableMetadata.HEADERS));
         } catch (Exception e) {
-            dirtyManager.collect(consumedRow, e, null, runtimeContext);
+            dirtyManager.collect(consumedRow, e, null);
         }
         return null;
     }

File: flinkx-connectors/flinkx-connector-kafka/src/main/java/com/dtstack/flinkx/connector/kafka/source/DynamicKafkaDeserializationSchema.java
Patch:
@@ -215,8 +215,7 @@ public void deserialize(ConsumerRecord<byte[], byte[]> record, Collector<RowData
             }
             keyCollector.buffer.clear();
         } catch (Exception e) {
-            dirtyManager.collect(
-                    new String(record.value(), StandardCharsets.UTF_8), e, null, runtimeContext);
+            dirtyManager.collect(new String(record.value(), StandardCharsets.UTF_8), e, null);
         }
     }
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/dirty/manager/DirtyManager.java
Patch:
@@ -122,7 +122,7 @@ public LongCounter getFailedConsumedMetric() {
         return consumer.getFailedConsumed();
     }
 
-    public void collect(Object data, Throwable cause, String field, RuntimeContext runtimeContext) {
+    public void collect(Object data, Throwable cause, String field) {
         if (executor == null) {
             execute();
         }

File: flinkx-core/src/main/java/com/dtstack/flinkx/sink/format/BaseRichOutputFormat.java
Patch:
@@ -443,7 +443,7 @@ protected void writeSingleRecord(RowData rowData, LongCounter numWriteCounter) {
             writeSingleRecordInternal(rowData);
             numWriteCounter.add(1L);
         } catch (WriteRecordException e) {
-            dirtyManager.collect(e.getRowData(), e, null, getRuntimeContext());
+            dirtyManager.collect(e.getRowData(), e, null);
             if (LOG.isTraceEnabled()) {
                 LOG.trace(
                         "write error rowData, rowData = {}, e = {}",

File: flinkx-core/src/main/java/com/dtstack/flinkx/source/format/BaseRichInputFormat.java
Patch:
@@ -191,7 +191,7 @@ public RowData nextRecord(RowData rowData) {
         try {
             internalRow = nextRecordInternal(rowData);
         } catch (ReadRecordException e) {
-            dirtyManager.collect(e.getRowData(), e, null, getRuntimeContext());
+            dirtyManager.collect(e.getRowData(), e, null);
         }
         if (internalRow != null) {
             updateDuration();

File: flinkx-connectors/flinkx-connector-postgresql/src/main/java/com/dtstack/flinkx/connector/postgresql/dialect/PostgresqlDialect.java
Patch:
@@ -78,6 +78,7 @@ public Optional<String> getUpsertStatement(
                         .collect(Collectors.joining(", "));
         updateClause =
                 Arrays.stream(fieldNames)
+                        .filter(f -> !Arrays.asList(uniqueKeyFields).contains(f))
                         .map(f -> quoteIdentifier(f) + "=EXCLUDED." + quoteIdentifier(f))
                         .collect(Collectors.joining(", "));
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/ConnectorNameConvertUtil.java
Patch:
@@ -38,6 +38,7 @@ public class ConnectorNameConvertUtil {
         connectorNameMap.put("tidb", new Tuple2<>("mysql", "mysql"));
         connectorNameMap.put("restapi", new Tuple2<>("http", "http"));
         connectorNameMap.put("adbpostgresql", new Tuple2<>("postgresql", "postgresql"));
+        connectorNameMap.put("dorisbatch", new Tuple2<>("doris", "doris"));
     }
 
     public static String convertClassPrefix(String originName) {

File: flinkx-connectors/flinkx-connector-clickhouse/src/main/java/com/dtstack/flinkx/connector/clickhouse/converter/ClickhouseRawTypeConverter.java
Patch:
@@ -107,8 +107,9 @@ public static DataType apply(String type) {
             case "TIME":
                 return DataTypes.TIME();
             case "TIMESTAMP":
-            case "DATETIME":
                 return DataTypes.TIMESTAMP();
+            case "DATETIME":
+                return DataTypes.TIMESTAMP(0);
             case "NOTHING":
             case "NULLABLE":
             case "NULL":

File: flinkx-connectors/flinkx-connector-db2/src/main/java/com/dtstack/flinkx/connector/db2/converter/Db2RawTypeConverter.java
Patch:
@@ -64,7 +64,7 @@ public static DataType apply(String type) {
                 return DataTypes.TIME();
             case "TIMESTAMP":
             case "DATETIME":
-                return DataTypes.TIMESTAMP();
+                return DataTypes.TIMESTAMP(0);
             case "BLOB":
             case "BOOLEAN":
                 return DataTypes.BYTES();

File: flinkx-connectors/flinkx-connector-dm/src/main/java/com/dtstack/flinkx/connector/dm/converter/DmRawTypeConverter.java
Patch:
@@ -77,8 +77,9 @@ public static DataType apply(String type) {
                 return DataTypes.DOUBLE();
             case "BIT":
                 return DataTypes.BOOLEAN();
-            case "DATE":
             case "YEAR":
+                return DataTypes.INTERVAL(DataTypes.YEAR());
+            case "DATE":
                 return DataTypes.DATE();
             case "TIME":
                 return DataTypes.TIME();

File: flinkx-connectors/flinkx-connector-doris/src/main/java/com/dtstack/flinkx/connector/doris/options/DorisKeys.java
Patch:
@@ -34,6 +34,8 @@ public final class DorisKeys {
 
     public static final String BATCH_INTERNAL_MS_KEY = "batchIntervalMs";
 
+    public static final String FLUSH_INTERNAL_MS_KEY = "flushIntervalMills";
+
     public static final String MAX_RETRIES_KEY = "maxRetries";
 
     public static final String DATABASE_KEY = "database";

File: flinkx-connectors/flinkx-connector-doris/src/main/java/com/dtstack/flinkx/connector/doris/sink/DorisSinkFactory.java
Patch:
@@ -51,6 +51,7 @@
 import static com.dtstack.flinkx.connector.doris.options.DorisKeys.FE_NODES_KEY;
 import static com.dtstack.flinkx.connector.doris.options.DorisKeys.FIELD_DELIMITER;
 import static com.dtstack.flinkx.connector.doris.options.DorisKeys.FIELD_DELIMITER_KEY;
+import static com.dtstack.flinkx.connector.doris.options.DorisKeys.FLUSH_INTERNAL_MS_KEY;
 import static com.dtstack.flinkx.connector.doris.options.DorisKeys.LINE_DELIMITER;
 import static com.dtstack.flinkx.connector.doris.options.DorisKeys.LINE_DELIMITER_KEY;
 import static com.dtstack.flinkx.connector.doris.options.DorisKeys.LOAD_OPTIONS_KEY;
@@ -148,6 +149,7 @@ public DorisSinkFactory(SyncConf syncConf) {
                                 parameter.getStringVal(WRITE_MODE_KEY, DORIS_WRITE_MODE_DEFAULT))
                         .setUsername(parameter.getStringVal(USER_NAME_KEY))
                         .setBatchSize(parameter.getIntVal(BATCH_SIZE_KEY, 1000))
+                        .setFlushIntervalMills(parameter.getLongVal(FLUSH_INTERNAL_MS_KEY, 10000L))
                         .build();
         options.setColumn(syncConf.getWriter().getFieldList());
         super.initFlinkxCommonConf(options);

File: flinkx-connectors/flinkx-connector-filesystem/src/main/java/com/dtstack/flinkx/table/filesystem/stream/StreamingFileWriter.java
Patch:
@@ -197,7 +197,7 @@ public void initializeState(StateInitializationContext context) throws Exception
         ExecutionConfig.GlobalJobParameters params =
                 streamingRuntimeContext.getExecutionConfig().getGlobalJobParameters();
         DirtyConf dc = DirtyConfUtil.parseFromMap(params.toMap());
-        this.dirtyManager = new DirtyManager(dc);
+        this.dirtyManager = new DirtyManager(dc, streamingRuntimeContext);
         initStatisticsAccumulator();
         initRestoreInfo();
         initAccumulatorCollector();

File: flinkx-connectors/flinkx-connector-ftp/src/main/java/com/dtstack/flinkx/connector/ftp/source/FtpInputFormatBuilder.java
Patch:
@@ -46,7 +46,7 @@ protected void checkFormat() {
             throw new FlinkxRuntimeException("Please Set protocol");
         }
         if (StringUtils.isBlank(ftpConfig.getHost())) {
-            throw new FlinkxRuntimeException("Please Set gost");
+            throw new FlinkxRuntimeException("Please Set host");
         }
         if (StringUtils.isBlank(ftpConfig.getPath())) {
             throw new FlinkxRuntimeException("Please Set path");

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/sink/JdbcSinkFactory.java
Patch:
@@ -80,6 +80,9 @@ ConnectionConf.class, new ConnectionAdapter("SinkConnectionConf"))
         jdbcConf.setColumn(syncConf.getWriter().getFieldList());
         Properties properties = syncConf.getWriter().getProperties("properties", null);
         jdbcConf.setProperties(properties);
+        if (StringUtils.isNotEmpty(syncConf.getWriter().getSemantic())) {
+            jdbcConf.setSemantic(syncConf.getWriter().getSemantic());
+        }
         super.initFlinkxCommonConf(jdbcConf);
         resetTableInfo();
         rebuildJdbcConf(jdbcConf);

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/source/JdbcSourceFactory.java
Patch:
@@ -71,6 +71,7 @@ ConnectionConf.class, new ConnectionAdapter("SourceConnectionConf"))
                         .create();
         GsonUtil.setTypeAdapter(gson);
         jdbcConf = gson.fromJson(gson.toJson(syncConf.getReader().getParameter()), getConfClass());
+        if (StringUtils.isBlank(jdbcConf.getIncreColumn())) jdbcConf.setPolling(false);
         jdbcConf.setColumn(syncConf.getReader().getFieldList());
 
         Properties properties = syncConf.getWriter().getProperties("properties", null);

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/util/JdbcUtil.java
Patch:
@@ -170,7 +170,8 @@ public static List<String> getTableIndex(String schema, String tableName, Connec
         ResultSet rs = dbConn.getMetaData().getIndexInfo(null, schema, tableName, true, false);
         List<String> indexList = new LinkedList<>();
         while (rs.next()) {
-            indexList.add(rs.getString(9));
+            String index = rs.getString(9);
+            if (StringUtils.isNotBlank(index)) indexList.add(index);
         }
         return indexList;
     }

File: flinkx-connectors/flinkx-connector-kafka/src/main/java/com/dtstack/flinkx/connector/kafka/serialization/ticdc/TicdcDeserializationSchema.java
Patch:
@@ -130,14 +130,12 @@ public void deserialize(ConsumerRecord<byte[], byte[]> record, Collector<RowData
                     for (RowData rowDatum : rowData) {
                         collector.collect(rowDatum);
                     }
-                    return;
                 }
 
                 if (eventValue instanceof TicdcEventDDL) {
                     // ddl
                     RowData fromDdl = fromDdl(eventKey, (TicdcEventDDL) eventValue);
                     collector.collect(fromDdl);
-                    return;
                 }
             }
 

File: flinkx-connectors/flinkx-connector-kafka/src/main/java/com/dtstack/flinkx/connector/kafka/sink/DynamicKafkaSerializationSchema.java
Patch:
@@ -156,7 +156,7 @@ protected void beforeOpen() {
         ExecutionConfig.GlobalJobParameters params =
                 context.getExecutionConfig().getGlobalJobParameters();
         DirtyConf dc = DirtyConfUtil.parseFromMap(params.toMap());
-        this.dirtyManager = new DirtyManager(dc);
+        this.dirtyManager = new DirtyManager(dc, context);
 
         initStatisticsAccumulator();
         initRestoreInfo();
@@ -254,7 +254,7 @@ public ProducerRecord<byte[], byte[]> serialize(RowData consumedRow, @Nullable L
                     valueSerialized,
                     readMetadata(consumedRow, KafkaDynamicSink.WritableMetadata.HEADERS));
         } catch (Exception e) {
-            dirtyManager.collect(consumedRow.toString(), e, null, runtimeContext);
+            dirtyManager.collect(consumedRow, e, null, runtimeContext);
         }
         return null;
     }

File: flinkx-connectors/flinkx-connector-kafka/src/main/java/com/dtstack/flinkx/connector/kafka/source/DynamicKafkaDeserializationSchema.java
Patch:
@@ -142,7 +142,7 @@ private void initDirtyManager() {
         ExecutionConfig.GlobalJobParameters params =
                 context.getExecutionConfig().getGlobalJobParameters();
         DirtyConf dc = DirtyConfUtil.parseFromMap(params.toMap());
-        this.dirtyManager = new DirtyManager(dc);
+        this.dirtyManager = new DirtyManager(dc, context);
     }
 
     @Override

File: flinkx-connectors/flinkx-connector-mongodb/src/main/java/com/dtstack/flinkx/connector/mongodb/datasync/MongoConverterFactory.java
Patch:
@@ -39,16 +39,17 @@ public class MongoConverterFactory {
     RowType rowType;
     List<String> fieldNames;
     List<String> fieldTypes;
+    MongodbDataSyncConf mongodbDataSyncConf;
 
     public MongoConverterFactory(MongodbDataSyncConf mongodbDataSyncConf) {
+        this.mongodbDataSyncConf = mongodbDataSyncConf;
         fieldNames = new ArrayList<>();
         fieldTypes = new ArrayList<>();
         List<FieldConf> fields = mongodbDataSyncConf.getColumn();
         for (FieldConf field : fields) {
             fieldNames.add(field.getName());
             fieldTypes.add(field.getType());
         }
-
         rowType = TableUtil.createRowType(fieldNames, fieldTypes, MongodbRawTypeConverter::apply);
     }
 
@@ -57,6 +58,6 @@ public MongodbRowConverter createRowConverter() {
     }
 
     public MongodbColumnConverter createColumnConverter() {
-        return new MongodbColumnConverter(rowType, fieldNames.toArray(new String[] {}));
+        return new MongodbColumnConverter(rowType, mongodbDataSyncConf);
     }
 }

File: flinkx-connectors/flinkx-connector-mysql/src/main/java/com/dtstack/flinkx/connector/mysql/converter/MysqlRawTypeConverter.java
Patch:
@@ -53,7 +53,7 @@ public static DataType apply(String type) {
                 return DataTypes.INTERVAL(DataTypes.YEAR());
             case "TIMESTAMP":
             case "DATETIME":
-                return DataTypes.TIMESTAMP();
+                return DataTypes.TIMESTAMP(0);
             case "TINYBLOB":
             case "BLOB":
             case "MEDIUMBLOB":

File: flinkx-connectors/flinkx-connector-oraclelogminer/src/main/java/com/dtstack/flinkx/connector/oraclelogminer/converter/LogMinerColumnConverter.java
Patch:
@@ -355,7 +355,7 @@ protected IDeserializationConverter createInternalConverter(String type) {
                 return (IDeserializationConverter<String, AbstractBaseColumn>)
                         val -> {
                             val = LogParser.parseTime(val);
-                            return new TimestampColumn(DateUtil.getTimestampFromStr(val));
+                            return new TimestampColumn(DateUtil.getTimestampFromStr(val), 0);
                         };
             case "TIMESTAMP":
                 return (IDeserializationConverter<String, AbstractBaseColumn>)

File: flinkx-connectors/flinkx-connector-phoenix5/src/main/java/com/dtstack/flinkx/connector/phoenix5/converter/HBaseRawTypeConverter.java
Patch:
@@ -91,8 +91,9 @@ public static DataType apply(String type) {
             case "YEAR":
                 return DataTypes.INTERVAL(DataTypes.YEAR());
             case "TIMESTAMP":
-            case "DATETIME":
                 return DataTypes.TIMESTAMP();
+            case "DATETIME":
+                return DataTypes.TIMESTAMP(0);
             case "TINYBLOB":
             case "BLOB":
             case "MEDIUMBLOB":

File: flinkx-connectors/flinkx-connector-phoenix5/src/main/java/com/dtstack/flinkx/connector/phoenix5/converter/Phoenix5RawTypeConverter.java
Patch:
@@ -93,8 +93,9 @@ public static DataType apply(String type) {
             case "YEAR":
                 return DataTypes.INTERVAL(DataTypes.YEAR());
             case "TIMESTAMP":
-            case "DATETIME":
                 return DataTypes.TIMESTAMP();
+            case "DATETIME":
+                return DataTypes.TIMESTAMP(0);
                 // case "TINYINT":
             case "TINYBLOB":
             case "BLOB":

File: flinkx-connectors/flinkx-connector-phoenix5/src/main/java/com/dtstack/flinkx/connector/phoenix5/sink/Phoenix5OutputFormat.java
Patch:
@@ -89,7 +89,7 @@ protected String prepareTemplates() {
                                     jdbcConf.getTable(),
                                     columnNameList.toArray(new String[0]),
                                     jdbcConf.getUpdateKey() != null
-                                            ? jdbcConf.getUpdateKey().toArray(new String[0])
+                                            ? jdbcConf.getUniqueKey().toArray(new String[0])
                                             : null,
                                     jdbcConf.isAllReplace())
                             .get();

File: flinkx-connectors/flinkx-connector-saphana/src/main/java/com/dtstack/flinkx/connector/saphana/converter/SaphanaRawTypeConverter.java
Patch:
@@ -40,7 +40,6 @@ public class SaphanaRawTypeConverter {
      * @link https://data-flair.training/blogs/sql-data-types-in-sap-hana/
      * @param type
      * @return
-     * @throws SQLException
      */
     public static DataType apply(String type) {
         switch (type.toUpperCase(Locale.ENGLISH)) {
@@ -49,9 +48,9 @@ public static DataType apply(String type) {
             case "TIME":
                 return DataTypes.TIME();
             case "SECONDDATE":
-                return DataTypes.TIMESTAMP();
+                return DataTypes.TIMESTAMP(0);
             case "TIMESTAMP":
-                return DataTypes.TIMESTAMP();
+                return DataTypes.TIMESTAMP(7);
                 // Numeric Data Type
             case "TINYINT":
                 return DataTypes.TINYINT();

File: flinkx-connectors/flinkx-connector-solr/src/main/java/com/dtstack/flinkx/connector/solr/converter/SolrColumnConverter.java
Patch:
@@ -103,7 +103,7 @@ public SolrInputDocument toExternal(RowData rowData, SolrInputDocument solrInput
     protected SolrSerializationConverter wrapIntoNullableSolrExternalConverter(
             SolrSerializationConverter solrSerializationConverter) {
         return (val, pos, name, solrInputDocument) -> {
-            if (val == null) {
+            if (((ColumnRowData) val).getField(pos) == null) {
                 solrInputDocument.setField(name, null);
             } else {
                 solrSerializationConverter.serialize(val, pos, name, solrInputDocument);

File: flinkx-connectors/flinkx-connector-solr/src/main/java/com/dtstack/flinkx/connector/solr/converter/SolrRawTypeConverter.java
Patch:
@@ -51,6 +51,7 @@ public static DataType apply(String type) {
             case "DOUBLE":
                 return DataTypes.DOUBLE();
             case "DATE":
+                // todo Date type and accuracy need to be further confirmed
                 return DataTypes.DATE();
             default:
                 throw new UnsupportedTypeException(type);

File: flinkx-connectors/flinkx-connector-solr/src/main/java/com/dtstack/flinkx/connector/solr/sink/SolrOutputFormat.java
Patch:
@@ -75,7 +75,9 @@ protected void writeMultipleRecordsInternal() throws WriteRecordException {
 
     @Override
     protected void openInternal(int taskNumber, int numTasks) throws IOException {
-        solrClientWrapper = new CloudSolrClientKerberosWrapper(solrConf);
+        solrClientWrapper =
+                new CloudSolrClientKerberosWrapper(
+                        solrConf, getRuntimeContext().getDistributedCache());
         solrClientWrapper.init();
     }
 

File: flinkx-connectors/flinkx-connector-solr/src/main/java/com/dtstack/flinkx/connector/solr/source/SolrInputFormat.java
Patch:
@@ -70,7 +70,9 @@ protected InputSplit[] createInputSplitsInternal(int splitNum) throws Exception
 
     @Override
     protected void openInternal(InputSplit inputSplit) {
-        solrClientWrapper = new CloudSolrClientKerberosWrapper(solrConf);
+        solrClientWrapper =
+                new CloudSolrClientKerberosWrapper(
+                        solrConf, getRuntimeContext().getDistributedCache());
         solrClientWrapper.init();
 
         GenericInputSplit genericInputSplit = (GenericInputSplit) inputSplit;

File: flinkx-connectors/flinkx-connector-sqlservercdc/src/main/java/com/dtstack/flinkx/connector/sqlservercdc/convert/SqlServerCdcRawTypeConverter.java
Patch:
@@ -65,9 +65,11 @@ public static DataType apply(String type) {
             case "TIME":
                 return DataTypes.TIME();
             case "DATETIME":
+                return DataTypes.TIMESTAMP(3);
             case "DATETIME2":
+                return DataTypes.TIMESTAMP(7);
             case "SMALLDATETIME":
-                return DataTypes.TIMESTAMP();
+                return DataTypes.TIMESTAMP(0);
             case "BINARY":
             case "VARBINARY":
                 // BYTES 底层调用的是VARBINARY最大长度

File: flinkx-connectors/flinkx-connector-stream/src/main/java/com/dtstack/flinkx/connector/stream/converter/StreamRawTypeConverter.java
Patch:
@@ -63,6 +63,7 @@ public static DataType apply(String type) throws UnsupportedTypeException {
             case "TIME":
                 return DataTypes.TIME();
             case "DATETIME":
+                return DataTypes.TIMESTAMP(0);
             case "TIMESTAMP":
                 return DataTypes.TIMESTAMP();
             case "TIMESTAMP WITH LOCAL TIME ZONE":

File: flinkx-core/src/main/java/com/dtstack/flinkx/constants/ConstantValue.java
Patch:
@@ -73,4 +73,6 @@ public class ConstantValue {
     public static final String CONNECTOR_DIR_NAME = "connector";
 
     public static final String DIRTY_DATA_DIR_NAME = "dirty-data-collector";
+
+    public static final String RESTORE_DIR_NAME = "restore-plugins";
 }

File: flinkx-core/src/main/java/com/dtstack/flinkx/dirty/utils/DirtyConfUtil.java
Patch:
@@ -62,9 +62,9 @@ public static DirtyConf parseFromMap(Map<String, String> confMap) {
         if (type.equals("jdbc")) {
             type = "mysql";
         }
-        long maxConsumed = Long.parseLong(String.valueOf(confMap.getOrDefault(MAX_ROWS_KEY, "1")));
+        long maxConsumed = Long.parseLong(String.valueOf(confMap.getOrDefault(MAX_ROWS_KEY, "0")));
         long maxFailed =
-                Long.parseLong(String.valueOf(confMap.getOrDefault(MAX_FAILED_ROWS_KEY, "1")));
+                Long.parseLong(String.valueOf(confMap.getOrDefault(MAX_FAILED_ROWS_KEY, "0")));
         long printRate = Long.parseLong(String.valueOf(confMap.getOrDefault(PRINT_INTERVAL, "1")));
         String pluginDir = MapUtils.getString(confMap, DIRTY_DIR);
 
@@ -86,7 +86,7 @@ public static DirtyConf parseFromMap(Map<String, String> confMap) {
         dirtyConf.setType(type);
         dirtyConf.setMaxConsumed(maxConsumed < 0 ? Long.MAX_VALUE : maxConsumed);
         dirtyConf.setMaxFailedConsumed(maxFailed < 0 ? Long.MAX_VALUE : maxFailed);
-        dirtyConf.setPrintRate(printRate);
+        dirtyConf.setPrintRate(printRate <= 0 ? Long.MAX_VALUE : printRate);
         dirtyConf.setPluginProperties(pluginProperties);
         dirtyConf.setLocalPluginPath(pluginDir);
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/security/KerberosUtil.java
Patch:
@@ -232,7 +232,7 @@ public static String loadFile(Map<String, Object> kerberosConfig, String filePat
         return loadFile(kerberosConfig, filePath, null);
     }
 
-    private static void checkFileExists(String filePath) {
+    public static void checkFileExists(String filePath) {
         File file = new File(filePath);
         if (file.exists()) {
             if (file.isDirectory()) {

File: flinkx-dirtydata-collectors/flinkx-dirtydata-collector-log/src/main/java/com/dtstack/flinkx/dirty/log/LogDirtyDataCollector.java
Patch:
@@ -43,7 +43,7 @@ protected void init(DirtyConf conf) {
 
     @Override
     protected void consume(DirtyDataEntry dirty) {
-        if ((consumedCounter.getLocalValue() - 1) % printRate == 0) {
+        if (consumedCounter.getLocalValue() % printRate == 0) {
             StringJoiner dirtyMessage =
                     new StringJoiner("\n")
                             .add("\n====================Dirty Data=====================")

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/ConnectorNameConvertUtil.java
Patch:
@@ -38,6 +38,7 @@ public class ConnectorNameConvertUtil {
         connectorNameMap.put("tidb", new Tuple2<>("mysql", "mysql"));
         connectorNameMap.put("restapi", new Tuple2<>("http", "http"));
         connectorNameMap.put("adbpostgresql", new Tuple2<>("postgresql", "postgresql"));
+        connectorNameMap.put("dorisbatch", new Tuple2<>("doris", "doris"));
     }
 
     public static String convertClassPrefix(String originName) {

File: flinkx-connectors/flinkx-connector-solr/src/main/java/com/dtstack/flinkx/connector/solr/sink/SolrOutputFormat.java
Patch:
@@ -75,7 +75,9 @@ protected void writeMultipleRecordsInternal() throws WriteRecordException {
 
     @Override
     protected void openInternal(int taskNumber, int numTasks) throws IOException {
-        solrClientWrapper = new CloudSolrClientKerberosWrapper(solrConf);
+        solrClientWrapper =
+                new CloudSolrClientKerberosWrapper(
+                        solrConf, getRuntimeContext().getDistributedCache());
         solrClientWrapper.init();
     }
 

File: flinkx-connectors/flinkx-connector-solr/src/main/java/com/dtstack/flinkx/connector/solr/source/SolrInputFormat.java
Patch:
@@ -70,7 +70,9 @@ protected InputSplit[] createInputSplitsInternal(int splitNum) throws Exception
 
     @Override
     protected void openInternal(InputSplit inputSplit) {
-        solrClientWrapper = new CloudSolrClientKerberosWrapper(solrConf);
+        solrClientWrapper =
+                new CloudSolrClientKerberosWrapper(
+                        solrConf, getRuntimeContext().getDistributedCache());
         solrClientWrapper.init();
 
         GenericInputSplit genericInputSplit = (GenericInputSplit) inputSplit;

File: flinkx-core/src/main/java/com/dtstack/flinkx/security/KerberosUtil.java
Patch:
@@ -210,7 +210,7 @@ public static String loadFile(Map<String, Object> kerberosConfig, String filePat
         return loadFile(kerberosConfig, filePath, null);
     }
 
-    private static void checkFileExists(String filePath) {
+    public static void checkFileExists(String filePath) {
         File file = new File(filePath);
         if (file.exists()) {
             if (file.isDirectory()) {

File: flinkx-connectors/flinkx-connector-solr/src/main/java/com/dtstack/flinkx/connector/solr/converter/SolrColumnConverter.java
Patch:
@@ -103,7 +103,7 @@ public SolrInputDocument toExternal(RowData rowData, SolrInputDocument solrInput
     protected SolrSerializationConverter wrapIntoNullableSolrExternalConverter(
             SolrSerializationConverter solrSerializationConverter) {
         return (val, pos, name, solrInputDocument) -> {
-            if (val == null) {
+            if (((ColumnRowData) val).getField(pos) == null) {
                 solrInputDocument.setField(name, null);
             } else {
                 solrSerializationConverter.serialize(val, pos, name, solrInputDocument);

File: flinkx-connectors/flinkx-connector-doris/src/main/java/com/dtstack/flinkx/connector/doris/rest/DorisLoadClient.java
Patch:
@@ -325,7 +325,7 @@ private void wrap(
         if (nameMapped) {
             wrapColumnsFromRowData(value, columns, insertV, deleteV, identityMap, delete);
         } else {
-            columns = getColumnName(conf.getColumn());
+            columns.addAll(getColumnName(conf.getColumn()));
             if (columns.isEmpty()) {
                 // neither nameMapping nor column are set.
                 wrapColumnsFromRowData(value, columns, insertV, deleteV, identityMap, delete);

File: flinkx-core/src/main/java/com/dtstack/flinkx/dirty/utils/DirtyConfUtil.java
Patch:
@@ -62,9 +62,9 @@ public static DirtyConf parseFromMap(Map<String, String> confMap) {
         if (type.equals("jdbc")) {
             type = "mysql";
         }
-        long maxConsumed = Long.parseLong(String.valueOf(confMap.getOrDefault(MAX_ROWS_KEY, "1")));
+        long maxConsumed = Long.parseLong(String.valueOf(confMap.getOrDefault(MAX_ROWS_KEY, "0")));
         long maxFailed =
-                Long.parseLong(String.valueOf(confMap.getOrDefault(MAX_FAILED_ROWS_KEY, "1")));
+                Long.parseLong(String.valueOf(confMap.getOrDefault(MAX_FAILED_ROWS_KEY, "0")));
         long printRate = Long.parseLong(String.valueOf(confMap.getOrDefault(PRINT_INTERVAL, "1")));
         String pluginDir = MapUtils.getString(confMap, DIRTY_DIR);
 

File: flinkx-connectors/flinkx-connector-mongodb/src/main/java/com/dtstack/flinkx/connector/mongodb/converter/MongodbColumnConverter.java
Patch:
@@ -41,7 +41,6 @@
 
 import java.sql.Date;
 import java.sql.Time;
-import java.text.ParseException;
 import java.util.ArrayList;
 import java.util.List;
 
@@ -97,7 +96,7 @@ protected MongoSerializationConverter wrapIntoNullableMongodbExternalConverter(
     }
 
     @Override
-    public RowData toInternal(Document document) throws ParseException {
+    public RowData toInternal(Document document) {
         List<FieldConf> fieldList = commonConf.getColumn();
         ColumnRowData result = new ColumnRowData(fieldList.size());
         int convertIndex = 0;

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/DateUtil.java
Patch:
@@ -197,7 +197,7 @@ public static java.sql.Timestamp columnToTimestamp(
             return new java.sql.Timestamp(d.getTime());
         }
 
-        throw new IllegalArgumentException(
+        throw new UnsupportedOperationException(
                 "Can't convert " + column.getClass().getName() + " to Date");
     }
 

File: flinkx-connectors/flinkx-connector-mongodb/src/main/java/com/dtstack/flinkx/connector/mongodb/converter/MongodbColumnConverter.java
Patch:
@@ -41,6 +41,7 @@
 
 import java.sql.Date;
 import java.sql.Time;
+import java.text.ParseException;
 import java.util.ArrayList;
 import java.util.List;
 
@@ -96,7 +97,7 @@ protected MongoSerializationConverter wrapIntoNullableMongodbExternalConverter(
     }
 
     @Override
-    public RowData toInternal(Document document) {
+    public RowData toInternal(Document document) throws ParseException {
         List<FieldConf> fieldList = commonConf.getColumn();
         ColumnRowData result = new ColumnRowData(fieldList.size());
         int convertIndex = 0;

File: flinkx-connectors/flinkx-connector-ftp/src/main/java/com/dtstack/flinkx/connector/ftp/source/FtpInputFormatBuilder.java
Patch:
@@ -46,7 +46,7 @@ protected void checkFormat() {
             throw new FlinkxRuntimeException("Please Set protocol");
         }
         if (StringUtils.isBlank(ftpConfig.getHost())) {
-            throw new FlinkxRuntimeException("Please Set gost");
+            throw new FlinkxRuntimeException("Please Set host");
         }
         if (StringUtils.isBlank(ftpConfig.getPath())) {
             throw new FlinkxRuntimeException("Please Set path");

File: flinkx-connectors/flinkx-connector-sqlserver/src/main/java/com/dtstack/flinkx/connector/sqlserver/sink/SqlserverSinkFactory.java
Patch:
@@ -65,6 +65,8 @@ protected void resetTableInfo() {
                 && jdbcConf.getTable().endsWith("]")
                 && StringUtils.isBlank(jdbcConf.getSchema())) {
             JdbcUtil.resetSchemaAndTable(jdbcConf, "\\[", "\\]");
+        } else {
+            super.resetTableInfo();
         }
     }
 }

File: flinkx-connectors/flinkx-connector-sqlserver/src/main/java/com/dtstack/flinkx/connector/sqlserver/source/SqlserverSourceFactory.java
Patch:
@@ -56,6 +56,8 @@ protected void resetTableInfo() {
                 && jdbcConf.getTable().endsWith("]")
                 && StringUtils.isBlank(jdbcConf.getSchema())) {
             JdbcUtil.resetSchemaAndTable(jdbcConf, "\\[", "\\]");
+        } else {
+            super.resetTableInfo();
         }
     }
 }

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/util/JdbcUtil.java
Patch:
@@ -170,7 +170,8 @@ public static List<String> getTableIndex(String schema, String tableName, Connec
         ResultSet rs = dbConn.getMetaData().getIndexInfo(null, schema, tableName, true, false);
         List<String> indexList = new LinkedList<>();
         while (rs.next()) {
-            indexList.add(rs.getString(9));
+            String index = rs.getString(9);
+            if (StringUtils.isNotBlank(index)) indexList.add(index);
         }
         return indexList;
     }

File: flinkx-connectors/flinkx-connector-doris/src/main/java/com/dtstack/flinkx/connector/doris/options/DorisKeys.java
Patch:
@@ -34,6 +34,8 @@ public final class DorisKeys {
 
     public static final String BATCH_INTERNAL_MS_KEY = "batchIntervalMs";
 
+    public static final String FLUSH_INTERNAL_MS_KEY = "flushIntervalMills";
+
     public static final String MAX_RETRIES_KEY = "maxRetries";
 
     public static final String DATABASE_KEY = "database";

File: flinkx-connectors/flinkx-connector-doris/src/main/java/com/dtstack/flinkx/connector/doris/sink/DorisSinkFactory.java
Patch:
@@ -51,6 +51,7 @@
 import static com.dtstack.flinkx.connector.doris.options.DorisKeys.FE_NODES_KEY;
 import static com.dtstack.flinkx.connector.doris.options.DorisKeys.FIELD_DELIMITER;
 import static com.dtstack.flinkx.connector.doris.options.DorisKeys.FIELD_DELIMITER_KEY;
+import static com.dtstack.flinkx.connector.doris.options.DorisKeys.FLUSH_INTERNAL_MS_KEY;
 import static com.dtstack.flinkx.connector.doris.options.DorisKeys.LINE_DELIMITER;
 import static com.dtstack.flinkx.connector.doris.options.DorisKeys.LINE_DELIMITER_KEY;
 import static com.dtstack.flinkx.connector.doris.options.DorisKeys.LOAD_OPTIONS_KEY;
@@ -148,6 +149,7 @@ public DorisSinkFactory(SyncConf syncConf) {
                                 parameter.getStringVal(WRITE_MODE_KEY, DORIS_WRITE_MODE_DEFAULT))
                         .setUsername(parameter.getStringVal(USER_NAME_KEY))
                         .setBatchSize(parameter.getIntVal(BATCH_SIZE_KEY, 1000))
+                        .setFlushIntervalMills(parameter.getLongVal(FLUSH_INTERNAL_MS_KEY, 10000L))
                         .build();
         options.setColumn(syncConf.getWriter().getFieldList());
         super.initFlinkxCommonConf(options);

File: flinkx-connectors/flinkx-connector-kafka/src/main/java/com/dtstack/flinkx/connector/kafka/serialization/ticdc/TicdcDeserializationSchema.java
Patch:
@@ -130,14 +130,12 @@ public void deserialize(ConsumerRecord<byte[], byte[]> record, Collector<RowData
                     for (RowData rowDatum : rowData) {
                         collector.collect(rowDatum);
                     }
-                    return;
                 }
 
                 if (eventValue instanceof TicdcEventDDL) {
                     // ddl
                     RowData fromDdl = fromDdl(eventKey, (TicdcEventDDL) eventValue);
                     collector.collect(fromDdl);
-                    return;
                 }
             }
 

File: flinkx-connectors/flinkx-connector-oracle/src/main/java/com/dtstack/flinkx/connector/oracle/converter/OracleRawTypeConverter.java
Patch:
@@ -67,6 +67,7 @@ public static DataType apply(String type) {
             case "VARCHAR2":
             case "NCHAR":
             case "NVARCHAR2":
+            case "LONG":
                 return DataTypes.VARCHAR(OracleRowConverter.CLOB_LENGTH - 1);
             case "CLOB":
             case "NCLOB":
@@ -90,8 +91,6 @@ public static DataType apply(String type) {
                 return new AtomicDataType(new BlobType(true, LogicalTypeRoot.VARBINARY));
             case "BINARY_FLOAT":
                 return DataTypes.FLOAT();
-            case "LONG":
-                // when mode is update and allReplace is false, LONG type is not support
             default:
                 if (TIMESTAMP_PREDICATE.test(type)) {
                     return DataTypes.TIMESTAMP();

File: flinkx-connectors/flinkx-connector-sqlserver/src/main/java/com/dtstack/flinkx/connector/sqlserver/sink/SqlserverSinkFactory.java
Patch:
@@ -65,6 +65,8 @@ protected void resetTableInfo() {
                 && jdbcConf.getTable().endsWith("]")
                 && StringUtils.isBlank(jdbcConf.getSchema())) {
             JdbcUtil.resetSchemaAndTable(jdbcConf, "\\[", "\\]");
+        } else {
+            super.resetTableInfo();
         }
     }
 }

File: flinkx-connectors/flinkx-connector-sqlserver/src/main/java/com/dtstack/flinkx/connector/sqlserver/source/SqlserverSourceFactory.java
Patch:
@@ -56,6 +56,8 @@ protected void resetTableInfo() {
                 && jdbcConf.getTable().endsWith("]")
                 && StringUtils.isBlank(jdbcConf.getSchema())) {
             JdbcUtil.resetSchemaAndTable(jdbcConf, "\\[", "\\]");
+        } else {
+            super.resetTableInfo();
         }
     }
 }

File: flinkx-connectors/flinkx-connector-stream/src/main/java/com/dtstack/flinkx/connector/stream/converter/StreamColumnConverter.java
Patch:
@@ -39,6 +39,7 @@
 import java.math.BigDecimal;
 import java.sql.Date;
 import java.sql.Time;
+import java.time.LocalDate;
 import java.time.LocalTime;
 import java.util.ArrayList;
 import java.util.List;
@@ -110,7 +111,7 @@ protected IDeserializationConverter<RowData, AbstractBaseColumn> createInternalC
             case "DECIMAL":
                 return val -> new BigDecimalColumn(JMockData.mock(BigDecimal.class));
             case "DATE":
-                return val -> new SqlDateColumn(JMockData.mock(Date.class));
+                return val -> new SqlDateColumn(Date.valueOf(LocalDate.now()));
             case "DATETIME":
                 return val -> new TimestampColumn(System.currentTimeMillis(), 0);
             case "TIMESTAMP":

File: flinkx-core/src/main/java/com/dtstack/flinkx/element/column/StringColumn.java
Patch:
@@ -52,11 +52,14 @@ public StringColumn(final String data, String format) {
         if (StringUtils.isNotBlank(format)) {
             this.format = DateUtil.buildDateFormatter(format);
             isCustomFormat = true;
+        } else {
+            this.format = DateUtil.buildDateFormatter("yyyy-MM-dd HH:mm:ss");
         }
     }
 
     public StringColumn(Byte aByte) {
         super(aByte);
+        this.format = DateUtil.buildDateFormatter("yyyy-MM-dd HH:mm:ss");
     }
 
     @Override

File: flinkx-connectors/flinkx-connector-doris/src/main/java/com/dtstack/flinkx/connector/doris/sink/DorisOutputFormat.java
Patch:
@@ -80,7 +80,6 @@ protected void writeSingleRecordInternal(RowData rowData) throws WriteRecordExce
             client.load(rowData, true);
         } catch (Exception e) {
             String errormessage = recordConvertDetailErrorMessage(-1, rowData);
-            LOG.error(errormessage, e);
             throw new WriteRecordException(errormessage, e, -1, rowData);
         }
     }

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/sink/JdbcOutputFormat.java
Patch:
@@ -345,7 +345,6 @@ protected void processWriteException(Exception e, int index, RowData row)
 
         if (index < row.getArity()) {
             String message = recordConvertDetailErrorMessage(index, row);
-            LOG.error(message, e);
             throw new WriteRecordException(message, e, index, row);
         }
         throw new WriteRecordException(e.getMessage(), e);

File: flinkx-connectors/flinkx-connector-kafka/src/main/java/com/dtstack/flinkx/connector/kafka/sink/DynamicKafkaSerializationSchema.java
Patch:
@@ -254,7 +254,7 @@ public ProducerRecord<byte[], byte[]> serialize(RowData consumedRow, @Nullable L
                     valueSerialized,
                     readMetadata(consumedRow, KafkaDynamicSink.WritableMetadata.HEADERS));
         } catch (Exception e) {
-            dirtyManager.collect(consumedRow.toString(), e, null, runtimeContext);
+            dirtyManager.collect(consumedRow, e, null, runtimeContext);
         }
         return null;
     }

File: flinkx-core/src/main/java/com/dtstack/flinkx/sink/format/BaseRichOutputFormat.java
Patch:
@@ -35,7 +35,6 @@
 import com.dtstack.flinkx.throwable.NoRestartException;
 import com.dtstack.flinkx.throwable.WriteRecordException;
 import com.dtstack.flinkx.util.ExceptionUtil;
-import com.dtstack.flinkx.util.GsonUtil;
 import com.dtstack.flinkx.util.JsonUtil;
 
 import org.apache.flink.api.common.ExecutionConfig;
@@ -444,8 +443,7 @@ protected void writeSingleRecord(RowData rowData, LongCounter numWriteCounter) {
             writeSingleRecordInternal(rowData);
             numWriteCounter.add(1L);
         } catch (WriteRecordException e) {
-            dirtyManager.collect(
-                    GsonUtil.GSON.toJson(e.getRowData()), e, null, getRuntimeContext());
+            dirtyManager.collect(e.getRowData(), e, null, getRuntimeContext());
             if (LOG.isTraceEnabled()) {
                 LOG.trace(
                         "write error rowData, rowData = {}, e = {}",

File: flinkx-core/src/main/java/com/dtstack/flinkx/source/format/BaseRichInputFormat.java
Patch:
@@ -32,7 +32,6 @@
 import com.dtstack.flinkx.throwable.ReadRecordException;
 import com.dtstack.flinkx.util.DataSyncFactoryUtil;
 import com.dtstack.flinkx.util.ExceptionUtil;
-import com.dtstack.flinkx.util.GsonUtil;
 import com.dtstack.flinkx.util.JsonUtil;
 
 import org.apache.flink.api.common.ExecutionConfig;
@@ -192,8 +191,7 @@ public RowData nextRecord(RowData rowData) {
         try {
             internalRow = nextRecordInternal(rowData);
         } catch (ReadRecordException e) {
-            dirtyManager.collect(
-                    GsonUtil.GSON.toJson(e.getRowData()), e, null, getRuntimeContext());
+            dirtyManager.collect(e.getRowData(), e, null, getRuntimeContext());
         }
         if (internalRow != null) {
             updateDuration();

File: flinkx-connectors/flinkx-connector-mongodb/src/main/java/com/dtstack/flinkx/connector/mongodb/converter/MongodbColumnConverter.java
Patch:
@@ -147,7 +147,7 @@ private MongoDeserializationConverter createMongoInternalConverter(LogicalType t
             case TIME_WITHOUT_TIME_ZONE:
             case TIMESTAMP_WITH_TIME_ZONE:
             case TIMESTAMP_WITHOUT_TIME_ZONE:
-                return val -> new TimestampColumn((java.util.Date) val,0);
+                return val -> new TimestampColumn((java.util.Date) val, 0);
             case BINARY:
             case VARBINARY:
                 return val -> new BytesColumn(((Binary) val).getData());

File: flinkx-connectors/flinkx-connector-mongodb/src/main/java/com/dtstack/flinkx/connector/mongodb/datasync/MongoConverterFactory.java
Patch:
@@ -39,16 +39,17 @@ public class MongoConverterFactory {
     RowType rowType;
     List<String> fieldNames;
     List<String> fieldTypes;
+    MongodbDataSyncConf mongodbDataSyncConf;
 
     public MongoConverterFactory(MongodbDataSyncConf mongodbDataSyncConf) {
+        this.mongodbDataSyncConf = mongodbDataSyncConf;
         fieldNames = new ArrayList<>();
         fieldTypes = new ArrayList<>();
         List<FieldConf> fields = mongodbDataSyncConf.getColumn();
         for (FieldConf field : fields) {
             fieldNames.add(field.getName());
             fieldTypes.add(field.getType());
         }
-
         rowType = TableUtil.createRowType(fieldNames, fieldTypes, MongodbRawTypeConverter::apply);
     }
 
@@ -57,6 +58,6 @@ public MongodbRowConverter createRowConverter() {
     }
 
     public MongodbColumnConverter createColumnConverter() {
-        return new MongodbColumnConverter(rowType, fieldNames.toArray(new String[] {}));
+        return new MongodbColumnConverter(rowType, mongodbDataSyncConf);
     }
 }

File: flinkx-restore/flinkx-restore-mysql/src/main/java/com/dtstack/flinkx/restore/mysql/utils/DataSourceUtil.java
Patch:
@@ -99,10 +99,8 @@ public static DataSource getDataSource(Properties properties, String driverName)
         dataSource.setPoolPreparedStatements(false);
         dataSource.setConnectionInitSqls(Collections.singletonList("set names 'utf8'"));
 
-        dataSource.setRemoveAbandoned(true);
-        dataSource.setRemoveAbandonedTimeout(600);
+        dataSource.setRemoveAbandoned(false);
         dataSource.setLogAbandoned(true);
-        //        dataSource.setBreakAfterAcquireFailure(true);
         dataSource.setTimeBetweenConnectErrorMillis(60000);
         dataSource.setConnectionErrorRetryAttempts(3);
 

File: flinkx-restore/flinkx-restore-mysql/src/main/java/com/dtstack/flinkx/restore/mysql/utils/DataSourceUtil.java
Patch:
@@ -22,7 +22,7 @@ public class DataSourceUtil {
 
     private DataSourceUtil() {}
 
-    public static final String JDBC_URL_KEY = "jdbc.url";
+    public static final String JDBC_URL_KEY = "jdbcUrl";
 
     public static final String USER_NAME_KEY = "username";
 

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/converter/JdbcColumnConverter.java
Patch:
@@ -122,6 +122,7 @@ protected IDeserializationConverter createInternalConverter(LogicalType type) {
                 return val -> new BigDecimalColumn(((Integer) val).byteValue());
             case SMALLINT:
             case INTEGER:
+                return val -> new BigDecimalColumn((Integer) val);
             case INTERVAL_YEAR_MONTH:
                 return (IDeserializationConverter<Object, AbstractBaseColumn>)
                         val -> {

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/source/distribute/DistributedJdbcInputFormatBuilder.java
Patch:
@@ -46,7 +46,9 @@ public void setSourceList(List<DataSourceConf> sourceList) {
     protected void checkFormat() {
         JdbcConf conf = format.getJdbcConf();
         StringBuilder sb = new StringBuilder(256);
-        boolean hasGlobalAccountInfo = !StringUtils.isBlank(conf.getUsername()) && !StringUtils.isBlank(conf.getPassword());
+        boolean hasGlobalAccountInfo =
+                !StringUtils.isBlank(conf.getUsername())
+                        && !StringUtils.isBlank(conf.getPassword());
 
         if (conf.getConnection() == null || conf.getConnection().size() == 0) {
             sb.append("One or more data sources must be specified;\n");

File: flinkx-core/src/main/java/com/dtstack/flinkx/sink/format/BaseRichOutputFormat.java
Patch:
@@ -38,9 +38,6 @@
 import com.dtstack.flinkx.util.ExceptionUtil;
 import com.dtstack.flinkx.util.JsonUtil;
 
-import jdk.nashorn.internal.ir.debug.ObjectSizeCalculator;
-import org.apache.commons.lang3.StringUtils;
-
 import org.apache.flink.api.common.ExecutionConfig;
 import org.apache.flink.api.common.accumulators.LongCounter;
 import org.apache.flink.api.common.io.CleanupWhenUnsuccessful;
@@ -52,6 +49,8 @@
 import org.apache.flink.streaming.api.operators.StreamingRuntimeContext;
 import org.apache.flink.table.data.RowData;
 
+import jdk.nashorn.internal.ir.debug.ObjectSizeCalculator;
+import org.apache.commons.lang3.StringUtils;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/source/distribute/DistributedJdbcInputFormatBuilder.java
Patch:
@@ -46,7 +46,9 @@ public void setSourceList(List<DataSourceConf> sourceList) {
     protected void checkFormat() {
         JdbcConf conf = format.getJdbcConf();
         StringBuilder sb = new StringBuilder(256);
-        boolean hasGlobalAccountInfo = !StringUtils.isBlank(conf.getUsername()) && !StringUtils.isBlank(conf.getPassword());
+        boolean hasGlobalAccountInfo =
+                !StringUtils.isBlank(conf.getUsername())
+                        && !StringUtils.isBlank(conf.getPassword());
 
         if (conf.getConnection() == null || conf.getConnection().size() == 0) {
             sb.append("One or more data sources must be specified;\n");

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/sink/JdbcOutputFormat.java
Patch:
@@ -188,9 +188,6 @@ protected void writeSingleRecordInternal(RowData row) throws WriteRecordExceptio
         int index = 0;
         try {
             stmtProxy.writeSingleRecordInternal(row);
-            if (Semantic.EXACTLY_ONCE == semantic) {
-                JdbcUtil.commit(dbConn);
-            }
         } catch (Exception e) {
             JdbcUtil.rollBack(dbConn);
             processWriteException(e, index, row);

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/sink/JdbcSinkFactory.java
Patch:
@@ -80,6 +80,9 @@ ConnectionConf.class, new ConnectionAdapter("SinkConnectionConf"))
         jdbcConf.setColumn(syncConf.getWriter().getFieldList());
         Properties properties = syncConf.getWriter().getProperties("properties", null);
         jdbcConf.setProperties(properties);
+        if (StringUtils.isNotEmpty(syncConf.getWriter().getSemantic())) {
+            jdbcConf.setSemantic(syncConf.getWriter().getSemantic());
+        }
         super.initFlinkxCommonConf(jdbcConf);
         resetTableInfo();
         rebuildJdbcConf(jdbcConf);

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/DataSyncFactoryUtil.java
Patch:
@@ -142,7 +142,8 @@ public static Pair<FetcherBase, StoreBase> discoverMonitor(
             String fetcherPluginClassName =
                     PluginUtil.getPluginClassName(pluginType, OperatorType.fetcher);
             Set<URL> urlList =
-                    PluginUtil.getJarFileDirPath(pluginType, syncConf.getPluginRoot(), null, "restore-plugins");
+                    PluginUtil.getJarFileDirPath(
+                            pluginType, syncConf.getPluginRoot(), null, "restore-plugins");
 
             StoreBase store =
                     ClassLoaderManager.newInstance(

File: flinkx-core/src/main/java/com/dtstack/flinkx/sink/format/BaseRichOutputFormat.java
Patch:
@@ -38,8 +38,6 @@
 import com.dtstack.flinkx.util.GsonUtil;
 import com.dtstack.flinkx.util.JsonUtil;
 
-import jdk.nashorn.internal.ir.debug.ObjectSizeCalculator;
-
 import org.apache.flink.api.common.ExecutionConfig;
 import org.apache.flink.api.common.accumulators.LongCounter;
 import org.apache.flink.api.common.io.CleanupWhenUnsuccessful;
@@ -51,6 +49,7 @@
 import org.apache.flink.streaming.api.operators.StreamingRuntimeContext;
 import org.apache.flink.table.data.RowData;
 
+import jdk.nashorn.internal.ir.debug.ObjectSizeCalculator;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 

File: flinkx-connectors/flinkx-connector-doris/src/main/java/com/dtstack/flinkx/connector/doris/rest/DorisStreamLoad.java
Patch:
@@ -56,7 +56,7 @@
 public class DorisStreamLoad implements Serializable {
 
     private static final Logger LOG = LoggerFactory.getLogger(DorisStreamLoad.class);
-
+    private static final ObjectMapper OM = new ObjectMapper();
     private static final List<String> DORIS_SUCCESS_STATUS =
             new ArrayList<>(Arrays.asList("Success", "Publish Timeout"));
     private final String authEncoding;
@@ -126,8 +126,7 @@ public void load(Carrier carrier, String loadUrlStr) throws IOException {
         if (loadResponse.status != 200) {
             throw new ConnectException("stream load error, detail : " + loadResponse);
         } else {
-            ObjectMapper obj = new ObjectMapper();
-            RespContent respContent = obj.readValue(loadResponse.respContent, RespContent.class);
+            RespContent respContent = OM.readValue(loadResponse.respContent, RespContent.class);
             if (!DORIS_SUCCESS_STATUS.contains(respContent.getStatus())) {
                 throw new IOException("stream load error: " + getDetailErrorLog(respContent));
             }

File: flinkx-core/src/main/java/com/dtstack/flinkx/sink/format/BaseRichOutputFormat.java
Patch:
@@ -437,9 +437,7 @@ private void initTimingSubmitTask() {
                                     }
                                     try {
                                         if (!rows.isEmpty()) {
-                                            int size = rows.size();
                                             writeRecordInternal();
-                                            numWriteCounter.add(size);
                                         }
                                     } catch (Exception e) {
                                         LOG.error(

File: flinkx-connectors/flinkx-connector-filesystem/src/main/java/com/dtstack/flinkx/table/filesystem/stream/StreamingFileWriter.java
Patch:
@@ -197,7 +197,7 @@ public void initializeState(StateInitializationContext context) throws Exception
         ExecutionConfig.GlobalJobParameters params =
                 streamingRuntimeContext.getExecutionConfig().getGlobalJobParameters();
         DirtyConf dc = DirtyConfUtil.parseFromMap(params.toMap());
-        this.dirtyManager = new DirtyManager(dc);
+        this.dirtyManager = new DirtyManager(dc, streamingRuntimeContext);
         initStatisticsAccumulator();
         initRestoreInfo();
         initAccumulatorCollector();

File: flinkx-connectors/flinkx-connector-kafka/src/main/java/com/dtstack/flinkx/connector/kafka/sink/DynamicKafkaSerializationSchema.java
Patch:
@@ -156,7 +156,7 @@ protected void beforeOpen() {
         ExecutionConfig.GlobalJobParameters params =
                 context.getExecutionConfig().getGlobalJobParameters();
         DirtyConf dc = DirtyConfUtil.parseFromMap(params.toMap());
-        this.dirtyManager = new DirtyManager(dc);
+        this.dirtyManager = new DirtyManager(dc, context);
 
         initStatisticsAccumulator();
         initRestoreInfo();

File: flinkx-connectors/flinkx-connector-kafka/src/main/java/com/dtstack/flinkx/connector/kafka/source/DynamicKafkaDeserializationSchema.java
Patch:
@@ -142,7 +142,7 @@ private void initDirtyManager() {
         ExecutionConfig.GlobalJobParameters params =
                 context.getExecutionConfig().getGlobalJobParameters();
         DirtyConf dc = DirtyConfUtil.parseFromMap(params.toMap());
-        this.dirtyManager = new DirtyManager(dc);
+        this.dirtyManager = new DirtyManager(dc, context);
     }
 
     @Override

File: flinkx-core/src/main/java/com/dtstack/flinkx/sink/format/BaseRichOutputFormat.java
Patch:
@@ -202,7 +202,7 @@ public void open(int taskNumber, int numTasks) throws IOException {
         ExecutionConfig.GlobalJobParameters params =
                 context.getExecutionConfig().getGlobalJobParameters();
         DirtyConf dc = DirtyConfUtil.parseFromMap(params.toMap());
-        this.dirtyManager = new DirtyManager(dc);
+        this.dirtyManager = new DirtyManager(dc, this.context);
 
         checkpointMode =
                 context.getCheckpointMode() == null
@@ -470,7 +470,6 @@ protected void writeSingleRecord(RowData rowData, LongCounter numWriteCounter) {
         } catch (WriteRecordException e) {
             dirtyManager.collect(
                     GsonUtil.GSON.toJson(e.getRowData()), e, null, getRuntimeContext());
-            errCounter.add(1);
             if (LOG.isTraceEnabled()) {
                 LOG.trace(
                         "write error rowData, rowData = {}, e = {}",

File: flinkx-core/src/main/java/com/dtstack/flinkx/source/format/BaseRichInputFormat.java
Patch:
@@ -140,7 +140,7 @@ public void open(InputSplit inputSplit) throws IOException {
         ExecutionConfig.GlobalJobParameters params =
                 context.getExecutionConfig().getGlobalJobParameters();
         DirtyConf dc = DirtyConfUtil.parseFromMap(params.toMap());
-        this.dirtyManager = new DirtyManager(dc);
+        this.dirtyManager = new DirtyManager(dc, this.context);
 
         if (inputSplit instanceof ErrorInputSplit) {
             throw new RuntimeException(((ErrorInputSplit) inputSplit).getErrorMessage());

File: flinkx-core/src/main/java/com/dtstack/flinkx/sink/format/BaseRichOutputFormat.java
Patch:
@@ -33,6 +33,7 @@
 import com.dtstack.flinkx.sink.ErrorLimiter;
 import com.dtstack.flinkx.throwable.WriteRecordException;
 import com.dtstack.flinkx.util.ExceptionUtil;
+import com.dtstack.flinkx.util.GsonUtil;
 import com.dtstack.flinkx.util.JsonUtil;
 
 import org.apache.flink.api.common.ExecutionConfig;
@@ -467,7 +468,8 @@ protected void writeSingleRecord(RowData rowData, LongCounter numWriteCounter) {
             writeSingleRecordInternal(rowData);
             numWriteCounter.add(1L);
         } catch (WriteRecordException e) {
-            dirtyManager.collect(String.valueOf(rowData), e, null, getRuntimeContext());
+            dirtyManager.collect(
+                    GsonUtil.GSON.toJson(e.getRowData()), e, null, getRuntimeContext());
             errCounter.add(1);
             if (LOG.isTraceEnabled()) {
                 LOG.trace(

File: flinkx-dirtydata-collectors/flinkx-dirtydata-collector-log/src/main/java/com/dtstack/flinkx/dirty/log/LogDirtyDataCollector.java
Patch:
@@ -43,7 +43,7 @@ protected void init(DirtyConf conf) {
 
     @Override
     protected void consume(DirtyDataEntry dirty) {
-        if ((consumedCounter.getLocalValue() - 1) % printRate == 0) {
+        if (consumedCounter.getLocalValue() % printRate == 0) {
             StringJoiner dirtyMessage =
                     new StringJoiner("\n")
                             .add("\n====================Dirty Data=====================")

File: flinkx-connectors/flinkx-connector-oracle/src/main/java/com/dtstack/flinkx/connector/oracle/converter/OracleColumnConverter.java
Patch:
@@ -30,7 +30,6 @@
 import com.dtstack.flinkx.element.column.StringColumn;
 import com.dtstack.flinkx.element.column.TimestampColumn;
 
-import org.apache.flink.table.data.StringData;
 import org.apache.flink.table.types.logical.LogicalType;
 import org.apache.flink.table.types.logical.RowType;
 
@@ -78,7 +77,7 @@ protected IDeserializationConverter createInternalConverter(LogicalType type) {
                 if (type instanceof ClobType) {
                     return val -> {
                         oracle.sql.CLOB clob = (oracle.sql.CLOB) val;
-                        return StringData.fromString(ConvertUtil.convertClob(clob));
+                        return new StringColumn(ConvertUtil.convertClob(clob));
                     };
                 }
                 return val -> new StringColumn((String) val);

File: flinkx-core/src/main/java/com/dtstack/flinkx/dirty/utils/DirtyConfUtil.java
Patch:
@@ -23,10 +23,10 @@
 import com.dtstack.flinkx.throwable.NoRestartException;
 import com.dtstack.flinkx.util.PropertiesUtil;
 
-import org.apache.commons.collections.MapUtils;
-
 import org.apache.flink.shaded.guava18.com.google.common.collect.Maps;
 
+import org.apache.commons.collections.MapUtils;
+
 import java.io.File;
 import java.util.Locale;
 import java.util.Map;

File: flinkx-core/src/main/java/com/dtstack/flinkx/dirty/utils/DirtyConfUtil.java
Patch:
@@ -23,10 +23,10 @@
 import com.dtstack.flinkx.throwable.NoRestartException;
 import com.dtstack.flinkx.util.PropertiesUtil;
 
-import org.apache.flink.shaded.guava18.com.google.common.collect.Maps;
-
 import org.apache.commons.collections.MapUtils;
 
+import org.apache.flink.shaded.guava18.com.google.common.collect.Maps;
+
 import java.io.File;
 import java.util.Locale;
 import java.util.Map;
@@ -86,7 +86,7 @@ public static DirtyConf parseFromMap(Map<String, String> confMap) {
         dirtyConf.setType(type);
         dirtyConf.setMaxConsumed(maxConsumed < 0 ? Long.MAX_VALUE : maxConsumed);
         dirtyConf.setMaxFailedConsumed(maxFailed < 0 ? Long.MAX_VALUE : maxFailed);
-        dirtyConf.setPrintRate(printRate);
+        dirtyConf.setPrintRate(printRate <= 0 ? Long.MAX_VALUE : printRate);
         dirtyConf.setPluginProperties(pluginProperties);
         dirtyConf.setLocalPluginPath(pluginDir);
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/cdc/worker/Worker.java
Patch:
@@ -69,6 +69,8 @@ private void send() {
             for (int i = 0; i < size; i++) {
                 RowData data = queue.peek();
                 if (data == null) {
+                    // if queue is empty, remove this queue.
+                    queuesChamberlain.removeEmptyQueue(tableIdentity);
                     break;
                 }
 

File: flinkx-connectors/flinkx-connector-kafka/src/main/java/com/dtstack/flinkx/connector/kafka/serialization/ticdc/TicdcColumnConverter.java
Patch:
@@ -194,7 +194,8 @@ private void processColumnList(
             TicdcEventColumn eventColumn = eventColumns.get(i);
             if (null != eventColumn.getV()) {
                 AbstractBaseColumn column =
-                        (AbstractBaseColumn) converters.get(i).deserialize(eventColumn.getV());
+                        (AbstractBaseColumn)
+                                converters.get(i).deserialize(eventColumn.getV().toString());
 
                 columnList.add(column);
             } else {

File: flinkx-dirtydata-collectors/flinkx-dirtydata-collector-log/src/main/java/com/dtstack/flinkx/dirty/log/LogDirtyDataCollector.java
Patch:
@@ -42,8 +42,8 @@ protected void init(DirtyConf conf) {
     }
 
     @Override
-    protected void consume(DirtyDataEntry dirty) throws Exception {
-        if (consumedCounter.getLocalValue() % printRate == 0) {
+    protected void consume(DirtyDataEntry dirty) {
+        if ((consumedCounter.getLocalValue() - 1) % printRate == 0) {
             StringJoiner dirtyMessage =
                     new StringJoiner("\n")
                             .add("\n====================Dirty Data=====================")

File: flinkx-core/src/main/java/com/dtstack/flinkx/Main.java
Patch:
@@ -70,8 +70,6 @@
 import org.slf4j.LoggerFactory;
 
 import java.net.URL;
-import java.net.URLDecoder;
-import java.nio.charset.StandardCharsets;
 import java.util.Arrays;
 import java.util.List;
 import java.util.Optional;
@@ -94,7 +92,7 @@ public static void main(String[] args) throws Exception {
         LOG.info("-------------------------------------------");
 
         Options options = new OptionParser(args).getOptions();
-        String job = URLDecoder.decode(options.getJob(), StandardCharsets.UTF_8.name());
+        String job = options.getJob();
         Properties confProperties = PropertiesUtil.parseConf(options.getConfProp());
         StreamExecutionEnvironment env = EnvFactory.createStreamExecutionEnvironment(options);
         StreamTableEnvironment tEnv =

File: flinkx-local-test/src/main/java/com/dtstack/flinkx/local/test/LocalTest.java
Patch:
@@ -95,7 +95,7 @@ public static void main(String[] args) throws Exception {
             argsList.add("-jobType");
             argsList.add("sql");
             argsList.add("-job");
-            argsList.add(URLEncoder.encode(content, StandardCharsets.UTF_8.name()));
+            argsList.add(content);
 //            argsList.add("-flinkConfDir");
 //            argsList.add("/opt/dtstack/flink-1.12.2/conf/");
             argsList.add("-jobName");

File: flinkx-core/src/main/java/com/dtstack/flinkx/cdc/worker/Worker.java
Patch:
@@ -69,6 +69,8 @@ private void send() {
             for (int i = 0; i < size; i++) {
                 RowData data = queue.peek();
                 if (data == null) {
+                    // if queue is empty, remove this queue.
+                    queuesChamberlain.removeEmptyQueue(tableIdentity);
                     break;
                 }
 

File: flinkx-connectors/flinkx-connector-kafka/src/main/java/com/dtstack/flinkx/connector/kafka/serialization/ticdc/TicdcColumnConverter.java
Patch:
@@ -194,7 +194,8 @@ private void processColumnList(
             TicdcEventColumn eventColumn = eventColumns.get(i);
             if (null != eventColumn.getV()) {
                 AbstractBaseColumn column =
-                        (AbstractBaseColumn) converters.get(i).deserialize(eventColumn.getV());
+                        (AbstractBaseColumn)
+                                converters.get(i).deserialize(eventColumn.getV().toString());
 
                 columnList.add(column);
             } else {

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/sink/JdbcOutputFormatBuilder.java
Patch:
@@ -19,7 +19,6 @@
 
 import com.dtstack.flinkx.connector.jdbc.conf.JdbcConf;
 import com.dtstack.flinkx.connector.jdbc.dialect.JdbcDialect;
-import com.dtstack.flinkx.connector.jdbc.util.JdbcUtil;
 import com.dtstack.flinkx.converter.AbstractRowConverter;
 import com.dtstack.flinkx.sink.format.BaseRichOutputFormatBuilder;
 
@@ -60,8 +59,6 @@ protected void checkFormat() {
         }
         if (StringUtils.isBlank(jdbcConf.getJdbcUrl())) {
             sb.append("No jdbc url supplied;\n");
-        } else {
-            jdbcConf.setJdbcUrl(JdbcUtil.formatJdbcUrl(jdbcConf.getJdbcUrl(), null));
         }
         if (sb.length() > 0) {
             throw new IllegalArgumentException(sb.toString());

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/source/JdbcInputFormatBuilder.java
Patch:
@@ -21,7 +21,6 @@
 import com.dtstack.flinkx.conf.FieldConf;
 import com.dtstack.flinkx.connector.jdbc.conf.JdbcConf;
 import com.dtstack.flinkx.connector.jdbc.dialect.JdbcDialect;
-import com.dtstack.flinkx.connector.jdbc.util.JdbcUtil;
 import com.dtstack.flinkx.constants.ConstantValue;
 import com.dtstack.flinkx.enums.ColumnType;
 import com.dtstack.flinkx.enums.Semantic;
@@ -67,8 +66,6 @@ protected void checkFormat() {
         }
         if (StringUtils.isBlank(conf.getJdbcUrl())) {
             sb.append("No jdbc url supplied;\n");
-        } else {
-            conf.setJdbcUrl(JdbcUtil.formatJdbcUrl(conf.getJdbcUrl(), null));
         }
         if (conf.isIncrement()) {
             if (StringUtils.isBlank(conf.getIncreColumn())) {

File: flinkx-connectors/flinkx-connector-mysql/src/main/java/com/dtstack/flinkx/connector/mysql/sink/MysqlSinkFactory.java
Patch:
@@ -20,6 +20,7 @@
 
 import com.dtstack.flinkx.conf.SyncConf;
 import com.dtstack.flinkx.connector.jdbc.sink.JdbcSinkFactory;
+import com.dtstack.flinkx.connector.jdbc.util.JdbcUtil;
 import com.dtstack.flinkx.connector.mysql.dialect.MysqlDialect;
 
 /**
@@ -31,5 +32,6 @@ public class MysqlSinkFactory extends JdbcSinkFactory {
 
     public MysqlSinkFactory(SyncConf syncConf) {
         super(syncConf, new MysqlDialect());
+        JdbcUtil.putExtParam(jdbcConf);
     }
 }

File: flinkx-connectors/flinkx-connector-mysql/src/main/java/com/dtstack/flinkx/connector/mysql/source/MysqlSourceFactory.java
Patch:
@@ -20,6 +20,7 @@
 
 import com.dtstack.flinkx.conf.SyncConf;
 import com.dtstack.flinkx.connector.jdbc.source.JdbcSourceFactory;
+import com.dtstack.flinkx.connector.jdbc.util.JdbcUtil;
 import com.dtstack.flinkx.connector.mysql.dialect.MysqlDialect;
 
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
@@ -44,6 +45,7 @@ public MysqlSourceFactory(SyncConf syncConf, StreamExecutionEnvironment env) {
                 && jdbcConf.getFetchSize() == 0) {
             jdbcConf.setFetchSize(1000);
         }
+        JdbcUtil.putExtParam(jdbcConf);
     }
 
     @Override

File: flinkx-dirtydata-collectors/flinkx-dirtydata-collector-log/src/main/java/com/dtstack/flinkx/dirty/log/LogDirtyDataCollector.java
Patch:
@@ -42,8 +42,8 @@ protected void init(DirtyConf conf) {
     }
 
     @Override
-    protected void consume(DirtyDataEntry dirty) throws Exception {
-        if (consumedCounter.getLocalValue() % printRate == 0) {
+    protected void consume(DirtyDataEntry dirty) {
+        if ((consumedCounter.getLocalValue() - 1) % printRate == 0) {
             StringJoiner dirtyMessage =
                     new StringJoiner("\n")
                             .add("\n====================Dirty Data=====================")

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/sink/JdbcOutputFormatBuilder.java
Patch:
@@ -19,7 +19,6 @@
 
 import com.dtstack.flinkx.connector.jdbc.conf.JdbcConf;
 import com.dtstack.flinkx.connector.jdbc.dialect.JdbcDialect;
-import com.dtstack.flinkx.connector.jdbc.util.JdbcUtil;
 import com.dtstack.flinkx.converter.AbstractRowConverter;
 import com.dtstack.flinkx.sink.format.BaseRichOutputFormatBuilder;
 
@@ -60,8 +59,6 @@ protected void checkFormat() {
         }
         if (StringUtils.isBlank(jdbcConf.getJdbcUrl())) {
             sb.append("No jdbc url supplied;\n");
-        } else {
-            jdbcConf.setJdbcUrl(JdbcUtil.formatJdbcUrl(jdbcConf.getJdbcUrl(), null));
         }
         if (sb.length() > 0) {
             throw new IllegalArgumentException(sb.toString());

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/source/JdbcInputFormatBuilder.java
Patch:
@@ -21,7 +21,6 @@
 import com.dtstack.flinkx.conf.FieldConf;
 import com.dtstack.flinkx.connector.jdbc.conf.JdbcConf;
 import com.dtstack.flinkx.connector.jdbc.dialect.JdbcDialect;
-import com.dtstack.flinkx.connector.jdbc.util.JdbcUtil;
 import com.dtstack.flinkx.constants.ConstantValue;
 import com.dtstack.flinkx.enums.ColumnType;
 import com.dtstack.flinkx.enums.Semantic;
@@ -67,8 +66,6 @@ protected void checkFormat() {
         }
         if (StringUtils.isBlank(conf.getJdbcUrl())) {
             sb.append("No jdbc url supplied;\n");
-        } else {
-            conf.setJdbcUrl(JdbcUtil.formatJdbcUrl(conf.getJdbcUrl(), null));
         }
         if (conf.isIncrement()) {
             if (StringUtils.isBlank(conf.getIncreColumn())) {

File: flinkx-connectors/flinkx-connector-mysql/src/main/java/com/dtstack/flinkx/connector/mysql/sink/MysqlSinkFactory.java
Patch:
@@ -20,6 +20,7 @@
 
 import com.dtstack.flinkx.conf.SyncConf;
 import com.dtstack.flinkx.connector.jdbc.sink.JdbcSinkFactory;
+import com.dtstack.flinkx.connector.jdbc.util.JdbcUtil;
 import com.dtstack.flinkx.connector.mysql.dialect.MysqlDialect;
 
 /**
@@ -31,5 +32,6 @@ public class MysqlSinkFactory extends JdbcSinkFactory {
 
     public MysqlSinkFactory(SyncConf syncConf) {
         super(syncConf, new MysqlDialect());
+        JdbcUtil.putExtParam(jdbcConf);
     }
 }

File: flinkx-connectors/flinkx-connector-mysql/src/main/java/com/dtstack/flinkx/connector/mysql/source/MysqlSourceFactory.java
Patch:
@@ -20,6 +20,7 @@
 
 import com.dtstack.flinkx.conf.SyncConf;
 import com.dtstack.flinkx.connector.jdbc.source.JdbcSourceFactory;
+import com.dtstack.flinkx.connector.jdbc.util.JdbcUtil;
 import com.dtstack.flinkx.connector.mysql.dialect.MysqlDialect;
 
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
@@ -44,6 +45,7 @@ public MysqlSourceFactory(SyncConf syncConf, StreamExecutionEnvironment env) {
                 && jdbcConf.getFetchSize() == 0) {
             jdbcConf.setFetchSize(1000);
         }
+        JdbcUtil.putExtParam(jdbcConf);
     }
 
     @Override

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/PluginUtil.java
Patch:
@@ -323,7 +323,7 @@ public static void registerPluginUrlToCachedFile(
             for (URL url : urlSet) {
                 String classFileName = String.format(CLASS_FILE_NAME_FMT, i);
                 env.registerCachedFile(url.getPath(), classFileName, true);
-                urlList.add(url.toString());
+                urlList.add(url.getPath());
                 add.invoke(contextClassLoader, url);
                 i++;
             }

File: flinkx-connectors/flinkx-connector-binlog/src/main/java/com/dtstack/flinkx/connector/binlog/inputformat/BinlogInputFormatBuilder.java
Patch:
@@ -134,8 +134,8 @@ protected void checkFormat() {
         Properties properties = new Properties();
         properties.put("user", binlogConf.getUsername());
         properties.put("password", binlogConf.getPassword());
-        properties.put("socketTimeout", binlogConf.getQueryTimeOut());
-        properties.put("connectTimeout", binlogConf.getConnectTimeOut());
+        properties.put("socketTimeout", String.valueOf(binlogConf.getQueryTimeOut()));
+        properties.put("connectTimeout", String.valueOf(binlogConf.getConnectTimeOut()));
 
         try (Connection conn =
                 RetryUtil.executeWithRetry(

File: flinkx-connectors/flinkx-connector-binlog/src/main/java/com/dtstack/flinkx/connector/binlog/inputformat/BinlogInputFormatBuilder.java
Patch:
@@ -132,8 +132,8 @@ protected void checkFormat() {
         Properties properties = new Properties();
         properties.put("user", binlogConf.getUsername());
         properties.put("password", binlogConf.getPassword());
-        properties.put("socketTimeout", binlogConf.getQueryTimeOut());
-        properties.put("connectTimeout", binlogConf.getConnectTimeOut());
+        properties.put("socketTimeout", String.valueOf(binlogConf.getQueryTimeOut()));
+        properties.put("connectTimeout", String.valueOf(binlogConf.getConnectTimeOut()));
 
         try (Connection conn =
                 RetryUtil.executeWithRetry(

File: flinkx-connectors/flinkx-connector-mysql/src/main/java/com/dtstack/flinkx/connector/mysql/dialect/MysqlDialect.java
Patch:
@@ -106,7 +106,7 @@ public Optional<String> getReplaceStatement(
                         .map(this::quoteIdentifier)
                         .collect(Collectors.joining(", "));
         String placeholders =
-                Arrays.stream(fieldNames).map(f -> "?").collect(Collectors.joining(", "));
+                Arrays.stream(fieldNames).map(f -> ":" + f).collect(Collectors.joining(", "));
         return Optional.of(
                 "REPLACE INTO "
                         + buildTableInfoWithSchema(schema, tableName)

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/sink/JdbcDynamicTableSink.java
Patch:
@@ -76,7 +76,7 @@ public ChangelogMode getChangelogMode(ChangelogMode requestedMode) {
     private void validatePrimaryKey(ChangelogMode requestedMode) {
         checkState(
                 ChangelogMode.insertOnly().equals(requestedMode)
-                        || !CollectionUtil.isNullOrEmpty(jdbcConf.getUpdateKey()),
+                        || !CollectionUtil.isNullOrEmpty(jdbcConf.getUniqueKey()),
                 "please declare primary key for sink table when query contains update/delete record.");
     }
 
@@ -98,7 +98,7 @@ public SinkFunctionProvider getSinkRuntimeProvider(Context context) {
         }
         jdbcConf.setColumn(columnList);
         jdbcConf.setMode(
-                (CollectionUtil.isNullOrEmpty(jdbcConf.getUpdateKey()))
+                (CollectionUtil.isNullOrEmpty(jdbcConf.getUniqueKey()))
                         ? EWriteMode.INSERT.name()
                         : EWriteMode.UPDATE.name());
 

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/table/JdbcDynamicTableFactory.java
Patch:
@@ -174,7 +174,7 @@ protected JdbcConf getSinkConnectionConf(ReadableConfig readableConfig, TableSch
 
         List<String> keyFields =
                 schema.getPrimaryKey().map(UniqueConstraint::getColumns).orElse(null);
-        jdbcConf.setUpdateKey(keyFields);
+        jdbcConf.setUniqueKey(keyFields);
         resetTableInfo(jdbcConf);
         return jdbcConf;
     }

File: flinkx-clients/src/main/java/com/dtstack/flinkx/client/util/YarnSessionClientUtil.java
Patch:
@@ -170,12 +170,14 @@ public static ClusterSpecification createClusterSpecification(Properties conProp
                         Math.max(
                                 MIN_JM_MEMORY,
                                 ValueUtil.getInt(conProp.getProperty(JOBMANAGER_MEMORY_MB)));
+                jobManagerMemoryMb = jobManagerMemoryMb >> 20;
             }
             if (conProp.containsKey(TASKMANAGER_MEMORY_MB)) {
                 taskManagerMemoryMb =
                         Math.max(
                                 MIN_TM_MEMORY,
                                 ValueUtil.getInt(conProp.getProperty(TASKMANAGER_MEMORY_MB)));
+                taskManagerMemoryMb = taskManagerMemoryMb >> 20;
             }
             if (conProp.containsKey(SLOTS_PER_TASKMANAGER)) {
                 slotsPerTaskManager = ValueUtil.getInt(conProp.get(SLOTS_PER_TASKMANAGER));

File: flinkx-clients/src/main/java/com/dtstack/flinkx/client/yarn/YarnPerJobClusterClientHelper.java
Patch:
@@ -184,6 +184,7 @@ private ClusterSpecification createClusterSpecification(JobDeployer jobDeployer)
                                 ValueUtil.getInt(
                                         conProp.getProperty(
                                                 JobManagerOptions.TOTAL_PROCESS_MEMORY.key())));
+                jobManagerMemoryMb = jobManagerMemoryMb >> 20;
             }
             if (conProp.containsKey(TaskManagerOptions.TOTAL_PROCESS_MEMORY.key())) {
                 taskManagerMemoryMb =
@@ -192,6 +193,8 @@ private ClusterSpecification createClusterSpecification(JobDeployer jobDeployer)
                                 ValueUtil.getInt(
                                         conProp.getProperty(
                                                 TaskManagerOptions.TOTAL_PROCESS_MEMORY.key())));
+
+                taskManagerMemoryMb = taskManagerMemoryMb >> 20;
             }
             if (conProp.containsKey(NUM_TASK_SLOTS.key())) {
                 slotsPerTaskManager = ValueUtil.getInt(conProp.get(NUM_TASK_SLOTS.key()));

File: flinkx-connectors/flinkx-connector-binlog/src/main/java/com/dtstack/flinkx/connector/binlog/source/BinlogSourceFactory.java
Patch:
@@ -60,8 +60,7 @@ public DataStream<RowData> createSource() {
         AbstractCDCRowConverter rowConverter;
         if (useAbstractBaseColumn) {
             rowConverter =
-                    new BinlogColumnConverter(
-                            binlogConf.isPavingData(), binlogConf.isSplitUpdate());
+                    new BinlogColumnConverter(binlogConf.isPavingData(), binlogConf.isSplit());
         } else {
             final RowType rowType =
                     TableUtil.createRowType(binlogConf.getColumn(), getRawTypeConverter());

File: flinkx-connectors/flinkx-connector-binlog/src/main/java/com/dtstack/flinkx/connector/binlog/table/BinlogDynamicTableFactory.java
Patch:
@@ -138,7 +138,7 @@ private BinlogConf getBinlogConf(ReadableConfig config) {
         binlogConf.setParallel(config.get(BinlogOptions.PARALLEL));
         binlogConf.setParallelThreadSize(config.get(BinlogOptions.PARALLEL_THREAD_SIZE));
         binlogConf.setGTIDMode(config.get(BinlogOptions.IS_GTID_MODE));
-        binlogConf.setSplitUpdate(true);
+        binlogConf.setSplit(true);
 
         return binlogConf;
     }

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/lookup/JdbcAllTableFunction.java
Patch:
@@ -31,10 +31,9 @@ public class JdbcAllTableFunction extends AbstractAllTableFunction {
 
     private static final long serialVersionUID = 1L;
     private static final Logger LOG = LoggerFactory.getLogger(JdbcAllTableFunction.class);
-
+    protected final JdbcDialect jdbcDialect;
     private final JdbcConf jdbcConf;
     private final String query;
-    protected final JdbcDialect jdbcDialect;
 
     public JdbcAllTableFunction(
             JdbcConf jdbcConf,

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/source/JdbcDynamicTableSource.java
Patch:
@@ -55,10 +55,10 @@ public class JdbcDynamicTableSource
 
     protected final JdbcConf jdbcConf;
     protected final LookupConf lookupConf;
-    protected TableSchema physicalSchema;
     protected final String dialectName;
     protected final JdbcDialect jdbcDialect;
     protected final JdbcInputFormatBuilder builder;
+    protected TableSchema physicalSchema;
 
     public JdbcDynamicTableSource(
             JdbcConf jdbcConf,

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/source/JdbcSourceFactory.java
Patch:
@@ -55,11 +55,10 @@
  */
 public abstract class JdbcSourceFactory extends SourceFactory {
 
-    protected JdbcConf jdbcConf;
-    protected JdbcDialect jdbcDialect;
-
     private static final int DEFAULT_FETCH_SIZE = 1024;
     private static final int DEFAULT_QUERY_TIMEOUT = 300;
+    protected JdbcConf jdbcConf;
+    protected JdbcDialect jdbcDialect;
 
     public JdbcSourceFactory(
             SyncConf syncConf, StreamExecutionEnvironment env, JdbcDialect jdbcDialect) {

File: flinkx-connectors/flinkx-connector-oraclelogminer/src/main/java/com/dtstack/flinkx/connector/oraclelogminer/source/OraclelogminerSourceFactory.java
Patch:
@@ -73,7 +73,7 @@ public DataStream<RowData> createSource() {
         if (useAbstractBaseColumn) {
             rowConverter =
                     new LogMinerColumnConverter(
-                            logMinerConf.isPavingData(), logMinerConf.isSplitUpdate());
+                            logMinerConf.isPavingData(), logMinerConf.isSplit());
         } else {
             final RowType rowType =
                     TableUtil.createRowType(logMinerConf.getColumn(), getRawTypeConverter());

File: flinkx-connectors/flinkx-connector-oraclelogminer/src/main/java/com/dtstack/flinkx/connector/oraclelogminer/table/OraclelogminerDynamicTableFactory.java
Patch:
@@ -123,7 +123,7 @@ private LogMinerConf getLogMinerConf(ReadableConfig config) {
         logMinerConf.setTransactionExpireTime(config.get(LogminerOptions.TRANSACTION_EXPIRE_TIME));
 
         logMinerConf.setPavingData(true);
-        logMinerConf.setSplitUpdate(true);
+        logMinerConf.setSplit(true);
 
         return logMinerConf;
     }

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/ConnectorNameConvertUtil.java
Patch:
@@ -37,6 +37,7 @@ public class ConnectorNameConvertUtil {
         connectorNameMap.put("hbase", new Tuple2<>("hbase14", "HBase14"));
         connectorNameMap.put("tidb", new Tuple2<>("mysql", "mysql"));
         connectorNameMap.put("restapi", new Tuple2<>("http", "http"));
+        connectorNameMap.put("adbpostgresql", new Tuple2<>("postgresql", "postgresql"));
     }
 
     public static String convertClassPrefix(String originName) {

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/ConnectorNameConvertUtil.java
Patch:
@@ -29,7 +29,7 @@
  */
 public class ConnectorNameConvertUtil {
 
-    // tuple f0 package name,f1 class name
+    // tuple f0 package name && directory name,f1 class name
     private static Map<String, Tuple2<String, String>> connectorNameMap = new HashMap<>();
 
     static {

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/PluginUtil.java
Patch:
@@ -105,7 +105,7 @@ public static Set<URL> getJarFileDirPath(
                         .replace(SOURCE_SUFFIX, "")
                         .replace(WRITER_SUFFIX, "")
                         .replace(SINK_SUFFIX, "");
-
+        name = ConnectorNameConvertUtil.convertPackageName(name);
         getJarUrlList(pluginRoot, suffix, name, urlSet);
         getJarUrlList(remotePluginPath, suffix, name, urlSet);
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/ConnectorNameConvertUtil.java
Patch:
@@ -29,7 +29,7 @@
  */
 public class ConnectorNameConvertUtil {
 
-    // tuple f0 package name,f1 class name
+    // tuple f0 package name && directory name,f1 class name
     private static Map<String, Tuple2<String, String>> connectorNameMap = new HashMap<>();
 
     static {

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/PluginUtil.java
Patch:
@@ -105,7 +105,7 @@ public static Set<URL> getJarFileDirPath(
                         .replace(SOURCE_SUFFIX, "")
                         .replace(WRITER_SUFFIX, "")
                         .replace(SINK_SUFFIX, "");
-
+        name = ConnectorNameConvertUtil.convertPackageName(name);
         getJarUrlList(pluginRoot, suffix, name, urlSet);
         getJarUrlList(remotePluginPath, suffix, name, urlSet);
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/FileSystemUtil.java
Patch:
@@ -125,7 +125,7 @@ public static UserGroupInformation getUGI(
         KerberosUtil.refreshConfig();
 
         return KerberosUtil.loginAndReturnUgi(
-                getConfiguration(hadoopConfig, defaultFs).get((KerberosUtil.KEY_PRINCIPAL_FILE)),
+                getConfiguration(hadoopConfig, defaultFs),
                 principal,
                 keytabFileName);
     }

File: flinkx-connectors/flinkx-connector-elasticsearch7/src/main/java/com/dtstack/flinkx/connector/elasticsearch7/table/lookup/ElasticsearchAllTableFunction.java
Patch:
@@ -24,9 +24,6 @@
 import com.dtstack.flinkx.converter.AbstractRowConverter;
 import com.dtstack.flinkx.lookup.AbstractAllTableFunction;
 import com.dtstack.flinkx.lookup.conf.LookupConf;
-
-import com.dtstack.flinkx.throwable.FlinkxException;
-
 import com.dtstack.flinkx.throwable.FlinkxRuntimeException;
 
 import org.apache.flink.table.data.GenericRowData;

File: flinkx-connectors/flinkx-connector-binlog/src/main/java/com/dtstack/flinkx/connector/binlog/inputformat/BinlogInputFormatBuilder.java
Patch:
@@ -27,6 +27,7 @@
 import com.dtstack.flinkx.util.GsonUtil;
 import com.dtstack.flinkx.util.RetryUtil;
 import com.dtstack.flinkx.util.TelnetUtil;
+
 import com.google.common.collect.Lists;
 import org.apache.commons.collections.CollectionUtils;
 import org.apache.commons.collections.MapUtils;

File: flinkx-connectors/flinkx-connector-binlog/src/main/java/com/dtstack/flinkx/connector/binlog/source/BinlogSourceFactory.java
Patch:
@@ -60,8 +60,7 @@ public DataStream<RowData> createSource() {
         AbstractCDCRowConverter rowConverter;
         if (useAbstractBaseColumn) {
             rowConverter =
-                    new BinlogColumnConverter(
-                            binlogConf.isPavingData(), binlogConf.isSplit());
+                    new BinlogColumnConverter(binlogConf.isPavingData(), binlogConf.isSplit());
         } else {
             final RowType rowType =
                     TableUtil.createRowType(binlogConf.getColumn(), getRawTypeConverter());

File: flinkx-connectors/flinkx-connector-phoenix5/src/main/java/com/dtstack/flinkx/connector/phoenix5/sink/Phoenix5OutputFormat.java
Patch:
@@ -19,7 +19,6 @@
 package com.dtstack.flinkx.connector.phoenix5.sink;
 
 import com.dtstack.flinkx.connector.jdbc.sink.JdbcOutputFormat;
-import com.dtstack.flinkx.connector.jdbc.statement.FieldNamedPreparedStatement;
 import com.dtstack.flinkx.connector.jdbc.util.JdbcUtil;
 import com.dtstack.flinkx.connector.phoenix5.converter.Phoenix5RawTypeConverter;
 import com.dtstack.flinkx.connector.phoenix5.util.Phoenix5Util;

File: flinkx-core/src/main/java/com/dtstack/flinkx/mapping/NameMapping.java
Patch:
@@ -22,6 +22,7 @@
 
 import com.dtstack.flinkx.element.ColumnRowData;
 import com.dtstack.flinkx.element.column.StringColumn;
+
 import org.apache.flink.table.data.RowData;
 
 import java.io.Serializable;

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/DataSyncFactoryUtil.java
Patch:
@@ -33,6 +33,7 @@
 import com.dtstack.flinkx.source.SourceFactory;
 import com.dtstack.flinkx.throwable.FlinkxRuntimeException;
 import com.dtstack.flinkx.throwable.NoRestartException;
+
 import org.apache.flink.api.common.functions.RuntimeContext;
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
 

File: flinkx-restore/flinkx-restore-mysql/src/main/java/com/dtstack/flinkx/restore/mysql/MysqlFetcher.java
Patch:
@@ -5,11 +5,14 @@
 import com.dtstack.flinkx.cdc.monitor.MonitorConf;
 import com.dtstack.flinkx.cdc.monitor.fetch.FetcherBase;
 import com.dtstack.flinkx.restore.mysql.utils.DataSourceUtil;
+
 import org.apache.flink.table.data.RowData;
+
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
 import javax.sql.DataSource;
+
 import java.sql.Connection;
 import java.sql.PreparedStatement;
 import java.sql.ResultSet;

File: flinkx-restore/flinkx-restore-mysql/src/main/java/com/dtstack/flinkx/restore/mysql/MysqlStore.java
Patch:
@@ -4,11 +4,14 @@
 import com.dtstack.flinkx.cdc.monitor.MonitorConf;
 import com.dtstack.flinkx.cdc.monitor.store.StoreBase;
 import com.dtstack.flinkx.restore.mysql.utils.DataSourceUtil;
+
 import org.apache.flink.table.data.RowData;
+
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
 import javax.sql.DataSource;
+
 import java.sql.Connection;
 import java.sql.PreparedStatement;
 import java.sql.SQLException;

File: flinkx-restore/flinkx-restore-mysql/src/main/java/com/dtstack/flinkx/restore/mysql/utils/DataSourceUtil.java
Patch:
@@ -5,6 +5,7 @@
 import org.slf4j.LoggerFactory;
 
 import javax.sql.DataSource;
+
 import java.sql.Connection;
 import java.sql.PreparedStatement;
 import java.sql.SQLException;

File: flinkx-connectors/flinkx-connector-binlog/src/main/java/com/dtstack/flinkx/connector/binlog/source/BinlogSourceFactory.java
Patch:
@@ -61,7 +61,7 @@ public DataStream<RowData> createSource() {
         if (useAbstractBaseColumn) {
             rowConverter =
                     new BinlogColumnConverter(
-                            binlogConf.isPavingData(), binlogConf.isSplitUpdate());
+                            binlogConf.isPavingData(), binlogConf.isSplit());
         } else {
             final RowType rowType =
                     TableUtil.createRowType(binlogConf.getColumn(), getRawTypeConverter());

File: flinkx-connectors/flinkx-connector-binlog/src/main/java/com/dtstack/flinkx/connector/binlog/table/BinlogDynamicTableFactory.java
Patch:
@@ -138,7 +138,7 @@ private BinlogConf getBinlogConf(ReadableConfig config) {
         binlogConf.setParallel(config.get(BinlogOptions.PARALLEL));
         binlogConf.setParallelThreadSize(config.get(BinlogOptions.PARALLEL_THREAD_SIZE));
         binlogConf.setGTIDMode(config.get(BinlogOptions.IS_GTID_MODE));
-        binlogConf.setSplitUpdate(true);
+        binlogConf.setSplit(true);
 
         return binlogConf;
     }

File: flinkx-connectors/flinkx-connector-elasticsearch7/src/main/java/com/dtstack/flinkx/connector/elasticsearch7/utils/ElasticsearchUtil.java
Patch:
@@ -59,7 +59,7 @@ public static RestHighLevelClient createClient(ElasticsearchConf elasticsearchCo
                                 .setConnectTimeout(elasticsearchConf.getConnectTimeout())
                                 .setConnectionRequestTimeout(elasticsearchConf.getRequestTimeout())
                                 .setSocketTimeout(elasticsearchConf.getSocketTimeout()));
-        if (elasticsearchConf.isAuthMesh()) {
+        if (elasticsearchConf.getPassword() != null && elasticsearchConf.getUsername() != null) {
             // 进行用户和密码认证
             final CredentialsProvider credentialsProvider = new BasicCredentialsProvider();
             credentialsProvider.setCredentials(

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/sink/PreparedStmtProxy.java
Patch:
@@ -144,10 +144,12 @@ public ResultSet executeQuery() throws SQLException {
         return currentFieldNamedPstmt.executeQuery();
     }
 
+    @Override
     public void addBatch() throws SQLException {
         currentFieldNamedPstmt.executeBatch();
     }
 
+    @Override
     public int[] executeBatch() throws SQLException {
         return currentFieldNamedPstmt.executeBatch();
     }

File: flinkx-connectors/flinkx-connector-phoenix5/src/main/java/com/dtstack/flinkx/connector/phoenix5/sink/Phoenix5OutputFormat.java
Patch:
@@ -55,9 +55,6 @@ protected void openInternal(int taskNumber, int numTasks) {
                 dbConn.setAutoCommit(false);
             }
             initColumnList();
-            fieldNamedPreparedStatement =
-                    FieldNamedPreparedStatement.prepareStatement(
-                            dbConn, prepareTemplates(), this.columnNameList.toArray(new String[0]));
 
             LOG.info("subTask[{}] wait finished", taskNumber);
         } catch (SQLException sqe) {

File: flinkx-core/src/main/java/com/dtstack/flinkx/cdc/CdcConf.java
Patch:
@@ -1,7 +1,7 @@
 package com.dtstack.flinkx.cdc;
 
-import com.dtstack.flinkx.cdc.store.FetcherConf;
-import com.dtstack.flinkx.cdc.store.StoreConf;
+import com.dtstack.flinkx.cdc.monitor.fetch.FetcherConf;
+import com.dtstack.flinkx.cdc.monitor.store.StoreConf;
 
 import java.io.Serializable;
 import java.util.StringJoiner;

File: flinkx-core/src/main/java/com/dtstack/flinkx/cdc/RestorationFlatMap.java
Patch:
@@ -1,8 +1,8 @@
 package com.dtstack.flinkx.cdc;
 
-import com.dtstack.flinkx.cdc.store.FetcherBase;
-import com.dtstack.flinkx.cdc.store.Monitor;
-import com.dtstack.flinkx.cdc.store.StoreBase;
+import com.dtstack.flinkx.cdc.monitor.Monitor;
+import com.dtstack.flinkx.cdc.monitor.fetch.FetcherBase;
+import com.dtstack.flinkx.cdc.monitor.store.StoreBase;
 import com.dtstack.flinkx.cdc.worker.WorkerManager;
 import com.dtstack.flinkx.element.ColumnRowData;
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/cdc/monitor/fetch/FetcherConf.java
Patch:
@@ -1,4 +1,4 @@
-package com.dtstack.flinkx.cdc.store;
+package com.dtstack.flinkx.cdc.monitor.fetch;
 
 import java.io.Serializable;
 import java.util.Map;

File: flinkx-core/src/main/java/com/dtstack/flinkx/cdc/monitor/store/StoreBase.java
Patch:
@@ -1,4 +1,4 @@
-package com.dtstack.flinkx.cdc.store;
+package com.dtstack.flinkx.cdc.monitor.store;
 
 import com.dtstack.flinkx.cdc.QueuesChamberlain;
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/cdc/monitor/store/StoreConf.java
Patch:
@@ -1,4 +1,4 @@
-package com.dtstack.flinkx.cdc.store;
+package com.dtstack.flinkx.cdc.monitor.store;
 
 import java.io.Serializable;
 import java.util.Map;

File: flinkx-core/src/main/java/com/dtstack/flinkx/cdc/worker/ChunkSplitter.java
Patch:
@@ -36,6 +36,7 @@ public class ChunkSplitter {
 
     /**
      * 创建worker任务分片
+     *
      * @param tableIdentities tableIdentities
      * @param chunkNum how many chunks want.
      * @return Chunk[]

File: flinkx-core/src/main/java/com/dtstack/flinkx/cdc/worker/Worker.java
Patch:
@@ -91,7 +91,7 @@ private void dealDmL(Deque<RowData> queue) {
     @Override
     public Integer call() throws Exception {
         send();
-        //返回当前分片的chunkNum给到WorkerOverseer
+        // 返回当前分片的chunkNum给到WorkerOverseer
         return chunk.getChunkNum();
     }
 }

File: flinkx-core/src/main/java/com/dtstack/flinkx/cdc/worker/WorkerOverseer.java
Patch:
@@ -91,7 +91,7 @@ private void wakeUp() {
         final int workerNum = workerExecutor.getMaximumPoolSize();
         Set<String> tableIdentities = chamberlain.unblockTableIdentities();
         if (!tableIdentities.isEmpty()) {
-            //创建任务分片
+            // 创建任务分片
             Chunk[] chunks = ChunkSplitter.createChunk(tableIdentities, workerNum);
             for (Chunk chunk : chunks) {
                 Worker worker = new Worker(chamberlain, collector, chunk, workerSize);

File: flinkx-core/src/main/java/com/dtstack/flinkx/conf/SyncConf.java
Patch:
@@ -21,11 +21,11 @@
 import com.dtstack.flinkx.mapping.NameMappingConf;
 import com.dtstack.flinkx.util.GsonUtil;
 
+import org.apache.flink.util.Preconditions;
+
 import org.apache.commons.collections.CollectionUtils;
 import org.apache.commons.lang3.StringUtils;
 
-import org.apache.flink.util.Preconditions;
-
 import java.io.Serializable;
 import java.util.List;
 import java.util.Map;

File: flinkx-core/src/main/java/com/dtstack/flinkx/mapping/MappingClient.java
Patch:
@@ -20,11 +20,11 @@
 
 package com.dtstack.flinkx.mapping;
 
+import org.apache.flink.table.data.RowData;
+
 import org.apache.commons.collections.CollectionUtils;
 import org.apache.commons.lang3.StringUtils;
 
-import org.apache.flink.table.data.RowData;
-
 import java.io.Serializable;
 import java.util.ArrayList;
 import java.util.HashMap;

File: flinkx-core/src/main/java/com/dtstack/flinkx/mapping/NameMapping.java
Patch:
@@ -21,7 +21,6 @@
 package com.dtstack.flinkx.mapping;
 
 import com.dtstack.flinkx.element.ColumnRowData;
-
 import com.dtstack.flinkx.element.column.NullColumn;
 import com.dtstack.flinkx.element.column.StringColumn;
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/mapping/NameMappingConf.java
Patch:
@@ -35,8 +35,8 @@ public class NameMappingConf implements Serializable {
     private static final long serialVersionUID = 1L;
 
     /**
-     * 名称匹配规则,配置source端对sink端表名映射关系
-     * like this: { "source":"aaa", "sink":"bbb" }, { "source":"ccc","sink":"ddd" }, ...
+     * 名称匹配规则,配置source端对sink端表名映射关系 like this: { "source":"aaa", "sink":"bbb" }, {
+     * "source":"ccc","sink":"ddd" }, ...
      */
     private List<Map<String, String>> tableMappings;
 

File: flinkx-restore/flinkx-restore-mysql/src/main/java/com/dtstack/flinkx/restore/mysql/MysqlStore.java
Patch:
@@ -1,8 +1,8 @@
 package com.dtstack.flinkx.restore.mysql;
 
 import com.dtstack.flinkx.cdc.DdlRowData;
-import com.dtstack.flinkx.cdc.store.StoreBase;
-import com.dtstack.flinkx.cdc.store.StoreConf;
+import com.dtstack.flinkx.cdc.monitor.store.StoreBase;
+import com.dtstack.flinkx.cdc.monitor.store.StoreConf;
 import com.dtstack.flinkx.restore.mysql.utils.DruidDataSourceUtil;
 
 import org.apache.flink.table.data.RowData;

File: flinkx-restore/flinkx-restore-mysql/src/main/java/com/dtstack/flinkx/restore/mysql/utils/DruidDataSourceUtil.java
Patch:
@@ -3,6 +3,7 @@
 import com.alibaba.druid.pool.DruidDataSource;
 
 import javax.sql.DataSource;
+
 import java.sql.SQLException;
 import java.util.Collections;
 import java.util.Map;

File: flinkx-core/src/main/java/com/dtstack/flinkx/Main.java
Patch:
@@ -18,8 +18,8 @@
 package com.dtstack.flinkx;
 
 import com.dtstack.flinkx.cdc.CdcConf;
-import com.dtstack.flinkx.cdc.mapping.NameMappingConf;
-import com.dtstack.flinkx.cdc.mapping.NameMappingFlatMap;
+import com.dtstack.flinkx.mapping.NameMappingConf;
+import com.dtstack.flinkx.mapping.NameMappingFlatMap;
 import com.dtstack.flinkx.cdc.RestorationFlatMap;
 import com.dtstack.flinkx.cdc.store.FetcherBase;
 import com.dtstack.flinkx.cdc.store.StoreBase;

File: flinkx-core/src/main/java/com/dtstack/flinkx/conf/ContentConf.java
Patch:
@@ -17,7 +17,7 @@
  */
 package com.dtstack.flinkx.conf;
 
-import com.dtstack.flinkx.cdc.mapping.NameMappingConf;
+import com.dtstack.flinkx.mapping.NameMappingConf;
 
 import java.io.Serializable;
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/conf/SyncConf.java
Patch:
@@ -18,7 +18,7 @@
 package com.dtstack.flinkx.conf;
 
 import com.dtstack.flinkx.cdc.CdcConf;
-import com.dtstack.flinkx.cdc.mapping.NameMappingConf;
+import com.dtstack.flinkx.mapping.NameMappingConf;
 import com.dtstack.flinkx.util.GsonUtil;
 
 import org.apache.commons.collections.CollectionUtils;

File: flinkx-core/src/main/java/com/dtstack/flinkx/mapping/Mapping.java
Patch:
@@ -18,7 +18,7 @@
  *
  */
 
-package com.dtstack.flinkx.cdc.mapping;
+package com.dtstack.flinkx.mapping;
 
 import com.dtstack.flinkx.cdc.DdlRowData;
 import com.dtstack.flinkx.element.ColumnRowData;

File: flinkx-core/src/main/java/com/dtstack/flinkx/mapping/MappingClient.java
Patch:
@@ -18,7 +18,7 @@
  *
  */
 
-package com.dtstack.flinkx.cdc.mapping;
+package com.dtstack.flinkx.mapping;
 
 import org.apache.commons.collections.CollectionUtils;
 import org.apache.commons.lang3.StringUtils;

File: flinkx-core/src/main/java/com/dtstack/flinkx/mapping/NameMapping.java
Patch:
@@ -18,7 +18,7 @@
  *
  */
 
-package com.dtstack.flinkx.cdc.mapping;
+package com.dtstack.flinkx.mapping;
 
 import com.dtstack.flinkx.element.ColumnRowData;
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/mapping/NameMappingConf.java
Patch:
@@ -18,7 +18,7 @@
  *
  */
 
-package com.dtstack.flinkx.cdc.mapping;
+package com.dtstack.flinkx.mapping;
 
 import java.io.Serializable;
 import java.util.List;

File: flinkx-core/src/main/java/com/dtstack/flinkx/mapping/NameMappingFlatMap.java
Patch:
@@ -18,7 +18,7 @@
  *
  */
 
-package com.dtstack.flinkx.cdc.mapping;
+package com.dtstack.flinkx.mapping;
 
 import org.apache.flink.api.common.functions.RichFlatMapFunction;
 import org.apache.flink.configuration.Configuration;

File: flinkx-core/src/main/java/com/dtstack/flinkx/mapping/PatternMapping.java
Patch:
@@ -18,7 +18,7 @@
  *
  */
 
-package com.dtstack.flinkx.cdc.mapping;
+package com.dtstack.flinkx.mapping;
 
 import org.apache.flink.table.data.RowData;
 

File: flinkx-connectors/flinkx-connector-elasticsearch7/src/main/java/com/dtstack/flinkx/connector/elasticsearch7/utils/ElasticsearchUtil.java
Patch:
@@ -59,7 +59,7 @@ public static RestHighLevelClient createClient(ElasticsearchConf elasticsearchCo
                                 .setConnectTimeout(elasticsearchConf.getConnectTimeout())
                                 .setConnectionRequestTimeout(elasticsearchConf.getRequestTimeout())
                                 .setSocketTimeout(elasticsearchConf.getSocketTimeout()));
-        if (elasticsearchConf.isAuthMesh()) {
+        if (elasticsearchConf.getPassword() != null && elasticsearchConf.getUsername() != null) {
             // 进行用户和密码认证
             final CredentialsProvider credentialsProvider = new BasicCredentialsProvider();
             credentialsProvider.setCredentials(

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/ConnectorNameConvertUtil.java
Patch:
@@ -29,7 +29,7 @@
  */
 public class ConnectorNameConvertUtil {
 
-    // tuple f0 package name,f1 class name
+    // tuple f0 package name && directory name,f1 class name
     private static Map<String, Tuple2<String, String>> connectorNameMap = new HashMap<>();
 
     static {

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/PluginUtil.java
Patch:
@@ -105,7 +105,7 @@ public static Set<URL> getJarFileDirPath(
                         .replace(SOURCE_SUFFIX, "")
                         .replace(WRITER_SUFFIX, "")
                         .replace(SINK_SUFFIX, "");
-
+        name = ConnectorNameConvertUtil.convertPackageName(name);
         getJarUrlList(pluginRoot, suffix, name, urlSet);
         getJarUrlList(remotePluginPath, suffix, name, urlSet);
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/ConnectorNameConvertUtil.java
Patch:
@@ -29,7 +29,7 @@
  */
 public class ConnectorNameConvertUtil {
 
-    // tuple f0 package name,f1 class name
+    // tuple f0 package name && directory name,f1 class name
     private static Map<String, Tuple2<String, String>> connectorNameMap = new HashMap<>();
 
     static {

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/PluginUtil.java
Patch:
@@ -105,7 +105,7 @@ public static Set<URL> getJarFileDirPath(
                         .replace(SOURCE_SUFFIX, "")
                         .replace(WRITER_SUFFIX, "")
                         .replace(SINK_SUFFIX, "");
-
+        name = ConnectorNameConvertUtil.convertPackageName(name);
         getJarUrlList(pluginRoot, suffix, name, urlSet);
         getJarUrlList(remotePluginPath, suffix, name, urlSet);
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/DataSyncFactoryUtil.java
Patch:
@@ -38,7 +38,6 @@
 import org.apache.flink.api.common.functions.RuntimeContext;
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
 
-import java.io.File;
 import java.lang.reflect.Constructor;
 import java.net.URL;
 import java.util.Set;
@@ -140,7 +139,7 @@ public static FetcherBase discoverFetcher(FetcherConf fetcherConf, SyncConf conf
                     PluginUtil.getPluginClassName(pluginType, OperatorType.fetcher);
 
             Set<URL> urlList =
-                    PluginUtil.getJarFileDirPath(pluginType, config.getPluginRoot(), null);
+                    PluginUtil.getJarFileDirPath(pluginType, config.getPluginRoot(), null, "");
 
             return ClassLoaderManager.newInstance(
                     urlList,
@@ -160,7 +159,7 @@ public static StoreBase discoverStore(StoreConf storeConf, SyncConf config) {
             String storePluginClassName =
                     PluginUtil.getPluginClassName(pluginType, OperatorType.store);
             Set<URL> urlList =
-                    PluginUtil.getJarFileDirPath(pluginType, config.getPluginRoot(), null);
+                    PluginUtil.getJarFileDirPath(pluginType, config.getPluginRoot(), null, "" );
 
             return ClassLoaderManager.newInstance(
                     urlList,

File: flinkx-core/src/main/java/com/dtstack/flinkx/cdc/QueuesChamberlain.java
Patch:
@@ -69,7 +69,7 @@ public void putRowData(RowData data, String tableIdentifier) {
             } else {
                 // 说明此时不存在该tableIdentifier的数据队列
                 Deque<RowData> dataDeque = new LinkedList<>();
-                dataDeque.addFirst(data);
+                dataDeque.add(data);
                 unblockQueues.put(tableIdentifier, dataDeque);
             }
         } finally {

File: flinkx-core/src/main/java/com/dtstack/flinkx/cdc/worker/WorkerManager.java
Patch:
@@ -49,7 +49,7 @@ public class WorkerManager implements Serializable {
 
     private final QueuesChamberlain chamberlain;
 
-    private Overseer overseer;
+    private WorkerOverseer overseer;
 
     private Collector<RowData> collector;
 
@@ -112,7 +112,7 @@ public void setCollector(Collector<RowData> collector) {
 
     /** 开启Overseer线程,持续监听unblockQueues */
     private void openOverseer() {
-        overseer = new Overseer(workerExecutor, chamberlain, collector, workerSize);
+        overseer = new WorkerOverseer(workerExecutor, chamberlain, collector, workerSize);
         overseerExecutor.execute(overseer);
     }
 }

File: flinkx-core/src/main/java/com/dtstack/flinkx/Main.java
Patch:
@@ -173,7 +173,8 @@ private static void exeSyncJob(
             CdcConf cdcConf = config.getCdcConf();
             Fetcher fetcher = DataSyncFactoryUtil.discoverFetcher(cdcConf.getFetcher(), config);
             Store store = DataSyncFactoryUtil.discoverStore(cdcConf.getStore(), config);
-            dataStreamSource.flatMap(new RestorationFlatMap(fetcher, store, cdcConf));
+            dataStreamSource =
+                    dataStreamSource.flatMap(new RestorationFlatMap(fetcher, store, cdcConf));
         }
 
         SpeedConf speed = config.getSpeed();

File: flinkx-connectors/flinkx-connector-hbase-1.4/src/main/java/com/dtstack/flinkx/connector/hbase14/table/HBaseDynamicTableSink.java
Patch:
@@ -83,7 +83,7 @@ public SinkFunctionProvider getSinkRuntimeProvider(Context context) {
         builder.setEncoding(conf.getEncoding());
         builder.setHbaseConfig(conf.getHbaseConfig());
         builder.setNullMode(conf.getNullMode());
-        builder.setTableName(conf.getTable());
+        builder.setTableName(conf.getTableName());
         builder.setRowkeyExpress(conf.getRowkeyExpress());
         builder.setVersionColumnIndex(conf.getVersionColumnIndex());
         builder.setVersionColumnValues(conf.getVersionColumnValue());

File: flinkx-connectors/flinkx-connector-hbase-1.4/src/main/java/com/dtstack/flinkx/connector/hbase14/table/HBaseOptions.java
Patch:
@@ -27,7 +27,7 @@
 
 public class HBaseOptions extends BaseFileOptions {
     public static final ConfigOption<String> TABLE_NAME =
-            ConfigOptions.key("table")
+            ConfigOptions.key("table-name")
                     .stringType()
                     .noDefaultValue()
                     .withDescription("The name of HBase table to connect.");

File: flinkx-connectors/flinkx-connector-hbase-1.4/src/main/java/com/dtstack/flinkx/connector/hbase14/table/Hbase14DynamicTableFactory.java
Patch:
@@ -165,7 +165,7 @@ private HBaseConf getHbaseConf(ReadableConfig config, Map<String, String> option
         HBaseConf conf = new HBaseConf();
         conf.setHbaseConfig(getHBaseClientProperties(options));
         String hTableName = config.get(TABLE_NAME);
-        conf.setTable(hTableName);
+        conf.setTableName(hTableName);
         String nullStringLiteral = config.get(NULL_STRING_LITERAL);
         conf.setNullMode(nullStringLiteral);
         return conf;

File: flinkx-connectors/flinkx-connector-kudu/src/main/java/com/dtstack/flinkx/connector/kudu/sink/KuduSinkFactory.java
Patch:
@@ -51,6 +51,7 @@ public KuduSinkFactory(SyncConf syncConf) {
                 JsonUtil.toObject(
                         JsonUtil.toJson(syncConf.getWriter().getParameter()), KuduSinkConf.class);
         sinkConf.setColumn(syncConf.getWriter().getFieldList());
+        sinkConf.setKerberos(sinkConf.conventHadoopConfig());
         super.initFlinkxCommonConf(sinkConf);
     }
 

File: flinkx-connectors/flinkx-connector-kudu/src/main/java/com/dtstack/flinkx/connector/kudu/source/KuduSourceFactory.java
Patch:
@@ -51,6 +51,7 @@ public KuduSourceFactory(SyncConf syncConf, StreamExecutionEnvironment env) {
                 JsonUtil.toObject(
                         JsonUtil.toJson(syncConf.getReader().getParameter()), KuduSourceConf.class);
         sourceConf.setColumn(syncConf.getReader().getFieldList());
+        sourceConf.setKerberos(sourceConf.conventHadoopConfig());
         super.initFlinkxCommonConf(sourceConf);
     }
 

File: flinkx-connectors/flinkx-connector-hbase-1.4/src/main/java/com/dtstack/flinkx/connector/hbase14/sink/HBase14SinkFactory.java
Patch:
@@ -39,7 +39,6 @@ public HBase14SinkFactory(SyncConf config) {
                         GsonUtil.GSON.toJson(config.getWriter().getParameter()), HBaseConf.class);
         super.initFlinkxCommonConf(hbaseConf);
         hbaseConf.setColumnMetaInfos(syncConf.getWriter().getFieldList());
-        hbaseConf.setTableName(syncConf.getWriter().getTable().getTableName());
     }
 
     @Override
@@ -51,7 +50,7 @@ public DataStreamSink<RowData> createSink(DataStream<RowData> dataSet) {
         builder.setHbaseConfig(hbaseConf.getHbaseConfig());
         builder.setNullMode(hbaseConf.getNullMode());
         builder.setRowkeyExpress(hbaseConf.getRowkeyExpress());
-        builder.setTableName(hbaseConf.getTableName());
+        builder.setTableName(hbaseConf.getTable());
         builder.setVersionColumnIndex(hbaseConf.getVersionColumnIndex());
         builder.setVersionColumnValues(hbaseConf.getVersionColumnValue());
         builder.setWalFlag(hbaseConf.getWalFlag());

File: flinkx-connectors/flinkx-connector-hbase-1.4/src/main/java/com/dtstack/flinkx/connector/hbase14/source/HBase14SourceFactory.java
Patch:
@@ -44,7 +44,6 @@ public HBase14SourceFactory(SyncConf syncConf, StreamExecutionEnvironment env) {
                         GsonUtil.GSON.toJson(syncConf.getReader().getParameter()), HBaseConf.class);
         super.initFlinkxCommonConf(config);
         config.setColumnMetaInfos(syncConf.getReader().getFieldList());
-        config.setTableName(syncConf.getReader().getTable().getTableName());
     }
 
     @Override
@@ -61,7 +60,7 @@ public DataStream<RowData> createSource() {
         builder.setColumnMetaInfos(config.getColumnMetaInfos());
         builder.setEncoding(config.getEncoding());
         builder.setHbaseConfig(config.getHbaseConfig());
-        builder.setTableName(config.getTableName());
+        builder.setTableName(config.getTable());
         builder.setEndRowKey(config.getEndRowkey());
         builder.setIsBinaryRowkey(config.isBinaryRowkey());
         builder.setScanCacheSize(config.getScanCacheSize());

File: flinkx-connectors/flinkx-connector-hbase-1.4/src/main/java/com/dtstack/flinkx/connector/hbase14/table/HBaseDynamicTableSink.java
Patch:
@@ -83,7 +83,7 @@ public SinkFunctionProvider getSinkRuntimeProvider(Context context) {
         builder.setEncoding(conf.getEncoding());
         builder.setHbaseConfig(conf.getHbaseConfig());
         builder.setNullMode(conf.getNullMode());
-        builder.setTableName(conf.getTableName());
+        builder.setTableName(conf.getTable());
         builder.setRowkeyExpress(conf.getRowkeyExpress());
         builder.setVersionColumnIndex(conf.getVersionColumnIndex());
         builder.setVersionColumnValues(conf.getVersionColumnValue());

File: flinkx-connectors/flinkx-connector-hbase-1.4/src/main/java/com/dtstack/flinkx/connector/hbase14/table/HBaseOptions.java
Patch:
@@ -27,7 +27,7 @@
 
 public class HBaseOptions extends BaseFileOptions {
     public static final ConfigOption<String> TABLE_NAME =
-            ConfigOptions.key("table-name")
+            ConfigOptions.key("table")
                     .stringType()
                     .noDefaultValue()
                     .withDescription("The name of HBase table to connect.");

File: flinkx-connectors/flinkx-connector-hbase-1.4/src/main/java/com/dtstack/flinkx/connector/hbase14/table/Hbase14DynamicTableFactory.java
Patch:
@@ -165,7 +165,7 @@ private HBaseConf getHbaseConf(ReadableConfig config, Map<String, String> option
         HBaseConf conf = new HBaseConf();
         conf.setHbaseConfig(getHBaseClientProperties(options));
         String hTableName = config.get(TABLE_NAME);
-        conf.setTableName(hTableName);
+        conf.setTable(hTableName);
         String nullStringLiteral = config.get(NULL_STRING_LITERAL);
         conf.setNullMode(nullStringLiteral);
         return conf;

File: flinkx-connectors/flinkx-connector-kudu/src/main/java/com/dtstack/flinkx/connector/kudu/conf/KuduCommonConf.java
Patch:
@@ -110,7 +110,9 @@ public void setAdminOperationTimeout(Long adminOperationTimeout) {
     }
 
     public KerberosConfig getKerberos() {
-        kerberos.judgeAndSetKrbEnabled();
+        if (kerberos != null) {
+            kerberos.judgeAndSetKrbEnabled();
+        }
         return kerberos;
     }
 

File: flinkx-connectors/flinkx-connector-kudu/src/main/java/com/dtstack/flinkx/connector/kudu/util/KuduUtil.java
Patch:
@@ -72,7 +72,8 @@ public class KuduUtil {
     public static KuduClient getKuduClient(KuduCommonConf config) {
         try {
             KerberosConfig kerberosConfig = config.getKerberos();
-            if (kerberosConfig.isEnableKrb()) {
+            if (kerberosConfig != null && kerberosConfig.isEnableKrb()) {
+
                 UserGroupInformation ugi = KerberosUtil.loginAndReturnUgi(kerberosConfig);
                 return ugi.doAs(
                         (PrivilegedExceptionAction<KuduClient>)

File: flinkx-connectors/flinkx-connector-doris/src/main/java/com/dtstack/flinkx/connector/doris/converter/DorisColumnConverter.java
Patch:
@@ -16,9 +16,9 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.dorisbatch.converter;
+package com.dtstack.flinkx.connector.doris.converter;
 
-import com.dtstack.flinkx.connector.dorisbatch.options.DorisConf;
+import com.dtstack.flinkx.connector.doris.options.DorisConf;
 import com.dtstack.flinkx.converter.AbstractRowConverter;
 import com.dtstack.flinkx.converter.ISerializationConverter;
 import com.dtstack.flinkx.element.ColumnRowData;

File: flinkx-connectors/flinkx-connector-doris/src/main/java/com/dtstack/flinkx/connector/doris/converter/DorisRowTypeConvert.java
Patch:
@@ -1,4 +1,4 @@
-package com.dtstack.flinkx.connector.dorisbatch.converter;
+package com.dtstack.flinkx.connector.doris.converter;
 
 /**
  * Doris 和 Flink 列类型映射关系

File: flinkx-connectors/flinkx-connector-doris/src/main/java/com/dtstack/flinkx/connector/doris/exception/DorisConnectFailedException.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.dorisbatch.exception;
+package com.dtstack.flinkx.connector.doris.exception;
 
 /**
  * @author tiezhu@dtstack

File: flinkx-connectors/flinkx-connector-doris/src/main/java/com/dtstack/flinkx/connector/doris/options/DorisConf.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.dorisbatch.options;
+package com.dtstack.flinkx.connector.doris.options;
 
 import com.dtstack.flinkx.conf.FlinkxCommonConf;
 import com.dtstack.flinkx.util.GsonUtil;

File: flinkx-connectors/flinkx-connector-doris/src/main/java/com/dtstack/flinkx/connector/doris/options/DorisConfBuilder.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.dorisbatch.options;
+package com.dtstack.flinkx.connector.doris.options;
 
 import java.util.List;
 import java.util.Properties;

File: flinkx-connectors/flinkx-connector-doris/src/main/java/com/dtstack/flinkx/connector/doris/options/DorisKeys.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.dorisbatch.options;
+package com.dtstack.flinkx.connector.doris.options;
 
 /**
  * @author tiezhu@dtstack

File: flinkx-connectors/flinkx-connector-doris/src/main/java/com/dtstack/flinkx/connector/doris/options/LoadConf.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.dorisbatch.options;
+package com.dtstack.flinkx.connector.doris.options;
 
 import java.io.Serializable;
 

File: flinkx-connectors/flinkx-connector-doris/src/main/java/com/dtstack/flinkx/connector/doris/options/LoadConfBuilder.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.dorisbatch.options;
+package com.dtstack.flinkx.connector.doris.options;
 
 /**
  * @author tiezhu@dtstack

File: flinkx-connectors/flinkx-connector-doris/src/main/java/com/dtstack/flinkx/connector/doris/rest/DorisStreamLoad.java
Patch:
@@ -16,10 +16,10 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.dorisbatch.rest;
+package com.dtstack.flinkx.connector.doris.rest;
 
-import com.dtstack.flinkx.connector.dorisbatch.options.DorisConf;
-import com.dtstack.flinkx.connector.dorisbatch.rest.module.RespContent;
+import com.dtstack.flinkx.connector.doris.options.DorisConf;
+import com.dtstack.flinkx.connector.doris.rest.module.RespContent;
 
 import com.fasterxml.jackson.databind.ObjectMapper;
 import org.apache.commons.lang3.StringUtils;

File: flinkx-connectors/flinkx-connector-doris/src/main/java/com/dtstack/flinkx/connector/doris/rest/module/Backend.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.dorisbatch.rest.module;
+package com.dtstack.flinkx.connector.doris.rest.module;
 
 import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
 import com.fasterxml.jackson.annotation.JsonProperty;

File: flinkx-connectors/flinkx-connector-doris/src/main/java/com/dtstack/flinkx/connector/doris/rest/module/BackendRow.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.dorisbatch.rest.module;
+package com.dtstack.flinkx.connector.doris.rest.module;
 
 import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
 import com.fasterxml.jackson.annotation.JsonProperty;

File: flinkx-connectors/flinkx-connector-doris/src/main/java/com/dtstack/flinkx/connector/doris/rest/module/Field.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.dorisbatch.rest.module;
+package com.dtstack.flinkx.connector.doris.rest.module;
 
 import org.apache.commons.lang3.builder.ToStringBuilder;
 

File: flinkx-connectors/flinkx-connector-doris/src/main/java/com/dtstack/flinkx/connector/doris/rest/module/PartitionDefinition.java
Patch:
@@ -16,9 +16,9 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.dorisbatch.rest.module;
+package com.dtstack.flinkx.connector.doris.rest.module;
 
-import com.dtstack.flinkx.connector.dorisbatch.options.DorisConf;
+import com.dtstack.flinkx.connector.doris.options.DorisConf;
 
 import java.io.Serializable;
 import java.util.Collections;

File: flinkx-connectors/flinkx-connector-doris/src/main/java/com/dtstack/flinkx/connector/doris/rest/module/QueryPlan.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.dorisbatch.rest.module;
+package com.dtstack.flinkx.connector.doris.rest.module;
 
 import java.util.Map;
 import java.util.Objects;

File: flinkx-connectors/flinkx-connector-doris/src/main/java/com/dtstack/flinkx/connector/doris/rest/module/RespContent.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.dorisbatch.rest.module;
+package com.dtstack.flinkx.connector.doris.rest.module;
 
 import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
 import com.fasterxml.jackson.annotation.JsonProperty;

File: flinkx-connectors/flinkx-connector-doris/src/main/java/com/dtstack/flinkx/connector/doris/rest/module/Schema.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.dorisbatch.rest.module;
+package com.dtstack.flinkx.connector.doris.rest.module;
 
 import org.apache.commons.lang3.builder.ToStringBuilder;
 

File: flinkx-connectors/flinkx-connector-doris/src/main/java/com/dtstack/flinkx/connector/doris/rest/module/Tablet.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.dorisbatch.rest.module;
+package com.dtstack.flinkx.connector.doris.rest.module;
 
 import java.util.List;
 import java.util.Objects;

File: flinkx-connectors/flinkx-connector-doris/src/main/java/com/dtstack/flinkx/connector/doris/sink/DorisOutputFormatBuilder.java
Patch:
@@ -16,9 +16,9 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.dorisbatch.sink;
+package com.dtstack.flinkx.connector.doris.sink;
 
-import com.dtstack.flinkx.connector.dorisbatch.options.DorisConf;
+import com.dtstack.flinkx.connector.doris.options.DorisConf;
 import com.dtstack.flinkx.sink.format.BaseRichOutputFormatBuilder;
 
 /**

File: flinkx-connectors/flinkx-connector-mysql/src/main/java/com/dtstack/flinkx/connector/mysql/table/MysqlDynamicTableFactory.java
Patch:
@@ -23,6 +23,7 @@
 import com.dtstack.flinkx.connector.jdbc.table.JdbcDynamicTableFactory;
 import com.dtstack.flinkx.connector.jdbc.util.JdbcUtil;
 import com.dtstack.flinkx.connector.mysql.dialect.MysqlDialect;
+
 import org.apache.flink.configuration.ReadableConfig;
 
 /**

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/source/JdbcDynamicTableSource.java
Patch:
@@ -125,6 +125,8 @@ public ScanRuntimeProvider getScanRuntimeProvider(ScanContext runtimeProviderCon
         }
         jdbcConf.setColumn(columnList);
 
+        // TODO sql任务使用增量同步或者间隔轮询时暂不支持增量指标写入外部存储，暂时设置为false
+        jdbcConf.setInitReporter(false);
         String increColumn = jdbcConf.getIncreColumn();
         if (StringUtils.isNotBlank(increColumn)) {
             FieldConf fieldConf =

File: flinkx-core/src/main/java/com/dtstack/flinkx/cdc/QueuesChamberlain.java
Patch:
@@ -69,7 +69,7 @@ public void putRowData(RowData data, String tableIdentifier) {
             } else {
                 // 说明此时不存在该tableIdentifier的数据队列
                 Deque<RowData> dataDeque = new LinkedList<>();
-                dataDeque.addFirst(data);
+                dataDeque.add(data);
                 unblockQueues.put(tableIdentifier, dataDeque);
             }
         } finally {

File: flinkx-core/src/main/java/com/dtstack/flinkx/cdc/worker/WorkerManager.java
Patch:
@@ -49,7 +49,7 @@ public class WorkerManager implements Serializable {
 
     private final QueuesChamberlain chamberlain;
 
-    private Overseer overseer;
+    private WorkerOverseer overseer;
 
     private Collector<RowData> collector;
 
@@ -112,7 +112,7 @@ public void setCollector(Collector<RowData> collector) {
 
     /** 开启Overseer线程,持续监听unblockQueues */
     private void openOverseer() {
-        overseer = new Overseer(workerExecutor, chamberlain, collector, workerSize);
+        overseer = new WorkerOverseer(workerExecutor, chamberlain, collector, workerSize);
         overseerExecutor.execute(overseer);
     }
 }

File: flinkx-core/src/main/java/com/dtstack/flinkx/Main.java
Patch:
@@ -173,7 +173,8 @@ private static void exeSyncJob(
             CdcConf cdcConf = config.getCdcConf();
             Fetcher fetcher = DataSyncFactoryUtil.discoverFetcher(cdcConf.getFetcher(), config);
             Store store = DataSyncFactoryUtil.discoverStore(cdcConf.getStore(), config);
-            dataStreamSource.flatMap(new RestorationFlatMap(fetcher, store, cdcConf));
+            dataStreamSource =
+                    dataStreamSource.flatMap(new RestorationFlatMap(fetcher, store, cdcConf));
         }
 
         SpeedConf speed = config.getSpeed();

File: flinkx-connectors/flinkx-connector-mongodb/src/main/java/com/dtstack/flinkx/connector/mongodb/table/options/MongoClientOptions.java
Patch:
@@ -28,11 +28,11 @@
  */
 public class MongoClientOptions {
 
-    public static final ConfigOption<String> URL =
-            ConfigOptions.key("url")
+    public static final ConfigOption<String> URI =
+            ConfigOptions.key("uri")
                     .stringType()
                     .noDefaultValue()
-                    .withDescription("the MongoDB url.");
+                    .withDescription("the MongoDB uri.");
 
     public static final ConfigOption<String> COLLECTION =
             ConfigOptions.key("collection")

File: flinkx-core/src/main/java/org/apache/flink/table/factories/FactoryUtil.java
Patch:
@@ -18,6 +18,7 @@
 
 package org.apache.flink.table.factories;
 
+import com.dtstack.flinkx.constants.ConstantValue;
 import com.dtstack.flinkx.util.FactoryHelper;
 
 import org.apache.flink.annotation.PublicEvolving;
@@ -270,7 +271,7 @@ public static <T extends Factory> T discoverFactory(
             String s = factoryIdentifier.substring(0, factoryIdentifier.length() - 2);
             FactoryHelper factoryHelper = factoryHelperThreadLocal.get();
             if (factoryHelper != null) {
-                factoryHelper.registerCachedFile(s, classLoader, false);
+                factoryHelper.registerCachedFile(s, classLoader, ConstantValue.CONNECTOR_DIR_NAME);
             }
         }
 

File: flinkx-core/src/main/java/org/apache/flink/table/factories/TableFactoryService.java
Patch:
@@ -18,6 +18,7 @@
 
 package org.apache.flink.table.factories;
 
+import com.dtstack.flinkx.constants.ConstantValue;
 import com.dtstack.flinkx.util.FactoryHelper;
 
 import org.apache.flink.api.java.tuple.Tuple2;
@@ -157,7 +158,8 @@ private static <T extends TableFactory> T findSingleInternal(
         if (StringUtils.isNotBlank(factoryIdentifier)) {
             factoryHelperThreadLocal
                     .get()
-                    .registerCachedFile(factoryIdentifier, classLoader.get(), true);
+                    .registerCachedFile(
+                            factoryIdentifier, classLoader.get(), ConstantValue.CONNECTOR_DIR_NAME);
         }
         // dtstack fixed end
 

File: flinkx-core/src/test/java/com/dtstack/flinkx/util/PluginUtilTest.java
Patch:
@@ -33,6 +33,7 @@ public void testGetJarFileDirPath() {
                 "F:\\dtstack_workplace\\project_workplace\\flinkx\\code\\flinkx\\syncplugins";
 
         Assert.assertEquals(
-                4, PluginUtil.getJarFileDirPath(pluginName, pluginRoot, remotePluginPath).size());
+                4,
+                PluginUtil.getJarFileDirPath(pluginName, pluginRoot, remotePluginPath, "").size());
     }
 }

File: flinkx-connectors/flinkx-connector-hbase-1.4/src/main/java/com/dtstack/flinkx/connector/hbase14/table/Hbase14DynamicTableFactory.java
Patch:
@@ -119,7 +119,9 @@ public DynamicTableSource createDynamicTableSource(Context context) {
         LookupConf lookupConf =
                 getLookupConf(config, context.getObjectIdentifier().getObjectName());
         HBaseTableSchema hbaseSchema = HBaseTableSchema.fromTableSchema(physicalSchema);
-        return new HBaseDynamicTableSource(conf, physicalSchema, lookupConf, hbaseSchema);
+        String nullStringLiteral = helper.getOptions().get(NULL_STRING_LITERAL);
+        return new HBaseDynamicTableSource(
+                conf, physicalSchema, lookupConf, hbaseSchema, nullStringLiteral);
     }
 
     private static void validatePrimaryKey(TableSchema schema) {

File: flinkx-connectors/flinkx-connector-redis/src/main/java/com/dtstack/flinkx/connector/redis/lookup/RedisAllTableFunction.java
Patch:
@@ -72,7 +72,9 @@ public void eval(Object... keys) {
         keyPattern
                 .append("_")
                 .append(Arrays.stream(keys).map(String::valueOf).collect(Collectors.joining("_")));
-        List<Map<String, Object>> cacheList = cacheRef.get().get(keyPattern.toString());
+        List<Map<String, Object>> cacheList =
+                ((Map<String, List<Map<String, Object>>>) cacheRef.get())
+                        .get(keyPattern.toString());
 
         // 有数据才往下发，(左/内)连接flink会做相应的处理
         if (!CollectionUtils.isEmpty(cacheList)) {

File: flinkx-core/src/main/java/com/dtstack/flinkx/lookup/AbstractAllTableFunction.java
Patch:
@@ -55,8 +55,7 @@ public abstract class AbstractAllTableFunction extends TableFunction<RowData> {
     /** 和维表join字段的名称 */
     protected final String[] keyNames;
     /** 缓存 */
-    protected AtomicReference<Map<String, List<Map<String, Object>>>> cacheRef =
-            new AtomicReference<>();
+    protected AtomicReference<Object> cacheRef = new AtomicReference<>();
     /** 定时加载 */
     private ScheduledExecutorService es;
     /** 维表配置 */
@@ -148,7 +147,8 @@ protected void buildCache(
      */
     public void eval(Object... keys) {
         String cacheKey = Arrays.stream(keys).map(String::valueOf).collect(Collectors.joining("_"));
-        List<Map<String, Object>> cacheList = cacheRef.get().get(cacheKey);
+        List<Map<String, Object>> cacheList =
+                ((Map<String, List<Map<String, Object>>>) (cacheRef.get())).get(cacheKey);
         // 有数据才往下发，(左/内)连接flink会做相应的处理
         if (!CollectionUtils.isEmpty(cacheList)) {
             cacheList.forEach(one -> collect(fillData(one)));

File: flinkx-connectors/flinkx-connector-mongodb/src/main/java/com/dtstack/flinkx/connector/mongodb/table/options/MongoClientOptions.java
Patch:
@@ -28,11 +28,11 @@
  */
 public class MongoClientOptions {
 
-    public static final ConfigOption<String> URL =
-            ConfigOptions.key("url")
+    public static final ConfigOption<String> URI =
+            ConfigOptions.key("uri")
                     .stringType()
                     .noDefaultValue()
-                    .withDescription("the MongoDB url.");
+                    .withDescription("the MongoDB uri.");
 
     public static final ConfigOption<String> COLLECTION =
             ConfigOptions.key("collection")

File: flinkx-connectors/flinkx-connector-hbase-1.4/src/main/java/com/dtstack/flinkx/connector/hbase14/table/HBaseDynamicTableSink.java
Patch:
@@ -83,7 +83,7 @@ public SinkFunctionProvider getSinkRuntimeProvider(Context context) {
         builder.setEncoding(conf.getEncoding());
         builder.setHbaseConfig(conf.getHbaseConfig());
         builder.setNullMode(conf.getNullMode());
-        builder.setTableName(conf.getTable());
+        builder.setTableName(conf.getTableName());
         builder.setRowkeyExpress(conf.getRowkeyExpress());
         builder.setVersionColumnIndex(conf.getVersionColumnIndex());
         builder.setVersionColumnValues(conf.getVersionColumnValue());

File: flinkx-connectors/flinkx-connector-hbase-1.4/src/main/java/com/dtstack/flinkx/connector/hbase14/table/HBaseOptions.java
Patch:
@@ -27,7 +27,7 @@
 
 public class HBaseOptions extends BaseFileOptions {
     public static final ConfigOption<String> TABLE_NAME =
-            ConfigOptions.key("table")
+            ConfigOptions.key("table-name")
                     .stringType()
                     .noDefaultValue()
                     .withDescription("The name of HBase table to connect.");

File: flinkx-connectors/flinkx-connector-hbase-1.4/src/main/java/com/dtstack/flinkx/connector/hbase14/table/Hbase14DynamicTableFactory.java
Patch:
@@ -165,7 +165,7 @@ private HBaseConf getHbaseConf(ReadableConfig config, Map<String, String> option
         HBaseConf conf = new HBaseConf();
         conf.setHbaseConfig(getHBaseClientProperties(options));
         String hTableName = config.get(TABLE_NAME);
-        conf.setTable(hTableName);
+        conf.setTableName(hTableName);
         String nullStringLiteral = config.get(NULL_STRING_LITERAL);
         conf.setNullMode(nullStringLiteral);
         return conf;

File: flinkx-connectors/flinkx-connector-kudu/src/main/java/com/dtstack/flinkx/connector/kudu/sink/KuduSinkFactory.java
Patch:
@@ -51,6 +51,7 @@ public KuduSinkFactory(SyncConf syncConf) {
                 JsonUtil.toObject(
                         JsonUtil.toJson(syncConf.getWriter().getParameter()), KuduSinkConf.class);
         sinkConf.setColumn(syncConf.getWriter().getFieldList());
+        sinkConf.setKerberos(sinkConf.conventHadoopConfig());
         super.initFlinkxCommonConf(sinkConf);
     }
 

File: flinkx-connectors/flinkx-connector-kudu/src/main/java/com/dtstack/flinkx/connector/kudu/source/KuduSourceFactory.java
Patch:
@@ -51,6 +51,7 @@ public KuduSourceFactory(SyncConf syncConf, StreamExecutionEnvironment env) {
                 JsonUtil.toObject(
                         JsonUtil.toJson(syncConf.getReader().getParameter()), KuduSourceConf.class);
         sourceConf.setColumn(syncConf.getReader().getFieldList());
+        sourceConf.setKerberos(sourceConf.conventHadoopConfig());
         super.initFlinkxCommonConf(sourceConf);
     }
 

File: flinkx-connectors/flinkx-connector-hbase-1.4/src/main/java/com/dtstack/flinkx/connector/hbase14/sink/HBase14SinkFactory.java
Patch:
@@ -39,7 +39,6 @@ public HBase14SinkFactory(SyncConf config) {
                         GsonUtil.GSON.toJson(config.getWriter().getParameter()), HBaseConf.class);
         super.initFlinkxCommonConf(hbaseConf);
         hbaseConf.setColumnMetaInfos(syncConf.getWriter().getFieldList());
-        hbaseConf.setTableName(syncConf.getWriter().getTable().getTableName());
     }
 
     @Override
@@ -51,7 +50,7 @@ public DataStreamSink<RowData> createSink(DataStream<RowData> dataSet) {
         builder.setHbaseConfig(hbaseConf.getHbaseConfig());
         builder.setNullMode(hbaseConf.getNullMode());
         builder.setRowkeyExpress(hbaseConf.getRowkeyExpress());
-        builder.setTableName(hbaseConf.getTableName());
+        builder.setTableName(hbaseConf.getTable());
         builder.setVersionColumnIndex(hbaseConf.getVersionColumnIndex());
         builder.setVersionColumnValues(hbaseConf.getVersionColumnValue());
         builder.setWalFlag(hbaseConf.getWalFlag());

File: flinkx-connectors/flinkx-connector-hbase-1.4/src/main/java/com/dtstack/flinkx/connector/hbase14/source/HBase14SourceFactory.java
Patch:
@@ -44,7 +44,6 @@ public HBase14SourceFactory(SyncConf syncConf, StreamExecutionEnvironment env) {
                         GsonUtil.GSON.toJson(syncConf.getReader().getParameter()), HBaseConf.class);
         super.initFlinkxCommonConf(config);
         config.setColumnMetaInfos(syncConf.getReader().getFieldList());
-        config.setTableName(syncConf.getReader().getTable().getTableName());
     }
 
     @Override
@@ -61,7 +60,7 @@ public DataStream<RowData> createSource() {
         builder.setColumnMetaInfos(config.getColumnMetaInfos());
         builder.setEncoding(config.getEncoding());
         builder.setHbaseConfig(config.getHbaseConfig());
-        builder.setTableName(config.getTableName());
+        builder.setTableName(config.getTable());
         builder.setEndRowKey(config.getEndRowkey());
         builder.setIsBinaryRowkey(config.isBinaryRowkey());
         builder.setScanCacheSize(config.getScanCacheSize());

File: flinkx-connectors/flinkx-connector-hbase-1.4/src/main/java/com/dtstack/flinkx/connector/hbase14/table/HBaseDynamicTableSink.java
Patch:
@@ -83,7 +83,7 @@ public SinkFunctionProvider getSinkRuntimeProvider(Context context) {
         builder.setEncoding(conf.getEncoding());
         builder.setHbaseConfig(conf.getHbaseConfig());
         builder.setNullMode(conf.getNullMode());
-        builder.setTableName(conf.getTableName());
+        builder.setTableName(conf.getTable());
         builder.setRowkeyExpress(conf.getRowkeyExpress());
         builder.setVersionColumnIndex(conf.getVersionColumnIndex());
         builder.setVersionColumnValues(conf.getVersionColumnValue());

File: flinkx-connectors/flinkx-connector-hbase-1.4/src/main/java/com/dtstack/flinkx/connector/hbase14/table/HBaseOptions.java
Patch:
@@ -27,7 +27,7 @@
 
 public class HBaseOptions extends BaseFileOptions {
     public static final ConfigOption<String> TABLE_NAME =
-            ConfigOptions.key("table-name")
+            ConfigOptions.key("table")
                     .stringType()
                     .noDefaultValue()
                     .withDescription("The name of HBase table to connect.");

File: flinkx-connectors/flinkx-connector-hbase-1.4/src/main/java/com/dtstack/flinkx/connector/hbase14/table/Hbase14DynamicTableFactory.java
Patch:
@@ -165,7 +165,7 @@ private HBaseConf getHbaseConf(ReadableConfig config, Map<String, String> option
         HBaseConf conf = new HBaseConf();
         conf.setHbaseConfig(getHBaseClientProperties(options));
         String hTableName = config.get(TABLE_NAME);
-        conf.setTableName(hTableName);
+        conf.setTable(hTableName);
         String nullStringLiteral = config.get(NULL_STRING_LITERAL);
         conf.setNullMode(nullStringLiteral);
         return conf;

File: flinkx-connectors/flinkx-connector-kudu/src/main/java/com/dtstack/flinkx/connector/kudu/conf/KuduCommonConf.java
Patch:
@@ -110,7 +110,9 @@ public void setAdminOperationTimeout(Long adminOperationTimeout) {
     }
 
     public KerberosConfig getKerberos() {
-        kerberos.judgeAndSetKrbEnabled();
+        if (kerberos != null) {
+            kerberos.judgeAndSetKrbEnabled();
+        }
         return kerberos;
     }
 

File: flinkx-connectors/flinkx-connector-kudu/src/main/java/com/dtstack/flinkx/connector/kudu/util/KuduUtil.java
Patch:
@@ -72,7 +72,8 @@ public class KuduUtil {
     public static KuduClient getKuduClient(KuduCommonConf config) {
         try {
             KerberosConfig kerberosConfig = config.getKerberos();
-            if (kerberosConfig.isEnableKrb()) {
+            if (kerberosConfig != null && kerberosConfig.isEnableKrb()) {
+
                 UserGroupInformation ugi = KerberosUtil.loginAndReturnUgi(kerberosConfig);
                 return ugi.doAs(
                         (PrivilegedExceptionAction<KuduClient>)

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/source/JdbcDynamicTableSource.java
Patch:
@@ -125,6 +125,8 @@ public ScanRuntimeProvider getScanRuntimeProvider(ScanContext runtimeProviderCon
         }
         jdbcConf.setColumn(columnList);
 
+        // TODO sql任务使用增量同步或者间隔轮询时暂不支持增量指标写入外部存储，暂时设置为false
+        jdbcConf.setInitReporter(false);
         String increColumn = jdbcConf.getIncreColumn();
         if (StringUtils.isNotBlank(increColumn)) {
             FieldConf fieldConf =

File: flinkx-connectors/flinkx-connector-hbase-1.4/src/main/java/com/dtstack/flinkx/connector/hbase14/table/HBaseDynamicTableSink.java
Patch:
@@ -83,7 +83,7 @@ public SinkFunctionProvider getSinkRuntimeProvider(Context context) {
         builder.setEncoding(conf.getEncoding());
         builder.setHbaseConfig(conf.getHbaseConfig());
         builder.setNullMode(conf.getNullMode());
-        builder.setTableName(conf.getTable());
+        builder.setTableName(conf.getTableName());
         builder.setRowkeyExpress(conf.getRowkeyExpress());
         builder.setVersionColumnIndex(conf.getVersionColumnIndex());
         builder.setVersionColumnValues(conf.getVersionColumnValue());

File: flinkx-connectors/flinkx-connector-hbase-1.4/src/main/java/com/dtstack/flinkx/connector/hbase14/table/HBaseDynamicTableSource.java
Patch:
@@ -91,7 +91,7 @@ public ScanRuntimeProvider getScanRuntimeProvider(ScanContext runtimeProviderCon
         builder.setConfig(conf);
         builder.setEncoding(conf.getEncoding());
         builder.setHbaseConfig(conf.getHbaseConfig());
-        builder.setTableName(conf.getTable());
+        builder.setTableName(conf.getTableName());
         AbstractRowConverter rowConverter = new HBaseConverter(rowType);
         builder.setRowConverter(rowConverter);
         return ParallelSourceFunctionProvider.of(

File: flinkx-connectors/flinkx-connector-hbase-1.4/src/main/java/com/dtstack/flinkx/connector/hbase14/table/HBaseOptions.java
Patch:
@@ -27,7 +27,7 @@
 
 public class HBaseOptions extends BaseFileOptions {
     public static final ConfigOption<String> TABLE_NAME =
-            ConfigOptions.key("table")
+            ConfigOptions.key("table-name")
                     .stringType()
                     .noDefaultValue()
                     .withDescription("The name of HBase table to connect.");

File: flinkx-connectors/flinkx-connector-hbase-1.4/src/main/java/com/dtstack/flinkx/connector/hbase14/table/Hbase14DynamicTableFactory.java
Patch:
@@ -163,7 +163,7 @@ private HBaseConf getHbaseConf(ReadableConfig config, Map<String, String> option
         HBaseConf conf = new HBaseConf();
         conf.setHbaseConfig(getHBaseClientProperties(options));
         String hTableName = config.get(TABLE_NAME);
-        conf.setTable(hTableName);
+        conf.setTableName(hTableName);
         String nullStringLiteral = config.get(NULL_STRING_LITERAL);
         conf.setNullMode(nullStringLiteral);
         return conf;

File: flinkx-connectors/flinkx-connector-hbase-1.4/src/main/java/com/dtstack/flinkx/connector/hbase14/table/lookup/HBaseAllTableFunction.java
Patch:
@@ -155,7 +155,7 @@ protected void loadData(Object cacheRef) {
                 conn = ConnectionFactory.createConnection(conf);
             }
 
-            table = conn.getTable(TableName.valueOf(hbaseConf.getTable()));
+            table = conn.getTable(TableName.valueOf(hbaseConf.getTableName()));
             resultScanner = table.getScanner(new Scan());
             for (Result r : resultScanner) {
                 Map<String, Object> kv = new HashMap<>();

File: flinkx-connectors/flinkx-connector-hbase-1.4/src/main/java/com/dtstack/flinkx/connector/hbase14/table/lookup/HBaseLruTableFunction.java
Patch:
@@ -86,7 +86,7 @@ public HBaseLruTableFunction(
     public void open(FunctionContext context) throws Exception {
         super.open(context);
         this.serde = new AsyncHBaseSerde(hbaseTableSchema, conf.getNullMode());
-        tableName = conf.getTable();
+        tableName = conf.getTableName();
         colNames =
                 conf.getColumnMetaInfos().stream().map(FieldConf::getName).toArray(String[]::new);
         Map<String, Object> hbaseConfig = conf.getHbaseConfig();

File: flinkx-connectors/flinkx-connector-kudu/src/main/java/com/dtstack/flinkx/connector/kudu/sink/KuduSinkFactory.java
Patch:
@@ -51,6 +51,7 @@ public KuduSinkFactory(SyncConf syncConf) {
                 JsonUtil.toObject(
                         JsonUtil.toJson(syncConf.getWriter().getParameter()), KuduSinkConf.class);
         sinkConf.setColumn(syncConf.getWriter().getFieldList());
+        sinkConf.setKerberos(sinkConf.conventHadoopConfig());
         super.initFlinkxCommonConf(sinkConf);
     }
 

File: flinkx-connectors/flinkx-connector-kudu/src/main/java/com/dtstack/flinkx/connector/kudu/source/KuduSourceFactory.java
Patch:
@@ -51,6 +51,7 @@ public KuduSourceFactory(SyncConf syncConf, StreamExecutionEnvironment env) {
                 JsonUtil.toObject(
                         JsonUtil.toJson(syncConf.getReader().getParameter()), KuduSourceConf.class);
         sourceConf.setColumn(syncConf.getReader().getFieldList());
+        sourceConf.setKerberos(sourceConf.conventHadoopConfig());
         super.initFlinkxCommonConf(sourceConf);
     }
 

File: flinkx-connectors/flinkx-connector-hbase-1.4/src/main/java/com/dtstack/flinkx/connector/hbase14/sink/HBase14SinkFactory.java
Patch:
@@ -39,7 +39,6 @@ public HBase14SinkFactory(SyncConf config) {
                         GsonUtil.GSON.toJson(config.getWriter().getParameter()), HBaseConf.class);
         super.initFlinkxCommonConf(hbaseConf);
         hbaseConf.setColumnMetaInfos(syncConf.getWriter().getFieldList());
-        hbaseConf.setTableName(syncConf.getWriter().getTable().getTableName());
     }
 
     @Override
@@ -51,7 +50,7 @@ public DataStreamSink<RowData> createSink(DataStream<RowData> dataSet) {
         builder.setHbaseConfig(hbaseConf.getHbaseConfig());
         builder.setNullMode(hbaseConf.getNullMode());
         builder.setRowkeyExpress(hbaseConf.getRowkeyExpress());
-        builder.setTableName(hbaseConf.getTableName());
+        builder.setTableName(hbaseConf.getTable());
         builder.setVersionColumnIndex(hbaseConf.getVersionColumnIndex());
         builder.setVersionColumnValues(hbaseConf.getVersionColumnValue());
         builder.setWalFlag(hbaseConf.getWalFlag());

File: flinkx-connectors/flinkx-connector-hbase-1.4/src/main/java/com/dtstack/flinkx/connector/hbase14/source/HBase14SourceFactory.java
Patch:
@@ -44,7 +44,6 @@ public HBase14SourceFactory(SyncConf syncConf, StreamExecutionEnvironment env) {
                         GsonUtil.GSON.toJson(syncConf.getReader().getParameter()), HBaseConf.class);
         super.initFlinkxCommonConf(config);
         config.setColumnMetaInfos(syncConf.getReader().getFieldList());
-        config.setTableName(syncConf.getReader().getTable().getTableName());
     }
 
     @Override
@@ -61,7 +60,7 @@ public DataStream<RowData> createSource() {
         builder.setColumnMetaInfos(config.getColumnMetaInfos());
         builder.setEncoding(config.getEncoding());
         builder.setHbaseConfig(config.getHbaseConfig());
-        builder.setTableName(config.getTableName());
+        builder.setTableName(config.getTable());
         builder.setEndRowKey(config.getEndRowkey());
         builder.setIsBinaryRowkey(config.isBinaryRowkey());
         builder.setScanCacheSize(config.getScanCacheSize());

File: flinkx-connectors/flinkx-connector-hbase-1.4/src/main/java/com/dtstack/flinkx/connector/hbase14/table/HBaseDynamicTableSink.java
Patch:
@@ -83,7 +83,7 @@ public SinkFunctionProvider getSinkRuntimeProvider(Context context) {
         builder.setEncoding(conf.getEncoding());
         builder.setHbaseConfig(conf.getHbaseConfig());
         builder.setNullMode(conf.getNullMode());
-        builder.setTableName(conf.getTableName());
+        builder.setTableName(conf.getTable());
         builder.setRowkeyExpress(conf.getRowkeyExpress());
         builder.setVersionColumnIndex(conf.getVersionColumnIndex());
         builder.setVersionColumnValues(conf.getVersionColumnValue());

File: flinkx-connectors/flinkx-connector-hbase-1.4/src/main/java/com/dtstack/flinkx/connector/hbase14/table/HBaseDynamicTableSource.java
Patch:
@@ -91,7 +91,7 @@ public ScanRuntimeProvider getScanRuntimeProvider(ScanContext runtimeProviderCon
         builder.setConfig(conf);
         builder.setEncoding(conf.getEncoding());
         builder.setHbaseConfig(conf.getHbaseConfig());
-        builder.setTableName(conf.getTableName());
+        builder.setTableName(conf.getTable());
         AbstractRowConverter rowConverter = new HBaseConverter(rowType);
         builder.setRowConverter(rowConverter);
         return ParallelSourceFunctionProvider.of(

File: flinkx-connectors/flinkx-connector-hbase-1.4/src/main/java/com/dtstack/flinkx/connector/hbase14/table/HBaseOptions.java
Patch:
@@ -27,7 +27,7 @@
 
 public class HBaseOptions extends BaseFileOptions {
     public static final ConfigOption<String> TABLE_NAME =
-            ConfigOptions.key("table-name")
+            ConfigOptions.key("table")
                     .stringType()
                     .noDefaultValue()
                     .withDescription("The name of HBase table to connect.");

File: flinkx-connectors/flinkx-connector-hbase-1.4/src/main/java/com/dtstack/flinkx/connector/hbase14/table/Hbase14DynamicTableFactory.java
Patch:
@@ -163,7 +163,7 @@ private HBaseConf getHbaseConf(ReadableConfig config, Map<String, String> option
         HBaseConf conf = new HBaseConf();
         conf.setHbaseConfig(getHBaseClientProperties(options));
         String hTableName = config.get(TABLE_NAME);
-        conf.setTableName(hTableName);
+        conf.setTable(hTableName);
         String nullStringLiteral = config.get(NULL_STRING_LITERAL);
         conf.setNullMode(nullStringLiteral);
         return conf;

File: flinkx-connectors/flinkx-connector-hbase-1.4/src/main/java/com/dtstack/flinkx/connector/hbase14/table/lookup/HBaseAllTableFunction.java
Patch:
@@ -155,7 +155,7 @@ protected void loadData(Object cacheRef) {
                 conn = ConnectionFactory.createConnection(conf);
             }
 
-            table = conn.getTable(TableName.valueOf(hbaseConf.getTableName()));
+            table = conn.getTable(TableName.valueOf(hbaseConf.getTable()));
             resultScanner = table.getScanner(new Scan());
             for (Result r : resultScanner) {
                 Map<String, Object> kv = new HashMap<>();

File: flinkx-connectors/flinkx-connector-hbase-1.4/src/main/java/com/dtstack/flinkx/connector/hbase14/table/lookup/HBaseLruTableFunction.java
Patch:
@@ -86,7 +86,7 @@ public HBaseLruTableFunction(
     public void open(FunctionContext context) throws Exception {
         super.open(context);
         this.serde = new AsyncHBaseSerde(hbaseTableSchema, conf.getNullMode());
-        tableName = conf.getTableName();
+        tableName = conf.getTable();
         colNames =
                 conf.getColumnMetaInfos().stream().map(FieldConf::getName).toArray(String[]::new);
         Map<String, Object> hbaseConfig = conf.getHbaseConfig();

File: flinkx-connectors/flinkx-connector-kudu/src/main/java/com/dtstack/flinkx/connector/kudu/conf/KuduCommonConf.java
Patch:
@@ -110,7 +110,9 @@ public void setAdminOperationTimeout(Long adminOperationTimeout) {
     }
 
     public KerberosConfig getKerberos() {
-        kerberos.judgeAndSetKrbEnabled();
+        if (kerberos != null) {
+            kerberos.judgeAndSetKrbEnabled();
+        }
         return kerberos;
     }
 

File: flinkx-connectors/flinkx-connector-kudu/src/main/java/com/dtstack/flinkx/connector/kudu/util/KuduUtil.java
Patch:
@@ -72,7 +72,8 @@ public class KuduUtil {
     public static KuduClient getKuduClient(KuduCommonConf config) {
         try {
             KerberosConfig kerberosConfig = config.getKerberos();
-            if (kerberosConfig.isEnableKrb()) {
+            if (kerberosConfig != null && kerberosConfig.isEnableKrb()) {
+
                 UserGroupInformation ugi = KerberosUtil.loginAndReturnUgi(kerberosConfig);
                 return ugi.doAs(
                         (PrivilegedExceptionAction<KuduClient>)

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/StringUtil.java
Patch:
@@ -355,6 +355,7 @@ public static String stringToTimestampStr(String location, ColumnType type) {
         try {
             switch (type) {
                 case TIMESTAMP:
+                case DATETIME:
                     return String.valueOf(Timestamp.valueOf(location).getTime());
                 case DATE:
                     return String.valueOf(

File: flinkx-core/src/main/java/com/dtstack/flinkx/element/ColumnRowData.java
Patch:
@@ -17,6 +17,7 @@
  */
 package com.dtstack.flinkx.element;
 
+import com.dtstack.flinkx.element.column.NullColumn;
 import com.dtstack.flinkx.throwable.FlinkxRuntimeException;
 
 import org.apache.flink.table.data.ArrayData;
@@ -234,7 +235,8 @@ private String buildString(StringBuilder sb) {
             if (i != 0) {
                 sb.append(",");
             }
-            sb.append(StringUtils.arrayAwareToString(columnList.get(i).asString()));
+            sb.append(StringUtils.arrayAwareToString((columnList.get(i) == null
+                    ? new NullColumn() : columnList.get(i)).asString()));
         }
         sb.append(")");
         return sb.toString();

File: flinkx-connectors/flinkx-connector-mongodb/src/main/java/com/dtstack/flinkx/connector/mongodb/table/options/MongoClientOptions.java
Patch:
@@ -28,11 +28,11 @@
  */
 public class MongoClientOptions {
 
-    public static final ConfigOption<String> URI =
-            ConfigOptions.key("uri")
+    public static final ConfigOption<String> URL =
+            ConfigOptions.key("url")
                     .stringType()
                     .noDefaultValue()
-                    .withDescription("the MongoDB uri.");
+                    .withDescription("the MongoDB url.");
 
     public static final ConfigOption<String> COLLECTION =
             ConfigOptions.key("collection")

File: flinkx-connectors/flinkx-connector-mongodb/src/main/java/com/dtstack/flinkx/connector/mongodb/table/options/MongoClientOptions.java
Patch:
@@ -28,11 +28,11 @@
  */
 public class MongoClientOptions {
 
-    public static final ConfigOption<String> URI =
-            ConfigOptions.key("uri")
+    public static final ConfigOption<String> URL =
+            ConfigOptions.key("url")
                     .stringType()
                     .noDefaultValue()
-                    .withDescription("the MongoDB uri.");
+                    .withDescription("the MongoDB url.");
 
     public static final ConfigOption<String> COLLECTION =
             ConfigOptions.key("collection")

File: flinkx-connectors/flinkx-connector-mongodb/src/main/java/com/dtstack/flinkx/connector/mongodb/table/options/MongoClientOptions.java
Patch:
@@ -28,11 +28,11 @@
  */
 public class MongoClientOptions {
 
-    public static final ConfigOption<String> URL =
-            ConfigOptions.key("url")
+    public static final ConfigOption<String> URI =
+            ConfigOptions.key("uri")
                     .stringType()
                     .noDefaultValue()
-                    .withDescription("the MongoDB url.");
+                    .withDescription("the MongoDB uri.");
 
     public static final ConfigOption<String> COLLECTION =
             ConfigOptions.key("collection")

File: flinkx-connectors/flinkx-connector-mongodb/src/main/java/com/dtstack/flinkx/connector/mongodb/table/options/MongoClientOptions.java
Patch:
@@ -28,11 +28,11 @@
  */
 public class MongoClientOptions {
 
-    public static final ConfigOption<String> URL =
-            ConfigOptions.key("url")
+    public static final ConfigOption<String> URI =
+            ConfigOptions.key("uri")
                     .stringType()
                     .noDefaultValue()
-                    .withDescription("the MongoDB url.");
+                    .withDescription("the MongoDB uri.");
 
     public static final ConfigOption<String> COLLECTION =
             ConfigOptions.key("collection")

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/StringUtil.java
Patch:
@@ -355,6 +355,7 @@ public static String stringToTimestampStr(String location, ColumnType type) {
         try {
             switch (type) {
                 case TIMESTAMP:
+                case DATETIME:
                     return String.valueOf(Timestamp.valueOf(location).getTime());
                 case DATE:
                     return String.valueOf(

File: flinkx-core/src/main/java/com/dtstack/flinkx/element/ColumnRowData.java
Patch:
@@ -17,6 +17,7 @@
  */
 package com.dtstack.flinkx.element;
 
+import com.dtstack.flinkx.element.column.NullColumn;
 import com.dtstack.flinkx.throwable.FlinkxRuntimeException;
 
 import org.apache.flink.table.data.ArrayData;
@@ -234,7 +235,8 @@ private String buildString(StringBuilder sb) {
             if (i != 0) {
                 sb.append(",");
             }
-            sb.append(StringUtils.arrayAwareToString(columnList.get(i).asString()));
+            sb.append(StringUtils.arrayAwareToString((columnList.get(i) == null
+                    ? new NullColumn() : columnList.get(i)).asString()));
         }
         sb.append(")");
         return sb.toString();

File: flinkx-connectors/flinkx-connector-mongodb/src/main/java/com/dtstack/flinkx/connector/mongodb/table/options/MongoClientOptions.java
Patch:
@@ -28,11 +28,11 @@
  */
 public class MongoClientOptions {
 
-    public static final ConfigOption<String> URL =
-            ConfigOptions.key("url")
+    public static final ConfigOption<String> URI =
+            ConfigOptions.key("uri")
                     .stringType()
                     .noDefaultValue()
-                    .withDescription("the MongoDB url.");
+                    .withDescription("the MongoDB uri.");
 
     public static final ConfigOption<String> COLLECTION =
             ConfigOptions.key("collection")

File: flinkx-connectors/flinkx-connector-file/src/main/java/com/dtstack/flinkx/connector/file/source/FileInputFormat.java
Patch:
@@ -96,7 +96,7 @@ protected RowData nextRecordInternal(RowData rowData) throws ReadRecordException
         try {
             rowData = rowConverter.toInternal(line);
         } catch (Exception e) {
-            throw new ReadRecordException("", e, 0, rowData);
+            throw new ReadRecordException("", e, 0, line);
         }
         return rowData;
     }

File: flinkx-core/src/main/java/com/dtstack/flinkx/source/format/BaseRichInputFormat.java
Patch:
@@ -191,7 +191,7 @@ public RowData nextRecord(RowData rowData) {
         try {
             internalRow = nextRecordInternal(rowData);
         } catch (ReadRecordException e) {
-            dirtyManager.collect(rowData.toString(), e, null, getRuntimeContext());
+            dirtyManager.collect(e.getRowData().toString(), e, null, getRuntimeContext());
         }
         if (internalRow != null) {
             updateDuration();

File: flinkx-connectors/flinkx-connector-elasticsearch7/src/main/java/com/dtstack/flinkx/connector/elasticsearch7/table/Elasticsearch7DynamicTableFactory.java
Patch:
@@ -93,6 +93,7 @@ public class Elasticsearch7DynamicTableFactory
                             BULK_FLUSH_BACKOFF_TYPE_OPTION,
                             BULK_FLUSH_BACKOFF_MAX_RETRIES_OPTION,
                             BULK_FLUSH_BACKOFF_DELAY_OPTION,
+                            SINK_PARALLELISM,
                             CONNECTION_MAX_RETRY_TIMEOUT_OPTION,
                             CONNECTION_PATH_PREFIX,
                             FORMAT_OPTION,

File: flinkx-connectors/flinkx-connector-kudu/src/main/java/com/dtstack/flinkx/connector/kudu/table/KuduDynamicTableFactory.java
Patch:
@@ -65,6 +65,7 @@
 import static com.dtstack.flinkx.table.options.SinkOptions.SINK_BUFFER_FLUSH_INTERVAL;
 import static com.dtstack.flinkx.table.options.SinkOptions.SINK_BUFFER_FLUSH_MAX_ROWS;
 import static com.dtstack.flinkx.table.options.SinkOptions.SINK_MAX_RETRIES;
+import static com.dtstack.flinkx.table.options.SinkOptions.SINK_PARALLELISM;
 
 /**
  * @author tiezhu
@@ -156,6 +157,7 @@ public Set<ConfigOption<?>> optionalOptions() {
         optionalOptions.add(SINK_BUFFER_FLUSH_MAX_ROWS);
         optionalOptions.add(SINK_BUFFER_FLUSH_INTERVAL);
         optionalOptions.add(SINK_MAX_RETRIES);
+        optionalOptions.add(SINK_PARALLELISM);
 
         // kerberos
         optionalOptions.add(PRINCIPAL);

File: flinkx-core/src/main/java/com/dtstack/flinkx/dirty/consumer/DirtyDataCollector.java
Patch:
@@ -53,7 +53,7 @@ public abstract class DirtyDataCollector implements Runnable, Serializable {
     /**
      * This is the limit on the max consumed-data. The consumer would to be killed with throwing a
      * {@link NoRestartException} when the consumed-count exceed the limit. The default is 1, which
-     * means task fails once dirty data occurs..
+     * means task fails once dirty data occurs.
      */
     protected long maxConsumed = 1L;
 
@@ -131,9 +131,9 @@ protected void addFailedConsumed(Throwable cause, long failedCount) {
                     "dirty-plugins consume failed.",
                     cause,
                     printRate,
-                    failedConsumed.get().getLocalValue());
+                    FAILED_CONSUMED_COUNTER.getLocalValue());
 
-            if (failedConsumed.get().getLocalValue() >= maxFailedConsumed) {
+            if (FAILED_CONSUMED_COUNTER.getLocalValue() >= maxFailedConsumed) {
                 throw new NoRestartException(
                         String.format(
                                 "The dirty consumer shutdown, due to the failed-consumed count exceed the max-failed-consumed [%s]",

File: flinkx-dirtydata-collectors/flinkx-dirtydata-collector-log/src/main/java/com/dtstack/flinkx/dirty/log/LogDirtyDataCollector.java
Patch:
@@ -43,7 +43,7 @@ protected void init(DirtyConf conf) {
 
     @Override
     protected void consume(DirtyDataEntry dirty) throws Exception {
-        if (consumed.get().getLocalValue() % printRate == 0) {
+        if (CONSUMED_COUNTER.getLocalValue() % printRate == 0) {
             StringJoiner dirtyMessage =
                     new StringJoiner("\n")
                             .add("\n====================Dirty Data=====================")

File: flinkx-dirtydata-collectors/flinkx-dirtydata-collector-mysql/src/main/java/com/dtstack/flinkx/dirty/mysql/MysqlDirtyDataCollector.java
Patch:
@@ -278,7 +278,7 @@ private void flush() {
     protected void consume(DirtyDataEntry dirty) throws Exception {
         entities.add(dirty);
 
-        if (consumed.get().getLocalValue() % batchSize == 0) {
+        if (CONSUMED_COUNTER.getLocalValue() % batchSize == 0) {
             flush();
         }
     }

File: flinkx-connectors/flinkx-connector-file/src/main/java/com/dtstack/flinkx/connector/file/table/FileDynamicTableFactory.java
Patch:
@@ -19,8 +19,6 @@
 package com.dtstack.flinkx.connector.file.table;
 
 import com.dtstack.flinkx.conf.BaseFileConf;
-import com.dtstack.flinkx.connector.file.options.FileOptions;
-import com.dtstack.flinkx.connector.file.source.FileDynamicTableSource;
 
 import org.apache.flink.api.common.serialization.DeserializationSchema;
 import org.apache.flink.configuration.ConfigOption;
@@ -53,7 +51,6 @@ public DynamicTableSource createDynamicTableSource(Context context) {
         final FactoryUtil.TableFactoryHelper helper =
                 FactoryUtil.createTableFactoryHelper(this, context);
         final ReadableConfig config = helper.getOptions();
-        helper.validate();
 
         TableSchema physicalSchema =
                 TableSchemaUtils.getPhysicalSchema(context.getCatalogTable().getSchema());
@@ -69,6 +66,7 @@ private BaseFileConf getFileConfByOptions(ReadableConfig config) {
         BaseFileConf fileConf = new BaseFileConf();
         fileConf.setPath(config.get(FileOptions.PATH));
         fileConf.setEncoding(config.get(FileOptions.ENCODING));
+        fileConf.setFromLine(config.get(FileOptions.SCAN_LINE));
         return fileConf;
     }
 

File: flinkx-connectors/flinkx-connector-file/src/main/java/com/dtstack/flinkx/connector/file/table/FileDynamicTableSource.java
Patch:
@@ -16,10 +16,11 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.file.source;
+package com.dtstack.flinkx.connector.file.table;
 
 import com.dtstack.flinkx.conf.BaseFileConf;
 import com.dtstack.flinkx.connector.file.converter.FileRowConverter;
+import com.dtstack.flinkx.connector.file.source.FileInputFormatBuilder;
 import com.dtstack.flinkx.source.DtInputFormatSourceFunction;
 import com.dtstack.flinkx.table.connector.source.ParallelSourceFunctionProvider;
 

File: flinkx-connectors/flinkx-connector-http/src/main/java/com/dtstack/flinkx/connector/http/client/HttpRequestParam.java
Patch:
@@ -15,9 +15,9 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package com.dtstack.flinkx.connector.restapi.client;
+package com.dtstack.flinkx.connector.http.client;
 
-import com.dtstack.flinkx.connector.restapi.common.MetaParam;
+import com.dtstack.flinkx.connector.http.common.MetaParam;
 import com.dtstack.flinkx.util.GsonUtil;
 import com.dtstack.flinkx.util.MapUtil;
 

File: flinkx-connectors/flinkx-connector-http/src/main/java/com/dtstack/flinkx/connector/http/client/ResponseValue.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.restapi.client;
+package com.dtstack.flinkx.connector.http.client;
 
 /**
  * 返回值

File: flinkx-connectors/flinkx-connector-http/src/main/java/com/dtstack/flinkx/connector/http/client/RestHandler.java
Patch:
@@ -16,10 +16,10 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.restapi.client;
+package com.dtstack.flinkx.connector.http.client;
 
-import com.dtstack.flinkx.connector.restapi.common.HttpRestConfig;
-import com.dtstack.flinkx.connector.restapi.common.MetaParam;
+import com.dtstack.flinkx.connector.http.common.HttpRestConfig;
+import com.dtstack.flinkx.connector.http.common.MetaParam;
 
 import java.util.List;
 import java.util.Map;

File: flinkx-connectors/flinkx-connector-http/src/main/java/com/dtstack/flinkx/connector/http/client/Strategy.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.restapi.client;
+package com.dtstack.flinkx.connector.http.client;
 
 import java.io.Serializable;
 

File: flinkx-connectors/flinkx-connector-http/src/main/java/com/dtstack/flinkx/connector/http/common/ConstantValue.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.restapi.common;
+package com.dtstack.flinkx.connector.http.common;
 
 import com.google.common.collect.Sets;
 

File: flinkx-connectors/flinkx-connector-http/src/main/java/com/dtstack/flinkx/connector/http/common/HttpKeys.java
Patch:
@@ -15,13 +15,13 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package com.dtstack.flinkx.connector.restapi.common;
+package com.dtstack.flinkx.connector.http.common;
 
 /**
  * @author : shifang
  * @date : 2020/3/19
  */
-public class RestapiKeys {
+public class HttpKeys {
     public static final String KEY_HEADER = "header";
     public static final String KEY_BODY = "body";
     public static final String KEY_PARAMS = "params";

File: flinkx-connectors/flinkx-connector-http/src/main/java/com/dtstack/flinkx/connector/http/common/HttpMethod.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.restapi.common;
+package com.dtstack.flinkx.connector.http.common;
 
 /**
  * @author : shifang

File: flinkx-connectors/flinkx-connector-http/src/main/java/com/dtstack/flinkx/connector/http/common/HttpRestConfig.java
Patch:
@@ -15,10 +15,10 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package com.dtstack.flinkx.connector.restapi.common;
+package com.dtstack.flinkx.connector.http.common;
 
 import com.dtstack.flinkx.conf.FlinkxCommonConf;
-import com.dtstack.flinkx.connector.restapi.client.Strategy;
+import com.dtstack.flinkx.connector.http.client.Strategy;
 
 import java.util.ArrayList;
 import java.util.List;

File: flinkx-connectors/flinkx-connector-http/src/main/java/com/dtstack/flinkx/connector/http/common/MetaParam.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.restapi.common;
+package com.dtstack.flinkx.connector.http.common;
 
 import com.dtstack.flinkx.constants.ConstantValue;
 
@@ -123,9 +123,9 @@ public String getAllName() {
      */
     public String getVariableName() {
         return new StringBuilder()
-                .append(com.dtstack.flinkx.connector.restapi.common.ConstantValue.PREFIX)
+                .append(com.dtstack.flinkx.connector.http.common.ConstantValue.PREFIX)
                 .append(getAllName())
-                .append(com.dtstack.flinkx.connector.restapi.common.ConstantValue.SUFFIX)
+                .append(com.dtstack.flinkx.connector.http.common.ConstantValue.SUFFIX)
                 .toString();
     }
 

File: flinkx-connectors/flinkx-connector-http/src/main/java/com/dtstack/flinkx/connector/http/common/ParamType.java
Patch:
@@ -15,7 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package com.dtstack.flinkx.connector.restapi.common;
+package com.dtstack.flinkx.connector.http.common;
 
 /**
  * ParamType

File: flinkx-connectors/flinkx-connector-http/src/main/java/com/dtstack/flinkx/connector/http/converter/HttpRawTypeConverter.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.restapi.convert;
+package com.dtstack.flinkx.connector.http.converter;
 
 import com.dtstack.flinkx.throwable.UnsupportedTypeException;
 
@@ -30,7 +30,7 @@
  * @program flinkx
  * @create 2021/05/24
  */
-public class RestapiRawTypeConverter {
+public class HttpRawTypeConverter {
 
     /** 将restapi返回的参数根据定义的类型，转换成flink的DataType类型。 */
     public static DataType apply(String type) {

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/ConnectorNameConvertUtil.java
Patch:
@@ -36,6 +36,7 @@ public class ConnectorNameConvertUtil {
         connectorNameMap.put("es", new Tuple2<>("elasticsearch6", "elasticsearch6"));
         connectorNameMap.put("hbase", new Tuple2<>("hbase14", "HBase14"));
         connectorNameMap.put("tidb", new Tuple2<>("mysql", "mysql"));
+        connectorNameMap.put("restapi", new Tuple2<>("http", "http"));
     }
 
     public static String convertClassPrefix(String originName) {

File: flinkx-connectors/flinkx-connector-kudu/src/main/java/com/dtstack/flinkx/connector/kudu/table/lookup/KuduAllTableFunction.java
Patch:
@@ -148,11 +148,9 @@ private KuduScanner getKuduScannerWithRetry(KuduLookupConf kuduLookupConf) {
     private KuduScanner buildScanner(
             KuduScanner.KuduScannerBuilder scannerBuilder, KuduLookupConf kuduLookupConf) {
         Integer batchSizeBytes = kuduLookupConf.getBatchSizeBytes();
-        Long limitNum = kuduLookupConf.getLimitNum();
         Boolean isFaultTolerant = kuduLookupConf.getFaultTolerant();
 
         return scannerBuilder
-                .limit(limitNum)
                 .batchSizeBytes(batchSizeBytes)
                 .setFaultTolerant(isFaultTolerant)
                 .setProjectedColumnNames(Arrays.asList(fieldsName))

File: flinkx-connectors/flinkx-connector-hive/src/main/java/com/dtstack/flinkx/connector/hive/options/HiveOptions.java
Patch:
@@ -18,6 +18,7 @@
 package com.dtstack.flinkx.connector.hive.options;
 
 import com.dtstack.flinkx.connector.hdfs.options.HdfsOptions;
+import com.dtstack.flinkx.connector.hive.enums.PartitionEnum;
 
 import org.apache.flink.configuration.ConfigOption;
 import org.apache.flink.configuration.ConfigOptions;
@@ -52,7 +53,7 @@ public class HiveOptions extends HdfsOptions {
     public static final ConfigOption<String> PARTITION_TYPE =
             ConfigOptions.key("partition-type")
                     .stringType()
-                    .noDefaultValue()
+                    .defaultValue(PartitionEnum.DAY.name())
                     .withDescription(
                             "Partition types, including DAY, HOUR, and MINUTE. If the partition does not exist, it will be created automatically. The time of the automatically created partition is based on the server time of the current task.");
 

File: flinkx-connectors/flinkx-connector-kafka/src/main/java/com/dtstack/flinkx/connector/kafka/serialization/RowSerializationSchema.java
Patch:
@@ -39,9 +39,8 @@
  */
 public class RowSerializationSchema extends DynamicKafkaSerializationSchema {
 
-    protected final Logger LOG = LoggerFactory.getLogger(getClass());
-
     private static final long serialVersionUID = 1L;
+    protected final Logger LOG = LoggerFactory.getLogger(getClass());
     /** kafka key converter */
     private final AbstractRowConverter<String, Object, byte[], String> keyConverter;
     /** kafka value converter */

File: flinkx-connectors/flinkx-connector-sqlservercdc/src/main/java/com/dtstack/flinkx/connector/sqlservercdc/conf/SqlServerCdcConf.java
Patch:
@@ -37,7 +37,7 @@ public class SqlServerCdcConf extends FlinkxCommonConf {
     private String cat;
     private boolean pavingData;
     private List<String> tableList;
-    private Long pollInterval;
+    private Long pollInterval = 1000L;
     private String lsn;
     private boolean splitUpdate;
     private String timestampFormat = "sql";

File: flinkx-connectors/flinkx-connector-sqlservercdc/src/main/java/com/dtstack/flinkx/connector/sqlservercdc/conf/SqlServerCdcConf.java
Patch:
@@ -37,7 +37,7 @@ public class SqlServerCdcConf extends FlinkxCommonConf {
     private String cat;
     private boolean pavingData;
     private List<String> tableList;
-    private Long pollInterval;
+    private Long pollInterval = 1000L;
     private String lsn;
     private boolean splitUpdate;
     private String timestampFormat = "sql";

File: flinkx-connectors/flinkx-connector-sqlserver/src/main/java/com/dtstack/flinkx/connector/sqlserver/table/SqlserverDynamicTableFactory.java
Patch:
@@ -25,6 +25,7 @@
 import com.dtstack.flinkx.connector.sqlserver.dialect.SqlserverDialect;
 import com.dtstack.flinkx.connector.sqlserver.sink.SqlserverOutputFormat;
 import com.dtstack.flinkx.connector.sqlserver.source.SqlserverInputFormat;
+
 import org.apache.flink.table.connector.source.DynamicTableSource;
 
 import java.util.Map;

File: flinkx-connectors/flinkx-connector-sqlserver/src/main/java/com/dtstack/flinkx/connector/sqlserver/table/SqlserverDynamicTableFactory.java
Patch:
@@ -25,6 +25,7 @@
 import com.dtstack.flinkx.connector.sqlserver.dialect.SqlserverDialect;
 import com.dtstack.flinkx.connector.sqlserver.sink.SqlserverOutputFormat;
 import com.dtstack.flinkx.connector.sqlserver.source.SqlserverInputFormat;
+
 import org.apache.flink.table.connector.source.DynamicTableSource;
 
 import java.util.Map;

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/source/JdbcInputFormat.java
Patch:
@@ -204,7 +204,7 @@ public boolean reachedEnd() {
                                     String.format(
                                             "cannot connect to %s, username = %s, please check %s is available.",
                                             jdbcConf.getJdbcUrl(),
-                                            jdbcConf.getJdbcUrl(),
+                                            jdbcConf.getUsername(),
                                             jdbcDialect.dialectName());
                             throw new FlinkxRuntimeException(message);
                         }

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/source/JdbcInputFormat.java
Patch:
@@ -204,7 +204,7 @@ public boolean reachedEnd() {
                                     String.format(
                                             "cannot connect to %s, username = %s, please check %s is available.",
                                             jdbcConf.getJdbcUrl(),
-                                            jdbcConf.getJdbcUrl(),
+                                            jdbcConf.getUsername(),
                                             jdbcDialect.dialectName());
                             throw new FlinkxRuntimeException(message);
                         }

File: flinkx-connectors/flinkx-connector-sqlservercdc/src/main/java/com/dtstack/flinkx/connector/sqlservercdc/convert/SqlServerCdcColumnConverter.java
Patch:
@@ -33,11 +33,10 @@
 import com.dtstack.flinkx.element.column.TimestampColumn;
 import com.dtstack.flinkx.util.StringUtil;
 
+import org.apache.flink.calcite.shaded.com.google.common.collect.Maps;
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.types.RowKind;
 
-import com.google.common.collect.Maps;
-
 import java.math.BigDecimal;
 import java.math.BigInteger;
 import java.nio.charset.StandardCharsets;

File: flinkx-connectors/flinkx-connector-sqlservercdc/src/main/java/com/dtstack/flinkx/connector/sqlservercdc/convert/SqlServerCdcRowConverter.java
Patch:
@@ -22,6 +22,7 @@
 import com.dtstack.flinkx.converter.AbstractCDCRowConverter;
 import com.dtstack.flinkx.converter.IDeserializationConverter;
 
+import org.apache.flink.calcite.shaded.com.google.common.collect.Maps;
 import org.apache.flink.formats.json.TimestampFormat;
 import org.apache.flink.table.api.TableException;
 import org.apache.flink.table.data.DecimalData;
@@ -33,8 +34,6 @@
 import org.apache.flink.table.types.logical.RowType;
 import org.apache.flink.types.RowKind;
 
-import com.google.common.collect.Maps;
-
 import java.math.BigDecimal;
 import java.nio.charset.StandardCharsets;
 import java.sql.Date;

File: flinkx-connectors/flinkx-connector-sqlservercdc/src/main/java/com/dtstack/flinkx/connector/sqlservercdc/inputFormat/SqlServerCdcInputFormat.java
Patch:
@@ -124,7 +124,7 @@ protected void openInternal(InputSplit inputSplit) {
     protected RowData nextRecordInternal(RowData row) throws ReadRecordException {
         RowData rowData = null;
         try {
-            rowData = queue.take();
+            rowData = queue.poll(100, TimeUnit.MILLISECONDS);
         } catch (InterruptedException e) {
             LOG.error("takeEvent interrupted error:{}", ExceptionUtil.getErrorMessage(e));
             throw new ReadRecordException("takeEvent interrupted error", e);

File: flinkx-connectors/flinkx-connector-sqlservercdc/src/main/java/com/dtstack/flinkx/connector/sqlservercdc/convert/SqlServerCdcColumnConverter.java
Patch:
@@ -33,11 +33,10 @@
 import com.dtstack.flinkx.element.column.TimestampColumn;
 import com.dtstack.flinkx.util.StringUtil;
 
+import org.apache.flink.calcite.shaded.com.google.common.collect.Maps;
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.types.RowKind;
 
-import com.google.common.collect.Maps;
-
 import java.math.BigDecimal;
 import java.math.BigInteger;
 import java.nio.charset.StandardCharsets;

File: flinkx-connectors/flinkx-connector-sqlservercdc/src/main/java/com/dtstack/flinkx/connector/sqlservercdc/convert/SqlServerCdcRowConverter.java
Patch:
@@ -22,6 +22,7 @@
 import com.dtstack.flinkx.converter.AbstractCDCRowConverter;
 import com.dtstack.flinkx.converter.IDeserializationConverter;
 
+import org.apache.flink.calcite.shaded.com.google.common.collect.Maps;
 import org.apache.flink.formats.json.TimestampFormat;
 import org.apache.flink.table.api.TableException;
 import org.apache.flink.table.data.DecimalData;
@@ -33,8 +34,6 @@
 import org.apache.flink.table.types.logical.RowType;
 import org.apache.flink.types.RowKind;
 
-import com.google.common.collect.Maps;
-
 import java.math.BigDecimal;
 import java.nio.charset.StandardCharsets;
 import java.sql.Date;

File: flinkx-connectors/flinkx-connector-sqlservercdc/src/main/java/com/dtstack/flinkx/connector/sqlservercdc/inputFormat/SqlServerCdcInputFormat.java
Patch:
@@ -124,7 +124,7 @@ protected void openInternal(InputSplit inputSplit) {
     protected RowData nextRecordInternal(RowData row) throws ReadRecordException {
         RowData rowData = null;
         try {
-            rowData = queue.take();
+            rowData = queue.poll(100, TimeUnit.MILLISECONDS);
         } catch (InterruptedException e) {
             LOG.error("takeEvent interrupted error:{}", ExceptionUtil.getErrorMessage(e));
             throw new ReadRecordException("takeEvent interrupted error", e);

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/lookup/JdbcLruTableFunction.java
Patch:
@@ -316,8 +316,7 @@ private void handleQuery(
                                     String.format(
                                             "\nget data with sql [%s],data [%s] failed! \ncause: [%s]",
                                             query, Arrays.toString(keys), rs.cause().getMessage()));
-                            dealFillDataError(future, rs.cause());
-                            return;
+                            throw new RuntimeException(rs.cause().getMessage(), rs.cause());
                         }
 
                         List<JsonArray> cacheContent = new ArrayList<>();

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/lookup/JdbcLruTableFunction.java
Patch:
@@ -316,8 +316,7 @@ private void handleQuery(
                                     String.format(
                                             "\nget data with sql [%s],data [%s] failed! \ncause: [%s]",
                                             query, Arrays.toString(keys), rs.cause().getMessage()));
-                            dealFillDataError(future, rs.cause());
-                            return;
+                            throw new RuntimeException(rs.cause().getMessage(), rs.cause());
                         }
 
                         List<JsonArray> cacheContent = new ArrayList<>();

File: flinkx-connectors/flinkx-connector-binlog/src/main/java/com/dtstack/flinkx/connector/binlog/converter/BinlogColumnConverter.java
Patch:
@@ -32,11 +32,11 @@
 import com.dtstack.flinkx.element.column.TimestampColumn;
 import com.dtstack.flinkx.util.DateUtil;
 
+import org.apache.flink.calcite.shaded.com.google.common.collect.Maps;
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.types.RowKind;
 
 import com.alibaba.otter.canal.protocol.CanalEntry;
-import com.google.common.collect.Maps;
 import org.apache.commons.collections.CollectionUtils;
 
 import java.nio.charset.StandardCharsets;

File: flinkx-core/src/main/java/com/dtstack/flinkx/options/OptionParser.java
Patch:
@@ -56,7 +56,7 @@ public OptionParser(String[] args) throws Exception {
                 options.addOption(name, optionRequired.hasArg(), optionRequired.description());
             }
         }
-        CommandLine cl = parser.parse(options, args, true);
+        CommandLine cl = parser.parse(options, args);
 
         for (Field field : fields) {
             String name = field.getName();

File: flinkx-core/src/main/java/com/dtstack/flinkx/options/OptionParser.java
Patch:
@@ -56,7 +56,7 @@ public OptionParser(String[] args) throws Exception {
                 options.addOption(name, optionRequired.hasArg(), optionRequired.description());
             }
         }
-        CommandLine cl = parser.parse(options, args);
+        CommandLine cl = parser.parse(options, args, true);
 
         for (Field field : fields) {
             String name = field.getName();

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/PluginUtil.java
Patch:
@@ -50,6 +50,7 @@
 import java.util.concurrent.CompletableFuture;
 import java.util.concurrent.Future;
 
+import static com.dtstack.flinkx.constants.ConstantValue.CONNECTOR_DIR_NAME;
 import static com.dtstack.flinkx.constants.ConstantValue.POINT_SYMBOL;
 
 /**
@@ -215,12 +216,12 @@ public static void registerPluginUrlToCachedFile(
         Set<URL> sourceUrlList =
                 getJarFileDirPath(
                         config.getReader().getName(),
-                        config.getPluginRoot(),
+                        config.getPluginRoot() + SP + CONNECTOR_DIR_NAME,
                         config.getRemotePluginPath());
         Set<URL> sinkUrlList =
                 getJarFileDirPath(
                         config.getWriter().getName(),
-                        config.getPluginRoot(),
+                        config.getPluginRoot() + SP + CONNECTOR_DIR_NAME,
                         config.getRemotePluginPath());
         Set<URL> metricUrlList =
                 getJarFileDirPath(

File: flinkx-connectors/flinkx-connector-hdfs/src/main/java/com/dtstack/flinkx/connector/hdfs/util/HdfsUtil.java
Patch:
@@ -80,7 +80,9 @@ public class HdfsUtil {
     public static Object getWritableValue(Object writable) {
         Object ret;
 
-        if (writable instanceof IntWritable) {
+        if (writable == null) {
+            ret = null;
+        } else if (writable instanceof IntWritable) {
             ret = ((IntWritable) writable).get();
         } else if (writable instanceof Text) {
             ret = writable.toString();

File: flinkx-core/src/main/java/com/dtstack/flinkx/Main.java
Patch:
@@ -30,7 +30,6 @@
 import com.dtstack.flinkx.sql.parser.SqlParser;
 import com.dtstack.flinkx.throwable.FlinkxRuntimeException;
 import com.dtstack.flinkx.util.DataSyncFactoryUtil;
-import com.dtstack.flinkx.util.ExceptionUtil;
 import com.dtstack.flinkx.util.ExecuteProcessHelper;
 import com.dtstack.flinkx.util.FactoryHelper;
 import com.dtstack.flinkx.util.PluginUtil;
@@ -62,14 +61,12 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import java.io.IOException;
 import java.net.URL;
 import java.net.URLDecoder;
 import java.util.Arrays;
 import java.util.List;
 import java.util.Optional;
 import java.util.Properties;
-import java.util.concurrent.ExecutionException;
 
 /**
  * The main class entry

File: flinkx-core/src/main/java/com/dtstack/flinkx/Main.java
Patch:
@@ -62,12 +62,14 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+import java.io.IOException;
 import java.net.URL;
 import java.net.URLDecoder;
 import java.util.Arrays;
 import java.util.List;
 import java.util.Optional;
 import java.util.Properties;
+import java.util.concurrent.ExecutionException;
 
 /**
  * The main class entry
@@ -135,7 +137,7 @@ private static void exeSqlJob(
                 }
             }
         } catch (Exception e) {
-            LOG.error(ExceptionUtil.getErrorMessage(e));
+            throw new FlinkxRuntimeException(e);
         } finally {
             FactoryUtil.getFactoryHelperThreadLocal().remove();
             TableFactoryService.getFactoryHelperThreadLocal().remove();

File: flinkx-connectors/flinkx-connector-binlog/src/main/java/com/dtstack/flinkx/connector/binlog/converter/BinlogRowConverter.java
Patch:
@@ -44,6 +44,7 @@
 import java.time.format.DateTimeFormatter;
 import java.time.temporal.TemporalAccessor;
 import java.time.temporal.TemporalQueries;
+import java.util.ArrayList;
 import java.util.LinkedList;
 import java.util.List;
 import java.util.Map;
@@ -59,9 +60,9 @@ public class BinlogRowConverter extends AbstractCDCRowConverter<BinlogEventRow,
     public BinlogRowConverter(RowType rowType, TimestampFormat timestampFormat) {
         super.fieldNameList = rowType.getFieldNames();
         this.timestampFormat = timestampFormat;
-        super.converters = new IDeserializationConverter[rowType.getFieldCount()];
+        super.converters = new ArrayList<>();
         for (int i = 0; i < rowType.getFieldCount(); i++) {
-            super.converters[i] = createInternalConverter(rowType.getTypeAt(i));
+            super.converters.add(createInternalConverter(rowType.getTypeAt(i)));
         }
     }
 

File: flinkx-connectors/flinkx-connector-oraclelogminer/src/main/java/com/dtstack/flinkx/connector/oraclelogminer/converter/LogMinerRowConverter.java
Patch:
@@ -42,6 +42,7 @@
 import java.time.LocalTime;
 import java.time.temporal.TemporalAccessor;
 import java.time.temporal.TemporalQueries;
+import java.util.ArrayList;
 import java.util.LinkedList;
 import java.util.List;
 import java.util.Map;
@@ -55,9 +56,9 @@ public class LogMinerRowConverter extends AbstractCDCRowConverter<EventRow, Logi
 
     public LogMinerRowConverter(RowType rowType) {
         super.fieldNameList = rowType.getFieldNames();
-        super.converters = new IDeserializationConverter[rowType.getFieldCount()];
+        super.converters = new ArrayList<>();
         for (int i = 0; i < rowType.getFieldCount(); i++) {
-            super.converters[i] = createInternalConverter(rowType.getTypeAt(i));
+            super.converters.add(createInternalConverter(rowType.getTypeAt(i)));
         }
     }
 

File: flinkx-connectors/flinkx-connector-pgwal/src/main/java/com/dtstack/flinkx/connector/pgwal/converter/PGWalRowConverter.java
Patch:
@@ -42,6 +42,7 @@
 import java.time.format.DateTimeFormatter;
 import java.time.temporal.TemporalAccessor;
 import java.time.temporal.TemporalQueries;
+import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.LinkedList;
 import java.util.Map;
@@ -53,9 +54,9 @@ public class PGWalRowConverter extends AbstractCDCRowConverter<ChangeLog, Logica
     public PGWalRowConverter(RowType rowType, TimestampFormat timestampFormat) {
         super.fieldNameList = rowType.getFieldNames();
         this.timestampFormat = timestampFormat;
-        super.converters = new IDeserializationConverter[rowType.getFieldCount()];
+        super.converters = new ArrayList<>();
         for (int i = 0; i < rowType.getFieldCount(); i++) {
-            super.converters[i] = createInternalConverter(rowType.getTypeAt(i));
+            super.converters.add(createInternalConverter(rowType.getTypeAt(i)));
         }
     }
 

File: flinkx-connectors/flinkx-connector-phoenix5/src/main/java/com/dtstack/flinkx/connector/phoenix5/converter/HBaseRowConverter.java
Patch:
@@ -95,9 +95,9 @@ public HBaseRowConverter(RowType rowType, RowProjector rowProjector) {
         phoenixTypeList = new ArrayList<>(fieldNames.size());
 
         for (int i = 0; i < rowType.getFieldCount(); i++) {
-            toInternalConverters[i] =
+            toInternalConverters.add(
                     wrapIntoNullableInternalConverter(
-                            createInternalConverter(rowType.getTypeAt(i)));
+                            createInternalConverter(rowType.getTypeAt(i))));
             phoenixTypeList.add(getPDataType(fieldTypes[i].getTypeRoot().toString()));
             this.rowProjector = rowProjector;
         }
@@ -137,7 +137,7 @@ public RowData toInternal(NoTagsKeyValue input) throws Exception {
             ColumnProjector columnProjector = rowProjector.getColumnProjector(i);
             PDataType pDataType = phoenixTypeList.get(i);
             Object value = columnProjector.getValue(resultTuple, pDataType, pointer);
-            genericRowData.setField(i, toInternalConverters[i].deserialize(value));
+            genericRowData.setField(i, toInternalConverters.get(i).deserialize(value));
         }
         return genericRowData;
     }

File: flinkx-connectors/flinkx-connector-restapi/src/main/java/com/dtstack/flinkx/connector/restapi/convert/RestapiRowConverter.java
Patch:
@@ -52,9 +52,9 @@ public class RestapiRowConverter extends AbstractRowConverter<String, Object, Ob
     public RestapiRowConverter(RowType rowType, HttpRestConfig httpRestConfig) {
         super(rowType);
         for (int i = 0; i < rowType.getFieldCount(); i++) {
-            toInternalConverters[i] =
+            toInternalConverters.add(
                     wrapIntoNullableInternalConverter(
-                            createInternalConverter(rowType.getTypeAt(i)));
+                            createInternalConverter(rowType.getTypeAt(i))));
         }
         this.httpRestConfig = httpRestConfig;
     }
@@ -72,7 +72,7 @@ public RowData toInternal(String input) throws Exception {
             if (value instanceof LinkedTreeMap) {
                 value = value.toString();
             }
-            genericRowData.setField(pos, toInternalConverters[pos].deserialize(value));
+            genericRowData.setField(pos, toInternalConverters.get(pos).deserialize(value));
         }
         return genericRowData;
     }

File: flinkx-connectors/flinkx-connector-sqlserver/src/main/java/com/dtstack/flinkx/connector/sqlserver/converter/SqlserverJtdsColumnConverter.java
Patch:
@@ -64,7 +64,7 @@ public RowData toInternal(ResultSet resultSet) throws Exception {
                 } else {
                     baseColumn =
                             (AbstractBaseColumn)
-                                    toInternalConverters[converterIndex].deserialize(field);
+                                    toInternalConverters.get(converterIndex).deserialize(field);
                 }
                 converterIndex++;
             }

File: flinkx-connectors/flinkx-connector-sqlserver/src/main/java/com/dtstack/flinkx/connector/sqlserver/converter/SqlserverMicroSoftColumnConverter.java
Patch:
@@ -77,7 +77,7 @@ public RowData toInternal(ResultSet resultSet) throws Exception {
                 } else {
                     baseColumn =
                             (AbstractBaseColumn)
-                                    toInternalConverters[converterIndex].deserialize(field);
+                                    toInternalConverters.get(converterIndex).deserialize(field);
                 }
                 converterIndex++;
             }

File: flinkx-connectors/flinkx-connector-sqlservercdc/src/main/java/com/dtstack/flinkx/connector/sqlservercdc/convert/SqlServerCdcRowConverter.java
Patch:
@@ -46,6 +46,7 @@
 import java.time.ZoneOffset;
 import java.time.temporal.TemporalAccessor;
 import java.time.temporal.TemporalQueries;
+import java.util.ArrayList;
 import java.util.LinkedList;
 import java.util.List;
 import java.util.Locale;
@@ -63,9 +64,9 @@ public class SqlServerCdcRowConverter
     public SqlServerCdcRowConverter(RowType rowType, TimestampFormat timestampFormat) {
         super.fieldNameList = rowType.getFieldNames();
         this.timestampFormat = timestampFormat;
-        super.converters = new IDeserializationConverter[rowType.getFieldCount()];
+        super.converters = new ArrayList<>();
         for (int i = 0; i < rowType.getFieldCount(); i++) {
-            super.converters[i] = createInternalConverter(rowType.getTypeAt(i));
+            super.converters.add(createInternalConverter(rowType.getTypeAt(i)));
         }
     }
 

File: flinkx-connectors/flinkx-connector-hive/src/main/java/com/dtstack/flinkx/connector/hive/util/HiveDbUtil.java
Patch:
@@ -127,7 +127,9 @@ private static Connection getConnectionWithKerberos(
 
         UserGroupInformation ugi;
         try {
-            ugi = KerberosUtil.loginAndReturnUgi(conf.get(KerberosUtil.KEY_PRINCIPAL_FILE), principal, keytabFileName);
+            ugi =
+                    KerberosUtil.loginAndReturnUgi(
+                            conf.get(KerberosUtil.KEY_PRINCIPAL_FILE), principal, keytabFileName);
         } catch (Exception e) {
             throw new RuntimeException("Login kerberos error:", e);
         }

File: flinkx-connectors/flinkx-connector-binlog/src/main/java/com/dtstack/flinkx/connector/binlog/converter/MysqlBinlogRawTypeConverter.java
Patch:
@@ -18,11 +18,11 @@
 
 package com.dtstack.flinkx.connector.binlog.converter;
 
+import com.dtstack.flinkx.throwable.UnsupportedTypeException;
+
 import org.apache.flink.table.api.DataTypes;
 import org.apache.flink.table.types.DataType;
 
-import com.dtstack.flinkx.throwable.UnsupportedTypeException;
-
 import java.util.Locale;
 
 /**

File: flinkx-connectors/flinkx-connector-binlog/src/main/java/com/dtstack/flinkx/connector/binlog/inputformat/BinlogInputFormatBuilder.java
Patch:
@@ -26,6 +26,7 @@
 import com.dtstack.flinkx.util.GsonUtil;
 import com.dtstack.flinkx.util.RetryUtil;
 import com.dtstack.flinkx.util.TelnetUtil;
+
 import com.google.common.collect.Lists;
 import com.google.common.collect.Sets;
 import org.apache.commons.collections.CollectionUtils;

File: flinkx-connectors/flinkx-connector-cassandra/src/main/java/com/dtstack/flinkx/connector/cassandra/conf/CassandraCommonConf.java
Patch:
@@ -19,11 +19,12 @@
 package com.dtstack.flinkx.connector.cassandra.conf;
 
 import com.dtstack.flinkx.conf.FlinkxCommonConf;
-import org.apache.commons.lang3.builder.ToStringBuilder;
-import org.apache.commons.lang3.builder.ToStringStyle;
 
 import org.apache.flink.configuration.ReadableConfig;
 
+import org.apache.commons.lang3.builder.ToStringBuilder;
+import org.apache.commons.lang3.builder.ToStringStyle;
+
 import static com.dtstack.flinkx.connector.cassandra.optinos.CassandraCommonOptions.CLUSTER_NAME;
 import static com.dtstack.flinkx.connector.cassandra.optinos.CassandraCommonOptions.CONNECT_TIMEOUT_MILLISECONDS;
 import static com.dtstack.flinkx.connector.cassandra.optinos.CassandraCommonOptions.CONSISTENCY;

File: flinkx-connectors/flinkx-connector-cassandra/src/main/java/com/dtstack/flinkx/connector/cassandra/converter/CassandraRawTypeConverter.java
Patch:
@@ -19,6 +19,7 @@
 package com.dtstack.flinkx.connector.cassandra.converter;
 
 import com.dtstack.flinkx.throwable.UnsupportedTypeException;
+
 import org.apache.flink.table.api.DataTypes;
 import org.apache.flink.table.types.DataType;
 

File: flinkx-connectors/flinkx-connector-cassandra/src/main/java/com/dtstack/flinkx/connector/cassandra/converter/CassandraRowConverter.java
Patch:
@@ -18,8 +18,6 @@
 
 package com.dtstack.flinkx.connector.cassandra.converter;
 
-import com.datastax.driver.core.BoundStatement;
-import com.datastax.driver.core.Row;
 import com.dtstack.flinkx.converter.AbstractRowConverter;
 import com.dtstack.flinkx.converter.IDeserializationConverter;
 import com.dtstack.flinkx.converter.ISerializationConverter;
@@ -36,6 +34,9 @@
 import org.apache.flink.table.types.logical.RowType;
 import org.apache.flink.table.types.logical.TimestampType;
 
+import com.datastax.driver.core.BoundStatement;
+import com.datastax.driver.core.Row;
+
 import java.math.BigDecimal;
 import java.math.BigInteger;
 import java.nio.ByteBuffer;

File: flinkx-connectors/flinkx-connector-cassandra/src/main/java/com/dtstack/flinkx/connector/cassandra/sink/CassandraOutputFormatBuilder.java
Patch:
@@ -22,6 +22,7 @@
 import com.dtstack.flinkx.converter.AbstractRowConverter;
 import com.dtstack.flinkx.sink.format.BaseRichOutputFormatBuilder;
 import com.dtstack.flinkx.throwable.NoRestartException;
+
 import org.apache.commons.lang3.StringUtils;
 
 /**

File: flinkx-connectors/flinkx-connector-cassandra/src/main/java/com/dtstack/flinkx/connector/cassandra/sink/CassandraSinkFactory.java
Patch:
@@ -33,7 +33,6 @@
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.types.logical.RowType;
 
-import java.util.ArrayList;
 import java.util.List;
 
 /**

File: flinkx-connectors/flinkx-connector-cassandra/src/main/java/com/dtstack/flinkx/connector/cassandra/source/CassandraInputFormatBuilder.java
Patch:
@@ -21,6 +21,7 @@
 import com.dtstack.flinkx.connector.cassandra.conf.CassandraSourceConf;
 import com.dtstack.flinkx.source.format.BaseRichInputFormatBuilder;
 import com.dtstack.flinkx.throwable.NoRestartException;
+
 import org.apache.commons.lang3.StringUtils;
 
 /**

File: flinkx-connectors/flinkx-connector-cassandra/src/main/java/com/dtstack/flinkx/connector/cassandra/source/CassandraSourceFactory.java
Patch:
@@ -70,8 +70,7 @@ public DataStream<RowData> createSource() {
         List<String> columnNameList = new ArrayList<>();
         fieldConfList.forEach(fieldConf -> columnNameList.add(fieldConf.getName()));
 
-        final RowType rowType =
-                TableUtil.createRowType(fieldConfList, getRawTypeConverter());
+        final RowType rowType = TableUtil.createRowType(fieldConfList, getRawTypeConverter());
         builder.setRowConverter(new CassandraRowConverter(rowType, columnNameList));
 
         return createInput(builder.finish());

File: flinkx-connectors/flinkx-connector-clickhouse/src/main/java/com/dtstack/flinkx/connector/clickhouse/sink/ClickhouseOutputFormat.java
Patch:
@@ -33,6 +33,7 @@ public class ClickhouseOutputFormat extends JdbcOutputFormat {
 
     @Override
     protected Connection getConnection() throws SQLException {
-        return ClickhouseUtil.getConnection(jdbcConf.getJdbcUrl(), jdbcConf.getUsername(), jdbcConf.getPassword());
+        return ClickhouseUtil.getConnection(
+                jdbcConf.getJdbcUrl(), jdbcConf.getUsername(), jdbcConf.getPassword());
     }
 }

File: flinkx-connectors/flinkx-connector-clickhouse/src/main/java/com/dtstack/flinkx/connector/clickhouse/source/ClickhouseInputFormat.java
Patch:
@@ -33,6 +33,7 @@ public class ClickhouseInputFormat extends JdbcInputFormat {
 
     @Override
     protected Connection getConnection() throws SQLException {
-        return ClickhouseUtil.getConnection(jdbcConf.getJdbcUrl(), jdbcConf.getUsername(), jdbcConf.getPassword());
+        return ClickhouseUtil.getConnection(
+                jdbcConf.getJdbcUrl(), jdbcConf.getUsername(), jdbcConf.getPassword());
     }
 }

File: flinkx-connectors/flinkx-connector-clickhouse/src/main/java/com/dtstack/flinkx/connector/clickhouse/source/ClickhouseSourceFactory.java
Patch:
@@ -32,7 +32,6 @@
  * @author: xiuzhu
  * @create: 2021/05/10
  */
-
 public class ClickhouseSourceFactory extends JdbcSourceFactory {
 
     public ClickhouseSourceFactory(SyncConf syncConf, StreamExecutionEnvironment env) {

File: flinkx-connectors/flinkx-connector-clickhouse/src/main/java/com/dtstack/flinkx/connector/clickhouse/table/ClickhouseDynamicTableFactory.java
Patch:
@@ -30,8 +30,7 @@
  * @program: flinkx
  * @author: xiuzhu
  * @create: 2021/05/08
- **/
-
+ */
 public class ClickhouseDynamicTableFactory extends JdbcDynamicTableFactory {
 
     /** 通过该值查找具体插件 */

File: flinkx-connectors/flinkx-connector-clickhouse/src/main/java/com/dtstack/flinkx/connector/clickhouse/util/ClickhouseUtil.java
Patch:
@@ -29,15 +29,15 @@
 import java.util.Properties;
 
 /**
- * Date: 2019/11/05
- * Company: www.dtstack.com
+ * Date: 2019/11/05 Company: www.dtstack.com
  *
  * @author tudou
  */
 public class ClickhouseUtil {
     private static final int MAX_RETRY_TIMES = 3;
 
-    public static Connection getConnection(String url, String username, String password) throws SQLException {
+    public static Connection getConnection(String url, String username, String password)
+            throws SQLException {
         Properties properties = new Properties();
         properties.put(ClickHouseQueryParam.USER.getKey(), username);
         properties.put(ClickHouseQueryParam.PASSWORD.getKey(), password);

File: flinkx-connectors/flinkx-connector-db2/src/main/java/com/dtstack/flinkx/connector/db2/converter/Db2RowConverter.java
Patch:
@@ -35,7 +35,6 @@
 import java.sql.Timestamp;
 
 /**
- *
  * @program: flinkx-all
  * @author: xuchao
  * @create: 2021/05/20 17:08

File: flinkx-connectors/flinkx-connector-dm/src/main/java/com/dtstack/flinkx/connector/dm/sink/DmSinkFactory.java
Patch:
@@ -22,9 +22,7 @@
 import com.dtstack.flinkx.connector.dm.dialect.DmDialect;
 import com.dtstack.flinkx.connector.jdbc.sink.JdbcSinkFactory;
 
-/**
- * @author kunni
- */
+/** @author kunni */
 public class DmSinkFactory extends JdbcSinkFactory {
 
     public DmSinkFactory(SyncConf syncConf) {

File: flinkx-connectors/flinkx-connector-emqx/src/main/java/com/dtstack/flinkx/connector/emqx/converter/EmqxRowConverter.java
Patch:
@@ -18,12 +18,13 @@
 
 package com.dtstack.flinkx.connector.emqx.converter;
 
+import com.dtstack.flinkx.converter.AbstractRowConverter;
+
 import org.apache.flink.api.common.serialization.DeserializationSchema;
 import org.apache.flink.api.common.serialization.SerializationSchema;
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.types.logical.LogicalType;
 
-import com.dtstack.flinkx.converter.AbstractRowConverter;
 import org.eclipse.paho.client.mqttv3.MqttMessage;
 
 import java.nio.charset.StandardCharsets;

File: flinkx-connectors/flinkx-connector-emqx/src/main/java/com/dtstack/flinkx/connector/emqx/sink/EmqxOutputFormat.java
Patch:
@@ -18,12 +18,13 @@
 
 package com.dtstack.flinkx.connector.emqx.sink;
 
-import org.apache.flink.table.data.RowData;
-
 import com.dtstack.flinkx.connector.emqx.conf.EmqxConf;
 import com.dtstack.flinkx.connector.emqx.util.MqttConnectUtil;
 import com.dtstack.flinkx.sink.format.BaseRichOutputFormat;
 import com.dtstack.flinkx.throwable.WriteRecordException;
+
+import org.apache.flink.table.data.RowData;
+
 import org.eclipse.paho.client.mqttv3.MqttClient;
 import org.eclipse.paho.client.mqttv3.MqttException;
 import org.eclipse.paho.client.mqttv3.MqttMessage;

File: flinkx-connectors/flinkx-connector-emqx/src/main/java/com/dtstack/flinkx/connector/emqx/sink/EmqxOutputFormatBuilder.java
Patch:
@@ -20,6 +20,7 @@
 
 import com.dtstack.flinkx.connector.emqx.conf.EmqxConf;
 import com.dtstack.flinkx.sink.format.BaseRichOutputFormatBuilder;
+
 import org.apache.commons.lang3.StringUtils;
 
 /**

File: flinkx-connectors/flinkx-connector-emqx/src/main/java/com/dtstack/flinkx/connector/emqx/source/EmqxInputFormatBuilder.java
Patch:
@@ -20,6 +20,7 @@
 
 import com.dtstack.flinkx.connector.emqx.conf.EmqxConf;
 import com.dtstack.flinkx.source.format.BaseRichInputFormatBuilder;
+
 import org.apache.commons.lang3.StringUtils;
 
 /**

File: flinkx-connectors/flinkx-connector-emqx/src/main/java/com/dtstack/flinkx/connector/emqx/util/DataTypeConventerUtil.java
Patch:
@@ -32,7 +32,7 @@
  * @author chuixue
  * @create 2021-06-18 14:26
  * @description
- **/
+ */
 public class DataTypeConventerUtil {
     public static int[] createValueFormatProjection(DataType physicalDataType) {
         final LogicalType physicalType = physicalDataType.getLogicalType();

File: flinkx-connectors/flinkx-connector-emqx/src/main/java/com/dtstack/flinkx/connector/emqx/util/MqttConnectUtil.java
Patch:
@@ -20,6 +20,7 @@
 
 import com.dtstack.flinkx.connector.emqx.conf.EmqxConf;
 import com.dtstack.flinkx.util.ExceptionUtil;
+
 import org.apache.commons.lang3.StringUtils;
 import org.eclipse.paho.client.mqttv3.MqttClient;
 import org.eclipse.paho.client.mqttv3.MqttConnectOptions;
@@ -80,7 +81,7 @@ public static MqttClient getMqttClient(EmqxConf emqxConf, String clientId) {
         return client;
     }
 
-    public static void close(MqttClient client){
+    public static void close(MqttClient client) {
         try {
             if (client != null && client.isConnected()) {
                 client.disconnect();

File: flinkx-connectors/flinkx-connector-file/src/main/java/com/dtstack/flinkx/connector/file/converter/FileRowConverter.java
Patch:
@@ -30,7 +30,6 @@
  * @author: xiuzhu
  * @create: 2021/06/24
  */
-
 public class FileRowConverter extends AbstractRowConverter<String, String, String, LogicalType> {
 
     private DeserializationSchema<RowData> valueDeserialization;
@@ -48,6 +47,4 @@ public RowData toInternal(String input) throws Exception {
     public String toExternal(RowData rowData, String output) throws Exception {
         throw new FlinkxRuntimeException("Sink type conversion is not supported! ");
     }
-
-
 }

File: flinkx-connectors/flinkx-connector-file/src/main/java/com/dtstack/flinkx/connector/file/options/FileOptions.java
Patch:
@@ -28,7 +28,6 @@
  * @author: xiuzhu
  * @create: 2021/06/25
  */
-
 public class FileOptions extends BaseFileOptions {
 
     public static final ConfigOption<String> FORMAT =
@@ -38,5 +37,4 @@ public class FileOptions extends BaseFileOptions {
                     .withDescription(
                             "Defines the format identifier for encoding value data. "
                                     + "The identifier is used to discover a suitable format factory.");
-
 }

File: flinkx-connectors/flinkx-connector-file/src/main/java/com/dtstack/flinkx/connector/file/source/FileInputBufferedReader.java
Patch:
@@ -33,7 +33,6 @@
  * @author: xiuzhu
  * @create: 2021/06/24
  */
-
 public class FileInputBufferedReader {
 
     private List<String> paths;
@@ -78,7 +77,8 @@ public void nextFileStream() throws IOException {
         if (pathIterator.hasNext()) {
             String filePath = pathIterator.next();
             String encoding = fileConf.getEncoding();
-            InputStreamReader isr = new InputStreamReader(new FileInputStream(new File(filePath)), encoding);
+            InputStreamReader isr =
+                    new InputStreamReader(new FileInputStream(new File(filePath)), encoding);
             br = new BufferedReader(isr);
         } else {
             br = null;

File: flinkx-connectors/flinkx-connector-file/src/main/java/com/dtstack/flinkx/connector/file/source/FileInputSplit.java
Patch:
@@ -28,7 +28,6 @@
  * @author: xiuzhu
  * @create: 2021/06/24
  */
-
 public class FileInputSplit implements InputSplit {
 
     private int splitNumber;

File: flinkx-connectors/flinkx-connector-file/src/main/java/com/dtstack/flinkx/connector/file/table/FileDynamicTableFactory.java
Patch:
@@ -74,7 +74,8 @@ private BaseFileConf getFileConfByOptions(ReadableConfig config) {
 
     private DecodingFormat<DeserializationSchema<RowData>> getDecodingFormat(
             FactoryUtil.TableFactoryHelper helper) {
-        return helper.discoverDecodingFormat(DeserializationFormatFactory.class, FileOptions.FORMAT);
+        return helper.discoverDecodingFormat(
+                DeserializationFormatFactory.class, FileOptions.FORMAT);
     }
 
     @Override

File: flinkx-connectors/flinkx-connector-ftp/src/main/java/com/dtstack/flinkx/connector/ftp/conf/ConfigConstants.java
Patch:
@@ -20,8 +20,8 @@
 
 /**
  * The class containing Ftp configuration constants
- * <p>
- * Company: www.dtstack.com
+ *
+ * <p>Company: www.dtstack.com
  *
  * @author huyifan.zju@163.com
  */
@@ -40,5 +40,4 @@ public class ConfigConstants {
     public static final String FTP_PROTOCOL = "ftp";
 
     public static final String DEFAULT_FIELD_DELIMITER = ",";
-
 }

File: flinkx-connectors/flinkx-connector-ftp/src/main/java/com/dtstack/flinkx/connector/ftp/converter/FtpRawTypeConverter.java
Patch:
@@ -32,7 +32,6 @@
  * @author: xiuzhu
  * @create: 2021/06/28
  */
-
 public class FtpRawTypeConverter {
 
     public static DataType apply(String type) throws UnsupportedTypeException {
@@ -64,7 +63,9 @@ public static DataType apply(String type) throws UnsupportedTypeException {
                 if (precision != null) {
                     String[] split = precision.split(ConstantValue.COMMA_SYMBOL);
                     if (split.length == 2) {
-                        return DataTypes.DECIMAL(Integer.parseInt(split[0].trim()), Integer.parseInt(split[1].trim()));
+                        return DataTypes.DECIMAL(
+                                Integer.parseInt(split[0].trim()),
+                                Integer.parseInt(split[1].trim()));
                     }
                 }
                 return DataTypes.DECIMAL(38, 18);

File: flinkx-connectors/flinkx-connector-ftp/src/main/java/com/dtstack/flinkx/connector/ftp/converter/FtpRowConverter.java
Patch:
@@ -30,7 +30,6 @@
  * @author: xiuzhu
  * @create: 2021/06/19
  */
-
 public class FtpRowConverter extends AbstractRowConverter<String, String, String, LogicalType> {
 
     private DeserializationSchema<RowData> valueDeserialization;

File: flinkx-connectors/flinkx-connector-ftp/src/main/java/com/dtstack/flinkx/connector/ftp/handler/FtpHandlerFactory.java
Patch:
@@ -40,7 +40,8 @@ public static IFtpHandler createFtpHandler(String protocolStr) {
     }
 
     enum Protocol {
-        FTP, SFTP;
+        FTP,
+        SFTP;
 
         public static Protocol getByName(String name) {
             if (StringUtils.isEmpty(name)) {

File: flinkx-connectors/flinkx-connector-ftp/src/main/java/com/dtstack/flinkx/connector/ftp/sink/FtpSinkFactory.java
Patch:
@@ -47,8 +47,9 @@ public class FtpSinkFactory extends SinkFactory {
 
     public FtpSinkFactory(SyncConf syncConf) {
         super(syncConf);
-        ftpConfig = JsonUtil.toObject(
-                JsonUtil.toJson(syncConf.getWriter().getParameter()), FtpConfig.class);
+        ftpConfig =
+                JsonUtil.toObject(
+                        JsonUtil.toJson(syncConf.getWriter().getParameter()), FtpConfig.class);
 
         if (ftpConfig.getPort() == null) {
             ftpConfig.setDefaultPort();

File: flinkx-connectors/flinkx-connector-ftp/src/main/java/com/dtstack/flinkx/connector/ftp/source/FtpInputFormat.java
Patch:
@@ -145,7 +145,8 @@ protected RowData nextRecordInternal(RowData rowData) throws ReadRecordException
                         Object value = null;
                         if (fieldConf.getValue() != null) {
                             value = fieldConf.getValue();
-                        } else if (fieldConf.getIndex() != null && fieldConf.getIndex() < fields.length) {
+                        } else if (fieldConf.getIndex() != null
+                                && fieldConf.getIndex() < fields.length) {
                             value = fields[fieldConf.getIndex()];
                         }
                         genericRowData.setField(i, value);

File: flinkx-connectors/flinkx-connector-ftp/src/main/java/com/dtstack/flinkx/connector/ftp/source/FtpInputSplit.java
Patch:
@@ -25,8 +25,8 @@
 
 /**
  * The Class describing each InputSplit of Ftp
- * <p>
- * Company: www.dtstack.com
+ *
+ * <p>Company: www.dtstack.com
  *
  * @author huyifan.zju@163.com
  */

File: flinkx-connectors/flinkx-connector-gbase/src/main/java/com/dtstack/flinkx/connector/gbase/converter/GBaseRawTypeConverter.java
Patch:
@@ -18,11 +18,11 @@
 
 package com.dtstack.flinkx.connector.gbase.converter;
 
+import com.dtstack.flinkx.throwable.UnsupportedTypeException;
+
 import org.apache.flink.table.api.DataTypes;
 import org.apache.flink.table.types.DataType;
 
-import com.dtstack.flinkx.throwable.UnsupportedTypeException;
-
 import java.util.Locale;
 
 /**

File: flinkx-connectors/flinkx-connector-gbase/src/main/java/com/dtstack/flinkx/connector/gbase/dialect/GBaseDialect.java
Patch:
@@ -21,6 +21,7 @@
 import com.dtstack.flinkx.connector.gbase.converter.GBaseRawTypeConverter;
 import com.dtstack.flinkx.connector.jdbc.dialect.JdbcDialect;
 import com.dtstack.flinkx.converter.RawTypeConverter;
+
 import org.apache.commons.lang3.StringUtils;
 
 import java.util.Arrays;

File: flinkx-connectors/flinkx-connector-hbase-base/src/main/java/com/dtstack/flinkx/connector/hbase14/HBaseTableSchema.java
Patch:
@@ -42,7 +42,7 @@
  * @author wuren
  * @program flinkx
  * @create 2021/04/30
- **/
+ */
 
 /** Helps to specify an HBase Table's schema. */
 public class HBaseTableSchema implements Serializable {

File: flinkx-connectors/flinkx-connector-hdfs/src/main/java/com/dtstack/flinkx/connector/hdfs/InputSplit/HdfsParquetSplit.java
Patch:
@@ -22,8 +22,7 @@
 import java.util.List;
 
 /**
- * Date: 2021/06/08
- * Company: www.dtstack.com
+ * Date: 2021/06/08 Company: www.dtstack.com
  *
  * @author tudou
  */

File: flinkx-connectors/flinkx-connector-hdfs/src/main/java/com/dtstack/flinkx/connector/hdfs/enums/FileType.java
Patch:
@@ -18,13 +18,13 @@
 package com.dtstack.flinkx.connector.hdfs.enums;
 
 import com.dtstack.flinkx.throwable.UnsupportedTypeException;
+
 import org.apache.commons.lang3.StringUtils;
 
 import java.util.Locale;
 
 /**
- * Date: 2021/06/17
- * Company: www.dtstack.com
+ * Date: 2021/06/17 Company: www.dtstack.com
  *
  * @author tudou
  */

File: flinkx-connectors/flinkx-connector-hdfs/src/main/java/com/dtstack/flinkx/connector/hdfs/sink/HdfsOrcOutputFormat.java
Patch:
@@ -17,8 +17,6 @@
  */
 package com.dtstack.flinkx.connector.hdfs.sink;
 
-import org.apache.flink.table.data.RowData;
-
 import com.dtstack.flinkx.conf.FieldConf;
 import com.dtstack.flinkx.connector.hdfs.converter.HdfsOrcColumnConverter;
 import com.dtstack.flinkx.connector.hdfs.enums.CompressType;
@@ -32,6 +30,9 @@
 import com.dtstack.flinkx.util.ExceptionUtil;
 import com.dtstack.flinkx.util.FileSystemUtil;
 import com.dtstack.flinkx.util.ReflectionUtils;
+
+import org.apache.flink.table.data.RowData;
+
 import com.google.common.collect.Maps;
 import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.ql.io.orc.OrcFile;

File: flinkx-connectors/flinkx-connector-hdfs/src/main/java/com/dtstack/flinkx/connector/hdfs/sink/HdfsOutputFormatBuilder.java
Patch:
@@ -22,6 +22,7 @@
 import com.dtstack.flinkx.constants.ConstantValue;
 import com.dtstack.flinkx.sink.format.FileOutputFormatBuilder;
 import com.dtstack.flinkx.throwable.FlinkxRuntimeException;
+
 import org.apache.commons.lang3.StringUtils;
 
 /**

File: flinkx-connectors/flinkx-connector-hdfs/src/main/java/com/dtstack/flinkx/connector/hdfs/sink/HdfsTextOutputFormat.java
Patch:
@@ -17,8 +17,6 @@
  */
 package com.dtstack.flinkx.connector.hdfs.sink;
 
-import org.apache.flink.table.data.RowData;
-
 import com.dtstack.flinkx.conf.FieldConf;
 import com.dtstack.flinkx.connector.hdfs.enums.CompressType;
 import com.dtstack.flinkx.connector.hdfs.enums.FileType;
@@ -27,6 +25,9 @@
 import com.dtstack.flinkx.throwable.FlinkxRuntimeException;
 import com.dtstack.flinkx.throwable.WriteRecordException;
 import com.dtstack.flinkx.util.ExceptionUtil;
+
+import org.apache.flink.table.data.RowData;
+
 import org.apache.commons.compress.compressors.bzip2.BZip2CompressorOutputStream;
 import org.apache.commons.compress.compressors.gzip.GzipCompressorOutputStream;
 import org.apache.hadoop.fs.Path;

File: flinkx-connectors/flinkx-connector-hive/src/main/java/com/dtstack/flinkx/connector/hive/entity/HiveFormatState.java
Patch:
@@ -24,8 +24,7 @@
 import java.util.Map;
 
 /**
- * Date: 2021/06/22
- * Company: www.dtstack.com
+ * Date: 2021/06/22 Company: www.dtstack.com
  *
  * @author tudou
  */

File: flinkx-connectors/flinkx-connector-hive/src/main/java/com/dtstack/flinkx/connector/hive/parser/Apache2MetadataParser.java
Patch:
@@ -23,12 +23,11 @@
 import java.util.Map;
 
 /**
- * Date: 2021/06/22
- * Company: www.dtstack.com
+ * Date: 2021/06/22 Company: www.dtstack.com
  *
  * @author tudou
  */
-public class Apache2MetadataParser extends AbstractHiveMetadataParser{
+public class Apache2MetadataParser extends AbstractHiveMetadataParser {
 
     @Override
     public void fillTableInfo(TableInfo tableInfo, List<Map<String, Object>> result) {

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/JdbcDialectWrapper.java
Patch:
@@ -11,7 +11,6 @@ public JdbcDialectWrapper(org.apache.flink.connector.jdbc.dialect.JdbcDialect di
         this.dialect = dialect;
     }
 
-
     @Override
     public String dialectName() {
         return dialect.dialectName();

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/table/JdbcDynamicTableFactory.java
Patch:
@@ -51,7 +51,6 @@
 import java.util.Collections;
 import java.util.HashSet;
 import java.util.List;
-import java.util.Locale;
 import java.util.Map;
 import java.util.Optional;
 import java.util.Set;

File: flinkx-connectors/flinkx-connector-kafka/src/main/java/com/dtstack/flinkx/connector/kafka/option/KafkaOptions.java
Patch:
@@ -25,7 +25,7 @@
  * @author chuixue
  * @create 2021-06-07 15:53
  * @description
- **/
+ */
 public class KafkaOptions {
     public static final ConfigOption<String> DEFAULT_CODEC =
             ConfigOptions.key("default.codec")

File: flinkx-connectors/flinkx-connector-kafka/src/main/java/com/dtstack/flinkx/connector/kafka/source/Calculate.java
Patch:
@@ -25,8 +25,8 @@
 
 /**
  * company: www.dtstack.com
- * @author: toutian
- * create: 2019/12/24
+ *
+ * @author: toutian create: 2019/12/24
  */
 @FunctionalInterface
 public interface Calculate extends Serializable {

File: flinkx-connectors/flinkx-connector-kudu/src/main/java/com/dtstack/flinkx/connector/kudu/conf/KuduSinkConf.java
Patch:
@@ -20,10 +20,10 @@
 
 import com.dtstack.flinkx.sink.WriteMode;
 
-import org.apache.commons.lang3.builder.ToStringBuilder;
-
 import org.apache.flink.configuration.ReadableConfig;
 
+import org.apache.commons.lang3.builder.ToStringBuilder;
+
 import java.util.Locale;
 
 import static com.dtstack.flinkx.connector.kudu.options.KuduOptions.FLUSH_INTERVAL;

File: flinkx-connectors/flinkx-connector-kudu/src/main/java/com/dtstack/flinkx/connector/kudu/lookup/KuduAllTableFunction.java
Patch:
@@ -24,10 +24,10 @@
 import com.dtstack.flinkx.converter.AbstractRowConverter;
 import com.dtstack.flinkx.lookup.AbstractAllTableFunction;
 import com.dtstack.flinkx.util.ThreadUtil;
-import com.google.common.collect.Maps;
 
 import org.apache.flink.table.data.GenericRowData;
 
+import com.google.common.collect.Maps;
 import org.apache.kudu.client.KuduClient;
 import org.apache.kudu.client.KuduException;
 import org.apache.kudu.client.KuduScanner;
@@ -37,7 +37,6 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import java.io.IOException;
 import java.util.Arrays;
 import java.util.List;
 import java.util.Map;

File: flinkx-connectors/flinkx-connector-kudu/src/main/java/com/dtstack/flinkx/connector/kudu/sink/KuduOutputFormat.java
Patch:
@@ -18,15 +18,16 @@
 
 package com.dtstack.flinkx.connector.kudu.sink;
 
-import org.apache.flink.table.data.RowData;
-
 import com.dtstack.flinkx.connector.kudu.conf.KuduSinkConf;
 import com.dtstack.flinkx.connector.kudu.util.KuduUtil;
 import com.dtstack.flinkx.sink.WriteMode;
 import com.dtstack.flinkx.sink.format.BaseRichOutputFormat;
 import com.dtstack.flinkx.throwable.FlinkxRuntimeException;
 import com.dtstack.flinkx.throwable.NoRestartException;
 import com.dtstack.flinkx.throwable.WriteRecordException;
+
+import org.apache.flink.table.data.RowData;
+
 import org.apache.kudu.client.KuduClient;
 import org.apache.kudu.client.KuduException;
 import org.apache.kudu.client.KuduSession;

File: flinkx-connectors/flinkx-connector-kudu/src/main/java/com/dtstack/flinkx/connector/kudu/sink/KuduOutputFormatBuilder.java
Patch:
@@ -21,6 +21,7 @@
 import com.dtstack.flinkx.connector.kudu.conf.KuduSinkConf;
 import com.dtstack.flinkx.sink.format.BaseRichOutputFormatBuilder;
 import com.dtstack.flinkx.throwable.NoRestartException;
+
 import org.apache.commons.lang3.StringUtils;
 
 /**

File: flinkx-connectors/flinkx-connector-kudu/src/main/java/com/dtstack/flinkx/connector/kudu/util/KuduUtil.java
Patch:
@@ -25,6 +25,7 @@
 import com.dtstack.flinkx.security.KerberosConfig;
 import com.dtstack.flinkx.security.KerberosUtils;
 import com.dtstack.flinkx.throwable.NoRestartException;
+
 import org.apache.commons.lang.StringUtils;
 import org.apache.commons.lang.math.NumberUtils;
 import org.apache.hadoop.security.UserGroupInformation;

File: flinkx-connectors/flinkx-connector-mysql/src/main/java/com/dtstack/flinkx/connector/mysql/converter/MysqlRawTypeConverter.java
Patch:
@@ -11,12 +11,11 @@
  * @program: flinkx
  * @author: wuren
  * @create: 2021/04/14
- **/
+ */
 public class MysqlRawTypeConverter {
 
     /**
-     * 将MySQL数据库中的类型，转换成flink的DataType类型。
-     * 转换关系参考 com.mysql.jdbc.MysqlDefs 类里面的信息。
+     * 将MySQL数据库中的类型，转换成flink的DataType类型。 转换关系参考 com.mysql.jdbc.MysqlDefs 类里面的信息。
      * com.mysql.jdbc.ResultSetImpl.getObject(int)
      */
     public static DataType apply(String type) {

File: flinkx-connectors/flinkx-connector-mysql/src/main/java/com/dtstack/flinkx/connector/mysql/source/MysqlSourceFactory.java
Patch:
@@ -18,11 +18,12 @@
 
 package com.dtstack.flinkx.connector.mysql.source;
 
-import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
-
 import com.dtstack.flinkx.conf.SyncConf;
 import com.dtstack.flinkx.connector.jdbc.source.JdbcSourceFactory;
 import com.dtstack.flinkx.connector.mysql.dialect.MysqlDialect;
+
+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
+
 import org.apache.commons.lang3.StringUtils;
 
 /**

File: flinkx-connectors/flinkx-connector-mysql/src/main/java/com/dtstack/flinkx/connector/mysql/table/MysqlDynamicTableFactory.java
Patch:
@@ -26,7 +26,7 @@
  * @program: flinkx
  * @author: wuren
  * @create: 2021/03/17
- **/
+ */
 public class MysqlDynamicTableFactory extends JdbcDynamicTableFactory {
 
     // 默认是Mysql流式拉取

File: flinkx-connectors/flinkx-connector-oraclelogminer/src/main/java/com/dtstack/flinkx/connector/oraclelogminer/converter/LogMinerColumnConverter.java
Patch:
@@ -93,9 +93,9 @@ public LinkedList<RowData> toInternal(EventRow eventRow) throws Exception {
                 || Objects.isNull(metadata)
                 || beforeColumnList.size() != converters.length
                 || !beforeColumnList.stream()
-                .map(EventRowData::getName)
-                .collect(Collectors.toCollection(HashSet::new))
-                .containsAll(metadata.getFieldList())) {
+                        .map(EventRowData::getName)
+                        .collect(Collectors.toCollection(HashSet::new))
+                        .containsAll(metadata.getFieldList())) {
             Pair<List<String>, List<String>> latestMetaData =
                     JdbcUtil.getTableMetaData(schema, table, connection);
             converters =

File: flinkx-connectors/flinkx-connector-oraclelogminer/src/main/java/com/dtstack/flinkx/connector/oraclelogminer/entity/EventRow.java
Patch:
@@ -27,8 +27,7 @@
  * Date: 2021/08/13 Company: www.dtstack.com
  *
  * @author dujie
- *
- * 事件数据
+ *     <p>事件数据
  */
 public class EventRow implements Serializable {
     private static final long serialVersionUID = 1L;

File: flinkx-connectors/flinkx-connector-oraclelogminer/src/main/java/com/dtstack/flinkx/connector/oraclelogminer/entity/OracleInfo.java
Patch:
@@ -22,8 +22,7 @@
  * Date: 2021/08/13 Company: www.dtstack.com
  *
  * @author dujie
- *
- * logminer监听的oracle数据源信息
+ *     <p>logminer监听的oracle数据源信息
  */
 public class OracleInfo {
 

File: flinkx-connectors/flinkx-connector-oraclelogminer/src/main/java/com/dtstack/flinkx/connector/oraclelogminer/entity/RecordLog.java
Patch:
@@ -24,8 +24,7 @@
  * Date: 2021/08/13 Company: www.dtstack.com
  *
  * @author dujie
- *
- * v$logmnr_contents 对应的实体 logminer读取出的数据实体
+ *     <p>v$logmnr_contents 对应的实体 logminer读取出的数据实体
  */
 public class RecordLog {
 

File: flinkx-connectors/flinkx-connector-oraclelogminer/src/main/java/com/dtstack/flinkx/connector/oraclelogminer/listener/LogParser.java
Patch:
@@ -51,7 +51,6 @@
 import java.util.Iterator;
 import java.util.LinkedList;
 import java.util.List;
-import java.util.Map;
 import java.util.Objects;
 
 /**
@@ -266,7 +265,7 @@ public LinkedList<RowData> parse(QueueData pair, AbstractCDCRowConverter rowConv
 
         Long ts = idWorker.nextId();
 
-        if(LOG.isDebugEnabled()){
+        if (LOG.isDebugEnabled()) {
             printDelay(pair.getScn(), ts, timestamp);
         }
 
@@ -292,5 +291,4 @@ private void printDelay(BigInteger scn, long ts, Timestamp timestamp) {
 
         LOG.debug("scn {} ,delay {} ms", scn, res - opTime);
     }
-
 }

File: flinkx-connectors/flinkx-connector-oraclelogminer/src/main/java/com/dtstack/flinkx/connector/oraclelogminer/listener/Transaction.java
Patch:
@@ -24,7 +24,6 @@
 
 import java.io.Serializable;
 import java.math.BigInteger;
-import java.util.LinkedList;
 import java.util.List;
 
 /**
@@ -69,6 +68,6 @@ public void remove(RecordLog recordLog) {
     }
 
     public void removeLast() {
-        recordLogs.remove(recordLogs.size()-1);
+        recordLogs.remove(recordLogs.size() - 1);
     }
 }

File: flinkx-connectors/flinkx-connector-oraclelogminer/src/main/java/com/dtstack/flinkx/connector/oraclelogminer/table/OraclelogminerDynamicTableFactory.java
Patch:
@@ -118,7 +118,8 @@ private LogMinerConf getLogMinerConf(ReadableConfig config) {
 
         logMinerConf.setIoThreads(config.get(LogminerOptions.IO_THREADS));
 
-        logMinerConf.setTransactionCacheNumSize(config.get(LogminerOptions.TRANSACTION_CACHE_NUM_SIZE));
+        logMinerConf.setTransactionCacheNumSize(
+                config.get(LogminerOptions.TRANSACTION_CACHE_NUM_SIZE));
         logMinerConf.setTransactionExpireTime(config.get(LogminerOptions.TRANSACTION_EXPIRE_TIME));
 
         logMinerConf.setPavingData(true);

File: flinkx-connectors/flinkx-connector-phoenix5/src/main/java/com/dtstack/flinkx/connector/phoenix5/sink/Phoenix5OutputFormat.java
Patch:
@@ -18,15 +18,13 @@
 
 package com.dtstack.flinkx.connector.phoenix5.sink;
 
-import com.dtstack.flinkx.enums.Semantic;
-import org.apache.flink.table.types.logical.RowType;
-
 import com.dtstack.flinkx.connector.jdbc.sink.JdbcOutputFormat;
 import com.dtstack.flinkx.connector.jdbc.statement.FieldNamedPreparedStatement;
 import com.dtstack.flinkx.connector.jdbc.util.JdbcUtil;
 import com.dtstack.flinkx.connector.phoenix5.converter.Phoenix5RawTypeConverter;
 import com.dtstack.flinkx.connector.phoenix5.util.Phoenix5Util;
 import com.dtstack.flinkx.enums.EWriteMode;
+import com.dtstack.flinkx.enums.Semantic;
 import com.dtstack.flinkx.util.TableUtil;
 
 import org.apache.flink.table.types.logical.RowType;

File: flinkx-connectors/flinkx-connector-redis/src/main/java/com/dtstack/flinkx/connector/redis/lookup/RedisAllTableFunction.java
Patch:
@@ -18,14 +18,15 @@
 
 package com.dtstack.flinkx.connector.redis.lookup;
 
-import org.apache.flink.table.data.GenericRowData;
-
 import com.dtstack.flinkx.connector.redis.conf.RedisConf;
 import com.dtstack.flinkx.connector.redis.connection.RedisSyncClient;
 import com.dtstack.flinkx.connector.redis.enums.RedisConnectType;
 import com.dtstack.flinkx.converter.AbstractRowConverter;
 import com.dtstack.flinkx.lookup.AbstractAllTableFunction;
 import com.dtstack.flinkx.lookup.conf.LookupConf;
+
+import org.apache.flink.table.data.GenericRowData;
+
 import com.google.common.collect.Lists;
 import org.apache.commons.collections.CollectionUtils;
 import org.slf4j.Logger;

File: flinkx-connectors/flinkx-connector-redis/src/main/java/com/dtstack/flinkx/connector/redis/sink/RedisOutputFormat.java
Patch:
@@ -18,12 +18,13 @@
 
 package com.dtstack.flinkx.connector.redis.sink;
 
-import org.apache.flink.table.data.RowData;
-
 import com.dtstack.flinkx.connector.redis.conf.RedisConf;
 import com.dtstack.flinkx.connector.redis.connection.RedisSyncClient;
 import com.dtstack.flinkx.sink.format.BaseRichOutputFormat;
 import com.dtstack.flinkx.throwable.WriteRecordException;
+
+import org.apache.flink.table.data.RowData;
+
 import redis.clients.jedis.JedisCommands;
 
 /**

File: flinkx-connectors/flinkx-connector-restapi/src/main/java/com/dtstack/flinkx/connector/restapi/common/MyHttpRequestRetryHandler.java
Patch:
@@ -77,7 +77,6 @@ public boolean retryRequest(IOException exception, int executionCount, HttpConte
         return !idempotent;
     }
 
-
     public static final class Builder {
         private int executionMaxCount;
 

File: flinkx-connectors/flinkx-connector-restapi/src/main/java/com/dtstack/flinkx/connector/restapi/common/MyServiceUnavailableRetryStrategy.java
Patch:
@@ -23,8 +23,7 @@
 
 /**
  * @author : shifang
- * @date : 2020/3/12
- * 自定义httpClient重试策略,默认重试次数为5,重试时间间隔为2s
+ * @date : 2020/3/12 自定义httpClient重试策略,默认重试次数为5,重试时间间隔为2s
  */
 public class MyServiceUnavailableRetryStrategy implements ServiceUnavailableRetryStrategy {
     private int executionCount;
@@ -36,7 +35,8 @@ public MyServiceUnavailableRetryStrategy(Builder builder) {
     }
 
     @Override
-    public boolean retryRequest(HttpResponse httpResponse, int executionCount, HttpContext httpContext) {
+    public boolean retryRequest(
+            HttpResponse httpResponse, int executionCount, HttpContext httpContext) {
         int successCode = 200;
         return httpResponse.getStatusLine().getStatusCode() != successCode
                 && executionCount < this.executionCount;

File: flinkx-connectors/flinkx-connector-restapi/src/main/java/com/dtstack/flinkx/connector/restapi/inputformat/RestapiInputFormatBuilder.java
Patch:
@@ -28,6 +28,7 @@
 import com.dtstack.flinkx.source.format.BaseRichInputFormatBuilder;
 import com.dtstack.flinkx.util.GsonUtil;
 import com.dtstack.flinkx.util.StringUtil;
+
 import com.google.common.collect.Sets;
 import org.apache.commons.collections.CollectionUtils;
 import org.apache.commons.lang.text.StrBuilder;

File: flinkx-connectors/flinkx-connector-restapi/src/main/java/com/dtstack/flinkx/connector/restapi/outputformat/RestapiOutputFormat.java
Patch:
@@ -17,13 +17,14 @@
  */
 package com.dtstack.flinkx.connector.restapi.outputformat;
 
-import org.apache.flink.table.data.RowData;
-
 import com.dtstack.flinkx.connector.restapi.common.HttpUtil;
 import com.dtstack.flinkx.connector.restapi.common.RestapiWriterConfig;
 import com.dtstack.flinkx.element.ColumnRowData;
 import com.dtstack.flinkx.sink.format.BaseRichOutputFormat;
 import com.dtstack.flinkx.util.ExceptionUtil;
+
+import org.apache.flink.table.data.RowData;
+
 import com.google.common.collect.Maps;
 import com.google.gson.Gson;
 import com.google.gson.GsonBuilder;

File: flinkx-connectors/flinkx-connector-socket/src/main/java/com/dtstack/flinkx/connector/socket/inputformat/SocketInputFormatBuilder.java
Patch:
@@ -23,6 +23,7 @@
 import com.dtstack.flinkx.source.format.BaseRichInputFormatBuilder;
 import com.dtstack.flinkx.util.TelnetUtil;
 import com.dtstack.flinkx.util.ValueUtil;
+
 import org.apache.commons.lang3.StringUtils;
 
 /**

File: flinkx-connectors/flinkx-connector-socket/src/main/java/com/dtstack/flinkx/connector/socket/source/SocketSourceFactory.java
Patch:
@@ -45,7 +45,6 @@ public SocketSourceFactory(SyncConf config, StreamExecutionEnvironment env) {
         super.initFlinkxCommonConf(socketConfig);
     }
 
-
     @Override
     public DataStream<RowData> createSource() {
         SocketInputFormatBuilder builder = new SocketInputFormatBuilder();

File: flinkx-core/src/main/java/com/dtstack/flinkx/conf/FlinkxCommonConf.java
Patch:
@@ -17,16 +17,13 @@
  */
 package com.dtstack.flinkx.conf;
 
-import com.dtstack.flinkx.enums.Semantic;
-
 import java.io.Serializable;
 import java.lang.reflect.Field;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
-import java.util.Locale;
 import java.util.Map;
 
 /**

File: flinkx-connectors/flinkx-connector-db2/src/main/java/com/dtstack/flinkx/connector/db2/converter/Db2RawTypeConverter.java
Patch:
@@ -25,8 +25,8 @@
 import java.util.Locale;
 
 /**
- * convert db2 type to flink type
- * Company: www.dtstack.com
+ * convert db2 type to flink type Company: www.dtstack.com
+ *
  * @author xuchao
  * @date 2021-06-15
  */

File: flinkx-connectors/flinkx-connector-db2/src/main/java/com/dtstack/flinkx/connector/db2/table/Db2DynamicTableFactory.java
Patch:
@@ -17,7 +17,6 @@
  */
 package com.dtstack.flinkx.connector.db2.table;
 
-
 import com.dtstack.flinkx.connector.db2.dialect.Db2Dialect;
 import com.dtstack.flinkx.connector.db2.sink.Db2OutputFormat;
 import com.dtstack.flinkx.connector.db2.source.Db2InputFormat;
@@ -28,6 +27,7 @@
 
 /**
  * Company: www.dtstack.com
+ *
  * @author xuchao
  * @date 2021-06-15
  */

File: flinkx-connectors/flinkx-connector-db2/src/main/java/com/dtstack/flinkx/connector/db2/converter/Db2RawTypeConverter.java
Patch:
@@ -25,8 +25,8 @@
 import java.util.Locale;
 
 /**
- * convert db2 type to flink type
- * Company: www.dtstack.com
+ * convert db2 type to flink type Company: www.dtstack.com
+ *
  * @author xuchao
  * @date 2021-06-15
  */

File: flinkx-connectors/flinkx-connector-db2/src/main/java/com/dtstack/flinkx/connector/db2/table/Db2DynamicTableFactory.java
Patch:
@@ -17,7 +17,6 @@
  */
 package com.dtstack.flinkx.connector.db2.table;
 
-
 import com.dtstack.flinkx.connector.db2.dialect.Db2Dialect;
 import com.dtstack.flinkx.connector.db2.sink.Db2OutputFormat;
 import com.dtstack.flinkx.connector.db2.source.Db2InputFormat;
@@ -28,6 +27,7 @@
 
 /**
  * Company: www.dtstack.com
+ *
  * @author xuchao
  * @date 2021-06-15
  */

File: flinkx-connectors/flinkx-connector-elasticsearch6/src/main/java/com/dtstack/flinkx/connector/elasticsearch6/sink/Elasticsearch6OutputFormat.java
Patch:
@@ -18,13 +18,14 @@
 
 package com.dtstack.flinkx.connector.elasticsearch6.sink;
 
-import org.apache.flink.table.data.RowData;
-
 import com.dtstack.flinkx.connector.elasticsearch6.conf.Elasticsearch6Conf;
 import com.dtstack.flinkx.connector.elasticsearch6.utils.Elasticsearch6RequestHelper;
 import com.dtstack.flinkx.connector.elasticsearch6.utils.Elasticsearch6Util;
 import com.dtstack.flinkx.sink.format.BaseRichOutputFormat;
 import com.dtstack.flinkx.throwable.WriteRecordException;
+
+import org.apache.flink.table.data.RowData;
+
 import org.elasticsearch.action.DocWriteRequest;
 import org.elasticsearch.action.bulk.BulkItemResponse;
 import org.elasticsearch.action.bulk.BulkRequest;

File: flinkx-connectors/flinkx-connector-elasticsearch6/src/main/java/com/dtstack/flinkx/connector/elasticsearch6/sink/Elasticsearch6OutputFormatBuilder.java
Patch:
@@ -20,6 +20,7 @@
 
 import com.dtstack.flinkx.connector.elasticsearch6.conf.Elasticsearch6Conf;
 import com.dtstack.flinkx.sink.format.BaseRichOutputFormatBuilder;
+
 import com.google.common.base.Preconditions;
 
 /**

File: flinkx-connectors/flinkx-connector-elasticsearch6/src/main/java/com/dtstack/flinkx/connector/elasticsearch6/source/Elasticsearch6InputFormatBuilder.java
Patch:
@@ -20,6 +20,7 @@
 
 import com.dtstack.flinkx.connector.elasticsearch6.conf.Elasticsearch6Conf;
 import com.dtstack.flinkx.source.format.BaseRichInputFormatBuilder;
+
 import com.google.common.base.Preconditions;
 
 /**

File: flinkx-connectors/flinkx-connector-elasticsearch7/src/main/java/com/dtstack/flinkx/connector/elasticsearch7/sink/Elasticsearch7SinkFactory.java
Patch:
@@ -61,7 +61,7 @@ public DataStreamSink<RowData> createSink(DataStream<RowData> dataSet) {
         final RowType rowType = TableUtil.createRowType(
                 elasticsearchConf.getColumn(),
                 getRawTypeConverter());
-        builder.setRowConverter(new ElasticsearchColumnConverter(elasticsearchConf, rowType));
+        builder.setRowConverter(new ElasticsearchColumnConverter(rowType));
         return createOutput(dataSet, builder.finish());
     }
 

File: flinkx-connectors/flinkx-connector-elasticsearch7/src/main/java/com/dtstack/flinkx/connector/elasticsearch7/sink/ElasticsearchOutputFormat.java
Patch:
@@ -3,8 +3,8 @@
 import com.dtstack.flinkx.connector.elasticsearch7.conf.ElasticsearchConf;
 import com.dtstack.flinkx.connector.elasticsearch7.utils.ElasticsearchRequestHelper;
 import com.dtstack.flinkx.connector.elasticsearch7.utils.ElasticsearchUtil;
-import com.dtstack.flinkx.exception.WriteRecordException;
-import com.dtstack.flinkx.outputformat.BaseRichOutputFormat;
+import com.dtstack.flinkx.sink.format.BaseRichOutputFormat;
+import com.dtstack.flinkx.throwable.WriteRecordException;
 
 import org.apache.flink.table.data.RowData;
 

File: flinkx-connectors/flinkx-connector-elasticsearch7/src/main/java/com/dtstack/flinkx/connector/elasticsearch7/sink/ElasticsearchOutputFormatBuilder.java
Patch:
@@ -19,7 +19,8 @@
 package com.dtstack.flinkx.connector.elasticsearch7.sink;
 
 import com.dtstack.flinkx.connector.elasticsearch7.conf.ElasticsearchConf;
-import com.dtstack.flinkx.outputformat.BaseRichOutputFormatBuilder;
+import com.dtstack.flinkx.sink.format.BaseRichOutputFormatBuilder;
+
 import com.google.common.base.Preconditions;
 
 /**

File: flinkx-connectors/flinkx-connector-elasticsearch7/src/main/java/com/dtstack/flinkx/connector/elasticsearch7/source/ElasticsearchInputFormatBuilder.java
Patch:
@@ -19,8 +19,8 @@
 package com.dtstack.flinkx.connector.elasticsearch7.source;
 
 import com.dtstack.flinkx.connector.elasticsearch7.conf.ElasticsearchConf;
-import com.dtstack.flinkx.converter.AbstractRowConverter;
-import com.dtstack.flinkx.inputformat.BaseRichInputFormatBuilder;
+import com.dtstack.flinkx.source.format.BaseRichInputFormatBuilder;
+
 import com.google.common.base.Preconditions;
 
 /**

File: flinkx-core/src/main/java/com/dtstack/flinkx/enums/ColumnType.java
Patch:
@@ -71,7 +71,8 @@ public enum ColumnType {
     TIME,
     DECIMAL,
     YEAR,
-    BIT;
+    BIT,
+    OBJECT;
 
     public static List<ColumnType> TIME_TYPE = Arrays.asList(DATE, DATETIME, TIME, TIMESTAMP);
 

File: flinkx-connectors/flinkx-connector-sqlserver/src/main/java/com/dtstack/flinkx/connector/sqlserver/converter/SqlserverJtdsRawTypeConverter.java
Patch:
@@ -19,6 +19,7 @@
 package com.dtstack.flinkx.connector.sqlserver.converter;
 
 import com.dtstack.flinkx.throwable.UnsupportedTypeException;
+
 import org.apache.flink.table.api.DataTypes;
 import org.apache.flink.table.types.DataType;
 
@@ -30,15 +31,13 @@
  * @author shitou
  * @date 2021/5/19 14:26
  */
-public class SqlserverRawTypeConverter {
+public class SqlserverJtdsRawTypeConverter {
 
     /**
      * Convert the data type in SqlServer to the DataType type in flink
      *
      * @param type
-     *
      * @return
-     *
      * @throws UnsupportedTypeException
      */
     public static DataType apply(String type) throws UnsupportedTypeException {

File: flinkx-connectors/flinkx-connector-sqlserver/src/main/java/com/dtstack/flinkx/connector/sqlserver/table/SqlserverDynamicTableFactory.java
Patch:
@@ -29,6 +29,8 @@
 /**
  * Company：www.dtstack.com
  *
+ * <p>** sql task currently only supports Microsoft driver **
+ *
  * @author shitou
  * @date 2021/5/21 17:39
  */

File: flinkx-clients/src/main/java/com/dtstack/flinkx/client/KerberosInfo.java
Patch:
@@ -17,12 +17,13 @@
  */
 package com.dtstack.flinkx.client;
 
+import com.dtstack.flinkx.security.KerberosUtils;
+import com.dtstack.flinkx.util.ExceptionUtil;
+
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.configuration.SecurityOptions;
 import org.apache.flink.runtime.util.HadoopUtils;
 
-import com.dtstack.flinkx.security.KerberosUtils;
-import com.dtstack.flinkx.util.ExceptionUtil;
 import org.apache.commons.lang3.StringUtils;
 import org.apache.hadoop.security.SecurityUtil;
 import org.apache.hadoop.security.UserGroupInformation;

File: flinkx-clients/src/main/java/com/dtstack/flinkx/client/constants/ConfigConstant.java
Patch:
@@ -17,16 +17,14 @@
  */
 package com.dtstack.flinkx.client.constants;
 
-
 /**
  * @program: flinkx
  * @author: xiuzhu
  * @create: 2021/06/13
  */
 public class ConfigConstant {
 
-    public final static String KUBERNETES_HOST_ALIASES_KEY = "kubernetes.host.aliases";
+    public static final String KUBERNETES_HOST_ALIASES_KEY = "kubernetes.host.aliases";
 
     public static final String KUBERNETES_HOST_ALIASES_ENV = "KUBERNETES_HOST_ALIASES";
-
 }

File: flinkx-clients/src/main/java/com/dtstack/flinkx/client/local/LocalClusterClientHelper.java
Patch:
@@ -19,7 +19,6 @@
 
 import com.dtstack.flinkx.Main;
 import com.dtstack.flinkx.client.ClusterClientHelper;
-
 import com.dtstack.flinkx.client.JobDeployer;
 
 import org.apache.flink.client.program.ClusterClient;

File: flinkx-connectors/flinkx-connector-clickhouse/src/main/java/com/dtstack/flinkx/connector/clickhouse/sink/ClickhouseOutputFormat.java
Patch:
@@ -32,7 +32,7 @@
 public class ClickhouseOutputFormat extends JdbcOutputFormat {
 
     @Override
-    protected Connection getConnection() throws SQLException{
+    protected Connection getConnection() throws SQLException {
         return ClickhouseUtil.getConnection(jdbcConf.getJdbcUrl(), jdbcConf.getUsername(), jdbcConf.getPassword());
     }
 }

File: flinkx-connectors/flinkx-connector-clickhouse/src/main/java/com/dtstack/flinkx/connector/clickhouse/source/ClickhouseInputFormat.java
Patch:
@@ -32,7 +32,7 @@
 public class ClickhouseInputFormat extends JdbcInputFormat {
 
     @Override
-    protected Connection getConnection() throws SQLException{
+    protected Connection getConnection() throws SQLException {
         return ClickhouseUtil.getConnection(jdbcConf.getJdbcUrl(), jdbcConf.getUsername(), jdbcConf.getPassword());
     }
 }

File: flinkx-connectors/flinkx-connector-clickhouse/src/main/java/com/dtstack/flinkx/connector/clickhouse/util/ClickhouseUtil.java
Patch:
@@ -19,6 +19,7 @@
 package com.dtstack.flinkx.connector.clickhouse.util;
 
 import com.dtstack.flinkx.util.SysUtil;
+
 import ru.yandex.clickhouse.BalancedClickhouseDataSource;
 import ru.yandex.clickhouse.settings.ClickHouseQueryParam;
 

File: flinkx-connectors/flinkx-connector-file/src/main/java/com/dtstack/flinkx/connector/file/converter/FileRowConverter.java
Patch:
@@ -19,7 +19,6 @@
 package com.dtstack.flinkx.connector.file.converter;
 
 import com.dtstack.flinkx.converter.AbstractRowConverter;
-
 import com.dtstack.flinkx.throwable.FlinkxRuntimeException;
 
 import org.apache.flink.api.common.serialization.DeserializationSchema;

File: flinkx-connectors/flinkx-connector-file/src/main/java/com/dtstack/flinkx/connector/file/source/FileInputBufferedReader.java
Patch:
@@ -27,7 +27,6 @@
 import java.io.InputStreamReader;
 import java.util.Iterator;
 import java.util.List;
-import java.util.stream.Stream;
 
 /**
  * @program: flinkx

File: flinkx-connectors/flinkx-connector-file/src/main/java/com/dtstack/flinkx/connector/file/source/FileInputFormat.java
Patch:
@@ -18,14 +18,14 @@
 
 package com.dtstack.flinkx.connector.file.source;
 
-import org.apache.flink.core.io.InputSplit;
-import org.apache.flink.table.data.RowData;
-
 import com.dtstack.flinkx.conf.BaseFileConf;
 import com.dtstack.flinkx.source.format.BaseRichInputFormat;
 import com.dtstack.flinkx.throwable.ReadRecordException;
 import com.dtstack.flinkx.util.GsonUtil;
 
+import org.apache.flink.core.io.InputSplit;
+import org.apache.flink.table.data.RowData;
+
 import java.io.File;
 import java.io.IOException;
 import java.util.ArrayList;

File: flinkx-connectors/flinkx-connector-file/src/main/java/com/dtstack/flinkx/connector/file/source/FileInputFormatBuilder.java
Patch:
@@ -20,6 +20,7 @@
 
 import com.dtstack.flinkx.conf.BaseFileConf;
 import com.dtstack.flinkx.source.format.BaseRichInputFormatBuilder;
+
 import org.apache.commons.lang3.StringUtils;
 
 /**

File: flinkx-connectors/flinkx-connector-file/src/main/java/com/dtstack/flinkx/connector/file/table/FileDynamicTableFactory.java
Patch:
@@ -20,7 +20,6 @@
 
 import com.dtstack.flinkx.conf.BaseFileConf;
 import com.dtstack.flinkx.connector.file.options.FileOptions;
-
 import com.dtstack.flinkx.connector.file.source.FileDynamicTableSource;
 
 import org.apache.flink.api.common.serialization.DeserializationSchema;

File: flinkx-connectors/flinkx-connector-ftp/src/main/java/com/dtstack/flinkx/connector/ftp/conf/ConfigConstants.java
Patch:
@@ -20,8 +20,9 @@
 
 /**
  * The class containing Ftp configuration constants
- *
+ * <p>
  * Company: www.dtstack.com
+ *
  * @author huyifan.zju@163.com
  */
 public class ConfigConstants {
@@ -32,7 +33,7 @@ public class ConfigConstants {
 
     public static final int DEFAULT_TIMEOUT = 5000;
 
-    public static final String  DEFAULT_FTP_CONNECT_PATTERN = "PASV";
+    public static final String DEFAULT_FTP_CONNECT_PATTERN = "PASV";
 
     public static final String SFTP_PROTOCOL = "sftp";
 

File: flinkx-connectors/flinkx-connector-ftp/src/main/java/com/dtstack/flinkx/connector/ftp/converter/FtpColumnConverter.java
Patch:
@@ -18,9 +18,6 @@
 
 package com.dtstack.flinkx.connector.ftp.converter;
 
-import org.apache.flink.table.data.GenericRowData;
-import org.apache.flink.table.data.RowData;
-
 import com.dtstack.flinkx.conf.FieldConf;
 import com.dtstack.flinkx.connector.ftp.conf.FtpConfig;
 import com.dtstack.flinkx.converter.AbstractRowConverter;
@@ -35,6 +32,9 @@
 import com.dtstack.flinkx.throwable.UnsupportedTypeException;
 import com.dtstack.flinkx.util.DateUtil;
 
+import org.apache.flink.table.data.GenericRowData;
+import org.apache.flink.table.data.RowData;
+
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Locale;

File: flinkx-connectors/flinkx-connector-ftp/src/main/java/com/dtstack/flinkx/connector/ftp/enums/EFtpMode.java
Patch:
@@ -32,5 +32,5 @@ public enum EFtpMode {
     /**
      * 主动方式
      */
-    PORT;
+    PORT
 }

File: flinkx-connectors/flinkx-connector-ftp/src/main/java/com/dtstack/flinkx/connector/ftp/handler/FtpHandlerFactory.java
Patch:
@@ -26,11 +26,11 @@
  */
 public class FtpHandlerFactory {
 
-    public static IFtpHandler createFtpHandler(String protocolStr){
+    public static IFtpHandler createFtpHandler(String protocolStr) {
         IFtpHandler ftpHandler;
 
         Protocol protocol = Protocol.getByName(protocolStr);
-        if(Protocol.SFTP.equals(protocol)) {
+        if (Protocol.SFTP.equals(protocol)) {
             ftpHandler = new SftpHandler();
         } else {
             ftpHandler = new FtpHandler();
@@ -39,7 +39,7 @@ public static IFtpHandler createFtpHandler(String protocolStr){
         return ftpHandler;
     }
 
-    enum Protocol{
+    enum Protocol {
         FTP, SFTP;
 
         public static Protocol getByName(String name) {

File: flinkx-connectors/flinkx-connector-ftp/src/main/java/com/dtstack/flinkx/connector/ftp/sink/FtpOutputFormatBuilder.java
Patch:
@@ -21,6 +21,7 @@
 import com.dtstack.flinkx.connector.ftp.conf.FtpConfig;
 import com.dtstack.flinkx.sink.format.FileOutputFormatBuilder;
 import com.dtstack.flinkx.throwable.FlinkxRuntimeException;
+
 import org.apache.commons.lang.StringUtils;
 
 /**

File: flinkx-connectors/flinkx-connector-ftp/src/main/java/com/dtstack/flinkx/connector/ftp/sink/FtpSinkFactory.java
Patch:
@@ -25,13 +25,12 @@
 import com.dtstack.flinkx.connector.ftp.converter.FtpRawTypeConverter;
 import com.dtstack.flinkx.converter.RawTypeConverter;
 import com.dtstack.flinkx.sink.SinkFactory;
-
 import com.dtstack.flinkx.util.JsonUtil;
+import com.dtstack.flinkx.util.StringUtil;
 
 import org.apache.flink.streaming.api.datastream.DataStream;
 import org.apache.flink.streaming.api.datastream.DataStreamSink;
 import org.apache.flink.table.data.RowData;
-import com.dtstack.flinkx.util.StringUtil;
 
 import java.util.List;
 
@@ -55,7 +54,7 @@ public FtpSinkFactory(SyncConf syncConf) {
             ftpConfig.setDefaultPort();
         }
 
-        if(!ConfigConstants.DEFAULT_FIELD_DELIMITER.equals(ftpConfig.getFieldDelimiter())){
+        if (!ConfigConstants.DEFAULT_FIELD_DELIMITER.equals(ftpConfig.getFieldDelimiter())) {
             String fieldDelimiter = StringUtil.convertRegularExpr(ftpConfig.getFieldDelimiter());
             ftpConfig.setFieldDelimiter(fieldDelimiter);
         }

File: flinkx-connectors/flinkx-connector-ftp/src/main/java/com/dtstack/flinkx/connector/ftp/source/FtpInputFormatBuilder.java
Patch:
@@ -21,6 +21,7 @@
 import com.dtstack.flinkx.connector.ftp.conf.FtpConfig;
 import com.dtstack.flinkx.source.format.BaseRichInputFormatBuilder;
 import com.dtstack.flinkx.throwable.FlinkxRuntimeException;
+
 import org.apache.commons.lang.StringUtils;
 
 /** @author jiangbo */

File: flinkx-connectors/flinkx-connector-ftp/src/main/java/com/dtstack/flinkx/connector/ftp/source/FtpInputSplit.java
Patch:
@@ -25,8 +25,9 @@
 
 /**
  * The Class describing each InputSplit of Ftp
- *
+ * <p>
  * Company: www.dtstack.com
+ *
  * @author huyifan.zju@163.com
  */
 public class FtpInputSplit implements InputSplit {

File: flinkx-connectors/flinkx-connector-ftp/src/main/java/com/dtstack/flinkx/connector/ftp/source/FtpSeqInputStream.java
Patch:
@@ -27,8 +27,9 @@
 
 /**
  * The InputStream Implementation that read multiple ftp files one by one.
- *
+ * <p>
  * Company: www.dtstack.com
+ *
  * @author huyifan.zju@163.com
  */
 public class FtpSeqInputStream extends InputStream {
@@ -58,8 +59,7 @@ final void nextStream() throws IOException {
             if (in == null) {
                 throw new NullPointerException();
             }
-        }
-        else {
+        } else {
             in = null;
         }
 

File: flinkx-connectors/flinkx-connector-greenplum/src/main/java/com/dtstack/flinkx/connector/greenplum/converter/GreenplumRawTypeConverter.java
Patch:
@@ -18,11 +18,11 @@
 
 package com.dtstack.flinkx.connector.greenplum.converter;
 
+import com.dtstack.flinkx.throwable.UnsupportedTypeException;
+
 import org.apache.flink.table.api.DataTypes;
 import org.apache.flink.table.types.DataType;
 
-import com.dtstack.flinkx.throwable.UnsupportedTypeException;
-
 import java.util.Locale;
 
 /**

File: flinkx-connectors/flinkx-connector-greenplum/src/main/java/com/dtstack/flinkx/connector/greenplum/dialect/GreenplumDialect.java
Patch:
@@ -67,9 +67,7 @@ public Optional<String> getUpsertStatement(
 
     @Override
     public Optional<String> getReplaceStatement(
-            String schema,
-            String tableName,
-            String[] fieldNames) {
+            String schema, String tableName, String[] fieldNames) {
         throw new RuntimeException("Greenplum does not support replace sql");
     }
 }

File: flinkx-connectors/flinkx-connector-greenplum/src/main/java/com/dtstack/flinkx/connector/greenplum/source/GreenplumSourceFactory.java
Patch:
@@ -18,12 +18,12 @@
 
 package com.dtstack.flinkx.connector.greenplum.source;
 
-import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
-
 import com.dtstack.flinkx.conf.SyncConf;
 import com.dtstack.flinkx.connector.greenplum.dialect.GreenplumDialect;
 import com.dtstack.flinkx.connector.jdbc.source.JdbcSourceFactory;
 
+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
+
 /**
  * company www.dtstack.com
  *
@@ -34,5 +34,4 @@ public class GreenplumSourceFactory extends JdbcSourceFactory {
     public GreenplumSourceFactory(SyncConf syncConf, StreamExecutionEnvironment env) {
         super(syncConf, env, new GreenplumDialect());
     }
-
 }

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/conf/ConnectionConf.java
Patch:
@@ -21,8 +21,7 @@
 import java.util.List;
 
 /**
- * Date: 2021/04/12
- * Company: www.dtstack.com
+ * Date: 2021/04/12 Company: www.dtstack.com
  *
  * @author tudou
  */
@@ -36,12 +35,14 @@ public abstract class ConnectionConf implements Serializable {
 
     /**
      * 获取JDBC URL连接
+     *
      * @return
      */
     public abstract String obtainJdbcUrl();
 
     /**
      * 设置JDBC URL连接
+     *
      * @param jdbcUrl JDBC URL连接
      */
     public abstract void putJdbcUrl(String jdbcUrl);

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/conf/JdbcLookupConf.java
Patch:
@@ -26,7 +26,7 @@
  * @author chuixue
  * @create 2021-04-10 22:10
  * @description
- **/
+ */
 public class JdbcLookupConf extends LookupConf {
     /** vertx pool size */
     protected int asyncPoolSize = 5;
@@ -36,6 +36,7 @@ public Map<String, Object> getDruidConf() {
     }
 
     protected Map<String, Object> druidConf;
+
     public int getAsyncPoolSize() {
         return asyncPoolSize;
     }

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/options/JdbcCommonOptions.java
Patch:
@@ -25,7 +25,7 @@
  * @author chuixue
  * @create 2021-04-10 16:22
  * @description
- **/
+ */
 public class JdbcCommonOptions {
     public static final ConfigOption<String> URL =
             ConfigOptions.key("url")

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/options/JdbcLookupOptions.java
Patch:
@@ -18,11 +18,11 @@
 
 package com.dtstack.flinkx.connector.jdbc.options;
 
+import com.dtstack.flinkx.lookup.options.LookupOptions;
+
 import org.apache.flink.configuration.ConfigOption;
 import org.apache.flink.configuration.ConfigOptions;
 
-import com.dtstack.flinkx.lookup.options.LookupOptions;
-
 import java.util.LinkedHashMap;
 import java.util.Map;
 

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/sink/JdbcOutputFormatBuilder.java
Patch:
@@ -21,6 +21,7 @@
 import com.dtstack.flinkx.connector.jdbc.dialect.JdbcDialect;
 import com.dtstack.flinkx.converter.AbstractRowConverter;
 import com.dtstack.flinkx.sink.format.BaseRichOutputFormatBuilder;
+
 import org.apache.commons.lang.StringUtils;
 
 /** @author sishu.yss @Company: www.dtstack.com */

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/source/JdbcInputFormatBuilder.java
Patch:
@@ -24,6 +24,7 @@
 import com.dtstack.flinkx.constants.ConstantValue;
 import com.dtstack.flinkx.enums.ColumnType;
 import com.dtstack.flinkx.source.format.BaseRichInputFormatBuilder;
+
 import org.apache.commons.lang.StringUtils;
 
 import java.util.Arrays;

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/statement/FieldNamedPreparedStatementImpl.java
Patch:
@@ -181,7 +181,6 @@ public void setObject(int fieldIndex, Object x) throws SQLException {
         }
     }
 
-
     @Override
     public void setBlob(int fieldIndex, InputStream is) throws SQLException {
         for (int index : indexMapping[fieldIndex]) {
@@ -192,7 +191,7 @@ public void setBlob(int fieldIndex, InputStream is) throws SQLException {
     @Override
     public void setClob(int fieldIndex, Reader reader) throws SQLException {
         for (int index : indexMapping[fieldIndex]) {
-            statement.setClob(index,reader);
+            statement.setClob(index, reader);
         }
     }
 
@@ -236,7 +235,6 @@ public static FieldNamedPreparedStatement prepareStatement(
      *
      * @param sql sql to parse
      * @param paramMap map to hold parameter-index mappings
-     *
      * @return the parsed sql
      */
     public static String parseNamedStatement(String sql, Map<String, List<Integer>> paramMap) {

File: flinkx-connectors/flinkx-connector-phoenix5/src/main/java/com/dtstack/flinkx/connector/phoenix5/converter/HBaseRawTypeConverter.java
Patch:
@@ -17,11 +17,11 @@
  */
 package com.dtstack.flinkx.connector.phoenix5.converter;
 
+import com.dtstack.flinkx.throwable.UnsupportedTypeException;
+
 import org.apache.flink.table.api.DataTypes;
 import org.apache.flink.table.types.DataType;
 
-import com.dtstack.flinkx.throwable.UnsupportedTypeException;
-
 import java.util.Locale;
 
 /**

File: flinkx-connectors/flinkx-connector-phoenix5/src/main/java/com/dtstack/flinkx/connector/phoenix5/sink/Phoenix5OutputFormat.java
Patch:
@@ -18,15 +18,16 @@
 
 package com.dtstack.flinkx.connector.phoenix5.sink;
 
-import org.apache.flink.table.types.logical.RowType;
-
 import com.dtstack.flinkx.connector.jdbc.sink.JdbcOutputFormat;
 import com.dtstack.flinkx.connector.jdbc.statement.FieldNamedPreparedStatement;
 import com.dtstack.flinkx.connector.jdbc.util.JdbcUtil;
 import com.dtstack.flinkx.connector.phoenix5.converter.Phoenix5RawTypeConverter;
 import com.dtstack.flinkx.connector.phoenix5.util.Phoenix5Util;
 import com.dtstack.flinkx.enums.EWriteMode;
 import com.dtstack.flinkx.util.TableUtil;
+
+import org.apache.flink.table.types.logical.RowType;
+
 import org.apache.commons.lang3.tuple.Pair;
 
 import java.sql.Connection;

File: flinkx-connectors/flinkx-connector-phoenix5/src/main/java/com/dtstack/flinkx/connector/phoenix5/sink/Phoenix5OutputFormatBuilder.java
Patch:
@@ -20,6 +20,7 @@
 import com.dtstack.flinkx.connector.jdbc.conf.JdbcConf;
 import com.dtstack.flinkx.connector.jdbc.sink.JdbcOutputFormat;
 import com.dtstack.flinkx.connector.jdbc.sink.JdbcOutputFormatBuilder;
+
 import org.apache.commons.lang.StringUtils;
 
 /**

File: flinkx-connectors/flinkx-connector-phoenix5/src/main/java/com/dtstack/flinkx/connector/phoenix5/source/HBaseInputFormatBuilder.java
Patch:
@@ -21,6 +21,7 @@
 import com.dtstack.flinkx.connector.phoenix5.conf.Phoenix5Conf;
 import com.dtstack.flinkx.constants.ConstantValue;
 import com.dtstack.flinkx.source.format.BaseRichInputFormatBuilder;
+
 import org.apache.commons.lang.StringUtils;
 
 /**

File: flinkx-connectors/flinkx-connector-phoenix5/src/main/java/com/dtstack/flinkx/connector/phoenix5/source/Phoenix5InputFormat.java
Patch:
@@ -18,10 +18,11 @@
 
 package com.dtstack.flinkx.connector.phoenix5.source;
 
-import org.apache.flink.core.io.InputSplit;
-
 import com.dtstack.flinkx.connector.jdbc.source.JdbcInputFormat;
 import com.dtstack.flinkx.connector.phoenix5.util.Phoenix5Util;
+
+import org.apache.flink.core.io.InputSplit;
+
 import org.apache.commons.lang3.tuple.Pair;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;

File: flinkx-connectors/flinkx-connector-phoenix5/src/main/java/com/dtstack/flinkx/connector/phoenix5/source/Phoenix5InputFormatBuilder.java
Patch:
@@ -22,6 +22,7 @@
 import com.dtstack.flinkx.connector.jdbc.source.JdbcInputFormat;
 import com.dtstack.flinkx.connector.jdbc.source.JdbcInputFormatBuilder;
 import com.dtstack.flinkx.constants.ConstantValue;
+
 import org.apache.commons.lang.StringUtils;
 
 /**

File: flinkx-connectors/flinkx-connector-phoenix5/src/main/java/com/dtstack/flinkx/connector/phoenix5/source/Phoenix5InputSplit.java
Patch:
@@ -17,10 +17,10 @@
  */
 package com.dtstack.flinkx.connector.phoenix5.source;
 
-import org.apache.commons.lang3.tuple.Pair;
-
 import org.apache.flink.core.io.GenericInputSplit;
 
+import org.apache.commons.lang3.tuple.Pair;
+
 import java.util.Vector;
 
 /**

File: flinkx-connectors/flinkx-connector-phoenix5/src/main/java/com/dtstack/flinkx/connector/phoenix5/table/Phoenix5DynamicTableFactory.java
Patch:
@@ -18,9 +18,6 @@
 
 package com.dtstack.flinkx.connector.phoenix5.table;
 
-import org.apache.flink.configuration.ConfigOption;
-import org.apache.flink.table.connector.source.DynamicTableSource;
-
 import com.dtstack.flinkx.connector.jdbc.dialect.JdbcDialect;
 import com.dtstack.flinkx.connector.jdbc.sink.JdbcOutputFormatBuilder;
 import com.dtstack.flinkx.connector.jdbc.source.JdbcInputFormatBuilder;
@@ -31,6 +28,9 @@
 import com.dtstack.flinkx.connector.phoenix5.source.Phoenix5InputFormat;
 import com.dtstack.flinkx.connector.phoenix5.source.Phoenix5InputFormatBuilder;
 
+import org.apache.flink.configuration.ConfigOption;
+import org.apache.flink.table.connector.source.DynamicTableSource;
+
 import java.util.Set;
 
 import static com.dtstack.flinkx.connector.phoenix5.conf.Phoenix5Options.READ_FROM_HBASE;

File: flinkx-connectors/flinkx-connector-phoenix5/src/main/java/com/dtstack/flinkx/connector/phoenix5/util/Phoenix5Util.java
Patch:
@@ -17,14 +17,15 @@
  */
 package com.dtstack.flinkx.connector.phoenix5.util;
 
-import org.apache.flink.util.Preconditions;
-
 import com.dtstack.flinkx.conf.FieldConf;
 import com.dtstack.flinkx.connector.jdbc.conf.JdbcConf;
 import com.dtstack.flinkx.constants.ConstantValue;
 import com.dtstack.flinkx.throwable.FlinkxRuntimeException;
 import com.dtstack.flinkx.util.ClassUtil;
 import com.dtstack.flinkx.util.TelnetUtil;
+
+import org.apache.flink.util.Preconditions;
+
 import org.apache.commons.lang3.StringUtils;
 import org.apache.commons.lang3.tuple.Pair;
 import org.apache.phoenix.query.QueryServices;

File: flinkx-connectors/flinkx-connector-saphana/src/main/java/com/dtstack/flinkx/connector/saphana/source/SaphanaInputFormat.java
Patch:
@@ -18,10 +18,10 @@
 
 package com.dtstack.flinkx.connector.saphana.source;
 
-import org.apache.flink.core.io.InputSplit;
-
 import com.dtstack.flinkx.connector.jdbc.source.JdbcInputFormat;
 
+import org.apache.flink.core.io.InputSplit;
+
 import java.util.Properties;
 
 /**

File: flinkx-connectors/flinkx-connector-saphana/src/main/java/com/dtstack/flinkx/connector/saphana/source/SaphanaSourceFactory.java
Patch:
@@ -16,16 +16,15 @@
  * limitations under the License.
  */
 
-
 package com.dtstack.flinkx.connector.saphana.source;
 
-import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
-
 import com.dtstack.flinkx.conf.SyncConf;
 import com.dtstack.flinkx.connector.jdbc.source.JdbcInputFormatBuilder;
 import com.dtstack.flinkx.connector.jdbc.source.JdbcSourceFactory;
 import com.dtstack.flinkx.connector.saphana.dialect.SaphanaDialect;
 
+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
+
 /**
  * company www.dtstack.com
  *

File: flinkx-connectors/flinkx-connector-stream/src/main/java/com/dtstack/flinkx/connector/stream/converter/StreamColumnConverter.java
Patch:
@@ -17,8 +17,6 @@
  */
 package com.dtstack.flinkx.connector.stream.converter;
 
-import org.apache.flink.table.data.RowData;
-
 import com.dtstack.flinkx.conf.FieldConf;
 import com.dtstack.flinkx.conf.FlinkxCommonConf;
 import com.dtstack.flinkx.converter.AbstractRowConverter;
@@ -31,6 +29,9 @@
 import com.dtstack.flinkx.element.column.ByteColumn;
 import com.dtstack.flinkx.element.column.StringColumn;
 import com.dtstack.flinkx.element.column.TimestampColumn;
+
+import org.apache.flink.table.data.RowData;
+
 import com.github.jsonzou.jmockdata.JMockData;
 
 import java.math.BigDecimal;

File: flinkx-connectors/flinkx-connector-stream/src/main/java/com/dtstack/flinkx/connector/stream/converter/StreamRawTypeConverter.java
Patch:
@@ -18,11 +18,11 @@
 
 package com.dtstack.flinkx.connector.stream.converter;
 
+import com.dtstack.flinkx.throwable.UnsupportedTypeException;
+
 import org.apache.flink.table.api.DataTypes;
 import org.apache.flink.table.types.DataType;
 
-import com.dtstack.flinkx.throwable.UnsupportedTypeException;
-
 import java.util.Locale;
 
 /**

File: flinkx-connectors/flinkx-connector-stream/src/main/java/com/dtstack/flinkx/connector/stream/source/StreamInputFormatBuilder.java
Patch:
@@ -20,6 +20,7 @@
 
 import com.dtstack.flinkx.connector.stream.conf.StreamConf;
 import com.dtstack.flinkx.source.format.BaseRichInputFormatBuilder;
+
 import org.apache.commons.collections.CollectionUtils;
 
 /**

File: flinkx-core/src/main/java/com/dtstack/flinkx/classloader/ClassLoaderSupplierCallBack.java
Patch:
@@ -16,7 +16,6 @@
  * limitations under the License.
  */
 
-
 package com.dtstack.flinkx.classloader;
 
 /**
@@ -26,7 +25,8 @@
  */
 public class ClassLoaderSupplierCallBack {
 
-    public static <R> R callbackAndReset(ClassLoaderSupplier<R> supplier, ClassLoader toSetClassLoader) throws Exception {
+    public static <R> R callbackAndReset(
+            ClassLoaderSupplier<R> supplier, ClassLoader toSetClassLoader) throws Exception {
         ClassLoader oldClassLoader = Thread.currentThread().getContextClassLoader();
         Thread.currentThread().setContextClassLoader(toSetClassLoader);
         try {

File: flinkx-core/src/main/java/com/dtstack/flinkx/constants/ConstantValue.java
Patch:
@@ -16,7 +16,6 @@
  * limitations under the License.
  */
 
-
 package com.dtstack.flinkx.constants;
 
 /**
@@ -41,10 +40,8 @@ public class ConstantValue {
     public static final String LEFT_PARENTHESIS_SYMBOL = "(";
     public static final String RIGHT_PARENTHESIS_SYMBOL = ")";
 
-
     public static final String DATA_TYPE_UNSIGNED = "UNSIGNED";
 
-
     public static final String KEY_HTTP = "http";
 
     public static final String PROTOCOL_HTTP = "http://";

File: flinkx-core/src/main/java/com/dtstack/flinkx/converter/IDeserializationConverter.java
Patch:
@@ -25,6 +25,7 @@ public interface IDeserializationConverter<T, E> extends Serializable {
 
     /**
      * Runtime converter to convert field to {@link RowData} type object
+     *
      * @param field
      * @return
      * @throws Exception

File: flinkx-core/src/main/java/com/dtstack/flinkx/converter/ISerializationConverter.java
Patch:
@@ -22,15 +22,15 @@
 import java.io.Serializable;
 
 /**
- * Date: 2021/04/30
- * Company: www.dtstack.com
+ * Date: 2021/04/30 Company: www.dtstack.com
  *
  * @author tudou
  */
 public interface ISerializationConverter<T> extends Serializable {
 
     /**
      * 类型T一般是 Object，HBase这种特殊的就是byte[]
+     *
      * @param rowData
      * @param pos
      * @param output

File: flinkx-core/src/main/java/com/dtstack/flinkx/converter/RawTypeConvertible.java
Patch:
@@ -18,8 +18,6 @@
 
 package com.dtstack.flinkx.converter;
 
-import com.dtstack.flinkx.converter.RawTypeConverter;
-
 /**
  * The class implement this will be convert Raw Type to Flink Type. Implementations are
  * SourceFactory、SinkFactory、InputFormat、OutputFormat. When Flink running, Input/OutputFormat

File: flinkx-core/src/main/java/com/dtstack/flinkx/decoder/IDecode.java
Patch:
@@ -20,8 +20,7 @@
 import java.util.Map;
 
 /**
- * Date: 2019/11/21
- * Company: www.dtstack.com
+ * Date: 2019/11/21 Company: www.dtstack.com
  *
  * @author tudou
  */
@@ -34,5 +33,4 @@ public interface IDecode {
      * @return 解码后的数据
      */
     Map<String, Object> decode(String message);
-
 }

File: flinkx-core/src/main/java/com/dtstack/flinkx/decoder/JsonDecoder.java
Patch:
@@ -18,6 +18,7 @@
 package com.dtstack.flinkx.decoder;
 
 import com.dtstack.flinkx.util.JsonUtil;
+
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -26,8 +27,7 @@
 import java.util.Map;
 
 /**
- * Date: 2019/11/21
- * Company: www.dtstack.com
+ * Date: 2019/11/21 Company: www.dtstack.com
  *
  * @author tudou
  */

File: flinkx-core/src/main/java/com/dtstack/flinkx/decoder/TextDecoder.java
Patch:
@@ -22,8 +22,7 @@
 import java.util.Map;
 
 /**
- * Date: 2019/11/21
- * Company: www.dtstack.com
+ * Date: 2019/11/21 Company: www.dtstack.com
  *
  * @author tudou
  */
@@ -35,5 +34,4 @@ public class TextDecoder implements IDecode, Serializable {
     public Map<String, Object> decode(final String message) {
         return Collections.singletonMap("message", message);
     }
-
 }

File: flinkx-core/src/main/java/com/dtstack/flinkx/element/column/BigDecimalColumn.java
Patch:
@@ -26,8 +26,7 @@
 import java.util.Date;
 
 /**
- * Date: 2021/04/26
- * Company: www.dtstack.com
+ * Date: 2021/04/26 Company: www.dtstack.com
  *
  * @author tudou
  */
@@ -105,7 +104,7 @@ public BigDecimal asBigDecimal() {
         if (null == data) {
             return null;
         }
-        return (BigDecimal)data;
+        return (BigDecimal) data;
     }
 
     @Override

File: flinkx-core/src/main/java/com/dtstack/flinkx/element/column/BooleanColumn.java
Patch:
@@ -24,8 +24,7 @@
 import java.sql.Timestamp;
 
 /**
- * Date: 2021/04/27
- * Company: www.dtstack.com
+ * Date: 2021/04/27 Company: www.dtstack.com
  *
  * @author tudou
  */

File: flinkx-core/src/main/java/com/dtstack/flinkx/element/column/MapColumn.java
Patch:
@@ -28,8 +28,7 @@
 import java.util.Map;
 
 /**
- * Date: 2021/04/26
- * Company: www.dtstack.com
+ * Date: 2021/04/26 Company: www.dtstack.com
  *
  * @author tudou
  */

File: flinkx-core/src/main/java/com/dtstack/flinkx/element/column/NullColumn.java
Patch:
@@ -24,8 +24,7 @@
 import java.util.Date;
 
 /**
- * Date: 2021/04/26
- * Company: www.dtstack.com
+ * Date: 2021/04/26 Company: www.dtstack.com
  *
  * @author tudou
  */

File: flinkx-core/src/main/java/com/dtstack/flinkx/element/column/StringColumn.java
Patch:
@@ -20,6 +20,7 @@
 import com.dtstack.flinkx.element.AbstractBaseColumn;
 import com.dtstack.flinkx.throwable.CastException;
 import com.dtstack.flinkx.util.DateUtil;
+
 import org.apache.commons.lang3.math.NumberUtils;
 
 import java.math.BigDecimal;
@@ -71,7 +72,7 @@ public Date asDate() {
             // 如果string是时间戳
             time = NumberUtils.createLong(data);
         } catch (Exception ignored) {
-            //doNothing
+            // doNothing
         }
         SimpleDateFormat formatter = DateUtil.buildDateFormatter(format);
         if (time != null) {

File: flinkx-core/src/main/java/com/dtstack/flinkx/element/column/TimestampColumn.java
Patch:
@@ -25,8 +25,7 @@
 import java.util.Date;
 
 /**
- * Date: 2021/04/27
- * Company: www.dtstack.com
+ * Date: 2021/04/27 Company: www.dtstack.com
  *
  * @author tudou
  */

File: flinkx-core/src/main/java/com/dtstack/flinkx/enums/OperatorType.java
Patch:
@@ -18,12 +18,11 @@
 package com.dtstack.flinkx.enums;
 
 /**
- * Date: 2021/04/07
- * Company: www.dtstack.com
+ * Date: 2021/04/07 Company: www.dtstack.com
  *
  * @author tudou
  */
-public enum  OperatorType {
+public enum OperatorType {
     source,
     sink,
     metric

File: flinkx-core/src/main/java/com/dtstack/flinkx/lookup/options/LookupOptions.java
Patch:
@@ -27,7 +27,7 @@
  * @author chuixue
  * @create 2021-04-10 16:10
  * @description lookup common config
- **/
+ */
 public class LookupOptions {
     // look up config options
     public static final ConfigOption<Long> LOOKUP_CACHE_PERIOD =

File: flinkx-core/src/main/java/com/dtstack/flinkx/metrics/SimpleAccumulatorGauge.java
Patch:
@@ -26,8 +26,7 @@
 /**
  * company: www.dtstack.com
  *
- * @author: toutian
- * create: 2019/3/21
+ * @author: toutian create: 2019/3/21
  */
 public class SimpleAccumulatorGauge<T extends Serializable> implements Gauge<T> {
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/metrics/StringAccumulator.java
Patch:
@@ -25,7 +25,7 @@
  * @explanation
  * @date 2018/12/21
  */
-public class StringAccumulator implements Accumulator<String,String> {
+public class StringAccumulator implements Accumulator<String, String> {
 
     private String localValue;
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/metrics/ValueAccumulator.java
Patch:
@@ -20,8 +20,7 @@
 import org.apache.flink.api.common.accumulators.LongCounter;
 
 /**
- * Date: 2021/05/18
- * Company: www.dtstack.com
+ * Date: 2021/05/18 Company: www.dtstack.com
  *
  * @author tudou
  */

File: flinkx-core/src/main/java/com/dtstack/flinkx/security/KerberosConfig.java
Patch:
@@ -19,6 +19,7 @@
 package com.dtstack.flinkx.security;
 
 import com.dtstack.flinkx.throwable.FlinkxRuntimeException;
+
 import com.google.common.base.Strings;
 
 import java.io.Serializable;

File: flinkx-core/src/main/java/com/dtstack/flinkx/security/KerberosUtil.java
Patch:
@@ -22,6 +22,7 @@
 import com.dtstack.flinkx.throwable.FlinkxRuntimeException;
 import com.dtstack.flinkx.util.JsonUtil;
 import com.dtstack.flinkx.util.Md5Util;
+
 import org.apache.commons.collections.MapUtils;
 import org.apache.commons.lang.StringUtils;
 import org.apache.flink.api.common.cache.DistributedCache;

File: flinkx-core/src/main/java/com/dtstack/flinkx/security/SftpHandler.java
Patch:
@@ -19,6 +19,7 @@
 package com.dtstack.flinkx.security;
 
 import com.dtstack.flinkx.util.RetryUtil;
+
 import com.jcraft.jsch.ChannelSftp;
 import com.jcraft.jsch.JSch;
 import com.jcraft.jsch.Session;

File: flinkx-core/src/main/java/com/dtstack/flinkx/sink/WriteErrorTypes.java
Patch:
@@ -20,17 +20,17 @@
 /**
  * This Class defined several types of errors when writing record
  *
- * Company: www.dtstack.com
+ * <p>Company: www.dtstack.com
+ *
  * @author huyifan.zju@163.com
  */
 public class WriteErrorTypes {
 
-    public static final String ERR_NULL_POINTER= "npe";
+    public static final String ERR_NULL_POINTER = "npe";
 
     public static final String ERR_PRIMARY_CONFLICT = "duplicate";
 
     public static final String ERR_FORMAT_TRANSFORM = "conversion";
 
     public static final String ERR_OTHERS = "other";
-
 }

File: flinkx-core/src/main/java/com/dtstack/flinkx/sink/format/BaseRichOutputFormatBuilder.java
Patch:
@@ -21,6 +21,7 @@
 import com.dtstack.flinkx.conf.FlinkxCommonConf;
 import com.dtstack.flinkx.constants.ConstantValue;
 import com.dtstack.flinkx.converter.AbstractRowConverter;
+
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/sink/options/SinkOptions.java
Patch:
@@ -23,10 +23,11 @@
 
 /**
  * TODO move to table package
+ *
  * @author chuixue
  * @create 2021-06-21 19:41
  * @description
- **/
+ */
 public class SinkOptions {
     // write config options
     public static final ConfigOption<Integer> SINK_BUFFER_FLUSH_MAX_ROWS =

File: flinkx-core/src/main/java/com/dtstack/flinkx/source/format/BaseRichInputFormatBuilder.java
Patch:
@@ -20,6 +20,7 @@
 
 import com.dtstack.flinkx.conf.FlinkxCommonConf;
 import com.dtstack.flinkx.converter.AbstractRowConverter;
+
 import com.google.common.base.Preconditions;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;

File: flinkx-core/src/main/java/com/dtstack/flinkx/source/options/SourceOptions.java
Patch:
@@ -25,7 +25,7 @@
  * @author chuixue
  * @create 2021-04-10 16:19
  * @description
- **/
+ */
 public class SourceOptions {
     // read config options
     public static final ConfigOption<String> SCAN_RESTORE_COLUMNNAME =

File: flinkx-core/src/main/java/com/dtstack/flinkx/sql/FunctionManager.java
Patch:
@@ -18,13 +18,14 @@
 
 package com.dtstack.flinkx.sql;
 
+import com.dtstack.flinkx.throwable.FlinkxSqlParseException;
+
 import org.apache.flink.table.api.TableEnvironment;
 import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;
 import org.apache.flink.table.functions.AggregateFunction;
 import org.apache.flink.table.functions.ScalarFunction;
 import org.apache.flink.table.functions.TableFunction;
 
-import com.dtstack.flinkx.throwable.FlinkxSqlParseException;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/sql/parser/CreateFunctionStmtParser.java
Patch:
@@ -18,13 +18,13 @@
 
 package com.dtstack.flinkx.sql.parser;
 
-import org.apache.flink.table.api.StatementSet;
-import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;
-
 import com.dtstack.flinkx.classloader.ClassLoaderManager;
 import com.dtstack.flinkx.sql.FunctionManager;
 import com.dtstack.flinkx.throwable.FlinkxSqlParseException;
 
+import org.apache.flink.table.api.StatementSet;
+import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;
+
 import java.lang.reflect.InvocationTargetException;
 import java.net.URL;
 import java.net.URLClassLoader;

File: flinkx-core/src/main/java/com/dtstack/flinkx/table/connector/sink/ParallelOutputFormatProvider.java
Patch:
@@ -28,7 +28,7 @@
  * @program: luna-flink
  * @author: wuren
  * @create: 2021/04/02
- **/
+ */
 public interface ParallelOutputFormatProvider extends OutputFormatProvider {
 
     /** Helper method for creating a OutputFormat provider with a provided sink parallelism. */
@@ -45,5 +45,4 @@ public Optional<Integer> getParallelism() {
             }
         };
     }
-
 }

File: flinkx-core/src/main/java/com/dtstack/flinkx/table/connector/source/ParallelInputFormatProvider.java
Patch:
@@ -29,14 +29,14 @@
  * @program: flinkx
  * @author: wuren
  * @create: 2021/04/19
- **/
+ */
 public interface ParallelInputFormatProvider extends InputFormatProvider, ParallelismProvider {
 
     /** Helper method for creating a InputFormat provider with a provided source parallelism. */
     static InputFormatProvider of(InputFormat<RowData, ?> inputFormat, Integer parallelism) {
         return new ParallelInputFormatProvider() {
             @Override
-            public InputFormat<RowData, ?>  createInputFormat() {
+            public InputFormat<RowData, ?> createInputFormat() {
                 return inputFormat;
             }
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/table/connector/source/ParallelTableFunctionProvider.java
Patch:
@@ -29,9 +29,8 @@
  * @program: flinkx
  * @author: wuren
  * @create: 2021/04/19
- **/
-public interface ParallelTableFunctionProvider
-        extends TableFunctionProvider, ParallelismProvider {
+ */
+public interface ParallelTableFunctionProvider extends TableFunctionProvider, ParallelismProvider {
 
     /** Helper method for creating a TableFunction provider with a provided lookup parallelism. */
     static TableFunctionProvider of(TableFunction<RowData> tableFunction, Integer parallelism) {

File: flinkx-core/src/main/java/com/dtstack/flinkx/table/options/BaseFileOptions.java
Patch:
@@ -21,8 +21,7 @@
 import org.apache.flink.configuration.ConfigOptions;
 
 /**
- * Date: 2021/06/17
- * Company: www.dtstack.com
+ * Date: 2021/06/17 Company: www.dtstack.com
  *
  * @author tudou
  */

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/ExecuteProcessHelper.java
Patch:
@@ -18,12 +18,13 @@
 
 package com.dtstack.flinkx.util;
 
+import com.dtstack.flinkx.enums.ClusterMode;
+import com.dtstack.flinkx.enums.EPluginLoadMode;
+
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
 
 import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ObjectMapper;
 
-import com.dtstack.flinkx.enums.ClusterMode;
-import com.dtstack.flinkx.enums.EPluginLoadMode;
 import com.google.common.base.Strings;
 import com.google.common.collect.Lists;
 import org.apache.commons.io.Charsets;

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/FileSystemUtil.java
Patch:
@@ -19,6 +19,7 @@
 package com.dtstack.flinkx.util;
 
 import com.dtstack.flinkx.security.KerberosUtil;
+
 import org.apache.commons.collections.MapUtils;
 import org.apache.commons.lang.StringUtils;
 import org.apache.flink.api.common.cache.DistributedCache;

File: flinkx-connectors/flinkx-connector-sqlserver/src/main/java/com/dtstack/flinkx/connector/sqlserver/converter/SqlserverJtdsRawTypeConverter.java
Patch:
@@ -19,6 +19,7 @@
 package com.dtstack.flinkx.connector.sqlserver.converter;
 
 import com.dtstack.flinkx.throwable.UnsupportedTypeException;
+
 import org.apache.flink.table.api.DataTypes;
 import org.apache.flink.table.types.DataType;
 
@@ -30,15 +31,13 @@
  * @author shitou
  * @date 2021/5/19 14:26
  */
-public class SqlserverRawTypeConverter {
+public class SqlserverJtdsRawTypeConverter {
 
     /**
      * Convert the data type in SqlServer to the DataType type in flink
      *
      * @param type
-     *
      * @return
-     *
      * @throws UnsupportedTypeException
      */
     public static DataType apply(String type) throws UnsupportedTypeException {

File: flinkx-connectors/flinkx-connector-sqlserver/src/main/java/com/dtstack/flinkx/connector/sqlserver/table/SqlserverDynamicTableFactory.java
Patch:
@@ -29,6 +29,8 @@
 /**
  * Company：www.dtstack.com
  *
+ * <p>** sql task currently only supports Microsoft driver **
+ *
  * @author shitou
  * @date 2021/5/21 17:39
  */

File: flinkx-core/src/main/java/com/dtstack/flinkx/enums/ColumnType.java
Patch:
@@ -54,7 +54,8 @@ public enum ColumnType {
      * date type
      */
     DATE, TIMESTAMP, TIME,
-    DECIMAL, YEAR, BIT;
+    DECIMAL, YEAR, BIT,
+    OBJECT;
 
     public static List<ColumnType> TIME_TYPE = Arrays.asList(
             DATE, DATETIME, TIME, TIMESTAMP

File: flinkx-connectors/flinkx-connector-elasticsearch6/src/main/java/com/dtstack/flinkx/connector/elasticsearch6/options/DtElasticsearch6Options.java
Patch:
@@ -22,5 +22,4 @@ public class DtElasticsearch6Options {
                     .intType()
                     .defaultValue(1)
                     .withDescription("Parallelism for connector running.");
-
 }

File: flinkx-connectors/flinkx-connector-elasticsearch6/src/main/java/com/dtstack/flinkx/connector/elasticsearch6/sink/Elasticsearch6OutputFormat.java
Patch:
@@ -18,13 +18,14 @@
 
 package com.dtstack.flinkx.connector.elasticsearch6.sink;
 
-import org.apache.flink.table.data.RowData;
-
 import com.dtstack.flinkx.connector.elasticsearch6.conf.Elasticsearch6Conf;
 import com.dtstack.flinkx.connector.elasticsearch6.utils.Elasticsearch6RequestHelper;
 import com.dtstack.flinkx.connector.elasticsearch6.utils.Elasticsearch6Util;
 import com.dtstack.flinkx.sink.format.BaseRichOutputFormat;
 import com.dtstack.flinkx.throwable.WriteRecordException;
+
+import org.apache.flink.table.data.RowData;
+
 import org.elasticsearch.action.DocWriteRequest;
 import org.elasticsearch.action.bulk.BulkItemResponse;
 import org.elasticsearch.action.bulk.BulkRequest;

File: flinkx-connectors/flinkx-connector-elasticsearch6/src/main/java/com/dtstack/flinkx/connector/elasticsearch6/sink/Elasticsearch6OutputFormatBuilder.java
Patch:
@@ -20,6 +20,7 @@
 
 import com.dtstack.flinkx.connector.elasticsearch6.conf.Elasticsearch6Conf;
 import com.dtstack.flinkx.sink.format.BaseRichOutputFormatBuilder;
+
 import com.google.common.base.Preconditions;
 
 /**

File: flinkx-connectors/flinkx-connector-elasticsearch6/src/main/java/com/dtstack/flinkx/connector/elasticsearch6/source/Elasticsearch6InputFormatBuilder.java
Patch:
@@ -20,6 +20,7 @@
 
 import com.dtstack.flinkx.connector.elasticsearch6.conf.Elasticsearch6Conf;
 import com.dtstack.flinkx.source.format.BaseRichInputFormatBuilder;
+
 import com.google.common.base.Preconditions;
 
 /**

File: flinkx-connectors/flinkx-connector-ftp/src/main/java/com/dtstack/flinkx/connector/ftp/conf/ConfigConstants.java
Patch:
@@ -20,8 +20,9 @@
 
 /**
  * The class containing Ftp configuration constants
- *
+ * <p>
  * Company: www.dtstack.com
+ *
  * @author huyifan.zju@163.com
  */
 public class ConfigConstants {
@@ -32,7 +33,7 @@ public class ConfigConstants {
 
     public static final int DEFAULT_TIMEOUT = 5000;
 
-    public static final String  DEFAULT_FTP_CONNECT_PATTERN = "PASV";
+    public static final String DEFAULT_FTP_CONNECT_PATTERN = "PASV";
 
     public static final String SFTP_PROTOCOL = "sftp";
 

File: flinkx-connectors/flinkx-connector-ftp/src/main/java/com/dtstack/flinkx/connector/ftp/converter/FtpColumnConverter.java
Patch:
@@ -18,9 +18,6 @@
 
 package com.dtstack.flinkx.connector.ftp.converter;
 
-import org.apache.flink.table.data.GenericRowData;
-import org.apache.flink.table.data.RowData;
-
 import com.dtstack.flinkx.conf.FieldConf;
 import com.dtstack.flinkx.connector.ftp.conf.FtpConfig;
 import com.dtstack.flinkx.converter.AbstractRowConverter;
@@ -35,6 +32,9 @@
 import com.dtstack.flinkx.throwable.UnsupportedTypeException;
 import com.dtstack.flinkx.util.DateUtil;
 
+import org.apache.flink.table.data.GenericRowData;
+import org.apache.flink.table.data.RowData;
+
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Locale;

File: flinkx-connectors/flinkx-connector-ftp/src/main/java/com/dtstack/flinkx/connector/ftp/enums/EFtpMode.java
Patch:
@@ -32,5 +32,5 @@ public enum EFtpMode {
     /**
      * 主动方式
      */
-    PORT;
+    PORT
 }

File: flinkx-connectors/flinkx-connector-ftp/src/main/java/com/dtstack/flinkx/connector/ftp/handler/FtpHandlerFactory.java
Patch:
@@ -26,11 +26,11 @@
  */
 public class FtpHandlerFactory {
 
-    public static IFtpHandler createFtpHandler(String protocolStr){
+    public static IFtpHandler createFtpHandler(String protocolStr) {
         IFtpHandler ftpHandler;
 
         Protocol protocol = Protocol.getByName(protocolStr);
-        if(Protocol.SFTP.equals(protocol)) {
+        if (Protocol.SFTP.equals(protocol)) {
             ftpHandler = new SftpHandler();
         } else {
             ftpHandler = new FtpHandler();
@@ -39,7 +39,7 @@ public static IFtpHandler createFtpHandler(String protocolStr){
         return ftpHandler;
     }
 
-    enum Protocol{
+    enum Protocol {
         FTP, SFTP;
 
         public static Protocol getByName(String name) {

File: flinkx-connectors/flinkx-connector-ftp/src/main/java/com/dtstack/flinkx/connector/ftp/sink/FtpOutputFormatBuilder.java
Patch:
@@ -21,6 +21,7 @@
 import com.dtstack.flinkx.connector.ftp.conf.FtpConfig;
 import com.dtstack.flinkx.sink.format.FileOutputFormatBuilder;
 import com.dtstack.flinkx.throwable.FlinkxRuntimeException;
+
 import org.apache.commons.lang.StringUtils;
 
 /**

File: flinkx-connectors/flinkx-connector-ftp/src/main/java/com/dtstack/flinkx/connector/ftp/sink/FtpSinkFactory.java
Patch:
@@ -25,13 +25,12 @@
 import com.dtstack.flinkx.connector.ftp.converter.FtpRawTypeConverter;
 import com.dtstack.flinkx.converter.RawTypeConverter;
 import com.dtstack.flinkx.sink.SinkFactory;
-
 import com.dtstack.flinkx.util.JsonUtil;
+import com.dtstack.flinkx.util.StringUtil;
 
 import org.apache.flink.streaming.api.datastream.DataStream;
 import org.apache.flink.streaming.api.datastream.DataStreamSink;
 import org.apache.flink.table.data.RowData;
-import com.dtstack.flinkx.util.StringUtil;
 
 import java.util.List;
 
@@ -55,7 +54,7 @@ public FtpSinkFactory(SyncConf syncConf) {
             ftpConfig.setDefaultPort();
         }
 
-        if(!ConfigConstants.DEFAULT_FIELD_DELIMITER.equals(ftpConfig.getFieldDelimiter())){
+        if (!ConfigConstants.DEFAULT_FIELD_DELIMITER.equals(ftpConfig.getFieldDelimiter())) {
             String fieldDelimiter = StringUtil.convertRegularExpr(ftpConfig.getFieldDelimiter());
             ftpConfig.setFieldDelimiter(fieldDelimiter);
         }

File: flinkx-connectors/flinkx-connector-ftp/src/main/java/com/dtstack/flinkx/connector/ftp/source/FtpInputFormatBuilder.java
Patch:
@@ -21,6 +21,7 @@
 import com.dtstack.flinkx.connector.ftp.conf.FtpConfig;
 import com.dtstack.flinkx.source.format.BaseRichInputFormatBuilder;
 import com.dtstack.flinkx.throwable.FlinkxRuntimeException;
+
 import org.apache.commons.lang.StringUtils;
 
 /** @author jiangbo */

File: flinkx-connectors/flinkx-connector-ftp/src/main/java/com/dtstack/flinkx/connector/ftp/source/FtpInputSplit.java
Patch:
@@ -25,8 +25,9 @@
 
 /**
  * The Class describing each InputSplit of Ftp
- *
+ * <p>
  * Company: www.dtstack.com
+ *
  * @author huyifan.zju@163.com
  */
 public class FtpInputSplit implements InputSplit {

File: flinkx-connectors/flinkx-connector-ftp/src/main/java/com/dtstack/flinkx/connector/ftp/source/FtpSeqInputStream.java
Patch:
@@ -27,8 +27,9 @@
 
 /**
  * The InputStream Implementation that read multiple ftp files one by one.
- *
+ * <p>
  * Company: www.dtstack.com
+ *
  * @author huyifan.zju@163.com
  */
 public class FtpSeqInputStream extends InputStream {
@@ -58,8 +59,7 @@ final void nextStream() throws IOException {
             if (in == null) {
                 throw new NullPointerException();
             }
-        }
-        else {
+        } else {
             in = null;
         }
 

File: flinkx-connectors/flinkx-connector-oraclelogminer/src/main/java/com/dtstack/flinkx/connector/oraclelogminer/listener/LogParser.java
Patch:
@@ -43,7 +43,7 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import java.math.BigDecimal;
+import java.math.BigInteger;
 import java.nio.charset.StandardCharsets;
 import java.sql.Timestamp;
 import java.util.ArrayList;
@@ -284,7 +284,7 @@ public LinkedList<RowData> parse(QueueData pair, AbstractCDCRowConverter rowConv
         return rowConverter.toInternal(eventRow);
     }
 
-    private void printDelay( BigDecimal scn, long ts, Timestamp timestamp) {
+    private void printDelay(BigInteger scn, long ts, Timestamp timestamp) {
 
         long res = ts >> 22;
 

File: flinkx-connectors/flinkx-connector-elasticsearch6/src/main/java/com/dtstack/flinkx/connector/elasticsearch6/conf/Elasticsearch6Conf.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.es.conf;
+package com.dtstack.flinkx.connector.elasticsearch6.conf;
 
 import com.dtstack.flinkx.conf.FlinkxCommonConf;
 
@@ -29,7 +29,7 @@
  * @author: lany
  * @create: 2021/06/16 15:36
  */
-public class EsConf extends FlinkxCommonConf implements Serializable {
+public class Elasticsearch6Conf extends FlinkxCommonConf implements Serializable {
 
     private static final long serialVersionUID = 2L;
 

File: flinkx-connectors/flinkx-connector-elasticsearch6/src/main/java/com/dtstack/flinkx/connector/elasticsearch6/converter/Elasticsearch6ColumnConverter.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.es.converter;
+package com.dtstack.flinkx.connector.elasticsearch6.converter;
 
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.types.logical.LogicalType;
@@ -55,13 +55,13 @@
  * @author: lany
  * @create: 2021/06/16 15:36
  */
-public class EsColumnConverter extends AbstractRowConverter<Map<String, Object>, Object, Map<String, Object>, LogicalType> {
+public class Elasticsearch6ColumnConverter extends AbstractRowConverter<Map<String, Object>, Object, Map<String, Object>, LogicalType> {
 
     private static final long serialVersionUID = 2L;
 
     private List<Tuple3<String,Integer, LogicalType>> typeIndexList = new ArrayList<>();
 
-    public EsColumnConverter(RowType rowType) {
+    public Elasticsearch6ColumnConverter(RowType rowType) {
         super(rowType);
         List<String> fieldNames = rowType.getFieldNames();
         for (int i = 0; i< rowType.getFieldCount(); i++) {

File: flinkx-connectors/flinkx-connector-elasticsearch6/src/main/java/com/dtstack/flinkx/connector/elasticsearch6/converter/Elasticsearch6RawTypeConverter.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.es.converter;
+package com.dtstack.flinkx.connector.elasticsearch6.converter;
 
 import org.apache.flink.table.api.DataTypes;
 import org.apache.flink.table.types.DataType;
@@ -31,7 +31,7 @@
  * @author: lany
  * @create: 2021/06/26 21:17
  */
-public class EsRawTypeConverter {
+public class Elasticsearch6RawTypeConverter {
 
     public static DataType apply(String type) {
         switch (type.toUpperCase(Locale.ENGLISH)) {

File: flinkx-connectors/flinkx-connector-elasticsearch6/src/main/java/com/dtstack/flinkx/connector/elasticsearch6/options/DtElasticsearch6Options.java
Patch:
@@ -1,4 +1,4 @@
-package com.dtstack.flinkx.connector.es.options;
+package com.dtstack.flinkx.connector.elasticsearch6.options;
 
 import org.apache.flink.configuration.ConfigOption;
 import org.apache.flink.configuration.ConfigOptions;
@@ -9,7 +9,7 @@
  * @author: lany
  * @create: 2021/06/29 21:40
  */
-public class DtEsOptions {
+public class DtElasticsearch6Options {
 
     public static final ConfigOption<Integer> DT_BULK_FLUSH_MAX_ACTIONS_OPTION =
             ConfigOptions.key("bulk-flush.max-actions")

File: flinkx-connectors/flinkx-connector-elasticsearch6/src/main/java/com/dtstack/flinkx/connector/elasticsearch6/utils/Elasticsearch6Constants.java
Patch:
@@ -16,15 +16,15 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.es.utils;
+package com.dtstack.flinkx.connector.elasticsearch6.utils;
 
 /**
  * @description:
  * @program: flinkx-all
  * @author: lany
  * @create: 2021/06/19 13:41
  */
-public interface EsConstants {
+public interface Elasticsearch6Constants {
 
     /**
      * address separator

File: flinkx-connectors/flinkx-connector-elasticsearch6/src/main/java/com/dtstack/flinkx/connector/elasticsearch6/utils/Elasticsearch6RequestHelper.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.es.utils;
+package com.dtstack.flinkx.connector.elasticsearch6.utils;
 
 
 import org.elasticsearch.action.delete.DeleteRequest;
@@ -41,7 +41,7 @@
  * @author: lany
  * @create: 2021/06/19 13:59
  */
-public class EsRequestHelper {
+public class Elasticsearch6RequestHelper {
 
     /**
      * create Elasticsearch UpdateRequest.

File: flinkx-connectors/flinkx-connector-hbase-base/src/main/java/com/dtstack/flinkx/connector/hbase14/HBaseSerde.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.hbase;
+package com.dtstack.flinkx.connector.hbase14;
 
 import org.apache.flink.table.data.DecimalData;
 import org.apache.flink.table.data.GenericRowData;

File: flinkx-connectors/flinkx-connector-hbase-base/src/main/java/com/dtstack/flinkx/connector/hbase14/HBaseTableSchema.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.hbase;
+package com.dtstack.flinkx.connector.hbase14;
 
 import org.apache.flink.api.common.typeinfo.TypeInformation;
 import org.apache.flink.api.java.typeutils.TypeExtractor;

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/PluginUtil.java
Patch:
@@ -168,7 +168,8 @@ private static String camelize(String pluginName, String suffix) {
         suffix = suffix.toLowerCase();
         StringBuilder sb = new StringBuilder(32);
         sb.append(PACKAGE_PREFIX);
-        sb.append(left).append(ConstantValue.POINT_SYMBOL).append(suffix).append(ConstantValue.POINT_SYMBOL);
+        sb.append(ConnectorNameConvertUtil.convertPackageName(left)).append(ConstantValue.POINT_SYMBOL).append(suffix).append(ConstantValue.POINT_SYMBOL);
+        left = ConnectorNameConvertUtil.convertClassPrefix(left);
         sb.append(left.substring(0, 1).toUpperCase()).append(left.substring(1));
         sb.append(suffix.substring(0, 1).toUpperCase()).append(suffix.substring(1));
         sb.append(GENERIC_SUFFIX);

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/ReflectionUtils.java
Patch:
@@ -43,6 +43,7 @@ public static Method getDeclaredMethod(Object object, String methodName, Class<?
         for(Class<?> clazz = object.getClass() ; clazz != Object.class ; clazz = clazz.getSuperclass()) {
             try {
                 method = clazz.getDeclaredMethod(methodName, parameterTypes) ;
+                method.setAccessible(true);
                 return method ;
             } catch (Exception e) {
                 //do nothing then can get method from super Class

File: flinkx-connectors/flinkx-connector-elasticsearch6/src/main/java/com/dtstack/flinkx/connector/elasticsearch6/sink/Elasticsearch6DynamicTableSink.java
Patch:
@@ -64,7 +64,9 @@ public SinkRuntimeProvider getSinkRuntimeProvider(Context context) {
         builder.setRowConverter(new Elasticsearch6RowConverter(rowType));
         builder.setEsConf(elasticsearchConf);
 
-        return SinkFunctionProvider.of(new DtOutputFormatSinkFunction<>(builder.finish()), 1);
+        return SinkFunctionProvider.of(
+                new DtOutputFormatSinkFunction<>(builder.finish()),
+                elasticsearchConf.getParallelism());
     }
 
     @Override

File: flinkx-connectors/flinkx-connector-elasticsearch6/src/main/java/com/dtstack/flinkx/connector/elasticsearch6/sink/Elasticsearch6SinkFactory.java
Patch:
@@ -50,7 +50,6 @@ public Elasticsearch6SinkFactory(SyncConf syncConf) {
                         JsonUtil.toJson(syncConf.getWriter().getParameter()), Elasticsearch6Conf.class);
         elasticsearchConf.setColumn(syncConf.getWriter().getFieldList());
         super.initFlinkxCommonConf(elasticsearchConf);
-        elasticsearchConf.setParallelism(1);
     }
 
     @Override

File: flinkx-connectors/flinkx-connector-elasticsearch6/src/main/java/com/dtstack/flinkx/connector/elasticsearch6/source/Elasticsearch6DynamicTableSource.java
Patch:
@@ -89,7 +89,9 @@ public ScanRuntimeProvider getScanRuntimeProvider(ScanContext runtimeProviderCon
         builder.setEsConf(elasticsearchConf);
 
         return ParallelSourceFunctionProvider.of(
-                new DtInputFormatSourceFunction<>(builder.finish(), typeInformation), false, 1);
+                new DtInputFormatSourceFunction<>(builder.finish(), typeInformation),
+                false,
+                elasticsearchConf.getParallelism());
     }
 
     @Override

File: flinkx-connectors/flinkx-connector-elasticsearch6/src/main/java/com/dtstack/flinkx/connector/elasticsearch6/source/Elasticsearch6SourceFactory.java
Patch:
@@ -60,7 +60,6 @@ public Elasticsearch6SourceFactory(SyncConf syncConf, StreamExecutionEnvironment
         super.initFlinkxCommonConf(elasticsearchConf);
         elasticsearchConf.setColumn(fieldList);
         elasticsearchConf.setFieldNames(fieldNames);
-        elasticsearchConf.setParallelism(1);
     }
 
     @Override

File: flinkx-connectors/flinkx-connector-elasticsearch7/src/main/java/com/dtstack/flinkx/connector/elasticsearch7/sink/Elasticsearch7SinkFactory.java
Patch:
@@ -61,7 +61,7 @@ public DataStreamSink<RowData> createSink(DataStream<RowData> dataSet) {
         final RowType rowType = TableUtil.createRowType(
                 elasticsearchConf.getColumn(),
                 getRawTypeConverter());
-        builder.setRowConverter(new ElasticsearchColumnConverter(elasticsearchConf, rowType));
+        builder.setRowConverter(new ElasticsearchColumnConverter(rowType));
         return createOutput(dataSet, builder.finish());
     }
 

File: flinkx-connectors/flinkx-connector-elasticsearch7/src/main/java/com/dtstack/flinkx/connector/elasticsearch7/sink/ElasticsearchOutputFormat.java
Patch:
@@ -3,8 +3,8 @@
 import com.dtstack.flinkx.connector.elasticsearch7.conf.ElasticsearchConf;
 import com.dtstack.flinkx.connector.elasticsearch7.utils.ElasticsearchRequestHelper;
 import com.dtstack.flinkx.connector.elasticsearch7.utils.ElasticsearchUtil;
-import com.dtstack.flinkx.exception.WriteRecordException;
-import com.dtstack.flinkx.outputformat.BaseRichOutputFormat;
+import com.dtstack.flinkx.sink.format.BaseRichOutputFormat;
+import com.dtstack.flinkx.throwable.WriteRecordException;
 
 import org.apache.flink.table.data.RowData;
 

File: flinkx-connectors/flinkx-connector-elasticsearch7/src/main/java/com/dtstack/flinkx/connector/elasticsearch7/sink/ElasticsearchOutputFormatBuilder.java
Patch:
@@ -19,7 +19,8 @@
 package com.dtstack.flinkx.connector.elasticsearch7.sink;
 
 import com.dtstack.flinkx.connector.elasticsearch7.conf.ElasticsearchConf;
-import com.dtstack.flinkx.outputformat.BaseRichOutputFormatBuilder;
+import com.dtstack.flinkx.sink.format.BaseRichOutputFormatBuilder;
+
 import com.google.common.base.Preconditions;
 
 /**

File: flinkx-connectors/flinkx-connector-elasticsearch7/src/main/java/com/dtstack/flinkx/connector/elasticsearch7/source/ElasticsearchInputFormatBuilder.java
Patch:
@@ -19,8 +19,8 @@
 package com.dtstack.flinkx.connector.elasticsearch7.source;
 
 import com.dtstack.flinkx.connector.elasticsearch7.conf.ElasticsearchConf;
-import com.dtstack.flinkx.converter.AbstractRowConverter;
-import com.dtstack.flinkx.inputformat.BaseRichInputFormatBuilder;
+import com.dtstack.flinkx.source.format.BaseRichInputFormatBuilder;
+
 import com.google.common.base.Preconditions;
 
 /**

File: flinkx-clients/src/main/java/com/dtstack/flinkx/client/kubernetes/KubernetesApplicationClusterClientHelper.java
Patch:
@@ -158,7 +158,7 @@ private void setDeployerConfig(Configuration configuration, Options launcherOpti
         String coreJarFileName = PluginInfoUtil.getCoreJarName(launcherOptions.getFlinkxDistDir());
         String remoteCoreJarPath =
                 "local://"
-                        + launcherOptions.getRemotePluginPath()
+                        + launcherOptions.getRemoteFlinkxDistDir()
                         + File.separator
                         + coreJarFileName;
         configuration.set(PipelineOptions.JARS, Collections.singletonList(remoteCoreJarPath));

File: flinkx-connectors/flinkx-connector-kafka/src/main/java/com/dtstack/flinkx/connector/kafka/source/KafkaConsumer.java
Patch:
@@ -116,7 +116,7 @@ public void snapshotState(FunctionSnapshotContext context) throws Exception {
 
     @Override
     public void initializeState(FunctionInitializationContext context) throws Exception {
-        //super.initializeState(context);
+        flinkKafkaConsumer.initializeState(context);
         OperatorStateStore stateStore = context.getOperatorStateStore();
         LOG.info("Start initialize input format state, is restored:{}", context.isRestored());
         unionOffsetStates = stateStore.getUnionListState(new ListStateDescriptor<>(

File: flinkx-connectors/flinkx-connector-pgwal/src/main/java/com/dtstack/flinkx/connector/pgwal/table/PGWalDynamicTableFactory.java
Patch:
@@ -60,7 +60,7 @@
  *
  */
 public class PGWalDynamicTableFactory extends JdbcDynamicTableFactory {
-    public static final String IDENTIFIER = "PGWalFactory";
+    public static final String IDENTIFIER = "pgwal";
 
     private static final ConfigOption<String> DRIVER =
             ConfigOptions.key("driver")

File: flinkx-connectors/flinkx-connector-pgwal/src/main/java/com/dtstack/flinkx/connector/api/DataProcessor.java
Patch:
@@ -6,7 +6,7 @@
 
 public interface DataProcessor<T> {
 
-    List<T> process(ServiceProcessor.Context entity) throws IOException, SQLException;
+    List<T> process(ServiceProcessor.Context entity) throws Exception;
 
     boolean moreData();
 

File: flinkx-connectors/flinkx-connector-pgwal/src/main/java/com/dtstack/flinkx/connector/api/PGCDCServiceProcessor.java
Patch:
@@ -75,6 +75,8 @@ public void process(Context context) throws SQLException {
                 dataProcessor().process(context);
             } catch (IOException e) {
                 dataProcessor().processException(e);
+            } catch (Exception e) {
+                e.printStackTrace();
             }
         }
     }

File: flinkx-connectors/flinkx-connector-pgwal/src/main/java/com/dtstack/flinkx/connector/api/PGDataProcessor.java
Patch:
@@ -43,7 +43,7 @@ public PGDataProcessor(Map<String, Object> param) {
     }
 
     @Override
-    public List<RowData> process(ServiceProcessor.Context context) throws SQLException {
+    public List<RowData> process(ServiceProcessor.Context context) throws Exception {
         assert context.contains("data");
         ChangeLog changeLog = decoder.decode(context.get("data", ByteBuffer.class));
         if (StringUtils.isBlank(changeLog.getId())) {

File: flinkx-connectors/flinkx-connector-pgwal/src/main/java/com/dtstack/flinkx/connector/pgwal/converter/PGWalColumnConverter.java
Patch:
@@ -92,7 +92,7 @@ class Context {
 
     @Override
     @SuppressWarnings("unchecked")
-    public LinkedList<RowData> toInternal(ChangeLog entity) {
+    public LinkedList<RowData> toInternal(ChangeLog entity) throws Exception {
         LinkedList<RowData> result = new LinkedList<>();
         PgMessageTypeEnum eventType = entity.getType();
         String schema = entity.getSchema();
@@ -249,7 +249,7 @@ private List<String> parseColumnList(
             List<Object> entryColumnList,
             List<AbstractBaseColumn> columnList,
             List<String> headerList,
-            String after) {
+            String after) throws Exception {
         List<String> originList = new ArrayList<>();
         for (int i = 0; i < entryColumnList.size(); i++) {
             Object entryColumn = entryColumnList.get(i);

File: flinkx-connectors/flinkx-connector-pgwal/src/main/java/com/dtstack/flinkx/connector/pgwal/inputformat/PGWalInputFormat.java
Patch:
@@ -17,6 +17,8 @@
  */
 package com.dtstack.flinkx.connector.pgwal.inputformat;
 
+import com.dtstack.flinkx.source.format.BaseRichInputFormat;
+
 import org.apache.flink.core.io.GenericInputSplit;
 import org.apache.flink.core.io.InputSplit;
 import org.apache.flink.table.data.RowData;
@@ -27,7 +29,6 @@
 import com.dtstack.flinkx.connector.pgwal.util.ReplicationSlotInfoWrapper;
 import com.dtstack.flinkx.converter.AbstractCDCRowConverter;
 import com.dtstack.flinkx.element.ErrorMsgRowData;
-import com.dtstack.flinkx.inputformat.BaseRichInputFormat;
 import com.dtstack.flinkx.restore.FormatState;
 import com.dtstack.flinkx.util.ExceptionUtil;
 import com.dtstack.flinkx.util.RetryUtil;

File: flinkx-connectors/flinkx-connector-pgwal/src/main/java/com/dtstack/flinkx/connector/pgwal/inputformat/PGWalInputFormatBuilder.java
Patch:
@@ -20,7 +20,7 @@
 import com.dtstack.flinkx.connector.pgwal.conf.PGWalConf;
 import com.dtstack.flinkx.connector.pgwal.util.PGUtil;
 import com.dtstack.flinkx.converter.AbstractCDCRowConverter;
-import com.dtstack.flinkx.inputformat.BaseRichInputFormatBuilder;
+import com.dtstack.flinkx.source.format.BaseRichInputFormatBuilder;
 import com.dtstack.flinkx.util.ClassUtil;
 import com.dtstack.flinkx.util.GsonUtil;
 import com.google.common.collect.Lists;
@@ -75,6 +75,5 @@ protected void checkFormat() {
         }
 
         ClassUtil.forName(PGUtil.DRIVER_NAME, getClass().getClassLoader());
-
     }
 }

File: flinkx-connectors/flinkx-connector-pgwal/src/main/java/com/dtstack/flinkx/connector/pgwal/source/PGWalDynamicTableSource.java
Patch:
@@ -18,6 +18,8 @@
 
 package com.dtstack.flinkx.connector.pgwal.source;
 
+import com.dtstack.flinkx.source.DtInputFormatSourceFunction;
+
 import org.apache.flink.api.common.typeinfo.TypeInformation;
 import org.apache.flink.formats.json.TimestampFormat;
 import org.apache.flink.table.api.TableSchema;
@@ -32,7 +34,6 @@
 import com.dtstack.flinkx.connector.pgwal.conf.PGWalConf;
 import com.dtstack.flinkx.connector.pgwal.converter.PGWalRowConverter;
 import com.dtstack.flinkx.connector.pgwal.inputformat.PGWalInputFormatBuilder;
-import com.dtstack.flinkx.streaming.api.functions.source.DtInputFormatSourceFunction;
 import com.dtstack.flinkx.table.connector.source.ParallelSourceFunctionProvider;
 import com.google.common.base.Preconditions;
 

File: flinkx-connectors/flinkx-connector-pgwal/src/main/java/com/dtstack/flinkx/connector/pgwal/table/PGWalDynamicTableFactory.java
Patch:
@@ -17,6 +17,8 @@
  */
 package com.dtstack.flinkx.connector.pgwal.table;
 
+import com.dtstack.flinkx.source.DtInputFormatSourceFunction;
+
 import org.apache.flink.api.common.typeinfo.TypeInformation;
 import org.apache.flink.configuration.ConfigOption;
 import org.apache.flink.configuration.ConfigOptions;
@@ -41,7 +43,6 @@
 import com.dtstack.flinkx.connector.api.PGCDCServiceProcessor;
 import com.dtstack.flinkx.connector.pgwal.conf.PGWalConf;
 import com.dtstack.flinkx.connector.pgwal.options.PGWalOptions;
-import com.dtstack.flinkx.streaming.api.functions.source.DtInputFormatSourceFunction;
 import com.dtstack.flinkx.table.connector.source.ParallelSourceFunctionProvider;
 
 import java.sql.SQLException;

File: flinkx-core/src/main/java/com/dtstack/flinkx/options/Options.java
Patch:
@@ -28,6 +28,7 @@
 import com.dtstack.flinkx.enums.ClusterMode;
 import com.dtstack.flinkx.enums.ConnectorLoadMode;
 import org.apache.commons.lang.StringUtils;
+import org.apache.logging.log4j.core.Core;
 
 
 /**
@@ -126,6 +127,8 @@ public Configuration loadFlinkConfiguration() {
                 flinkConfiguration.setString(CoreOptions.CLASSLOADER_RESOLVE_ORDER, "parent-first");
             }
             flinkConfiguration.setString(ConfigConstant.FLINK_PLUGIN_LOAD_MODE_KEY, pluginLoadMode);
+
+            flinkConfiguration.set(CoreOptions.CHECK_LEAKED_CLASSLOADER, false);
         }
         return flinkConfiguration;
     }

File: flinkx-core/src/main/java/com/dtstack/flinkx/options/Options.java
Patch:
@@ -28,6 +28,7 @@
 import com.dtstack.flinkx.enums.ClusterMode;
 import com.dtstack.flinkx.enums.ConnectorLoadMode;
 import org.apache.commons.lang.StringUtils;
+import org.apache.logging.log4j.core.Core;
 
 
 /**
@@ -126,6 +127,8 @@ public Configuration loadFlinkConfiguration() {
                 flinkConfiguration.setString(CoreOptions.CLASSLOADER_RESOLVE_ORDER, "parent-first");
             }
             flinkConfiguration.setString(ConfigConstant.FLINK_PLUGIN_LOAD_MODE_KEY, pluginLoadMode);
+
+            flinkConfiguration.set(CoreOptions.CHECK_LEAKED_CLASSLOADER, false);
         }
         return flinkConfiguration;
     }

File: flinkx-core/src/main/java/com/dtstack/flinkx/decoder/JsonDecoder.java
Patch:
@@ -17,7 +17,7 @@
  */
 package com.dtstack.flinkx.decoder;
 
-import com.dtstack.flinkx.util.GsonUtil;
+import com.dtstack.flinkx.util.JsonUtil;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -35,14 +35,14 @@ public class JsonDecoder implements IDecode, Serializable {
 
     private static final long serialVersionUID = 1L;
 
-    private static Logger LOG = LoggerFactory.getLogger(JsonDecoder.class);
+    private static final Logger LOG = LoggerFactory.getLogger(JsonDecoder.class);
 
     private static final String KEY_MESSAGE = "message";
 
     @Override
     public Map<String, Object> decode(final String message) {
         try {
-            Map<String, Object> event = GsonUtil.GSON.fromJson(message, GsonUtil.gsonMapTypeToken);
+            Map<String, Object> event = JsonUtil.toObject(message, JsonUtil.MAP_TYPE_REFERENCE);
             if (!event.containsKey(KEY_MESSAGE)) {
                 event.put(KEY_MESSAGE, message);
             }

File: flinkx-core/src/main/java/com/dtstack/flinkx/exec/ExecuteProcessHelper.java
Patch:
@@ -21,7 +21,7 @@
 import com.dtstack.flinkx.enums.ClusterMode;
 import com.dtstack.flinkx.enums.EPluginLoadMode;
 import com.dtstack.flinkx.util.SampleUtils;
-import com.fasterxml.jackson.databind.ObjectMapper;
+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.base.Strings;
 import com.google.common.collect.Lists;
 import org.apache.commons.io.Charsets;

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/DtStringUtil.java
Patch:
@@ -19,7 +19,7 @@
 
 package com.dtstack.flinkx.util;
 
-import com.fasterxml.jackson.databind.ObjectMapper;
+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.common.base.Strings;
 import com.google.common.collect.Maps;
 import org.apache.commons.lang3.StringUtils;
@@ -302,7 +302,7 @@ public static String addJdbcParam(String dbUrl, Map<String, String> addParams, b
             isFirst = false;
         }
 
-        return preStr + "?" + sb.toString();
+        return preStr + "?" + sb;
     }
 
     public static boolean isJson(String str) {

File: flinkx-connectors/flinkx-connector-kafka/src/main/java/com/dtstack/flinkx/connector/kafka/converter/KafkaColumnConverter.java
Patch:
@@ -19,8 +19,6 @@
 package com.dtstack.flinkx.connector.kafka.converter;
 
 import com.dtstack.flinkx.conf.FieldConf;
-import org.apache.flink.table.data.RowData;
-
 import com.dtstack.flinkx.connector.kafka.conf.KafkaConf;
 import com.dtstack.flinkx.converter.AbstractRowConverter;
 import com.dtstack.flinkx.converter.IDeserializationConverter;
@@ -37,6 +35,8 @@
 import com.dtstack.flinkx.util.DateUtil;
 import com.dtstack.flinkx.util.MapUtil;
 import org.apache.commons.collections.CollectionUtils;
+import org.apache.flink.table.data.RowData;
+import org.apache.flink.util.CollectionUtil;
 
 import java.math.BigDecimal;
 import java.nio.charset.StandardCharsets;

File: flinkx-core/src/main/java/com/dtstack/flinkx/inputformat/BaseRichInputFormat.java
Patch:
@@ -24,8 +24,10 @@
 import com.dtstack.flinkx.exception.ReadRecordException;
 import com.dtstack.flinkx.metrics.AccumulatorCollector;
 import com.dtstack.flinkx.metrics.BaseMetric;
+import com.dtstack.flinkx.metrics.CustomReporter;
 import com.dtstack.flinkx.restore.FormatState;
 import com.dtstack.flinkx.source.ByteRateLimiter;
+import com.dtstack.flinkx.util.DataSyncFactoryUtil;
 import com.dtstack.flinkx.util.ExceptionUtil;
 import com.dtstack.flinkx.util.JsonUtil;
 import jdk.nashorn.internal.ir.debug.ObjectSizeCalculator;

File: flinkx-connectors/flinkx-connector-clickhouse/src/main/java/com/dtstack/flinkx/connector/clickhouse/table/ClickhouseDynamicTableFactory.java
Patch:
@@ -18,10 +18,10 @@
 
 package com.dtstack.flinkx.connector.clickhouse.table;
 
-import com.dtstack.flinkx.connector.clickhouse.ClickhouseDialect;
+import com.dtstack.flinkx.connector.clickhouse.dialect.ClickhouseDialect;
 import com.dtstack.flinkx.connector.clickhouse.sink.ClickhouseOutputFormat;
 import com.dtstack.flinkx.connector.clickhouse.source.ClickhouseInputFormat;
-import com.dtstack.flinkx.connector.jdbc.JdbcDialect;
+import com.dtstack.flinkx.connector.jdbc.dialect.JdbcDialect;
 import com.dtstack.flinkx.connector.jdbc.sink.JdbcOutputFormatBuilder;
 import com.dtstack.flinkx.connector.jdbc.source.JdbcInputFormatBuilder;
 import com.dtstack.flinkx.connector.jdbc.table.JdbcDynamicTableFactory;

File: flinkx-connectors/flinkx-connector-db2/src/main/java/com/dtstack/flinkx/connector/db2/table/Db2DynamicTableFactory.java
Patch:
@@ -18,11 +18,10 @@
 package com.dtstack.flinkx.connector.db2.table;
 
 
-import com.dtstack.flinkx.connector.db2.Db2Dialect;
+import com.dtstack.flinkx.connector.db2.dialect.Db2Dialect;
 import com.dtstack.flinkx.connector.db2.sink.Db2OutputFormat;
 import com.dtstack.flinkx.connector.db2.source.Db2InputFormat;
-import com.dtstack.flinkx.connector.jdbc.JdbcDialect;
-import com.dtstack.flinkx.connector.jdbc.sink.JdbcOutputFormat;
+import com.dtstack.flinkx.connector.jdbc.dialect.JdbcDialect;
 import com.dtstack.flinkx.connector.jdbc.sink.JdbcOutputFormatBuilder;
 import com.dtstack.flinkx.connector.jdbc.source.JdbcInputFormatBuilder;
 import com.dtstack.flinkx.connector.jdbc.table.JdbcDynamicTableFactory;

File: flinkx-connectors/flinkx-connector-dm/src/main/java/com/dtstack/flinkx/connector/dm/table/DmDynamicTableFactory.java
Patch:
@@ -18,8 +18,8 @@
 
 package com.dtstack.flinkx.connector.dm.table;
 
-import com.dtstack.flinkx.connector.dm.DmDialect;
-import com.dtstack.flinkx.connector.jdbc.JdbcDialect;
+import com.dtstack.flinkx.connector.dm.dialect.DmDialect;
+import com.dtstack.flinkx.connector.jdbc.dialect.JdbcDialect;
 import com.dtstack.flinkx.connector.jdbc.table.JdbcDynamicTableFactory;
 
 /**

File: flinkx-connectors/flinkx-connector-gbase/src/main/java/com/dtstack/flinkx/connector/gbase/table/GBaseDynamicTableFactory.java
Patch:
@@ -18,8 +18,8 @@
 
 package com.dtstack.flinkx.connector.gbase.table;
 
-import com.dtstack.flinkx.connector.gbase.GBaseDialect;
-import com.dtstack.flinkx.connector.jdbc.JdbcDialect;
+import com.dtstack.flinkx.connector.gbase.dialect.GBaseDialect;
+import com.dtstack.flinkx.connector.jdbc.dialect.JdbcDialect;
 import com.dtstack.flinkx.connector.jdbc.table.JdbcDynamicTableFactory;
 
 /**

File: flinkx-connectors/flinkx-connector-greenplum/src/main/java/com/dtstack/flinkx/connector/greenplum/table/GreenplumDynamicTableFactory.java
Patch:
@@ -18,8 +18,8 @@
 
 package com.dtstack.flinkx.connector.greenplum.table;
 
-import com.dtstack.flinkx.connector.greenplum.GreenplumDialect;
-import com.dtstack.flinkx.connector.jdbc.JdbcDialect;
+import com.dtstack.flinkx.connector.greenplum.dialect.GreenplumDialect;
+import com.dtstack.flinkx.connector.jdbc.dialect.JdbcDialect;
 import com.dtstack.flinkx.connector.jdbc.table.JdbcDynamicTableFactory;
 
 /**

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/lookup/JdbcAllTableFunction.java
Patch:
@@ -4,7 +4,7 @@
 import org.apache.flink.table.data.GenericRowData;
 import org.apache.flink.table.types.logical.RowType;
 
-import com.dtstack.flinkx.connector.jdbc.JdbcDialect;
+import com.dtstack.flinkx.connector.jdbc.dialect.JdbcDialect;
 import com.dtstack.flinkx.connector.jdbc.conf.JdbcConf;
 import com.dtstack.flinkx.connector.jdbc.util.JdbcUtil;
 import com.dtstack.flinkx.lookup.AbstractAllTableFunction;

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/lookup/JdbcLruTableFunction.java
Patch:
@@ -22,7 +22,7 @@
 import org.apache.flink.table.functions.FunctionContext;
 import org.apache.flink.table.types.logical.RowType;
 
-import com.dtstack.flinkx.connector.jdbc.JdbcDialect;
+import com.dtstack.flinkx.connector.jdbc.dialect.JdbcDialect;
 import com.dtstack.flinkx.connector.jdbc.conf.JdbcConf;
 import com.dtstack.flinkx.connector.jdbc.conf.JdbcLookupConf;
 import com.dtstack.flinkx.enums.ECacheContentType;
@@ -80,7 +80,7 @@ public class JdbcLruTableFunction extends AbstractLruTableFunction {
     private static final long serialVersionUID = 1L;
     private static final Logger LOG = LoggerFactory.getLogger(JdbcLruTableFunction.class);
     /** when network is unhealthy block query */
-    private AtomicBoolean connectionStatus = new AtomicBoolean(true);
+    private final AtomicBoolean connectionStatus = new AtomicBoolean(true);
     /** query data thread */
     private transient ThreadPoolExecutor executor;
     /** vertx */

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/sink/JdbcDynamicTableSink.java
Patch:
@@ -27,7 +27,7 @@
 import org.apache.flink.util.CollectionUtil;
 
 import com.dtstack.flinkx.conf.FieldConf;
-import com.dtstack.flinkx.connector.jdbc.JdbcDialect;
+import com.dtstack.flinkx.connector.jdbc.dialect.JdbcDialect;
 import com.dtstack.flinkx.connector.jdbc.conf.JdbcConf;
 import com.dtstack.flinkx.enums.EWriteMode;
 import com.dtstack.flinkx.streaming.api.functions.sink.DtOutputFormatSinkFunction;

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/sink/JdbcOutputFormatBuilder.java
Patch:
@@ -17,7 +17,7 @@
  */
 package com.dtstack.flinkx.connector.jdbc.sink;
 
-import com.dtstack.flinkx.connector.jdbc.JdbcDialect;
+import com.dtstack.flinkx.connector.jdbc.dialect.JdbcDialect;
 import com.dtstack.flinkx.connector.jdbc.conf.JdbcConf;
 import com.dtstack.flinkx.converter.AbstractRowConverter;
 import com.dtstack.flinkx.outputformat.BaseRichOutputFormatBuilder;

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/source/JdbcDynamicTableSource.java
Patch:
@@ -32,7 +32,7 @@
 import org.apache.flink.util.Preconditions;
 
 import com.dtstack.flinkx.conf.FieldConf;
-import com.dtstack.flinkx.connector.jdbc.JdbcDialect;
+import com.dtstack.flinkx.connector.jdbc.dialect.JdbcDialect;
 import com.dtstack.flinkx.connector.jdbc.conf.JdbcConf;
 import com.dtstack.flinkx.connector.jdbc.lookup.JdbcAllTableFunction;
 import com.dtstack.flinkx.connector.jdbc.lookup.JdbcLruTableFunction;

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/source/JdbcInputFormatBuilder.java
Patch:
@@ -19,7 +19,7 @@
 package com.dtstack.flinkx.connector.jdbc.source;
 
 import com.dtstack.flinkx.conf.FieldConf;
-import com.dtstack.flinkx.connector.jdbc.JdbcDialect;
+import com.dtstack.flinkx.connector.jdbc.dialect.JdbcDialect;
 import com.dtstack.flinkx.connector.jdbc.conf.JdbcConf;
 import com.dtstack.flinkx.constants.ConstantValue;
 import com.dtstack.flinkx.enums.ColumnType;

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/table/JdbcDynamicTableFactory.java
Patch:
@@ -30,7 +30,7 @@
 import org.apache.flink.table.utils.TableSchemaUtils;
 import org.apache.flink.util.Preconditions;
 
-import com.dtstack.flinkx.connector.jdbc.JdbcDialect;
+import com.dtstack.flinkx.connector.jdbc.dialect.JdbcDialect;
 import com.dtstack.flinkx.connector.jdbc.conf.JdbcConf;
 import com.dtstack.flinkx.connector.jdbc.conf.JdbcLookupConf;
 import com.dtstack.flinkx.connector.jdbc.conf.SinkConnectionConf;

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/util/JdbcUtil.java
Patch:
@@ -21,7 +21,7 @@
 import org.apache.flink.util.CollectionUtil;
 
 import com.dtstack.flinkx.conf.FieldConf;
-import com.dtstack.flinkx.connector.jdbc.JdbcDialect;
+import com.dtstack.flinkx.connector.jdbc.dialect.JdbcDialect;
 import com.dtstack.flinkx.connector.jdbc.conf.JdbcConf;
 import com.dtstack.flinkx.converter.RawTypeConverter;
 import com.dtstack.flinkx.throwable.FlinkxRuntimeException;

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/util/SqlUtil.java
Patch:
@@ -18,7 +18,7 @@
 
 package com.dtstack.flinkx.connector.jdbc.util;
 
-import com.dtstack.flinkx.connector.jdbc.JdbcDialect;
+import com.dtstack.flinkx.connector.jdbc.dialect.JdbcDialect;
 import com.dtstack.flinkx.connector.jdbc.conf.JdbcConf;
 import com.dtstack.flinkx.connector.jdbc.source.JdbcInputSplit;
 import com.dtstack.flinkx.constants.ConstantValue;

File: flinkx-connectors/flinkx-connector-kingbase/src/main/java/com/dtstack/flinkx/connector/kingbase/table/KingbaseDynamicTableFactory.java
Patch:
@@ -17,12 +17,11 @@
  */
 package com.dtstack.flinkx.connector.kingbase.table;
 
-import com.dtstack.flinkx.connector.jdbc.JdbcDialect;
-import com.dtstack.flinkx.connector.jdbc.sink.JdbcOutputFormat;
+import com.dtstack.flinkx.connector.jdbc.dialect.JdbcDialect;
 import com.dtstack.flinkx.connector.jdbc.sink.JdbcOutputFormatBuilder;
 import com.dtstack.flinkx.connector.jdbc.source.JdbcInputFormatBuilder;
 import com.dtstack.flinkx.connector.jdbc.table.JdbcDynamicTableFactory;
-import com.dtstack.flinkx.connector.kingbase.KingbaseDialect;
+import com.dtstack.flinkx.connector.kingbase.dialect.KingbaseDialect;
 import com.dtstack.flinkx.connector.kingbase.sink.KingbaseOutputFormat;
 import com.dtstack.flinkx.connector.kingbase.source.KingbaseInputFormat;
 

File: flinkx-connectors/flinkx-connector-mysql/src/main/java/com/dtstack/flinkx/connector/mysql/table/MysqlDynamicTableFactory.java
Patch:
@@ -18,9 +18,9 @@
 
 package com.dtstack.flinkx.connector.mysql.table;
 
-import com.dtstack.flinkx.connector.jdbc.JdbcDialect;
+import com.dtstack.flinkx.connector.jdbc.dialect.JdbcDialect;
 import com.dtstack.flinkx.connector.jdbc.table.JdbcDynamicTableFactory;
-import com.dtstack.flinkx.connector.mysql.MysqlDialect;
+import com.dtstack.flinkx.connector.mysql.dialect.MysqlDialect;
 
 /**
  * @program: flinkx

File: flinkx-connectors/flinkx-connector-oracle/src/main/java/com/dtstack/flinkx/connector/oracle/table/OracleDynamicTableFactory.java
Patch:
@@ -18,9 +18,9 @@
 
 package com.dtstack.flinkx.connector.oracle.table;
 
-import com.dtstack.flinkx.connector.jdbc.JdbcDialect;
+import com.dtstack.flinkx.connector.jdbc.dialect.JdbcDialect;
 import com.dtstack.flinkx.connector.jdbc.table.JdbcDynamicTableFactory;
-import com.dtstack.flinkx.connector.oracle.OracleDialect;
+import com.dtstack.flinkx.connector.oracle.dialect.OracleDialect;
 
 /**
  * company www.dtstack.com

File: flinkx-connectors/flinkx-connector-postgresql/src/main/java/com/dtstack/flinkx/connector/postgresql/table/PostgresqlDynamicTableFactory.java
Patch:
@@ -18,9 +18,9 @@
 
 package com.dtstack.flinkx.connector.postgresql.table;
 
-import com.dtstack.flinkx.connector.jdbc.JdbcDialect;
+import com.dtstack.flinkx.connector.jdbc.dialect.JdbcDialect;
 import com.dtstack.flinkx.connector.jdbc.table.JdbcDynamicTableFactory;
-import com.dtstack.flinkx.connector.postgresql.PostgresqlDialect;
+import com.dtstack.flinkx.connector.postgresql.dialect.PostgresqlDialect;
 
 /**
  * @program: flinkx

File: flinkx-connectors/flinkx-connector-saphana/src/main/java/com/dtstack/flinkx/connector/saphana/table/SaphanaDynamicTableFactory.java
Patch:
@@ -18,9 +18,9 @@
 
 package com.dtstack.flinkx.connector.saphana.table;
 
-import com.dtstack.flinkx.connector.jdbc.JdbcDialect;
+import com.dtstack.flinkx.connector.jdbc.dialect.JdbcDialect;
 import com.dtstack.flinkx.connector.jdbc.table.JdbcDynamicTableFactory;
-import com.dtstack.flinkx.connector.saphana.SaphanaDialect;
+import com.dtstack.flinkx.connector.saphana.dialect.SaphanaDialect;
 
 /**
  * company www.dtstack.com

File: flinkx-connectors/flinkx-connector-sqlserver/src/main/java/com/dtstack/flinkx/connector/sqlserver/sink/SqlserverSinkFactory.java
Patch:
@@ -21,7 +21,7 @@
 import com.dtstack.flinkx.conf.SyncConf;
 import com.dtstack.flinkx.connector.jdbc.sink.JdbcOutputFormatBuilder;
 import com.dtstack.flinkx.connector.jdbc.sink.JdbcSinkFactory;
-import com.dtstack.flinkx.connector.sqlserver.SqlserverDialect;
+import com.dtstack.flinkx.connector.sqlserver.dialect.SqlserverDialect;
 import com.dtstack.flinkx.connector.sqlserver.converter.SqlserverRawTypeConverter;
 import com.dtstack.flinkx.converter.RawTypeConverter;
 
@@ -34,7 +34,7 @@
 public class SqlserverSinkFactory extends JdbcSinkFactory {
 
     public SqlserverSinkFactory(SyncConf syncConf){
-        super(syncConf);
+        super(syncConf, null);
         jdbcDialect = new SqlserverDialect(jdbcConf.isWithNoLock());
     }
 

File: flinkx-connectors/flinkx-connector-sqlserver/src/main/java/com/dtstack/flinkx/connector/sqlserver/table/SqlserverDynamicTableFactory.java
Patch:
@@ -18,11 +18,11 @@
 
 package com.dtstack.flinkx.connector.sqlserver.table;
 
-import com.dtstack.flinkx.connector.jdbc.JdbcDialect;
+import com.dtstack.flinkx.connector.jdbc.dialect.JdbcDialect;
 import com.dtstack.flinkx.connector.jdbc.sink.JdbcOutputFormatBuilder;
 import com.dtstack.flinkx.connector.jdbc.source.JdbcInputFormatBuilder;
 import com.dtstack.flinkx.connector.jdbc.table.JdbcDynamicTableFactory;
-import com.dtstack.flinkx.connector.sqlserver.SqlserverDialect;
+import com.dtstack.flinkx.connector.sqlserver.dialect.SqlserverDialect;
 import com.dtstack.flinkx.connector.sqlserver.sink.SqlserverOutputFormat;
 import com.dtstack.flinkx.connector.sqlserver.source.SqlserverInputFormat;
 

File: flinkx-connectors/flinkx-connector-stream/src/main/java/com/dtstack/flinkx/connector/stream/sink/StreamSinkFactory.java
Patch:
@@ -59,10 +59,9 @@ public DataStreamSink<RowData> createSink(DataStream<RowData> dataSet) {
         builder.setStreamConf(streamConf);
         AbstractRowConverter converter;
         if (useAbstractBaseColumn) {
-            converter = new StreamColumnConverter();
+            converter = new StreamColumnConverter(streamConf);
         } else {
-            final RowType rowType =
-                    TableUtil.createRowType(streamConf.getColumn(), getRawTypeConverter());
+            final RowType rowType = TableUtil.createRowType(streamConf.getColumn(), getRawTypeConverter());
             converter = new StreamRowConverter(rowType);
         }
 

File: flinkx-connectors/flinkx-connector-stream/src/main/java/com/dtstack/flinkx/connector/stream/source/StreamSourceFactory.java
Patch:
@@ -44,9 +44,7 @@ public class StreamSourceFactory extends SourceFactory {
 
     public StreamSourceFactory(SyncConf config, StreamExecutionEnvironment env) {
         super(config, env);
-        streamConf =
-                GsonUtil.GSON.fromJson(
-                        GsonUtil.GSON.toJson(config.getReader().getParameter()), StreamConf.class);
+        streamConf = GsonUtil.GSON.fromJson(GsonUtil.GSON.toJson(config.getReader().getParameter()), StreamConf.class);
         streamConf.setColumn(config.getReader().getFieldList());
         super.initFlinkxCommonConf(streamConf);
     }

File: flinkx-connectors/flinkx-connector-tidb/src/main/java/com/dtstack/flinkx/connector/tidb/dialect/TidbDialect.java
Patch:
@@ -16,9 +16,9 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.tidb;
+package com.dtstack.flinkx.connector.tidb.dialect;
 
-import com.dtstack.flinkx.connector.mysql.MysqlDialect;
+import com.dtstack.flinkx.connector.mysql.dialect.MysqlDialect;
 
 /**
  * Company：www.dtstack.com

File: flinkx-connectors/flinkx-connector-tidb/src/main/java/com/dtstack/flinkx/connector/tidb/sink/TidbOutputFormat.java
Patch:
@@ -27,5 +27,4 @@
  * @date 2021/6/22 15:34
  */
 public class TidbOutputFormat extends MysqlOutputFormat {
-
 }

File: flinkx-connectors/flinkx-connector-tidb/src/main/java/com/dtstack/flinkx/connector/tidb/sink/TidbSinkFactory.java
Patch:
@@ -38,5 +38,4 @@ public TidbSinkFactory(SyncConf syncConf){
     protected JdbcOutputFormatBuilder getBuilder() {
         return new JdbcOutputFormatBuilder(new TidbOutputFormat());
     }
-
 }

File: flinkx-connectors/flinkx-connector-tidb/src/main/java/com/dtstack/flinkx/connector/tidb/table/TidbDynamicTableFactory.java
Patch:
@@ -18,9 +18,9 @@
 
 package com.dtstack.flinkx.connector.tidb.table;
 
-import com.dtstack.flinkx.connector.jdbc.JdbcDialect;
+import com.dtstack.flinkx.connector.jdbc.dialect.JdbcDialect;
 import com.dtstack.flinkx.connector.mysql.table.MysqlDynamicTableFactory;
-import com.dtstack.flinkx.connector.tidb.TidbDialect;
+import com.dtstack.flinkx.connector.tidb.dialect.TidbDialect;
 
 /**
  * Company：www.dtstack.com

File: flinkx-core/src/main/java/com/dtstack/flinkx/element/column/StringColumn.java
Patch:
@@ -67,7 +67,7 @@ public Date asDate() {
         try {
             //如果string是时间戳
             time = NumberUtils.createLong(data);
-        } catch (UnsupportedOperationException ignored) {
+        } catch (Exception ignored) {
             //doNothing
         }
         SimpleDateFormat formatter = DateUtil.buildDateFormatter(format);

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/TableUtil.java
Patch:
@@ -96,8 +96,7 @@ public static RowType getRowType(DataType[] dataTypes, String[] fieldNames){
      * @param types field types
      * @return
      */
-    public static RowType createRowType(
-            List<String> fieldNames, List<String> types, RawTypeConverter converter) {
+    public static RowType createRowType(List<String> fieldNames, List<String> types, RawTypeConverter converter) {
         TableSchema.Builder builder = TableSchema.builder();
         for (int i = 0; i < types.size(); i++) {
             DataType dataType = converter.apply(types.get(i));

File: flinkx-connectors/flinkx-connector-kafka/src/main/java/com/dtstack/flinkx/connector/kafka/source/KafkaSourceFactory.java
Patch:
@@ -70,6 +70,7 @@ public DataStream<RowData> createSource() {
         KafkaConsumer consumer = new KafkaConsumer(
                 Lists.newArrayList(kafkaConf.getTopic()),
                 new RowDeserializationSchema(
+                        kafkaConf,
                         new KafkaColumnConverter(kafkaConf),
                         (Calculate & Serializable) (subscriptionState, tp) ->
                                 subscriptionState.partitionLag(

File: flinkx-connectors/flinkx-connector-kafka/src/main/java/com/dtstack/flinkx/connector/kafka/source/KafkaSourceFactory.java
Patch:
@@ -70,6 +70,7 @@ public DataStream<RowData> createSource() {
         KafkaConsumer consumer = new KafkaConsumer(
                 Lists.newArrayList(kafkaConf.getTopic()),
                 new RowDeserializationSchema(
+                        kafkaConf,
                         new KafkaColumnConverter(kafkaConf),
                         (Calculate & Serializable) (subscriptionState, tp) ->
                                 subscriptionState.partitionLag(

File: flinkx-connectors/flinkx-connector-elasticsearch6/src/main/java/com/dtstack/flinkx/connector/es/utils/EsConstants.java
Patch:
@@ -41,5 +41,5 @@ public interface EsConstants {
      */
     Integer ES_DEFAULT_PORT = 9200;
 
-    String IDENTIFIER = "elasticsearch6-x";
+    String IDENTIFIER = "es-x";
 }

File: flinkx-connectors/flinkx-connector-elasticsearch6/src/main/java/com/dtstack/flinkx/connector/es/utils/EsConstants.java
Patch:
@@ -41,5 +41,5 @@ public interface EsConstants {
      */
     Integer ES_DEFAULT_PORT = 9200;
 
-    String IDENTIFIER = "elasticsearch6-x";
+    String IDENTIFIER = "es-x";
 }

File: flinkx-connectors/flinkx-connector-oracle/src/main/java/com/dtstack/flinkx/connector/oracle/OracleDialect.java
Patch:
@@ -134,7 +134,7 @@ public String buildDualQueryStatement(String[] column) {
     private String buildEqualConditions(String[] uniqueKeyFields) {
         return Arrays.stream(uniqueKeyFields)
                 .map(col -> "T1." + quoteIdentifier(col) + " = T2." + quoteIdentifier(col))
-                .collect(Collectors.joining(", "));
+                .collect(Collectors.joining(" and "));
     }
 
     /** build T1."A"=T2."A" or T1."A"=nvl(T2."A",T1."A") */

File: flinkx-connectors/flinkx-connector-oracle/src/main/java/com/dtstack/flinkx/connector/oracle/OracleDialect.java
Patch:
@@ -134,7 +134,7 @@ public String buildDualQueryStatement(String[] column) {
     private String buildEqualConditions(String[] uniqueKeyFields) {
         return Arrays.stream(uniqueKeyFields)
                 .map(col -> "T1." + quoteIdentifier(col) + " = T2." + quoteIdentifier(col))
-                .collect(Collectors.joining(", "));
+                .collect(Collectors.joining(" and "));
     }
 
     /** build T1."A"=T2."A" or T1."A"=nvl(T2."A",T1."A") */

File: flinkx-core/src/main/java/com/dtstack/flinkx/inputformat/BaseRichInputFormat.java
Patch:
@@ -341,9 +341,9 @@ protected RowData loadConstantData(RowData rawRowData, List<FieldConf> fieldConf
     }
 
     /**
-     * 使用自定义的指标输出器把增量指标打到普罗米修斯
+     * 使用自定义的指标输出器把增量指标打到自定义插件
      */
-    protected boolean useCustomPrometheusReporter() {
+    protected boolean useCustomReporter() {
         return false;
     }
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/streaming/api/functions/sink/DtOutputFormatSinkFunction.java
Patch:
@@ -17,6 +17,8 @@
 
 package com.dtstack.flinkx.streaming.api.functions.sink;
 
+import com.dtstack.flinkx.util.ExceptionUtil;
+
 import org.apache.flink.annotation.PublicEvolving;
 import org.apache.flink.api.common.ExecutionConfig;
 import org.apache.flink.api.common.functions.RuntimeContext;
@@ -135,7 +137,7 @@ protected void cleanup() {
                 ((CleanupWhenUnsuccessful) format).tryCleanupOnError();
             }
         } catch (Throwable t) {
-            LOG.error("Cleanup on error failed.", t);
+            LOG.error("Cleanup on error failed. {}", ExceptionUtil.getErrorMessage(t));
         }
     }
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/streaming/api/functions/sink/DtOutputFormatSinkFunction.java
Patch:
@@ -17,6 +17,8 @@
 
 package com.dtstack.flinkx.streaming.api.functions.sink;
 
+import com.dtstack.flinkx.util.ExceptionUtil;
+
 import org.apache.flink.annotation.PublicEvolving;
 import org.apache.flink.api.common.ExecutionConfig;
 import org.apache.flink.api.common.functions.RuntimeContext;
@@ -135,7 +137,7 @@ protected void cleanup() {
                 ((CleanupWhenUnsuccessful) format).tryCleanupOnError();
             }
         } catch (Throwable t) {
-            LOG.error("Cleanup on error failed.", t);
+            LOG.error("Cleanup on error failed. {}", ExceptionUtil.getErrorMessage(t));
         }
     }
 

File: flinkx-connectors/flinkx-connector-cassandra/src/main/java/com/dtstack/flinkx/connector/cassandra/sink/CassandraSinkFactory.java
Patch:
@@ -65,11 +65,9 @@ public DataStreamSink<RowData> createSink(DataStream<RowData> dataSet) {
 
         builder.setSinkConf(sinkConf);
         List<FieldConf> fieldConfList = sinkConf.getColumn();
-        List<String> columnNameList = new ArrayList<>();
-        fieldConfList.forEach(fieldConf -> columnNameList.add(fieldConf.getName()));
 
         final RowType rowType = TableUtil.createRowType(fieldConfList, getRawTypeConverter());
-        builder.setRowConverter(new CassandraColumnConverter(rowType, columnNameList));
+        builder.setRowConverter(new CassandraColumnConverter(rowType, fieldConfList));
 
         return createOutput(dataSet, builder.finish());
     }

File: flinkx-connectors/flinkx-connector-clickhouse/src/main/java/com/dtstack/flinkx/connector/clickhouse/source/ClickhouseInputFormat.java
Patch:
@@ -41,7 +41,7 @@ public void openInternal(InputSplit inputSplit) {
         super.openInternal(inputSplit);
         RowType rowType =
                 TableUtil.createRowType(columnNameList, columnTypeList, ClickhouseRawTypeConverter::apply);
-        setRowConverter(rowConverter ==null ? jdbcDialect.getColumnConverter(rowType, jdbcConf) : rowConverter);
+        setRowConverter(rowConverter == null ? jdbcDialect.getColumnConverter(rowType, jdbcConf) : rowConverter);
     }
 
     @Override

File: flinkx-connectors/flinkx-connector-stream/src/main/java/com/dtstack/flinkx/connector/stream/converter/StreamColumnConverter.java
Patch:
@@ -75,7 +75,7 @@ protected IDeserializationConverter createInternalConverter(String type) {
                 return val -> new BooleanColumn(JMockData.mock(boolean.class));
             case "TINYINT":
             case "BYTE":
-                return val -> new BigDecimalColumn(JMockData.mock(byte.class));
+                return val -> new ByteColumn(JMockData.mock(byte.class));
             case "CHAR":
             case "CHARACTER":
                 return val -> new StringColumn(JMockData.mock(char.class).toString());

File: flinkx-local-test/src/main/java/com/dtstack/flinkx/local/test/LocalTest.java
Patch:
@@ -46,7 +46,7 @@ public static void main(String[] args) throws Exception {
 //        confProperties.setProperty("flink.checkpoint.interval", "10000");
         String userDir = System.getProperty("user.dir");
 
-        String jobPath = "/Users/wtz/IdeaProjects/flinkxThree/flinkx-examples/sql/kudu/kudu_source_stream.sql";
+        String jobPath = userDir + "/flinkx-local-test/src/main/demo/json/socket/socket_stream.json";
         String flinkxPluginPath = userDir + "/flinkxplugins";
 
         // 任务配置参数

File: flinkx-connectors/flinkx-connector-clickhouse/src/main/java/com/dtstack/flinkx/connector/clickhouse/source/ClickhouseInputFormat.java
Patch:
@@ -41,7 +41,7 @@ public void openInternal(InputSplit inputSplit) {
         super.openInternal(inputSplit);
         RowType rowType =
                 TableUtil.createRowType(columnNameList, columnTypeList, ClickhouseRawTypeConverter::apply);
-        setRowConverter(rowConverter ==null ? jdbcDialect.getColumnConverter(rowType) : rowConverter);
+        setRowConverter(rowConverter ==null ? jdbcDialect.getColumnConverter(rowType, jdbcConf) : rowConverter);
     }
 
     @Override

File: flinkx-connectors/flinkx-connector-db2/src/main/java/com/dtstack/flinkx/connector/db2/sink/Db2OutputFormat.java
Patch:
@@ -39,7 +39,7 @@ public class Db2OutputFormat extends JdbcOutputFormat {
     protected void openInternal(int taskNumber, int numTasks) {
         super.openInternal(taskNumber, numTasks);
         RowType rowType = TableUtil.createRowType(columnNameList, columnTypeList, Db2RawTypeConverter::apply);
-        setRowConverter(jdbcDialect.getColumnConverter(rowType));
+        setRowConverter(rowConverter ==null ? jdbcDialect.getColumnConverter(rowType) : rowConverter);
     }
 
     @Override

File: flinkx-connectors/flinkx-connector-db2/src/main/java/com/dtstack/flinkx/connector/db2/source/Db2InputFormat.java
Patch:
@@ -40,7 +40,7 @@ public class Db2InputFormat extends JdbcInputFormat {
     public void openInternal(InputSplit inputSplit) {
         super.openInternal(inputSplit);
         RowType rowType = TableUtil.createRowType(columnNameList, columnTypeList, Db2RawTypeConverter::apply);
-        setRowConverter(rowConverter ==null ? jdbcDialect.getColumnConverter(rowType) : rowConverter);
+        setRowConverter(rowConverter ==null ? jdbcDialect.getColumnConverter(rowType, jdbcConf) : rowConverter);
     }
 
     @Override

File: flinkx-connectors/flinkx-connector-dm/src/main/java/com/dtstack/flinkx/connector/dm/source/DmInputFormat.java
Patch:
@@ -34,7 +34,7 @@ public class DmInputFormat extends JdbcInputFormat {
     public void openInternal(InputSplit inputSplit) {
         super.openInternal(inputSplit);
         RowType rowType = TableUtil.createRowType(columnNameList, columnTypeList, DmRawTypeConverter::apply);
-        setRowConverter(rowConverter ==null ? jdbcDialect.getColumnConverter(rowType) : rowConverter);
+        setRowConverter(rowConverter ==null ? jdbcDialect.getColumnConverter(rowType, jdbcConf) : rowConverter);
     }
 
 }

File: flinkx-connectors/flinkx-connector-gbase/src/main/java/com/dtstack/flinkx/connector/gbase/source/GBaseInputFormat.java
Patch:
@@ -35,6 +35,6 @@ public class GBaseInputFormat extends JdbcInputFormat {
     public void openInternal(InputSplit inputSplit) {
         super.openInternal(inputSplit);
         RowType rowType = TableUtil.createRowType(columnNameList, columnTypeList, GBaseRawTypeConverter::apply);
-        setRowConverter(rowConverter ==null ? jdbcDialect.getColumnConverter(rowType) : rowConverter);
+        setRowConverter(rowConverter ==null ? jdbcDialect.getColumnConverter(rowType, jdbcConf) : rowConverter);
     }
 }

File: flinkx-connectors/flinkx-connector-greenplum/src/main/java/com/dtstack/flinkx/connector/greenplum/source/GreenplumInputFormat.java
Patch:
@@ -37,6 +37,6 @@ public void openInternal(InputSplit inputSplit) {
         super.openInternal(inputSplit);
         RowType rowType =
                 TableUtil.createRowType(columnNameList, columnTypeList, GreenplumRawTypeConverter::apply);
-        setRowConverter(rowConverter ==null ? jdbcDialect.getColumnConverter(rowType) : rowConverter);
+        setRowConverter(rowConverter ==null ? jdbcDialect.getColumnConverter(rowType, jdbcConf) : rowConverter);
     }
 }

File: flinkx-connectors/flinkx-connector-kingbase/src/main/java/com/dtstack/flinkx/connector/kingbase/source/KingbaseInputFormat.java
Patch:
@@ -45,7 +45,7 @@ public void openInternal(InputSplit inputSplit) {
         RowType rowType =
                 TableUtil.createRowType(
                         columnNameList, columnTypeList, KingbaseRawTypeConverter::apply);
-        setRowConverter(rowConverter ==null ? jdbcDialect.getColumnConverter(rowType) : rowConverter);
+        setRowConverter(rowConverter ==null ? jdbcDialect.getColumnConverter(rowType, jdbcConf) : rowConverter);
     }
 
     @Override

File: flinkx-connectors/flinkx-connector-mysql/src/main/java/com/dtstack/flinkx/connector/mysql/source/MysqlInputFormat.java
Patch:
@@ -37,6 +37,6 @@ public class MysqlInputFormat extends JdbcInputFormat {
     public void openInternal(InputSplit inputSplit) {
         super.openInternal(inputSplit);
         RowType rowType = TableUtil.createRowType(columnNameList, columnTypeList, MysqlRawTypeConverter::apply);
-        setRowConverter(rowConverter ==null ? jdbcDialect.getColumnConverter(rowType) : rowConverter);
+        setRowConverter(rowConverter ==null ? jdbcDialect.getColumnConverter(rowType, jdbcConf) : rowConverter);
     }
 }

File: flinkx-connectors/flinkx-connector-oracle/src/main/java/com/dtstack/flinkx/connector/oracle/source/OracleInputFormat.java
Patch:
@@ -38,6 +38,6 @@ public void openInternal(InputSplit inputSplit) {
         RowType rowType =
                 TableUtil.createRowType(
                         columnNameList, columnTypeList, OracleRawTypeConverter::apply);
-        setRowConverter(rowConverter ==null ? jdbcDialect.getColumnConverter(rowType) : rowConverter);
+        setRowConverter(rowConverter ==null ? jdbcDialect.getColumnConverter(rowType, jdbcConf) : rowConverter);
     }
 }

File: flinkx-connectors/flinkx-connector-postgresql/src/main/java/com/dtstack/flinkx/connector/postgresql/source/PostgresqlInputFormat.java
Patch:
@@ -37,6 +37,6 @@ public void openInternal(InputSplit inputSplit) {
         super.openInternal(inputSplit);
         RowType rowType =
                 TableUtil.createRowType(columnNameList, columnTypeList, PostgresqlRawTypeConverter::apply);
-        setRowConverter(rowConverter ==null ? jdbcDialect.getColumnConverter(rowType) : rowConverter);
+        setRowConverter(rowConverter ==null ? jdbcDialect.getColumnConverter(rowType, jdbcConf) : rowConverter);
     }
 }

File: flinkx-connectors/flinkx-connector-saphana/src/main/java/com/dtstack/flinkx/connector/saphana/source/SaphanaInputFormat.java
Patch:
@@ -43,6 +43,6 @@ public void openInternal(InputSplit inputSplit) {
                 TableUtil.createRowType(
                         columnNameList, columnTypeList, SaphanaRawTypeConverter::apply);
         setRowConverter(
-                rowConverter == null ? jdbcDialect.getColumnConverter(rowType) : rowConverter);
+                rowConverter == null ? jdbcDialect.getColumnConverter(rowType, jdbcConf) : rowConverter);
     }
 }

File: flinkx-connectors/flinkx-connector-sqlserver/src/main/java/com/dtstack/flinkx/connector/sqlserver/source/SqlserverInputFormat.java
Patch:
@@ -50,7 +50,7 @@ public void openInternal(InputSplit inputSplit) {
         RowType rowType =
             TableUtil.createRowType(
                 columnNameList, columnTypeList, SqlserverRawTypeConverter::apply);
-        setRowConverter(rowConverter ==null ? jdbcDialect.getColumnConverter(rowType) : rowConverter);
+        setRowConverter(rowConverter ==null ? jdbcDialect.getColumnConverter(rowType, jdbcConf) : rowConverter);
 
     }
 

File: flinkx-connectors/flinkx-connector-hive/src/main/java/com/dtstack/flinkx/connector/hive/sink/HiveOutputFormat.java
Patch:
@@ -205,7 +205,7 @@ public synchronized FormatState getFormatState() throws Exception {
         //set metric after preCommit
         snapshotWriteCounter.add(rowsOfCurrentTransaction);
         rowsOfCurrentTransaction = 0;
-        formatState.setNumberWrite(snapshotWriteCounter.getLocalValue());
+        formatState.setNumberWrite(numWriteCounter.getLocalValue());
         formatState.setMetric(outputMetric.getMetricCounters());
         formatState.setState(new HiveFormatState(formatStateMap));
         flushEnable.compareAndSet(true, false);

File: flinkx-connectors/flinkx-connector-kafka/src/main/java/com/dtstack/flinkx/connector/kafka/sink/DynamicKafkaSerializationSchema.java
Patch:
@@ -381,7 +381,6 @@ private void initRestoreInfo() {
             conversionErrCounter.add(formatState.getMetricValue(Metrics.NUM_CONVERSION_ERRORS));
             otherErrCounter.add(formatState.getMetricValue(Metrics.NUM_OTHER_ERRORS));
 
-            //use snapshot write count
             numWriteCounter.add(formatState.getMetricValue(Metrics.NUM_WRITES));
 
             snapshotWriteCounter.add(formatState.getMetricValue(Metrics.SNAPSHOT_WRITES));

File: flinkx-connectors/flinkx-connector-redis/src/main/java/com/dtstack/flinkx/connector/redis/connection/RedisSyncClient.java
Patch:
@@ -19,6 +19,7 @@
 package com.dtstack.flinkx.connector.redis.connection;
 
 import com.dtstack.flinkx.connector.redis.conf.RedisConf;
+import com.dtstack.flinkx.util.ExceptionUtil;
 import org.apache.commons.lang3.StringUtils;
 import org.apache.commons.pool2.impl.GenericObjectPoolConfig;
 import org.slf4j.Logger;
@@ -146,7 +147,7 @@ public JedisCommands getJedis() {
             } catch (IllegalArgumentException e) {
                 throw e;
             } catch (Exception e) {
-                LOG.error("connect failed:{} , sleep 3 seconds reconnect", e.getCause().getMessage());
+                LOG.error("connect failed:{} , sleep 3 seconds reconnect", ExceptionUtil.getErrorMessage(e));
                 try {
                     TimeUnit.SECONDS.sleep(3);
                 } catch (InterruptedException interruptedException) {
@@ -175,7 +176,7 @@ public void close(JedisCommands jedis) {
                 pool.close();
             }
         } catch (Exception e) {
-            LOG.error(e.getMessage());
+            LOG.error(ExceptionUtil.getErrorMessage(e));
         }
     }
 

File: flinkx-connectors/flinkx-connector-redis/src/main/java/com/dtstack/flinkx/connector/redis/connection/RedisSyncClient.java
Patch:
@@ -19,6 +19,7 @@
 package com.dtstack.flinkx.connector.redis.connection;
 
 import com.dtstack.flinkx.connector.redis.conf.RedisConf;
+import com.dtstack.flinkx.util.ExceptionUtil;
 import org.apache.commons.lang3.StringUtils;
 import org.apache.commons.pool2.impl.GenericObjectPoolConfig;
 import org.slf4j.Logger;
@@ -146,7 +147,7 @@ public JedisCommands getJedis() {
             } catch (IllegalArgumentException e) {
                 throw e;
             } catch (Exception e) {
-                LOG.error("connect failed:{} , sleep 3 seconds reconnect", e.getCause().getMessage());
+                LOG.error("connect failed:{} , sleep 3 seconds reconnect", ExceptionUtil.getErrorMessage(e));
                 try {
                     TimeUnit.SECONDS.sleep(3);
                 } catch (InterruptedException interruptedException) {
@@ -175,7 +176,7 @@ public void close(JedisCommands jedis) {
                 pool.close();
             }
         } catch (Exception e) {
-            LOG.error(e.getMessage());
+            LOG.error(ExceptionUtil.getErrorMessage(e));
         }
     }
 

File: flinkx-local-test/src/main/java/com/dtstack/flinkx/local/test/LocalTest.java
Patch:
@@ -31,7 +31,6 @@
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Properties;
-import java.util.concurrent.SynchronousQueue;
 
 /**
  * @author jiangbo

File: flinkx-connectors/flinkx-connector-stream/src/main/java/com/dtstack/flinkx/connector/stream/conf/StreamConf.java
Patch:
@@ -36,7 +36,7 @@ public class StreamConf extends FlinkxCommonConf {
     //writer
     private boolean print = false;
 
-    private long permitsPerSecond = 10;
+    private long permitsPerSecond = 0;
 
     public List<Long> getSliceRecordCount() {
         return sliceRecordCount;

File: flinkx-connectors/flinkx-connector-stream/src/main/java/com/dtstack/flinkx/connector/stream/options/StreamOptions.java
Patch:
@@ -52,7 +52,7 @@ public class StreamOptions {
     public static final ConfigOption<Long> ROWS_PER_SECOND =
             key("rows-per-second")
                     .longType()
-                    .defaultValue(10L)
+                    .defaultValue(0L)
                     .withDescription(
                             "rows-per-second.");
 

File: flinkx-local-test/src/main/java/com/dtstack/flinkx/local/test/LocalTest.java
Patch:
@@ -45,8 +45,7 @@ public static void main(String[] args) throws Exception {
 //        confProperties.setProperty("flink.checkpoint.interval", "10000");
         String userDir = System.getProperty("user.dir");
 
-        String jobPath = userDir + "/flinkx-local-test/src/main/demo/json/db2/db2_db2_realtime.json";
-        //String jobPath = userDir + "/flinkx-local-test/src/main/demo/sql/db2/db2_source_realtime.sql";
+        String jobPath = userDir + "/flinkx-local-test/src/main/demo/flinksql.sql";
         String flinkxPluginPath = userDir + "/flinkxplugins";
 
         // 任务配置参数

File: flinkx-core/src/main/java/com/dtstack/flinkx/security/KerberosConfig.java
Patch:
@@ -50,8 +50,6 @@ public KerberosConfig(String principal, String keytab, String krb5conf) {
         judgeAndSetKrbEnabled();
     }
 
-    public KerberosConfig() {}
-
     public String getPrincipal() {
         return principal;
     }

File: flinkx-connectors/flinkx-connector-hdfs/src/main/java/com/dtstack/flinkx/connector/hdfs/conf/HdfsConf.java
Patch:
@@ -35,9 +35,9 @@ public class HdfsConf extends BaseFileConf {
     private String defaultFS;
     private String fileType;
     /** hadoop高可用相关配置 **/
-    private Map<String, Object> hadoopConfig = new HashMap<>(2);
+    private Map<String, Object> hadoopConfig = new HashMap<>(16);
     private String filterRegex = "";
-    private String fieldDelimiter;
+    private String fieldDelimiter = "\001";
     private int rowGroupSize = ParquetWriter.DEFAULT_BLOCK_SIZE;
     private boolean enableDictionary = true;
     private List<String> fullColumnName;

File: flinkx-connectors/flinkx-connector-hdfs/src/main/java/com/dtstack/flinkx/connector/hdfs/options/HdfsOptions.java
Patch:
@@ -53,7 +53,7 @@ public class HdfsOptions extends BaseFileOptions {
     public static final ConfigOption<String> FIELD_DELIMITER =
             ConfigOptions.key("fieldDelimiter")
                     .stringType()
-                    .noDefaultValue()
+                    .defaultValue("\001")
                     .withDescription("The separator of the field when fileType is text");
 
     public static final ConfigOption<Boolean> ENABLE_DICTIONARY =

File: flinkx-connectors/flinkx-connector-stream/src/main/java/com/dtstack/flinkx/connector/stream/sink/StreamOutputFormat.java
Patch:
@@ -69,9 +69,6 @@ protected void writeMultipleRecordsInternal() throws Exception{
 
     @Override
     protected void preCommit() {
-        if (lastRow != null) {
-            TablePrintUtil.printTable(lastRow, getFieldNames(lastRow));
-        }
     }
 
     public String[] getFieldNames(RowData rowData) {

File: flinkx-local-test/src/main/java/com/dtstack/flinkx/local/test/LocalTest.java
Patch:
@@ -40,11 +40,13 @@ public class LocalTest {
     public static Logger LOG = LoggerFactory.getLogger(LocalTest.class);
 
     public static void main(String[] args) throws Exception {
+        LOG.warn("-----");
         Properties confProperties = new Properties();
 //        confProperties.setProperty("flink.checkpoint.interval", "10000");
         String userDir = System.getProperty("user.dir");
 
-        String jobPath = userDir + "/flinkx-local-test/src/main/demo/json/db2/db2_db2_realtime.json";
+        //String jobPath = userDir + "/flinkx-local-test/src/main/demo/json/db2/db2_db2_batch.json";
+        String jobPath = userDir + "/flinkx-local-test/src/main/demo/sql/db2/db2_lookup.sql";
         String flinkxPluginPath = userDir + "/flinkxplugins";
 
         // 任务配置参数

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/JsonUtil.java
Patch:
@@ -80,7 +80,7 @@ public static String toPrintJson(Object obj) {
         try {
             Map<String, Object> result = objectMapper.readValue(objectMapper.writeValueAsString(obj), HashMap.class);
             MapUtil.replaceAllElement(result, Lists.newArrayList("pwd", "password"), "******");
-            return objectMapper.writerWithDefaultPrettyPrinter().writeValueAsString(obj);
+            return objectMapper.writerWithDefaultPrettyPrinter().writeValueAsString(result);
         }catch (Exception e){
             throw new RuntimeException("error parse [" + obj + "] to json", e);
         }

File: flinkx-core/src/main/java/com/dtstack/flinkx/sink/SinkFactory.java
Patch:
@@ -18,20 +18,18 @@
 
 package com.dtstack.flinkx.sink;
 
-import com.dtstack.flinkx.converter.RawTypeConvertible;
-
 import org.apache.flink.api.common.io.OutputFormat;
 import org.apache.flink.api.common.typeinfo.TypeInformation;
 import org.apache.flink.streaming.api.datastream.DataStream;
 import org.apache.flink.streaming.api.datastream.DataStreamSink;
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.util.Preconditions;
 
-import com.dtstack.flinkx.RawTypeConvertible;
 import com.dtstack.flinkx.conf.FieldConf;
 import com.dtstack.flinkx.conf.FlinkxCommonConf;
 import com.dtstack.flinkx.conf.SpeedConf;
 import com.dtstack.flinkx.conf.SyncConf;
+import com.dtstack.flinkx.converter.RawTypeConvertible;
 import com.dtstack.flinkx.streaming.api.functions.sink.DtOutputFormatSinkFunction;
 import com.dtstack.flinkx.util.PropertiesUtil;
 import com.dtstack.flinkx.util.TableUtil;

File: flinkx-connectors/flinkx-connector-oracle/src/main/java/com/dtstack/flinkx/connector/oracle/sink/OracleOutputFormat.java
Patch:
@@ -18,12 +18,12 @@
 
 package com.dtstack.flinkx.connector.oracle.sink;
 
-import org.apache.flink.table.types.logical.RowType;
-
 import com.dtstack.flinkx.connector.jdbc.sink.JdbcOutputFormat;
 import com.dtstack.flinkx.connector.oracle.converter.OracleRawTypeConverter;
 import com.dtstack.flinkx.util.TableUtil;
 
+import org.apache.flink.table.types.logical.RowType;
+
 /**
  * company www.dtstack.com
  *

File: flinkx-connectors/flinkx-connector-oracle/src/main/java/com/dtstack/flinkx/connector/oracle/source/OracleInputFormat.java
Patch:
@@ -18,13 +18,13 @@
 
 package com.dtstack.flinkx.connector.oracle.source;
 
-import org.apache.flink.core.io.InputSplit;
-import org.apache.flink.table.types.logical.RowType;
-
 import com.dtstack.flinkx.connector.jdbc.source.JdbcInputFormat;
 import com.dtstack.flinkx.connector.oracle.converter.OracleRawTypeConverter;
 import com.dtstack.flinkx.util.TableUtil;
 
+import org.apache.flink.core.io.InputSplit;
+import org.apache.flink.table.types.logical.RowType;
+
 /**
  * company www.dtstack.com
  *

File: flinkx-connectors/flinkx-connector-oracle/src/main/java/com/dtstack/flinkx/connector/oracle/source/OracleSourceFactory.java
Patch:
@@ -16,18 +16,17 @@
  * limitations under the License.
  */
 
-
 package com.dtstack.flinkx.connector.oracle.source;
 
-import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
-
 import com.dtstack.flinkx.conf.SyncConf;
 import com.dtstack.flinkx.connector.jdbc.source.JdbcInputFormatBuilder;
 import com.dtstack.flinkx.connector.jdbc.source.JdbcSourceFactory;
 import com.dtstack.flinkx.connector.oracle.OracleDialect;
 import com.dtstack.flinkx.connector.oracle.converter.OracleRawTypeConverter;
 import com.dtstack.flinkx.converter.RawTypeConverter;
 
+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
+
 /**
  * company www.dtstack.com
  *

File: flinkx-core/src/main/java/com/dtstack/flinkx/converter/IDeserializationConverter.java
Patch:
@@ -29,5 +29,5 @@ public interface IDeserializationConverter<T, E> extends Serializable {
      * @return
      * @throws Exception
      */
-    E deserialize(T field);
+    E deserialize(T field) throws Exception;
 }

File: flinkx-connectors/flinkx-connector-oracle/src/main/java/com/dtstack/flinkx/connector/oracle/converter/BlobType.java
Patch:
@@ -34,7 +34,7 @@ public BlobType(boolean isNullable, LogicalTypeRoot typeRoot) {
 
     @Override
     public String asSerializableString() {
-        return "Clob-9953";
+        return "Oracle-Blob";
     }
 
     @Override

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/statement/FieldNamedPreparedStatement.java
Patch:
@@ -263,5 +263,4 @@ static FieldNamedPreparedStatement prepareStatement(
      */
     void close() throws SQLException;
 
-    public PreparedStatement getPreparedStatement();
 }

File: flinkx-core/src/main/java/com/dtstack/flinkx/converter/IDeserializationConverter.java
Patch:
@@ -29,5 +29,5 @@ public interface IDeserializationConverter<T, E> extends Serializable {
      * @return
      * @throws Exception
      */
-    E deserialize(T field);
+    E deserialize(T field) throws Exception;
 }

File: flinkx-core/src/main/java/com/dtstack/flinkx/converter/IDeserializationConverter.java
Patch:
@@ -29,5 +29,5 @@ public interface IDeserializationConverter<T, E> extends Serializable {
      * @return
      * @throws Exception
      */
-    E deserialize(T field);
+    E deserialize(T field) throws Exception;
 }

File: flinkx-connectors/flinkx-connector-hive/src/main/java/com/dtstack/flinkx/connector/hive/sink/HiveOutputFormat.java
Patch:
@@ -379,7 +379,9 @@ private SimpleDateFormat getPartitionFormat() {
             default:
                 throw new UnsupportedOperationException("partitionEnum = " + hiveConf.getPartitionType() + " is undefined!");
         }
-        format.setTimeZone(TimeZone.getTimeZone("GMT+8"));
+        TimeZone timeZone = TimeZone.getDefault();
+        LOG.info("timeZone = {}", timeZone);
+        format.setTimeZone(timeZone);
         return format;
     }
 

File: flinkx-connectors/flinkx-connector-mysql/src/main/java/com/dtstack/flinkx/connector/mysql/converter/MysqlRawTypeConverter.java
Patch:
@@ -51,7 +51,7 @@ public static DataType apply(String type) {
             case "TIME":
                 return DataTypes.TIME();
             case "YEAR":
-                return DataTypes.DATE();
+                return DataTypes.INTERVAL(DataTypes.YEAR());
             case "TIMESTAMP":
             case "DATETIME":
                 return DataTypes.TIMESTAMP();

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/sink/JdbcSinkFactory.java
Patch:
@@ -80,7 +80,7 @@ public DataStreamSink<RowData> createSink(DataStream<RowData> dataSet) {
         // 同步任务使用transform
         if (!useAbstractBaseColumn){
             final RowType rowType = TableUtil.createRowType(jdbcConf.getColumn(), getRawTypeConverter());
-            rowConverter = new JdbcRowConverter(rowType);
+            rowConverter = jdbcDialect.getRowConverter(rowType);
         }
         builder.setRowConverter(rowConverter);
 

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/source/JdbcSourceFactory.java
Patch:
@@ -120,7 +120,7 @@ public DataStream<RowData> createSource() {
             });
 
             final RowType rowType = TableUtil.createRowType(fieldList, getRawTypeConverter());
-            rowConverter = new JdbcRowConverter(rowType);
+            rowConverter = jdbcDialect.getRowConverter(rowType);
         }
         builder.setRowConverter(rowConverter);
 

File: flinkx-connectors/flinkx-connector-binlog/src/main/java/com/dtstack/flinkx/connector/binlog/converter/MysqlBinlogRawTypeConverter.java
Patch:
@@ -55,7 +55,7 @@ public static DataType apply(String type) {
                 return DataTypes.FLOAT();
             case "DECIMAL":
             case "NUMERIC":
-                return DataTypes.DECIMAL(1, 0);
+                return DataTypes.DECIMAL(38, 18);
             case "DOUBLE":
                 return DataTypes.DOUBLE();
             case "CHAR":

File: flinkx-connectors/flinkx-connector-clickhouse/src/main/java/com/dtstack/flinkx/connector/clickhouse/sink/ClickhouseOutputFormat.java
Patch:
@@ -43,7 +43,7 @@ protected void openInternal(int taskNumber, int numTasks) {
         super.openInternal(taskNumber, numTasks);
         RowType rowType =
                 TableUtil.createRowType(columnNameList, columnTypeList, ClickhouseRawTypeConverter::apply);
-        setRowConverter(jdbcDialect.getColumnConverter(rowType));
+        setRowConverter(rowConverter ==null ? jdbcDialect.getColumnConverter(rowType) : rowConverter);
     }
 
     @Override

File: flinkx-connectors/flinkx-connector-clickhouse/src/main/java/com/dtstack/flinkx/connector/clickhouse/source/ClickhouseInputFormat.java
Patch:
@@ -41,7 +41,7 @@ public void openInternal(InputSplit inputSplit) {
         super.openInternal(inputSplit);
         RowType rowType =
                 TableUtil.createRowType(columnNameList, columnTypeList, ClickhouseRawTypeConverter::apply);
-        setRowConverter(jdbcDialect.getColumnConverter(rowType));
+        setRowConverter(rowConverter ==null ? jdbcDialect.getColumnConverter(rowType) : rowConverter);
     }
 
     @Override

File: flinkx-connectors/flinkx-connector-emqx/src/main/java/com/dtstack/flinkx/connector/emqx/sink/EmqxSinkFactory.java
Patch:
@@ -56,6 +56,9 @@ public RawTypeConverter getRawTypeConverter() {
 
     @Override
     public DataStreamSink<RowData> createSink(DataStream<RowData> dataSet) {
+        if (!useAbstractBaseColumn) {
+            throw new UnsupportedOperationException("Emqx not support transform");
+        }
         EmqxOutputFormatBuilder builder = new EmqxOutputFormatBuilder();
         builder.setEmqxConf(emqxConf);
         builder.setConverter(new EmqxColumnConverter(emqxConf));

File: flinkx-connectors/flinkx-connector-gbase/src/main/java/com/dtstack/flinkx/connector/gbase/sink/GBaseOutputFormat.java
Patch:
@@ -34,6 +34,6 @@ public class GBaseOutputFormat extends JdbcOutputFormat {
     protected void openInternal(int taskNumber, int numTasks) {
         super.openInternal(taskNumber, numTasks);
         RowType rowType = TableUtil.createRowType(columnNameList, columnTypeList, GBaseRawTypeConverter::apply);
-        setRowConverter(jdbcDialect.getColumnConverter(rowType));
+        setRowConverter(rowConverter ==null ? jdbcDialect.getColumnConverter(rowType) : rowConverter);
     }
 }

File: flinkx-connectors/flinkx-connector-gbase/src/main/java/com/dtstack/flinkx/connector/gbase/source/GBaseInputFormat.java
Patch:
@@ -35,6 +35,6 @@ public class GBaseInputFormat extends JdbcInputFormat {
     public void openInternal(InputSplit inputSplit) {
         super.openInternal(inputSplit);
         RowType rowType = TableUtil.createRowType(columnNameList, columnTypeList, GBaseRawTypeConverter::apply);
-        setRowConverter(jdbcDialect.getColumnConverter(rowType));
+        setRowConverter(rowConverter ==null ? jdbcDialect.getColumnConverter(rowType) : rowConverter);
     }
 }

File: flinkx-connectors/flinkx-connector-greenplum/src/main/java/com/dtstack/flinkx/connector/greenplum/sink/GreenplumOutputFormat.java
Patch:
@@ -36,6 +36,6 @@ protected void openInternal(int taskNumber, int numTasks) {
         super.openInternal(taskNumber, numTasks);
         RowType rowType =
                 TableUtil.createRowType(columnNameList, columnTypeList, GreenplumRawTypeConverter::apply);
-        setRowConverter(jdbcDialect.getColumnConverter(rowType));
+        setRowConverter(rowConverter ==null ? jdbcDialect.getColumnConverter(rowType) : rowConverter);
     }
 }

File: flinkx-connectors/flinkx-connector-greenplum/src/main/java/com/dtstack/flinkx/connector/greenplum/source/GreenplumInputFormat.java
Patch:
@@ -37,6 +37,6 @@ public void openInternal(InputSplit inputSplit) {
         super.openInternal(inputSplit);
         RowType rowType =
                 TableUtil.createRowType(columnNameList, columnTypeList, GreenplumRawTypeConverter::apply);
-        setRowConverter(jdbcDialect.getColumnConverter(rowType));
+        setRowConverter(rowConverter ==null ? jdbcDialect.getColumnConverter(rowType) : rowConverter);
     }
 }

File: flinkx-connectors/flinkx-connector-kafka/src/main/java/com/dtstack/flinkx/connector/kafka/sink/KafkaSinkFactory.java
Patch:
@@ -86,6 +86,9 @@ protected DataStreamSink<RowData> createOutput(
 
     @Override
     public DataStreamSink<RowData> createSink(DataStream<RowData> dataSet) {
+        if (!useAbstractBaseColumn) {
+            throw new UnsupportedOperationException("kafka not support transform");
+        }
         return createOutput(dataSet, null);
     }
 

File: flinkx-connectors/flinkx-connector-kafka/src/main/java/com/dtstack/flinkx/connector/kafka/source/KafkaSourceFactory.java
Patch:
@@ -61,6 +61,9 @@ public KafkaSourceFactory(SyncConf config, StreamExecutionEnvironment env) {
 
     @Override
     public DataStream<RowData> createSource() {
+        if (!useAbstractBaseColumn) {
+            throw new UnsupportedOperationException("kafka not support transform");
+        }
         Properties props = new Properties();
         props.put("group.id", kafkaConf.getGroupId());
         props.putAll(kafkaConf.getConsumerSettings());

File: flinkx-connectors/flinkx-connector-kingbase/src/main/java/com/dtstack/flinkx/connector/kingbase/sink/KingbaseOutputFormat.java
Patch:
@@ -44,7 +44,7 @@ protected void openInternal(int taskNumber, int numTasks) {
         // create row converter
         RowType rowType =
                 TableUtil.createRowType(columnNameList, columnTypeList, KingbaseRawTypeConverter::apply);
-        setRowConverter(jdbcDialect.getColumnConverter(rowType));
+        setRowConverter(rowConverter ==null ? jdbcDialect.getColumnConverter(rowType) : rowConverter);
     }
 
     /**

File: flinkx-connectors/flinkx-connector-kingbase/src/main/java/com/dtstack/flinkx/connector/kingbase/source/KingbaseInputFormat.java
Patch:
@@ -44,7 +44,7 @@ public void openInternal(InputSplit inputSplit) {
         RowType rowType =
                 TableUtil.createRowType(
                         columnNameList, columnTypeList, KingbaseRawTypeConverter::apply);
-        setRowConverter(jdbcDialect.getColumnConverter(rowType));
+        setRowConverter(rowConverter ==null ? jdbcDialect.getColumnConverter(rowType) : rowConverter);
     }
 
     @Override

File: flinkx-connectors/flinkx-connector-mysql/src/main/java/com/dtstack/flinkx/connector/mysql/converter/MysqlRawTypeConverter.java
Patch:
@@ -21,6 +21,7 @@ public class MysqlRawTypeConverter {
      */
     public static DataType apply(String type) {
         switch (type.toUpperCase(Locale.ENGLISH)) {
+            case "BOOLEAN":
             case "BIT":
                 return DataTypes.BOOLEAN();
             case "TINYINT":
@@ -38,11 +39,12 @@ public static DataType apply(String type) {
                 return DataTypes.FLOAT();
             case "DECIMAL":
             case "NUMERIC":
-                return DataTypes.DECIMAL(1, 0);
+                return DataTypes.DECIMAL(38, 18);
             case "DOUBLE":
                 return DataTypes.DOUBLE();
             case "CHAR":
             case "VARCHAR":
+            case "STRING":
                 return DataTypes.STRING();
             case "DATE":
                 return DataTypes.DATE();

File: flinkx-connectors/flinkx-connector-mysql/src/main/java/com/dtstack/flinkx/connector/mysql/source/MysqlInputFormat.java
Patch:
@@ -36,6 +36,6 @@ public class MysqlInputFormat extends JdbcInputFormat {
     public void openInternal(InputSplit inputSplit) {
         super.openInternal(inputSplit);
         RowType rowType = TableUtil.createRowType(columnNameList, columnTypeList, MysqlRawTypeConverter::apply);
-        setRowConverter(jdbcDialect.getColumnConverter(rowType));
+        setRowConverter(rowConverter ==null ? jdbcDialect.getColumnConverter(rowType) : rowConverter);
     }
 }

File: flinkx-connectors/flinkx-connector-oracle/src/main/java/com/dtstack/flinkx/connector/oracle/converter/OracleRawTypeConverter.java
Patch:
@@ -67,7 +67,7 @@ public static DataType apply(String type) {
             case "NUMBER":
             case "DECIMAL":
             case "FLOAT":
-                return DataTypes.DECIMAL(1, 0);
+                return DataTypes.DECIMAL(38, 18);
             case "DATE":
                 return DataTypes.DATE();
             case "RAW":

File: flinkx-connectors/flinkx-connector-oracle/src/main/java/com/dtstack/flinkx/connector/oracle/sink/OracleOutputFormat.java
Patch:
@@ -37,6 +37,6 @@ protected void openInternal(int taskNumber, int numTasks) {
         RowType rowType =
                 TableUtil.createRowType(
                         columnNameList, columnTypeList, OracleRawTypeConverter::apply);
-        setRowConverter(jdbcDialect.getColumnConverter(rowType));
+        setRowConverter(rowConverter ==null ? jdbcDialect.getColumnConverter(rowType) : rowConverter);
     }
 }

File: flinkx-connectors/flinkx-connector-oracle/src/main/java/com/dtstack/flinkx/connector/oracle/source/OracleInputFormat.java
Patch:
@@ -38,6 +38,6 @@ public void openInternal(InputSplit inputSplit) {
         RowType rowType =
                 TableUtil.createRowType(
                         columnNameList, columnTypeList, OracleRawTypeConverter::apply);
-        setRowConverter(jdbcDialect.getColumnConverter(rowType));
+        setRowConverter(rowConverter ==null ? jdbcDialect.getColumnConverter(rowType) : rowConverter);
     }
 }

File: flinkx-connectors/flinkx-connector-postgresql/src/main/java/com/dtstack/flinkx/connector/postgresql/sink/PostgresqlOutputFormat.java
Patch:
@@ -36,6 +36,6 @@ protected void openInternal(int taskNumber, int numTasks) {
         super.openInternal(taskNumber, numTasks);
         RowType rowType =
                 TableUtil.createRowType(columnNameList, columnTypeList, PostgresqlRawTypeConverter::apply);
-        setRowConverter(jdbcDialect.getColumnConverter(rowType));
+        setRowConverter(rowConverter ==null ? jdbcDialect.getColumnConverter(rowType) : rowConverter);
     }
 }

File: flinkx-connectors/flinkx-connector-postgresql/src/main/java/com/dtstack/flinkx/connector/postgresql/source/PostgresqlInputFormat.java
Patch:
@@ -37,6 +37,6 @@ public void openInternal(InputSplit inputSplit) {
         super.openInternal(inputSplit);
         RowType rowType =
                 TableUtil.createRowType(columnNameList, columnTypeList, PostgresqlRawTypeConverter::apply);
-        setRowConverter(jdbcDialect.getColumnConverter(rowType));
+        setRowConverter(rowConverter ==null ? jdbcDialect.getColumnConverter(rowType) : rowConverter);
     }
 }

File: flinkx-connectors/flinkx-connector-sqlserver/src/main/java/com/dtstack/flinkx/connector/sqlserver/converter/SqlserverRawTypeConverter.java
Patch:
@@ -59,7 +59,7 @@ public static DataType apply(String type) throws UnsupportedTypeException {
                 return DataTypes.DOUBLE();
             case "DECIMAL":
             case "NUMERIC":
-                return DataTypes.DECIMAL(1, 0);
+                return DataTypes.DECIMAL(38, 18);
             case "CHAR":
             case "VARCHAR":
             case "VARCHAR(MAX)":

File: flinkx-connectors/flinkx-connector-sqlserver/src/main/java/com/dtstack/flinkx/connector/sqlserver/sink/SqlserverOutputFormat.java
Patch:
@@ -46,7 +46,7 @@ protected void openInternal(int taskNumber, int numTasks) {
         super.openInternal(taskNumber, numTasks);
         RowType rowType =
                 TableUtil.createRowType(columnNameList, columnTypeList, SqlserverRawTypeConverter::apply);
-        setRowConverter(jdbcDialect.getColumnConverter(rowType));
+        setRowConverter(rowConverter ==null ? jdbcDialect.getColumnConverter(rowType) : rowConverter);
 
         Statement statement = null;
         String sql = ((SqlServerDialect)jdbcDialect).getIdentityInsertOnSql(jdbcConf.getSchema(), jdbcConf.getTable());

File: flinkx-connectors/flinkx-connector-sqlserver/src/main/java/com/dtstack/flinkx/connector/sqlserver/source/SqlserverInputFormat.java
Patch:
@@ -44,7 +44,7 @@ public void openInternal(InputSplit inputSplit) {
         RowType rowType =
                 TableUtil.createRowType(
                         columnNameList, columnTypeList, SqlserverRawTypeConverter::apply);
-        setRowConverter(jdbcDialect.getColumnConverter(rowType));
+        setRowConverter(rowConverter ==null ? jdbcDialect.getColumnConverter(rowType) : rowConverter);
     }
 
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/security/KerberosConfig.java
Patch:
@@ -22,8 +22,9 @@
 import com.google.common.base.Strings;
 
 /**
- * Kerberos of certain connectors could be enabled, it should use or extends this class.
- * e.g. KuduInputFormat class can combine it.
+ * Kerberos of certain connectors could be enabled, it should use or extends this class. e.g.
+ * KuduInputFormat class can combine it.
+ *
  * @author Ada Wong
  * @program flinkx
  * @create 2021/06/15

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/lookup/JdbcAllTableFunction.java
Patch:
@@ -9,14 +9,14 @@
 import com.dtstack.flinkx.connector.jdbc.util.JdbcUtil;
 import com.dtstack.flinkx.lookup.AbstractAllTableFunction;
 import com.dtstack.flinkx.lookup.conf.LookupConf;
-import com.google.common.collect.Maps;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
 import java.sql.Connection;
 import java.sql.ResultSet;
 import java.sql.SQLException;
 import java.sql.Statement;
+import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 
@@ -91,7 +91,7 @@ private void queryAndFillData(
         ResultSet resultSet = statement.executeQuery(query);
 
         while (resultSet.next()) {
-            Map<String, Object> oneRow = Maps.newHashMap();
+            Map<String, Object> oneRow = new HashMap<>();
             // 防止一条数据有问题，后面数据无法加载
             try {
                 GenericRowData rowData = (GenericRowData) rowConverter.toInternal(resultSet);

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/lookup/JdbcLruTableFunction.java
Patch:
@@ -48,6 +48,7 @@
 import java.math.BigDecimal;
 import java.sql.Timestamp;
 import java.time.Instant;
+import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collection;
 import java.util.List;
@@ -324,10 +325,10 @@ private void handleQuery(
                     return;
                 }
 
-                List<JsonArray> cacheContent = Lists.newArrayList();
+                List<JsonArray> cacheContent = new ArrayList<>();
                 int resultSize = rs.result().getResults().size();
                 if (resultSize > 0) {
-                    List<RowData> rowList = Lists.newArrayList();
+                    List<RowData> rowList = new ArrayList<>();
 
                     for (JsonArray line : rs.result().getResults()) {
                         try {

File: flinkx-local-test/src/main/java/com/dtstack/flinkx/local/test/LocalTest.java
Patch:
@@ -42,7 +42,8 @@ public class LocalTest {
     public static void main(String[] args) throws Exception {
         Properties confProperties = new Properties();
         String userDir = System.getProperty("user.dir");
-        String jobPath = userDir + "/flinkx-local-test/src/main/demo/sql/oracle/oracle_oracle.sql";
+
+        String jobPath = userDir + "/flinkx-local-test/src/main/demo/flinksql.sql";
         String flinkxPluginPath = userDir + "/flinkxplugins";
 
         // 任务配置参数

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/options/JdbcSourceOptions.java
Patch:
@@ -80,7 +80,8 @@ public class JdbcSourceOptions {
             ConfigOptions.key("scan.query-timeout")
                     .intType()
                     .defaultValue(1)
-                    .withDescription("scan parallelism.");
+                    .withDescription(
+                            "The new query timeout limit in seconds; zero means there is no limit; Default value 1s");
 
     public static final ConfigOption<Integer> SCAN_FETCH_SIZE =
             ConfigOptions.key("scan.fetch-size")

File: flinkx-connectors/flinkx-connector-stream/src/main/java/com/dtstack/flinkx/connector/stream/sink/StreamOutputFormat.java
Patch:
@@ -40,7 +40,6 @@ public class StreamOutputFormat extends BaseRichOutputFormat {
 
     // streamSinkConf属性
     private StreamConf streamConf;
-    private GenericRowData lastRow;
 
     @Override
     protected void openInternal(int taskNumber, int numTasks) {
@@ -50,9 +49,8 @@ protected void openInternal(int taskNumber, int numTasks) {
     @Override
     protected void writeSingleRecordInternal(RowData rowData) {
         try {
-            GenericRowData genericRowData = new GenericRowData(rowData.getArity());
             @SuppressWarnings("unchecked")
-            GenericRowData row = (GenericRowData) rowConverter.toExternal(rowData, genericRowData);
+            RowData row = (RowData)rowConverter.toExternal(rowData, new GenericRowData(rowData.getArity()));
             if (streamConf.getPrint()) {
                 TablePrintUtil.printTable(row, getFieldNames(rowData));
             }

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/sink/JdbcSinkFactory.java
Patch:
@@ -69,7 +69,7 @@ public DataStreamSink<RowData> createSink(DataStream<RowData> dataSet) {
         builder.setJdbcConf(jdbcConf);
         builder.setJdbcDialect(jdbcDialect);
         builder.setBatchSize(jdbcConf.getBatchSize());
-        builder.setFlushIntervalMillse(jdbcConf.getFlushIntervalMills());
+        builder.setFlushIntervalMills(jdbcConf.getFlushIntervalMills());
         return createOutput(dataSet, builder.finish());
     }
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/outputformat/BaseRichOutputFormatBuilder.java
Patch:
@@ -43,7 +43,7 @@ public void setInitAccumulatorAndDirty(boolean initAccumulatorAndDirty) {
         this.format.initAccumulatorAndDirty = initAccumulatorAndDirty;
     }
 
-    public void setFlushIntervalMillse(long flushIntervalMills) {
+    public void setFlushIntervalMills(long flushIntervalMills) {
         format.setFlushIntervalMills(flushIntervalMills);
     }
 

File: flinkx-connectors/flinkx-connector-clickhouse/src/main/java/com/dtstack/flinkx/connector/clickhouse/sink/ClickhouseOutputFormat.java
Patch:
@@ -42,7 +42,7 @@ public class ClickhouseOutputFormat extends JdbcOutputFormat {
     protected void openInternal(int taskNumber, int numTasks) {
         super.openInternal(taskNumber, numTasks);
         RowType rowType =
-                TableUtil.createRowType(column, columnType, ClickhouseRawTypeConverter::apply);
+                TableUtil.createRowType(columnNameList, columnTypeList, ClickhouseRawTypeConverter::apply);
         setRowConverter(jdbcDialect.getColumnConverter(rowType));
     }
 

File: flinkx-connectors/flinkx-connector-gbase/src/main/java/com/dtstack/flinkx/connector/gbase/sink/GBaseOutputFormat.java
Patch:
@@ -33,7 +33,7 @@ public class GBaseOutputFormat extends JdbcOutputFormat {
     @Override
     protected void openInternal(int taskNumber, int numTasks) {
         super.openInternal(taskNumber, numTasks);
-        RowType rowType = TableUtil.createRowType(column, columnType, GBaseRawTypeConverter::apply);
+        RowType rowType = TableUtil.createRowType(columnNameList, columnTypeList, GBaseRawTypeConverter::apply);
         setRowConverter(jdbcDialect.getColumnConverter(rowType));
     }
 }

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/converter/JdbcRowConverter.java
Patch:
@@ -33,9 +33,9 @@
 
 import com.dtstack.flinkx.connector.jdbc.statement.FieldNamedPreparedStatement;
 import com.dtstack.flinkx.converter.AbstractRowConverter;
-import io.vertx.core.json.JsonArray;
 import com.dtstack.flinkx.converter.IDeserializationConverter;
 import com.dtstack.flinkx.converter.ISerializationConverter;
+import io.vertx.core.json.JsonArray;
 
 import java.math.BigDecimal;
 import java.math.BigInteger;

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/lookup/JdbcAllTableFunction.java
Patch:
@@ -1,13 +1,12 @@
 package com.dtstack.flinkx.connector.jdbc.lookup;
 
-import com.dtstack.flinkx.connector.jdbc.util.JdbcUtil;
-
 import org.apache.flink.annotation.Internal;
 import org.apache.flink.table.data.GenericRowData;
 import org.apache.flink.table.types.logical.RowType;
 
 import com.dtstack.flinkx.connector.jdbc.JdbcDialect;
 import com.dtstack.flinkx.connector.jdbc.conf.JdbcConf;
+import com.dtstack.flinkx.connector.jdbc.util.JdbcUtil;
 import com.dtstack.flinkx.lookup.AbstractAllTableFunction;
 import com.dtstack.flinkx.lookup.conf.LookupConf;
 import com.google.common.collect.Maps;

File: flinkx-connectors/flinkx-connector-mysql/src/main/java/com/dtstack/flinkx/connector/mysql/sink/MysqlOutputFormat.java
Patch:
@@ -37,7 +37,7 @@ public class MysqlOutputFormat extends JdbcOutputFormat {
     @Override
     protected void openInternal(int taskNumber, int numTasks) {
         super.openInternal(taskNumber, numTasks);
-        RowType rowType = TableUtil.createRowType(column, columnType, MysqlRawTypeConverter::apply);
+        RowType rowType = TableUtil.createRowType(columnNameList, columnTypeList, MysqlRawTypeConverter::apply);
         setRowConverter(jdbcDialect.getColumnConverter(rowType));
     }
 }

File: flinkx-connectors/flinkx-connector-oracle/src/main/java/com/dtstack/flinkx/connector/oracle/sink/OracleOutputFormat.java
Patch:
@@ -36,7 +36,7 @@ protected void openInternal(int taskNumber, int numTasks) {
         super.openInternal(taskNumber, numTasks);
         RowType rowType =
                 TableUtil.createRowType(
-                        column, columnType, OracleRawTypeConverter::apply);
+                        columnNameList, columnTypeList, OracleRawTypeConverter::apply);
         setRowConverter(jdbcDialect.getColumnConverter(rowType));
     }
 }

File: flinkx-connectors/flinkx-connector-postgresql/src/main/java/com/dtstack/flinkx/connector/postgresql/sink/PostgresqlOutputFormat.java
Patch:
@@ -35,7 +35,7 @@ public class PostgresqlOutputFormat extends JdbcOutputFormat {
     protected void openInternal(int taskNumber, int numTasks) {
         super.openInternal(taskNumber, numTasks);
         RowType rowType =
-                TableUtil.createRowType(column, columnType, PostgresqlRawTypeConverter::apply);
+                TableUtil.createRowType(columnNameList, columnTypeList, PostgresqlRawTypeConverter::apply);
         setRowConverter(jdbcDialect.getColumnConverter(rowType));
     }
 }

File: flinkx-connectors/flinkx-connector-sqlserver/src/main/java/com/dtstack/flinkx/connector/sqlserver/sink/SqlserverOutputFormat.java
Patch:
@@ -45,7 +45,7 @@ public class SqlserverOutputFormat extends JdbcOutputFormat {
     protected void openInternal(int taskNumber, int numTasks) {
         super.openInternal(taskNumber, numTasks);
         RowType rowType =
-                TableUtil.createRowType(column, columnType, SqlserverRawTypeConverter::apply);
+                TableUtil.createRowType(columnNameList, columnTypeList, SqlserverRawTypeConverter::apply);
         setRowConverter(jdbcDialect.getColumnConverter(rowType));
 
         Statement statement = null;

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/source/JdbcInputFormatBuilder.java
Patch:
@@ -67,6 +67,9 @@ protected void checkFormat() {
         if (StringUtils.isEmpty(conf.getSplitPk()) && format.getNumPartitions() > 1){
             sb.append("Must specify the split column when the channel is greater than 1;\n");
         }
+        if (conf.isPolling() && format.getNumPartitions() > 1){
+            sb.append("Interval polling task parallelism cannot be greater than 1;\n");
+        }
         if (conf.getFetchSize() > ConstantValue.MAX_BATCH_SIZE) {
             sb.append("The number of fetchSize must be less than [200000];\n");
         }

File: flinkx-core/src/main/java/org/apache/flink/table/factories/FactoryUtil.java
Patch:
@@ -251,9 +251,11 @@ public static DynamicTableSink createTableSink(
         } catch (Throwable t) {
             throw new ValidationException(
                     String.format(
-                            "Unable to create a sink for writing table '%s'.\n\n"
+                            "%s \n\n"
+                                    + "Unable to create a sink for writing table '%s'.\n\n"
                                     + "Table options are:\n\n"
                                     + "%s",
+                            t.getMessage(),
                             objectIdentifier.asSummaryString(),
                             catalogTable.getOptions().entrySet().stream()
                                     .map(e -> stringifyOption(e.getKey(), e.getValue()))

File: flinkx-connectors/flinkx-connector-gbase/src/main/java/com/dtstack/flinkx/connector/gbase/source/GBaseInputFormat.java
Patch:
@@ -34,7 +34,7 @@ public class GBaseInputFormat extends JdbcInputFormat {
     @Override
     public void openInternal(InputSplit inputSplit) {
         super.openInternal(inputSplit);
-        RowType rowType = TableUtil.createRowType(column, columnType, GBaseRawTypeConverter::apply);
+        RowType rowType = TableUtil.createRowType(columnNameList, columnTypeList, GBaseRawTypeConverter::apply);
         setRowConverter(jdbcDialect.getColumnConverter(rowType));
     }
 }

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/sink/JdbcOutputFormat.java
Patch:
@@ -93,8 +93,7 @@ protected void openInternal(int taskNumber, int numTasks) {
             //默认关闭事务自动提交，手动控制事务
             dbConn.setAutoCommit(false);
 
-            Pair<List<String>, List<String>> pair =
-                    JdbcUtil.getTableMetaData(jdbcConf.getSchema(), jdbcConf.getTable(), dbConn);
+            Pair<List<String>, List<String>> pair = JdbcUtil.getTableMetaData(jdbcConf.getSchema(), jdbcConf.getTable(), dbConn);
             List<String> fullColumn = pair.getLeft();
             List<String> fullColumnType = pair.getRight();
 

File: flinkx-connectors/flinkx-connector-oracle/src/main/java/com/dtstack/flinkx/connector/oracle/source/OracleInputFormat.java
Patch:
@@ -37,7 +37,7 @@ public void openInternal(InputSplit inputSplit) {
         super.openInternal(inputSplit);
         RowType rowType =
                 TableUtil.createRowType(
-                        column, columnType, OracleRawTypeConverter::apply);
+                        columnNameList, columnTypeList, OracleRawTypeConverter::apply);
         setRowConverter(jdbcDialect.getColumnConverter(rowType));
     }
 }

File: flinkx-connectors/flinkx-connector-postgresql/src/main/java/com/dtstack/flinkx/connector/postgresql/source/PostgresqlInputFormat.java
Patch:
@@ -36,7 +36,7 @@ public class PostgresqlInputFormat extends JdbcInputFormat {
     public void openInternal(InputSplit inputSplit) {
         super.openInternal(inputSplit);
         RowType rowType =
-                TableUtil.createRowType(column, columnType, PostgresqlRawTypeConverter::apply);
+                TableUtil.createRowType(columnNameList, columnTypeList, PostgresqlRawTypeConverter::apply);
         setRowConverter(jdbcDialect.getColumnConverter(rowType));
     }
 }

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/JsonUtil.java
Patch:
@@ -17,6 +17,7 @@
  */
 package com.dtstack.flinkx.util;
 
+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.annotation.JsonInclude;
 import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.core.JsonProcessingException;
 import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.DeserializationFeature;
 import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.MapperFeature;
@@ -34,7 +35,8 @@ public class JsonUtil {
 
     public static final ObjectMapper objectMapper = new ObjectMapper()
             .configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false)
-            .configure(MapperFeature.ACCEPT_CASE_INSENSITIVE_PROPERTIES, true);
+            .configure(MapperFeature.ACCEPT_CASE_INSENSITIVE_PROPERTIES, true)
+            .setSerializationInclusion(JsonInclude.Include.NON_NULL);
 
 
     /**

File: flinkx-connectors/flinkx-connector-oracle/src/main/java/com/dtstack/flinkx/connector/oracle/converter/OracleRowConverter.java
Patch:
@@ -56,8 +56,6 @@
 public class OracleRowConverter
         extends JdbcRowConverter {
 
-    private static final Logger LOG = LoggerFactory.getLogger(OracleColumnConverter.class);
-
     private static final long serialVersionUID = 1L;
 
     public OracleRowConverter(RowType rowType) {
@@ -109,7 +107,6 @@ protected IDeserializationConverter createInternalConverter(LogicalType type) {
                         try {
                             return TimestampData.fromTimestamp(((TIMESTAMP) val).timestampValue());
                         } catch (SQLException e) {
-                            LOG.error("this value is not correct,val [{}]\n{}", val, ExceptionUtil.getErrorMessage(e));
                             throw new UnsupportedTypeException(
                                     "Unsupported type:" + type + ",value:" + val);
                         }

File: flinkx-core/src/main/java/com/dtstack/flinkx/Main.java
Patch:
@@ -57,6 +57,7 @@
 import com.dtstack.flinkx.sink.SinkFactory;
 import com.dtstack.flinkx.source.SourceFactory;
 import com.dtstack.flinkx.util.DataSyncFactoryUtil;
+import com.dtstack.flinkx.util.ExceptionUtil;
 import com.dtstack.flinkx.util.MathUtil;
 import com.dtstack.flinkx.util.PluginUtil;
 import com.dtstack.flinkx.util.PrintUtil;
@@ -152,7 +153,7 @@ private static void exeSqlJob(
                 try {
                     PrintUtil.printResult(v.getAccumulators().get());
                 } catch (Exception e) {
-                    e.printStackTrace();
+                    LOG.error("error to execute sql job, e = {}", ExceptionUtil.getErrorMessage(e));
                 }
             });
         }

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/ExceptionUtil.java
Patch:
@@ -30,16 +30,15 @@
  */
 public class ExceptionUtil {
 
-    private static Logger logger = LoggerFactory.getLogger(ExceptionUtil.class);
+    private static final Logger logger = LoggerFactory.getLogger(ExceptionUtil.class);
 
     public static String getErrorMessage(Throwable e) {
         if (null == e) {
             return null;
         }
 
         try (StringWriter stringWriter = new StringWriter();
-             PrintWriter writer = new PrintWriter(stringWriter)) {
-            e.printStackTrace(writer);
+                PrintWriter writer = new PrintWriter(stringWriter)) {
             writer.flush();
             stringWriter.flush();
             StringBuffer buffer = stringWriter.getBuffer();

File: flinkx-connectors/flinkx-connector-stream/src/main/java/com/dtstack/flinkx/connector/stream/converter/StreamRawTypeConverter.java
Patch:
@@ -53,6 +53,7 @@ public static DataType apply(String type) throws UnsupportedTypeException {
                 return DataTypes.DECIMAL(38, 18);
 
             case "STRING":
+            case "VARCHAR":
             case "CHAR":
             case "CHARACTER":
                 return DataTypes.STRING();
@@ -69,7 +70,7 @@ public static DataType apply(String type) throws UnsupportedTypeException {
                 return DataTypes.BOOLEAN();
 
             default:
-                throw new UnsupportedTypeException(type);
+                return DataTypes.STRING();
         }
     }
 }

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/conf/JdbcConf.java
Patch:
@@ -369,7 +369,7 @@ public void setRestoreColumnType(String restoreColumnType) {
         this.restoreColumnType = restoreColumnType;
     }
 
-    public boolean isAllReplace() {
+    public boolean getAllReplace() {
         return allReplace;
     }
 

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/options/JdbcSinkOptions.java
Patch:
@@ -53,7 +53,7 @@ public class JdbcSinkOptions {
     public static final ConfigOption<Boolean> SINK_ALLREPLACE =
             ConfigOptions.key("sink.allReplace")
                     .booleanType()
-                    .defaultValue(true)
+                    .defaultValue(false)
                     .withDescription("the max retry times if writing records to database failed.");
 
     public static final ConfigOption<Integer> SINK_PARALLELISM =

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/sink/JdbcDynamicTableSink.java
Patch:
@@ -99,7 +99,6 @@ public SinkFunctionProvider getSinkRuntimeProvider(Context context) {
         jdbcConf.setColumn(columnList);
         jdbcConf.setMode((CollectionUtil.isNullOrEmpty(jdbcConf.getUpdateKey())) ? EWriteMode.INSERT
                 .name() : EWriteMode.UPDATE.name());
-        jdbcConf.setUpdateKey(jdbcConf.getUpdateKey());
 
         builder.setJdbcDialect(jdbcDialect);
         builder.setBatchSize(jdbcConf.getBatchSize());

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/sink/JdbcOutputFormat.java
Patch:
@@ -276,7 +276,7 @@ protected String prepareTemplates() {
                                     jdbcConf.getTable(),
                                     column.toArray(new String[0]),
                                     jdbcConf.getUpdateKey().toArray(new String[0]),
-                                    jdbcConf.isAllReplace())
+                                    jdbcConf.getAllReplace())
                             .get();
         } else {
             throw new IllegalArgumentException("Unknown write mode:" + jdbcConf.getMode());

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/source/JdbcInputFormat.java
Patch:
@@ -140,7 +140,6 @@ public boolean reachedEnd() {
                         //重新连接后还是不可用则认为数据库异常，任务失败
                         if(!dbConn.isValid(3)){
                             String message = String.format("cannot connect to %s, username = %s, please check %s is available.", jdbcConf.getJdbcUrl(), jdbcConf.getJdbcUrl(), jdbcDialect.dialectName());
-                            LOG.error(message);
                             throw new RuntimeException(message);
                         }
                     }
@@ -653,7 +652,6 @@ protected void analyzeMetaData() {
                     resultSet,
                     GsonUtil.GSON.toJson(columnType),
                     ExceptionUtil.getErrorMessage(e));
-            LOG.error(message);
             throw new RuntimeException(message);
         }
     }

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/table/JdbcDynamicTableFactory.java
Patch:
@@ -144,6 +144,7 @@ protected JdbcConf getSinkConnectionConf(ReadableConfig readableConfig, TableSch
         conf.setJdbcUrl(readableConfig.get(URL));
         conf.setTable(Arrays.asList(readableConfig.get(TABLE_NAME)));
         conf.setSchema(readableConfig.get(SCHEMA));
+        conf.setAllReplace(readableConfig.get(SINK_ALLREPLACE));
 
         jdbcConf.setUsername(readableConfig.get(USERNAME));
         jdbcConf.setPassword(readableConfig.get(PASSWORD));

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/source/JdbcDynamicTableSource.java
Patch:
@@ -83,8 +83,6 @@ public LookupRuntimeProvider getLookupRuntimeProvider(LookupContext context) {
         // 通过该参数得到类型转换器，将数据库中的字段转成对应的类型
         final RowType rowType = (RowType) physicalSchema.toRowDataType().getLogicalType();
 
-        jdbcConf.getProperties().setProperty("useCursorFetch", "true");
-
         if (lookupConf.getCache().equalsIgnoreCase(CacheType.LRU.toString())) {
             return ParallelAsyncTableFunctionProvider.of(
                     new JdbcLruTableFunction(

File: flinkx-connectors/flinkx-connector-oracle/src/main/java/com/dtstack/flinkx/connector/oracle/converter/OracleColumnConverter.java
Patch:
@@ -48,7 +48,6 @@
 public class OracleColumnConverter
         extends JdbcColumnConverter{
 
-
     public OracleColumnConverter(RowType rowType) {
         super(rowType);
     }

File: flinkx-connectors/flinkx-connector-oracle/src/main/java/com/dtstack/flinkx/connector/oracle/sink/OracleOutputFormat.java
Patch:
@@ -31,7 +31,6 @@
  */
 public class OracleOutputFormat extends JdbcOutputFormat {
 
-
     @Override
     protected void openInternal(int taskNumber, int numTasks) {
         super.openInternal(taskNumber, numTasks);
@@ -40,5 +39,4 @@ protected void openInternal(int taskNumber, int numTasks) {
                         column, columnType, OracleRawTypeConverter::apply);
         setRowConverter(jdbcDialect.getColumnConverter(rowType));
     }
-
 }

File: flinkx-connectors/flinkx-connector-oracle/src/main/java/com/dtstack/flinkx/connector/oracle/source/OracleInputFormat.java
Patch:
@@ -32,7 +32,6 @@
  */
 public class OracleInputFormat extends JdbcInputFormat {
 
-
     @Override
     public void openInternal(InputSplit inputSplit) {
         super.openInternal(inputSplit);
@@ -41,6 +40,4 @@ public void openInternal(InputSplit inputSplit) {
                         column, columnType, OracleRawTypeConverter::apply);
         setRowConverter(jdbcDialect.getColumnConverter(rowType));
     }
-
-
 }

File: flinkx-connectors/flinkx-connector-oracle/src/main/java/com/dtstack/flinkx/connector/oracle/table/OracleDynamicTableFactory.java
Patch:
@@ -41,6 +41,4 @@ public String factoryIdentifier() {
     protected JdbcDialect getDialect() {
         return new OracleDialect();
     }
-
-
 }

File: flinkx-connectors/flinkx-connector-stream/src/main/java/com/dtstack/flinkx/connector/stream/converter/StreamRawTypeConverter.java
Patch:
@@ -52,6 +52,7 @@ public static DataType apply(String type) throws UnsupportedTypeException {
             case "DECIMAL":
                 return DataTypes.DECIMAL(38, 18);
 
+            case "STRING":
             case "CHAR":
             case "CHARACTER":
                 return DataTypes.STRING();

File: flinkx-connectors/flinkx-connector-oracle/src/main/java/com/dtstack/flinkx/connector/oracle/converter/OracleRawTypeConverter.java
Patch:
@@ -19,11 +19,11 @@
 
 package com.dtstack.flinkx.connector.oracle.converter;
 
-import com.dtstack.flinkx.throwable.UnsupportedTypeException;
-
 import org.apache.flink.table.api.DataTypes;
 import org.apache.flink.table.types.DataType;
 
+import com.dtstack.flinkx.throwable.UnsupportedTypeException;
+
 import java.sql.SQLException;
 import java.util.Locale;
 import java.util.function.Predicate;

File: flinkx-connectors/flinkx-connector-sqlservercdc/src/main/java/com/dtstack/flinkx/connector/sqlservercdc/entity/ChangeTablePointer.java
Patch:
@@ -5,6 +5,7 @@
  */
 package com.dtstack.flinkx.connector.sqlservercdc.entity;
 
+import com.dtstack.flinkx.connector.sqlservercdc.util.SqlServerCdcUtil;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 

File: flinkx-connectors/flinkx-connector-sqlservercdc/src/main/java/com/dtstack/flinkx/connector/sqlservercdc/inputFormat/SqlserverCdcInputFormatBuilder.java
Patch:
@@ -19,7 +19,7 @@
 
 import com.dtstack.flinkx.connector.sqlservercdc.conf.SqlServerCdcConf;
 import com.dtstack.flinkx.connector.sqlservercdc.entity.Lsn;
-import com.dtstack.flinkx.connector.sqlservercdc.entity.SqlServerCdcUtil;
+import com.dtstack.flinkx.connector.sqlservercdc.util.SqlServerCdcUtil;
 import com.dtstack.flinkx.connector.sqlservercdc.entity.SqlServerCdcEnum;
 import com.dtstack.flinkx.constants.ConstantValue;
 import com.dtstack.flinkx.converter.AbstractCDCRowConverter;
@@ -42,7 +42,7 @@
 import java.util.Locale;
 import java.util.Set;
 
-import static com.dtstack.flinkx.connector.sqlservercdc.entity.SqlServerCdcUtil.DRIVER;
+import static com.dtstack.flinkx.connector.sqlservercdc.util.SqlServerCdcUtil.DRIVER;
 
 /**
  * Date: 2019/12/03

File: flinkx-connectors/flinkx-connector-sqlservercdc/src/main/java/com/dtstack/flinkx/connector/sqlservercdc/listener/SqlServerCdcListener.java
Patch:
@@ -21,7 +21,7 @@
 import com.dtstack.flinkx.connector.sqlservercdc.entity.ChangeTablePointer;
 import com.dtstack.flinkx.connector.sqlservercdc.entity.Lsn;
 import com.dtstack.flinkx.connector.sqlservercdc.entity.SqlServerCdcEventRow;
-import com.dtstack.flinkx.connector.sqlservercdc.entity.SqlServerCdcUtil;
+import com.dtstack.flinkx.connector.sqlservercdc.util.SqlServerCdcUtil;
 import com.dtstack.flinkx.connector.sqlservercdc.entity.SqlServerCdcEnum;
 import com.dtstack.flinkx.connector.sqlservercdc.entity.TableId;
 import com.dtstack.flinkx.connector.sqlservercdc.entity.TxLogPosition;

File: flinkx-connectors/flinkx-connector-oracle/src/main/java/com/dtstack/flinkx/connector/oracle/source/OracleInputFormat.java
Patch:
@@ -39,7 +39,6 @@
 public class OracleInputFormat extends JdbcInputFormat {
 
 
-
     @Override
     public void openInternal(InputSplit inputSplit) {
         super.openInternal(inputSplit);

File: flinkx-connectors/flinkx-connector-oracle/src/main/java/com/dtstack/flinkx/connector/oracle/source/OracleSourceFactory.java
Patch:
@@ -35,7 +35,7 @@
  * @author jier
  */
 public class OracleSourceFactory extends JdbcSourceFactory {
-    // 默认是流式拉取
+
     private static final int DEFAULT_FETCH_SIZE = Integer.MIN_VALUE;
 
     public OracleSourceFactory(SyncConf syncConf, StreamExecutionEnvironment env) {

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/lookup/JdbcAllTableFunction.java
Patch:
@@ -34,7 +34,7 @@ public class JdbcAllTableFunction extends AbstractAllTableFunction {
 
     private final JdbcConf jdbcConf;
     private final String query;
-    private final JdbcDialect jdbcDialect;
+    protected final JdbcDialect jdbcDialect;
 
     public JdbcAllTableFunction(
             JdbcConf jdbcConf,

File: flinkx-connectors/flinkx-connector-mysql/src/main/java/com/dtstack/flinkx/connector/mysql/converter/MysqlRawTypeConverter.java
Patch:
@@ -43,8 +43,7 @@ public static DataType apply(String type) throws SQLException {
                 return DataTypes.FLOAT();
             case "DECIMAL":
             case "NUMERIC":
-                // TODO 精度应该可以动态传进来？
-                return DataTypes.DECIMAL(38, 18);
+                return DataTypes.DECIMAL(1, 0);
             case "DOUBLE":
                 return DataTypes.DOUBLE();
             case "CHAR":

File: flinkx-connectors/flinkx-connector-gbase/src/main/java/com/dtstack/flinkx/connector/gbase/GbaseDialect.java
Patch:
@@ -60,6 +60,7 @@ public String buildDualQueryStatement(String[] column) {
         return sb.toString();
     }
 
+    @Override
     public Optional<String> getUpsertStatement(
             String schema,
             String tableName,

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/lookup/JdbcLruTableFunction.java
Patch:
@@ -49,6 +49,7 @@
 import java.math.BigDecimal;
 import java.sql.Timestamp;
 import java.time.Instant;
+import java.util.Arrays;
 import java.util.Collection;
 import java.util.List;
 import java.util.Map;
@@ -326,7 +327,7 @@ private void handleQuery(
                             String.format(
                                     "\nget data with sql [%s],data [%s] failed! \ncause: [%s]",
                                     query,
-                                    keys,
+                                    Arrays.toString(keys),
                                     rs.cause().getMessage()
                             )
                     );

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/converter/JdbcRowConverter.java
Patch:
@@ -154,7 +154,7 @@ protected IDeserializationConverter createInternalConverter(LogicalType type) {
                 return val -> TimestampData.fromTimestamp((Timestamp) val);
             case CHAR:
             case VARCHAR:
-                return val -> StringData.fromString((String) val);
+                return val -> StringData.fromString(val.toString());
             case BINARY:
             case VARBINARY:
                 return val -> (byte[]) val;

File: flinkx-connectors/flinkx-connector-mysql/src/main/java/com/dtstack/flinkx/connector/mysql/sink/MysqlOutputFormat.java
Patch:
@@ -44,7 +44,7 @@ protected void openInternal(int taskNumber, int numTasks) {
         try {
             LogicalType rowType =
                     TableUtil.createRowType(
-                            fullColumn, fullColumnType, MysqlRawTypeConverter::apply);
+                            column, columnType, MysqlRawTypeConverter::apply);
             setRowConverter(jdbcDialect.getColumnConverter((RowType) rowType));
         } catch (SQLException e) {
             LOG.error("", e);

File: flinkx-connectors/flinkx-connector-mysql/src/main/java/com/dtstack/flinkx/connector/mysql/source/MysqlInputFormat.java
Patch:
@@ -45,7 +45,7 @@ public void openInternal(InputSplit inputSplit) {
         try {
             LogicalType rowType =
                     TableUtil.createRowType(
-                            fullColumnNameList, fullColumnTypeList, MysqlRawTypeConverter::apply);
+                            column, columnType, MysqlRawTypeConverter::apply);
             setRowConverter(jdbcDialect.getColumnConverter((RowType) rowType));
         } catch (SQLException e) {
             LOG.error("", e);

File: flinkx-connectors/flinkx-connector-kafka/src/main/java/com/dtstack/flinkx/connector/kafka/sink/KafkaDynamicSink.java
Patch:
@@ -22,7 +22,6 @@
 import org.apache.flink.api.common.serialization.SerializationSchema;
 import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;
 import org.apache.flink.streaming.connectors.kafka.partitioner.FlinkKafkaPartitioner;
-import com.dtstack.flinkx.connector.kafka.sink.DynamicKafkaSerializationSchema.MetadataConverter;
 import org.apache.flink.streaming.connectors.kafka.table.KafkaSinkSemantic;
 import org.apache.flink.table.api.DataTypes;
 import org.apache.flink.table.connector.ChangelogMode;
@@ -38,6 +37,7 @@
 import org.apache.flink.table.types.utils.DataTypeUtils;
 import org.apache.flink.util.Preconditions;
 
+import com.dtstack.flinkx.connector.kafka.sink.DynamicKafkaSerializationSchema.MetadataConverter;
 import org.apache.kafka.common.header.Header;
 
 import javax.annotation.Nullable;
@@ -299,8 +299,7 @@ protected FlinkKafkaProducer<RowData> createKafkaProducer(
                         metadataPositions,
                         upsertMode);
 
-        return new FlinkKafkaProducer<>(
-                topic,
+        return new KafkaProducer(topic,
                 kafkaSerializer,
                 properties,
                 FlinkKafkaProducer.Semantic.valueOf(semantic.toString()),

File: flinkx-core/src/main/java/com/dtstack/flinkx/element/column/BytesColumn.java
Patch:
@@ -31,15 +31,15 @@
  * @author tudou
  */
 public class BytesColumn extends AbstractBaseColumn {
-    private Charset encoding = StandardCharsets.UTF_8;
+    private String encoding = StandardCharsets.UTF_8.name();
 
     public BytesColumn(byte[] data) {
         super(data);
     }
 
     public BytesColumn(byte[] data, String encoding) {
         super(data);
-        this.encoding = Charset.forName(encoding);
+        this.encoding = encoding;
     }
 
     @Override
@@ -65,7 +65,7 @@ public String asString() {
         if (null == data) {
             return null;
         }
-        return new String((byte[])data, encoding);
+        return new String((byte[])data, Charset.forName(encoding));
     }
 
     @Override

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/DateUtil.java
Patch:
@@ -351,7 +351,7 @@ public static Timestamp getTimestampFromStr(String timeStr) {
             Instant instant = Instant.from(ISO_INSTANT.parse(timeStr));
             return new Timestamp(instant.getEpochSecond() * MILLIS_PER_SECOND);
         }
-        Date date = stringToDate(timeStr);
+        Date date = stringToDate(timeStr, null);
         return null == date ? null : new Timestamp(date.getTime());
     }
 

File: flinkx-local-test/src/main/java/com/dtstack/flinkx/local/test/LocalTest.java
Patch:
@@ -42,7 +42,6 @@ public class LocalTest {
     public static void main(String[] args) throws Exception {
         Properties confProperties = new Properties();
         String userDir = System.getProperty("user.dir");
-        System.out.println(userDir);
 
         String jobPath = userDir + "/flinkx-local-test/src/main/demo/flinksql.sql";
         String flinkxPluginPath = userDir + "/syncplugins";

File: flinkx-core/src/main/java/com/dtstack/flinkx/element/column/BytesColumn.java
Patch:
@@ -31,15 +31,15 @@
  * @author tudou
  */
 public class BytesColumn extends AbstractBaseColumn {
-    private Charset encoding = StandardCharsets.UTF_8;
+    private String encoding = StandardCharsets.UTF_8.name();
 
     public BytesColumn(byte[] data) {
         super(data);
     }
 
     public BytesColumn(byte[] data, String encoding) {
         super(data);
-        this.encoding = Charset.forName(encoding);
+        this.encoding = encoding;
     }
 
     @Override
@@ -65,7 +65,7 @@ public String asString() {
         if (null == data) {
             return null;
         }
-        return new String((byte[])data, encoding);
+        return new String((byte[])data, Charset.forName(encoding));
     }
 
     @Override

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/DateUtil.java
Patch:
@@ -351,7 +351,7 @@ public static Timestamp getTimestampFromStr(String timeStr) {
             Instant instant = Instant.from(ISO_INSTANT.parse(timeStr));
             return new Timestamp(instant.getEpochSecond() * MILLIS_PER_SECOND);
         }
-        Date date = stringToDate(timeStr);
+        Date date = stringToDate(timeStr, null);
         return null == date ? null : new Timestamp(date.getTime());
     }
 

File: flinkx-local-test/src/main/java/com/dtstack/flinkx/local/test/LocalTest.java
Patch:
@@ -42,7 +42,6 @@ public class LocalTest {
     public static void main(String[] args) throws Exception {
         Properties confProperties = new Properties();
         String userDir = System.getProperty("user.dir");
-        System.out.println(userDir);
 
         String jobPath = userDir + "/flinkx-local-test/src/main/demo/flinksql.sql";
         String flinkxPluginPath = userDir + "/syncplugins";

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/options/JdbcSinkOptions.java
Patch:
@@ -59,6 +59,6 @@ public class JdbcSinkOptions {
     public static final ConfigOption<Integer> SINK_PARALLELISM =
             ConfigOptions.key("sink.parallelism")
                     .intType()
-                    .defaultValue(1)
+                    .defaultValue(null)
                     .withDescription("sink.parallelism.");
 }

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/options/JdbcSourceOptions.java
Patch:
@@ -51,7 +51,7 @@ public class JdbcSourceOptions {
     public static final ConfigOption<Integer> SCAN_PARALLELISM =
             ConfigOptions.key("scan.parallelism")
                     .intType()
-                    .defaultValue(1)
+                    .defaultValue(null)
                     .withDescription("scan parallelism.");
     public static final ConfigOption<Integer> SCAN_QUERY_TIMEOUT =
             ConfigOptions.key("scan.query-timeout")

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/source/JdbcInputFormat.java
Patch:
@@ -63,6 +63,7 @@
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
+import java.util.Objects;
 import java.util.concurrent.TimeUnit;
 
 /**
@@ -505,7 +506,8 @@ protected String buildQuerySql(InputSplit inputSplit) {
         StringBuilder sql = new StringBuilder(128);
         sql.append(String.join(" AND ", whereList.toArray(new String[0])));
 
-        if (jdbcConf.getParallelism() > 1 && StringUtils.isNotBlank(jdbcConf.getSplitPk())) {
+        if ((Objects.nonNull(jdbcConf.getParallelism()) && jdbcConf.getParallelism() > 1)
+                && StringUtils.isNotBlank(jdbcConf.getSplitPk())) {
             sql.append(" ORDER BY ")
                     .append(jdbcDialect.quoteIdentifier(jdbcConf.getSplitPk()))
                     .append(" ASC");

File: flinkx-core/src/main/java/com/dtstack/flinkx/conf/FlinkxCommonConf.java
Patch:
@@ -46,7 +46,7 @@ public class FlinkxCommonConf implements Serializable {
     /** 是否校验format */
     private boolean checkFormat = true;
     /** 并行度 */
-    private int parallelism = 1;
+    private Integer parallelism = 1;
 
     public long getSpeedBytes() {
         return speedBytes;
@@ -104,11 +104,11 @@ public void setCheckFormat(boolean checkFormat) {
         this.checkFormat = checkFormat;
     }
 
-    public int getParallelism() {
+    public Integer getParallelism() {
         return parallelism;
     }
 
-    public void setParallelism(int parallelism) {
+    public void setParallelism(Integer parallelism) {
         this.parallelism = parallelism;
     }
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/lookup/conf/LookupConf.java
Patch:
@@ -47,7 +47,7 @@ public class LookupConf implements Serializable {
     /** 异步超时时长 */
     protected int asyncTimeout = 10000;
     /** 维表并行度 */
-    protected int parallelism = 1;
+    protected Integer parallelism = 1;
 
     public String getTableName() {
         return tableName;
@@ -130,11 +130,11 @@ public LookupConf setAsyncTimeout(int asyncTimeout) {
         return this;
     }
 
-    public int getParallelism() {
+    public Integer getParallelism() {
         return parallelism;
     }
 
-    public LookupConf setParallelism(int parallelism) {
+    public LookupConf setParallelism(Integer parallelism) {
         this.parallelism = parallelism;
         return this;
     }

File: flinkx-core/src/main/java/com/dtstack/flinkx/lookup/constants/LookupOptions.java
Patch:
@@ -84,6 +84,6 @@ public class LookupOptions {
     public static final ConfigOption<Integer> LOOKUP_PARALLELISM =
             ConfigOptions.key("lookup.parallelism")
                     .intType()
-                    .defaultValue(1)
+                    .defaultValue(null)
                     .withDescription("lookup.parallelism.");
 }

File: flinkx-connectors/flinkx-connector-stream/src/main/java/com/dtstack/flinkx/connector/stream/sink/StreamDynamicTableSink.java
Patch:
@@ -80,7 +80,7 @@ public SinkFunctionProvider getSinkRuntimeProvider(Context context) {
         builder.setStreamSinkConf(sinkConf);
         builder.setConverter(new StreamRowConverter(rowType));
 
-        return SinkFunctionProvider.of(new DtOutputFormatSinkFunction(builder.finish()), 1);
+        return SinkFunctionProvider.of(new DtOutputFormatSinkFunction(builder.finish()), null);
     }
 
     @Override

File: flinkx-connectors/flinkx-connector-stream/src/main/java/com/dtstack/flinkx/connector/stream/source/StreamDynamicTableSource.java
Patch:
@@ -83,7 +83,7 @@ public ScanRuntimeProvider getScanRuntimeProvider(ScanContext runtimeProviderCon
         builder.setRowConverter(new StreamRowConverter(rowType));
         builder.setStreamConf(streamConf);
 
-        return ParallelSourceFunctionProvider.of(new DtInputFormatSourceFunction<>(builder.finish(), typeInformation), false, 1);
+        return ParallelSourceFunctionProvider.of(new DtInputFormatSourceFunction<>(builder.finish(), typeInformation), false, null);
     }
 
     @Override

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/options/JdbcCommonOptions.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.jdbc.constants;
+package com.dtstack.flinkx.connector.jdbc.options;
 
 import org.apache.flink.configuration.ConfigOption;
 import org.apache.flink.configuration.ConfigOptions;

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/options/JdbcLookupOptions.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.jdbc.constants;
+package com.dtstack.flinkx.connector.jdbc.options;
 
 import com.dtstack.flinkx.lookup.constants.LookupOptions;
 

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/options/JdbcSinkOptions.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.jdbc.constants;
+package com.dtstack.flinkx.connector.jdbc.options;
 
 import org.apache.flink.configuration.ConfigOption;
 import org.apache.flink.configuration.ConfigOptions;

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/sink/JdbcOutputFormat.java
Patch:
@@ -71,6 +71,7 @@ public class JdbcOutputFormat extends BaseRichOutputFormat {
 
     protected transient Connection dbConn;
     protected transient FieldNamedPreparedStatement fieldNamedPreparedStatement;
+    // TODO 这几个名Source Sinke要对齐下，column、columnType、fullColumn、fullColumnType
     protected List<String> column;
     protected List<String> columnType;
     protected List<String> fullColumnType;

File: flinkx-connectors/flinkx-connector-stream/src/main/java/com/dtstack/flinkx/connector/stream/converter/StreamColumnConverter.java
Patch:
@@ -41,7 +41,7 @@
  */
 public class StreamColumnConverter extends AbstractRowConverter<RowData, RowData, RowData, String> {
 
-    private static final long serialVersionUID = 2780645964685625080L;
+    private static final long serialVersionUID = 1L;
     private static final AtomicLong id = new AtomicLong(0L);
 
     public StreamColumnConverter(List<String> typeList) {

File: flinkx-connectors/flinkx-connector-stream/src/main/java/com/dtstack/flinkx/connector/stream/converter/StreamRowConverter.java
Patch:
@@ -50,7 +50,7 @@
  */
 public class StreamRowConverter extends AbstractRowConverter<RowData, RowData, RowData, LogicalType> {
 
-    private static final long serialVersionUID = -6831309858122276980L;
+    private static final long serialVersionUID = 1L;
 
     public StreamRowConverter(RowType rowType) {
         super(rowType);

File: flinkx-connectors/flinkx-connector-stream/src/main/java/com/dtstack/flinkx/connector/stream/options/StreamOptions.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.connector.stream.constants;
+package com.dtstack.flinkx.connector.stream.options;
 
 import org.apache.flink.configuration.ConfigOption;
 
@@ -27,7 +27,7 @@
  * @create 2021-04-09 10:19
  * @description 常量
  **/
-public class StreamConstants {
+public class StreamOptions {
     public static final ConfigOption<String> PRINT_IDENTIFIER =
             key("print-identifier")
                     .stringType()

File: flinkx-connectors/flinkx-connector-stream/src/main/java/com/dtstack/flinkx/connector/stream/table/StreamDynamicTableFactory.java
Patch:
@@ -36,9 +36,9 @@
 import java.util.HashSet;
 import java.util.Set;
 
-import static com.dtstack.flinkx.connector.stream.constants.StreamConstants.NUMBER_OF_ROWS;
-import static com.dtstack.flinkx.connector.stream.constants.StreamConstants.PRINT_IDENTIFIER;
-import static com.dtstack.flinkx.connector.stream.constants.StreamConstants.STANDARD_ERROR;
+import static com.dtstack.flinkx.connector.stream.options.StreamOptions.NUMBER_OF_ROWS;
+import static com.dtstack.flinkx.connector.stream.options.StreamOptions.PRINT_IDENTIFIER;
+import static com.dtstack.flinkx.connector.stream.options.StreamOptions.STANDARD_ERROR;
 
 /**
  * @author chuixue

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/PrintUtil.java
Patch:
@@ -40,10 +40,10 @@ public class PrintUtil {
 
     private static Logger LOG = LoggerFactory.getLogger(PrintUtil.class);
 
-    public static void printResult(JobExecutionResult result){
+    public static void printResult(Map<String, Object> result){
         List<String> names = Lists.newArrayList();
         List<String> values = Lists.newArrayList();
-        result.getAllAccumulatorResults().forEach((name, val) -> {
+        result.forEach((name, val) -> {
             names.add(name);
             values.add(String.valueOf(val));
         });

File: flinkx-core/src/main/java/org/apache/flink/table/factories/FactoryUtil.java
Patch:
@@ -79,8 +79,8 @@ public final class FactoryUtil {
     /** 上下文环境 */
     private static StreamExecutionEnvironment env = null;
 
-    /** 插件加载方式，默认走CLASSLOADER方式 */
-    private static String connectorLoadMode = ConnectorLoadMode.CLASSLOADER.name();
+    /** 插件加载方式，默认走SPI方式 */
+    private static String connectorLoadMode = ConnectorLoadMode.SPI.name();
 
     /** shipfile需要的jar */
     private static List<URL> classPathSet = new ArrayList<>();

File: flinkx-local-test/src/main/java/com/dtstack/flinkx/local/test/LocalTest.java
Patch:
@@ -44,7 +44,7 @@ public static void main(String[] args) throws Exception {
         String userDir = System.getProperty("user.dir");
         System.out.println(userDir);
 
-        String jobPath = "/Users/tudou/Library/Application Support/JetBrains/IntelliJIdea2021.1/scratches/merge/binlog_stream.json";
+        String jobPath = userDir + "/flinkx-local-test/src/main/demo/flinksql.sql";
         String flinkxPluginPath = userDir + "/syncplugins";
 
         // 任务配置参数
@@ -73,8 +73,8 @@ public static void main(String[] args) throws Exception {
             argsList.add(URLEncoder.encode(content, StandardCharsets.UTF_8.name()));
             argsList.add("-jobName");
             argsList.add("flinkStreamSQLLocalTest");
-//            argsList.add("-pluginRoot");
-//            argsList.add(flinkxPluginPath);
+            argsList.add("-pluginRoot");
+            argsList.add(flinkxPluginPath);
             argsList.add("-remotePluginPath");
             argsList.add(flinkxPluginPath);
             argsList.add("-pluginLoadMode");

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/PrintUtil.java
Patch:
@@ -40,10 +40,10 @@ public class PrintUtil {
 
     private static Logger LOG = LoggerFactory.getLogger(PrintUtil.class);
 
-    public static void printResult(JobExecutionResult result){
+    public static void printResult(Map<String, Object> result){
         List<String> names = Lists.newArrayList();
         List<String> values = Lists.newArrayList();
-        result.getAllAccumulatorResults().forEach((name, val) -> {
+        result.forEach((name, val) -> {
             names.add(name);
             values.add(String.valueOf(val));
         });

File: flinkx-core/src/main/java/org/apache/flink/table/factories/FactoryUtil.java
Patch:
@@ -79,8 +79,8 @@ public final class FactoryUtil {
     /** 上下文环境 */
     private static StreamExecutionEnvironment env = null;
 
-    /** 插件加载方式，默认走CLASSLOADER方式 */
-    private static String connectorLoadMode = ConnectorLoadMode.CLASSLOADER.name();
+    /** 插件加载方式，默认走SPI方式 */
+    private static String connectorLoadMode = ConnectorLoadMode.SPI.name();
 
     /** shipfile需要的jar */
     private static List<URL> classPathSet = new ArrayList<>();

File: flinkx-connectors/flinkx-connector-stream/src/main/java/com/dtstack/flinkx/connector/stream/source/StreamDynamicTableSource.java
Patch:
@@ -81,7 +81,7 @@ public ScanRuntimeProvider getScanRuntimeProvider(ScanContext runtimeProviderCon
         streamConf.setColumn(fieldConfList);
 
         StreamInputFormatBuilder builder = new StreamInputFormatBuilder();
-        builder.setAbstractRowConverter(new StreamRowConverter(rowType));
+        builder.setRowConverter(new StreamRowConverter(rowType));
         builder.setStreamConf(streamConf);
 
         return ParallelSourceFunctionProvider.of(new DtInputFormatSourceFunction<>(builder.finish(), typeInformation), false, 1);

File: flinkx-connectors/flinkx-connector-stream/src/main/java/com/dtstack/flinkx/connector/stream/source/StreamSourceFactory.java
Patch:
@@ -70,7 +70,7 @@ public DataStream<RowData> createSource() {
             rowConverter = new StreamRowConverter(rowType);
         }
 
-        builder.setAbstractRowConverter(rowConverter);
+        builder.setRowConverter(rowConverter);
 
         return createInput(builder.finish());
     }

File: flinkx-core/src/main/java/com/dtstack/flinkx/inputformat/BaseRichInputFormatBuilder.java
Patch:
@@ -40,7 +40,7 @@ public void setConfig(FlinkxCommonConf config) {
         format.setConfig(config);
     }
 
-    public void setAbstractRowConverter(AbstractRowConverter rowConverter) {
+    public void setRowConverter(AbstractRowConverter rowConverter) {
         format.setRowConverter(rowConverter);
     }
 

File: flinkx-connectors/flinkx-connector-mysql/src/main/java/com/dtstack/flinkx/connector/mysql/converter/MysqlRawTypeConverter.java
Patch:
@@ -11,7 +11,7 @@
  * @author: wuren
  * @create: 2021/04/14
  **/
-public class MysqlTypeConverter {
+public class MysqlRawTypeConverter {
 
     /**
      * 将mysql数据库中的类型，转换成flink的DataType类型

File: flinkx-connectors/flinkx-connector-stream/src/main/java/com/dtstack/flinkx/connector/stream/converter/StreamColumnConverter.java
Patch:
@@ -21,9 +21,9 @@
 import org.apache.flink.table.data.RowData;
 
 import com.dtstack.flinkx.element.AbstractBaseColumn;
-import com.dtstack.flinkx.element.BigDecimalColumn;
+import com.dtstack.flinkx.element.column.BigDecimalColumn;
 import com.dtstack.flinkx.element.ColumnRowData;
-import com.dtstack.flinkx.element.StringColumn;
+import com.dtstack.flinkx.element.column.StringColumn;
 import com.github.jsonzou.jmockdata.JMockData;
 
 import java.math.BigDecimal;

File: flinkx-connectors/flinkx-connector-mysql/src/main/java/com/dtstack/flinkx/connector/mysql/sink/MysqlSink.java
Patch:
@@ -26,8 +26,7 @@
 import com.dtstack.flinkx.connector.mysql.outputFormat.MysqlOutputFormat;
 
 /**
- * Date: 2021/04/13
- * Company: www.dtstack.com
+ * Date: 2021/04/13 Company: www.dtstack.com
  *
  * @author tudou
  */

File: flinkx-core/src/main/java/com/dtstack/flinkx/CastFunction.java
Patch:
@@ -61,7 +61,7 @@ private Casting createNumericCasting (LogicalType sourceType, LogicalType target
         LogicalTypeRoot targetTypeRoot = targetType.getTypeRoot();
         // TODO 这个地方可能需要改成枚举， 一次匹配 Source 和 Target两种类型，inspired by flink
         if (targetTypeRoot.equals(LogicalTypeRoot.BIGINT)) {
-            return sourceType.accept(new CastBigIntLogicalTypeVisitor());
+            return sourceType.accept(new CastToBigIntLogicalTypeVisitor());
         } else {
             throw new Exception("Unsupported cast from to");
         }

File: flinkx-core/src/main/java/com/dtstack/flinkx/CastToBigIntLogicalTypeVisitor.java
Patch:
@@ -36,7 +36,7 @@
  * @author: wuren
  * @create: 2021/04/19
  **/
-public class CastBigIntLogicalTypeVisitor implements LogicalTypeVisitor<Casting> {
+public class CastToBigIntLogicalTypeVisitor implements LogicalTypeVisitor<Casting> {
 
     @Override
     public Casting visit(CharType charType) {

File: flinkx-connectors/flinkx-connector-jdbc-base/src/main/java/com/dtstack/flinkx/connector/jdbc/inputFormat/JdbcInputFormat.java
Patch:
@@ -49,7 +49,6 @@
 
 import org.apache.flink.table.data.StringData;
 import org.apache.flink.table.types.logical.LogicalType;
-import org.apache.flink.table.types.logical.RowType;
 
 import org.apache.hadoop.fs.FSDataOutputStream;
 import org.apache.hadoop.fs.FileSystem;

File: flinkx-connectors/flinkx-connector-mysql/src/main/java/com/dtstack/flinkx/connector/mysql/sink/MysqlSink.java
Patch:
@@ -21,6 +21,7 @@
 import com.dtstack.flinkx.connector.jdbc.outputformat.JdbcOutputFormatBuilder;
 import com.dtstack.flinkx.connector.jdbc.sink.JdbcDataSink;
 import com.dtstack.flinkx.connector.mysql.MySQLDialect;
+import com.dtstack.flinkx.connector.mysql.MysqlLogicalTypeFactory;
 import com.dtstack.flinkx.connector.mysql.outputFormat.MysqlOutputFormat;
 
 /**
@@ -33,7 +34,8 @@ public class MysqlSink extends JdbcDataSink {
 
     public MysqlSink(SyncConf syncConf) {
         super(syncConf);
-        super.JdbcDialect = new MySQLDialect();
+        super.jdbcDialect = new MySQLDialect();
+        jdbcLogicalTypeFactory = new MysqlLogicalTypeFactory(jdbcConf, jdbcDialect);
     }
 
     @Override

File: flinkx-connectors/flinkx-connector-mysql/src/main/java/com/dtstack/flinkx/connector/mysql/source/MysqlSource.java
Patch:
@@ -17,6 +17,8 @@
  */
 package com.dtstack.flinkx.connector.mysql.source;
 
+import com.dtstack.flinkx.connector.mysql.MysqlLogicalTypeFactory;
+
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
 
 import com.dtstack.flinkx.conf.SyncConf;
@@ -43,6 +45,7 @@ public MysqlSource(SyncConf syncConf, StreamExecutionEnvironment env) {
                 && jdbcConf.getFetchSize() == 0){
             jdbcConf.setFetchSize(1000);
         }
+        jdbcLogicalTypeFactory = new MysqlLogicalTypeFactory(jdbcConf, jdbcDialect);
     }
 
     @Override

File: flinkx-connectors/flinkx-connector-stream/src/main/java/com/dtstack/flinkx/connector/stream/source/StreamSource.java
Patch:
@@ -15,6 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package com.dtstack.flinkx.connector.stream.source;
 
 import com.dtstack.flinkx.util.TableUtil;
@@ -55,7 +56,7 @@ public DataStream<RowData> readData() {
         return createInput(builder.finish());
     }
 
-    // TODO 和 StreamSource重复了，看看如何删减
+    // TODO Stream是否抽象一个LogicalTypeFactory类
     @Override
     public LogicalType getLogicalType() {
         DataType dataType = TableUtil.getDataType(streamConf.getColumn());

File: flinkx-local-test/src/main/java/com/dtstack/flinkx/local/test/LocalTest.java
Patch:
@@ -44,8 +44,8 @@ public static void main(String[] args) throws Exception{
 //        String jobPath = "/Users/tudou/Library/Preferences/IntelliJIdea2019.3/scratches/merge/stream_mysql.json";
 //        String jobPath = "/Users/tudou/Library/Preferences/IntelliJIdea2019.3/scratches/merge/scratch.sql";
         // 不要删，注释就行。
-//        String jobPath = "/Users/chuixue/Desktop/tmp/sqlFile.sql";
-        String jobPath = "/Users/luna/src/dtstack/tomb/flinkx12/read_format_mysql.json";
+//        String jobPath = "/Users/luna/src/dtstack/tomb/flinkx12/test.sql";
+        String jobPath = "/Users/luna/src/dtstack/tomb/flinkx12/read_cast_mysql.json";
         // 任务配置参数
         List<String> argsList = new ArrayList<>();
         argsList.add("-mode");

File: flinkx-connectors/flinkx-connector-stream/src/main/java/com/dtstack/flinkx/connector/stream/table/StreamDynamicTableFactory.java
Patch:
@@ -46,7 +46,7 @@
  * @description
  **/
 public class StreamDynamicTableFactory implements DynamicTableSinkFactory, DynamicTableSourceFactory {
-    public static final String IDENTIFIER = "stream";
+    public static final String IDENTIFIER = "stream-x";
 
     @Override
     public String factoryIdentifier() {

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/StringUtil.java
Patch:
@@ -79,6 +79,7 @@ public static String convertRegularExpr (String str) {
         return str;
     }
 
+    // TODO 类型可以改成使用LogicalType
     public static Object string2col(String str, String type, SimpleDateFormat customTimeFormat) {
         if(str == null || str.length() == 0 || type == null){
             return str;

File: flinkx-connectors/flinkx-connector-mysql/src/main/java/com/dtstack/flinkx/connector/mysql/converter/MysqlTypeConverter.java
Patch:
@@ -16,7 +16,8 @@ public class MysqlTypeConverter {
     // MySQL支持的数据类型: com.mysql.jdbc.MysqlDefs
     // com.mysql.jdbc.ResultSetImpl.getObject(int)
     //TODO 仔细梳理每个数据库支持的数据类型
-    public static DataType convertToDataType(String type) throws SQLException {
+
+    public static DataType apply(String type) throws SQLException  {
 
         switch (type.toUpperCase(Locale.ENGLISH)) {
             case "BIT":

File: flinkx-core/src/main/java/com/dtstack/flinkx/outputformat/BaseFileOutputFormat.java
Patch:
@@ -284,6 +284,7 @@ public FormatState getFormatState() throws Exception{
      * checkpoint成功时操作
      * @param checkpointId
      */
+    @Override
     public void notifyCheckpointComplete(long checkpointId){
         //todo 移动成功，清空标记
     };
@@ -292,6 +293,7 @@ public void notifyCheckpointComplete(long checkpointId){
      * checkpoint失败时操作
      * @param checkpointId
      */
+    @Override
     public void notifyCheckpointAborted(long checkpointId){
         //todo 根据标记检测是否已经有文件被移动到正式目录，若有，则移动回.data目录，然后清空标记
     };

File: flinkx-core/src/main/java/com/dtstack/flinkx/streaming/api/functions/source/DtInputFormatSourceFunction.java
Patch:
@@ -281,6 +281,7 @@ public void initializeState(FunctionInitializationContext context) throws Except
 	 */
 	public void throwException(Exception e) throws Exception {
 		if(null != e) {
+			LOG.error("DtInputFormatSourceFunction error, info: {}",ExceptionUtil.getErrorMessage(e), e);
 			throw e;
 		}
 	}

File: flinkx-cassandra/flinkx-cassandra-core/src/main/java/com/dtstack/flinkx/cassandra/CassandraUtil.java
Patch:
@@ -20,6 +20,7 @@
 
 import com.datastax.driver.core.*;
 import com.datastax.driver.core.LocalDate;
+import com.dtstack.flinkx.util.ExceptionUtil;
 import com.google.common.base.Preconditions;
 import org.apache.commons.collections.MapUtils;
 import org.apache.commons.lang.StringUtils;
@@ -210,7 +211,7 @@ private static<T> Optional<byte[]> objectToBytes(T obj){
             sOut.flush();
             bytes= out.toByteArray();
         } catch (IOException e) {
-            e.printStackTrace();
+            LOG.warn("object convent byte[] failed, error info {}", ExceptionUtil.getErrorMessage(e), e);
         }
         return Optional.ofNullable(bytes);
     }

File: flinkx-cassandra/flinkx-cassandra-reader/src/main/java/com/dtstack/flinkx/cassandra/reader/CassandraInputFormat.java
Patch:
@@ -144,7 +144,7 @@ private InputSplit[] splitJob(int minNumSplits, ArrayList<CassandraInputSplit> s
                     .divide(BigDecimal.valueOf(minNumSplits),2, BigDecimal.ROUND_HALF_EVEN);
             for ( int i = 0; i < minNumSplits; i++ ) {
                 BigInteger l = minToken.add(step.multiply(BigDecimal.valueOf(i))).toBigInteger();
-                BigInteger r = minToken.add(step.multiply(BigDecimal.valueOf(i+1))).toBigInteger();
+                BigInteger r = minToken.add(step.multiply(BigDecimal.valueOf(i+1L))).toBigInteger();
                 if( i == minNumSplits - 1 ) {
                     r = maxToken.toBigInteger();
                 }
@@ -157,7 +157,7 @@ private InputSplit[] splitJob(int minNumSplits, ArrayList<CassandraInputSplit> s
                     .divide(BigDecimal.valueOf(minNumSplits),2, BigDecimal.ROUND_HALF_EVEN);
             for ( int i = 0; i < minNumSplits; i++ ) {
                 long l = minToken.add(step.multiply(BigDecimal.valueOf(i))).longValue();
-                long r = minToken.add(step.multiply(BigDecimal.valueOf(i+1))).longValue();
+                long r = minToken.add(step.multiply(BigDecimal.valueOf(i+1L))).longValue();
                 if( i == minNumSplits - 1 ) {
                     r = maxToken.longValue();
                 }

File: flinkx-phoenix5/flinkx-phoenix5-reader/src/main/java/com/dtstack/flinkx/phoenix5/format/Phoenix5InputFormat.java
Patch:
@@ -177,8 +177,9 @@ public void openInternal(InputSplit inputSplit) throws IOException {
                     hConfiguration.set(HConstants.ZOOKEEPER_ZNODE_PARENT, (String) rootNode);
                 }
                 hConfiguration.setBoolean(HConstants.CLUSTER_DISTRIBUTED, true);
-                org.apache.hadoop.hbase.client.Connection hConn = ConnectionFactory.createConnection(hConfiguration);
-                hTable = hConn.getTable(TableName.valueOf(table));
+                try (org.apache.hadoop.hbase.client.Connection hConn = ConnectionFactory.createConnection(hConfiguration)) {
+                    hTable = hConn.getTable(TableName.valueOf(table));
+                }
                 resultIterator = hTable.getScanner(scan).iterator();
             } catch (Exception e) {
                 String message = String.format("openInputFormat() failed, dbUrl = %s, properties = %s, e = %s", dbUrl, GsonUtil.GSON.toJson(properties), ExceptionUtil.getErrorMessage(e));

File: flinkx-restapi/flinkx-restapi-reader/src/main/java/com/dtstack/flinkx/restapi/client/DefaultRestHandler.java
Patch:
@@ -137,7 +137,7 @@ public Object getResponseValue(Map<String, Object> map, String key) {
         for (int i = 0; i < split.length; i++) {
             o = getValue(tempMap, split[i]);
             if (o == null) {
-                throw new RuntimeException(key + " not exist on responseValue [" + GsonUtil.GSON.toJson(map) + "] ");
+                throw new RuntimeException(key + " on responseValue [" + GsonUtil.GSON.toJson(map) + "]  is null");
             }
             if (i != split.length - 1) {
                 if (!(o instanceof Map)) {

File: flinkx-restapi/flinkx-restapi-reader/src/main/java/com/dtstack/flinkx/restapi/client/DefaultRestHandler.java
Patch:
@@ -137,7 +137,7 @@ public Object getResponseValue(Map<String, Object> map, String key) {
         for (int i = 0; i < split.length; i++) {
             o = getValue(tempMap, split[i]);
             if (o == null) {
-                throw new RuntimeException(key + " not exist on responseValue [" + GsonUtil.GSON.toJson(map) + "] ");
+                throw new RuntimeException(key + " on responseValue [" + GsonUtil.GSON.toJson(map) + "]  is null");
             }
             if (i != split.length - 1) {
                 if (!(o instanceof Map)) {

File: flinkx-hdfs/flinkx-hdfs-core/src/main/java/com/dtstack/flinkx/hdfs/HdfsConfigKeys.java
Patch:
@@ -34,9 +34,6 @@ public class HdfsConfigKeys {
 
     public static final String KEY_HADOOP_CONFIG = "hadoopConfig";
 
-    //hadoop是否是ha环境
-    public static final String KEY_HADOOP_HA= "isHa";
-
     public static final String KEY_FILTER = "filterRegex";
 
     public static final String KEY_FILE_TYPE = "fileType";

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/BaseHdfsOutputFormat.java
Patch:
@@ -53,9 +53,6 @@ public abstract class BaseHdfsOutputFormat extends BaseFileOutputFormat {
     /** hdfs高可用配置 */
     protected Map<String,Object> hadoopConfig;
 
-    //hadoop是否是高可用
-    protected boolean isHa;
-
     protected String defaultFs;
 
     protected List<String> columnTypes;

File: flinkx-kafka/flinkx-kafka-writer/src/main/java/com/dtstack/flinkx/kafka/PartitionAssigner.java
Patch:
@@ -38,7 +38,7 @@ public int partition(String topic, Object key, byte[] bytes, Object value, byte[
             return 0;
         }
         Integer partitionCountForTopic = cluster.partitionCountForTopic(topic);
-        return key.toString().hashCode() % partitionCountForTopic;
+        return Math.abs(key.toString().hashCode() % partitionCountForTopic);
     }
 
     @Override

File: flinkx-rdb/flinkx-rdb-writer/src/main/java/com/dtstack/flinkx/rdb/outputformat/JdbcOutputFormat.java
Patch:
@@ -45,6 +45,7 @@
 import java.util.Arrays;
 import java.util.HashMap;
 import java.util.List;
+import java.util.Locale;
 import java.util.Map;
 
 /**
@@ -114,7 +115,7 @@ public class JdbcOutputFormat extends BaseRichOutputFormat {
             "AND t.table_name = '%s'";
 
     protected final static String CONN_CLOSE_ERROR_MSG = "No operations allowed";
-    protected static List<String> STRING_TYPES = Arrays.asList("CHAR", "VARCHAR","TINYBLOB","TINYTEXT","BLOB","TEXT", "MEDIUMBLOB", "MEDIUMTEXT", "LONGBLOB", "LONGTEXT");
+    protected static List<String> STRING_TYPES = Arrays.asList("CHAR", "VARCHAR", "VARCHAR2", "NVARCHAR2", "NVARCHAR", "TINYBLOB","TINYTEXT","BLOB","TEXT", "MEDIUMBLOB", "MEDIUMTEXT", "LONGBLOB", "LONGTEXT");
 
     protected PreparedStatement prepareTemplates() throws SQLException {
         if(CollectionUtils.isEmpty(fullColumn)) {
@@ -340,7 +341,7 @@ protected Object getField(Row row, int index) {
         //field为空字符串，且写入目标类型不为字符串类型的字段，则将object设置为null
         if(field instanceof String
                 && StringUtils.isBlank((String) field)
-                &&!STRING_TYPES.contains(type)){
+                &&!STRING_TYPES.contains(type.toUpperCase(Locale.ENGLISH))){
             return null;
         }
 

File: flinkx-hdfs/flinkx-hdfs-core/src/main/java/com/dtstack/flinkx/hdfs/HdfsConfigKeys.java
Patch:
@@ -34,9 +34,6 @@ public class HdfsConfigKeys {
 
     public static final String KEY_HADOOP_CONFIG = "hadoopConfig";
 
-    //hadoop是否是ha环境
-    public static final String KEY_HADOOP_HA= "isHa";
-
     public static final String KEY_FILTER = "filterRegex";
 
     public static final String KEY_FILE_TYPE = "fileType";

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/BaseHdfsOutputFormat.java
Patch:
@@ -53,9 +53,6 @@ public abstract class BaseHdfsOutputFormat extends BaseFileOutputFormat {
     /** hdfs高可用配置 */
     protected Map<String,Object> hadoopConfig;
 
-    //hadoop是否是高可用
-    protected boolean isHa;
-
     protected String defaultFs;
 
     protected List<String> columnTypes;

File: flinkx-oraclelogminer/flinkx-oraclelogminer-reader/src/main/java/com/dtstack/flinkx/oraclelogminer/format/LogMinerListener.java
Patch:
@@ -26,7 +26,6 @@
 import com.google.common.util.concurrent.ThreadFactoryBuilder;
 import net.sf.jsqlparser.JSQLParserException;
 import org.apache.commons.lang3.tuple.Pair;
-import org.mortbay.log.Log;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -133,7 +132,7 @@ public void run() {
                 }
                 sb.append(",\ne = ").append(ExceptionUtil.getErrorMessage(e));
                 String msg = sb.toString();
-                Log.warn(msg);
+                LOG.warn(msg);
                 try {
                     queue.put(new QueueData(0L, Collections.singletonMap("e", msg)));
                     Thread.sleep(2000L);

File: flinkx-oraclelogminer/flinkx-oraclelogminer-reader/src/main/java/com/dtstack/flinkx/oraclelogminer/format/LogMinerConnection.java
Patch:
@@ -128,11 +128,12 @@ public void connect() {
             ClassUtil.forName(logMinerConfig.getDriverName(), getClass().getClassLoader());
 
             connection = RetryUtil.executeWithRetry(() -> DriverManager.getConnection(logMinerConfig.getJdbcUrl(), logMinerConfig.getUsername(), logMinerConfig.getPassword()), RETRY_TIMES, SLEEP_TIME,false);
-
+            setSessionFormat();
             LOG.info("get connection successfully, url:{}, username:{}", logMinerConfig.getJdbcUrl(), logMinerConfig.getUsername());
         } catch (Exception e){
             String message = String.format("get connection failed，url:[%s], username:[%s], e:%s", logMinerConfig.getJdbcUrl(), logMinerConfig.getUsername(), ExceptionUtil.getErrorMessage(e));
             LOG.error(message);
+            closeResources(null,null,connection);
             throw new RuntimeException(message, e);
         }
     }

File: flinkx-binlog/flinkx-binlog-core/src/main/java/com/dtstack/flinkx/binlog/BinlogUtil.java
Patch:
@@ -45,9 +45,9 @@ public class BinlogUtil {
 
     public static final String DRIVER_NAME = "com.mysql.jdbc.Driver";
     //是否开启binlog
-    private static final String CHECK_BINLOG_ENABLE = "show variables where variable_name = 'log_bin';;";
+    private static final String CHECK_BINLOG_ENABLE = "show variables where variable_name = 'log_bin';";
     //查看binlog format
-    private static final String CHECK_BINLOG_FORMAT = "show variables where variable_name = 'binlog_format';;";
+    private static final String CHECK_BINLOG_FORMAT = "show variables where variable_name = 'binlog_format';";
     //校验用户是否有权限
     private static final String CHECK_USER_PRIVILEGE = "show master status ;";
 

File: flinkx-mongodb/flinkx-mongodb-reader/src/main/java/com/dtstack/flinkx/mongodb/reader/MongodbInputFormat.java
Patch:
@@ -68,6 +68,7 @@ public void openInputFormat() throws IOException {
 
     @Override
     protected void openInternal(InputSplit inputSplit) throws IOException {
+        LOG.info("inputSplit = {}", inputSplit);
         MongodbInputSplit split = (MongodbInputSplit) inputSplit;
         FindIterable<Document> findIterable;
 
@@ -137,7 +138,8 @@ public InputSplit[] createInputSplitsInternal(int minNumSplits) throws IOExcepti
             MongoDatabase db = client.getDatabase(mongodbConfig.getDatabase());
             MongoCollection<Document> collection = db.getCollection(mongodbConfig.getCollectionName());
 
-            long docNum = filter == null ? collection.countDocuments() : collection.countDocuments(filter);
+            //不使用 collection.countDocuments() 获取总数是因为这个方法在大数据量时超时，导致出现超时异常结束任务
+            long docNum = collection.estimatedDocumentCount();
             if(docNum <= minNumSplits){
                 splits.add(new MongodbInputSplit(0,(int)docNum));
                 return splits.toArray(new MongodbInputSplit[splits.size()]);

File: flinkx-postgresql/flinkx-postgresql-writer/src/main/java/com/dtstack/flinkx/postgresql/writer/PostgresqlWriter.java
Patch:
@@ -39,11 +39,10 @@
  */
 public class PostgresqlWriter extends JdbcDataWriter {
 
-    public String schema;
 
     public PostgresqlWriter(DataTransferConfig config) {
         super(config);
-        schema = config.getJob().getContent().get(0).getWriter().getParameter().getConnection().get(0).getSchema();
+        String schema = config.getJob().getContent().get(0).getWriter().getParameter().getConnection().get(0).getSchema();
         if (StringUtils.isNotEmpty(schema)){
             table = schema + ConstantValue.POINT_SYMBOL + table;
         }

File: flinkx-test/src/main/java/com/dtstack/flinkx/test/LocalTest.java
Patch:
@@ -138,7 +138,7 @@ public static void main(String[] args) throws Exception{
 //        conf.setString("metrics.reporter.promgateway.randomJobNameSuffix","true");
 //        conf.setString("metrics.reporter.promgateway.deleteOnShutdown","true");
 
-        String jobPath = "C:\\Users\\少年不识愁\\Desktop\\任务脚本\\stream_postgresql.json";
+        String jobPath = "D:\\dtstack\\flinkx-all\\flinkx-test\\src\\main\\resources\\dev_test_job\\metadatasqlserver_stream.json";
         JobExecutionResult result = LocalTest.runJob(new File(jobPath), confProperties, null);
         ResultPrintUtil.printResult(result);
     }

File: flinkx-hdfs/flinkx-hdfs-core/src/main/java/com/dtstack/flinkx/hdfs/HdfsConfigKeys.java
Patch:
@@ -34,6 +34,9 @@ public class HdfsConfigKeys {
 
     public static final String KEY_HADOOP_CONFIG = "hadoopConfig";
 
+    //hadoop是否是ha环境
+    public static final String KEY_HADOOP_HA= "isHa";
+
     public static final String KEY_FILTER = "filterRegex";
 
     public static final String KEY_FILE_TYPE = "fileType";

File: flinkx-hdfs/flinkx-hdfs-reader/src/main/java/com/dtstack/flinkx/hdfs/reader/BaseHdfsInputFormat.java
Patch:
@@ -45,6 +45,9 @@ public abstract class BaseHdfsInputFormat extends BaseRichInputFormat {
 
     protected Map<String,Object> hadoopConfig;
 
+    //hadoop是否是高可用
+    protected boolean isHa;
+
     protected List<MetaColumn> metaColumns;
 
     protected String inputPath;

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/BaseHdfsOutputFormat.java
Patch:
@@ -24,7 +24,6 @@
 import com.dtstack.flinkx.util.FileSystemUtil;
 import com.dtstack.flinkx.util.SysUtil;
 import com.google.gson.Gson;
-import com.google.gson.JsonParser;
 import org.apache.commons.lang3.StringUtils;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileStatus;
@@ -54,6 +53,9 @@ public abstract class BaseHdfsOutputFormat extends BaseFileOutputFormat {
     /** hdfs高可用配置 */
     protected Map<String,Object> hadoopConfig;
 
+    //hadoop是否是高可用
+    protected boolean isHa;
+
     protected String defaultFs;
 
     protected List<String> columnTypes;

File: flinkx-phoenix/flinkx-phoenix-reader/src/main/java/com/dtstack/flinkx/phoenix/reader/PhoenixReader.java
Patch:
@@ -39,7 +39,6 @@ public class PhoenixReader extends JdbcDataReader {
     public PhoenixReader(DataTransferConfig config, StreamExecutionEnvironment env) {
         super(config, env);
         setDatabaseInterface(new PhoenixMeta());
-        dbUrl = DbUtil.formatJdbcUrl(dbUrl, Collections.singletonMap("zeroDateTimeBehavior", "convertToNull"));
     }
 
     @Override

File: flinkx-phoenix/flinkx-phoenix-writer/src/main/java/com/dtstack/flinkx/phoenix/writer/PhoenixWriter.java
Patch:
@@ -38,7 +38,6 @@ public class PhoenixWriter extends JdbcDataWriter {
     public PhoenixWriter(DataTransferConfig config) {
         super(config);
         setDatabaseInterface(new PhoenixMeta());
-        dbUrl = DbUtil.formatJdbcUrl(dbUrl, Collections.singletonMap("zeroDateTimeBehavior", "convertToNull"));
     }
 
     @Override

File: flinkx-phoenix5/flinkx-phoenix5-reader/src/main/java/com/dtstack/flinkx/phoenix5/reader/Phoenix5Reader.java
Patch:
@@ -39,7 +39,6 @@ public class Phoenix5Reader extends JdbcDataReader {
     public Phoenix5Reader(DataTransferConfig config, StreamExecutionEnvironment env) {
         super(config, env);
         setDatabaseInterface(new PhoenixMeta());
-        dbUrl = DbUtil.formatJdbcUrl(dbUrl, Collections.singletonMap("zeroDateTimeBehavior", "convertToNull"));
     }
 
     @Override

File: flinkx-phoenix5/flinkx-phoenix5-writer/src/main/java/com/dtstack/flinkx/phoenix5/writer/Phoenix5Writer.java
Patch:
@@ -38,7 +38,6 @@ public class Phoenix5Writer extends JdbcDataWriter {
     public Phoenix5Writer(DataTransferConfig config) {
         super(config);
         setDatabaseInterface(new PhoenixMeta());
-        dbUrl = DbUtil.formatJdbcUrl(dbUrl, Collections.singletonMap("zeroDateTimeBehavior", "convertToNull"));
     }
 
     @Override

File: flinkx-kafka/flinkx-kafka-reader/src/main/java/com/dtstack/flinkx/kafka/client/KafkaClient.java
Patch:
@@ -96,7 +96,7 @@ public KafkaClient(Properties clientProps, long pollTimeout, KafkaBaseInputForma
                     "| therefore, no data will be read in this channel! |\n" +
                     "****************************************************");
             return;
-        }if(StartupMode.TIMESTAMP.equals(mode)){
+        }else if(StartupMode.TIMESTAMP.equals(mode)){
             Map<TopicPartition, Long> timestampMap = new HashMap<>(Math.max((int) (stateList.size()/.75f) + 1, 16));
             for (kafkaState state : stateList) {
                 TopicPartition tp = new TopicPartition(state.getTopic(), state.getPartition());

File: flinkx-kafka/flinkx-kafka-reader/src/main/java/com/dtstack/flinkx/kafka/client/KafkaClient.java
Patch:
@@ -96,7 +96,7 @@ public KafkaClient(Properties clientProps, long pollTimeout, KafkaBaseInputForma
                     "| therefore, no data will be read in this channel! |\n" +
                     "****************************************************");
             return;
-        }if(StartupMode.TIMESTAMP.equals(mode)){
+        }else if(StartupMode.TIMESTAMP.equals(mode)){
             Map<TopicPartition, Long> timestampMap = new HashMap<>(Math.max((int) (stateList.size()/.75f) + 1, 16));
             for (kafkaState state : stateList) {
                 TopicPartition tp = new TopicPartition(state.getTopic(), state.getPartition());

File: flinkx-phoenix/flinkx-phoenix-reader/src/main/java/com/dtstack/flinkx/phoenix/reader/PhoenixReader.java
Patch:
@@ -39,7 +39,6 @@ public class PhoenixReader extends JdbcDataReader {
     public PhoenixReader(DataTransferConfig config, StreamExecutionEnvironment env) {
         super(config, env);
         setDatabaseInterface(new PhoenixMeta());
-        dbUrl = DbUtil.formatJdbcUrl(dbUrl, Collections.singletonMap("zeroDateTimeBehavior", "convertToNull"));
     }
 
     @Override

File: flinkx-phoenix/flinkx-phoenix-writer/src/main/java/com/dtstack/flinkx/phoenix/writer/PhoenixWriter.java
Patch:
@@ -38,7 +38,6 @@ public class PhoenixWriter extends JdbcDataWriter {
     public PhoenixWriter(DataTransferConfig config) {
         super(config);
         setDatabaseInterface(new PhoenixMeta());
-        dbUrl = DbUtil.formatJdbcUrl(dbUrl, Collections.singletonMap("zeroDateTimeBehavior", "convertToNull"));
     }
 
     @Override

File: flinkx-phoenix5/flinkx-phoenix5-writer/src/main/java/com/dtstack/flinkx/phoenix5/writer/Phoenix5Writer.java
Patch:
@@ -38,7 +38,6 @@ public class Phoenix5Writer extends JdbcDataWriter {
     public Phoenix5Writer(DataTransferConfig config) {
         super(config);
         setDatabaseInterface(new Phoenix5DatabaseMeta());
-        dbUrl = DbUtil.formatJdbcUrl(dbUrl, Collections.singletonMap("zeroDateTimeBehavior", "convertToNull"));
     }
 
     @Override

File: flinkx-phoenix/flinkx-phoenix-reader/src/main/java/com/dtstack/flinkx/phoenix/reader/PhoenixReader.java
Patch:
@@ -39,7 +39,6 @@ public class PhoenixReader extends JdbcDataReader {
     public PhoenixReader(DataTransferConfig config, StreamExecutionEnvironment env) {
         super(config, env);
         setDatabaseInterface(new PhoenixMeta());
-        dbUrl = DbUtil.formatJdbcUrl(dbUrl, Collections.singletonMap("zeroDateTimeBehavior", "convertToNull"));
     }
 
     @Override

File: flinkx-phoenix/flinkx-phoenix-writer/src/main/java/com/dtstack/flinkx/phoenix/writer/PhoenixWriter.java
Patch:
@@ -38,7 +38,6 @@ public class PhoenixWriter extends JdbcDataWriter {
     public PhoenixWriter(DataTransferConfig config) {
         super(config);
         setDatabaseInterface(new PhoenixMeta());
-        dbUrl = DbUtil.formatJdbcUrl(dbUrl, Collections.singletonMap("zeroDateTimeBehavior", "convertToNull"));
     }
 
     @Override

File: flinkx-phoenix5/flinkx-phoenix5-reader/src/main/java/com/dtstack/flinkx/phoenix5/reader/Phoenix5Reader.java
Patch:
@@ -39,7 +39,6 @@ public class Phoenix5Reader extends JdbcDataReader {
     public Phoenix5Reader(DataTransferConfig config, StreamExecutionEnvironment env) {
         super(config, env);
         setDatabaseInterface(new PhoenixMeta());
-        dbUrl = DbUtil.formatJdbcUrl(dbUrl, Collections.singletonMap("zeroDateTimeBehavior", "convertToNull"));
     }
 
     @Override

File: flinkx-phoenix5/flinkx-phoenix5-writer/src/main/java/com/dtstack/flinkx/phoenix5/writer/Phoenix5Writer.java
Patch:
@@ -38,7 +38,6 @@ public class Phoenix5Writer extends JdbcDataWriter {
     public Phoenix5Writer(DataTransferConfig config) {
         super(config);
         setDatabaseInterface(new PhoenixMeta());
-        dbUrl = DbUtil.formatJdbcUrl(dbUrl, Collections.singletonMap("zeroDateTimeBehavior", "convertToNull"));
     }
 
     @Override

File: flinkx-test/src/main/java/com/dtstack/flinkx/test/LocalTest.java
Patch:
@@ -138,7 +138,7 @@ public static void main(String[] args) throws Exception{
 //        conf.setString("metrics.reporter.promgateway.randomJobNameSuffix","true");
 //        conf.setString("metrics.reporter.promgateway.deleteOnShutdown","true");
 
-        String jobPath = "D:\\dtstack\\flinkx-all\\flinkx-test\\src\\main\\resources\\dev_test_job\\metadatasqlserver_stream.json";
+        String jobPath = "C:\\Users\\少年不识愁\\Desktop\\任务脚本\\stream_postgresql.json";
         JobExecutionResult result = LocalTest.runJob(new File(jobPath), confProperties, null);
         ResultPrintUtil.printResult(result);
     }

File: flinkx-phoenix/flinkx-phoenix-reader/src/main/java/com/dtstack/flinkx/phoenix/reader/PhoenixReader.java
Patch:
@@ -39,7 +39,6 @@ public class PhoenixReader extends JdbcDataReader {
     public PhoenixReader(DataTransferConfig config, StreamExecutionEnvironment env) {
         super(config, env);
         setDatabaseInterface(new PhoenixMeta());
-        dbUrl = DbUtil.formatJdbcUrl(dbUrl, Collections.singletonMap("zeroDateTimeBehavior", "convertToNull"));
     }
 
     @Override

File: flinkx-phoenix/flinkx-phoenix-writer/src/main/java/com/dtstack/flinkx/phoenix/writer/PhoenixWriter.java
Patch:
@@ -38,7 +38,6 @@ public class PhoenixWriter extends JdbcDataWriter {
     public PhoenixWriter(DataTransferConfig config) {
         super(config);
         setDatabaseInterface(new PhoenixMeta());
-        dbUrl = DbUtil.formatJdbcUrl(dbUrl, Collections.singletonMap("zeroDateTimeBehavior", "convertToNull"));
     }
 
     @Override

File: flinkx-phoenix5/flinkx-phoenix5-reader/src/main/java/com/dtstack/flinkx/phoenix5/reader/Phoenix5Reader.java
Patch:
@@ -39,7 +39,6 @@ public class Phoenix5Reader extends JdbcDataReader {
     public Phoenix5Reader(DataTransferConfig config, StreamExecutionEnvironment env) {
         super(config, env);
         setDatabaseInterface(new PhoenixMeta());
-        dbUrl = DbUtil.formatJdbcUrl(dbUrl, Collections.singletonMap("zeroDateTimeBehavior", "convertToNull"));
     }
 
     @Override

File: flinkx-phoenix5/flinkx-phoenix5-writer/src/main/java/com/dtstack/flinkx/phoenix5/writer/Phoenix5Writer.java
Patch:
@@ -38,7 +38,6 @@ public class Phoenix5Writer extends JdbcDataWriter {
     public Phoenix5Writer(DataTransferConfig config) {
         super(config);
         setDatabaseInterface(new PhoenixMeta());
-        dbUrl = DbUtil.formatJdbcUrl(dbUrl, Collections.singletonMap("zeroDateTimeBehavior", "convertToNull"));
     }
 
     @Override

File: flinkx-hdfs/flinkx-hdfs-core/src/main/java/com/dtstack/flinkx/hdfs/HdfsConfigKeys.java
Patch:
@@ -34,6 +34,9 @@ public class HdfsConfigKeys {
 
     public static final String KEY_HADOOP_CONFIG = "hadoopConfig";
 
+    //hadoop是否是ha环境
+    public static final String KEY_HADOOP_HA= "isHa";
+
     public static final String KEY_FILTER = "filterRegex";
 
     public static final String KEY_FILE_TYPE = "fileType";

File: flinkx-hdfs/flinkx-hdfs-reader/src/main/java/com/dtstack/flinkx/hdfs/reader/BaseHdfsInputFormat.java
Patch:
@@ -45,6 +45,9 @@ public abstract class BaseHdfsInputFormat extends BaseRichInputFormat {
 
     protected Map<String,Object> hadoopConfig;
 
+    //hadoop是否是高可用
+    protected boolean isHa;
+
     protected List<MetaColumn> metaColumns;
 
     protected String inputPath;

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/BaseHdfsOutputFormat.java
Patch:
@@ -24,7 +24,6 @@
 import com.dtstack.flinkx.util.FileSystemUtil;
 import com.dtstack.flinkx.util.SysUtil;
 import com.google.gson.Gson;
-import com.google.gson.JsonParser;
 import org.apache.commons.lang3.StringUtils;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileStatus;
@@ -54,6 +53,9 @@ public abstract class BaseHdfsOutputFormat extends BaseFileOutputFormat {
     /** hdfs高可用配置 */
     protected Map<String,Object> hadoopConfig;
 
+    //hadoop是否是高可用
+    protected boolean isHa;
+
     protected String defaultFs;
 
     protected List<String> columnTypes;

File: flinkx-mongodb/flinkx-mongodb-reader/src/main/java/com/dtstack/flinkx/mongodb/reader/MongodbInputFormat.java
Patch:
@@ -68,6 +68,7 @@ public void openInputFormat() throws IOException {
 
     @Override
     protected void openInternal(InputSplit inputSplit) throws IOException {
+        LOG.info("inputSplit = {}", inputSplit);
         MongodbInputSplit split = (MongodbInputSplit) inputSplit;
         FindIterable<Document> findIterable;
 
@@ -137,7 +138,8 @@ public InputSplit[] createInputSplitsInternal(int minNumSplits) throws IOExcepti
             MongoDatabase db = client.getDatabase(mongodbConfig.getDatabase());
             MongoCollection<Document> collection = db.getCollection(mongodbConfig.getCollectionName());
 
-            long docNum = filter == null ? collection.countDocuments() : collection.countDocuments(filter);
+            //不使用 collection.countDocuments() 获取总数是因为这个方法在大数据量时超时，导致出现超时异常结束任务
+            long docNum = collection.estimatedDocumentCount();
             if(docNum <= minNumSplits){
                 splits.add(new MongodbInputSplit(0,(int)docNum));
                 return splits.toArray(new MongodbInputSplit[splits.size()]);

File: flinkx-core/src/main/java/com/dtstack/flinkx/inputformat/BaseRichInputFormat.java
Patch:
@@ -288,7 +288,7 @@ public Row nextRecord(Row row) throws IOException {
                 numReadCounter.add(1);
             }
             if(bytesReadCounter!=null){
-                bytesReadCounter.add(internalRow.toString().length());
+                bytesReadCounter.add(internalRow.toString().getBytes().length);
             }
         }
 

File: flinkx-hive/flinkx-hive-writer/src/main/java/com/dtstack/flinkx/hive/writer/HiveOutputFormat.java
Patch:
@@ -232,7 +232,7 @@ public void writeRecord(Row row) throws IOException {
 
             //row包含map嵌套的数据内容和channel， 而rowData是非常简单的纯数据，此处补上数据差额
             if (fromLogData && bytesWriteCounter != null) {
-                bytesWriteCounter.add((long) row.toString().length() - rowData.toString().length());
+                bytesWriteCounter.add((long)row.toString().getBytes().length - rowData.toString().getBytes().length);
             }
         } catch (Exception e) {
             // 写入产生的脏数据已经由hdfsOutputFormat处理了，这里不用再处理了，只打印日志

File: flinkx-core/src/main/java/com/dtstack/flinkx/decoder/JsonDecoder.java
Patch:
@@ -17,7 +17,7 @@
  */
 package com.dtstack.flinkx.decoder;
 
-import com.dtstack.flinkx.util.MapUtil;
+import com.dtstack.flinkx.util.GsonUtil;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -36,10 +36,9 @@ public class JsonDecoder implements IDecode {
     private static final String KEY_MESSAGE = "message";
 
     @Override
-    @SuppressWarnings("unchecked")
     public Map<String, Object> decode(final String message) {
         try {
-            Map<String, Object> event = MapUtil.objectToMap(message);
+            Map<String, Object> event = GsonUtil.GSON.fromJson(message, GsonUtil.gsonMapTypeToken);
             if (!event.containsKey(KEY_MESSAGE)) {
                 event.put(KEY_MESSAGE, message);
             }

File: flinkx-core/src/main/java/com/dtstack/flinkx/streaming/api/functions/source/DtInputFormatSourceFunction.java
Patch:
@@ -166,6 +166,9 @@ public void run(SourceContext<OUT> ctx) throws Exception {
 					throw finallyException;
 				}
 			}
+			if(null != tryException) {
+				throw tryException;
+			}
 		}
 	}
 

File: flinkx-emqx/flinkx-emqx-writer/src/main/java/com/dtstack/flinkx/emqx/format/EmqxOutputFormat.java
Patch:
@@ -21,9 +21,9 @@
 import com.dtstack.flinkx.exception.WriteRecordException;
 import com.dtstack.flinkx.outputformat.BaseRichOutputFormat;
 import com.dtstack.flinkx.util.ExceptionUtil;
+import com.dtstack.flinkx.util.MapUtil;
 import org.apache.commons.lang3.StringUtils;
 import org.apache.flink.types.Row;
-import com.fasterxml.jackson.databind.ObjectMapper;
 import org.eclipse.paho.client.mqttv3.MqttClient;
 import org.eclipse.paho.client.mqttv3.MqttConnectOptions;
 import org.eclipse.paho.client.mqttv3.MqttException;
@@ -54,7 +54,6 @@ public class EmqxOutputFormat extends BaseRichOutputFormat {
 
     private transient MqttClient client;
     protected static JsonDecoder jsonDecoder = new JsonDecoder();
-    protected static ObjectMapper objectMapper = new ObjectMapper();
 
 
     @Override
@@ -96,7 +95,7 @@ protected void writeSingleRecordInternal(Row row) throws WriteRecordException {
             }else{
                 map = Collections.singletonMap("message", row.toString());
             }
-            MqttMessage message = new MqttMessage(objectMapper.writeValueAsString(map).getBytes());
+            MqttMessage message = new MqttMessage(MapUtil.writeValueAsString(map).getBytes());
             message.setQos(qos);
             client.publish(topic, message);
         } catch (Throwable e) {

File: flinkx-ftp/flinkx-ftp-core/src/main/java/com/dtstack/flinkx/ftp/FtpHandler.java
Patch:
@@ -69,6 +69,8 @@ public void loginFtpServer(FtpConfig ftpConfig) {
             // 不需要写死ftp server的OS TYPE,FTPClient getSystemType()方法会自动识别
             ftpClient.setConnectTimeout(ftpConfig.getTimeout());
             ftpClient.setDataTimeout(ftpConfig.getTimeout());
+            //设置控制连接超时
+            ftpClient.setSoTimeout(ftpConfig.getTimeout());
             if (EFtpMode.PASV.name().equals(ftpConfig.getConnectPattern())) {
                 ftpClient.enterRemotePassiveMode();
                 ftpClient.enterLocalPassiveMode();

File: flinkx-ftp/flinkx-ftp-reader/src/main/java/com/dtstack/flinkx/ftp/reader/FtpInputFormat.java
Patch:
@@ -94,10 +94,10 @@ public void openInternal(InputSplit split) throws IOException {
         List<String> paths = inputSplit.getPaths();
 
         if (ftpConfig.getIsFirstLineHeader()){
-            br = new FtpSeqBufferedReader(ftpHandler,paths.iterator());
+            br = new FtpSeqBufferedReader(ftpHandler,paths.iterator(),ftpConfig);
             br.setFromLine(1);
         } else {
-            br = new FtpSeqBufferedReader(ftpHandler,paths.iterator());
+            br = new FtpSeqBufferedReader(ftpHandler,paths.iterator(),ftpConfig);
             br.setFromLine(0);
         }
         br.setFileEncoding(ftpConfig.getEncoding());

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsOrcOutputFormat.java
Patch:
@@ -240,7 +240,7 @@ private void getData(List<Object> recordList, int index, Row row) throws WriteRe
             case VARCHAR:
             case CHAR:
                 if (column instanceof Timestamp){
-                    SimpleDateFormat fm = DateUtil.getDateTimeFormatter();
+                    SimpleDateFormat fm = DateUtil.getDateTimeFormatterForMillisencond();
                     recordList.add(fm.format(column));
                 }else if (column instanceof Map || column instanceof List){
                     recordList.add(gson.toJson(column));

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsParquetOutputFormat.java
Patch:
@@ -238,7 +238,7 @@ private void addDataToGroup(Group group, Object valObj, int i) throws Exception{
             case "varchar" :
             case "string" :
                 if (valObj instanceof Timestamp){
-                    val=DateUtil.getDateTimeFormatter().format(valObj);
+                    val=DateUtil.getDateTimeFormatterForMillisencond().format(valObj);
                     group.add(colName,val);
                 }else if (valObj instanceof Map || valObj instanceof List){
                     group.add(colName,gson.toJson(valObj));

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsTextOutputFormat.java
Patch:
@@ -203,7 +203,7 @@ private void appendDataToString(StringBuilder sb, Object column, ColumnType colu
                 case VARCHAR:
                 case CHAR:
                     if (column instanceof Timestamp){
-                        SimpleDateFormat fm = DateUtil.getDateTimeFormatter();
+                        SimpleDateFormat fm = DateUtil.getDateTimeFormatterForMillisencond();
                         sb.append(fm.format(column));
                     }else if (column instanceof Map || column instanceof List){
                         sb.append(gson.toJson(column));

File: flinkx-launcher/src/main/java/com/dtstack/flinkx/launcher/perJob/FlinkPerJobUtil.java
Patch:
@@ -61,10 +61,10 @@ public static ClusterSpecification createClusterSpecification(Properties conProp
         int slotsPerTaskManager = 1;
 
         if (conProp != null) {
-            if (conProp.contains(JOBMANAGER_MEMORY_MB)) {
+            if (conProp.containsKey(JOBMANAGER_MEMORY_MB)) {
                 jobmanagerMemoryMb = Math.max(MIN_JM_MEMORY, ValueUtil.getInt(conProp.getProperty(JOBMANAGER_MEMORY_MB)));
             }
-            if (conProp.contains(TASKMANAGER_MEMORY_MB)) {
+            if (conProp.containsKey(TASKMANAGER_MEMORY_MB)) {
                 taskmanagerMemoryMb = Math.max(MIN_JM_MEMORY, ValueUtil.getInt(conProp.getProperty(TASKMANAGER_MEMORY_MB)));
             }
             if (conProp.containsKey(SLOTS_PER_TASKMANAGER)) {

File: flinkx-metadata/flinkx-metadata-reader/src/main/java/com/dtstack/flinkx/metadata/inputformat/BaseMetadataInputFormat.java
Patch:
@@ -82,7 +82,9 @@ public abstract class BaseMetadataInputFormat extends BaseRichInputFormat {
     @Override
     protected void openInternal(InputSplit inputSplit) throws IOException {
         try {
-            connection.set(getConnection());
+            if(connection.get() == null){
+                connection.set(getConnection());
+            }
             statement.set(connection.get().createStatement());
             currentDb.set(((MetadataInputSplit) inputSplit).getDbName());
             switchDatabase(currentDb.get());

File: flinkx-rdb/flinkx-rdb-reader/src/main/java/com.dtstack.flinkx.rdb.inputformat/JdbcInputFormat.java
Patch:
@@ -33,9 +33,9 @@
 import com.dtstack.flinkx.util.FileSystemUtil;
 import com.dtstack.flinkx.util.GsonUtil;
 import com.dtstack.flinkx.util.RetryUtil;
+import com.dtstack.flinkx.util.MapUtil;
 import com.dtstack.flinkx.util.StringUtil;
 import com.dtstack.flinkx.util.UrlUtil;
-import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.gson.Gson;
 import org.apache.commons.lang3.StringUtils;
 import org.apache.flink.core.io.InputSplit;
@@ -724,7 +724,7 @@ private void uploadMetricData() throws IOException {
             if (endLocationAccumulator != null) {
                 metrics.put(Metrics.END_LOCATION, endLocationAccumulator.getLocalValue());
             }
-            out.writeUTF(new ObjectMapper().writeValueAsString(metrics));
+            out.writeUTF(MapUtil.writeValueAsString(metrics));
         } catch (Exception e) {
             LOG.error("hadoop conf:{}", hadoopConfig);
             throw new IOException("Upload metric to HDFS error", e);

File: flinkx-metadata-phoenix5/flinkx-metadata-phoenix5-reader/src/main/java/com/dtstack/flinkx/metadataphoenix5/util/PhoenixMetadataCons.java
Patch:
@@ -67,9 +67,7 @@ public class PhoenixMetadataCons extends MetaDataCons {
 
     public static final String AUTHENTICATION_TYPE = "kerberos";
 
-    public static final String HBASE_ZOOKEEPER_QUORUM = "hbase.zookeeper.quorum";
-
-    public static final String ZOOKEEPER_ZNODE_PARENT = "zookeeper.znode.parent";
+    public static final String KEYTAB_FILE = "keytabFileName";
 
 
 }

File: flinkx-metadata-vertica/flinkx-metadata-vertica-reader/src/main/java/com/dtstack/flinkx/metadatavertica/inputformat/MetadataverticaInputFormat.java
Patch:
@@ -21,6 +21,7 @@
 import com.dtstack.flinkx.constants.ConstantValue;
 import com.dtstack.flinkx.metadata.inputformat.BaseMetadataInputFormat;
 import com.dtstack.flinkx.util.ExceptionUtil;
+import org.apache.commons.lang.StringUtils;
 
 import java.sql.ResultSet;
 import java.sql.SQLException;
@@ -145,7 +146,8 @@ public List<Map<String, Object>> queryColumn(String tableName) {
                 map.put(KEY_COLUMN_DEFAULT, resultSet.getString(RESULT_SET_COLUMN_DEF));
                 // 分区列信息,vertical partition express 中字段自动增加表名
                 String expressColumn = tableName + ConstantValue.POINT_SYMBOL + columnName;
-                if (ptColumnMap.get(tableName) != null && ptColumnMap.get(tableName).contains(expressColumn)) {
+                String partitionExpression = ptColumnMap.get(tableName);
+                if (StringUtils.isNotBlank(partitionExpression) && partitionExpression.contains(expressColumn)) {
                     ptColumns.add(map);
                 }else{
                     columns.add(map);

File: flinkx-ftp/flinkx-ftp-core/src/main/java/com/dtstack/flinkx/ftp/FtpHandler.java
Patch:
@@ -68,6 +68,8 @@ public void loginFtpServer(FtpConfig ftpConfig) {
             // 不需要写死ftp server的OS TYPE,FTPClient getSystemType()方法会自动识别
             ftpClient.setConnectTimeout(ftpConfig.getTimeout());
             ftpClient.setDataTimeout(ftpConfig.getTimeout());
+            //设置控制连接超时
+            ftpClient.setSoTimeout(ftpConfig.getTimeout());
             if (EFtpMode.PASV.name().equals(ftpConfig.getConnectPattern())) {
                 ftpClient.enterRemotePassiveMode();
                 ftpClient.enterLocalPassiveMode();

File: flinkx-ftp/flinkx-ftp-reader/src/main/java/com/dtstack/flinkx/ftp/reader/FtpInputFormat.java
Patch:
@@ -94,10 +94,10 @@ public void openInternal(InputSplit split) throws IOException {
         List<String> paths = inputSplit.getPaths();
 
         if (ftpConfig.getIsFirstLineHeader()){
-            br = new FtpSeqBufferedReader(ftpHandler,paths.iterator());
+            br = new FtpSeqBufferedReader(ftpHandler,paths.iterator(),ftpConfig);
             br.setFromLine(1);
         } else {
-            br = new FtpSeqBufferedReader(ftpHandler,paths.iterator());
+            br = new FtpSeqBufferedReader(ftpHandler,paths.iterator(),ftpConfig);
             br.setFromLine(0);
         }
         br.setFileEncoding(ftpConfig.getEncoding());

File: flinkx-metadata-phoenix5/flinkx-metadata-phoenix5-reader/src/main/java/com/dtstack/flinkx/metadataphoenix5/inputformat/Metadataphoenix5InputFormat.java
Patch:
@@ -152,7 +152,6 @@ public List<Map<String, Object>> queryColumn(String tableName) {
         List<Map<String, Object>> column = new LinkedList<>();
         String sql;
         if(isDefaultSchema()){
-            currentDb.set(null);
             sql = String.format(SQL_DEFAULT_COLUMN, tableName);
         }else {
             sql = String.format(SQL_COLUMN, currentDb.get(), tableName);

File: flinkx-core/src/main/java/com/dtstack/flinkx/streaming/api/functions/source/DtInputFormatSourceFunction.java
Patch:
@@ -166,6 +166,9 @@ public void run(SourceContext<OUT> ctx) throws Exception {
 					throw finallyException;
 				}
 			}
+			if(null != tryException) {
+				throw tryException;
+			}
 		}
 	}
 

File: flinkx-metadata-phoenix5/flinkx-metadata-phoenix5-reader/src/main/java/com/dtstack/flinkx/metadataphoenix5/inputformat/Metadataphoenix5InputFormat.java
Patch:
@@ -152,7 +152,6 @@ public List<Map<String, Object>> queryColumn(String tableName) {
         List<Map<String, Object>> column = new LinkedList<>();
         String sql;
         if(isDefaultSchema()){
-            currentDb.set(null);
             sql = String.format(SQL_DEFAULT_COLUMN, tableName);
         }else {
             sql = String.format(SQL_COLUMN, currentDb.get(), tableName);

File: flinkx-hdfs/flinkx-hdfs-core/src/main/java/com/dtstack/flinkx/hdfs/HdfsConfigKeys.java
Patch:
@@ -34,6 +34,9 @@ public class HdfsConfigKeys {
 
     public static final String KEY_HADOOP_CONFIG = "hadoopConfig";
 
+    //hadoop是否是ha环境
+    public static final String KEY_HADOOP_HA= "isHa";
+
     public static final String KEY_FILTER = "filterRegex";
 
     public static final String KEY_FILE_TYPE = "fileType";

File: flinkx-hdfs/flinkx-hdfs-reader/src/main/java/com/dtstack/flinkx/hdfs/reader/BaseHdfsInputFormat.java
Patch:
@@ -45,6 +45,9 @@ public abstract class BaseHdfsInputFormat extends BaseRichInputFormat {
 
     protected Map<String,Object> hadoopConfig;
 
+    //hadoop是否是高可用
+    protected boolean isHa;
+
     protected List<MetaColumn> metaColumns;
 
     protected String inputPath;

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/BaseHdfsOutputFormat.java
Patch:
@@ -24,7 +24,6 @@
 import com.dtstack.flinkx.util.FileSystemUtil;
 import com.dtstack.flinkx.util.SysUtil;
 import com.google.gson.Gson;
-import com.google.gson.JsonParser;
 import org.apache.commons.lang3.StringUtils;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileStatus;
@@ -54,6 +53,9 @@ public abstract class BaseHdfsOutputFormat extends BaseFileOutputFormat {
     /** hdfs高可用配置 */
     protected Map<String,Object> hadoopConfig;
 
+    //hadoop是否是高可用
+    protected boolean isHa;
+
     protected String defaultFs;
 
     protected List<String> columnTypes;

File: flinkx-core/src/main/java/com/dtstack/flinkx/streaming/api/functions/source/DtInputFormatSourceFunction.java
Patch:
@@ -157,9 +157,6 @@ public void run(SourceContext<OUT> ctx) throws Exception {
 				if (format instanceof RichInputFormat) {
 					((RichInputFormat) format).closeInputFormat();
 				}
-				if(null != tryException) {
-					throw tryException;
-				}
 			}catch (Exception finallyException){
 				if(null != tryException){
 					LOG.error(ExceptionUtil.getErrorMessage(finallyException));
@@ -169,6 +166,9 @@ public void run(SourceContext<OUT> ctx) throws Exception {
 					throw finallyException;
 				}
 			}
+			if(null != tryException) {
+				throw tryException;
+			}
 		}
 	}
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/decoder/JsonDecoder.java
Patch:
@@ -17,7 +17,7 @@
  */
 package com.dtstack.flinkx.decoder;
 
-import com.dtstack.flinkx.util.MapUtil;
+import com.dtstack.flinkx.util.GsonUtil;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -36,10 +36,9 @@ public class JsonDecoder implements IDecode {
     private static final String KEY_MESSAGE = "message";
 
     @Override
-    @SuppressWarnings("unchecked")
     public Map<String, Object> decode(final String message) {
         try {
-            Map<String, Object> event = MapUtil.objectToMap(message);
+            Map<String, Object> event = GsonUtil.GSON.fromJson(message, GsonUtil.gsonMapTypeToken);
             if (!event.containsKey(KEY_MESSAGE)) {
                 event.put(KEY_MESSAGE, message);
             }

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsOrcOutputFormat.java
Patch:
@@ -233,7 +233,7 @@ private void getData(List<Object> recordList, int index, Row row) throws WriteRe
             case VARCHAR:
             case CHAR:
                 if (column instanceof Timestamp){
-                    SimpleDateFormat fm = DateUtil.getDateTimeFormatter();
+                    SimpleDateFormat fm = DateUtil.getDateTimeFormatterForMillisencond();
                     recordList.add(fm.format(column));
                 }else {
                     recordList.add(rowData);

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsParquetOutputFormat.java
Patch:
@@ -218,7 +218,7 @@ private void addDataToGroup(Group group, Object valObj, int i) throws Exception{
             case "varchar" :
             case "string" :
                 if (valObj instanceof Timestamp){
-                    val=DateUtil.getDateTimeFormatter().format(valObj);
+                    val=DateUtil.getDateTimeFormatterForMillisencond().format(valObj);
                     group.add(colName,val);
                 }else {
                     group.add(colName,val);

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsTextOutputFormat.java
Patch:
@@ -198,7 +198,7 @@ private void appendDataToString(StringBuilder sb, Object column, ColumnType colu
                 case VARCHAR:
                 case CHAR:
                     if (column instanceof Timestamp){
-                        SimpleDateFormat fm = DateUtil.getDateTimeFormatter();
+                        SimpleDateFormat fm = DateUtil.getDateTimeFormatterForMillisencond();
                         sb.append(fm.format(column));
                     }else {
                         sb.append(rowData);

File: flinkx-core/src/main/java/com/dtstack/flinkx/inputformat/BaseRichInputFormat.java
Patch:
@@ -288,7 +288,7 @@ public Row nextRecord(Row row) throws IOException {
                 numReadCounter.add(1);
             }
             if(bytesReadCounter!=null){
-                bytesReadCounter.add(internalRow.toString().length());
+                bytesReadCounter.add(internalRow.toString().getBytes().length);
             }
         }
 

File: flinkx-hive/flinkx-hive-writer/src/main/java/com/dtstack/flinkx/hive/writer/HiveOutputFormat.java
Patch:
@@ -232,7 +232,7 @@ public void writeRecord(Row row) throws IOException {
 
             //row包含map嵌套的数据内容和channel， 而rowData是非常简单的纯数据，此处补上数据差额
             if (fromLogData && bytesWriteCounter != null) {
-                bytesWriteCounter.add((long) row.toString().length() - rowData.toString().length());
+                bytesWriteCounter.add((long)row.toString().getBytes().length - rowData.toString().getBytes().length);
             }
         } catch (Exception e) {
             // 写入产生的脏数据已经由hdfsOutputFormat处理了，这里不用再处理了，只打印日志

File: flinkx-core/src/main/java/com/dtstack/flinkx/decoder/JsonDecoder.java
Patch:
@@ -17,7 +17,7 @@
  */
 package com.dtstack.flinkx.decoder;
 
-import com.dtstack.flinkx.util.MapUtil;
+import com.dtstack.flinkx.util.GsonUtil;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -36,10 +36,9 @@ public class JsonDecoder implements IDecode {
     private static final String KEY_MESSAGE = "message";
 
     @Override
-    @SuppressWarnings("unchecked")
     public Map<String, Object> decode(final String message) {
         try {
-            Map<String, Object> event = MapUtil.objectToMap(message);
+            Map<String, Object> event = GsonUtil.GSON.fromJson(message, GsonUtil.gsonMapTypeToken);
             if (!event.containsKey(KEY_MESSAGE)) {
                 event.put(KEY_MESSAGE, message);
             }

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/GsonUtil.java
Patch:
@@ -51,6 +51,7 @@ public class GsonUtil {
     @SuppressWarnings("unchecked")
     private static Gson getGson() {
         GSON = new GsonBuilder()
+                .disableHtmlEscaping()
                 .setPrettyPrinting()
                 .create();
         try {

File: flinkx-hbase/flinkx-hbase-reader/src/main/java/com/dtstack/flinkx/hbase/reader/HbaseInputFormat.java
Patch:
@@ -69,8 +69,10 @@ public class HbaseInputFormat extends BaseRichInputFormat {
     protected List<String> columnTypes;
     protected boolean isBinaryRowkey;
     protected String encoding;
+    /**
+     * 客户端每次 rpc fetch 的行数
+     */
     protected int scanCacheSize;
-    protected int scanBatchSize;
     private transient Connection connection;
     private transient Scan scan;
     private transient Table table;
@@ -238,7 +240,6 @@ public void openInternal(InputSplit inputSplit) throws IOException {
         scan.setStartRow(startRow);
         scan.setStopRow(stopRow);
         scan.setCaching(scanCacheSize);
-        scan.setBatch(scanBatchSize);
         resultScanner = table.getScanner(scan);
     }
 

File: flinkx-hbase/flinkx-hbase-reader/src/main/java/com/dtstack/flinkx/hbase/reader/HbaseReader.java
Patch:
@@ -54,7 +54,6 @@ public class HbaseReader extends BaseDataReader {
     protected boolean isBinaryRowkey;
     protected String tableName;
     protected int scanCacheSize;
-    protected int scanBatchSize;
 
     public HbaseReader(DataTransferConfig config, StreamExecutionEnvironment env) {
         super(config, env);
@@ -71,7 +70,6 @@ public HbaseReader(DataTransferConfig config, StreamExecutionEnvironment env) {
 
         encoding = readerConfig.getParameter().getStringVal(HbaseConfigKeys.KEY_ENCODING);
         scanCacheSize = readerConfig.getParameter().getIntVal(HbaseConfigKeys.KEY_SCAN_CACHE_SIZE, HbaseConfigConstants.DEFAULT_SCAN_CACHE_SIZE);
-        scanBatchSize = readerConfig.getParameter().getIntVal(HbaseConfigKeys.KEY_SCAN_BATCH_SIZE, HbaseConfigConstants.DEFAULT_SCAN_BATCH_SIZE);
 
         List columns = readerConfig.getParameter().getColumn();
         if(columns != null && columns.size() > 0) {
@@ -110,7 +108,6 @@ public DataStream<Row> readData() {
         builder.setBytes(bytes);
         builder.setMonitorUrls(monitorUrls);
         builder.setScanCacheSize(scanCacheSize);
-        builder.setScanBatchSize(scanBatchSize);
         builder.setMonitorUrls(monitorUrls);
         builder.setTestConfig(testConfig);
         builder.setLogConfig(logConfig);

File: flinkx-hive/flinkx-hive-core/src/main/java/com/dtstack/flinkx/hive/util/AddressUtil.java
Patch:
@@ -18,6 +18,7 @@
 
 package com.dtstack.flinkx.hive.util;
 
+import com.dtstack.flinkx.util.ExceptionUtil;
 import org.apache.commons.net.telnet.TelnetClient;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -47,7 +48,7 @@ public static boolean telnet(String ip,int port){
                     client.disconnect();
                 }
             } catch (Exception e){
-                logger.error("{}",e);
+                logger.error("{}", ExceptionUtil.getErrorMessage(e));
             }
         }
     }

File: flinkx-oraclelogminer/flinkx-oraclelogminer-reader/src/main/java/com/dtstack/flinkx/oraclelogminer/format/LogFile.java
Patch:
@@ -15,8 +15,6 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-
-
 package com.dtstack.flinkx.oraclelogminer.format;
 
 import java.util.Objects;

File: flinkx-oraclelogminer/flinkx-oraclelogminer-reader/src/main/java/com/dtstack/flinkx/oraclelogminer/reader/OraclelogminerReader.java
Patch:
@@ -45,7 +45,7 @@ public OraclelogminerReader(DataTransferConfig config, StreamExecutionEnvironmen
         try {
             logMinerConfig = objectMapper.readValue(objectMapper.writeValueAsString(readerConfig.getParameter().getAll()), LogMinerConfig.class);
         } catch (Exception e) {
-            throw new RuntimeException("解析mongodb配置出错:", e);
+            throw new RuntimeException("parse logMiner config error:", e);
         }
 
         buildTableListenerRegex();

File: flinkx-postgresql/flinkx-postgresql-core/src/main/java/com/dtstack/flinkx/postgresql/PostgresqlTypeConverter.java
Patch:
@@ -77,7 +77,7 @@ public Object convert(Object data,String typeName) {
             if(dataValue.contains(".")){
                 dataValue =  new BigDecimal(dataValue).stripTrailingZeros().toPlainString();
             }
-            data = Integer.parseInt(dataValue);
+            data = Long.parseLong(dataValue);
         }
 
         return data;

File: flinkx-rdb/flinkx-rdb-core/src/main/java/com/dtstack/flinkx/rdb/util/DbUtil.java
Patch:
@@ -208,7 +208,7 @@ public static void closeDbResources(ResultSet rs, Statement stmt, Connection con
      */
     public static void commit(Connection conn){
         try {
-            if (!conn.isClosed() && !conn.getAutoCommit()){
+            if (null != conn && !conn.isClosed() && !conn.getAutoCommit()){
                 conn.commit();
             }
         } catch (SQLException e){
@@ -222,7 +222,7 @@ public static void commit(Connection conn){
      */
     public static void rollBack(Connection conn){
         try {
-            if (!conn.isClosed() && !conn.getAutoCommit()){
+            if (null != conn && !conn.isClosed() && !conn.getAutoCommit()){
                 conn.rollback();
             }
         } catch (SQLException e){

File: flinkx-phoenix/flinkx-phoenix-reader/src/main/java/com/dtstack/flinkx/phoenix/reader/PhoenixReader.java
Patch:
@@ -39,7 +39,7 @@ public class PhoenixReader extends JdbcDataReader {
     public PhoenixReader(DataTransferConfig config, StreamExecutionEnvironment env) {
         super(config, env);
         setDatabaseInterface(new PhoenixMeta());
-        //dbUrl = DbUtil.formatJdbcUrl(dbUrl, Collections.singletonMap("zeroDateTimeBehavior", "convertToNull"));
+      //  dbUrl = DbUtil.formatJdbcUrl(dbUrl, Collections.singletonMap("zeroDateTimeBehavior", "convertToNull"));
     }
 
     @Override

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsOrcOutputFormat.java
Patch:
@@ -233,7 +233,7 @@ private void getData(List<Object> recordList, int index, Row row) throws WriteRe
             case VARCHAR:
             case CHAR:
                 if (column instanceof Timestamp){
-                    SimpleDateFormat fm = DateUtil.getDateTimeFormatter();
+                    SimpleDateFormat fm = DateUtil.getDateTimeFormatterForMillisencond();
                     recordList.add(fm.format(column));
                 }else {
                     recordList.add(rowData);

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsParquetOutputFormat.java
Patch:
@@ -218,7 +218,7 @@ private void addDataToGroup(Group group, Object valObj, int i) throws Exception{
             case "varchar" :
             case "string" :
                 if (valObj instanceof Timestamp){
-                    val=DateUtil.getDateTimeFormatter().format(valObj);
+                    val=DateUtil.getDateTimeFormatterForMillisencond().format(valObj);
                     group.add(colName,val);
                 }else {
                     group.add(colName,val);

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsTextOutputFormat.java
Patch:
@@ -198,7 +198,7 @@ private void appendDataToString(StringBuilder sb, Object column, ColumnType colu
                 case VARCHAR:
                 case CHAR:
                     if (column instanceof Timestamp){
-                        SimpleDateFormat fm = DateUtil.getDateTimeFormatter();
+                        SimpleDateFormat fm = DateUtil.getDateTimeFormatterForMillisencond();
                         sb.append(fm.format(column));
                     }else {
                         sb.append(rowData);

File: flinkx-metadata-hbase/flinkx-metadata-hbase-reader/src/main/java/com/dtstack/flinkx/metadatahbase/inputformat/MetadatahbaseInputFormat.java
Patch:
@@ -119,7 +119,9 @@ protected List<Object> showTables() throws SQLException {
                 TableName tableName = table.getTableName();
                 // 排除系统表
                 if(!tableName.isSystemTable()){
-                    tableNameList.add(tableName.getNameAsString());
+                    //此时的表名带有namespace,需要去除
+                    String tableWithNameSpace = tableName.getNameAsString();
+                    tableNameList.add(tableWithNameSpace.split(ConstantValue.COLON_SYMBOL)[1]);
                 }
             }
         }catch (IOException e){

File: flinkx-metadata-hbase/flinkx-metadata-hbase-reader/src/main/java/com/dtstack/flinkx/metadatahbase/inputformat/MetadatahbaseInputFormat.java
Patch:
@@ -119,7 +119,9 @@ protected List<Object> showTables() throws SQLException {
                 TableName tableName = table.getTableName();
                 // 排除系统表
                 if(!tableName.isSystemTable()){
-                    tableNameList.add(tableName.getNameAsString());
+                    //此时的表名带有namespace,需要去除
+                    String tableWithNameSpace = tableName.getNameAsString();
+                    tableNameList.add(tableWithNameSpace.split(ConstantValue.COLON_SYMBOL)[1]);
                 }
             }
         }catch (IOException e){

File: flinkx-metadata-hbase/flinkx-metadata-hbase-reader/src/main/java/com/dtstack/flinkx/metadatahbase/reader/MetadatahbaseReader.java
Patch:
@@ -52,6 +52,9 @@ public MetadatahbaseReader(DataTransferConfig config, StreamExecutionEnvironment
         }
         path = config.getJob().getContent().get(0).getReader()
                 .getParameter().getStringVal(KEY_PATH, DEFAULT_PATH);
+        if(!hadoopConfig.containsKey(HConstants.ZOOKEEPER_ZNODE_PARENT)){
+            hadoopConfig.put(HConstants.ZOOKEEPER_ZNODE_PARENT, path);
+        }
     }
 
     @Override

File: flinkx-metadata-hbase/flinkx-metadata-hbase-reader/src/main/java/com/dtstack/flinkx/metadatahbase/reader/MetadatahbaseReader.java
Patch:
@@ -52,6 +52,9 @@ public MetadatahbaseReader(DataTransferConfig config, StreamExecutionEnvironment
         }
         path = config.getJob().getContent().get(0).getReader()
                 .getParameter().getStringVal(KEY_PATH, DEFAULT_PATH);
+        if(!hadoopConfig.containsKey(HConstants.ZOOKEEPER_ZNODE_PARENT)){
+            hadoopConfig.put(HConstants.ZOOKEEPER_ZNODE_PARENT, path);
+        }
     }
 
     @Override

File: flinkx-ftp/flinkx-ftp-core/src/main/java/com/dtstack/flinkx/ftp/FtpHandler.java
Patch:
@@ -68,6 +68,8 @@ public void loginFtpServer(FtpConfig ftpConfig) {
             // 不需要写死ftp server的OS TYPE,FTPClient getSystemType()方法会自动识别
             ftpClient.setConnectTimeout(ftpConfig.getTimeout());
             ftpClient.setDataTimeout(ftpConfig.getTimeout());
+            //设置控制连接超时
+            ftpClient.setSoTimeout(ftpConfig.getTimeout());
             if (EFtpMode.PASV.name().equals(ftpConfig.getConnectPattern())) {
                 ftpClient.enterRemotePassiveMode();
                 ftpClient.enterLocalPassiveMode();

File: flinkx-ftp/flinkx-ftp-reader/src/main/java/com/dtstack/flinkx/ftp/reader/FtpInputFormat.java
Patch:
@@ -94,10 +94,10 @@ public void openInternal(InputSplit split) throws IOException {
         List<String> paths = inputSplit.getPaths();
 
         if (ftpConfig.getIsFirstLineHeader()){
-            br = new FtpSeqBufferedReader(ftpHandler,paths.iterator());
+            br = new FtpSeqBufferedReader(ftpHandler,paths.iterator(),ftpConfig);
             br.setFromLine(1);
         } else {
-            br = new FtpSeqBufferedReader(ftpHandler,paths.iterator());
+            br = new FtpSeqBufferedReader(ftpHandler,paths.iterator(),ftpConfig);
             br.setFromLine(0);
         }
         br.setFileEncoding(ftpConfig.getEncoding());

File: flinkx-oraclelogminer/flinkx-oraclelogminer-reader/src/main/java/com/dtstack/flinkx/oraclelogminer/entity/QueueData.java
Patch:
@@ -17,8 +17,6 @@
  */
 package com.dtstack.flinkx.oraclelogminer.entity;
 
-import com.google.gson.Gson;
-
 import java.util.Map;
 
 /**
@@ -48,7 +46,7 @@ public Map<String, Object> getData() {
     public String toString() {
         return "QueueData{" +
                 "scn=" + scn +
-                ", data=" + new Gson().toJson(data) +
+                ", data=" + data +
                 '}';
     }
 }

File: flinkx-oraclelogminer/flinkx-oraclelogminer-reader/src/main/java/com/dtstack/flinkx/oraclelogminer/format/LogMinerConfig.java
Patch:
@@ -65,9 +65,9 @@ public class LogMinerConfig implements Serializable {
     private List<String> table;
 
     /**
-     * LogMiner执行查询SQL的超时参数
+     * LogMiner执行查询SQL的超时参数，单位秒
      */
-    private Long queryTimeout = 3000L;
+    private Long queryTimeout = 300L;
 
     /**
      * Oracle 12c第二个版本之后LogMiner不支持自动添加日志

File: flinkx-oraclelogminer/flinkx-oraclelogminer-reader/src/main/java/com/dtstack/flinkx/oraclelogminer/format/OracleLogMinerInputFormat.java
Patch:
@@ -91,7 +91,7 @@ public FormatState getFormatState() {
     }
 
     @Override
-    public boolean reachedEnd() throws IOException {
+    public boolean reachedEnd() {
         return false;
     }
 

File: flinkx-clickhouse/flinkx-clickhouse-core/src/main/java/com/dtstack/flinkx/clickhouse/core/ClickhouseUtil.java
Patch:
@@ -37,8 +37,8 @@ public class ClickhouseUtil {
 
     public static Connection getConnection(String url, String username, String password) throws SQLException {
         Properties properties = new Properties();
-        properties.put(ClickHouseQueryParam.USER, username);
-        properties.put(ClickHouseQueryParam.PASSWORD, password);
+        properties.put(ClickHouseQueryParam.USER.getKey(), username);
+        properties.put(ClickHouseQueryParam.PASSWORD.getKey(), password);
         boolean failed = true;
         Connection conn = null;
         for (int i = 0; i < MAX_RETRY_TIMES && failed; ++i) {

File: flinkx-core/src/main/java/com/dtstack/flinkx/reader/DataReaderFactory.java
Patch:
@@ -41,7 +41,7 @@ public static BaseDataReader getDataReader(DataTransferConfig config, StreamExec
         try {
             String pluginName = config.getJob().getContent().get(0).getReader().getName();
             String pluginClassName = PluginUtil.getPluginClassName(pluginName);
-            Set<URL> urlList = PluginUtil.getJarFileDirPath(pluginName, config.getPluginRoot(), config.getRemotePluginPath());
+            Set<URL> urlList = PluginUtil.getJarFileDirPath(pluginName, config.getPluginRoot(), null);
 
             return ClassLoaderManager.newInstance(urlList, cl -> {
                 Class<?> clazz = cl.loadClass(pluginClassName);

File: flinkx-core/src/main/java/com/dtstack/flinkx/writer/DataWriterFactory.java
Patch:
@@ -40,7 +40,7 @@ public static BaseDataWriter getDataWriter(DataTransferConfig config) {
         try {
             String pluginName = config.getJob().getContent().get(0).getWriter().getName();
             String pluginClassName = PluginUtil.getPluginClassName(pluginName);
-            Set<URL> urlList = PluginUtil.getJarFileDirPath(pluginName, config.getPluginRoot(), config.getRemotePluginPath());
+            Set<URL> urlList = PluginUtil.getJarFileDirPath(pluginName, config.getPluginRoot(), null);
 
             return ClassLoaderManager.newInstance(urlList, cl -> {
                 Class<?> clazz = cl.loadClass(pluginClassName);

File: flinkx-hbase/flinkx-hbase-reader/src/main/java/com/dtstack/flinkx/hbase/reader/HbaseInputFormat.java
Patch:
@@ -69,8 +69,10 @@ public class HbaseInputFormat extends BaseRichInputFormat {
     protected List<String> columnTypes;
     protected boolean isBinaryRowkey;
     protected String encoding;
+    /**
+     * 客户端每次 rpc fetch 的行数
+     */
     protected int scanCacheSize;
-    protected int scanBatchSize;
     private transient Connection connection;
     private transient Scan scan;
     private transient Table table;
@@ -238,7 +240,6 @@ public void openInternal(InputSplit inputSplit) throws IOException {
         scan.setStartRow(startRow);
         scan.setStopRow(stopRow);
         scan.setCaching(scanCacheSize);
-        scan.setBatch(scanBatchSize);
         resultScanner = table.getScanner(scan);
     }
 

File: flinkx-hbase/flinkx-hbase-reader/src/main/java/com/dtstack/flinkx/hbase/reader/HbaseReader.java
Patch:
@@ -54,7 +54,6 @@ public class HbaseReader extends BaseDataReader {
     protected boolean isBinaryRowkey;
     protected String tableName;
     protected int scanCacheSize;
-    protected int scanBatchSize;
 
     public HbaseReader(DataTransferConfig config, StreamExecutionEnvironment env) {
         super(config, env);
@@ -71,7 +70,6 @@ public HbaseReader(DataTransferConfig config, StreamExecutionEnvironment env) {
 
         encoding = readerConfig.getParameter().getStringVal(HbaseConfigKeys.KEY_ENCODING);
         scanCacheSize = readerConfig.getParameter().getIntVal(HbaseConfigKeys.KEY_SCAN_CACHE_SIZE, HbaseConfigConstants.DEFAULT_SCAN_CACHE_SIZE);
-        scanBatchSize = readerConfig.getParameter().getIntVal(HbaseConfigKeys.KEY_SCAN_BATCH_SIZE, HbaseConfigConstants.DEFAULT_SCAN_BATCH_SIZE);
 
         List columns = readerConfig.getParameter().getColumn();
         if(columns != null && columns.size() > 0) {
@@ -110,7 +108,6 @@ public DataStream<Row> readData() {
         builder.setBytes(bytes);
         builder.setMonitorUrls(monitorUrls);
         builder.setScanCacheSize(scanCacheSize);
-        builder.setScanBatchSize(scanBatchSize);
         builder.setMonitorUrls(monitorUrls);
         builder.setTestConfig(testConfig);
         builder.setLogConfig(logConfig);

File: flinkx-hive/flinkx-hive-core/src/main/java/com/dtstack/flinkx/hive/util/AddressUtil.java
Patch:
@@ -18,6 +18,7 @@
 
 package com.dtstack.flinkx.hive.util;
 
+import com.dtstack.flinkx.util.ExceptionUtil;
 import org.apache.commons.net.telnet.TelnetClient;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -47,7 +48,7 @@ public static boolean telnet(String ip,int port){
                     client.disconnect();
                 }
             } catch (Exception e){
-                logger.error("{}",e);
+                logger.error("{}", ExceptionUtil.getErrorMessage(e));
             }
         }
     }

File: flinkx-kafka/flinkx-kafka-writer/src/main/java/com/dtstack/flinkx/kafka/writer/KafkaWriter.java
Patch:
@@ -18,6 +18,7 @@
 package com.dtstack.flinkx.kafka.writer;
 
 import com.dtstack.flinkx.config.DataTransferConfig;
+import com.dtstack.flinkx.kafkabase.writer.HeartBeatController;
 import com.dtstack.flinkx.kafkabase.writer.KafkaBaseWriter;
 import org.apache.flink.streaming.api.datastream.DataStream;
 import org.apache.flink.streaming.api.datastream.DataStreamSink;
@@ -50,6 +51,8 @@ public DataStreamSink<?> writeData(DataStream<Row> dataSet) {
         format.setDirtyPath(dirtyPath);
         format.setDirtyHadoopConfig(dirtyHadoopConfig);
         format.setSrcFieldNames(srcCols);
+        format.setHeartBeatController(new HeartBeatController());
+
         return createOutput(dataSet, format);
     }
 }

File: flinkx-kafka09/flinkx-kafka09-writer/src/main/java/com/dtstack/flinkx/kafka09/writer/Kafka09Writer.java
Patch:
@@ -20,6 +20,7 @@
 import com.dtstack.flinkx.config.DataTransferConfig;
 import com.dtstack.flinkx.config.WriterConfig;
 import com.dtstack.flinkx.kafkabase.KafkaConfigKeys;
+import com.dtstack.flinkx.kafkabase.writer.HeartBeatController;
 import com.dtstack.flinkx.kafkabase.writer.KafkaBaseWriter;
 import org.apache.commons.lang.StringUtils;
 import org.apache.flink.streaming.api.datastream.DataStream;

File: flinkx-kafka10/flinkx-kafka10-writer/src/main/java/com/dtstack/flinkx/kafka10/writer/Kafka10Writer.java
Patch:
@@ -18,6 +18,7 @@
 package com.dtstack.flinkx.kafka10.writer;
 
 import com.dtstack.flinkx.config.DataTransferConfig;
+import com.dtstack.flinkx.kafkabase.writer.HeartBeatController;
 import com.dtstack.flinkx.kafkabase.writer.KafkaBaseWriter;
 import org.apache.flink.streaming.api.datastream.DataStream;
 import org.apache.flink.streaming.api.datastream.DataStreamSink;
@@ -46,6 +47,8 @@ public DataStreamSink<?> writeData(DataStream<Row> dataSet) {
         format.setTableFields(tableFields);
         format.setProducerSettings(producerSettings);
         format.setRestoreConfig(restoreConfig);
+        format.setHeartBeatController(new HeartBeatController());
+
         format.setDirtyPath(dirtyPath);
         format.setDirtyHadoopConfig(dirtyHadoopConfig);
         format.setSrcFieldNames(srcCols);

File: flinkx-kafka11/flinkx-kafka11-writer/src/main/java/com/dtstack/flinkx/kafka11/writer/Kafka11Writer.java
Patch:
@@ -18,6 +18,7 @@
 package com.dtstack.flinkx.kafka11.writer;
 
 import com.dtstack.flinkx.config.DataTransferConfig;
+import com.dtstack.flinkx.kafkabase.writer.HeartBeatController;
 import com.dtstack.flinkx.kafkabase.writer.KafkaBaseWriter;
 import org.apache.flink.streaming.api.datastream.DataStream;
 import org.apache.flink.streaming.api.datastream.DataStreamSink;
@@ -46,6 +47,7 @@ public DataStreamSink<?> writeData(DataStream<Row> dataSet) {
         format.setProducerSettings(producerSettings);
         format.setRestoreConfig(restoreConfig);
         format.setTableFields(tableFields);
+        format.setHeartBeatController(new HeartBeatController());
         format.setDirtyPath(dirtyPath);
         format.setDirtyHadoopConfig(dirtyHadoopConfig);
         format.setSrcFieldNames(srcCols);

File: flinkx-oraclelogminer/flinkx-oraclelogminer-reader/src/main/java/com/dtstack/flinkx/oraclelogminer/format/LogMinerListener.java
Patch:
@@ -117,7 +117,7 @@ public void run() {
             try {
                 if (logMinerConnection.hasNext()) {
                     log = logMinerConnection.next();
-                    queue.put(logParser.parse(log));
+                    queue.put(logParser.parse(log, logMinerConnection.isOracle10));
                 } else {
                     logMinerConnection.closeStmt();
                     logMinerConnection.startOrUpdateLogMiner(positionManager.getPosition());

File: flinkx-postgresql/flinkx-postgresql-core/src/main/java/com/dtstack/flinkx/postgresql/PostgresqlTypeConverter.java
Patch:
@@ -77,7 +77,7 @@ public Object convert(Object data,String typeName) {
             if(dataValue.contains(".")){
                 dataValue =  new BigDecimal(dataValue).stripTrailingZeros().toPlainString();
             }
-            data = Integer.parseInt(dataValue);
+            data = Long.parseLong(dataValue);
         }
 
         return data;

File: flinkx-rdb/flinkx-rdb-core/src/main/java/com/dtstack/flinkx/rdb/util/DbUtil.java
Patch:
@@ -209,7 +209,7 @@ public static void closeDbResources(ResultSet rs, Statement stmt, Connection con
      */
     public static void commit(Connection conn){
         try {
-            if (!conn.isClosed() && !conn.getAutoCommit()){
+            if (null != conn && !conn.isClosed() && !conn.getAutoCommit()){
                 conn.commit();
             }
         } catch (SQLException e){
@@ -223,7 +223,7 @@ public static void commit(Connection conn){
      */
     public static void rollBack(Connection conn){
         try {
-            if (!conn.isClosed() && !conn.getAutoCommit()){
+            if (null != conn && !conn.isClosed() && !conn.getAutoCommit()){
                 conn.rollback();
             }
         } catch (SQLException e){

File: flinkx-core/src/main/java/com/dtstack/flinkx/constants/ConstantValue.java
Patch:
@@ -33,7 +33,6 @@ public class ConstantValue {
     public static final String SINGLE_QUOTE_MARK_SYMBOL = "'";
     public static final String DOUBLE_QUOTE_MARK_SYMBOL = "\"";
     public static final String COMMA_SYMBOL = ",";
-    public static final String COLON_SYMBOL = ":";
 
     public static final String SINGLE_SLASH_SYMBOL = "/";
     public static final String DOUBLE_SLASH_SYMBOL = "//";

File: flinkx-core/src/main/java/com/dtstack/flinkx/constants/ConstantValue.java
Patch:
@@ -33,7 +33,6 @@ public class ConstantValue {
     public static final String SINGLE_QUOTE_MARK_SYMBOL = "'";
     public static final String DOUBLE_QUOTE_MARK_SYMBOL = "\"";
     public static final String COMMA_SYMBOL = ",";
-    public static final String COLON_SYMBOL = ":";
 
     public static final String SINGLE_SLASH_SYMBOL = "/";
     public static final String DOUBLE_SLASH_SYMBOL = "//";

File: flinkx-core/src/main/java/com/dtstack/flinkx/streaming/api/functions/source/DtInputFormatSourceFunction.java
Patch:
@@ -157,6 +157,9 @@ public void run(SourceContext<OUT> ctx) throws Exception {
 				if (format instanceof RichInputFormat) {
 					((RichInputFormat) format).closeInputFormat();
 				}
+				if(null != tryException) {
+					throw tryException;
+				}
 			}catch (Exception finallyException){
 				if(null != tryException){
 					LOG.error(ExceptionUtil.getErrorMessage(finallyException));

File: flinkx-phoenix/flinkx-phoenix-reader/src/main/java/com/dtstack/flinkx/phoenix/reader/PhoenixReader.java
Patch:
@@ -39,7 +39,7 @@ public class PhoenixReader extends JdbcDataReader {
     public PhoenixReader(DataTransferConfig config, StreamExecutionEnvironment env) {
         super(config, env);
         setDatabaseInterface(new PhoenixMeta());
-        dbUrl = DbUtil.formatJdbcUrl(dbUrl, Collections.singletonMap("zeroDateTimeBehavior", "convertToNull"));
+        //dbUrl = DbUtil.formatJdbcUrl(dbUrl, Collections.singletonMap("zeroDateTimeBehavior", "convertToNull"));
     }
 
     @Override

File: flinkx-phoenix/flinkx-phoenix-writer/src/main/java/com/dtstack/flinkx/phoenix/writer/PhoenixWriter.java
Patch:
@@ -38,7 +38,7 @@ public class PhoenixWriter extends JdbcDataWriter {
     public PhoenixWriter(DataTransferConfig config) {
         super(config);
         setDatabaseInterface(new PhoenixMeta());
-        dbUrl = DbUtil.formatJdbcUrl(dbUrl, Collections.singletonMap("zeroDateTimeBehavior", "convertToNull"));
+       // dbUrl = DbUtil.formatJdbcUrl(dbUrl, Collections.singletonMap("zeroDateTimeBehavior", "convertToNull"));
     }
 
     @Override

File: flinkx-phoenix5/flinkx-phoenix5-reader/src/main/java/com/dtstack/flinkx/phoenix5/reader/Phoenix5Reader.java
Patch:
@@ -39,7 +39,7 @@ public class Phoenix5Reader extends JdbcDataReader {
     public Phoenix5Reader(DataTransferConfig config, StreamExecutionEnvironment env) {
         super(config, env);
         setDatabaseInterface(new PhoenixMeta());
-        dbUrl = DbUtil.formatJdbcUrl(dbUrl, Collections.singletonMap("zeroDateTimeBehavior", "convertToNull"));
+        //dbUrl = DbUtil.formatJdbcUrl(dbUrl, Collections.singletonMap("zeroDateTimeBehavior", "convertToNull"));
     }
 
     @Override

File: flinkx-phoenix5/flinkx-phoenix5-writer/src/main/java/com/dtstack/flinkx/phoenix5/writer/Phoenix5Writer.java
Patch:
@@ -38,7 +38,7 @@ public class Phoenix5Writer extends JdbcDataWriter {
     public Phoenix5Writer(DataTransferConfig config) {
         super(config);
         setDatabaseInterface(new PhoenixMeta());
-        dbUrl = DbUtil.formatJdbcUrl(dbUrl, Collections.singletonMap("zeroDateTimeBehavior", "convertToNull"));
+      //  dbUrl = DbUtil.formatJdbcUrl(dbUrl, Collections.singletonMap("zeroDateTimeBehavior", "convertToNull"));
     }
 
     @Override

File: flinkx-emqx/flinkx-emqx-writer/src/main/java/com/dtstack/flinkx/emqx/format/EmqxOutputFormat.java
Patch:
@@ -21,9 +21,9 @@
 import com.dtstack.flinkx.exception.WriteRecordException;
 import com.dtstack.flinkx.outputformat.BaseRichOutputFormat;
 import com.dtstack.flinkx.util.ExceptionUtil;
+import com.dtstack.flinkx.util.MapUtil;
 import org.apache.commons.lang3.StringUtils;
 import org.apache.flink.types.Row;
-import org.codehaus.jackson.map.ObjectMapper;
 import org.eclipse.paho.client.mqttv3.MqttClient;
 import org.eclipse.paho.client.mqttv3.MqttConnectOptions;
 import org.eclipse.paho.client.mqttv3.MqttException;
@@ -54,7 +54,6 @@ public class EmqxOutputFormat extends BaseRichOutputFormat {
 
     private transient MqttClient client;
     protected static JsonDecoder jsonDecoder = new JsonDecoder();
-    protected static ObjectMapper objectMapper = new ObjectMapper();
 
 
     @Override
@@ -96,7 +95,7 @@ protected void writeSingleRecordInternal(Row row) throws WriteRecordException {
             }else{
                 map = Collections.singletonMap("message", row.toString());
             }
-            MqttMessage message = new MqttMessage(objectMapper.writeValueAsString(map).getBytes());
+            MqttMessage message = new MqttMessage(MapUtil.writeValueAsString(map).getBytes());
             message.setQos(qos);
             client.publish(topic, message);
         } catch (Throwable e) {

File: flinkx-kafka/flinkx-kafka-writer/src/main/java/com/dtstack/flinkx/kafka/writer/KafkaOutputFormat.java
Patch:
@@ -21,6 +21,7 @@
 import com.dtstack.flinkx.kafkabase.Formatter;
 import com.dtstack.flinkx.kafkabase.writer.KafkaBaseOutputFormat;
 import com.dtstack.flinkx.util.ExceptionUtil;
+import com.dtstack.flinkx.util.MapUtil;
 import org.apache.flink.configuration.Configuration;
 import org.apache.kafka.clients.producer.KafkaProducer;
 import org.apache.kafka.clients.producer.ProducerConfig;
@@ -59,7 +60,7 @@ public void configure(Configuration parameters) {
     protected void emit(Map event) throws IOException {
         heartBeatController.acquire();
         String tp = Formatter.format(event, topic, timezone);
-        producer.send(new ProducerRecord<>(tp, event.toString(), objectMapper.writeValueAsString(event)), (metadata, exception) -> {
+        producer.send(new ProducerRecord<>(tp, event.toString(), MapUtil.writeValueAsString(event)), (metadata, exception) -> {
         if(Objects.nonNull(exception)){
             String errorMessage = String.format("send data failed,data 【%s】 ,error info  %s",event,ExceptionUtil.getErrorMessage(exception));
             LOG.warn(errorMessage);

File: flinkx-kafka09/flinkx-kafka09-writer/src/main/java/com/dtstack/flinkx/kafka09/writer/Kafka09OutputFormat.java
Patch:
@@ -19,8 +19,8 @@
 
 import com.dtstack.flinkx.kafkabase.Formatter;
 import com.dtstack.flinkx.kafkabase.writer.AddressUtil;
-import com.dtstack.flinkx.kafkabase.writer.HeartBeatController;
 import com.dtstack.flinkx.kafkabase.writer.KafkaBaseOutputFormat;
+import com.dtstack.flinkx.util.MapUtil;
 import org.apache.flink.configuration.Configuration;
 import org.apache.kafka.clients.producer.KafkaProducer;
 import org.apache.kafka.clients.producer.ProducerRecord;
@@ -73,7 +73,7 @@ public void configure(Configuration parameters) {
     protected void emit(Map event) throws IOException {
         heartBeatController.acquire();
         String tp = Formatter.format(event, topic, timezone);
-        producer.send(new ProducerRecord<>(tp, event.toString(), objectMapper.writeValueAsString(event)), (metadata, exception) -> {
+        producer.send(new ProducerRecord<>(tp, event.toString(), MapUtil.writeValueAsString(event)), (metadata, exception) -> {
             if (Objects.nonNull(exception)) {
                 LOG.warn("kafka writeSingleRecordInternal error:{}", exception.getMessage(), exception);
                 heartBeatController.onFailed(exception);

File: flinkx-kafka10/flinkx-kafka10-writer/src/main/java/com/dtstack/flinkx/kafka10/writer/Kafka10OutputFormat.java
Patch:
@@ -20,6 +20,7 @@
 import com.dtstack.flinkx.kafkabase.Formatter;
 import com.dtstack.flinkx.kafkabase.writer.KafkaBaseOutputFormat;
 import com.dtstack.flinkx.util.ExceptionUtil;
+import com.dtstack.flinkx.util.MapUtil;
 import org.apache.flink.configuration.Configuration;
 import org.apache.kafka.clients.producer.KafkaProducer;
 import org.apache.kafka.clients.producer.ProducerConfig;
@@ -58,7 +59,7 @@ public void configure(Configuration parameters) {
     protected void emit(Map event) throws IOException {
         heartBeatController.acquire();
         String tp = Formatter.format(event, topic, timezone);
-        producer.send(new ProducerRecord<>(tp, event.toString(), objectMapper.writeValueAsString(event)), (metadata, exception) -> {
+        producer.send(new ProducerRecord<>(tp, event.toString(), MapUtil.writeValueAsString(event)), (metadata, exception) -> {
             if(Objects.nonNull(exception)){
                 String errorMessage = String.format("send data failed,data 【%s】 ,error info  %s",event,ExceptionUtil.getErrorMessage(exception));
                 LOG.warn(errorMessage);

File: flinkx-kafka11/flinkx-kafka11-writer/src/main/java/com/dtstack/flinkx/kafka11/writer/Kafka11OutputFormat.java
Patch:
@@ -20,6 +20,7 @@
 import com.dtstack.flinkx.kafkabase.Formatter;
 import com.dtstack.flinkx.kafkabase.writer.KafkaBaseOutputFormat;
 import com.dtstack.flinkx.util.ExceptionUtil;
+import com.dtstack.flinkx.util.MapUtil;
 import org.apache.flink.configuration.Configuration;
 import org.apache.kafka.clients.producer.KafkaProducer;
 import org.apache.kafka.clients.producer.ProducerConfig;
@@ -58,7 +59,7 @@ public void configure(Configuration parameters) {
     protected void emit(Map event) throws IOException {
         heartBeatController.acquire();
         String tp = Formatter.format(event, topic, timezone);
-        producer.send(new ProducerRecord<>(tp, event.toString(), objectMapper.writeValueAsString(event)), (metadata, exception) -> {
+        producer.send(new ProducerRecord<>(tp, event.toString(), MapUtil.writeValueAsString(event)), (metadata, exception) -> {
             if(Objects.nonNull(exception)){
                 String errorMessage = String.format("send data failed,data 【%s】 ,error info  %s",event,ExceptionUtil.getErrorMessage(exception));
                 LOG.warn(errorMessage);

File: flinkx-kb/flinkx-kb-writer/src/main/java/com/dtstack/flinkx/kafkabase/writer/KafkaBaseOutputFormat.java
Patch:
@@ -25,7 +25,6 @@
 import com.dtstack.flinkx.util.ExceptionUtil;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.types.Row;
-import org.codehaus.jackson.map.ObjectMapper;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -54,7 +53,6 @@ public class KafkaBaseOutputFormat extends BaseRichOutputFormat {
     protected Map<String, String> producerSettings;
     protected List<String> tableFields;
     protected static JsonDecoder jsonDecoder = new JsonDecoder();
-    protected static ObjectMapper objectMapper = new ObjectMapper();
     //和kafkaBroker连通性控制器
     protected HeartBeatController heartBeatController;
 

File: flinkx-rdb/flinkx-rdb-reader/src/main/java/com.dtstack.flinkx.rdb.inputformat/JdbcInputFormat.java
Patch:
@@ -31,6 +31,7 @@
 import com.dtstack.flinkx.util.ClassUtil;
 import com.dtstack.flinkx.util.ExceptionUtil;
 import com.dtstack.flinkx.util.FileSystemUtil;
+import com.dtstack.flinkx.util.MapUtil;
 import com.dtstack.flinkx.util.StringUtil;
 import com.dtstack.flinkx.util.UrlUtil;
 import com.google.gson.Gson;
@@ -42,7 +43,6 @@
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.fs.permission.FsPermission;
 import org.apache.hadoop.io.IOUtils;
-import org.codehaus.jackson.map.ObjectMapper;
 
 import java.io.IOException;
 import java.math.BigInteger;
@@ -757,7 +757,7 @@ private void uploadMetricData() throws IOException {
             if (endLocationAccumulator != null) {
                 metrics.put(Metrics.END_LOCATION, endLocationAccumulator.getLocalValue());
             }
-            out.writeUTF(new ObjectMapper().writeValueAsString(metrics));
+            out.writeUTF(MapUtil.writeValueAsString(metrics));
         } catch (Exception e) {
             LOG.error("hadoop conf:{}", hadoopConfig);
             throw new IOException("Upload metric to HDFS error", e);

File: flinkx-oraclelogminer/flinkx-oraclelogminer-reader/src/main/java/com/dtstack/flinkx/oraclelogminer/format/LogMinerListener.java
Patch:
@@ -97,6 +97,7 @@ public void start() {
         logMinerConnection.checkPrivileges();
 
         Long startScn = logMinerConnection.getStartScn(positionManager.getPosition());
+        logMinerConnection.setPreScn(startScn);
         positionManager.updatePosition(startScn);
 
         logMinerSelectSql = SqlUtil.buildSelectSql(logMinerConfig.getCat(), logMinerConfig.getListenerTables());

File: flinkx-oraclelogminer/flinkx-oraclelogminer-reader/src/main/java/com/dtstack/flinkx/oraclelogminer/format/LogMinerConfig.java
Patch:
@@ -65,9 +65,9 @@ public class LogMinerConfig implements Serializable {
     private List<String> table;
 
     /**
-     * LogMiner执行查询SQL的超时参数
+     * LogMiner执行查询SQL的超时参数，单位秒
      */
-    private Long queryTimeout = 3000L;
+    private Long queryTimeout = 300L;
 
     /**
      * Oracle 12c第二个版本之后LogMiner不支持自动添加日志

File: flinkx-oraclelogminer/flinkx-oraclelogminer-reader/src/main/java/com/dtstack/flinkx/oraclelogminer/format/OracleLogMinerInputFormat.java
Patch:
@@ -91,7 +91,7 @@ public FormatState getFormatState() {
     }
 
     @Override
-    public boolean reachedEnd() throws IOException {
+    public boolean reachedEnd() {
         return false;
     }
 

File: flinkx-oraclelogminer/flinkx-oraclelogminer-reader/src/main/java/com/dtstack/flinkx/oraclelogminer/format/LogParser.java
Patch:
@@ -68,11 +68,11 @@ private static String cleanString(String str) {
             str = str.replace("TIMESTAMP ", "");
         }
 
-        if (str.startsWith("'") && str.endsWith("'")) {
+        if (str.startsWith("'") && str.endsWith("'") && str.length() != 1) {
             str = str.substring(1, str.length() - 1);
         }
 
-        if (str.startsWith("\"") && str.endsWith("\"")) {
+        if (str.startsWith("\"") && str.endsWith("\"") && str.length() != 1) {
             str = str.substring(1, str.length() - 1);
         }
 

File: flinkx-oraclelogminer/flinkx-oraclelogminer-reader/src/main/java/com/dtstack/flinkx/oraclelogminer/entity/QueueData.java
Patch:
@@ -17,8 +17,6 @@
  */
 package com.dtstack.flinkx.oraclelogminer.entity;
 
-import com.google.gson.Gson;
-
 import java.util.Map;
 
 /**
@@ -48,7 +46,7 @@ public Map<String, Object> getData() {
     public String toString() {
         return "QueueData{" +
                 "scn=" + scn +
-                ", data=" + new Gson().toJson(data) +
+                ", data=" + data +
                 '}';
     }
 }

File: flinkx-kafka/flinkx-kafka-writer/src/main/java/com/dtstack/flinkx/kafka/writer/KafkaWriter.java
Patch:
@@ -18,6 +18,7 @@
 package com.dtstack.flinkx.kafka.writer;
 
 import com.dtstack.flinkx.config.DataTransferConfig;
+import com.dtstack.flinkx.kafkabase.writer.HeartBeatController;
 import com.dtstack.flinkx.kafkabase.writer.KafkaBaseWriter;
 import org.apache.flink.streaming.api.datastream.DataStream;
 import org.apache.flink.streaming.api.datastream.DataStreamSink;
@@ -50,6 +51,8 @@ public DataStreamSink<?> writeData(DataStream<Row> dataSet) {
         format.setDirtyPath(dirtyPath);
         format.setDirtyHadoopConfig(dirtyHadoopConfig);
         format.setSrcFieldNames(srcCols);
+        format.setHeartBeatController(new HeartBeatController());
+
         return createOutput(dataSet, format);
     }
 }

File: flinkx-kafka09/flinkx-kafka09-writer/src/main/java/com/dtstack/flinkx/kafka09/writer/Kafka09Writer.java
Patch:
@@ -20,6 +20,7 @@
 import com.dtstack.flinkx.config.DataTransferConfig;
 import com.dtstack.flinkx.config.WriterConfig;
 import com.dtstack.flinkx.kafkabase.KafkaConfigKeys;
+import com.dtstack.flinkx.kafkabase.writer.HeartBeatController;
 import com.dtstack.flinkx.kafkabase.writer.KafkaBaseWriter;
 import org.apache.commons.lang.StringUtils;
 import org.apache.flink.streaming.api.datastream.DataStream;

File: flinkx-kafka10/flinkx-kafka10-writer/src/main/java/com/dtstack/flinkx/kafka10/writer/Kafka10Writer.java
Patch:
@@ -18,6 +18,7 @@
 package com.dtstack.flinkx.kafka10.writer;
 
 import com.dtstack.flinkx.config.DataTransferConfig;
+import com.dtstack.flinkx.kafkabase.writer.HeartBeatController;
 import com.dtstack.flinkx.kafkabase.writer.KafkaBaseWriter;
 import org.apache.flink.streaming.api.datastream.DataStream;
 import org.apache.flink.streaming.api.datastream.DataStreamSink;
@@ -46,6 +47,8 @@ public DataStreamSink<?> writeData(DataStream<Row> dataSet) {
         format.setTableFields(tableFields);
         format.setProducerSettings(producerSettings);
         format.setRestoreConfig(restoreConfig);
+        format.setHeartBeatController(new HeartBeatController());
+
         format.setDirtyPath(dirtyPath);
         format.setDirtyHadoopConfig(dirtyHadoopConfig);
         format.setSrcFieldNames(srcCols);

File: flinkx-kafka11/flinkx-kafka11-writer/src/main/java/com/dtstack/flinkx/kafka11/writer/Kafka11Writer.java
Patch:
@@ -18,6 +18,7 @@
 package com.dtstack.flinkx.kafka11.writer;
 
 import com.dtstack.flinkx.config.DataTransferConfig;
+import com.dtstack.flinkx.kafkabase.writer.HeartBeatController;
 import com.dtstack.flinkx.kafkabase.writer.KafkaBaseWriter;
 import org.apache.flink.streaming.api.datastream.DataStream;
 import org.apache.flink.streaming.api.datastream.DataStreamSink;
@@ -46,6 +47,7 @@ public DataStreamSink<?> writeData(DataStream<Row> dataSet) {
         format.setProducerSettings(producerSettings);
         format.setRestoreConfig(restoreConfig);
         format.setTableFields(tableFields);
+        format.setHeartBeatController(new HeartBeatController());
         format.setDirtyPath(dirtyPath);
         format.setDirtyHadoopConfig(dirtyHadoopConfig);
         format.setSrcFieldNames(srcCols);

File: flinkx-oraclelogminer/flinkx-oraclelogminer-reader/src/main/java/com/dtstack/flinkx/oraclelogminer/format/LogMinerConnection.java
Patch:
@@ -636,10 +636,10 @@ public void closeStmt(){
             if(logMinerSelectStmt != null && !logMinerSelectStmt.isClosed()){
                 logMinerSelectStmt.close();
             }
-            logMinerSelectStmt = null;
         }catch (SQLException e){
-            throw new RuntimeException("关闭logMinerStartStmt出错", e);
+            LOG.warn("关闭logMinerStartStmt出错", e);
         }
+        logMinerSelectStmt = null;
     }
 
     enum ReadPosition{

File: flinkx-hbase/flinkx-hbase-reader/src/main/java/com/dtstack/flinkx/hbase/reader/HbaseInputFormat.java
Patch:
@@ -69,8 +69,10 @@ public class HbaseInputFormat extends BaseRichInputFormat {
     protected List<String> columnTypes;
     protected boolean isBinaryRowkey;
     protected String encoding;
+    /**
+     * 客户端每次 rpc fetch 的行数
+     */
     protected int scanCacheSize;
-    protected int scanBatchSize;
     private transient Connection connection;
     private transient Scan scan;
     private transient Table table;
@@ -238,7 +240,6 @@ public void openInternal(InputSplit inputSplit) throws IOException {
         scan.setStartRow(startRow);
         scan.setStopRow(stopRow);
         scan.setCaching(scanCacheSize);
-        scan.setBatch(scanBatchSize);
         resultScanner = table.getScanner(scan);
     }
 

File: flinkx-hbase/flinkx-hbase-reader/src/main/java/com/dtstack/flinkx/hbase/reader/HbaseReader.java
Patch:
@@ -54,7 +54,6 @@ public class HbaseReader extends BaseDataReader {
     protected boolean isBinaryRowkey;
     protected String tableName;
     protected int scanCacheSize;
-    protected int scanBatchSize;
 
     public HbaseReader(DataTransferConfig config, StreamExecutionEnvironment env) {
         super(config, env);
@@ -71,7 +70,6 @@ public HbaseReader(DataTransferConfig config, StreamExecutionEnvironment env) {
 
         encoding = readerConfig.getParameter().getStringVal(HbaseConfigKeys.KEY_ENCODING);
         scanCacheSize = readerConfig.getParameter().getIntVal(HbaseConfigKeys.KEY_SCAN_CACHE_SIZE, HbaseConfigConstants.DEFAULT_SCAN_CACHE_SIZE);
-        scanBatchSize = readerConfig.getParameter().getIntVal(HbaseConfigKeys.KEY_SCAN_BATCH_SIZE, HbaseConfigConstants.DEFAULT_SCAN_BATCH_SIZE);
 
         List columns = readerConfig.getParameter().getColumn();
         if(columns != null && columns.size() > 0) {
@@ -110,7 +108,6 @@ public DataStream<Row> readData() {
         builder.setBytes(bytes);
         builder.setMonitorUrls(monitorUrls);
         builder.setScanCacheSize(scanCacheSize);
-        builder.setScanBatchSize(scanBatchSize);
         builder.setMonitorUrls(monitorUrls);
         builder.setTestConfig(testConfig);
         builder.setLogConfig(logConfig);

File: flinkx-core/src/main/java/com/dtstack/flinkx/reader/DataReaderFactory.java
Patch:
@@ -41,7 +41,7 @@ public static BaseDataReader getDataReader(DataTransferConfig config, StreamExec
         try {
             String pluginName = config.getJob().getContent().get(0).getReader().getName();
             String pluginClassName = PluginUtil.getPluginClassName(pluginName);
-            Set<URL> urlList = PluginUtil.getJarFileDirPath(pluginName, config.getPluginRoot(), config.getRemotePluginPath());
+            Set<URL> urlList = PluginUtil.getJarFileDirPath(pluginName, config.getPluginRoot(), null);
 
             return ClassLoaderManager.newInstance(urlList, cl -> {
                 Class<?> clazz = cl.loadClass(pluginClassName);

File: flinkx-core/src/main/java/com/dtstack/flinkx/writer/DataWriterFactory.java
Patch:
@@ -40,7 +40,7 @@ public static BaseDataWriter getDataWriter(DataTransferConfig config) {
         try {
             String pluginName = config.getJob().getContent().get(0).getWriter().getName();
             String pluginClassName = PluginUtil.getPluginClassName(pluginName);
-            Set<URL> urlList = PluginUtil.getJarFileDirPath(pluginName, config.getPluginRoot(), config.getRemotePluginPath());
+            Set<URL> urlList = PluginUtil.getJarFileDirPath(pluginName, config.getPluginRoot(), null);
 
             return ClassLoaderManager.newInstance(urlList, cl -> {
                 Class<?> clazz = cl.loadClass(pluginClassName);

File: flinkx-kafka/flinkx-kafka-writer/src/main/java/com/dtstack/flinkx/kafka/writer/KafkaOutputFormat.java
Patch:
@@ -21,6 +21,7 @@
 import com.dtstack.flinkx.kafkabase.Formatter;
 import com.dtstack.flinkx.kafkabase.writer.KafkaBaseOutputFormat;
 import com.dtstack.flinkx.util.ExceptionUtil;
+import com.dtstack.flinkx.util.MapUtil;
 import org.apache.flink.configuration.Configuration;
 import org.apache.kafka.clients.producer.KafkaProducer;
 import org.apache.kafka.clients.producer.ProducerConfig;
@@ -57,7 +58,7 @@ public void configure(Configuration parameters) {
     @Override
     protected void emit(Map event) throws IOException {
         String tp = Formatter.format(event, topic, timezone);
-        producer.send(new ProducerRecord<>(tp, event.toString(), objectMapper.writeValueAsString(event)), (metadata, exception) -> {
+        producer.send(new ProducerRecord<>(tp, event.toString(), MapUtil.writeValueAsString(event)), (metadata, exception) -> {
         if(Objects.nonNull(exception)){
             String errorMessage = String.format("send data failed,data 【%s】 ,error info  %s",event,ExceptionUtil.getErrorMessage(exception));
             LOG.warn(errorMessage);

File: flinkx-kafka09/flinkx-kafka09-writer/src/main/java/com/dtstack/flinkx/kafka09/writer/Kafka09OutputFormat.java
Patch:
@@ -21,6 +21,7 @@
 import com.dtstack.flinkx.kafkabase.writer.AddressUtil;
 import com.dtstack.flinkx.kafkabase.writer.KafkaBaseOutputFormat;
 import com.dtstack.flinkx.util.GsonUtil;
+import com.dtstack.flinkx.util.MapUtil;
 import kafka.javaapi.producer.Producer;
 import kafka.producer.KeyedMessage;
 import kafka.producer.ProducerConfig;
@@ -77,7 +78,7 @@ public void configure(Configuration parameters) {
     protected void emit(Map event) throws IOException {
         heartBeatController.acquire();
         String tp = Formatter.format(event, topic, timezone);
-        producer.send(new ProducerRecord<>(tp, event.toString(), objectMapper.writeValueAsString(event)), (metadata, exception) -> {
+        producer.send(new ProducerRecord<>(tp, event.toString(), MapUtil.writeValueAsString(event)), (metadata, exception) -> {
             if (Objects.nonNull(exception)) {
                 LOG.warn("kafka writeSingleRecordInternal error:{}", exception.getMessage(), exception);
                 heartBeatController.onFailed(exception);

File: flinkx-kafka10/flinkx-kafka10-writer/src/main/java/com/dtstack/flinkx/kafka10/writer/Kafka10OutputFormat.java
Patch:
@@ -20,6 +20,7 @@
 import com.dtstack.flinkx.kafkabase.Formatter;
 import com.dtstack.flinkx.kafkabase.writer.KafkaBaseOutputFormat;
 import com.dtstack.flinkx.util.ExceptionUtil;
+import com.dtstack.flinkx.util.MapUtil;
 import org.apache.flink.configuration.Configuration;
 import org.apache.kafka.clients.producer.KafkaProducer;
 import org.apache.kafka.clients.producer.ProducerConfig;
@@ -56,7 +57,7 @@ public void configure(Configuration parameters) {
     @Override
     protected void emit(Map event) throws IOException {
         String tp = Formatter.format(event, topic, timezone);
-        producer.send(new ProducerRecord<>(tp, event.toString(), objectMapper.writeValueAsString(event)), (metadata, exception) -> {
+        producer.send(new ProducerRecord<>(tp, event.toString(), MapUtil.writeValueAsString(event)), (metadata, exception) -> {
             if(Objects.nonNull(exception)){
                 String errorMessage = String.format("send data failed,data 【%s】 ,error info  %s",event,ExceptionUtil.getErrorMessage(exception));
                 LOG.warn(errorMessage);

File: flinkx-kafka11/flinkx-kafka11-writer/src/main/java/com/dtstack/flinkx/kafka11/writer/Kafka11OutputFormat.java
Patch:
@@ -20,6 +20,7 @@
 import com.dtstack.flinkx.kafkabase.Formatter;
 import com.dtstack.flinkx.kafkabase.writer.KafkaBaseOutputFormat;
 import com.dtstack.flinkx.util.ExceptionUtil;
+import com.dtstack.flinkx.util.MapUtil;
 import org.apache.flink.configuration.Configuration;
 import org.apache.kafka.clients.producer.KafkaProducer;
 import org.apache.kafka.clients.producer.ProducerConfig;
@@ -56,7 +57,7 @@ public void configure(Configuration parameters) {
     @Override
     protected void emit(Map event) throws IOException {
         String tp = Formatter.format(event, topic, timezone);
-        producer.send(new ProducerRecord<>(tp, event.toString(), objectMapper.writeValueAsString(event)), (metadata, exception) -> {
+        producer.send(new ProducerRecord<>(tp, event.toString(), MapUtil.writeValueAsString(event)), (metadata, exception) -> {
             if(Objects.nonNull(exception)){
                 String errorMessage = String.format("send data failed,data 【%s】 ,error info  %s",event,ExceptionUtil.getErrorMessage(exception));
                 LOG.warn(errorMessage);

File: flinkx-kb/flinkx-kb-writer/src/main/java/com/dtstack/flinkx/kafkabase/writer/KafkaBaseOutputFormat.java
Patch:
@@ -24,7 +24,6 @@
 import com.dtstack.flinkx.util.ExceptionUtil;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.types.Row;
-import org.codehaus.jackson.map.ObjectMapper;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -51,7 +50,6 @@ public class KafkaBaseOutputFormat extends BaseRichOutputFormat {
     protected Map<String, String> producerSettings;
     protected List<String> tableFields;
     protected static JsonDecoder jsonDecoder = new JsonDecoder();
-    protected static ObjectMapper objectMapper = new ObjectMapper();
     //连续发送数据错误次数
     protected  int failedTimes = 0;
 

File: flinkx-kafka/flinkx-kafka-writer/src/main/java/com/dtstack/flinkx/kafka/writer/KafkaOutputFormat.java
Patch:
@@ -21,6 +21,7 @@
 import com.dtstack.flinkx.kafkabase.Formatter;
 import com.dtstack.flinkx.kafkabase.writer.KafkaBaseOutputFormat;
 import com.dtstack.flinkx.util.ExceptionUtil;
+import com.dtstack.flinkx.util.MapUtil;
 import org.apache.flink.configuration.Configuration;
 import org.apache.kafka.clients.producer.KafkaProducer;
 import org.apache.kafka.clients.producer.ProducerConfig;
@@ -57,7 +58,7 @@ public void configure(Configuration parameters) {
     @Override
     protected void emit(Map event) throws IOException {
         String tp = Formatter.format(event, topic, timezone);
-        producer.send(new ProducerRecord<>(tp, event.toString(), objectMapper.writeValueAsString(event)), (metadata, exception) -> {
+        producer.send(new ProducerRecord<>(tp, event.toString(), MapUtil.writeValueAsString(event)), (metadata, exception) -> {
         if(Objects.nonNull(exception)){
             String errorMessage = String.format("send data failed,data 【%s】 ,error info  %s",event,ExceptionUtil.getErrorMessage(exception));
             LOG.warn(errorMessage);

File: flinkx-kafka09/flinkx-kafka09-writer/src/main/java/com/dtstack/flinkx/kafka09/writer/Kafka09OutputFormat.java
Patch:
@@ -21,6 +21,7 @@
 import com.dtstack.flinkx.kafkabase.writer.AddressUtil;
 import com.dtstack.flinkx.kafkabase.writer.KafkaBaseOutputFormat;
 import com.dtstack.flinkx.util.GsonUtil;
+import com.dtstack.flinkx.util.MapUtil;
 import kafka.javaapi.producer.Producer;
 import kafka.producer.KeyedMessage;
 import kafka.producer.ProducerConfig;
@@ -77,7 +78,7 @@ public void configure(Configuration parameters) {
     protected void emit(Map event) throws IOException {
         heartBeatController.acquire();
         String tp = Formatter.format(event, topic, timezone);
-        producer.send(new ProducerRecord<>(tp, event.toString(), objectMapper.writeValueAsString(event)), (metadata, exception) -> {
+        producer.send(new ProducerRecord<>(tp, event.toString(), MapUtil.writeValueAsString(event)), (metadata, exception) -> {
             if (Objects.nonNull(exception)) {
                 LOG.warn("kafka writeSingleRecordInternal error:{}", exception.getMessage(), exception);
                 heartBeatController.onFailed(exception);

File: flinkx-kafka10/flinkx-kafka10-writer/src/main/java/com/dtstack/flinkx/kafka10/writer/Kafka10OutputFormat.java
Patch:
@@ -20,6 +20,7 @@
 import com.dtstack.flinkx.kafkabase.Formatter;
 import com.dtstack.flinkx.kafkabase.writer.KafkaBaseOutputFormat;
 import com.dtstack.flinkx.util.ExceptionUtil;
+import com.dtstack.flinkx.util.MapUtil;
 import org.apache.flink.configuration.Configuration;
 import org.apache.kafka.clients.producer.KafkaProducer;
 import org.apache.kafka.clients.producer.ProducerConfig;
@@ -56,7 +57,7 @@ public void configure(Configuration parameters) {
     @Override
     protected void emit(Map event) throws IOException {
         String tp = Formatter.format(event, topic, timezone);
-        producer.send(new ProducerRecord<>(tp, event.toString(), objectMapper.writeValueAsString(event)), (metadata, exception) -> {
+        producer.send(new ProducerRecord<>(tp, event.toString(), MapUtil.writeValueAsString(event)), (metadata, exception) -> {
             if(Objects.nonNull(exception)){
                 String errorMessage = String.format("send data failed,data 【%s】 ,error info  %s",event,ExceptionUtil.getErrorMessage(exception));
                 LOG.warn(errorMessage);

File: flinkx-kafka11/flinkx-kafka11-writer/src/main/java/com/dtstack/flinkx/kafka11/writer/Kafka11OutputFormat.java
Patch:
@@ -20,6 +20,7 @@
 import com.dtstack.flinkx.kafkabase.Formatter;
 import com.dtstack.flinkx.kafkabase.writer.KafkaBaseOutputFormat;
 import com.dtstack.flinkx.util.ExceptionUtil;
+import com.dtstack.flinkx.util.MapUtil;
 import org.apache.flink.configuration.Configuration;
 import org.apache.kafka.clients.producer.KafkaProducer;
 import org.apache.kafka.clients.producer.ProducerConfig;
@@ -56,7 +57,7 @@ public void configure(Configuration parameters) {
     @Override
     protected void emit(Map event) throws IOException {
         String tp = Formatter.format(event, topic, timezone);
-        producer.send(new ProducerRecord<>(tp, event.toString(), objectMapper.writeValueAsString(event)), (metadata, exception) -> {
+        producer.send(new ProducerRecord<>(tp, event.toString(), MapUtil.writeValueAsString(event)), (metadata, exception) -> {
             if(Objects.nonNull(exception)){
                 String errorMessage = String.format("send data failed,data 【%s】 ,error info  %s",event,ExceptionUtil.getErrorMessage(exception));
                 LOG.warn(errorMessage);

File: flinkx-kb/flinkx-kb-writer/src/main/java/com/dtstack/flinkx/kafkabase/writer/KafkaBaseOutputFormat.java
Patch:
@@ -24,7 +24,6 @@
 import com.dtstack.flinkx.util.ExceptionUtil;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.types.Row;
-import org.codehaus.jackson.map.ObjectMapper;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -51,7 +50,6 @@ public class KafkaBaseOutputFormat extends BaseRichOutputFormat {
     protected Map<String, String> producerSettings;
     protected List<String> tableFields;
     protected static JsonDecoder jsonDecoder = new JsonDecoder();
-    protected static ObjectMapper objectMapper = new ObjectMapper();
     //连续发送数据错误次数
     protected  int failedTimes = 0;
 

File: flinkx-kb/flinkx-kb-writer/src/main/java/com/dtstack/flinkx/kafkabase/writer/HeartBeatController.java
Patch:
@@ -63,9 +63,9 @@ public void acquire() {
         if (Objects.isNull(e)) {
             return;
         }
-        //连续发送3次数据错误或出现连接异常
-        if (failedTimes.get() >= detectingRetryTimes || e.toString().contains("Timeout") ) {
-            String message = "Error data is received 3 times continuously or datasource has error->" + ExceptionUtil.getErrorMessage(e);
+        //连续发送3次数据错误
+        if (failedTimes.get() >= detectingRetryTimes ) {
+            String message = "Failed to send data for three consecutive times，Please check whether the data source is normal，errorInfo->" + ExceptionUtil.getErrorMessage(e);
             logger.error(message);
             throw new DataSourceException("kafka",message, e);
         }

File: flinkx-kafka/flinkx-kafka-writer/src/main/java/com/dtstack/flinkx/kafka/writer/KafkaOutputFormat.java
Patch:
@@ -46,7 +46,7 @@ public void configure(Configuration parameters) {
         super.configure(parameters);
         props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
         props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
-        props.put(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG, 86400000);
+        props.put(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG, 60000);
         props.put(ProducerConfig.RETRIES_CONFIG, 1000000);
         props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, 1);
         if (producerSettings != null) {

File: flinkx-kafka09/flinkx-kafka09-writer/src/main/java/com/dtstack/flinkx/kafka09/writer/Kafka09OutputFormat.java
Patch:
@@ -58,7 +58,6 @@ public void configure(Configuration parameters) {
             props.putAll(producerSettings);
         }
         props.put("metadata.broker.list", brokerList);
-        props.put("bootstrap.servers", brokerList);
         producer = new KafkaProducer<>(props);
 
         LOG.info("brokerList {}", brokerList);

File: flinkx-kafka10/flinkx-kafka10-writer/src/main/java/com/dtstack/flinkx/kafka10/writer/Kafka10OutputFormat.java
Patch:
@@ -45,7 +45,7 @@ public void configure(Configuration parameters) {
         super.configure(parameters);
         props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
         props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
-        props.put(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG, 86400000);
+        props.put(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG, 60000);
         props.put(ProducerConfig.RETRIES_CONFIG, 1000000);
         props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, 1);
         if (producerSettings != null) {

File: flinkx-kafka11/flinkx-kafka11-writer/src/main/java/com/dtstack/flinkx/kafka11/writer/Kafka11OutputFormat.java
Patch:
@@ -45,7 +45,7 @@ public void configure(Configuration parameters) {
         super.configure(parameters);
         props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
         props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
-        props.put(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG, 86400000);
+        props.put(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG, 60000);
         props.put(ProducerConfig.RETRIES_CONFIG, 1000000);
         props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, 1);
         if (producerSettings != null) {

File: flinkx-hive/flinkx-hive-core/src/main/java/com/dtstack/flinkx/hive/util/AddressUtil.java
Patch:
@@ -18,6 +18,7 @@
 
 package com.dtstack.flinkx.hive.util;
 
+import com.dtstack.flinkx.util.ExceptionUtil;
 import org.apache.commons.net.telnet.TelnetClient;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -47,7 +48,7 @@ public static boolean telnet(String ip,int port){
                     client.disconnect();
                 }
             } catch (Exception e){
-                logger.error("{}",e);
+                logger.error("{}", ExceptionUtil.getErrorMessage(e));
             }
         }
     }

File: flinkx-rdb/flinkx-rdb-core/src/main/java/com/dtstack/flinkx/rdb/util/DbUtil.java
Patch:
@@ -208,7 +208,7 @@ public static void closeDbResources(ResultSet rs, Statement stmt, Connection con
      */
     public static void commit(Connection conn){
         try {
-            if (!conn.isClosed() && !conn.getAutoCommit()){
+            if (null != conn && !conn.isClosed() && !conn.getAutoCommit()){
                 conn.commit();
             }
         } catch (SQLException e){
@@ -222,7 +222,7 @@ public static void commit(Connection conn){
      */
     public static void rollBack(Connection conn){
         try {
-            if (!conn.isClosed() && !conn.getAutoCommit()){
+            if (null != conn && !conn.isClosed() && !conn.getAutoCommit()){
                 conn.rollback();
             }
         } catch (SQLException e){

File: flinkx-oraclelogminer/flinkx-oraclelogminer-reader/src/main/java/com/dtstack/flinkx/oraclelogminer/format/LogParser.java
Patch:
@@ -38,6 +38,7 @@
 
 import java.sql.Timestamp;
 import java.util.Collections;
+import java.util.HashMap;
 import java.util.Iterator;
 import java.util.LinkedHashMap;
 import java.util.List;

File: flinkx-oraclelogminer/flinkx-oraclelogminer-reader/src/main/java/com/dtstack/flinkx/oraclelogminer/format/LogMinerListener.java
Patch:
@@ -103,7 +103,7 @@ public void run() {
             try {
                 if (logMinerConnection.hasNext()) {
                     log = logMinerConnection.next();
-                    queue.put(logParser.parse(log));
+                    queue.put(logParser.parse(log,logMinerConnection.isOracle10));
                 } else {
                     logMinerConnection.closeStmt();
                     logMinerConnection.startOrUpdateLogMiner(positionManager.getPosition());

File: flinkx-postgresql/flinkx-postgresql-core/src/main/java/com/dtstack/flinkx/postgresql/PostgresqlTypeConverter.java
Patch:
@@ -77,7 +77,7 @@ public Object convert(Object data,String typeName) {
             if(dataValue.contains(".")){
                 dataValue =  new BigDecimal(dataValue).stripTrailingZeros().toPlainString();
             }
-            data = Integer.parseInt(dataValue);
+            data = Long.parseLong(dataValue);
         }
 
         return data;

File: flinkx-binlog/flinkx-binlog-reader/src/main/java/com/dtstack/flinkx/binlog/format/BinlogInputFormatBuilder.java
Patch:
@@ -102,14 +102,14 @@ protected void checkFormat() {
 
         //校验binlog cat
         if (StringUtils.isNotEmpty(binlogConfig.getCat())) {
-            HashSet<String> list = Sets.newHashSet("INSERT", "UPDATE", "DELETE");
+            HashSet<String> set = Sets.newHashSet("INSERT", "UPDATE", "DELETE");
             List<String> cats = Lists.newArrayList(binlogConfig.getCat().toUpperCase().split(","));
-            cats.removeIf(s -> list.contains(s.toUpperCase(Locale.ENGLISH)));
+            cats.removeIf(s -> set.contains(s.toUpperCase(Locale.ENGLISH)));
             if (CollectionUtils.isNotEmpty(cats)) {
                 sb.append("binlog cat not support-> ")
                         .append(GsonUtil.GSON.toJson(cats))
                         .append(",just support->")
-                        .append(GsonUtil.GSON.toJson(list))
+                        .append(GsonUtil.GSON.toJson(set))
                         .append(";\n");
             }
         }

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/GsonUtil.java
Patch:
@@ -51,6 +51,7 @@ public class GsonUtil {
     @SuppressWarnings("unchecked")
     private static Gson getGson() {
         GSON = new GsonBuilder()
+                .disableHtmlEscaping()
                 .setPrettyPrinting()
                 .create();
         try {

File: flinkx-oraclelogminer/flinkx-oraclelogminer-reader/src/main/java/com/dtstack/flinkx/oraclelogminer/format/LogFile.java
Patch:
@@ -15,8 +15,6 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-
-
 package com.dtstack.flinkx.oraclelogminer.format;
 
 import java.util.Objects;

File: flinkx-oraclelogminer/flinkx-oraclelogminer-reader/src/main/java/com/dtstack/flinkx/oraclelogminer/reader/OraclelogminerReader.java
Patch:
@@ -45,7 +45,7 @@ public OraclelogminerReader(DataTransferConfig config, StreamExecutionEnvironmen
         try {
             logMinerConfig = objectMapper.readValue(objectMapper.writeValueAsString(readerConfig.getParameter().getAll()), LogMinerConfig.class);
         } catch (Exception e) {
-            throw new RuntimeException("解析mongodb配置出错:", e);
+            throw new RuntimeException("parse logMiner config error:", e);
         }
 
         buildTableListenerRegex();

File: flinkx-oraclelogminer/flinkx-oraclelogminer-reader/src/main/java/com/dtstack/flinkx/oraclelogminer/format/LogMinerListener.java
Patch:
@@ -85,7 +85,7 @@ public void init() {
 
     public void start() {
         logMinerConnection.connect();
-        logMinerConnection.checkPrivileges();
+        logMinerConnection.queryOracleVersion();
         logMinerConnection.queryDataBaseEncoding();
 
         Long startScn = logMinerConnection.getStartScn(positionManager.getPosition());

File: flinkx-oraclelogminer/flinkx-oraclelogminer-reader/src/main/java/com/dtstack/flinkx/oraclelogminer/format/LogMinerConnection.java
Patch:
@@ -90,8 +90,7 @@ public class LogMinerConnection {
 
     private long lastQueryTime;
 
-    //这里可能有坑，中房之前是1秒，宁波港是10秒，取个中间数5秒
-    private static final long QUERY_LOG_INTERVAL = 5000;
+    private static final long QUERY_LOG_INTERVAL = 10000;
 
     private boolean logMinerStarted = false;
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/GsonUtil.java
Patch:
@@ -51,6 +51,7 @@ public class GsonUtil {
     @SuppressWarnings("unchecked")
     private static Gson getGson() {
         GSON = new GsonBuilder()
+                .disableHtmlEscaping()
                 .setPrettyPrinting()
                 .create();
         try {

File: flinkx-oraclelogminer/flinkx-oraclelogminer-reader/src/main/java/com/dtstack/flinkx/oraclelogminer/format/LogFile.java
Patch:
@@ -15,8 +15,6 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-
-
 package com.dtstack.flinkx.oraclelogminer.format;
 
 import java.util.Objects;

File: flinkx-oraclelogminer/flinkx-oraclelogminer-reader/src/main/java/com/dtstack/flinkx/oraclelogminer/reader/OraclelogminerReader.java
Patch:
@@ -45,7 +45,7 @@ public OraclelogminerReader(DataTransferConfig config, StreamExecutionEnvironmen
         try {
             logMinerConfig = objectMapper.readValue(objectMapper.writeValueAsString(readerConfig.getParameter().getAll()), LogMinerConfig.class);
         } catch (Exception e) {
-            throw new RuntimeException("解析mongodb配置出错:", e);
+            throw new RuntimeException("parse logMiner config error:", e);
         }
 
         buildTableListenerRegex();

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/GsonUtil.java
Patch:
@@ -51,6 +51,7 @@ public class GsonUtil {
     @SuppressWarnings("unchecked")
     private static Gson getGson() {
         GSON = new GsonBuilder()
+                .disableHtmlEscaping()
                 .setPrettyPrinting()
                 .create();
         try {

File: flinkx-oraclelogminer/flinkx-oraclelogminer-reader/src/main/java/com/dtstack/flinkx/oraclelogminer/reader/OraclelogminerReader.java
Patch:
@@ -45,7 +45,7 @@ public OraclelogminerReader(DataTransferConfig config, StreamExecutionEnvironmen
         try {
             logMinerConfig = objectMapper.readValue(objectMapper.writeValueAsString(readerConfig.getParameter().getAll()), LogMinerConfig.class);
         } catch (Exception e) {
-            throw new RuntimeException("解析mongodb配置出错:", e);
+            throw new RuntimeException("parse logMiner config error:", e);
         }
 
         buildTableListenerRegex();

File: flinkx-oraclelogminer/flinkx-oraclelogminer-reader/src/main/java/com/dtstack/flinkx/oraclelogminer/reader/OraclelogminerReader.java
Patch:
@@ -45,7 +45,7 @@ public OraclelogminerReader(DataTransferConfig config, StreamExecutionEnvironmen
         try {
             logMinerConfig = objectMapper.readValue(objectMapper.writeValueAsString(readerConfig.getParameter().getAll()), LogMinerConfig.class);
         } catch (Exception e) {
-            throw new RuntimeException("解析mongodb配置出错:", e);
+            throw new RuntimeException("parse logMiner config error:", e);
         }
 
         buildTableListenerRegex();

File: flinkx-postgresql/flinkx-postgresql-core/src/main/java/com/dtstack/flinkx/postgresql/PostgresqlTypeConverter.java
Patch:
@@ -77,7 +77,7 @@ public Object convert(Object data,String typeName) {
             if(dataValue.contains(".")){
                 dataValue =  new BigDecimal(dataValue).stripTrailingZeros().toPlainString();
             }
-            data = Integer.parseInt(dataValue);
+            data = Long.parseLong(dataValue);
         }
 
         return data;

File: flinkx-core/src/main/java/com/dtstack/flinkx/classloader/PluginUtil.java
Patch:
@@ -20,12 +20,12 @@
 package com.dtstack.flinkx.classloader;
 
 import com.dtstack.flinkx.config.DataTransferConfig;
-import org.apache.commons.compress.utils.Lists;
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
 
 import java.io.File;
 import java.net.MalformedURLException;
 import java.net.URL;
+import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.HashSet;
 import java.util.List;
@@ -79,7 +79,7 @@ public static Set<URL> getJarFileDirPath(String pluginName, String pluginRoot, S
     }
 
     private static List<String> getJarNames(File pluginPath) {
-        List<String> jarNames = Lists.newArrayList();
+        List<String> jarNames = new ArrayList<>();
         if (pluginPath.exists() && pluginPath.isDirectory()) {
             File[] jarFiles = pluginPath.listFiles((dir, name) ->
                     name.toLowerCase().startsWith(JAR_PREFIX) && name.toLowerCase().endsWith(".jar"));

File: flinkx-core/src/main/java/com/dtstack/flinkx/reader/DataReaderFactory.java
Patch:
@@ -41,7 +41,7 @@ public static BaseDataReader getDataReader(DataTransferConfig config, StreamExec
         try {
             String pluginName = config.getJob().getContent().get(0).getReader().getName();
             String pluginClassName = PluginUtil.getPluginClassName(pluginName);
-            Set<URL> urlList = PluginUtil.getJarFileDirPath(pluginName, config.getPluginRoot(), config.getRemotePluginPath());
+            Set<URL> urlList = PluginUtil.getJarFileDirPath(pluginName, config.getPluginRoot(), null);
 
             return ClassLoaderManager.newInstance(urlList, cl -> {
                 Class<?> clazz = cl.loadClass(pluginClassName);

File: flinkx-core/src/main/java/com/dtstack/flinkx/writer/DataWriterFactory.java
Patch:
@@ -40,7 +40,7 @@ public static BaseDataWriter getDataWriter(DataTransferConfig config) {
         try {
             String pluginName = config.getJob().getContent().get(0).getWriter().getName();
             String pluginClassName = PluginUtil.getPluginClassName(pluginName);
-            Set<URL> urlList = PluginUtil.getJarFileDirPath(pluginName, config.getPluginRoot(), config.getRemotePluginPath());
+            Set<URL> urlList = PluginUtil.getJarFileDirPath(pluginName, config.getPluginRoot(), null);
 
             return ClassLoaderManager.newInstance(urlList, cl -> {
                 Class<?> clazz = cl.loadClass(pluginClassName);

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsOrcOutputFormat.java
Patch:
@@ -243,7 +243,7 @@ private void getData(List<Object> recordList, int index, Row row) throws WriteRe
                     SimpleDateFormat fm = DateUtil.getDateTimeFormatter();
                     recordList.add(fm.format(column));
                 }else if (column instanceof Map || column instanceof List){
-                    recordList.add(jp.parse(GsonUtil.GSON.toJson(column)).toString());
+                    recordList.add(gson.toJson(column));
                 }else {
                     recordList.add(rowData);
                 }

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsParquetOutputFormat.java
Patch:
@@ -241,7 +241,7 @@ private void addDataToGroup(Group group, Object valObj, int i) throws Exception{
                     val=DateUtil.getDateTimeFormatter().format(valObj);
                     group.add(colName,val);
                 }else if (valObj instanceof Map || valObj instanceof List){
-                    group.add(colName,jp.parse(GsonUtil.GSON.toJson(valObj)).toString());
+                    group.add(colName,gson.toJson(valObj));
                 }else {
                     group.add(colName,val);
                 }

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsTextOutputFormat.java
Patch:
@@ -206,9 +206,6 @@ private void appendDataToString(StringBuilder sb, Object column, ColumnType colu
                         SimpleDateFormat fm = DateUtil.getDateTimeFormatter();
                         sb.append(fm.format(column));
                     }else if (column instanceof Map || column instanceof List){
-                        //之所以text格式的hdfs写入 需要使用jp.parse 解析gson转换后的json 是因为gson.tojson转换后的json是一个标准格式json
-                        //标准json会存在换行 制表符等 使用jp.parse之后会转为一个各个key连在一起的string
-                        //text有一个delimiter换行符 默认是\u0001，如果是标准json会使得text格式的hive表读取混乱，所以使用jp.parse对标准json格式转换一次
                         sb.append(gson.toJson(column));
                     }else {
                         sb.append(rowData);

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsOrcOutputFormat.java
Patch:
@@ -243,7 +243,7 @@ private void getData(List<Object> recordList, int index, Row row) throws WriteRe
                     SimpleDateFormat fm = DateUtil.getDateTimeFormatter();
                     recordList.add(fm.format(column));
                 }else if (column instanceof Map || column instanceof List){
-                    recordList.add(jp.parse(GsonUtil.GSON.toJson(column)).toString());
+                    recordList.add(gson.toJson(column));
                 }else {
                     recordList.add(rowData);
                 }

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsParquetOutputFormat.java
Patch:
@@ -241,7 +241,7 @@ private void addDataToGroup(Group group, Object valObj, int i) throws Exception{
                     val=DateUtil.getDateTimeFormatter().format(valObj);
                     group.add(colName,val);
                 }else if (valObj instanceof Map || valObj instanceof List){
-                    group.add(colName,jp.parse(GsonUtil.GSON.toJson(valObj)).toString());
+                    group.add(colName,gson.toJson(valObj));
                 }else {
                     group.add(colName,val);
                 }

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsTextOutputFormat.java
Patch:
@@ -209,7 +209,7 @@ private void appendDataToString(StringBuilder sb, Object column, ColumnType colu
                         //之所以text格式的hdfs写入 需要使用jp.parse 解析gson转换后的json 是因为gson.tojson转换后的json是一个标准格式json
                         //标准json会存在换行 制表符等 使用jp.parse之后会转为一个各个key连在一起的string
                         //text有一个delimiter换行符 默认是\u0001，如果是标准json会使得text格式的hive表读取混乱，所以使用jp.parse对标准json格式转换一次
-                        sb.append(jp.parse(GsonUtil.GSON.toJson(column)).toString());
+                        sb.append(gson.toJson(column));
                     }else {
                         sb.append(rowData);
                     }

File: flinkx-hbase/flinkx-hbase-reader/src/main/java/com/dtstack/flinkx/hbase/reader/HbaseReader.java
Patch:
@@ -54,7 +54,6 @@ public class HbaseReader extends BaseDataReader {
     protected boolean isBinaryRowkey;
     protected String tableName;
     protected int scanCacheSize;
-    protected int scanBatchSize;
 
     public HbaseReader(DataTransferConfig config, StreamExecutionEnvironment env) {
         super(config, env);
@@ -71,7 +70,6 @@ public HbaseReader(DataTransferConfig config, StreamExecutionEnvironment env) {
 
         encoding = readerConfig.getParameter().getStringVal(HbaseConfigKeys.KEY_ENCODING);
         scanCacheSize = readerConfig.getParameter().getIntVal(HbaseConfigKeys.KEY_SCAN_CACHE_SIZE, HbaseConfigConstants.DEFAULT_SCAN_CACHE_SIZE);
-        scanBatchSize = readerConfig.getParameter().getIntVal(HbaseConfigKeys.KEY_SCAN_BATCH_SIZE, HbaseConfigConstants.DEFAULT_SCAN_BATCH_SIZE);
 
         List columns = readerConfig.getParameter().getColumn();
         if(columns != null && columns.size() > 0) {
@@ -110,7 +108,6 @@ public DataStream<Row> readData() {
         builder.setBytes(bytes);
         builder.setMonitorUrls(monitorUrls);
         builder.setScanCacheSize(scanCacheSize);
-        builder.setScanBatchSize(scanBatchSize);
         builder.setMonitorUrls(monitorUrls);
         builder.setTestConfig(testConfig);
         builder.setLogConfig(logConfig);

File: flinkx-clickhouse/flinkx-clickhouse-core/src/main/java/com/dtstack/flinkx/clickhouse/core/ClickhouseUtil.java
Patch:
@@ -37,8 +37,8 @@ public class ClickhouseUtil {
 
     public static Connection getConnection(String url, String username, String password) throws SQLException {
         Properties properties = new Properties();
-        properties.put(ClickHouseQueryParam.USER, username);
-        properties.put(ClickHouseQueryParam.PASSWORD, password);
+        properties.put(ClickHouseQueryParam.USER.getKey(), username);
+        properties.put(ClickHouseQueryParam.PASSWORD.getKey(), password);
         boolean failed = true;
         Connection conn = null;
         for (int i = 0; i < MAX_RETRY_TIMES && failed; ++i) {

File: flinkx-hbase/flinkx-hbase-reader/src/main/java/com/dtstack/flinkx/hbase/reader/HbaseInputFormatBuilder.java
Patch:
@@ -96,9 +96,6 @@ protected void checkFormat() {
         Preconditions.checkArgument(format.scanCacheSize <= HbaseConfigConstants.MAX_SCAN_CACHE_SIZE && format.scanCacheSize >= HbaseConfigConstants.MIN_SCAN_CACHE_SIZE,
                 "scanCacheSize should be between " + HbaseConfigConstants.MIN_SCAN_CACHE_SIZE +  " and " + HbaseConfigConstants.MAX_SCAN_CACHE_SIZE);
 
-        Preconditions.checkArgument(format.scanBatchSize <= HbaseConfigConstants.MAX_SCAN_BATCH_SIZE && format.scanBatchSize >= HbaseConfigConstants.MIN_SCAN_BATCH_SIZE,
-                "scanBatchSize should be between " + HbaseConfigConstants.MIN_SCAN_BATCH_SIZE + " and " + HbaseConfigConstants.MAX_SCAN_BATCH_SIZE);
-
         for(int i = 0; i < format.columnTypes.size(); ++i) {
             Preconditions.checkArgument(StringUtils.isNotEmpty(format.columnTypes.get(i)));
             Preconditions.checkArgument(StringUtils.isNotEmpty(format.columnNames.get(i))

File: flinkx-oraclelogminer/flinkx-oraclelogminer-reader/src/main/java/com/dtstack/flinkx/oraclelogminer/format/LogMinerConnection.java
Patch:
@@ -479,8 +479,8 @@ public boolean hasNext() throws SQLException, UnsupportedEncodingException, Deco
                     //?, -> ',
                     hexStr = hexStr.replace("3f2c", "272c");
 
-                    //?空格 -> '空格
-                    hexStr = hexStr.replace("3f20", "2720");
+                    //?空格a -> '空格a
+                    hexStr = hexStr.replace("3f2061", "272061");
                     LOG.info("final redo sql is: {}", new String(Hex.decodeHex(hexStr.toCharArray()), "GBK"));
                 }
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/GsonUtil.java
Patch:
@@ -51,6 +51,7 @@ public class GsonUtil {
     @SuppressWarnings("unchecked")
     private static Gson getGson() {
         GSON = new GsonBuilder()
+                .disableHtmlEscaping()
                 .setPrettyPrinting()
                 .create();
         try {

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsOrcOutputFormat.java
Patch:
@@ -243,7 +243,7 @@ private void getData(List<Object> recordList, int index, Row row) throws WriteRe
                     SimpleDateFormat fm = DateUtil.getDateTimeFormatter();
                     recordList.add(fm.format(column));
                 }else if (column instanceof Map || column instanceof List){
-                    recordList.add(GsonUtil.GSON.toJson(column));
+                    recordList.add(jp.parse(GsonUtil.GSON.toJson(column)).toString());
                 }else {
                     recordList.add(rowData);
                 }

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsParquetOutputFormat.java
Patch:
@@ -241,7 +241,7 @@ private void addDataToGroup(Group group, Object valObj, int i) throws Exception{
                     val=DateUtil.getDateTimeFormatter().format(valObj);
                     group.add(colName,val);
                 }else if (valObj instanceof Map || valObj instanceof List){
-                    group.add(colName,GsonUtil.GSON.toJson(valObj));
+                    group.add(colName,jp.parse(GsonUtil.GSON.toJson(valObj)).toString());
                 }else {
                     group.add(colName,val);
                 }

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsTextOutputFormat.java
Patch:
@@ -54,8 +54,6 @@ public class HdfsTextOutputFormat extends BaseHdfsOutputFormat {
 
     private static final int BUFFER_SIZE = 1000;
 
-    private JsonParser jp = new JsonParser();
-
     @Override
     public void flushDataInternal() throws IOException {
         LOG.info("Close current text stream, write data size:[{}]", bytesWriteCounter.getLocalValue());

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsOrcOutputFormat.java
Patch:
@@ -242,7 +242,7 @@ private void getData(List<Object> recordList, int index, Row row) throws WriteRe
                 if (column instanceof Timestamp){
                     SimpleDateFormat fm = DateUtil.getDateTimeFormatter();
                     recordList.add(fm.format(column));
-                }else if (column instanceof Map){
+                }else if (column instanceof Map || column instanceof List){
                     recordList.add(GsonUtil.GSON.toJson(column));
                 }else {
                     recordList.add(rowData);

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsParquetOutputFormat.java
Patch:
@@ -53,6 +53,7 @@
 import java.sql.Timestamp;
 import java.util.Date;
 import java.util.HashMap;
+import java.util.List;
 import java.util.Map;
 
 /**
@@ -239,7 +240,7 @@ private void addDataToGroup(Group group, Object valObj, int i) throws Exception{
                 if (valObj instanceof Timestamp){
                     val=DateUtil.getDateTimeFormatter().format(valObj);
                     group.add(colName,val);
-                }else if (valObj instanceof Map){
+                }else if (valObj instanceof Map || valObj instanceof List){
                     group.add(colName,GsonUtil.GSON.toJson(valObj));
                 }else {
                     group.add(colName,val);

File: flinkx-hive/flinkx-hive-core/src/main/java/com/dtstack/flinkx/hive/util/AddressUtil.java
Patch:
@@ -18,6 +18,7 @@
 
 package com.dtstack.flinkx.hive.util;
 
+import com.dtstack.flinkx.util.ExceptionUtil;
 import org.apache.commons.net.telnet.TelnetClient;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -47,7 +48,7 @@ public static boolean telnet(String ip,int port){
                     client.disconnect();
                 }
             } catch (Exception e){
-                logger.error("{}",e);
+                logger.error("{}", ExceptionUtil.getErrorMessage(e));
             }
         }
     }

File: flinkx-rdb/flinkx-rdb-core/src/main/java/com/dtstack/flinkx/rdb/util/DbUtil.java
Patch:
@@ -208,7 +208,7 @@ public static void closeDbResources(ResultSet rs, Statement stmt, Connection con
      */
     public static void commit(Connection conn){
         try {
-            if (!conn.isClosed() && !conn.getAutoCommit()){
+            if (null != conn && !conn.isClosed() && !conn.getAutoCommit()){
                 conn.commit();
             }
         } catch (SQLException e){
@@ -222,7 +222,7 @@ public static void commit(Connection conn){
      */
     public static void rollBack(Connection conn){
         try {
-            if (!conn.isClosed() && !conn.getAutoCommit()){
+            if (null != conn && !conn.isClosed() && !conn.getAutoCommit()){
                 conn.rollback();
             }
         } catch (SQLException e){

File: flinkx-metadata/flinkx-metadata-core/src/main/java/com/dtstack/flinkx/metadata/util/ConnUtil.java
Patch:
@@ -76,10 +76,8 @@ public static Connection getConnection(String url, String username, String passw
         for (int i = 0; i < MAX_RETRY_TIMES && failed; ++i) {
             try {
                 dbConn = getConnectionInternal(url, username, password);
+                failed = false;
             } catch (Exception e) {
-                if (dbConn != null) {
-                    dbConn.close();
-                }
                 if (i == MAX_RETRY_TIMES - 1) {
                     throw e;
                 } else {

File: flinkx-core/src/main/java/com/dtstack/flinkx/config/DataTransferConfig.java
Patch:
@@ -83,7 +83,7 @@ private static void checkConfig(DataTransferConfig config) {
         Preconditions.checkNotNull(config);
 
         JobConfig jobConfig = config.getJob();
-        Preconditions.checkNotNull(jobConfig, "Must spedify job element");
+        Preconditions.checkNotNull(jobConfig, "Must specify job element");
 
         List<ContentConfig> contentConfig = jobConfig.getContent();
         Preconditions.checkNotNull(contentConfig, "Must specify content array");

File: flinkx-core/src/main/java/com/dtstack/flinkx/inputformat/BaseRichInputFormat.java
Patch:
@@ -423,7 +423,7 @@ public void setDataTransferConfig(DataTransferConfig dataTransferConfig){
         this.dataTransferConfig = dataTransferConfig;
     }
 
-    public DataTransferConfig getDataTransferConfig(){
+    public DataTransferConfig getDataTransferConfig() {
         return dataTransferConfig;
     }
 }

File: flinkx-oraclelogminer/flinkx-oraclelogminer-reader/src/main/java/com/dtstack/flinkx/oraclelogminer/format/OracleLogMinerInputFormat.java
Patch:
@@ -44,7 +44,7 @@ public class OracleLogMinerInputFormat extends BaseRichInputFormat {
     private transient PositionManager positionManager;
 
     @Override
-    protected InputSplit[] createInputSplitsInternal(int i) throws Exception {
+    protected InputSplit[] createInputSplitsInternal(int i) {
         return new InputSplit[]{new GenericInputSplit(1,1)};
     }
 
@@ -64,13 +64,13 @@ private void initPosition() {
     }
 
     @Override
-    protected void openInternal(InputSplit inputSplit) throws IOException {
+    protected void openInternal(InputSplit inputSplit) {
         logMinerListener.init();
         logMinerListener.start();
     }
 
     @Override
-    protected Row nextRecordInternal(Row row) throws IOException {
+    protected Row nextRecordInternal(Row row) {
         Map<String, Object> data = logMinerListener.getData();
         if(null != data) {
             return Row.of(data);

File: flinkx-oraclelogminer/flinkx-oraclelogminer-reader/src/main/java/com/dtstack/flinkx/oraclelogminer/reader/OraclelogminerReader.java
Patch:
@@ -45,7 +45,7 @@ public OraclelogminerReader(DataTransferConfig config, StreamExecutionEnvironmen
         try {
             logMinerConfig = objectMapper.readValue(objectMapper.writeValueAsString(readerConfig.getParameter().getAll()), LogMinerConfig.class);
         } catch (Exception e) {
-            throw new RuntimeException("解析mongodb配置出错:", e);
+            throw new RuntimeException("parse logMiner config error:", e);
         }
 
         buildTableListenerRegex();

File: flinkx-core/src/main/java/com/dtstack/flinkx/config/ErrorLimitConfig.java
Patch:
@@ -18,6 +18,8 @@
 
 package com.dtstack.flinkx.config;
 
+import com.dtstack.flinkx.util.ValueUtil;
+
 import java.util.HashMap;
 import java.util.Map;
 
@@ -57,7 +59,7 @@ public void setRecord(Integer record) {
     }
 
     public Double getPercentage() {
-        return (Double) getVal(KEY_ERROR_PERCENTAGE_LIMIT);
+        return ValueUtil.getDoubleVal(getVal(KEY_ERROR_PERCENTAGE_LIMIT));
     }
 
     public void setPercentage(Double percentage) {

File: flinkx-kingbase/flinkx-kingbase-writer/src/main/java/com/dtstack/flinkx/kingbase/format/KingbaseOutputFormat.java
Patch:
@@ -239,6 +239,7 @@ private boolean checkIsCopyMode(String insertMode){
         return true;
     }
 
+    @Override
     public void setSchema(String schema){
         this.schema = schema;
     }

File: flinkx-metadata/flinkx-metadata-core/src/main/java/com/dtstack/flinkx/metadata/util/ConnUtil.java
Patch:
@@ -76,10 +76,8 @@ public static Connection getConnection(String url, String username, String passw
         for (int i = 0; i < MAX_RETRY_TIMES && failed; ++i) {
             try {
                 dbConn = getConnectionInternal(url, username, password);
+                failed = false;
             } catch (Exception e) {
-                if (dbConn != null) {
-                    dbConn.close();
-                }
                 if (i == MAX_RETRY_TIMES - 1) {
                     throw e;
                 } else {

File: flinkx-metadata/flinkx-metadata-core/src/main/java/com/dtstack/flinkx/metadata/util/ConnUtil.java
Patch:
@@ -76,10 +76,8 @@ public static Connection getConnection(String url, String username, String passw
         for (int i = 0; i < MAX_RETRY_TIMES && failed; ++i) {
             try {
                 dbConn = getConnectionInternal(url, username, password);
+                failed = false;
             } catch (Exception e) {
-                if (dbConn != null) {
-                    dbConn.close();
-                }
                 if (i == MAX_RETRY_TIMES - 1) {
                     throw e;
                 } else {

File: flinkx-metadata/flinkx-metadata-reader/src/main/java/com/dtstack/flinkx/metadata/inputformat/BaseMetadataInputFormat.java
Patch:
@@ -69,7 +69,6 @@ public abstract class BaseMetadataInputFormat extends BaseRichInputFormat {
 
     @Override
     protected void openInternal(InputSplit inputSplit) throws IOException {
-        LOG.info("inputSplit = {}", inputSplit);
         try {
             connection.set(getConnection());
             statement.set(connection.get().createStatement());
@@ -89,6 +88,7 @@ protected void openInternal(InputSplit inputSplit) throws IOException {
             LOG.error("获取table列表异常, dbUrl = {}, username = {}, inputSplit = {}, e = {}", dbUrl, username, inputSplit, ExceptionUtil.getErrorMessage(e));
             tableList = new LinkedList<>();
         }
+        LOG.info("curentDb = {}, tableList = {}", currentDb.get(), tableList);
         tableIterator.set(tableList.iterator());
     }
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/config/DataTransferConfig.java
Patch:
@@ -83,7 +83,7 @@ private static void checkConfig(DataTransferConfig config) {
         Preconditions.checkNotNull(config);
 
         JobConfig jobConfig = config.getJob();
-        Preconditions.checkNotNull(jobConfig, "Must spedify job element");
+        Preconditions.checkNotNull(jobConfig, "Must specify job element");
 
         List<ContentConfig> contentConfig = jobConfig.getContent();
         Preconditions.checkNotNull(contentConfig, "Must specify content array");

File: flinkx-oraclelogminer/flinkx-oraclelogminer-reader/src/main/java/com/dtstack/flinkx/oraclelogminer/format/OracleLogMinerInputFormat.java
Patch:
@@ -44,7 +44,7 @@ public class OracleLogMinerInputFormat extends BaseRichInputFormat {
     private transient PositionManager positionManager;
 
     @Override
-    protected InputSplit[] createInputSplitsInternal(int i) throws Exception {
+    protected InputSplit[] createInputSplitsInternal(int i) {
         return new InputSplit[]{new GenericInputSplit(1,1)};
     }
 
@@ -64,13 +64,13 @@ private void initPosition() {
     }
 
     @Override
-    protected void openInternal(InputSplit inputSplit) throws IOException {
+    protected void openInternal(InputSplit inputSplit) {
         logMinerListener.init();
         logMinerListener.start();
     }
 
     @Override
-    protected Row nextRecordInternal(Row row) throws IOException {
+    protected Row nextRecordInternal(Row row) {
         Map<String, Object> data = logMinerListener.getData();
         if(null != data) {
             return Row.of(data);

File: flinkx-oraclelogminer/flinkx-oraclelogminer-reader/src/main/java/com/dtstack/flinkx/oraclelogminer/reader/OraclelogminerReader.java
Patch:
@@ -45,7 +45,7 @@ public OraclelogminerReader(DataTransferConfig config, StreamExecutionEnvironmen
         try {
             logMinerConfig = objectMapper.readValue(objectMapper.writeValueAsString(readerConfig.getParameter().getAll()), LogMinerConfig.class);
         } catch (Exception e) {
-            throw new RuntimeException("解析mongodb配置出错:", e);
+            throw new RuntimeException("parse logMiner config error:", e);
         }
 
         buildTableListenerRegex();

File: flinkx-oraclelogminer/flinkx-oraclelogminer-reader/src/main/java/com/dtstack/flinkx/oraclelogminer/format/LogMinerListener.java
Patch:
@@ -165,8 +165,8 @@ public Map<String, Object> getData() {
             }
            String message = String.format( "LogMinerListener obtain an error data, data = %s", GsonUtil.GSON.toJson(data));
             LOG.error(message);
-            if(++failedTimes > 3){
-                throw new RuntimeException("Error data is received 5 times continuously,error info->"+message);
+            if(++failedTimes >= 3){
+                throw new RuntimeException("Error data is received 3 times continuously,error info->"+message);
             }
         } catch (InterruptedException e) {
             LOG.warn("Get data from queue error:", e);

File: flinkx-metadata-oracle/flinkx-metadata-oracle-reader/src/main/java/com/dtstack/flinkx/metadataoracle/inputformat/MetadataoracleInputFormat.java
Patch:
@@ -181,6 +181,7 @@ Map<String, List<Map<String, Object>>> queryColumnList() throws SQLException {
 
     @Override
     protected void init() throws SQLException {
+        tableList = null;
         StringBuilder stringBuilder = new StringBuilder(2 * tableList.size());
         if(tableList.size() <= MAX_TABLE_SIZE){
             for(int index=0;index<tableList.size();index++){

File: flinkx-kb/flinkx-kb-writer/src/main/java/com/dtstack/flinkx/kafkabase/writer/HeartBeatController.java
Patch:
@@ -17,6 +17,7 @@
  */
 package com.dtstack.flinkx.kafkabase.writer;
 
+import com.dtstack.flinkx.exception.DataSourceException;
 import com.dtstack.flinkx.util.ExceptionUtil;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -64,9 +65,9 @@ public void acquire() {
         }
         //连续发送3次数据错误或出现连接异常
         if (failedTimes.get() >= detectingRetryTimes || e.toString().contains("Timeout") ) {
-            String message = "Error data is received 3 times continuously or datasource has error" + ExceptionUtil.getErrorMessage(e);
+            String message = "Error data is received 3 times continuously or datasource has error->" + ExceptionUtil.getErrorMessage(e);
             logger.error(message);
-            throw new RuntimeException(message, e);
+            throw new DataSourceException("kafka",message, e);
         }
     }
 }

File: flinkx-kafka/flinkx-kafka-writer/src/main/java/com/dtstack/flinkx/kafka/writer/KafkaWriter.java
Patch:
@@ -18,6 +18,7 @@
 package com.dtstack.flinkx.kafka.writer;
 
 import com.dtstack.flinkx.config.DataTransferConfig;
+import com.dtstack.flinkx.kafkabase.writer.HeartBeatController;
 import com.dtstack.flinkx.kafkabase.writer.KafkaBaseWriter;
 import org.apache.flink.streaming.api.datastream.DataStream;
 import org.apache.flink.streaming.api.datastream.DataStreamSink;
@@ -47,6 +48,7 @@ public DataStreamSink<?> writeData(DataStream<Row> dataSet) {
         format.setProducerSettings(producerSettings);
         format.setRestoreConfig(restoreConfig);
         format.setTableFields(tableFields);
+        format.setHeartBeatController(new HeartBeatController());
 
         return createOutput(dataSet, format);
     }

File: flinkx-kafka09/flinkx-kafka09-writer/src/main/java/com/dtstack/flinkx/kafka09/writer/Kafka09Writer.java
Patch:
@@ -20,6 +20,7 @@
 import com.dtstack.flinkx.config.DataTransferConfig;
 import com.dtstack.flinkx.config.WriterConfig;
 import com.dtstack.flinkx.kafkabase.KafkaConfigKeys;
+import com.dtstack.flinkx.kafkabase.writer.HeartBeatController;
 import com.dtstack.flinkx.kafkabase.writer.KafkaBaseWriter;
 import org.apache.commons.lang.StringUtils;
 import org.apache.flink.streaming.api.datastream.DataStream;

File: flinkx-kafka10/flinkx-kafka10-writer/src/main/java/com/dtstack/flinkx/kafka10/writer/Kafka10Writer.java
Patch:
@@ -18,6 +18,7 @@
 package com.dtstack.flinkx.kafka10.writer;
 
 import com.dtstack.flinkx.config.DataTransferConfig;
+import com.dtstack.flinkx.kafkabase.writer.HeartBeatController;
 import com.dtstack.flinkx.kafkabase.writer.KafkaBaseWriter;
 import org.apache.flink.streaming.api.datastream.DataStream;
 import org.apache.flink.streaming.api.datastream.DataStreamSink;
@@ -46,6 +47,7 @@ public DataStreamSink<?> writeData(DataStream<Row> dataSet) {
         format.setTableFields(tableFields);
         format.setProducerSettings(producerSettings);
         format.setRestoreConfig(restoreConfig);
+        format.setHeartBeatController(new HeartBeatController());
 
         return createOutput(dataSet, format);
     }

File: flinkx-kafka11/flinkx-kafka11-writer/src/main/java/com/dtstack/flinkx/kafka11/writer/Kafka11Writer.java
Patch:
@@ -18,6 +18,7 @@
 package com.dtstack.flinkx.kafka11.writer;
 
 import com.dtstack.flinkx.config.DataTransferConfig;
+import com.dtstack.flinkx.kafkabase.writer.HeartBeatController;
 import com.dtstack.flinkx.kafkabase.writer.KafkaBaseWriter;
 import org.apache.flink.streaming.api.datastream.DataStream;
 import org.apache.flink.streaming.api.datastream.DataStreamSink;
@@ -46,7 +47,7 @@ public DataStreamSink<?> writeData(DataStream<Row> dataSet) {
         format.setProducerSettings(producerSettings);
         format.setRestoreConfig(restoreConfig);
         format.setTableFields(tableFields);
-
+        format.setHeartBeatController(new HeartBeatController());
         return createOutput(dataSet, format);
     }
 }

File: flinkx-kb/flinkx-kb-writer/src/main/java/com/dtstack/flinkx/kafkabase/writer/HeartBeatController.java
Patch:
@@ -15,10 +15,9 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package com.dtstack.flinkx.kafka09.writer;
+package com.dtstack.flinkx.kafkabase.writer;
 
 import com.dtstack.flinkx.util.ExceptionUtil;
-import org.apache.kafka.common.errors.TimeoutException;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -64,7 +63,7 @@ public void acquire() {
             return;
         }
         //连续发送3次数据错误或出现连接异常
-        if (failedTimes.get() >= detectingRetryTimes || e instanceof TimeoutException ) {
+        if (failedTimes.get() >= detectingRetryTimes || e.toString().contains("Timeout") ) {
             String message = "Error data is received 3 times continuously or datasource has error" + ExceptionUtil.getErrorMessage(e);
             logger.error(message);
             throw new RuntimeException(message, e);

File: flinkx-core/src/main/java/com/dtstack/flinkx/config/ErrorLimitConfig.java
Patch:
@@ -18,6 +18,8 @@
 
 package com.dtstack.flinkx.config;
 
+import com.dtstack.flinkx.util.ValueUtil;
+
 import java.util.HashMap;
 import java.util.Map;
 
@@ -57,7 +59,7 @@ public void setRecord(Integer record) {
     }
 
     public Double getPercentage() {
-        return (Double) getVal(KEY_ERROR_PERCENTAGE_LIMIT);
+        return ValueUtil.getDoubleVal(getVal(KEY_ERROR_PERCENTAGE_LIMIT));
     }
 
     public void setPercentage(Double percentage) {

File: flinkx-core/src/main/java/com/dtstack/flinkx/constants/ConstantValue.java
Patch:
@@ -27,6 +27,7 @@ public class ConstantValue {
 
     public static final String STAR_SYMBOL = "*";
     public static final String POINT_SYMBOL = ".";
+    public static final String TWO_POINT_SYMBOL = "..";
     public static final String EQUAL_SYMBOL = "=";
     public static final String SINGLE_QUOTE_MARK_SYMBOL = "'";
     public static final String DOUBLE_QUOTE_MARK_SYMBOL = "\"";

File: flinkx-core/src/main/java/com/dtstack/flinkx/outputformat/BaseFileOutputFormat.java
Patch:
@@ -106,7 +106,7 @@ protected void openInternal(int taskNumber, int numTasks) throws IOException {
         nextBlock();
     }
 
-    private void initPath(){
+    protected void initPath(){
         if(StringUtils.isNotBlank(fileName)) {
             outputFilePath = path + SP + fileName;
         } else {
@@ -122,7 +122,7 @@ private void initPath(){
                 taskNumber, currentBlockFileNamePrefix, tmpPath, finishedPath);
     }
 
-    private void initFileIndex() {
+    protected void initFileIndex() {
         if (null != formatState && formatState.getFileIndex() > -1) {
             blockIndex = formatState.getFileIndex() + 1;
         }

File: flinkx-ftp/flinkx-ftp-reader/src/main/java/com/dtstack/flinkx/ftp/reader/FtpInputFormat.java
Patch:
@@ -43,8 +43,6 @@ public class FtpInputFormat extends BaseRichInputFormat {
 
     protected FtpConfig ftpConfig;
 
-    protected String charsetName = "utf-8";
-
     protected List<MetaColumn> metaColumns;
 
     private transient FtpSeqBufferedReader br;
@@ -102,7 +100,7 @@ public void openInternal(InputSplit split) throws IOException {
             br = new FtpSeqBufferedReader(ftpHandler,paths.iterator());
             br.setFromLine(0);
         }
-        br.setCharsetName(charsetName);
+        br.setFileEncoding(ftpConfig.getEncoding());
     }
 
     @Override

File: flinkx-metadata-sqlserver/flinkx-metadata-sqlserver-reader/src/main/java/com/dtstack/flinkx/metadatasqlserver/constants/SqlServerMetadataCons.java
Patch:
@@ -31,6 +31,8 @@ public class SqlServerMetadataCons extends MetaDataCons {
 
     public static final String KEY_PARTITION_COLUMN = "partitionColumn";
     public static final String KEY_FILE_GROUP_NAME = "fileGroupName";
+    public static final String KEY_SCHEMA_NAME = "schemaName";
+    public static final String KEY_TABLE_NAME = "tableName";
     public static final String KEY_TABLE_SCHEMA = "tableSchema";
 
     public static final String KEY_ZERO = "0";

File: flinkx-ftp/flinkx-ftp-reader/src/main/java/com/dtstack/flinkx/ftp/reader/FtpSeqBufferedReader.java
Patch:
@@ -77,7 +77,7 @@ private void nextStream() throws IOException{
             String file = iter.next();
             InputStream in = ftpHandler.getInputStream(file);
             if (in == null) {
-                throw new NullPointerException();
+                throw new RuntimeException(String.format("can not get inputStream for file [%s], please check file read and write permissions", file));
             }
 
             br = new BufferedReader(new InputStreamReader(in, charsetName));

File: flinkx-metadata-mysql/flinkx-metadata-mysql-reader/src/main/java/com/dtstack/flinkx/metadatamysql/constants/MysqlMetadataCons.java
Patch:
@@ -47,5 +47,5 @@ public class MysqlMetadataCons extends TidbMetadataCons {
     public static final String RESULT_INDEX_COMMENT = "Index_comment";
 
     public static final String SQL_QUERY_TABLE_INFO = "SELECT * FROM  INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = '%s' AND TABLE_NAME = '%s'";
-    public static final String SQL_QUERY_INDEX = "SHOW INDEX FROM %s";
+    public static final String SQL_QUERY_INDEX = "SHOW INDEX FROM `%s`";
 }

File: flinkx-metadata-tidb/flinkx-metadata-tidb-reader/src/main/java/com/dtstack/flinkx/metadatatidb/constants/TidbMetadataCons.java
Patch:
@@ -56,7 +56,7 @@ public class TidbMetadataCons extends MetaDataCons {
     public static final String SQL_SWITCH_DATABASE = "USE `%s`";
     public static final String SQL_SHOW_TABLES = "SHOW FULL TABLES WHERE Table_type = 'BASE TABLE'";
     public static final String SQL_QUERY_TABLE_INFO = "SHOW TABLE STATUS LIKE '%s'";
-    public static final String SQL_QUERY_COLUMN = "SHOW FULL COLUMNS FROM %s";
+    public static final String SQL_QUERY_COLUMN = "SHOW FULL COLUMNS FROM `%s`";
     public static final String SQL_QUERY_HEALTHY = "SHOW STATS_HEALTHY WHERE Table_name='%s' AND Db_name = schema()";
     public static final String SQL_QUERY_UPDATE_TIME = "SHOW STATS_META WHERE Table_name = '%s'";
     public static final String SQL_QUERY_PARTITION = "SELECT * FROM information_schema.partitions WHERE table_schema = schema() AND table_name='%s'";

File: flinkx-rdb/flinkx-rdb-reader/src/main/java/com.dtstack.flinkx.rdb.inputformat/JdbcInputFormat.java
Patch:
@@ -843,6 +843,8 @@ private void queryStartLocation() throws SQLException{
                 //执行到此处代表轮询任务startLocation为空，且数据库中无数据，此时需要查询增量字段的最小值
                 ps.setFetchDirection(ResultSet.FETCH_FORWARD);
                 resultSet.close();
+                //如果事务不提交 就会导致数据库即使插入数据 也无法读到数据
+                dbConn.commit();
                 resultSet = ps.executeQuery();
                 hasNext = resultSet.next();
                 //每隔五分钟打印一次

File: flinkx-metadata-oracle/flinkx-metadata-oracle-reader/src/main/java/com/dtstack/flinkx/metadataoracle/inputformat/MetadataoracleInputFormat.java
Patch:
@@ -309,6 +309,7 @@ protected void init() throws SQLException {
         if(tableList.size()==0){
             return;
         }
+        allTable = null;
         if (start < tableList.size()){
             // 取出子数组，注意避免越界
             List<Object> splitTableList = tableList.subList(start, Math.min(start+MAX_TABLE_SIZE, tableList.size()));

File: flinkx-metadata-oracle/flinkx-metadata-oracle-reader/src/main/java/com/dtstack/flinkx/metadataoracle/inputformat/MetadataoracleInputFormat.java
Patch:
@@ -228,6 +228,8 @@ protected Map<String, List<Map<String, String>>> queryColumnList() throws SQLExc
         try (ResultSet rs = statement.get().executeQuery(sql)) {
             while (rs.next()) {
                 Map<String, String> column = new HashMap<>(16);
+                // oracle中，resultSet的LONG、LONG ROW类型要放第一个取出来
+                column.put(KEY_COLUMN_DEFAULT, rs.getString(5));
                 column.put(KEY_COLUMN_NAME, rs.getString(1));
                 String type = rs.getString(2);
                 String length = rs.getString(7);
@@ -240,7 +242,6 @@ protected Map<String, List<Map<String, String>>> queryColumnList() throws SQLExc
                 }
                 column.put(KEY_COLUMN_COMMENT, rs.getString(3));
                 String tableName = rs.getString(4);
-                column.put(KEY_COLUMN_DEFAULT, rs.getString(5));
                 column.put(KEY_COLUMN_NULL, rs.getString(6));
                 column.put(KEY_COLUMN_SCALE, length);
                 String index = rs.getString(8);

File: flinkx-ftp/flinkx-ftp-core/src/main/java/com/dtstack/flinkx/ftp/FtpHandler.java
Patch:
@@ -28,6 +28,8 @@
 import org.apache.commons.net.ftp.FTPReply;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
+
+import java.io.FileNotFoundException;
 import java.io.IOException;
 import java.io.InputStream;
 import java.io.OutputStream;
@@ -110,7 +112,7 @@ public boolean isDirExist(String directoryPath) {
             ftpClient.enterLocalPassiveMode();
             FTPFile[] ftpFiles = ftpClient.listFiles(new String(directoryPath.getBytes(StandardCharsets.UTF_8), FTP.DEFAULT_CONTROL_ENCODING));
             if(ftpFiles.length == 0){
-                throw new RuntimeException("file or path is not exist, please check the path or the permissions of account, path = " + directoryPath);
+                throw new FileNotFoundException("file or path is not exist, please check the path or the permissions of account, path = " + directoryPath);
             }else {
                 return FTPReply.isPositiveCompletion(ftpClient.cwd(directoryPath));
             }
@@ -134,7 +136,6 @@ public boolean isFileExist(String filePath) {
         return !isDirExist(filePath);
     }
 
-
     @Override
     public List<String> getFiles(String path) {
         List<String> sources = new ArrayList<>();
@@ -165,7 +166,6 @@ public List<String> getFiles(String path) {
 
     /**
      * 递归获取指定路径下的所有文件(暂无过滤)
-     * isDirExist()、isFileExist()方法在Windows系统下判断有误
      * @param path
      * @param file
      * @return

File: flinkx-ftp/flinkx-ftp-reader/src/main/java/com/dtstack/flinkx/ftp/reader/FtpInputFormat.java
Patch:
@@ -43,8 +43,6 @@ public class FtpInputFormat extends BaseRichInputFormat {
 
     protected FtpConfig ftpConfig;
 
-    protected String charsetName = "utf-8";
-
     protected List<MetaColumn> metaColumns;
 
     private transient FtpSeqBufferedReader br;
@@ -102,7 +100,7 @@ public void openInternal(InputSplit split) throws IOException {
             br = new FtpSeqBufferedReader(ftpHandler,paths.iterator());
             br.setFromLine(0);
         }
-        br.setCharsetName(charsetName);
+        br.setFileEncoding(ftpConfig.getEncoding());
     }
 
     @Override

File: flinkx-metadata-tidb/flinkx-metadata-tidb-reader/src/main/java/com/dtstack/flinkx/metadatatidb/constants/TidbMetadataCons.java
Patch:
@@ -53,6 +53,7 @@ public class TidbMetadataCons extends MetaDataCons {
     public static final String RESULT_COMMENT = "Comment";
 
     /** sql语句 */
+    public static final String SQL_SWITCH_DATABASE = "USE `%s`";
     public static final String SQL_SHOW_TABLES = "SHOW FULL TABLES WHERE Table_type = 'BASE TABLE'";
     public static final String SQL_QUERY_TABLE_INFO = "SHOW TABLE STATUS LIKE '%s'";
     public static final String SQL_QUERY_COLUMN = "SHOW FULL COLUMNS FROM %s";

File: flinkx-restapi/flinkx-restapi-core/src/main/java/com/dtstack/flinkx/restapi/common/RestapiKeys.java
Patch:
@@ -29,4 +29,5 @@ public class RestapiKeys {
     public static final String KEY_COLUMN = "column";
     public static final String KEY_URL = "url";
     public static final String KEY_BATCH_INTERVAL = "batchInterval";
+    public static final String KEY_BATCH = "batchId";
 }

File: flinkx-core/src/main/java/com/dtstack/flinkx/enums/EDatabaseType.java
Patch:
@@ -54,5 +54,6 @@ public enum EDatabaseType {
     polarDB,
     Phoenix,
     dm,
-    SapHana
+    SapHana,
+    KingBase
 }

File: flinkx-core/src/main/java/com/dtstack/flinkx/metrics/BaseMetric.java
Patch:
@@ -56,7 +56,7 @@ public void addMetric(String metricName, LongCounter counter, boolean meterView)
         metricCounters.put(metricName, counter);
         flinkxOutput.gauge(metricName, new SimpleAccumulatorGauge<Long>(counter));
         if (meterView){
-            flinkxOutput.meter(metricName + Metrics.SUFFIX_RATE, new SimpleLongCounterMeterView(counter, 60));
+            flinkxOutput.meter(metricName + Metrics.SUFFIX_RATE, new SimpleLongCounterMeterView(counter, 20));
         }
     }
 

File: flinkx-emqx/flinkx-emqx-writer/src/main/java/com/dtstack/flinkx/emqx/writer/EmqxWriter.java
Patch:
@@ -66,6 +66,9 @@ public DataStreamSink<?> writeData(DataStream<Row> dataSet) {
         builder.setPassword(password);
         builder.setCleanSession(isCleanSession);
         builder.setQos(qos);
+        builder.setDirtyPath(dirtyPath);
+        builder.setDirtyHadoopConfig(dirtyHadoopConfig);
+        builder.setSrcCols(srcCols);
         return createOutput(dataSet, builder.finish());
     }
 }

File: flinkx-kafka/flinkx-kafka-writer/src/main/java/com/dtstack/flinkx/kafka/writer/KafkaWriter.java
Patch:
@@ -47,7 +47,9 @@ public DataStreamSink<?> writeData(DataStream<Row> dataSet) {
         format.setProducerSettings(producerSettings);
         format.setRestoreConfig(restoreConfig);
         format.setTableFields(tableFields);
-
+        format.setDirtyPath(dirtyPath);
+        format.setDirtyHadoopConfig(dirtyHadoopConfig);
+        format.setSrcFieldNames(srcCols);
         return createOutput(dataSet, format);
     }
 }

File: flinkx-kafka10/flinkx-kafka10-writer/src/main/java/com/dtstack/flinkx/kafka10/writer/Kafka10Writer.java
Patch:
@@ -46,7 +46,9 @@ public DataStreamSink<?> writeData(DataStream<Row> dataSet) {
         format.setTableFields(tableFields);
         format.setProducerSettings(producerSettings);
         format.setRestoreConfig(restoreConfig);
-
+        format.setDirtyPath(dirtyPath);
+        format.setDirtyHadoopConfig(dirtyHadoopConfig);
+        format.setSrcFieldNames(srcCols);
         return createOutput(dataSet, format);
     }
 }

File: flinkx-kafka11/flinkx-kafka11-writer/src/main/java/com/dtstack/flinkx/kafka11/writer/Kafka11Writer.java
Patch:
@@ -46,7 +46,9 @@ public DataStreamSink<?> writeData(DataStream<Row> dataSet) {
         format.setProducerSettings(producerSettings);
         format.setRestoreConfig(restoreConfig);
         format.setTableFields(tableFields);
-
+        format.setDirtyPath(dirtyPath);
+        format.setDirtyHadoopConfig(dirtyHadoopConfig);
+        format.setSrcFieldNames(srcCols);
         return createOutput(dataSet, format);
     }
 }

File: flinkx-kb/flinkx-kb-writer/src/main/java/com/dtstack/flinkx/kafkabase/writer/KafkaBaseWriter.java
Patch:
@@ -62,7 +62,9 @@ public DataStreamSink<?> writeData(DataStream<Row> dataSet) {
         format.setProducerSettings(producerSettings);
         format.setRestoreConfig(restoreConfig);
         format.setTableFields(tableFields);
-
+        format.setDirtyPath(dirtyPath);
+        format.setDirtyHadoopConfig(dirtyHadoopConfig);
+        format.setSrcFieldNames(srcCols);
         return createOutput(dataSet, format);
     }
 }

File: flinkx-kudu/flinkx-kudu-writer/src/main/java/com/dtstack/flinkx/kudu/writer/KuduWriter.java
Patch:
@@ -95,6 +95,9 @@ public DataStreamSink<?> writeData(DataStream<Row> dataSet) {
         builder.setErrors(errors);
         builder.setErrorRatio(errorRatio);
         builder.setHadoopConfig(hadoopConfig);
+        builder.setDirtyPath(dirtyPath);
+        builder.setDirtyHadoopConfig(dirtyHadoopConfig);
+        builder.setSrcCols(srcCols);
         return createOutput(dataSet,builder.finish());
     }
 }

File: flinkx-launcher/src/main/java/com/dtstack/flinkx/launcher/perjob/PerJobSubmitter.java
Patch:
@@ -18,6 +18,7 @@
 
 package com.dtstack.flinkx.launcher.perjob;
 
+import com.dtstack.flinkx.launcher.KerberosInfo;
 import com.dtstack.flinkx.options.Options;
 import com.dtstack.flinkx.util.MapUtil;
 import org.apache.commons.lang.StringUtils;
@@ -54,6 +55,7 @@ public static String submit(Options options, JobGraph jobGraph) throws Exception
         ClusterSpecification clusterSpecification = FlinkPerJobResourceUtil.createClusterSpecification(conProp);
         PerJobClusterClientBuilder perJobClusterClientBuilder = new PerJobClusterClientBuilder();
         Configuration config = StringUtils.isEmpty(options.getFlinkconf()) ? new Configuration() : GlobalConfiguration.loadConfiguration(options.getFlinkconf());
+        perJobClusterClientBuilder.setKerberosInfo(new KerberosInfo(options.getKrb5conf(),options.getKeytab(),options.getPrincipal(),config));
         perJobClusterClientBuilder.init(options.getYarnconf(), config, conProp);
 
         AbstractYarnClusterDescriptor descriptor = perJobClusterClientBuilder.createPerJobClusterDescriptor(conProp, options, jobGraph);

File: flinkx-oraclelogminer/flinkx-oraclelogminer-reader/src/main/java/com/dtstack/flinkx/oraclelogminer/format/LogFile.java
Patch:
@@ -15,8 +15,6 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-
-
 package com.dtstack.flinkx.oraclelogminer.format;
 
 import java.util.Objects;

File: flinkx-oraclelogminer/flinkx-oraclelogminer-reader/src/main/java/com/dtstack/flinkx/oraclelogminer/format/LogMinerConfig.java
Patch:
@@ -61,7 +61,7 @@ public class LogMinerConfig implements Serializable {
 
     private List<String> table;
 
-    private Long queryTimeout;
+    private Long queryTimeout = 100L;
 
     /**
      * Oracle 12c第二个版本之后LogMiner不支持自动添加日志

File: flinkx-restapi/flinkx-restapi-writer/src/main/java/com/dtstack/flinkx/restapi/writer/RestapiWriter.java
Patch:
@@ -97,6 +97,9 @@ public DataStreamSink<?> writeData(DataStream<Row> dataSet) {
         builder.setColumn(column);
         builder.setParams(params);
         builder.setBatchInterval(batchInterval);
+        builder.setDirtyPath(dirtyPath);
+        builder.setDirtyHadoopConfig(dirtyHadoopConfig);
+        builder.setSrcCols(srcCols);
 
         return createOutput(dataSet, builder.finish());
     }

File: flinkx-sqlservercdc/flinkx-sqlservercdc-core/src/main/java/com/dtstack/flinkx/sqlservercdc/SqlServerCdcUtil.java
Patch:
@@ -64,7 +64,7 @@ public class SqlServerCdcUtil {
 
     public static void changeDatabase(Connection conn, String databaseName) throws SQLException {
         try (Statement statement = conn.createStatement()) {
-            statement.execute(" use " + databaseName);
+            statement.execute(" use " + "\""+databaseName+"\"");
         }
     }
 

File: flinkx-test/src/main/java/com/dtstack/flinkx/test/PluginNameConstrant.java
Patch:
@@ -58,6 +58,7 @@ public class PluginNameConstrant {
     public static final String METADATASQLSERVER_READER = "metadatasqlserverreader";
     public static final String GREENPLUM_READER = "greenplumreader";
     public static final String PHOENIX5_READER = "phoenix5reader";
+    public static final String KINGBASE_READER = "kingbasereader";
 
     public static final String STREAM_WRITER = "streamwriter";
     public static final String CARBONDATA_WRITER = "carbondatawriter";
@@ -88,4 +89,5 @@ public class PluginNameConstrant {
     public static final String DM_WRITER = "dmwriter";
     public static final String GREENPLUM_WRITER = "greenplumwriter";
     public static final String PHOENIX5_WRITER = "phoenix5writer";
+    public static final String KINGBASE_WRITER = "kingbasewriter";
 }

File: flinkx-rdb/flinkx-rdb-reader/src/main/java/com.dtstack.flinkx.rdb.inputformat/JdbcInputFormat.java
Patch:
@@ -53,6 +53,7 @@
 import java.sql.SQLException;
 import java.sql.Statement;
 import java.sql.Timestamp;
+import java.text.SimpleDateFormat;
 import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.List;

File: flinkx-ftp/flinkx-ftp-reader/src/main/java/com/dtstack/flinkx/ftp/reader/FtpSeqBufferedReader.java
Patch:
@@ -77,7 +77,7 @@ private void nextStream() throws IOException{
             String file = iter.next();
             InputStream in = ftpHandler.getInputStream(file);
             if (in == null) {
-                throw new NullPointerException();
+                throw new RuntimeException(String.format("can not get inputStream for file [%s], please check file read and write permissions", file));
             }
 
             br = new BufferedReader(new InputStreamReader(in, charsetName));

File: flinkx-core/src/main/java/com/dtstack/flinkx/metrics/BaseMetric.java
Patch:
@@ -56,7 +56,7 @@ public void addMetric(String metricName, LongCounter counter, boolean meterView)
         metricCounters.put(metricName, counter);
         flinkxOutput.gauge(metricName, new SimpleAccumulatorGauge<Long>(counter));
         if (meterView){
-            flinkxOutput.meter(metricName + Metrics.SUFFIX_RATE, new SimpleLongCounterMeterView(counter, 60));
+            flinkxOutput.meter(metricName + Metrics.SUFFIX_RATE, new SimpleLongCounterMeterView(counter, 20));
         }
     }
 

File: flinkx-restapi/flinkx-restapi-core/src/main/java/com/dtstack/flinkx/restapi/common/RestapiKeys.java
Patch:
@@ -29,4 +29,5 @@ public class RestapiKeys {
     public static final String KEY_COLUMN = "column";
     public static final String KEY_URL = "url";
     public static final String KEY_BATCH_INTERVAL = "batchInterval";
+    public static final String KEY_BATCH = "batchId";
 }

File: flinkx-metadata-mysql/flinkx-metadata-mysql-reader/src/main/java/com/dtstack/flinkx/metadatamysql/constants/MysqlMetadataCons.java
Patch:
@@ -47,5 +47,5 @@ public class MysqlMetadataCons extends TidbMetadataCons {
     public static final String RESULT_INDEX_COMMENT = "Index_comment";
 
     public static final String SQL_QUERY_TABLE_INFO = "SELECT * FROM  INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = '%s' AND TABLE_NAME = '%s'";
-    public static final String SQL_QUERY_INDEX = "SHOW INDEX FROM %s";
+    public static final String SQL_QUERY_INDEX = "SHOW INDEX FROM `%s`";
 }

File: flinkx-ftp/flinkx-ftp-reader/src/main/java/com/dtstack/flinkx/ftp/reader/FtpSeqBufferedReader.java
Patch:
@@ -77,7 +77,7 @@ private void nextStream() throws IOException{
             String file = iter.next();
             InputStream in = ftpHandler.getInputStream(file);
             if (in == null) {
-                throw new NullPointerException();
+                throw new RuntimeException(String.format("can not get inputStream for file [%s]", file));
             }
 
             br = new BufferedReader(new InputStreamReader(in, charsetName));

File: flinkx-rdb/flinkx-rdb-writer/src/main/java/com/dtstack/flinkx/rdb/outputformat/JdbcOutputFormat.java
Patch:
@@ -175,6 +175,8 @@ protected void openInternal(int taskNumber, int numTasks){
             LOG.info("subTask[{}}] wait finished", taskNumber);
         } catch (SQLException sqe) {
             throw new IllegalArgumentException("open() failed.", sqe);
+        }finally {
+            DbUtil.commit(dbConn);
         }
     }
 
@@ -199,7 +201,6 @@ protected List<String> analyzeTable() {
             throw new RuntimeException(e);
         } finally {
             DbUtil.closeDbResources(rs, stmt,null, false);
-            DbUtil.commit(dbConn);
         }
 
         return ret;

File: flinkx-rdb/flinkx-rdb-core/src/main/java/com/dtstack/flinkx/rdb/util/DbUtil.java
Patch:
@@ -191,6 +191,8 @@ public static void closeDbResources(ResultSet rs, Statement stmt, Connection con
             try {
                 if(commit){
                     commit(conn);
+                }else {
+                    rollBack(conn);
                 }
 
                 conn.close();

File: flinkx-rdb/flinkx-rdb-writer/src/main/java/com/dtstack/flinkx/rdb/outputformat/JdbcOutputFormat.java
Patch:
@@ -199,6 +199,7 @@ protected List<String> analyzeTable() {
             throw new RuntimeException(e);
         } finally {
             DbUtil.closeDbResources(rs, stmt,null, false);
+            DbUtil.commit(dbConn);
         }
 
         return ret;

File: flinkx-core/src/main/java/com/dtstack/flinkx/constants/ConstantValue.java
Patch:
@@ -27,6 +27,7 @@ public class ConstantValue {
 
     public static final String STAR_SYMBOL = "*";
     public static final String POINT_SYMBOL = ".";
+    public static final String TWO_POINT_SYMBOL = "..";
     public static final String EQUAL_SYMBOL = "=";
     public static final String SINGLE_QUOTE_MARK_SYMBOL = "'";
     public static final String DOUBLE_QUOTE_MARK_SYMBOL = "\"";

File: flinkx-core/src/main/java/com/dtstack/flinkx/outputformat/BaseFileOutputFormat.java
Patch:
@@ -106,7 +106,7 @@ protected void openInternal(int taskNumber, int numTasks) throws IOException {
         nextBlock();
     }
 
-    private void initPath(){
+    protected void initPath(){
         if(StringUtils.isNotBlank(fileName)) {
             outputFilePath = path + SP + fileName;
         } else {
@@ -122,7 +122,7 @@ private void initPath(){
                 taskNumber, currentBlockFileNamePrefix, tmpPath, finishedPath);
     }
 
-    private void initFileIndex() {
+    protected void initFileIndex() {
         if (null != formatState && formatState.getFileIndex() > -1) {
             blockIndex = formatState.getFileIndex() + 1;
         }

File: flinkx-metadata-tidb/flinkx-metadata-tidb-reader/src/main/java/com/dtstack/flinkx/metadatatidb/constants/TidbMetadataCons.java
Patch:
@@ -55,7 +55,7 @@ public class TidbMetadataCons extends MetaDataCons {
     /** sql语句 */
     public static final String SQL_SHOW_TABLES = "SHOW FULL TABLES WHERE Table_type = 'BASE TABLE'";
     public static final String SQL_QUERY_TABLE_INFO = "SHOW TABLE STATUS LIKE '%s'";
-    public static final String SQL_QUERY_COLUMN = "SHOW FULL COLUMNS FROM %s";
+    public static final String SQL_QUERY_COLUMN = "SHOW FULL COLUMNS FROM `%s`";
     public static final String SQL_QUERY_HEALTHY = "SHOW STATS_HEALTHY WHERE Table_name='%s' AND Db_name = schema()";
     public static final String SQL_QUERY_UPDATE_TIME = "SHOW STATS_META WHERE Table_name = '%s'";
     public static final String SQL_QUERY_PARTITION = "SELECT * FROM information_schema.partitions WHERE table_schema = schema() AND table_name='%s'";

File: flinkx-core/src/main/java/com/dtstack/flinkx/enums/EDatabaseType.java
Patch:
@@ -54,5 +54,6 @@ public enum EDatabaseType {
     polarDB,
     Phoenix,
     dm,
-    SapHana
+    SapHana,
+    KingBase
 }

File: flinkx-rdb/flinkx-rdb-reader/src/main/java/com.dtstack.flinkx.rdb.inputformat/JdbcInputFormat.java
Patch:
@@ -801,7 +801,7 @@ protected void executeQuery(String startLocation) throws SQLException {
                 //从数据库中获取起始位置
                 queryStartLocation();
             }else{
-                ps = dbConn.prepareStatement(querySql);
+                ps = dbConn.prepareStatement(querySql, resultSetType, resultSetConcurrency);
                 ps.setFetchSize(fetchSize);
                 ps.setQueryTimeout(queryTimeOut);
                 queryForPolling(startLocation);
@@ -826,7 +826,7 @@ private void queryStartLocation() throws SQLException{
                 .append(ConstantValue.DOUBLE_QUOTE_MARK_SYMBOL)
                 .append(incrementConfig.getColumnName())
                 .append(ConstantValue.DOUBLE_QUOTE_MARK_SYMBOL);
-        ps = dbConn.prepareStatement(builder.toString(), ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);
+        ps = dbConn.prepareStatement(builder.toString(), resultSetType, resultSetConcurrency);
         ps.setFetchSize(fetchSize);
         //第一次查询数据库中增量字段的最大值
         ps.setFetchDirection(ResultSet.FETCH_REVERSE);
@@ -861,7 +861,7 @@ private void queryStartLocation() throws SQLException{
                 .append(incrementConfig.getColumnName())
                 .append(ConstantValue.DOUBLE_QUOTE_MARK_SYMBOL);
         querySql = builder.toString();
-        ps = dbConn.prepareStatement(querySql);
+        ps = dbConn.prepareStatement(querySql, resultSetType, resultSetConcurrency);
         ps.setFetchDirection(ResultSet.FETCH_REVERSE);
         ps.setFetchSize(fetchSize);
         ps.setQueryTimeout(queryTimeOut);

File: flinkx-test/src/main/java/com/dtstack/flinkx/test/PluginNameConstrant.java
Patch:
@@ -58,6 +58,7 @@ public class PluginNameConstrant {
     public static final String METADATASQLSERVER_READER = "metadatasqlserverreader";
     public static final String GREENPLUM_READER = "greenplumreader";
     public static final String PHOENIX5_READER = "phoenix5reader";
+    public static final String KINGBASE_READER = "kingbasereader";
 
     public static final String STREAM_WRITER = "streamwriter";
     public static final String CARBONDATA_WRITER = "carbondatawriter";
@@ -88,4 +89,5 @@ public class PluginNameConstrant {
     public static final String DM_WRITER = "dmwriter";
     public static final String GREENPLUM_WRITER = "greenplumwriter";
     public static final String PHOENIX5_WRITER = "phoenix5writer";
+    public static final String KINGBASE_WRITER = "kingbasewriter";
 }

File: flinkx-metadata-sqlserver/flinkx-metadata-sqlserver-reader/src/main/java/com/dtstack/flinkx/metadatasqlserver/constants/SqlServerMetadataCons.java
Patch:
@@ -34,8 +34,6 @@ public class SqlServerMetadataCons extends MetaDataCons {
     public static final String KEY_SCHEMA_NAME = "schemaName";
     public static final String KEY_TABLE_NAME = "tableName";
     public static final String KEY_TABLE_SCHEMA = "tableSchema";
-    public static final String KEY_SCHEMA_NAME = "schemaName";
-    public static final String KEY_TABLE_NAME = "tableName";
 
     public static final String KEY_ZERO = "0";
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/latch/MetricLatch.java
Patch:
@@ -19,6 +19,7 @@
 package com.dtstack.flinkx.latch;
 
 import com.dtstack.flinkx.constants.ConstantValue;
+import com.dtstack.flinkx.util.GsonUtil;
 import com.dtstack.flinkx.util.UrlUtil;
 import com.google.gson.Gson;
 import com.google.gson.internal.LinkedTreeMap;
@@ -76,10 +77,11 @@ private int getIntMetricVal(String requestUrl) {
         try(InputStream inputStream = UrlUtil.open(requestUrl)) {
             try(Reader rd = new InputStreamReader(inputStream, StandardCharsets.UTF_8)) {
                 Map<String,Object> map = gson.fromJson(rd, Map.class);
+                LOG.info("requestUrl = {}, and return map = {}", requestUrl, GsonUtil.GSON.toJson(map));
                 List<LinkedTreeMap> userTaskAccumulators = (List<LinkedTreeMap>) map.get("user-task-accumulators");
                 for(LinkedTreeMap accumulator : userTaskAccumulators) {
                     if(metricName != null && metricName.equals(accumulator.get("name"))) {
-                        return Integer.valueOf((String )accumulator.get("value"));
+                        return Integer.parseInt((String )accumulator.get("value"));
                     }
                 }
             } catch (Exception e) {

File: flinkx-rdb/flinkx-rdb-writer/src/main/java/com/dtstack/flinkx/rdb/outputformat/JdbcOutputFormat.java
Patch:
@@ -264,8 +264,6 @@ protected void writeMultipleRecordsInternal() throws Exception {
 
             if(restoreConfig.isRestore()){
                 rowsOfCurrentTransaction += rows.size();
-            }else{
-                dbConn.commit();
             }
         } catch (Exception e){
             if (restoreConfig.isRestore()){

File: flinkx-emqx/flinkx-emqx-writer/src/main/java/com/dtstack/flinkx/emqx/writer/EmqxWriter.java
Patch:
@@ -66,6 +66,9 @@ public DataStreamSink<?> writeData(DataStream<Row> dataSet) {
         builder.setPassword(password);
         builder.setCleanSession(isCleanSession);
         builder.setQos(qos);
+        builder.setDirtyPath(dirtyPath);
+        builder.setDirtyHadoopConfig(dirtyHadoopConfig);
+        builder.setSrcCols(srcCols);
         return createOutput(dataSet, builder.finish());
     }
 }

File: flinkx-kafka/flinkx-kafka-writer/src/main/java/com/dtstack/flinkx/kafka/writer/KafkaWriter.java
Patch:
@@ -47,7 +47,9 @@ public DataStreamSink<?> writeData(DataStream<Row> dataSet) {
         format.setProducerSettings(producerSettings);
         format.setRestoreConfig(restoreConfig);
         format.setTableFields(tableFields);
-
+        format.setDirtyPath(dirtyPath);
+        format.setDirtyHadoopConfig(dirtyHadoopConfig);
+        format.setSrcFieldNames(srcCols);
         return createOutput(dataSet, format);
     }
 }

File: flinkx-kafka09/flinkx-kafka09-writer/src/main/java/com/dtstack/flinkx/kafka09/writer/Kafka09Writer.java
Patch:
@@ -58,7 +58,9 @@ public DataStreamSink<?> writeData(DataStream<Row> dataSet) {
         format.setBrokerList(brokerList);
         format.setProducerSettings(producerSettings);
         format.setRestoreConfig(restoreConfig);
-
+        format.setDirtyPath(dirtyPath);
+        format.setDirtyHadoopConfig(dirtyHadoopConfig);
+        format.setSrcFieldNames(srcCols);
         return createOutput(dataSet, format);
     }
 }

File: flinkx-kafka10/flinkx-kafka10-writer/src/main/java/com/dtstack/flinkx/kafka10/writer/Kafka10Writer.java
Patch:
@@ -46,7 +46,9 @@ public DataStreamSink<?> writeData(DataStream<Row> dataSet) {
         format.setTableFields(tableFields);
         format.setProducerSettings(producerSettings);
         format.setRestoreConfig(restoreConfig);
-
+        format.setDirtyPath(dirtyPath);
+        format.setDirtyHadoopConfig(dirtyHadoopConfig);
+        format.setSrcFieldNames(srcCols);
         return createOutput(dataSet, format);
     }
 }

File: flinkx-kafka11/flinkx-kafka11-writer/src/main/java/com/dtstack/flinkx/kafka11/writer/Kafka11Writer.java
Patch:
@@ -46,7 +46,9 @@ public DataStreamSink<?> writeData(DataStream<Row> dataSet) {
         format.setProducerSettings(producerSettings);
         format.setRestoreConfig(restoreConfig);
         format.setTableFields(tableFields);
-
+        format.setDirtyPath(dirtyPath);
+        format.setDirtyHadoopConfig(dirtyHadoopConfig);
+        format.setSrcFieldNames(srcCols);
         return createOutput(dataSet, format);
     }
 }

File: flinkx-kb/flinkx-kb-writer/src/main/java/com/dtstack/flinkx/kafkabase/writer/KafkaBaseWriter.java
Patch:
@@ -62,7 +62,9 @@ public DataStreamSink<?> writeData(DataStream<Row> dataSet) {
         format.setProducerSettings(producerSettings);
         format.setRestoreConfig(restoreConfig);
         format.setTableFields(tableFields);
-
+        format.setDirtyPath(dirtyPath);
+        format.setDirtyHadoopConfig(dirtyHadoopConfig);
+        format.setSrcFieldNames(srcCols);
         return createOutput(dataSet, format);
     }
 }

File: flinkx-kudu/flinkx-kudu-writer/src/main/java/com/dtstack/flinkx/kudu/writer/KuduWriter.java
Patch:
@@ -95,6 +95,9 @@ public DataStreamSink<?> writeData(DataStream<Row> dataSet) {
         builder.setErrors(errors);
         builder.setErrorRatio(errorRatio);
         builder.setHadoopConfig(hadoopConfig);
+        builder.setDirtyPath(dirtyPath);
+        builder.setDirtyHadoopConfig(dirtyHadoopConfig);
+        builder.setSrcCols(srcCols);
         return createOutput(dataSet,builder.finish());
     }
 }

File: flinkx-restapi/flinkx-restapi-writer/src/main/java/com/dtstack/flinkx/restapi/writer/RestapiWriter.java
Patch:
@@ -97,6 +97,9 @@ public DataStreamSink<?> writeData(DataStream<Row> dataSet) {
         builder.setColumn(column);
         builder.setParams(params);
         builder.setBatchInterval(batchInterval);
+        builder.setDirtyPath(dirtyPath);
+        builder.setDirtyHadoopConfig(dirtyHadoopConfig);
+        builder.setSrcCols(srcCols);
 
         return createOutput(dataSet, builder.finish());
     }

File: flinkx-sqlservercdc/flinkx-sqlservercdc-core/src/main/java/com/dtstack/flinkx/sqlservercdc/SqlServerCdcUtil.java
Patch:
@@ -64,7 +64,7 @@ public class SqlServerCdcUtil {
 
     public static void changeDatabase(Connection conn, String databaseName) throws SQLException {
         try (Statement statement = conn.createStatement()) {
-            statement.execute(" use " + databaseName);
+            statement.execute(" use " + "\""+databaseName+"\"");
         }
     }
 

File: flinkx-redis/flinkx-redis-writer/src/main/java/com/dtstack/flinkx/redis/writer/RedisOutputFormat.java
Patch:
@@ -84,7 +84,9 @@ public void configure(Configuration parameters) {
 
         Properties properties = new Properties();
         properties.put(KEY_HOST_PORT,hostPort);
-        properties.put(KEY_PASSWORD,password);
+        if(StringUtils.isNotBlank(password)){
+            properties.put(KEY_PASSWORD, password);
+        }
         properties.put(KEY_TIMEOUT,timeout);
         properties.put(KEY_DB,database);
 

File: flinkx-metadata-sqlserver/flinkx-metadata-sqlserver-reader/src/main/java/com/dtstack/flinkx/metadatasqlserver/constants/SqlServerMetadataCons.java
Patch:
@@ -35,6 +35,8 @@ public class SqlServerMetadataCons extends MetaDataCons {
     public static final String KEY_PARTITION_COLUMN = "partitionColumn";
     public static final String KEY_COLUMN_NAME = "columnName";
     public static final String KEY_FILE_GROUP_NAME = "fileGroupName";
+    public static final String KEY_SCHEMA_NAME = "schemaName";
+    public static final String KEY_TABLE_NAME = "tableName";
     public static final String KEY_TABLE_SCHEMA = "tableSchema";
     public static final char DEFAULT_DELIMITER = '.';
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/enums/EDatabaseType.java
Patch:
@@ -54,5 +54,6 @@ public enum EDatabaseType {
     polarDB,
     Phoenix,
     dm,
-    SapHana
+    SapHana,
+    KingBase
 }

File: flinkx-rdb/flinkx-rdb-reader/src/main/java/com.dtstack.flinkx.rdb.inputformat/JdbcInputFormat.java
Patch:
@@ -801,7 +801,7 @@ protected void executeQuery(String startLocation) throws SQLException {
                 //从数据库中获取起始位置
                 queryStartLocation();
             }else{
-                ps = dbConn.prepareStatement(querySql);
+                ps = dbConn.prepareStatement(querySql, resultSetType, resultSetConcurrency);
                 ps.setFetchSize(fetchSize);
                 ps.setQueryTimeout(queryTimeOut);
                 queryForPolling(startLocation);
@@ -826,7 +826,7 @@ private void queryStartLocation() throws SQLException{
                 .append(ConstantValue.DOUBLE_QUOTE_MARK_SYMBOL)
                 .append(incrementConfig.getColumnName())
                 .append(ConstantValue.DOUBLE_QUOTE_MARK_SYMBOL);
-        ps = dbConn.prepareStatement(builder.toString(), ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);
+        ps = dbConn.prepareStatement(builder.toString(), resultSetType, resultSetConcurrency);
         ps.setFetchSize(fetchSize);
         //第一次查询数据库中增量字段的最大值
         ps.setFetchDirection(ResultSet.FETCH_REVERSE);
@@ -861,7 +861,7 @@ private void queryStartLocation() throws SQLException{
                 .append(incrementConfig.getColumnName())
                 .append(ConstantValue.DOUBLE_QUOTE_MARK_SYMBOL);
         querySql = builder.toString();
-        ps = dbConn.prepareStatement(querySql);
+        ps = dbConn.prepareStatement(querySql, resultSetType, resultSetConcurrency);
         ps.setFetchDirection(ResultSet.FETCH_REVERSE);
         ps.setFetchSize(fetchSize);
         ps.setQueryTimeout(queryTimeOut);

File: flinkx-test/src/main/java/com/dtstack/flinkx/test/PluginNameConstrant.java
Patch:
@@ -58,6 +58,7 @@ public class PluginNameConstrant {
     public static final String METADATASQLSERVER_READER = "metadatasqlserverreader";
     public static final String GREENPLUM_READER = "greenplumreader";
     public static final String PHOENIX5_READER = "phoenix5reader";
+    public static final String KINGBASE_READER = "kingbasereader";
 
     public static final String STREAM_WRITER = "streamwriter";
     public static final String CARBONDATA_WRITER = "carbondatawriter";
@@ -88,4 +89,5 @@ public class PluginNameConstrant {
     public static final String DM_WRITER = "dmwriter";
     public static final String GREENPLUM_WRITER = "greenplumwriter";
     public static final String PHOENIX5_WRITER = "phoenix5writer";
+    public static final String KINGBASE_WRITER = "kingbasewriter";
 }

File: flinkx-core/src/main/java/com/dtstack/flinkx/constants/ConstantValue.java
Patch:
@@ -27,6 +27,7 @@ public class ConstantValue {
 
     public static final String STAR_SYMBOL = "*";
     public static final String POINT_SYMBOL = ".";
+    public static final String TWO_POINT_SYMBOL = "..";
     public static final String EQUAL_SYMBOL = "=";
     public static final String SINGLE_QUOTE_MARK_SYMBOL = "'";
     public static final String DOUBLE_QUOTE_MARK_SYMBOL = "\"";

File: flinkx-core/src/main/java/com/dtstack/flinkx/outputformat/BaseFileOutputFormat.java
Patch:
@@ -106,7 +106,7 @@ protected void openInternal(int taskNumber, int numTasks) throws IOException {
         nextBlock();
     }
 
-    private void initPath(){
+    protected void initPath(){
         if(StringUtils.isNotBlank(fileName)) {
             outputFilePath = path + SP + fileName;
         } else {
@@ -122,7 +122,7 @@ private void initPath(){
                 taskNumber, currentBlockFileNamePrefix, tmpPath, finishedPath);
     }
 
-    private void initFileIndex() {
+    protected void initFileIndex() {
         if (null != formatState && formatState.getFileIndex() > -1) {
             blockIndex = formatState.getFileIndex() + 1;
         }

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/StringUtil.java
Patch:
@@ -198,10 +198,10 @@ public static String col2string(Object column, String type) {
     }
 
 
-    public static String row2string(Row row, List<String> columnTypes, String delimiter, List<String> columnNames) throws WriteRecordException {
+    public static String row2string(Row row, List<String> columnTypes, String delimiter) throws WriteRecordException {
         // convert row to string
         int cnt = row.getArity();
-        StringBuilder sb = new StringBuilder();
+        StringBuilder sb = new StringBuilder(128);
 
         int i = 0;
         try {

File: flinkx-core/src/main/java/com/dtstack/flinkx/constants/ConstantValue.java
Patch:
@@ -27,6 +27,7 @@ public class ConstantValue {
 
     public static final String STAR_SYMBOL = "*";
     public static final String POINT_SYMBOL = ".";
+    public static final String TWO_POINT_SYMBOL = "..";
     public static final String EQUAL_SYMBOL = "=";
     public static final String SINGLE_QUOTE_MARK_SYMBOL = "'";
     public static final String DOUBLE_QUOTE_MARK_SYMBOL = "\"";

File: flinkx-core/src/main/java/com/dtstack/flinkx/outputformat/BaseFileOutputFormat.java
Patch:
@@ -106,7 +106,7 @@ protected void openInternal(int taskNumber, int numTasks) throws IOException {
         nextBlock();
     }
 
-    private void initPath(){
+    protected void initPath(){
         if(StringUtils.isNotBlank(fileName)) {
             outputFilePath = path + SP + fileName;
         } else {
@@ -122,7 +122,7 @@ private void initPath(){
                 taskNumber, currentBlockFileNamePrefix, tmpPath, finishedPath);
     }
 
-    private void initFileIndex() {
+    protected void initFileIndex() {
         if (null != formatState && formatState.getFileIndex() > -1) {
             blockIndex = formatState.getFileIndex() + 1;
         }

File: flinkx-core/src/main/java/com/dtstack/flinkx/Main.java
Patch:
@@ -189,6 +189,8 @@ private static void speedTest(DataTransferConfig config) {
         } else if (WRITER.equalsIgnoreCase(testConfig.getSpeedTest())){
             ContentConfig contentConfig = config.getJob().getContent().get(0);
             contentConfig.getReader().setName(STREAM_READER);
+        }else {
+            return;
         }
 
         config.getJob().getSetting().getSpeed().setBytes(-1);

File: flinkx-core/src/main/java/com/dtstack/flinkx/reader/ByteRateLimiter.java
Patch:
@@ -87,7 +87,7 @@ private void updateRate(){
         BigDecimal thisWriteRatio = BigDecimal.valueOf(totalRecords == 0 ? 0 : thisRecords / (double) totalRecords);
 
         if (totalRecords > MIN_RECORD_NUMBER_UPDATE_RATE && totalBytes != 0
-                && thisWriteRatio.compareTo(new BigDecimal(0)) == 0) {
+                && thisWriteRatio.compareTo(BigDecimal.ZERO) != 0) {
             double bpr = totalBytes / (double)totalRecords;
             double permitsPerSecond = expectedBytePerSecond / bpr * thisWriteRatio.doubleValue();
             rateLimiter.setRate(permitsPerSecond);

File: flinkx-hive/flinkx-hive-core/src/main/java/com/dtstack/flinkx/hive/util/HiveDbUtil.java
Patch:
@@ -63,7 +63,7 @@ public final class HiveDbUtil {
     public static final String PARAM_DELIMITER = "&";
     public static final String KEY_PRINCIPAL = "principal";
 
-    public static Pattern HIVE_JDBC_PATTERN = Pattern.compile("(?i)jdbc:hive2://(?<host>[0-9a-zA-Z\\.]+):(?<port>\\d+)/(?<db>[0-9a-z_%]+)(?<param>[\\?;#].*)*");
+    public static Pattern HIVE_JDBC_PATTERN = Pattern.compile("(?i)jdbc:hive2://(?<host>[^:]+):(?<port>\\d+)/(?<db>[^;]+)(?<param>[\\?;#].*)*");
     public static final String HOST_KEY = "host";
     public static final String PORT_KEY = "port";
     public static final String DB_KEY = "db";
@@ -264,7 +264,8 @@ public static String parseIpAndPort(String url) {
         if (matcher.find()) {
             addr = matcher.group(HOST_KEY) + ":" + matcher.group(PORT_KEY);
         } else {
-            addr = url.substring(url.indexOf("//") + 2, url.lastIndexOf("/"));
+            addr = url.substring(url.indexOf("//") + 2);
+            addr=  addr.substring(0,addr.indexOf("/"));
         }
         return addr;
     }

File: flinkx-launcher/src/main/java/com/dtstack/flinkx/launcher/perjob/PerJobSubmitter.java
Patch:
@@ -18,6 +18,7 @@
 
 package com.dtstack.flinkx.launcher.perjob;
 
+import com.dtstack.flinkx.launcher.KerberosInfo;
 import com.dtstack.flinkx.options.Options;
 import com.dtstack.flinkx.util.MapUtil;
 import org.apache.commons.lang.StringUtils;
@@ -54,6 +55,7 @@ public static String submit(Options options, JobGraph jobGraph) throws Exception
         ClusterSpecification clusterSpecification = FlinkPerJobResourceUtil.createClusterSpecification(conProp);
         PerJobClusterClientBuilder perJobClusterClientBuilder = new PerJobClusterClientBuilder();
         Configuration config = StringUtils.isEmpty(options.getFlinkconf()) ? new Configuration() : GlobalConfiguration.loadConfiguration(options.getFlinkconf());
+        perJobClusterClientBuilder.setKerberosInfo(new KerberosInfo(options.getKrb5conf(),options.getKeytab(),options.getPrincipal(),config));
         perJobClusterClientBuilder.init(options.getYarnconf(), config, conProp);
 
         AbstractYarnClusterDescriptor descriptor = perJobClusterClientBuilder.createPerJobClusterDescriptor(conProp, options, jobGraph);

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/StringUtil.java
Patch:
@@ -198,10 +198,10 @@ public static String col2string(Object column, String type) {
     }
 
 
-    public static String row2string(Row row, List<String> columnTypes, String delimiter, List<String> columnNames) throws WriteRecordException {
+    public static String row2string(Row row, List<String> columnTypes, String delimiter) throws WriteRecordException {
         // convert row to string
         int cnt = row.getArity();
-        StringBuilder sb = new StringBuilder();
+        StringBuilder sb = new StringBuilder(128);
 
         int i = 0;
         try {

File: flinkx-ftp/flinkx-ftp-writer/src/main/java/com/dtstack/flinkx/ftp/writer/FtpOutputFormat.java
Patch:
@@ -67,6 +67,7 @@ public class FtpOutputFormat extends BaseFileOutputFormat {
 
     @Override
     protected void openSource() throws IOException {
+        writeMode = ftpConfig.writeMode;
         ftpHandler = FtpHandlerFactory.createFtpHandler(ftpConfig.getProtocol());
         ftpHandler.loginFtpServer(ftpConfig);
     }

File: flinkx-hive/flinkx-hive-writer/src/main/java/com/dtstack/flinkx/hive/writer/HiveOutputFormat.java
Patch:
@@ -197,7 +197,8 @@ public void writeRecord(Row row) throws IOException {
                         event = GsonUtil.GSON.fromJson((String) tempObj, GsonUtil.gsonMapTypeToken);
                     }catch (JsonSyntaxException e){
                         // is not a json string
-                        LOG.warn("bad json string:【{}】", tempObj);
+                        //tempObj 不是map类型 则event直接往下传递
+                       // LOG.warn("bad json string:【{}】", tempObj);
                     }
                 }
             }

File: flinkx-restapi/flinkx-restapi-writer/src/main/java/com/dtstack/flinkx/restapi/outputformat/RestapiOutputFormat.java
Patch:
@@ -75,9 +75,9 @@ protected void writeSingleRecordInternal(Row row) throws WriteRecordException {
         CloseableHttpClient httpClient = HttpUtil.getHttpClient();
         int index = 0;
         Map<String, Object> requestBody = Maps.newHashMap();
-        Object dataRow;
+        List<Object> dataRow = new ArrayList<>();
         try {
-            dataRow = getDataFromRow(row, column);
+            dataRow.add(getDataFromRow(row, column));
             params.put(KEY_BATCH, UUID.randomUUID().toString().substring(0, 8));
             if (!params.isEmpty()) {
                 Iterator iterator = params.entrySet().iterator();

File: flinkx-restapi/flinkx-restapi-writer/src/main/java/com/dtstack/flinkx/restapi/outputformat/RestapiOutputFormat.java
Patch:
@@ -59,7 +59,7 @@ public class RestapiOutputFormat extends BaseRichOutputFormat {
 
     protected Map<String, String> header;
 
-    protected static final int DEFAULT_TIME_OUT = 1800000;
+    protected static final int DEFAULT_TIME_OUT = 300000;
 
     protected Gson gson;
 
@@ -75,9 +75,9 @@ protected void writeSingleRecordInternal(Row row) throws WriteRecordException {
         CloseableHttpClient httpClient = HttpUtil.getHttpClient();
         int index = 0;
         Map<String, Object> requestBody = Maps.newHashMap();
-        Object dataRow;
+        List<Object> dataRow = new ArrayList<>();
         try {
-            dataRow = getDataFromRow(row, column);
+            dataRow.add(getDataFromRow(row, column));
             params.put(KEY_BATCH, UUID.randomUUID().toString().substring(0, 8));
             if (!params.isEmpty()) {
                 Iterator iterator = params.entrySet().iterator();

File: flinkx-metadata-hive2/flinkx-metadata-hive2-reader/src/main/java/com/dtstack/flinkx/metadatahive2/inputformat/Metadatahive2InputFormat.java
Patch:
@@ -215,10 +215,10 @@ protected Map<String, Object> queryMetaData(String tableName) throws SQLExceptio
             }
             switch (metaDataFlag){
                 case 0:
-                    columnList.add(parseColumn(lineDataInternal, result.size()));
+                    columnList.add(parseColumn(lineDataInternal, columnList.size()+1));
                     break;
                 case 1:
-                    partitionColumnList.add(parseColumn(lineDataInternal, result.size()));
+                    partitionColumnList.add(parseColumn(lineDataInternal, partitionColumnList.size()+1));
                     break;
                 case 2:
                     parseTableProperties(lineDataInternal, tableProperties, it);

File: flinkx-metadata-hive2/flinkx-metadata-hive2-reader/src/main/java/com/dtstack/flinkx/metadatahive2/inputformat/Metadatahive2InputFormat.java
Patch:
@@ -213,10 +213,10 @@ protected Map<String, Object> queryMetaData(String tableName) throws SQLExceptio
             }
             switch (metaDataFlag){
                 case 0:
-                    columnList.add(parseColumn(lineDataInternal, result.size()));
+                    columnList.add(parseColumn(lineDataInternal, columnList.size()+1));
                     break;
                 case 1:
-                    partitionColumnList.add(parseColumn(lineDataInternal, result.size()));
+                    partitionColumnList.add(parseColumn(lineDataInternal, partitionColumnList.size()+1));
                     break;
                 case 2:
                     parseTableProperties(lineDataInternal, tableProperties, it);

File: flinkx-hive/flinkx-hive-writer/src/main/java/com/dtstack/flinkx/hive/writer/HiveOutputFormat.java
Patch:
@@ -197,7 +197,8 @@ public void writeRecord(Row row) throws IOException {
                         event = GsonUtil.GSON.fromJson((String) tempObj, GsonUtil.gsonMapTypeToken);
                     }catch (JsonSyntaxException e){
                         // is not a json string
-                        LOG.warn("bad json string:【{}】", tempObj);
+                        //tempObj 不是map类型 则event直接往下传递
+                       // LOG.warn("bad json string:【{}】", tempObj);
                     }
                 }
             }

File: flinkx-hdfs/flinkx-hdfs-reader/src/main/java/com/dtstack/flinkx/hdfs/reader/HdfsOrcInputFormat.java
Patch:
@@ -102,6 +102,7 @@ private void openOrcReader(InputSplit inputSplit) throws IOException{
         numReadCounter = getRuntimeContext().getLongCounter("numRead");
         HdfsOrcInputSplit hdfsOrcInputSplit = (HdfsOrcInputSplit) inputSplit;
         OrcSplit orcSplit = hdfsOrcInputSplit.getOrcSplit();
+        findCurrentPartition(orcSplit.getPath());
         recordReader = inputFormat.getRecordReader(orcSplit, conf, Reporter.NULL);
         key = recordReader.createKey();
         value = recordReader.createValue();

File: flinkx-hdfs/flinkx-hdfs-reader/src/main/java/com/dtstack/flinkx/hdfs/reader/HdfsOrcInputFormat.java
Patch:
@@ -102,6 +102,7 @@ private void openOrcReader(InputSplit inputSplit) throws IOException{
         numReadCounter = getRuntimeContext().getLongCounter("numRead");
         HdfsOrcInputSplit hdfsOrcInputSplit = (HdfsOrcInputSplit) inputSplit;
         OrcSplit orcSplit = hdfsOrcInputSplit.getOrcSplit();
+        findCurrentPartition(orcSplit.getPath());
         recordReader = inputFormat.getRecordReader(orcSplit, conf, Reporter.NULL);
         key = recordReader.createKey();
         value = recordReader.createValue();

File: flinkx-restapi/flinkx-restapi-writer/src/main/java/com/dtstack/flinkx/restapi/outputformat/RestapiOutputFormat.java
Patch:
@@ -88,7 +88,7 @@ protected void writeSingleRecordInternal(Row row) throws WriteRecordException {
             }
             body.put("data", dataRow);
             requestBody.put("json", body);
-            LOG.debug("当前发送的数据为:{}", gson.toJson(requestBody));
+            LOG.info("send data:{}", gson.toJson(requestBody));
             sendRequest(httpClient, requestBody, method, header, url);
         } catch (Exception e) {
             requestErrorMessage(e, index, row);
@@ -120,7 +120,7 @@ protected void writeMultipleRecordsInternal() throws Exception {
             }
             body.put("data", dataRow);
             requestBody.put("json", body);
-            LOG.debug("当前发送的数据为:{}", gson.toJson(requestBody));
+            LOG.info("this batch size = {}, send data:{}", rows.size(), gson.toJson(requestBody));
             sendRequest(httpClient, requestBody, method, header, url);
         } catch (Exception e) {
             LOG.error(ExceptionUtil.getErrorMessage(e));
@@ -155,7 +155,7 @@ private void sendRequest(CloseableHttpClient httpClient,
                              String method,
                              Map<String, String> header,
                              String url) throws IOException {
-        LOG.debug("当前发送的数据为:{}", gson.toJson(requestBody));
+        LOG.debug("send data:{}", gson.toJson(requestBody));
         HttpRequestBase request = HttpUtil.getRequest(method, requestBody, header, url);
         //设置请求和传输超时时间
         RequestConfig requestConfig = RequestConfig.custom()

File: flinkx-ftp/flinkx-ftp-core/src/main/java/com/dtstack/flinkx/ftp/IFtpHandler.java
Patch:
@@ -116,4 +116,6 @@ public interface IFtpHandler {
      * @throws Exception 可能会出现文件不存在，连接异常等
      */
     void rename(String oldPath, String newPath) throws Exception;
+
+    void completePendingCommand() throws IOException;
 }

File: flinkx-restapi/flinkx-restapi-core/src/main/java/com/dtstack/flinkx/restapi/common/RestapiKeys.java
Patch:
@@ -29,4 +29,5 @@ public class RestapiKeys {
     public static final String KEY_COLUMN = "column";
     public static final String KEY_URL = "url";
     public static final String KEY_BATCH_INTERVAL = "batchInterval";
+    public static final String KEY_BATCH = "batchId";
 }

File: flinkx-restapi/flinkx-restapi-core/src/main/java/com/dtstack/flinkx/restapi/common/RestapiKeys.java
Patch:
@@ -29,5 +29,5 @@ public class RestapiKeys {
     public static final String KEY_COLUMN = "column";
     public static final String KEY_URL = "url";
     public static final String KEY_BATCH_INTERVAL = "batchInterval";
-    public static final String KEY_BATCH = "batch";
+    public static final String KEY_BATCH = "batchId";
 }

File: flinkx-restapi/flinkx-restapi-core/src/main/java/com/dtstack/flinkx/restapi/common/RestapiKeys.java
Patch:
@@ -29,4 +29,5 @@ public class RestapiKeys {
     public static final String KEY_COLUMN = "column";
     public static final String KEY_URL = "url";
     public static final String KEY_BATCH_INTERVAL = "batchInterval";
+    public static final String KEY_BATCH = "batch";
 }

File: flinkx-restapi/flinkx-restapi-writer/src/main/java/com/dtstack/flinkx/restapi/outputformat/RestapiOutputFormat.java
Patch:
@@ -73,9 +73,9 @@ protected void writeSingleRecordInternal(Row row) throws WriteRecordException {
         CloseableHttpClient httpClient = HttpUtil.getHttpClient();
         int index = 0;
         Map<String, Object> requestBody = Maps.newHashMap();
-        Object dataRow;
+        List<Object> dataRow = new ArrayList<>();
         try {
-            dataRow = getDataFromRow(row, column);
+            dataRow.add(getDataFromRow(row, column));
             if (!params.isEmpty()) {
                 Iterator iterator = params.entrySet().iterator();
                 while (iterator.hasNext()) {

File: flinkx-restapi/flinkx-restapi-writer/src/main/java/com/dtstack/flinkx/restapi/outputformat/RestapiOutputFormat.java
Patch:
@@ -68,9 +68,9 @@ protected void writeSingleRecordInternal(Row row) throws WriteRecordException {
         CloseableHttpClient httpClient = HttpUtil.getHttpClient();
         int index = 0;
         Map<String, Object> requestBody = Maps.newHashMap();
-        Object dataRow;
+        List<Object> dataRow = new ArrayList<>();
         try {
-            dataRow = getDataFromRow(row, column);
+            dataRow.add(getDataFromRow(row, column));
             if (!params.isEmpty()) {
                 Iterator iterator = params.entrySet().iterator();
                 while (iterator.hasNext()) {

File: flinkx-metadata/flinkx-metadata-reader/src/main/java/com/dtstack/flinkx/metadata/inputformat/BaseMetadataInputFormat.java
Patch:
@@ -126,7 +126,7 @@ protected Row nextRecordInternal(Row row) throws IOException{
             metaData.put(MetaDataCons.KEY_ERROR_MSG, ExceptionUtil.getErrorMessage(e));
             LOG.error(ExceptionUtil.getErrorMessage(e));
         }
-
+        LOG.info("query metadata: {}", metaData);
         return Row.of(metaData);
     }
 

File: flinkx-restapi/flinkx-restapi-writer/src/main/java/com/dtstack/flinkx/restapi/outputformat/RestapiOutputFormat.java
Patch:
@@ -85,7 +85,7 @@ protected void writeSingleRecordInternal(Row row) throws WriteRecordException {
             }
             body.put("data", dataRow);
             requestBody.put("json", body);
-            LOG.debug("send data:{}", gson.toJson(requestBody));
+            LOG.info("send data:{}", gson.toJson(requestBody));
             sendRequest(httpClient, requestBody, method, header, url);
         } catch (Exception e) {
             requestErrorMessage(e, index, row);
@@ -151,7 +151,7 @@ private void sendRequest(CloseableHttpClient httpClient,
                              String method,
                              Map<String, String> header,
                              String url) throws IOException {
-        LOG.debug("当前发送的数据为:{}", gson.toJson(requestBody));
+        LOG.debug("send data:{}", gson.toJson(requestBody));
         HttpRequestBase request = HttpUtil.getRequest(method, requestBody, header, url);
         //设置请求和传输超时时间
         RequestConfig requestConfig = RequestConfig.custom()

File: flinkx-restapi/flinkx-restapi-writer/src/main/java/com/dtstack/flinkx/restapi/outputformat/RestapiOutputFormat.java
Patch:
@@ -73,9 +73,9 @@ protected void writeSingleRecordInternal(Row row) throws WriteRecordException {
         CloseableHttpClient httpClient = HttpUtil.getHttpClient();
         int index = 0;
         Map<String, Object> requestBody = Maps.newHashMap();
-        Object dataRow;
+        List<Object> dataRow = new ArrayList<>();
         try {
-            dataRow = getDataFromRow(row, column);
+            dataRow.add(getDataFromRow(row, column));
             if (!params.isEmpty()) {
                 Iterator iterator = params.entrySet().iterator();
                 while (iterator.hasNext()) {

File: flinkx-hive/flinkx-hive-writer/src/main/java/com/dtstack/flinkx/hive/writer/HiveOutputFormat.java
Patch:
@@ -197,7 +197,8 @@ public void writeRecord(Row row) throws IOException {
                         event = GsonUtil.GSON.fromJson((String) tempObj, GsonUtil.gsonMapTypeToken);
                     }catch (JsonSyntaxException e){
                         // is not a json string
-                        LOG.warn("bad json string:【{}】", tempObj);
+                        //tempObj 不是map类型 则event直接往下传递
+                       // LOG.warn("bad json string:【{}】", tempObj);
                     }
                 }
             }

File: flinkx-hive/flinkx-hive-writer/src/main/java/com/dtstack/flinkx/hive/writer/HiveOutputFormat.java
Patch:
@@ -197,8 +197,7 @@ public void writeRecord(Row row) throws IOException {
                         event = GsonUtil.GSON.fromJson((String) tempObj, GsonUtil.gsonMapTypeToken);
                     }catch (JsonSyntaxException e){
                         // is not a json string
-                        //tempObj 不是map类型 则event直接往下传递
-                       // LOG.warn("bad json string:【{}】", tempObj);
+                        LOG.warn("bad json string:【{}】", tempObj);
                     }
                 }
             }

File: flinkx-metadata-hive2/flinkx-metadata-hive2-reader/src/main/java/com/dtstack/flinkx/metadatahive2/inputformat/Metadatahive2InputFormat.java
Patch:
@@ -34,6 +34,7 @@
 import java.util.Map;
 
 import static com.dtstack.flinkx.metadata.MetaDataCons.KEY_INDEX_COMMENT;
+import static com.dtstack.flinkx.metadata.MetaDataCons.KEY_TABLE_COMMENT;
 import static com.dtstack.flinkx.metadatahive2.constants.Hive2MetaDataCons.KEY_COLUMN;
 import static com.dtstack.flinkx.metadatahive2.constants.Hive2MetaDataCons.KEY_COLUMN_COMMENT;
 import static com.dtstack.flinkx.metadatahive2.constants.Hive2MetaDataCons.KEY_COLUMN_DATA_TYPE;
@@ -340,7 +341,7 @@ void parseTableProperties(Map<String, String> lineDataInternal, Map<String, Obje
 
                 nameInternal = nameInternal.trim();
                 if (nameInternal.contains(KEY_INDEX_COMMENT)) {
-                    tableProperties.put(KEY_COLUMN_COMMENT, StringUtils.trim(unicodeToStr(lineDataInternal.get(paraSecond))));
+                    tableProperties.put(KEY_TABLE_COMMENT, StringUtils.trim(unicodeToStr(lineDataInternal.get(paraSecond))));
                 }
 
                 if (nameInternal.contains(KEY_TOTALSIZE)) {

File: flinkx-metadata-hive2/flinkx-metadata-hive2-reader/src/main/java/com/dtstack/flinkx/metadatahive2/inputformat/Metadatahive2InputFormat.java
Patch:
@@ -33,6 +33,7 @@
 import java.util.List;
 import java.util.Map;
 
+import static com.dtstack.flinkx.metadata.MetaDataCons.KEY_INDEX_COMMENT;
 import static com.dtstack.flinkx.metadatahive2.constants.Hive2MetaDataCons.KEY_COLUMN;
 import static com.dtstack.flinkx.metadatahive2.constants.Hive2MetaDataCons.KEY_COLUMN_COMMENT;
 import static com.dtstack.flinkx.metadatahive2.constants.Hive2MetaDataCons.KEY_COLUMN_DATA_TYPE;
@@ -244,7 +245,7 @@ protected Map<String, Object> queryMetaData(String tableName) throws SQLExceptio
 
     private Map<String, Object> parseColumn(Map<String, String> lineDataInternal, int index){
         String dataTypeInternal = lineDataInternal.get(KEY_COLUMN_DATA_TYPE);
-        String commentInternal = lineDataInternal.get(KEY_COLUMN_COMMENT);
+        String commentInternal = lineDataInternal.get(KEY_INDEX_COMMENT);
         String colNameInternal = lineDataInternal.get(KEY_COL_NAME);
 
         Map<String, Object> lineResult = new HashMap<>(16);
@@ -338,7 +339,7 @@ void parseTableProperties(Map<String, String> lineDataInternal, Map<String, Obje
                 }
 
                 nameInternal = nameInternal.trim();
-                if (nameInternal.contains(KEY_COLUMN_COMMENT)) {
+                if (nameInternal.contains(KEY_INDEX_COMMENT)) {
                     tableProperties.put(KEY_COLUMN_COMMENT, StringUtils.trim(unicodeToStr(lineDataInternal.get(paraSecond))));
                 }
 

File: flinkx-ftp/flinkx-ftp-core/src/main/java/com/dtstack/flinkx/ftp/FtpHandler.java
Patch:
@@ -126,6 +126,7 @@ public boolean isDirExist(String directoryPath) {
     public boolean isFileExist(String filePath) {
         boolean isExitFlag = false;
         try {
+            ftpClient.enterLocalPassiveMode();
             FTPFile[] ftpFiles = ftpClient.listFiles(new String(filePath.getBytes(StandardCharsets.UTF_8),FTP.DEFAULT_CONTROL_ENCODING));
             if (ftpFiles.length == 1 && ftpFiles[0].isFile()) {
                 isExitFlag = true;
@@ -147,6 +148,7 @@ public List<String> getFiles(String path) {
                 path = path + SP;
             }
             try {
+                ftpClient.enterLocalPassiveMode();
                 FTPFile[] ftpFiles = ftpClient.listFiles(new String(path.getBytes(StandardCharsets.UTF_8),FTP.DEFAULT_CONTROL_ENCODING));
                 if(ftpFiles != null) {
                     for(FTPFile ftpFile : ftpFiles) {
@@ -280,6 +282,7 @@ public void deleteAllFilesInDir(String dir, List<String> exclude) {
     @Override
     public InputStream getInputStream(String filePath) {
         try {
+            ftpClient.enterLocalPassiveMode();
             InputStream is = ftpClient.retrieveFileStream(new String(filePath.getBytes(StandardCharsets.UTF_8),FTP.DEFAULT_CONTROL_ENCODING));
             return is;
         } catch (IOException e) {

File: flinkx-rdb/flinkx-rdb-core/src/main/java/com/dtstack/flinkx/rdb/util/DbUtil.java
Patch:
@@ -18,6 +18,7 @@
 package com.dtstack.flinkx.rdb.util;
 
 import com.dtstack.flinkx.constants.ConstantValue;
+import com.dtstack.flinkx.rdb.DatabaseInterface;
 import com.dtstack.flinkx.rdb.ParameterValuesProvider;
 import com.dtstack.flinkx.reader.MetaColumn;
 import com.dtstack.flinkx.util.ClassUtil;

File: flinkx-sqlserver/flinkx-sqlserver-reader/src/main/java/com/dtstack/flinkx/sqlserver/reader/SqlserverQuerySqlBuilder.java
Patch:
@@ -19,6 +19,7 @@
 
 import com.dtstack.flinkx.rdb.datareader.JdbcDataReader;
 import com.dtstack.flinkx.rdb.datareader.QuerySqlBuilder;
+import com.dtstack.flinkx.rdb.util.DbUtil;
 import com.dtstack.flinkx.sqlserver.SqlServerConstants;
 import org.apache.commons.lang3.StringUtils;
 
@@ -43,7 +44,7 @@ public SqlserverQuerySqlBuilder(JdbcDataReader reader) {
 
     @Override
     protected String buildQuerySql() {
-        List<String> selectColumns = buildSelectColumns(databaseInterface, metaColumns);
+        List<String> selectColumns = DbUtil.buildSelectColumns(databaseInterface, metaColumns);
         boolean splitWithRowNum = addRowNumColumn(databaseInterface, selectColumns, isSplitByKey, splitKey);
 
         StringBuilder sb = new StringBuilder();

File: flinkx-phoenix5/flinkx-phoenix5-writer/src/main/java/com/dtstack/flinkx/phoenix5/format/Phoenix5OutputFormat.java
Patch:
@@ -71,7 +71,7 @@ protected void openInternal(int taskNumber, int numTasks){
             ClassUtil.forName(driverName, childFirstClassLoader);
             properties.setProperty("user", username);
             properties.setProperty("password", password);
-            dbConn = PhoenixUtil.getConnectionInternal(dbUrl, properties, childFirstClassLoader);
+            dbConn = PhoenixUtil.getHelper(childFirstClassLoader).getConn(dbUrl, properties);
 
             if (restoreConfig.isRestore()){
                 dbConn.setAutoCommit(false);

File: flinkx-hdfs/flinkx-hdfs-reader/src/main/java/com/dtstack/flinkx/hdfs/reader/HdfsOrcInputFormat.java
Patch:
@@ -102,6 +102,7 @@ private void openOrcReader(InputSplit inputSplit) throws IOException{
         numReadCounter = getRuntimeContext().getLongCounter("numRead");
         HdfsOrcInputSplit hdfsOrcInputSplit = (HdfsOrcInputSplit) inputSplit;
         OrcSplit orcSplit = hdfsOrcInputSplit.getOrcSplit();
+        findCurrentPartition(orcSplit.getPath());
         recordReader = inputFormat.getRecordReader(orcSplit, conf, Reporter.NULL);
         key = recordReader.createKey();
         value = recordReader.createValue();

File: flinkx-hdfs/flinkx-hdfs-reader/src/main/java/com/dtstack/flinkx/hdfs/reader/HdfsOrcInputFormat.java
Patch:
@@ -102,6 +102,7 @@ private void openOrcReader(InputSplit inputSplit) throws IOException{
         numReadCounter = getRuntimeContext().getLongCounter("numRead");
         HdfsOrcInputSplit hdfsOrcInputSplit = (HdfsOrcInputSplit) inputSplit;
         OrcSplit orcSplit = hdfsOrcInputSplit.getOrcSplit();
+        findCurrentPartition(orcSplit.getPath());
         recordReader = inputFormat.getRecordReader(orcSplit, conf, Reporter.NULL);
         key = recordReader.createKey();
         value = recordReader.createValue();

File: flinkx-kafka/flinkx-kafka-writer/src/main/java/com/dtstack/flinkx/kafka/writer/KafkaOutputFormat.java
Patch:
@@ -42,6 +42,7 @@ public class KafkaOutputFormat extends KafkaBaseOutputFormat {
 
     @Override
     public void configure(Configuration parameters) {
+        super.configure(parameters);
         props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
         props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
         props.put(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG, 86400000);

File: flinkx-kafka10/flinkx-kafka10-writer/src/main/java/com/dtstack/flinkx/kafka10/writer/Kafka10OutputFormat.java
Patch:
@@ -41,6 +41,7 @@ public class Kafka10OutputFormat extends KafkaBaseOutputFormat {
 
     @Override
     public void configure(Configuration parameters) {
+        super.configure(parameters);
         props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
         props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
         props.put(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG, 86400000);

File: flinkx-kafka11/flinkx-kafka11-writer/src/main/java/com/dtstack/flinkx/kafka11/writer/Kafka11OutputFormat.java
Patch:
@@ -41,6 +41,7 @@ public class Kafka11OutputFormat extends KafkaBaseOutputFormat {
 
     @Override
     public void configure(Configuration parameters) {
+        super.configure(parameters);
         props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
         props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
         props.put(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG, 86400000);

File: flinkx-kafka/flinkx-kafka-writer/src/main/java/com/dtstack/flinkx/kafka/writer/KafkaOutputFormat.java
Patch:
@@ -42,6 +42,7 @@ public class KafkaOutputFormat extends KafkaBaseOutputFormat {
 
     @Override
     public void configure(Configuration parameters) {
+        super.configure(parameters);
         props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
         props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
         props.put(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG, 86400000);

File: flinkx-kafka10/flinkx-kafka10-writer/src/main/java/com/dtstack/flinkx/kafka10/writer/Kafka10OutputFormat.java
Patch:
@@ -41,6 +41,7 @@ public class Kafka10OutputFormat extends KafkaBaseOutputFormat {
 
     @Override
     public void configure(Configuration parameters) {
+        super.configure(parameters);
         props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
         props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
         props.put(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG, 86400000);

File: flinkx-kafka11/flinkx-kafka11-writer/src/main/java/com/dtstack/flinkx/kafka11/writer/Kafka11OutputFormat.java
Patch:
@@ -41,6 +41,7 @@ public class Kafka11OutputFormat extends KafkaBaseOutputFormat {
 
     @Override
     public void configure(Configuration parameters) {
+        super.configure(parameters);
         props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
         props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
         props.put(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG, 86400000);

File: flinkx-kafka09/flinkx-kafka09-writer/src/main/java/com/dtstack/flinkx/kafka09/writer/Kafka09Writer.java
Patch:
@@ -58,6 +58,7 @@ public DataStreamSink<?> writeData(DataStream<Row> dataSet) {
         format.setBrokerList(brokerList);
         format.setProducerSettings(producerSettings);
         format.setRestoreConfig(restoreConfig);
+        format.setHeartBeatController(new HeartBeatController());
 
         return createOutput(dataSet, format);
     }

File: flinkx-kafka09/flinkx-kafka09-writer/src/main/java/com/dtstack/flinkx/kafka09/writer/Kafka09Writer.java
Patch:
@@ -58,6 +58,7 @@ public DataStreamSink<?> writeData(DataStream<Row> dataSet) {
         format.setBrokerList(brokerList);
         format.setProducerSettings(producerSettings);
         format.setRestoreConfig(restoreConfig);
+        format.setHeartBeatController(new HeartBeatController());
 
         return createOutput(dataSet, format);
     }

File: flinkx-binlog/flinkx-binlog-reader/src/main/java/com/dtstack/flinkx/binlog/reader/BinlogEventSink.java
Patch:
@@ -152,7 +152,8 @@ public Row takeEvent() throws IOException {
         Row row = null;
         try {
             Map<String, Object> map = queue.take();
-            if(map.size() == 1){
+            //@see com.dtstack.flinkx.binlog.reader.HeartBeatController.onFailed 检测到异常之后 会添加key为e的错误数据
+            if(map.size() == 1 && map.containsKey("e")){
                 throw new RuntimeException((String) map.get("e"));
             }else{
                 row = Row.of(map);

File: flinkx-kafka09/flinkx-kafka09-writer/src/main/java/com/dtstack/flinkx/kafka09/writer/Kafka09OutputFormat.java
Patch:
@@ -57,6 +57,7 @@ public void configure(Configuration parameters) {
             props.putAll(producerSettings);
         }
         props.put("metadata.broker.list", brokerList);
+        props.put("bootstrap.servers", brokerList);
         producer = new KafkaProducer<>(props);
     }
 

File: flinkx-metadata-oracle/flinkx-metadata-oracle-reader/src/main/java/com/dtstack/flinkx/metadataoracle/constants/OracleMetaDataCons.java
Patch:
@@ -73,5 +73,5 @@ public class OracleMetaDataCons extends MetaDataCons {
     /**
      * 使用ALL_TABLES，降低权限要求
      */
-    public static final String SQL_SHOW_TABLES = "SELECT TABLE_NAME FROM ALL_TABLES WHERE OWNER = %s";
+    public static final String SQL_SHOW_TABLES = "SELECT TABLE_NAME FROM ALL_TABLES WHERE OWNER = %s AND NESTED = 'NO' AND IOT_NAME IS NULL ";
 }
\ No newline at end of file

File: flinkx-metadata-oracle/flinkx-metadata-oracle-reader/src/main/java/com/dtstack/flinkx/metadataoracle/constants/OracleMetaDataCons.java
Patch:
@@ -73,5 +73,5 @@ public class OracleMetaDataCons extends MetaDataCons {
     /**
      * 使用ALL_TABLES，降低权限要求
      */
-    public static final String SQL_SHOW_TABLES = "SELECT TABLE_NAME FROM ALL_TABLES WHERE OWNER = %s";
+    public static final String SQL_SHOW_TABLES = "SELECT TABLE_NAME FROM ALL_TABLES WHERE OWNER = %s AND NESTED = 'NO' AND IOT_NAME IS NULL ";
 }
\ No newline at end of file

File: flinkx-metadata-tidb/flinkx-metadata-tidb-reader/src/main/java/com/dtstack/flinkx/metadatatidb/constants/TidbMetadataCons.java
Patch:
@@ -27,7 +27,6 @@ public class TidbMetadataCons extends MetaDataCons {
 
     public static final String DRIVER_NAME = "com.mysql.jdbc.Driver";
     public static final String KEY_PARTITION_COLUMN = "partitionColumn";
-    public static final String KEY_HEALTHY = "healthy";
     public static final String KEY_UPDATE_TIME = "updateTime";
     public static final String KEY_TES = "YES";
     public static final String KEY_PRI = "PRI";
@@ -44,7 +43,6 @@ public class TidbMetadataCons extends MetaDataCons {
     public static final String RESULT_PARTITION_TABLE_ROWS = "TABLE_ROWS";
     public static final String RESULT_PARTITION_DATA_LENGTH = "DATA_LENGTH";
     public static final String RESULT_PARTITIONNAME = "Partition_name";
-    public static final String RESULT_HEALTHY = "Healthy";
     public static final String RESULT_PARTITION_EXPRESSION = "PARTITION_EXPRESSION";
     public static final String RESULT_CREATE_TIME = "Create_time";
     public static final String RESULT_UPDATE_TIME = "Update_time";
@@ -54,7 +52,6 @@ public class TidbMetadataCons extends MetaDataCons {
     public static final String SQL_SHOW_TABLES = "SHOW FULL TABLES WHERE Table_type = 'BASE TABLE'";
     public static final String SQL_QUERY_TABLE_INFO = "SHOW TABLE STATUS LIKE '%s'";
     public static final String SQL_QUERY_COLUMN = "SHOW FULL COLUMNS FROM %s";
-    public static final String SQL_QUERY_HEALTHY = "SHOW STATS_HEALTHY WHERE Table_name='%s' AND Db_name = schema()";
     public static final String SQL_QUERY_UPDATE_TIME = "SHOW STATS_META WHERE Table_name = '%s'";
     public static final String SQL_QUERY_PARTITION = "SELECT * FROM information_schema.partitions WHERE table_schema = schema() AND table_name='%s'";
     public static final String SQL_QUERY_PARTITION_COLUMN = "SELECT DISTINCT PARTITION_EXPRESSION FROM information_schema.partitions WHERE table_schema = schema() AND table_name='%s'";

File: flinkx-metadata-tidb/flinkx-metadata-tidb-reader/src/main/java/com/dtstack/flinkx/metadatatidb/constants/TidbMetadataCons.java
Patch:
@@ -27,7 +27,6 @@ public class TidbMetadataCons extends MetaDataCons {
 
     public static final String DRIVER_NAME = "com.mysql.jdbc.Driver";
     public static final String KEY_PARTITION_COLUMN = "partitionColumn";
-    public static final String KEY_HEALTHY = "healthy";
     public static final String KEY_UPDATE_TIME = "updateTime";
     public static final String KEY_TES = "YES";
     public static final String KEY_PRI = "PRI";
@@ -44,7 +43,6 @@ public class TidbMetadataCons extends MetaDataCons {
     public static final String RESULT_PARTITION_TABLE_ROWS = "TABLE_ROWS";
     public static final String RESULT_PARTITION_DATA_LENGTH = "DATA_LENGTH";
     public static final String RESULT_PARTITIONNAME = "Partition_name";
-    public static final String RESULT_HEALTHY = "Healthy";
     public static final String RESULT_PARTITION_EXPRESSION = "PARTITION_EXPRESSION";
     public static final String RESULT_CREATE_TIME = "Create_time";
     public static final String RESULT_UPDATE_TIME = "Update_time";
@@ -54,7 +52,6 @@ public class TidbMetadataCons extends MetaDataCons {
     public static final String SQL_SHOW_TABLES = "SHOW FULL TABLES WHERE Table_type = 'BASE TABLE'";
     public static final String SQL_QUERY_TABLE_INFO = "SHOW TABLE STATUS LIKE '%s'";
     public static final String SQL_QUERY_COLUMN = "SHOW FULL COLUMNS FROM %s";
-    public static final String SQL_QUERY_HEALTHY = "SHOW STATS_HEALTHY WHERE Table_name='%s' AND Db_name = schema()";
     public static final String SQL_QUERY_UPDATE_TIME = "SHOW STATS_META WHERE Table_name = '%s'";
     public static final String SQL_QUERY_PARTITION = "SELECT * FROM information_schema.partitions WHERE table_schema = schema() AND table_name='%s'";
     public static final String SQL_QUERY_PARTITION_COLUMN = "SELECT DISTINCT PARTITION_EXPRESSION FROM information_schema.partitions WHERE table_schema = schema() AND table_name='%s'";

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/StringUtil.java
Patch:
@@ -303,6 +303,8 @@ public static List<String> splitIgnoreQuota(String str, char delimiter){
 
     /**
      * 调用{@linkplain com.dtstack.flinkx.util.StringUtil}的splitIgnoreQuota处理 并对返回结果按照.拼接
+     * @param table [dbo.schema1].[table]
+     * @return dbo.schema1.table
      */
     public static String splitIgnoreQuotaAndJoinByPoint(String table) {
         List<String> strings = StringUtil.splitIgnoreQuota(table, ConstantValue.POINT_SYMBOL.charAt(0));

File: flinkx-sqlservercdc/flinkx-sqlservercdc-reader/src/main/java/com/dtstack/flinkx/sqlservercdc/reader/SqlservercdcReader.java
Patch:
@@ -62,6 +62,7 @@ public SqlservercdcReader(DataTransferConfig config, StreamExecutionEnvironment
         List<String> tables = (List<String>) readerConfig.getParameter().getVal(SqlServerCdcConfigKeys.KEY_TABLE_LIST);
         if (CollectionUtils.isNotEmpty(tables)) {
             tableList = new ArrayList<>(tables.size());
+            //兼容[].[]
             tables.forEach(item -> tableList.add(StringUtil.splitIgnoreQuotaAndJoinByPoint(item)));
         } else {
             tableList = Collections.emptyList();

File: flinkx-launcher/src/main/java/com/dtstack/flinkx/launcher/perjob/PerJobSubmitter.java
Patch:
@@ -18,6 +18,7 @@
 
 package com.dtstack.flinkx.launcher.perjob;
 
+import com.dtstack.flinkx.launcher.KerberosInfo;
 import com.dtstack.flinkx.options.Options;
 import com.dtstack.flinkx.util.MapUtil;
 import org.apache.commons.lang.StringUtils;
@@ -54,6 +55,7 @@ public static String submit(Options options, JobGraph jobGraph) throws Exception
         ClusterSpecification clusterSpecification = FlinkPerJobResourceUtil.createClusterSpecification(conProp);
         PerJobClusterClientBuilder perJobClusterClientBuilder = new PerJobClusterClientBuilder();
         Configuration config = StringUtils.isEmpty(options.getFlinkconf()) ? new Configuration() : GlobalConfiguration.loadConfiguration(options.getFlinkconf());
+        perJobClusterClientBuilder.setKerberosInfo(new KerberosInfo(options.getKrb5conf(),options.getKeytab(),options.getPrincipal(),config));
         perJobClusterClientBuilder.init(options.getYarnconf(), config, conProp);
 
         AbstractYarnClusterDescriptor descriptor = perJobClusterClientBuilder.createPerJobClusterDescriptor(conProp, options, jobGraph);

File: flinkx-core/src/main/java/com/dtstack/flinkx/enums/EDatabaseType.java
Patch:
@@ -54,5 +54,6 @@ public enum EDatabaseType {
     polarDB,
     Phoenix,
     dm,
-    SapHana
+    SapHana,
+    KingBase
 }

File: flinkx-hive/flinkx-hive-core/src/main/java/com/dtstack/flinkx/hive/util/HiveDbUtil.java
Patch:
@@ -63,7 +63,7 @@ public final class HiveDbUtil {
     public static final String PARAM_DELIMITER = "&";
     public static final String KEY_PRINCIPAL = "principal";
 
-    public static Pattern HIVE_JDBC_PATTERN = Pattern.compile("(?i)jdbc:hive2://(?<host>[0-9a-zA-Z\\.]+):(?<port>\\d+)/(?<db>[0-9a-z_%]+)(?<param>[\\?;#].*)*");
+    public static Pattern HIVE_JDBC_PATTERN = Pattern.compile("(?i)jdbc:hive2://(?<host>[^:]+):(?<port>\\d+)/(?<db>[^;]+)(?<param>[\\?;#].*)*");
     public static final String HOST_KEY = "host";
     public static final String PORT_KEY = "port";
     public static final String DB_KEY = "db";
@@ -264,7 +264,8 @@ public static String parseIpAndPort(String url) {
         if (matcher.find()) {
             addr = matcher.group(HOST_KEY) + ":" + matcher.group(PORT_KEY);
         } else {
-            addr = url.substring(url.indexOf("//") + 2, url.lastIndexOf("/"));
+            addr = url.substring(url.indexOf("//") + 2);
+            addr=  addr.substring(0,addr.indexOf("/"));
         }
         return addr;
     }

File: flinkx-test/src/main/java/com/dtstack/flinkx/test/PluginNameConstrant.java
Patch:
@@ -58,6 +58,7 @@ public class PluginNameConstrant {
     public static final String METADATASQLSERVER_READER = "metadatasqlserverreader";
     public static final String GREENPLUM_READER = "greenplumreader";
     public static final String PHOENIX5_READER = "phoenix5reader";
+    public static final String KINGBASE_READER = "kingbasereader";
 
     public static final String STREAM_WRITER = "streamwriter";
     public static final String CARBONDATA_WRITER = "carbondatawriter";
@@ -88,4 +89,5 @@ public class PluginNameConstrant {
     public static final String DM_WRITER = "dmwriter";
     public static final String GREENPLUM_WRITER = "greenplumwriter";
     public static final String PHOENIX5_WRITER = "phoenix5writer";
+    public static final String KINGBASE_WRITER = "kingbasewriter";
 }

File: flinkx-hive/flinkx-hive-core/src/main/java/com/dtstack/flinkx/hive/util/HiveDbUtil.java
Patch:
@@ -63,7 +63,7 @@ public final class HiveDbUtil {
     public static final String PARAM_DELIMITER = "&";
     public static final String KEY_PRINCIPAL = "principal";
 
-    public static Pattern HIVE_JDBC_PATTERN = Pattern.compile("(?i)jdbc:hive2://(?<host>[0-9a-zA-Z\\.]+):(?<port>\\d+)/(?<db>[0-9a-z_%]+)(?<param>[\\?;#].*)*");
+    public static Pattern HIVE_JDBC_PATTERN = Pattern.compile("(?i)jdbc:hive2://(?<host>[^:]+):(?<port>\\d+)/(?<db>[^;]+)(?<param>[\\?;#].*)*");
     public static final String HOST_KEY = "host";
     public static final String PORT_KEY = "port";
     public static final String DB_KEY = "db";
@@ -264,7 +264,8 @@ public static String parseIpAndPort(String url) {
         if (matcher.find()) {
             addr = matcher.group(HOST_KEY) + ":" + matcher.group(PORT_KEY);
         } else {
-            addr = url.substring(url.indexOf("//") + 2, url.lastIndexOf("/"));
+            addr = url.substring(url.indexOf("//") + 2);
+            addr=  addr.substring(0,addr.indexOf("/"));
         }
         return addr;
     }

File: flinkx-hive/flinkx-hive-core/src/main/java/com/dtstack/flinkx/hive/util/HiveDbUtil.java
Patch:
@@ -63,7 +63,7 @@ public final class HiveDbUtil {
     public static final String PARAM_DELIMITER = "&";
     public static final String KEY_PRINCIPAL = "principal";
 
-    public static Pattern HIVE_JDBC_PATTERN = Pattern.compile("(?i)jdbc:hive2://(?<host>[0-9a-zA-Z\\.]+):(?<port>\\d+)/(?<db>[0-9a-z_%]+)(?<param>[\\?;#].*)*");
+    public static Pattern HIVE_JDBC_PATTERN = Pattern.compile("(?i)jdbc:hive2://(?<host>[^:]+):(?<port>\\d+)/(?<db>[^;]+)(?<param>[\\?;#].*)*");
     public static final String HOST_KEY = "host";
     public static final String PORT_KEY = "port";
     public static final String DB_KEY = "db";
@@ -264,7 +264,8 @@ public static String parseIpAndPort(String url) {
         if (matcher.find()) {
             addr = matcher.group(HOST_KEY) + ":" + matcher.group(PORT_KEY);
         } else {
-            addr = url.substring(url.indexOf("//") + 2, url.lastIndexOf("/"));
+            addr = url.substring(url.indexOf("//") + 2);
+            addr=  addr.substring(0,addr.indexOf("/"));
         }
         return addr;
     }

File: flinkx-hive/flinkx-hive-core/src/main/java/com/dtstack/flinkx/hive/util/HiveDbUtil.java
Patch:
@@ -63,7 +63,7 @@ public final class HiveDbUtil {
     public static final String PARAM_DELIMITER = "&";
     public static final String KEY_PRINCIPAL = "principal";
 
-    public static Pattern HIVE_JDBC_PATTERN = Pattern.compile("(?i)jdbc:hive2://(?<host>[0-9a-zA-Z\\.]+):(?<port>\\d+)/(?<db>[0-9a-z_%]+)(?<param>[\\?;#].*)*");
+    public static Pattern HIVE_JDBC_PATTERN = Pattern.compile("(?i)jdbc:hive2://(?<host>[^:]+):(?<port>\\d+)/(?<db>[^;]+)(?<param>[\\?;#].*)*");
     public static final String HOST_KEY = "host";
     public static final String PORT_KEY = "port";
     public static final String DB_KEY = "db";
@@ -264,7 +264,8 @@ public static String parseIpAndPort(String url) {
         if (matcher.find()) {
             addr = matcher.group(HOST_KEY) + ":" + matcher.group(PORT_KEY);
         } else {
-            addr = url.substring(url.indexOf("//") + 2, url.lastIndexOf("/"));
+            addr = url.substring(url.indexOf("//") + 2);
+            addr=  addr.substring(0,addr.indexOf("/"));
         }
         return addr;
     }

File: flinkx-hive/flinkx-hive-core/src/main/java/com/dtstack/flinkx/hive/util/HiveDbUtil.java
Patch:
@@ -63,7 +63,7 @@ public final class HiveDbUtil {
     public static final String PARAM_DELIMITER = "&";
     public static final String KEY_PRINCIPAL = "principal";
 
-    public static Pattern HIVE_JDBC_PATTERN = Pattern.compile("(?i)jdbc:hive2://(?<host>[0-9a-zA-Z\\.]+):(?<port>\\d+)/(?<db>[0-9a-z_%]+)(?<param>[\\?;#].*)*");
+    public static Pattern HIVE_JDBC_PATTERN = Pattern.compile("(?i)jdbc:hive2://(?<host>[^:]+):(?<port>\\d+)/(?<db>[^;]+)(?<param>[\\?;#].*)*");
     public static final String HOST_KEY = "host";
     public static final String PORT_KEY = "port";
     public static final String DB_KEY = "db";
@@ -264,7 +264,8 @@ public static String parseIpAndPort(String url) {
         if (matcher.find()) {
             addr = matcher.group(HOST_KEY) + ":" + matcher.group(PORT_KEY);
         } else {
-            addr = url.substring(url.indexOf("//") + 2, url.lastIndexOf("/"));
+            addr = url.substring(url.indexOf("//") + 2);
+            addr=  addr.substring(0,addr.indexOf("/"));
         }
         return addr;
     }

File: flinkx-kudu/flinkx-kudu-writer/src/main/java/com/dtstack/flinkx/kudu/writer/KuduOutputFormat.java
Patch:
@@ -40,6 +40,7 @@
 
 import java.io.IOException;
 import java.nio.ByteBuffer;
+import java.sql.Date;
 import java.sql.Timestamp;
 import java.util.List;
 
@@ -132,7 +133,7 @@ private void writeData(Row row) throws WriteRecordException {
                         case INT32: partialRow.addInt(columnIndex, ValueUtil.getIntegerVal(val)); break;
                         case INT64: partialRow.addLong(columnIndex, ValueUtil.getLongVal(val)); break;
                         case UNIXTIME_MICROS:
-                            if (val instanceof Timestamp) {
+                            if (val instanceof Timestamp || val instanceof Date) {
                                 partialRow.addTimestamp(columnIndex, ValueUtil.getTimestampVal(val));
                             } else {
                                 partialRow.addLong(columnIndex, ValueUtil.getLongVal(val));

File: flinkx-kudu/flinkx-kudu-writer/src/main/java/com/dtstack/flinkx/kudu/writer/KuduOutputFormat.java
Patch:
@@ -40,6 +40,7 @@
 
 import java.io.IOException;
 import java.nio.ByteBuffer;
+import java.sql.Date;
 import java.sql.Timestamp;
 import java.util.List;
 
@@ -132,7 +133,7 @@ private void writeData(Row row) throws WriteRecordException {
                         case INT32: partialRow.addInt(columnIndex, ValueUtil.getIntegerVal(val)); break;
                         case INT64: partialRow.addLong(columnIndex, ValueUtil.getLongVal(val)); break;
                         case UNIXTIME_MICROS:
-                            if (val instanceof Timestamp) {
+                            if (val instanceof Timestamp || val instanceof Date) {
                                 partialRow.addTimestamp(columnIndex, ValueUtil.getTimestampVal(val));
                             } else {
                                 partialRow.addLong(columnIndex, ValueUtil.getLongVal(val));

File: flinkx-kudu/flinkx-kudu-writer/src/main/java/com/dtstack/flinkx/kudu/writer/KuduOutputFormat.java
Patch:
@@ -40,6 +40,7 @@
 
 import java.io.IOException;
 import java.nio.ByteBuffer;
+import java.sql.Date;
 import java.sql.Timestamp;
 import java.util.List;
 
@@ -132,7 +133,7 @@ private void writeData(Row row) throws WriteRecordException {
                         case INT32: partialRow.addInt(columnIndex, ValueUtil.getIntegerVal(val)); break;
                         case INT64: partialRow.addLong(columnIndex, ValueUtil.getLongVal(val)); break;
                         case UNIXTIME_MICROS:
-                            if (val instanceof Timestamp) {
+                            if (val instanceof Timestamp || val instanceof Date) {
                                 partialRow.addTimestamp(columnIndex, ValueUtil.getTimestampVal(val));
                             } else {
                                 partialRow.addLong(columnIndex, ValueUtil.getLongVal(val));

File: flinkx-kudu/flinkx-kudu-writer/src/main/java/com/dtstack/flinkx/kudu/writer/KuduOutputFormat.java
Patch:
@@ -40,6 +40,7 @@
 
 import java.io.IOException;
 import java.nio.ByteBuffer;
+import java.sql.Date;
 import java.sql.Timestamp;
 import java.util.List;
 
@@ -132,7 +133,7 @@ private void writeData(Row row) throws WriteRecordException {
                         case INT32: partialRow.addInt(columnIndex, ValueUtil.getIntegerVal(val)); break;
                         case INT64: partialRow.addLong(columnIndex, ValueUtil.getLongVal(val)); break;
                         case UNIXTIME_MICROS:
-                            if (val instanceof Timestamp) {
+                            if (val instanceof Timestamp || val instanceof Date) {
                                 partialRow.addTimestamp(columnIndex, ValueUtil.getTimestampVal(val));
                             } else {
                                 partialRow.addLong(columnIndex, ValueUtil.getLongVal(val));

File: flinkx-metadata/flinkx-metadata-reader/src/main/java/com/dtstack/flinkx/metadata/inputformat/BaseMetadataInputFormat.java
Patch:
@@ -82,8 +82,9 @@ protected void openInternal(InputSplit inputSplit) throws IOException {
             tableIterator.set(tableList.iterator());
             init();
         } catch (SQLException | ClassNotFoundException e) {
-            LOG.error("获取table列表异常, dbUrl = {}, username = {}, inputSplit = {}, e = {}", dbUrl, username, inputSplit, ExceptionUtil.getErrorMessage(e));
-            throw new IOException("获取table列表异常", e);
+            String message = String.format("获取table列表异常, dbUrl = %s, username = %s, inputSplit = %s, e = %s", dbUrl, username, inputSplit, ExceptionUtil.getErrorMessage(e));
+            LOG.error(message);
+            throw new IOException(message, e);
         }
     }
 

File: flinkx-metadata/flinkx-metadata-reader/src/main/java/com/dtstack/flinkx/metadata/inputformat/BaseMetadataInputFormat.java
Patch:
@@ -82,8 +82,9 @@ protected void openInternal(InputSplit inputSplit) throws IOException {
             tableIterator.set(tableList.iterator());
             init();
         } catch (SQLException | ClassNotFoundException e) {
-            LOG.error("获取table列表异常, dbUrl = {}, username = {}, inputSplit = {}, e = {}", dbUrl, username, inputSplit, ExceptionUtil.getErrorMessage(e));
-            throw new IOException("获取table列表异常", e);
+            String message = String.format("获取table列表异常, dbUrl = %s, username = %s, inputSplit = %s, e = %s", dbUrl, username, inputSplit, ExceptionUtil.getErrorMessage(e));
+            LOG.error(message);
+            throw new IOException(message, e);
         }
     }
 

File: flinkx-hive/flinkx-hive-core/src/main/java/com/dtstack/flinkx/hive/util/HiveDbUtil.java
Patch:
@@ -63,7 +63,7 @@ public final class HiveDbUtil {
     public static final String PARAM_DELIMITER = "&";
     public static final String KEY_PRINCIPAL = "principal";
 
-    public static Pattern HIVE_JDBC_PATTERN = Pattern.compile("(?i)jdbc:hive2://(?<host>[0-9a-zA-Z\\.]+):(?<port>\\d+)/(?<db>[0-9a-z_%]+)(?<param>[\\?;#].*)*");
+    public static Pattern HIVE_JDBC_PATTERN = Pattern.compile("(?i)jdbc:hive2://(?<host>[^:]+):(?<port>\\d+)/(?<db>[^;]+)(?<param>[\\?;#].*)*");
     public static final String HOST_KEY = "host";
     public static final String PORT_KEY = "port";
     public static final String DB_KEY = "db";
@@ -264,7 +264,8 @@ public static String parseIpAndPort(String url) {
         if (matcher.find()) {
             addr = matcher.group(HOST_KEY) + ":" + matcher.group(PORT_KEY);
         } else {
-            addr = url.substring(url.indexOf("//") + 2, url.lastIndexOf("/"));
+            addr = url.substring(url.indexOf("//") + 2);
+            addr=  addr.substring(0,addr.indexOf("/"));
         }
         return addr;
     }

File: flinkx-redis/flinkx-redis-writer/src/main/java/com/dtstack/flinkx/redis/writer/RedisOutputFormat.java
Patch:
@@ -84,7 +84,9 @@ public void configure(Configuration parameters) {
 
         Properties properties = new Properties();
         properties.put(KEY_HOST_PORT,hostPort);
-        properties.put(KEY_PASSWORD,password);
+        if(StringUtils.isNotBlank(password)){
+            properties.put(KEY_PASSWORD,password);
+        }
         properties.put(KEY_TIMEOUT,timeout);
         properties.put(KEY_DB,database);
 

File: flinkx-metadata-mysql/flinkx-metadata-mysql-reader/src/main/java/com/dtstack/flinkx/metadatamysql/inputformat/MetadatamysqlInputFormat.java
Patch:
@@ -84,9 +84,9 @@ protected List<Map<String, String>> queryIndex(String tableName) throws SQLExcep
             while (rs.next()) {
                 Map<String, String> perIndex = new HashMap<>(16);
                 perIndex.put(KEY_COLUMN_NAME, rs.getString(RESULT_KEY_NAME));
-                perIndex.put(KEY_KEY_NAME, rs.getString(RESULT_COLUMN_NAME));
+                perIndex.put(KEY_INDEX_NAME, rs.getString(RESULT_COLUMN_NAME));
                 perIndex.put(KEY_COLUMN_TYPE, rs.getString(RESULT_INDEX_TYPE));
-                perIndex.put(KEY_COLUMN_COMMENT, rs.getString(RESULT_INDEX_COMMENT));
+                perIndex.put(KEY_INDEX_COMMENT, rs.getString(RESULT_INDEX_COMMENT));
                 index.add(perIndex);
             }
         } catch (SQLException e) {

File: flinkx-metadata-oracle/flinkx-metadata-oracle-reader/src/main/java/com/dtstack/flinkx/metadataoracle/inputformat/MetadataoracleInputFormat.java
Patch:
@@ -35,6 +35,7 @@
 import static com.dtstack.flinkx.metadata.MetaDataCons.KEY_COLUMN_PRIMARY;
 import static com.dtstack.flinkx.metadata.MetaDataCons.KEY_COLUMN_SCALE;
 import static com.dtstack.flinkx.metadata.MetaDataCons.KEY_FALSE;
+import static com.dtstack.flinkx.metadata.MetaDataCons.KEY_INDEX_NAME;
 import static com.dtstack.flinkx.metadata.MetaDataCons.KEY_TRUE;
 import static com.dtstack.flinkx.metadata.MetaDataCons.MAX_TABLE_SIZE;
 import static com.dtstack.flinkx.metadataoracle.constants.OracleMetaDataCons.KEY_COLUMN;
@@ -153,8 +154,8 @@ protected Map<String, List<Map<String, String>>> queryIndexList() throws SQLExce
         try (ResultSet rs = statement.get().executeQuery(sql)) {
             while (rs.next()) {
                 Map<String, String> column = new HashMap<>(16);
-                column.put(KEY_COLUMN_NAME, rs.getString(1));
-                column.put(KEY_INDEX_COLUMN_NAME, rs.getString(2));
+                column.put(KEY_INDEX_NAME, rs.getString(1));
+                column.put(KEY_COLUMN_NAME, rs.getString(2));
                 column.put(KEY_COLUMN_TYPE, rs.getString(3));
                 String tableName = rs.getString(4);
                 if(indexListMap.containsKey(tableName)){

File: flinkx-metadata-sqlserver/flinkx-metadata-sqlserver-reader/src/main/java/com/dtstack/flinkx/metadatasqlserver/inputformat/MetadatasqlserverInputFormat.java
Patch:
@@ -40,6 +40,7 @@
 import static com.dtstack.flinkx.metadata.MetaDataCons.KEY_COLUMN_NAME;
 import static com.dtstack.flinkx.metadata.MetaDataCons.KEY_COLUMN_PRIMARY;
 import static com.dtstack.flinkx.metadata.MetaDataCons.KEY_FALSE;
+import static com.dtstack.flinkx.metadata.MetaDataCons.KEY_INDEX_NAME;
 import static com.dtstack.flinkx.metadata.MetaDataCons.KEY_TRUE;
 import static com.dtstack.flinkx.metadatasqlserver.constants.SqlServerMetadataCons.KEY_SCHEMA_NAME;
 import static com.dtstack.flinkx.metadatasqlserver.constants.SqlServerMetadataCons.KEY_TABLE_NAME;
@@ -154,7 +155,7 @@ protected List<Map<String, String>> queryIndex() throws SQLException{
         try(ResultSet resultSet = statement.get().executeQuery(sql)){
             while (resultSet.next()){
                 Map<String, String> perIndex = new HashMap<>(16);
-                perIndex.put(KEY_COLUMN_NAME, resultSet.getString(1));
+                perIndex.put(KEY_INDEX_NAME, resultSet.getString(1));
                 perIndex.put(MetaDataCons.KEY_COLUMN_NAME,  resultSet.getString(2));
                 perIndex.put(MetaDataCons.KEY_COLUMN_TYPE, resultSet.getString(3));
                 index.add(perIndex);

File: flinkx-redis/flinkx-redis-writer/src/main/java/com/dtstack/flinkx/redis/writer/RedisOutputFormat.java
Patch:
@@ -84,7 +84,7 @@ public void configure(Configuration parameters) {
 
         Properties properties = new Properties();
         properties.put(KEY_HOST_PORT,hostPort);
-        if(StringUtils.isBlank(password)){
+        if(StringUtils.isNotBlank(password)){
             properties.put(KEY_PASSWORD,password);
         }
         properties.put(KEY_TIMEOUT,timeout);

File: flinkx-redis/flinkx-redis-writer/src/main/java/com/dtstack/flinkx/redis/writer/RedisOutputFormat.java
Patch:
@@ -84,7 +84,7 @@ public void configure(Configuration parameters) {
 
         Properties properties = new Properties();
         properties.put(KEY_HOST_PORT,hostPort);
-        if(password!=null){
+        if(StringUtils.isBlank(password)){
             properties.put(KEY_PASSWORD,password);
         }
         properties.put(KEY_TIMEOUT,timeout);

File: flinkx-redis/flinkx-redis-writer/src/main/java/com/dtstack/flinkx/redis/writer/RedisOutputFormat.java
Patch:
@@ -84,7 +84,9 @@ public void configure(Configuration parameters) {
 
         Properties properties = new Properties();
         properties.put(KEY_HOST_PORT,hostPort);
-        properties.put(KEY_PASSWORD,password);
+        if(password!=null){
+            properties.put(KEY_PASSWORD,password);
+        }
         properties.put(KEY_TIMEOUT,timeout);
         properties.put(KEY_DB,database);
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/FileSystemUtil.java
Patch:
@@ -107,6 +107,7 @@ public static UserGroupInformation getUGI(Map<String, Object> hadoopConfig, Stri
         keytabFileName = KerberosUtil.loadFile(hadoopConfig, keytabFileName);
         String principal = KerberosUtil.getPrincipal(hadoopConfig, keytabFileName);
         KerberosUtil.loadKrb5Conf(hadoopConfig);
+        KerberosUtil.refreshConfig();
 
         UserGroupInformation ugi = KerberosUtil.loginAndReturnUgi(getConfiguration(hadoopConfig, defaultFs), principal, keytabFileName);
         UserGroupInformation.setLoginUser(ugi);

File: flinkx-hbase/flinkx-hbase-core/src/main/java/com/dtstack/flinkx/hbase/HbaseHelper.java
Patch:
@@ -98,6 +98,7 @@ public static UserGroupInformation getUgi(Map<String,Object> hbaseConfigMap) thr
         keytabFileName = KerberosUtil.loadFile(hbaseConfigMap, keytabFileName);
         String principal = KerberosUtil.getPrincipal(hbaseConfigMap, keytabFileName);
         KerberosUtil.loadKrb5Conf(hbaseConfigMap);
+        KerberosUtil.refreshConfig();
 
         Configuration conf = FileSystemUtil.getConfiguration(hbaseConfigMap, null);
 

File: flinkx-hive/flinkx-hive-core/src/main/java/com/dtstack/flinkx/hive/util/HiveDbUtil.java
Patch:
@@ -108,6 +108,7 @@ private static Connection getConnectionWithKerberos(ConnectionInfo connectionInf
         keytabFileName = KerberosUtil.loadFile(connectionInfo.getHiveConf(), keytabFileName);
         String principal = KerberosUtil.getPrincipal(connectionInfo.getHiveConf(), keytabFileName);
         KerberosUtil.loadKrb5Conf(connectionInfo.getHiveConf());
+        KerberosUtil.refreshConfig();
 
         Configuration conf = FileSystemUtil.getConfiguration(connectionInfo.getHiveConf(), null);
 

File: flinkx-kb/flinkx-kb-writer/src/main/java/com/dtstack/flinkx/kafkabase/writer/KafkaBaseOutputFormat.java
Patch:
@@ -94,8 +94,8 @@ protected void writeSingleRecordInternal(Row row) throws WriteRecordException {
 
         } catch (Throwable e) {
             LOG.error("kafka writeSingleRecordInternal error:{}", ExceptionUtil.getErrorMessage(e));
-            //连续发送3次数据错误，就直接认为数据源异常，退出任务 或者出现broker连接不上直接抛出异常
-            if(++failedTimes >= 3 || e.getMessage().contains("Broker may not be available")){
+            //连续发送3次数据错误 或者出现broker 连接异常，就直接认为数据源异常，退出任务 或者出现broker连接不上，超时直接抛出异常
+            if(++failedTimes >= 3 || e.getMessage().contains("Broker may not be available")||e.getMessage().contains("TimeoutException")){
                 throw new RuntimeException(e.getMessage(), e);
             }
             throw new WriteRecordException(e.getMessage(), e);

File: flinkx-kudu/flinkx-kudu-writer/src/main/java/com/dtstack/flinkx/kudu/writer/KuduOutputFormat.java
Patch:
@@ -40,6 +40,7 @@
 
 import java.io.IOException;
 import java.nio.ByteBuffer;
+import java.sql.Date;
 import java.sql.Timestamp;
 import java.util.List;
 
@@ -132,7 +133,7 @@ private void writeData(Row row) throws WriteRecordException {
                         case INT32: partialRow.addInt(columnIndex, ValueUtil.getIntegerVal(val)); break;
                         case INT64: partialRow.addLong(columnIndex, ValueUtil.getLongVal(val)); break;
                         case UNIXTIME_MICROS:
-                            if (val instanceof Timestamp) {
+                            if (val instanceof Timestamp || val instanceof Date) {
                                 partialRow.addTimestamp(columnIndex, ValueUtil.getTimestampVal(val));
                             } else {
                                 partialRow.addLong(columnIndex, ValueUtil.getLongVal(val));

File: flinkx-metadata-sqlserver/flinkx-metadata-sqlserver-reader/src/main/java/com/dtstack/flinkx/metadatasqlserver/constants/SqlServerMetadataCons.java
Patch:
@@ -52,10 +52,11 @@ public class SqlServerMetadataCons extends MetaDataCons {
             "LEFT JOIN sys.extended_properties AS ep ON a.id = ep.major_id AND ep.minor_id = 0 \n" +
             "WHERE (a.type = 'u') AND (b.indid IN (0, 1)) and a.name = %s AND OBJECT_SCHEMA_NAME(a.id, DB_ID()) = %s ";
 
-    public static final String SQL_SHOW_TABLE_COLUMN = "SELECT B.name AS name, TY.name as type, C.value AS comment, B.is_nullable as nullable, COLUMNPROPERTY(B.object_id ,B.name,'PRECISION') as presice \n" +
+    public static final String SQL_SHOW_TABLE_COLUMN = "SELECT B.name AS name, TY.name as type, C.value AS comment, B.is_nullable as nullable, COLUMNPROPERTY(B.object_id ,B.name,'PRECISION') as presice, D.text \n" +
             "FROM sys.tables A INNER JOIN sys.columns B ON B.object_id = A.object_id \n" +
             "INNER JOIN sys.types TY ON B.system_type_id = TY.system_type_id \n" +
             "LEFT JOIN sys.extended_properties C ON C.major_id = B.object_id AND C.minor_id = B.column_id \n" +
+            "left join syscomments D on A.object_id=D.id " +
             "WHERE A.name = %s and OBJECT_SCHEMA_NAME(A.object_id, DB_ID())=%s";
 
     public static final String SQL_SHOW_TABLE_INDEX = "SELECT a.name, d.name as columnName, type_desc as type \n" +

File: flinkx-metadata-sqlserver/flinkx-metadata-sqlserver-reader/src/main/java/com/dtstack/flinkx/metadatasqlserver/inputformat/MetadatasqlserverInputFormat.java
Patch:
@@ -182,6 +182,7 @@ protected List<Map<String, Object>> queryColumn() throws SQLException {
                 perColumn.put(MetaDataCons.KEY_COLUMN_COMMENT, resultSet.getString(3));
                 perColumn.put(MetaDataCons.KEY_COLUMN_NULL, StringUtils.equals(resultSet.getString(4), KEY_ZERO) ? KEY_FALSE : KEY_TRUE);
                 perColumn.put(MetaDataCons.KEY_COLUMN_SCALE, resultSet.getString(5));
+                perColumn.put(MetaDataCons.KEY_COLUMN_DEFAULT, resultSet.getString(6));
                 perColumn.put(MetaDataCons.KEY_COLUMN_INDEX, column.size()+1);
                 column.add(perColumn);
             }

File: flinkx-metadata/flinkx-metadata-reader/src/main/java/com/dtstack/flinkx/metadata/inputformat/BaseMetadataInputFormat.java
Patch:
@@ -82,8 +82,9 @@ protected void openInternal(InputSplit inputSplit) throws IOException {
             tableIterator.set(tableList.iterator());
             init();
         } catch (SQLException | ClassNotFoundException e) {
-            LOG.error("获取table列表异常, dbUrl = {}, username = {}, inputSplit = {}, e = {}", dbUrl, username, inputSplit, ExceptionUtil.getErrorMessage(e));
-            throw new IOException("获取table列表异常", e);
+            String message = String.format("获取table列表异常, dbUrl = %s, username = %s, inputSplit = %s, e = %s", dbUrl, username, inputSplit, ExceptionUtil.getErrorMessage(e));
+            LOG.error(message);
+            throw new IOException(message, e);
         }
     }
 

File: flinkx-phoenix/flinkx-phoenix-reader/src/main/java/com/dtstack/flinkx/phoenix/format/PhoenixInputFormat.java
Patch:
@@ -68,7 +68,7 @@ public void openInternal(InputSplit inputSplit) throws IOException {
 
             statement = dbConn.createStatement(resultSetType, resultSetConcurrency);
 
-            statement.setFetchSize(0);
+            statement.setFetchSize(fetchSize);
 
             statement.setQueryTimeout(queryTimeOut);
             String querySql = buildQuerySql(inputSplit);

File: flinkx-phoenix5/flinkx-phoenix5-reader/src/main/java/com/dtstack/flinkx/phoenix5/format/Phoenix5InputFormat.java
Patch:
@@ -96,7 +96,7 @@ public void openInternal(InputSplit inputSplit) throws IOException {
 
             statement = dbConn.createStatement(resultSetType, resultSetConcurrency);
 
-            statement.setFetchSize(0);
+            statement.setFetchSize(fetchSize);
 
             statement.setQueryTimeout(queryTimeOut);
             String querySql = buildQuerySql(inputSplit);

File: flinkx-kudu/flinkx-kudu-core/src/main/java/com/dtstack/flinkx/kudu/core/KuduUtil.java
Patch:
@@ -157,6 +157,7 @@ public static Type getType(String columnType){
             case "varchar":
             case "text":
             case "string" : return  Type.STRING;
+            case "unixtime_micros":
             case "timestamp" : return  Type.UNIXTIME_MICROS;
             default:
                 throw new IllegalArgumentException("Not support column type:" + columnType);

File: flinkx-kudu/flinkx-kudu-core/src/main/java/com/dtstack/flinkx/kudu/core/KuduUtil.java
Patch:
@@ -157,6 +157,7 @@ public static Type getType(String columnType){
             case "varchar":
             case "text":
             case "string" : return  Type.STRING;
+            case "unixtime_micros":
             case "timestamp" : return  Type.UNIXTIME_MICROS;
             default:
                 throw new IllegalArgumentException("Not support column type:" + columnType);

File: flinkx-kingbase/flinkx-kingbase-writer/src/main/java/com/dtstack/flinkx/kingbase/format/KingbaseOutputFormat.java
Patch:
@@ -31,8 +31,7 @@
 import java.sql.SQLException;
 
 /**
- *
- *
+ * 特殊类型转换
  * Company: www.dtstack.com
  * @author kunni@dtstack.com
  */

File: flinkx-phoenix/flinkx-phoenix-reader/src/main/java/com/dtstack/flinkx/phoenix/format/PhoenixInputFormat.java
Patch:
@@ -96,7 +96,7 @@ public void openInternal(InputSplit inputSplit) throws IOException {
 
             statement = dbConn.createStatement(resultSetType, resultSetConcurrency);
 
-            statement.setFetchSize(databaseInterface.getFetchSize());
+            statement.setFetchSize(fetchSize);
 
             statement.setQueryTimeout(queryTimeOut);
             String querySql = buildQuerySql(inputSplit);

File: flinkx-phoenix5/flinkx-phoenix5-reader/src/main/java/com/dtstack/flinkx/phoenix5/format/Phoenix5InputFormat.java
Patch:
@@ -96,7 +96,7 @@ public void openInternal(InputSplit inputSplit) throws IOException {
 
             statement = dbConn.createStatement(resultSetType, resultSetConcurrency);
 
-            statement.setFetchSize(databaseInterface.getFetchSize());
+            statement.setFetchSize(fetchSize);
 
             statement.setQueryTimeout(queryTimeOut);
             String querySql = buildQuerySql(inputSplit);

File: flinkx-core/src/main/java/com/dtstack/flinkx/latch/BaseLatch.java
Patch:
@@ -28,7 +28,7 @@
  */
 public abstract class BaseLatch {
 
-    protected static int MAX_RETRY_TIMES = 100;
+    protected static int MAX_RETRY_TIMES = 30;
 
     /**
      * 从Flink REST API获取累加器里的值
@@ -52,7 +52,7 @@ public void waitUntil(int val) {
     }
 
     protected void sleep() {
-        SysUtil.sleep(300);
+        SysUtil.sleep(1000);
     }
 
     /**

File: flinkx-phoenix/flinkx-phoenix-reader/src/main/java/com/dtstack/flinkx/phoenix/format/PhoenixInputFormat.java
Patch:
@@ -68,7 +68,7 @@ public void openInternal(InputSplit inputSplit) throws IOException {
 
             statement = dbConn.createStatement(resultSetType, resultSetConcurrency);
 
-            statement.setFetchSize(0);
+            statement.setFetchSize(databaseInterface.getFetchSize());
 
             statement.setQueryTimeout(queryTimeOut);
             String querySql = buildQuerySql(inputSplit);

File: flinkx-phoenix5/flinkx-phoenix5-reader/src/main/java/com/dtstack/flinkx/phoenix5/format/Phoenix5InputFormat.java
Patch:
@@ -96,7 +96,7 @@ public void openInternal(InputSplit inputSplit) throws IOException {
 
             statement = dbConn.createStatement(resultSetType, resultSetConcurrency);
 
-            statement.setFetchSize(0);
+            statement.setFetchSize(databaseInterface.getFetchSize());
 
             statement.setQueryTimeout(queryTimeOut);
             String querySql = buildQuerySql(inputSplit);

File: flinkx-core/src/main/java/com/dtstack/flinkx/inputformat/BaseRichInputFormat.java
Patch:
@@ -188,7 +188,7 @@ private void showConfig(){
                 writerParameter.put(KEY_PASSWORD, KEY_CONFUSED_PASSWORD);
             }
         }
-    LOG.info("configInfo : {}", GsonUtil.GSON.toJson(map));
+        LOG.info("configInfo : \n{}", GsonUtil.GSON.toJson(map));
     }
 
     private void checkIfCreateSplitFailed(InputSplit inputSplit){

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/GsonUtil.java
Patch:
@@ -50,7 +50,9 @@ public class GsonUtil {
 
     @SuppressWarnings("unchecked")
     private static Gson getGson() {
-        GSON = new GsonBuilder().create();
+        GSON = new GsonBuilder()
+                .setPrettyPrinting()
+                .create();
         try {
             Field factories = Gson.class.getDeclaredField("factories");
             factories.setAccessible(true);

File: flinkx-kingbase/flinkx-kingbase-core/src/main/java/com/dtstack/flinkx/kingbase/KingBaseDatabaseMeta.java
Patch:
@@ -65,12 +65,12 @@ public String quoteValue(String value, String column) {
 
     @Override
     public String getSplitFilter(String columnName) {
-        return String.format("%s mod ${N} = ${M}", getStartQuote() + columnName + getEndQuote());
+        return String.format("mod(%s, ${N}) = ${M}", getStartQuote() + columnName + getEndQuote());
     }
 
     @Override
     public String getSplitFilterWithTmpTable(String tmpTable, String columnName) {
-        return String.format("%s.%s mod ${N} = ${M}", tmpTable, getStartQuote() + columnName + getEndQuote());
+        return String.format("mod(%s.%s, ${N}) = ${M}", tmpTable, getStartQuote() + columnName + getEndQuote());
     }
 
     @Override

File: flinkx-kingbase/flinkx-kingbase-reader/src/main/java/com/dtstack/flinkx/kingbase/format/KingbaseInputFormat.java
Patch:
@@ -34,7 +34,7 @@
  * @author kunni@Dtstack.com
  */
 
-public class KingBaseInputFormat extends JdbcInputFormat {
+public class KingbaseInputFormat extends JdbcInputFormat {
 
     @Override
     public Row nextRecordInternal(Row row) throws IOException {

File: flinkx-kingbase/flinkx-kingbase-reader/src/main/java/com/dtstack/flinkx/kingbase/reader/KingbaseReader.java
Patch:
@@ -20,7 +20,7 @@
 
 import com.dtstack.flinkx.config.DataTransferConfig;
 import com.dtstack.flinkx.kingbase.KingBaseDatabaseMeta;
-import com.dtstack.flinkx.kingbase.format.KingBaseInputFormat;
+import com.dtstack.flinkx.kingbase.format.KingbaseInputFormat;
 import com.dtstack.flinkx.rdb.datareader.JdbcDataReader;
 import com.dtstack.flinkx.rdb.inputformat.JdbcInputFormatBuilder;
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
@@ -41,6 +41,6 @@ public KingbaseReader(DataTransferConfig config, StreamExecutionEnvironment env)
 
     @Override
     protected JdbcInputFormatBuilder getBuilder() {
-        return new JdbcInputFormatBuilder(new KingBaseInputFormat());
+        return new JdbcInputFormatBuilder(new KingbaseInputFormat());
     }
 }

File: flinkx-kingbase/flinkx-kingbase-writer/src/main/java/com/dtstack/flinkx/kingbase/format/KingbaseOutputFormat.java
Patch:
@@ -37,7 +37,7 @@
  * @author kunni@dtstack.com
  */
 
-public class KingBaseOutputFormat extends JdbcOutputFormat {
+public class KingbaseOutputFormat extends JdbcOutputFormat {
 
     private static final String COPY_SQL_TEMPL = "COPY %s(%s) FROM STDIN DELIMITER '%s' NULL AS '%s'";
 

File: flinkx-kingbase/flinkx-kingbase-writer/src/main/java/com/dtstack/flinkx/kingbase/writer/KingbaseWriter.java
Patch:
@@ -21,7 +21,7 @@
 import com.dtstack.flinkx.config.DataTransferConfig;
 import com.dtstack.flinkx.kingbase.KingBaseDatabaseMeta;
 import com.dtstack.flinkx.kingbase.KingBaseTypeConverter;
-import com.dtstack.flinkx.kingbase.format.KingBaseOutputFormat;
+import com.dtstack.flinkx.kingbase.format.KingbaseOutputFormat;
 import com.dtstack.flinkx.rdb.datawriter.JdbcDataWriter;
 import com.dtstack.flinkx.rdb.outputformat.JdbcOutputFormatBuilder;
 import com.dtstack.flinkx.streaming.api.functions.sink.DtOutputFormatSinkFunction;
@@ -46,7 +46,7 @@ public KingbaseWriter(DataTransferConfig config) {
 
     @Override
     public DataStreamSink<?> writeData(DataStream<Row> dataSet) {
-        KingBaseOutputFormat kingBaseOutputFormat = new KingBaseOutputFormat();
+        KingbaseOutputFormat kingBaseOutputFormat = new KingbaseOutputFormat();
         JdbcOutputFormatBuilder builder = new JdbcOutputFormatBuilder(kingBaseOutputFormat);
         builder.setDriverName(databaseInterface.getDriverClass());
         builder.setDbUrl(dbUrl);

File: flinkx-core/src/main/java/com/dtstack/flinkx/inputformat/BaseRichInputFormat.java
Patch:
@@ -188,7 +188,7 @@ private void showConfig(){
                 writerParameter.put(KEY_PASSWORD, KEY_CONFUSED_PASSWORD);
             }
         }
-    LOG.info("configInfo : {}", GsonUtil.GSON.toJson(map));
+        LOG.info("configInfo : \n{}", GsonUtil.GSON.toJson(map));
     }
 
     private void checkIfCreateSplitFailed(InputSplit inputSplit){

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/GsonUtil.java
Patch:
@@ -50,7 +50,9 @@ public class GsonUtil {
 
     @SuppressWarnings("unchecked")
     private static Gson getGson() {
-        GSON = new GsonBuilder().create();
+        GSON = new GsonBuilder()
+                .setPrettyPrinting()
+                .create();
         try {
             Field factories = Gson.class.getDeclaredField("factories");
             factories.setAccessible(true);

File: flinkx-core/src/main/java/com/dtstack/flinkx/inputformat/BaseRichInputFormat.java
Patch:
@@ -188,7 +188,7 @@ private void showConfig(){
                 writerParameter.put(KEY_PASSWORD, KEY_CONFUSED_PASSWORD);
             }
         }
-    LOG.info("configInfo : {}", GsonUtil.GSON.toJson(map));
+        LOG.info("configInfo : \n{}", GsonUtil.GSON.toJson(map));
     }
 
     private void checkIfCreateSplitFailed(InputSplit inputSplit){

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/GsonUtil.java
Patch:
@@ -50,7 +50,9 @@ public class GsonUtil {
 
     @SuppressWarnings("unchecked")
     private static Gson getGson() {
-        GSON = new GsonBuilder().create();
+        GSON = new GsonBuilder()
+                .setPrettyPrinting()
+                .create();
         try {
             Field factories = Gson.class.getDeclaredField("factories");
             factories.setAccessible(true);

File: flinkx-sqlservercdc/flinkx-sqlservercdc-core/src/main/java/com/dtstack/flinkx/sqlservercdc/SqlServerCdcUtil.java
Patch:
@@ -64,7 +64,7 @@ public class SqlServerCdcUtil {
 
     public static void changeDatabase(Connection conn, String databaseName) throws SQLException {
         try (Statement statement = conn.createStatement()) {
-            statement.execute(" use " + databaseName);
+            statement.execute(" use " + "\""+databaseName+"\"");
         }
     }
 

File: flinkx-mysql/flinkx-mysql-reader/src/main/java/com/dtstack/flinkx/mysql/format/MysqlInputFormat.java
Patch:
@@ -25,9 +25,6 @@
 import org.apache.flink.types.Row;
 
 import java.io.IOException;
-import java.math.BigInteger;
-import java.sql.SQLException;
-import java.util.ArrayList;
 
 import static com.dtstack.flinkx.rdb.util.DbUtil.clobToString;
 

File: flinkx-binlog/flinkx-binlog-reader/src/main/java/com/dtstack/flinkx/binlog/reader/BinlogReader.java
Patch:
@@ -48,6 +48,7 @@ public BinlogReader(DataTransferConfig config, StreamExecutionEnvironment env) {
     @Override
     public DataStream<Row> readData() {
         BinlogInputFormat format = new BinlogInputFormat();
+        format.setDataTransferConfig(dataTransferConfig);
         format.setBinlogConfig(binlogConfig);
         format.setRestoreConfig(restoreConfig);
         format.setLogConfig(logConfig);

File: flinkx-carbondata/flinkx-carbondata-reader/src/main/java/com/dtstack/flinkx/carbondata/reader/CarbondataReader.java
Patch:
@@ -95,6 +95,7 @@ public CarbondataReader(DataTransferConfig config, StreamExecutionEnvironment en
     @Override
     public DataStream<Row> readData() {
         CarbondataInputFormatBuilder builder = new CarbondataInputFormatBuilder();
+        builder.setDataTransferConfig(dataTransferConfig);
         builder.setColumnNames(columnName);
         builder.setColumnTypes(columnType);
         builder.setColumnValues(columnValue);

File: flinkx-cassandra/flinkx-cassandra-reader/src/main/java/com/dtstack/flinkx/cassandra/reader/CassandraReader.java
Patch:
@@ -106,6 +106,7 @@ public CassandraReader(DataTransferConfig config, StreamExecutionEnvironment env
     @Override
     public DataStream<Row> readData() {
         CassandraInputFormatBuilder builder = new CassandraInputFormatBuilder();
+        builder.setDataTransferConfig(dataTransferConfig);
         builder.setTable(table);
         builder.setWhere(where);
         builder.setConsistancyLevel(consistancyLevel);

File: flinkx-core/src/main/java/com/dtstack/flinkx/reader/BaseDataReader.java
Patch:
@@ -64,6 +64,8 @@ public abstract class BaseDataReader {
 
     protected long exceptionIndex;
 
+    protected DataTransferConfig dataTransferConfig;
+
     /**
      * reuse hadoopConfig for metric
      */
@@ -89,6 +91,7 @@ public void setSrcCols(List<String> srcCols) {
 
     protected BaseDataReader(DataTransferConfig config, StreamExecutionEnvironment env) {
         this.env = env;
+        this.dataTransferConfig = config;
         this.numPartitions = config.getJob().getSetting().getSpeed().getChannel();
         this.bytes = config.getJob().getSetting().getSpeed().getBytes();
         this.monitorUrls = config.getMonitorUrls();

File: flinkx-core/src/main/java/com/dtstack/flinkx/reader/DataReaderFactory.java
Patch:
@@ -41,7 +41,7 @@ public static BaseDataReader getDataReader(DataTransferConfig config, StreamExec
         try {
             String pluginName = config.getJob().getContent().get(0).getReader().getName();
             String pluginClassName = PluginUtil.getPluginClassName(pluginName);
-            Set<URL> urlList = PluginUtil.getJarFileDirPath(pluginName, config.getPluginRoot());
+            Set<URL> urlList = PluginUtil.getJarFileDirPath(pluginName, config.getPluginRoot(), config.getRemotePluginPath());
 
             return ClassLoaderManager.newInstance(urlList, cl -> {
                 Class<?> clazz = cl.loadClass(pluginClassName);

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/RetryUtil.java
Patch:
@@ -74,7 +74,7 @@ public <T> T doRetry(Callable<T> callable, int retryTimes, long sleepTimeInMilli
                 } catch (Exception e) {
                     saveException = e;
                     if (i == 0) {
-                        LOG.error(String.format("Exception when calling callable, 异常Msg:%s", saveException.getMessage()), saveException);
+                        LOG.error(String.format("Exception when calling callable, 异常Msg:%s", ExceptionUtil.getErrorMessage(saveException)), saveException);
                     }
 
                     if (null != retryExceptionClasss && !retryExceptionClasss.isEmpty()) {

File: flinkx-core/src/main/java/com/dtstack/flinkx/writer/DataWriterFactory.java
Patch:
@@ -40,7 +40,7 @@ public static BaseDataWriter getDataWriter(DataTransferConfig config) {
         try {
             String pluginName = config.getJob().getContent().get(0).getWriter().getName();
             String pluginClassName = PluginUtil.getPluginClassName(pluginName);
-            Set<URL> urlList = PluginUtil.getJarFileDirPath(pluginName, config.getPluginRoot());
+            Set<URL> urlList = PluginUtil.getJarFileDirPath(pluginName, config.getPluginRoot(), config.getRemotePluginPath());
 
             return ClassLoaderManager.newInstance(urlList, cl -> {
                 Class<?> clazz = cl.loadClass(pluginClassName);

File: flinkx-core/src/main/java/com/dtstack/flinkx/writer/ErrorLimiter.java
Patch:
@@ -29,8 +29,7 @@
  * Company: www.dtstack.com
  * @author huyifan.zju@163.com
  */
-public class
-ErrorLimiter {
+public class ErrorLimiter {
 
     private final Integer maxErrors;
     private final Double maxErrorRatio;

File: flinkx-emqx/flinkx-emqx-reader/src/main/java/com/dtstack/flinkx/emqx/reader/EmqxReader.java
Patch:
@@ -47,6 +47,7 @@ public EmqxReader(DataTransferConfig config, StreamExecutionEnvironment env) {
     @Override
     public DataStream<Row> readData() {
         EmqxInputFormatBuilder builder = new EmqxInputFormatBuilder();
+        builder.setDataTransferConfig(dataTransferConfig);
         builder.setBroker(broker);
         builder.setTopic(topic);
         builder.setUsername(username);

File: flinkx-es/flinkx-es-reader/src/main/java/com/dtstack/flinkx/es/reader/EsReader.java
Patch:
@@ -104,6 +104,7 @@ public EsReader(DataTransferConfig config, StreamExecutionEnvironment env) {
     @Override
     public DataStream<Row> readData() {
         EsInputFormatBuilder builder = new EsInputFormatBuilder();
+        builder.setDataTransferConfig(dataTransferConfig);
         builder.setColumnNames(columnName);
         builder.setColumnTypes(columnType);
         builder.setColumnValues(columnValue);

File: flinkx-es/flinkx-es-writer/src/main/java/com/dtstack/flinkx/es/writer/EsOutputFormat.java
Patch:
@@ -119,8 +119,8 @@ private void processFailResponse(BulkResponse response){
                     dirtyDataManager.writeData(rows.get(i), exception);
                 }
 
-                if(numWriteCounter != null ){
-                    numWriteCounter.add(1);
+                if (errCounter != null) {
+                    errCounter.add(1);
                 }
             }
         }

File: flinkx-ftp/flinkx-ftp-reader/src/main/java/com/dtstack/flinkx/ftp/reader/FtpReader.java
Patch:
@@ -68,6 +68,7 @@ public FtpReader(DataTransferConfig config, StreamExecutionEnvironment env) {
     @Override
     public DataStream<Row> readData() {
         FtpInputFormatBuilder builder = new FtpInputFormatBuilder();
+        builder.setDataTransferConfig(dataTransferConfig);
         builder.setFtpConfig(ftpConfig);
         builder.setMetaColumn(metaColumns);
         builder.setTestConfig(testConfig);

File: flinkx-gbase/flinkx-gbase-core/src/main/java/com/dtstack/flinkx/gbase/GbaseDatabaseMeta.java
Patch:
@@ -119,7 +119,7 @@ public String getRowNumColumn(String orderBy) {
 
     @Override
     public int getFetchSize(){
-        return 1000;
+        return Integer.MIN_VALUE;
     }
 
     @Override

File: flinkx-greenplum/flinkx-greenplum-reader/src/main/java/com/dtstack/flinkx/greenplum/reader/GreenplumReader.java
Patch:
@@ -53,6 +53,7 @@ protected JdbcInputFormatBuilder getBuilder() {
     @Override
     public DataStream<Row> readData() {
         JdbcInputFormatBuilder builder = new JdbcInputFormatBuilder(new GreenplumInputFormat());
+        builder.setDataTransferConfig(dataTransferConfig);
         builder.setDriverName(databaseInterface.getDriverClass());
         builder.setDbUrl(dbUrl);
         builder.setUsername(username);

File: flinkx-hbase/flinkx-hbase-core/src/main/java/com/dtstack/flinkx/hbase/HbaseHelper.java
Patch:
@@ -55,7 +55,7 @@ public class HbaseHelper {
     private final static String KEY_HBASE_SECURITY_AUTHORIZATION = "hbase.security.authorization";
 
     public static org.apache.hadoop.hbase.client.Connection getHbaseConnection(Map<String,Object> hbaseConfigMap) {
-        Validate.isTrue(MapUtils.isEmpty(hbaseConfigMap), "hbaseConfig不能为空Map结构!");
+        Validate.isTrue(MapUtils.isNotEmpty(hbaseConfigMap), "hbaseConfig不能为空Map结构!");
 
         if(openKerberos(hbaseConfigMap)){
             return getConnectionWithKerberos(hbaseConfigMap);

File: flinkx-hbase/flinkx-hbase-reader/src/main/java/com/dtstack/flinkx/hbase/reader/HbaseReader.java
Patch:
@@ -96,7 +96,7 @@ public HbaseReader(DataTransferConfig config, StreamExecutionEnvironment env) {
     @Override
     public DataStream<Row> readData() {
         HbaseInputFormatBuilder builder = new HbaseInputFormatBuilder();
-
+        builder.setDataTransferConfig(dataTransferConfig);
         builder.setColumnFormats(columnFormat);
         builder.setColumnNames(columnName);
         builder.setColumnTypes(columnType);

File: flinkx-hdfs/flinkx-hdfs-reader/src/main/java/com/dtstack/flinkx/hdfs/reader/HdfsReader.java
Patch:
@@ -70,6 +70,7 @@ public HdfsReader(DataTransferConfig config, StreamExecutionEnvironment env) {
     @Override
     public DataStream<Row> readData() {
         HdfsInputFormatBuilder builder = new HdfsInputFormatBuilder(fileType);
+        builder.setDataTransferConfig(dataTransferConfig);
         builder.setInputPaths(path);
         builder.setMetaColumn(metaColumns);
         builder.setHadoopConfig(hadoopConfig);

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsOrcOutputFormat.java
Patch:
@@ -50,6 +50,7 @@
 import java.sql.Timestamp;
 import java.text.SimpleDateFormat;
 import java.util.ArrayList;
+import java.util.Arrays;
 import java.util.HashMap;
 import java.util.List;
 
@@ -201,7 +202,7 @@ private void getData(List<Object> recordList, int index, Row row) throws WriteRe
 
         ColumnType columnType = ColumnType.fromString(columnTypes.get(j));
         String rowData = column.toString();
-        if(rowData == null || rowData.length() == 0){
+        if(rowData == null || (rowData.length() == 0 && !ColumnType.isStringType(columnType)) ){
             recordList.add(null);
             return;
         }

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsParquetOutputFormat.java
Patch:
@@ -155,7 +155,7 @@ public void writeSingleRecordToFile(Row row) throws WriteRecordException {
                 int colIndex = colIndices[i];
                 if(colIndex > -1){
                     Object valObj = row.getField(colIndex);
-                    if(valObj == null || valObj.toString().length() == 0){
+                    if(valObj == null || (valObj.toString().length() == 0 && !ColumnType.isStringType(fullColumnTypes.get(i)))){
                         continue;
                     }
 

File: flinkx-kb/flinkx-kb-reader/src/main/java/com/dtstack/flinkx/kafkabase/reader/KafkaBaseReader.java
Patch:
@@ -60,6 +60,7 @@ public KafkaBaseReader(DataTransferConfig config, StreamExecutionEnvironment env
     @Override
     public DataStream<Row> readData() {
         KafkaBaseInputFormat format = getFormat();
+        format.setDataTransferConfig(dataTransferConfig);
         format.setTopic(topic);
         format.setGroupId(groupId);
         format.setCodec(codec);

File: flinkx-kudu/flinkx-kudu-reader/src/main/java/com/dtstack/flinkx/kudu/reader/KuduReader.java
Patch:
@@ -83,6 +83,7 @@ public KuduReader(DataTransferConfig config, StreamExecutionEnvironment env) {
     @Override
     public DataStream<Row> readData() {
         KuduInputFormatBuilder builder = new KuduInputFormatBuilder();
+        builder.setDataTransferConfig(dataTransferConfig);
         builder.setColumns(columns);
         builder.setMonitorUrls(monitorUrls);
         builder.setBytes(bytes);

File: flinkx-metadata-hive1/flinkx-metadata-hive1-reader/src/main/java/com/dtstack/flinkx/metadatahive1/reader/Metadatahive1Reader.java
Patch:
@@ -6,7 +6,7 @@
 
 public class Metadatahive1Reader extends Metadatahive2Reader {
 
-    public static final String DRIVER_NAME = "shade.hive1.jdbc.HiveDriver";
+    public static final String DRIVER_NAME = "org.apache.hive.jdbc.HiveDriver";
 
     public Metadatahive1Reader(DataTransferConfig config, StreamExecutionEnvironment env) {
         super(config, env);

File: flinkx-metadata-hive2/flinkx-metadata-hive2-reader/src/main/java/com/dtstack/flinkx/metadatahive2/constants/Hive2MetaDataCons.java
Patch:
@@ -26,7 +26,7 @@
  */
 @SuppressWarnings("all")
 public class Hive2MetaDataCons extends MetaDataCons {
-    public static final String DRIVER_NAME = "shade.hive2.HiveDriver";
+    public static final String DRIVER_NAME = "org.apache.hive.jdbc.HiveDriver";
     public static final String KEY_HADOOP_CONFIG = "hadoopConfig";
 
     public static final String KEY_SOURCE = "source";

File: flinkx-metadata-mysql/flinkx-metadata-mysql-reader/src/main/java/com/dtstack/flinkx/metadatamysql/inputformat/MetadatamysqlInputFormat.java
Patch:
@@ -38,6 +38,8 @@
 
 public class MetadatamysqlInputFormat extends MetadatatidbInputFormat {
 
+    private static final long serialVersionUID = 1L;
+
     @Override
     protected Map<String, Object> queryMetaData(String tableName) throws SQLException {
         Map<String, Object> result = new HashMap<>(16);

File: flinkx-metadata-oracle/flinkx-metadata-oracle-reader/src/main/java/com/dtstack/flinkx/metadataoracle/constants/OracleMetaDataCons.java
Patch:
@@ -36,8 +36,6 @@ public class OracleMetaDataCons extends MetaDataCons {
     public static final String KEY_TOTAL_SIZE = "totalSize";
     public static final String KEY_INDEX_COLUMN_NAME = "columnName";
 
-    /** 统一采用DBA表
-     */
     public static final String SQL_QUERY_INDEX = "SELECT COLUMNS.INDEX_NAME, COLUMNS.COLUMN_NAME, INDEXES.INDEX_TYPE  FROM DBA_IND_COLUMNS COLUMNS JOIN DBA_INDEXES INDEXES \n" +
             "ON COLUMNS.INDEX_NAME = INDEXES.INDEX_NAME AND COLUMNS.TABLE_NAME = INDEXES.TABLE_NAME AND COLUMNS.TABLE_OWNER = INDEXES.TABLE_OWNER \n" +
             "WHERE COLUMNS.TABLE_OWNER = %s AND COLUMNS.TABLE_NAME = %s";
@@ -48,6 +46,5 @@ public class OracleMetaDataCons extends MetaDataCons {
             "FROM DBA_TABLES TABLES JOIN DBA_OBJECTS OBJS ON TABLES.OWNER = OBJS.OWNER AND TABLES.TABLE_NAME = OBJS.OBJECT_NAME\n" +
             "JOIN DBA_TAB_COMMENTS COMMENTS ON TABLES.OWNER = COMMENTS.OWNER AND TABLES.TABLE_NAME = COMMENTS.TABLE_NAME\n" +
             "WHERE TABLES.OWNER = %s AND TABLES.TABLE_NAME = %s";
-    public static final String SQL_SHOW_DATABASES = "SELECT USERNAME FROM DBA_USERS";
     public static final String SQL_SHOW_TABLES = "SELECT TABLE_NAME FROM DBA_TABLES WHERE OWNER = %s";
 }
\ No newline at end of file

File: flinkx-metadata/flinkx-metadata-reader/src/main/java/com/dtstack/flinkx/metadata/reader/MetadataReader.java
Patch:
@@ -57,7 +57,7 @@ protected MetadataReader(DataTransferConfig config, StreamExecutionEnvironment e
     @Override
     public DataStream<Row> readData() {
         MetadataInputFormatBuilder builder = getBuilder();
-
+        builder.setDataTransferConfig(dataTransferConfig);
         builder.setDbUrl(jdbcUrl);
         builder.setPassword(password);
         builder.setUsername(username);

File: flinkx-mongodb/flinkx-mongodb-reader/src/main/java/com/dtstack/flinkx/mongodb/reader/MongodbReader.java
Patch:
@@ -57,6 +57,7 @@ public MongodbReader(DataTransferConfig config, StreamExecutionEnvironment env)
     @Override
     public DataStream<Row> readData() {
         MongodbInputFormatBuilder builder = new MongodbInputFormatBuilder();
+        builder.setDataTransferConfig(dataTransferConfig);
         builder.setMetaColumns(metaColumns);
         builder.setMongodbConfig(mongodbConfig);
 

File: flinkx-odps/flinkx-odps-reader/src/main/java/com/dtstack/flinkx/odps/reader/OdpsReader.java
Patch:
@@ -59,7 +59,7 @@ public OdpsReader(DataTransferConfig config, StreamExecutionEnvironment env) {
     @Override
     public DataStream<Row> readData() {
         OdpsInputFormatBuilder builder = new OdpsInputFormatBuilder();
-
+        builder.setDataTransferConfig(dataTransferConfig);
         builder.setMetaColumn(metaColumns);
         builder.setOdpsConfig(odpsConfig);
         builder.setTableName(tableName);

File: flinkx-oracle/flinkx-oracle-reader/src/main/java/com/dtstack/flinkx/oracle/format/OracleInputFormat.java
Patch:
@@ -75,7 +75,8 @@ protected String getTimeStr(Long location, String incrementColType){
         timeStr = DbUtil.getNanosTimeStr(ts.toString());
 
         if(ColumnType.TIMESTAMP.name().equals(incrementColType)){
-            timeStr = String.format("TO_TIMESTAMP('%s','YYYY-MM-DD HH24:MI:SS:FF6')",timeStr);
+            //纳秒精度为9位
+            timeStr = String.format("TO_TIMESTAMP('%s','YYYY-MM-DD HH24:MI:SS:FF9')", timeStr);
         } else {
             timeStr = timeStr.substring(0, 19);
             timeStr = String.format("TO_DATE('%s','YYYY-MM-DD HH24:MI:SS')", timeStr);

File: flinkx-oraclelogminer/flinkx-oraclelogminer-reader/src/main/java/com/dtstack/flinkx/oraclelogminer/reader/OraclelogminerReader.java
Patch:
@@ -63,6 +63,7 @@ private void buildTableListenerRegex(){
     @Override
     public DataStream<Row> readData() {
         OracleLogMinerInputFormatBuilder builder = new OracleLogMinerInputFormatBuilder();
+        builder.setDataTransferConfig(dataTransferConfig);
         builder.setLogMinerConfig(logMinerConfig);
         builder.setRestoreConfig(restoreConfig);
 

File: flinkx-oraclelogminer/flinkx-oraclelogminer-reader/src/main/java/com/dtstack/flinkx/oraclelogminer/util/SqlUtil.java
Patch:
@@ -23,6 +23,7 @@
 
 import java.util.ArrayList;
 import java.util.Arrays;
+import java.util.Collections;
 import java.util.List;
 
 /**
@@ -185,11 +186,11 @@ public class SqlUtil {
 
     public final static String SQL_QUERY_ROLES = "SELECT * FROM USER_ROLE_PRIVS";
 
-    public final static String SQL_QUERY_PRIVILEGES = "select * from session_privs";
+    public final static String SQL_QUERY_PRIVILEGES = "SELECT * FROM SESSION_PRIVS";
 
     private final static List<String> SUPPORTED_OPERATIONS = Arrays.asList("UPDATE", "INSERT", "DELETE");
 
-    public static List<String> EXCLUDE_SCHEMAS = Arrays.asList("SYS");
+    public static List<String> EXCLUDE_SCHEMAS = Collections.singletonList("SYS");
 
     public static String buildSelectSql(String listenerOptions, String listenerTables){
         StringBuilder sqlBuilder = new StringBuilder(SQL_SELECT_DATA);

File: flinkx-postgresql/flinkx-postgresql-reader/src/main/java/com/dtstack/flinkx/postgresql/reader/PostgresqlReader.java
Patch:
@@ -54,6 +54,7 @@ protected JdbcInputFormatBuilder getBuilder() {
     @Override
     public DataStream<Row> readData() {
         JdbcInputFormatBuilder builder = new JdbcInputFormatBuilder(new PostgresqlInputFormat());
+        builder.setDataTransferConfig(dataTransferConfig);
         builder.setDriverName(databaseInterface.getDriverClass());
         builder.setDbUrl(dbUrl);
         builder.setUsername(username);

File: flinkx-rdb/flinkx-rdb-core/src/main/java/com/dtstack/flinkx/rdb/util/DbUtil.java
Patch:
@@ -143,7 +143,7 @@ public static Connection getConnection(String url, String username, String passw
                 try {
                     dbConn = getConnectionInternal(url, username, password);
                     try (Statement statement = dbConn.createStatement()){
-                        statement.execute("select 111");
+                        statement.execute("SELECT 1 FROM dual");
                         failed = false;
                     }
                 } catch (Exception e) {

File: flinkx-rdb/flinkx-rdb-reader/src/main/java/com.dtstack.flinkx.rdb.datareader/DistributedJdbcDataReader.java
Patch:
@@ -83,6 +83,7 @@ protected DistributedJdbcDataReader(DataTransferConfig config, StreamExecutionEn
     @Override
     public DataStream<Row> readData() {
         DistributedJdbcInputFormatBuilder builder = getBuilder();
+        builder.setDataTransferConfig(dataTransferConfig);
         builder.setDrivername(databaseInterface.getDriverClass());
         builder.setUsername(username);
         builder.setPassword(password);

File: flinkx-rdb/flinkx-rdb-reader/src/main/java/com.dtstack.flinkx.rdb.datareader/JdbcDataReader.java
Patch:
@@ -100,6 +100,7 @@ public JdbcDataReader(DataTransferConfig config, StreamExecutionEnvironment env)
     @Override
     public DataStream<Row> readData() {
         JdbcInputFormatBuilder builder = getBuilder();
+        builder.setDataTransferConfig(dataTransferConfig);
         builder.setDriverName(databaseInterface.getDriverClass());
         builder.setDbUrl(dbUrl);
         builder.setUsername(username);

File: flinkx-restapi/flinkx-restapi-reader/src/main/java/com/dtstack/flinkx/restapi/reader/RestapiReader.java
Patch:
@@ -61,7 +61,7 @@ public RestapiReader(DataTransferConfig config, StreamExecutionEnvironment env)
     @Override
     public DataStream<Row> readData() {
         RestapiInputFormatBuilder builder = new RestapiInputFormatBuilder();
-
+        builder.setDataTransferConfig(dataTransferConfig);
         builder.setHeader(header);
         builder.setMethod(method);
         builder.setUrl(url);

File: flinkx-sqlservercdc/flinkx-sqlservercdc-reader/src/main/java/com/dtstack/flinkx/sqlservercdc/reader/SqlservercdcReader.java
Patch:
@@ -63,6 +63,7 @@ public SqlservercdcReader(DataTransferConfig config, StreamExecutionEnvironment
     @Override
     public DataStream<Row> readData() {
         SqlserverCdcInputFormatBuilder builder = new SqlserverCdcInputFormatBuilder();
+        builder.setDataTransferConfig(dataTransferConfig);
         builder.setUsername(username);
         builder.setPassword(password);
         builder.setUrl(url);

File: flinkx-stream/flinkx-stream-reader/src/main/java/com/dtstack/flinkx/stream/reader/StreamReader.java
Patch:
@@ -61,6 +61,7 @@ public StreamReader(DataTransferConfig config, StreamExecutionEnvironment env) {
     @Override
     public DataStream<Row> readData() {
         StreamInputFormatBuilder builder = new StreamInputFormatBuilder();
+        builder.setDataTransferConfig(dataTransferConfig);
         builder.setColumns(columns);
         builder.setSliceRecordCount(sliceRecordCount);
         builder.setMonitorUrls(monitorUrls);

File: flinkx-test/src/main/java/com/dtstack/flinkx/test/PluginNameConstrant.java
Patch:
@@ -55,6 +55,7 @@ public class PluginNameConstrant {
     public static final String METADATATIDB_READER = "metadatatidbreader";
     public static final String METADATAORACLE_READER = "metadataoraclereader";
     public static final String METADATAMYSQL_READER = "metadatamysqlreader";
+    public static final String METADATASQLSERVER_READER = "metadatasqlserverreader";
     public static final String GREENPLUM_READER = "greenplumreader";
     public static final String PHOENIX5_READER = "phoenix5reader";
 

File: flinkx-metadata/flinkx-metadata-core/src/main/java/com/dtstack/flinkx/metadata/MetaDataCons.java
Patch:
@@ -56,4 +56,6 @@ public class MetaDataCons {
     public static final String SQL_SHOW_DATABASES = "SHOW DATABASES";
     public static final String SQL_SHOW_TABLES = "SHOW TABLES";
     public static final String SQL_SWITCH_DATABASE = "USE %s";
+
+    public static final int MAX_TABLE_SIZE = 20;
 }
\ No newline at end of file

File: flinkx-core/src/main/java/com/dtstack/flinkx/reader/DataReaderFactory.java
Patch:
@@ -41,7 +41,7 @@ public static BaseDataReader getDataReader(DataTransferConfig config, StreamExec
         try {
             String pluginName = config.getJob().getContent().get(0).getReader().getName();
             String pluginClassName = PluginUtil.getPluginClassName(pluginName);
-            Set<URL> urlList = PluginUtil.getJarFileDirPath(pluginName, config.getPluginRoot());
+            Set<URL> urlList = PluginUtil.getJarFileDirPath(pluginName, config.getPluginRoot(), config.getRemotePluginPath());
 
             return ClassLoaderManager.newInstance(urlList, cl -> {
                 Class<?> clazz = cl.loadClass(pluginClassName);

File: flinkx-core/src/main/java/com/dtstack/flinkx/writer/DataWriterFactory.java
Patch:
@@ -40,7 +40,7 @@ public static BaseDataWriter getDataWriter(DataTransferConfig config) {
         try {
             String pluginName = config.getJob().getContent().get(0).getWriter().getName();
             String pluginClassName = PluginUtil.getPluginClassName(pluginName);
-            Set<URL> urlList = PluginUtil.getJarFileDirPath(pluginName, config.getPluginRoot());
+            Set<URL> urlList = PluginUtil.getJarFileDirPath(pluginName, config.getPluginRoot(), config.getRemotePluginPath());
 
             return ClassLoaderManager.newInstance(urlList, cl -> {
                 Class<?> clazz = cl.loadClass(pluginClassName);

File: flinkx-metadata/flinkx-metadata-core/src/main/java/com/dtstack/flinkx/metadata/MetaDataCons.java
Patch:
@@ -58,4 +58,6 @@ public class MetaDataCons {
     public static final String SQL_SHOW_DATABASES = "SHOW DATABASES";
     public static final String SQL_SHOW_TABLES = "SHOW TABLES";
     public static final String SQL_SWITCH_DATABASE = "USE %s";
+
+    public static final int MAX_TABLE_SIZE = 20;
 }
\ No newline at end of file

File: flinkx-oracle/flinkx-oracle-reader/src/main/java/com/dtstack/flinkx/oracle/format/OracleInputFormat.java
Patch:
@@ -75,7 +75,8 @@ protected String getTimeStr(Long location, String incrementColType){
         timeStr = DbUtil.getNanosTimeStr(ts.toString());
 
         if(ColumnType.TIMESTAMP.name().equals(incrementColType)){
-            timeStr = String.format("TO_TIMESTAMP('%s','YYYY-MM-DD HH24:MI:SS:FF6')",timeStr);
+            //纳秒精度为9位
+            timeStr = String.format("TO_TIMESTAMP('%s','YYYY-MM-DD HH24:MI:SS:FF9')", timeStr);
         } else {
             timeStr = timeStr.substring(0, 19);
             timeStr = String.format("TO_DATE('%s','YYYY-MM-DD HH24:MI:SS')", timeStr);

File: flinkx-mysql/flinkx-mysql-reader/src/main/java/com/dtstack/flinkx/mysql/format/MysqlInputFormat.java
Patch:
@@ -39,7 +39,7 @@ public class MysqlInputFormat extends JdbcInputFormat {
     @Override
     public void openInternal(InputSplit inputSplit) throws IOException {
         // 避免result.next阻塞
-        if(incrementConfig.isPolling() && StringUtils.isEmpty(incrementConfig.getStartLocation())){
+        if(incrementConfig.isPolling() && StringUtils.isEmpty(incrementConfig.getStartLocation()) && fetchSize==databaseInterface.getFetchSize()){
             fetchSize = 1000;
         }
         super.openInternal(inputSplit);

File: flinkx-rdb/flinkx-rdb-reader/src/main/java/com.dtstack.flinkx.rdb.inputformat/JdbcInputFormat.java
Patch:
@@ -168,7 +168,9 @@ public void openInputFormat() throws IOException {
     public void openInternal(InputSplit inputSplit) throws IOException {
         try {
             LOG.info("inputSplit = {}", inputSplit);
-            type = ColumnType.fromString(incrementConfig.getColumnType());
+            if(incrementConfig.isIncrement()){
+                type = ColumnType.fromString(incrementConfig.getColumnType());
+            }
             ClassUtil.forName(driverName, getClass().getClassLoader());
             //从配置中获取起始位置
             generalStartLocation = incrementConfig.getStartLocation();

File: flinkx-rdb/flinkx-rdb-reader/src/main/java/com.dtstack.flinkx.rdb.inputformat/JdbcInputFormat.java
Patch:
@@ -168,7 +168,9 @@ public void openInputFormat() throws IOException {
     public void openInternal(InputSplit inputSplit) throws IOException {
         try {
             LOG.info("inputSplit = {}", inputSplit);
-            type = ColumnType.fromString(incrementConfig.getColumnType());
+            if(incrementConfig.isIncrement()){
+                type = ColumnType.fromString(incrementConfig.getColumnType());
+            }
             ClassUtil.forName(driverName, getClass().getClassLoader());
             //从配置中获取起始位置
             generalStartLocation = incrementConfig.getStartLocation();

File: flinkx-rdb/flinkx-rdb-reader/src/main/java/com.dtstack.flinkx.rdb.inputformat/JdbcInputFormat.java
Patch:
@@ -168,7 +168,9 @@ public void openInputFormat() throws IOException {
     public void openInternal(InputSplit inputSplit) throws IOException {
         try {
             LOG.info("inputSplit = {}", inputSplit);
-            type = ColumnType.fromString(incrementConfig.getColumnType());
+            if(incrementConfig.isIncrement()){
+                type = ColumnType.fromString(incrementConfig.getColumnType());
+            }
             ClassUtil.forName(driverName, getClass().getClassLoader());
             //从配置中获取起始位置
             generalStartLocation = incrementConfig.getStartLocation();

File: flinkx-rdb/flinkx-rdb-reader/src/main/java/com.dtstack.flinkx.rdb.inputformat/JdbcInputFormat.java
Patch:
@@ -168,7 +168,9 @@ public void openInputFormat() throws IOException {
     public void openInternal(InputSplit inputSplit) throws IOException {
         try {
             LOG.info("inputSplit = {}", inputSplit);
-            type = ColumnType.fromString(incrementConfig.getColumnType());
+            if(incrementConfig.isIncrement()){
+                type = ColumnType.fromString(incrementConfig.getColumnType());
+            }
             ClassUtil.forName(driverName, getClass().getClassLoader());
             //从配置中获取起始位置
             generalStartLocation = incrementConfig.getStartLocation();

File: flinkx-rdb/flinkx-rdb-reader/src/main/java/com.dtstack.flinkx.rdb.inputformat/JdbcInputFormat.java
Patch:
@@ -168,7 +168,9 @@ public void openInputFormat() throws IOException {
     public void openInternal(InputSplit inputSplit) throws IOException {
         try {
             LOG.info("inputSplit = {}", inputSplit);
-            type = ColumnType.fromString(incrementConfig.getColumnType());
+            if(incrementConfig.isIncrement()){
+                type = ColumnType.fromString(incrementConfig.getColumnType());
+            }
             ClassUtil.forName(driverName, getClass().getClassLoader());
             //从配置中获取起始位置
             generalStartLocation = incrementConfig.getStartLocation();

File: flinkx-rdb/flinkx-rdb-reader/src/main/java/com.dtstack.flinkx.rdb.inputformat/JdbcInputFormat.java
Patch:
@@ -168,7 +168,9 @@ public void openInputFormat() throws IOException {
     public void openInternal(InputSplit inputSplit) throws IOException {
         try {
             LOG.info("inputSplit = {}", inputSplit);
-            type = ColumnType.fromString(incrementConfig.getColumnType());
+            if(StringUtils.isNotEmpty(incrementConfig.getColumnType())){
+                type = ColumnType.fromString(incrementConfig.getColumnType());
+            }
             ClassUtil.forName(driverName, getClass().getClassLoader());
             //从配置中获取起始位置
             generalStartLocation = incrementConfig.getStartLocation();

File: flinkx-core/src/main/java/com/dtstack/flinkx/decoder/JsonDecoder.java
Patch:
@@ -17,7 +17,7 @@
  */
 package com.dtstack.flinkx.decoder;
 
-import org.codehaus.jackson.map.ObjectMapper;
+import com.fasterxml.jackson.databind.ObjectMapper;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 

File: flinkx-emqx/flinkx-emqx-writer/src/main/java/com/dtstack/flinkx/emqx/format/EmqxOutputFormat.java
Patch:
@@ -23,7 +23,7 @@
 import com.dtstack.flinkx.util.ExceptionUtil;
 import org.apache.commons.lang3.StringUtils;
 import org.apache.flink.types.Row;
-import org.codehaus.jackson.map.ObjectMapper;
+import com.fasterxml.jackson.databind.ObjectMapper;
 import org.eclipse.paho.client.mqttv3.MqttClient;
 import org.eclipse.paho.client.mqttv3.MqttConnectOptions;
 import org.eclipse.paho.client.mqttv3.MqttException;

File: flinkx-kb/flinkx-kb-writer/src/main/java/com/dtstack/flinkx/kafkabase/writer/KafkaBaseOutputFormat.java
Patch:
@@ -24,7 +24,7 @@
 import com.dtstack.flinkx.util.ExceptionUtil;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.types.Row;
-import org.codehaus.jackson.map.ObjectMapper;
+import com.fasterxml.jackson.databind.ObjectMapper;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 

File: flinkx-rdb/flinkx-rdb-reader/src/main/java/com.dtstack.flinkx.rdb.inputformat/JdbcInputFormat.java
Patch:
@@ -33,6 +33,7 @@
 import com.dtstack.flinkx.util.FileSystemUtil;
 import com.dtstack.flinkx.util.StringUtil;
 import com.dtstack.flinkx.util.UrlUtil;
+import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.gson.Gson;
 import org.apache.commons.lang3.StringUtils;
 import org.apache.flink.api.common.accumulators.LongMaximum;
@@ -43,7 +44,6 @@
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.fs.permission.FsPermission;
 import org.apache.hadoop.io.IOUtils;
-import org.codehaus.jackson.map.ObjectMapper;
 
 import java.io.IOException;
 import java.sql.Connection;

File: flinkx-gbase/flinkx-gbase-core/src/main/java/com/dtstack/flinkx/gbase/GbaseDatabaseMeta.java
Patch:
@@ -119,7 +119,7 @@ public String getRowNumColumn(String orderBy) {
 
     @Override
     public int getFetchSize(){
-        return 1000;
+        return Integer.MIN_VALUE;
     }
 
     @Override

File: flinkx-metadata-sqlserver/flinkx-metadata-sqlserver-reader/src/main/java/com/dtstack/flinkx/metadatasqlserver/inputformat/MetadatasqlserverInputFormat.java
Patch:
@@ -24,8 +24,8 @@
 import com.dtstack.flinkx.metadatasqlserver.constants.SqlServerMetadataCons;
 import com.dtstack.flinkx.util.ExceptionUtil;
 import com.dtstack.flinkx.util.StringUtil;
-import javafx.util.Pair;
 import org.apache.commons.lang.StringUtils;
+import org.apache.commons.lang3.tuple.Pair;
 import org.apache.flink.types.Row;
 
 import java.io.IOException;
@@ -55,7 +55,7 @@ protected List<Object> showTables() throws SQLException {
         List<Object> tableNameList = new LinkedList<>();
         try (ResultSet rs = statement.get().executeQuery(SqlServerMetadataCons.SQL_SHOW_TABLES)) {
             while (rs.next()) {
-                tableNameList.add(new Pair<>(rs.getString(1), rs.getString(2)));
+                tableNameList.add(Pair.of(rs.getString(1), rs.getString(2)));
             }
         }
         return tableNameList;

File: flinkx-metadata-sqlserver/flinkx-metadata-sqlserver-reader/src/main/java/com/dtstack/flinkx/metadatasqlserver/inputformat/MetadatasqlserverInputFormat.java
Patch:
@@ -24,8 +24,8 @@
 import com.dtstack.flinkx.metadatasqlserver.constants.SqlServerMetadataCons;
 import com.dtstack.flinkx.util.ExceptionUtil;
 import com.dtstack.flinkx.util.StringUtil;
-import javafx.util.Pair;
 import org.apache.commons.lang.StringUtils;
+import org.apache.commons.lang3.tuple.Pair;
 import org.apache.flink.types.Row;
 
 import java.io.IOException;
@@ -55,7 +55,7 @@ protected List<Object> showTables() throws SQLException {
         List<Object> tableNameList = new LinkedList<>();
         try (ResultSet rs = statement.get().executeQuery(SqlServerMetadataCons.SQL_SHOW_TABLES)) {
             while (rs.next()) {
-                tableNameList.add(new Pair<>(rs.getString(1), rs.getString(2)));
+                tableNameList.add(Pair.of(rs.getString(1), rs.getString(2)));
             }
         }
         return tableNameList;

File: flinkx-metadata-sqlserver/flinkx-metadata-sqlserver-reader/src/main/java/com/dtstack/flinkx/metadatasqlserver/inputformat/MetadatasqlserverInputFormat.java
Patch:
@@ -24,8 +24,8 @@
 import com.dtstack.flinkx.metadatasqlserver.constants.SqlServerMetadataCons;
 import com.dtstack.flinkx.util.ExceptionUtil;
 import com.dtstack.flinkx.util.StringUtil;
-import javafx.util.Pair;
 import org.apache.commons.lang.StringUtils;
+import org.apache.commons.lang3.tuple.Pair;
 import org.apache.flink.types.Row;
 
 import java.io.IOException;
@@ -55,7 +55,7 @@ protected List<Object> showTables() throws SQLException {
         List<Object> tableNameList = new LinkedList<>();
         try (ResultSet rs = statement.get().executeQuery(SqlServerMetadataCons.SQL_SHOW_TABLES)) {
             while (rs.next()) {
-                tableNameList.add(new Pair<>(rs.getString(1), rs.getString(2)));
+                tableNameList.add(Pair.of(rs.getString(1), rs.getString(2)));
             }
         }
         return tableNameList;

File: flinkx-metadata-sqlserver/flinkx-metadata-sqlserver-reader/src/main/java/com/dtstack/flinkx/metadatasqlserver/inputformat/MetadatasqlserverInputFormat.java
Patch:
@@ -24,8 +24,8 @@
 import com.dtstack.flinkx.metadatasqlserver.constants.SqlServerMetadataCons;
 import com.dtstack.flinkx.util.ExceptionUtil;
 import com.dtstack.flinkx.util.StringUtil;
-import javafx.util.Pair;
 import org.apache.commons.lang.StringUtils;
+import org.apache.commons.lang3.tuple.Pair;
 import org.apache.flink.types.Row;
 
 import java.io.IOException;
@@ -55,7 +55,7 @@ protected List<Object> showTables() throws SQLException {
         List<Object> tableNameList = new LinkedList<>();
         try (ResultSet rs = statement.get().executeQuery(SqlServerMetadataCons.SQL_SHOW_TABLES)) {
             while (rs.next()) {
-                tableNameList.add(new Pair<>(rs.getString(1), rs.getString(2)));
+                tableNameList.add(Pair.of(rs.getString(1), rs.getString(2)));
             }
         }
         return tableNameList;

File: flinkx-oraclelogminer/flinkx-oraclelogminer-reader/src/main/java/com/dtstack/flinkx/oraclelogminer/util/SqlUtil.java
Patch:
@@ -23,6 +23,7 @@
 
 import java.util.ArrayList;
 import java.util.Arrays;
+import java.util.Collections;
 import java.util.List;
 
 /**
@@ -185,11 +186,11 @@ public class SqlUtil {
 
     public final static String SQL_QUERY_ROLES = "SELECT * FROM USER_ROLE_PRIVS";
 
-    public final static String SQL_QUERY_PRIVILEGES = "select * from session_privs";
+    public final static String SQL_QUERY_PRIVILEGES = "SELECT * FROM SESSION_PRIVS";
 
     private final static List<String> SUPPORTED_OPERATIONS = Arrays.asList("UPDATE", "INSERT", "DELETE");
 
-    public static List<String> EXCLUDE_SCHEMAS = Arrays.asList("SYS");
+    public static List<String> EXCLUDE_SCHEMAS = Collections.singletonList("SYS");
 
     public static String buildSelectSql(String listenerOptions, String listenerTables){
         StringBuilder sqlBuilder = new StringBuilder(SQL_SELECT_DATA);

File: flinkx-gbase/flinkx-gbase-core/src/main/java/com/dtstack/flinkx/gbase/GbaseDatabaseMeta.java
Patch:
@@ -119,7 +119,7 @@ public String getRowNumColumn(String orderBy) {
 
     @Override
     public int getFetchSize(){
-        return 1000;
+        return Integer.MIN_VALUE;
     }
 
     @Override

File: flinkx-launcher/src/main/java/com/dtstack/flinkx/launcher/perjob/PerJobSubmitter.java
Patch:
@@ -18,6 +18,7 @@
 
 package com.dtstack.flinkx.launcher.perjob;
 
+import com.dtstack.flinkx.launcher.KerberosInfo;
 import com.dtstack.flinkx.options.Options;
 import com.dtstack.flinkx.util.MapUtil;
 import org.apache.commons.lang.StringUtils;
@@ -54,6 +55,7 @@ public static String submit(Options options, JobGraph jobGraph) throws Exception
         ClusterSpecification clusterSpecification = FlinkPerJobResourceUtil.createClusterSpecification(conProp);
         PerJobClusterClientBuilder perJobClusterClientBuilder = new PerJobClusterClientBuilder();
         Configuration config = StringUtils.isEmpty(options.getFlinkconf()) ? new Configuration() : GlobalConfiguration.loadConfiguration(options.getFlinkconf());
+        perJobClusterClientBuilder.setKerberosInfo(new KerberosInfo(options.getKrb5conf(),options.getKeytab(),options.getPrincipal(),config));
         perJobClusterClientBuilder.init(options.getYarnconf(), config, conProp);
 
         AbstractYarnClusterDescriptor descriptor = perJobClusterClientBuilder.createPerJobClusterDescriptor(conProp, options, jobGraph);

File: flinkx-metadata-hive2/flinkx-metadata-hive2-reader/src/main/java/com/dtstack/flinkx/metadatahive2/inputformat/Metadatahive2InputFormat.java
Patch:
@@ -93,8 +93,8 @@ protected void switchDatabase(String databaseName) throws SQLException {
     }
 
     @Override
-    protected List<String> showTables() throws SQLException {
-        List<String> tables = new ArrayList<>();
+    protected List<Object> showTables() throws SQLException {
+        List<Object> tables = new ArrayList<>();
         try (ResultSet rs = statement.get().executeQuery(SQL_SHOW_TABLES)) {
            int pos = rs.getMetaData().getColumnCount()==1?1:2;
             while (rs.next()) {

File: flinkx-metadata-oracle/flinkx-metadata-oracle-reader/src/main/java/com/dtstack/flinkx/metadataoracle/inputformat/MetadataoracleInputFormat.java
Patch:
@@ -55,8 +55,8 @@ public class MetadataoracleInputFormat extends BaseMetadataInputFormat {
     private static final long serialVersionUID = 1L;
 
     @Override
-    protected List<String> showTables() throws SQLException {
-        List<String> tableNameList = new LinkedList<>();
+    protected List<Object> showTables() throws SQLException {
+        List<Object> tableNameList = new LinkedList<>();
         String sql = String.format(SQL_SHOW_TABLES, quote(currentDb.get()));
         try (ResultSet rs = statement.get().executeQuery(sql)) {
             while (rs.next()) {

File: flinkx-metadata-tidb/flinkx-metadata-tidb-reader/src/main/java/com/dtstack/flinkx/metadatatidb/inputformat/MetadatatidbInputFormat.java
Patch:
@@ -43,8 +43,8 @@ public class MetadatatidbInputFormat extends BaseMetadataInputFormat {
     private static final long serialVersionUID = 1L;
 
     @Override
-    protected List<String> showTables() throws SQLException {
-        List<String> tables = new ArrayList<>();
+    protected List<Object> showTables() throws SQLException {
+        List<Object> tables = new ArrayList<>();
         try (ResultSet rs = statement.get().executeQuery(SQL_SHOW_TABLES)) {
             while (rs.next()) {
                 tables.add(rs.getString(1));

File: flinkx-metadata/flinkx-metadata-reader/src/main/java/com/dtstack/flinkx/metadata/inputformat/MetadataInputSplit.java
Patch:
@@ -37,9 +37,9 @@ public class MetadataInputSplit implements InputSplit {
 
     private String dbName;
 
-    private List<String> tableList;
+    private List<Object> tableList;
 
-    public MetadataInputSplit(int splitNumber, String dbName, List<String> tableList) {
+    public MetadataInputSplit(int splitNumber, String dbName, List<Object> tableList) {
         this.splitNumber = splitNumber;
         this.dbName = dbName;
         this.tableList = tableList;
@@ -49,7 +49,7 @@ public String getDbName() {
         return dbName;
     }
 
-    public List<String> getTableList() {
+    public List<Object> getTableList() {
         return tableList;
     }
 

File: flinkx-metadata-hive2/flinkx-metadata-hive2-reader/src/main/java/com/dtstack/flinkx/metadatahive2/inputformat/Metadatahive2InputFormat.java
Patch:
@@ -93,8 +93,8 @@ protected void switchDatabase(String databaseName) throws SQLException {
     }
 
     @Override
-    protected List<String> showTables() throws SQLException {
-        List<String> tables = new ArrayList<>();
+    protected List<Object> showTables() throws SQLException {
+        List<Object> tables = new ArrayList<>();
         try (ResultSet rs = statement.get().executeQuery(SQL_SHOW_TABLES)) {
            int pos = rs.getMetaData().getColumnCount()==1?1:2;
             while (rs.next()) {

File: flinkx-metadata-oracle/flinkx-metadata-oracle-reader/src/main/java/com/dtstack/flinkx/metadataoracle/inputformat/MetadataoracleInputFormat.java
Patch:
@@ -55,8 +55,8 @@ public class MetadataoracleInputFormat extends BaseMetadataInputFormat {
     private static final long serialVersionUID = 1L;
 
     @Override
-    protected List<String> showTables() throws SQLException {
-        List<String> tableNameList = new LinkedList<>();
+    protected List<Object> showTables() throws SQLException {
+        List<Object> tableNameList = new LinkedList<>();
         String sql = String.format(SQL_SHOW_TABLES, quote(currentDb.get()));
         try (ResultSet rs = statement.get().executeQuery(sql)) {
             while (rs.next()) {

File: flinkx-metadata-tidb/flinkx-metadata-tidb-reader/src/main/java/com/dtstack/flinkx/metadatatidb/inputformat/MetadatatidbInputFormat.java
Patch:
@@ -43,8 +43,8 @@ public class MetadatatidbInputFormat extends BaseMetadataInputFormat {
     private static final long serialVersionUID = 1L;
 
     @Override
-    protected List<String> showTables() throws SQLException {
-        List<String> tables = new ArrayList<>();
+    protected List<Object> showTables() throws SQLException {
+        List<Object> tables = new ArrayList<>();
         try (ResultSet rs = statement.get().executeQuery(SQL_SHOW_TABLES)) {
             while (rs.next()) {
                 tables.add(rs.getString(1));

File: flinkx-metadata/flinkx-metadata-reader/src/main/java/com/dtstack/flinkx/metadata/inputformat/MetadataInputSplit.java
Patch:
@@ -37,9 +37,9 @@ public class MetadataInputSplit implements InputSplit {
 
     private String dbName;
 
-    private List<String> tableList;
+    private List<Object> tableList;
 
-    public MetadataInputSplit(int splitNumber, String dbName, List<String> tableList) {
+    public MetadataInputSplit(int splitNumber, String dbName, List<Object> tableList) {
         this.splitNumber = splitNumber;
         this.dbName = dbName;
         this.tableList = tableList;
@@ -49,7 +49,7 @@ public String getDbName() {
         return dbName;
     }
 
-    public List<String> getTableList() {
+    public List<Object> getTableList() {
         return tableList;
     }
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/Main.java
Patch:
@@ -184,6 +184,8 @@ private static void speedTest(DataTransferConfig config) {
         } else if (WRITER.equalsIgnoreCase(testConfig.getSpeedTest())){
             ContentConfig contentConfig = config.getJob().getContent().get(0);
             contentConfig.getReader().setName(STREAM_READER);
+        }else {
+            return;
         }
 
         config.getJob().getSetting().getSpeed().setBytes(-1);

File: flinkx-core/src/main/java/com/dtstack/flinkx/reader/ByteRateLimiter.java
Patch:
@@ -87,7 +87,7 @@ private void updateRate(){
         BigDecimal thisWriteRatio = BigDecimal.valueOf(totalRecords == 0 ? 0 : thisRecords / (double) totalRecords);
 
         if (totalRecords > MIN_RECORD_NUMBER_UPDATE_RATE && totalBytes != 0
-                && thisWriteRatio.compareTo(new BigDecimal(0)) == 0) {
+                && thisWriteRatio.compareTo(BigDecimal.ZERO) != 0) {
             double bpr = totalBytes / (double)totalRecords;
             double permitsPerSecond = expectedBytePerSecond / bpr * thisWriteRatio.doubleValue();
             rateLimiter.setRate(permitsPerSecond);

File: flinkx-sqlserver/flinkx-sqlserver-core/src/main/java/com/dtstack/flinkx/sqlserver/SqlServerDatabaseMeta.java
Patch:
@@ -39,6 +39,8 @@
  */
 public class SqlServerDatabaseMeta extends BaseDatabaseMeta {
 
+    private static final long serialVersionUID = 1L;
+
     @Override
     public EDatabaseType getDatabaseType() {
         return EDatabaseType.SQLServer;

File: flinkx-hbase/flinkx-hbase-core/src/main/java/com/dtstack/flinkx/hbase/HbaseHelper.java
Patch:
@@ -55,7 +55,7 @@ public class HbaseHelper {
     private final static String KEY_HBASE_SECURITY_AUTHORIZATION = "hbase.security.authorization";
 
     public static org.apache.hadoop.hbase.client.Connection getHbaseConnection(Map<String,Object> hbaseConfigMap) {
-        Validate.isTrue(MapUtils.isEmpty(hbaseConfigMap), "hbaseConfig不能为空Map结构!");
+        Validate.isTrue(MapUtils.isNotEmpty(hbaseConfigMap), "hbaseConfig不能为空Map结构!");
 
         if(openKerberos(hbaseConfigMap)){
             return getConnectionWithKerberos(hbaseConfigMap);

File: flinkx-binlog/flinkx-binlog-reader/src/main/java/com/dtstack/flinkx/binlog/reader/BinlogInputFormat.java
Patch:
@@ -98,6 +98,7 @@ public void openInputFormat() throws IOException {
         super.openInputFormat();
 
         LOG.info("binlog configure...");
+        LOG.info("binlog FilterBefore:{},tableBefore: {}",binlogConfig.getFilter(),binlogConfig.getTable());
 
         if (StringUtils.isNotEmpty(binlogConfig.getCat())) {
             LOG.info("{}", categories);
@@ -150,6 +151,7 @@ public void openInputFormat() throws IOException {
                 //检验schema下任意一张表的权限
                 checkSourceAuthority(database, null);
             }
+            LOG.info("binlog FilterAfter:{},tableAfter: {}",binlogConfig.getFilter(),binlogConfig.getTable());
         }
     }
 
@@ -210,6 +212,7 @@ protected void openInternal(InputSplit inputSplit) throws IOException {
         }
 
         if (StringUtils.isNotEmpty(binlogConfig.getFilter())) {
+            LOG.info("binlogFilter最终值：{}",binlogConfig.getFilter());
             controller.setEventFilter(new AviaterRegexFilter(binlogConfig.getFilter()));
         }
 

File: flinkx-es/flinkx-es-writer/src/main/java/com/dtstack/flinkx/es/writer/EsOutputFormat.java
Patch:
@@ -119,8 +119,8 @@ private void processFailResponse(BulkResponse response){
                     dirtyDataManager.writeData(rows.get(i), exception);
                 }
 
-                if(numWriteCounter != null ){
-                    numWriteCounter.add(1);
+                if (errCounter != null) {
+                    errCounter.add(1);
                 }
             }
         }

File: flinkx-metadata-hive1/flinkx-metadata-hive1-reader/src/main/java/com/dtstack/flinkx/metadatahive1/reader/Metadatahive1Reader.java
Patch:
@@ -6,7 +6,7 @@
 
 public class Metadatahive1Reader extends Metadatahive2Reader {
 
-    public static final String DRIVER_NAME = "shade.hive1.jdbc.HiveDriver";
+    public static final String DRIVER_NAME = "org.apache.hive.jdbc.HiveDriver";
 
     public Metadatahive1Reader(DataTransferConfig config, StreamExecutionEnvironment env) {
         super(config, env);

File: flinkx-metadata-hive2/flinkx-metadata-hive2-reader/src/main/java/com/dtstack/flinkx/metadatahive2/constants/Hive2MetaDataCons.java
Patch:
@@ -26,7 +26,7 @@
  */
 @SuppressWarnings("all")
 public class Hive2MetaDataCons extends MetaDataCons {
-    public static final String DRIVER_NAME = "shade.hive2.HiveDriver";
+    public static final String DRIVER_NAME = "org.apache.hive.jdbc.HiveDriver";
     public static final String KEY_HADOOP_CONFIG = "hadoopConfig";
 
     public static final String KEY_SOURCE = "source";

File: flinkx-metadata-oracle/flinkx-metadata-oracle-reader/src/main/java/com/dtstack/flinkx/metadataoracle/constants/OracleMetaDataCons.java
Patch:
@@ -36,8 +36,6 @@ public class OracleMetaDataCons extends MetaDataCons {
     public static final String KEY_TOTAL_SIZE = "totalSize";
     public static final String KEY_INDEX_COLUMN_NAME = "columnName";
 
-    /** 统一采用DBA表
-     */
     public static final String SQL_QUERY_INDEX = "SELECT COLUMNS.INDEX_NAME, COLUMNS.COLUMN_NAME, INDEXES.INDEX_TYPE  FROM DBA_IND_COLUMNS COLUMNS JOIN DBA_INDEXES INDEXES \n" +
             "ON COLUMNS.INDEX_NAME = INDEXES.INDEX_NAME AND COLUMNS.TABLE_NAME = INDEXES.TABLE_NAME AND COLUMNS.TABLE_OWNER = INDEXES.TABLE_OWNER \n" +
             "WHERE COLUMNS.TABLE_OWNER = %s AND COLUMNS.TABLE_NAME = %s";
@@ -48,6 +46,5 @@ public class OracleMetaDataCons extends MetaDataCons {
             "FROM DBA_TABLES TABLES JOIN DBA_OBJECTS OBJS ON TABLES.OWNER = OBJS.OWNER AND TABLES.TABLE_NAME = OBJS.OBJECT_NAME\n" +
             "JOIN DBA_TAB_COMMENTS COMMENTS ON TABLES.OWNER = COMMENTS.OWNER AND TABLES.TABLE_NAME = COMMENTS.TABLE_NAME\n" +
             "WHERE TABLES.OWNER = %s AND TABLES.TABLE_NAME = %s";
-    public static final String SQL_SHOW_DATABASES = "SELECT USERNAME FROM DBA_USERS";
     public static final String SQL_SHOW_TABLES = "SELECT TABLE_NAME FROM DBA_TABLES WHERE OWNER = %s";
 }
\ No newline at end of file

File: flinkx-metadata/flinkx-metadata-core/src/main/java/com/dtstack/flinkx/metadata/MetaDataCons.java
Patch:
@@ -31,6 +31,8 @@ public class MetaDataCons {
     public static final String KEY_DB_NAME = "dbName";
     public static final String KEY_TABLE_LIST = "tableList";
 
+    public static final String KEY_TOTAL_TABLE = "totalTable";
+    public static final String KEY_RESOLVED_TABLE = "resolvedTable";
     public static final String KEY_SCHEMA = "schema";
     public static final String KEY_COLUMN = "column";
     public static final String KEY_STORED_TYPE = "storedType";

File: flinkx-rdb/flinkx-rdb-core/src/main/java/com/dtstack/flinkx/rdb/util/DbUtil.java
Patch:
@@ -143,7 +143,7 @@ public static Connection getConnection(String url, String username, String passw
                 try {
                     dbConn = getConnectionInternal(url, username, password);
                     try (Statement statement = dbConn.createStatement()){
-                        statement.execute("select 111");
+                        statement.execute("SELECT 1 FROM dual");
                         failed = false;
                     }
                 } catch (Exception e) {

File: flinkx-test/src/main/java/com/dtstack/flinkx/test/LocalTest.java
Patch:
@@ -137,7 +137,7 @@ public static void main(String[] args) throws Exception{
 //        conf.setString("metrics.reporter.promgateway.randomJobNameSuffix","true");
 //        conf.setString("metrics.reporter.promgateway.deleteOnShutdown","true");
 
-        String jobPath = "D:\\dtstack\\flinkx-all\\flinkx-test\\src\\main\\resources\\dev_test_job\\metadatahive1kbr_stream.json";
+        String jobPath = "D:\\dtstack\\flinkx-all\\flinkx-test\\src\\main\\resources\\dev_test_job\\metadataoracle_stream.json";
         JobExecutionResult result = LocalTest.runJob(new File(jobPath), confProperties, null);
         ResultPrintUtil.printResult(result);
     }

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsOrcOutputFormat.java
Patch:
@@ -50,6 +50,7 @@
 import java.sql.Timestamp;
 import java.text.SimpleDateFormat;
 import java.util.ArrayList;
+import java.util.Arrays;
 import java.util.HashMap;
 import java.util.List;
 
@@ -201,7 +202,7 @@ private void getData(List<Object> recordList, int index, Row row) throws WriteRe
 
         ColumnType columnType = ColumnType.fromString(columnTypes.get(j));
         String rowData = column.toString();
-        if(rowData == null || rowData.length() == 0){
+        if(rowData == null || (rowData.length() == 0 && !ColumnType.isStringType(columnType)) ){
             recordList.add(null);
             return;
         }

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsParquetOutputFormat.java
Patch:
@@ -155,7 +155,7 @@ public void writeSingleRecordToFile(Row row) throws WriteRecordException {
                 int colIndex = colIndices[i];
                 if(colIndex > -1){
                     Object valObj = row.getField(colIndex);
-                    if(valObj == null || valObj.toString().length() == 0){
+                    if(valObj == null || (valObj.toString().length() == 0 && !ColumnType.isStringType(fullColumnTypes.get(i)))){
                         continue;
                     }
 

File: flinkx-test/src/main/java/com/dtstack/flinkx/test/LocalTest.java
Patch:
@@ -71,6 +71,8 @@
 import com.dtstack.flinkx.oracle.reader.OracleReader;
 import com.dtstack.flinkx.oracle.writer.OracleWriter;
 import com.dtstack.flinkx.oraclelogminer.reader.OraclelogminerReader;
+//import com.dtstack.flinkx.phoenix.reader.PhoenixReader;
+//import com.dtstack.flinkx.phoenix.writer.PhoenixWriter;
 import com.dtstack.flinkx.phoenix5.reader.Phoenix5Reader;
 import com.dtstack.flinkx.phoenix5.writer.Phoenix5Writer;
 import com.dtstack.flinkx.polardb.reader.PolardbReader;

File: flinkx-binlog/flinkx-binlog-reader/src/main/java/com/dtstack/flinkx/binlog/reader/BinlogInputFormat.java
Patch:
@@ -145,7 +145,7 @@ public void openInputFormat() throws IOException {
                 //检验每个schema下的第一个表的权限
                 checkSourceAuthority(null, checkedTable.values());
             } else if (StringUtils.isBlank(binlogConfig.getFilter())) {
-                //如果table未指定 filter未指定 只消费此schema下的数据
+                //如果table未指定  filter未指定 只消费此schema下的数据
                 binlogConfig.setFilter(database + "\\..*");
                 //检验schema下任意一张表的权限
                 checkSourceAuthority(database, null);

File: flinkx-hbase/flinkx-hbase-core/src/main/java/com/dtstack/flinkx/hbase/HbaseHelper.java
Patch:
@@ -55,7 +55,7 @@ public class HbaseHelper {
     private final static String KEY_HBASE_SECURITY_AUTHORIZATION = "hbase.security.authorization";
 
     public static org.apache.hadoop.hbase.client.Connection getHbaseConnection(Map<String,Object> hbaseConfigMap) {
-        Validate.isTrue(MapUtils.isEmpty(hbaseConfigMap), "hbaseConfig不能为空Map结构!");
+        Validate.isTrue(MapUtils.isNotEmpty(hbaseConfigMap), "hbaseConfig不能为空Map结构!");
 
         if(openKerberos(hbaseConfigMap)){
             return getConnectionWithKerberos(hbaseConfigMap);

File: flinkx-core/src/main/java/com/dtstack/flinkx/config/JobConfig.java
Patch:
@@ -18,6 +18,7 @@
 
 package com.dtstack.flinkx.config;
 
+import com.dtstack.flinkx.util.GsonUtil;
 import com.dtstack.flinkx.util.MapUtil;
 import org.apache.commons.collections.CollectionUtils;
 import org.slf4j.Logger;
@@ -74,7 +75,7 @@ public JobConfig(Map<String, Object> map) {
                 }
             }
         }
-        LOG.info("configInfo : {}", map.toString());
+        LOG.info("configInfo : {}", GsonUtil.GSON.toJson(map));
     }
 
     public SettingConfig getSetting() {

File: flinkx-core/src/main/java/com/dtstack/flinkx/decoder/JsonDecoder.java
Patch:
@@ -17,7 +17,7 @@
  */
 package com.dtstack.flinkx.decoder;
 
-import org.codehaus.jackson.map.ObjectMapper;
+import com.fasterxml.jackson.databind.ObjectMapper;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 

File: flinkx-emqx/flinkx-emqx-writer/src/main/java/com/dtstack/flinkx/emqx/format/EmqxOutputFormat.java
Patch:
@@ -23,7 +23,7 @@
 import com.dtstack.flinkx.util.ExceptionUtil;
 import org.apache.commons.lang3.StringUtils;
 import org.apache.flink.types.Row;
-import org.codehaus.jackson.map.ObjectMapper;
+import com.fasterxml.jackson.databind.ObjectMapper;
 import org.eclipse.paho.client.mqttv3.MqttClient;
 import org.eclipse.paho.client.mqttv3.MqttConnectOptions;
 import org.eclipse.paho.client.mqttv3.MqttException;

File: flinkx-kb/flinkx-kb-writer/src/main/java/com/dtstack/flinkx/kafkabase/writer/KafkaBaseOutputFormat.java
Patch:
@@ -24,7 +24,7 @@
 import com.dtstack.flinkx.util.ExceptionUtil;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.types.Row;
-import org.codehaus.jackson.map.ObjectMapper;
+import com.fasterxml.jackson.databind.ObjectMapper;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 

File: flinkx-rdb/flinkx-rdb-reader/src/main/java/com.dtstack.flinkx.rdb.inputformat/JdbcInputFormat.java
Patch:
@@ -34,6 +34,7 @@
 import com.dtstack.flinkx.util.FileSystemUtil;
 import com.dtstack.flinkx.util.StringUtil;
 import com.dtstack.flinkx.util.UrlUtil;
+import com.fasterxml.jackson.databind.ObjectMapper;
 import com.google.gson.Gson;
 import org.apache.commons.lang3.StringUtils;
 import org.apache.flink.api.common.accumulators.LongMaximum;
@@ -44,7 +45,6 @@
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.fs.permission.FsPermission;
 import org.apache.hadoop.io.IOUtils;
-import org.codehaus.jackson.map.ObjectMapper;
 
 import java.io.IOException;
 import java.sql.Connection;

File: flinkx-metadata-sqlserver/flinkx-metadata-sqlserver-reader/src/main/java/com/dtstack/flinkx/metadatasqlserver/constants/SqlServerMetadataCons.java
Patch:
@@ -62,7 +62,7 @@ public class SqlServerMetadataCons extends MetaDataCons {
             "FROM sys.indexes a JOIN sysindexkeys b ON a.object_id=b.id AND a.index_id=b.indid \n" +
             "JOIN sysobjects c ON b.id=c.id JOIN syscolumns d ON b.id=d.id AND b.colid=d.colid \n" +
             "WHERE c.name=%s and OBJECT_SCHEMA_NAME(A.object_id, DB_ID())=%s \n" +
-            "AND  a.index_id = 0";
+            "AND  a.type in (0, 1)";
 
     public static final String SQL_SHOW_PARTITION = "select ps.name, p.rows, pf.create_date, ds2.name as filegroup \n" +
             "from sys.indexes i join sys.partition_schemes ps on i.data_space_id = ps.data_space_id \n" +

File: flinkx-test/src/main/java/com/dtstack/flinkx/test/LocalTest.java
Patch:
@@ -46,6 +46,7 @@
 import com.dtstack.flinkx.hbase.writer.HbaseWriter;
 import com.dtstack.flinkx.hdfs.reader.HdfsReader;
 import com.dtstack.flinkx.hdfs.writer.HdfsWriter;
+import com.dtstack.flinkx.hive.writer.HiveWriter;
 import com.dtstack.flinkx.kafka.reader.KafkaReader;
 import com.dtstack.flinkx.kafka.writer.KafkaWriter;
 import com.dtstack.flinkx.kafka09.reader.Kafka09Reader;
@@ -70,8 +71,6 @@
 import com.dtstack.flinkx.oracle.reader.OracleReader;
 import com.dtstack.flinkx.oracle.writer.OracleWriter;
 import com.dtstack.flinkx.oraclelogminer.reader.OraclelogminerReader;
-//import com.dtstack.flinkx.phoenix.reader.PhoenixReader;
-//import com.dtstack.flinkx.phoenix.writer.PhoenixWriter;
 import com.dtstack.flinkx.phoenix5.reader.Phoenix5Reader;
 import com.dtstack.flinkx.phoenix5.writer.Phoenix5Writer;
 import com.dtstack.flinkx.polardb.reader.PolardbReader;
@@ -259,7 +258,7 @@ private static BaseDataWriter buildDataWriter(DataTransferConfig config){
             case PluginNameConstrant.MONGODB_WRITER : writer = new MongodbWriter(config); break;
             case PluginNameConstrant.ODPS_WRITER : writer = new OdpsWriter(config); break;
             case PluginNameConstrant.REDIS_WRITER : writer = new RedisWriter(config); break;
-//            case PluginNameConstrant.HIVE_WRITER : writer = new HiveWriter(config); break;
+            case PluginNameConstrant.HIVE_WRITER : writer = new HiveWriter(config); break;
             case PluginNameConstrant.KAFKA09_WRITER : writer = new Kafka09Writer(config); break;
             case PluginNameConstrant.KAFKA10_WRITER : writer = new Kafka10Writer(config); break;
             case PluginNameConstrant.KAFKA11_WRITER : writer = new Kafka11Writer(config); break;

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/RetryUtil.java
Patch:
@@ -74,7 +74,7 @@ public <T> T doRetry(Callable<T> callable, int retryTimes, long sleepTimeInMilli
                 } catch (Exception e) {
                     saveException = e;
                     if (i == 0) {
-                        LOG.error(String.format("Exception when calling callable, 异常Msg:%s", saveException.getMessage()), saveException);
+                        LOG.error(String.format("Exception when calling callable, 异常Msg:%s", ExceptionUtil.getErrorMessage(saveException)), saveException);
                     }
 
                     if (null != retryExceptionClasss && !retryExceptionClasss.isEmpty()) {

File: flinkx-rdb/flinkx-rdb-reader/src/main/java/com.dtstack.flinkx.rdb.inputformat/JdbcInputFormat.java
Patch:
@@ -429,7 +429,7 @@ public boolean isReturn() {
 
             @Override
             public void processError(Exception e) {
-                LOG.warn("", e);
+                LOG.warn(ExceptionUtil.getErrorMessage(e));
             }
         });
 

File: flinkx-gbase/flinkx-gbase-reader/src/main/java/com/dtstack/flinkx/gbase/format/GbaseInputFormat.java
Patch:
@@ -27,6 +27,7 @@
 import org.apache.flink.types.Row;
 
 import java.io.IOException;
+import java.math.BigInteger;
 import java.sql.SQLException;
 import java.util.ArrayList;
 
@@ -51,7 +52,7 @@ public void openInternal(InputSplit inputSplit) throws IOException {
             String startLocation = incrementConfig.getStartLocation();
             if (incrementConfig.isPolling()) {
                 if (StringUtils.isNotEmpty(startLocation)) {
-                    endLocationAccumulator.add(Long.parseLong(startLocation));
+                    endLocationAccumulator.add(new BigInteger(startLocation));
                 }
                 isTimestamp = "timestamp".equalsIgnoreCase(incrementConfig.getColumnType());
             } else if ((incrementConfig.isIncrement() && incrementConfig.isUseMaxFunc())) {

File: flinkx-mysql/flinkx-mysql-reader/src/main/java/com/dtstack/flinkx/mysql/format/MysqlInputFormat.java
Patch:
@@ -29,6 +29,7 @@
 import org.apache.flink.types.Row;
 
 import java.io.IOException;
+import java.math.BigInteger;
 import java.sql.SQLException;
 import java.util.ArrayList;
 
@@ -53,7 +54,7 @@ public void openInternal(InputSplit inputSplit) throws IOException {
             String startLocation = incrementConfig.getStartLocation();
             if (incrementConfig.isPolling()) {
                 if (StringUtils.isNotEmpty(startLocation)) {
-                    endLocationAccumulator.add(Long.parseLong(startLocation));
+                    endLocationAccumulator.add(new BigInteger(startLocation));
                 }
                 isTimestamp = "timestamp".equalsIgnoreCase(incrementConfig.getColumnType());
             } else if ((incrementConfig.isIncrement() && incrementConfig.isUseMaxFunc())) {

File: flinkx-postgresql/flinkx-postgresql-reader/src/main/java/com/dtstack/flinkx/postgresql/format/PostgresqlInputFormat.java
Patch:
@@ -29,6 +29,7 @@
 import org.apache.flink.types.Row;
 
 import java.io.IOException;
+import java.math.BigInteger;
 import java.sql.SQLException;
 import java.util.ArrayList;
 
@@ -53,7 +54,7 @@ public void openInternal(InputSplit inputSplit) throws IOException {
             String startLocation = incrementConfig.getStartLocation();
             if (incrementConfig.isPolling()) {
                 if (StringUtils.isNotEmpty(startLocation)) {
-                    endLocationAccumulator.add(Long.parseLong(startLocation));
+                    endLocationAccumulator.add(new BigInteger(startLocation));
                 }
                 isTimestamp = "timestamp".equalsIgnoreCase(incrementConfig.getColumnType());
             } else if ((incrementConfig.isIncrement() && incrementConfig.isUseMaxFunc())) {

File: flinkx-hdfs/flinkx-hdfs-core/src/main/java/com/dtstack/flinkx/hdfs/HdfsUtil.java
Patch:
@@ -40,7 +40,7 @@ public class HdfsUtil {
     public static final String NULL_VALUE = "\\N";
 
     public static Object string2col(String str, String type, SimpleDateFormat customDateFormat) {
-        if (str == null || str.length() == 0){
+        if (str == null){
             return null;
         }
 

File: flinkx-hdfs/flinkx-hdfs-reader/src/main/java/com/dtstack/flinkx/hdfs/reader/HdfsTextInputFormat.java
Patch:
@@ -22,6 +22,7 @@
 import com.dtstack.flinkx.reader.MetaColumn;
 import jodd.util.StringUtil;
 import org.apache.commons.io.output.ByteArrayOutputStream;
+import org.apache.commons.lang3.StringUtils;
 import org.apache.flink.core.io.InputSplit;
 import org.apache.flink.types.Row;
 import org.apache.hadoop.fs.Path;
@@ -85,7 +86,7 @@ public void openInternal(InputSplit inputSplit) throws IOException {
     @Override
     public Row nextRecordInternal(Row row) throws IOException {
         String line = new String(((Text)value).getBytes(), 0, ((Text)value).getLength(), charsetName);
-        String[] fields = line.split(delimiter);
+        String[] fields = StringUtils.splitPreserveAllTokens(line, delimiter);
 
         if (metaColumns.size() == 1 && "*".equals(metaColumns.get(0).getName())){
             row = new Row(fields.length);

File: flinkx-mongodb/flinkx-mongodb-core/src/main/java/com/dtstack/flinkx/mongodb/MongodbConfigKeys.java
Patch:
@@ -57,4 +57,6 @@ public class MongodbConfigKeys {
     public final static String KEY_MAX_WAIT_TIME = "maxWaitTime";
 
     public final static String KEY_SOCKET_TIMEOUT = "socketTimeout";
+
+    public final static String KEY_SERVER_SELECTION_TIMEOUT = "serverSelectionTimeout";
 }

File: flinkx-rdb/flinkx-rdb-reader/src/main/java/com.dtstack.flinkx.rdb.inputformat/DistributedJdbcInputFormat.java
Patch:
@@ -206,7 +206,7 @@ protected void closeCurrentSource(){
 
     @Override
     protected void closeInternal() throws IOException {
-
+        closeCurrentSource();
     }
 
     @Override

File: flinkx-test/src/main/java/com/dtstack/flinkx/test/LocalTest.java
Patch:
@@ -46,6 +46,7 @@
 import com.dtstack.flinkx.hbase.writer.HbaseWriter;
 import com.dtstack.flinkx.hdfs.reader.HdfsReader;
 import com.dtstack.flinkx.hdfs.writer.HdfsWriter;
+import com.dtstack.flinkx.hive.writer.HiveWriter;
 import com.dtstack.flinkx.kafka.reader.KafkaReader;
 import com.dtstack.flinkx.kafka.writer.KafkaWriter;
 import com.dtstack.flinkx.kafka09.reader.Kafka09Reader;
@@ -70,8 +71,6 @@
 import com.dtstack.flinkx.oracle.reader.OracleReader;
 import com.dtstack.flinkx.oracle.writer.OracleWriter;
 import com.dtstack.flinkx.oraclelogminer.reader.OraclelogminerReader;
-//import com.dtstack.flinkx.phoenix.reader.PhoenixReader;
-//import com.dtstack.flinkx.phoenix.writer.PhoenixWriter;
 import com.dtstack.flinkx.phoenix5.reader.Phoenix5Reader;
 import com.dtstack.flinkx.phoenix5.writer.Phoenix5Writer;
 import com.dtstack.flinkx.polardb.reader.PolardbReader;
@@ -259,7 +258,7 @@ private static BaseDataWriter buildDataWriter(DataTransferConfig config){
             case PluginNameConstrant.MONGODB_WRITER : writer = new MongodbWriter(config); break;
             case PluginNameConstrant.ODPS_WRITER : writer = new OdpsWriter(config); break;
             case PluginNameConstrant.REDIS_WRITER : writer = new RedisWriter(config); break;
-//            case PluginNameConstrant.HIVE_WRITER : writer = new HiveWriter(config); break;
+            case PluginNameConstrant.HIVE_WRITER : writer = new HiveWriter(config); break;
             case PluginNameConstrant.KAFKA09_WRITER : writer = new Kafka09Writer(config); break;
             case PluginNameConstrant.KAFKA10_WRITER : writer = new Kafka10Writer(config); break;
             case PluginNameConstrant.KAFKA11_WRITER : writer = new Kafka11Writer(config); break;

File: flinkx-mongodb/flinkx-mongodb-core/src/main/java/com/dtstack/flinkx/mongodb/MongodbConfig.java
Patch:
@@ -19,6 +19,8 @@
 
 package com.dtstack.flinkx.mongodb;
 
+import com.mongodb.AuthenticationMechanism;
+
 import java.io.Serializable;
 import java.util.List;
 
@@ -36,7 +38,7 @@ public class MongodbConfig implements Serializable {
 
     private String password;
 
-    private String authenticationMechanism;
+    private String authenticationMechanism = AuthenticationMechanism.SCRAM_SHA_1.getMechanismName();
 
     private String database;
 

File: flinkx-rdb/flinkx-rdb-core/src/main/java/com/dtstack/flinkx/rdb/util/DbUtil.java
Patch:
@@ -143,7 +143,7 @@ public static Connection getConnection(String url, String username, String passw
                 try {
                     dbConn = getConnectionInternal(url, username, password);
                     try (Statement statement = dbConn.createStatement()){
-                        statement.execute("select 111");
+                        statement.execute("SELECT 1 FROM dual");
                         failed = false;
                     }
                 } catch (Exception e) {

File: flinkx-core/src/main/java/com/dtstack/flinkx/outputformat/BaseRichOutputFormat.java
Patch:
@@ -226,6 +226,7 @@ public void open(int taskNumber, int numTasks) throws IOException {
             waitWhile("#1");
         }
 
+
         openInternal(taskNumber, numTasks);
         if(needWaitBeforeWriteRecords()) {
             beforeWriteRecords();

File: flinkx-phoenix/flinkx-phoenix-core/src/main/java/com/dtstack/flinkx/phoenix/PhoenixMeta.java
Patch:
@@ -89,12 +89,13 @@ public String getUpsertStatement(List<String> column, String table, Map<String,L
 
     @Override
     public String getSplitFilter(String columnName) {
-        return String.format("%s mod ${N} = ${M}", getStartQuote() + columnName + getEndQuote());
+        // phoenix不支持mod，只支持%取余
+        return String.format("%s %% ${N} = ${M}", getStartQuote() + columnName + getEndQuote());
     }
 
     @Override
     public String getSplitFilterWithTmpTable(String tmpTable, String columnName){
-        return String.format("%s.%s mod ${N} = ${M}", tmpTable, getStartQuote() + columnName + getEndQuote());
+        return String.format("%s.%s %% ${N} = ${M}", tmpTable, getStartQuote() + columnName + getEndQuote());
     }
 
     @Override

File: flinkx-test/src/main/java/com/dtstack/flinkx/test/PluginNameConstrant.java
Patch:
@@ -56,6 +56,7 @@ public class PluginNameConstrant {
     public static final String METADATAORACLE_READER = "metadataoraclereader";
     public static final String METADATAMYSQL_READER = "metadatamysqlreader";
     public static final String GREENPLUM_READER = "greenplumreader";
+    public static final String PHOENIX5_READER = "phoenix5reader";
 
     public static final String STREAM_WRITER = "streamwriter";
     public static final String CARBONDATA_WRITER = "carbondatawriter";
@@ -85,4 +86,5 @@ public class PluginNameConstrant {
     public static final String RESTAPI_WRITER = "restapiwriter";
     public static final String DM_WRITER = "dmwriter";
     public static final String GREENPLUM_WRITER = "greenplumwriter";
+    public static final String PHOENIX5_WRITER = "phoenix5writer";
 }

File: flinkx-test/src/main/java/com/dtstack/flinkx/test/LocalTest.java
Patch:
@@ -122,7 +122,7 @@ public class LocalTest {
     public static Configuration conf = new Configuration();
 
     public static void main(String[] args) throws Exception{
-        setLogLevel(Level.DEBUG.toString());
+        setLogLevel(Level.INFO.toString());
         Properties confProperties = new Properties();
 //        confProperties.put("flink.checkpoint.interval", "10000");
 //        confProperties.put("flink.checkpoint.stateBackend", "file:///tmp/flinkx_checkpoint");

File: flinkx-metadata-tidb/flinkx-metadata-tidb-reader/src/main/java/com/dtstack/flinkx/metadatatidb/inputformat/MetadatatidbInputFormat.java
Patch:
@@ -100,7 +100,7 @@ protected Map<String, Object> queryMetaData(String tableName) throws SQLExceptio
         column.removeIf((Map<String, Object> perColumn)->{
             for(Map<String, Object> perPartitionColumn : partitionColumn){
                 if(StringUtils.equals((String)perPartitionColumn.get(KEY_COLUMN_NAME), (String)perColumn.get(KEY_COLUMN_NAME))){
-                    perPartitionColumn.put(KEY_COLUMN_TYPE, perColumn.get(RESULT_TYPE));
+                    perPartitionColumn.put(KEY_COLUMN_TYPE, perColumn.get(KEY_COLUMN_TYPE));
                     perPartitionColumn.put(KEY_NULL, perColumn.get(KEY_NULL));
                     perPartitionColumn.put(KEY_DEFAULT, perColumn.get(KEY_DEFAULT));
                     perPartitionColumn.put(KEY_COLUMN_COMMENT, perColumn.get(KEY_COLUMN_COMMENT));

File: flinkx-binlog/flinkx-binlog-core/src/main/java/com/dtstack/flinkx/binlog/BinlogJournalValidator.java
Patch:
@@ -28,7 +28,9 @@
 import java.util.ArrayList;
 import java.util.List;
 
-
+/**
+ * @author toutian
+ */
 public class BinlogJournalValidator {
 
     private static final Logger LOG = LoggerFactory.getLogger(BinlogJournalValidator.class);

File: flinkx-binlog/flinkx-binlog-core/src/main/java/com/google/common/collect/MapMakerHelper.java
Patch:
@@ -23,6 +23,9 @@
 import java.lang.reflect.Method;
 import java.util.concurrent.ConcurrentMap;
 
+/**
+ * @author toutian
+ */
 public class MapMakerHelper {
 
     public static MapMaker softValues(MapMaker mapMaker) {

File: flinkx-binlog/flinkx-binlog-core/src/main/java/com/google/common/collect/MigrateMap.java
Patch:
@@ -21,6 +21,9 @@
 
 import java.util.concurrent.ConcurrentMap;
 
+/**
+ * @author toutian
+ */
 public class MigrateMap {
 
     @SuppressWarnings("deprecation")

File: flinkx-binlog/flinkx-binlog-reader/src/main/java/com/dtstack/flinkx/binlog/reader/BinlogPositionManager.java
Patch:
@@ -26,6 +26,9 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+/**
+ * @author toutian
+ */
 public class BinlogPositionManager extends AbstractLogPositionManager {
 
     private static final Logger LOG = LoggerFactory.getLogger(BinlogPositionManager.class);

File: flinkx-carbondata/flinkx-carbondata-core/src/main/java/com/dtstack/flinkx/carbondata/CarbondataUtil.java
Patch:
@@ -53,7 +53,7 @@ public static CarbonTable buildCarbonTable(String dbName, String tableName, Stri
         return CarbonTable.buildFromTableInfo(wrapperTableInfo);
     }
 
-    public static void initFileFactory(Map<String,String> hadoopConfig, String defaultFS) {
+    public static void initFileFactory(Map<String,String> hadoopConfig, String defaultFs) {
         Configuration conf = new Configuration();
         conf.clear();
         if(hadoopConfig != null) {
@@ -62,8 +62,8 @@ public static void initFileFactory(Map<String,String> hadoopConfig, String defau
             }
         }
 
-        if(StringUtils.isNotBlank(defaultFS)) {
-            conf.set("fs.default.name", defaultFS);
+        if(StringUtils.isNotBlank(defaultFs)) {
+            conf.set("fs.default.name", defaultFs);
         }
         conf.set("fs.hdfs.impl.disable.cache", "true");
 

File: flinkx-carbondata/flinkx-carbondata-writer/src/main/java/com/dtstack/flinkx/carbondata/writer/dict/CarbonDictionaryUtil.java
Patch:
@@ -52,7 +52,7 @@
 
 
 /**
- * A object which provide a method to generate global dictionary from CSV files.
+ * @author demotto
  */
 public class CarbonDictionaryUtil {
 

File: flinkx-carbondata/flinkx-carbondata-writer/src/main/java/com/dtstack/flinkx/carbondata/writer/dict/DictionaryDetailHelper.java
Patch:
@@ -54,7 +54,7 @@ public boolean accept(CarbonFile pathname) {
             }
         });
         // 2 put dictionary file names to fileNamesMap
-        Map<String, Integer> fileNamesMap = new HashMap<>();
+        Map<String, Integer> fileNamesMap = new HashMap<>(carbonFiles.length);
         for(int i = 0; i < carbonFiles.length; ++i) {
             fileNamesMap.put(carbonFiles[i].getName(), i);
         }

File: flinkx-carbondata/flinkx-carbondata-writer/src/main/java/com/dtstack/flinkx/carbondata/writer/recordwriter/CarbonPartitionRecordWriter.java
Patch:
@@ -21,10 +21,7 @@
 
 
 import com.dtstack.flinkx.carbondata.writer.dict.CarbonTypeConverter;
-import com.dtstack.flinkx.util.DateUtil;
-import com.dtstack.flinkx.util.StringUtil;
 import org.apache.carbondata.core.metadata.datatype.DataType;
-import org.apache.carbondata.core.metadata.datatype.DataTypes;
 import org.apache.carbondata.core.metadata.schema.PartitionInfo;
 import org.apache.carbondata.core.metadata.schema.partition.PartitionType;
 import org.apache.carbondata.core.metadata.schema.table.CarbonTable;

File: flinkx-clickhouse/flinkx-clickhouse-core/src/main/java/com/dtstack/flinkx/clickhouse/core/ClickhouseDatabaseMeta.java
Patch:
@@ -41,12 +41,12 @@ public String getDriverClass() {
     }
 
     @Override
-    public String getSQLQueryFields(String tableName) {
+    public String getSqlQueryFields(String tableName) {
         return "SELECT * FROM " + tableName + " LIMIT 1";
     }
 
     @Override
-    public String getSQLQueryColumnFields(List<String> column, String table) {
+    public String getSqlQueryColumnFields(List<String> column, String table) {
         return "SELECT " + quoteColumns(column) + " FROM " + quoteTable(table) + " LIMIT 1";
     }
 

File: flinkx-clickhouse/flinkx-clickhouse-writer/src/main/java/com/dtstack/flinkx/clickhouse/format/ClickhouseOutputFormat.java
Patch:
@@ -19,7 +19,7 @@
 
 import com.dtstack.flinkx.clickhouse.core.ClickhouseUtil;
 import com.dtstack.flinkx.rdb.outputformat.JdbcOutputFormat;
-import com.dtstack.flinkx.rdb.util.DBUtil;
+import com.dtstack.flinkx.rdb.util.DbUtil;
 import com.dtstack.flinkx.util.ClassUtil;
 import com.dtstack.flinkx.util.ExceptionUtil;
 import org.apache.commons.collections.CollectionUtils;
@@ -46,7 +46,7 @@ public class ClickhouseOutputFormat extends JdbcOutputFormat {
     protected void openInternal(int taskNumber, int numTasks) {
         try {
             ClassUtil.forName(driverName, getClass().getClassLoader());
-            dbConn = ClickhouseUtil.getConnection(dbURL, username, password);
+            dbConn = ClickhouseUtil.getConnection(dbUrl, username, password);
 
             if (restoreConfig.isRestore()) {
                 dbConn.setAutoCommit(false);
@@ -91,7 +91,7 @@ private void initFullColumnAndType() throws SQLException {
             LOG.error("error to get {} schema, e = {}", table, ExceptionUtil.getErrorMessage(e));
             throw e;
         }finally {
-            DBUtil.closeDBResources(rs, stmt,null, false);
+            DbUtil.closeDbResources(rs, stmt,null, false);
         }
 
         if(CollectionUtils.isEmpty(fullColumn)) {

File: flinkx-core/src/main/java/com/dtstack/flink/api/java/MyLocalStreamEnvironment.java
Patch:
@@ -22,7 +22,6 @@
 import org.apache.flink.api.common.InvalidProgramException;
 import org.apache.flink.api.common.JobExecutionResult;
 import org.apache.flink.api.java.ExecutionEnvironment;
-import org.apache.flink.configuration.ConfigConstants;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.configuration.RestOptions;
 import org.apache.flink.configuration.TaskManagerOptions;
@@ -48,6 +47,8 @@
  *
  * <p>When this environment is instantiated, it uses a default parallelism of {@code 1}. The default
  * parallelism can be set via {@link #setParallelism(int)}.
+ *
+ * @author jiangbo
  */
 @Public
 public class MyLocalStreamEnvironment extends StreamExecutionEnvironment {
@@ -123,7 +124,7 @@ public JobExecutionResult execute(String jobName) throws Exception {
         Configuration configuration = new Configuration();
         configuration.addAll(jobGraph.getJobConfiguration());
         configuration.setString(TaskManagerOptions.MANAGED_MEMORY_SIZE, "0");
-        configuration.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, jobGraph.getMaximumParallelism());
+        configuration.setInteger(TaskManagerOptions.NUM_TASK_SLOTS.key(), jobGraph.getMaximumParallelism());
 
         // add (and override) the settings with what the user defined
         configuration.addAll(this.configuration);

File: flinkx-core/src/main/java/com/dtstack/flinkx/classloader/ClassLoaderSupplierCallBack.java
Patch:
@@ -20,9 +20,9 @@
 package com.dtstack.flinkx.classloader;
 
 /**
- * company: www.dtstack.com
- * author: toutian
- * create: 2019/10/14
+ * @company: www.dtstack.com
+ * @author: toutian
+ * @create: 2019/10/14
  */
 public class ClassLoaderSupplierCallBack {
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/classloader/PluginUtil.java
Patch:
@@ -24,9 +24,7 @@
 import java.io.File;
 import java.net.MalformedURLException;
 import java.net.URL;
-import java.util.ArrayList;
 import java.util.HashSet;
-import java.util.List;
 import java.util.Set;
 
 /**

File: flinkx-core/src/main/java/com/dtstack/flinkx/config/DataTransferConfig.java
Patch:
@@ -101,7 +101,6 @@ private static void checkConfig(DataTransferConfig config) {
 
     public static DataTransferConfig parse(String json) {
         Gson gson = new Gson();
-        //DataTransferConfig config = gson.fromJson(json, DataTransferConfig.class);
         Map<String,Object> map = gson.fromJson(json, Map.class);
         map = MapUtil.convertToHashMap(map);
         DataTransferConfig config = new DataTransferConfig(map);

File: flinkx-core/src/main/java/com/dtstack/flinkx/config/ErrorLimitConfig.java
Patch:
@@ -41,7 +41,7 @@ public ErrorLimitConfig(Map<String, Object> map) {
     }
 
     public static ErrorLimitConfig defaultConfig(){
-        Map<String, Object> map = new HashMap<>();
+        Map<String, Object> map = new HashMap<>(2);
         map.put("record",DEFAULT_ERROR_RECORD_LIMIT);
         map.put("percentage",DEFAULT_ERROR_PERCENTAGE_LIMIT);
         return new ErrorLimitConfig(map);

File: flinkx-core/src/main/java/com/dtstack/flinkx/decoder/PlainDecoder.java
Patch:
@@ -15,7 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package com.dtstack.flinkx.kafkaBase.decoder;
+package com.dtstack.flinkx.decoder;
 
 import java.util.Collections;
 import java.util.Map;

File: flinkx-core/src/main/java/com/dtstack/flinkx/latch/LocalLatch.java
Patch:
@@ -28,7 +28,7 @@
  * Company: www.dtstack.com
  * @author huyifan.zju@163.com
  */
-public class LocalLatch extends Latch {
+public class LocalLatch extends BaseLatch {
 
     private static Map<String, AtomicInteger> valMap = new ConcurrentHashMap<>();
     private AtomicInteger val;

File: flinkx-core/src/main/java/com/dtstack/flinkx/metrics/BaseMetric.java
Patch:
@@ -64,7 +64,7 @@ public Map<String, LongCounter> getMetricCounters() {
         return metricCounters;
     }
 
-    public void waitForMetricReport(){
+    public void waitForReportMetrics() {
         try {
             Thread.sleep(delayPeriodMill);
         } catch (InterruptedException e){

File: flinkx-core/src/main/java/com/dtstack/flinkx/metrics/SimpleLongCounterMeterView.java
Patch:
@@ -36,6 +36,8 @@
  * smoother transitions between rates.
  *
  * <p>The events are counted by a {@link Counter}.
+ *
+ * @author toutian
  */
 public class SimpleLongCounterMeterView implements Meter, View {
 	/** The underlying counter maintaining the count. */

File: flinkx-core/src/main/java/com/dtstack/flinkx/outputformat/FileOutputFormatBuilder.java
Patch:
@@ -28,11 +28,11 @@
  * @author jiangbo
  * @date 2019/8/28
  */
-public class FileOutputFormatBuilder extends RichOutputFormatBuilder {
+public class FileOutputFormatBuilder extends BaseRichOutputFormatBuilder {
 
-    protected FileOutputFormat format;
+    protected BaseFileOutputFormat format;
 
-    public void setFormat(FileOutputFormat format) {
+    public void setFormat(BaseFileOutputFormat format) {
         this.format = format;
         super.format = format;
     }

File: flinkx-core/src/main/java/com/dtstack/flinkx/reader/DataReaderFactory.java
Patch:
@@ -37,7 +37,7 @@ public class DataReaderFactory {
     private DataReaderFactory() {
     }
 
-    public static DataReader getDataReader(DataTransferConfig config, StreamExecutionEnvironment env) {
+    public static BaseDataReader getDataReader(DataTransferConfig config, StreamExecutionEnvironment env) {
         try {
             String pluginName = config.getJob().getContent().get(0).getReader().getName();
             String pluginClassName = PluginUtil.getPluginClassName(pluginName);
@@ -46,7 +46,7 @@ public static DataReader getDataReader(DataTransferConfig config, StreamExecutio
             return ClassLoaderManager.newInstance(urlList, cl -> {
                 Class<?> clazz = cl.loadClass(pluginClassName);
                 Constructor constructor = clazz.getConstructor(DataTransferConfig.class, StreamExecutionEnvironment.class);
-                return (DataReader)constructor.newInstance(config, env);
+                return (BaseDataReader)constructor.newInstance(config, env);
             });
         } catch (Exception e) {
             throw new RuntimeException(e);

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/ClassUtil.java
Patch:
@@ -33,10 +33,10 @@ public class ClassUtil {
 
     private static final Logger LOG = LoggerFactory.getLogger(ClassUtil.class);
 
-    public final static String lock_str = "jdbc_lock_str";
+    public final static Object LOCK_STR = new Object();
 
     public static void forName(String clazz, ClassLoader classLoader)  {
-        synchronized (lock_str){
+        synchronized (LOCK_STR){
             try {
                 Class.forName(clazz, true, classLoader);
                 DriverManager.setLoginTimeout(10);

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/FileSystemUtil.java
Patch:
@@ -108,7 +108,7 @@ public static UserGroupInformation getUGI(Map<String, Object> hadoopConfig, Stri
         String principal = KerberosUtil.getPrincipal(hadoopConfig, keytabFileName);
         KerberosUtil.loadKrb5Conf(hadoopConfig);
 
-        UserGroupInformation ugi = KerberosUtil.loginAndReturnUGI(getConfiguration(hadoopConfig, defaultFs), principal, keytabFileName);
+        UserGroupInformation ugi = KerberosUtil.loginAndReturnUgi(getConfiguration(hadoopConfig, defaultFs), principal, keytabFileName);
         UserGroupInformation.setLoginUser(ugi);
 
         return ugi;

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/RowUtil.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.flink.types.Row;
 import org.apache.flink.util.Preconditions;
 
-import java.util.HashMap;
+import java.util.LinkedHashMap;
 import java.util.Map;
 
 /**
@@ -37,7 +37,7 @@ public class RowUtil {
 
     public static String rowToJson(Row row, String[] colName) {
         Preconditions.checkNotNull(colName);
-        Map<String,Object> map = new HashMap<>();
+        Map<String,Object> map = new LinkedHashMap<>(colName.length);
 
         for(int i = 0; i < colName.length; ++i) {
             String key = colName[i];

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/StringUtil.java
Patch:
@@ -38,6 +38,8 @@
  */
 public class StringUtil {
 
+    public static final int STEP_SIZE = 2;
+
     /**
      * Handle the escaped escape charactor.
      *
@@ -228,7 +230,7 @@ public static byte[] hexStringToByteArray(String hexString) {
         int length = hexString.length();
 
         byte[] bytes = new byte[length / 2];
-        for (int i = 0; i < length; i += 2) {
+        for (int i = 0; i < length; i += STEP_SIZE) {
             bytes[i / 2] = (byte) ((Character.digit(hexString.charAt(i), 16) << 4)
                     + Character.digit(hexString.charAt(i+1), 16));
         }

File: flinkx-core/src/main/java/com/dtstack/flinkx/writer/DataWriterFactory.java
Patch:
@@ -36,7 +36,7 @@ public class DataWriterFactory {
 
     private DataWriterFactory() {}
 
-    public static DataWriter getDataWriter(DataTransferConfig config) {
+    public static BaseDataWriter getDataWriter(DataTransferConfig config) {
         try {
             String pluginName = config.getJob().getContent().get(0).getWriter().getName();
             String pluginClassName = PluginUtil.getPluginClassName(pluginName);
@@ -45,7 +45,7 @@ public static DataWriter getDataWriter(DataTransferConfig config) {
             return ClassLoaderManager.newInstance(urlList, cl -> {
                 Class<?> clazz = cl.loadClass(pluginClassName);
                 Constructor constructor = clazz.getConstructor(DataTransferConfig.class);
-                return (DataWriter)constructor.newInstance(config);
+                return (BaseDataWriter)constructor.newInstance(config);
             });
         } catch (Exception e) {
             throw new RuntimeException(e);

File: flinkx-db2/flinkx-db2-core/src/main/java/com/dtstack/flinkx/db2/Db2DatabaseMeta.java
Patch:
@@ -51,12 +51,12 @@ protected String makeValues(List<String> column) {
     }
 
     @Override
-    public String getSQLQueryFields(String tableName) {
+    public String getSqlQueryFields(String tableName) {
         return "SELECT * FROM " + tableName + " FETCH FIRST  1 ROWS ONLY";
     }
 
     @Override
-    public String getSQLQueryColumnFields(List<String> column, String table) {
+    public String getSqlQueryColumnFields(List<String> column, String table) {
         return "SELECT " + quoteColumns(column) + " FROM " + quoteTable(table) + " FETCH FIRST  1 ROWS ONLY";
     }
 

File: flinkx-db2/flinkx-db2-reader/src/main/java/com/dtstack/flinkx/db2/format/Db2InputFormat.java
Patch:
@@ -22,7 +22,7 @@
 
 import java.io.IOException;
 
-import static com.dtstack.flinkx.rdb.util.DBUtil.clobToString;
+import static com.dtstack.flinkx.rdb.util.DbUtil.clobToString;
 
 /**
  * Date: 2019/09/20

File: flinkx-db2/flinkx-db2-writer/src/main/java/com/dtstack/flinkx/db2/format/Db2OutputFormat.java
Patch:
@@ -37,7 +37,7 @@ public class Db2OutputFormat extends JdbcOutputFormat {
 
     @Override
     protected Map<String, List<String>> probePrimaryKeys(String table, Connection dbConn) throws SQLException {
-        Map<String, List<String>> map = new HashMap<>();
+        Map<String, List<String>> map = new HashMap<>(16);
         ResultSet rs = dbConn.getMetaData().getIndexInfo(null, null, table.toUpperCase(), true, false);
         while(rs.next()) {
             String indexName = rs.getString("INDEX_NAME");
@@ -46,7 +46,7 @@ protected Map<String, List<String>> probePrimaryKeys(String table, Connection db
             }
             map.get(indexName).add(rs.getString("COLUMN_NAME"));
         }
-        Map<String,List<String>> retMap = new HashMap<>();
+        Map<String,List<String>> retMap = new HashMap<>((map.size()<<2)/3);
         for(Map.Entry<String,List<String>> entry: map.entrySet()) {
             String k = entry.getKey();
             List<String> v = entry.getValue();

File: flinkx-dm/flinkx-dm-core/src/main/java/com/dtstack/flinkx/dm/DmDatabaseMeta.java
Patch:
@@ -47,7 +47,7 @@ protected String makeValues(List<String> column) {
     public String quoteTable(String table) {
         table = table.replace("\"","");
         String[] part = table.split("\\.");
-        if(part.length == 2) {
+        if(part.length == DB_TABLE_PART_SIZE) {
             table = getStartQuote() + part[0] + getEndQuote() + "." + getStartQuote() + part[1] + getEndQuote();
         } else {
             table = getStartQuote() + table + getEndQuote();
@@ -66,12 +66,12 @@ public String getDriverClass() {
     }
 
     @Override
-    public String getSQLQueryFields(String tableName) {
+    public String getSqlQueryFields(String tableName) {
         return "SELECT * FROM " + tableName + " LIMIT 1";
     }
 
     @Override
-    public String getSQLQueryColumnFields(List<String> column, String table) {
+    public String getSqlQueryColumnFields(List<String> column, String table) {
         return "SELECT " + quoteColumns(column) + " FROM " + quoteTable(table) + " LIMIT 1";
     }
 

File: flinkx-emqx/flinkx-emqx-reader/src/main/java/com/dtstack/flinkx/emqx/format/EmqxInputFormatBuilder.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */package com.dtstack.flinkx.emqx.format;
 
-import com.dtstack.flinkx.inputformat.RichInputFormatBuilder;
+import com.dtstack.flinkx.inputformat.BaseRichInputFormatBuilder;
 import org.apache.commons.lang3.StringUtils;
 
 /**
@@ -25,7 +25,7 @@
  *
  * @author tudou
  */
-public class EmqxInputFormatBuilder extends RichInputFormatBuilder {
+public class EmqxInputFormatBuilder extends BaseRichInputFormatBuilder {
 
     private EmqxInputFormat format;
 

File: flinkx-emqx/flinkx-emqx-writer/src/main/java/com/dtstack/flinkx/emqx/format/EmqxOutputFormat.java
Patch:
@@ -17,9 +17,9 @@
  */
 package com.dtstack.flinkx.emqx.format;
 
-import com.dtstack.flinkx.emqx.decoder.JsonDecoder;
+import com.dtstack.flinkx.decoder.JsonDecoder;
 import com.dtstack.flinkx.exception.WriteRecordException;
-import com.dtstack.flinkx.outputformat.RichOutputFormat;
+import com.dtstack.flinkx.outputformat.BaseRichOutputFormat;
 import com.dtstack.flinkx.util.ExceptionUtil;
 import org.apache.commons.lang3.StringUtils;
 import org.apache.flink.types.Row;
@@ -41,7 +41,7 @@
  *
  * @author tudou
  */
-public class EmqxOutputFormat extends RichOutputFormat {
+public class EmqxOutputFormat extends BaseRichOutputFormat {
     private static final Logger LOG = LoggerFactory.getLogger(EmqxOutputFormat.class);
     private static final String CLIENT_ID_PRE = "writer";
 

File: flinkx-emqx/flinkx-emqx-writer/src/main/java/com/dtstack/flinkx/emqx/format/EmqxOutputFormatBuilder.java
Patch:
@@ -17,7 +17,7 @@
  */
 package com.dtstack.flinkx.emqx.format;
 
-import com.dtstack.flinkx.outputformat.RichOutputFormatBuilder;
+import com.dtstack.flinkx.outputformat.BaseRichOutputFormatBuilder;
 import org.apache.commons.lang3.StringUtils;
 
 /**
@@ -26,7 +26,7 @@
  *
  * @author tudou
  */
-public class EmqxOutputFormatBuilder extends RichOutputFormatBuilder {
+public class EmqxOutputFormatBuilder extends BaseRichOutputFormatBuilder {
     private EmqxOutputFormat format;
 
     public EmqxOutputFormatBuilder(){

File: flinkx-es/flinkx-es-core/src/main/java/com/dtstack/flinkx/es/EsUtil.java
Patch:
@@ -103,7 +103,7 @@ public static Row jsonMapToRow(Map<String,Object> map, List<String> fields, List
 
     public static Map<String, Object> rowToJsonMap(Row row, List<String> fields, List<String> types) throws WriteRecordException {
         Preconditions.checkArgument(row.getArity() == fields.size());
-        Map<String,Object> jsonMap = new HashMap<>();
+        Map<String,Object> jsonMap = new HashMap<>((fields.size()<<2)/3);
         int i = 0;
         try {
             for(; i < fields.size(); ++i) {
@@ -113,7 +113,7 @@ public static Map<String, Object> rowToJsonMap(Row row, List<String> fields, Lis
                 for(int j = 0; j < parts.length - 1; ++j) {
                     String key = parts[j];
                     if(currMap.get(key) == null) {
-                        currMap.put(key, new HashMap<String,Object>());
+                        currMap.put(key, new HashMap<String,Object>(16));
                     }
                     currMap = (Map<String, Object>) currMap.get(key);
                 }

File: flinkx-es/flinkx-es-writer/src/main/java/com/dtstack/flinkx/es/writer/EsOutputFormat.java
Patch:
@@ -21,7 +21,7 @@
 import com.dtstack.flinkx.es.EsUtil;
 import com.dtstack.flinkx.exception.WriteRecordException;
 import com.dtstack.flinkx.util.StringUtil;
-import com.dtstack.flinkx.outputformat.RichOutputFormat;
+import com.dtstack.flinkx.outputformat.BaseRichOutputFormat;
 import org.apache.commons.lang.StringUtils;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.types.Row;
@@ -40,7 +40,7 @@
  * Company: www.dtstack.com
  * @author huyifan.zju@163.com
  */
-public class EsOutputFormat extends RichOutputFormat {
+public class EsOutputFormat extends BaseRichOutputFormat {
 
     protected String address;
 

File: flinkx-es/flinkx-es-writer/src/main/java/com/dtstack/flinkx/es/writer/EsOutputFormatBuilder.java
Patch:
@@ -18,7 +18,7 @@
 
 package com.dtstack.flinkx.es.writer;
 
-import com.dtstack.flinkx.outputformat.RichOutputFormatBuilder;
+import com.dtstack.flinkx.outputformat.BaseRichOutputFormatBuilder;
 import java.util.List;
 import java.util.Map;
 
@@ -28,7 +28,7 @@
  * Company: www.dtstack.com
  * @author huyifan_zju@163.com
  */
-public class EsOutputFormatBuilder extends RichOutputFormatBuilder {
+public class EsOutputFormatBuilder extends BaseRichOutputFormatBuilder {
 
     private EsOutputFormat format;
 

File: flinkx-es/flinkx-es-writer/src/main/java/com/dtstack/flinkx/es/writer/EsWriter.java
Patch:
@@ -21,7 +21,7 @@
 import com.dtstack.flinkx.config.DataTransferConfig;
 import com.dtstack.flinkx.config.WriterConfig;
 import com.dtstack.flinkx.es.EsConfigKeys;
-import com.dtstack.flinkx.writer.DataWriter;
+import com.dtstack.flinkx.writer.BaseDataWriter;
 import org.apache.commons.collections.CollectionUtils;
 import org.apache.flink.streaming.api.datastream.DataStream;
 import org.apache.flink.streaming.api.datastream.DataStreamSink;
@@ -37,7 +37,7 @@
  * Company: www.dtstack.com
  * @author huyifan.zju@163.com
  */
-public class EsWriter extends DataWriter {
+public class EsWriter extends BaseDataWriter {
 
     public static final int DEFAULT_BULK_ACTION = 100;
 

File: flinkx-examples/src/main/java/com/dtstack/flinkx/examples/ExampleGenerator.java
Patch:
@@ -31,7 +31,6 @@
 import java.io.IOException;
 import java.io.InputStreamReader;
 import java.io.OutputStreamWriter;
-import java.nio.charset.Charset;
 import java.nio.charset.StandardCharsets;
 import java.util.ArrayList;
 import java.util.List;

File: flinkx-ftp/flinkx-ftp-core/src/main/java/com/dtstack/flinkx/ftp/FtpConfig.java
Patch:
@@ -58,7 +58,7 @@ public class FtpConfig implements Serializable {
 
     public int timeout = FtpConfigConstants.DEFAULT_TIMEOUT;
 
-    public long maxFileSize = 1024 * 1024 * 1024;
+    public long maxFileSize = 1024 * 1024 * 1024L;
 
     public String getUsername() {
         return username;

File: flinkx-ftp/flinkx-ftp-reader/src/main/java/com/dtstack/flinkx/ftp/reader/FtpSeqInputStream.java
Patch:
@@ -67,7 +67,8 @@ final void nextStream() throws IOException {
     @Override
     public int available() throws IOException {
         if (in == null) {
-            return 0; // no way to signal EOF from available()
+            // no way to signal EOF from available()
+            return 0;
         }
         return in.available();
     }
@@ -85,7 +86,7 @@ public int read() throws IOException {
     }
 
     @Override
-    public int read(byte b[], int off, int len) throws IOException {
+    public int read(byte[] b, int off, int len) throws IOException {
         if (in == null) {
             return -1;
         } else if (b == null) {

File: flinkx-ftp/flinkx-ftp-writer/src/main/java/com/dtstack/flinkx/ftp/writer/FtpOutputFormatBuilder.java
Patch:
@@ -19,9 +19,7 @@
 package com.dtstack.flinkx.ftp.writer;
 
 import com.dtstack.flinkx.ftp.FtpConfig;
-import com.dtstack.flinkx.ftp.FtpConfigConstants;
 import com.dtstack.flinkx.outputformat.FileOutputFormatBuilder;
-import org.apache.commons.lang.StringUtils;
 import java.util.List;
 
 /**

File: flinkx-gbase/flinkx-gbase-core/src/main/java/com/dtstack/flinkx/gbase/GbaseDatabaseMeta.java
Patch:
@@ -78,7 +78,7 @@ protected String makeValues(List<String> column) {
     }
 
     @Override
-    public String getSQLQueryFields(String tableName) {
+    public String getSqlQueryFields(String tableName) {
         return "SELECT * FROM " + tableName + " LIMIT 0";
     }
 
@@ -93,7 +93,7 @@ public String getEndQuote() {
     }
 
     @Override
-    public String getSQLQueryColumnFields(List<String> column, String table) {
+    public String getSqlQueryColumnFields(List<String> column, String table) {
         return "SELECT " + quoteColumns(column) + " FROM " + quoteTable(table) + " LIMIT 0";
     }
 

File: flinkx-gbase/flinkx-gbase-writer/src/main/java/com/dtstack/flinkx/gbase/writer/GbaseWriter.java
Patch:
@@ -24,7 +24,7 @@
 import com.dtstack.flinkx.gbase.format.GbaseOutputFormat;
 import com.dtstack.flinkx.rdb.datawriter.JdbcDataWriter;
 import com.dtstack.flinkx.rdb.outputformat.JdbcOutputFormatBuilder;
-import com.dtstack.flinkx.rdb.util.DBUtil;
+import com.dtstack.flinkx.rdb.util.DbUtil;
 
 /**
  * @author jiangbo
@@ -35,7 +35,7 @@ public class GbaseWriter extends JdbcDataWriter {
     public GbaseWriter(DataTransferConfig config) {
         super(config);
         setDatabaseInterface(new GbaseDatabaseMeta());
-        dbUrl = DBUtil.formatJdbcUrl(dbUrl, null);
+        dbUrl = DbUtil.formatJdbcUrl(dbUrl, null);
     }
 
     @Override

File: flinkx-hbase/flinkx-hbase-core/src/main/java/com/dtstack/flinkx/hbase/HbaseConfigConstants.java
Patch:
@@ -44,7 +44,7 @@ public class HbaseConfigConstants {
 
     public static final String DEFAULT_NULL_MODE = "skip";
 
-    public static final long DEFAULT_WRITE_BUFFER_SIZE = 8 * 1024 * 1024;
+    public static final long DEFAULT_WRITE_BUFFER_SIZE = 8 * 1024 * 1024L;
 
     public static final boolean DEFAULT_WAL_FLAG = false;
 

File: flinkx-hbase/flinkx-hbase-reader/src/main/java/com/dtstack/flinkx/hbase/reader/HbaseInputFormatBuilder.java
Patch:
@@ -18,7 +18,7 @@
 package com.dtstack.flinkx.hbase.reader;
 
 import com.dtstack.flinkx.hbase.HbaseConfigConstants;
-import com.dtstack.flinkx.inputformat.RichInputFormatBuilder;
+import com.dtstack.flinkx.inputformat.BaseRichInputFormatBuilder;
 import org.apache.commons.lang.StringUtils;
 import org.apache.flink.util.Preconditions;
 import java.util.List;
@@ -30,7 +30,7 @@
  * Company: www.dtstack.com
  * @author huyifan.zju@163.com
  */
-public class HbaseInputFormatBuilder extends RichInputFormatBuilder {
+public class HbaseInputFormatBuilder extends BaseRichInputFormatBuilder {
 
     private HbaseInputFormat format;
 

File: flinkx-hbase/flinkx-hbase-writer/src/main/java/com/dtstack/flinkx/hbase/writer/HbaseOutputFormatBuilder.java
Patch:
@@ -19,7 +19,7 @@
 package com.dtstack.flinkx.hbase.writer;
 
 import com.dtstack.flinkx.hbase.HbaseConfigConstants;
-import com.dtstack.flinkx.outputformat.RichOutputFormatBuilder;
+import com.dtstack.flinkx.outputformat.BaseRichOutputFormatBuilder;
 import org.apache.commons.lang.StringUtils;
 import com.google.common.base.Preconditions;
 import java.util.List;
@@ -31,7 +31,7 @@
  * Company: www.dtstack.com
  * @author huyifan.zju@163.com
  */
-public class HbaseOutputFormatBuilder extends RichOutputFormatBuilder {
+public class HbaseOutputFormatBuilder extends BaseRichOutputFormatBuilder {
 
     private HbaseOutputFormat format;
 

File: flinkx-hbase/flinkx-hbase-writer/src/main/java/com/dtstack/flinkx/hbase/writer/function/FunctionFactory.java
Patch:
@@ -35,7 +35,7 @@ public static IFunction createFuntion(String functionName) {
         IFunction function = null;
         switch (functionName.toUpperCase()) {
             case "MD5":
-                function = new MD5Function();
+                function = new Md5Function();
                 break;
             case "STRING":
                 function = new StringFunction();

File: flinkx-hdfs/flinkx-hdfs-core/src/main/java/com/dtstack/flinkx/hdfs/ECompressType.java
Patch:
@@ -19,7 +19,6 @@
 package com.dtstack.flinkx.hdfs;
 
 import org.apache.commons.lang.StringUtils;
-import org.apache.parquet.hadoop.metadata.CompressionCodecName;
 
 /**
  * @author jiangbo

File: flinkx-hdfs/flinkx-hdfs-core/src/main/java/com/dtstack/flinkx/hdfs/HdfsConfigKeys.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information
@@ -60,4 +60,6 @@ public class HdfsConfigKeys {
 
     public static final String KEY_FLUSH_INTERVAL = "flushInterval";
 
+    public static final String KEY_ENABLE_DICTIONARY = "enableDictionary";
+
 }

File: flinkx-hdfs/flinkx-hdfs-core/src/main/java/org/apache/hadoop/hive/shims/ShimLoader.java
Patch:
@@ -27,6 +27,7 @@
 /**
  * ShimLoader.
  *
+ * @author jiangbo
  */
 public abstract class ShimLoader {
     private static HadoopShims hadoopShims;

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsTextOutputFormat.java
Patch:
@@ -42,7 +42,7 @@
  * Company: www.dtstack.com
  * @author huyifan.zju@163.com
  */
-public class HdfsTextOutputFormat extends HdfsOutputFormat {
+public class HdfsTextOutputFormat extends BaseHdfsOutputFormat {
 
     private static final int NEWLINE = 10;
     private transient OutputStream stream;
@@ -124,7 +124,7 @@ public void writeSingleRecordToFile(Row row) throws WriteRecordException {
                     sb.append(delimiter);
                 }
 
-                appendDataToString(sb, i, row.getField(j), ColumnType.fromString(columnTypes.get(j)));
+                appendDataToString(sb, row.getField(j), ColumnType.fromString(columnTypes.get(j)));
             }
         } catch(Exception e) {
             if(i < row.getArity()) {
@@ -151,7 +151,7 @@ public void writeSingleRecordToFile(Row row) throws WriteRecordException {
         }
     }
 
-    private void appendDataToString(StringBuilder sb, int ii, Object column, ColumnType columnType) {
+    private void appendDataToString(StringBuilder sb, Object column, ColumnType columnType) {
         if(column == null) {
             sb.append(HdfsUtil.NULL_VALUE);
             return;

File: flinkx-hive/flinkx-hive-core/src/main/java/com/dtstack/flinkx/hive/util/AbstractHiveMetadataParser.java
Patch:
@@ -25,7 +25,9 @@
 import java.util.List;
 import java.util.Map;
 
-import static com.dtstack.flinkx.hive.EStoreType.*;
+import static com.dtstack.flinkx.hive.EStoreType.ORC;
+import static com.dtstack.flinkx.hive.EStoreType.PARQUET;
+import static com.dtstack.flinkx.hive.EStoreType.TEXT;
 
 /**
  * @author jiangbo

File: flinkx-hive/flinkx-hive-core/src/main/java/com/dtstack/flinkx/hive/util/Cdh2HiveMetadataParser.java
Patch:
@@ -29,7 +29,7 @@
  * @author jiangbo
  * @date 2019/11/29
  */
-public class CDH2HiveMetadataParser extends AbstractHiveMetadataParser {
+public class Cdh2HiveMetadataParser extends AbstractHiveMetadataParser {
 
     @Override
     public void fillTableInfo(TableInfo tableInfo, List<Map<String, Object>> result) {

File: flinkx-kafka/flinkx-kafka-reader/src/main/java/com/dtstack/flinkx/kafka/reader/KafkaConsumer.java
Patch:
@@ -16,8 +16,8 @@
  */
 package com.dtstack.flinkx.kafka.reader;
 
-import com.dtstack.flinkx.kafkaBase.reader.KafkaBaseConsumer;
-import com.dtstack.flinkx.kafkaBase.reader.KafkaBaseInputFormat;
+import com.dtstack.flinkx.kafkabase.reader.KafkaBaseConsumer;
+import com.dtstack.flinkx.kafkabase.reader.KafkaBaseInputFormat;
 
 import java.util.Arrays;
 import java.util.Properties;

File: flinkx-kafka/flinkx-kafka-reader/src/main/java/com/dtstack/flinkx/kafka/reader/KafkaInputFormat.java
Patch:
@@ -19,7 +19,7 @@
 
 package com.dtstack.flinkx.kafka.reader;
 
-import com.dtstack.flinkx.kafkaBase.reader.KafkaBaseInputFormat;
+import com.dtstack.flinkx.kafkabase.reader.KafkaBaseInputFormat;
 
 import java.io.IOException;
 import java.util.Properties;

File: flinkx-kafka/flinkx-kafka-reader/src/main/java/com/dtstack/flinkx/kafka/reader/KafkaReader.java
Patch:
@@ -18,8 +18,8 @@
 package com.dtstack.flinkx.kafka.reader;
 
 import com.dtstack.flinkx.config.DataTransferConfig;
-import com.dtstack.flinkx.kafkaBase.reader.KafkaBaseInputFormat;
-import com.dtstack.flinkx.kafkaBase.reader.KafkaBaseReader;
+import com.dtstack.flinkx.kafkabase.reader.KafkaBaseInputFormat;
+import com.dtstack.flinkx.kafkabase.reader.KafkaBaseReader;
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
 import org.apache.kafka.clients.producer.ProducerConfig;
 

File: flinkx-kafka/flinkx-kafka-writer/src/main/java/com/dtstack/flinkx/kafka/writer/KafkaOutputFormat.java
Patch:
@@ -18,8 +18,8 @@
 
 package com.dtstack.flinkx.kafka.writer;
 
-import com.dtstack.flinkx.kafkaBase.Formatter;
-import com.dtstack.flinkx.kafkaBase.writer.KafkaBaseOutputFormat;
+import com.dtstack.flinkx.kafkabase.Formatter;
+import com.dtstack.flinkx.kafkabase.writer.KafkaBaseOutputFormat;
 import org.apache.flink.configuration.Configuration;
 import org.apache.kafka.clients.producer.KafkaProducer;
 import org.apache.kafka.clients.producer.ProducerConfig;

File: flinkx-kafka/flinkx-kafka-writer/src/main/java/com/dtstack/flinkx/kafka/writer/KafkaWriter.java
Patch:
@@ -18,7 +18,7 @@
 package com.dtstack.flinkx.kafka.writer;
 
 import com.dtstack.flinkx.config.DataTransferConfig;
-import com.dtstack.flinkx.kafkaBase.writer.KafkaBaseWriter;
+import com.dtstack.flinkx.kafkabase.writer.KafkaBaseWriter;
 import org.apache.flink.streaming.api.datastream.DataStream;
 import org.apache.flink.streaming.api.datastream.DataStreamSink;
 import org.apache.flink.types.Row;

File: flinkx-kb/flinkx-kb-core/src/main/java/com/dtstack/flinkx/kafkabase/Formatter.java
Patch:
@@ -15,7 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package com.dtstack.flinkx.kafkaBase;
+package com.dtstack.flinkx.kafkabase;
 
 import org.joda.time.DateTime;
 import org.joda.time.DateTimeZone;
@@ -48,7 +48,7 @@ public static String format(Map event, String format, String timezone) {
         while (m.find()) {
             String match = m.group();
             String key = (String) match.subSequence(2, match.length() - 1);
-            if (key.equalsIgnoreCase("+s")) {
+            if ("+s".equalsIgnoreCase(key)) {
                 Object o = event.get("@timestamp");
                 if (o.getClass() == Long.class) {
                     m.appendReplacement(sb, o.toString());

File: flinkx-kudu/flinkx-kudu-writer/src/main/java/com/dtstack/flinkx/kudu/writer/KuduOutputFormatBuilder.java
Patch:
@@ -20,7 +20,7 @@
 package com.dtstack.flinkx.kudu.writer;
 
 import com.dtstack.flinkx.kudu.core.KuduConfig;
-import com.dtstack.flinkx.outputformat.RichOutputFormatBuilder;
+import com.dtstack.flinkx.outputformat.BaseRichOutputFormatBuilder;
 import com.dtstack.flinkx.reader.MetaColumn;
 
 import java.util.List;
@@ -29,7 +29,7 @@
  * @author jiangbo
  * @date 2019/7/31
  */
-public class KuduOutputFormatBuilder extends RichOutputFormatBuilder {
+public class KuduOutputFormatBuilder extends BaseRichOutputFormatBuilder {
 
     private KuduOutputFormat format;
 

File: flinkx-launcher/src/main/java/com/dtstack/flinkx/launcher/YarnConfLoader.java
Patch:
@@ -18,6 +18,7 @@
 
 package com.dtstack.flinkx.launcher;
 
+import com.dtstack.flinkx.constants.ConstantValue;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.yarn.conf.YarnConfiguration;
 
@@ -42,7 +43,7 @@ public static YarnConfiguration getYarnConf(String yarnConfDir) {
             if(dir.exists() && dir.isDirectory()) {
 
                 File[] xmlFileList = new File(yarnConfDir).listFiles((dir1, name) -> {
-                    if(name.endsWith(".xml")){
+                    if(name.endsWith(ConstantValue.FILE_SUFFIX_XML)){
                         return true;
                     }
                     return false;

File: flinkx-launcher/src/main/java/com/dtstack/flinkx/launcher/perjob/FlinkPerJobResourceUtil.java
Patch:
@@ -15,7 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package com.dtstack.flinkx.launcher.perJob;
+package com.dtstack.flinkx.launcher.perjob;
 
 import com.dtstack.flinkx.util.ValueUtil;
 import org.apache.flink.client.deployment.ClusterSpecification;
@@ -27,7 +27,7 @@
  * Company: www.dtstack.com
  * @author tudou
  */
-public class FLinkPerJobResourceUtil {
+public class FlinkPerJobResourceUtil {
     /**
      * Minimum memory requirements, checked by the Client.
      * the minimum memory should be higher than the min heap cutoff

File: flinkx-metadata/flinkx-metadata-reader/src/main/java/com/dtstack/flinkx/metadata/inputformat/BaseMetadataInputFormat.java
Patch:
@@ -17,7 +17,7 @@
  */
 package com.dtstack.flinkx.metadata.inputformat;
 
-import com.dtstack.flinkx.inputformat.RichInputFormat;
+import com.dtstack.flinkx.inputformat.BaseRichInputFormat;
 import com.dtstack.flinkx.metadata.util.ConnUtil;
 import com.dtstack.flinkx.util.ExceptionUtil;
 import org.apache.commons.collections.CollectionUtils;
@@ -39,7 +39,7 @@
  * @author : tiezhu
  * @date : 2020/3/20
  */
-public abstract class BaseMetadataInputFormat extends RichInputFormat {
+public abstract class BaseMetadataInputFormat extends BaseRichInputFormat {
 
     protected String dbUrl;
 

File: flinkx-metadata/flinkx-metadata-reader/src/main/java/com/dtstack/flinkx/metadata/inputformat/MetadataInputFormatBuilder.java
Patch:
@@ -17,7 +17,7 @@
  */
 package com.dtstack.flinkx.metadata.inputformat;
 
-import com.dtstack.flinkx.inputformat.RichInputFormatBuilder;
+import com.dtstack.flinkx.inputformat.BaseRichInputFormatBuilder;
 
 import java.util.List;
 import java.util.Map;
@@ -26,7 +26,7 @@
  * @author : tiezhu
  * @date : 2020/3/8
  */
-public class MetadataInputFormatBuilder extends RichInputFormatBuilder {
+public class MetadataInputFormatBuilder extends BaseRichInputFormatBuilder {
     private BaseMetadataInputFormat format;
 
     public MetadataInputFormatBuilder(BaseMetadataInputFormat format) {

File: flinkx-mongodb/flinkx-mongodb-reader/src/main/java/com/dtstack/flinkx/mongodb/reader/MongodbReader.java
Patch:
@@ -21,7 +21,7 @@
 import com.dtstack.flinkx.config.DataTransferConfig;
 import com.dtstack.flinkx.config.ReaderConfig;
 import com.dtstack.flinkx.mongodb.MongodbConfig;
-import com.dtstack.flinkx.reader.DataReader;
+import com.dtstack.flinkx.reader.BaseDataReader;
 import com.dtstack.flinkx.reader.MetaColumn;
 import org.apache.flink.streaming.api.datastream.DataStream;
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
@@ -35,7 +35,7 @@
  * @Company: www.dtstack.com
  * @author jiangbo
  */
-public class MongodbReader extends DataReader {
+public class MongodbReader extends BaseDataReader {
 
     private List<MetaColumn> metaColumns;
 

File: flinkx-stream/flinkx-stream-writer/src/main/java/com/dtstack/flinkx/stream/writer/StreamWriter.java
Patch:
@@ -46,7 +46,7 @@ public StreamWriter(DataTransferConfig config) {
         super(config);
         print = config.getJob().getContent().get(0).getWriter().getParameter().getBooleanVal("print",false);
         writeDelimiter = config.getJob().getContent().get(0).getWriter().getParameter().getStringVal("writeDelimiter", "|");
-        batchInterval = config.getJob().getContent().get(0).getWriter().getParameter().getIntVal("batchInterval", 20);
+        batchInterval = config.getJob().getContent().get(0).getWriter().getParameter().getIntVal("batchInterval", 1);
 
         List column = config.getJob().getContent().get(0).getWriter().getParameter().getColumn();
         metaColumns = MetaColumn.getMetaColumns(column);

File: flinkx-rdb/flinkx-rdb-reader/src/main/java/com.dtstack.flinkx.rdb.inputformat/DistributedJdbcInputFormat.java
Patch:
@@ -206,7 +206,7 @@ protected void closeCurrentSource(){
 
     @Override
     protected void closeInternal() throws IOException {
-
+        closeCurrentSource();
     }
 
     @Override

File: flinkx-binlog/flinkx-binlog-core/src/main/java/com/dtstack/flinkx/binlog/BinlogJournalValidator.java
Patch:
@@ -28,7 +28,9 @@
 import java.util.ArrayList;
 import java.util.List;
 
-
+/**
+ * @author toutian
+ */
 public class BinlogJournalValidator {
 
     private static final Logger LOG = LoggerFactory.getLogger(BinlogJournalValidator.class);

File: flinkx-binlog/flinkx-binlog-core/src/main/java/com/google/common/collect/MapMakerHelper.java
Patch:
@@ -23,6 +23,9 @@
 import java.lang.reflect.Method;
 import java.util.concurrent.ConcurrentMap;
 
+/**
+ * @author toutian
+ */
 public class MapMakerHelper {
 
     public static MapMaker softValues(MapMaker mapMaker) {

File: flinkx-binlog/flinkx-binlog-core/src/main/java/com/google/common/collect/MigrateMap.java
Patch:
@@ -21,6 +21,9 @@
 
 import java.util.concurrent.ConcurrentMap;
 
+/**
+ * @author toutian
+ */
 public class MigrateMap {
 
     @SuppressWarnings("deprecation")

File: flinkx-binlog/flinkx-binlog-reader/src/main/java/com/dtstack/flinkx/binlog/reader/BinlogPositionManager.java
Patch:
@@ -26,6 +26,9 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+/**
+ * @author toutian
+ */
 public class BinlogPositionManager extends AbstractLogPositionManager {
 
     private static final Logger LOG = LoggerFactory.getLogger(BinlogPositionManager.class);

File: flinkx-carbondata/flinkx-carbondata-core/src/main/java/com/dtstack/flinkx/carbondata/CarbondataUtil.java
Patch:
@@ -53,7 +53,7 @@ public static CarbonTable buildCarbonTable(String dbName, String tableName, Stri
         return CarbonTable.buildFromTableInfo(wrapperTableInfo);
     }
 
-    public static void initFileFactory(Map<String,String> hadoopConfig, String defaultFS) {
+    public static void initFileFactory(Map<String,String> hadoopConfig, String defaultFs) {
         Configuration conf = new Configuration();
         conf.clear();
         if(hadoopConfig != null) {
@@ -62,8 +62,8 @@ public static void initFileFactory(Map<String,String> hadoopConfig, String defau
             }
         }
 
-        if(StringUtils.isNotBlank(defaultFS)) {
-            conf.set("fs.default.name", defaultFS);
+        if(StringUtils.isNotBlank(defaultFs)) {
+            conf.set("fs.default.name", defaultFs);
         }
         conf.set("fs.hdfs.impl.disable.cache", "true");
 

File: flinkx-carbondata/flinkx-carbondata-writer/src/main/java/com/dtstack/flinkx/carbondata/writer/dict/CarbonDictionaryUtil.java
Patch:
@@ -52,7 +52,7 @@
 
 
 /**
- * A object which provide a method to generate global dictionary from CSV files.
+ * @author demotto
  */
 public class CarbonDictionaryUtil {
 

File: flinkx-carbondata/flinkx-carbondata-writer/src/main/java/com/dtstack/flinkx/carbondata/writer/dict/DictionaryDetailHelper.java
Patch:
@@ -54,7 +54,7 @@ public boolean accept(CarbonFile pathname) {
             }
         });
         // 2 put dictionary file names to fileNamesMap
-        Map<String, Integer> fileNamesMap = new HashMap<>();
+        Map<String, Integer> fileNamesMap = new HashMap<>(carbonFiles.length);
         for(int i = 0; i < carbonFiles.length; ++i) {
             fileNamesMap.put(carbonFiles[i].getName(), i);
         }

File: flinkx-carbondata/flinkx-carbondata-writer/src/main/java/com/dtstack/flinkx/carbondata/writer/recordwriter/CarbonPartitionRecordWriter.java
Patch:
@@ -21,10 +21,7 @@
 
 
 import com.dtstack.flinkx.carbondata.writer.dict.CarbonTypeConverter;
-import com.dtstack.flinkx.util.DateUtil;
-import com.dtstack.flinkx.util.StringUtil;
 import org.apache.carbondata.core.metadata.datatype.DataType;
-import org.apache.carbondata.core.metadata.datatype.DataTypes;
 import org.apache.carbondata.core.metadata.schema.PartitionInfo;
 import org.apache.carbondata.core.metadata.schema.partition.PartitionType;
 import org.apache.carbondata.core.metadata.schema.table.CarbonTable;

File: flinkx-clickhouse/flinkx-clickhouse-core/src/main/java/com/dtstack/flinkx/clickhouse/core/ClickhouseDatabaseMeta.java
Patch:
@@ -41,12 +41,12 @@ public String getDriverClass() {
     }
 
     @Override
-    public String getSQLQueryFields(String tableName) {
+    public String getSqlQueryFields(String tableName) {
         return "SELECT * FROM " + tableName + " LIMIT 1";
     }
 
     @Override
-    public String getSQLQueryColumnFields(List<String> column, String table) {
+    public String getSqlQueryColumnFields(List<String> column, String table) {
         return "SELECT " + quoteColumns(column) + " FROM " + quoteTable(table) + " LIMIT 1";
     }
 

File: flinkx-clickhouse/flinkx-clickhouse-writer/src/main/java/com/dtstack/flinkx/clickhouse/format/ClickhouseOutputFormat.java
Patch:
@@ -19,7 +19,7 @@
 
 import com.dtstack.flinkx.clickhouse.core.ClickhouseUtil;
 import com.dtstack.flinkx.rdb.outputformat.JdbcOutputFormat;
-import com.dtstack.flinkx.rdb.util.DBUtil;
+import com.dtstack.flinkx.rdb.util.DbUtil;
 import com.dtstack.flinkx.util.ClassUtil;
 import com.dtstack.flinkx.util.ExceptionUtil;
 import org.apache.commons.collections.CollectionUtils;
@@ -46,7 +46,7 @@ public class ClickhouseOutputFormat extends JdbcOutputFormat {
     protected void openInternal(int taskNumber, int numTasks) {
         try {
             ClassUtil.forName(driverName, getClass().getClassLoader());
-            dbConn = ClickhouseUtil.getConnection(dbURL, username, password);
+            dbConn = ClickhouseUtil.getConnection(dbUrl, username, password);
 
             if (restoreConfig.isRestore()) {
                 dbConn.setAutoCommit(false);
@@ -91,7 +91,7 @@ private void initFullColumnAndType() throws SQLException {
             LOG.error("error to get {} schema, e = {}", table, ExceptionUtil.getErrorMessage(e));
             throw e;
         }finally {
-            DBUtil.closeDBResources(rs, stmt,null, false);
+            DbUtil.closeDbResources(rs, stmt,null, false);
         }
 
         if(CollectionUtils.isEmpty(fullColumn)) {

File: flinkx-core/src/main/java/com/dtstack/flink/api/java/MyLocalStreamEnvironment.java
Patch:
@@ -22,7 +22,6 @@
 import org.apache.flink.api.common.InvalidProgramException;
 import org.apache.flink.api.common.JobExecutionResult;
 import org.apache.flink.api.java.ExecutionEnvironment;
-import org.apache.flink.configuration.ConfigConstants;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.configuration.RestOptions;
 import org.apache.flink.configuration.TaskManagerOptions;
@@ -48,6 +47,8 @@
  *
  * <p>When this environment is instantiated, it uses a default parallelism of {@code 1}. The default
  * parallelism can be set via {@link #setParallelism(int)}.
+ *
+ * @author jiangbo
  */
 @Public
 public class MyLocalStreamEnvironment extends StreamExecutionEnvironment {
@@ -123,7 +124,7 @@ public JobExecutionResult execute(String jobName) throws Exception {
         Configuration configuration = new Configuration();
         configuration.addAll(jobGraph.getJobConfiguration());
         configuration.setString(TaskManagerOptions.MANAGED_MEMORY_SIZE, "0");
-        configuration.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, jobGraph.getMaximumParallelism());
+        configuration.setInteger(TaskManagerOptions.NUM_TASK_SLOTS.key(), jobGraph.getMaximumParallelism());
 
         // add (and override) the settings with what the user defined
         configuration.addAll(this.configuration);

File: flinkx-core/src/main/java/com/dtstack/flinkx/classloader/ClassLoaderSupplierCallBack.java
Patch:
@@ -20,9 +20,9 @@
 package com.dtstack.flinkx.classloader;
 
 /**
- * company: www.dtstack.com
- * author: toutian
- * create: 2019/10/14
+ * @company: www.dtstack.com
+ * @author: toutian
+ * @create: 2019/10/14
  */
 public class ClassLoaderSupplierCallBack {
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/classloader/PluginUtil.java
Patch:
@@ -24,9 +24,7 @@
 import java.io.File;
 import java.net.MalformedURLException;
 import java.net.URL;
-import java.util.ArrayList;
 import java.util.HashSet;
-import java.util.List;
 import java.util.Set;
 
 /**

File: flinkx-core/src/main/java/com/dtstack/flinkx/config/DataTransferConfig.java
Patch:
@@ -101,7 +101,6 @@ private static void checkConfig(DataTransferConfig config) {
 
     public static DataTransferConfig parse(String json) {
         Gson gson = new Gson();
-        //DataTransferConfig config = gson.fromJson(json, DataTransferConfig.class);
         Map<String,Object> map = gson.fromJson(json, Map.class);
         map = MapUtil.convertToHashMap(map);
         DataTransferConfig config = new DataTransferConfig(map);

File: flinkx-core/src/main/java/com/dtstack/flinkx/config/ErrorLimitConfig.java
Patch:
@@ -41,7 +41,7 @@ public ErrorLimitConfig(Map<String, Object> map) {
     }
 
     public static ErrorLimitConfig defaultConfig(){
-        Map<String, Object> map = new HashMap<>();
+        Map<String, Object> map = new HashMap<>(2);
         map.put("record",DEFAULT_ERROR_RECORD_LIMIT);
         map.put("percentage",DEFAULT_ERROR_PERCENTAGE_LIMIT);
         return new ErrorLimitConfig(map);

File: flinkx-core/src/main/java/com/dtstack/flinkx/decoder/PlainDecoder.java
Patch:
@@ -15,7 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package com.dtstack.flinkx.kafkaBase.decoder;
+package com.dtstack.flinkx.decoder;
 
 import java.util.Collections;
 import java.util.Map;

File: flinkx-core/src/main/java/com/dtstack/flinkx/latch/LocalLatch.java
Patch:
@@ -28,7 +28,7 @@
  * Company: www.dtstack.com
  * @author huyifan.zju@163.com
  */
-public class LocalLatch extends Latch {
+public class LocalLatch extends BaseLatch {
 
     private static Map<String, AtomicInteger> valMap = new ConcurrentHashMap<>();
     private AtomicInteger val;

File: flinkx-core/src/main/java/com/dtstack/flinkx/metrics/BaseMetric.java
Patch:
@@ -64,7 +64,7 @@ public Map<String, LongCounter> getMetricCounters() {
         return metricCounters;
     }
 
-    public void waitForMetricReport(){
+    public void waitForReportMetrics() {
         try {
             Thread.sleep(delayPeriodMill);
         } catch (InterruptedException e){

File: flinkx-core/src/main/java/com/dtstack/flinkx/metrics/SimpleLongCounterMeterView.java
Patch:
@@ -36,6 +36,8 @@
  * smoother transitions between rates.
  *
  * <p>The events are counted by a {@link Counter}.
+ *
+ * @author toutian
  */
 public class SimpleLongCounterMeterView implements Meter, View {
 	/** The underlying counter maintaining the count. */

File: flinkx-core/src/main/java/com/dtstack/flinkx/outputformat/FileOutputFormatBuilder.java
Patch:
@@ -28,11 +28,11 @@
  * @author jiangbo
  * @date 2019/8/28
  */
-public class FileOutputFormatBuilder extends RichOutputFormatBuilder {
+public class FileOutputFormatBuilder extends BaseRichOutputFormatBuilder {
 
-    protected FileOutputFormat format;
+    protected BaseFileOutputFormat format;
 
-    public void setFormat(FileOutputFormat format) {
+    public void setFormat(BaseFileOutputFormat format) {
         this.format = format;
         super.format = format;
     }

File: flinkx-core/src/main/java/com/dtstack/flinkx/reader/DataReaderFactory.java
Patch:
@@ -37,7 +37,7 @@ public class DataReaderFactory {
     private DataReaderFactory() {
     }
 
-    public static DataReader getDataReader(DataTransferConfig config, StreamExecutionEnvironment env) {
+    public static BaseDataReader getDataReader(DataTransferConfig config, StreamExecutionEnvironment env) {
         try {
             String pluginName = config.getJob().getContent().get(0).getReader().getName();
             String pluginClassName = PluginUtil.getPluginClassName(pluginName);
@@ -46,7 +46,7 @@ public static DataReader getDataReader(DataTransferConfig config, StreamExecutio
             return ClassLoaderManager.newInstance(urlList, cl -> {
                 Class<?> clazz = cl.loadClass(pluginClassName);
                 Constructor constructor = clazz.getConstructor(DataTransferConfig.class, StreamExecutionEnvironment.class);
-                return (DataReader)constructor.newInstance(config, env);
+                return (BaseDataReader)constructor.newInstance(config, env);
             });
         } catch (Exception e) {
             throw new RuntimeException(e);

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/ClassUtil.java
Patch:
@@ -33,10 +33,10 @@ public class ClassUtil {
 
     private static final Logger LOG = LoggerFactory.getLogger(ClassUtil.class);
 
-    public final static String lock_str = "jdbc_lock_str";
+    public final static Object LOCK_STR = new Object();
 
     public static void forName(String clazz, ClassLoader classLoader)  {
-        synchronized (lock_str){
+        synchronized (LOCK_STR){
             try {
                 Class.forName(clazz, true, classLoader);
                 DriverManager.setLoginTimeout(10);

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/FileSystemUtil.java
Patch:
@@ -108,7 +108,7 @@ public static UserGroupInformation getUGI(Map<String, Object> hadoopConfig, Stri
         String principal = KerberosUtil.getPrincipal(hadoopConfig, keytabFileName);
         KerberosUtil.loadKrb5Conf(hadoopConfig);
 
-        UserGroupInformation ugi = KerberosUtil.loginAndReturnUGI(getConfiguration(hadoopConfig, defaultFs), principal, keytabFileName);
+        UserGroupInformation ugi = KerberosUtil.loginAndReturnUgi(getConfiguration(hadoopConfig, defaultFs), principal, keytabFileName);
         UserGroupInformation.setLoginUser(ugi);
 
         return ugi;

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/RowUtil.java
Patch:
@@ -23,7 +23,7 @@
 import org.apache.flink.types.Row;
 import org.apache.flink.util.Preconditions;
 
-import java.util.HashMap;
+import java.util.LinkedHashMap;
 import java.util.Map;
 
 /**
@@ -37,7 +37,7 @@ public class RowUtil {
 
     public static String rowToJson(Row row, String[] colName) {
         Preconditions.checkNotNull(colName);
-        Map<String,Object> map = new HashMap<>();
+        Map<String,Object> map = new LinkedHashMap<>(colName.length);
 
         for(int i = 0; i < colName.length; ++i) {
             String key = colName[i];

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/StringUtil.java
Patch:
@@ -38,6 +38,8 @@
  */
 public class StringUtil {
 
+    public static final int STEP_SIZE = 2;
+
     /**
      * Handle the escaped escape charactor.
      *
@@ -228,7 +230,7 @@ public static byte[] hexStringToByteArray(String hexString) {
         int length = hexString.length();
 
         byte[] bytes = new byte[length / 2];
-        for (int i = 0; i < length; i += 2) {
+        for (int i = 0; i < length; i += STEP_SIZE) {
             bytes[i / 2] = (byte) ((Character.digit(hexString.charAt(i), 16) << 4)
                     + Character.digit(hexString.charAt(i+1), 16));
         }

File: flinkx-core/src/main/java/com/dtstack/flinkx/writer/DataWriterFactory.java
Patch:
@@ -36,7 +36,7 @@ public class DataWriterFactory {
 
     private DataWriterFactory() {}
 
-    public static DataWriter getDataWriter(DataTransferConfig config) {
+    public static BaseDataWriter getDataWriter(DataTransferConfig config) {
         try {
             String pluginName = config.getJob().getContent().get(0).getWriter().getName();
             String pluginClassName = PluginUtil.getPluginClassName(pluginName);
@@ -45,7 +45,7 @@ public static DataWriter getDataWriter(DataTransferConfig config) {
             return ClassLoaderManager.newInstance(urlList, cl -> {
                 Class<?> clazz = cl.loadClass(pluginClassName);
                 Constructor constructor = clazz.getConstructor(DataTransferConfig.class);
-                return (DataWriter)constructor.newInstance(config);
+                return (BaseDataWriter)constructor.newInstance(config);
             });
         } catch (Exception e) {
             throw new RuntimeException(e);

File: flinkx-db2/flinkx-db2-core/src/main/java/com/dtstack/flinkx/db2/Db2DatabaseMeta.java
Patch:
@@ -51,12 +51,12 @@ protected String makeValues(List<String> column) {
     }
 
     @Override
-    public String getSQLQueryFields(String tableName) {
+    public String getSqlQueryFields(String tableName) {
         return "SELECT * FROM " + tableName + " FETCH FIRST  1 ROWS ONLY";
     }
 
     @Override
-    public String getSQLQueryColumnFields(List<String> column, String table) {
+    public String getSqlQueryColumnFields(List<String> column, String table) {
         return "SELECT " + quoteColumns(column) + " FROM " + quoteTable(table) + " FETCH FIRST  1 ROWS ONLY";
     }
 

File: flinkx-db2/flinkx-db2-reader/src/main/java/com/dtstack/flinkx/db2/format/Db2InputFormat.java
Patch:
@@ -22,7 +22,7 @@
 
 import java.io.IOException;
 
-import static com.dtstack.flinkx.rdb.util.DBUtil.clobToString;
+import static com.dtstack.flinkx.rdb.util.DbUtil.clobToString;
 
 /**
  * Date: 2019/09/20

File: flinkx-db2/flinkx-db2-writer/src/main/java/com/dtstack/flinkx/db2/format/Db2OutputFormat.java
Patch:
@@ -37,7 +37,7 @@ public class Db2OutputFormat extends JdbcOutputFormat {
 
     @Override
     protected Map<String, List<String>> probePrimaryKeys(String table, Connection dbConn) throws SQLException {
-        Map<String, List<String>> map = new HashMap<>();
+        Map<String, List<String>> map = new HashMap<>(16);
         ResultSet rs = dbConn.getMetaData().getIndexInfo(null, null, table.toUpperCase(), true, false);
         while(rs.next()) {
             String indexName = rs.getString("INDEX_NAME");
@@ -46,7 +46,7 @@ protected Map<String, List<String>> probePrimaryKeys(String table, Connection db
             }
             map.get(indexName).add(rs.getString("COLUMN_NAME"));
         }
-        Map<String,List<String>> retMap = new HashMap<>();
+        Map<String,List<String>> retMap = new HashMap<>((map.size()<<2)/3);
         for(Map.Entry<String,List<String>> entry: map.entrySet()) {
             String k = entry.getKey();
             List<String> v = entry.getValue();

File: flinkx-dm/flinkx-dm-core/src/main/java/com/dtstack/flinkx/dm/DmDatabaseMeta.java
Patch:
@@ -47,7 +47,7 @@ protected String makeValues(List<String> column) {
     public String quoteTable(String table) {
         table = table.replace("\"","");
         String[] part = table.split("\\.");
-        if(part.length == 2) {
+        if(part.length == DB_TABLE_PART_SIZE) {
             table = getStartQuote() + part[0] + getEndQuote() + "." + getStartQuote() + part[1] + getEndQuote();
         } else {
             table = getStartQuote() + table + getEndQuote();
@@ -66,12 +66,12 @@ public String getDriverClass() {
     }
 
     @Override
-    public String getSQLQueryFields(String tableName) {
+    public String getSqlQueryFields(String tableName) {
         return "SELECT * FROM " + tableName + " LIMIT 1";
     }
 
     @Override
-    public String getSQLQueryColumnFields(List<String> column, String table) {
+    public String getSqlQueryColumnFields(List<String> column, String table) {
         return "SELECT " + quoteColumns(column) + " FROM " + quoteTable(table) + " LIMIT 1";
     }
 

File: flinkx-emqx/flinkx-emqx-reader/src/main/java/com/dtstack/flinkx/emqx/format/EmqxInputFormatBuilder.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */package com.dtstack.flinkx.emqx.format;
 
-import com.dtstack.flinkx.inputformat.RichInputFormatBuilder;
+import com.dtstack.flinkx.inputformat.BaseRichInputFormatBuilder;
 import org.apache.commons.lang3.StringUtils;
 
 /**
@@ -25,7 +25,7 @@
  *
  * @author tudou
  */
-public class EmqxInputFormatBuilder extends RichInputFormatBuilder {
+public class EmqxInputFormatBuilder extends BaseRichInputFormatBuilder {
 
     private EmqxInputFormat format;
 

File: flinkx-emqx/flinkx-emqx-writer/src/main/java/com/dtstack/flinkx/emqx/format/EmqxOutputFormat.java
Patch:
@@ -17,9 +17,9 @@
  */
 package com.dtstack.flinkx.emqx.format;
 
-import com.dtstack.flinkx.emqx.decoder.JsonDecoder;
+import com.dtstack.flinkx.decoder.JsonDecoder;
 import com.dtstack.flinkx.exception.WriteRecordException;
-import com.dtstack.flinkx.outputformat.RichOutputFormat;
+import com.dtstack.flinkx.outputformat.BaseRichOutputFormat;
 import com.dtstack.flinkx.util.ExceptionUtil;
 import org.apache.commons.lang3.StringUtils;
 import org.apache.flink.types.Row;
@@ -41,7 +41,7 @@
  *
  * @author tudou
  */
-public class EmqxOutputFormat extends RichOutputFormat {
+public class EmqxOutputFormat extends BaseRichOutputFormat {
     private static final Logger LOG = LoggerFactory.getLogger(EmqxOutputFormat.class);
     private static final String CLIENT_ID_PRE = "writer";
 

File: flinkx-emqx/flinkx-emqx-writer/src/main/java/com/dtstack/flinkx/emqx/format/EmqxOutputFormatBuilder.java
Patch:
@@ -17,7 +17,7 @@
  */
 package com.dtstack.flinkx.emqx.format;
 
-import com.dtstack.flinkx.outputformat.RichOutputFormatBuilder;
+import com.dtstack.flinkx.outputformat.BaseRichOutputFormatBuilder;
 import org.apache.commons.lang3.StringUtils;
 
 /**
@@ -26,7 +26,7 @@
  *
  * @author tudou
  */
-public class EmqxOutputFormatBuilder extends RichOutputFormatBuilder {
+public class EmqxOutputFormatBuilder extends BaseRichOutputFormatBuilder {
     private EmqxOutputFormat format;
 
     public EmqxOutputFormatBuilder(){

File: flinkx-es/flinkx-es-core/src/main/java/com/dtstack/flinkx/es/EsUtil.java
Patch:
@@ -103,7 +103,7 @@ public static Row jsonMapToRow(Map<String,Object> map, List<String> fields, List
 
     public static Map<String, Object> rowToJsonMap(Row row, List<String> fields, List<String> types) throws WriteRecordException {
         Preconditions.checkArgument(row.getArity() == fields.size());
-        Map<String,Object> jsonMap = new HashMap<>();
+        Map<String,Object> jsonMap = new HashMap<>((fields.size()<<2)/3);
         int i = 0;
         try {
             for(; i < fields.size(); ++i) {
@@ -113,7 +113,7 @@ public static Map<String, Object> rowToJsonMap(Row row, List<String> fields, Lis
                 for(int j = 0; j < parts.length - 1; ++j) {
                     String key = parts[j];
                     if(currMap.get(key) == null) {
-                        currMap.put(key, new HashMap<String,Object>());
+                        currMap.put(key, new HashMap<String,Object>(16));
                     }
                     currMap = (Map<String, Object>) currMap.get(key);
                 }

File: flinkx-es/flinkx-es-writer/src/main/java/com/dtstack/flinkx/es/writer/EsOutputFormat.java
Patch:
@@ -21,7 +21,7 @@
 import com.dtstack.flinkx.es.EsUtil;
 import com.dtstack.flinkx.exception.WriteRecordException;
 import com.dtstack.flinkx.util.StringUtil;
-import com.dtstack.flinkx.outputformat.RichOutputFormat;
+import com.dtstack.flinkx.outputformat.BaseRichOutputFormat;
 import org.apache.commons.lang.StringUtils;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.types.Row;
@@ -40,7 +40,7 @@
  * Company: www.dtstack.com
  * @author huyifan.zju@163.com
  */
-public class EsOutputFormat extends RichOutputFormat {
+public class EsOutputFormat extends BaseRichOutputFormat {
 
     protected String address;
 

File: flinkx-es/flinkx-es-writer/src/main/java/com/dtstack/flinkx/es/writer/EsOutputFormatBuilder.java
Patch:
@@ -18,7 +18,7 @@
 
 package com.dtstack.flinkx.es.writer;
 
-import com.dtstack.flinkx.outputformat.RichOutputFormatBuilder;
+import com.dtstack.flinkx.outputformat.BaseRichOutputFormatBuilder;
 import java.util.List;
 import java.util.Map;
 
@@ -28,7 +28,7 @@
  * Company: www.dtstack.com
  * @author huyifan_zju@163.com
  */
-public class EsOutputFormatBuilder extends RichOutputFormatBuilder {
+public class EsOutputFormatBuilder extends BaseRichOutputFormatBuilder {
 
     private EsOutputFormat format;
 

File: flinkx-es/flinkx-es-writer/src/main/java/com/dtstack/flinkx/es/writer/EsWriter.java
Patch:
@@ -21,7 +21,7 @@
 import com.dtstack.flinkx.config.DataTransferConfig;
 import com.dtstack.flinkx.config.WriterConfig;
 import com.dtstack.flinkx.es.EsConfigKeys;
-import com.dtstack.flinkx.writer.DataWriter;
+import com.dtstack.flinkx.writer.BaseDataWriter;
 import org.apache.commons.collections.CollectionUtils;
 import org.apache.flink.streaming.api.datastream.DataStream;
 import org.apache.flink.streaming.api.datastream.DataStreamSink;
@@ -37,7 +37,7 @@
  * Company: www.dtstack.com
  * @author huyifan.zju@163.com
  */
-public class EsWriter extends DataWriter {
+public class EsWriter extends BaseDataWriter {
 
     public static final int DEFAULT_BULK_ACTION = 100;
 

File: flinkx-examples/src/main/java/com/dtstack/flinkx/examples/ExampleGenerator.java
Patch:
@@ -31,7 +31,6 @@
 import java.io.IOException;
 import java.io.InputStreamReader;
 import java.io.OutputStreamWriter;
-import java.nio.charset.Charset;
 import java.nio.charset.StandardCharsets;
 import java.util.ArrayList;
 import java.util.List;

File: flinkx-ftp/flinkx-ftp-core/src/main/java/com/dtstack/flinkx/ftp/FtpConfig.java
Patch:
@@ -58,7 +58,7 @@ public class FtpConfig implements Serializable {
 
     public int timeout = FtpConfigConstants.DEFAULT_TIMEOUT;
 
-    public long maxFileSize = 1024 * 1024 * 1024;
+    public long maxFileSize = 1024 * 1024 * 1024L;
 
     public String getUsername() {
         return username;

File: flinkx-ftp/flinkx-ftp-reader/src/main/java/com/dtstack/flinkx/ftp/reader/FtpSeqInputStream.java
Patch:
@@ -67,7 +67,8 @@ final void nextStream() throws IOException {
     @Override
     public int available() throws IOException {
         if (in == null) {
-            return 0; // no way to signal EOF from available()
+            // no way to signal EOF from available()
+            return 0;
         }
         return in.available();
     }
@@ -85,7 +86,7 @@ public int read() throws IOException {
     }
 
     @Override
-    public int read(byte b[], int off, int len) throws IOException {
+    public int read(byte[] b, int off, int len) throws IOException {
         if (in == null) {
             return -1;
         } else if (b == null) {

File: flinkx-ftp/flinkx-ftp-writer/src/main/java/com/dtstack/flinkx/ftp/writer/FtpOutputFormatBuilder.java
Patch:
@@ -19,9 +19,7 @@
 package com.dtstack.flinkx.ftp.writer;
 
 import com.dtstack.flinkx.ftp.FtpConfig;
-import com.dtstack.flinkx.ftp.FtpConfigConstants;
 import com.dtstack.flinkx.outputformat.FileOutputFormatBuilder;
-import org.apache.commons.lang.StringUtils;
 import java.util.List;
 
 /**

File: flinkx-gbase/flinkx-gbase-core/src/main/java/com/dtstack/flinkx/gbase/GbaseDatabaseMeta.java
Patch:
@@ -78,7 +78,7 @@ protected String makeValues(List<String> column) {
     }
 
     @Override
-    public String getSQLQueryFields(String tableName) {
+    public String getSqlQueryFields(String tableName) {
         return "SELECT * FROM " + tableName + " LIMIT 0";
     }
 
@@ -93,7 +93,7 @@ public String getEndQuote() {
     }
 
     @Override
-    public String getSQLQueryColumnFields(List<String> column, String table) {
+    public String getSqlQueryColumnFields(List<String> column, String table) {
         return "SELECT " + quoteColumns(column) + " FROM " + quoteTable(table) + " LIMIT 0";
     }
 

File: flinkx-gbase/flinkx-gbase-writer/src/main/java/com/dtstack/flinkx/gbase/writer/GbaseWriter.java
Patch:
@@ -24,7 +24,7 @@
 import com.dtstack.flinkx.gbase.format.GbaseOutputFormat;
 import com.dtstack.flinkx.rdb.datawriter.JdbcDataWriter;
 import com.dtstack.flinkx.rdb.outputformat.JdbcOutputFormatBuilder;
-import com.dtstack.flinkx.rdb.util.DBUtil;
+import com.dtstack.flinkx.rdb.util.DbUtil;
 
 /**
  * @author jiangbo
@@ -35,7 +35,7 @@ public class GbaseWriter extends JdbcDataWriter {
     public GbaseWriter(DataTransferConfig config) {
         super(config);
         setDatabaseInterface(new GbaseDatabaseMeta());
-        dbUrl = DBUtil.formatJdbcUrl(dbUrl, null);
+        dbUrl = DbUtil.formatJdbcUrl(dbUrl, null);
     }
 
     @Override

File: flinkx-hbase/flinkx-hbase-core/src/main/java/com/dtstack/flinkx/hbase/HbaseConfigConstants.java
Patch:
@@ -44,7 +44,7 @@ public class HbaseConfigConstants {
 
     public static final String DEFAULT_NULL_MODE = "skip";
 
-    public static final long DEFAULT_WRITE_BUFFER_SIZE = 8 * 1024 * 1024;
+    public static final long DEFAULT_WRITE_BUFFER_SIZE = 8 * 1024 * 1024L;
 
     public static final boolean DEFAULT_WAL_FLAG = false;
 

File: flinkx-hbase/flinkx-hbase-reader/src/main/java/com/dtstack/flinkx/hbase/reader/HbaseInputFormatBuilder.java
Patch:
@@ -18,7 +18,7 @@
 package com.dtstack.flinkx.hbase.reader;
 
 import com.dtstack.flinkx.hbase.HbaseConfigConstants;
-import com.dtstack.flinkx.inputformat.RichInputFormatBuilder;
+import com.dtstack.flinkx.inputformat.BaseRichInputFormatBuilder;
 import org.apache.commons.lang.StringUtils;
 import org.apache.flink.util.Preconditions;
 import java.util.List;
@@ -30,7 +30,7 @@
  * Company: www.dtstack.com
  * @author huyifan.zju@163.com
  */
-public class HbaseInputFormatBuilder extends RichInputFormatBuilder {
+public class HbaseInputFormatBuilder extends BaseRichInputFormatBuilder {
 
     private HbaseInputFormat format;
 

File: flinkx-hbase/flinkx-hbase-writer/src/main/java/com/dtstack/flinkx/hbase/writer/HbaseOutputFormatBuilder.java
Patch:
@@ -19,7 +19,7 @@
 package com.dtstack.flinkx.hbase.writer;
 
 import com.dtstack.flinkx.hbase.HbaseConfigConstants;
-import com.dtstack.flinkx.outputformat.RichOutputFormatBuilder;
+import com.dtstack.flinkx.outputformat.BaseRichOutputFormatBuilder;
 import org.apache.commons.lang.StringUtils;
 import com.google.common.base.Preconditions;
 import java.util.List;
@@ -31,7 +31,7 @@
  * Company: www.dtstack.com
  * @author huyifan.zju@163.com
  */
-public class HbaseOutputFormatBuilder extends RichOutputFormatBuilder {
+public class HbaseOutputFormatBuilder extends BaseRichOutputFormatBuilder {
 
     private HbaseOutputFormat format;
 

File: flinkx-hbase/flinkx-hbase-writer/src/main/java/com/dtstack/flinkx/hbase/writer/function/FunctionFactory.java
Patch:
@@ -35,7 +35,7 @@ public static IFunction createFuntion(String functionName) {
         IFunction function = null;
         switch (functionName.toUpperCase()) {
             case "MD5":
-                function = new MD5Function();
+                function = new Md5Function();
                 break;
             case "STRING":
                 function = new StringFunction();

File: flinkx-hdfs/flinkx-hdfs-core/src/main/java/com/dtstack/flinkx/hdfs/ECompressType.java
Patch:
@@ -19,7 +19,6 @@
 package com.dtstack.flinkx.hdfs;
 
 import org.apache.commons.lang.StringUtils;
-import org.apache.parquet.hadoop.metadata.CompressionCodecName;
 
 /**
  * @author jiangbo

File: flinkx-hdfs/flinkx-hdfs-core/src/main/java/com/dtstack/flinkx/hdfs/HdfsConfigKeys.java
Patch:
@@ -1,4 +1,4 @@
-/**
+/*
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information
@@ -60,4 +60,6 @@ public class HdfsConfigKeys {
 
     public static final String KEY_FLUSH_INTERVAL = "flushInterval";
 
+    public static final String KEY_ENABLE_DICTIONARY = "enableDictionary";
+
 }

File: flinkx-hdfs/flinkx-hdfs-core/src/main/java/org/apache/hadoop/hive/shims/ShimLoader.java
Patch:
@@ -27,6 +27,7 @@
 /**
  * ShimLoader.
  *
+ * @author jiangbo
  */
 public abstract class ShimLoader {
     private static HadoopShims hadoopShims;

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsTextOutputFormat.java
Patch:
@@ -42,7 +42,7 @@
  * Company: www.dtstack.com
  * @author huyifan.zju@163.com
  */
-public class HdfsTextOutputFormat extends HdfsOutputFormat {
+public class HdfsTextOutputFormat extends BaseHdfsOutputFormat {
 
     private static final int NEWLINE = 10;
     private transient OutputStream stream;
@@ -124,7 +124,7 @@ public void writeSingleRecordToFile(Row row) throws WriteRecordException {
                     sb.append(delimiter);
                 }
 
-                appendDataToString(sb, i, row.getField(j), ColumnType.fromString(columnTypes.get(j)));
+                appendDataToString(sb, row.getField(j), ColumnType.fromString(columnTypes.get(j)));
             }
         } catch(Exception e) {
             if(i < row.getArity()) {
@@ -151,7 +151,7 @@ public void writeSingleRecordToFile(Row row) throws WriteRecordException {
         }
     }
 
-    private void appendDataToString(StringBuilder sb, int ii, Object column, ColumnType columnType) {
+    private void appendDataToString(StringBuilder sb, Object column, ColumnType columnType) {
         if(column == null) {
             sb.append(HdfsUtil.NULL_VALUE);
             return;

File: flinkx-hive/flinkx-hive-core/src/main/java/com/dtstack/flinkx/hive/util/AbstractHiveMetadataParser.java
Patch:
@@ -25,7 +25,9 @@
 import java.util.List;
 import java.util.Map;
 
-import static com.dtstack.flinkx.hive.EStoreType.*;
+import static com.dtstack.flinkx.hive.EStoreType.ORC;
+import static com.dtstack.flinkx.hive.EStoreType.PARQUET;
+import static com.dtstack.flinkx.hive.EStoreType.TEXT;
 
 /**
  * @author jiangbo

File: flinkx-hive/flinkx-hive-core/src/main/java/com/dtstack/flinkx/hive/util/Cdh2HiveMetadataParser.java
Patch:
@@ -29,7 +29,7 @@
  * @author jiangbo
  * @date 2019/11/29
  */
-public class CDH2HiveMetadataParser extends AbstractHiveMetadataParser {
+public class Cdh2HiveMetadataParser extends AbstractHiveMetadataParser {
 
     @Override
     public void fillTableInfo(TableInfo tableInfo, List<Map<String, Object>> result) {

File: flinkx-kafka/flinkx-kafka-reader/src/main/java/com/dtstack/flinkx/kafka/reader/KafkaConsumer.java
Patch:
@@ -16,8 +16,8 @@
  */
 package com.dtstack.flinkx.kafka.reader;
 
-import com.dtstack.flinkx.kafkaBase.reader.KafkaBaseConsumer;
-import com.dtstack.flinkx.kafkaBase.reader.KafkaBaseInputFormat;
+import com.dtstack.flinkx.kafkabase.reader.KafkaBaseConsumer;
+import com.dtstack.flinkx.kafkabase.reader.KafkaBaseInputFormat;
 
 import java.util.Arrays;
 import java.util.Properties;

File: flinkx-kafka/flinkx-kafka-reader/src/main/java/com/dtstack/flinkx/kafka/reader/KafkaInputFormat.java
Patch:
@@ -19,7 +19,7 @@
 
 package com.dtstack.flinkx.kafka.reader;
 
-import com.dtstack.flinkx.kafkaBase.reader.KafkaBaseInputFormat;
+import com.dtstack.flinkx.kafkabase.reader.KafkaBaseInputFormat;
 
 import java.io.IOException;
 import java.util.Properties;

File: flinkx-kafka/flinkx-kafka-reader/src/main/java/com/dtstack/flinkx/kafka/reader/KafkaReader.java
Patch:
@@ -18,8 +18,8 @@
 package com.dtstack.flinkx.kafka.reader;
 
 import com.dtstack.flinkx.config.DataTransferConfig;
-import com.dtstack.flinkx.kafkaBase.reader.KafkaBaseInputFormat;
-import com.dtstack.flinkx.kafkaBase.reader.KafkaBaseReader;
+import com.dtstack.flinkx.kafkabase.reader.KafkaBaseInputFormat;
+import com.dtstack.flinkx.kafkabase.reader.KafkaBaseReader;
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
 import org.apache.kafka.clients.producer.ProducerConfig;
 

File: flinkx-kafka/flinkx-kafka-writer/src/main/java/com/dtstack/flinkx/kafka/writer/KafkaOutputFormat.java
Patch:
@@ -18,8 +18,8 @@
 
 package com.dtstack.flinkx.kafka.writer;
 
-import com.dtstack.flinkx.kafkaBase.Formatter;
-import com.dtstack.flinkx.kafkaBase.writer.KafkaBaseOutputFormat;
+import com.dtstack.flinkx.kafkabase.Formatter;
+import com.dtstack.flinkx.kafkabase.writer.KafkaBaseOutputFormat;
 import org.apache.flink.configuration.Configuration;
 import org.apache.kafka.clients.producer.KafkaProducer;
 import org.apache.kafka.clients.producer.ProducerConfig;

File: flinkx-kafka/flinkx-kafka-writer/src/main/java/com/dtstack/flinkx/kafka/writer/KafkaWriter.java
Patch:
@@ -18,7 +18,7 @@
 package com.dtstack.flinkx.kafka.writer;
 
 import com.dtstack.flinkx.config.DataTransferConfig;
-import com.dtstack.flinkx.kafkaBase.writer.KafkaBaseWriter;
+import com.dtstack.flinkx.kafkabase.writer.KafkaBaseWriter;
 import org.apache.flink.streaming.api.datastream.DataStream;
 import org.apache.flink.streaming.api.datastream.DataStreamSink;
 import org.apache.flink.types.Row;

File: flinkx-kb/flinkx-kb-core/src/main/java/com/dtstack/flinkx/kafkabase/Formatter.java
Patch:
@@ -15,7 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package com.dtstack.flinkx.kafkaBase;
+package com.dtstack.flinkx.kafkabase;
 
 import org.joda.time.DateTime;
 import org.joda.time.DateTimeZone;
@@ -48,7 +48,7 @@ public static String format(Map event, String format, String timezone) {
         while (m.find()) {
             String match = m.group();
             String key = (String) match.subSequence(2, match.length() - 1);
-            if (key.equalsIgnoreCase("+s")) {
+            if ("+s".equalsIgnoreCase(key)) {
                 Object o = event.get("@timestamp");
                 if (o.getClass() == Long.class) {
                     m.appendReplacement(sb, o.toString());

File: flinkx-kudu/flinkx-kudu-writer/src/main/java/com/dtstack/flinkx/kudu/writer/KuduOutputFormatBuilder.java
Patch:
@@ -20,7 +20,7 @@
 package com.dtstack.flinkx.kudu.writer;
 
 import com.dtstack.flinkx.kudu.core.KuduConfig;
-import com.dtstack.flinkx.outputformat.RichOutputFormatBuilder;
+import com.dtstack.flinkx.outputformat.BaseRichOutputFormatBuilder;
 import com.dtstack.flinkx.reader.MetaColumn;
 
 import java.util.List;
@@ -29,7 +29,7 @@
  * @author jiangbo
  * @date 2019/7/31
  */
-public class KuduOutputFormatBuilder extends RichOutputFormatBuilder {
+public class KuduOutputFormatBuilder extends BaseRichOutputFormatBuilder {
 
     private KuduOutputFormat format;
 

File: flinkx-launcher/src/main/java/com/dtstack/flinkx/launcher/YarnConfLoader.java
Patch:
@@ -18,6 +18,7 @@
 
 package com.dtstack.flinkx.launcher;
 
+import com.dtstack.flinkx.constants.ConstantValue;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.yarn.conf.YarnConfiguration;
 
@@ -42,7 +43,7 @@ public static YarnConfiguration getYarnConf(String yarnConfDir) {
             if(dir.exists() && dir.isDirectory()) {
 
                 File[] xmlFileList = new File(yarnConfDir).listFiles((dir1, name) -> {
-                    if(name.endsWith(".xml")){
+                    if(name.endsWith(ConstantValue.FILE_SUFFIX_XML)){
                         return true;
                     }
                     return false;

File: flinkx-launcher/src/main/java/com/dtstack/flinkx/launcher/perjob/FlinkPerJobResourceUtil.java
Patch:
@@ -15,7 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package com.dtstack.flinkx.launcher.perJob;
+package com.dtstack.flinkx.launcher.perjob;
 
 import com.dtstack.flinkx.util.ValueUtil;
 import org.apache.flink.client.deployment.ClusterSpecification;
@@ -27,7 +27,7 @@
  * Company: www.dtstack.com
  * @author tudou
  */
-public class FLinkPerJobResourceUtil {
+public class FlinkPerJobResourceUtil {
     /**
      * Minimum memory requirements, checked by the Client.
      * the minimum memory should be higher than the min heap cutoff

File: flinkx-metadata/flinkx-metadata-reader/src/main/java/com/dtstack/flinkx/metadata/inputformat/BaseMetadataInputFormat.java
Patch:
@@ -17,7 +17,7 @@
  */
 package com.dtstack.flinkx.metadata.inputformat;
 
-import com.dtstack.flinkx.inputformat.RichInputFormat;
+import com.dtstack.flinkx.inputformat.BaseRichInputFormat;
 import com.dtstack.flinkx.metadata.util.ConnUtil;
 import com.dtstack.flinkx.util.ExceptionUtil;
 import org.apache.commons.collections.CollectionUtils;
@@ -39,7 +39,7 @@
  * @author : tiezhu
  * @date : 2020/3/20
  */
-public abstract class BaseMetadataInputFormat extends RichInputFormat {
+public abstract class BaseMetadataInputFormat extends BaseRichInputFormat {
 
     protected String dbUrl;
 

File: flinkx-metadata/flinkx-metadata-reader/src/main/java/com/dtstack/flinkx/metadata/inputformat/MetadataInputFormatBuilder.java
Patch:
@@ -17,7 +17,7 @@
  */
 package com.dtstack.flinkx.metadata.inputformat;
 
-import com.dtstack.flinkx.inputformat.RichInputFormatBuilder;
+import com.dtstack.flinkx.inputformat.BaseRichInputFormatBuilder;
 
 import java.util.List;
 import java.util.Map;
@@ -26,7 +26,7 @@
  * @author : tiezhu
  * @date : 2020/3/8
  */
-public class MetadataInputFormatBuilder extends RichInputFormatBuilder {
+public class MetadataInputFormatBuilder extends BaseRichInputFormatBuilder {
     private BaseMetadataInputFormat format;
 
     public MetadataInputFormatBuilder(BaseMetadataInputFormat format) {

File: flinkx-mongodb/flinkx-mongodb-reader/src/main/java/com/dtstack/flinkx/mongodb/reader/MongodbReader.java
Patch:
@@ -21,7 +21,7 @@
 import com.dtstack.flinkx.config.DataTransferConfig;
 import com.dtstack.flinkx.config.ReaderConfig;
 import com.dtstack.flinkx.mongodb.MongodbConfig;
-import com.dtstack.flinkx.reader.DataReader;
+import com.dtstack.flinkx.reader.BaseDataReader;
 import com.dtstack.flinkx.reader.MetaColumn;
 import org.apache.flink.streaming.api.datastream.DataStream;
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
@@ -35,7 +35,7 @@
  * @Company: www.dtstack.com
  * @author jiangbo
  */
-public class MongodbReader extends DataReader {
+public class MongodbReader extends BaseDataReader {
 
     private List<MetaColumn> metaColumns;
 

File: flinkx-hdfs/flinkx-hdfs-core/src/main/java/com/dtstack/flinkx/hdfs/HdfsUtil.java
Patch:
@@ -40,7 +40,7 @@ public class HdfsUtil {
     public static final String NULL_VALUE = "\\N";
 
     public static Object string2col(String str, String type, SimpleDateFormat customDateFormat) {
-        if (str == null || str.length() == 0){
+        if (str == null){
             return null;
         }
 

File: flinkx-hdfs/flinkx-hdfs-reader/src/main/java/com/dtstack/flinkx/hdfs/reader/HdfsTextInputFormat.java
Patch:
@@ -22,6 +22,7 @@
 import com.dtstack.flinkx.reader.MetaColumn;
 import jodd.util.StringUtil;
 import org.apache.commons.io.output.ByteArrayOutputStream;
+import org.apache.commons.lang3.StringUtils;
 import org.apache.flink.core.io.InputSplit;
 import org.apache.flink.types.Row;
 import org.apache.hadoop.fs.Path;
@@ -85,7 +86,7 @@ public void openInternal(InputSplit inputSplit) throws IOException {
     @Override
     public Row nextRecordInternal(Row row) throws IOException {
         String line = new String(((Text)value).getBytes(), 0, ((Text)value).getLength(), charsetName);
-        String[] fields = line.split(delimiter);
+        String[] fields = StringUtils.splitPreserveAllTokens(line, delimiter);
 
         if (metaColumns.size() == 1 && "*".equals(metaColumns.get(0).getName())){
             row = new Row(fields.length);

File: flinkx-hdfs/flinkx-hdfs-core/src/main/java/com/dtstack/flinkx/hdfs/HdfsConfigKeys.java
Patch:
@@ -60,4 +60,6 @@ public class HdfsConfigKeys {
 
     public static final String KEY_FLUSH_INTERVAL = "flushInterval";
 
+    public static final String KEY_ENABLE_DICTIONARY = "enableDictionary";
+
 }

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/BaseHdfsOutputFormat.java
Patch:
@@ -68,6 +68,8 @@ public abstract class BaseHdfsOutputFormat extends BaseFileOutputFormat {
 
     protected Configuration conf;
 
+    protected boolean enableDictionary;
+
     protected transient Map<String, ColumnTypeUtil.DecimalInfo> decimalColInfo;
 
     @Override

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsParquetOutputFormat.java
Patch:
@@ -91,6 +91,7 @@ protected void nextBlock(){
                     .withCompressionCodec(getCompressType())
                     .withConf(conf)
                     .withType(schema)
+                    .withDictionaryEncoding(enableDictionary)
                     .withRowGroupSize(rowGroupSize);
             writer = builder.build();
 

File: flinkx-hdfs/flinkx-hdfs-core/src/main/java/com/dtstack/flinkx/hdfs/HdfsUtil.java
Patch:
@@ -40,7 +40,7 @@ public class HdfsUtil {
     public static final String NULL_VALUE = "\\N";
 
     public static Object string2col(String str, String type, SimpleDateFormat customDateFormat) {
-        if (str == null || str.length() == 0){
+        if (str == null){
             return null;
         }
 

File: flinkx-hdfs/flinkx-hdfs-reader/src/main/java/com/dtstack/flinkx/hdfs/reader/HdfsTextInputFormat.java
Patch:
@@ -22,6 +22,7 @@
 import com.dtstack.flinkx.reader.MetaColumn;
 import jodd.util.StringUtil;
 import org.apache.commons.io.output.ByteArrayOutputStream;
+import org.apache.commons.lang3.StringUtils;
 import org.apache.flink.core.io.InputSplit;
 import org.apache.flink.types.Row;
 import org.apache.hadoop.fs.Path;
@@ -85,7 +86,7 @@ public void openInternal(InputSplit inputSplit) throws IOException {
     @Override
     public Row nextRecordInternal(Row row) throws IOException {
         String line = new String(((Text)value).getBytes(), 0, ((Text)value).getLength(), charsetName);
-        String[] fields = line.split(delimiter);
+        String[] fields = StringUtils.splitPreserveAllTokens(line, delimiter);
 
         if (metaColumns.size() == 1 && "*".equals(metaColumns.get(0).getName())){
             row = new Row(fields.length);

File: flinkx-core/src/main/java/com/dtstack/flinkx/outputformat/FileOutputFormat.java
Patch:
@@ -293,11 +293,12 @@ protected void afterCloseInternal()  {
                     moveAllTemporaryDataFileToDirectory();
 
                     LOG.info("The task ran successfully,clear temporary data files");
+                    closeSource();
                     clearTemporaryDataFiles();
                 }
+            }else{
+                closeSource();
             }
-
-            closeSource();
         } catch(Exception ex) {
             throw new RuntimeException(ex);
         }

File: flinkx-hbase/flinkx-hbase-core/src/main/java/com/dtstack/flinkx/hbase/HbaseHelper.java
Patch:
@@ -90,7 +90,7 @@ private static org.apache.hadoop.hbase.client.Connection getConnectionWithKerber
         String keytabFileName = KerberosUtil.getPrincipalFileName(hbaseConfigMap);
 
         keytabFileName = KerberosUtil.loadFile(hbaseConfigMap, keytabFileName);
-        String principal = KerberosUtil.findPrincipalFromKeytab(keytabFileName);
+        String principal = KerberosUtil.getPrincipal(hbaseConfigMap, keytabFileName);
         KerberosUtil.loadKrb5Conf(hbaseConfigMap);
 
         Configuration conf = FileSystemUtil.getConfiguration(hbaseConfigMap, null);

File: flinkx-hive/flinkx-hive-core/src/main/java/com/dtstack/flinkx/hive/util/DBUtil.java
Patch:
@@ -109,7 +109,7 @@ private static Connection getConnectionWithKerberos(ConnectionInfo connectionInf
         String keytabFileName = KerberosUtil.getPrincipalFileName(connectionInfo.getHiveConf());
 
         keytabFileName = KerberosUtil.loadFile(connectionInfo.getHiveConf(), keytabFileName);
-        String principal = KerberosUtil.findPrincipalFromKeytab(keytabFileName);
+        String principal = KerberosUtil.getPrincipal(connectionInfo.getHiveConf(), keytabFileName);
         KerberosUtil.loadKrb5Conf(connectionInfo.getHiveConf());
 
         Configuration conf = FileSystemUtil.getConfiguration(connectionInfo.getHiveConf(), null);

File: flinkx-hbase/flinkx-hbase-core/src/main/java/com/dtstack/flinkx/hbase/HbaseHelper.java
Patch:
@@ -90,7 +90,7 @@ private static org.apache.hadoop.hbase.client.Connection getConnectionWithKerber
         String keytabFileName = KerberosUtil.getPrincipalFileName(hbaseConfigMap);
 
         keytabFileName = KerberosUtil.loadFile(hbaseConfigMap, keytabFileName);
-        String principal = KerberosUtil.findPrincipalFromKeytab(keytabFileName);
+        String principal = KerberosUtil.getPrincipal(hbaseConfigMap, keytabFileName);
         KerberosUtil.loadKrb5Conf(hbaseConfigMap);
 
         Configuration conf = FileSystemUtil.getConfiguration(hbaseConfigMap, null);

File: flinkx-hive/flinkx-hive-core/src/main/java/com/dtstack/flinkx/hive/util/DBUtil.java
Patch:
@@ -109,7 +109,7 @@ private static Connection getConnectionWithKerberos(ConnectionInfo connectionInf
         String keytabFileName = KerberosUtil.getPrincipalFileName(connectionInfo.getHiveConf());
 
         keytabFileName = KerberosUtil.loadFile(connectionInfo.getHiveConf(), keytabFileName);
-        String principal = KerberosUtil.findPrincipalFromKeytab(keytabFileName);
+        String principal = KerberosUtil.getPrincipal(connectionInfo.getHiveConf(), keytabFileName);
         KerberosUtil.loadKrb5Conf(connectionInfo.getHiveConf());
 
         Configuration conf = FileSystemUtil.getConfiguration(connectionInfo.getHiveConf(), null);

File: flinkx-es/flinkx-es-writer/src/main/java/com/dtstack/flinkx/es/writer/EsOutputFormat.java
Patch:
@@ -119,8 +119,8 @@ private void processFailResponse(BulkResponse response){
                     dirtyDataManager.writeData(rows.get(i), exception);
                 }
 
-                if(numWriteCounter != null ){
-                    numWriteCounter.add(1);
+                if (errCounter != null) {
+                    errCounter.add(1);
                 }
             }
         }

File: flinkx-core/src/main/java/com/dtstack/flinkx/Main.java
Patch:
@@ -145,6 +145,8 @@ private static void configRestartStrategy(StreamExecutionEnvironment env, DataTr
             } else {
                 env.setRestartStrategy(RestartStrategies.noRestart());
             }
+        } else {
+            env.setRestartStrategy(RestartStrategies.noRestart());
         }
     }
 

File: flinkx-ftp/flinkx-ftp-core/src/main/java/com/dtstack/flinkx/ftp/SftpHandler.java
Patch:
@@ -43,9 +43,9 @@
  * Company: www.dtstack.com
  * @author huyifan.zju@163.com
  */
-public class SFtpHandler implements IFtpHandler {
+public class SftpHandler implements IFtpHandler {
 
-    private static final Logger LOG = LoggerFactory.getLogger(SFtpHandler.class);
+    private static final Logger LOG = LoggerFactory.getLogger(SftpHandler.class);
 
     private Session session = null;
 

File: flinkx-test/src/main/java/com/dtstack/flinkx/test/LocalTest.java
Patch:
@@ -133,7 +133,7 @@ public static void main(String[] args) throws Exception{
         conf.setString("metrics.reporter.promgateway.randomJobNameSuffix","true");
         conf.setString("metrics.reporter.promgateway.deleteOnShutdown","false");
 
-        String jobPath = "D:\\project\\flinkx\\flinkx-test\\src\\main\\resources\\dev_test_job\\hive2metareader.json";
+        String jobPath = "D:\\project\\flinkx\\flinkx-test\\src\\main\\resources\\dev_test_job\\stream_template.json";
         JobExecutionResult result = LocalTest.runJob(new File(jobPath), confProperties, null);
         ResultPrintUtil.printResult(result);
     }

File: flinkx-test/src/main/java/com/dtstack/flinkx/test/LocalTest.java
Patch:
@@ -133,7 +133,7 @@ public static void main(String[] args) throws Exception{
         conf.setString("metrics.reporter.promgateway.randomJobNameSuffix","true");
         conf.setString("metrics.reporter.promgateway.deleteOnShutdown","false");
 
-        String jobPath = "D:\\project\\flinkx\\flinkx-test\\src\\main\\resources\\dev_test_job\\hive2metareader.json";
+        String jobPath = "D:\\project\\flinkx\\flinkx-test\\src\\main\\resources\\dev_test_job\\stream_template.json";
         JobExecutionResult result = LocalTest.runJob(new File(jobPath), confProperties, null);
         ResultPrintUtil.printResult(result);
     }

File: flinkx-gbase/flinkx-gbase-reader/src/main/java/com/dtstack/flinkx/gbase/format/GbaseInputFormat.java
Patch:
@@ -49,7 +49,9 @@ public void openInternal(InputSplit inputSplit) throws IOException {
 
             String startLocation = incrementConfig.getStartLocation();
             if (incrementConfig.isPolling()) {
-                endLocationAccumulator.add(Long.parseLong(startLocation));
+                if (StringUtils.isNotEmpty(startLocation)) {
+                    endLocationAccumulator.add(Long.parseLong(startLocation));
+                }
                 isTimestamp = "timestamp".equalsIgnoreCase(incrementConfig.getColumnType());
             } else if ((incrementConfig.isIncrement() && incrementConfig.isUseMaxFunc())) {
                 getMaxValue(inputSplit);

File: flinkx-mysql/flinkx-mysql-reader/src/main/java/com/dtstack/flinkx/mysql/format/MysqlInputFormat.java
Patch:
@@ -51,7 +51,9 @@ public void openInternal(InputSplit inputSplit) throws IOException {
 
             String startLocation = incrementConfig.getStartLocation();
             if (incrementConfig.isPolling()) {
-                endLocationAccumulator.add(Long.parseLong(startLocation));
+                if (StringUtils.isNotEmpty(startLocation)) {
+                    endLocationAccumulator.add(Long.parseLong(startLocation));
+                }
                 isTimestamp = "timestamp".equalsIgnoreCase(incrementConfig.getColumnType());
             } else if ((incrementConfig.isIncrement() && incrementConfig.isUseMaxFunc())) {
                 getMaxValue(inputSplit);

File: flinkx-postgresql/flinkx-postgresql-reader/src/main/java/com/dtstack/flinkx/postgresql/format/PostgresqlInputFormat.java
Patch:
@@ -51,7 +51,9 @@ public void openInternal(InputSplit inputSplit) throws IOException {
 
             String startLocation = incrementConfig.getStartLocation();
             if (incrementConfig.isPolling()) {
-                endLocationAccumulator.add(Long.parseLong(startLocation));
+                if (StringUtils.isNotEmpty(startLocation)) {
+                    endLocationAccumulator.add(Long.parseLong(startLocation));
+                }
                 isTimestamp = "timestamp".equalsIgnoreCase(incrementConfig.getColumnType());
             } else if ((incrementConfig.isIncrement() && incrementConfig.isUseMaxFunc())) {
                 getMaxValue(inputSplit);

File: flinkx-rdb/flinkx-rdb-reader/src/main/java/com.dtstack.flinkx.rdb.inputformat/JdbcInputFormat.java
Patch:
@@ -159,7 +159,9 @@ public void openInternal(InputSplit inputSplit) throws IOException {
             initMetric(inputSplit);
             String startLocation = incrementConfig.getStartLocation();
             if (incrementConfig.isPolling()) {
-                endLocationAccumulator.add(Long.parseLong(startLocation));
+                if (StringUtils.isNotEmpty(startLocation)) {
+                    endLocationAccumulator.add(Long.parseLong(startLocation));
+                }
                 isTimestamp = "timestamp".equalsIgnoreCase(incrementConfig.getColumnType());
             } else if ((incrementConfig.isIncrement() && incrementConfig.isUseMaxFunc())) {
                 getMaxValue(inputSplit);

File: flinkx-core/src/main/java/com/dtstack/flinkx/inputformat/RichInputFormat.java
Patch:
@@ -70,7 +70,7 @@ public abstract class RichInputFormat extends org.apache.flink.api.common.io.Ric
 
     protected FormatState formatState;
 
-    protected TestConfig testConfig;
+    protected TestConfig testConfig = TestConfig.defaultConfig();
 
     protected transient BaseMetric inputMetric;
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/inputformat/RichInputFormat.java
Patch:
@@ -63,7 +63,7 @@ public abstract class RichInputFormat extends org.apache.flink.api.common.io.Ric
 
     protected FormatState formatState;
 
-    protected TestConfig testConfig;
+    protected TestConfig testConfig = TestConfig.defaultConfig();
 
     protected transient BaseMetric inputMetric;
 

File: flinkx-metadata/flinkx-metadata-reader/src/main/java/com/dtstack/flinkx/metadata/inputformat/BaseMetadataInputFormat.java
Patch:
@@ -131,7 +131,7 @@ protected Row nextRecordInternal(Row row) throws IOException {
         metaData.put("operaType", "createTable");
 
         String tableName = tableIterator.next();
-        metaData.put("schema", currentDb);
+        metaData.put("schema", currentDb.get());
         metaData.put("table", tableName);
 
         try {

File: flinkx-metadata/flinkx-metadata-reader/src/main/java/com/dtstack/flinkx/metadata/inputformat/BaseMetadataInputFormat.java
Patch:
@@ -51,11 +51,11 @@ public abstract class BaseMetadataInputFormat extends RichInputFormat {
 
     protected List<Map<String, Object>> dbTableList;
 
-    protected transient ThreadLocal<Connection> connection = new ThreadLocal<>();
+    protected static transient ThreadLocal<Connection> connection = new ThreadLocal<>();
 
-    protected transient ThreadLocal<Statement> statement = new ThreadLocal<>();
+    protected static transient ThreadLocal<Statement> statement = new ThreadLocal<>();
 
-    protected transient ThreadLocal<String> currentDb = new ThreadLocal<>();
+    protected static transient ThreadLocal<String> currentDb = new ThreadLocal<>();
 
     protected transient Iterator<String> tableIterator;
 

File: flinkx-metadata/flinkx-metadata-reader/src/main/java/com/dtstack/flinkx/metadata/inputformat/BaseMetadataInputFormat.java
Patch:
@@ -212,7 +212,7 @@ protected Map<String, Object> unitMetaData(String currentDb, String currentTable
 
         result.put(MetaDataCons.KEY_PARTITION_COLUMNS, partitionDetailInformation);
 
-        result.put(MetaDataCons.KEY_PARTITIONS, getPartitionList());
+        result.put(MetaDataCons.KEY_PARTITIONS, getPartitionList(currentTable));
 
         return result;
     }
@@ -301,5 +301,5 @@ protected Map<String, Object> unitMetaData(String currentDb, String currentTable
      *
      * @return 分区字段名列表
      */
-    public abstract List<String> getPartitionList();
+    public abstract List<String> getPartitionList(String currentTable);
 }

File: flinkx-binlog/flinkx-binlog-reader/src/main/java/com/dtstack/flinkx/binlog/reader/BinlogReader.java
Patch:
@@ -1,4 +1,4 @@
-/*
+/**
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information
@@ -51,6 +51,7 @@ public DataStream<Row> readData() {
         format.setBinlogConfig(binlogConfig);
         format.setRestoreConfig(restoreConfig);
         format.setLogConfig(logConfig);
+        format.setTestConfig(testConfig);
         return createInput(format);
     }
 

File: flinkx-carbondata/flinkx-carbondata-reader/src/main/java/com/dtstack/flinkx/carbondata/reader/CarbondataReader.java
Patch:
@@ -24,7 +24,6 @@
 import org.apache.flink.streaming.api.datastream.DataStream;
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
 import org.apache.flink.types.Row;
-
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
@@ -102,6 +101,7 @@ public DataStream<Row> readData() {
         builder.setBytes(bytes);
         builder.setMonitorUrls(monitorUrls);
         builder.setLogConfig(logConfig);
+        builder.setTestConfig(testConfig);
         return createInput(builder.finish());
     }
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/restore/FormatState.java
Patch:
@@ -45,7 +45,7 @@ public class FormatState implements Serializable {
 
     private String jobId;
 
-    private int fileIndex;
+    private int fileIndex = -1;
 
     public FormatState() {
     }

File: flinkx-es/flinkx-es-reader/src/main/java/com/dtstack/flinkx/es/reader/EsReader.java
Patch:
@@ -111,6 +111,7 @@ public DataStream<Row> readData() {
         builder.setQuery(query);
         builder.setBytes(bytes);
         builder.setMonitorUrls(monitorUrls);
+        builder.setTestConfig(testConfig);
         builder.setLogConfig(logConfig);
 
         return createInput(builder.finish());

File: flinkx-ftp/flinkx-ftp-reader/src/main/java/com/dtstack/flinkx/ftp/reader/FtpReader.java
Patch:
@@ -71,6 +71,7 @@ public DataStream<Row> readData() {
         FtpInputFormatBuilder builder = new FtpInputFormatBuilder();
         builder.setFtpConfig(ftpConfig);
         builder.setMetaColumn(metaColumns);
+        builder.setTestConfig(testConfig);
         builder.setLogConfig(logConfig);
 
         return createInput(builder.finish());

File: flinkx-hbase/flinkx-hbase-reader/src/main/java/com/dtstack/flinkx/hbase/reader/HbaseReader.java
Patch:
@@ -107,6 +107,7 @@ public DataStream<Row> readData() {
         builder.setScanCacheSize(scanCacheSize);
         builder.setScanBatchSize(scanBatchSize);
         builder.setMonitorUrls(monitorUrls);
+        builder.setTestConfig(testConfig);
         builder.setLogConfig(logConfig);
 
         return createInput(builder.finish());

File: flinkx-hdfs/flinkx-hdfs-reader/src/main/java/com/dtstack/flinkx/hdfs/reader/HdfsReader.java
Patch:
@@ -79,6 +79,7 @@ public DataStream<Row> readData() {
         builder.setBytes(bytes);
         builder.setMonitorUrls(monitorUrls);
         builder.setRestoreConfig(restoreConfig);
+        builder.setTestConfig(testConfig);
         builder.setLogConfig(logConfig);
 
         return createInput(builder.finish());

File: flinkx-kudu/flinkx-kudu-reader/src/main/java/com/dtstack/flinkx/kudu/reader/KuduReader.java
Patch:
@@ -75,6 +75,7 @@ public DataStream<Row> readData() {
         builder.setMonitorUrls(monitorUrls);
         builder.setBytes(bytes);
         builder.setKuduConfig(kuduConfig);
+        builder.setTestConfig(testConfig);
         builder.setLogConfig(logConfig);
 
         return createInput(builder.finish());

File: flinkx-mongodb/flinkx-mongodb-reader/src/main/java/com/dtstack/flinkx/mongodb/reader/MongodbReader.java
Patch:
@@ -62,6 +62,7 @@ public DataStream<Row> readData() {
 
         builder.setMonitorUrls(monitorUrls);
         builder.setBytes(bytes);
+        builder.setTestConfig(testConfig);
 
         return createInput(builder.finish());
     }

File: flinkx-odps/flinkx-odps-reader/src/main/java/com/dtstack/flinkx/odps/reader/OdpsReader.java
Patch:
@@ -64,6 +64,7 @@ public DataStream<Row> readData() {
         builder.setPartition(partition);
         builder.setMonitorUrls(monitorUrls);
         builder.setBytes(bytes);
+        builder.setTestConfig(testConfig);
         builder.setLogConfig(logConfig);
 
         return createInput(builder.finish());

File: flinkx-postgresql/flinkx-postgresql-reader/src/main/java/com/dtstack/flinkx/postgresql/reader/PostgresqlReader.java
Patch:
@@ -72,6 +72,7 @@ public DataStream<Row> readData() {
         builder.setCustomSql(customSql);
         builder.setRestoreConfig(restoreConfig);
         builder.setHadoopConfig(hadoopConfig);
+        builder.setTestConfig(testConfig);
 
         QuerySqlBuilder sqlBuilder = new PostgresqlQuerySqlBuilder(this);
         builder.setQuery(sqlBuilder.buildSql());

File: flinkx-rdb/flinkx-rdb-reader/src/main/java/com.dtstack.flinkx.rdb.datareader/DistributedJdbcDataReader.java
Patch:
@@ -97,6 +97,7 @@ public DataStream<Row> readData() {
         builder.setWhere(where);
         builder.setFetchSize(fetchSize == 0 ? databaseInterface.getFetchSize() : fetchSize);
         builder.setQueryTimeOut(queryTimeOut == 0 ? databaseInterface.getQueryTimeout() : queryTimeOut);
+        builder.setTestConfig(testConfig);
         builder.setLogConfig(logConfig);
 
         RichInputFormat format =  builder.finish();

File: flinkx-rdb/flinkx-rdb-reader/src/main/java/com.dtstack.flinkx.rdb.datareader/JdbcDataReader.java
Patch:
@@ -118,6 +118,7 @@ public DataStream<Row> readData() {
         builder.setCustomSql(customSql);
         builder.setRestoreConfig(restoreConfig);
         builder.setHadoopConfig(hadoopConfig);
+        builder.setTestConfig(testConfig);
         builder.setLogConfig(logConfig);
 
         QuerySqlBuilder sqlBuilder = new QuerySqlBuilder(this);

File: flinkx-stream/flinkx-stream-reader/src/main/java/com/dtstack/flinkx/stream/reader/StreamReader.java
Patch:
@@ -66,7 +66,9 @@ public DataStream<Row> readData() {
         builder.setMonitorUrls(monitorUrls);
         builder.setBytes(bytes);
         builder.setRestoreConfig(restoreConfig);
+        builder.setTestConfig(testConfig);
         builder.setLogConfig(logConfig);
+
         return createInput(builder.finish());
     }
 }

File: flinkx-binlog/flinkx-binlog-reader/src/main/java/com/dtstack/flinkx/binlog/reader/BinlogReader.java
Patch:
@@ -93,6 +93,7 @@ public DataStream<Row> readData() {
         format.setPavingData(pavingData);
         format.setTable(table);
         format.setRestoreConfig(restoreConfig);
+        format.setTestConfig(testConfig);
         return createInput(format);
     }
 

File: flinkx-carbondata/flinkx-carbondata-reader/src/main/java/com/dtstack/flinkx/carbondata/reader/CarbondataReader.java
Patch:
@@ -100,6 +100,7 @@ public DataStream<Row> readData() {
         builder.setHadoopConfig(hadoopConfig);
         builder.setBytes(bytes);
         builder.setMonitorUrls(monitorUrls);
+        builder.setTestConfig(testConfig);
         return createInput(builder.finish());
     }
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/restore/FormatState.java
Patch:
@@ -45,7 +45,7 @@ public class FormatState implements Serializable {
 
     private String jobId;
 
-    private int fileIndex;
+    private int fileIndex = -1;
 
     public FormatState() {
     }

File: flinkx-es/flinkx-es-reader/src/main/java/com/dtstack/flinkx/es/reader/EsReader.java
Patch:
@@ -104,6 +104,7 @@ public DataStream<Row> readData() {
         builder.setQuery(query);
         builder.setBytes(bytes);
         builder.setMonitorUrls(monitorUrls);
+        builder.setTestConfig(testConfig);
 
         return createInput(builder.finish());
     }

File: flinkx-ftp/flinkx-ftp-reader/src/main/java/com/dtstack/flinkx/ftp/reader/FtpReader.java
Patch:
@@ -69,6 +69,7 @@ public DataStream<Row> readData() {
         FtpInputFormatBuilder builder = new FtpInputFormatBuilder();
         builder.setFtpConfig(ftpConfig);
         builder.setMetaColumn(metaColumns);
+        builder.setTestConfig(testConfig);
 
         return createInput(builder.finish());
     }

File: flinkx-hbase/flinkx-hbase-reader/src/main/java/com/dtstack/flinkx/hbase/reader/HbaseReader.java
Patch:
@@ -106,6 +106,7 @@ public DataStream<Row> readData() {
         builder.setScanCacheSize(scanCacheSize);
         builder.setScanBatchSize(scanBatchSize);
         builder.setMonitorUrls(monitorUrls);
+        builder.setTestConfig(testConfig);
 
         return createInput(builder.finish());
     }

File: flinkx-hdfs/flinkx-hdfs-reader/src/main/java/com/dtstack/flinkx/hdfs/reader/HdfsReader.java
Patch:
@@ -79,6 +79,7 @@ public DataStream<Row> readData() {
         builder.setBytes(bytes);
         builder.setMonitorUrls(monitorUrls);
         builder.setRestoreConfig(restoreConfig);
+        builder.setTestConfig(testConfig);
 
         return createInput(builder.finish());
     }

File: flinkx-kafka/flinkx-kafka-reader/src/main/java/com/dtstack/flinkx/kafka/reader/KafkaReader.java
Patch:
@@ -75,6 +75,7 @@ public DataStream<Row> readData() {
         format.setBlankIgnore(blankIgnore);
         format.setConsumerSettings(consumerSettings);
         format.setRestoreConfig(restoreConfig);
+        format.setTestConfig(testConfig);
 
         return createInput(format);
     }

File: flinkx-kafka09/flinkx-kafka09-reader/src/main/java/com/dtstack/flinkx/kafka09/reader/Kafka09Reader.java
Patch:
@@ -60,6 +60,7 @@ public DataStream<Row> readData() {
         format.setConsumerSettings(consumerSettings);
         format.setEncoding(encoding);
         format.setRestoreConfig(restoreConfig);
+        format.setTestConfig(testConfig);
 
         return createInput(format);
     }

File: flinkx-kafka10/flinkx-kafka10-reader/src/main/java/com/dtstack/flinkx/kafka10/reader/Kafka10Reader.java
Patch:
@@ -73,6 +73,7 @@ public DataStream<Row> readData() {
         format.setBlankIgnore(blankIgnore);
         format.setConsumerSettings(consumerSettings);
         format.setRestoreConfig(restoreConfig);
+        format.setTestConfig(testConfig);
 
         return createInput(format);
     }

File: flinkx-kafka11/flinkx-kafka11-reader/src/main/java/com/dtstack/flinkx/kafka11/reader/Kafka11Reader.java
Patch:
@@ -73,6 +73,7 @@ public DataStream<Row> readData() {
         format.setBlankIgnore(blankIgnore);
         format.setConsumerSettings(consumerSettings);
         format.setRestoreConfig(restoreConfig);
+        format.setTestConfig(testConfig);
 
         return createInput(format);
     }

File: flinkx-kudu/flinkx-kudu-reader/src/main/java/com/dtstack/flinkx/kudu/reader/KuduReader.java
Patch:
@@ -75,6 +75,7 @@ public DataStream<Row> readData() {
         builder.setMonitorUrls(monitorUrls);
         builder.setBytes(bytes);
         builder.setKuduConfig(kuduConfig);
+        builder.setTestConfig(testConfig);
 
         return createInput(builder.finish());
     }

File: flinkx-mongodb/flinkx-mongodb-reader/src/main/java/com/dtstack/flinkx/mongodb/reader/MongodbReader.java
Patch:
@@ -97,6 +97,7 @@ public DataStream<Row> readData() {
 
         builder.setMonitorUrls(monitorUrls);
         builder.setBytes(bytes);
+        builder.setTestConfig(testConfig);
 
         return createInput(builder.finish());
     }

File: flinkx-odps/flinkx-odps-reader/src/main/java/com/dtstack/flinkx/odps/reader/OdpsReader.java
Patch:
@@ -62,6 +62,7 @@ public DataStream<Row> readData() {
         builder.setPartition(partition);
         builder.setMonitorUrls(monitorUrls);
         builder.setBytes(bytes);
+        builder.setTestConfig(testConfig);
 
         return createInput(builder.finish());
     }

File: flinkx-postgresql/flinkx-postgresql-reader/src/main/java/com/dtstack/flinkx/postgresql/reader/PostgresqlReader.java
Patch:
@@ -68,6 +68,7 @@ public DataStream<Row> readData() {
         builder.setCustomSql(customSql);
         builder.setRestoreConfig(restoreConfig);
         builder.setHadoopConfig(hadoopConfig);
+        builder.setTestConfig(testConfig);
 
         QuerySqlBuilder sqlBuilder = new PostgresqlQuerySqlBuilder(this);
         builder.setQuery(sqlBuilder.buildSql());

File: flinkx-rdb/flinkx-rdb-reader/src/main/java/com.dtstack.flinkx.rdb.datareader/DistributedJdbcDataReader.java
Patch:
@@ -101,6 +101,7 @@ public DataStream<Row> readData() {
         builder.setWhere(where);
         builder.setFetchSize(fetchSize == 0 ? databaseInterface.getFetchSize() : fetchSize);
         builder.setQueryTimeOut(queryTimeOut == 0 ? databaseInterface.getQueryTimeout() : queryTimeOut);
+        builder.setTestConfig(testConfig);
 
         RichInputFormat format =  builder.finish();
         return createInput(format, (databaseInterface.getDatabaseType() + DISTRIBUTED_TAG + "reader").toLowerCase());

File: flinkx-rdb/flinkx-rdb-reader/src/main/java/com.dtstack.flinkx.rdb.datareader/JdbcDataReader.java
Patch:
@@ -119,6 +119,7 @@ public DataStream<Row> readData() {
         builder.setCustomSql(customSql);
         builder.setRestoreConfig(restoreConfig);
         builder.setHadoopConfig(hadoopConfig);
+        builder.setTestConfig(testConfig);
 
         QuerySqlBuilder sqlBuilder = new QuerySqlBuilder(this);
         builder.setQuery(sqlBuilder.buildSql());

File: flinkx-stream/flinkx-stream-reader/src/main/java/com/dtstack/flinkx/stream/reader/StreamReader.java
Patch:
@@ -66,6 +66,7 @@ public DataStream<Row> readData() {
         builder.setMonitorUrls(monitorUrls);
         builder.setBytes(bytes);
         builder.setRestoreConfig(restoreConfig);
+        builder.setTestConfig(testConfig);
         return createInput(builder.finish());
     }
 }

File: flinkx-test/src/main/java/com/dtstack/flinkx/test/LocalTest.java
Patch:
@@ -115,7 +115,7 @@ public class LocalTest {
 
     public static void main(String[] args) throws Exception{
         Properties confProperties = new Properties();
-        confProperties.put("flink.checkpoint.interval", "120000");
+        confProperties.put("flink.checkpoint.interval", "60000");
         confProperties.put("flink.checkpoint.stateBackend", "file:///tmp/flinkx_checkpoint");
 //
 //        conf.setString("metrics.reporter.promgateway.class","org.apache.flink.metrics.prometheus.PrometheusPushGatewayReporter");

File: flinkx-oraclelogminer/flinkx-oraclelogminer-reader/src/main/java/com/dtstack/flinkx/oraclelogminer/format/OracleLogMinerInputFormat.java
Patch:
@@ -130,13 +130,12 @@ private void initOffset(){
 
             offsetScn = getLogFileStartPositionByTime(logMinerConfig.getStartTime());
         } else  if(ReadPosition.SCN.name().equalsIgnoreCase(logMinerConfig.getReadPosition())){
-            scnCopy = Long.parseLong(logMinerConfig.getStartSCN());
-
             // 根据指定的scn获取对应日志文件的起始位置
             if(StringUtils.isEmpty(logMinerConfig.getStartSCN())){
                 throw new RuntimeException("读取模式为[scn]时必须指定[startSCN]");
             }
 
+            scnCopy = Long.parseLong(logMinerConfig.getStartSCN());
             offsetScn = getLogFileStartPositionByScn(Long.parseLong(logMinerConfig.getStartSCN()));
         } else {
             throw new RuntimeException("不支持的读取模式:" + logMinerConfig.getReadPosition());

File: flinkx-test/src/main/java/com/dtstack/flinkx/test/LocalTest.java
Patch:
@@ -127,7 +127,7 @@ public static void main(String[] args) throws Exception{
 //        conf.setString("metrics.reporter.promgateway.randomJobNameSuffix","true");
 //        conf.setString("metrics.reporter.promgateway.deleteOnShutdown","true");
 
-        String jobPath = "D:\\project\\dt-center-flinkx\\flinkx-test\\src\\main\\resources\\dev_test_job\\stream_template.json";
+        String jobPath = "D:\\project\\flinkx\\flinkx-test\\src\\main\\resources\\dev_test_job\\logminer_stream.json";
         JobExecutionResult result = LocalTest.runJob(new File(jobPath), confProperties, null);
         ResultPrintUtil.printResult(result);
     }

File: flinkx-restapi/flinkx-restapi-writer/src/main/java/com/dtstack/flinkx/restapi/outputformat/RestapiOutputFormat.java
Patch:
@@ -126,7 +126,7 @@ private List<Object> getDataFromRow(Row row, List<String> column) throws IOExcep
             result.add(objectMapper.writeValueAsString(columnData));
         } else {
             // 以下只针对元数据同步采集情况
-            result.addAll((Collection<?>) objectMapper.readValue(row.getField(index).toString(), Map.class).get("data"));
+            result.add(objectMapper.readValue(row.getField(index).toString(), Map.class).get("data"));
         }
         return result;
     }

File: flinkx-test/src/main/java/com/dtstack/flinkx/test/LocalTest.java
Patch:
@@ -129,7 +129,7 @@ public static void main(String[] args) throws Exception{
 //        conf.setString("metrics.reporter.promgateway.randomJobNameSuffix","true");
 //        conf.setString("metrics.reporter.promgateway.deleteOnShutdown","true");
 
-        String jobPath = "F:\\公司\\项目\\flinkx\\flinkx-metadata-hive2\\hive2metareader-streamwriter.json";
+        String jobPath = "F:\\公司\\项目\\flinkx\\flinkx-metadata-hive2\\hive2metareader.json";
         JobExecutionResult result = LocalTest.runJob(new File(jobPath), confProperties, null);
         ResultPrintUtil.printResult(result);
     }

File: flinkx-metadata/flinkx-metadata-reader/src/main/java/com/dtstack/flinkx/metadata/inputformat/MetadataInputFormat.java
Patch:
@@ -20,6 +20,7 @@
 import com.dtstack.flinkx.inputformat.RichInputFormat;
 import com.dtstack.flinkx.metadata.MetaDataCons;
 import com.dtstack.flinkx.metadata.util.ConnUtil;
+import com.google.common.collect.Lists;
 import com.google.common.collect.Maps;
 import org.apache.flink.core.io.InputSplit;
 import org.apache.flink.types.Row;
@@ -143,7 +144,7 @@ private InputSplit[] initSplits() throws SQLException {
      * @return result 查询结果拆分之后的数据
      */
     public List<String> getDataList(String queryDataSql) throws SQLException {
-        List<String> result = new ArrayList<>();
+        List<String> result = Lists.newArrayList();
         ResultSet resultSet = executeQuerySql(queryDataSql);
         while (resultSet.next()) {
             result.add(resultSet.getString(1));

File: flinkx-metadata/flinkx-metadata-reader/src/main/java/com/dtstack/flinkx/metadata/inputformat/MetadataInputFormat.java
Patch:
@@ -52,7 +52,6 @@ public abstract class MetadataInputFormat extends RichInputFormat {
     protected transient static Statement statement;
 
     protected Map<String, String> errorMessage = Maps.newHashMap();
-//    protected Map<String, Object> currentMessage = Maps.newHashMap();
     protected LinkedList<Map<String, Object>> resultMapList;
 
     @Override
@@ -87,7 +86,6 @@ protected void openInternal(InputSplit inputSplit) throws IOException {
                 hasNext = true;
                 resultMapList.add(resultMap);
             }
-//            currentMessage.put("data", resultMapList);
         } catch (SQLException e) {
             setErrorMessage(e, "openInternal error");
         }

File: flinkx-test/src/main/java/com/dtstack/flinkx/test/LocalTest.java
Patch:
@@ -129,7 +129,7 @@ public static void main(String[] args) throws Exception{
 //        conf.setString("metrics.reporter.promgateway.randomJobNameSuffix","true");
 //        conf.setString("metrics.reporter.promgateway.deleteOnShutdown","true");
 
-        String jobPath = "F:\\公司\\项目\\flinkx\\flinkx-metadata-hive2\\hive2metareader.json";
+        String jobPath = "F:\\公司\\项目\\flinkx\\flinkx-metadata-hive2\\hive2metareader-streamwriter.json";
         JobExecutionResult result = LocalTest.runJob(new File(jobPath), confProperties, null);
         ResultPrintUtil.printResult(result);
     }

File: flinkx-metadata/flinkx-metadata-reader/src/main/java/com/dtstack/flinkx/metadata/reader/MetadataReader.java
Patch:
@@ -42,6 +42,7 @@ public class MetadataReader extends DataReader {
     protected String password;
     protected String driverName;
 
+    @SuppressWarnings("unchecked")
     protected MetadataReader(DataTransferConfig config, StreamExecutionEnvironment env) {
         super(config, env);
 

File: flinkx-restapi/flinkx-restapi-writer/src/main/java/com/dtstack/flinkx/restapi/writer/RestapiWriter.java
Patch:
@@ -19,18 +19,15 @@
 
 import com.dtstack.flinkx.config.DataTransferConfig;
 import com.dtstack.flinkx.config.WriterConfig;
-import com.dtstack.flinkx.reader.MetaColumn;
 import com.dtstack.flinkx.restapi.common.RestapiKeys;
 import com.dtstack.flinkx.restapi.outputformat.RestapiOutputFormatBuilder;
 import com.dtstack.flinkx.writer.DataWriter;
 import org.apache.flink.streaming.api.datastream.DataStream;
 import org.apache.flink.streaming.api.datastream.DataStreamSink;
 import org.apache.flink.types.Row;
 
-import java.lang.reflect.Field;
 import java.util.ArrayList;
 import java.util.HashMap;
-import java.util.List;
 import java.util.Map;
 
 /**
@@ -46,6 +43,7 @@ public class RestapiWriter extends DataWriter {
     protected Map<String, Object> params = new HashMap<>();
     protected int batchInterval;
 
+    @SuppressWarnings("unchecked")
     public RestapiWriter(DataTransferConfig config) {
         super(config);
         Object tempObj;

File: flinkx-test/src/main/java/com/dtstack/flinkx/test/LocalTest.java
Patch:
@@ -129,7 +129,7 @@ public static void main(String[] args) throws Exception{
 //        conf.setString("metrics.reporter.promgateway.randomJobNameSuffix","true");
 //        conf.setString("metrics.reporter.promgateway.deleteOnShutdown","true");
 
-        String jobPath = "F:\\公司\\项目\\flinkx\\flinkx-metadata-hive2\\hive2metareader-streamwriter.json";
+        String jobPath = "F:\\公司\\项目\\flinkx\\flinkx-metadata-hive2\\hive2metareader.json";
         JobExecutionResult result = LocalTest.runJob(new File(jobPath), confProperties, null);
         ResultPrintUtil.printResult(result);
     }

File: flinkx-metadata/flinkx-metadata-reader/src/main/java/com/dtstack/flinkx/metadata/inputformat/MetadataInputFormat.java
Patch:
@@ -116,7 +116,7 @@ protected InputSplit[] initSplit() throws SQLException {
             tableList = getTableList(dbInfo);
         } else {
             for (Map item : dbList) {
-                if (item.get(MetaDataCons.KEY_TABLE_LIST).equals(null)) {
+                if (((ArrayList)item.get(MetaDataCons.KEY_TABLE_LIST)).size() == 0) {
                     // 查询当前库下的所有表的元数据信息
                     tableList.addAll(getTableList((String) item.get(MetaDataCons.KEY_DB_NAME)));
                 } else {
@@ -265,14 +265,14 @@ public Map<String, Object> unitMetaData(String currentQueryTable, String current
     public List<String> getTableList(List<String> dbList) throws SQLException {
         List<String> tableList = new ArrayList<>();
         for (String item : dbList) {
-            statement.execute(changeDBSql(item));
             tableList.addAll(getTableList(item));
         }
         return tableList;
     }
 
     public List<String> getTableList(String dbName) throws SQLException {
         List<String> tableList = new ArrayList<>();
+        statement.execute(changeDBSql(dbName));
         ResultSet resultSet = executeSql(queryTableSql());
         while (resultSet.next()) {
             tableList.add(dbName + "." + resultSet.getString(1));

File: flinkx-metadata/flinkx-metadata-reader/src/main/java/com/dtstack/flinkx/metadata/inputformat/MetadataInputFormat.java
Patch:
@@ -45,7 +45,7 @@ public abstract class MetadataInputFormat extends RichInputFormat {
     protected String username;
     protected String password;
 
-    protected List<Map<String, Object>> dbList;
+    protected List<Map> dbList;
 
     protected boolean hasNext;
 

File: flinkx-metadata-hive2/flinkx-metadata-hive2-reader/src/main/java/com/dtstack/flinkx/metadatahive2/inputformat/Metadatahive2InputFormat.java
Patch:
@@ -38,8 +38,9 @@ public class Metadatahive2InputFormat extends MetadataInputFormat {
     @Override
     protected void beforeUnit(String currentQueryTable, String currentDbName) {
         getColumn(currentQueryTable, currentDbName);
-        columnMap = transformDataToMap(executeSql(
-                buildDescSql(currentDbName + "." + currentQueryTable, false)));
+        columnMap = new HashMap<>();
+        columnMap.putAll(transformDataToMap(executeSql(
+                buildDescSql(currentDbName + "." + currentQueryTable, false))));
     }
 
     @Override

File: flinkx-metadata/flinkx-metadata-reader/src/main/java/com/dtstack/flinkx/metadata/inputformat/MetadataInputFormatBuilder.java
Patch:
@@ -67,9 +67,9 @@ protected void checkFormat() {
         }
         // 判断是否是全库全表查询
         if(format.dbList.isEmpty()){
-            format.isAllDB = true;
+            format.isAll = true;
         } else {
-            format.isAllDB = false;
+            format.isAll = false;
         }
     }
 

File: flinkx-restapi/flinkx-restapi-writer/src/main/java/com/dtstack/flinkx/restapi/outputformat/RestapiOutputFormat.java
Patch:
@@ -39,7 +39,7 @@ public class RestapiOutputFormat extends RichOutputFormat {
 
     protected String url;
     protected String method;
-    protected ArrayList<String> column = new ArrayList<>();
+    protected ArrayList<String> column;
 
     protected transient Map<String, Object> params;
     protected transient Map<String, Object> body;

File: flinkx-restapi/flinkx-restapi-writer/src/main/java/com/dtstack/flinkx/restapi/writer/RestapiWriter.java
Patch:
@@ -57,7 +57,7 @@ public RestapiWriter(DataTransferConfig config) {
         batchInterval = writerConfig.getParameter().getIntVal(RestapiKeys.KEY_BATCH_INTERVAL, 1);
         tempObj = writerConfig.getParameter().getVal(RestapiKeys.KEY_COLUMN);
         if (tempObj != null) {
-            column = (ArrayList<String>) tempObj;
+            column.addAll((ArrayList<String>) tempObj);
         }
 
         tempObj = writerConfig.getParameter().getVal(RestapiKeys.KEY_HEADER);
@@ -75,7 +75,7 @@ public RestapiWriter(DataTransferConfig config) {
         }
         tempObj = writerConfig.getParameter().getVal(RestapiKeys.KEY_PARAMS);
         if (tempObj != null) {
-            params = (HashMap)tempObj;
+            params = (HashMap) tempObj;
         }
     }
 

File: flinkx-metadata-hive2/flinkx-metadata-hive2-reader/src/main/java/com/dtstack/flinkx/metadatahive2/inputformat/Metadatahive2InputFormat.java
Patch:
@@ -19,7 +19,7 @@
 
 import com.dtstack.flinkx.metadata.MetaDataCons;
 import com.dtstack.flinkx.metadatahive2.common.Hive2MetaDataCons;
-import com.dtstack.flinkx.metadata.inputformat.MetaDataInputFormat;
+import com.dtstack.flinkx.metadata.inputformat.MetadataInputFormat;
 
 import java.sql.ResultSet;
 import java.sql.SQLException;
@@ -29,9 +29,10 @@
  * @author : tiezhu
  * @date : 2020/3/9
  */
-public class MetadataHive2InputFormat extends MetaDataInputFormat {
+public class Metadatahive2InputFormat extends MetadataInputFormat {
     protected List<String> tableColumn;
     protected List<String> partitionColumn;
+
     protected Map<String, Object> columnMap;
 
     @Override

File: flinkx-metadata/flinkx-metadata-reader/src/main/java/com/dtstack/flinkx/metadata/inputformat/MetadataInputFormatBuilder.java
Patch:
@@ -26,10 +26,10 @@
  * @author : tiezhu
  * @date : 2020/3/8
  */
-public class MetaDataInputFormatBuilder extends RichInputFormatBuilder {
-    private MetaDataInputFormat format;
+public class MetadataInputFormatBuilder extends RichInputFormatBuilder {
+    private MetadataInputFormat format;
 
-    public MetaDataInputFormatBuilder(MetaDataInputFormat format) {
+    public MetadataInputFormatBuilder(MetadataInputFormat format) {
         super.format = this.format = format;
     }
 

File: flinkx-metadata/flinkx-metadata-reader/src/main/java/com/dtstack/flinkx/metadata/inputformat/MetadataInputSplit.java
Patch:
@@ -24,7 +24,7 @@
  * @date : 2020/3/8
  * @description :
  */
-public class MetaDataInputSplit extends GenericInputSplit {
+public class MetadataInputSplit extends GenericInputSplit {
     private String dbUrl;
     private String tableName;
     private String dbName;
@@ -36,7 +36,7 @@ public class MetaDataInputSplit extends GenericInputSplit {
      * @param partitionNumber         The number of the split's partition.
      * @param totalNumberOfPartitions The total number of the splits (partitions).
      */
-    public MetaDataInputSplit(int partitionNumber, int totalNumberOfPartitions, String dbUrl, String tablesName, String dbName) {
+    public MetadataInputSplit(int partitionNumber, int totalNumberOfPartitions, String dbUrl, String tablesName, String dbName) {
         super(partitionNumber, totalNumberOfPartitions);
         this.dbUrl = dbUrl;
         this.tableName = tablesName;

File: flinkx-test/src/main/java/com/dtstack/flinkx/test/LocalTest.java
Patch:
@@ -129,7 +129,7 @@ public static void main(String[] args) throws Exception{
 //        conf.setString("metrics.reporter.promgateway.randomJobNameSuffix","true");
 //        conf.setString("metrics.reporter.promgateway.deleteOnShutdown","true");
 
-        String jobPath = "F:\\公司\\项目\\flinkx\\flinkx-metadata-hive2\\test.json";
+        String jobPath = "F:\\公司\\项目\\flinkx\\flinkx-metadata-hive2\\hive2metareader-streamwriter.json";
         JobExecutionResult result = LocalTest.runJob(new File(jobPath), confProperties, null);
         ResultPrintUtil.printResult(result);
     }

File: flinkx-restapi/flinkx-restapi-reader/src/main/java/com/dtstack/flinkx/restapi/inputformat/RestapiInputFormat.java
Patch:
@@ -36,7 +36,7 @@
  * @author : tiezhu
  * @date : 2020/3/12
  */
-public class RestAPIInputFormat extends RichInputFormat {
+public class RestapiInputFormat extends RichInputFormat {
     protected String url;
     protected Map<String, Object> header;
     protected String method;

File: flinkx-restapi/flinkx-restapi-reader/src/main/java/com/dtstack/flinkx/restapi/inputformat/RestapiInputFormatBuilder.java
Patch:
@@ -25,10 +25,10 @@
  * @author : tiezhu
  * @date : 2020/3/12
  */
-public class RestAPIInputFormatBuilder extends RichInputFormatBuilder {
-    protected RestAPIInputFormat format;
+public class RestapiInputFormatBuilder extends RichInputFormatBuilder {
+    protected RestapiInputFormat format;
 
-    public RestAPIInputFormatBuilder(){ super.format = format = new RestAPIInputFormat();}
+    public RestapiInputFormatBuilder(){ super.format = format = new RestapiInputFormat();}
 
     public void setUrl(String url){this.format.url = url;}
     public void setHeader(Map<String, Object> header){ this.format.header = header;}

File: flinkx-restapi/flinkx-restapi-reader/src/main/java/com/dtstack/flinkx/restapi/reader/RestapiReader.java
Patch:
@@ -20,7 +20,7 @@
 import com.dtstack.flinkx.config.DataTransferConfig;
 import com.dtstack.flinkx.config.ReaderConfig;
 import com.dtstack.flinkx.reader.DataReader;
-import com.dtstack.flinkx.restapi.inputformat.RestAPIInputFormatBuilder;
+import com.dtstack.flinkx.restapi.inputformat.RestapiInputFormatBuilder;
 import org.apache.flink.streaming.api.datastream.DataStream;
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
 import org.apache.flink.types.Row;
@@ -55,7 +55,7 @@ public RestapiReader(DataTransferConfig config, StreamExecutionEnvironment env)
 
     @Override
     public DataStream<Row> readData() {
-        RestAPIInputFormatBuilder builder = new RestAPIInputFormatBuilder();
+        RestapiInputFormatBuilder builder = new RestapiInputFormatBuilder();
 
         builder.setHeader(header);
         builder.setMethod(method);

File: flinkx-test/src/main/java/com/dtstack/flinkx/test/LocalTest.java
Patch:
@@ -129,7 +129,7 @@ public static void main(String[] args) throws Exception{
 //        conf.setString("metrics.reporter.promgateway.randomJobNameSuffix","true");
 //        conf.setString("metrics.reporter.promgateway.deleteOnShutdown","true");
 
-        String jobPath = "F:\\公司\\项目\\flinkx\\flinkx-metadata-hive2\\hive2metareader.json";
+        String jobPath = "F:\\公司\\项目\\flinkx\\flinkx-metadata-hive2\\hive2metareader-streamwriter.json";
         JobExecutionResult result = LocalTest.runJob(new File(jobPath), confProperties, null);
         ResultPrintUtil.printResult(result);
     }

File: flinkx-restapi/flinkx-restapi-writer/src/main/java/com/dtstack/flinkx/restapi/outputformat/RestAPIOutputFormat.java
Patch:
@@ -54,7 +54,7 @@ public class RestAPIOutputFormat extends RichOutputFormat {
     protected Map<String, String> header;
     protected String method;
     protected Map<String, Object> body;
-    protected ArrayList<String> column;
+    protected ArrayList<String> column ;
     protected Map<String, Object> params;
 
     @Override
@@ -78,7 +78,7 @@ protected void writeSingleRecordInternal(Row row) throws WriteRecordException {
                 dataRow = row.getField(index).toString();
             }
 
-            body.put("data", JsonUtils.jsonStrToObject(dataRow, Map.class));
+            body.put("data", (JsonUtils.jsonStrToObject(row.getField(index).toString(), Map.class)).get("data"));
             Iterator iterator = params.entrySet().iterator();
             while (iterator.hasNext()) {
                 Map.Entry entry = (Map.Entry) iterator.next();
@@ -123,6 +123,7 @@ protected void writeMultipleRecordsInternal() throws Exception {
             Map.Entry entry = (Map.Entry) iterator.next();
             body.put((String) entry.getKey(), entry.getValue());
         }
+        System.out.println(JsonUtils.objectToJsonStr(body));
     }
 
     private void requestErrorMessage(Exception e, int index, Row row) {

File: flinkx-test/src/main/java/com/dtstack/flinkx/test/LocalTest.java
Patch:
@@ -129,7 +129,7 @@ public static void main(String[] args) throws Exception{
 //        conf.setString("metrics.reporter.promgateway.randomJobNameSuffix","true");
 //        conf.setString("metrics.reporter.promgateway.deleteOnShutdown","true");
 
-        String jobPath = "F:\\公司\\项目\\flinkx\\flinkx-metadata-hive2\\test.json";
+        String jobPath = "F:\\公司\\项目\\flinkx\\flinkx-metadata-hive2\\hive2metareader.json";
         JobExecutionResult result = LocalTest.runJob(new File(jobPath), confProperties, null);
         ResultPrintUtil.printResult(result);
     }

File: flinkx-binlog/flinkx-binlog-reader/src/main/java/com/dtstack/flinkx/binlog/reader/BinlogReader.java
Patch:
@@ -72,7 +72,7 @@ public BinlogReader(DataTransferConfig config, StreamExecutionEnvironment env) {
         cat = readerConfig.getParameter().getStringVal(KEY_CATALOG);
         filter = readerConfig.getParameter().getStringVal(KEY_FILTER);
         period = readerConfig.getParameter().getLongVal(KEY_PERIOD, 1000L);
-        bufferSize = readerConfig.getParameter().getIntVal(KEY_BUFFER_SIZE, 1024);
+        bufferSize = readerConfig.getParameter().getIntVal(KEY_BUFFER_SIZE, 256);
         pavingData = readerConfig.getParameter().getBooleanVal(KEY_PAVING_DATA, false);
         table = (List<String>) readerConfig.getParameter().getVal(KEY_TABLE);
     }

File: flinkx-metadatasync/flinkx-metadatasync-reader/src/main/java/com/dtstack/flinkx/metadatasync/reader/reader/MetaDataReader.java
Patch:
@@ -37,7 +37,7 @@
  */
 public class MetaDataReader extends DataReader {
     protected String dbUrl;
-    protected String table;
+    protected List<String> table;
     protected String username;
     protected String pasword;
 
@@ -46,7 +46,7 @@ public MetaDataReader(DataTransferConfig config, StreamExecutionEnvironment env)
         ReaderConfig readerConfig = config.getJob().getContent().get(0).getReader();
 
         dbUrl = readerConfig.getParameter().getConnection().get(0).getJdbcUrl().get(0);
-        table = readerConfig.getParameter().getConnection().get(0).getTable().get(0);
+        table = readerConfig.getParameter().getConnection().get(0).getTable();
         username = readerConfig.getParameter().getStringVal("username");
         pasword = readerConfig.getParameter().getStringVal("password");
     }
@@ -59,6 +59,7 @@ public DataStream<Row> readData() {
         builder.setUsername(username);
         builder.setPassword(pasword);
         builder.setTable(table);
+        builder.setNumPartitions(numPartitions);
 
         return createInput(builder.finish());
     }

File: flinkx-clickhouse/flinkx-clickhouse-reader/src/main/java/com/dtstack/flinkx/clickhouse/reader/ClickhouseReader.java
Patch:
@@ -18,8 +18,10 @@
 package com.dtstack.flinkx.clickhouse.reader;
 
 import com.dtstack.flinkx.clickhouse.core.ClickhouseDatabaseMeta;
+import com.dtstack.flinkx.clickhouse.format.ClickhouseInputFormat;
 import com.dtstack.flinkx.config.DataTransferConfig;
 import com.dtstack.flinkx.rdb.datareader.JdbcDataReader;
+import com.dtstack.flinkx.rdb.inputformat.JdbcInputFormatBuilder;
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
 /**
  * Date: 2019/11/05
@@ -32,5 +34,6 @@ public class ClickhouseReader extends JdbcDataReader {
     public ClickhouseReader(DataTransferConfig config, StreamExecutionEnvironment env) {
         super(config, env);
         setDatabaseInterface(new ClickhouseDatabaseMeta());
+        super.builder = new JdbcInputFormatBuilder(new ClickhouseInputFormat());
     }
 }

File: flinkx-clickhouse/flinkx-clickhouse-reader/src/main/java/com/dtstack/flinkx/clickhouse/reader/ClickhouseReader.java
Patch:
@@ -18,8 +18,10 @@
 package com.dtstack.flinkx.clickhouse.reader;
 
 import com.dtstack.flinkx.clickhouse.core.ClickhouseDatabaseMeta;
+import com.dtstack.flinkx.clickhouse.format.ClickhouseInputFormat;
 import com.dtstack.flinkx.config.DataTransferConfig;
 import com.dtstack.flinkx.rdb.datareader.JdbcDataReader;
+import com.dtstack.flinkx.rdb.inputformat.JdbcInputFormatBuilder;
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
 /**
  * Date: 2019/11/05
@@ -32,5 +34,6 @@ public class ClickhouseReader extends JdbcDataReader {
     public ClickhouseReader(DataTransferConfig config, StreamExecutionEnvironment env) {
         super(config, env);
         setDatabaseInterface(new ClickhouseDatabaseMeta());
+        super.builder = new JdbcInputFormatBuilder(new ClickhouseInputFormat());
     }
 }

File: flinkx-emqx/flinkx-emqx-reader/src/main/java/com/dtstack/flinkx/emqx/reader/EmqxReader.java
Patch:
@@ -48,6 +48,7 @@ public DataStream<Row> readData() {
         builder.setCodec(codec);
         builder.setCleanSession(isCleanSession);
         builder.setQos(qos);
+        builder.setRestoreConfig(restoreConfig);
         return createInput(builder.finish());
     }
 }

File: flinkx-emqx/flinkx-emqx-reader/src/main/java/com/dtstack/flinkx/emqx/reader/EmqxReader.java
Patch:
@@ -48,6 +48,7 @@ public DataStream<Row> readData() {
         builder.setCodec(codec);
         builder.setCleanSession(isCleanSession);
         builder.setQos(qos);
+        builder.setRestoreConfig(restoreConfig);
         return createInput(builder.finish());
     }
 }

File: flinkx-sqlservercdc/flinkx-sqlservercdc-reader/src/main/java/com/dtstack/flinkx/sqlservercdc/format/SqlserverCdcInputFormat.java
Patch:
@@ -17,7 +17,6 @@
  */
 package com.dtstack.flinkx.sqlservercdc.format;
 
-import avro.shaded.com.google.common.util.concurrent.ThreadFactoryBuilder;
 import com.dtstack.flinkx.inputformat.RichInputFormat;
 import com.dtstack.flinkx.restore.FormatState;
 import com.dtstack.flinkx.sqlservercdc.Lsn;
@@ -26,6 +25,7 @@
 import com.dtstack.flinkx.sqlservercdc.listener.SqlServerCdcListener;
 import com.dtstack.flinkx.util.ClassUtil;
 import com.dtstack.flinkx.util.ExceptionUtil;
+import com.google.common.util.concurrent.ThreadFactoryBuilder;
 import org.apache.commons.collections.CollectionUtils;
 import org.apache.commons.lang.StringUtils;
 import org.apache.flink.core.io.GenericInputSplit;

File: flinkx-sqlservercdc/flinkx-sqlservercdc-reader/src/main/java/com/dtstack/flinkx/sqlservercdc/reader/SqlservercdcReader.java
Patch:
@@ -57,7 +57,7 @@ public SqlservercdcReader(DataTransferConfig config, StreamExecutionEnvironment
         pavingData = readerConfig.getParameter().getBooleanVal(SqlServerCdcConfigKeys.KEY_PAVING_DATA, false);
         tableList = (List<String>) readerConfig.getParameter().getVal(SqlServerCdcConfigKeys.KEY_TABLE_LIST);
         pollInterval = readerConfig.getLongVal(SqlServerCdcConfigKeys.KEY_POLL_INTERVAL, 1000);
-        lsn = readerConfig.getStringVal(SqlServerCdcConfigKeys.KEY_LSN);
+        lsn = readerConfig.getParameter().getStringVal(SqlServerCdcConfigKeys.KEY_LSN);
     }
 
     @Override

File: flinkx-sqlservercdc/flinkx-sqlservercdc-reader/src/main/java/com/dtstack/flinkx/sqlservercdc/format/SqlserverCdcInputFormat.java
Patch:
@@ -17,7 +17,6 @@
  */
 package com.dtstack.flinkx.sqlservercdc.format;
 
-import avro.shaded.com.google.common.util.concurrent.ThreadFactoryBuilder;
 import com.dtstack.flinkx.inputformat.RichInputFormat;
 import com.dtstack.flinkx.restore.FormatState;
 import com.dtstack.flinkx.sqlservercdc.Lsn;
@@ -26,6 +25,7 @@
 import com.dtstack.flinkx.sqlservercdc.listener.SqlServerCdcListener;
 import com.dtstack.flinkx.util.ClassUtil;
 import com.dtstack.flinkx.util.ExceptionUtil;
+import com.google.common.util.concurrent.ThreadFactoryBuilder;
 import org.apache.commons.collections.CollectionUtils;
 import org.apache.commons.lang.StringUtils;
 import org.apache.flink.core.io.GenericInputSplit;

File: flinkx-sqlservercdc/flinkx-sqlservercdc-reader/src/main/java/com/dtstack/flinkx/sqlservercdc/reader/SqlservercdcReader.java
Patch:
@@ -57,7 +57,7 @@ public SqlservercdcReader(DataTransferConfig config, StreamExecutionEnvironment
         pavingData = readerConfig.getParameter().getBooleanVal(SqlServerCdcConfigKeys.KEY_PAVING_DATA, false);
         tableList = (List<String>) readerConfig.getParameter().getVal(SqlServerCdcConfigKeys.KEY_TABLE_LIST);
         pollInterval = readerConfig.getLongVal(SqlServerCdcConfigKeys.KEY_POLL_INTERVAL, 1000);
-        lsn = readerConfig.getStringVal(SqlServerCdcConfigKeys.KEY_LSN);
+        lsn = readerConfig.getParameter().getStringVal(SqlServerCdcConfigKeys.KEY_LSN);
     }
 
     @Override

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/ResultPrintUtil.java
Patch:
@@ -33,10 +33,10 @@ public class ResultPrintUtil {
 
     public static void printResult(JobExecutionResult result){
         List<String> names = Lists.newArrayList();
-        List<Long> values = Lists.newArrayList();
+        List<String> values = Lists.newArrayList();
         result.getAllAccumulatorResults().forEach((name, val) -> {
             names.add(name);
-            values.add((Long) val);
+            values.add(String.valueOf(val));
         });
 
         int maxLength = 0;

File: flinkx-core/src/main/java/com/dtstack/flinkx/constants/PluginNameConstrant.java
Patch:
@@ -47,6 +47,7 @@ public class PluginNameConstrant {
     public static final String CLICKHOUSE_READER = "clickhousereader";
     public static final String POLARDB_READER = "polardbreader";
     public static final String CASSANDRA_READER = "cassandrareader";
+    public static final String PHOENIX_READER = "phoenixreader";
 
 
     public static final String STREAM_WRITER = "streamwriter";
@@ -73,4 +74,5 @@ public class PluginNameConstrant {
     public static final String CLICKHOUSE_WRITER = "clickhousewriter";
     public static final String POLARDB_WRITER = "polardbwriter";
     public static final String CASSANDRA_WRITER = "cassandrawriter";
+    public static final String PHOENIX_WRITER = "phoenixwriter";
 }

File: flinkx-phoenix/flinkx-phoenix-core/src/main/java/com/dtstack/flinkx/phoenix/PhoenixMeta.java
Patch:
@@ -56,12 +56,12 @@ public String getSQLQueryColumnFields(List<String> column, String table) {
     @Override
     public String getStartQuote() {
         // 对于字段名和表名的quote得用双引号，对于字段值为字符串的得用单引号表示常量
-        return "\"";
+        return "";
     }
 
     @Override
     public String getEndQuote() {
-        return "\"";
+        return "";
     }
 
     @Override

File: flinkx-phoenix/flinkx-phoenix-reader/src/main/java/com/dtstack/flinkx/phoenix/format/PhoenixInputFormat.java
Patch:
@@ -17,6 +17,7 @@
  */
 package com.dtstack.flinkx.phoenix.format;
 
+import com.dtstack.flinkx.phoenix.util.PhoenixUtil;
 import com.dtstack.flinkx.rdb.inputformat.JdbcInputFormat;
 import com.dtstack.flinkx.rdb.util.DBUtil;
 import com.dtstack.flinkx.reader.MetaColumn;
@@ -61,7 +62,7 @@ public void openInternal(InputSplit inputSplit) throws IOException {
                 return;
             }
 
-            dbConn = DBUtil.getConnection(dbURL, username, password);
+            dbConn = PhoenixUtil.getConnectionInternal(dbURL, username, password);
 
             // 部分驱动需要关闭事务自动提交，fetchSize参数才会起作用
             dbConn.setAutoCommit(false);

File: flinkx-binlog/flinkx-binlog-reader/src/main/java/com/dtstack/flinkx/binlog/reader/BinlogConfig.java
Patch:
@@ -31,7 +31,7 @@ public class BinlogConfig implements Serializable {
 
     public long period = 1000L;
 
-    public int bufferSize = 1024;
+    public int bufferSize = 256;
 
     public boolean pavingData = true;
 

File: flinkx-oracle/flinkx-oracle-reader/src/main/java/com/dtstack/flinkx/oracle/format/OracleInputFormat.java
Patch:
@@ -80,7 +80,6 @@ protected String getTimeStr(Long location, String incrementColType){
             timeStr = timeStr.substring(0, 19);
             timeStr = String.format("TO_DATE('%s','YYYY-MM-DD HH24:MI:SS')", timeStr);
         }
-        timeStr = String.format("'%s'",timeStr);
 
         return timeStr;
     }

File: flinkx-core/src/main/java/com/dtstack/flinkx/inputformat/RichInputFormat.java
Patch:
@@ -247,7 +247,7 @@ public void closeInputFormat() throws IOException {
             accumulatorCollector.close();
         }
 
-        if (useCustomPrometheusReporter()) {
+        if (useCustomPrometheusReporter() && null != customPrometheusReporter) {
             customPrometheusReporter.report();
             customPrometheusReporter.close();
         }

File: flinkx-core/src/main/java/com/dtstack/flinkx/metrics/AccumulatorCollector.java
Patch:
@@ -226,9 +226,6 @@ private void collectAccumulatorWithApi(){
                         }
                     }
                 }
-                if(DtLogger.isEnableTrace()){
-                    LOG.trace("monitorUrl = {}, response = {}", monitorUrl, response);
-                }
             } catch (Exception e){
                 checkErrorTimes();
                 LOG.error("Update data error,url:[{}],error info:", monitorUrl, e);

File: flinkx-mysql/flinkx-mysql-reader/src/main/java/com/dtstack/flinkx/mysql/format/MysqlInputFormat.java
Patch:
@@ -63,7 +63,6 @@ public void openInternal(InputSplit inputSplit) throws IOException {
                 return;
             }
 
-            fetchSize = Integer.MIN_VALUE;
             querySql = buildQuerySql(inputSplit);
             executeQuery(startLocation);
             columnCount = resultSet.getMetaData().getColumnCount();

File: flinkx-core/src/main/java/com/dtstack/flinkx/metrics/AccumulatorCollector.java
Patch:
@@ -222,9 +222,6 @@ private void collectAccumulatorWithApi(){
                         }
                     }
                 }
-                if(DtLogger.isEnableTrace()){
-                    LOG.trace("monitorUrl = {}, response = {}", monitorUrl, response);
-                }
             } catch (Exception e){
                 LOG.error("Update data error,url:[{}],error info:", monitorUrl, e);
             }

File: flinkx-mysql/flinkx-mysql-reader/src/main/java/com/dtstack/flinkx/mysql/format/MysqlInputFormat.java
Patch:
@@ -63,7 +63,6 @@ public void openInternal(InputSplit inputSplit) throws IOException {
                 return;
             }
 
-            fetchSize = Integer.MIN_VALUE;
             querySql = buildQuerySql(inputSplit);
             executeQuery(startLocation);
             columnCount = resultSet.getMetaData().getColumnCount();

File: flinkx-rdb/flinkx-rdb-reader/src/main/java/com.dtstack.flinkx.rdb.inputformat/JdbcInputFormat.java
Patch:
@@ -764,6 +764,9 @@ private void uploadMetricData() throws IOException {
      */
     protected void queryForPolling(String startLocation) throws SQLException {
         LOG.trace("polling startLocation = {}", startLocation);
+        if(StringUtils.isBlank(startLocation)){
+            return;
+        }
         if(isTimestamp){
             ps.setTimestamp(1, Timestamp.valueOf(startLocation));
         }else{

File: flinkx-rdb/flinkx-rdb-reader/src/main/java/com.dtstack.flinkx.rdb.inputformat/JdbcInputFormat.java
Patch:
@@ -805,6 +805,9 @@ private void uploadMetricData() throws IOException {
      */
     protected void queryForPolling(String startLocation) throws SQLException {
         LOG.trace("polling startLocation = {}", startLocation);
+        if(StringUtils.isBlank(startLocation)){
+            return;
+        }
         if(isTimestamp){
             ps.setTimestamp(1, Timestamp.valueOf(startLocation));
         }else{

File: flinkx-oracle/flinkx-oracle-reader/src/main/java/com/dtstack/flinkx/oracle/format/OracleInputFormat.java
Patch:
@@ -80,7 +80,6 @@ protected String getTimeStr(Long location, String incrementColType){
             timeStr = timeStr.substring(0, 19);
             timeStr = String.format("TO_DATE('%s','YYYY-MM-DD HH24:MI:SS')", timeStr);
         }
-        timeStr = String.format("'%s'",timeStr);
 
         return timeStr;
     }

File: flinkx-sqlservercdc/flinkx-sqlservercdc-core/src/main/java/com/dtstack/flinkx/sqlservercdc/SqlServerCdcUtil.java
Patch:
@@ -45,7 +45,7 @@ public class SqlServerCdcUtil {
 
     private static final String STATEMENTS_PLACEHOLDER = "#";
     private static final String CHECK_CDC_DATABASE = "select 1 from sys.databases where name='%s' AND is_cdc_enabled=1";
-    private static final String CHECK_CDC_TABLE = "select name from sys.tables where is_tracked_by_cdc = 1;";
+    private static final String CHECK_CDC_TABLE = "select sys.schemas.name+'.'+sys.tables.name from sys.tables, sys.schemas where sys.tables.is_tracked_by_cdc = 1 and sys.tables.schema_id = sys.schemas.schema_id;";
     private static final String GET_LIST_OF_CDC_ENABLED_TABLES = "EXEC sys.sp_cdc_help_change_data_capture";
     private static final String GET_MAX_LSN = "SELECT sys.fn_cdc_get_max_lsn()";
     private static final String INCREMENT_LSN = "SELECT sys.fn_cdc_increment_lsn(?)";

File: flinkx-sqlservercdc/flinkx-sqlservercdc-core/src/main/java/com/dtstack/flinkx/sqlservercdc/SqlServerCdcUtil.java
Patch:
@@ -45,7 +45,7 @@ public class SqlServerCdcUtil {
 
     private static final String STATEMENTS_PLACEHOLDER = "#";
     private static final String CHECK_CDC_DATABASE = "select 1 from sys.databases where name='%s' AND is_cdc_enabled=1";
-    private static final String CHECK_CDC_TABLE = "select name from sys.tables where is_tracked_by_cdc = 1;";
+    private static final String CHECK_CDC_TABLE = "select sys.schemas.name+'.'+sys.tables.name from sys.tables, sys.schemas where sys.tables.is_tracked_by_cdc = 1 and sys.tables.schema_id = sys.schemas.schema_id;";
     private static final String GET_LIST_OF_CDC_ENABLED_TABLES = "EXEC sys.sp_cdc_help_change_data_capture";
     private static final String GET_MAX_LSN = "SELECT sys.fn_cdc_get_max_lsn()";
     private static final String INCREMENT_LSN = "SELECT sys.fn_cdc_increment_lsn(?)";

File: flinkx-core/src/main/java/com/dtstack/flinkx/metrics/BaseMetric.java
Patch:
@@ -38,7 +38,7 @@ public class BaseMetric {
 
     protected final Logger LOG = LoggerFactory.getLogger(getClass());
 
-    private Long delayPeriodMill = 15000L;
+    private Long delayPeriodMill = 20000L;
 
     private MetricGroup flinkxOutput;
 

File: flinkx-emqx/flinkx-emqx-writer/src/main/java/com/dtstack/flinkx/emqx/format/EmqxOutputFormat.java
Patch:
@@ -52,8 +52,8 @@ public class EmqxOutputFormat extends RichOutputFormat {
     public int qos;
 
     private transient MqttClient client;
-    protected transient JsonDecoder jsonDecoder = new JsonDecoder();
-    protected transient static ObjectMapper objectMapper = new ObjectMapper();
+    protected static JsonDecoder jsonDecoder = new JsonDecoder();
+    protected static ObjectMapper objectMapper = new ObjectMapper();
 
 
     @Override
@@ -94,7 +94,7 @@ protected void writeSingleRecordInternal(Row row) throws WriteRecordException {
             message.setQos(qos);
             client.publish(topic, message);
         } catch (Throwable e) {
-            LOG.error("kafka writeSingleRecordInternal error:{}", ExceptionUtil.getErrorMessage(e));
+            LOG.error("emqx writeSingleRecordInternal error:{}", ExceptionUtil.getErrorMessage(e));
             throw new WriteRecordException(e.getMessage(), e);
         }
     }

File: flinkx-test/src/main/java/com/dtstack/flinkx/test/PluginNameConstrant.java
Patch:
@@ -47,6 +47,7 @@ public class PluginNameConstrant {
     public static final String CLICKHOUSE_READER = "clickhousereader";
     public static final String POLARDB_READER = "polardbreader";
     public static final String ORACLE_LOG_MINER_READER = "oraclelogminerreader";
+    public static final String EMQX_READER = "emqxreader";
 
 
     public static final String STREAM_WRITER = "streamwriter";
@@ -72,4 +73,5 @@ public class PluginNameConstrant {
     public static final String KAFKA_WRITER = "kafkawriter";
     public static final String CLICKHOUSE_WRITER = "clickhousewriter";
     public static final String POLARDB_WRITER = "polardbwriter";
+    public static final String EMQX_WRITER = "emqxwriter";
 }

File: flinkx-core/src/main/java/com/dtstack/flinkx/authenticate/KerberosUtil.java
Patch:
@@ -43,7 +43,7 @@ public class KerberosUtil {
 
     public static Logger LOG = LoggerFactory.getLogger(KerberosUtil.class);
 
-    private static final String SP = File.separator;
+    private static final String SP = "/";
 
     private static final String KEY_SFTP_CONF = "sftpConf";
     private static final String KEY_REMOTE_DIR = "remoteDir";

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/FileSystemUtil.java
Patch:
@@ -91,6 +91,8 @@ private static FileSystem getFsWithKerberos(Map<String, Object> hadoopConfig, St
         KerberosUtil.loadKrb5Conf(hadoopConfig);
 
         UserGroupInformation ugi = KerberosUtil.loginAndReturnUGI(getConfiguration(hadoopConfig, defaultFs), principal, keytabFileName);
+        UserGroupInformation.setLoginUser(ugi);
+
         return ugi.doAs(new PrivilegedAction<FileSystem>() {
             @Override
             public FileSystem run(){

File: flinkx-core/src/main/java/com/dtstack/flinkx/metrics/AccumulatorCollector.java
Patch:
@@ -32,6 +32,7 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+import java.io.InputStream;
 import java.util.*;
 import java.util.concurrent.*;
 
@@ -122,8 +123,7 @@ private void formatMonitorUrl(String monitorUrlStr){
 
     private void checkMonitorUrlIsValid(){
         for (String monitorUrl : monitorUrls) {
-            try {
-                URLUtil.open(monitorUrl);
+            try(InputStream ignored = URLUtil.open(monitorUrl)) {
                 return;
             } catch (Exception e) {
                 LOG.warn("Connect error with monitor url:{}", monitorUrl);

File: flinkx-hive/flinkx-hive-writer/src/main/java/com/dtstack/flinkx/hive/writer/HiveOutputFormat.java
Patch:
@@ -199,7 +199,7 @@ public void writeRecord(Row row) throws IOException {
         } catch (Exception e) {
             // 写入产生的脏数据已经由hdfsOutputFormat处理了，这里不用再处理了，只打印日志
             if (numWriteCounter.getLocalValue() % 1000 == 0) {
-                LOG.warn("写入hdfs", e);
+                LOG.warn("写入hdfs异常:", e);
             }
         }
     }

File: flinkx-hive/flinkx-hive-core/src/main/java/com/dtstack/flinkx/hive/util/HiveUtil.java
Patch:
@@ -90,7 +90,7 @@ public void createPartition(TableInfo tableInfo, String partition) {
         try {
             connection = DBUtil.getConnection(connectionInfo);
             String sql = String.format(CREATE_PARTITION_TEMPLATE, tableInfo.getTablePath(), partition);
-            DBUtil.executeSqlWithoutResultSet(connection, sql);
+            DBUtil.executeSqlWithoutResultSet(connectionInfo, connection, sql);
         } catch (Exception e) {
             logger.error("", e);
             throw e;
@@ -108,7 +108,7 @@ public void createPartition(TableInfo tableInfo, String partition) {
     private void createTable(Connection connection, TableInfo tableInfo) {
         try {
             String sql = String.format(tableInfo.getCreateTableSql(), tableInfo.getTablePath());
-            DBUtil.executeSqlWithoutResultSet(connection, sql);
+            DBUtil.executeSqlWithoutResultSet(connectionInfo, connection, sql);
         } catch (Exception e) {
             if (!isTableExistsException(e.getMessage())) {
                 logger.error("create table happens error:", e);

File: flinkx-hive/flinkx-hive-core/src/main/java/com/dtstack/flinkx/hive/util/HiveUtil.java
Patch:
@@ -213,6 +213,7 @@ public static String getCreateTableHql(TableInfo tableInfo) {
     }
 
     public static String getHiveColumnType(String originType) {
+        originType = originType.trim();
         int indexOfBrackets = originType.indexOf(LEFT_BRACKETS);
         if (indexOfBrackets > -1) {
             String type = originType.substring(0, indexOfBrackets);

File: flinkx-hive/flinkx-hive-writer/src/main/java/com/dtstack/flinkx/hive/writer/HiveWriter.java
Patch:
@@ -152,7 +152,7 @@ private void formatHiveTableInfo(String tablesColumn) {
                 tableInfo.setStore(fileType);
                 tableInfo.setTableName(tableName);
                 for (Map<String, Object> column : tableColumns) {
-                    tableInfo.addColumnAndType(MapUtils.getString(column, HiveUtil.TABLE_COLUMN_KEY), HiveUtil.convertType(MapUtils.getString(column, HiveUtil.TABLE_COLUMN_TYPE)));
+                    tableInfo.addColumnAndType(MapUtils.getString(column, HiveUtil.TABLE_COLUMN_KEY), HiveUtil.getHiveColumnType(MapUtils.getString(column, HiveUtil.TABLE_COLUMN_TYPE)));
                 }
                 String createTableSql = HiveUtil.getCreateTableHql(tableInfo);
                 tableInfo.setCreateTableSql(createTableSql);

File: flinkx-hdfs/flinkx-hdfs-reader/src/main/java/com/dtstack/flinkx/hdfs/reader/HdfsOrcInputFormat.java
Patch:
@@ -170,7 +170,7 @@ private List<String> parseColumnAndType(String typeStruct){
     public HdfsOrcInputSplit[] createInputSplitsInternal(int minNumSplits) throws IOException {
         JobConf jobConf = FileSystemUtil.getJobConf(hadoopConfig, defaultFS);
         org.apache.hadoop.mapred.FileInputFormat.setInputPaths(jobConf, inputPath);
-        org.apache.hadoop.mapred.FileInputFormat.setInputPathFilter(conf, HdfsPathFilter.class);
+        org.apache.hadoop.mapred.FileInputFormat.setInputPathFilter(buildConfig(), HdfsPathFilter.class);
 
         OrcInputFormat orcInputFormat = new OrcInputFormat();
         org.apache.hadoop.mapred.InputSplit[] splits = orcInputFormat.getSplits(jobConf, minNumSplits);

File: flinkx-hdfs/flinkx-hdfs-reader/src/main/java/com/dtstack/flinkx/hdfs/reader/HdfsTextInputFormat.java
Patch:
@@ -59,7 +59,7 @@ public void openInputFormat() throws IOException {
     @Override
     public InputSplit[] createInputSplitsInternal(int minNumSplits) throws IOException {
         JobConf jobConf = FileSystemUtil.getJobConf(hadoopConfig, defaultFS);
-        org.apache.hadoop.mapred.FileInputFormat.setInputPathFilter(conf, HdfsPathFilter.class);
+        org.apache.hadoop.mapred.FileInputFormat.setInputPathFilter(buildConfig(), HdfsPathFilter.class);
 
         org.apache.hadoop.mapred.FileInputFormat.setInputPaths(jobConf, inputPath);
         TextInputFormat inputFormat = new TextInputFormat();

File: flinkx-oraclelogminer/flinkx-oraclelogminer-reader/src/main/java/com/dtstack/flinkx/oraclelogminer/format/OracleLogMinerInputFormat.java
Patch:
@@ -25,6 +25,7 @@
 import com.dtstack.flinkx.util.ClassUtil;
 import org.apache.commons.lang.StringUtils;
 import org.apache.commons.lang.time.DateFormatUtils;
+import org.apache.flink.core.io.GenericInputSplit;
 import org.apache.flink.core.io.InputSplit;
 import org.apache.flink.types.Row;
 
@@ -58,7 +59,7 @@ public class OracleLogMinerInputFormat extends RichInputFormat {
 
     @Override
     protected InputSplit[] createInputSplitsInternal(int i) throws Exception {
-        return new InputSplit[0];
+        return new InputSplit[]{new GenericInputSplit(1,1)};
     }
 
     @Override

File: flinkx-oraclelogminer/flinkx-oraclelogminer-reader/src/main/java/com/dtstack/flinkx/oraclelogminer/format/OracleLogMinerInputFormat.java
Patch:
@@ -312,7 +312,9 @@ protected Row nextRecordInternal(Row row) throws IOException {
     public FormatState getFormatState() {
         super.getFormatState();
 
-        formatState.setState(offsetScn.toString());
+        if (formatState != null && offsetScn != null) {
+            formatState.setState(offsetScn.toString());
+        }
         return formatState;
     }
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/inputformat/RichInputFormat.java
Patch:
@@ -225,7 +225,9 @@ public void closeInputFormat() throws IOException {
             updateDuration();
         }
 
-        inputMetric.waitForMetricReport();
+        if (inputMetric != null) {
+            inputMetric.waitForMetricReport();
+        }
 
         if(byteRateLimiter != null){
             byteRateLimiter.stop();

File: flinkx-core/src/main/java/com/dtstack/flinkx/outputformat/RichOutputFormat.java
Patch:
@@ -456,7 +456,9 @@ public void close() throws IOException {
                     waitWhile("#4");
                 }
 
-                outputMetric.waitForMetricReport();
+                if (outputMetric != null) {
+                    outputMetric.waitForMetricReport();
+                }
             }finally {
                 if(dirtyDataManager != null) {
                     dirtyDataManager.close();

File: flinkx-core/src/main/java/com/dtstack/flinkx/inputformat/RichInputFormat.java
Patch:
@@ -262,7 +262,9 @@ public void closeInputFormat() throws IOException {
             updateDuration();
         }
 
-        inputMetric.waitForMetricReport();
+        if (inputMetric != null) {
+            inputMetric.waitForMetricReport();
+        }
 
         if(byteRateLimiter != null){
             byteRateLimiter.stop();

File: flinkx-core/src/main/java/com/dtstack/flinkx/outputformat/RichOutputFormat.java
Patch:
@@ -456,7 +456,9 @@ public void close() throws IOException {
                     waitWhile("#4");
                 }
 
-                outputMetric.waitForMetricReport();
+                if (outputMetric != null) {
+                    outputMetric.waitForMetricReport();
+                }
             }finally {
                 if(dirtyDataManager != null) {
                     dirtyDataManager.close();

File: flinkx-core/src/main/java/com/dtstack/flinkx/inputformat/RichInputFormat.java
Patch:
@@ -225,7 +225,9 @@ public void closeInputFormat() throws IOException {
             updateDuration();
         }
 
-        inputMetric.waitForMetricReport();
+        if (inputMetric != null) {
+            inputMetric.waitForMetricReport();
+        }
 
         if(byteRateLimiter != null){
             byteRateLimiter.stop();

File: flinkx-core/src/main/java/com/dtstack/flinkx/outputformat/RichOutputFormat.java
Patch:
@@ -456,7 +456,9 @@ public void close() throws IOException {
                     waitWhile("#4");
                 }
 
-                outputMetric.waitForMetricReport();
+                if (outputMetric != null) {
+                    outputMetric.waitForMetricReport();
+                }
             }finally {
                 if(dirtyDataManager != null) {
                     dirtyDataManager.close();

File: flinkx-binlog/flinkx-binlog-reader/src/main/java/com/dtstack/flinkx/binlog/reader/BinlogInputFormat.java
Patch:
@@ -117,7 +117,7 @@ protected void openInternal(InputSplit inputSplit) throws IOException {
 
         controller = new MysqlEventParser();
         controller.setConnectionCharset(Charset.forName("UTF-8"));
-        controller.setSlaveId(3344L);
+        controller.setSlaveId(new Object().hashCode());
         controller.setDetectingEnable(false);
         controller.setMasterInfo(new AuthenticationInfo(new InetSocketAddress(host, port), username, password));
         controller.setEnableTsdb(true);

File: flinkx-core/src/main/java/com/dtstack/flinkx/outputformat/RichOutputFormat.java
Patch:
@@ -26,7 +26,6 @@
 import com.dtstack.flinkx.latch.MetricLatch;
 import com.dtstack.flinkx.metrics.AccumulatorCollector;
 import com.dtstack.flinkx.metrics.BaseMetric;
-import com.dtstack.flinkx.metrics.MetricReporterHandler;
 import com.dtstack.flinkx.restore.FormatState;
 import com.dtstack.flinkx.util.ExceptionUtil;
 import com.dtstack.flinkx.util.URLUtil;
@@ -457,7 +456,7 @@ public void close() throws IOException {
                     waitWhile("#4");
                 }
 
-                MetricReporterHandler.reportMetrics(getRuntimeContext());
+                outputMetric.waitForMetricReport();
             }finally {
                 if(dirtyDataManager != null) {
                     dirtyDataManager.close();

File: flinkx-binlog/flinkx-binlog-reader/src/main/java/com/dtstack/flinkx/binlog/reader/BinlogInputFormat.java
Patch:
@@ -118,7 +118,8 @@ protected void openInternal(InputSplit inputSplit) throws IOException {
         controller = new MysqlEventParser();
         controller.setConnectionCharset(Charset.forName("UTF-8"));
         controller.setSlaveId(new Object().hashCode());
-        controller.setDetectingEnable(false);
+        controller.setDetectingEnable(true);
+        controller.setDetectingSQL("select CURRENT_DATE");
         controller.setMasterInfo(new AuthenticationInfo(new InetSocketAddress(host, port), username, password));
         controller.setEnableTsdb(true);
         controller.setDestination("example");

File: flinkx-core/src/main/java/com/dtstack/flinkx/outputformat/RichOutputFormat.java
Patch:
@@ -26,7 +26,6 @@
 import com.dtstack.flinkx.latch.MetricLatch;
 import com.dtstack.flinkx.metrics.AccumulatorCollector;
 import com.dtstack.flinkx.metrics.BaseMetric;
-import com.dtstack.flinkx.metrics.MetricReporterHandler;
 import com.dtstack.flinkx.restore.FormatState;
 import com.dtstack.flinkx.util.ExceptionUtil;
 import com.dtstack.flinkx.util.URLUtil;
@@ -457,7 +456,7 @@ public void close() throws IOException {
                     waitWhile("#4");
                 }
 
-                MetricReporterHandler.reportMetrics(getRuntimeContext());
+                outputMetric.waitForMetricReport();
             }finally {
                 if(dirtyDataManager != null) {
                     dirtyDataManager.close();

File: flinkx-core/src/main/java/com/dtstack/flinkx/outputformat/RichOutputFormat.java
Patch:
@@ -26,7 +26,6 @@
 import com.dtstack.flinkx.latch.MetricLatch;
 import com.dtstack.flinkx.metrics.AccumulatorCollector;
 import com.dtstack.flinkx.metrics.BaseMetric;
-import com.dtstack.flinkx.metrics.MetricReporterHandler;
 import com.dtstack.flinkx.restore.FormatState;
 import com.dtstack.flinkx.util.ExceptionUtil;
 import com.dtstack.flinkx.util.URLUtil;
@@ -457,7 +456,7 @@ public void close() throws IOException {
                     waitWhile("#4");
                 }
 
-                MetricReporterHandler.reportMetrics(getRuntimeContext());
+                outputMetric.waitForMetricReport();
             }finally {
                 if(dirtyDataManager != null) {
                     dirtyDataManager.close();

File: flinkx-core/src/main/java/com/dtstack/flinkx/log/DtLogger.java
Patch:
@@ -54,7 +54,7 @@ public class DtLogger {
 
 
     public static void config(LogConfig logConfig, String jobId) {
-        if (!logConfig.isLogger() || init) {
+        if (logConfig == null || !logConfig.isLogger() || init) {
             return;
         }
         synchronized (DtLogger.class) {

File: flinkx-oracle/flinkx-oracle-core/src/main/java/com/dtstack/flinkx/oracle/OracleDatabaseMeta.java
Patch:
@@ -37,7 +37,7 @@ public String quoteTable(String table) {
         table = table.replace("\"","");
         String[] part = table.split("\\.");
         if(part.length == 2) {
-            table = part[0] + "." + getStartQuote() + part[1] + getEndQuote();
+            table = getStartQuote() + part[0] + getEndQuote() + "." + getStartQuote() + part[1] + getEndQuote();
         } else {
             table = getStartQuote() + table + getEndQuote();
         }

File: flinkx-rdb/flinkx-rdb-reader/src/main/java/com.dtstack.flinkx.rdb.inputformat/JdbcInputFormat.java
Patch:
@@ -165,7 +165,7 @@ public void openInternal(InputSplit inputSplit) throws IOException {
                     startLocation = getLocation(incrementConfig.getColumnType(), incrementConfig.getStartLocation());
                 } else {
                     getMaxValue(inputSplit);
-                    startLocation = ((JdbcInputSplit) inputSplit).getEndLocation();
+                    startLocation = ((JdbcInputSplit) inputSplit).getStartLocation();
                 }
                 endLocationAccumulator.add(startLocation);
             } else if ((incrementConfig.isIncrement() && incrementConfig.isUseMaxFunc())) {

File: flinkx-rdb/flinkx-rdb-reader/src/main/java/com.dtstack.flinkx.rdb.inputformat/DistributedJdbcInputFormat.java
Patch:
@@ -167,8 +167,9 @@ protected boolean readNextRecord() throws IOException{
 
                         if (val instanceof String){
                             val = StringUtil.string2col(String.valueOf(val),metaColumn.getType(),metaColumn.getTimeFormat());
-                            currentRecord.setField(i,val);
                         }
+
+                        currentRecord.setField(i,val);
                     }
                 }
             } else {

File: flinkx-hdfs/flinkx-hdfs-reader/src/main/java/com/dtstack/flinkx/hdfs/reader/HdfsInputFormat.java
Patch:
@@ -25,7 +25,6 @@
 import org.apache.flink.core.io.InputSplit;
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.mapred.RecordReader;
-import sun.dc.pr.PathFiller;
 
 import java.io.IOException;
 import java.util.List;

File: flinkx-oraclelogminer/flinkx-oraclelogminer-reader/src/main/java/com/dtstack/flinkx/oraclelogminer/format/OracleLogMinerInputFormat.java
Patch:
@@ -111,9 +111,8 @@ private void initOffset(){
             // 获取最开始的scn
             offsetScn = getMinScn();
         } else if(ReadPosition.CURRENT.name().equalsIgnoreCase(logMinerConfig.getReadPosition())){
-            // FIXME 获取到的 scn 比实际的值要大
-            scnCopy = getCurrentScn();
-            offsetScn = getLogFileStartPositionByScn(scnCopy);
+            skipRecord = false;
+            offsetScn = getCurrentScn();
         } else if(ReadPosition.TIME.name().equalsIgnoreCase(logMinerConfig.getReadPosition())){
             skipRecord = false;
 

File: flinkx-oraclelogminer/flinkx-oraclelogminer-reader/src/main/java/com/dtstack/flinkx/oraclelogminer/util/LogMinerUtil.java
Patch:
@@ -51,8 +51,6 @@ public class LogMinerUtil {
     public final static String KEY_SQL_REDO = "SQL_REDO";
     public final static String KEY_CSF = "CSF";
     public final static String KEY_SCN = "SCN";
-    public final static String KEY_COMMIT_SCN = "COMMIT_SCN";
-    public final static String KEY_ROW_ID = "ROW_ID";
     public final static String KEY_CURRENT_SCN = "CURRENT_SCN";
     public final static String KEY_FIRST_CHANGE = "FIRST_CHANGE#";
 

File: flinkx-oraclelogminer/flinkx-oraclelogminer-reader/src/main/java/com/dtstack/flinkx/oraclelogminer/format/LogMinerConfig.java
Patch:
@@ -46,6 +46,9 @@ public class LogMinerConfig implements Serializable {
      */
     private String readPosition = "current";
 
+    /**
+     * 毫秒级时间戳
+     */
     private long startTime = 0;
 
     private String startSCN = "";

File: flinkx-hdfs/flinkx-hdfs-core/src/main/java/org/apache/hadoop/hive/shims/ShimLoader.java
Patch:
@@ -17,13 +17,13 @@
  */
 package org.apache.hadoop.hive.shims;
 
-import java.util.HashMap;
-import java.util.Map;
-
 import org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge;
 import org.apache.hadoop.util.VersionInfo;
 import org.apache.log4j.AppenderSkeleton;
 
+import java.util.HashMap;
+import java.util.Map;
+
 /**
  * ShimLoader.
  *

File: flinkx-core/src/main/java/com/dtstack/flinkx/writer/DirtyDataManager.java
Patch:
@@ -34,6 +34,7 @@
 import org.apache.hadoop.hdfs.client.HdfsDataOutputStream;
 
 import java.io.IOException;
+import java.nio.charset.StandardCharsets;
 import java.util.*;
 
 import static com.dtstack.flinkx.writer.WriteErrorTypes.*;
@@ -78,8 +79,8 @@ public String writeData(Row row, WriteRecordException ex) {
         String errorType = retrieveCategory(ex);
         String line = StringUtils.join(new String[]{content,errorType, gson.toJson(ex.toString()), DateUtil.timestampToString(new Date()) }, FIELD_DELIMITER);
         try {
-            stream.writeChars(line);
-            stream.writeChars(LINE_DELIMITER);
+            stream.write(line.getBytes(StandardCharsets.UTF_8));
+            stream.write(LINE_DELIMITER.getBytes(StandardCharsets.UTF_8));
             DFSOutputStream dfsOutputStream = (DFSOutputStream) stream.getWrappedStream();
             dfsOutputStream.hsync(syncFlags);
             return errorType;

File: flinkx-launcher/src/main/java/com/dtstack/flinkx/launcher/Launcher.java
Patch:
@@ -112,8 +112,6 @@ public static void main(String[] args) throws Exception {
 
                 clusterClient.run(program, Integer.parseInt(launcherOptions.getParallelism()));
                 clusterClient.shutdown();
-                clusterClient.run(program, Integer.parseInt(launcherOptions.getParallelism()));
-                clusterClient.shutdown();
             }else{
                 String confProp = launcherOptions.getConfProp();
                 if (StringUtils.isBlank(confProp)){

File: flinkx-rdb/flinkx-rdb-writer/src/main/java/com/dtstack/flinkx/rdb/outputformat/JdbcOutputFormat.java
Patch:
@@ -47,7 +47,7 @@
  */
 public class JdbcOutputFormat extends RichOutputFormat {
 
-    private static final Logger LOG = LoggerFactory.getLogger(JdbcOutputFormat.class);
+    protected static final Logger LOG = LoggerFactory.getLogger(JdbcOutputFormat.class);
 
     protected static final long serialVersionUID = 1L;
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/metrics/BaseMetric.java
Patch:
@@ -108,7 +108,7 @@ private void initPeriod() {
             LOG.info("InputMetric.scheduledFutureTask.schedulePeriodMill:{} ...", schedulePeriodMill);
 
             if (schedulePeriodMill > DEFAULT_PERIOD_MILLISECONDS) {
-                this.delayPeriodMill = (long) (schedulePeriodMill * 1.2);
+                this.delayPeriodMill = (long) (schedulePeriodMill * 2);
             }
         } catch (Exception e) {
             LOG.error("", e);

File: flinkx-core/src/main/java/com/dtstack/flinkx/reader/MetaColumn.java
Patch:
@@ -112,7 +112,7 @@ public static List<MetaColumn> getMetaColumns(List columns){
                             mc.setIndex(doubleColIndex.intValue());
                         }
                     } else {
-                        mc.setIndex(i);
+                        mc.setIndex(-1);
                     }
 
                     mc.setName(sm.get("name") != null ? String.valueOf(sm.get("name")) : null);

File: flinkx-core/src/main/java/com/dtstack/flinkx/metrics/BaseMetric.java
Patch:
@@ -108,7 +108,7 @@ private void initPeriod() {
             LOG.info("InputMetric.scheduledFutureTask.schedulePeriodMill:{} ...", schedulePeriodMill);
 
             if (schedulePeriodMill > DEFAULT_PERIOD_MILLISECONDS) {
-                this.delayPeriodMill = (long) (schedulePeriodMill * 1.2);
+                this.delayPeriodMill = (long) (schedulePeriodMill * 2);
             }
         } catch (Exception e) {
             LOG.error("", e);

File: flinkx-core/src/main/java/com/dtstack/flinkx/reader/MetaColumn.java
Patch:
@@ -112,7 +112,7 @@ public static List<MetaColumn> getMetaColumns(List columns){
                             mc.setIndex(doubleColIndex.intValue());
                         }
                     } else {
-                        mc.setIndex(i);
+                        mc.setIndex(-1);
                     }
 
                     mc.setName(sm.get("name") != null ? String.valueOf(sm.get("name")) : null);

File: flinkx-core/src/main/java/com/dtstack/flinkx/outputformat/RichOutputFormat.java
Patch:
@@ -324,8 +324,9 @@ protected void writeSingleRecord(Row row) {
         try {
             writeSingleRecordInternal(row);
 
-            if(!restoreConfig.isRestore()){
+            if(!restoreConfig.isRestore() || restoreConfig.isStream()){
                 numWriteCounter.add(1);
+                snapshotWriteCounter.add(1);
             }
         } catch(WriteRecordException e) {
             saveErrorData(row, e);

File: flinkx-core/src/main/java/com/dtstack/flinkx/outputformat/FileOutputFormat.java
Patch:
@@ -232,11 +232,11 @@ public FormatState getFormatState() {
             }
 
             snapshotWriteCounter.add(sumRowsOfBlock);
+            numWriteCounter.add(sumRowsOfBlock);
             formatState.setNumberWrite(numWriteCounter.getLocalValue());
             if (!restoreConfig.isStream()){
                 formatState.setState(lastRow.getField(restoreConfig.getRestoreColumnIndex()));
             }
-
             sumRowsOfBlock = 0;
             formatState.setJobId(jobId);
             formatState.setFileIndex(blockIndex-1);
@@ -252,7 +252,6 @@ public FormatState getFormatState() {
     @Override
     public void closeInternal() throws IOException {
         readyCheckpoint = false;
-
         //最后触发一次 block文件重命名，为 .data 目录下的文件移动到数据目录做准备
         if(isTaskEndsNormally()){
             flushData();
@@ -261,6 +260,7 @@ public void closeInternal() throws IOException {
                 moveTemporaryDataBlockFileToDirectory();
             }
         }
+        numWriteCounter.add(sumRowsOfBlock);
     }
 
     @Override

File: flinkx-rdb/flinkx-rdb-writer/src/main/java/com/dtstack/flinkx/rdb/outputformat/JdbcOutputFormat.java
Patch:
@@ -285,6 +285,7 @@ public FormatState getFormatState(){
                 LOG.info("getFormatState:Commit connection success");
 
                 snapshotWriteCounter.add(rowsOfCurrentTransaction);
+                numWriteCounter.add(rowsOfCurrentTransaction);
                 rowsOfCurrentTransaction = 0;
 
                 formatState.setState(lastRow.getField(restoreConfig.getRestoreColumnIndex()));
@@ -360,6 +361,7 @@ public void closeInternal() {
         readyCheckpoint = false;
         boolean commit = true;
         try{
+            numWriteCounter.add(rowsOfCurrentTransaction);
             String state = getTaskState();
             // Do not commit a transaction when the task is canceled or failed
             if(!RUNNING_STATE.equals(state) && restoreConfig.isRestore()){

File: flinkx-core/src/main/java/com/dtstack/flinkx/outputformat/FileOutputFormat.java
Patch:
@@ -127,7 +127,7 @@ protected void actionBeforeWriteData(){
 
         try{
             // 覆盖模式并且不是从检查点恢复时先删除数据目录
-            if(!APPEND_MODE.equalsIgnoreCase(writeMode) && formatState != null && formatState.getState() == null){
+            if(!APPEND_MODE.equalsIgnoreCase(writeMode) && (formatState == null || formatState.getState() == null)){
                 coverageData();
             }
 

File: flinkx-hive/flinkx-hive-writer/src/main/java/com/dtstack/flinkx/hive/writer/HiveWriter.java
Patch:
@@ -102,8 +102,8 @@ public HiveWriter(DataTransferConfig config) {
 
         mode = writerConfig.getParameter().getStringVal(KEY_WRITE_MODE, EWriteModeType.APPEND.name());
         jdbcUrl = writerConfig.getParameter().getStringVal(KEY_JDBC_URL);
-        password = writerConfig.getParameter().getStringVal(KEY_USERNAME);
-        username = writerConfig.getParameter().getStringVal(KEY_PASSWORD);
+        username = writerConfig.getParameter().getStringVal(KEY_USERNAME);
+        password = writerConfig.getParameter().getStringVal(KEY_PASSWORD);
 
         String distributeTable = writerConfig.getParameter().getStringVal(KEY_DISTRIBUTE_TABLE);
         formatHiveDistributeInfo(distributeTable);

File: flinkx-hive/flinkx-hive-writer/src/main/java/com/dtstack/flinkx/hive/writer/HiveWriter.java
Patch:
@@ -102,8 +102,8 @@ public HiveWriter(DataTransferConfig config) {
         mode = writerConfig.getParameter().getStringVal(KEY_WRITE_MODE, EWriteModeType.APPEND.name());
         jdbcUrl = writerConfig.getParameter().getStringVal(KEY_JDBC_URL);
         formatHiveJdbcUrlInfo();
-        password = writerConfig.getParameter().getStringVal(KEY_USERNAME);
-        username = writerConfig.getParameter().getStringVal(KEY_PASSWORD);
+        username = writerConfig.getParameter().getStringVal(KEY_USERNAME);
+        password = writerConfig.getParameter().getStringVal(KEY_PASSWORD);
 
         String distributeTable = writerConfig.getParameter().getStringVal(KEY_DISTRIBUTE_TABLE);
         formatHiveDistributeInfo(distributeTable);

File: flinkx-hive/flinkx-hive-writer/src/main/java/com/dtstack/flinkx/hive/writer/HiveWriter.java
Patch:
@@ -102,8 +102,8 @@ public HiveWriter(DataTransferConfig config) {
 
         mode = writerConfig.getParameter().getStringVal(KEY_WRITE_MODE, EWriteModeType.APPEND.name());
         jdbcUrl = writerConfig.getParameter().getStringVal(KEY_JDBC_URL);
-        password = writerConfig.getParameter().getStringVal(KEY_USERNAME);
-        username = writerConfig.getParameter().getStringVal(KEY_PASSWORD);
+        username = writerConfig.getParameter().getStringVal(KEY_USERNAME);
+        password = writerConfig.getParameter().getStringVal(KEY_PASSWORD);
 
         String distributeTable = writerConfig.getParameter().getStringVal(KEY_DISTRIBUTE_TABLE);
         formatHiveDistributeInfo(distributeTable);

File: flinkx-hive/flinkx-hive-writer/src/main/java/com/dtstack/flinkx/hive/writer/HiveWriter.java
Patch:
@@ -102,8 +102,8 @@ public HiveWriter(DataTransferConfig config) {
 
         mode = writerConfig.getParameter().getStringVal(KEY_WRITE_MODE, EWriteModeType.APPEND.name());
         jdbcUrl = writerConfig.getParameter().getStringVal(KEY_JDBC_URL);
-        password = writerConfig.getParameter().getStringVal(KEY_USERNAME);
-        username = writerConfig.getParameter().getStringVal(KEY_PASSWORD);
+        username = writerConfig.getParameter().getStringVal(KEY_USERNAME);
+        password = writerConfig.getParameter().getStringVal(KEY_PASSWORD);
 
         String distributeTable = writerConfig.getParameter().getStringVal(KEY_DISTRIBUTE_TABLE);
         formatHiveDistributeInfo(distributeTable);

File: flinkx-ftp/flinkx-ftp-reader/src/main/java/com/dtstack/flinkx/ftp/reader/FtpInputFormatBuilder.java
Patch:
@@ -3,7 +3,7 @@
 import com.dtstack.flinkx.inputformat.RichInputFormatBuilder;
 import com.dtstack.flinkx.reader.MetaColumn;
 import org.apache.commons.lang.StringUtils;
-import org.apache.flink.hadoop.shaded.com.google.common.base.Preconditions;
+import com.google.common.base.Preconditions;
 import java.util.List;
 
 

File: flinkx-hbase/flinkx-hbase-reader/src/main/java/com/dtstack/flinkx/hbase/reader/HbaseInputFormat.java
Patch:
@@ -41,7 +41,7 @@
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
-import org.apache.flink.hadoop.shaded.com.google.common.collect.Maps;
+import com.google.common.collect.Maps;
 
 
 /**

File: flinkx-hbase/flinkx-hbase-writer/src/main/java/com/dtstack/flinkx/hbase/writer/HbaseOutputFormat.java
Patch:
@@ -42,7 +42,7 @@
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
-import org.apache.flink.hadoop.shaded.com.google.common.collect.Maps;
+import com.google.common.collect.Maps;
 
 /**
  * The Hbase Implementation of OutputFormat

File: flinkx-hbase/flinkx-hbase-writer/src/main/java/com/dtstack/flinkx/hbase/writer/HbaseOutputFormatBuilder.java
Patch:
@@ -21,7 +21,7 @@
 import com.dtstack.flinkx.hbase.HbaseConfigConstants;
 import com.dtstack.flinkx.outputformat.RichOutputFormatBuilder;
 import org.apache.commons.lang.StringUtils;
-import org.apache.flink.hadoop.shaded.com.google.common.base.Preconditions;
+import com.google.common.base.Preconditions;
 import java.util.List;
 import java.util.Map;
 

File: flinkx-core/src/main/java/org/apache/flink/streaming/api/functions/source/DtInputFormatSourceFunction.java
Patch:
@@ -17,6 +17,7 @@
 
 package org.apache.flink.streaming.api.functions.source;
 
+import com.dtstack.flinkx.config.RestoreConfig;
 import com.dtstack.flinkx.restore.FormatState;
 import org.apache.flink.annotation.Internal;
 import org.apache.flink.api.common.io.InputFormat;
@@ -88,7 +89,8 @@ public void open(Configuration parameters) throws Exception {
 		}
 
         if (format instanceof com.dtstack.flinkx.inputformat.RichInputFormat){
-            isStream = ((com.dtstack.flinkx.inputformat.RichInputFormat) format).getRestoreConfig().isStream();
+			RestoreConfig restoreConfig = ((com.dtstack.flinkx.inputformat.RichInputFormat) format).getRestoreConfig();
+			isStream = restoreConfig != null && restoreConfig.isStream();
             if(formatStateMap != null){
                 ((com.dtstack.flinkx.inputformat.RichInputFormat) format).setRestoreState(formatStateMap.get(context.getIndexOfThisSubtask()));
             }

File: flinkx-core/src/main/java/org/apache/flink/streaming/api/functions/source/DtInputFormatSourceFunction.java
Patch:
@@ -17,6 +17,7 @@
 
 package org.apache.flink.streaming.api.functions.source;
 
+import com.dtstack.flinkx.config.RestoreConfig;
 import com.dtstack.flinkx.restore.FormatState;
 import org.apache.flink.annotation.Internal;
 import org.apache.flink.api.common.io.InputFormat;
@@ -88,7 +89,8 @@ public void open(Configuration parameters) throws Exception {
 		}
 
         if (format instanceof com.dtstack.flinkx.inputformat.RichInputFormat){
-            isStream = ((com.dtstack.flinkx.inputformat.RichInputFormat) format).getRestoreConfig().isStream();
+			RestoreConfig restoreConfig = ((com.dtstack.flinkx.inputformat.RichInputFormat) format).getRestoreConfig();
+			isStream = restoreConfig != null && restoreConfig.isStream();
             if(formatStateMap != null){
                 ((com.dtstack.flinkx.inputformat.RichInputFormat) format).setRestoreState(formatStateMap.get(context.getIndexOfThisSubtask()));
             }

File: flinkx-hive/flinkx-hive-writer/src/main/java/com/dtstack/flinkx/hive/writer/HiveOutputFormat.java
Patch:
@@ -47,7 +47,7 @@
  */
 public class HiveOutputFormat extends RichOutputFormat {
 
-    private static Logger logger = LoggerFactory.getLogger(HiveOutputFormat.class);
+    private static final Logger logger = LoggerFactory.getLogger(HiveOutputFormat.class);
 
     private static final String SP = "/";
 
@@ -222,6 +222,8 @@ private Pair<HdfsOutputFormat, TableInfo> getHdfsOutputFormat(String tablePath,
             hdfsOutputFormatBuilder.setColumnTypes(tableInfo.getColumnTypes());
 
             outputFormat = (HdfsOutputFormat) hdfsOutputFormatBuilder.finish();
+            outputFormat.setDirtyDataManager(dirtyDataManager);
+            outputFormat.setErrorLimiter(errorLimiter);
             outputFormat.setRuntimeContext(getRuntimeContext());
             outputFormat.configure(parameters);
             outputFormat.open(taskNumber, numTasks);

File: flinkx-hive/flinkx-hive-writer/src/main/java/com/dtstack/flinkx/hive/writer/HiveOutputFormat.java
Patch:
@@ -47,7 +47,7 @@
  */
 public class HiveOutputFormat extends RichOutputFormat {
 
-    private static Logger logger = LoggerFactory.getLogger(HiveOutputFormat.class);
+    private static final Logger logger = LoggerFactory.getLogger(HiveOutputFormat.class);
 
     private static final String SP = "/";
 
@@ -222,6 +222,8 @@ private Pair<HdfsOutputFormat, TableInfo> getHdfsOutputFormat(String tablePath,
             hdfsOutputFormatBuilder.setColumnTypes(tableInfo.getColumnTypes());
 
             outputFormat = (HdfsOutputFormat) hdfsOutputFormatBuilder.finish();
+            outputFormat.setDirtyDataManager(dirtyDataManager);
+            outputFormat.setErrorLimiter(errorLimiter);
             outputFormat.setRuntimeContext(getRuntimeContext());
             outputFormat.configure(parameters);
             outputFormat.open(taskNumber, numTasks);

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/URLUtil.java
Patch:
@@ -64,6 +64,7 @@ public String call() throws Exception{
                     respBody = EntityUtils.toString(entity,charset);
                 }
 
+                response.close();
                 return respBody;
             }
         },MAX_RETRY_TIMES,SLEEP_TIME_MILLI_SECOND,false);

File: flinkx-kudu/flinkx-kudu-writer/src/main/java/com/dtstack/flinkx/kudu/writer/KuduWriter.java
Patch:
@@ -79,6 +79,8 @@ public DataStreamSink<?> writeData(DataStream<Row> dataSet) {
         builder.setKuduConfig(kuduConfig);
         builder.setWriteMode(writeMode);
         builder.setBatchInterval(batchInterval);
+        builder.setErrors(errors);
+        builder.setErrorRatio(errorRatio);
 
         DtOutputFormatSinkFunction formatSinkFunction = new DtOutputFormatSinkFunction(builder.finish());
         DataStreamSink<?> dataStreamSink = dataSet.addSink(formatSinkFunction);

File: flinkx-core/src/main/java/com/dtstack/flinkx/inputformat/RichInputFormat.java
Patch:
@@ -197,7 +197,6 @@ private Row setChannelInformation(Row internalRow){
      */
     public FormatState getFormatState() {
         if (formatState != null && numReadCounter != null && inputMetric!= null) {
-            formatState.setState(numReadCounter.getLocalValue());
             formatState.setMetric(inputMetric.getMetricCounters());
         }
         return formatState;

File: flinkx-core/src/main/java/com/dtstack/flinkx/inputformat/RichInputFormat.java
Patch:
@@ -197,7 +197,6 @@ private Row setChannelInformation(Row internalRow){
      */
     public FormatState getFormatState() {
         if (formatState != null && numReadCounter != null && inputMetric!= null) {
-            formatState.setState(numReadCounter.getLocalValue());
             formatState.setMetric(inputMetric.getMetricCounters());
         }
         return formatState;

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsOutputFormat.java
Patch:
@@ -127,7 +127,7 @@ protected void waitForActionFinishedBeforeWrite() {
     protected void cleanDirtyData() {
         int fileIndex = formatState.getFileIndex();
         String lastJobId = formatState.getJobId();
-        LOG.info("fileIndex = {}, lastJobId = {}",fileIndex, lastJobId);
+        LOG.info("start to cleanDirtyData, fileIndex = {}, lastJobId = {}",fileIndex, lastJobId);
         if(StringUtils.isBlank(lastJobId)){
             return;
         }

File: flinkx-core/src/main/java/com/dtstack/flinkx/outputformat/RichOutputFormat.java
Patch:
@@ -240,8 +240,7 @@ protected void initRestoreInfo(){
                 conversionErrCounter.add(formatState.getMetricValue(Metrics.NUM_CONVERSION_ERRORS));
                 otherErrCounter.add(formatState.getMetricValue(Metrics.NUM_OTHER_ERRORS));
 
-                //use snapshot write count
-                numWriteCounter.add(formatState.getMetricValue(Metrics.SNAPSHOT_WRITES));
+                numWriteCounter.add(formatState.getMetricValue(Metrics.NUM_WRITES));
 
                 snapshotWriteCounter.add(formatState.getMetricValue(Metrics.SNAPSHOT_WRITES));
                 bytesWriteCounter.add(formatState.getMetricValue(Metrics.WRITE_BYTES));

File: flinkx-rdb/flinkx-rdb-writer/src/main/java/com/dtstack/flinkx/rdb/outputformat/JdbcOutputFormat.java
Patch:
@@ -273,7 +273,7 @@ public FormatState getFormatState(){
         }
         snapshotWriteCounter.add(rowsOfCurrentTransaction);
         formatState.setState(lastRow.getField(restoreConfig.getRestoreColumnIndex()));
-        formatState.setNumberWrite(snapshotWriteCounter.getLocalValue());
+        formatState.setNumberWrite(numWriteCounter.getLocalValue());
         super.getFormatState();
         LOG.info("format state:{}", formatState.getState());
         return formatState;

File: flinkx-core/src/main/java/com/dtstack/flinkx/outputformat/FileOutputFormat.java
Patch:
@@ -236,11 +236,11 @@ public void flushOutputFormat() {
             try{
                 if (sumRowsOfBlock != 0) {
                     moveTemporaryDataFileToDirectory();
+                    sumRowsOfBlock = 0;
                 }
             } catch (Exception e){
                 throw new RuntimeException("Move temporary file to data directory error when flush data:", e);
             }
-            sumRowsOfBlock = 0;
         }
     }
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/outputformat/RichOutputFormat.java
Patch:
@@ -240,8 +240,7 @@ protected void initRestoreInfo(){
                 conversionErrCounter.add(formatState.getMetricValue(Metrics.NUM_CONVERSION_ERRORS));
                 otherErrCounter.add(formatState.getMetricValue(Metrics.NUM_OTHER_ERRORS));
 
-                //use snapshot write count
-                numWriteCounter.add(formatState.getMetricValue(Metrics.SNAPSHOT_WRITES));
+                numWriteCounter.add(formatState.getMetricValue(Metrics.NUM_WRITES));
 
                 snapshotWriteCounter.add(formatState.getMetricValue(Metrics.SNAPSHOT_WRITES));
                 bytesWriteCounter.add(formatState.getMetricValue(Metrics.WRITE_BYTES));

File: flinkx-rdb/flinkx-rdb-writer/src/main/java/com/dtstack/flinkx/rdb/outputformat/JdbcOutputFormat.java
Patch:
@@ -273,7 +273,7 @@ public FormatState getFormatState(){
         }
         snapshotWriteCounter.add(rowsOfCurrentTransaction);
         formatState.setState(lastRow.getField(restoreConfig.getRestoreColumnIndex()));
-        formatState.setNumberWrite(snapshotWriteCounter.getLocalValue());
+        formatState.setNumberWrite(numWriteCounter.getLocalValue());
         super.getFormatState();
         LOG.info("format state:{}", formatState.getState());
         return formatState;

File: flinkx-core/src/main/java/com/dtstack/flinkx/outputformat/RichOutputFormat.java
Patch:
@@ -241,7 +241,7 @@ protected void initRestoreInfo(){
                 otherErrCounter.add(formatState.getMetricValue(Metrics.NUM_OTHER_ERRORS));
 
                 //use snapshot write count
-                numWriteCounter.add(formatState.getMetricValue(Metrics.SNAPSHOT_WRITES));
+                numWriteCounter.add(formatState.getMetricValue(Metrics.NUM_WRITES));
 
                 snapshotWriteCounter.add(formatState.getMetricValue(Metrics.SNAPSHOT_WRITES));
                 bytesWriteCounter.add(formatState.getMetricValue(Metrics.WRITE_BYTES));

File: flinkx-rdb/flinkx-rdb-writer/src/main/java/com/dtstack/flinkx/rdb/outputformat/JdbcOutputFormat.java
Patch:
@@ -273,7 +273,7 @@ public FormatState getFormatState(){
         }
         snapshotWriteCounter.add(rowsOfCurrentTransaction);
         formatState.setState(lastRow.getField(restoreConfig.getRestoreColumnIndex()));
-        formatState.setNumberWrite(snapshotWriteCounter.getLocalValue());
+        formatState.setNumberWrite(numWriteCounter.getLocalValue());
         super.getFormatState();
         LOG.info("format state:{}", formatState.getState());
         return formatState;

File: flinkx-core/src/main/java/com/dtstack/flinkx/outputformat/FileOutputFormat.java
Patch:
@@ -213,7 +213,7 @@ public FormatState getFormatState() {
         if (restoreConfig.isStream() || readyCheckpoint){
             lastWriteSize = bytesWriteCounter.getLocalValue();
             snapshotWriteCounter.add(sumRowsOfBlock);
-            formatState.setNumberWrite(snapshotWriteCounter.getLocalValue());
+            formatState.setNumberWrite(numWriteCounter.getLocalValue());
             if (!restoreConfig.isStream()){
                 formatState.setState(lastRow.getField(restoreConfig.getRestoreColumnIndex()));
             }

File: flinkx-core/src/main/java/com/dtstack/flinkx/outputformat/FileOutputFormat.java
Patch:
@@ -236,11 +236,11 @@ public void flushOutputFormat() {
             try{
                 if (sumRowsOfBlock != 0) {
                     moveTemporaryDataFileToDirectory();
+                    sumRowsOfBlock = 0;
                 }
             } catch (Exception e){
                 throw new RuntimeException("Move temporary file to data directory error when flush data:", e);
             }
-            sumRowsOfBlock = 0;
         }
     }
 

File: flinkx-hive/flinkx-hive-writer/src/main/java/com/dtstack/flinkx/hive/writer/HiveOutputFormat.java
Patch:
@@ -137,10 +137,12 @@ public FormatState getFormatState() {
 
     @Override
     public void flushOutputFormat() {
+        LOG.info("flushOutputFormat start");
         Iterator<Map.Entry<String, HdfsOutputFormat>> entryIterator = outputFormats.entrySet().iterator();
         while (entryIterator.hasNext()) {
             Map.Entry<String, HdfsOutputFormat> entry = entryIterator.next();
-            entry.getValue().getFormatState();
+            entry.getValue().flushOutputFormat();
+            LOG.info("flushOutputFormat entry = {}", entry);
             if (partitionFormat.isTimeout(entry.getValue().getLastWriteTime())) {
                 try {
                     entry.getValue().close();

File: flinkx-db2/flinkx-db2-writer/src/main/java/com/dtstack/flinkx/db2/format/Db2OutputFormat.java
Patch:
@@ -15,7 +15,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package com.dtstack.flinkx.db2;
+package com.dtstack.flinkx.db2.format;
 
 import com.dtstack.flinkx.rdb.outputformat.JdbcOutputFormat;
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/outputformat/FileOutputFormat.java
Patch:
@@ -127,11 +127,11 @@ protected void actionBeforeWriteData(){
 
         try{
             // 覆盖模式并且不是从检查点恢复时先删除数据目录
-            if(!APPEND_MODE.equalsIgnoreCase(writeMode) && formatState.getState() == null){
+            if(!APPEND_MODE.equalsIgnoreCase(writeMode) && formatState != null && formatState.getState() == null){
                 coverageData();
             }
         } catch (Exception e){
-            LOG.error("writeMode = {}, formatState = {}, e = {}", writeMode, formatState.getState(), ExceptionUtil.getErrorMessage(e));
+            LOG.error("e = {}", ExceptionUtil.getErrorMessage(e));
             throw new RuntimeException(e);
         }
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/outputformat/FileOutputFormat.java
Patch:
@@ -21,6 +21,7 @@
 
 import com.dtstack.flinkx.exception.WriteRecordException;
 import com.dtstack.flinkx.restore.FormatState;
+import com.dtstack.flinkx.util.ExceptionUtil;
 import org.apache.commons.lang.ObjectUtils;
 import org.apache.commons.lang.StringUtils;
 import org.apache.flink.types.Row;
@@ -130,7 +131,8 @@ protected void actionBeforeWriteData(){
                 coverageData();
             }
         } catch (Exception e){
-            throw new RuntimeException("");
+            LOG.error("writeMode = {}, formatState = {}, e = {}", writeMode, formatState.getState(), ExceptionUtil.getErrorMessage(e));
+            throw new RuntimeException(e);
         }
 
         try {

File: flinkx-binlog/flinkx-binlog-reader/src/main/java/com/dtstack/flinkx/binlog/reader/BinlogInputFormat.java
Patch:
@@ -162,10 +162,10 @@ protected Row nextRecordInternal(Row row) throws IOException {
 
     @Override
     protected void closeInternal() throws IOException {
-        LOG.info("binlog closeInternal..., entryPosition:{}", formatState != null ? formatState.getState() : null);
-
         if (controller != null) {
             controller.stop();
+            controller = null;
+            LOG.info("binlog closeInternal..., entryPosition:{}", formatState != null ? formatState.getState() : null);
         }
 
     }

File: flinkx-binlog/flinkx-binlog-reader/src/main/java/com/dtstack/flinkx/binlog/reader/BinlogEventSink.java
Patch:
@@ -17,7 +17,6 @@
  */
 package com.dtstack.flinkx.binlog.reader;
 
-import com.alibaba.fastjson.JSON;
 import com.alibaba.otter.canal.common.AbstractCanalLifeCycle;
 import com.alibaba.otter.canal.protocol.CanalEntry;
 import com.alibaba.otter.canal.sink.exception.CanalSinkException;
@@ -100,6 +99,7 @@ private void processRowChange(CanalEntry.RowChange rowChange, String schema, Str
             message.put("schema", schema);
             message.put("table", table);
             message.put("ts", ts);
+            message.put("ingestion", System.nanoTime());
 
             if (pavingData){
                 for (CanalEntry.Column column : rowData.getAfterColumnsList()) {

File: flinkx-hive/flinkx-hive-core/src/main/java/com/dtstack/flinkx/hive/util/DBUtil.java
Patch:
@@ -230,7 +230,7 @@ private static Connection getHiveConnection(String url, Properties prop) throws
 
         if (StringUtils.isNotEmpty(host) && StringUtils.isNotEmpty(db)) {
             param = param == null ? "" : param;
-            url = String.format("jdbc:hive2://%s:%s/%s%s", host, port, db, param);
+            url = String.format("jdbc:hive2://%s:%s/%s", host, port, param);
             Connection connection = DriverManager.getConnection(url, prop);
             if (StringUtils.isNotEmpty(db)) {
                 try {

File: flinkx-core/src/main/java/com/dtstack/flinkx/latch/Latch.java
Patch:
@@ -28,7 +28,7 @@
  */
 public abstract class Latch {
 
-    protected int MAX_RETRY_TIMES = 10;
+    protected int MAX_RETRY_TIMES = 100;
 
     public abstract int getVal();
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/outputformat/RichOutputFormat.java
Patch:
@@ -209,7 +209,6 @@ public void open(int taskNumber, int numTasks) throws IOException {
             beforeWriteRecords();
             waitWhile("#2");
         }
-
     }
 
     private void initAccumulatorCollector(){
@@ -437,9 +436,9 @@ public void close() throws IOException {
             try{
                 closeInternal();
                 if(needWaitAfterCloseInternal()) {
+                    afterCloseInternal();
                     waitWhile("#4");
                 }
-                afterCloseInternal();
 
                 if(outputMetric != null){
                     outputMetric.waitForReportMetrics();

File: flinkx-core/src/main/java/com/dtstack/flinkx/inputformat/RichInputFormat.java
Patch:
@@ -139,7 +139,7 @@ private void initStatisticsAccumulator(){
         bytesReadCounter = getRuntimeContext().getLongCounter(Metrics.READ_BYTES);
         durationCounter = getRuntimeContext().getLongCounter(Metrics.READ_DURATION);
 
-        inputMetric = new BaseMetric(getRuntimeContext(), "reader", StringUtils.isEmpty(monitorUrls));
+        inputMetric = new BaseMetric(getRuntimeContext());
         inputMetric.addMetric(Metrics.NUM_READS, numReadCounter, true);
         inputMetric.addMetric(Metrics.READ_BYTES, bytesReadCounter, true);
         inputMetric.addMetric(Metrics.READ_DURATION, durationCounter);

File: flinkx-core/src/main/java/com/dtstack/flinkx/outputformat/RichOutputFormat.java
Patch:
@@ -273,7 +273,7 @@ protected void initStatisticsAccumulator(){
         bytesWriteCounter = context.getLongCounter(Metrics.WRITE_BYTES);
         durationCounter = context.getLongCounter(Metrics.WRITE_DURATION);
 
-        outputMetric = new BaseMetric(context, "writer", StringUtils.isEmpty(monitorUrl));
+        outputMetric = new BaseMetric(context);
         outputMetric.addMetric(Metrics.NUM_ERRORS, errCounter);
         outputMetric.addMetric(Metrics.NUM_NULL_ERRORS, nullErrCounter);
         outputMetric.addMetric(Metrics.NUM_DUPLICATE_ERRORS, duplicateErrCounter);

File: flinkx-core/src/main/java/com/dtstack/flinkx/inputformat/RichInputFormat.java
Patch:
@@ -196,7 +196,7 @@ private Row setChannelInformation(Row internalRow){
      * @return DataRecoverPoint
      */
     public FormatState getFormatState() {
-        if (formatState != null) {
+        if (formatState != null && numReadCounter != null && inputMetric!= null) {
             formatState.setState(numReadCounter.getLocalValue());
             formatState.setMetric(inputMetric.getMetricCounters());
         }

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsOrcOutputFormat.java
Patch:
@@ -233,7 +233,7 @@ private void getData(List<Object> recordList, int index, Row row) throws WriteRe
                 hiveDecimal = HiveDecimal.enforcePrecisionScale(hiveDecimal, decimalInfo.getPrecision(), decimalInfo.getScale());
                 if(hiveDecimal == null){
                     throw new WriteRecordException(String.format("decimal数据的precision和scale和元数据不匹配:decimal(%s, %s)",
-                            decimalInfo.getPrecision(), decimalInfo.getScale()), null, index, row);
+                            decimalInfo.getPrecision(), decimalInfo.getScale()), new IllegalArgumentException(), index, row);
                 }
 
                 HiveDecimalWritable hiveDecimalWritable = new HiveDecimalWritable(hiveDecimal);

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsParquetOutputFormat.java
Patch:
@@ -221,7 +221,7 @@ public void writeSingleRecordToFile(Row row) throws WriteRecordException {
                         hiveDecimal = HiveDecimal.enforcePrecisionScale(hiveDecimal, decimalInfo.getPrecision(), decimalInfo.getScale());
                         if(hiveDecimal == null){
                             throw new WriteRecordException(String.format("decimal数据的precision和scale和元数据不匹配:decimal(%s, %s)",
-                                    decimalInfo.getPrecision(), decimalInfo.getScale()), null, i, row);
+                                    decimalInfo.getPrecision(), decimalInfo.getScale()), new IllegalArgumentException(), i, row);
                         }
 
                         group.add(colName,decimalToBinary(hiveDecimal, decimalInfo.getPrecision(), decimalInfo.getScale()));

File: flinkx-ftp/flinkx-ftp-writer/src/main/java/com/dtstack/flinkx/ftp/writer/FtpOutputFormat.java
Patch:
@@ -269,7 +269,7 @@ protected void moveTemporaryDataFileToDirectory(){
             List<String> files = ftpHandler.getFiles(path + SP + tmpPath);
             for (String file : files) {
                 String fileName = file.substring(file.lastIndexOf(SP) + 1);
-                if (fileName.endsWith(FILE_SUFFIX) && !fileName.startsWith(DOT)){
+                if (fileName.endsWith(FILE_SUFFIX) && fileName.startsWith(String.valueOf(taskNumber))){
                     String newPath = path + SP + fileName;
                     LOG.info("Move file {} to path {}", file, newPath);
                     ftpHandler.rename(file, newPath);

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsOutputFormat.java
Patch:
@@ -249,7 +249,7 @@ protected void coverageData() throws IOException{
 
     @Override
     protected void moveTemporaryDataFileToDirectory() throws IOException{
-        PathFilter pathFilter = path -> !path.getName().startsWith(".");
+        PathFilter pathFilter = path -> path.getName().startsWith(String.valueOf(taskNumber));
         Path dir = new Path(outputFilePath);
         List<FileStatus> dataFiles = new ArrayList<>();
         Path tmpDir = new Path(outputFilePath + SP + DATA_SUBDIR);

File: flinkx-hive/flinkx-hive-writer/src/main/java/com/dtstack/flinkx/hive/writer/HiveOutputFormat.java
Patch:
@@ -133,7 +133,7 @@ protected void writeSingleRecordInternal(Row row) throws WriteRecordException {
 
     @Override
     public FormatState getFormatState() {
-        if (!restoreConfig.isRestore()){
+        if (!restoreConfig.isRestore()) {
             LOG.info("return null for formatState");
             return null;
         }

File: flinkx-ftp/flinkx-ftp-writer/src/main/java/com/dtstack/flinkx/ftp/writer/FtpOutputFormat.java
Patch:
@@ -269,7 +269,7 @@ protected void moveTemporaryDataFileToDirectory(){
             List<String> files = ftpHandler.getFiles(path + SP + tmpPath);
             for (String file : files) {
                 String fileName = file.substring(file.lastIndexOf(SP) + 1);
-                if (fileName.endsWith(FILE_SUFFIX) && !fileName.startsWith(DOT)){
+                if (fileName.endsWith(FILE_SUFFIX) && fileName.startsWith(String.valueOf(taskNumber))){
                     String newPath = path + SP + fileName;
                     LOG.info("Move file {} to path {}", file, newPath);
                     ftpHandler.rename(file, newPath);

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsOutputFormat.java
Patch:
@@ -244,7 +244,7 @@ protected void coverageData() throws IOException{
 
     @Override
     protected void moveTemporaryDataFileToDirectory() throws IOException{
-        PathFilter pathFilter = path -> !path.getName().startsWith(".");
+        PathFilter pathFilter = path -> path.getName().startsWith(String.valueOf(taskNumber));
         Path dir = new Path(outputFilePath);
         List<FileStatus> dataFiles = new ArrayList<>();
         Path tmpDir = new Path(outputFilePath + SP + DATA_SUBDIR);

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsTextOutputFormat.java
Patch:
@@ -225,7 +225,7 @@ protected String recordConvertDetailErrorMessage(int pos, Row row) {
     }
 
     @Override
-    public void closeInternal() throws IOException {
+    public void closeSource() throws IOException {
         OutputStream s = this.stream;
         if(s != null) {
             s.flush();

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsOrcOutputFormat.java
Patch:
@@ -154,10 +154,10 @@ public void writeSingleRecordToFile(Row row) throws WriteRecordException {
             }
 
             this.recordWriter.write(NullWritable.get(), this.orcSerde.serialize(recordList, this.inspector));
+            rowsOfCurrentBlock++;
 
             if(restoreConfig.isRestore()){
                 lastRow = row;
-                rowsOfCurrentBlock++;
             }
         } catch(Exception e) {
             if(e instanceof WriteRecordException){
@@ -273,8 +273,6 @@ protected String recordConvertDetailErrorMessage(int pos, Row row) {
 
     @Override
     protected void closeSource() throws IOException {
-        super.closeSource();
-
         RecordWriter rw = this.recordWriter;
         if(rw != null) {
             LOG.info("close:Current block writer record:" + rowsOfCurrentBlock);

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsParquetOutputFormat.java
Patch:
@@ -235,10 +235,10 @@ public void writeSingleRecordToFile(Row row) throws WriteRecordException {
             }
 
             writer.write(group);
+            rowsOfCurrentBlock++;
 
             if(restoreConfig.isRestore()){
                 lastRow = row;
-                rowsOfCurrentBlock++;
             }
         } catch (Exception e){
             if(e instanceof WriteRecordException){
@@ -279,8 +279,6 @@ protected String recordConvertDetailErrorMessage(int pos, Row row) {
 
     @Override
     protected void closeSource() throws IOException {
-        super.closeSource();
-
         if (writer != null){
             writer.close();
         }

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsTextOutputFormat.java
Patch:
@@ -202,10 +202,10 @@ public void writeSingleRecordToFile(Row row) throws WriteRecordException {
             bytes = sb.toString().getBytes(this.charsetName);
             this.stream.write(bytes);
             this.stream.write(NEWLINE);
+            rowsOfCurrentBlock++;
 
             if(restoreConfig.isRestore()){
                 lastRow = row;
-                rowsOfCurrentBlock++;
             }
 
             if(rowsOfCurrentBlock % BUFFER_SIZE == 0) {

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsOrcOutputFormat.java
Patch:
@@ -154,10 +154,10 @@ public void writeSingleRecordToFile(Row row) throws WriteRecordException {
             }
 
             this.recordWriter.write(NullWritable.get(), this.orcSerde.serialize(recordList, this.inspector));
+            rowsOfCurrentBlock++;
 
             if(restoreConfig.isRestore()){
                 lastRow = row;
-                rowsOfCurrentBlock++;
             }
         } catch(Exception e) {
             if(e instanceof WriteRecordException){
@@ -273,8 +273,6 @@ protected String recordConvertDetailErrorMessage(int pos, Row row) {
 
     @Override
     protected void closeSource() throws IOException {
-        super.closeSource();
-
         RecordWriter rw = this.recordWriter;
         if(rw != null) {
             LOG.info("close:Current block writer record:" + rowsOfCurrentBlock);

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsParquetOutputFormat.java
Patch:
@@ -235,10 +235,10 @@ public void writeSingleRecordToFile(Row row) throws WriteRecordException {
             }
 
             writer.write(group);
+            rowsOfCurrentBlock++;
 
             if(restoreConfig.isRestore()){
                 lastRow = row;
-                rowsOfCurrentBlock++;
             }
         } catch (Exception e){
             if(e instanceof WriteRecordException){
@@ -279,8 +279,6 @@ protected String recordConvertDetailErrorMessage(int pos, Row row) {
 
     @Override
     protected void closeSource() throws IOException {
-        super.closeSource();
-
         if (writer != null){
             writer.close();
         }

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsTextOutputFormat.java
Patch:
@@ -202,10 +202,10 @@ public void writeSingleRecordToFile(Row row) throws WriteRecordException {
             bytes = sb.toString().getBytes(this.charsetName);
             this.stream.write(bytes);
             this.stream.write(NEWLINE);
+            rowsOfCurrentBlock++;
 
             if(restoreConfig.isRestore()){
                 lastRow = row;
-                rowsOfCurrentBlock++;
             }
 
             if(rowsOfCurrentBlock % BUFFER_SIZE == 0) {
@@ -226,8 +226,6 @@ protected String recordConvertDetailErrorMessage(int pos, Row row) {
 
     @Override
     public void closeInternal() throws IOException {
-        super.closeSource();
-
         OutputStream s = this.stream;
         if(s != null) {
             s.flush();

File: flinkx-hive/flinkx-hive-writer/src/main/java/com/dtstack/flinkx/hive/writer/HiveOutputFormat.java
Patch:
@@ -28,7 +28,6 @@
 import com.dtstack.flinkx.hive.util.PathConverterUtil;
 import com.dtstack.flinkx.outputformat.RichOutputFormat;
 import com.dtstack.flinkx.restore.FormatState;
-import com.google.common.collect.Maps;
 import org.apache.commons.collections.MapUtils;
 import org.apache.commons.math3.util.Pair;
 import org.apache.flink.types.Row;
@@ -47,8 +46,6 @@
  */
 public class HiveOutputFormat extends RichOutputFormat {
 
-    private static final long serialVersionUID = -6012196822223887479L;
-
     private static Logger logger = LoggerFactory.getLogger(HiveOutputFormat.class);
 
     private static final String SP = "/";

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsOrcOutputFormat.java
Patch:
@@ -154,10 +154,10 @@ public void writeSingleRecordToFile(Row row) throws WriteRecordException {
             }
 
             this.recordWriter.write(NullWritable.get(), this.orcSerde.serialize(recordList, this.inspector));
+            rowsOfCurrentBlock++;
 
             if(restoreConfig.isRestore()){
                 lastRow = row;
-                rowsOfCurrentBlock++;
             }
         } catch(Exception e) {
             if(e instanceof WriteRecordException){
@@ -273,8 +273,6 @@ protected String recordConvertDetailErrorMessage(int pos, Row row) {
 
     @Override
     protected void closeSource() throws IOException {
-        super.closeSource();
-
         RecordWriter rw = this.recordWriter;
         if(rw != null) {
             LOG.info("close:Current block writer record:" + rowsOfCurrentBlock);

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsParquetOutputFormat.java
Patch:
@@ -235,10 +235,10 @@ public void writeSingleRecordToFile(Row row) throws WriteRecordException {
             }
 
             writer.write(group);
+            rowsOfCurrentBlock++;
 
             if(restoreConfig.isRestore()){
                 lastRow = row;
-                rowsOfCurrentBlock++;
             }
         } catch (Exception e){
             if(e instanceof WriteRecordException){
@@ -279,8 +279,6 @@ protected String recordConvertDetailErrorMessage(int pos, Row row) {
 
     @Override
     protected void closeSource() throws IOException {
-        super.closeSource();
-
         if (writer != null){
             writer.close();
         }

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsTextOutputFormat.java
Patch:
@@ -202,10 +202,10 @@ public void writeSingleRecordToFile(Row row) throws WriteRecordException {
             bytes = sb.toString().getBytes(this.charsetName);
             this.stream.write(bytes);
             this.stream.write(NEWLINE);
+            rowsOfCurrentBlock++;
 
             if(restoreConfig.isRestore()){
                 lastRow = row;
-                rowsOfCurrentBlock++;
             }
 
             if(rowsOfCurrentBlock % BUFFER_SIZE == 0) {
@@ -226,8 +226,6 @@ protected String recordConvertDetailErrorMessage(int pos, Row row) {
 
     @Override
     public void closeInternal() throws IOException {
-        super.closeSource();
-
         OutputStream s = this.stream;
         if(s != null) {
             s.flush();

File: flinkx-rdb/flinkx-rdb-reader/src/main/java/com.dtstack.flinkx.rdb.datareader/QuerySqlBuilder.java
Patch:
@@ -59,7 +59,7 @@ public QuerySqlBuilder(JdbcDataReader reader) {
         table = reader.table;
         metaColumns = reader.metaColumns;
         splitKey = reader.splitKey;
-        customFilter = reader.customSql;
+        customFilter = reader.where;
         customSql = reader.customSql;
         isSplitByKey = reader.getNumPartitions() > 1 && StringUtils.isNotEmpty(splitKey);
         isIncrement = reader.incrementConfig.isIncrement();

File: flinkx-hive/flinkx-hive-writer/src/main/java/com/dtstack/flinkx/hive/writer/HiveOutputFormat.java
Patch:
@@ -99,7 +99,7 @@ public class HiveOutputFormat extends RichOutputFormat {
     private transient HiveUtil hiveUtil;
     private transient TimePartitionFormat partitionFormat;
     private transient Map<String, TableInfo> tableCache = new HashMap<>();
-    private transient Map<String, HdfsOutputFormat> outputFormats = new HashMap();
+    private transient Map<String, HdfsOutputFormat> outputFormats = new HashMap<String, HdfsOutputFormat>();
 
     @Override
     public void configure(org.apache.flink.configuration.Configuration parameters) {

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/outputformat/JdbcOutputFormat.java
Patch:
@@ -115,6 +115,8 @@ protected PreparedStatement prepareTemplates() throws SQLException {
             throw new IllegalArgumentException("Unknown write mode:" + mode);
         }
 
+        LOG.info("write sql:{}", singleSql);
+
         return dbConn.prepareStatement(singleSql);
     }
 

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsOrcOutputFormat.java
Patch:
@@ -134,7 +134,6 @@ protected void nextBlock(){
 
     @Override
     public void writeSingleRecordToFile(Row row) throws WriteRecordException {
-        super.writeSingleRecordInternal(row);
 
         if (recordWriter == null){
             nextBlock();

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsParquetOutputFormat.java
Patch:
@@ -159,7 +159,6 @@ public float getDeviation(){
 
     @Override
     public void writeSingleRecordToFile(Row row) throws WriteRecordException {
-        super.writeSingleRecordInternal(row);
 
         if(writer == null){
             nextBlock();

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsTextOutputFormat.java
Patch:
@@ -104,7 +104,6 @@ protected void nextBlock(){
 
     @Override
     public void writeSingleRecordToFile(Row row) throws WriteRecordException {
-        super.writeSingleRecordInternal(row);
 
         if(stream == null){
             nextBlock();

File: flinkx-core/src/main/java/com/dtstack/flinkx/metrics/AccumulatorCollector.java
Patch:
@@ -206,7 +206,7 @@ private void collectAccumulatorWithApi(){
                 List<LinkedTreeMap> userTaskAccumulators = (List<LinkedTreeMap>) map.get(KEY_ACCUMULATORS);
                 for(LinkedTreeMap accumulator : userTaskAccumulators) {
                     String name = (String) accumulator.get(KEY_NAME);
-                    if(name != null) {
+                    if(name != null && !"tableCol".equalsIgnoreCase(name)) {
                         long value = Double.valueOf((String) accumulator.get(KEY_VALUE)).longValue();
                         ValueAccumulator valueAccumulator = valueAccumulatorMap.get(name);
                         if(valueAccumulator != null){

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsOutputFormat.java
Patch:
@@ -21,6 +21,7 @@
 import com.dtstack.flinkx.hdfs.HdfsUtil;
 import com.dtstack.flinkx.outputformat.RichOutputFormat;
 import com.dtstack.flinkx.restore.FormatState;
+import com.dtstack.flinkx.util.ColumnTypeUtil;
 import com.dtstack.flinkx.util.SysUtil;
 import com.google.common.collect.Lists;
 import org.apache.commons.lang.StringUtils;
@@ -110,6 +111,8 @@ public abstract class HdfsOutputFormat extends RichOutputFormat {
 
     protected long lastWriteSize;
 
+    protected transient Map<String, ColumnTypeUtil.DecimalInfo> decimalColInfo;
+
     private long nextNumForCheckDataSize = DEFAULT_RECORD_NUM_FOR_CHECK;
 
     protected void initColIndices() {

File: flinkx-postgresql/flinkx-postgresql-writer/src/main/java/com/dtstack/flinkx/postgresql/writer/PostgresqlWriter.java
Patch:
@@ -66,6 +66,7 @@ public DataStreamSink<?> writeData(DataStream<Row> dataSet) {
         builder.setFullColumn(fullColumn);
         builder.setUpdateKey(updateKey);
         builder.setTypeConverter(typeConverter);
+        builder.setRestoreConfig(restoreConfig);
         builder.setInsertSqlMode(insertSqlMode);
 
         OutputFormatSinkFunction sinkFunction = new OutputFormatSinkFunction(builder.finish());

File: flinkx-postgresql/flinkx-postgresql-writer/src/main/java/com/dtstack/flinkx/postgresql/writer/PostgresqlWriter.java
Patch:
@@ -66,6 +66,7 @@ public DataStreamSink<?> writeData(DataStream<Row> dataSet) {
         builder.setFullColumn(fullColumn);
         builder.setUpdateKey(updateKey);
         builder.setTypeConverter(typeConverter);
+        builder.setRestoreConfig(restoreConfig);
         builder.setInsertSqlMode(insertSqlMode);
 
         OutputFormatSinkFunction sinkFunction = new OutputFormatSinkFunction(builder.finish());

File: flinkx-kafka09/flinkx-kafka09-reader/src/main/java/com/dtstack/flinkx/kafka09/reader/Kafka09Reader.java
Patch:
@@ -25,7 +25,6 @@
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
 import org.apache.flink.types.Row;
 
-import java.util.List;
 import java.util.Map;
 
 import static com.dtstack.flinkx.kafka09.KafkaConfigKeys.*;

File: flinkx-kafka10/flinkx-kafka10-reader/src/main/java/com/dtstack/flinkx/kafka10/reader/Kafka10Reader.java
Patch:
@@ -61,7 +61,7 @@ public Kafka10Reader(DataTransferConfig config, StreamExecutionEnvironment env)
         consumerSettings = (Map<String, String>) readerConfig.getParameter().getVal(KEY_CONSUMER_SETTINGS);
 
         if (!consumerSettings.containsKey(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG)){
-            throw new IllegalArgumentException("bootstrap.servers must set in consumerSettings");
+            throw new IllegalArgumentException(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG + " must set in consumerSettings");
         }
     }
 

File: flinkx-kafka10/flinkx-kafka10-writer/src/main/java/com/dtstack/flinkx/kafka10/writer/Kafka10Writer.java
Patch:
@@ -51,7 +51,7 @@ public Kafka10Writer(DataTransferConfig config) {
         producerSettings = (Map<String, String>) writerConfig.getParameter().getVal(KEY_PRODUCER_SETTINGS);
 
         if (!producerSettings.containsKey(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG)){
-            throw new IllegalArgumentException("bootstrap.servers must set in producerSettings");
+            throw new IllegalArgumentException(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG + " must set in producerSettings");
         }
     }
 

File: flinkx-kafka11/flinkx-kafka11-reader/src/main/java/com/dtstack/flinkx/kafka11/reader/Kafka11Reader.java
Patch:
@@ -61,7 +61,7 @@ public Kafka11Reader(DataTransferConfig config, StreamExecutionEnvironment env)
         consumerSettings = (Map<String, String>) readerConfig.getParameter().getVal(KEY_CONSUMER_SETTINGS);
 
         if (!consumerSettings.containsKey(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG)){
-            throw new IllegalArgumentException("bootstrap.servers must set in consumerSettings");
+            throw new IllegalArgumentException(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG + " must set in consumerSettings");
         }
     }
 

File: flinkx-kafka11/flinkx-kafka11-writer/src/main/java/com/dtstack/flinkx/kafka11/writer/Kafka11Writer.java
Patch:
@@ -51,7 +51,7 @@ public Kafka11Writer(DataTransferConfig config) {
         producerSettings = (Map<String, String>) writerConfig.getParameter().getVal(KEY_PRODUCER_SETTINGS);
 
         if (!producerSettings.containsKey(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG)){
-            throw new IllegalArgumentException("bootstrap.servers must set in producerSettings");
+            throw new IllegalArgumentException(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG + " must set in producerSettings");
         }
     }
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/outputformat/RichOutputFormat.java
Patch:
@@ -228,8 +228,6 @@ protected void initRestoreInfo(){
             } else {
                 initState = formatState.getState();
 
-//                numWriteCounter.add(formatState.getNumberWrite());
-
                 errCounter.add(formatState.getMetricValue(Metrics.NUM_ERRORS));
                 nullErrCounter.add(formatState.getMetricValue(Metrics.NUM_NULL_ERRORS));
                 duplicateErrCounter.add(formatState.getMetricValue(Metrics.NUM_DUPLICATE_ERRORS));

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/FileSystemUtil.java
Patch:
@@ -53,7 +53,7 @@ public static FileSystem getFileSystem(Map<String, Object> hadoopConfig, String
     }
 
     private static boolean openKerberos(Map<String, Object> hadoopConfig){
-        if(!MapUtils.getBoolean(hadoopConfig, KEY_HADOOP_SECURITY_AUTHORIZATION)){
+        if(!MapUtils.getBoolean(hadoopConfig, KEY_HADOOP_SECURITY_AUTHORIZATION, false)){
             return false;
         }
 

File: flinkx-hdfs/flinkx-hdfs-core/src/main/java/com/dtstack/flinkx/hdfs/HdfsUtil.java
Patch:
@@ -43,6 +43,8 @@
  */
 public class HdfsUtil {
 
+    public static final String NULL_VALUE = "\\N";
+
     private static final String HADOOP_CONFIGE = System.getProperty("user.dir") + "/conf/hadoop/";
 
     private static final String HADOOP_CONF_DIR = System.getenv("HADOOP_CONF_DIR");

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsTextOutputFormat.java
Patch:
@@ -20,6 +20,7 @@
 
 import com.dtstack.flinkx.common.ColumnType;
 import com.dtstack.flinkx.exception.WriteRecordException;
+import com.dtstack.flinkx.hdfs.HdfsUtil;
 import com.dtstack.flinkx.util.DateUtil;
 import org.apache.commons.compress.compressors.bzip2.BZip2CompressorOutputStream;
 import org.apache.commons.compress.compressors.gzip.GzipCompressorOutputStream;
@@ -87,13 +88,14 @@ public void writeSingleRecordInternal(Row row) throws WriteRecordException {
                 Object column = row.getField(j);
 
                 if(column == null) {
+                    sb.append(HdfsUtil.NULL_VALUE);
                     continue;
                 }
 
                 String rowData = column.toString();
                 ColumnType columnType = ColumnType.fromString(columnTypes.get(j));
 
-                if(rowData == null || rowData.length() == 0){
+                if(rowData.length() == 0){
                     sb.append("");
                 } else {
                     switch (columnType) {

File: flinkx-postgresql/flinkx-postgresql-writer/src/main/java/com/dtstack/flinkx/postgresql/writer/PostgresqlWriter.java
Patch:
@@ -50,7 +50,7 @@ public DataStreamSink<?> writeData(DataStream<Row> dataSet) {
         builder.setDBUrl(dbUrl);
         builder.setUsername(username);
         builder.setPassword(password);
-        builder.setBatchInterval(getBatchSize());
+        builder.setBatchInterval(batchSize);
         builder.setMonitorUrls(monitorUrls);
         builder.setPreSql(preSql);
         builder.setPostSql(postSql);

File: flinkx-hdfs/flinkx-hdfs-core/src/main/java/com/dtstack/flinkx/hdfs/HdfsUtil.java
Patch:
@@ -43,6 +43,8 @@
  */
 public class HdfsUtil {
 
+    public static final String NULL_VALUE = "\\N";
+
     private static final String HADOOP_CONFIGE = System.getProperty("user.dir") + "/conf/hadoop/";
 
     private static final String HADOOP_CONF_DIR = System.getenv("HADOOP_CONF_DIR");

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsTextOutputFormat.java
Patch:
@@ -20,6 +20,7 @@
 
 import com.dtstack.flinkx.common.ColumnType;
 import com.dtstack.flinkx.exception.WriteRecordException;
+import com.dtstack.flinkx.hdfs.HdfsUtil;
 import com.dtstack.flinkx.util.DateUtil;
 import org.apache.commons.compress.compressors.bzip2.BZip2CompressorOutputStream;
 import org.apache.commons.compress.compressors.gzip.GzipCompressorOutputStream;
@@ -87,13 +88,14 @@ public void writeSingleRecordInternal(Row row) throws WriteRecordException {
                 Object column = row.getField(j);
 
                 if(column == null) {
+                    sb.append(HdfsUtil.NULL_VALUE);
                     continue;
                 }
 
                 String rowData = column.toString();
                 ColumnType columnType = ColumnType.fromString(columnTypes.get(j));
 
-                if(rowData == null || rowData.length() == 0){
+                if(rowData.length() == 0){
                     sb.append("");
                 } else {
                     switch (columnType) {

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsOutputFormat.java
Patch:
@@ -336,7 +336,9 @@ protected void afterCloseInternal()  {
             fs.createNewFile(new Path(finishedPath));
             LOG.info("Create finished tag dir:{}", finishedPath);
 
-            numWriteCounter.add(rowsOfCurrentBlock);
+            if(restoreConfig.isRestore()){
+                numWriteCounter.add(rowsOfCurrentBlock);
+            }
 
             if(taskNumber == 0) {
                 waitForAllTasksToFinish();

File: flinkx-rdb/flinkx-rdb-writer/src/main/java/com.dtstack.flinkx.rdb.datawriter/JdbcConfigKeys.java
Patch:
@@ -34,4 +34,5 @@ public class JdbcConfigKeys {
     public static final String KEY_BATCH_SIZE = "batchSize";
     public static final String KEY_UPDATE_KEY = "updateKey";
     public static final String KEY_FULL_COLUMN = "fullColumn";
+    public static final String KEY_INSERT_SQL_MODE = "insertSqlMode";
 }

File: flinkx-rdb/flinkx-rdb-reader/src/main/java/com.dtstack.flinkx.rdb.datareader/JdbcConfigKeys.java
Patch:
@@ -46,5 +46,7 @@ public class JdbcConfigKeys {
 
     public static final String KEY_CUSTOM_SQL = "customSql";
 
+    public static final String KEY_ORDER_BY_COLUMN = "orderByColumn";
+
     public static final String KEY_USE_MAX_FUNC = "useMaxFunc";
 }

File: flinkx-rdb/flinkx-rdb-reader/src/main/java/com.dtstack.flinkx.rdb.inputformat/DistributedJdbcInputFormat.java
Patch:
@@ -22,6 +22,7 @@
 import com.dtstack.flinkx.inputformat.RichInputFormat;
 import com.dtstack.flinkx.rdb.DataSource;
 import com.dtstack.flinkx.rdb.DatabaseInterface;
+import com.dtstack.flinkx.rdb.datareader.QuerySqlBuilder;
 import com.dtstack.flinkx.rdb.type.TypeConverterInterface;
 import com.dtstack.flinkx.rdb.util.DBUtil;
 import com.dtstack.flinkx.reader.MetaColumn;
@@ -117,8 +118,8 @@ private void openNextSource() throws SQLException{
         DataSource currentSource = sourceList.get(sourceIndex);
         currentConn = DBUtil.getConnection(currentSource.getJdbcUrl(), currentSource.getUserName(), currentSource.getPassword());
         currentConn.setAutoCommit(false);
-        String queryTemplate = DBUtil.getQuerySql(databaseInterface, currentSource.getTable(),metaColumns,splitKey,
-                where, currentSource.isSplitByKey(), false, false);
+        String queryTemplate = new QuerySqlBuilder(databaseInterface, currentSource.getTable(),metaColumns,splitKey,
+                where, currentSource.isSplitByKey(), false, false).buildSql();
         currentStatement = currentConn.createStatement(resultSetType, resultSetConcurrency);
 
         if (currentSource.isSplitByKey()){

File: flinkx-test/src/main/java/com/dtstack/flinkx/test/LocalTest.java
Patch:
@@ -92,7 +92,7 @@ public class LocalTest {
     public static final String TEST_RESOURCE_DIR = "flinkx-test/src/main/resources/dev_test_job/";
 
     public static void main(String[] args) throws Exception{
-        String jobPath = TEST_RESOURCE_DIR + "es_stream.json";
+        String jobPath = TEST_RESOURCE_DIR + "gbase_template.json";
         JobExecutionResult result = LocalTest.runJob(new File(jobPath), null, null);
         ResultPrintUtil.printResult(result);
     }

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsOutputFormat.java
Patch:
@@ -336,7 +336,9 @@ protected void afterCloseInternal()  {
             fs.createNewFile(new Path(finishedPath));
             LOG.info("Create finished tag dir:{}", finishedPath);
 
-            numWriteCounter.add(rowsOfCurrentBlock);
+            if(restoreConfig.isRestore()){
+                numWriteCounter.add(rowsOfCurrentBlock);
+            }
 
             if(taskNumber == 0) {
                 waitForAllTasksToFinish();

File: flinkx-hive/flinkx-hive-writer/src/main/java/com/dtstack/flinkx/hive/writer/HiveOutputFormat.java
Patch:
@@ -276,6 +276,7 @@ private HdfsOutputFormatBuilder getHdfsOutputFormatBuilder() {
         builder.setMaxFileSize(maxFileSize);
         builder.setFlushBlockInterval(interval);
         builder.setRestoreConfig(RestoreConfig.defaultConfig());
+        builder.setInitAccumulatorAndDirty(false);
 
         return builder;
     }

File: flinkx-hive/flinkx-hive-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsOutputFormat.java
Patch:
@@ -163,10 +163,10 @@ public void open(int taskNumber, int numTasks) throws IOException {
 
         initStatisticsAccumulator();
         initJobInfo();
-        initRestoreInfo();
 
         openInternal(taskNumber, numTasks);
 
+        initRestoreInfo();
     }
 
     @Override

File: flinkx-hive/flinkx-hive-writer/src/main/java/com/dtstack/flinkx/hive/writer/HiveOutputFormat.java
Patch:
@@ -229,7 +229,7 @@ private TableInfo checkCreateTable(String tablePath, Map event) throws Exception
                         logger.info("tablePath:{} even:{}", tablePath, event);
 
                         String tableName = tablePath;
-                        if (autoCreateTable) {
+                        if (autoCreateTable && event != null) {
                             tableName = MapUtils.getString(event, "table");
                             tableName = distributeTableMapping.getOrDefault(tableName, tableName);
                         }
@@ -275,6 +275,7 @@ private HdfsOutputFormatBuilder getHdfsOutputFormatBuilder() {
         builder.setRestoreConfig(restoreConfig);
         builder.setMaxFileSize(maxFileSize);
         builder.setFlushBlockInterval(interval);
+        builder.setRestoreConfig(RestoreConfig.defaultConfig());
 
         return builder;
     }

File: flinkx-hive/flinkx-hive-core/src/main/java/com/dtstack/flinkx/hive/TimePartitionFormat.java
Patch:
@@ -70,7 +70,7 @@ public static PartitionEnum getPartitionEnum() {
         return partitionEnum;
     }
 
-    enum PartitionEnum {
+    public enum PartitionEnum {
         DAY, HOUR, MINUTE
     }
 

File: flinkx-binlog/flinkx-binlog-core/src/main/java/com/dtstack/flinkx/binlog/BinlogConfigKeys.java
Patch:
@@ -39,4 +39,6 @@ public class BinlogConfigKeys {
 
     public final static String KEY_BUFFER_SIZE = "bufferSize";
 
+    public final static String KEY_PAVING_DATA = "pavingData";
+
 }

File: flinkx-hive/flinkx-hive-writer/src/main/java/com/dtstack/flinkx/hive/writer/HiveOutputFormat.java
Patch:
@@ -90,7 +90,7 @@ public class HiveOutputFormat extends RichOutputFormat {
      */
     protected Map<String, String> hadoopConfig;
 
-    protected String store;
+    protected String fileType;
 
     /**
      * 写入模式
@@ -373,7 +373,7 @@ private void closeDirtyDataManagerWriter(boolean reopen) {
     }
 
     private HdfsOutputFormatBuilder getHdfsOutputFormatBuilder() {
-        HdfsOutputFormatBuilder builder = new HdfsOutputFormatBuilder(store);
+        HdfsOutputFormatBuilder builder = new HdfsOutputFormatBuilder(fileType);
         builder.setHadoopConfig(hadoopConfig);
         builder.setDefaultFS(defaultFS);
         builder.setPath(path);

File: flinkx-hive/flinkx-hive-core/src/main/java/com/dtstack/flinkx/hive/ECompressType.java
Patch:
@@ -21,9 +21,7 @@
 import org.apache.commons.lang.StringUtils;
 
 /**
- * @author jiangbo
- * @explanation
- * @date 2019/4/3
+ * @author toutian
  */
 public enum ECompressType {
 

File: flinkx-hive/flinkx-hive-core/src/main/java/com/dtstack/flinkx/hive/EStoreType.java
Patch:
@@ -18,6 +18,9 @@
 
 package com.dtstack.flinkx.hive;
 
+/**
+ * @author toutian
+ */
 public enum EStoreType {
     TEXT, ORC
 }

File: flinkx-hive/flinkx-hive-core/src/main/java/com/dtstack/flinkx/hive/EWriteModeType.java
Patch:
@@ -18,7 +18,9 @@
 
 package com.dtstack.flinkx.hive;
 
-
+/**
+ * @author toutian
+ */
 public enum EWriteModeType {
     APPEND, OVERWRITE
 }

File: flinkx-hive/flinkx-hive-core/src/main/java/com/dtstack/flinkx/hive/TableInfo.java
Patch:
@@ -21,6 +21,9 @@
 import java.util.ArrayList;
 import java.util.List;
 
+/**
+ * @author toutian
+ */
 public class TableInfo {
 
     private String database;

File: flinkx-hive/flinkx-hive-core/src/main/java/com/dtstack/flinkx/hive/dirty/HiveDirtyDataManager.java
Patch:
@@ -37,6 +37,9 @@
 import java.util.Date;
 import java.util.Map;
 
+/**
+ * @author toutian
+ */
 public class HiveDirtyDataManager {
 
     private static Logger logger = LoggerFactory.getLogger(HiveDirtyDataManager.class);

File: flinkx-hive/flinkx-hive-core/src/main/java/com/dtstack/flinkx/hive/util/DBUtil.java
Patch:
@@ -38,6 +38,9 @@
 import java.util.regex.Matcher;
 import java.util.regex.Pattern;
 
+/**
+ * @author toutian
+ */
 public final class DBUtil {
 
     public static final String SQLSTATE_USERNAME_PWD_ERROR = "28000";

File: flinkx-hive/flinkx-hive-core/src/main/java/com/dtstack/flinkx/hive/util/DateUtil.java
Patch:
@@ -27,7 +27,9 @@
 import java.util.Map;
 import java.util.TimeZone;
 
-
+/**
+ * @author toutian
+ */
 public class DateUtil {
 
     private static final String TIME_ZONE = "GMT+8";

File: flinkx-hive/flinkx-hive-core/src/main/java/com/dtstack/flinkx/hive/util/HiveUtil.java
Patch:
@@ -37,8 +37,7 @@
 import static com.dtstack.flinkx.hive.EWriteModeType.*;
 
 /**
- * @author: haisi
- * @date 2019-07-02 17:47
+ * @author toutian
  */
 public class HiveUtil {
 

File: flinkx-hive/flinkx-hive-core/src/main/java/com/dtstack/flinkx/hive/util/PathConverterUtil.java
Patch:
@@ -26,10 +26,8 @@
 import java.util.regex.Pattern;
 
 /**
- * @author: haisi
- * @date 2019-06-18 14:44
+ * @author toutian
  */
-
 public class PathConverterUtil {
 
     private static Logger logger = LoggerFactory.getLogger(PathConverterUtil.class);

File: flinkx-hive/flinkx-hive-core/src/main/java/com/dtstack/flinkx/hive/util/RetryUtil.java
Patch:
@@ -28,6 +28,9 @@
 import java.util.concurrent.ThreadPoolExecutor;
 import java.util.concurrent.TimeUnit;
 
+/**
+ * @author toutian
+ */
 public final class RetryUtil {
 
     private static final Logger LOG = LoggerFactory.getLogger(RetryUtil.class);

File: flinkx-hive/flinkx-hive-core/src/main/java/org/apache/hadoop/mapred/TextOutputFormatBak.java
Patch:
@@ -36,6 +36,8 @@
 
 /** 
  * An {@link OutputFormat} that writes plain text files.
+ *
+ * @author toutian
  */
 @InterfaceAudience.Public
 @InterfaceStability.Stable

File: flinkx-hive/flinkx-hive-writer/src/main/java/com/dtstack/flinkx/hive/writer/HiveOutputFormat.java
Patch:
@@ -61,7 +61,7 @@
 import java.util.concurrent.locks.ReentrantLock;
 
 /**
- * @author sishu.yss
+ * @author toutian
  */
 public class HiveOutputFormat extends RichOutputFormat {
 

File: flinkx-hive/flinkx-hive-core/src/main/java/com/dtstack/flinkx/hive/HdfsConfigKeys.java
Patch:
@@ -29,7 +29,9 @@ public class HdfsConfigKeys {
 //    public static final String KEY_FIELD_DELIMITER = "fieldDelimiter";
     public static final String KEY_DELIMITER = "delimiter";
 
-//    public static final String KEY_DEFAULT_FS = "defaultFS";
+    public static final String KEY_DEFAULT_FS = "defaultFS";
+
+    public static final String KEY_FS_DEFAULT_FS = "fs.defaultFS";
 //
 //    public static final String KEY_PATH = "path";
 

File: flinkx-postgresql/flinkx-postgresql-writer/src/main/java/com/dtstack/flinkx/postgresql/writer/PostgresqlOutputFormat.java
Patch:
@@ -57,7 +57,7 @@ protected PreparedStatement prepareTemplates() throws SQLException {
 
     @Override
     protected void writeSingleRecordInternal(Row row) throws WriteRecordException {
-        if(!EWriteMode.INSERT.name().equalsIgnoreCase(mode)){
+        if(!checkIsCopyMode(insertSqlMode)){
             super.writeSingleRecordInternal(row);
             return;
         }
@@ -85,7 +85,7 @@ protected void writeSingleRecordInternal(Row row) throws WriteRecordException {
 
     @Override
     protected void writeMultipleRecordsInternal() throws Exception {
-        if(!EWriteMode.INSERT.name().equalsIgnoreCase(mode)){
+        if(!checkIsCopyMode(insertSqlMode)){
             super.writeMultipleRecordsInternal();
             return;
         }

File: flinkx-hbase/flinkx-hbase-reader/src/main/java/com/dtstack/flinkx/hbase/reader/HbaseInputFormat.java
Patch:
@@ -69,13 +69,13 @@ public class HbaseInputFormat extends RichInputFormat {
     private transient Table table;
     private transient ResultScanner resultScanner;
     private transient Result next;
-    private transient Map<String,String[]> nameMaps = Maps.newConcurrentMap();
+    private transient Map<String,String[]> nameMaps;
 
 
     @Override
     public void configure(Configuration configuration) {
         LOG.info("HbaseOutputFormat configure start");
-
+        nameMaps = Maps.newConcurrentMap();
         try {
             connection = ConnectionFactory.createConnection(getConfig());
         } catch (Exception e) {

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/datawriter/JdbcConfigKeys.java
Patch:
@@ -34,4 +34,5 @@ public class JdbcConfigKeys {
     public static final String KEY_BATCH_SIZE = "batchSize";
     public static final String KEY_UPDATE_KEY = "updateKey";
     public static final String KEY_FULL_COLUMN = "fullColumn";
+    public static final String KEY_INSERT_SQL_MODE = "insertSqlMode";
 }

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/outputformat/JdbcOutputFormat.java
Patch:
@@ -71,6 +71,9 @@ public class JdbcOutputFormat extends RichOutputFormat {
 
     protected String mode = EWriteMode.INSERT.name();
 
+    /**just for postgresql,use copy replace insert*/
+    protected String insertSqlMode;
+
     protected String table;
 
     protected List<String> column;

File: flinkx-core/src/main/java/com/dtstack/flinkx/constants/Metrics.java
Patch:
@@ -50,6 +50,8 @@ public class Metrics {
 
     public static String JOB_ID = "<job_id>";
 
+    public static String SUBTASK_INDEX = "<subtask_index>";
+
     public static String NUM_READS = "numRead";
 
     public static String END_LOCATION = "endLocation";

File: flinkx-core/src/main/java/com/dtstack/flinkx/inputformat/RichInputFormat.java
Patch:
@@ -223,7 +223,9 @@ public void close() throws IOException {
 
     @Override
     public void closeInputFormat() throws IOException {
-        updateDuration();
+        if(durationCounter != null){
+            updateDuration();
+        }
 
         if(inputMetric != null){
             inputMetric.waitForReportMetrics();

File: flinkx-core/src/main/java/com/dtstack/flinkx/outputformat/RichOutputFormat.java
Patch:
@@ -412,7 +412,9 @@ public void close() throws IOException {
                 writeRecordInternal();
             }
 
-            updateDuration();
+            if(durationCounter != null){
+                updateDuration();
+            }
 
             if(needWaitBeforeCloseInternal()) {
                 beforeCloseInternal();

File: flinkx-rdb/flinkx-rdb-reader/src/main/java/com.dtstack.flinkx.rdb.inputformat/JdbcInputFormat.java
Patch:
@@ -164,8 +164,9 @@ public void openInternal(InputSplit inputSplit) throws IOException {
             }
 
             dbConn = DBUtil.getConnection(dbURL, username, password);
-            Statement statement = dbConn.createStatement(resultSetType, resultSetConcurrency);
+            dbConn.setAutoCommit(false);
 
+            Statement statement = dbConn.createStatement(resultSetType, resultSetConcurrency);
             if(EDatabaseType.MySQL == databaseInterface.getDatabaseType()
                     || EDatabaseType.GBase == databaseInterface.getDatabaseType()){
                 statement.setFetchSize(Integer.MIN_VALUE);

File: flinkx-core/src/main/java/org/apache/flink/streaming/runtime/partitioner/DTRebalancePartitioner.java
Patch:
@@ -23,12 +23,12 @@
 import org.apache.flink.types.Row;
 
 /**
- * Rewrite the [RebalancePartitioner] to distribute data based on the channel specified in the data
+ * Rewrite the [DTRebalancePartitioner] to distribute data based on the channel specified in the data
  *
  * @param <T> Type of the elements in the Stream being rebalanced
  */
 @Internal
-public class RebalancePartitioner<T> extends StreamPartitioner<T> {
+public class DTRebalancePartitioner<T> extends StreamPartitioner<T> {
     private static final long serialVersionUID = 1L;
 
     private final int[] returnArray = new int[] {-1};

File: flinkx-rdb/flinkx-rdb-reader/src/main/java/com.dtstack.flinkx.rdb.inputformat/JdbcInputFormat.java
Patch:
@@ -164,7 +164,6 @@ public void openInternal(InputSplit inputSplit) throws IOException {
             }
 
             dbConn = DBUtil.getConnection(dbURL, username, password);
-            dbConn.setAutoCommit(false);
             Statement statement = dbConn.createStatement(resultSetType, resultSetConcurrency);
 
             if(EDatabaseType.MySQL == databaseInterface.getDatabaseType()

File: flinkx-core/src/main/java/com/dtstack/flinkx/Main.java
Patch:
@@ -139,7 +139,9 @@ public static void main(String[] args) throws Exception {
         }
 
         JobExecutionResult result = env.execute(jobIdString);
-        ResultPrintUtil.printResult(result);
+        if(StringUtils.isBlank(monitor)){
+            ResultPrintUtil.printResult(result);
+        }
     }
 
     private static Properties parseConf(String confStr) throws Exception{

File: flinkx-core/src/main/java/com/dtstack/flinkx/reader/MetaColumn.java
Patch:
@@ -157,14 +157,14 @@ public static List<String> getColumnNames(List columns){
         return columnNames;
     }
 
-    public static int getColumnIndex(List columns, String name){
+    public static MetaColumn getMetaColumn(List columns, String name){
         List<MetaColumn> metaColumns = getMetaColumns(columns);
         for (MetaColumn metaColumn : metaColumns) {
             if(StringUtils.isNotEmpty(metaColumn.getName()) && metaColumn.getName().equals(name)){
-                return metaColumn.getIndex();
+                return metaColumn;
             }
         }
 
-        return -1;
+        return null;
     }
 }

File: flinkx-rdb/flinkx-rdb-reader/src/main/java/com.dtstack.flinkx.rdb.inputformat/JdbcInputFormat.java
Patch:
@@ -489,7 +489,9 @@ private String getLocation(String columnType, Object columnVal){
         }
 
         if (ColumnType.isTimeType(columnType)){
-            if(columnVal instanceof Timestamp){
+            if(columnVal instanceof Long){
+                location = columnVal.toString();
+            } else if(columnVal instanceof Timestamp){
                 long time = ((Timestamp)columnVal).getTime() / 1000;
 
                 String nanosStr = String.valueOf(((Timestamp)columnVal).getNanos());

File: flinkx-core/src/main/java/com/dtstack/flinkx/inputformat/RichInputFormat.java
Patch:
@@ -77,18 +77,19 @@ public void openInputFormat() throws IOException {
         }
 
         initStatisticsAccumulator();
-        openByteRateLImiter();
+        openByteRateLimiter();
         initRestoreInfo();
     }
 
     @Override
     public void open(InputSplit inputSplit) throws IOException {
         indexOfSubtask = inputSplit.getSplitNumber();
 
+        formatState.setNumOfSubTask(indexOfSubtask);
         openInternal(inputSplit);
     }
 
-    private void openByteRateLImiter(){
+    private void openByteRateLimiter(){
         if (StringUtils.isNotBlank(this.monitorUrls) && this.bytes > 0) {
             this.byteRateLimiter = new ByteRateLimiter(getRuntimeContext(), this.monitorUrls, this.bytes, 2);
             this.byteRateLimiter.start();

File: flinkx-core/src/main/java/com/dtstack/flinkx/inputformat/RichInputFormat.java
Patch:
@@ -25,6 +25,7 @@
 import com.dtstack.flinkx.reader.ByteRateLimiter;
 import com.dtstack.flinkx.restore.FormatState;
 import com.dtstack.flinkx.util.LimitedQueue;
+import org.apache.commons.lang.StringUtils;
 import org.apache.flink.api.common.accumulators.LongCounter;
 import org.apache.flink.api.common.io.DefaultInputSplitAssigner;
 import org.apache.flink.api.common.io.statistics.BaseStatistics;
@@ -120,7 +121,7 @@ private void initStatisticsAccumulator(){
         bytesReadCounter = getRuntimeContext().getLongCounter(Metrics.READ_BYTES);
         durationCounter = getRuntimeContext().getLongCounter(Metrics.READ_DURATION);
 
-        inputMetric = new BaseMetric(getRuntimeContext(), "reader");
+        inputMetric = new BaseMetric(getRuntimeContext(), "reader", StringUtils.isEmpty(monitorUrls));
         inputMetric.addMetric(Metrics.NUM_READS, numReadCounter);
         inputMetric.addMetric(Metrics.READ_BYTES, bytesReadCounter);
         inputMetric.addMetric(Metrics.READ_DURATION, durationCounter);

File: flinkx-core/src/main/java/com/dtstack/flinkx/outputformat/RichOutputFormat.java
Patch:
@@ -248,7 +248,7 @@ private void initStatisticsAccumulator(){
         bytesWriteCounter = context.getLongCounter(Metrics.WRITE_BYTES);
         durationCounter = context.getLongCounter(Metrics.WRITE_DURATION);
 
-        outputMetric = new BaseMetric(context, "writer");
+        outputMetric = new BaseMetric(context, "writer", StringUtils.isEmpty(monitorUrl));
         outputMetric.addMetric(Metrics.NUM_ERRORS, errCounter);
         outputMetric.addMetric(Metrics.NUM_NULL_ERRORS, nullErrCounter);
         outputMetric.addMetric(Metrics.NUM_DUPLICATE_ERRORS, duplicateErrCounter);

File: flinkx-test/src/main/java/com/dtstack/flinkx/test/LocalTest.java
Patch:
@@ -53,6 +53,7 @@
 import com.dtstack.flinkx.sqlserver.writer.SqlserverWriter;
 import com.dtstack.flinkx.stream.reader.StreamReader;
 import com.dtstack.flinkx.stream.writer.StreamWriter;
+import com.dtstack.flinkx.util.ResultPrintUtil;
 import com.dtstack.flinkx.writer.DataWriter;
 import org.apache.commons.lang.StringUtils;
 import org.apache.flink.api.common.JobExecutionResult;
@@ -90,7 +91,8 @@ public class LocalTest {
 
     public static void main(String[] args) throws Exception{
         String jobPath = TEST_RESOURCE_DIR + "stream_template.json";
-        LocalTest.runJob(new File(jobPath), null, null);
+        JobExecutionResult result = LocalTest.runJob(new File(jobPath), null, null);
+        ResultPrintUtil.printResult(result);
     }
 
     public static JobExecutionResult runJob(File jobFile, Properties confProperties, String savepointPath) throws Exception{

File: flinkx-core/src/main/java/com/dtstack/flinkx/inputformat/RichInputFormat.java
Patch:
@@ -31,6 +31,7 @@
 import org.apache.flink.core.io.InputSplit;
 import org.apache.flink.core.io.InputSplitAssigner;
 import org.apache.flink.types.Row;
+import org.apache.lucene.util.RamUsageEstimator;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import java.io.IOException;
@@ -134,7 +135,7 @@ public Row nextRecord(Row row) throws IOException {
 
         Row internalRow = nextRecordInternal(row);
         internalRow = setChannelInformation(internalRow);
-        bytesReadCounter.add(internalRow.toString().length());
+        bytesReadCounter.add(RamUsageEstimator.sizeOf(internalRow)/8);
 
         return internalRow;
     }

File: flinkx-core/src/main/java/com/dtstack/flinkx/outputformat/RichOutputFormat.java
Patch:
@@ -39,6 +39,7 @@
 import org.apache.flink.hadoop.shaded.org.apache.http.impl.client.HttpClientBuilder;
 import org.apache.flink.streaming.api.operators.StreamingRuntimeContext;
 import org.apache.flink.types.Row;
+import org.apache.lucene.util.RamUsageEstimator;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import java.io.IOException;
@@ -371,7 +372,7 @@ public void writeRecord(Row row) throws IOException {
 
         updateDuration();
 
-        bytesWriteCounter.add(row.toString().length());
+        bytesWriteCounter.add(RamUsageEstimator.sizeOf(row)/8);
     }
 
     private Row setChannelInfo(Row row){

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsOrcOutputFormat.java
Patch:
@@ -134,9 +134,9 @@ protected void flushBlock() throws IOException{
     }
 
     @Override
-    protected float getCompressRate(){
+    protected float getDeviation(){
         ECompressType compressType = ECompressType.getByTypeAndFileType(compress, "orc");
-        return compressType.getCompressRate();
+        return compressType.getDeviation();
     }
 
     @Override

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsParquetOutputFormat.java
Patch:
@@ -142,9 +142,9 @@ protected void flushBlock() throws IOException{
     }
 
     @Override
-    protected float getCompressRate(){
+    protected float getDeviation(){
         ECompressType compressType = ECompressType.getByTypeAndFileType(compress, "parquet");
-        return compressType.getCompressRate();
+        return compressType.getDeviation();
     }
 
     @Override

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsOrcOutputFormat.java
Patch:
@@ -147,7 +147,9 @@ public void open() throws IOException {
     @Override
     public void writeSingleRecordInternal(Row row) throws WriteRecordException {
         if (restoreConfig.isRestore()){
-            nextBlock();
+            if(recordWriter == null){
+                nextBlock();
+            }
 
             if(lastRow != null){
                 readyCheckpoint = !ObjectUtils.equals(lastRow.getField(restoreConfig.getRestoreColumnIndex()),

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsParquetOutputFormat.java
Patch:
@@ -157,9 +157,10 @@ protected void open() throws IOException {
 
     @Override
     protected void writeSingleRecordInternal(Row row) throws WriteRecordException {
-
         if (restoreConfig.isRestore()){
-            nextBlock();
+            if(writer == null){
+                nextBlock();
+            }
 
             if(lastRow != null){
                 readyCheckpoint = !ObjectUtils.equals(lastRow.getField(restoreConfig.getRestoreColumnIndex()),

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsTextOutputFormat.java
Patch:
@@ -109,9 +109,10 @@ public void open() throws IOException {
 
     @Override
     public void writeSingleRecordInternal(Row row) throws WriteRecordException {
-
         if (restoreConfig.isRestore()){
-            nextBlock();
+            if(stream == null){
+                nextBlock();
+            }
 
             if(lastRow != null){
                 readyCheckpoint = !ObjectUtils.equals(lastRow.getField(restoreConfig.getRestoreColumnIndex()),

File: flinkx-core/src/main/java/com/dtstack/flinkx/inputformat/RichInputFormat.java
Patch:
@@ -135,7 +135,7 @@ public Row nextRecord(Row row) throws IOException {
         internalRow = setChannelInformation(internalRow);
         bytesReadCounter.add(ObjectSizeCalculator.getObjectSize(internalRow));
 
-        return nextRecordInternal(row);
+        return internalRow;
     }
 
     private Row setChannelInformation(Row internalRow){

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/RowUtil.java
Patch:
@@ -37,7 +37,6 @@ public class RowUtil {
 
     public static String rowToJson(Row row, String[] colName) {
         Preconditions.checkNotNull(colName);
-        Preconditions.checkArgument(row.getArity() == colName.length);
         Map<String,Object> map = new HashMap<>();
 
         for(int i = 0; i < colName.length; ++i) {

File: flinkx-core/src/main/java/com/dtstack/flinkx/outputformat/RichOutputFormat.java
Patch:
@@ -248,6 +248,8 @@ private void initStatisticsAccumulator(){
         outputMetric.addMetric(Metrics.NUM_WRITES, numWriteCounter);
         outputMetric.addMetric(Metrics.WRITE_BYTES, bytesWriteCounter);
         outputMetric.addMetric(Metrics.WRITE_DURATION, durationCounter);
+
+        startTime = System.currentTimeMillis();
     }
 
     private void openErrorLimiter(){

File: flinkx-core/src/main/java/com/dtstack/flinkx/outputformat/RichOutputFormat.java
Patch:
@@ -370,7 +370,7 @@ public void writeRecord(Row row) throws IOException {
 
         updateDuration();
 
-        writeBytesCounter.add(ObjectSizeCalculator.getObjectSize(row));
+        bytesWriteCounter.add(ObjectSizeCalculator.getObjectSize(row));
     }
 
     private Row setChannelInfo(Row row){

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsOutputFormat.java
Patch:
@@ -389,7 +389,7 @@ protected void checkWriteSize() {
             return;
         }
 
-        long writeSize = writeBytesCounter.getLocalValue();
+        long writeSize = bytesWriteCounter.getLocalValue();
         long currentFileSize = (long)getCompressRate() * (writeSize - lastWriteSize);
         if (currentFileSize > maxFileSize){
             try{

File: flinkx-kafka09/flinkx-kafka09-reader/src/main/java/com/dtstack/flinkx/kafka09/reader/Kafka09Reader.java
Patch:
@@ -35,7 +35,7 @@
  */
 public class Kafka09Reader extends DataReader {
 
-    private Map<String, Integer> topic;
+    private Map<String, Object> topic;
 
     private String codec;
 
@@ -46,7 +46,7 @@ public class Kafka09Reader extends DataReader {
     public Kafka09Reader(DataTransferConfig config, StreamExecutionEnvironment env) {
         super(config, env);
         ReaderConfig readerConfig = config.getJob().getContent().get(0).getReader();
-        topic = (Map<String, Integer>) readerConfig.getParameter().getVal(KEY_TOPIC);
+        topic = (Map<String, Object>) readerConfig.getParameter().getVal(KEY_TOPIC);
         codec = readerConfig.getParameter().getStringVal(KEY_CODEC, "plain");
         consumerSettings = (Map<String, String>) readerConfig.getParameter().getVal(KEY_CONSUMER_SETTINGS);
         encoding = readerConfig.getParameter().getStringVal(KEY_ENCODING, "utf-8");

File: flinkx-binlog/flinkx-binlog-reader/src/main/java/com/dtstack/flinkx/binlog/reader/BinlogEventSink.java
Patch:
@@ -28,7 +28,6 @@
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
-import java.util.concurrent.ArrayBlockingQueue;
 import java.util.concurrent.BlockingQueue;
 import java.util.concurrent.SynchronousQueue;
 
@@ -43,7 +42,7 @@ public class BinlogEventSink extends AbstractCanalLifeCycle implements com.aliba
 
     public BinlogEventSink(BinlogInputFormat format) {
         this.format = format;
-        queue = new SynchronousQueue<Row>(false);
+        queue = new SynchronousQueue<>(false);
     }
 
     @Override

File: flinkx-binlog/flinkx-binlog-reader/src/main/java/com/dtstack/flinkx/binlog/reader/BinlogReader.java
Patch:
@@ -64,7 +64,7 @@ public BinlogReader(DataTransferConfig config, StreamExecutionEnvironment env) {
         cat = readerConfig.getParameter().getStringVal(KEY_CATALOG);
         filter = readerConfig.getParameter().getStringVal(KEY_FILTER);
         period = readerConfig.getParameter().getLongVal(KEY_PERIOD, 1000L);
-        bufferSize = readerConfig.getParameter().getIntVal(KEY_BUFFER_SIZE, 1000);
+        bufferSize = readerConfig.getParameter().getIntVal(KEY_BUFFER_SIZE, 1024);
     }
 
     @Override

File: flinkx-kafka10/flinkx-kafka10-writer/src/main/java/com/dtstack/flinkx/kafka10/writer/Kafka10OutputFormat.java
Patch:
@@ -25,7 +25,7 @@ public class Kafka10OutputFormat extends RichOutputFormat {
 
     private static final Logger LOG = LoggerFactory.getLogger(Kafka10OutputFormat.class);
 
-    private static ObjectMapper objectMapper = new ObjectMapper();
+    private transient static ObjectMapper objectMapper = new ObjectMapper();
 
     private Properties props;
 
@@ -41,9 +41,9 @@ public class Kafka10OutputFormat extends RichOutputFormat {
 
     private Map<String, String> producerSettings;
 
-    private KafkaProducer producer;
+    private transient KafkaProducer producer;
 
-    private JsonDecoder jsonDecoder = new JsonDecoder();
+    private transient JsonDecoder jsonDecoder = new JsonDecoder();
 
     private AtomicBoolean isInit = new AtomicBoolean(false);
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/enums/ColumnType.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package com.dtstack.flinkx.common;
+package com.dtstack.flinkx.enums;
 
 import java.util.Arrays;
 import java.util.List;
@@ -29,7 +29,7 @@
  * @author huyifan.zju@163.com
  */
 public enum ColumnType {
-    STRING, VARCHAR, CHAR,NVARCHAR,TEXT,KEYWORD,BINARY,
+    STRING, VARCHAR,VARCHAR2, CHAR,NVARCHAR,TEXT,KEYWORD,BINARY,
     INT, MEDIUMINT, TINYINT, DATETIME, SMALLINT, BIGINT,LONG,SHORT,INTEGER,
     DOUBLE, FLOAT,
     BOOLEAN,

File: flinkx-hdfs/flinkx-hdfs-core/src/main/java/com/dtstack/flinkx/hdfs/HdfsUtil.java
Patch:
@@ -18,13 +18,11 @@
 
 package com.dtstack.flinkx.hdfs;
 
-import com.dtstack.flinkx.common.ColumnType;
+import com.dtstack.flinkx.enums.ColumnType;
 import com.dtstack.flinkx.util.DateUtil;
 import org.apache.commons.lang3.StringUtils;
-import org.apache.commons.lang3.math.NumberUtils;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hdfs.DistributedFileSystem;
-import org.apache.hadoop.hive.ql.io.HiveBinaryOutputFormat;
 import org.apache.hadoop.hive.serde2.io.DateWritable;
 import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;

File: flinkx-hdfs/flinkx-hdfs-reader/src/main/java/com/dtstack/flinkx/hdfs/reader/HdfsParquetInputFormat.java
Patch:
@@ -18,7 +18,7 @@
 
 package com.dtstack.flinkx.hdfs.reader;
 
-import com.dtstack.flinkx.common.ColumnType;
+import com.dtstack.flinkx.enums.ColumnType;
 import com.dtstack.flinkx.hdfs.HdfsUtil;
 import com.dtstack.flinkx.reader.MetaColumn;
 import com.google.common.collect.Lists;

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsOrcOutputFormat.java
Patch:
@@ -19,7 +19,7 @@
 
 package com.dtstack.flinkx.hdfs.writer;
 
-import com.dtstack.flinkx.common.ColumnType;
+import com.dtstack.flinkx.enums.ColumnType;
 import com.dtstack.flinkx.exception.WriteRecordException;
 import com.dtstack.flinkx.hdfs.HdfsUtil;
 import com.dtstack.flinkx.util.DateUtil;

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsParquetOutputFormat.java
Patch:
@@ -18,7 +18,7 @@
 
 package com.dtstack.flinkx.hdfs.writer;
 
-import com.dtstack.flinkx.common.ColumnType;
+import com.dtstack.flinkx.enums.ColumnType;
 import com.dtstack.flinkx.exception.WriteRecordException;
 import com.dtstack.flinkx.util.DateUtil;
 import org.apache.flink.types.Row;

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsTextOutputFormat.java
Patch:
@@ -18,7 +18,7 @@
 
 package com.dtstack.flinkx.hdfs.writer;
 
-import com.dtstack.flinkx.common.ColumnType;
+import com.dtstack.flinkx.enums.ColumnType;
 import com.dtstack.flinkx.exception.WriteRecordException;
 import com.dtstack.flinkx.util.DateUtil;
 import org.apache.commons.compress.compressors.bzip2.BZip2CompressorOutputStream;

File: flinkx-mongodb/flinkx-mongodb-core/src/main/java/com/dtstack/flinkx/mongodb/MongodbUtil.java
Patch:
@@ -18,7 +18,7 @@
 
 package com.dtstack.flinkx.mongodb;
 
-import com.dtstack.flinkx.enums.ColType;
+import com.dtstack.flinkx.enums.ColumnType;
 import com.dtstack.flinkx.exception.WriteRecordException;
 import com.dtstack.flinkx.reader.MetaColumn;
 import com.dtstack.flinkx.util.DateUtil;
@@ -135,7 +135,7 @@ private static Object convertField(Object val,MetaColumn column){
            val = ((BigDecimal) val).doubleValue();
         }
 
-        if (val instanceof Timestamp && !column.getType().equalsIgnoreCase(ColType.INTEGER.toString())){
+        if (val instanceof Timestamp && !column.getType().equalsIgnoreCase(ColumnType.INTEGER.name())){
             SimpleDateFormat format = DateUtil.getDateTimeFormatter();
             val= format.format(val);
         }

File: flinkx-odps/flinkx-odps-writer/src/main/java/com/dtstack/flinkx/odps/writer/OdpsOutputFormat.java
Patch:
@@ -25,7 +25,7 @@
 import com.aliyun.odps.tunnel.TableTunnel;
 import com.aliyun.odps.tunnel.TunnelException;
 import com.aliyun.odps.tunnel.io.TunnelBufferedWriter;
-import com.dtstack.flinkx.common.ColumnType;
+import com.dtstack.flinkx.enums.ColumnType;
 import com.dtstack.flinkx.exception.WriteRecordException;
 import com.dtstack.flinkx.odps.OdpsUtil;
 import com.dtstack.flinkx.outputformat.RichOutputFormat;

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/util/DBUtil.java
Patch:
@@ -17,7 +17,7 @@
  */
 package com.dtstack.flinkx.rdb.util;
 
-import com.dtstack.flinkx.common.ColumnType;
+import com.dtstack.flinkx.enums.ColumnType;
 import com.dtstack.flinkx.enums.EDatabaseType;
 import com.dtstack.flinkx.rdb.DatabaseInterface;
 import com.dtstack.flinkx.rdb.ParameterValuesProvider;

File: flinkx-test/src/main/java/com/dtstack/flinkx/test/LocalTest.java
Patch:
@@ -86,7 +86,7 @@ public class LocalTest {
 
     private static final int DELAY_INTERVAL = 10;
 
-    public static final String TEST_RESOURCE_DIR = "src/test/resources/dev_test_job/";
+    public static final String TEST_RESOURCE_DIR = "flinkx-test/src/main/resources/dev_test_job/";
 
     public static void main(String[] args) throws Exception{
         String jobPath = TEST_RESOURCE_DIR + "stream_template.json";

File: flinkx-kafka09/flinkx-kafka09-writer/src/main/java/com/dtstack/flinkx/kafka09/writer/Kafka09OutputFormat.java
Patch:
@@ -111,7 +111,6 @@ protected void writeSingleRecordInternal(Row row) throws WriteRecordException {
                 emit(jsonDecoder.decode(obj.toString()));
             }
         }
-
     }
 
     private void emit(Map<String, Object> event) {

File: flinkx-kafka09/flinkx-kafka09-reader/src/main/java/com/dtstack/flinkx/kafka09/reader/Kafka09Reader.java
Patch:
@@ -49,7 +49,7 @@ public Kafka09Reader(DataTransferConfig config, StreamExecutionEnvironment env)
         topic = (Map<String, Integer>) readerConfig.getParameter().getVal(KEY_TOPIC);
         codec = readerConfig.getParameter().getStringVal(KEY_CODEC, "plain");
         consumerSettings = (Map<String, String>) readerConfig.getParameter().getVal(KEY_CONSUMER_SETTINGS);
-        encoding = readerConfig.getParameter().getStringVal(KEY_ENCODING, "UTF8");
+        encoding = readerConfig.getParameter().getStringVal(KEY_ENCODING, "utf-8");
     }
 
     @Override

File: flinkx-binlog/flinkx-binlog-reader/src/main/java/com/dtstack/flinkx/binlog/reader/BinlogPositionManager.java
Patch:
@@ -26,7 +26,7 @@
 
 public class BinlogPositionManager extends AbstractLogPositionManager {
 
-    private static final Logger logger = LoggerFactory.getLogger(BinlogPositionManager.class);
+    private static final Logger LOG = LoggerFactory.getLogger(BinlogPositionManager.class);
 
     private final BinlogInputFormat format;
 
@@ -41,8 +41,8 @@ public LogPosition getLatestIndexBy(String destination) {
 
     @Override
     public void persistLogPosition(String destination, LogPosition logPosition) throws CanalParseException {
-        if(logger.isDebugEnabled()){
-            logger.debug("persistLogPosition: " + logPosition.toString());
+        if(LOG.isDebugEnabled()){
+            LOG.debug("persistLogPosition: " + logPosition.toString());
         }
         format.updateLastPos(logPosition.getPostion());
     }

File: flinkx-binlog/flinkx-binlog-reader/src/main/java/com/dtstack/flinkx/binlog/reader/BinlogReader.java
Patch:
@@ -79,6 +79,6 @@ public DataStream<Row> readData() {
         format.setStart(start);
         format.setFilter(filter);
         format.setBufferSize(bufferSize);
-        return createInput(format, "binlog");
+        return createInput(format, "binlogreader");
     }
 }

File: flinkx-ftp/flinkx-ftp-core/src/main/java/com/dtstack/flinkx/ftp/StandardFtpHandler.java
Patch:
@@ -278,7 +278,6 @@ public void deleteAllFilesInDir(String dir) {
     public InputStream getInputStream(String filePath) {
         try {
             InputStream is = ftpClient.retrieveFileStream(new String(filePath.getBytes(),FTP.DEFAULT_CONTROL_ENCODING));
-            ftpClient.getReply();
             return is;
         } catch (IOException e) {
             String message = String.format("读取文件 : [%s] 时出错,请确认文件：[%s]存在且配置的用户有权限读取", filePath, filePath);

File: flinkx-ftp/flinkx-ftp-core/src/main/java/com/dtstack/flinkx/ftp/StandardFtpHandler.java
Patch:
@@ -46,6 +46,9 @@ public class StandardFtpHandler implements FtpHandler {
 
     private FTPClient ftpClient = null;
 
+    public FTPClient getFtpClient() {
+        return ftpClient;
+    }
 
     @Override
     public void loginFtpServer(String host, String username, String password, int port, int timeout, String connectMode) {

File: flinkx-ftp/flinkx-ftp-core/src/main/java/com/dtstack/flinkx/ftp/StandardFtpHandler.java
Patch:
@@ -278,7 +278,6 @@ public void deleteAllFilesInDir(String dir) {
     public InputStream getInputStream(String filePath) {
         try {
             InputStream is = ftpClient.retrieveFileStream(new String(filePath.getBytes(),FTP.DEFAULT_CONTROL_ENCODING));
-            ftpClient.getReply();
             return is;
         } catch (IOException e) {
             String message = String.format("读取文件 : [%s] 时出错,请确认文件：[%s]存在且配置的用户有权限读取", filePath, filePath);

File: flinkx-ftp/flinkx-ftp-core/src/main/java/com/dtstack/flinkx/ftp/StandardFtpHandler.java
Patch:
@@ -46,6 +46,9 @@ public class StandardFtpHandler implements FtpHandler {
 
     private FTPClient ftpClient = null;
 
+    public FTPClient getFtpClient() {
+        return ftpClient;
+    }
 
     @Override
     public void loginFtpServer(String host, String username, String password, int port, int timeout, String connectMode) {

File: flinkx-ftp/flinkx-ftp-core/src/main/java/com/dtstack/flinkx/ftp/StandardFtpHandler.java
Patch:
@@ -46,6 +46,9 @@ public class StandardFtpHandler implements FtpHandler {
 
     private FTPClient ftpClient = null;
 
+    public FTPClient getFtpClient() {
+        return ftpClient;
+    }
 
     @Override
     public void loginFtpServer(String host, String username, String password, int port, int timeout, String connectMode) {

File: flinkx-hdfs/flinkx-hdfs-reader/src/main/java/com/dtstack/flinkx/hdfs/reader/HdfsParquetInputFormat.java
Patch:
@@ -281,7 +281,7 @@ private List<String> getAllPartitionPath(String tableLocation) throws IOExceptio
                 if(status.isDirectory()){
                     pathList.addAll(getAllPartitionPath(status.getPath().toString()));
                 } else {
-                    pathList.add(tableLocation);
+                    pathList.add(status.getPath().toString());
                 }
             }
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/inputformat/RichInputFormat.java
Patch:
@@ -94,7 +94,9 @@ public void close() throws IOException {
         try{
             closeInternal();
 
-            inputMetric.waitForReportMetrics();
+            if(inputMetric != null){
+                inputMetric.waitForReportMetrics();
+            }
         }catch (Exception e){
             throw new RuntimeException(e);
         }finally {

File: flinkx-core/src/main/java/com/dtstack/flinkx/outputformat/RichOutputFormat.java
Patch:
@@ -320,7 +320,9 @@ public void close() throws IOException {
                 writeRecordInternal();
             }
 
-            outputMetric.waitForReportMetrics();
+            if(outputMetric != null){
+                outputMetric.waitForReportMetrics();
+            }
 
             if(needWaitBeforeCloseInternal()) {
                 Latch latch = newLatch("#3");

File: flinkx-core/src/main/java/com/dtstack/flinkx/metrics/BaseMetric.java
Patch:
@@ -80,6 +80,8 @@ public void waitForReportMetrics(){
 
         try {
             Thread.sleep(delayPeriodMill);
+            LOG.info("Wait [{}] millisecond for [{}]", delayPeriodMill, sourceName);
+
             totalWaitMill += delayPeriodMill;
         } catch (InterruptedException e){
             SysUtil.sleep(delayPeriodMill);

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/outputformat/JdbcOutputFormat.java
Patch:
@@ -400,6 +400,8 @@ public void closeInternal() {
             LOG.error("Get task status error:{}", e.getMessage());
         }
 
+        numWriteCounter.add(rowsOfCurrentTransaction);
+
         DBUtil.closeDBResources(null, preparedStatement, dbConn, commit);
         dbConn = null;
 

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsOrcOutputFormat.java
Patch:
@@ -260,7 +260,9 @@ public void closeInternal() throws IOException {
             rw.close(Reporter.NULL);
             this.recordWriter = null;
 
-            moveTemporaryDataBlockFileToDirectory();
+            if(isTaskEndsNormally()){
+                moveTemporaryDataBlockFileToDirectory();
+            }
         }
     }
 

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsTextOutputFormat.java
Patch:
@@ -239,7 +239,9 @@ public void closeInternal() throws IOException {
             this.stream = null;
             s.close();
 
-            moveTemporaryDataBlockFileToDirectory();
+            if(isTaskEndsNormally()){
+                moveTemporaryDataBlockFileToDirectory();
+            }
         }
     }
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/outputformat/RichOutputFormat.java
Patch:
@@ -417,7 +417,7 @@ public void tryCleanupOnError() throws Exception {
 
     protected String getTaskState() throws IOException{
         if (StringUtils.isEmpty(monitorUrl)) {
-            return null;
+            return RUNNING_STATE;
         }
 
         String taskState;

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsOutputFormat.java
Patch:
@@ -315,7 +315,7 @@ protected void afterCloseInternal()  {
                 }
 
                 List<FileStatus> dataFiles = new ArrayList<>();
-                Path tmpDir = new Path(outputFilePath + SP + DATA_SUBDIR + SP + jobId);
+                Path tmpDir = new Path(outputFilePath + SP + DATA_SUBDIR);
                 FileStatus[] historyTmpDataDir = fs.listStatus(tmpDir);
                 for (FileStatus fileStatus : historyTmpDataDir) {
                     if (fileStatus.isDirectory()){

File: flinkx-hdfs/flinkx-hdfs-core/src/main/java/com/dtstack/flinkx/hdfs/HdfsUtil.java
Patch:
@@ -170,6 +170,8 @@ public static Object getWritableValue(Object writable) {
             ret = ((ByteWritable) writable).get();
         } else if(clz == DateWritable.class) {
             ret = ((DateWritable) writable).get();
+        } else if(writable instanceof DoubleWritable){
+            ret = ((DoubleWritable) writable).get();
         } else if(writable instanceof Writable) {
             ret = writable.toString();
         } else {

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/datareader/JdbcDataReader.java
Patch:
@@ -142,7 +142,7 @@ private void buildIncrementConfig(ReaderConfig readerConfig){
         int requestAccumulatorInterval = readerConfig.getParameter().getIntVal(JdbcConfigKeys.KEY_REQUEST_ACCUMULATOR_INTERVAL, 2);
 
         incrementConfig = new IncrementConfig();
-        if (incrementColumn != null){
+        if (incrementColumn != null && StringUtils.isNotEmpty(incrementColumn.toString())){
             String type = null;
             String name = null;
             int index = -1;

File: flinkx-ftp/flinkx-ftp-core/src/main/java/com/dtstack/flinkx/ftp/FtpHandler.java
Patch:
@@ -285,7 +285,6 @@ public void deleteAllFilesInDir(String dir, List<String> exclude) {
     public InputStream getInputStream(String filePath) {
         try {
             InputStream is = ftpClient.retrieveFileStream(new String(filePath.getBytes(),FTP.DEFAULT_CONTROL_ENCODING));
-            ftpClient.getReply();
             return is;
         } catch (IOException e) {
             String message = String.format("读取文件 : [%s] 时出错,请确认文件：[%s]存在且配置的用户有权限读取", filePath, filePath);

File: flinkx-ftp/flinkx-ftp-core/src/main/java/com/dtstack/flinkx/ftp/StandardFtpHandler.java
Patch:
@@ -275,7 +275,6 @@ public void deleteAllFilesInDir(String dir) {
     public InputStream getInputStream(String filePath) {
         try {
             InputStream is = ftpClient.retrieveFileStream(new String(filePath.getBytes(),FTP.DEFAULT_CONTROL_ENCODING));
-            ftpClient.getReply();
             return is;
         } catch (IOException e) {
             String message = String.format("读取文件 : [%s] 时出错,请确认文件：[%s]存在且配置的用户有权限读取", filePath, filePath);

File: flinkx-ftp/flinkx-ftp-core/src/main/java/com/dtstack/flinkx/ftp/StandardFtpHandler.java
Patch:
@@ -275,7 +275,6 @@ public void deleteAllFilesInDir(String dir) {
     public InputStream getInputStream(String filePath) {
         try {
             InputStream is = ftpClient.retrieveFileStream(new String(filePath.getBytes(),FTP.DEFAULT_CONTROL_ENCODING));
-            ftpClient.getReply();
             return is;
         } catch (IOException e) {
             String message = String.format("读取文件 : [%s] 时出错,请确认文件：[%s]存在且配置的用户有权限读取", filePath, filePath);

File: flinkx-core/src/main/java/com/dtstack/flink/client/MyLocalExecutor.java
Patch:
@@ -201,7 +201,7 @@ public JobExecutionResult executePlan(Plan plan) throws Exception {
                 JobGraphGenerator jgg = new JobGraphGenerator(configuration);
                 JobGraph jobGraph = jgg.compileJobGraph(op, plan.getJobId());
                 jobGraph.setClasspaths(classpaths);
-                System.out.println("jobGraph.classpaths=" + jobGraph.getClasspaths());
+
                 boolean sysoutPrint = isPrintingStatusDuringExecution();
                 return flink.submitJobAndWait(jobGraph, sysoutPrint);
             }

File: flinkx-core/src/main/java/com/dtstack/flinkx/Main.java
Patch:
@@ -175,7 +175,6 @@ private static void openCheckpointConf(StreamExecutionEnvironment env, Propertie
         }
 
         env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);
-        env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);
         env.getCheckpointConfig().enableExternalizedCheckpoints(
                 CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/RangeSplitUtil.java
Patch:
@@ -180,9 +180,6 @@ private static String bigIntegerToString(BigInteger bigInteger, int radix) {
             map.put(i, (char) (i));
         }
 
-//        String msg = String.format("%s 转为 %s 进制，结果为：%s", bigInteger.longValue(), radix, list);
-//        System.out.println(msg);
-
         for (Integer aList : list) {
             resultStringBuilder.append(map.get(aList));
         }

File: flinkx-core/src/main/java/org/apache/flink/streaming/api/functions/sink/DtOutputFormatSinkFunction.java
Patch:
@@ -154,11 +154,10 @@ public void initializeState(FunctionInitializationContext context) throws Except
 
         LOG.info("Is restored:{}", context.isRestored());
         if (context.isRestored()){
-            LOG.info("Output format state into:");
             formatStateMap = new HashMap<>();
             for (FormatState formatState : unionOffsetStates.get()) {
                 formatStateMap.put(formatState.getNumOfSubTask(), formatState);
-                LOG.info(formatState.toString());
+                LOG.info("Output format state into:{}" ,formatState.toString());
             }
         }
 

File: flinkx-ftp/flinkx-ftp-reader/src/main/java/com/dtstack/flinkx/ftp/reader/FtpSeqBufferedReader.java
Patch:
@@ -18,7 +18,7 @@
 
 package com.dtstack.flinkx.ftp.reader;
 
-import com.dtstack.flinkx.ftp.FtpHandler;
+import com.dtstack.flinkx.ftp.IFtpHandler;
 
 import java.io.*;
 import java.util.Iterator;
@@ -31,7 +31,7 @@
  */
 public class FtpSeqBufferedReader {
 
-    private FtpHandler ftpHandler;
+    private IFtpHandler ftpHandler;
 
     private Iterator<String> iter;
 
@@ -41,7 +41,7 @@ public class FtpSeqBufferedReader {
 
     private String charsetName = "utf-8";
 
-    public FtpSeqBufferedReader(FtpHandler ftpHandler, Iterator<String> iter) {
+    public FtpSeqBufferedReader(IFtpHandler ftpHandler, Iterator<String> iter) {
         this.ftpHandler = ftpHandler;
         this.iter = iter;
     }

File: flinkx-ftp/flinkx-ftp-reader/src/main/java/com/dtstack/flinkx/ftp/reader/FtpSeqInputStream.java
Patch:
@@ -18,7 +18,7 @@
 
 package com.dtstack.flinkx.ftp.reader;
 
-import com.dtstack.flinkx.ftp.FtpHandler;
+import com.dtstack.flinkx.ftp.IFtpHandler;
 import java.io.IOException;
 import java.io.InputStream;
 import java.util.Iterator;
@@ -31,11 +31,11 @@
  * @author huyifan.zju@163.com
  */
 public class FtpSeqInputStream extends InputStream {
-    private FtpHandler ftpHandler;
+    private IFtpHandler ftpHandler;
     private Iterator<String> iter;
     private InputStream in;
 
-    public FtpSeqInputStream(FtpHandler ftpHandler, List<String> files) {
+    public FtpSeqInputStream(IFtpHandler ftpHandler, List<String> files) {
         this.ftpHandler = ftpHandler;
         this.iter = files.iterator();
         try {

File: flinkx-ftp/flinkx-ftp-reader/src/test/java/com/dtstack/flinkx/ftp/reader/FtpTest.java
Patch:
@@ -1,8 +1,8 @@
 package com.dtstack.flinkx.ftp.reader;
 
 import com.dtstack.flinkx.ftp.FtpConfigConstants;
+import com.dtstack.flinkx.ftp.IFtpHandler;
 import com.dtstack.flinkx.ftp.FtpHandler;
-import com.dtstack.flinkx.ftp.StandardFtpHandler;
 
 import java.io.*;
 import java.util.*;
@@ -11,7 +11,7 @@ public class FtpTest {
 
     public static void main(String[] args) throws IOException {
 
-        FtpHandler ftpHandler = new StandardFtpHandler();
+        IFtpHandler ftpHandler = new FtpHandler();
         ftpHandler.loginFtpServer("node02",
                 "test",
                 "qbI#5pNd",

File: flinkx-ftp/flinkx-ftp-reader/src/test/java/com/dtstack/flinkx/ftp/reader/SftpTest.java
Patch:
@@ -1,7 +1,7 @@
 package com.dtstack.flinkx.ftp.reader;
 
 import com.dtstack.flinkx.ftp.FtpConfigConstants;
-import com.dtstack.flinkx.ftp.FtpHandler;
+import com.dtstack.flinkx.ftp.IFtpHandler;
 import com.dtstack.flinkx.ftp.SFtpHandler;
 
 import java.io.BufferedReader;
@@ -21,7 +21,7 @@ public static void main(String[] args) throws IOException {
         String username = "mysftp";
         String password = "oh1986mygod";
 
-        FtpHandler ftpHandler = new SFtpHandler();
+        IFtpHandler ftpHandler = new SFtpHandler();
         ftpHandler.loginFtpServer(host,username,password, FtpConfigConstants.DEFAULT_SFTP_PORT,
                 0,
                 FtpConfigConstants.DEFAULT_FTP_CONNECT_PATTERN);

File: flinkx-ftp/flinkx-ftp-writer/src/main/java/com/dtstack/flinkx/ftp/writer/FtpWriter.java
Patch:
@@ -111,6 +111,7 @@ public DataStreamSink<?> writeData(DataStream<Row> dataSet) {
         builder.setDirtyPath(dirtyPath);
         builder.setDirtyHadoopConfig(dirtyHadoopConfig);
         builder.setSrcCols(srcCols);
+        builder.setRestoreConfig(restoreConfig);
 
         DtOutputFormatSinkFunction sinkFunction = new DtOutputFormatSinkFunction(builder.finish());
         DataStreamSink<?> dataStreamSink = dataSet.addSink(sinkFunction);

File: flinkx-odps/flinkx-odps-writer/src/main/java/com/dtstack/flinkx/odps/writer/OdpsWriter.java
Patch:
@@ -95,6 +95,7 @@ public DataStreamSink<?> writeData(DataStream<Row> dataSet) {
         builder.setErrorRatio(errorRatio);
         builder.setErrors(errors);
         builder.setBufferSize(bufferSize);
+        builder.setRestoreConfig(restoreConfig);
 
         DtOutputFormatSinkFunction sinkFunction = new DtOutputFormatSinkFunction(builder.finish());
         DataStreamSink<?> dataStreamSink = dataSet.addSink(sinkFunction);

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/datareader/JdbcDataReader.java
Patch:
@@ -118,7 +118,6 @@ public DataStream<Row> readData() {
         builder.setCustomSql(customSql);
         builder.setRestoreConfig(restoreConfig);
         builder.setHadoopConfig(hadoopConfig);
-        builder.setExceptionIndex(exceptionIndex);
 
         boolean isSplitByKey = numPartitions > 1 && StringUtils.isNotEmpty(splitKey);
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/outputformat/RichOutputFormat.java
Patch:
@@ -414,7 +414,7 @@ public String getTaskState() throws IOException{
                 LOG.info("response:{}", response);
 
                 JsonObject obj = parser.parse(response).getAsJsonObject();
-                taskState = obj.getAsJsonObject("state").getAsString();
+                taskState = obj.get("state").getAsString();
                 LOG.info("Job state is:{}", taskState);
 
                 if(taskState != null){
@@ -423,7 +423,7 @@ public String getTaskState() throws IOException{
 
                 Thread.sleep(500);
             }catch (Exception e){
-                LOG.info("Get job state error:", e.getCause());
+                LOG.info("Get job state error:", e.getMessage());
             }
         }
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/reader/MetaColumn.java
Patch:
@@ -109,6 +109,8 @@ public static List<MetaColumn> getMetaColumns(List columns){
                             Double doubleColIndex = (Double) colIndex;
                             mc.setIndex(doubleColIndex.intValue());
                         }
+                    } else {
+                        mc.setIndex(i);
                     }
 
                     mc.setName(sm.get("name") != null ? String.valueOf(sm.get("name")) : null);

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/datareader/JdbcDataReader.java
Patch:
@@ -136,9 +136,9 @@ public DataStream<Row> readData() {
 
         String query;
         if (StringUtils.isNotEmpty(customSql)){
-            query = DBUtil.buildQuerySqlWithCustomSql(databaseInterface, customSql, isSplitByKey, splitKey, useMaxFunc);
+            query = DBUtil.buildQuerySqlWithCustomSql(databaseInterface, customSql, isSplitByKey, splitKey, StringUtils.isNotEmpty(increColumn));
         } else {
-            query = DBUtil.getQuerySql(databaseInterface, table, metaColumns, splitKey, where, isSplitByKey, useMaxFunc);
+            query = DBUtil.getQuerySql(databaseInterface, table, metaColumns, splitKey, where, isSplitByKey, StringUtils.isNotEmpty(increColumn));
         }
         builder.setQuery(query);
 

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/inputformat/JdbcInputFormat.java
Patch:
@@ -406,7 +406,7 @@ private String buildQuerySql(InputSplit inputSplit){
                         .replace("${M}", String.valueOf(jdbcInputSplit.getMod()));
             }
 
-            if (useMaxFunc){
+            if (StringUtils.isNotEmpty(increCol)){
                 String incrementFilter = DBUtil.buildIncrementFilter(databaseInterface, increColType, increCol,
                         jdbcInputSplit.getStartLocation(), jdbcInputSplit.getEndLocation(), customSql);
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/outputformat/RichOutputFormat.java
Patch:
@@ -222,7 +222,9 @@ public void open(int taskNumber, int numTasks) throws IOException {
             latch.waitUntil(numTasks);
         }
 
-        if(restoreConfig.isRestore()){
+        if(restoreConfig == null){
+            restoreConfig = RestoreConfig.defaultConfig();
+        } else if(restoreConfig.isRestore()){
             if(formatState == null){
                 formatState = new FormatState(taskNumber, null);
             } else {

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/inputformat/JdbcInputFormat.java
Patch:
@@ -496,7 +496,7 @@ private String getLocation(String columnType, Object columnVal){
                 long time = date.getTime();
                 location = time + fillZeroStr;
             }
-        } else if(ColumnType.isNumberType(incrementConfig.getColumnType())){
+        } else if(ColumnType.isNumberType(columnType)){
             location = String.valueOf(columnVal);
         } else {
             location = String.valueOf(columnVal);

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/StringUtil.java
Patch:
@@ -167,7 +167,7 @@ public static String col2string(Object column, String type) {
                 result = Boolean.valueOf(rowData.trim());
                 break;
             case DATE:
-                result = DateUtil.dateToString(DateUtil.columnToDate(column, null));
+                result = DateUtil.timestampToString(DateUtil.columnToDate(column, null));
                 break;
             case TIMESTAMP:
                 result = DateUtil.timestampToString(DateUtil.columnToTimestamp(column, null));

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/StringUtil.java
Patch:
@@ -167,7 +167,7 @@ public static String col2string(Object column, String type) {
                 result = Boolean.valueOf(rowData.trim());
                 break;
             case DATE:
-                result = DateUtil.dateToString(DateUtil.columnToDate(column, null));
+                result = DateUtil.timestampToString(DateUtil.columnToDate(column, null));
                 break;
             case TIMESTAMP:
                 result = DateUtil.timestampToString(DateUtil.columnToTimestamp(column, null));

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/StringUtil.java
Patch:
@@ -167,10 +167,10 @@ public static String col2string(Object column, String type) {
                 result = Boolean.valueOf(rowData.trim());
                 break;
             case DATE:
-                result = DateUtil.dateToString((java.util.Date)column);
+                result = DateUtil.dateToString(DateUtil.columnToDate(column, null));
                 break;
             case TIMESTAMP:
-                result = DateUtil.timestampToString((java.util.Date)column);
+                result = DateUtil.timestampToString(DateUtil.columnToTimestamp(column, null));
                 break;
             default:
                 result = rowData;

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/StringUtil.java
Patch:
@@ -167,10 +167,10 @@ public static String col2string(Object column, String type) {
                 result = Boolean.valueOf(rowData.trim());
                 break;
             case DATE:
-                result = DateUtil.dateToString((java.util.Date)column);
+                result = DateUtil.dateToString(DateUtil.columnToDate(column, null));
                 break;
             case TIMESTAMP:
-                result = DateUtil.timestampToString((java.util.Date)column);
+                result = DateUtil.timestampToString(DateUtil.columnToTimestamp(column, null));
                 break;
             default:
                 result = rowData;

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/datareader/JdbcDataReader.java
Patch:
@@ -138,9 +138,9 @@ public DataStream<Row> readData() {
 
         String query;
         if (StringUtils.isNotEmpty(customSql)){
-            query = DBUtil.buildQuerySqlWithCustomSql(databaseInterface, customSql, isSplitByKey, splitKey, useMaxFunc);
+            query = DBUtil.buildQuerySqlWithCustomSql(databaseInterface, customSql, isSplitByKey, splitKey, StringUtils.isNotEmpty(increColumn));
         } else {
-            query = DBUtil.getQuerySql(databaseInterface, table, metaColumns, splitKey, where, isSplitByKey, useMaxFunc);
+            query = DBUtil.getQuerySql(databaseInterface, table, metaColumns, splitKey, where, isSplitByKey, StringUtils.isNotEmpty(increColumn));
         }
         builder.setQuery(query);
 

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/inputformat/JdbcInputFormat.java
Patch:
@@ -420,7 +420,7 @@ private String buildQuerySql(InputSplit inputSplit){
                         .replace("${M}", String.valueOf(jdbcInputSplit.getMod()));
             }
 
-            if (useMaxFunc){
+            if (StringUtils.isNotEmpty(increCol)){
                 String incrementFilter = DBUtil.buildIncrementFilter(databaseInterface, increColType, increCol,
                         jdbcInputSplit.getStartLocation(), jdbcInputSplit.getEndLocation(), customSql);
 

File: flinkx-carbondata/flinkx-carbondata-reader/src/main/java/com/dtstack/flinkx/carbondata/reader/CarbondataInputFormatBuilder.java
Patch:
@@ -85,7 +85,7 @@ protected void checkFormat() {
         Preconditions.checkArgument(format.columnName.size() == format.columnType.size());
         Preconditions.checkArgument(format.columnName.size() == format.columnValue.size());
 
-        if (format.getRestoreConfig().isRestore()){
+        if (format.getRestoreConfig() != null && format.getRestoreConfig().isRestore()){
             throw new UnsupportedOperationException("This plugin not support restore from failed state");
         }
     }

File: flinkx-carbondata/flinkx-carbondata-writer/src/main/java/com/dtstack/flinkx/carbondata/writer/CarbondataOutputFormatBuilder.java
Patch:
@@ -85,7 +85,7 @@ protected void checkFormat() {
         Preconditions.checkNotNull(format.database);
         Preconditions.checkNotNull(format.column);
 
-        if (format.getRestoreConfig().isRestore()){
+        if (format.getRestoreConfig() != null && format.getRestoreConfig().isRestore()){
             throw new UnsupportedOperationException("This plugin not support restore from failed state");
         }
     }

File: flinkx-es/flinkx-es-reader/src/main/java/com/dtstack/flinkx/es/reader/EsInputFormatBuilder.java
Patch:
@@ -90,7 +90,7 @@ public EsInputFormatBuilder setClientConfig(Map<String, Object> clientConfig){
 
     @Override
     protected void checkFormat() {
-        if (format.getRestoreConfig().isRestore()){
+        if (format.getRestoreConfig() != null && format.getRestoreConfig().isRestore()){
             throw new UnsupportedOperationException("This plugin not support restore from failed state");
         }
     }

File: flinkx-es/flinkx-es-writer/src/main/java/com/dtstack/flinkx/es/writer/EsOutputFormatBuilder.java
Patch:
@@ -75,7 +75,7 @@ public EsOutputFormatBuilder setClientConfig(Map<String, Object> clientConfig){
 
     @Override
     protected void checkFormat() {
-        if (format.getRestoreConfig().isRestore()){
+        if (format.getRestoreConfig() != null && format.getRestoreConfig().isRestore()){
             throw new UnsupportedOperationException("This plugin not support restore from failed state");
         }
     }

File: flinkx-ftp/flinkx-ftp-reader/src/main/java/com/dtstack/flinkx/ftp/reader/FtpInputFormatBuilder.java
Patch:
@@ -71,7 +71,7 @@ public void setIsFirstLineHeader(boolean isFirstLineHeader){
 
     @Override
     protected void checkFormat() {
-        if (format.getRestoreConfig().isRestore()){
+        if (format.getRestoreConfig() != null && format.getRestoreConfig().isRestore()){
             throw new UnsupportedOperationException("This plugin not support restore from failed state");
         }
     }

File: flinkx-hbase/flinkx-hbase-reader/src/main/java/com/dtstack/flinkx/hbase/reader/HbaseInputFormatBuilder.java
Patch:
@@ -105,7 +105,7 @@ protected void checkFormat() {
                     || StringUtils.isNotEmpty(format.columnTypes.get(i)) );
         }
 
-        if (format.getRestoreConfig().isRestore()){
+        if (format.getRestoreConfig() != null && format.getRestoreConfig().isRestore()){
             throw new UnsupportedOperationException("This plugin not support restore from failed state");
         }
     }

File: flinkx-hbase/flinkx-hbase-writer/src/main/java/com/dtstack/flinkx/hbase/writer/HbaseOutputFormatBuilder.java
Patch:
@@ -117,7 +117,7 @@ protected void checkFormat() {
         Preconditions.checkNotNull(format.rowkeyColumnTypes);
         Preconditions.checkNotNull(format.rowkeyColumnValues);
 
-        if (format.getRestoreConfig().isRestore()){
+        if (format.getRestoreConfig() != null && format.getRestoreConfig().isRestore()){
             throw new UnsupportedOperationException("This plugin not support restore from failed state");
         }
     }

File: flinkx-hdfs/flinkx-hdfs-reader/src/main/java/com/dtstack/flinkx/hdfs/reader/HdfsInputFormatBuilder.java
Patch:
@@ -73,7 +73,7 @@ public void setDefaultFs(String defaultFs) {
 
     @Override
     protected void checkFormat() {
-        if (format.getRestoreConfig().isRestore()){
+        if (format.getRestoreConfig() != null && format.getRestoreConfig().isRestore()){
             throw new UnsupportedOperationException("This plugin not support restore from failed state");
         }
     }

File: flinkx-mongodb/flinkx-mongodb-reader/src/main/java/com/dtstack/flinkx/mongodb/reader/MongodbInputFormatBuilder.java
Patch:
@@ -88,7 +88,7 @@ protected void checkFormat() {
             throw new IllegalArgumentException("No collection supplied");
         }
 
-        if (format.getRestoreConfig().isRestore()){
+        if (format.getRestoreConfig() != null && format.getRestoreConfig().isRestore()){
             throw new UnsupportedOperationException("This plugin not support restore from failed state");
         }
     }

File: flinkx-mongodb/flinkx-mongodb-writer/src/main/java/com/dtstack/flinkx/mongodb/writer/MongodbOutputFormatBuilder.java
Patch:
@@ -90,7 +90,7 @@ protected void checkFormat() {
             throw new IllegalArgumentException("No collection supplied");
         }
 
-        if (format.getRestoreConfig().isRestore()){
+        if (format.getRestoreConfig() != null && format.getRestoreConfig().isRestore()){
             throw new UnsupportedOperationException("This plugin not support restore from failed state");
         }
     }

File: flinkx-odps/flinkx-odps-reader/src/main/java/com/dtstack/flinkx/odps/reader/OdpsInputFormatBuilder.java
Patch:
@@ -58,7 +58,7 @@ public void setPartition(String partition) {
 
     @Override
     protected void checkFormat() {
-        if (format.getRestoreConfig().isRestore()){
+        if (format.getRestoreConfig() != null && format.getRestoreConfig().isRestore()){
             throw new UnsupportedOperationException("This plugin not support restore from failed state");
         }
     }

File: flinkx-redis/flinkx-redis-writer/src/main/java/com/dtstack/flinkx/redis/writer/RedisOutputFormatBuilder.java
Patch:
@@ -92,7 +92,7 @@ protected void checkFormat() {
             throw new IllegalArgumentException("Field keyIndexes cannot be empty");
         }
 
-        if (format.getRestoreConfig().isRestore()){
+        if (format.getRestoreConfig() != null && format.getRestoreConfig().isRestore()){
             throw new UnsupportedOperationException("This plugin not support restore from failed state");
         }
     }

File: flinkx-core/src/main/java/com/dtstack/flinkx/reader/MetaColumn.java
Patch:
@@ -154,9 +154,9 @@ public static List<String> getColumnNames(List columns){
     }
 
     public static int getColumnIndex(List<MetaColumn> columns, String name){
-        for (MetaColumn column : columns) {
-            if (column.name.equals(name)){
-                return column.index;
+        for (int i = 0; i < columns.size(); i++) {
+            if(columns.get(i).getName().equals(name)){
+                return i;
             }
         }
 

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/datareader/JdbcDataReader.java
Patch:
@@ -136,9 +136,9 @@ public DataStream<Row> readData() {
 
         String query;
         if (StringUtils.isNotEmpty(customSql)){
-            query = DBUtil.buildQuerySqlWithCustomSql(databaseInterface, customSql, isSplitByKey, splitKey, useMaxFunc);
+            query = DBUtil.buildQuerySqlWithCustomSql(databaseInterface, customSql, isSplitByKey, splitKey, StringUtils.isNotEmpty(increColumn));
         } else {
-            query = DBUtil.getQuerySql(databaseInterface, table, metaColumns, splitKey, where, isSplitByKey, useMaxFunc);
+            query = DBUtil.getQuerySql(databaseInterface, table, metaColumns, splitKey, where, isSplitByKey, StringUtils.isNotEmpty(increColumn));
         }
         builder.setQuery(query);
 

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/inputformat/JdbcInputFormat.java
Patch:
@@ -406,7 +406,7 @@ private String buildQuerySql(InputSplit inputSplit){
                         .replace("${M}", String.valueOf(jdbcInputSplit.getMod()));
             }
 
-            if (useMaxFunc){
+            if (StringUtils.isNotEmpty(increCol)){
                 String incrementFilter = DBUtil.buildIncrementFilter(databaseInterface, increColType, increCol,
                         jdbcInputSplit.getStartLocation(), jdbcInputSplit.getEndLocation(), customSql);
 

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/datareader/JdbcDataReader.java
Patch:
@@ -136,9 +136,9 @@ public DataStream<Row> readData() {
 
         String query;
         if (StringUtils.isNotEmpty(customSql)){
-            query = DBUtil.buildQuerySqlWithCustomSql(databaseInterface, customSql, isSplitByKey, splitKey, useMaxFunc);
+            query = DBUtil.buildQuerySqlWithCustomSql(databaseInterface, customSql, isSplitByKey, splitKey, StringUtils.isNotEmpty(increColumn));
         } else {
-            query = DBUtil.getQuerySql(databaseInterface, table, metaColumns, splitKey, where, isSplitByKey, useMaxFunc);
+            query = DBUtil.getQuerySql(databaseInterface, table, metaColumns, splitKey, where, isSplitByKey, StringUtils.isNotEmpty(increColumn));
         }
         builder.setQuery(query);
 

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/inputformat/JdbcInputFormat.java
Patch:
@@ -406,7 +406,7 @@ private String buildQuerySql(InputSplit inputSplit){
                         .replace("${M}", String.valueOf(jdbcInputSplit.getMod()));
             }
 
-            if (useMaxFunc){
+            if (StringUtils.isNotEmpty(increCol)){
                 String incrementFilter = DBUtil.buildIncrementFilter(databaseInterface, increColType, increCol,
                         jdbcInputSplit.getStartLocation(), jdbcInputSplit.getEndLocation(), customSql);
 

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/inputformat/JdbcInputFormat.java
Patch:
@@ -289,23 +289,22 @@ private void initMetric(InputSplit split){
             getRuntimeContext().addAccumulator(Metrics.TABLE_COL,tableColAccumulator);
         }
 
+        endLocationAccumulator = new MaximumAccumulator();
         String endLocation = ((JdbcInputSplit)split).getEndLocation();
         if(!accumulatorMap.containsKey(Metrics.END_LOCATION) && endLocation != null){
-            endLocationAccumulator = new MaximumAccumulator();
-
             if(useMaxFunc){
                 endLocationAccumulator.add(endLocation);
             }
 
             getRuntimeContext().addAccumulator(Metrics.END_LOCATION,endLocationAccumulator);
         }
 
+        startLocationAccumulator = new StringAccumulator();
         if (!accumulatorMap.containsKey(Metrics.START_LOCATION) && startLocation != null){
             if(!useMaxFunc){
                 endLocationAccumulator.add(startLocation);
             }
 
-            startLocationAccumulator = new StringAccumulator();
             startLocationAccumulator.add(startLocation);
             getRuntimeContext().addAccumulator(Metrics.START_LOCATION,startLocationAccumulator);
         }

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/datareader/JdbcConfigKeys.java
Patch:
@@ -46,5 +46,5 @@ public class JdbcConfigKeys {
 
     public static final String KEY_CUSTOM_SQL = "customSql";
 
-    public static final String KEY_REALTIME_INCRE_SYNC = "realTimeIncreSync";
+    public static final String KEY_USE_MAX_FUNC = "useMaxFunc";
 }

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/inputformat/JdbcInputFormatBuilder.java
Patch:
@@ -105,8 +105,8 @@ public void setIncreColType(String increColType){
         format.increColType = increColType;
     }
 
-    public void setRealTimeIncreSync(boolean realTimeIncreSync){
-        format.realTimeIncreSync = realTimeIncreSync;
+    public void setUseMaxFunc(boolean useMaxFunc){
+        format.useMaxFunc = useMaxFunc;
     }
 
     public void setNumPartitions(int numPartitions){

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/util/DBUtil.java
Patch:
@@ -532,7 +532,7 @@ public static long getMillis(long startLocation){
     }
 
     public static String buildQuerySqlWithCustomSql(DatabaseInterface databaseInterface,String customSql,
-                                                    boolean isSplitByKey,String splitKey,boolean realTimeIncreSync){
+                                                    boolean isSplitByKey,String splitKey,boolean useMaxFunc){
         StringBuilder querySql = new StringBuilder();
         querySql.append(String.format(CUSTOM_SQL_TEMPLATE, customSql, TEMPORARY_TABLE_NAME));
         querySql.append(" WHERE 1=1 ");
@@ -541,7 +541,7 @@ public static String buildQuerySqlWithCustomSql(DatabaseInterface databaseInterf
             querySql.append(" And ").append(databaseInterface.getSplitFilterWithTmpTable(TEMPORARY_TABLE_NAME, splitKey));
         }
 
-        if (realTimeIncreSync){
+        if (useMaxFunc){
             querySql.append(" ").append(INCREMENT_FILTER_PLACEHOLDER);
         }
 

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/inputformat/JdbcInputFormat.java
Patch:
@@ -281,23 +281,22 @@ private void initMetric(InputSplit split){
             getRuntimeContext().addAccumulator(Metrics.TABLE_COL,tableColAccumulator);
         }
 
+        endLocationAccumulator = new MaximumAccumulator();
         String endLocation = ((JdbcInputSplit)split).getEndLocation();
         if(!accumulatorMap.containsKey(Metrics.END_LOCATION) && endLocation != null){
-            endLocationAccumulator = new MaximumAccumulator();
-
             if(useMaxFunc){
                 endLocationAccumulator.add(endLocation);
             }
 
             getRuntimeContext().addAccumulator(Metrics.END_LOCATION,endLocationAccumulator);
         }
 
+        startLocationAccumulator = new StringAccumulator();
         if (!accumulatorMap.containsKey(Metrics.START_LOCATION) && startLocation != null){
             if(!useMaxFunc){
                 endLocationAccumulator.add(startLocation);
             }
 
-            startLocationAccumulator = new StringAccumulator();
             startLocationAccumulator.add(startLocation);
             getRuntimeContext().addAccumulator(Metrics.START_LOCATION,startLocationAccumulator);
         }

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/datareader/JdbcConfigKeys.java
Patch:
@@ -46,5 +46,5 @@ public class JdbcConfigKeys {
 
     public static final String KEY_CUSTOM_SQL = "customSql";
 
-    public static final String KEY_REALTIME_INCRE_SYNC = "realTimeIncreSync";
+    public static final String KEY_USE_MAX_FUNC = "useMaxFunc";
 }

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/inputformat/JdbcInputFormatBuilder.java
Patch:
@@ -104,8 +104,8 @@ public void setIncreColType(String increColType){
         format.increColType = increColType;
     }
 
-    public void setRealTimeIncreSync(boolean realTimeIncreSync){
-        format.realTimeIncreSync = realTimeIncreSync;
+    public void setUseMaxFunc(boolean useMaxFunc){
+        format.useMaxFunc = useMaxFunc;
     }
 
     public void setNumPartitions(int numPartitions){

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/util/DBUtil.java
Patch:
@@ -532,7 +532,7 @@ public static long getMillis(long startLocation){
     }
 
     public static String buildQuerySqlWithCustomSql(DatabaseInterface databaseInterface,String customSql,
-                                                    boolean isSplitByKey,String splitKey,boolean realTimeIncreSync){
+                                                    boolean isSplitByKey,String splitKey,boolean useMaxFunc){
         StringBuilder querySql = new StringBuilder();
         querySql.append(String.format(CUSTOM_SQL_TEMPLATE, customSql, TEMPORARY_TABLE_NAME));
         querySql.append(" WHERE 1=1 ");
@@ -541,7 +541,7 @@ public static String buildQuerySqlWithCustomSql(DatabaseInterface databaseInterf
             querySql.append(" And ").append(databaseInterface.getSplitFilterWithTmpTable(TEMPORARY_TABLE_NAME, splitKey));
         }
 
-        if (realTimeIncreSync){
+        if (useMaxFunc){
             querySql.append(" ").append(INCREMENT_FILTER_PLACEHOLDER);
         }
 

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/inputformat/JdbcInputFormat.java
Patch:
@@ -281,23 +281,22 @@ private void initMetric(InputSplit split){
             getRuntimeContext().addAccumulator(Metrics.TABLE_COL,tableColAccumulator);
         }
 
+        endLocationAccumulator = new MaximumAccumulator();
         String endLocation = ((JdbcInputSplit)split).getEndLocation();
         if(!accumulatorMap.containsKey(Metrics.END_LOCATION) && endLocation != null){
-            endLocationAccumulator = new MaximumAccumulator();
-
             if(useMaxFunc){
                 endLocationAccumulator.add(endLocation);
             }
 
             getRuntimeContext().addAccumulator(Metrics.END_LOCATION,endLocationAccumulator);
         }
 
+        startLocationAccumulator = new StringAccumulator();
         if (!accumulatorMap.containsKey(Metrics.START_LOCATION) && startLocation != null){
             if(!useMaxFunc){
                 endLocationAccumulator.add(startLocation);
             }
 
-            startLocationAccumulator = new StringAccumulator();
             startLocationAccumulator.add(startLocation);
             getRuntimeContext().addAccumulator(Metrics.START_LOCATION,startLocationAccumulator);
         }

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/datareader/JdbcConfigKeys.java
Patch:
@@ -46,5 +46,5 @@ public class JdbcConfigKeys {
 
     public static final String KEY_CUSTOM_SQL = "customSql";
 
-    public static final String KEY_REALTIME_INCRE_SYNC = "realTimeIncreSync";
+    public static final String KEY_USE_MAX_FUNC = "useMaxFunc";
 }

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/inputformat/JdbcInputFormatBuilder.java
Patch:
@@ -104,8 +104,8 @@ public void setIncreColType(String increColType){
         format.increColType = increColType;
     }
 
-    public void setRealTimeIncreSync(boolean realTimeIncreSync){
-        format.realTimeIncreSync = realTimeIncreSync;
+    public void setUseMaxFunc(boolean useMaxFunc){
+        format.useMaxFunc = useMaxFunc;
     }
 
     public void setNumPartitions(int numPartitions){

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/util/DBUtil.java
Patch:
@@ -532,7 +532,7 @@ public static long getMillis(long startLocation){
     }
 
     public static String buildQuerySqlWithCustomSql(DatabaseInterface databaseInterface,String customSql,
-                                                    boolean isSplitByKey,String splitKey,boolean realTimeIncreSync){
+                                                    boolean isSplitByKey,String splitKey,boolean useMaxFunc){
         StringBuilder querySql = new StringBuilder();
         querySql.append(String.format(CUSTOM_SQL_TEMPLATE, customSql, TEMPORARY_TABLE_NAME));
         querySql.append(" WHERE 1=1 ");
@@ -541,7 +541,7 @@ public static String buildQuerySqlWithCustomSql(DatabaseInterface databaseInterf
             querySql.append(" And ").append(databaseInterface.getSplitFilterWithTmpTable(TEMPORARY_TABLE_NAME, splitKey));
         }
 
-        if (realTimeIncreSync){
+        if (useMaxFunc){
             querySql.append(" ").append(INCREMENT_FILTER_PLACEHOLDER);
         }
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/outputformat/RichOutputFormat.java
Patch:
@@ -223,7 +223,7 @@ public void open(int taskNumber, int numTasks) throws IOException {
         }
 
         if(restoreConfig.isRestore()){
-            formatState = new FormatState(numTasks, null);
+            formatState = new FormatState(taskNumber, null);
         }
     }
 

File: flinkx-core/src/main/java/org/apache/flink/streaming/api/functions/sink/DtOutputFormatSinkFunction.java
Patch:
@@ -128,10 +128,9 @@ private void cleanup() {
     public void snapshotState(FunctionSnapshotContext context) throws Exception {
         FormatState formatState = ((com.dtstack.flinkx.outputformat.RichOutputFormat) format).getFormatState();
         if (formatState != null){
-            LOG.info("Start create snapshot");
+            LOG.info("OutputFormat format state:{}", formatState.toString());
             unionOffsetStates.clear();
             unionOffsetStates.add(formatState);
-            LOG.info("Create snapshot success");
         }
     }
 

File: flinkx-stream/flinkx-stream-reader/src/main/java/com/dtstack/flinkx/stream/reader/StreamInputFormat.java
Patch:
@@ -57,6 +57,7 @@ public Row nextRecordInternal(Row row) throws IOException {
         if (restoreConfig.isRestore() && columns.get(0).getValue() == null){
             row.setField(0, recordRead);
         }
+
         return row;
     }
 

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsOutputFormat.java
Patch:
@@ -193,13 +193,14 @@ protected void writeMultipleRecordsInternal() throws Exception {
 
     @Override
     public void tryCleanupOnError() throws Exception {
-        if(fs != null) {
+        if(restoreConfig.isRestore() && fs != null) {
             Path finishedDir = new Path(outputFilePath + SP + FINISHED_SUBDIR);
             Path tmpDir = new Path(outputFilePath + SP + DATA_SUBDIR);
             fs.delete(finishedDir, true);
             fs.delete(tmpDir, true);
+
+            LOG.info(jobName + ": tryCleanupOnError over!");
         }
-        LOG.info(jobName + ": tryCleanupOnError over!");
     }
 
 

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsParquetOutputFormat.java
Patch:
@@ -186,6 +186,7 @@ protected void writeSingleRecordInternal(Row row) throws WriteRecordException {
             }
 
             writer.write(group);
+            rowsOfCurrentBlock++;
         } catch (Exception e){
             if(i < row.getArity()) {
                 throw new WriteRecordException(recordConvertDetailErrorMessage(i, row), e, i, row);

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/inputformat/DistributedJdbcInputFormat.java
Patch:
@@ -117,7 +117,8 @@ private void openNextSource() throws SQLException{
         DataSource currentSource = sourceList.get(sourceIndex);
         currentConn = DBUtil.getConnection(currentSource.getJdbcUrl(), currentSource.getUserName(), currentSource.getPassword());
         currentConn.setAutoCommit(false);
-        String queryTemplate = DBUtil.getQuerySql(databaseInterface, currentSource.getTable(),metaColumns,splitKey,where, currentSource.isSplitByKey());
+        String queryTemplate = DBUtil.getQuerySql(databaseInterface, currentSource.getTable(),metaColumns,splitKey,
+                where, currentSource.isSplitByKey(), false, false);
         currentStatement = currentConn.createStatement(resultSetType, resultSetConcurrency);
 
         if (currentSource.isSplitByKey()){

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/inputformat/JdbcInputFormat.java
Patch:
@@ -524,7 +524,7 @@ private void uploadMetricData() throws IOException {
             out = FileSystem.create(remotePath.getFileSystem(conf), remotePath, new FsPermission(FsPermission.createImmutable((short) 0777)));
 
             Map<String,Object> metrics = new HashMap<>(3);
-            metrics.put(Metrics.TABLE_COL, table + "-" + increCol);
+            metrics.put(Metrics.TABLE_COL, table + "-" + incrementConfig.getColumnName());
             metrics.put(Metrics.START_LOCATION, startLocationAccumulator.getLocalValue());
             metrics.put(Metrics.END_LOCATION, endLocationAccumulator.getLocalValue());
             out.writeUTF(new ObjectMapper().writeValueAsString(metrics));
@@ -535,7 +535,7 @@ private void uploadMetricData() throws IOException {
 
     @Override
     public void closeInternal() throws IOException {
-        if(increCol != null && hadoopConfig != null) {
+        if(incrementConfig.isIncrement() && hadoopConfig != null) {
             uploadMetricData();
         }
         DBUtil.closeDBResources(resultSet,statement,dbConn);

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/util/DBUtil.java
Patch:
@@ -170,7 +170,7 @@ public static void closeDBResources(ResultSet rs, Statement stmt,
 
     public static void commit(Connection conn){
         try {
-            if (!conn.getAutoCommit() && !conn.isClosed()){
+            if (!conn.isClosed() && !conn.getAutoCommit()){
                 LOG.info("Start commit connection");
                 conn.commit();
                 LOG.info("Commit connection successful");

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/util/DBUtil.java
Patch:
@@ -170,7 +170,7 @@ public static void closeDBResources(ResultSet rs, Statement stmt,
 
     public static void commit(Connection conn){
         try {
-            if (!conn.getAutoCommit() && !conn.isClosed()){
+            if (!conn.isClosed() && !conn.getAutoCommit()){
                 LOG.info("Start commit connection");
                 conn.commit();
                 LOG.info("Commit connection successful");

File: flinkx-hdfs/flinkx-hdfs-core/src/main/java/com/dtstack/flinkx/hdfs/HdfsUtil.java
Patch:
@@ -155,6 +155,8 @@ public static Object getWritableValue(Object writable) {
             ret = ((DateWritable) writable).get();
         } else if(writable instanceof Writable) {
             ret = writable.toString();
+        } else {
+            ret = writable.toString();
         }
 
         return ret;

File: flinkx-es/flinkx-es-core/src/main/java/com/dtstack/flinkx/es/EsConfigKeys.java
Patch:
@@ -34,6 +34,8 @@ public class EsConfigKeys {
 
     public static final String KEY_TYPE = "type";
 
+    public static final String KEY_BATCH_SIZE = "batchSize";
+
     public static final String KEY_BULK_ACTION = "bulkAction";
 
     public static final String KEY_COLUMN_NAME = "name";

File: flinkx-es/flinkx-es-core/src/main/java/com/dtstack/flinkx/es/EsUtil.java
Patch:
@@ -81,6 +81,7 @@ public static SearchResponse search(RestHighLevelClient client, String query, in
         } catch (IOException e) {
             throw new RuntimeException(e);
         }
+
     }
 
     public static long searchCount(RestHighLevelClient client, String query) {

File: flinkx-es/flinkx-es-core/src/main/java/com/dtstack/flinkx/es/EsConfigKeys.java
Patch:
@@ -34,6 +34,8 @@ public class EsConfigKeys {
 
     public static final String KEY_TYPE = "type";
 
+    public static final String KEY_BATCH_SIZE = "batchSize";
+
     public static final String KEY_BULK_ACTION = "bulkAction";
 
     public static final String KEY_COLUMN_NAME = "name";

File: flinkx-hdfs/flinkx-hdfs-core/src/main/java/com/dtstack/flinkx/hdfs/HdfsUtil.java
Patch:
@@ -155,6 +155,8 @@ public static Object getWritableValue(Object writable) {
             ret = ((DateWritable) writable).get();
         } else if(writable instanceof Writable) {
             ret = writable.toString();
+        } else {
+            ret = writable.toString();
         }
 
         return ret;

File: flinkx-hdfs/flinkx-hdfs-core/src/main/java/com/dtstack/flinkx/hdfs/HdfsUtil.java
Patch:
@@ -155,6 +155,8 @@ public static Object getWritableValue(Object writable) {
             ret = ((DateWritable) writable).get();
         } else if(writable instanceof Writable) {
             ret = writable.toString();
+        } else {
+            ret = writable.toString();
         }
 
         return ret;

File: flinkx-hdfs/flinkx-hdfs-core/src/main/java/com/dtstack/flinkx/hdfs/HdfsUtil.java
Patch:
@@ -155,6 +155,8 @@ public static Object getWritableValue(Object writable) {
             ret = ((DateWritable) writable).get();
         } else if(writable instanceof Writable) {
             ret = writable.toString();
+        } else {
+            ret = writable.toString();
         }
 
         return ret;

File: flinkx-hdfs/flinkx-hdfs-reader/src/main/java/com/dtstack/flinkx/hdfs/reader/HdfsOrcInputFormat.java
Patch:
@@ -206,6 +206,8 @@ public Row nextRecordInternal(Row row) throws IOException {
 
                 if(val instanceof String || val instanceof org.apache.hadoop.io.Text){
                     val = HdfsUtil.string2col(String.valueOf(val),metaColumn.getType(),metaColumn.getTimeFormat());
+                } else if(val != null){
+                    val = HdfsUtil.getWritableValue(val);
                 }
 
                 row.setField(i,val);

File: flinkx-es/flinkx-es-writer/src/main/java/com/dtstack/flinkx/es/writer/EsOutputFormat.java
Patch:
@@ -91,8 +91,8 @@ protected void writeMultipleRecordsInternal() throws Exception {
             IndexRequest request = StringUtils.isBlank(id) ? new IndexRequest(index, type) : new IndexRequest(index, type, id);
             request = request.source(EsUtil.rowToJsonMap(row, columnNames, columnTypes));
             bulkRequest.add(request);
-            client.bulk(bulkRequest);
         }
+        client.bulk(bulkRequest);
     }
 
     @Override

File: flinkx-es/flinkx-es-writer/src/main/java/com/dtstack/flinkx/es/writer/EsOutputFormat.java
Patch:
@@ -91,8 +91,8 @@ protected void writeMultipleRecordsInternal() throws Exception {
             IndexRequest request = StringUtils.isBlank(id) ? new IndexRequest(index, type) : new IndexRequest(index, type, id);
             request = request.source(EsUtil.rowToJsonMap(row, columnNames, columnTypes));
             bulkRequest.add(request);
-            client.bulk(bulkRequest);
         }
+        client.bulk(bulkRequest);
     }
 
     @Override

File: flinkx-es/flinkx-es-core/src/main/java/com/dtstack/flinkx/es/EsUtil.java
Patch:
@@ -138,10 +138,10 @@ public static Map<String, Object> rowToJsonMap(Row row, List<String> fields, Lis
                 String key = parts[parts.length - 1];
                 Object col = row.getField(i);
                 if(col != null) {
-                    Object value = StringUtil.col2string(col, types.get(i));
-                    currMap.put(key, value);
+                    col = StringUtil.string2col(String.valueOf(col), types.get(i), null);
                 }
 
+                currMap.put(key, col);
             }
         } catch(Exception ex) {
             String msg = "EsUtil.rowToJsonMap Writing record error: when converting field[" + i + "] in Row(" + row + ")";

File: flinkx-hdfs/flinkx-hdfs-reader/src/main/java/com/dtstack/flinkx/hdfs/reader/HdfsOrcInputFormat.java
Patch:
@@ -204,7 +204,7 @@ public Row nextRecordInternal(Row row) throws IOException {
                     val = metaColumn.getValue();
                 }
 
-                if(val instanceof String){
+                if(val instanceof String || val instanceof org.apache.hadoop.io.Text){
                     val = HdfsUtil.string2col(String.valueOf(val),metaColumn.getType(),metaColumn.getTimeFormat());
                 }
 

File: flinkx-hdfs/flinkx-hdfs-reader/src/main/java/com/dtstack/flinkx/hdfs/reader/HdfsOrcInputFormat.java
Patch:
@@ -204,7 +204,7 @@ public Row nextRecordInternal(Row row) throws IOException {
                     val = metaColumn.getValue();
                 }
 
-                if(val instanceof String){
+                if(val instanceof String || val instanceof org.apache.hadoop.io.Text){
                     val = HdfsUtil.string2col(String.valueOf(val),metaColumn.getType(),metaColumn.getTimeFormat());
                 }
 

File: flinkx-es/flinkx-es-core/src/main/java/com/dtstack/flinkx/es/EsUtil.java
Patch:
@@ -138,8 +138,7 @@ public static Map<String, Object> rowToJsonMap(Row row, List<String> fields, Lis
                 String key = parts[parts.length - 1];
                 Object col = row.getField(i);
                 if(col != null) {
-                    Object value = StringUtil.col2string(col, types.get(i));
-                    currMap.put(key, value);
+                    currMap.put(key, col);
                 }
 
             }

File: flinkx-core/src/main/java/com/dtstack/flinkx/metrics/promethues/PrometheusPushGatewayReporter.java
Patch:
@@ -20,7 +20,6 @@
 
 import org.apache.flink.metrics.Metric;
 import org.apache.flink.metrics.MetricConfig;
-import org.apache.flink.metrics.reporter.MetricReporter;
 import org.apache.flink.metrics.reporter.Scheduled;
 import io.prometheus.client.CollectorRegistry;
 import io.prometheus.client.exporter.PushGateway;
@@ -35,7 +34,7 @@
 import static  com.dtstack.flinkx.metrics.promethues.PrometheusPushGatewayReporterOptions.RANDOM_JOB_NAME_SUFFIX;
 
 /**
- * {@link MetricReporter} that exports {@link Metric Metrics} via Prometheus {@link PushGateway}.
+ * {@link com.dtstack.flinkx.metrics.base.reporter.MetricReporter} that exports {@link Metric Metrics} via Prometheus {@link PushGateway}.
  */
 public class PrometheusPushGatewayReporter extends AbstractPrometheusReporter implements Scheduled {
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/metrics/promethues/PrometheusReporter.java
Patch:
@@ -20,7 +20,6 @@
 
 import org.apache.flink.metrics.Metric;
 import org.apache.flink.metrics.MetricConfig;
-import org.apache.flink.metrics.reporter.MetricReporter;
 import org.apache.flink.util.NetUtils;
 import org.apache.flink.util.Preconditions;
 import io.prometheus.client.exporter.HTTPServer;
@@ -29,7 +28,7 @@
 import java.util.Iterator;
 
 /**
- * {@link MetricReporter} that exports {@link Metric Metrics} via Prometheus.
+ * {@link com.dtstack.flinkx.metrics.base.reporter.MetricReporter} that exports {@link Metric Metrics} via Prometheus.
  */
 public class PrometheusReporter extends AbstractPrometheusReporter {
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/metrics/scope/PipelineScopeFormat.java
Patch:
@@ -19,8 +19,6 @@
 package com.dtstack.flinkx.metrics.scope;
 
 
-import org.apache.flink.runtime.metrics.scope.ScopeFormat;
-
 public class PipelineScopeFormat extends ScopeFormat {
 
     public PipelineScopeFormat(String format) {

File: flinkx-core/src/main/java/com/dtstack/flinkx/metrics/scope/ScopeFormat.java
Patch:
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.flink.runtime.metrics.scope;
+package com.dtstack.flinkx.metrics.scope;
 
 import org.apache.flink.metrics.CharacterFilter;
 

File: flinkx-odps/flinkx-odps-reader/src/main/java/com/dtstack/flinkx/odps/reader/OdpsInputFormat.java
Patch:
@@ -186,7 +186,7 @@ public Row nextRecordInternal(Row row) throws IOException {
                     val = metaColumn.getValue();
                 }
 
-                if(val != null){
+                if(val != null && val instanceof String){
                     val = StringUtil.string2col(String.valueOf(val),metaColumn.getType(),metaColumn.getTimeFormat());
                 }
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/common/ColumnType.java
Patch:
@@ -29,7 +29,7 @@
  * @author huyifan.zju@163.com
  */
 public enum ColumnType {
-    STRING, VARCHAR, CHAR,NVARCHAR,TEXT,KEYWORD,
+    STRING, VARCHAR, CHAR,NVARCHAR,TEXT,KEYWORD,BINARY,
     INT, MEDIUMINT, TINYINT, DATETIME, SMALLINT, BIGINT,LONG,SHORT,INTEGER,
     DOUBLE, FLOAT,
     BOOLEAN,

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/inputformat/JdbcInputFormat.java
Patch:
@@ -284,7 +284,8 @@ private String getEndLocation() {
         Statement st = null;
         ResultSet rs = null;
         try {
-            String queryMaxValueSql = String.format("select max(%s) as max_value from %s", increCol, table);
+            String queryMaxValueSql = String.format("select max(%s) as max_value from %s",
+                    databaseInterface.quoteColumn(increCol), databaseInterface.quoteTable(table));
             String startSql = DBUtil.buildStartLocationSql(databaseInterface, increColType, increCol, startLocation);
             if(StringUtils.isNotEmpty(startSql)){
                 queryMaxValueSql += " where " + startSql;

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/datareader/JdbcConfigKeys.java
Patch:
@@ -42,4 +42,5 @@ public class JdbcConfigKeys {
 
     public static final String KEY_START_LOCATION = "startLocation";
 
+    public static final String KEY_REALTIME_INCRE_SYNC = "realTimeIncreSync";
 }

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/inputformat/MaximumAccumulator.java
Patch:
@@ -57,7 +57,7 @@ public void resetLocal() {
     @Override
     public void merge(Accumulator<String, String> other) {
         BigInteger local = new BigInteger(localValue);
-        if(local.compareTo(new BigInteger(other.getLocalValue())) > 0){
+        if(local.compareTo(new BigInteger(other.getLocalValue())) < 0){
             localValue = other.getLocalValue();
         }
     }

File: flinkx-es/flinkx-es-reader/src/main/java/com/dtstack/flinkx/es/reader/EsInputFormat.java
Patch:
@@ -81,7 +81,7 @@ public BaseStatistics getStatistics(BaseStatistics baseStatistics) throws IOExce
     public InputSplit[] createInputSplits(int splitNum) throws IOException {
         long cnt = EsUtil.searchCount(client, query);
         if (cnt < splitNum) {
-            EsInputSplit[] splits = new EsInputSplit[0];
+            EsInputSplit[] splits = new EsInputSplit[1];
             splits[0] = new EsInputSplit(0, (int)cnt);
             return splits;
         }

File: flinkx-core/src/main/java/com/dtstack/flinkx/outputformat/RichOutputFormat.java
Patch:
@@ -247,6 +247,7 @@ protected void writeSingleRecord(Row row) {
 
             if(errorLimiter != null) {
                 errorLimiter.setErrMsg(errMsg);
+                errorLimiter.setErrorData(row);
             }
 
             if(dirtyDataManager != null) {
@@ -284,7 +285,6 @@ protected void writeRecordInternal() {
         try {
             writeMultipleRecords();
         } catch(Exception e) {
-            e.printStackTrace();
             rows.forEach(this::writeSingleRecord);
         }
         rows.clear();

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/outputformat/JdbcOutputFormat.java
Patch:
@@ -253,8 +253,8 @@ protected Object getField(Row row, int index) {
             field = DateUtil.columnToTimestamp(field,null);
         }
 
-        if (type.equalsIgnoreCase(ColType.BIGINT.toString()) && field instanceof Timestamp){
-            field = ((Timestamp) field).getTime();
+        if (type.equalsIgnoreCase(ColType.BIGINT.toString()) && field instanceof java.util.Date){
+            field = ((java.util.Date) field).getTime();
         }
 
         field=dealOracleTimestampToVarcharOrLong(databaseInterface.getDatabaseType(),field,type);

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/outputformat/JdbcOutputFormat.java
Patch:
@@ -253,8 +253,8 @@ protected Object getField(Row row, int index) {
             field = DateUtil.columnToTimestamp(field,null);
         }
 
-        if (type.equalsIgnoreCase(ColType.BIGINT.toString()) && field instanceof Timestamp){
-            field = ((Timestamp) field).getTime();
+        if (type.equalsIgnoreCase(ColType.BIGINT.toString()) && field instanceof java.util.Date){
+            field = ((java.util.Date) field).getTime();
         }
 
         field=dealOracleTimestampToVarcharOrLong(databaseInterface.getDatabaseType(),field,type);

File: flinkx-core/src/main/java/com/dtstack/flinkx/writer/DirtyDataManager.java
Patch:
@@ -22,6 +22,7 @@
 import com.dtstack.flinkx.util.DateUtil;
 import com.dtstack.flinkx.util.RowUtil;
 import com.google.gson.Gson;
+import com.google.gson.GsonBuilder;
 import org.apache.commons.lang3.StringUtils;
 import org.apache.flink.types.Row;
 import org.apache.hadoop.conf.Configuration;
@@ -55,15 +56,14 @@ public class DirtyDataManager {
 
 
     private static List<String> PRIMARY_CONFLICT_KEYWORDS = new ArrayList<>();
-    private Gson gson = new Gson();
+    private Gson gson = new GsonBuilder().disableHtmlEscaping().create();
 
     static {
         PRIMARY_CONFLICT_KEYWORDS.add("duplicate entry");
         PRIMARY_CONFLICT_KEYWORDS.add("unique constraint");
         PRIMARY_CONFLICT_KEYWORDS.add("primary key constraint");
     }
 
-
     public DirtyDataManager(String path, Map<String,String> configMap, String[] fieldNames) {
         this.fieldNames = fieldNames;
         location = path + "/" + UUID.randomUUID() + ".txt";

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/inputformat/JdbcInputFormatBuilder.java
Patch:
@@ -92,7 +92,7 @@ public void setIncreCol(String increCol){
         format.increCol = increCol;
     }
 
-    public void setStartLocation(Long startLocation){
+    public void setStartLocation(String startLocation){
         format.startLocation = startLocation;
     }
 

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/util/DBUtil.java
Patch:
@@ -352,7 +352,7 @@ public static Object clobToString(Object obj) throws Exception{
     }
 
     public static String buildWhereSql(DatabaseInterface databaseInterface,String increColType,String where,
-                                       String increCol,Long startLocation){
+                                       String increCol,String startLocation){
         if (startLocation == null){
             return where;
         }
@@ -361,7 +361,7 @@ public static String buildWhereSql(DatabaseInterface databaseInterface,String in
         String startTimeStr;
 
         if(ColumnType.isTimeType(increColType) || (databaseInterface.getDatabaseType() == EDatabaseType.SQLServer && ColumnType.NVARCHAR.name().equals(increColType))){
-            startTimeStr = getStartTimeStr(databaseInterface.getDatabaseType(),startLocation);
+            startTimeStr = getStartTimeStr(databaseInterface.getDatabaseType(),Long.parseLong(startLocation));
 
             if (databaseInterface.getDatabaseType() == EDatabaseType.Oracle){
                 startTimeStr = String.format("TO_TIMESTAMP('%s','YYYY-MM-DD HH24:MI:SS:FF6')",startTimeStr);

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/inputformat/JdbcInputFormat.java
Patch:
@@ -260,10 +260,10 @@ public Row nextRecordInternal(Row row) throws IOException {
                     }
                 } else if(ColumnType.isNumberType(increColType)){
                     endLocationAccumulator.add(resultSet.getLong(increColIndex + 1));
-                } else if(databaseInterface.getDatabaseType() == EDatabaseType.Oracle){
+                } else {
                     String increVal = resultSet.getString(increColIndex + 1);
                     if(increVal != null){
-                        endLocationAccumulator.add(getLocation(increVal));
+                        endLocationAccumulator.add(Long.parseLong(increVal));
                     }
                 }
             }

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/inputformat/JdbcInputFormat.java
Patch:
@@ -30,7 +30,6 @@
 import com.dtstack.flinkx.util.StringUtil;
 import org.apache.commons.lang3.StringUtils;
 import org.apache.flink.api.common.accumulators.Accumulator;
-import org.apache.flink.api.common.accumulators.LongCounter;
 import org.apache.flink.api.common.accumulators.LongMaximum;
 import org.apache.flink.api.common.io.DefaultInputSplitAssigner;
 import org.apache.flink.api.common.io.statistics.BaseStatistics;
@@ -291,7 +290,7 @@ private long getLocation(Object increVal){
                 return Long.parseLong(time + fillZeroStr + nanosStr);
             }
         } else {
-            Date date = DateUtil.stringToDate(increVal.toString(),dateStrFormat.get());
+            Date date = DateUtil.stringToDate(increVal.toString(),null);
             String fillZeroStr = StringUtils.repeat("0",6);
             long time = date.getTime();
             return Long.parseLong(time + fillZeroStr);

File: flinkx-core/src/main/java/com/dtstack/flinkx/common/ColumnType.java
Patch:
@@ -29,8 +29,8 @@
  * @author huyifan.zju@163.com
  */
 public enum ColumnType {
-    STRING, VARCHAR, CHAR,NVARCHAR,
-    INT, MEDIUMINT, TINYINT, DATETIME, SMALLINT, BIGINT,LONG,SHORT,
+    STRING, VARCHAR, CHAR,NVARCHAR,TEXT,KEYWORD,
+    INT, MEDIUMINT, TINYINT, DATETIME, SMALLINT, BIGINT,LONG,SHORT,INTEGER,
     DOUBLE, FLOAT,
     BOOLEAN,
     DATE, TIMESTAMP,TIME,

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/StringUtil.java
Patch:
@@ -136,6 +136,7 @@ public static String col2string(Object column, String type) {
                 result = Short.valueOf(rowData.trim());
                 break;
             case INT:
+            case INTEGER:
                 result = Integer.valueOf(rowData.trim());
                 break;
             case BIGINT:
@@ -154,6 +155,7 @@ public static String col2string(Object column, String type) {
             case STRING:
             case VARCHAR:
             case CHAR:
+            case TEXT:
                 result = rowData;
                 break;
             case BOOLEAN:
@@ -166,7 +168,7 @@ public static String col2string(Object column, String type) {
                 result = DateUtil.timestampToString((java.util.Date)column);
                 break;
             default:
-                throw new IllegalArgumentException();
+                result = rowData;
         }
         return result.toString();
     }

File: flinkx-mongodb/flinkx-mongodb-reader/src/main/java/com/dtstack/flinkx/mongodb/reader/MongodbInputFormat.java
Patch:
@@ -62,9 +62,9 @@ public class MongodbInputFormat extends RichInputFormat {
 
     private Bson filter;
 
-    private MongoCollection<Document> collection;
+    private transient MongoCollection<Document> collection;
 
-    private MongoCursor<Document> cursor;
+    private transient MongoCursor<Document> cursor;
 
     @Override
     public void configure(Configuration parameters) {

File: flinkx-mongodb/flinkx-mongodb-writer/src/main/java/com/dtstack/flinkx/mongodb/writer/MongodbOutputFormat.java
Patch:
@@ -61,7 +61,7 @@ public class MongodbOutputFormat extends RichOutputFormat {
 
     protected String mode = WriteMode.INSERT.getMode();
 
-    private MongoCollection<Document> collection;
+    private transient MongoCollection<Document> collection;
 
     @Override
     public void configure(Configuration parameters) {

File: flinkx-odps/flinkx-odps-core/src/main/java/com/dtstack/flinkx/odps/OdpsUtil.java
Patch:
@@ -257,7 +257,7 @@ public static List<Pair<Long, Long>> splitRecordCount(long recordCount, int advi
         tempResult[tempResult.length - 1]++;
 
         for (int i = 0; i < tempResult.length - 1; i++) {
-            result.add(ImmutablePair.of(tempResult[i], (tempResult[i + 1] - tempResult[i])));
+            result.add(ImmutablePair.of(tempResult[i], (tempResult[i + 1] - tempResult[0])));
         }
         return result;
     }

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/outputformat/JdbcOutputFormat.java
Patch:
@@ -116,7 +116,7 @@ protected PreparedStatement prepareSingleTemplates() throws SQLException {
         } else if (EWriteMode.UPDATE.name().equalsIgnoreCase(mode)) {
             singleSql = databaseInterface.getUpsertStatement(column, table, updateKey);
         } else {
-            throw new IllegalArgumentException();
+            throw new IllegalArgumentException("Unknown write mode:" + mode);
         }
         return dbConn.prepareStatement(singleSql);
     }

File: flinkx-carbondata/flinkx-carbondata-writer/src/main/java/com/dtstack/flinkx/carbondata/writer/recordwriter/HivePartitionRecordWriter.java
Patch:
@@ -22,6 +22,7 @@
 
 import com.dtstack.flinkx.carbondata.writer.TaskNumberGenerator;
 import com.dtstack.flinkx.carbondata.writer.dict.CarbonTypeConverter;
+import com.dtstack.flinkx.carbondata.writer.dict.ExternalCatalogUtils;
 import org.apache.carbondata.core.constants.CarbonCommonConstants;
 import org.apache.carbondata.core.datastore.impl.FileFactory;
 import org.apache.carbondata.core.metadata.SegmentFileStore;
@@ -72,7 +73,7 @@ public class HivePartitionRecordWriter extends AbstractRecordWriter {
     public HivePartitionRecordWriter(CarbonTable carbonTable, String partition) {
         super(carbonTable);
         this.partition = updatePartition(carbonTable, partition);
-        writePath = carbonTable.getTablePath() + "/" + partition;
+        writePath = carbonTable.getTablePath() + "/" + this.partition;
         carbonLoadModel = createCarbonLoadModel();
         carbonLoadModelList.add(carbonLoadModel);
         context = createTaskContext();

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/inputformat/JdbcInputFormat.java
Patch:
@@ -241,7 +241,9 @@ public Row nextRecordInternal(Row row) throws IOException {
             if(increCol != null){
                 if (ColumnType.isTimeType(increColType)){
                     Timestamp increVal = resultSet.getTimestamp(increColIndex + 1);
-                    endLocationAccumulator.add(getLocation(increVal));
+                    if(increVal != null){
+                        endLocationAccumulator.add(getLocation(increVal));
+                    }
                 } else {
                     endLocationAccumulator.add(resultSet.getLong(increColIndex + 1));
                 }

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/inputformat/JdbcInputFormat.java
Patch:
@@ -124,9 +124,9 @@ private void setMetric(){
 
         endLocationAccumulator = new LongMaximum();
         getRuntimeContext().addAccumulator(Metrics.END_LOCATION,endLocationAccumulator);
-        endLocationAccumulator.add(startLocation);
 
         if (startLocation != null){
+            endLocationAccumulator.add(startLocation);
             startLocationAccumulator = getRuntimeContext().getLongCounter(Metrics.START_LOCATION);
             startLocationAccumulator.add(startLocation);
         }

File: flinkx-carbondata/flinkx-carbondata-writer/src/main/java/com/dtstack/flinkx/carbondata/writer/CarbonPartitionRecordWriterAssemble.java
Patch:
@@ -29,17 +29,17 @@ public CarbonPartitionRecordWriterAssemble(CarbonTable carbonTable) {
         super(carbonTable);
         PartitionInfo partitionInfo = carbonTable.getPartitionInfo();
         partitionIds =  partitionInfo.getPartitionIds();
+        counter = new int[partitionIds.size()];
         for(Integer partitionId : partitionIds) {
             CarbonLoadModel carbonLoadModel = createCarbonLoadModel();
             carbonLoadModelList.add(carbonLoadModel);
             TaskAttemptContext context = createTaskContext();
             context.getConfiguration().set("carbon.outputformat.taskno", String.valueOf(partitionId));
             taskAttemptContextList.add(context);
-            counterList.add(new Integer(0));
             RecordWriter recordWriter = null;
             try {
                 recordWriter = createRecordWriter(carbonLoadModel, context);
-                recordwriterList.add(recordWriter);
+                recordWriterList.add(recordWriter);
             } catch (IOException e) {
                 throw new RuntimeException(e);
             }

File: flinkx-carbondata/flinkx-carbondata-writer/src/main/java/com/dtstack/flinkx/carbondata/writer/HivePartitionRecordWriterAssemble.java
Patch:
@@ -46,11 +46,10 @@ public HivePartitionRecordWriterAssemble(CarbonTable carbonTable, String partiti
         carbonLoadModelList.add(carbonLoadModel);
         context = createTaskContext();
         taskAttemptContextList.add(context);
-        counterList.add(new Integer(0));
         RecordWriter recordWriter = null;
         try {
             recordWriter = createRecordWriter(carbonLoadModel, context);
-            recordwriterList.add(recordWriter);
+            recordWriterList.add(recordWriter);
         } catch (IOException e) {
             throw new RuntimeException(e);
         }

File: flinkx-carbondata/flinkx-carbondata-writer/src/main/java/com/dtstack/flinkx/carbondata/writer/SimpleRecordWriterAssemble.java
Patch:
@@ -16,11 +16,10 @@ public SimpleRecordWriterAssemble(CarbonTable carbonTable) {
         carbonLoadModelList.add(carbonLoadModel);
         TaskAttemptContext context = createTaskContext();
         taskAttemptContextList.add(context);
-        counterList.add(new Integer(0));
         RecordWriter recordWriter = null;
         try {
             recordWriter = createRecordWriter(carbonLoadModel, context);
-            recordwriterList.add(recordWriter);
+            recordWriterList.add(recordWriter);
         } catch (IOException e) {
             throw new RuntimeException(e);
         }

File: flinkx-carbondata/flinkx-carbondata-writer/src/main/java/com/dtstack/flinkx/carbondata/writer/TaskNumberGenerator.java
Patch:
@@ -11,8 +11,8 @@ private TaskNumberGenerator() {
      * Generate unique number to be used as partition number of file name
      */
     public static String generateUniqueNumber(int taskId, String segmentId, long partitionNumber) {
-        return String.valueOf((int)(Math.pow(10,2)) + Integer.valueOf(segmentId))
-                + String.valueOf((int)Math.pow(10,5) + taskId)
-                + String.valueOf(partitionNumber + (int)Math.pow(10,5));
+        return String.valueOf((long)(Math.pow(10,2)) + Integer.valueOf(segmentId))
+                + String.valueOf((long)Math.pow(10,5) + taskId)
+                + String.valueOf(partitionNumber + (long)Math.pow(10,5));
     }
 }

File: flinkx-odps/flinkx-odps-writer/src/main/java/com/dtstack/flinkx/odps/writer/OdpsOutputFormat.java
Patch:
@@ -134,6 +134,9 @@ private Record row2record(Row row, String[] columnTypes) throws WriteRecordExcep
                     case BOOLEAN:
                         record.setBoolean(i, Boolean.valueOf(rowData));
                         break;
+                    case INT:
+                    case TINYINT:
+                    case SMALLINT:
                     case BIGINT:
                         record.setBigint(i, Long.valueOf(rowData));
                         break;

File: flinkx-core/src/main/java/com/dtstack/flinkx/outputformat/RichOutputFormat.java
Patch:
@@ -333,6 +333,7 @@ public void close() throws IOException {
                     dirtyDataManager.close();
                 }
                 if(errorLimiter != null) {
+                    errorLimiter.updateErrorInfo();
                     errorLimiter.acquire();
                     errorLimiter.stop();
                 }

File: flinkx-oracle/flinkx-oracle-core/src/main/java/com/dtstack/flinkx/oracle/OracleDatabaseMeta.java
Patch:
@@ -35,6 +35,8 @@ public String quoteTable(String table) {
         String[] part = table.split("\\.");
         if(part.length == 2) {
             table = part[0] + "." + getStartQuote() + part[1] + getEndQuote();
+        } else {
+            table = getStartQuote() + table + getEndQuote();
         }
         return table;
     }

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/StringUtil.java
Patch:
@@ -24,6 +24,7 @@
 import org.apache.flink.types.Row;
 
 import java.math.BigDecimal;
+import java.sql.Date;
 import java.util.List;
 import java.util.regex.Matcher;
 import java.util.regex.Pattern;
@@ -100,6 +101,7 @@ public static Object string2col(String str, String type, FastDateFormat customTi
             case CHAR:
                 if(customTimeFormat != null){
                     ret = DateUtil.columnToDate(str,customTimeFormat);
+                    ret = DateUtil.timestampToString((Date)ret);
                 } else {
                     ret = str;
                 }

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/inputformat/JdbcInputFormat.java
Patch:
@@ -127,8 +127,8 @@ public void openInternal(InputSplit inputSplit) throws IOException {
                 statement.setQueryTimeout(queryTimeOut);
             }
             resultSet = statement.executeQuery(queryTemplate);
-            hasNext = resultSet.next();
             columnCount = resultSet.getMetaData().getColumnCount();
+            hasNext = resultSet.next();
 
             if(descColumnTypeList == null) {
                 descColumnTypeList = DBUtil.analyzeTable(dbURL, username, password,databaseInterface,table,metaColumns);

File: flinkx-ftp/flinkx-ftp-reader/src/main/java/com/dtstack/flinkx/ftp/reader/FtpInputFormat.java
Patch:
@@ -68,7 +68,7 @@ public class FtpInputFormat extends RichInputFormat {
 
     protected List<MetaColumn> metaColumns;
 
-    protected transient boolean isFirstLineHeader;
+    protected boolean isFirstLineHeader;
 
     private transient BufferedReader br;
 

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/outputformat/JdbcOutputFormat.java
Patch:
@@ -158,7 +158,7 @@ protected void openInternal(int taskNumber, int numTasks) throws IOException {
                 }
             }
 
-            if(databaseInterface.getDatabaseType() == EDatabaseType.SQLServer){
+            if(databaseInterface.getDatabaseType() == EDatabaseType.SQLServer && updateKey != null && updateKey.size() > 0){
                 dbConn.createStatement().execute(String.format("SET IDENTITY_INSERT [%s] ON",table));
             }
 
@@ -343,7 +343,7 @@ protected void beforeCloseInternal() {
         // 执行postsql
         if(taskNumber == 0) {
             try {
-                if (databaseInterface.getDatabaseType() == EDatabaseType.SQLServer){
+                if (databaseInterface.getDatabaseType() == EDatabaseType.SQLServer && updateKey != null && updateKey.size() > 0){
                     dbConn.createStatement().execute(String.format("SET IDENTITY_INSERT [%s] OFF",table));
                 }
             } catch (Exception e){

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsOrcOutputFormat.java
Patch:
@@ -77,7 +77,7 @@ protected void configInternal() {
             if(columnType.startsWith("DECIMAL")) {
                 columnType = "DECIMAL";
             }
-            ColumnType type = ColumnType.valueOf(columnType);
+            ColumnType type = ColumnType.getType(columnType);
             fullColTypeList.add(HdfsUtil.columnTypeToObjectInspetor(type));
         }
         this.inspector = ObjectInspectorFactory

File: flinkx-odps/flinkx-odps-writer/src/main/java/com/dtstack/flinkx/odps/writer/OdpsOutputFormat.java
Patch:
@@ -127,7 +127,7 @@ private Record row2record(Row row, String[] columnTypes) throws WriteRecordExcep
                     continue;
                 }
 
-                ColumnType columnType = ColumnType.valueOf(columnTypes[i].toUpperCase());
+                ColumnType columnType = ColumnType.getType(columnTypes[i].toUpperCase());
                 String rowData = column.toString();
 
                 switch (columnType) {

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/DateUtil.java
Patch:
@@ -124,7 +124,7 @@ public static long getMillSecond(String data){
             time = Long.valueOf(data) / 1000000 ;
         } else if(data.length() < 10){
             try {
-                int day = Integer.valueOf(data);
+                long day = Long.valueOf(data);
                 Date date = dateFormatter.parse(START_TIME);
                 Calendar cal = Calendar.getInstance();
                 long addMill = date.getTime() + day * 24 * 3600 * 1000;

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/outputformat/JdbcOutputFormat.java
Patch:
@@ -91,6 +91,8 @@ public class JdbcOutputFormat extends RichOutputFormat {
 
     private final static String TIMESTAMP_REGEX = "(?i)timestamp";
 
+    private final static String DATETIME_REGEX = "(?i)datetime";
+
     private final static String GET_ORACLE_INDEX_SQL = "SELECT " +
             "t.INDEX_NAME," +
             "t.COLUMN_NAME " +
@@ -252,7 +254,7 @@ protected Object getField(Row row, int index) {
         String type = columnType.get(index);
         if(type.matches(DATE_REGEX)) {
             field = DateUtil.columnToDate(field,null);
-        } else if(type.matches(TIMESTAMP_REGEX)){
+        } else if(type.matches(TIMESTAMP_REGEX) || type.matches(DATETIME_REGEX)){
             field = DateUtil.columnToTimestamp(field,null);
         }
 

File: flinkx-core/src/main/java/com/dtstack/flinkx/reader/MetaColumn.java
Patch:
@@ -112,9 +112,9 @@ public static List<MetaColumn> getMetaColumns(List columns){
                     }
 
                     mc.setName(sm.get("name") != null ? String.valueOf(sm.get("name")) : null);
-                    mc.setName(sm.get("type") != null ? String.valueOf(sm.get("type")) : null);
-                    mc.setName(sm.get("value") != null ? String.valueOf(sm.get("value")) : null);
-                    mc.setName(sm.get("splitter") != null ? String.valueOf(sm.get("splitter")) : null);
+                    mc.setType(sm.get("type") != null ? String.valueOf(sm.get("type")) : null);
+                    mc.setValue(sm.get("value") != null ? String.valueOf(sm.get("value")) : null);
+                    mc.setSplitter(sm.get("splitter") != null ? String.valueOf(sm.get("splitter")) : null);
 
                     if(sm.get("format") != null){
                         mc.setTimeFormat(DateUtil.getDateFormatter(String.valueOf(sm.get("format"))));

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsOutputFormat.java
Patch:
@@ -98,7 +98,7 @@ protected void initColIndices() {
         for(int i = 0; i < fullColumnNames.size(); ++i) {
             int j = 0;
             for(; j < columnNames.size(); ++j) {
-                if(fullColumnNames.get(i).equals(columnNames.get(j))) {
+                if(fullColumnNames.get(i).equalsIgnoreCase(columnNames.get(j))) {
                     colIndices[i] = j;
                     break;
                 }

File: flinkx-ftp/flinkx-ftp-reader/src/main/java/com/dtstack/flinkx/ftp/reader/FtpInputFormat.java
Patch:
@@ -140,7 +140,7 @@ public boolean reachedEnd() throws IOException {
     @Override
     public Row nextRecordInternal(Row row) throws IOException {
         String[] fields = line.split(delimiter);
-        if (metaColumns.size() == 1 && metaColumns.get(0).getName().equals("*")){
+        if (metaColumns.size() == 1 && "*".equals(metaColumns.get(0).getName())){
             row = new Row(fields.length);
             for (int i = 0; i < fields.length; i++) {
                 row.setField(i, fields[i]);

File: flinkx-hdfs/flinkx-hdfs-reader/src/main/java/com/dtstack/flinkx/hdfs/reader/HdfsOrcInputFormat.java
Patch:
@@ -180,7 +180,7 @@ private int name2index(String columnName) {
 
     @Override
     public Row nextRecordInternal(Row row) throws IOException {
-        if(metaColumns.size() == 1 && metaColumns.get(0).getName().equals("*")){
+        if(metaColumns.size() == 1 && "*".equals(metaColumns.get(0).getName())){
             row = new Row(fullColNames.length);
             for (int i = 0; i < fullColNames.length; i++) {
                 Object col = inspector.getStructFieldData(value, fields.get(i));

File: flinkx-hdfs/flinkx-hdfs-reader/src/main/java/com/dtstack/flinkx/hdfs/reader/HdfsParquetInputFormat.java
Patch:
@@ -118,7 +118,7 @@ private void nextFile() throws IOException{
 
     @Override
     protected Row nextRecordInternal(Row row) throws IOException {
-        if(metaColumns.size() == 1 && metaColumns.get(0).getName().equals("*")){
+        if(metaColumns.size() == 1 && "*".equals(metaColumns.get(0).getName())){
             row = new Row(fullColNames.size());
             for (int i = 0; i < fullColNames.size(); i++) {
                 Object val = getData(currentLine,fullColTypes.get(i),i);

File: flinkx-hdfs/flinkx-hdfs-reader/src/main/java/com/dtstack/flinkx/hdfs/reader/HdfsTextInputFormat.java
Patch:
@@ -89,7 +89,7 @@ public Row nextRecordInternal(Row row) throws IOException {
         String line = new String(data, charsetName);
         String[] fields = line.split(delimiter);
 
-        if (metaColumns.size() == 1 && metaColumns.get(0).getName().equals("*")){
+        if (metaColumns.size() == 1 && "*".equals(metaColumns.get(0).getName())){
             row = new Row(fields.length);
             for (int i = 0; i < fields.length; i++) {
                 row.setField(i, fields[i]);

File: flinkx-mongodb/flinkx-mongodb-reader/src/main/java/com/dtstack/flinkx/mongodb/reader/MongodbInputFormat.java
Patch:
@@ -96,7 +96,7 @@ protected void openInternal(InputSplit inputSplit) throws IOException {
     @Override
     public Row nextRecordInternal(Row row) throws IOException {
         Document doc = cursor.next();
-        if(metaColumns.size() == 1 && metaColumns.get(0).getName().equals("*")){
+        if(metaColumns.size() == 1 && "*".equals(metaColumns.get(0).getName())){
             row = new Row(doc.size());
             String[] names = doc.keySet().toArray(new String[0]);
             for (int i = 0; i < names.length; i++) {

File: flinkx-odps/flinkx-odps-reader/src/main/java/com/dtstack/flinkx/odps/reader/OdpsInputFormat.java
Patch:
@@ -161,7 +161,7 @@ record = recordReader.read();
 
     @Override
     public Row nextRecordInternal(Row row) throws IOException {
-        if (metaColumns.size() == 1 && metaColumns.get(0).getName().equals("*")){
+        if (metaColumns.size() == 1 && "*".equals(metaColumns.get(0).getName())){
             row = new Row(record.getColumnCount());
             for (int i = 0; i < record.getColumnCount(); i++) {
                 row.setField(i,record.get(i));

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/inputformat/DistributedJdbcInputFormat.java
Patch:
@@ -159,7 +159,7 @@ private boolean readNextRecord() throws IOException{
             if (hasNext){
                 currentRecord = new Row(columnCount);
                 DBUtil.getRow(databaseInterface.getDatabaseType(),currentRecord,descColumnTypeList,currentResultSet,typeConverter);
-                if(!metaColumns.get(0).getName().equals("*")){
+                if(!"*".equals(metaColumns.get(0).getName())){
                     for (int i = 0; i < columnCount; i++) {
                         Object val = currentRecord.getField(i);
                         if (val != null && val instanceof String){

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/inputformat/JdbcInputFormat.java
Patch:
@@ -179,7 +179,7 @@ public Row nextRecordInternal(Row row) throws IOException {
             }
 
             DBUtil.getRow(databaseInterface.getDatabaseType(),row,descColumnTypeList,resultSet,typeConverter);
-            if(!metaColumns.get(0).getName().equals("*")){
+            if(!"*".equals(metaColumns.get(0).getName())){
                 for (int i = 0; i < columnCount; i++) {
                     Object val = row.getField(i);
                     if (val != null && val instanceof String){

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/util/DBUtil.java
Patch:
@@ -347,7 +347,7 @@ public static String getQuerySql(DatabaseInterface databaseInterface,String tabl
         StringBuilder sb = new StringBuilder();
 
         List<String> selectColumns = new ArrayList<>();
-        if(metaColumns.size() == 1 && metaColumns.get(0).getName().equals("*")){
+        if(metaColumns.size() == 1 && "*".equals(metaColumns.get(0).getName())){
             selectColumns.add("*");
         } else {
             for (MetaColumn metaColumn : metaColumns) {

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/StringUtil.java
Patch:
@@ -115,7 +115,7 @@ public static Object string2col(String str, String type, FastDateFormat customTi
                 ret = DateUtil.columnToTimestamp(str,customTimeFormat);
                 break;
             default:
-                throw new IllegalArgumentException("Unsupported field type:" + type);
+                ret = str;
         }
 
         return ret;

File: flinkx-core/src/main/java/com/dtstack/flinkx/common/ColumnType.java
Patch:
@@ -27,7 +27,7 @@
  */
 public enum ColumnType {
     STRING, VARCHAR, CHAR,
-    INT, MEDIUMINT, TINYINT, DATETIME, SMALLINT, BIGINT,
+    INT, MEDIUMINT, TINYINT, DATETIME, SMALLINT, BIGINT,LONG,
     DOUBLE, FLOAT,
     BOOLEAN,
     DATE, TIMESTAMP, DECIMAL;

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/StringUtil.java
Patch:
@@ -129,6 +129,7 @@ public static String col2string(Object column, String type) {
                 result = Integer.valueOf(rowData);
                 break;
             case BIGINT:
+            case LONG:
                 result = Long.valueOf(rowData);
                 break;
             case FLOAT:

File: flinkx-carbondata/flinkx-carbondata-writer/src/main/java/com/dtstack/flinkx/carbondata/writer/CarbonOutputFormat.java
Patch:
@@ -242,10 +242,10 @@ protected void writeSingleRecordInternal(Row row) throws WriteRecordException {
                                 val = Boolean.valueOf(rowData);
                                 break;
                             case DATE:
-                                val = DateUtil.columnToDate(column);
+                                val = DateUtil.columnToDate(column,null);
                                 break;
                             case TIMESTAMP:
-                                val = DateUtil.columnToTimestamp(column);
+                                val = DateUtil.columnToTimestamp(column,null);
                                 break;
                             default:
                                 throw new IllegalArgumentException();

File: flinkx-core/src/main/java/com/dtstack/flinkx/outputformat/RichOutputFormat.java
Patch:
@@ -89,10 +89,10 @@ public abstract class RichOutputFormat extends org.apache.flink.api.common.io.Ri
     protected ErrorLimiter errorLimiter;
 
     /** 错误阈值 */
-    protected int errors;
+    protected Integer errors;
 
     /** 错误比例阈值 */
-    protected double errorRatio;
+    protected Double errorRatio;
 
     /** 任务名 */
     protected String jobName = "defaultJobName";
@@ -185,7 +185,7 @@ public void open(int taskNumber, int numTasks) throws IOException {
 
         //启动错误限制
         if(StringUtils.isNotBlank(monitorUrl)) {
-            if(errors > 0) {
+            if(errors != null || errorRatio != null) {
                 errorLimiter = new ErrorLimiter(context, monitorUrl, errors, errorRatio, 1);
                 errorLimiter.start();
             }

File: flinkx-core/src/main/java/com/dtstack/flinkx/outputformat/RichOutputFormatBuilder.java
Patch:
@@ -46,11 +46,11 @@ public void setSrcCols(List<String> srcCols) {
         format.setSrcFieldNames(srcCols);
     }
 
-    public void setErrors(int errors) {
+    public void setErrors(Integer errors) {
         format.errors = errors;
     }
 
-    public void setErrorRatio(double errorRatio) {
+    public void setErrorRatio(Double errorRatio) {
         format.errorRatio = errorRatio;
     }
 

File: flinkx-hdfs/flinkx-hdfs-reader/src/main/java/com/dtstack/flinkx/hdfs/reader/HdfsOrcInputFormat.java
Patch:
@@ -190,7 +190,7 @@ public Row nextRecordInternal(Row row) throws IOException {
                 if (col != null) {
                     col = HdfsUtil.getWritableValue(col);
                 }
-                row.setField(i, HdfsUtil.string2col(String.valueOf(col),type));
+                row.setField(i, col);
             } else if(val != null) {
                 Object col = HdfsUtil.string2col(val,type);
                 row.setField(i, col);

File: flinkx-mysql/flinkx-mysql-core/src/main/java/com/dtstack/flinkx/mysql/MySqlDatabaseMeta.java
Patch:
@@ -105,7 +105,7 @@ public String getMultiUpsertStatement(List<String> column, String table, int bat
 
     @Override
     public String getSplitFilter(String columnName) {
-        return String.format("%s mod ? = ?", getStartQuote() + columnName + getEndQuote());
+        return String.format("%s mod ${N} = ${M}", getStartQuote() + columnName + getEndQuote());
     }
 
     @Override

File: flinkx-oracle/flinkx-oracle-core/src/main/java/com/dtstack/flinkx/oracle/OracleDatabaseMeta.java
Patch:
@@ -59,7 +59,7 @@ public String getSQLQueryColumnFields(List<String> column, String table) {
 
     @Override
     public String getSplitFilter(String columnName) {
-        return String.format("mod(%s, ?) = ?", getStartQuote() + columnName + getEndQuote());
+        return String.format("mod(%s, ${N}) = ${M}", getStartQuote() + columnName + getEndQuote());
     }
 
     @Override

File: flinkx-postgresql/flinkx-postgresql-core/src/main/java/com/dtstack/flinkx/postgresql/PostgresqlDatabaseMeta.java
Patch:
@@ -116,7 +116,7 @@ public String getSQLQueryColumnFields(List<String> column, String table) {
 
     @Override
     public String getSplitFilter(String columnName) {
-        return String.format(" mod(%s,?) = ?", getStartQuote() + columnName + getEndQuote());
+        return String.format(" mod(%s,${N}) = ${M}", getStartQuote() + columnName + getEndQuote());
     }
 
     @Override

File: flinkx-sqlserver/flinkx-sqlserver-core/src/main/java/com/dtstack/flinkx/sqlserver/SqlServerDatabaseMeta.java
Patch:
@@ -52,7 +52,7 @@ public String getSQLQueryColumnFields(List<String> column, String table) {
 
     @Override
     public String getSplitFilter(String columnName) {
-        return String.format("%s %% ? = ?", getStartQuote() + columnName + getEndQuote());
+        return String.format("%s %% ${N} = ${M}", getStartQuote() + columnName + getEndQuote());
     }
 
     @Override

File: flinkx-sqlserver/flinkx-sqlserver-core/src/main/java/com/dtstack/flinkx/sqlserver/SqlServerDatabaseMeta.java
Patch:
@@ -37,7 +37,7 @@ public String getDatabaseType() {
 
     @Override
     public String getDriverClass() {
-        return "com.microsoft.sqlserver.jdbc.SQLServerDriver";
+        return "net.sourceforge.jtds.jdbc.Driver";
     }
 
     @Override

File: flinkx-launcher/src/main/java/com/dtstack/flinkx/launcher/LauncherOptionParser.java
Patch:
@@ -56,7 +56,7 @@ public class LauncherOptionParser {
 
     private BasicParser parser = new BasicParser();
 
-    private static LauncherOptions launcherOptions = new LauncherOptions();
+    private LauncherOptions launcherOptions = new LauncherOptions();
 
     public LauncherOptionParser(String[] args) {
         options.addOption(OPTION_MODE, true, "Running mode");
@@ -105,12 +105,12 @@ public LauncherOptionParser(String[] args) {
         }
     }
 
-    public static LauncherOptions getLauncherOptions(){
+    public LauncherOptions getLauncherOptions(){
         return launcherOptions;
     }
 
     private void printUsage() {
-
+        System.out.print(options.toString());
     }
 
 }

File: flinkx-launcher/src/main/java/com/dtstack/flinkx/launcher/LauncherOptions.java
Patch:
@@ -42,7 +42,7 @@ public class LauncherOptions {
 
       private int parallelism = 1;
 
-      private int priority = 1;
+    private int priority = 1;
 
       private String queue;
 

File: flinkx-hdfs/flinkx-hdfs-core/src/main/java/com/dtstack/flinkx/hdfs/HdfsUtil.java
Patch:
@@ -85,7 +85,7 @@ public static String getDefaultFs(){
     public static Object string2col(String str, String type) {
 
         Preconditions.checkNotNull(type);
-        ColumnType columnType = valueOf(type.toUpperCase());
+        ColumnType columnType = ColumnType.fromString(type.toUpperCase());
         Object ret;
         switch(columnType) {
             case TINYINT:
@@ -104,6 +104,7 @@ public static Object string2col(String str, String type) {
                 ret = Float.valueOf(str);
                 break;
             case DOUBLE:
+            case DECIMAL:
                 ret = Double.valueOf(str);
                 break;
             case STRING:

File: flinkx-hdfs/flinkx-hdfs-reader/src/main/java/com/dtstack/flinkx/hdfs/reader/HdfsOrcInputFormat.java
Patch:
@@ -184,18 +184,17 @@ public Row nextRecordInternal(Row row) throws IOException {
         for(int i = 0; i < columnIndex.size(); ++i) {
             Integer index = columnIndex.get(i);
             String val = columnValue.get(i);
+            String type = columnType.get(i);
             if(index != null) {
                 Object col = inspector.getStructFieldData(value, fields.get(index));
                 if (col != null) {
                     col = HdfsUtil.getWritableValue(col);
                 }
-                row.setField(i, col);
+                row.setField(i, HdfsUtil.string2col(String.valueOf(col),type));
             } else if(val != null) {
-                String type = columnType.get(i);
                 Object col = HdfsUtil.string2col(val,type);
                 row.setField(i, col);
             }
-
         }
         return row;
     }

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsTextOutputFormat.java
Patch:
@@ -106,7 +106,7 @@ public void writeSingleRecordInternal(Row row) throws WriteRecordException {
                         if (data.compareTo(new BigInteger(String.valueOf(Long.MAX_VALUE))) > 0){
                             sb.append(data);
                         } else {
-                            sb.append(rowData);
+                            sb.append(Long.valueOf(rowData));
                         }
                         break;
                     case FLOAT:

File: flinkx-hdfs/flinkx-hdfs-reader/src/main/java/com/dtstack/flinkx/hdfs/reader/HdfsTextInputFormat.java
Patch:
@@ -94,7 +94,7 @@ public Row nextRecordInternal(Row row) throws IOException {
                 if(index >= fields.length) {
                     row.setField(i, null);
                 } else {
-                    row.setField(i, fields[index]);
+                    row.setField(i, HdfsUtil.string2col(fields[index],columnType.get(i)));
                 }
             } else if(val != null) {
                 String type = columnType.get(i);

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsOrcOutputFormat.java
Patch:
@@ -38,6 +38,7 @@
 import org.apache.hadoop.mapred.Reporter;
 import java.io.IOException;
 import java.math.BigDecimal;
+import java.math.BigInteger;
 import java.util.ArrayList;
 import java.util.List;
 
@@ -121,7 +122,7 @@ public void writeSingleRecordInternal(Row row) throws WriteRecordException {
                         recordList.add(Integer.valueOf(rowData));
                         break;
                     case BIGINT:
-                        recordList.add(Long.valueOf(rowData));
+                        recordList.add(new BigInteger(rowData));
                         break;
                     case FLOAT:
                         recordList.add(Float.valueOf(rowData));

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HdfsTextOutputFormat.java
Patch:
@@ -29,6 +29,7 @@
 import java.io.IOException;
 import java.io.OutputStream;
 import java.math.BigDecimal;
+import java.math.BigInteger;
 import java.util.Date;
 
 /**
@@ -101,7 +102,7 @@ public void writeSingleRecordInternal(Row row) throws WriteRecordException {
                         sb.append(Integer.valueOf(rowData));
                         break;
                     case BIGINT:
-                        sb.append(Long.valueOf(rowData));
+                        sb.append(new BigInteger(rowData));
                         break;
                     case FLOAT:
                         sb.append(Float.valueOf(rowData));

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/util/DBUtil.java
Patch:
@@ -322,7 +322,8 @@ public static String getQuerySql(DatabaseInterface databaseInterface,String tabl
 
     public static String formatJdbcUrl(String pluginName,String dbUrl){
         if(pluginName.equalsIgnoreCase("mysqlreader")
-                || pluginName.equalsIgnoreCase("mysqldreader")) {
+                || pluginName.equalsIgnoreCase("mysqldreader")
+                || pluginName.equalsIgnoreCase("postgresqlreader")){
             String[] splits = dbUrl.split("\\?");
 
             Map<String,String> paramMap = new HashMap<String,String>();

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/outputformat/JdbcOutputFormat.java
Patch:
@@ -47,8 +47,6 @@ public class JdbcOutputFormat extends RichOutputFormat {
 
     protected static final long serialVersionUID = 1L;
 
-    protected static final Logger LOG = LoggerFactory.getLogger(JdbcOutputFormat.class);
-
     protected String username;
 
     protected String password;

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/util/DBUtil.java
Patch:
@@ -18,11 +18,11 @@
 package com.dtstack.flinkx.rdb.util;
 
 import com.dtstack.flinkx.rdb.DatabaseInterface;
+import com.dtstack.flinkx.rdb.ParameterValuesProvider;
 import com.dtstack.flinkx.rdb.type.TypeConverterInterface;
 import com.dtstack.flinkx.util.ClassUtil;
 import com.dtstack.flinkx.util.DateUtil;
 import com.dtstack.flinkx.util.SysUtil;
-import org.apache.flink.api.java.io.jdbc.split.ParameterValuesProvider;
 import org.apache.flink.types.Row;
 
 import java.io.Serializable;

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/inputformat/DistributedJdbcInputFormat.java
Patch:
@@ -187,7 +187,7 @@ public boolean reachedEnd() throws IOException {
                 closeCurrentSource();
                 openNextSource();
             }catch (SQLException e){
-                throw new IOException("open source error:" + currentSource.getJdbcUrl());
+                throw new IOException("open source error:" + currentSource.getJdbcUrl(),e);
             }
         }
 

File: flinkx-mysql/flinkx-mysql-dreader/src/main/java/com/dtstack/flinkx/mysqld/reader/MysqldReader.java
Patch:
@@ -1,13 +1,13 @@
-package com.dtstack.flinkx.mysql.dreader;
+package com.dtstack.flinkx.mysqld.reader;
 
 import com.dtstack.flinkx.config.DataTransferConfig;
 import com.dtstack.flinkx.mysql.MySqlDatabaseMeta;
 import com.dtstack.flinkx.rdb.datareader.DistributedJdbcDataReader;
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
 
-public class MysqlDReader extends DistributedJdbcDataReader {
+public class MysqldReader extends DistributedJdbcDataReader {
 
-    protected MysqlDReader(DataTransferConfig config, StreamExecutionEnvironment env) {
+    public MysqldReader(DataTransferConfig config, StreamExecutionEnvironment env) {
         super(config, env);
         setDatabaseInterface(new MySqlDatabaseMeta());
     }

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/inputformat/DistributedJdbcInputFormatBuilder.java
Patch:
@@ -1,6 +1,7 @@
 package com.dtstack.flinkx.rdb.inputformat;
 
 import com.dtstack.flinkx.inputformat.RichInputFormatBuilder;
+import com.dtstack.flinkx.rdb.DataSource;
 import com.dtstack.flinkx.rdb.DatabaseInterface;
 import com.dtstack.flinkx.rdb.type.TypeConverterInterface;
 
@@ -42,7 +43,7 @@ public void setSplitKey(String splitKey){
         format.splitKey = splitKey;
     }
 
-    public void setSourceList(List<DistributedJdbcInputSplit.DataSource> sourceList){
+    public void setSourceList(List<DataSource> sourceList){
         format.sourceList = sourceList;
     }
 

File: flinkx-redis/flinkx-redis-core/src/main/java/com/dtstack/flinkx/redis/JedisUtil.java
Patch:
@@ -45,7 +45,7 @@ public static Jedis getJedis(Properties properties){
 
             int timeOut = (Integer) properties.getOrDefault(KEY_TIMEOUT,TIMEOUT);
             String password = properties.getProperty(KEY_PASSWORD);
-            int db = Integer.parseInt(properties.getProperty(KEY_DB,DEFAULT_DB)) ;
+            int db = Integer.parseInt(properties.getOrDefault(KEY_DB,DEFAULT_DB).toString()) ;
 
             jedisPool = new JedisPool(getConfig(), host, Integer.valueOf(port),timeOut, password, db);
         }

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/util/DBUtil.java
Patch:
@@ -72,7 +72,7 @@ public static Connection getConnection(String url, String username, String passw
                         dbConn.close();
                     }
 
-                    if (i == MAX_RETRY_TIMES) {
+                    if (i == MAX_RETRY_TIMES - 1) {
                         throw e;
                     } else {
                         SysUtil.sleep(3000);

File: flinkx-postgresql/flinkx-postgresql-core/src/main/java/com/dtstack/flinkx/postgresql/PostgresqlDatabaseMeta.java
Patch:
@@ -4,6 +4,7 @@
 import org.apache.commons.lang3.StringUtils;
 
 import java.util.ArrayList;
+import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
 
@@ -39,7 +40,8 @@ private String makeUpdatePart (List<String> column) {
     }
 
     private String makeUpdateKey(Map<String,List<String>> updateKey){
-        return StringUtils.join(updateKey.get("postgresql_all_pkey"),",");
+        Iterator<Map.Entry<String,List<String>>> it = updateKey.entrySet().iterator();
+        return StringUtils.join(it.next().getValue(),",");
     }
 
     @Override

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/datareader/JdbcDataReader.java
Patch:
@@ -144,6 +144,7 @@ public DataStream<Row> readData() {
         builder.setTable(table);
         builder.setDatabaseInterface(databaseInterface);
         builder.setTypeConverter(typeConverter);
+        builder.setColumn(column);
 
         if(numPartitions > 1 && splitKey != null && splitKey.trim().length() != 0) {
             final int channels = numPartitions;

File: flinkx-postgresql/flinkx-postgresql-core/src/main/java/com/dtstack/flinkx/postgresql/PostgresqlTypeConverter.java
Patch:
@@ -21,6 +21,9 @@ public class PostgresqlTypeConverter implements TypeConverterInterface {
 
     @Override
     public Object convert(Object data,String typeName) {
+        if (data == null){
+            return null;
+        }
 
         if(doubleTypes.contains(typeName)){
             data = Double.parseDouble(String.valueOf(data));

File: flinkx-postgresql/flinkx-postgresql-core/src/main/java/com/dtstack/flinkx/postgresql/PostgresqlTypeConverter.java
Patch:
@@ -25,11 +25,11 @@ public Object convert(Object data,String typeName) {
         if(doubleTypes.contains(typeName)){
             data = Double.parseDouble(String.valueOf(data));
         } else if(bitTypes.contains(typeName)){
-            data = ((Boolean) data ? 1 : 0);
+            //
         } else if(stringTypes.contains(typeName)){
             data = String.valueOf(data);
         } else if(byteTypes.contains(typeName)){
-            // TODO support byte type later
+            data = Byte.valueOf(String.valueOf(data));
         }
 
         return data;

File: flinkx-postgresql/flinkx-postgresql-writer/src/main/java/com/dtstack/flinkx/postgresql/writer/PostgresqlWriter.java
Patch:
@@ -2,6 +2,7 @@
 
 import com.dtstack.flinkx.config.DataTransferConfig;
 import com.dtstack.flinkx.postgresql.PostgresqlDatabaseMeta;
+import com.dtstack.flinkx.postgresql.PostgresqlTypeConverter;
 import com.dtstack.flinkx.rdb.datawriter.JdbcDataWriter;
 
 /**
@@ -13,5 +14,6 @@ public class PostgresqlWriter extends JdbcDataWriter {
     public PostgresqlWriter(DataTransferConfig config) {
         super(config);
         setDatabaseInterface(new PostgresqlDatabaseMeta());
+        setTypeConverterInterface(new PostgresqlTypeConverter());
     }
 }

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/type/TypeConverterInterface.java
Patch:
@@ -1,10 +1,12 @@
 package com.dtstack.flinkx.rdb.type;
 
+import java.io.Serializable;
+
 /**
  * @author jiangbo
  * @date 2018/6/4 17:53
  */
-public interface TypeConverterInterface {
+public interface TypeConverterInterface extends Serializable {
 
     Object convert(Object data,String typeName);
 

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/util/DBUtil.java
Patch:
@@ -72,7 +72,7 @@ public static Connection getConnection(String url, String username, String passw
                         dbConn.close();
                     }
 
-                    if (i == MAX_RETRY_TIMES) {
+                    if (i == MAX_RETRY_TIMES - 1) {
                         throw e;
                     } else {
                         SysUtil.sleep(3000);

File: flinkx-core/src/main/java/org/apache/flink/runtime/metrics/groups/ComponentMetricGroup.java
Patch:
@@ -21,6 +21,7 @@
 import org.apache.flink.annotation.Internal;
 import org.apache.flink.runtime.metrics.MetricRegistry;
 
+import java.util.HashMap;
 import java.util.Map;
 import java.util.concurrent.ConcurrentHashMap;
 
@@ -73,6 +74,7 @@ public Map<String, String> getAllVariables() {
      *
      * @param variables map to enter variables and their values into
      */
+    @Override
     protected abstract void putVariables(Map<String, String> variables);
 
     /**

File: flinkx-mongodb/flinkx-mongodb-core/src/main/java/com/dtstack/flinkx/mongodb/MongodbUtil.java
Patch:
@@ -13,6 +13,8 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+import java.util.ArrayList;
+import java.util.Arrays;
 import java.util.List;
 import java.util.Map;
 import java.util.regex.Matcher;
@@ -136,7 +138,7 @@ public static Document convertRowToDoc(Row row,List<Column> columns) throws Writ
             Column column = columns.get(i);
             Object val = row.getField(i);
             if (StringUtils.isNotEmpty(column.getSplitter())){
-                val = String.valueOf(val).split(column.getSplitter());
+                val = Arrays.asList(String.valueOf(val).split(column.getSplitter()));
             }
 
             doc.append(column.getName(),val);

File: flinkx-mongodb/flinkx-mongodb-reader/src/main/java/com/dtstack/flinkx/mongodb/reader/MongodbInputFormatBuilder.java
Patch:
@@ -42,8 +42,8 @@ public void setColumns(List<Column> columns){
         format.columns = columns;
     }
 
-    public void setFilter(Map filter){
-        format.filterMap = filter;
+    public void setFilter(String filter){
+        format.filterJson = filter;
     }
 
     @Override

File: flinkx-redis/flinkx-redis-writer/src/main/java/com/dtstack/flinkx/redis/writer/RedisOutputFormat.java
Patch:
@@ -119,7 +119,9 @@ private List<Object> getFieldAndValue(Row row){
             values.add(row.getField(i));
         }
 
-        keyIndexes.forEach(i -> values.remove(i));
+        for (Integer keyIndex : keyIndexes) {
+            values.remove((int)keyIndex);
+        }
 
         return values;
     }

File: flinkx-redis/flinkx-redis-writer/src/main/java/com/dtstack/flinkx/redis/writer/RedisOutputFormatBuilder.java
Patch:
@@ -16,7 +16,7 @@ public class RedisOutputFormatBuilder extends RichOutputFormatBuilder {
     private RedisOutputFormat format;
 
     public RedisOutputFormatBuilder() {
-        format = new RedisOutputFormat();
+        super.format = format = new RedisOutputFormat();
     }
 
     public void setHostPort(String hostPort) {

File: flinkx-core/src/main/java/com/dtstack/flinkx/writer/ErrorLimiter.java
Patch:
@@ -117,9 +117,6 @@ public boolean isValid() {
     }
 
     public void start() {
-        if(scheduledExecutorService == null) {
-            return;
-        }
         scheduledExecutorService.scheduleAtFixedRate(
                 () -> {
                     Gson gson = new Gson();

File: flinkx-core/src/main/java/com/dtstack/flinkx/writer/ErrorLimiter.java
Patch:
@@ -117,6 +117,9 @@ public boolean isValid() {
     }
 
     public void start() {
+        if(scheduledExecutorService == null) {
+            return;
+        }
         scheduledExecutorService.scheduleAtFixedRate(
                 () -> {
                     Gson gson = new Gson();

File: flinkx-core/src/main/java/com/dtstack/flinkx/common/ColumnType.java
Patch:
@@ -27,7 +27,7 @@
  */
 public enum ColumnType {
     STRING, VARCHAR, CHAR,
-    INT, TINYINT, DATETIME, SMALLINT, BIGINT,
+    INT, MEDIUMINT, TINYINT, DATETIME, SMALLINT, BIGINT,
     DOUBLE, FLOAT,
     BOOLEAN,
     DATE, TIMESTAMP, DECIMAL;

File: flinkx-core/src/main/java/com/dtstack/flinkx/inputformat/RichInputFormat.java
Patch:
@@ -38,7 +38,7 @@
  *
  * 扩展了org.apache.flink.api.common.io.RichInputFormat, 因而可以通过{@link #getRuntimeContext()}获取运行时执行上下文
  * 自动完成
- * 用户只需覆盖openInternal等方法, 无需操心细节
+ * 用户只需覆盖openInternal氜chinput等方法, 无需操心细节
  *
  */
 public abstract class RichInputFormat extends org.apache.flink.api.common.io.RichInputFormat<Row, InputSplit> implements FinalizeOnMaster {

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/StringUtil.java
Patch:
@@ -82,6 +82,7 @@ public static Object string2col(String str, String type) {
             case INT:
                 ret = Integer.valueOf(str);
                 break;
+            case MEDIUMINT:
             case BIGINT:
                 ret = Long.valueOf(str);
                 break;

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/ClassUtil.java
Patch:
@@ -28,7 +28,7 @@ public class ClassUtil {
 
     public synchronized static void forName(String clazz, ClassLoader classLoader)  {
         try {
-            Class<?> driverClass = Class.forName(clazz);
+            Class<?> driverClass = Class.forName(clazz, true, classLoader);
             driverClass.newInstance();
         } catch (Exception e) {
             throw new RuntimeException(e);

File: flinkx-core/src/main/java/com/dtstack/flinkx/util/ClassUtil.java
Patch:
@@ -28,13 +28,14 @@ public class ClassUtil {
 
     public synchronized static void forName(String clazz, ClassLoader classLoader)  {
         try {
-            Class<?> driverClass = classLoader.loadClass(clazz);
+            Class<?> driverClass = Class.forName(clazz);
             driverClass.newInstance();
         } catch (Exception e) {
             throw new RuntimeException(e);
         }
     }
 
+
     public synchronized static void forName(String clazz) {
         try {
             Class<?> driverClass = Class.forName(clazz);

File: flinkx-odps/flinkx-odps-core/src/main/java/com/dtstack/flinkx/odps/OdpsUtil.java
Patch:
@@ -99,8 +99,8 @@ public static Odps initOdps(Map<String,String> odpsConfig) {
         }
 
         Odps odps = new Odps(account);
-        odps.getRestClient().setConnectTimeout(3);
-        odps.getRestClient().setReadTimeout(3);
+        odps.getRestClient().setConnectTimeout(10);
+        odps.getRestClient().setReadTimeout(60);
         odps.getRestClient().setRetryTimes(2);
         odps.setDefaultProject(defaultProject);
         odps.setEndpoint(odpsServer);

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/BaseDatabaseMeta.java
Patch:
@@ -172,7 +172,7 @@ protected String getUpdateSql(List<String> column, List<String> fullColumn, Stri
             if(fullColumn == null || column.contains(col)) {
                 list.add(prefixLeft + col + "=" + prefixRight + col);
             } else {
-                list.add("null");
+                list.add(prefixLeft + col + "=null");
             }
         }
         return StringUtils.join(list, ",");

File: flinkx-odps/flinkx-odps-reader/src/main/java/com/dtstack/flinkx/odps/reader/OdpsInputFormat.java
Patch:
@@ -91,7 +91,8 @@ public class OdpsInputFormat extends RichInputFormat {
     public void configure(Configuration configuration) {
         odps = OdpsUtil.initOdps(odpsConfig);
         table = OdpsUtil.getTable(odps, projectName, tableName);
-        isPartitioned = OdpsUtil.isPartitionedTable(table);
+        //isPartitioned = OdpsUtil.isPartitionedTable(table);
+        isPartitioned = StringUtils.isNotBlank(partition) ? true : false;
     }
 
     @Override

File: flinkx-es/flinkx-es-reader/src/main/java/com/dtstack/flinkx/es/reader/EsInputFormat.java
Patch:
@@ -97,6 +97,9 @@ public InputSplit[] createInputSplits(int splitNum) throws IOException {
         int lastSize = (int) (cnt - (splitNum - 1) * size);
         splitList.add(new EsInputSplit(lastFrom, lastSize));
 
+        if(client != null) {
+            client.close();
+        }
         return splitList.toArray(new EsInputSplit[splitNum]);
     }
 

File: flinkx-hbase/flinkx-hbase-reader/src/main/java/com/dtstack/flinkx/hbase/reader/HbaseReader.java
Patch:
@@ -51,7 +51,7 @@ public class HbaseReader extends DataReader {
     protected int scanCacheSize;
     protected int scanBatchSize;
 
-    protected HbaseReader(DataTransferConfig config, StreamExecutionEnvironment env) {
+    public HbaseReader(DataTransferConfig config, StreamExecutionEnvironment env) {
         super(config, env);
         ReaderConfig readerConfig = config.getJob().getContent().get(0).getReader();
         tableName = readerConfig.getParameter().getStringVal(HbaseConfigKeys.KEY_TABLE);

File: flinkx-hbase/flinkx-hbase-writer/src/main/java/com/dtstack/flinkx/hbase/writer/HbaseWriter.java
Patch:
@@ -21,6 +21,7 @@
 
 import com.dtstack.flinkx.config.DataTransferConfig;
 import com.dtstack.flinkx.config.WriterConfig;
+import com.dtstack.flinkx.util.ValueUtil;
 import com.dtstack.flinkx.writer.DataWriter;
 import org.apache.flink.streaming.api.datastream.DataStream;
 import org.apache.flink.streaming.api.datastream.DataStreamSink;
@@ -79,14 +80,15 @@ public HbaseWriter(DataTransferConfig config) {
             }
         }
 
+
         List rowkeyColumns = (List) writerConfig.getParameter().getVal(KEY_ROW_KEY_COLUMN);
         if(rowkeyColumns != null || rowkeyColumns.size() != 0) {
             rowkeyColumnIndices = new ArrayList<>();
             rowkeyColumnTypes = new ArrayList<>();
             rowkeyColumnValues = new ArrayList<>();
             for(int i = 0; i < rowkeyColumns.size(); ++i) {
                 Map<String,Object> sm = (Map) rowkeyColumns.get(i);
-                rowkeyColumnIndices.add(((Integer) sm.get(KEY_ROW_KEY_COLUMN_INDEX)).intValue());
+                rowkeyColumnIndices.add(ValueUtil.getInt(sm.get(KEY_ROW_KEY_COLUMN_INDEX)));
                 rowkeyColumnTypes.add((String) sm.get(KEY_ROW_KEY_COLUMN_TYPE));
                 rowkeyColumnValues.add((String) sm.get(KEY_ROW_KEY_COLUMN_VALUE));
             }

File: flinkx-core/src/main/java/com/dtstack/flinkx/inputformat/RichInputFormat.java
Patch:
@@ -91,6 +91,7 @@ public void close() throws IOException {
             byteRateLimiter.stop();
             byteRateLimiter = null;
         }
+        LOG.info("subtask input close finished");
     }
 
     protected abstract  void closeInternal() throws IOException;

File: flinkx-core/src/main/java/com/dtstack/flinkx/outputformat/RichOutputFormat.java
Patch:
@@ -311,6 +311,8 @@ public void close() throws IOException {
             errorLimiter.stop();
         }
 
+        LOG.info("subtask[" + taskNumber + "] close() finished");
+
     }
 
     public void closeInternal() throws IOException {

File: flinkx-hbase/flinkx-hbase-writer/src/main/java/com/dtstack/flinkx/hbase/writer/HbaseOutputFormat.java
Patch:
@@ -72,9 +72,9 @@ public class HbaseOutputFormat extends RichOutputFormat {
 
     protected String versionColumnValue;
 
-    private Connection connection;
+    private transient Connection connection;
 
-    private BufferedMutator bufferedMutator;
+    private transient BufferedMutator bufferedMutator;
 
     @Override
     public void configure(Configuration parameters) {

File: flinkx-hdfs/flinkx-hdfs-reader/src/main/java/com/dtstack/flinkx/hdfs/reader/HdfsReader.java
Patch:
@@ -30,8 +30,6 @@
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
-import java.util.regex.Matcher;
-import java.util.regex.Pattern;
 
 /**
  * The reader plugin of Hdfs

File: flinkx-rdb/src/main/java/com/dtstack/flinkx/rdb/inputformat/JdbcInputFormat.java
Patch:
@@ -43,7 +43,7 @@
 import com.dtstack.flinkx.inputformat.RichInputFormat;
 
 /**
- * InputFormat to read data from a database and generate Rows.
+ * InputFormat for reading data from a database and generate Rows.
  *
  * Company: www.dtstack.com
  * @author huyifan.zju@163.com

File: flinkx-ftp/flinkx-ftp-writer/src/main/java/com/dtstack/flinkx/ftp/writer/FtpOutputFormat.java
Patch:
@@ -76,9 +76,9 @@ public class FtpOutputFormat extends RichOutputFormat {
 
     protected List<String> columnNames;
 
-    private FtpHandler ftpHandler;
+    private transient FtpHandler ftpHandler;
 
-    private OutputStream os;
+    private transient OutputStream os;
 
     @Override
     public void configure(Configuration parameters) {

File: flinkx-hdfs/flinkx-hdfs-reader/src/main/java/com/dtstack/flinkx/hdfs/reader/HdfsInputFormat.java
Patch:
@@ -51,13 +51,13 @@ public abstract class HdfsInputFormat extends RichInputFormat {
 
     protected String delimiter;
 
-    protected RecordReader recordReader;
+    protected transient RecordReader recordReader;
 
     protected String charsetName = "UTF-8"; // 目前只支持UTF-8
 
-    protected JobConf conf;
+    protected transient JobConf conf;
 
-    protected org.apache.hadoop.mapred.InputFormat inputFormat;
+    protected transient org.apache.hadoop.mapred.InputFormat inputFormat;
 
     protected Object key;
 

File: flinkx-hdfs/flinkx-hdfs-writer/src/main/java/com/dtstack/flinkx/hdfs/writer/HiveUtil.java
Patch:
@@ -88,6 +88,9 @@ public static List<String> getPartitionCols(Connection dbConn, String table) thr
 
     public static void addPartitionsIfNotExists(Connection dbConn, String table, String partition) throws SQLException {
         List<String> partitionCols = getPartitionCols(dbConn, table);
+        if(partitionCols == null || partitionCols.size() == 0) {
+            return;
+        }
         List<String> partitions = showPartitions(dbConn, table);
         for(String part : partitions) {
             if(isSamePartition(part, partition)) {

File: flinkx-core/src/main/java/com/dtstack/flinkx/outputformat/RichOutputFormat.java
Patch:
@@ -307,6 +307,7 @@ public void close() throws IOException {
         }
 
         if(errorLimiter != null) {
+            errorLimiter.acquire();
             errorLimiter.stop();
         }
 

File: flinkx-ftp/flinkx-ftp-reader/src/main/java/com/dtstack/flinkx/ftp/reader/FtpInputFormat.java
Patch:
@@ -114,6 +114,8 @@ public InputSplitAssigner getInputSplitAssigner(InputSplit[] inputSplits) {
 
     @Override
     public void openInternal(InputSplit split) throws IOException {
+
+
         FtpInputSplit inputSplit = (FtpInputSplit)split;
         List<String> paths = inputSplit.getPaths();
         FtpSeqInputStream is = new FtpSeqInputStream(ftpHandler, paths);
@@ -135,7 +137,7 @@ public boolean reachedEnd() throws IOException {
     @Override
     public Row nextRecordInternal(Row row) throws IOException {
         row = new Row(columnIndex.size());
-        String fields[] = line.split(delimiter);
+        String[] fields = line.split(delimiter);
         for(int i = 0; i < columnIndex.size(); ++i) {
             Integer index = columnIndex.get(i);
             String val = columnValue.get(i);

