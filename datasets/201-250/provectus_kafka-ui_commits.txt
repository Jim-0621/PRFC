File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/rbac/extractor/CognitoAuthorityExtractor.java
Patch:
@@ -59,8 +59,8 @@ public Mono<Set<String>> extract(AccessControlService acs, Object value, Map<Str
             .stream()
             .filter(s -> s.getProvider().equals(Provider.OAUTH_COGNITO))
             .filter(s -> s.getType().equals("group"))
-            .anyMatch(subject -> Stream.of(groups)
-                .map(Object::toString)
+            .anyMatch(subject -> groups
+                .stream()
                 .anyMatch(cognitoGroup -> cognitoGroup.equals(subject.getValue()))
             ))
         .map(Role::getName)

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/config/CorsGlobalConfiguration.java
Patch:
@@ -7,8 +7,6 @@
 import org.springframework.http.HttpStatus;
 import org.springframework.http.server.reactive.ServerHttpRequest;
 import org.springframework.http.server.reactive.ServerHttpResponse;
-import org.springframework.web.reactive.config.CorsRegistry;
-import org.springframework.web.reactive.config.WebFluxConfigurer;
 import org.springframework.web.server.ServerWebExchange;
 import org.springframework.web.server.WebFilter;
 import org.springframework.web.server.WebFilterChain;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/config/WebclientProperties.java
Patch:
@@ -1,7 +1,6 @@
 package com.provectus.kafka.ui.config;
 
 import com.provectus.kafka.ui.exception.ValidationException;
-import java.beans.Transient;
 import javax.annotation.PostConstruct;
 import lombok.Data;
 import org.springframework.boot.context.properties.ConfigurationProperties;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/config/auth/AuthenticatedUser.java
Patch:
@@ -1,7 +1,6 @@
 package com.provectus.kafka.ui.config.auth;
 
 import java.util.Collection;
-import lombok.Value;
 
 public record AuthenticatedUser(String principal, Collection<String> groups) {
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/config/auth/RbacOAuth2User.java
Patch:
@@ -2,7 +2,6 @@
 
 import java.util.Collection;
 import java.util.Map;
-import lombok.Value;
 import org.springframework.security.core.GrantedAuthority;
 import org.springframework.security.oauth2.core.user.OAuth2User;
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/config/auth/RbacOidcUser.java
Patch:
@@ -2,7 +2,6 @@
 
 import java.util.Collection;
 import java.util.Map;
-import lombok.Value;
 import org.springframework.security.core.GrantedAuthority;
 import org.springframework.security.oauth2.core.oidc.OidcIdToken;
 import org.springframework.security.oauth2.core.oidc.OidcUserInfo;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/controller/ConsumerGroupsController.java
Patch:
@@ -22,7 +22,6 @@
 import java.util.Map;
 import java.util.Optional;
 import java.util.function.Supplier;
-import java.util.stream.Collectors;
 import lombok.RequiredArgsConstructor;
 import lombok.extern.slf4j.Slf4j;
 import org.springframework.beans.factory.annotation.Value;
@@ -200,7 +199,7 @@ private ConsumerGroupsPageResponseDTO convertPage(ConsumerGroupService.ConsumerG
         .consumerGroups(consumerGroupConsumerGroupsPage.consumerGroups()
             .stream()
             .map(ConsumerGroupMapper::toDto)
-            .collect(Collectors.toList()));
+            .toList());
   }
 
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/controller/SchemasController.java
Patch:
@@ -15,7 +15,6 @@
 import com.provectus.kafka.ui.service.SchemaRegistryService;
 import java.util.List;
 import java.util.Map;
-import java.util.stream.Collectors;
 import javax.validation.Valid;
 import lombok.RequiredArgsConstructor;
 import lombok.extern.slf4j.Slf4j;
@@ -235,7 +234,7 @@ public Mono<ResponseEntity<SchemaSubjectsResponseDTO>> getSchemas(String cluster
           List<String> subjectsToRender = filteredSubjects.stream()
               .skip(subjectToSkip)
               .limit(pageSize)
-              .collect(Collectors.toList());
+              .toList();
           return schemaRegistryService.getAllLatestVersionSchemas(getCluster(clusterName), subjectsToRender)
               .map(subjs -> subjs.stream().map(kafkaSrMapper::toDto).toList())
               .map(subjs -> new SchemaSubjectsResponseDTO().pageCount(totalPages).schemas(subjs));

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/controller/TopicsController.java
Patch:
@@ -143,7 +143,7 @@ public Mono<ResponseEntity<Flux<TopicConfigDTO>>> getTopicConfigs(
             .map(lst -> lst.stream()
                 .map(InternalTopicConfig::from)
                 .map(clusterMapper::toTopicConfig)
-                .collect(toList()))
+                .toList())
             .map(Flux::fromIterable)
             .map(ResponseEntity::ok)
     ).doOnEach(sig -> audit(context, sig));
@@ -207,7 +207,7 @@ public Mono<ResponseEntity<TopicsResponseDTO>> getTopics(String clusterName,
           return topicsService.loadTopics(getCluster(clusterName), topicsPage)
               .map(topicsToRender ->
                   new TopicsResponseDTO()
-                      .topics(topicsToRender.stream().map(clusterMapper::toTopic).collect(toList()))
+                      .topics(topicsToRender.stream().map(clusterMapper::toTopic).toList())
                       .pageCount(totalPages));
         })
         .map(ResponseEntity::ok)

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/emitter/OffsetsInfo.java
Patch:
@@ -5,7 +5,6 @@
 import java.util.HashSet;
 import java.util.Map;
 import java.util.Set;
-import java.util.stream.Collectors;
 import lombok.Getter;
 import lombok.extern.slf4j.Slf4j;
 import org.apache.commons.lang3.mutable.MutableLong;
@@ -28,7 +27,7 @@ class OffsetsInfo {
     this(consumer,
         consumer.partitionsFor(topic).stream()
             .map(pi -> new TopicPartition(topic, pi.partition()))
-            .collect(Collectors.toList())
+            .toList()
     );
   }
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/exception/GlobalErrorWebExceptionHandler.java
Patch:
@@ -106,7 +106,7 @@ private Mono<ServerResponse> render(WebExchangeBindException exception, ServerRe
           err.setFieldName(e.getKey());
           err.setRestrictions(List.copyOf(e.getValue()));
           return err;
-        }).collect(Collectors.toList());
+        }).toList();
 
     var message = fieldsErrors.isEmpty()
         ? exception.getMessage()

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/mapper/DescribeLogDirsMapper.java
Patch:
@@ -21,7 +21,7 @@ public List<BrokersLogdirsDTO> toBrokerLogDirsList(
     return logDirsInfo.entrySet().stream().map(
         mapEntry -> mapEntry.getValue().entrySet().stream()
             .map(e -> toBrokerLogDirs(mapEntry.getKey(), e.getKey(), e.getValue()))
-            .collect(Collectors.toList())
+            .toList()
     ).flatMap(Collection::stream).collect(Collectors.toList());
   }
 
@@ -35,7 +35,7 @@ private BrokersLogdirsDTO toBrokerLogDirs(Integer broker, String dirName,
     var topics = logDirInfo.replicaInfos.entrySet().stream()
         .collect(Collectors.groupingBy(e -> e.getKey().topic())).entrySet().stream()
         .map(e -> toTopicLogDirs(broker, e.getKey(), e.getValue()))
-        .collect(Collectors.toList());
+        .toList();
     result.setTopics(topics);
     return result;
   }
@@ -48,7 +48,7 @@ private BrokerTopicLogdirsDTO toTopicLogDirs(Integer broker, String name,
     topic.setPartitions(
         partitions.stream().map(
             e -> topicPartitionLogDir(
-                broker, e.getKey().partition(), e.getValue())).collect(Collectors.toList())
+                broker, e.getKey().partition(), e.getValue())).toList()
     );
     return topic;
   }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/model/InternalLogDirStats.java
Patch:
@@ -44,7 +44,7 @@ public InternalLogDirStats(Map<Integer, Map<String, DescribeLogDirsResponse.LogD
                 topicMap.getValue().replicaInfos.entrySet().stream()
                     .map(e -> Tuples.of(b.getKey(), e.getKey(), e.getValue().size))
             )
-        ).collect(toList());
+        ).toList();
 
     partitionsStats = topicPartitions.stream().collect(
         groupingBy(

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/serdes/builtin/Base64Serde.java
Patch:
@@ -6,7 +6,6 @@
 import java.util.Base64;
 import java.util.Map;
 import java.util.Optional;
-import org.apache.kafka.common.header.Headers;
 
 public class Base64Serde implements BuiltInSerde {
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/serdes/builtin/Int32Serde.java
Patch:
@@ -2,7 +2,6 @@
 
 import com.google.common.primitives.Ints;
 import com.provectus.kafka.ui.serde.api.DeserializeResult;
-import com.provectus.kafka.ui.serde.api.PropertyResolver;
 import com.provectus.kafka.ui.serde.api.SchemaDescription;
 import com.provectus.kafka.ui.serdes.BuiltInSerde;
 import java.util.Map;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/mapper/KafkaConnectMapper.java
Patch:
@@ -34,7 +34,7 @@ ConnectorPluginConfigValidationResponseDTO fromClient(
       com.provectus.kafka.ui.connect.model.ConnectorPluginConfigValidationResponse
           connectorPluginConfigValidationResponse);
 
-  default FullConnectorInfoDTO fullConnectorInfoFromTuple(InternalConnectInfo connectInfo) {
+  default FullConnectorInfoDTO fullConnectorInfo(InternalConnectInfo connectInfo) {
     ConnectorDTO connector = connectInfo.getConnector();
     List<TaskDTO> tasks = connectInfo.getTasks();
     int failedTasksCount = (int) tasks.stream()

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/integration/odd/ConnectorsExporter.java
Patch:
@@ -25,7 +25,7 @@ class ConnectorsExporter {
 
   Flux<DataEntityList> export(KafkaCluster cluster) {
     return kafkaConnectService.getConnects(cluster)
-        .flatMap(connect -> kafkaConnectService.getConnectorNames(cluster, connect.getName())
+        .flatMap(connect -> kafkaConnectService.getConnectorNamesWithErrorsSuppress(cluster, connect.getName())
             .flatMap(connectorName -> kafkaConnectService.getConnector(cluster, connect.getName(), connectorName))
             .flatMap(connectorDTO ->
                 kafkaConnectService.getConnectorTopics(cluster, connect.getName(), connectorDTO.getName())

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/AbstractIntegrationTest.java
Patch:
@@ -77,6 +77,8 @@ public void initialize(@NotNull ConfigurableApplicationContext context) {
       System.setProperty("kafka.clusters.0.kafkaConnect.0.userName", "kafka-connect");
       System.setProperty("kafka.clusters.0.kafkaConnect.0.password", "kafka-connect");
       System.setProperty("kafka.clusters.0.kafkaConnect.0.address", kafkaConnect.getTarget());
+      System.setProperty("kafka.clusters.0.kafkaConnect.1.name", "notavailable");
+      System.setProperty("kafka.clusters.0.kafkaConnect.1.address", "http://notavailable:6666");
       System.setProperty("kafka.clusters.0.masking.0.type", "REPLACE");
       System.setProperty("kafka.clusters.0.masking.0.replacement", "***");
       System.setProperty("kafka.clusters.0.masking.0.topicValuesPattern", "masking-test-.*");

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/service/integration/odd/ConnectorsExporterTest.java
Patch:
@@ -61,7 +61,7 @@ void exportsConnectorsAsDataTransformers() {
     when(kafkaConnectService.getConnects(CLUSTER))
         .thenReturn(Flux.just(connect));
 
-    when(kafkaConnectService.getConnectorNames(CLUSTER, connect.getName()))
+    when(kafkaConnectService.getConnectorNamesWithErrorsSuppress(CLUSTER, connect.getName()))
         .thenReturn(Flux.just(sinkConnector.getName(), sourceConnector.getName()));
 
     when(kafkaConnectService.getConnector(CLUSTER, connect.getName(), sinkConnector.getName()))

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/SchemaRegistryService.java
Patch:
@@ -125,7 +125,7 @@ public Mono<SubjectWithCompatibilityLevel> registerNewSchema(KafkaCluster cluste
         .onErrorMap(WebClientResponseException.Conflict.class,
             th -> new SchemaCompatibilityException())
         .onErrorMap(WebClientResponseException.UnprocessableEntity.class,
-            th -> new ValidationException("Invalid schema"))
+            th -> new ValidationException("Invalid schema. Error from registry: " + th.getResponseBodyAsString()))
         .then(getLatestSchemaVersionBySubject(cluster, subject));
   }
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/SchemaRegistryService.java
Patch:
@@ -14,8 +14,7 @@
 import com.provectus.kafka.ui.sr.model.NewSubject;
 import com.provectus.kafka.ui.sr.model.SchemaSubject;
 import com.provectus.kafka.ui.util.ReactiveFailover;
-import com.provectus.kafka.ui.util.WebClientConfigurator;
-import java.io.IOException;
+import java.nio.charset.Charset;
 import java.util.List;
 import java.util.stream.Collectors;
 import lombok.AllArgsConstructor;
@@ -92,7 +91,7 @@ public Mono<SubjectWithCompatibilityLevel> getLatestSchemaVersionBySubject(Kafka
   private Mono<SubjectWithCompatibilityLevel> getSchemaSubject(KafkaCluster cluster, String schemaName,
                                                                String version) {
     return api(cluster)
-        .mono(c -> c.getSubjectVersion(schemaName, version))
+        .mono(c -> c.getSubjectVersion(schemaName, version, false))
         .zipWith(getSchemaCompatibilityInfoOrGlobal(cluster, schemaName))
         .map(t -> new SubjectWithCompatibilityLevel(t.getT1(), t.getT2()))
         .onErrorResume(WebClientResponseException.NotFound.class, th -> Mono.error(new SchemaNotFoundException()));

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/integration/odd/schema/AvroExtractor.java
Patch:
@@ -1,7 +1,7 @@
 package com.provectus.kafka.ui.service.integration.odd.schema;
 
 import com.google.common.collect.ImmutableSet;
-import com.provectus.kafka.ui.sr.model.SchemaSubject;
+import io.confluent.kafka.schemaregistry.avro.AvroSchema;
 import java.util.ArrayList;
 import java.util.List;
 import org.apache.avro.Schema;
@@ -14,8 +14,8 @@ final class AvroExtractor {
   private AvroExtractor() {
   }
 
-  static List<DataSetField> extract(SchemaSubject subject, KafkaPath topicOddrn, boolean isKey) {
-    var schema = new Schema.Parser().parse(subject.getSchema());
+  static List<DataSetField> extract(AvroSchema avroSchema, KafkaPath topicOddrn, boolean isKey) {
+    var schema = avroSchema.rawSchema();
     List<DataSetField> result = new ArrayList<>();
     result.add(DataSetFieldsExtractors.rootField(topicOddrn, isKey));
     extract(

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/integration/odd/schema/JsonSchemaExtractor.java
Patch:
@@ -30,8 +30,8 @@ final class JsonSchemaExtractor {
   private JsonSchemaExtractor() {
   }
 
-  static List<DataSetField> extract(SchemaSubject subject, KafkaPath topicOddrn, boolean isKey) {
-    Schema schema = new JsonSchema(subject.getSchema()).rawSchema();
+  static List<DataSetField> extract(JsonSchema jsonSchema, KafkaPath topicOddrn, boolean isKey) {
+    Schema schema = jsonSchema.rawSchema();
     List<DataSetField> result = new ArrayList<>();
     result.add(DataSetFieldsExtractors.rootField(topicOddrn, isKey));
     extract(

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/integration/odd/schema/ProtoExtractor.java
Patch:
@@ -15,7 +15,6 @@
 import com.google.protobuf.UInt32Value;
 import com.google.protobuf.UInt64Value;
 import com.google.protobuf.Value;
-import com.provectus.kafka.ui.sr.model.SchemaSubject;
 import io.confluent.kafka.schemaregistry.protobuf.ProtobufSchema;
 import java.util.ArrayList;
 import java.util.List;
@@ -42,8 +41,8 @@ final class ProtoExtractor {
   private ProtoExtractor() {
   }
 
-  static List<DataSetField> extract(SchemaSubject subject, KafkaPath topicOddrn, boolean isKey) {
-    Descriptor schema = new ProtobufSchema(subject.getSchema()).toDescriptor();
+  static List<DataSetField> extract(ProtobufSchema protobufSchema, KafkaPath topicOddrn, boolean isKey) {
+    Descriptor schema = protobufSchema.toDescriptor();
     List<DataSetField> result = new ArrayList<>();
     result.add(DataSetFieldsExtractors.rootField(topicOddrn, isKey));
     var rootOddrn = topicOddrn.oddrn() + "/columns/" + (isKey ? "key" : "value");

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/service/integration/odd/schema/AvroExtractorTest.java
Patch:
@@ -2,7 +2,7 @@
 
 import static org.assertj.core.api.Assertions.assertThat;
 
-import com.provectus.kafka.ui.sr.model.SchemaSubject;
+import io.confluent.kafka.schemaregistry.avro.AvroSchema;
 import org.junit.jupiter.params.ParameterizedTest;
 import org.junit.jupiter.params.provider.ValueSource;
 import org.opendatadiscovery.client.model.DataSetField;
@@ -15,8 +15,7 @@ class AvroExtractorTest {
   @ValueSource(booleans = {true, false})
   void test(boolean isKey) {
     var list = AvroExtractor.extract(
-        new SchemaSubject()
-            .schema("""
+        new AvroSchema("""
                 {
                     "type": "record",
                     "name": "Message",

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/service/integration/odd/schema/JsonSchemaExtractorTest.java
Patch:
@@ -2,7 +2,7 @@
 
 import static org.assertj.core.api.Assertions.assertThat;
 
-import com.provectus.kafka.ui.sr.model.SchemaSubject;
+import io.confluent.kafka.schemaregistry.json.JsonSchema;
 import java.net.URI;
 import java.util.List;
 import java.util.Map;
@@ -40,7 +40,7 @@ void test(boolean isKey) {
         }
         """;
     var fields = JsonSchemaExtractor.extract(
-        new SchemaSubject().schema(jsonSchema),
+        new JsonSchema(jsonSchema),
         KafkaPath.builder()
             .cluster("localhost:9092")
             .topic("someTopic")

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/service/integration/odd/schema/ProtoExtractorTest.java
Patch:
@@ -2,7 +2,7 @@
 
 import static org.assertj.core.api.Assertions.assertThat;
 
-import com.provectus.kafka.ui.sr.model.SchemaSubject;
+import io.confluent.kafka.schemaregistry.protobuf.ProtobufSchema;
 import org.junit.jupiter.params.ParameterizedTest;
 import org.junit.jupiter.params.provider.ValueSource;
 import org.opendatadiscovery.client.model.DataSetField;
@@ -54,8 +54,7 @@ enum SampleEnum {
         }""";
 
     var list = ProtoExtractor.extract(
-        new SchemaSubject()
-            .schema(protoSchema),
+        new ProtobufSchema(protoSchema),
         KafkaPath.builder()
             .cluster("localhost:9092")
             .topic("someTopic")

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/controller/TopicsController.java
Patch:
@@ -167,12 +167,13 @@ public Mono<ResponseEntity<TopicsResponseDTO>> getTopics(String clusterName,
                                                            ServerWebExchange exchange) {
 
     return topicsService.getTopicsForPagination(getCluster(clusterName))
-        .flatMap(existingTopics -> {
+        .flatMap(topics -> accessControlService.filterViewableTopics(topics, clusterName))
+        .flatMap(topics -> {
           int pageSize = perPage != null && perPage > 0 ? perPage : DEFAULT_PAGE_SIZE;
           var topicsToSkip = ((page != null && page > 0 ? page : 1) - 1) * pageSize;
           var comparator = sortOrder == null || !sortOrder.equals(SortOrderDTO.DESC)
               ? getComparatorForTopic(orderBy) : getComparatorForTopic(orderBy).reversed();
-          List<InternalTopic> filtered = existingTopics.stream()
+          List<InternalTopic> filtered = topics.stream()
               .filter(topic -> !topic.isInternal()
                   || showInternal != null && showInternal)
               .filter(topic -> search == null || StringUtils.containsIgnoreCase(topic.getName(), search))
@@ -189,7 +190,6 @@ public Mono<ResponseEntity<TopicsResponseDTO>> getTopics(String clusterName,
 
           return topicsService.loadTopics(getCluster(clusterName), topicsPage)
               .flatMapMany(Flux::fromIterable)
-              .filterWhen(dto -> accessControlService.isTopicAccessible(dto, clusterName))
               .collectList()
               .map(topicsToRender ->
                   new TopicsResponseDTO()

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/util/AccessControlServiceMock.java
Patch:
@@ -5,6 +5,7 @@
 import static org.mockito.Mockito.when;
 
 import com.provectus.kafka.ui.service.rbac.AccessControlService;
+import java.util.Collections;
 import org.mockito.Mockito;
 import reactor.core.publisher.Mono;
 
@@ -16,7 +17,7 @@ public AccessControlService getMock() {
     when(mock.validateAccess(any())).thenReturn(Mono.empty());
     when(mock.isSchemaAccessible(anyString(), anyString())).thenReturn(Mono.just(true));
 
-    when(mock.isTopicAccessible(any(), anyString())).thenReturn(Mono.just(true));
+    when(mock.filterViewableTopics(any(), any())).then(invocation -> Mono.just(invocation.getArgument(0)));
 
     return mock;
   }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/model/InternalConsumerGroup.java
Patch:
@@ -74,14 +74,12 @@ private static Long calculateMessagesBehind(Map<TopicPartition, Long> offsets, M
 
   private static Integer calculateTopicNum(Map<TopicPartition, Long> offsets, Collection<InternalMember> members) {
 
-    long topicNum = Stream.concat(
+    return (int) Stream.concat(
         offsets.keySet().stream().map(TopicPartition::topic),
         members.stream()
             .flatMap(m -> m.getAssignment().stream().map(TopicPartition::topic))
     ).distinct().count();
 
-    return Integer.valueOf((int) topicNum);
-
   }
 
   private static Collection<InternalMember> initInternalMembers(ConsumerGroupDescription description) {

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/service/TopicsServicePaginationTest.java
Patch:
@@ -131,7 +131,7 @@ public void shouldCalculateCorrectPageCountForNonDivisiblePageSize() {
 
     assertThat(topics.getBody().getPageCount()).isEqualTo(4);
     assertThat(topics.getBody().getTopics()).hasSize(1);
-    assertThat(topics.getBody().getTopics().get(0).getName().equals("99"));
+    assertThat(topics.getBody().getTopics().get(0).getName()).isEqualTo("99");
   }
 
   @Test

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/schemas/SchemaDetails.java
Patch:
@@ -13,7 +13,7 @@ public class SchemaDetails extends BasePage {
   protected SelenideElement compatibilityField = $x("//h4[contains(text(),'Compatibility')]/../p");
   protected SelenideElement editSchemaBtn = $x("//button[contains(text(),'Edit Schema')]");
   protected SelenideElement removeBtn = $x("//*[contains(text(),'Remove')]");
-  protected SelenideElement confirmBtn = $x("//div[@role='dialog']//button[contains(text(),'Confirm')]");
+  protected SelenideElement schemaConfirmBtn = $x("//div[@role='dialog']//button[contains(text(),'Confirm')]");
   protected SelenideElement schemaTypeField = $x("//h4[contains(text(),'Type')]/../p");
   protected SelenideElement latestVersionField = $x("//h4[contains(text(),'Latest version')]/../p");
   protected SelenideElement compareVersionBtn = $x("//button[text()='Compare Versions']");
@@ -62,8 +62,8 @@ public SchemaDetails openCompareVersionMenu() {
   public SchemaDetails removeSchema() {
     clickByJavaScript(dotMenuBtn);
     removeBtn.shouldBe(Condition.enabled).click();
-    confirmBtn.shouldBe(Condition.visible).click();
-    confirmBtn.shouldBe(Condition.disappear);
+    schemaConfirmBtn.shouldBe(Condition.visible).click();
+    schemaConfirmBtn.shouldBe(Condition.disappear);
     return this;
   }
 }

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/topics/ProduceMessagePanel.java
Patch:
@@ -14,7 +14,7 @@ public class ProduceMessagePanel extends BasePage {
   protected SelenideElement keyTextArea = $x("//div[@id='key']/textarea");
   protected SelenideElement valueTextArea = $x("//div[@id='content']/textarea");
   protected SelenideElement headersTextArea = $x("//div[@id='headers']/textarea");
-  protected SelenideElement submitBtn = headersTextArea.$x("../../../..//button[@type='submit']");
+  protected SelenideElement submitProduceMessageBtn = headersTextArea.$x("../../../..//button[@type='submit']");
   protected SelenideElement partitionDdl = $x("//ul[@name='partition']");
   protected SelenideElement keySerdeDdl = $x("//ul[@name='keySerde']");
   protected SelenideElement contentSerdeDdl = $x("//ul[@name='valueSerde']");
@@ -48,8 +48,8 @@ public ProduceMessagePanel setHeadersFld(String value) {
 
   @Step
   public ProduceMessagePanel submitProduceMessage() {
-    clickByActions(submitBtn);
-    submitBtn.shouldBe(Condition.disappear);
+    clickByActions(submitProduceMessageBtn);
+    submitProduceMessageBtn.shouldBe(Condition.disappear);
     refresh();
     return this;
   }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/model/rbac/Permission.java
Patch:
@@ -1,5 +1,6 @@
 package com.provectus.kafka.ui.model.rbac;
 
+import static com.provectus.kafka.ui.model.rbac.Resource.ACL;
 import static com.provectus.kafka.ui.model.rbac.Resource.APPLICATIONCONFIG;
 import static com.provectus.kafka.ui.model.rbac.Resource.CLUSTERCONFIG;
 import static com.provectus.kafka.ui.model.rbac.Resource.KSQL;
@@ -27,7 +28,7 @@
 @EqualsAndHashCode
 public class Permission {
 
-  private static final List<Resource> RBAC_ACTION_EXEMPT_LIST = List.of(KSQL, CLUSTERCONFIG, APPLICATIONCONFIG);
+  private static final List<Resource> RBAC_ACTION_EXEMPT_LIST = List.of(KSQL, CLUSTERCONFIG, APPLICATIONCONFIG, ACL);
 
   Resource resource;
   List<String> actions;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/acl/AclsService.java
Patch:
@@ -3,6 +3,7 @@
 import com.google.common.collect.Sets;
 import com.provectus.kafka.ui.model.KafkaCluster;
 import com.provectus.kafka.ui.service.AdminClientService;
+import java.util.Comparator;
 import java.util.List;
 import java.util.Set;
 import lombok.RequiredArgsConstructor;
@@ -39,7 +40,8 @@ public Mono<Void> deleteAcl(KafkaCluster cluster, AclBinding aclBinding) {
   public Flux<AclBinding> listAcls(KafkaCluster cluster, ResourcePatternFilter filter) {
     return adminClientService.get(cluster)
         .flatMap(c -> c.listAcls(filter))
-        .flatMapIterable(acls -> acls);
+        .flatMapIterable(acls -> acls)
+        .sort(Comparator.comparing(AclBinding::toString));  //sorting to keep stable order on different calls
   }
 
   public Mono<String> getAclAsCsvString(KafkaCluster cluster) {

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/serdes/ConsumerRecordDeserializer.java
Patch:
@@ -123,11 +123,11 @@ private static Long getHeadersSize(ConsumerRecord<Bytes, Bytes> consumerRecord)
   }
 
   private static Long getKeySize(ConsumerRecord<Bytes, Bytes> consumerRecord) {
-    return consumerRecord.key() != null ? (long) consumerRecord.key().get().length : null;
+    return consumerRecord.key() != null ? (long) consumerRecord.serializedKeySize() : null;
   }
 
   private static Long getValueSize(ConsumerRecord<Bytes, Bytes> consumerRecord) {
-    return consumerRecord.value() != null ? (long) consumerRecord.value().get().length : null;
+    return consumerRecord.value() != null ? (long) consumerRecord.serializedValueSize() : null;
   }
 
   private static int headerSize(Header header) {

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/serdes/SerdesInitializer.java
Patch:
@@ -122,8 +122,6 @@ public ClusterSerdes init(Environment env,
         registeredSerdes,
         Optional.ofNullable(clusterProperties.getDefaultKeySerde())
             .map(name -> Preconditions.checkNotNull(registeredSerdes.get(name), "Default key serde not found"))
-            .or(() -> Optional.ofNullable(registeredSerdes.get(SchemaRegistrySerde.name())))
-            .or(() -> Optional.ofNullable(registeredSerdes.get(ProtobufFileSerde.name())))
             .orElse(null),
         Optional.ofNullable(clusterProperties.getDefaultValueSerde())
             .map(name -> Preconditions.checkNotNull(registeredSerdes.get(name), "Default value serde not found"))

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/KafkaConnectService.java
Patch:
@@ -109,6 +109,7 @@ private Predicate<FullConnectorInfoDTO> matchesSearchTerm(@Nullable final String
   private Stream<String> getStringsForSearch(FullConnectorInfoDTO fullConnectorInfo) {
     return Stream.of(
         fullConnectorInfo.getName(),
+        fullConnectorInfo.getConnect(),
         fullConnectorInfo.getStatus().getState().getValue(),
         fullConnectorInfo.getType().getValue());
   }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/util/jsonschema/AnyFieldSchema.java
Patch:
@@ -4,9 +4,9 @@
 import com.fasterxml.jackson.databind.ObjectMapper;
 
 // Specifies field that can contain any kind of value - primitive, complex and nulls
-public class AnyFieldSchema implements FieldSchema {
+class AnyFieldSchema implements FieldSchema {
 
-  public static AnyFieldSchema get() {
+  static AnyFieldSchema get() {
     return new AnyFieldSchema();
   }
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/util/jsonschema/ArrayFieldSchema.java
Patch:
@@ -4,10 +4,10 @@
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.fasterxml.jackson.databind.node.ObjectNode;
 
-public class ArrayFieldSchema implements FieldSchema {
+class ArrayFieldSchema implements FieldSchema {
   private final FieldSchema itemsSchema;
 
-  public ArrayFieldSchema(FieldSchema itemsSchema) {
+  ArrayFieldSchema(FieldSchema itemsSchema) {
     this.itemsSchema = itemsSchema;
   }
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/util/jsonschema/EnumJsonType.java
Patch:
@@ -7,10 +7,10 @@
 import java.util.Map;
 
 
-public class EnumJsonType extends JsonType {
+class EnumJsonType extends JsonType {
   private final List<String> values;
 
-  public EnumJsonType(List<String> values) {
+  EnumJsonType(List<String> values) {
     super(Type.ENUM);
     this.values = values;
   }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/util/jsonschema/FieldSchema.java
Patch:
@@ -3,6 +3,6 @@
 import com.fasterxml.jackson.databind.JsonNode;
 import com.fasterxml.jackson.databind.ObjectMapper;
 
-public interface FieldSchema {
+interface FieldSchema {
   JsonNode toJsonNode(ObjectMapper mapper);
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/util/jsonschema/OneOfFieldSchema.java
Patch:
@@ -5,11 +5,10 @@
 import java.util.List;
 import java.util.stream.Collectors;
 
-public class OneOfFieldSchema implements FieldSchema {
+class OneOfFieldSchema implements FieldSchema {
   private final List<FieldSchema> schemaList;
 
-  public OneOfFieldSchema(
-      List<FieldSchema> schemaList) {
+  OneOfFieldSchema(List<FieldSchema> schemaList) {
     this.schemaList = schemaList;
   }
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/util/jsonschema/RefFieldSchema.java
Patch:
@@ -4,10 +4,10 @@
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.fasterxml.jackson.databind.node.TextNode;
 
-public class RefFieldSchema implements FieldSchema {
+class RefFieldSchema implements FieldSchema {
   private final String ref;
 
-  public RefFieldSchema(String ref) {
+  RefFieldSchema(String ref) {
     this.ref = ref;
   }
 
@@ -16,7 +16,7 @@ public JsonNode toJsonNode(ObjectMapper mapper) {
     return mapper.createObjectNode().set("$ref", new TextNode(ref));
   }
 
-  public String getRef() {
+  String getRef() {
     return ref;
   }
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/util/jsonschema/SimpleFieldSchema.java
Patch:
@@ -3,10 +3,10 @@
 import com.fasterxml.jackson.databind.JsonNode;
 import com.fasterxml.jackson.databind.ObjectMapper;
 
-public class SimpleFieldSchema implements FieldSchema {
+class SimpleFieldSchema implements FieldSchema {
   private final JsonType type;
 
-  public SimpleFieldSchema(JsonType type) {
+  SimpleFieldSchema(JsonType type) {
     this.type = type;
   }
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/util/jsonschema/SimpleJsonType.java
Patch:
@@ -6,15 +6,15 @@
 import com.google.common.collect.ImmutableMap;
 import java.util.Map;
 
-public class SimpleJsonType extends JsonType {
+class SimpleJsonType extends JsonType {
 
   private final Map<String, JsonNode> additionalTypeProperties;
 
-  public SimpleJsonType(Type type) {
+  SimpleJsonType(Type type) {
     this(type, Map.of());
   }
 
-  public SimpleJsonType(Type type, Map<String, JsonNode> additionalTypeProperties) {
+  SimpleJsonType(Type type, Map<String, JsonNode> additionalTypeProperties) {
     super(type);
     this.additionalTypeProperties = additionalTypeProperties;
   }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/config/ClustersProperties.java
Patch:
@@ -27,6 +27,8 @@ public class ClustersProperties {
 
   String internalTopicPrefix;
 
+  Integer adminClientTimeout;
+
   PollingProperties polling = new PollingProperties();
 
   @Data

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/models/Connector.java
Patch:
@@ -7,5 +7,5 @@
 @Accessors(chain = true)
 public class Connector {
 
-    private String name, config;
+  private String name, config;
 }

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/ksqldb/models/Stream.java
Patch:
@@ -1,4 +1,4 @@
-package com.provectus.kafka.ui.pages.ksqlDb.models;
+package com.provectus.kafka.ui.pages.ksqldb.models;
 
 import lombok.Data;
 import lombok.experimental.Accessors;
@@ -7,5 +7,5 @@
 @Accessors(chain = true)
 public class Stream {
 
-    private String name, topicName, valueFormat, partitions;
+  private String name, topicName, valueFormat, partitions;
 }

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/ksqldb/models/Table.java
Patch:
@@ -1,4 +1,4 @@
-package com.provectus.kafka.ui.pages.ksqlDb.models;
+package com.provectus.kafka.ui.pages.ksqldb.models;
 
 import lombok.Data;
 import lombok.experimental.Accessors;
@@ -7,5 +7,5 @@
 @Accessors(chain = true)
 public class Table {
 
-    private String name, streamName;
+  private String name, streamName;
 }

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/utilities/qase/annotations/Status.java
Patch:
@@ -1,4 +1,4 @@
-package com.provectus.kafka.ui.utilities.qaseUtils.annotations;
+package com.provectus.kafka.ui.utilities.qase.annotations;
 
 import java.lang.annotation.ElementType;
 import java.lang.annotation.Retention;
@@ -9,5 +9,5 @@
 @Retention(RetentionPolicy.RUNTIME)
 public @interface Status {
 
-    com.provectus.kafka.ui.utilities.qaseUtils.enums.Status status();
+  com.provectus.kafka.ui.utilities.qase.enums.Status status();
 }

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/utilities/qase/annotations/Suite.java
Patch:
@@ -1,4 +1,4 @@
-package com.provectus.kafka.ui.utilities.qaseUtils.annotations;
+package com.provectus.kafka.ui.utilities.qase.annotations;
 
 import java.lang.annotation.ElementType;
 import java.lang.annotation.Retention;
@@ -9,5 +9,5 @@
 @Retention(RetentionPolicy.RUNTIME)
 public @interface Suite {
 
-    long id();
+  long id();
 }

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/variables/Browser.java
Patch:
@@ -2,6 +2,6 @@
 
 public interface Browser {
 
-    String CONTAINER = "container";
-    String LOCAL = "local";
+  String CONTAINER = "container";
+  String LOCAL = "local";
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/config/ClustersProperties.java
Patch:
@@ -1,14 +1,14 @@
 package com.provectus.kafka.ui.config;
 
 import com.provectus.kafka.ui.model.MetricsConfig;
+import jakarta.annotation.PostConstruct;
 import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
 import javax.annotation.Nullable;
-import javax.annotation.PostConstruct;
 import lombok.AllArgsConstructor;
 import lombok.Builder;
 import lombok.Data;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/config/auth/OAuthProperties.java
Patch:
@@ -1,9 +1,9 @@
 package com.provectus.kafka.ui.config.auth;
 
+import jakarta.annotation.PostConstruct;
 import java.util.HashMap;
 import java.util.Map;
 import java.util.Set;
-import javax.annotation.PostConstruct;
 import lombok.Data;
 import org.springframework.boot.context.properties.ConfigurationProperties;
 import org.springframework.util.Assert;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/exception/GlobalErrorWebExceptionHandler.java
Patch:
@@ -134,7 +134,7 @@ private Mono<ServerResponse> render(ResponseStatusException exception, ServerReq
         .timestamp(currentTimestamp())
         .stackTrace(Throwables.getStackTraceAsString(exception));
     return ServerResponse
-        .status(exception.getStatus())
+        .status(exception.getStatusCode())
         .contentType(MediaType.APPLICATION_JSON)
         .bodyValue(response);
   }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/analyze/TopicAnalysisStats.java
Patch:
@@ -2,7 +2,7 @@
 
 import com.provectus.kafka.ui.model.TopicAnalysisSizeStatsDTO;
 import com.provectus.kafka.ui.model.TopicAnalysisStatsDTO;
-import com.provectus.kafka.ui.model.TopicAnalysisStatsHourlyMsgCountsDTO;
+import com.provectus.kafka.ui.model.TopicAnalysisStatsHourlyMsgCountsInnerDTO;
 import java.time.Duration;
 import java.time.Instant;
 import java.util.Comparator;
@@ -78,10 +78,10 @@ void apply(ConsumerRecord<?, ?> rec) {
       }
     }
 
-    List<TopicAnalysisStatsHourlyMsgCountsDTO> toDto() {
+    List<TopicAnalysisStatsHourlyMsgCountsInnerDTO> toDto() {
       return hourlyStats.entrySet().stream()
           .sorted(Comparator.comparingLong(Map.Entry::getKey))
-          .map(e -> new TopicAnalysisStatsHourlyMsgCountsDTO()
+          .map(e -> new TopicAnalysisStatsHourlyMsgCountsInnerDTO()
               .hourStart(e.getKey())
               .count(e.getValue()))
           .collect(Collectors.toList());

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/rbac/AccessControlService.java
Patch:
@@ -21,14 +21,14 @@
 import com.provectus.kafka.ui.service.rbac.extractor.GoogleAuthorityExtractor;
 import com.provectus.kafka.ui.service.rbac.extractor.LdapAuthorityExtractor;
 import com.provectus.kafka.ui.service.rbac.extractor.ProviderAuthorityExtractor;
+import jakarta.annotation.PostConstruct;
 import java.util.Collections;
 import java.util.List;
 import java.util.Set;
 import java.util.function.Predicate;
 import java.util.regex.Pattern;
 import java.util.stream.Collectors;
 import javax.annotation.Nullable;
-import javax.annotation.PostConstruct;
 import lombok.RequiredArgsConstructor;
 import lombok.extern.slf4j.Slf4j;
 import org.apache.commons.collections.CollectionUtils;

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/AbstractIntegrationTest.java
Patch:
@@ -16,7 +16,7 @@
 import org.springframework.context.ConfigurableApplicationContext;
 import org.springframework.test.context.ActiveProfiles;
 import org.springframework.test.context.ContextConfiguration;
-import org.springframework.util.SocketUtils;
+import org.springframework.test.util.TestSocketUtils;
 import org.testcontainers.containers.KafkaContainer;
 import org.testcontainers.containers.Network;
 import org.testcontainers.utility.DockerImageName;
@@ -61,7 +61,7 @@ public void initialize(@NotNull ConfigurableApplicationContext context) {
       System.setProperty("kafka.clusters.0.bootstrapServers", kafka.getBootstrapServers());
       // List unavailable hosts to verify failover
       System.setProperty("kafka.clusters.0.schemaRegistry", String.format("http://localhost:%1$s,http://localhost:%1$s,%2$s",
-              SocketUtils.findAvailableTcpPort(), schemaRegistry.getUrl()));
+              TestSocketUtils.findAvailableTcpPort(), schemaRegistry.getUrl()));
       System.setProperty("kafka.clusters.0.kafkaConnect.0.name", "kafka-connect");
       System.setProperty("kafka.clusters.0.kafkaConnect.0.userName", "kafka-connect");
       System.setProperty("kafka.clusters.0.kafkaConnect.0.password", "kafka-connect");

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/service/ksql/KsqlServiceV2Test.java
Patch:
@@ -15,7 +15,6 @@
 import org.junit.jupiter.api.AfterAll;
 import org.junit.jupiter.api.BeforeAll;
 import org.junit.jupiter.api.Test;
-import org.springframework.util.unit.DataSize;
 import org.testcontainers.utility.DockerImageName;
 
 class KsqlServiceV2Test extends AbstractIntegrationTest {
@@ -27,8 +26,6 @@ class KsqlServiceV2Test extends AbstractIntegrationTest {
   private static final Set<String> STREAMS_TO_DELETE = new CopyOnWriteArraySet<>();
   private static final Set<String> TABLES_TO_DELETE = new CopyOnWriteArraySet<>();
 
-  private static final DataSize maxBuffSize = DataSize.ofMegabytes(20);
-
   @BeforeAll
   static void init() {
     KSQL_DB.start();

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/emitter/TailingEmitter.java
Patch:
@@ -2,7 +2,6 @@
 
 import com.provectus.kafka.ui.model.ConsumerPosition;
 import com.provectus.kafka.ui.model.TopicMessageEventDTO;
-import com.provectus.kafka.ui.serdes.ConsumerRecordDeserializer;
 import java.util.HashMap;
 import java.util.function.Supplier;
 import lombok.extern.slf4j.Slf4j;
@@ -20,9 +19,9 @@ public class TailingEmitter extends AbstractEmitter
 
   public TailingEmitter(Supplier<KafkaConsumer<Bytes, Bytes>> consumerSupplier,
                         ConsumerPosition consumerPosition,
-                        ConsumerRecordDeserializer recordDeserializer,
+                        MessagesProcessing messagesProcessing,
                         PollingSettings pollingSettings) {
-    super(recordDeserializer, pollingSettings);
+    super(messagesProcessing, pollingSettings);
     this.consumerSupplier = consumerSupplier;
     this.consumerPosition = consumerPosition;
   }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/StatisticsService.java
Patch:
@@ -41,7 +41,7 @@ private Mono<Statistics> getStatistics(KafkaCluster cluster) {
                     List.of(
                         metricsCollector.getBrokerMetrics(cluster, description.getNodes()),
                         getLogDirInfo(description, ac),
-                        featureService.getAvailableFeatures(cluster, description.getController()),
+                        featureService.getAvailableFeatures(cluster, description),
                         loadTopicConfigs(cluster),
                         describeTopics(cluster)),
                     results ->

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/ksqlDb/KsqlQueryForm.java
Patch:
@@ -16,7 +16,6 @@
 import static com.codeborne.selenide.Selenide.$x;
 
 public class KsqlQueryForm extends BasePage {
-    protected SelenideElement pageTitle = $x("//h1[text()='Query']");
     protected SelenideElement clearBtn = $x("//div/button[text()='Clear']");
     protected SelenideElement executeBtn = $x("//div/button[text()='Execute']");
     protected SelenideElement stopQueryBtn = $x("//div/button[text()='Stop query']");
@@ -31,7 +30,7 @@ public class KsqlQueryForm extends BasePage {
     @Step
     public KsqlQueryForm waitUntilScreenReady() {
         waitUntilSpinnerDisappear();
-        pageTitle.shouldBe(Condition.visible);
+        executeBtn.shouldBe(Condition.visible);
         return this;
     }
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/config/ClustersProperties.java
Patch:
@@ -25,6 +25,8 @@ public class ClustersProperties {
 
   List<Cluster> clusters = new ArrayList<>();
 
+  String internalTopicPrefix;
+
   @Data
   public static class Cluster {
     String name;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/config/auth/OAuthProperties.java
Patch:
@@ -1,7 +1,6 @@
 package com.provectus.kafka.ui.config.auth;
 
 import java.util.HashMap;
-import java.util.HashSet;
 import java.util.Map;
 import java.util.Set;
 import javax.annotation.PostConstruct;
@@ -32,13 +31,13 @@ public static class OAuth2Provider {
     private String clientName;
     private String redirectUri;
     private String authorizationGrantType;
-    private Set<String> scope = new HashSet<>();
+    private Set<String> scope;
     private String issuerUri;
     private String authorizationUri;
     private String tokenUri;
     private String userInfoUri;
     private String jwkSetUri;
     private String userNameAttribute;
-    private Map<String, String> customParams = new HashMap<>();
+    private Map<String, String> customParams;
   }
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/controller/AccessController.java
Patch:
@@ -66,7 +66,7 @@ private List<UserPermissionDTO> mapPermissions(List<Permission> permissions, Lis
           UserPermissionDTO dto = new UserPermissionDTO();
           dto.setClusters(clusters);
           dto.setResource(ResourceTypeDTO.fromValue(permission.getResource().toString().toUpperCase()));
-          dto.setValue(permission.getValue() != null ? permission.getValue().toString() : null);
+          dto.setValue(permission.getValue());
           dto.setActions(permission.getActions()
               .stream()
               .map(String::toUpperCase)

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/exception/ErrorCode.java
Patch:
@@ -29,7 +29,9 @@ public enum ErrorCode {
   RECREATE_TOPIC_TIMEOUT(4015, HttpStatus.REQUEST_TIMEOUT),
   INVALID_ENTITY_STATE(4016, HttpStatus.BAD_REQUEST),
   SCHEMA_NOT_DELETED(4017, HttpStatus.INTERNAL_SERVER_ERROR),
-  TOPIC_ANALYSIS_ERROR(4018, HttpStatus.BAD_REQUEST);
+  TOPIC_ANALYSIS_ERROR(4018, HttpStatus.BAD_REQUEST),
+  FILE_UPLOAD_EXCEPTION(4019, HttpStatus.INTERNAL_SERVER_ERROR),
+  ;
 
   static {
     // codes uniqueness check

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/mapper/ClusterMapper.java
Patch:
@@ -6,12 +6,12 @@
 import com.provectus.kafka.ui.model.BrokerDiskUsageDTO;
 import com.provectus.kafka.ui.model.BrokerMetricsDTO;
 import com.provectus.kafka.ui.model.ClusterDTO;
+import com.provectus.kafka.ui.model.ClusterFeature;
 import com.provectus.kafka.ui.model.ClusterMetricsDTO;
 import com.provectus.kafka.ui.model.ClusterStatsDTO;
 import com.provectus.kafka.ui.model.ConfigSourceDTO;
 import com.provectus.kafka.ui.model.ConfigSynonymDTO;
 import com.provectus.kafka.ui.model.ConnectDTO;
-import com.provectus.kafka.ui.model.Feature;
 import com.provectus.kafka.ui.model.InternalBroker;
 import com.provectus.kafka.ui.model.InternalBrokerConfig;
 import com.provectus.kafka.ui.model.InternalBrokerDiskUsage;
@@ -95,7 +95,7 @@ default ConfigSynonymDTO toConfigSynonym(ConfigEntry.ConfigSynonym config) {
 
   ConnectDTO toKafkaConnect(ClustersProperties.ConnectCluster connect);
 
-  List<ClusterDTO.FeaturesEnum> toFeaturesEnum(List<Feature> features);
+  List<ClusterDTO.FeaturesEnum> toFeaturesEnum(List<ClusterFeature> features);
 
   default List<PartitionDTO> map(Map<Integer, InternalPartition> map) {
     return map.values().stream().map(this::toPartition).collect(Collectors.toList());

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/model/ClusterFeature.java
Patch:
@@ -1,6 +1,6 @@
 package com.provectus.kafka.ui.model;
 
-public enum Feature {
+public enum ClusterFeature {
   KAFKA_CONNECT,
   KSQL_DB,
   SCHEMA_REGISTRY,

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/model/InternalClusterState.java
Patch:
@@ -23,7 +23,7 @@ public class InternalClusterState {
   private Integer underReplicatedPartitionCount;
   private List<BrokerDiskUsageDTO> diskUsage;
   private String version;
-  private List<Feature> features;
+  private List<ClusterFeature> features;
   private BigDecimal bytesInPerSec;
   private BigDecimal bytesOutPerSec;
   private Boolean readOnly;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/model/MetricsConfig.java
Patch:
@@ -17,4 +17,6 @@ public class MetricsConfig {
   private final boolean ssl;
   private final String username;
   private final String password;
+  private final String keystoreLocation;
+  private final String keystorePassword;
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/model/Statistics.java
Patch:
@@ -15,7 +15,7 @@ public class Statistics {
   ServerStatusDTO status;
   Throwable lastKafkaException;
   String version;
-  List<Feature> features;
+  List<ClusterFeature> features;
   ReactiveAdminClient.ClusterDescription clusterDescription;
   Metrics metrics;
   InternalLogDirStats logDirInfo;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/model/rbac/Resource.java
Patch:
@@ -5,6 +5,7 @@
 
 public enum Resource {
 
+  APPLICATIONCONFIG,
   CLUSTERCONFIG,
   TOPIC,
   CONSUMER,

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/ConsumerGroupService.java
Patch:
@@ -7,6 +7,7 @@
 import com.provectus.kafka.ui.model.KafkaCluster;
 import com.provectus.kafka.ui.model.SortOrderDTO;
 import com.provectus.kafka.ui.service.rbac.AccessControlService;
+import com.provectus.kafka.ui.util.SslPropertiesUtil;
 import java.util.ArrayList;
 import java.util.Collection;
 import java.util.Comparator;
@@ -214,6 +215,7 @@ public KafkaConsumer<Bytes, Bytes> createConsumer(KafkaCluster cluster) {
   public KafkaConsumer<Bytes, Bytes> createConsumer(KafkaCluster cluster,
                                                     Map<String, Object> properties) {
     Properties props = new Properties();
+    SslPropertiesUtil.addKafkaSslProperties(cluster.getOriginalProperties().getSsl(), props);
     props.putAll(cluster.getProperties());
     props.put(ConsumerConfig.CLIENT_ID_CONFIG, "kafka-ui-consumer-" + System.currentTimeMillis());
     props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, cluster.getBootstrapServers());

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/MessagesService.java
Patch:
@@ -18,6 +18,7 @@
 import com.provectus.kafka.ui.serdes.ConsumerRecordDeserializer;
 import com.provectus.kafka.ui.serdes.ProducerRecordCreator;
 import com.provectus.kafka.ui.util.ResultSizeLimiter;
+import com.provectus.kafka.ui.util.SslPropertiesUtil;
 import java.util.List;
 import java.util.Map;
 import java.util.Properties;
@@ -108,6 +109,7 @@ private Mono<RecordMetadata> sendMessageImpl(KafkaCluster cluster,
         );
 
     Properties properties = new Properties();
+    SslPropertiesUtil.addKafkaSslProperties(cluster.getOriginalProperties().getSsl(), properties);
     properties.putAll(cluster.getProperties());
     properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, cluster.getBootstrapServers());
     properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class);

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/ReactiveAdminClient.java
Patch:
@@ -10,7 +10,7 @@
 import com.provectus.kafka.ui.exception.IllegalEntityStateException;
 import com.provectus.kafka.ui.exception.NotFoundException;
 import com.provectus.kafka.ui.exception.ValidationException;
-import com.provectus.kafka.ui.util.NumberUtil;
+import com.provectus.kafka.ui.util.KafkaVersion;
 import com.provectus.kafka.ui.util.annotation.KafkaClientInternalsDependant;
 import java.io.Closeable;
 import java.util.ArrayList;
@@ -123,7 +123,7 @@ public static Mono<ReactiveAdminClient> create(AdminClient adminClient) {
 
   private static Set<SupportedFeature> getSupportedUpdateFeaturesForVersion(String versionStr) {
     try {
-      float version = NumberUtil.parserClusterVersion(versionStr);
+      float version = KafkaVersion.parse(versionStr);
       return SupportedFeature.forVersion(version);
     } catch (NumberFormatException e) {
       return SupportedFeature.defaultFeatures();
@@ -132,7 +132,7 @@ private static Set<SupportedFeature> getSupportedUpdateFeaturesForVersion(String
 
   // NOTE: if KafkaFuture returns null, that Mono will be empty(!), since Reactor does not support nullable results
   // (see MonoSink.success(..) javadoc for details)
-  private static <T> Mono<T> toMono(KafkaFuture<T> future) {
+  public static <T> Mono<T> toMono(KafkaFuture<T> future) {
     return Mono.<T>create(sink -> future.whenComplete((res, ex) -> {
       if (ex != null) {
         // KafkaFuture doc is unclear about what exception wrapper will be used

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/StatisticsService.java
Patch:
@@ -2,7 +2,7 @@
 
 import static com.provectus.kafka.ui.service.ReactiveAdminClient.ClusterDescription;
 
-import com.provectus.kafka.ui.model.Feature;
+import com.provectus.kafka.ui.model.ClusterFeature;
 import com.provectus.kafka.ui.model.InternalLogDirStats;
 import com.provectus.kafka.ui.model.KafkaCluster;
 import com.provectus.kafka.ui.model.Metrics;
@@ -51,7 +51,7 @@ private Mono<Statistics> getStatistics(KafkaCluster cluster) {
                             .version(ac.getVersion())
                             .metrics((Metrics) results[0])
                             .logDirInfo((InternalLogDirStats) results[1])
-                            .features((List<Feature>) results[2])
+                            .features((List<ClusterFeature>) results[2])
                             .topicConfigs((Map<String, List<ConfigEntry>>) results[3])
                             .topicDescriptions((Map<String, TopicDescription>) results[4])
                             .build()

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/TopicsService.java
Patch:
@@ -7,7 +7,7 @@
 import com.provectus.kafka.ui.exception.TopicNotFoundException;
 import com.provectus.kafka.ui.exception.TopicRecreationException;
 import com.provectus.kafka.ui.exception.ValidationException;
-import com.provectus.kafka.ui.model.Feature;
+import com.provectus.kafka.ui.model.ClusterFeature;
 import com.provectus.kafka.ui.model.InternalLogDirStats;
 import com.provectus.kafka.ui.model.InternalPartition;
 import com.provectus.kafka.ui.model.InternalPartitionsOffsets;
@@ -422,7 +422,7 @@ public Mono<PartitionsIncreaseResponseDTO> increaseTopicPartitions(
   }
 
   public Mono<Void> deleteTopic(KafkaCluster cluster, String topicName) {
-    if (statisticsCache.get(cluster).getFeatures().contains(Feature.TOPIC_DELETION)) {
+    if (statisticsCache.get(cluster).getFeatures().contains(ClusterFeature.TOPIC_DELETION)) {
       return adminClientService.get(cluster).flatMap(c -> c.deleteTopic(topicName))
           .doOnSuccess(t -> statisticsCache.onTopicDelete(cluster, topicName));
     } else {

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/masking/DataMasking.java
Patch:
@@ -41,9 +41,9 @@ boolean shouldBeApplied(String topic, Serde.Target target) {
 
   private final List<Mask> masks;
 
-  public static DataMasking create(List<ClustersProperties.Masking> config) {
+  public static DataMasking create(@Nullable List<ClustersProperties.Masking> config) {
     return new DataMasking(
-        config.stream().map(property -> {
+        Optional.ofNullable(config).orElse(List.of()).stream().map(property -> {
           Preconditions.checkNotNull(property.getType(), "masking type not specifed");
           Preconditions.checkArgument(
               StringUtils.isNotEmpty(property.getTopicKeysPattern())

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/masking/policies/Mask.java
Patch:
@@ -11,6 +11,8 @@
 
 class Mask extends MaskingPolicy {
 
+  static final List<String> DEFAULT_PATTERN = List.of("X", "x", "n", "-");
+
   private final UnaryOperator<String> masker;
 
   Mask(List<String> fieldNames, List<String> maskingChars) {

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/masking/policies/Replace.java
Patch:
@@ -10,6 +10,8 @@
 
 class Replace extends MaskingPolicy {
 
+  static final String DEFAULT_REPLACEMENT = "***DATA_MASKED***";
+
   private final String replacement;
 
   Replace(List<String> fieldNames, String replacementString) {

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/service/ksql/KsqlApiClientTest.java
Patch:
@@ -125,7 +125,7 @@ private void execCommandSync(KsqlApiClient client, String... ksqls) {
   }
 
   private KsqlApiClient ksqlClient() {
-    return new KsqlApiClient(KSQL_DB.url(), null, null, null);
+    return new KsqlApiClient(KSQL_DB.url(), null, null, null, null);
   }
 
 

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/service/ksql/KsqlServiceV2Test.java
Patch:
@@ -114,7 +114,7 @@ private static KafkaCluster cluster() {
   }
 
   private static KsqlApiClient ksqlClient() {
-    return new KsqlApiClient(KSQL_DB.url(), null, null, null);
+    return new KsqlApiClient(KSQL_DB.url(), null, null, null, null);
   }
 
 }

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/service/metrics/PrometheusMetricsRetrieverTest.java
Patch:
@@ -15,7 +15,7 @@
 
 class PrometheusMetricsRetrieverTest {
 
-  private final PrometheusMetricsRetriever retriever = new PrometheusMetricsRetriever(WebClient.create());
+  private final PrometheusMetricsRetriever retriever = new PrometheusMetricsRetriever();
 
   private final MockWebServer mockWebServer = new MockWebServer();
 
@@ -36,7 +36,7 @@ void callsMetricsEndpointAndConvertsResponceToRawMetric() {
 
     MetricsConfig metricsConfig = prepareMetricsConfig(url.port(), null, null);
 
-    StepVerifier.create(retriever.retrieve(url.host(), metricsConfig))
+    StepVerifier.create(retriever.retrieve(WebClient.create(), url.host(), metricsConfig))
         .expectNextSequence(expectedRawMetrics())
         // third metric should not be present, since it has "NaN" value
         .verifyComplete();
@@ -50,7 +50,7 @@ void callsSecureMetricsEndpointAndConvertsResponceToRawMetric() {
 
     MetricsConfig metricsConfig = prepareMetricsConfig(url.port(), "username", "password");
 
-    StepVerifier.create(retriever.retrieve(url.host(), metricsConfig))
+    StepVerifier.create(retriever.retrieve(WebClient.create(), url.host(), metricsConfig))
         .expectNextSequence(expectedRawMetrics())
         // third metric should not be present, since it has "NaN" value
         .verifyComplete();

File: kafka-ui-e2e-checks/src/test/java/com/provectus/kafka/ui/smokeSuite/schemas/SchemasTest.java
Patch:
@@ -77,7 +77,7 @@ public void updateSchemaAvro() {
         Assert.assertEquals(CompatibilityLevel.CompatibilityEnum.NONE.toString(), schemaDetails.getCompatibility(), "getCompatibility()");
     }
 
-    @QaseId(186)
+    @QaseId(44)
     @Test(priority = 3)
     public void compareVersionsOperation() {
         navigateToSchemaRegistryAndOpenDetails(AVRO_API.getName());

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/utilities/qaseUtils/QaseSetup.java
Patch:
@@ -15,7 +15,7 @@
 @Slf4j
 public class QaseSetup {
 
-    public static void testRunSetup() {
+    public static void qaseIntegrationSetup() {
         String qaseApiToken = System.getProperty("QASEIO_API_TOKEN");
         if (isEmpty(qaseApiToken)) {
             log.warn("Integration with Qase is disabled due to run config or token wasn't defined.");

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/variables/Suite.java
Patch:
@@ -4,7 +4,6 @@ public interface Suite {
 
     String CUSTOM = "custom";
     String MANUAL = "manual";
-    String QASE = "qase";
     String REGRESSION = "regression";
     String SANITY = "sanity";
     String SMOKE = "smoke";

File: kafka-ui-e2e-checks/src/test/java/com/provectus/kafka/ui/Facade.java
Patch:
@@ -19,6 +19,7 @@
 import com.provectus.kafka.ui.services.ApiService;
 
 public abstract class Facade {
+
     protected ApiService apiService = new ApiService();
     protected ConnectorCreateForm connectorCreateForm = new ConnectorCreateForm();
     protected KafkaConnectList kafkaConnectList = new KafkaConnectList();

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/BasePage.java
Patch:
@@ -23,6 +23,8 @@ public abstract class BasePage extends WebUtils {
     protected SelenideElement confirmationMdl = $x("//div[text()= 'Confirm the action']/..");
     protected SelenideElement confirmBtn = $x("//button[contains(text(),'Confirm')]");
     protected SelenideElement cancelBtn = $x("//button[contains(text(),'Cancel')]");
+    protected SelenideElement backBtn = $x("//button[contains(text(),'Back')]");
+    protected SelenideElement nextBtn = $x("//button[contains(text(),'Next')]");
     protected ElementsCollection ddlOptions = $$x("//li[@value]");
     protected ElementsCollection gridItems = $$x("//tr[@class]");
     protected String summaryCellLocator = "//div[contains(text(),'%s')]";

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/util/jsonschema/ProtobufSchemaConverterTest.java
Patch:
@@ -57,6 +57,7 @@ enum SampleEnum {
             message EmbeddedMsg {
                 int32 emb_f1 = 1;
                 TestMsg outer_ref = 2;
+                EmbeddedMsg self_ref = 3;
             }
         }""";
 
@@ -116,7 +117,8 @@ enum SampleEnum {
                     "properties":
                     {
                         "emb_f1": { "type": "integer", "maximum": 2147483647, "minimum": -2147483648 },
-                        "outer_ref": { "$ref": "#/definitions/test.TestMsg" }
+                        "outer_ref": { "$ref": "#/definitions/test.TestMsg" },
+                        "self_ref": { "$ref": "#/definitions/test.TestMsg.EmbeddedMsg" }
                     }
                 }
             },

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/brokers/BrokersList.java
Patch:
@@ -62,7 +62,7 @@ public List<SelenideElement> getAllEnabledElements() {
 
     private List<BrokersList.BrokerGridItem> initGridItems() {
         List<BrokersList.BrokerGridItem> gridItemList = new ArrayList<>();
-        allGridItems.shouldHave(CollectionCondition.sizeGreaterThan(0))
+        gridItems.shouldHave(CollectionCondition.sizeGreaterThan(0))
                 .forEach(item -> gridItemList.add(new BrokersList.BrokerGridItem(item)));
         return gridItemList;
     }

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/ksqlDb/KsqlDbList.java
Patch:
@@ -41,7 +41,7 @@ public KsqlDbList openDetailsTab(KsqlMenuTabs menu) {
 
   private List<KsqlDbList.KsqlTablesGridItem> initTablesItems() {
     List<KsqlDbList.KsqlTablesGridItem> gridItemList = new ArrayList<>();
-    allGridItems.shouldHave(CollectionCondition.sizeGreaterThan(0))
+    gridItems.shouldHave(CollectionCondition.sizeGreaterThan(0))
         .forEach(item -> gridItemList.add(new KsqlDbList.KsqlTablesGridItem(item)));
     return gridItemList;
   }
@@ -89,7 +89,7 @@ public String getIsWindowed() {
 
   private List<KsqlDbList.KsqlStreamsGridItem> initStreamsItems() {
     List<KsqlDbList.KsqlStreamsGridItem> gridItemList = new ArrayList<>();
-    allGridItems.shouldHave(CollectionCondition.sizeGreaterThan(0))
+    gridItems.shouldHave(CollectionCondition.sizeGreaterThan(0))
         .forEach(item -> gridItemList.add(new KsqlDbList.KsqlStreamsGridItem(item)));
     return gridItemList;
   }

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/topics/TopicDetails.java
Patch:
@@ -337,7 +337,7 @@ public int getMessageCountAmount() {
 
     private List<TopicDetails.MessageGridItem> initItems() {
         List<TopicDetails.MessageGridItem> gridItemList = new ArrayList<>();
-        allGridItems.shouldHave(CollectionCondition.sizeGreaterThan(0))
+        gridItems.shouldHave(CollectionCondition.sizeGreaterThan(0))
                 .forEach(item -> gridItemList.add(new TopicDetails.MessageGridItem(item)));
         return gridItemList;
     }

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/topics/TopicSettingsTab.java
Patch:
@@ -24,7 +24,7 @@ public TopicSettingsTab waitUntilScreenReady() {
 
     private List<SettingsGridItem> initGridItems() {
         List<SettingsGridItem> gridItemList = new ArrayList<>();
-        allGridItems.shouldHave(CollectionCondition.sizeGreaterThan(0))
+        gridItems.shouldHave(CollectionCondition.sizeGreaterThan(0))
                 .forEach(item -> gridItemList.add(new SettingsGridItem(item)));
         return gridItemList;
     }

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/topics/TopicsList.java
Patch:
@@ -163,7 +163,7 @@ public List<SelenideElement> getAllEnabledElements() {
 
     private List<TopicGridItem> initGridItems() {
         List<TopicGridItem> gridItemList = new ArrayList<>();
-        allGridItems.shouldHave(CollectionCondition.sizeGreaterThan(0))
+        gridItems.shouldHave(CollectionCondition.sizeGreaterThan(0))
                 .forEach(item -> gridItemList.add(new TopicGridItem(item)));
         return gridItemList;
     }

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/models/Connector.java
Patch:
@@ -8,5 +8,4 @@
 public class Connector {
 
     private String name, config;
-
 }

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/consumers/ConsumersDetails.java
Patch:
@@ -1,11 +1,11 @@
-package com.provectus.kafka.ui.pages.consumer;
-
-import static com.codeborne.selenide.Selenide.$x;
+package com.provectus.kafka.ui.pages.consumers;
 
 import com.codeborne.selenide.Condition;
 import com.provectus.kafka.ui.pages.BasePage;
 import io.qameta.allure.Step;
 
+import static com.codeborne.selenide.Selenide.$x;
+
 public class ConsumersDetails extends BasePage {
 
     protected String consumerIdHeaderLocator = "//h1[contains(text(),'%s')]";

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/ksqlDb/KsqlQueryForm.java
Patch:
@@ -1,4 +1,4 @@
-package com.provectus.kafka.ui.pages.ksqldb;
+package com.provectus.kafka.ui.pages.ksqlDb;
 
 import static com.codeborne.selenide.Condition.visible;
 import static com.codeborne.selenide.Selenide.$$x;
@@ -24,7 +24,7 @@ public class KsqlQueryForm extends BasePage {
   protected SelenideElement queryAreaValue = $x("//div[@class='ace_content']");
   protected SelenideElement queryArea = $x("//div[@id='ksql']/textarea[@class='ace_text-input']");
   protected ElementsCollection ksqlGridItems = $$x("//tbody//tr");
-  protected ElementsCollection keyField = $$x("//input[@aria-label='value']");
+  protected ElementsCollection keyField = $$x("//input[@aria-label='key']");
   protected ElementsCollection valueField = $$x("//input[@aria-label='value']");
 
   @Step
@@ -89,7 +89,7 @@ private List<KsqlQueryForm.KsqlResponseGridItem> initItems() {
   public KsqlQueryForm.KsqlResponseGridItem getTableByName(String name) {
     return initItems().stream()
         .filter(e -> e.getName().equalsIgnoreCase(name))
-        .findFirst().orElse(null);
+        .findFirst().orElseThrow();
   }
 
   public static class KsqlResponseGridItem extends BasePage {

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/ksqlDb/models/Stream.java
Patch:
@@ -1,10 +1,11 @@
-package com.provectus.kafka.ui.pages.ksqldb.models;
+package com.provectus.kafka.ui.pages.ksqlDb.models;
 
 import lombok.Data;
 import lombok.experimental.Accessors;
 
 @Data
 @Accessors(chain = true)
 public class Stream {
-  private String name, topicName, valueFormat, partitions;
+
+    private String name, topicName, valueFormat, partitions;
 }

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/ksqlDb/models/Table.java
Patch:
@@ -1,10 +1,11 @@
-package com.provectus.kafka.ui.pages.ksqldb.models;
+package com.provectus.kafka.ui.pages.ksqlDb.models;
 
 import lombok.Data;
 import lombok.experimental.Accessors;
 
 @Data
 @Accessors(chain = true)
 public class Table {
-  private String name, streamName;
+
+    private String name, streamName;
 }

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/utilities/qaseIoUtils/TestCaseGenerator.java
Patch:
@@ -23,9 +23,9 @@
 @Slf4j
 public class TestCaseGenerator {
 
-    public static boolean FAILED = false;
     private static final ApiClient apiClient = QaseClient.getApiClient();
     private static final CasesApi casesApi = new CasesApi(apiClient);
+    public static boolean FAILED = false;
 
     @SneakyThrows
     public static void createTestCaseIfNotExists(Method testMethod) {

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/service/MessagesServiceTest.java
Patch:
@@ -61,12 +61,12 @@ void loadMessagesReturnsExceptionWhenTopicNotFound() {
   }
 
   @Test
-  void maskingAppliedOnConfiguredClusters() {
+  void maskingAppliedOnConfiguredClusters() throws Exception {
     String testTopic = MASKED_TOPICS_PREFIX + UUID.randomUUID();
     try (var producer = KafkaTestProducer.forKafka(kafka)) {
       createTopic(new NewTopic(testTopic, 1, (short) 1));
       producer.send(testTopic, "message1");
-      producer.send(testTopic, "message2");
+      producer.send(testTopic, "message2").get();
 
       Flux<TopicMessageDTO> msgsFlux = messagesService.loadMessages(
           cluster,
@@ -91,4 +91,4 @@ void maskingAppliedOnConfiguredClusters() {
     }
   }
 
-}
\ No newline at end of file
+}

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/controller/TopicsController.java
Patch:
@@ -175,7 +175,7 @@ public Mono<ResponseEntity<TopicsResponseDTO>> getTopics(String clusterName,
           List<InternalTopic> filtered = existingTopics.stream()
               .filter(topic -> !topic.isInternal()
                   || showInternal != null && showInternal)
-              .filter(topic -> search == null || StringUtils.contains(topic.getName(), search))
+              .filter(topic -> search == null || StringUtils.containsIgnoreCase(topic.getName(), search))
               .sorted(comparator)
               .toList();
           var totalPages = (filtered.size() / pageSize)

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/models/Topic.java
Patch:
@@ -3,6 +3,7 @@
 import com.provectus.kafka.ui.pages.topic.enums.CleanupPolicyValue;
 import com.provectus.kafka.ui.pages.topic.enums.CustomParameterType;
 import com.provectus.kafka.ui.pages.topic.enums.MaxSizeOnDisk;
+import com.provectus.kafka.ui.pages.topic.enums.TimeToRetain;
 import lombok.Data;
 import lombok.experimental.Accessors;
 
@@ -14,4 +15,5 @@ public class Topic {
     private CustomParameterType customParameterType;
     private CleanupPolicyValue cleanupPolicyValue;
     private MaxSizeOnDisk maxSizeOnDisk;
+    private TimeToRetain timeToRetain;
 }

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/topic/TopicDetails.java
Patch:
@@ -51,7 +51,6 @@ public class TopicDetails extends BasePage {
   protected SelenideElement cleanUpPolicyField = $x("//div[contains(text(),'Clean Up Policy')]/../span/*");
   protected SelenideElement partitionsField = $x("//div[contains(text(),'Partitions')]/../span");
   protected SelenideElement backToCreateFiltersLink = $x("//div[text()='Back To create filters']");
-  protected SelenideElement confirmationMdl = $x("//div[text()= 'Confirm the action']/..");
   protected ElementsCollection messageGridItems = $$x("//tbody//tr");
   protected SelenideElement actualCalendarDate = $x("//div[@class='react-datepicker__current-month']");
   protected SelenideElement previousMonthButton = $x("//button[@aria-label='Previous Month']");
@@ -103,7 +102,7 @@ public TopicDetails clickEditSettingsMenu() {
 
   @Step
   public boolean isConfirmationMdlVisible(){
-    return isVisible(confirmationMdl);
+    return isConfirmationModalVisible();
   }
 
   @Step

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/util/jsonschema/AvroJsonSchemaConverter.java
Patch:
@@ -110,7 +110,7 @@ private FieldSchema createObjectSchema(Schema schema,
       return createRefField(definitionName);
     }
     // adding stub record, need to avoid infinite recursion
-    definitions.put(definitionName, new ObjectFieldSchema(Map.of(), List.of()));
+    definitions.put(definitionName, ObjectFieldSchema.EMPTY);
 
     final Map<String, FieldSchema> fields = schema.getFields().stream()
         .map(f -> Tuples.of(f.name(), convertField(f, definitions)))

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/util/jsonschema/ObjectFieldSchema.java
Patch:
@@ -10,6 +10,9 @@
 import reactor.util.function.Tuples;
 
 public class ObjectFieldSchema implements FieldSchema {
+
+  public static final ObjectFieldSchema EMPTY = new ObjectFieldSchema(Map.of(), List.of());
+
   private final Map<String, FieldSchema> properties;
   private final List<String> required;
 

File: kafka-ui-e2e-checks/src/test/java/com/provectus/kafka/ui/base/BaseTest.java
Patch:
@@ -62,6 +62,7 @@ public static void start() {
                 .addArguments("--disable-gpu")
                 .addArguments("--no-sandbox")
                 .addArguments("--verbose")
+                .addArguments("--lang=es")
             )
             .withLogConsumer(new Slf4jLogConsumer(log).withPrefix("[CHROME]: "));
         try {

File: kafka-ui-e2e-checks/src/test/java/com/provectus/kafka/ui/suite/schemas/SchemasTests.java
Patch:
@@ -1,7 +1,6 @@
 package com.provectus.kafka.ui.suite.schemas;
 
 import static com.provectus.kafka.ui.pages.NaviSideBar.SideMenuOption.SCHEMA_REGISTRY;
-import static com.provectus.kafka.ui.settings.BaseSource.CLUSTER_NAME;
 import static com.provectus.kafka.ui.utilities.FileUtils.fileToString;
 
 import com.codeborne.selenide.Condition;
@@ -41,7 +40,7 @@ public class SchemasTests extends BaseTest {
     @SneakyThrows
     public void beforeAll() {
         SCHEMA_LIST.addAll(List.of(AVRO_API, JSON_API, PROTOBUF_API));
-        SCHEMA_LIST.forEach(schema -> apiService.createSchema(CLUSTER_NAME, schema));
+        SCHEMA_LIST.forEach(schema -> apiService.createSchema(schema));
     }
 
     @DisplayName("should create AVRO schema")
@@ -228,7 +227,7 @@ void deleteSchemaProtobuf() {
 
     @AfterAll
     public void afterAll() {
-        SCHEMA_LIST.forEach(schema -> apiService.deleteSchema(CLUSTER_NAME, schema.getName()));
+        SCHEMA_LIST.forEach(schema -> apiService.deleteSchema(schema.getName()));
     }
 
     @Step

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/controller/ConsumerGroupsController.java
Patch:
@@ -189,8 +189,8 @@ public Mono<ResponseEntity<Void>> resetConsumerGroupOffsets(String clusterName,
   private ConsumerGroupsPageResponseDTO convertPage(ConsumerGroupService.ConsumerGroupsPage
                                                         consumerGroupConsumerGroupsPage) {
     return new ConsumerGroupsPageResponseDTO()
-        .pageCount(consumerGroupConsumerGroupsPage.getTotalPages())
-        .consumerGroups(consumerGroupConsumerGroupsPage.getConsumerGroups()
+        .pageCount(consumerGroupConsumerGroupsPage.totalPages())
+        .consumerGroups(consumerGroupConsumerGroupsPage.consumerGroups()
             .stream()
             .map(ConsumerGroupMapper::toDto)
             .collect(Collectors.toList()));

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/OffsetsResetService.java
Patch:
@@ -98,7 +98,7 @@ private Mono<ReactiveAdminClient> checkGroupCondition(KafkaCluster cluster, Stri
         .flatMap(ac ->
             // we need to call listConsumerGroups() to check group existence, because
             // describeConsumerGroups() will return consumer group even if it doesn't exist
-            ac.listConsumerGroups()
+            ac.listConsumerGroupNames()
                 .filter(cgs -> cgs.stream().anyMatch(g -> g.equals(groupId)))
                 .flatMap(cgs -> ac.describeConsumerGroups(List.of(groupId)))
                 .filter(cgs -> cgs.containsKey(groupId))

File: kafka-ui-e2e-checks/src/test/java/com/provectus/kafka/ui/base/BaseTest.java
Patch:
@@ -17,6 +17,7 @@
 import com.codeborne.selenide.WebDriverRunner;
 import com.provectus.kafka.ui.utilities.qaseIoUtils.DisplayNameGenerator;
 import io.qase.api.annotation.Step;
+import java.time.Duration;
 import java.util.List;
 import lombok.extern.slf4j.Slf4j;
 import org.assertj.core.api.SoftAssertions;
@@ -55,6 +56,7 @@ public static void start() {
         log.info("Using [{}] as image name for chrome", image.getUnversionedPart());
         webDriverContainer = new BrowserWebDriverContainer<>(image)
             .withEnv("JAVA_OPTS", "-Dwebdriver.chrome.whitelistedIps=")
+            .withStartupTimeout(Duration.ofSeconds(180))
             .withCapabilities(new ChromeOptions()
                 .addArguments("--disable-dev-shm-usage")
                 .addArguments("--disable-gpu")

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/topic/TopicsList.java
Patch:
@@ -15,7 +15,7 @@
 
 public class TopicsList extends BasePage {
 
-    protected SelenideElement topicListHeader = $x("//*[text()='Topics']");
+    protected SelenideElement topicListHeader = $x("//h1[text()='Topics']");
     protected SelenideElement addTopicBtn = $x("//button[normalize-space(text()) ='Add a Topic']");
     protected SelenideElement searchField = $x("//input[@placeholder='Search by Topic Name']");
     protected SelenideElement showInternalRadioBtn = $x("//input[@name='ShowInternalTopics']");

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/config/auth/OAuthProperties.java
Patch:
@@ -1,6 +1,7 @@
 package com.provectus.kafka.ui.config.auth;
 
 import java.util.HashMap;
+import java.util.HashSet;
 import java.util.Map;
 import java.util.Set;
 import javax.annotation.PostConstruct;
@@ -31,13 +32,13 @@ public static class OAuth2Provider {
     private String clientName;
     private String redirectUri;
     private String authorizationGrantType;
-    private Set<String> scope;
+    private Set<String> scope = new HashSet<>();
     private String issuerUri;
     private String authorizationUri;
     private String tokenUri;
     private String userInfoUri;
     private String jwkSetUri;
     private String userNameAttribute;
-    private Map<String, String> customParams;
+    private Map<String, String> customParams = new HashMap<>();
   }
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/config/auth/OAuthPropertiesConverter.java
Patch:
@@ -71,7 +71,7 @@ private static void applyGoogleTransformations(OAuth2Provider provider) {
   }
 
   private static boolean isGoogle(OAuth2Provider provider) {
-    return provider.getCustomParams().get(TYPE).equalsIgnoreCase(GOOGLE);
+    return GOOGLE.equalsIgnoreCase(provider.getCustomParams().get(TYPE));
   }
 }
 

File: kafka-ui-e2e-checks/src/test/java/com/provectus/kafka/ui/base/BaseTest.java
Patch:
@@ -34,7 +34,7 @@
 
 @Slf4j
 @DisplayNameGeneration(DisplayNameGenerator.class)
-public class BaseTest extends Facade {
+public abstract class BaseTest extends Facade {
 
   private static final String SELENIUM_IMAGE_NAME = "selenium/standalone-chrome:103.0";
   private static final String SELENIARM_STANDALONE_CHROMIUM = "seleniarm/standalone-chromium:103.0";

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/NaviSideBar.java
Patch:
@@ -1,7 +1,7 @@
 package com.provectus.kafka.ui.pages;
 
 import static com.codeborne.selenide.Selenide.$x;
-import static com.provectus.kafka.ui.settings.Source.CLUSTER_NAME;
+import static com.provectus.kafka.ui.settings.BaseSource.CLUSTER_NAME;
 
 import com.codeborne.selenide.Condition;
 import com.codeborne.selenide.SelenideElement;

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/settings/BaseSource.java
Patch:
@@ -1,6 +1,6 @@
 package com.provectus.kafka.ui.settings;
 
-public abstract class Source {
+public abstract class BaseSource {
 
   public static String BASE_API_URL = System.getProperty("BASE_URL", "http://localhost:8080");
   public static String BASE_WEB_URL = System.getProperty("BASE_DOCKER_URL", "http://host.testcontainers.internal:8080");

File: kafka-ui-e2e-checks/src/test/java/com/provectus/kafka/ui/base/Facade.java
Patch:
@@ -1,6 +1,6 @@
 package com.provectus.kafka.ui.base;
 
-import com.provectus.kafka.ui.helpers.ApiHelper;
+import com.provectus.kafka.ui.services.ApiService;
 import com.provectus.kafka.ui.pages.NaviSideBar;
 import com.provectus.kafka.ui.pages.TopPanel;
 import com.provectus.kafka.ui.pages.brokers.BrokersConfigTab;
@@ -21,7 +21,7 @@
 import com.provectus.kafka.ui.pages.topic.TopicsList;
 
 public abstract class Facade {
-    protected ApiHelper apiHelper = new ApiHelper();
+    protected ApiService apiService = new ApiService();
     protected ConnectorCreateForm connectorCreateForm = new ConnectorCreateForm();
     protected KafkaConnectList kafkaConnectList = new KafkaConnectList();
     protected ConnectorDetails connectorDetails = new ConnectorDetails();

File: kafka-ui-e2e-checks/src/test/java/com/provectus/kafka/ui/suite/schemas/SchemasTests.java
Patch:
@@ -1,7 +1,7 @@
 package com.provectus.kafka.ui.suite.schemas;
 
 import static com.provectus.kafka.ui.pages.NaviSideBar.SideMenuOption.SCHEMA_REGISTRY;
-import static com.provectus.kafka.ui.settings.Source.CLUSTER_NAME;
+import static com.provectus.kafka.ui.settings.BaseSource.CLUSTER_NAME;
 import static com.provectus.kafka.ui.utilities.FileUtils.fileToString;
 
 import com.codeborne.selenide.Condition;
@@ -41,7 +41,7 @@ public class SchemasTests extends BaseTest {
     @SneakyThrows
     public void beforeAll() {
         SCHEMA_LIST.addAll(List.of(AVRO_API, JSON_API, PROTOBUF_API));
-        SCHEMA_LIST.forEach(schema -> apiHelper.createSchema(CLUSTER_NAME, schema));
+        SCHEMA_LIST.forEach(schema -> apiService.createSchema(CLUSTER_NAME, schema));
     }
 
     @DisplayName("should create AVRO schema")
@@ -228,7 +228,7 @@ void deleteSchemaProtobuf() {
 
     @AfterAll
     public void afterAll() {
-        SCHEMA_LIST.forEach(schema -> apiHelper.deleteSchema(CLUSTER_NAME, schema.getName()));
+        SCHEMA_LIST.forEach(schema -> apiService.deleteSchema(CLUSTER_NAME, schema.getName()));
     }
 
     @Step

File: kafka-ui-e2e-checks/src/test/java/com/provectus/kafka/ui/suite/topics/TopicMessagesTests.java
Patch:
@@ -2,7 +2,7 @@
 
 import static com.provectus.kafka.ui.pages.BasePage.AlertHeader.SUCCESS;
 import static com.provectus.kafka.ui.pages.topic.TopicDetails.TopicMenu.MESSAGES;
-import static com.provectus.kafka.ui.settings.Source.CLUSTER_NAME;
+import static com.provectus.kafka.ui.settings.BaseSource.CLUSTER_NAME;
 import static com.provectus.kafka.ui.utilities.FileUtils.fileToString;
 import static org.apache.commons.lang.RandomStringUtils.randomAlphabetic;
 
@@ -38,7 +38,7 @@ public class TopicMessagesTests extends BaseTest {
   @BeforeAll
   public void beforeAll() {
     TOPIC_LIST.addAll(List.of(TOPIC_FOR_MESSAGES));
-    TOPIC_LIST.forEach(topic -> apiHelper.createTopic(CLUSTER_NAME, topic.getName()));
+    TOPIC_LIST.forEach(topic -> apiService.createTopic(CLUSTER_NAME, topic.getName()));
   }
 
   @DisplayName("produce message")
@@ -136,6 +136,6 @@ void checkingMessageFilteringByOffset() {
 
   @AfterAll
   public void afterAll() {
-    TOPIC_LIST.forEach(topic -> apiHelper.deleteTopic(CLUSTER_NAME, topic.getName()));
+    TOPIC_LIST.forEach(topic -> apiService.deleteTopic(CLUSTER_NAME, topic.getName()));
   }
 }

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/brokers/BrokersConfigTab.java
Patch:
@@ -11,13 +11,13 @@
 import java.util.stream.Collectors;
 import java.util.stream.Stream;
 
-public class BrokersConfigTabPanel extends BasePage {
+public class BrokersConfigTab extends BasePage {
 
   protected List<SelenideElement> editBtn = $$x("//button[@aria-label='editAction']");
   protected SelenideElement searchByKeyField = $x("//input[@placeholder='Search by Key']");
 
   @Step
-  public BrokersConfigTabPanel waitUntilScreenReady(){
+  public BrokersConfigTab waitUntilScreenReady(){
     waitUntilSpinnerDisappear();
     searchByKeyField.shouldBe(Condition.visible);
     return this;

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/BasePage.java
Patch:
@@ -21,7 +21,7 @@ public abstract class BasePage extends WebUtils {
   protected ElementsCollection allGridItems = $$x("//tr[@class]");
   protected String summaryCellLocator = "//div[contains(text(),'%s')]";
   protected String tableElementNameLocator = "//tbody//a[contains(text(),'%s')]";
-  protected String columnHeaderLocator = "//table//tr/th/div[text()='%s']";
+  protected String columnHeaderLocator = "//table//tr/th//div[text()='%s']";
 
   protected void waitUntilSpinnerDisappear() {
     log.debug("\nwaitUntilSpinnerDisappear");

File: kafka-ui-e2e-checks/src/test/java/com/provectus/kafka/ui/base/Facade.java
Patch:
@@ -3,6 +3,7 @@
 import com.provectus.kafka.ui.helpers.ApiHelper;
 import com.provectus.kafka.ui.pages.NaviSideBar;
 import com.provectus.kafka.ui.pages.TopPanel;
+import com.provectus.kafka.ui.pages.brokers.BrokersConfigTabPanel;
 import com.provectus.kafka.ui.pages.brokers.BrokersDetails;
 import com.provectus.kafka.ui.pages.brokers.BrokersList;
 import com.provectus.kafka.ui.pages.connector.ConnectorCreateForm;
@@ -36,4 +37,5 @@ public abstract class Facade {
     protected TopPanel topPanel = new TopPanel();
     protected BrokersList brokersList = new BrokersList();
     protected BrokersDetails brokersDetails = new BrokersDetails();
+    protected BrokersConfigTabPanel brokersConfigTabPanel = new BrokersConfigTabPanel();
 }

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/brokers/BrokersList.java
Patch:
@@ -30,7 +30,7 @@ public BrokersList openBroker(String brokerId) {
   }
 
   private List<SelenideElement> getUptimeSummaryCells() {
-    return Stream.of("Broker Count", "Active Controllers", "Version")
+    return Stream.of("Broker Count", "Active Controller", "Version")
         .map(name -> $x(String.format(summaryCellLocator, name)))
         .collect(Collectors.toList());
   }
@@ -87,7 +87,7 @@ public BrokerGridItem(SelenideElement element) {
     }
 
     private SelenideElement getIdElm() {
-      return element.$x("./td[1]/a");
+      return element.$x("./td[1]/div/a");
     }
 
     @Step

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/BasePage.java
Patch:
@@ -1,8 +1,10 @@
 package com.provectus.kafka.ui.pages;
 
+import static com.codeborne.selenide.Selenide.$$x;
 import static com.codeborne.selenide.Selenide.$x;
 
 import com.codeborne.selenide.Condition;
+import com.codeborne.selenide.ElementsCollection;
 import com.codeborne.selenide.SelenideElement;
 import com.provectus.kafka.ui.utilities.WebUtils;
 import lombok.extern.slf4j.Slf4j;
@@ -16,6 +18,7 @@ public abstract class BasePage extends WebUtils {
   protected SelenideElement dotMenuBtn = $x("//button[@aria-label='Dropdown Toggle']");
   protected SelenideElement alertHeader = $x("//div[@role='alert']//div[@role='heading']");
   protected SelenideElement alertMessage = $x("//div[@role='alert']//div[@role='contentinfo']");
+  protected ElementsCollection allGridItems = $$x("//tr[@class]");
   protected String summaryCellLocator = "//div[contains(text(),'%s')]";
   protected String tableElementNameLocator = "//tbody//a[contains(text(),'%s')]";
   protected String columnHeaderLocator = "//table//tr/th/div[text()='%s']";

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/topic/TopicDetails.java
Patch:
@@ -218,7 +218,7 @@ public int getMessageCountAmount() {
 
   private List<TopicDetails.MessageGridItem> initItems() {
     List<TopicDetails.MessageGridItem> gridItemList = new ArrayList<>();
-    messageGridItems.shouldHave(CollectionCondition.sizeGreaterThan(0))
+    allGridItems.shouldHave(CollectionCondition.sizeGreaterThan(0))
         .forEach(item -> gridItemList.add(new TopicDetails.MessageGridItem(item)));
     return gridItemList;
   }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/exception/ErrorCode.java
Patch:
@@ -7,6 +7,8 @@
 
 public enum ErrorCode {
 
+  FORBIDDEN(403, HttpStatus.FORBIDDEN),
+
   UNEXPECTED(5000, HttpStatus.INTERNAL_SERVER_ERROR),
   KSQL_API_ERROR(5001, HttpStatus.INTERNAL_SERVER_ERROR),
   BINDING_FAIL(4001, HttpStatus.BAD_REQUEST),

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/serdes/builtin/sr/JsonSchemaSchemaRegistrySerializer.java
Patch:
@@ -4,7 +4,7 @@
 import com.fasterxml.jackson.databind.JsonNode;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.provectus.kafka.ui.exception.ValidationException;
-import com.provectus.kafka.ui.util.annotations.KafkaClientInternalsDependant;
+import com.provectus.kafka.ui.util.annotation.KafkaClientInternalsDependant;
 import io.confluent.kafka.schemaregistry.ParsedSchema;
 import io.confluent.kafka.schemaregistry.client.SchemaMetadata;
 import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/ClusterService.java
Patch:
@@ -38,6 +38,7 @@ public Mono<ClusterStatsDTO> getClusterStats(KafkaCluster cluster) {
   }
 
   public Mono<ClusterMetricsDTO> getClusterMetrics(KafkaCluster cluster) {
+
     return Mono.just(
         clusterMapper.toClusterMetrics(
             statisticsCache.get(cluster).getMetrics()));

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/ReactiveAdminClient.java
Patch:
@@ -11,7 +11,7 @@
 import com.provectus.kafka.ui.exception.ValidationException;
 import com.provectus.kafka.ui.util.MapUtil;
 import com.provectus.kafka.ui.util.NumberUtil;
-import com.provectus.kafka.ui.util.annotations.KafkaClientInternalsDependant;
+import com.provectus.kafka.ui.util.annotation.KafkaClientInternalsDependant;
 import java.io.Closeable;
 import java.util.ArrayList;
 import java.util.Arrays;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/util/annotation/KafkaClientInternalsDependant.java
Patch:
@@ -1,4 +1,4 @@
-package com.provectus.kafka.ui.util.annotations;
+package com.provectus.kafka.ui.util.annotation;
 
 /**
  * All code places that depend on kafka-client's internals or implementation-specific logic

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/TopPanel.java
Patch:
@@ -14,10 +14,10 @@ public class TopPanel extends BasePage{
     protected SelenideElement discordBtn = $x("//a[contains(@href,'https://discord.com/invite')]");
 
     public List<SelenideElement> getAllVisibleElements() {
-        return Arrays.asList(kafkaLogo, kafkaVersion, logOutBtn, gitBtn, discordBtn);
+        return Arrays.asList(kafkaLogo, kafkaVersion, gitBtn, discordBtn);
     }
 
     public List<SelenideElement> getAllEnabledElements() {
-        return Arrays.asList(logOutBtn, gitBtn, discordBtn, kafkaLogo);
+        return Arrays.asList(gitBtn, discordBtn, kafkaLogo);
     }
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/emitter/TailingEmitter.java
Patch:
@@ -41,6 +41,7 @@ public void accept(FluxSink<TopicMessageEventDTO> sink) {
       sink.complete();
       log.debug("Tailing finished");
     } catch (InterruptException kafkaInterruptException) {
+      log.debug("Tailing finished due to thread interruption");
       sink.complete();
     } catch (Exception e) {
       log.error("Error consuming {}", consumerPosition, e);

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/models/Topic.java
Patch:
@@ -9,8 +9,8 @@
 @Data
 @Accessors(chain = true)
 public class Topic {
-    private String name, timeToRetainData, maxMessageBytes, messageKey, messageContent,
-            partitions, customParameterValue;
+    private String name, timeToRetainData, maxMessageBytes, messageKey, messageContent, customParameterValue;
+    private int numberOfPartitions;
     private CustomParameterType customParameterType;
     private CleanupPolicyValue cleanupPolicyValue;
     private MaxSizeOnDisk maxSizeOnDisk;

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/topic/TopicDetails.java
Patch:
@@ -85,8 +85,8 @@ public String getCleanUpPolicy() {
   }
 
   @Step
-  public String getPartitions() {
-    return partitionsField.getText();
+  public int getPartitions() {
+    return Integer.parseInt(partitionsField.getText().trim());
   }
 
   @Step

File: kafka-ui-serde-api/src/main/java/com/provectus/kafka/ui/serde/api/PropertyResolver.java
Patch:
@@ -6,8 +6,7 @@
 
 /**
  * Provides access to configuration properties.
- *
- * @implNote Actual implementation uses {@code org.springframework.boot.context.properties.bind.Binder} class
+ *Actual implementation uses {@code org.springframework.boot.context.properties.bind.Binder} class
  * to bind values to target types. Target type params can be custom configs classes, not only simple types and strings.
  *
  */

File: kafka-ui-serde-api/src/main/java/com/provectus/kafka/ui/serde/api/Serde.java
Patch:
@@ -7,15 +7,15 @@
  * Main interface of  serialization/deserialization logic.
  * It provides ability to serialize, deserialize topic's keys and values, and optionally provides
  * information about data schema inside topic.
- * <p/>
+ * <p>
  * <b>Lifecycle:</b><br/>
  * 1. on application startup kafka-ui scans configs and finds all custom serde definitions<br/>
  * 2. for each custom serde its own separated child-first classloader is created<br/>
  * 3. kafka-ui loads class defined in configuration and instantiates instance of that class using default, non-arg constructor<br/>
  * 4. {@code configure(...)} method called<br/>
  * 5. various methods called during application runtime<br/>
  * 6. on application shutdown kafka-ui calls {@code close()} method on serde instance<br/>
- * <p/>
+ * <p>
  * <b>Implementation considerations:</b><br/>
  * 1. Implementation class should have default/non-arg contructor<br/>
  * 2. All methods except {@code configure(...)} and {@code close()} can be called from different threads. So, your code should be thread-safe.<br/>

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/AdminClientServiceImpl.java
Patch:
@@ -37,7 +37,7 @@ private Mono<ReactiveAdminClient> createAdminClient(KafkaCluster cluster) {
       properties
           .put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, cluster.getBootstrapServers());
       properties.put(AdminClientConfig.REQUEST_TIMEOUT_MS_CONFIG, clientTimeout);
-      properties.putIfAbsent(AdminClientConfig.CLIENT_ID_CONFIG, "kafka-ui-app");
+      properties.putIfAbsent(AdminClientConfig.CLIENT_ID_CONFIG, "kafka-ui-admin-client-" + System.currentTimeMillis());
       return AdminClient.create(properties);
     })
         .flatMap(ReactiveAdminClient::create)

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/ConsumerGroupService.java
Patch:
@@ -11,7 +11,6 @@
 import java.util.List;
 import java.util.Map;
 import java.util.Properties;
-import java.util.UUID;
 import java.util.function.ToIntFunction;
 import java.util.stream.Collectors;
 import javax.annotation.Nullable;
@@ -197,7 +196,7 @@ public KafkaConsumer<Bytes, Bytes> createConsumer(KafkaCluster cluster,
                                                     Map<String, Object> properties) {
     Properties props = new Properties();
     props.putAll(cluster.getProperties());
-    props.put(ConsumerConfig.CLIENT_ID_CONFIG, "kafka-ui-" + UUID.randomUUID());
+    props.put(ConsumerConfig.CLIENT_ID_CONFIG, "kafka-ui-consumer-" + System.currentTimeMillis());
     props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, cluster.getBootstrapServers());
     props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, BytesDeserializer.class);
     props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, BytesDeserializer.class);

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/config/ClustersProperties.java
Patch:
@@ -40,6 +40,7 @@ public static class Cluster {
     String defaultKeySerde;
     String defaultValueSerde;
     List<Masking> masking = new ArrayList<>();
+    long pollingThrottleRate = 0;
   }
 
   @Data

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/controller/MessagesController.java
Patch:
@@ -30,6 +30,7 @@
 import org.springframework.web.server.ServerWebExchange;
 import reactor.core.publisher.Flux;
 import reactor.core.publisher.Mono;
+import reactor.core.scheduler.Schedulers;
 
 @RestController
 @RequiredArgsConstructor
@@ -135,6 +136,7 @@ public Mono<ResponseEntity<TopicSerdeSuggestionDTO>> getSerdes(String clusterNam
             .value(use == SerdeUsageDTO.SERIALIZE
                 ? deserializationService.getSerdesForSerialize(getCluster(clusterName), topicName, VALUE)
                 : deserializationService.getSerdesForDeserialize(getCluster(clusterName), topicName, VALUE))
-    ).map(ResponseEntity::ok);
+    ).subscribeOn(Schedulers.boundedElastic())
+        .map(ResponseEntity::ok);
   }
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/model/KafkaCluster.java
Patch:
@@ -1,8 +1,10 @@
 package com.provectus.kafka.ui.model;
 
 import com.provectus.kafka.ui.service.masking.DataMasking;
+import com.provectus.kafka.ui.util.PollingThrottler;
 import java.util.List;
 import java.util.Properties;
+import java.util.function.Supplier;
 import lombok.AccessLevel;
 import lombok.AllArgsConstructor;
 import lombok.Builder;
@@ -23,4 +25,5 @@ public class KafkaCluster {
   private final boolean disableLogDirsCollection;
   private final MetricsConfig metricsConfig;
   private final DataMasking masking;
+  private final Supplier<PollingThrottler> throttler;
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/TopicsService.java
Patch:
@@ -51,7 +51,6 @@
 public class TopicsService {
 
   private final AdminClientService adminClientService;
-  private final DeserializationService deserializationService;
   private final StatisticsCache statisticsCache;
   @Value("${topic.recreate.maxRetries:15}")
   private int recreateMaxRetries;

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/emitter/TailingEmitterTest.java
Patch:
@@ -143,4 +143,4 @@ private void waitUntilTailingInitialized(List<TopicMessageEventDTO> fluxOutput)
             .anyMatch(msg -> msg.getType() == TopicMessageEventDTO.TypeEnum.CONSUMING));
   }
 
-}
\ No newline at end of file
+}

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/serdes/SerdesInitializer.java
Patch:
@@ -171,7 +171,7 @@ private SerdeInstance createSerdeWithBuiltInSerdeName(SerdeConfig serdeConfig,
     var clazz = builtInSerdeClasses.get(name);
     BuiltInSerde serde = createSerdeInstance(clazz);
     if (serdeConfig.getProperties().isEmpty()) {
-      if (!autoConfigureSerde(serde, serdeProps, globalProps)) {
+      if (!autoConfigureSerde(serde, clusterProps, globalProps)) {
         // no properties provided and serde does not support auto-configuration
         throw new ValidationException(name + " serde is not configured");
       }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/serdes/SerdeInstance.java
Patch:
@@ -16,9 +16,9 @@
 public class SerdeInstance implements Closeable {
 
   @Getter
-  private final String name;
+  final String name;
 
-  private final Serde serde;
+  final Serde serde;
 
   @Nullable
   final Pattern topicKeyPattern;
@@ -27,7 +27,7 @@ public class SerdeInstance implements Closeable {
   final Pattern topicValuePattern;
 
   @Nullable // will be set for custom serdes
-  private final ClassLoader classLoader;
+  final ClassLoader classLoader;
 
   private <T> T wrapWithClassloader(Supplier<T> call) {
     if (classLoader == null) {

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/DeserializationService.java
Patch:
@@ -9,18 +9,17 @@
 import com.provectus.kafka.ui.serdes.ConsumerRecordDeserializer;
 import com.provectus.kafka.ui.serdes.ProducerRecordCreator;
 import com.provectus.kafka.ui.serdes.SerdeInstance;
+import com.provectus.kafka.ui.serdes.SerdesInitializer;
 import java.io.Closeable;
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
 import java.util.concurrent.ConcurrentHashMap;
 import javax.annotation.Nullable;
 import javax.validation.ValidationException;
-import lombok.extern.slf4j.Slf4j;
 import org.springframework.core.env.Environment;
 import org.springframework.stereotype.Component;
 
-@Slf4j
 @Component
 public class DeserializationService implements Closeable {
 
@@ -29,10 +28,11 @@ public class DeserializationService implements Closeable {
   public DeserializationService(Environment env,
                                 ClustersStorage clustersStorage,
                                 ClustersProperties clustersProperties) {
+    var serdesInitializer = new SerdesInitializer();
     for (int i = 0; i < clustersProperties.getClusters().size(); i++) {
       var clusterProperties = clustersProperties.getClusters().get(i);
       var cluster = clustersStorage.getClusterByName(clusterProperties.getName()).get();
-      clusterSerdes.put(cluster.getName(), new ClusterSerdes(env, clustersProperties, i));
+      clusterSerdes.put(cluster.getName(), serdesInitializer.init(env, clustersProperties, i));
     }
   }
 

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/BasePage.java
Patch:
@@ -18,7 +18,7 @@ public abstract class BasePage extends WebUtils {
   protected SelenideElement alertMessage = $x("//div[@role='alert']//div[@role='contentinfo']");
   protected String summaryCellLocator = "//div[contains(text(),'%s')]";
   protected String tableElementNameLocator = "//tbody//a[contains(text(),'%s')]";
-  protected String сolumnHeaderLocator = "//table//tr/th/div[text()='%s']";
+  protected String columnHeaderLocator = "//table//tr/th/div[text()='%s']";
 
   protected void waitUntilSpinnerDisappear() {
     log.debug("\nwaitUntilSpinnerDisappear");

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/brokers/BrokersDetails.java
Patch:
@@ -26,13 +26,13 @@ public BrokersDetails waitUntilScreenReady() {
 
   private List<SelenideElement> getVisibleColumnHeaders() {
     return Stream.of("Name", "Topics", "Error", "Partitions")
-        .map(name -> $x(String.format(сolumnHeaderLocator, name)))
+        .map(name -> $x(String.format(columnHeaderLocator, name)))
         .collect(Collectors.toList());
   }
 
   private List<SelenideElement> getEnabledColumnHeaders() {
     return Stream.of("Name", "Error")
-        .map(name -> $x(String.format(сolumnHeaderLocator, name)))
+        .map(name -> $x(String.format(columnHeaderLocator, name)))
         .collect(Collectors.toList());
   }
 

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/connector/KafkaConnectList.java
Patch:
@@ -5,12 +5,9 @@
 import com.codeborne.selenide.Condition;
 import com.codeborne.selenide.SelenideElement;
 import com.provectus.kafka.ui.pages.BasePage;
-import com.provectus.kafka.ui.utilities.WaitUtils;
 import io.qameta.allure.Step;
-import lombok.experimental.ExtensionMethod;
 
 
-@ExtensionMethod(WaitUtils.class)
 public class KafkaConnectList extends BasePage {
 
     protected SelenideElement createConnectorBtn = $x("//button[contains(text(),'Create Connector')]");

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/topic/TopicDetails.java
Patch:
@@ -10,15 +10,12 @@
 import com.codeborne.selenide.ElementsCollection;
 import com.codeborne.selenide.SelenideElement;
 import com.provectus.kafka.ui.pages.BasePage;
-import com.provectus.kafka.ui.utilities.WaitUtils;
 import io.qameta.allure.Step;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.List;
-import lombok.experimental.ExtensionMethod;
 import org.openqa.selenium.By;
 
-@ExtensionMethod({WaitUtils.class})
 public class TopicDetails extends BasePage {
 
   protected SelenideElement clearMessagesBtn = $x(("//div[contains(text(), 'Clear messages')]"));

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/BasePage.java
Patch:
@@ -16,7 +16,9 @@ public abstract class BasePage extends WebUtils {
   protected SelenideElement dotMenuBtn = $x("//button[@aria-label='Dropdown Toggle']");
   protected SelenideElement alertHeader = $x("//div[@role='alert']//div[@role='heading']");
   protected SelenideElement alertMessage = $x("//div[@role='alert']//div[@role='contentinfo']");
+  protected String summaryCellLocator = "//div[contains(text(),'%s')]";
   protected String tableElementNameLocator = "//tbody//a[contains(text(),'%s')]";
+  protected String сolumnHeaderLocator = "//table//tr/th/div[text()='%s']";
 
   protected void waitUntilSpinnerDisappear() {
     log.debug("\nwaitUntilSpinnerDisappear");

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/topic/TopicsList.java
Patch:
@@ -21,7 +21,6 @@ public class TopicsList extends BasePage {
     protected SelenideElement addTopicBtn = $x("//button[normalize-space(text()) ='Add a Topic']");
     protected SelenideElement searchField = $x("//input[@placeholder='Search by Topic Name']");
     protected SelenideElement showInternalRadioBtn = $x("//input[@name='ShowInternalTopics']");
-    protected String сolumnHeaderLocator = "//table//tr/th/div[text()='%s']";
     protected String actionButtonLocator = "//button[text()='%s']";
 
     @Step

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/topic/TopicDetails.java
Patch:
@@ -3,6 +3,7 @@
 import static com.codeborne.selenide.Selenide.$;
 import static com.codeborne.selenide.Selenide.$$x;
 import static com.codeborne.selenide.Selenide.$x;
+import static org.apache.commons.lang.math.RandomUtils.nextInt;
 
 import com.codeborne.selenide.CollectionCondition;
 import com.codeborne.selenide.Condition;
@@ -142,7 +143,7 @@ public TopicDetails.MessageGridItem getMessage(int offset) {
 
   @Step
   public TopicDetails.MessageGridItem getRandomMessage() {
-    return getMessage(initItems().size() - 1);
+    return getMessage(nextInt(initItems().size() - 1));
   }
 
   public static class MessageGridItem extends BasePage {

File: kafka-ui-e2e-checks/src/test/java/com/provectus/kafka/ui/suite/SmokeTests.java
Patch:
@@ -1,4 +1,4 @@
-package com.provectus.kafka.ui;
+package com.provectus.kafka.ui.suite;
 
 import com.codeborne.selenide.Condition;
 import com.provectus.kafka.ui.base.BaseTest;
@@ -29,4 +29,4 @@ public void checkBasePageElements(){
                                 .as(element.getSearchCriteria() + " isEnabled()").isTrue());
         softly.assertAll();
     }
-}
\ No newline at end of file
+}

File: kafka-ui-e2e-checks/src/test/java/com/provectus/kafka/ui/suite/connectors/ConnectorsTests.java
Patch:
@@ -1,4 +1,4 @@
-package com.provectus.kafka.ui.tests;
+package com.provectus.kafka.ui.suite.connectors;
 
 import static com.provectus.kafka.ui.pages.BasePage.AlertHeader.SUCCESS;
 import static com.provectus.kafka.ui.pages.NaviSideBar.SideMenuOption.KAFKA_CONNECT;

File: kafka-ui-e2e-checks/src/test/java/com/provectus/kafka/ui/suite/schemas/SchemasTests.java
Patch:
@@ -1,4 +1,4 @@
-package com.provectus.kafka.ui.tests;
+package com.provectus.kafka.ui.suite.schemas;
 
 import static com.provectus.kafka.ui.pages.NaviSideBar.SideMenuOption.SCHEMA_REGISTRY;
 import static com.provectus.kafka.ui.settings.Source.CLUSTER_NAME;

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/topic/TopicDetails.java
Patch:
@@ -21,7 +21,7 @@ public class TopicDetails {
     protected SelenideElement clearMessagesBtn = $x(("//div[contains(text(), 'Clear messages')]"));
     protected SelenideElement overviewTab = $x("//a[contains(text(),'Overview')]");
     protected SelenideElement messagesTab = $x("//a[contains(text(),'Messages')]");
-    protected SelenideElement editSettingsTab = $x("//li[@role][contains(text(),'Edit settings')]");
+    protected SelenideElement editSettingsMenu = $x("//li[@role][contains(text(),'Edit settings')]");
     protected SelenideElement removeTopicBtn = $x("//ul[@role='menu']//div[contains(text(),'Remove Topic')]");
     protected SelenideElement confirmBtn = $x("//div[@role='dialog']//button[contains(text(),'Confirm')]");
     protected SelenideElement produceMessageBtn = $x("//div//button[text()='Produce Message']");
@@ -39,7 +39,7 @@ public TopicDetails waitUntilScreenReady() {
     }
 
     @Step
-    public TopicDetails openTopicMenu(TopicMenu menu) {
+    public TopicDetails openDetailsTab(TopicMenu menu) {
         $(By.linkText(menu.getValue())).shouldBe(Condition.visible).click();
         return this;
     }
@@ -52,7 +52,7 @@ public TopicDetails openDotMenu() {
 
     @Step
     public TopicDetails clickEditSettingsMenu() {
-        editSettingsTab.shouldBe(Condition.visible).click();
+        editSettingsMenu.shouldBe(Condition.visible).click();
         return this;
     }
 

File: kafka-ui-e2e-checks/src/test/java/com/provectus/kafka/ui/tests/TopicTests.java
Patch:
@@ -175,7 +175,7 @@ void produceMessage() {
                 .openTopic(TOPIC_FOR_MESSAGES.getName());
         topicDetails
                 .waitUntilScreenReady()
-                .openTopicMenu(TopicDetails.TopicMenu.MESSAGES)
+                .openDetailsTab(TopicDetails.TopicMenu.MESSAGES)
                 .clickProduceMessageBtn();
         produceMessagePanel
                 .waitUntilScreenReady()
@@ -204,7 +204,7 @@ void clearMessage() {
                 .openTopic(TOPIC_FOR_MESSAGES.getName());
         topicDetails
                 .waitUntilScreenReady()
-                .openTopicMenu(TopicDetails.TopicMenu.OVERVIEW)
+                .openDetailsTab(TopicDetails.TopicMenu.OVERVIEW)
                 .clickProduceMessageBtn();
         produceMessagePanel
                 .waitUntilScreenReady()
@@ -238,7 +238,7 @@ void redirectToConsumerFromTopic() {
                 .openTopic(topicName);
         topicDetails
                 .waitUntilScreenReady()
-                .openTopicMenu(TopicDetails.TopicMenu.CONSUMERS)
+                .openDetailsTab(TopicDetails.TopicMenu.CONSUMERS)
                 .openConsumerGroup(consumerGroupId);
         consumersDetails
                 .waitUntilScreenReady();

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/models/Schema.java
Patch:
@@ -14,19 +14,19 @@ public class Schema {
     private SchemaType type;
 
     public static Schema createSchemaAvro(){
-        return new Schema().setName(randomAlphabetic(10))
+        return new Schema().setName("schema_avro-" + randomAlphabetic(5))
                 .setType(SchemaType.AVRO)
                 .setValuePath(System.getProperty("user.dir") + "/src/main/resources/testData/schema_avro_value.json");
     }
 
     public static Schema createSchemaJson(){
-        return new Schema().setName(randomAlphabetic(10))
+        return new Schema().setName("schema_json-" + randomAlphabetic(5))
                 .setType(SchemaType.JSON)
                 .setValuePath(System.getProperty("user.dir") + "/src/main/resources/testData/schema_Json_Value.json");
     }
 
     public static Schema createSchemaProtobuf(){
-        return new Schema().setName(randomAlphabetic(10))
+        return new Schema().setName("schema_protobuf-" + randomAlphabetic(5))
                 .setType(SchemaType.PROTOBUF)
                 .setValuePath(System.getProperty("user.dir") + "/src/main/resources/testData/schema_protobuf_value.txt");
     }

File: kafka-ui-e2e-checks/src/test/java/com/provectus/kafka/ui/base/Facade.java
Patch:
@@ -2,6 +2,7 @@
 
 import com.provectus.kafka.ui.helpers.ApiHelper;
 import com.provectus.kafka.ui.pages.NaviSideBar;
+import com.provectus.kafka.ui.pages.TopPanel;
 import com.provectus.kafka.ui.pages.connector.ConnectorCreateForm;
 import com.provectus.kafka.ui.pages.connector.ConnectorDetails;
 import com.provectus.kafka.ui.pages.connector.KafkaConnectList;
@@ -30,4 +31,5 @@ public abstract class Facade {
     protected ConsumersDetails consumersDetails = new ConsumersDetails();
     protected ConsumersList consumersList = new ConsumersList();
     protected NaviSideBar naviSideBar = new NaviSideBar();
+    protected TopPanel topPanel = new TopPanel();
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/emitter/BackwardRecordEmitter.java
Patch:
@@ -80,7 +80,7 @@ public void accept(FluxSink<TopicMessageEventDTO> sink) {
           log.debug("sink is cancelled after partitions poll iteration");
         }
       }
-      sink.complete();
+      sendFinishStatsAndCompleteSink(sink);
       log.debug("Polling finished");
     } catch (Exception e) {
       log.error("Error occurred while consuming records", e);

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/emitter/ForwardRecordEmitter.java
Patch:
@@ -54,7 +54,7 @@ public void accept(FluxSink<TopicMessageEventDTO> sink) {
           }
         }
       }
-      sink.complete();
+      sendFinishStatsAndCompleteSink(sink);
       log.info("Polling finished");
     } catch (Exception e) {
       log.error("Error occurred while consuming records", e);

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/AdminClientServiceImpl.java
Patch:
@@ -37,6 +37,7 @@ private Mono<ReactiveAdminClient> createAdminClient(KafkaCluster cluster) {
       properties
           .put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, cluster.getBootstrapServers());
       properties.put(AdminClientConfig.REQUEST_TIMEOUT_MS_CONFIG, clientTimeout);
+      properties.putIfAbsent(AdminClientConfig.CLIENT_ID_CONFIG, "kafka-ui-app");
       return AdminClient.create(properties);
     })
         .flatMap(ReactiveAdminClient::create)

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/ConsumerGroupService.java
Patch:
@@ -49,8 +49,8 @@ private Mono<List<InternalConsumerGroup>> getConsumerGroups(
           var tpsFromGroupOffsets = groupOffsetsMap.values().stream()
               .flatMap(v -> v.keySet().stream())
               .collect(Collectors.toSet());
-          // 2. getting end offsets for partitions with in committed offsets
-          return ac.listOffsets(tpsFromGroupOffsets, OffsetSpec.latest())
+          // 2. getting end offsets for partitions with committed offsets
+          return ac.listOffsets(tpsFromGroupOffsets, OffsetSpec.latest(), false)
               .map(endOffsets ->
                   descriptions.stream()
                       .map(desc -> {
@@ -68,7 +68,7 @@ public Mono<List<InternalTopicConsumerGroup>> getConsumerGroupsForTopic(KafkaClu
                                                                           String topic) {
     return adminClientService.get(cluster)
         // 1. getting topic's end offsets
-        .flatMap(ac -> ac.listOffsets(topic, OffsetSpec.latest())
+        .flatMap(ac -> ac.listTopicOffsets(topic, OffsetSpec.latest(), false)
             .flatMap(endOffsets -> {
               var tps = new ArrayList<>(endOffsets.keySet());
               // 2. getting all consumer groups

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/MessagesService.java
Patch:
@@ -65,8 +65,8 @@ public Mono<Void> deleteTopicMessages(KafkaCluster cluster, String topicName,
   private Mono<Map<TopicPartition, Long>> offsetsForDeletion(KafkaCluster cluster, String topicName,
                                                              List<Integer> partitionsToInclude) {
     return adminClientService.get(cluster).flatMap(ac ->
-        ac.listOffsets(topicName, OffsetSpec.earliest())
-            .zipWith(ac.listOffsets(topicName, OffsetSpec.latest()),
+        ac.listTopicOffsets(topicName, OffsetSpec.earliest(), true)
+            .zipWith(ac.listTopicOffsets(topicName, OffsetSpec.latest(), true),
                 (start, end) ->
                     end.entrySet().stream()
                         .filter(e -> partitionsToInclude.isEmpty()

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/model/InternalTopic.java
Patch:
@@ -42,9 +42,7 @@ public static InternalTopic from(TopicDescription topicDescription,
                                    Metrics metrics,
                                    InternalLogDirStats logDirInfo) {
     var topic = InternalTopic.builder();
-    topic.internal(
-        topicDescription.isInternal() || topicDescription.name().startsWith("_")
-    );
+    topic.internal(topicDescription.isInternal());
     topic.name(topicDescription.name());
 
     List<InternalPartition> partitions = topicDescription.partitions().stream()

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/TopicsService.java
Patch:
@@ -68,7 +68,7 @@ public Mono<List<InternalTopic>> loadTopics(KafkaCluster c, List<String> topics)
     }
     return adminClientService.get(c)
         .flatMap(ac ->
-            ac.describeTopics(topics).zipWith(ac.getTopicsConfig(topics),
+            ac.describeTopics(topics).zipWith(ac.getTopicsConfig(topics, false),
                 (descriptions, configs) -> {
                   statisticsCache.update(c, descriptions, configs);
                   return getPartitionOffsets(descriptions, ac).map(offsets -> {
@@ -160,7 +160,7 @@ public Mono<InternalTopic> getTopicDetails(KafkaCluster cluster, String topicNam
 
   public Mono<List<ConfigEntry>> getTopicConfigs(KafkaCluster cluster, String topicName) {
     return adminClientService.get(cluster)
-        .flatMap(ac -> ac.getTopicsConfig(List.of(topicName)))
+        .flatMap(ac -> ac.getTopicsConfig(List.of(topicName), true))
         .map(m -> m.values().stream().findFirst().orElseThrow(TopicNotFoundException::new));
   }
 

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/KafkaConsumerTests.java
Patch:
@@ -3,10 +3,10 @@
 import static org.assertj.core.api.Assertions.assertThat;
 import static org.springframework.http.MediaType.TEXT_EVENT_STREAM;
 
-import com.provectus.kafka.ui.api.model.TopicConfig;
 import com.provectus.kafka.ui.model.BrokerConfigDTO;
 import com.provectus.kafka.ui.model.PartitionsIncreaseDTO;
 import com.provectus.kafka.ui.model.PartitionsIncreaseResponseDTO;
+import com.provectus.kafka.ui.model.TopicConfigDTO;
 import com.provectus.kafka.ui.model.TopicCreationDTO;
 import com.provectus.kafka.ui.model.TopicDetailsDTO;
 import com.provectus.kafka.ui.model.TopicMessageEventDTO;
@@ -206,12 +206,12 @@ public void shouldRetrieveTopicConfig() {
             .expectStatus()
             .isOk();
 
-    List<TopicConfig> configs = webTestClient.get()
+    List<TopicConfigDTO> configs = webTestClient.get()
             .uri("/api/clusters/{clusterName}/topics/{topicName}/config", LOCAL, topicName)
             .exchange()
             .expectStatus()
             .isOk()
-            .expectBodyList(TopicConfig.class)
+            .expectBodyList(TopicConfigDTO.class)
             .returnResult()
             .getResponseBody();
 

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/connector/KafkaConnectList.java
Patch:
@@ -35,7 +35,8 @@ public KafkaConnectList clickCreateConnectorBtn() {
 
     @Step
     public KafkaConnectList openConnector(String connectorName) {
-        $x(String.format(tabElementLocator,connectorName)).shouldBe(Condition.visible).click();
+        $x(String.format(tabElementLocator,connectorName))
+                .shouldBe(Condition.enabled).click();
         return this;
     }
 

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/schema/SchemaRegistryList.java
Patch:
@@ -30,7 +30,8 @@ public SchemaRegistryList clickCreateSchema() {
 
     @Step
     public SchemaRegistryList openSchema(String schemaName) {
-        $x(String.format(schemaTabElementLocator,schemaName)).shouldBe(Condition.visible).click();
+        $x(String.format(schemaTabElementLocator,schemaName))
+                .shouldBe(Condition.enabled).click();
         return this;
     }
 

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/topic/TopicsList.java
Patch:
@@ -42,7 +42,8 @@ public boolean isTopicVisible(String topicName) {
 
     @Step
     public TopicsList openTopic(String topicName) {
-        $(By.linkText(topicName)).click();
+        $(By.linkText(topicName))
+                .shouldBe(Condition.enabled).click();
         return this;
     }
 }

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/emitter/TailingEmitterTest.java
Patch:
@@ -111,10 +111,11 @@ private Flux<TopicMessageEventDTO> createTailingFlux(
 
     return applicationContext.getBean(MessagesService.class)
         .loadMessages(cluster, topicName,
-            new ConsumerPosition(SeekTypeDTO.LATEST, Map.of(), SeekDirectionDTO.TAILING),
+            new ConsumerPosition(SeekTypeDTO.LATEST, topic, null),
             query,
             MessageFilterTypeDTO.STRING_CONTAINS,
             0,
+            SeekDirectionDTO.TAILING,
             "String",
             "String");
   }
@@ -137,7 +138,7 @@ private void waitUntilTailingInitialized(List<TopicMessageEventDTO> fluxOutput)
     Awaitility.await()
         .pollInSameThread()
         .pollDelay(Duration.ofMillis(100))
-        .atMost(Duration.ofSeconds(10))
+        .atMost(Duration.ofSeconds(200))
         .until(() -> fluxOutput.stream()
             .anyMatch(msg -> msg.getType() == TopicMessageEventDTO.TypeEnum.CONSUMING));
   }

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/service/MessagesServiceTest.java
Patch:
@@ -45,7 +45,7 @@ void sendMessageReturnsExceptionWhenTopicNotFound() {
   @Test
   void loadMessagesReturnsExceptionWhenTopicNotFound() {
     StepVerifier.create(messagesService
-            .loadMessages(cluster, NON_EXISTING_TOPIC, null, null, null, 1, "String", "String"))
+            .loadMessages(cluster, NON_EXISTING_TOPIC, null, null, null, 1, null, "String", "String"))
         .expectError(TopicNotFoundException.class)
         .verify();
   }

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/service/SendAndReadTests.java
Patch:
@@ -502,12 +502,13 @@ public void doAssert(Consumer<TopicMessageDTO> msgAssert) {
                 topic,
                 new ConsumerPosition(
                     SeekTypeDTO.BEGINNING,
-                    Map.of(new TopicPartition(topic, 0), 0L),
-                    SeekDirectionDTO.FORWARD
+                    topic,
+                    Map.of(new TopicPartition(topic, 0), 0L)
                 ),
                 null,
                 null,
                 1,
+                SeekDirectionDTO.FORWARD,
                 msgToSend.getKeySerde().get(),
                 msgToSend.getValueSerde().get()
             ).filter(e -> e.getType().equals(TopicMessageEventDTO.TypeEnum.MESSAGE))

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/models/Topic.java
Patch:
@@ -6,5 +6,5 @@
 @Data
 @Accessors(chain = true)
 public class Topic {
-    private String name, compactPolicyValue, timeToRetainData, maxSizeOnDisk, maxMessageBytes, messageKey, messageContent ;
+    private String name, cleanupPolicyValue, timeToRetainData, maxSizeOnDisk, maxMessageBytes, messageKey, messageContent ;
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/FeatureService.java
Patch:
@@ -60,6 +60,6 @@ private Mono<Boolean> isTopicDeletionEnabled(KafkaCluster cluster, Node controll
                 .filter(e -> e.name().equals(DELETE_TOPIC_ENABLED_SERVER_PROPERTY))
                 .map(e -> Boolean.parseBoolean(e.value()))
                 .findFirst()
-                .orElse(false));
+                .orElse(true));
   }
 }

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/connector/KafkaConnectList.java
Patch:
@@ -28,14 +28,14 @@ public ConnectorCreateForm clickCreateConnectorButton() {
 
     @Step
     public KafkaConnectList openConnector(String connectorName) {
-        $(By.linkText(connectorName)).click();
+        $x("//tbody//td[1][text()='" + connectorName + "']").shouldBe(Condition.enabled).click();
         return this;
     }
 
     @Step
     public boolean isConnectorVisible(String connectorName) {
         $(By.xpath("//table")).shouldBe(Condition.visible);
-        return isVisible($x("//tbody//td[1]//a[text()='" + connectorName + "']"));
+        return isVisible($x("//tbody//td[1][text()='" + connectorName + "']"));
     }
 
     @Step

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/connector/ConnectorDetails.java
Patch:
@@ -22,7 +22,6 @@ public class ConnectorDetails {
     public ConnectorDetails waitUntilScreenReady() {
         $(By.xpath("//a[text() ='Tasks']")).shouldBe(Condition.visible);
         $(By.xpath("//a[text() ='Config']")).shouldBe(Condition.visible);
-        $(By.xpath("//a[text() ='Overview']")).shouldBe(Condition.visible);
         return this;
     }
 

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/utilities/qaseIoUtils/QaseExtension.java
Patch:
@@ -37,7 +37,6 @@ public class QaseExtension implements TestExecutionListener {
     private static final String QASE_PROJECT = "KAFKAUI";
     private static final String QASE_ENABLE = "true";
 
-
     static {
         String qaseApiToken = System.getProperty("QASEIO_API_TOKEN");
         if (qaseApiToken == null || StringUtils.isEmpty(qaseApiToken)) {

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/topic/ProduceMessagePanel.java
Patch:
@@ -43,6 +43,6 @@ public TopicDetails submitProduceMessage() {
         submitBtn.shouldBe(Condition.enabled).click();
         submitBtn.shouldBe(Condition.disappear);
         refresh();
-        return new TopicDetails().waitUntilScreenReady();
+        return new TopicDetails();
     }
 }

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/settings/Source.java
Patch:
@@ -4,4 +4,5 @@ public abstract class Source {
 
   public static String BASE_API_URL = System.getProperty("BASE_URL", "http://localhost:8080");
   public static String BASE_WEB_URL = System.getProperty("BASE_DOCKER_URL", "http://host.testcontainers.internal:8080");
+  public static final String CLUSTER_NAME = "local";
 }

File: kafka-ui-e2e-checks/src/test/java/com/provectus/kafka/ui/SmokeTests.java
Patch:
@@ -14,8 +14,6 @@ public class SmokeTests extends BaseTest {
     @CaseId(198)
     @DisplayName("main page should load")
     void mainPageLoads() {
-        mainPage.goTo()
-                .waitUntilScreenReady();
         compareScreenshots("main");
     }
 }

File: kafka-ui-e2e-checks/src/test/java/com/provectus/kafka/ui/base/Facade.java
Patch:
@@ -1,7 +1,7 @@
 package com.provectus.kafka.ui.base;
 
 import com.provectus.kafka.ui.helpers.ApiHelper;
-import com.provectus.kafka.ui.pages.MainPage;
+import com.provectus.kafka.ui.pages.NaviSideBar;
 import com.provectus.kafka.ui.pages.connector.ConnectorCreateForm;
 import com.provectus.kafka.ui.pages.connector.ConnectorDetails;
 import com.provectus.kafka.ui.pages.connector.KafkaConnectList;
@@ -14,7 +14,6 @@
 import com.provectus.kafka.ui.pages.topic.TopicsList;
 
 public abstract class Facade {
-    protected MainPage mainPage = new MainPage();
     protected ApiHelper apiHelper = new ApiHelper();
     protected ConnectorCreateForm connectorCreateForm = new ConnectorCreateForm();
     protected KafkaConnectList kafkaConnectList = new KafkaConnectList();
@@ -26,4 +25,5 @@ public abstract class Facade {
     protected TopicCreateEditForm topicCreateEditForm = new TopicCreateEditForm();
     protected TopicsList topicsList = new TopicsList();
     protected TopicDetails topicDetails = new TopicDetails();
+    protected NaviSideBar naviSideBar = new NaviSideBar();
 }

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/topic/ProduceMessagePanel.java
Patch:
@@ -39,10 +39,10 @@ public ProduceMessagePanel setHeaderFiled(String value) {
     }
 
     @Step
-    public TopicView submitProduceMessage() {
+    public TopicDetails submitProduceMessage() {
         submitBtn.shouldBe(Condition.enabled).click();
         submitBtn.shouldBe(Condition.disappear);
         refresh();
-        return new TopicView().waitUntilScreenReady();
+        return new TopicDetails().waitUntilScreenReady();
     }
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/serdes/builtin/sr/SchemaRegistrySerde.java
Patch:
@@ -142,6 +142,7 @@ public Optional<SchemaDescription> getSchema(String topic, Target type) {
             new SchemaDescription(
                 convertSchema(schemaMetadata),
                 Map.of(
+                    "subject", subject,
                     "schemaId", schemaMetadata.getId(),
                     "latestVersion", schemaMetadata.getVersion(),
                     "type", schemaMetadata.getSchemaType() // AVRO / PROTOBUF / JSON

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/connector/ConnectorCreateView.java
Patch:
@@ -7,7 +7,7 @@
 
 import static com.codeborne.selenide.Selenide.$;
 import static com.codeborne.selenide.Selenide.sleep;
-import static com.provectus.kafka.ui.utilities.WebUtils.javaExecutorClick;
+import static com.provectus.kafka.ui.utilities.WebUtils.clickByJavaScript;
 import static com.provectus.kafka.ui.utilities.screenshots.Screenshooter.log;
 
 public class ConnectorCreateView {
@@ -25,7 +25,7 @@ public ConnectorsView setConnectorConfig(String connectName, String configJson)
         contentTextArea.setValue("");
         contentTextArea.setValue(String.valueOf(configJson.toCharArray()));
         nameField.click();
-        javaExecutorClick(submitButton);
+        clickByJavaScript(submitButton);
         sleep(4000);
         log.info("Connector config is submitted");
         return new ConnectorsView();

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/connector/ConnectorsList.java
Patch:
@@ -11,7 +11,7 @@
 import static com.codeborne.selenide.Selenide.$;
 import static com.codeborne.selenide.Selenide.$x;
 import static com.provectus.kafka.ui.utilities.WebUtils.isVisible;
-import static com.provectus.kafka.ui.utilities.WebUtils.javaExecutorClick;
+import static com.provectus.kafka.ui.utilities.WebUtils.clickByJavaScript;
 
 @ExtensionMethod(WaitUtils.class)
 public class ConnectorsList {
@@ -32,7 +32,7 @@ public ConnectorsList waitUntilScreenReady() {
 
     @Step("Click on button 'Create Connector'")
     public ConnectorCreateView clickCreateConnectorButton() {
-        javaExecutorClick($x("//button[text()='Create Connector']"));
+        clickByJavaScript($x("//button[text()='Create Connector']"));
         return new ConnectorCreateView();
     }
 

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/schema/SchemaCreateView.java
Patch:
@@ -7,7 +7,7 @@
 
 import static com.codeborne.selenide.Selenide.$;
 import static com.codeborne.selenide.Selenide.$x;
-import static com.provectus.kafka.ui.utilities.WebUtils.javaExecutorClick;
+import static com.provectus.kafka.ui.utilities.WebUtils.clickByJavaScript;
 
 public class SchemaCreateView {
 
@@ -22,7 +22,7 @@ public SchemaCreateView selectSchemaTypeFromDropdown(SchemaType schemaType) {
     }
     @Step
     public SchemaView clickSubmit() {
-        javaExecutorClick(submitSchemaButton);
+        clickByJavaScript(submitSchemaButton);
         return new SchemaView();
     }
     @Step

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/schema/SchemaEditView.java
Patch:
@@ -11,7 +11,7 @@
 
 import static com.codeborne.selenide.Selenide.$;
 import static com.codeborne.selenide.Selenide.$x;
-import static com.provectus.kafka.ui.utilities.WebUtils.javaExecutorClick;
+import static com.provectus.kafka.ui.utilities.WebUtils.clickByJavaScript;
 
 public class SchemaEditView {
 
@@ -32,7 +32,7 @@ public SchemaEditView selectCompatibilityLevelFromDropdown(CompatibilityLevel.Co
     }
     @Step
     public SchemaView clickSubmit() {
-        javaExecutorClick($(By.xpath("//button[@type='submit']")));
+        clickByJavaScript($(By.xpath("//button[@type='submit']")));
         return new SchemaView();
     }
 

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/schema/SchemaRegistryList.java
Patch:
@@ -7,15 +7,15 @@
 
 import static com.codeborne.selenide.Selenide.*;
 import static com.provectus.kafka.ui.utilities.WebUtils.isVisible;
-import static com.provectus.kafka.ui.utilities.WebUtils.javaExecutorClick;
+import static com.provectus.kafka.ui.utilities.WebUtils.clickByJavaScript;
 
 public class SchemaRegistryList {
 
     private final SelenideElement schemaButton = $(By.xpath("//*[contains(text(),'Create Schema')]"));
 
     @Step
     public SchemaCreateView clickCreateSchema() {
-        javaExecutorClick(schemaButton);
+        clickByJavaScript(schemaButton);
         return new SchemaCreateView();
     }
 

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/schema/SchemaView.java
Patch:
@@ -6,7 +6,7 @@
 import org.openqa.selenium.By;
 
 import static com.codeborne.selenide.Selenide.*;
-import static com.provectus.kafka.ui.utilities.WebUtils.javaExecutorClick;
+import static com.provectus.kafka.ui.utilities.WebUtils.clickByJavaScript;
 
 public class SchemaView {
 
@@ -30,7 +30,7 @@ public SchemaEditView openEditSchema(){
     }
     @Step
     public SchemaRegistryList removeSchema() {
-        javaExecutorClick(dotMenuBtn);
+        clickByJavaScript(dotMenuBtn);
         $(By.xpath("//*[contains(text(),'Remove')]")).click();
         SelenideElement confirmButton = $x("//div[@role=\"dialog\"]//button[text()='Confirm']");
         confirmButton.shouldBe(Condition.enabled).click();

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/topic/TopicCreateEditSettingsView.java
Patch:
@@ -8,7 +8,7 @@
 import org.openqa.selenium.By;
 
 import static com.codeborne.selenide.Selenide.*;
-import static com.provectus.kafka.ui.utilities.WebUtils.javaExecutorClick;
+import static com.provectus.kafka.ui.utilities.WebUtils.clickByJavaScript;
 import static org.assertj.core.api.Assertions.assertThat;
 
 public class TopicCreateEditSettingsView {
@@ -83,7 +83,7 @@ public TopicCreateEditSettingsView selectRetentionBytes(Long optionValue) {
     }
     @Step
     public TopicView sendData() {
-        javaExecutorClick($x("//button[@type='submit']"));
+        clickByJavaScript($x("//button[@type='submit']"));
         return new TopicView();
     }
     @Step

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/topic/TopicsList.java
Patch:
@@ -11,7 +11,7 @@
 
 import static com.codeborne.selenide.Selenide.*;
 import static com.provectus.kafka.ui.utilities.WebUtils.isVisible;
-import static com.provectus.kafka.ui.utilities.WebUtils.javaExecutorClick;
+import static com.provectus.kafka.ui.utilities.WebUtils.clickByJavaScript;
 
 @ExtensionMethod(WaitUtils.class)
 public class TopicsList {
@@ -33,7 +33,7 @@ public TopicsList waitUntilScreenReady() {
 
     @Step
     public TopicCreateEditSettingsView pressCreateNewTopic() {
-        javaExecutorClick($x("//button[normalize-space(text()) ='Add a Topic']"));
+        clickByJavaScript($x("//button[normalize-space(text()) ='Add a Topic']"));
         return new TopicCreateEditSettingsView();
     }
 

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/connector/ConnectorCreateView.java
Patch:
@@ -2,17 +2,14 @@
 
 import com.codeborne.selenide.Condition;
 import com.codeborne.selenide.SelenideElement;
-import com.provectus.kafka.ui.utilities.WaitUtils;
 import io.qameta.allure.Step;
-import lombok.experimental.ExtensionMethod;
 import org.openqa.selenium.By;
 
 import static com.codeborne.selenide.Selenide.$;
 import static com.codeborne.selenide.Selenide.sleep;
 import static com.provectus.kafka.ui.utilities.WebUtils.javaExecutorClick;
 import static com.provectus.kafka.ui.utilities.screenshots.Screenshooter.log;
 
-@ExtensionMethod(WaitUtils.class)
 public class ConnectorCreateView {
 
     SelenideElement nameField = $(By.xpath("//input[@name='name']"));

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/connector/ConnectorsView.java
Patch:
@@ -4,17 +4,14 @@
 import com.codeborne.selenide.Selenide;
 import com.codeborne.selenide.SelenideElement;
 import com.provectus.kafka.ui.settings.Source;
-import com.provectus.kafka.ui.utilities.WaitUtils;
 import io.qameta.allure.Step;
-import lombok.experimental.ExtensionMethod;
 import org.openqa.selenium.By;
 import org.openqa.selenium.Keys;
 
 import static com.codeborne.selenide.Selenide.*;
 import static com.provectus.kafka.ui.utilities.WebUtils.javaExecutorClick;
 import static com.provectus.kafka.ui.utilities.screenshots.Screenshooter.log;
 
-@ExtensionMethod(WaitUtils.class)
 public class ConnectorsView {
     private static final String path = "/ui/clusters/%s/connects/first/connectors/%s";
     protected SelenideElement submitButton = $(By.xpath("//button[@type='submit']"));

File: kafka-ui-e2e-checks/src/test/java/com/provectus/kafka/ui/SmokeTests.java
Patch:
@@ -1,8 +1,8 @@
 package com.provectus.kafka.ui;
 
 import com.provectus.kafka.ui.base.BaseTest;
-import com.provectus.kafka.ui.utilities.qaseIoUtils.enums.Status;
 import com.provectus.kafka.ui.utilities.qaseIoUtils.annotations.AutomationStatus;
+import com.provectus.kafka.ui.utilities.qaseIoUtils.enums.Status;
 import io.qase.api.annotation.CaseId;
 import org.junit.jupiter.api.DisplayName;
 import org.junit.jupiter.api.Test;
@@ -14,7 +14,7 @@ public class SmokeTests extends BaseTest {
     @CaseId(198)
     @DisplayName("main page should load")
     void mainPageLoads() {
-        pages.open()
+        mainPage.goTo()
                 .waitUntilScreenReady();
         compareScreenshots("main");
     }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/emitter/BackwardRecordEmitter.java
Patch:
@@ -1,7 +1,7 @@
 package com.provectus.kafka.ui.emitter;
 
 import com.provectus.kafka.ui.model.TopicMessageEventDTO;
-import com.provectus.kafka.ui.serde.RecordSerDe;
+import com.provectus.kafka.ui.serdes.ConsumerRecordDeserializer;
 import com.provectus.kafka.ui.util.OffsetsSeekBackward;
 import java.time.Duration;
 import java.util.ArrayList;
@@ -35,7 +35,7 @@ public class BackwardRecordEmitter
   public BackwardRecordEmitter(
       Function<Map<String, Object>, KafkaConsumer<Bytes, Bytes>> consumerSupplier,
       OffsetsSeekBackward offsetsSeek,
-      RecordSerDe recordDeserializer) {
+      ConsumerRecordDeserializer recordDeserializer) {
     super(recordDeserializer);
     this.offsetsSeek = offsetsSeek;
     this.consumerSupplier = consumerSupplier;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/emitter/ForwardRecordEmitter.java
Patch:
@@ -1,7 +1,7 @@
 package com.provectus.kafka.ui.emitter;
 
 import com.provectus.kafka.ui.model.TopicMessageEventDTO;
-import com.provectus.kafka.ui.serde.RecordSerDe;
+import com.provectus.kafka.ui.serdes.ConsumerRecordDeserializer;
 import com.provectus.kafka.ui.util.OffsetsSeek;
 import java.util.function.Supplier;
 import lombok.extern.slf4j.Slf4j;
@@ -22,7 +22,7 @@ public class ForwardRecordEmitter
   public ForwardRecordEmitter(
       Supplier<KafkaConsumer<Bytes, Bytes>> consumerSupplier,
       OffsetsSeek offsetsSeek,
-      RecordSerDe recordDeserializer) {
+      ConsumerRecordDeserializer recordDeserializer) {
     super(recordDeserializer);
     this.consumerSupplier = consumerSupplier;
     this.offsetsSeek = offsetsSeek;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/emitter/TailingEmitter.java
Patch:
@@ -1,7 +1,7 @@
 package com.provectus.kafka.ui.emitter;
 
 import com.provectus.kafka.ui.model.TopicMessageEventDTO;
-import com.provectus.kafka.ui.serde.RecordSerDe;
+import com.provectus.kafka.ui.serdes.ConsumerRecordDeserializer;
 import com.provectus.kafka.ui.util.OffsetsSeek;
 import java.util.function.Supplier;
 import lombok.extern.slf4j.Slf4j;
@@ -17,7 +17,7 @@ public class TailingEmitter extends AbstractEmitter
   private final Supplier<KafkaConsumer<Bytes, Bytes>> consumerSupplier;
   private final OffsetsSeek offsetsSeek;
 
-  public TailingEmitter(RecordSerDe recordDeserializer,
+  public TailingEmitter(ConsumerRecordDeserializer recordDeserializer,
                         Supplier<KafkaConsumer<Bytes, Bytes>> consumerSupplier,
                         OffsetsSeek offsetsSeek) {
     super(recordDeserializer);

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/ConsumerGroupService.java
Patch:
@@ -91,13 +91,12 @@ public Mono<List<InternalConsumerGroup>> getConsumerGroupsForTopic(KafkaCluster
                   )
                   .flatMap((Map<String, Map<TopicPartition, Long>> groupOffsets) ->
                       // 4. getting description for groups with non-emtpy offsets
-                      ac.describeConsumerGroups(new ArrayList<>(groupOffsets.keySet()))
+                      ac.describeConsumerGroups(groupOffsets.keySet())
                           .map((Map<String, ConsumerGroupDescription> descriptions) ->
                               descriptions.values().stream().map(desc ->
-                                      // 5. gathering and filter non-target-topic data
+                                      // 5. gathering into InternalConsumerGroup
                                       InternalConsumerGroup.create(
                                               desc, groupOffsets.get(desc.groupId()), endOffsets)
-                                          .retainDataForPartitions(p -> p.topic().equals(topic))
                                   )
                                   .collect(Collectors.toList())));
             }));

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/ReactiveAdminClient.java
Patch:
@@ -334,7 +334,7 @@ public Mono<List<String>> listConsumerGroups() {
         .map(lst -> lst.stream().map(ConsumerGroupListing::groupId).collect(toList()));
   }
 
-  public Mono<Map<String, ConsumerGroupDescription>> describeConsumerGroups(List<String> groupIds) {
+  public Mono<Map<String, ConsumerGroupDescription>> describeConsumerGroups(Collection<String> groupIds) {
     return toMono(client.describeConsumerGroups(groupIds).all());
   }
 
@@ -372,6 +372,7 @@ public Mono<Map<TopicPartition, Long>> listOffsets(String topic,
 
   public Mono<Map<TopicPartition, Long>> listOffsets(Collection<TopicPartition> partitions,
                                                      OffsetSpec offsetSpec) {
+    //TODO: need to split this into multiple calls if number of target partitions is big
     return toMono(
         client.listOffsets(partitions.stream().collect(toMap(tp -> tp, tp -> offsetSpec))).all())
         .map(offsets -> offsets.entrySet()

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/emitter/TailingEmitterTest.java
Patch:
@@ -114,7 +114,9 @@ private Flux<TopicMessageEventDTO> createTailingFlux(
             new ConsumerPosition(SeekTypeDTO.LATEST, Map.of(), SeekDirectionDTO.TAILING),
             query,
             MessageFilterTypeDTO.STRING_CONTAINS,
-            0);
+            0,
+            "String",
+            "String");
   }
 
   private List<TopicMessageEventDTO> startTailing(String filterQuery) {

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/service/MessagesServiceTest.java
Patch:
@@ -44,7 +44,8 @@ void sendMessageReturnsExceptionWhenTopicNotFound() {
 
   @Test
   void loadMessagesReturnsExceptionWhenTopicNotFound() {
-    StepVerifier.create(messagesService.loadMessages(cluster, NON_EXISTING_TOPIC, null, null, null, 1))
+    StepVerifier.create(messagesService
+            .loadMessages(cluster, NON_EXISTING_TOPIC, null, null, null, 1, "String", "String"))
         .expectError(TopicNotFoundException.class)
         .verify();
   }

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/connector/ConnectorsView.java
Patch:
@@ -23,9 +23,9 @@ public ConnectorsView goTo(String cluster, String connector) {
         return this;
     }
 
-    @Step("Open 'Edit Config' of connector")
-    public ConnectorUpdateView openEditConfig() {
-        BrowserUtils.javaExecutorClick($x("//button[text()='Edit Config']"));
+    @Step()
+    public ConnectorUpdateView openConfigTab() {
+        BrowserUtils.javaExecutorClick($(By.xpath("//a[text() ='Config']")));
         return new ConnectorUpdateView();
     }
 

File: kafka-ui-e2e-checks/src/test/java/com/provectus/kafka/ui/tests/ConnectorsTests.java
Patch:
@@ -81,7 +81,7 @@ public void updateConnector() {
                 .waitUntilScreenReady()
                 .openConnector(CONNECTOR_FOR_UPDATE.getName());
         pages.connectorsView.connectorIsVisibleOnOverview();
-        pages.connectorsView.openEditConfig()
+        pages.connectorsView.openConfigTab()
                 .updConnectorConfig(CONNECTOR_FOR_UPDATE.getConfig());
         pages.openConnectorsList(CLUSTER_NAME);
         Assertions.assertTrue(pages.connectorsList.isConnectorVisible(CONNECTOR_FOR_UPDATE.getName()),"isConnectorVisible()");

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/topic/TopicView.java
Patch:
@@ -33,7 +33,7 @@ public TopicView waitUntilScreenReady() {
     @Step
     public TopicCreateEditSettingsView openEditSettings() {
         BrowserUtils.javaExecutorClick(dotMenuBtn);
-        $x("//a[text()= '" + DotMenuHeaderItems.EDIT_SETTINGS.getValue() + "']").click();
+        $x("//li[@role][text()='Edit settings']").click();
         return new TopicCreateEditSettingsView();
     }
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/emitter/BackwardRecordEmitter.java
Patch:
@@ -118,7 +118,7 @@ private List<ConsumerRecord<Bytes, Bytes>> partitionPollIteration(
     var recordsToSend = new ArrayList<ConsumerRecord<Bytes, Bytes>>();
 
     // we use empty polls counting to verify that partition was fully read
-    for (int emptyPolls = 0; recordsToSend.size() < desiredMsgsToPoll && emptyPolls < 3; ) {
+    for (int emptyPolls = 0; recordsToSend.size() < desiredMsgsToPoll && emptyPolls < NO_MORE_DATA_EMPTY_POLLS_COUNT;) {
       var polledRecords = poll(sink, consumer, POLL_TIMEOUT);
       log.debug("{} records polled from {}", polledRecords.count(), tp);
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/analyze/TopicAnalysisService.java
Patch:
@@ -1,5 +1,7 @@
 package com.provectus.kafka.ui.service.analyze;
 
+import static com.provectus.kafka.ui.emitter.AbstractEmitter.NO_MORE_DATA_EMPTY_POLLS_COUNT;
+
 import com.provectus.kafka.ui.exception.TopicAnalysisException;
 import com.provectus.kafka.ui.model.KafkaCluster;
 import com.provectus.kafka.ui.model.TopicAnalysisDTO;
@@ -118,7 +120,7 @@ public void run() {
         consumer.seekToBeginning(topicPartitions);
 
         var waitingOffsets = new WaitingOffsets(topicId.topicName, consumer, topicPartitions);
-        for (int emptyPolls = 0; !waitingOffsets.endReached() && emptyPolls < 3; ) {
+        for (int emptyPolls = 0; !waitingOffsets.endReached() && emptyPolls < NO_MORE_DATA_EMPTY_POLLS_COUNT;) {
           var polled = consumer.poll(Duration.ofSeconds(3));
           emptyPolls = polled.isEmpty() ? emptyPolls + 1 : 0;
           polled.forEach(r -> {

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/Pages.java
Patch:
@@ -3,6 +3,7 @@
 import com.provectus.kafka.ui.pages.connector.ConnectorsList;
 import com.provectus.kafka.ui.pages.connector.ConnectorsView;
 import com.provectus.kafka.ui.pages.schema.SchemaRegistryList;
+import com.provectus.kafka.ui.pages.topic.ProduceMessagePanel;
 import com.provectus.kafka.ui.pages.topic.TopicView;
 import com.provectus.kafka.ui.pages.topic.TopicsList;
 import io.qameta.allure.Step;
@@ -14,7 +15,7 @@ public class Pages {
     public MainPage mainPage = new MainPage();
     public TopicsList topicsList = new TopicsList();
     public TopicView topicView = new TopicView();
-    public ProduceMessagePage produceMessagePage = new ProduceMessagePage();
+    public ProduceMessagePanel produceMessagePanel = new ProduceMessagePanel();
     public ConnectorsList connectorsList = new ConnectorsList();
     public ConnectorsView connectorsView = new ConnectorsView();
     public SchemaRegistryList schemaRegistry = new SchemaRegistryList();

File: kafka-ui-e2e-checks/src/test/java/com/provectus/kafka/ui/SmokeTests.java
Patch:
@@ -8,6 +8,7 @@
 import org.junit.jupiter.api.Test;
 
 public class SmokeTests extends BaseTest {
+
     @Test
     @AutomationStatus(status = Status.AUTOMATED)
     @CaseId(198)
@@ -17,5 +18,4 @@ void mainPageLoads() {
                 .waitUntilScreenReady();
         compareScreenshots("main");
     }
-
 }

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/topic/TopicsList.java
Patch:
@@ -15,7 +15,7 @@
 @ExtensionMethod(WaitUtils.class)
 public class TopicsList {
 
-    private static final String path = "/ui/clusters/%s/topics";
+    private static final String path = "/ui/clusters/%s/all-topics";
 
     @Step
     public TopicsList goTo(String cluster) {
@@ -26,7 +26,7 @@ public TopicsList goTo(String cluster) {
     @Step
     public TopicsList waitUntilScreenReady() {
         $(By.xpath("//*[contains(text(),'Loading')]")).shouldBe(Condition.disappear);
-        $(By.xpath("//h1[text()='All Topics']")).shouldBe(Condition.visible);
+        $(By.xpath("//h1[text()='Topics']")).shouldBe(Condition.visible);
         return this;
     }
 

File: kafka-ui-e2e-checks/src/test/java/com/provectus/kafka/ui/base/BaseTest.java
Patch:
@@ -41,7 +41,7 @@ public class BaseTest {
   protected Pages pages = Pages.INSTANCE;
   protected Helpers helpers = Helpers.INSTANCE;
 
-  private Screenshooter screenshooter = new Screenshooter();
+  private final Screenshooter screenshooter = new Screenshooter();
 
   protected static BrowserWebDriverContainer<?> webDriverContainer = null;
 

File: kafka-ui-e2e-checks/src/test/java/com/provectus/kafka/ui/tests/TopicTests.java
Patch:
@@ -101,7 +101,8 @@ public void deleteTopic() {
                 .waitUntilScreenReady()
                 .openTopic(TOPIC_TO_DELETE)
                 .waitUntilScreenReady()
-                .deleteTopic()
+                .deleteTopic();
+        pages.openTopicsList(CLUSTER_NAME)
                 .waitUntilScreenReady()
                 .isTopicNotVisible(TOPIC_TO_DELETE);
     }

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/emitter/MessageFiltersTest.java
Patch:
@@ -140,7 +140,7 @@ void canRunMultiStatementScripts() {
 
 
     @Test
-    void filterSpeedIsAtLeast10kPerSec() {
+    void filterSpeedIsAtLeast5kPerSec() {
       var f = groovyScriptFilter("value.name.first == 'user1' && keyAsText.startsWith('a') ");
 
       List<TopicMessageDTO> toFilter = new ArrayList<>();
@@ -159,7 +159,7 @@ void filterSpeedIsAtLeast10kPerSec() {
       long matched = toFilter.stream().filter(f).count();
       long took = System.currentTimeMillis() - before;
 
-      assertThat(took).isLessThan(500);
+      assertThat(took).isLessThan(1000);
       assertThat(matched).isGreaterThan(0);
     }
   }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/config/auth/BasicAuthSecurityConfig.java
Patch:
@@ -2,7 +2,7 @@
 
 import com.provectus.kafka.ui.util.EmptyRedirectStrategy;
 import java.net.URI;
-import lombok.extern.log4j.Log4j2;
+import lombok.extern.slf4j.Slf4j;
 import org.springframework.boot.autoconfigure.condition.ConditionalOnProperty;
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
@@ -17,7 +17,7 @@
 @Configuration
 @EnableWebFluxSecurity
 @ConditionalOnProperty(value = "auth.type", havingValue = "LOGIN_FORM")
-@Log4j2
+@Slf4j
 public class BasicAuthSecurityConfig extends AbstractAuthSecurityConfig {
 
   public static final String LOGIN_URL = "/auth";

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/config/auth/DisabledAuthSecurityConfig.java
Patch:
@@ -1,6 +1,6 @@
 package com.provectus.kafka.ui.config.auth;
 
-import lombok.extern.log4j.Log4j2;
+import lombok.extern.slf4j.Slf4j;
 import org.springframework.boot.SpringApplication;
 import org.springframework.boot.autoconfigure.condition.ConditionalOnProperty;
 import org.springframework.context.ApplicationContext;
@@ -14,7 +14,7 @@
 @Configuration
 @EnableWebFluxSecurity
 @ConditionalOnProperty(value = "auth.type", havingValue = "DISABLED")
-@Log4j2
+@Slf4j
 public class DisabledAuthSecurityConfig extends AbstractAuthSecurityConfig {
 
   @Bean

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/config/auth/LdapSecurityConfig.java
Patch:
@@ -1,7 +1,7 @@
 package com.provectus.kafka.ui.config.auth;
 
 import java.util.List;
-import lombok.extern.log4j.Log4j2;
+import lombok.extern.slf4j.Slf4j;
 import org.springframework.beans.factory.annotation.Value;
 import org.springframework.boot.autoconfigure.condition.ConditionalOnProperty;
 import org.springframework.context.annotation.Bean;
@@ -25,7 +25,7 @@
 @Configuration
 @EnableWebFluxSecurity
 @ConditionalOnProperty(value = "auth.type", havingValue = "LDAP")
-@Log4j2
+@Slf4j
 public class LdapSecurityConfig extends AbstractAuthSecurityConfig {
 
   @Value("${spring.ldap.urls}")

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/SchemaRegistryService.java
Patch:
@@ -66,6 +66,7 @@ public class SchemaRegistryService {
 
   private static final String UNRECOGNIZED_FIELD_SCHEMA_TYPE = "Unrecognized field: schemaType";
   private static final String INCOMPATIBLE_WITH_AN_EARLIER_SCHEMA = "incompatible with an earlier schema";
+  private static final String INVALID_SCHEMA = "Invalid Schema";
 
   private final WebClient webClient;
 
@@ -237,7 +238,8 @@ private Mono<Throwable> getMonoError(ErrorResponse x) {
     } else if (isIncompatibleSchemaMessage(x.getMessage())) {
       return Mono.error(new SchemaCompatibilityException(x.getMessage()));
     } else {
-      return Mono.error(new UnprocessableEntityException(x.getMessage()));
+      log.error(x.getMessage());
+      return Mono.error(new UnprocessableEntityException(INVALID_SCHEMA));
     }
   }
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/ConsumerGroupService.java
Patch:
@@ -161,7 +161,7 @@ private Comparator<ConsumerGroupDescription> getPaginationComparator(ConsumerGro
         };
         return Comparator.comparingInt(statesPriorities);
       case MEMBERS:
-        return Comparator.comparingInt(cg -> -cg.members().size());
+        return Comparator.comparingInt(cg -> cg.members().size());
       default:
         throw new IllegalStateException("Unsupported order by: " + orderBy);
     }

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/container/KafkaConnectContainer.java
Patch:
@@ -12,6 +12,7 @@ public class KafkaConnectContainer extends GenericContainer<KafkaConnectContaine
   public KafkaConnectContainer(String version) {
     super("confluentinc/cp-kafka-connect:" + version);
     addExposedPort(CONNECT_PORT);
+
     waitStrategy = Wait.forHttp("/")
         .withStartupTimeout(Duration.ofMinutes(5));
   }
@@ -37,6 +38,7 @@ public KafkaConnectContainer withKafka(Network network, String bootstrapServers)
     withEnv("CONNECT_INTERNAL_KEY_CONVERTER", "org.apache.kafka.connect.json.JsonConverter");
     withEnv("CONNECT_INTERNAL_VALUE_CONVERTER", "org.apache.kafka.connect.json.JsonConverter");
     withEnv("CONNECT_REST_ADVERTISED_HOST_NAME", "kafka-connect");
+    withEnv("CONNECT_REST_PORT", String.valueOf(CONNECT_PORT));
     withEnv("CONNECT_PLUGIN_PATH", "/usr/share/java,/usr/share/confluent-hub-components");
     return self();
   }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/SchemaRegistryService.java
Patch:
@@ -381,7 +381,7 @@ private URI buildUri(InternalSchemaRegistry schemaRegistry, String path, List<St
     final var builder = UriComponentsBuilder
         .fromHttpUrl(schemaRegistry.getUri() + path);
     builder.queryParams(queryParams);
-    return builder.buildAndExpand(uriVariables.toArray()).toUri();
+    return builder.build(uriVariables.toArray());
   }
 
   private Function<ClientResponse, Mono<? extends Throwable>> errorOnSchemaDeleteFailure(String schemaName) {

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/connector/ConnectorCreateView.java
Patch:
@@ -2,15 +2,15 @@
 
 import com.codeborne.selenide.Condition;
 import com.codeborne.selenide.SelenideElement;
-import com.provectus.kafka.ui.utils.BrowserUtils;
 import com.provectus.kafka.ui.extensions.WaitUtils;
+import com.provectus.kafka.ui.utils.BrowserUtils;
 import io.qameta.allure.Step;
 import lombok.experimental.ExtensionMethod;
 import org.openqa.selenium.By;
 
 import static com.codeborne.selenide.Selenide.$;
+import static com.codeborne.selenide.Selenide.sleep;
 import static com.provectus.kafka.ui.screenshots.Screenshooter.log;
-import static java.lang.Thread.sleep;
 
 @ExtensionMethod(WaitUtils.class)
 public class ConnectorCreateView {
@@ -22,7 +22,7 @@ public class ConnectorCreateView {
     private static final String path = "/ui/clusters/secondLocal/connectors/create_new";
 
     @Step("Set connector config JSON")
-    public ConnectorsView setConnectorConfig(String connectName, String configJson) throws InterruptedException {
+    public ConnectorsView setConnectorConfig(String connectName, String configJson) {
         nameField.setValue(connectName);
         $("#config").click();
         contentTextArea.setValue("");

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/schema/SchemaRegistryList.java
Patch:
@@ -4,7 +4,6 @@
 import com.codeborne.selenide.SelenideElement;
 import com.provectus.kafka.ui.utils.BrowserUtils;
 import io.qameta.allure.Step;
-import lombok.SneakyThrows;
 import org.openqa.selenium.By;
 
 import static com.codeborne.selenide.Selenide.*;
@@ -23,10 +22,9 @@ public SchemaView openSchema(String schemaName) {
         return new SchemaView();
     }
 
-    @SneakyThrows
     @Step
     public SchemaRegistryList isNotVisible(String schemaName) {
-        $x(String.format("//*[contains(text(),'%s')]",schemaName)).shouldNotBe(Condition.visible);
+        $x(String.format("//*[contains(text(),'%s')]", schemaName)).shouldNotBe(Condition.visible);
         return this;
     }
 

File: kafka-ui-e2e-checks/src/test/java/com/provectus/kafka/ui/SmokeTests.java
Patch:
@@ -4,15 +4,13 @@
 import com.provectus.kafka.ui.utils.qaseIO.Status;
 import com.provectus.kafka.ui.utils.qaseIO.annotation.AutomationStatus;
 import io.qase.api.annotation.CaseId;
-import lombok.SneakyThrows;
 import org.junit.jupiter.api.DisplayName;
 import org.junit.jupiter.api.Test;
 
 public class SmokeTests extends BaseTest {
     @Test
     @AutomationStatus(status = Status.AUTOMATED)
     @CaseId(198)
-    @SneakyThrows
     @DisplayName("main page should load")
     void mainPageLoads() {
         pages.open()

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/MainPage.java
Patch:
@@ -26,7 +26,7 @@ public MainPage goTo() {
     }
 
     @Step
-    public MainPage isOnPage() {
+    public MainPage waitUntilScreenReady() {
         $(By.xpath("//*[contains(text(),'Loading')]")).shouldBe(Condition.disappear);
         $("input[name=switchRoundedDefault]").parent().$("span").shouldBe(Condition.visible);
         return this;

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/connector/ConnectorCreateView.java
Patch:
@@ -34,8 +34,8 @@ public ConnectorsView setConnectorConfig(String connectName, String configJson)
         return new ConnectorsView();
     }
 
-    @Step("Verify that page 'Create Connector' opened")
-    public ConnectorCreateView isOnConnectorCreatePage() {
+    @Step
+    public ConnectorCreateView waitUntilScreenReady() {
         nameField.shouldBe(Condition.visible);
         return this;
     }

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/connector/ConnectorsList.java
Patch:
@@ -24,7 +24,7 @@ public ConnectorsList goTo(String cluster) {
     }
 
     @Step
-    public ConnectorsList isOnPage() {
+    public ConnectorsList waitUntilScreenReady() {
         $(By.xpath("//h1[text()='Connectors']")).shouldBe(Condition.visible);
         return this;
     }

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/schema/SchemaView.java
Patch:
@@ -12,7 +12,7 @@
 public class SchemaView {
 
     @Step
-    public SchemaView isOnSchemaViewPage() {
+    public SchemaView waitUntilScreenReady() {
         $("div#schema").shouldBe(Condition.visible);
         return this;
     }

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/topic/TopicView.java
Patch:
@@ -29,7 +29,7 @@ public TopicView goTo(String cluster, String topic) {
     }
 
     @Step
-    public TopicView isOnTopicViewPage() {
+    public TopicView waitUntilScreenReady() {
         $(By.linkText("Overview")).shouldBe(Condition.visible);
         return this;
     }

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/topic/TopicsList.java
Patch:
@@ -25,7 +25,7 @@ public TopicsList goTo(String cluster) {
     }
 
     @Step
-    public TopicsList isOnPage() {
+    public TopicsList waitUntilScreenReady() {
         $(By.xpath("//*[contains(text(),'Loading')]")).shouldBe(Condition.disappear);
         $(By.xpath("//h1[text()='All Topics']")).shouldBe(Condition.visible);
         return this;

File: kafka-ui-e2e-checks/src/test/java/com/provectus/kafka/ui/SmokeTests.java
Patch:
@@ -16,7 +16,7 @@ public class SmokeTests extends BaseTest {
     @DisplayName("main page should load")
     void mainPageLoads() {
         pages.open()
-                .isOnPage();
+                .waitUntilScreenReady();
         compareScreenshots("main");
     }
 

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/topic/TopicsList.java
Patch:
@@ -55,7 +55,7 @@ public TopicView openTopic(String topicName) {
     @SneakyThrows
     public TopicsList isTopicNotVisible(String topicName) {
         $$x("//table/tbody/tr/td[2]")
-                .shouldBe(CollectionCondition.sizeGreaterThanOrEqual(4))
+                .shouldBe(CollectionCondition.sizeGreaterThan(0))
                 .find(Condition.exactText(topicName))
                 .shouldBe(Condition.not(Condition.visible));
         return this;

File: kafka-ui-e2e-checks/src/test/java/com/provectus/kafka/ui/tests/TopicTests.java
Patch:
@@ -64,7 +64,6 @@ public void createTopic() {
                 .goToSideMenu(SECOND_LOCAL, MainPage.SideMenuOptions.TOPICS)
                 .topicIsNotVisible(NEW_TOPIC);
     }
-
     @Disabled("Due to issue https://github.com/provectus/kafka-ui/issues/1500 ignore this test")
     @SneakyThrows
     @DisplayName("should update a topic")
@@ -104,7 +103,6 @@ public void updateTopic() {
     @AutomationStatus(status = Status.AUTOMATED)
     @CaseId(207)
     @Test
-    @Disabled // TODO: https://github.com/provectus/kafka-ui/issues/2373
     public void deleteTopic() {
         pages.openTopicsList(SECOND_LOCAL)
                 .isOnPage()

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/mapper/DescribeLogDirsMapper.java
Patch:
@@ -8,6 +8,7 @@
 import java.util.Map;
 import java.util.stream.Collectors;
 import org.apache.kafka.common.TopicPartition;
+import org.apache.kafka.common.protocol.Errors;
 import org.apache.kafka.common.requests.DescribeLogDirsResponse;
 import org.springframework.stereotype.Component;
 
@@ -28,7 +29,7 @@ private BrokersLogdirsDTO toBrokerLogDirs(Integer broker, String dirName,
                                             DescribeLogDirsResponse.LogDirInfo logDirInfo) {
     BrokersLogdirsDTO result = new BrokersLogdirsDTO();
     result.setName(dirName);
-    if (logDirInfo.error != null) {
+    if (logDirInfo.error != null && logDirInfo.error != Errors.NONE) {
       result.setError(logDirInfo.error.message());
     }
     var topics = logDirInfo.replicaInfos.entrySet().stream()

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/helpers/TestConfiguration.java
Patch:
@@ -14,10 +14,8 @@ public class TestConfiguration {
       Boolean.parseBoolean(System.getProperty("SAVE_PAGE_SOURCE", "false"));
   public static Boolean REOPEN_BROWSER_ON_FAIL =
       Boolean.parseBoolean(System.getProperty("REOPEN_BROWSER_ON_FAIL", "true"));
-  public static String BROWSER = System.getProperty("BROWSER", "chrome");
+  public static String BROWSER = System.getProperty("BROWSER", "chromium");
   public static String BROWSER_SIZE = System.getProperty("BROWSER_SIZE", "1920x1080");
   public static Boolean ENABLE_VNC = Boolean.parseBoolean(System.getProperty("ENABLE_VNC", "true"));
-  public static String IMAGE_NAME = System.getProperty("SELENIUM_DOCKER_IMAGE", "selenium/standalone-chrome");
-  public static String IMAGE_TAG = System.getProperty("SELENIUM_IMAGE_TAG", "103.0");
 
 }

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/helpers/TestConfiguration.java
Patch:
@@ -1,4 +1,4 @@
-package com.provectus.kafka.ui.base;
+package com.provectus.kafka.ui.helpers;
 
 public class TestConfiguration {
   public static boolean CLEAR_REPORTS_DIR =
@@ -18,6 +18,6 @@ public class TestConfiguration {
   public static String BROWSER_SIZE = System.getProperty("BROWSER_SIZE", "1920x1080");
   public static Boolean ENABLE_VNC = Boolean.parseBoolean(System.getProperty("ENABLE_VNC", "true"));
   public static String IMAGE_NAME = System.getProperty("SELENIUM_DOCKER_IMAGE", "selenium/standalone-chrome");
-  public static String IMAGE_TAG = System.getProperty("SELENIUM_IMAGE_TAG", "102.0");
+  public static String IMAGE_TAG = System.getProperty("SELENIUM_IMAGE_TAG", "103.0");
 
 }

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/MainPage.java
Patch:
@@ -3,7 +3,7 @@
 import com.codeborne.selenide.Condition;
 import com.codeborne.selenide.Selenide;
 import com.codeborne.selenide.SelenideElement;
-import com.provectus.kafka.ui.base.TestConfiguration;
+import com.provectus.kafka.ui.helpers.TestConfiguration;
 import com.provectus.kafka.ui.extensions.WaitUtils;
 import com.provectus.kafka.ui.pages.topic.TopicsList;
 import io.qameta.allure.Step;
@@ -48,7 +48,7 @@ public enum SideMenuOptions {
     CONSUMERS("Consumers"),
     SCHEMA_REGISTRY("Schema Registry");
 
-    String value;
+    final String value;
 
     SideMenuOptions(String value) {
       this.value = value;

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/connector/ConnectorCreateView.java
Patch:
@@ -2,8 +2,8 @@
 
 import com.codeborne.selenide.Condition;
 import com.codeborne.selenide.SelenideElement;
-import com.provectus.kafka.ui.extensions.WaitUtils;
 import com.provectus.kafka.ui.utils.BrowserUtils;
+import com.provectus.kafka.ui.extensions.WaitUtils;
 import io.qameta.allure.Step;
 import lombok.experimental.ExtensionMethod;
 import org.openqa.selenium.By;

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/connector/ConnectorsView.java
Patch:
@@ -2,7 +2,7 @@
 
 import com.codeborne.selenide.Condition;
 import com.codeborne.selenide.Selenide;
-import com.provectus.kafka.ui.base.TestConfiguration;
+import com.provectus.kafka.ui.helpers.TestConfiguration;
 import com.provectus.kafka.ui.extensions.WaitUtils;
 import com.provectus.kafka.ui.utils.BrowserUtils;
 import io.qameta.allure.Step;
@@ -18,7 +18,7 @@ public class ConnectorsView {
 
     @Step
     public ConnectorsView goTo(String cluster, String connector) {
-        Selenide.open(TestConfiguration.BASE_WEB_URL + path.format(cluster, connector));
+        Selenide.open(String.format(TestConfiguration.BASE_WEB_URL + path, cluster, connector));
         return this;
     }
 

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/schema/SchemaRegistryList.java
Patch:
@@ -11,7 +11,7 @@
 
 public class SchemaRegistryList {
 
-    private SelenideElement schemaButton = $(By.xpath("//*[contains(text(),'Create Schema')]"));
+    private final SelenideElement schemaButton = $(By.xpath("//*[contains(text(),'Create Schema')]"));
 
     public SchemaCreateView clickCreateSchema() {
         BrowserUtils.javaExecutorClick(schemaButton);

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/pages/topic/TopicsList.java
Patch:
@@ -3,7 +3,7 @@
 import com.codeborne.selenide.CollectionCondition;
 import com.codeborne.selenide.Condition;
 import com.codeborne.selenide.Selenide;
-import com.provectus.kafka.ui.base.TestConfiguration;
+import com.provectus.kafka.ui.helpers.TestConfiguration;
 import com.provectus.kafka.ui.extensions.WaitUtils;
 import com.provectus.kafka.ui.utils.BrowserUtils;
 import io.qameta.allure.Step;
@@ -33,7 +33,7 @@ public TopicsList isOnPage() {
 
     @Step
     public TopicCreateEditSettingsView pressCreateNewTopic(){
-        BrowserUtils.javaExecutorClick($(".qEXNn.sc-bYEvvW"));
+        BrowserUtils.javaExecutorClick($x("//button[normalize-space(text()) ='Add a Topic']"));
         return new TopicCreateEditSettingsView();
     }
 

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/screenshots/NoReferenceScreenshotFoundException.java
Patch:
@@ -2,6 +2,6 @@
 
 public class NoReferenceScreenshotFoundException extends Throwable {
     public NoReferenceScreenshotFoundException(String name) {
-        super(("no reference screenshot found for %s".formatted(name)));
+        super("no reference screenshot found for " + name);
     }
 }

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/utils/CamelCaseToSpacedDisplayNameGenerator.java
Patch:
@@ -1,4 +1,4 @@
-package com.provectus.kafka.ui.base;
+package com.provectus.kafka.ui.utils;
 
 import org.junit.jupiter.api.DisplayNameGenerator;
 import org.junit.platform.commons.util.ClassUtils;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/model/KafkaCluster.java
Patch:
@@ -21,7 +21,7 @@ public class KafkaCluster {
   private final String jmxPassword;
   private final String bootstrapServers;
   private final InternalSchemaRegistry schemaRegistry;
-  private final String ksqldbServer;
+  private final InternalKsqlServer ksqldbServer;
   private final List<KafkaConnectCluster> kafkaConnect;
   private final String schemaNameTemplate;
   private final String keySchemaNameTemplate;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/KsqlService.java
Patch:
@@ -28,10 +28,10 @@ public Mono<KsqlCommandResponseDTO> executeKsqlCommand(KafkaCluster cluster,
               e instanceof ClusterNotFoundException ? e : new KsqlDbNotFoundException();
           return Mono.error(throwable);
         })
-        .flatMap(host -> getStatementStrategyForKsqlCommand(ksqlCommand)
-            .map(statement -> statement.host(host))
+        .flatMap(ksqlServer -> getStatementStrategyForKsqlCommand(ksqlCommand)
+            .map(statement -> statement.host(ksqlServer.getUrl()))
         )
-        .flatMap(ksqlClient::execute);
+        .flatMap(baseStrategy -> ksqlClient.execute(baseStrategy, cluster));
   }
 
   private Mono<BaseStrategy> getStatementStrategyForKsqlCommand(

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/service/ksql/KsqlApiClientTest.java
Patch:
@@ -9,6 +9,7 @@
 import com.fasterxml.jackson.databind.node.TextNode;
 import com.provectus.kafka.ui.AbstractIntegrationTest;
 import com.provectus.kafka.ui.container.KsqlDbContainer;
+import com.provectus.kafka.ui.model.InternalKsqlServer;
 import com.provectus.kafka.ui.model.KafkaCluster;
 import java.time.Duration;
 import java.util.List;
@@ -42,7 +43,8 @@ static void stopContainer() {
   // Tutorial is here: https://ksqldb.io/quickstart.html
   @Test
   void ksqTutorialQueriesWork() {
-    var client = new KsqlApiClient(KafkaCluster.builder().ksqldbServer(KSQL_DB.url()).build(), maxBuffSize);
+    var client = new KsqlApiClient(KafkaCluster.builder().ksqldbServer(
+            InternalKsqlServer.builder().url(KSQL_DB.url()).build()).build(), maxBuffSize);
     execCommandSync(client,
         "CREATE STREAM riderLocations (profileId VARCHAR, latitude DOUBLE, longitude DOUBLE) "
             + "WITH (kafka_topic='locations', value_format='json', partitions=1);",

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/KafkaConnectService.java
Patch:
@@ -2,7 +2,7 @@
 
 import com.fasterxml.jackson.core.type.TypeReference;
 import com.fasterxml.jackson.databind.ObjectMapper;
-import com.provectus.kafka.ui.client.KafkaConnectClients;
+import com.provectus.kafka.ui.client.KafkaConnectClientsFactory;
 import com.provectus.kafka.ui.connect.api.KafkaConnectClientApi;
 import com.provectus.kafka.ui.connect.model.ConnectorStatus;
 import com.provectus.kafka.ui.connect.model.ConnectorStatusConnector;
@@ -21,7 +21,6 @@
 import com.provectus.kafka.ui.model.ConnectorTaskStatusDTO;
 import com.provectus.kafka.ui.model.FullConnectorInfoDTO;
 import com.provectus.kafka.ui.model.KafkaCluster;
-import com.provectus.kafka.ui.model.KafkaConnectCluster;
 import com.provectus.kafka.ui.model.NewConnectorDTO;
 import com.provectus.kafka.ui.model.TaskDTO;
 import com.provectus.kafka.ui.model.connect.InternalConnectInfo;
@@ -51,6 +50,7 @@ public class KafkaConnectService {
   private final KafkaConnectMapper kafkaConnectMapper;
   private final ObjectMapper objectMapper;
   private final KafkaConfigSanitizer kafkaConfigSanitizer;
+  private final KafkaConnectClientsFactory kafkaConnectClientsFactory;
 
   public Mono<Flux<ConnectDTO>> getConnects(KafkaCluster cluster) {
     return Mono.just(
@@ -328,6 +328,6 @@ private Mono<KafkaConnectClientApi> withConnectClient(KafkaCluster cluster, Stri
             .filter(connect -> connect.getName().equals(connectName))
             .findFirst())
         .switchIfEmpty(Mono.error(ConnectNotFoundException::new))
-        .map(KafkaConnectClients::withKafkaConnectConfig);
+        .map(kafkaConnectClientsFactory::withKafkaConnectConfig);
   }
 }

File: kafka-ui-e2e-checks/src/test/java/com/provectus/kafka/ui/tests/ConnectorsTests.java
Patch:
@@ -72,7 +72,6 @@ public void createConnector() {
                 .connectorIsVisibleInList(SINK_CONNECTOR, TOPIC_FOR_CONNECTOR);
     }
 
-    //disable test due 500 error during create connector via api
     @SneakyThrows
     @DisplayName("should update a connector")
     @Test
@@ -83,7 +82,8 @@ public void updateConnector() {
                 pages.connectorsView.connectorIsVisibleOnOverview();
         pages.connectorsView.openEditConfig()
                         .updConnectorConfig(FileUtils.getResourceAsString("config_for_update_connector.json"));
-        pages.openConnectorsList(LOCAL_CLUSTER).connectorIsVisibleInList(CONNECTOR_FOR_UPDATE, TOPIC_FOR_UPDATE_CONNECTOR);
+        pages.openConnectorsList(LOCAL_CLUSTER)
+                .connectorIsVisibleInList(CONNECTOR_FOR_UPDATE, TOPIC_FOR_UPDATE_CONNECTOR);
     }
 
     @SneakyThrows

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/exception/ErrorCode.java
Patch:
@@ -26,7 +26,8 @@ public enum ErrorCode {
   INVALID_REQUEST(4014, HttpStatus.BAD_REQUEST),
   RECREATE_TOPIC_TIMEOUT(4015, HttpStatus.REQUEST_TIMEOUT),
   INVALID_ENTITY_STATE(4016, HttpStatus.BAD_REQUEST),
-  SCHEMA_NOT_DELETED(4017, HttpStatus.INTERNAL_SERVER_ERROR);
+  SCHEMA_NOT_DELETED(4017, HttpStatus.INTERNAL_SERVER_ERROR),
+  TOPIC_ANALYSIS_ERROR(4018, HttpStatus.BAD_REQUEST);
 
   static {
     // codes uniqueness check

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/service/TopicsServicePaginationTest.java
Patch:
@@ -17,6 +17,7 @@
 import com.provectus.kafka.ui.model.SortOrderDTO;
 import com.provectus.kafka.ui.model.TopicColumnsToSortDTO;
 import com.provectus.kafka.ui.model.TopicDTO;
+import com.provectus.kafka.ui.service.analyze.TopicAnalysisService;
 import com.provectus.kafka.ui.util.JmxClusterUtil;
 import java.util.ArrayList;
 import java.util.Comparator;
@@ -41,7 +42,8 @@ class TopicsServicePaginationTest {
   private final ClustersStorage clustersStorage = mock(ClustersStorage.class);
   private final ClusterMapper clusterMapper = new ClusterMapperImpl();
 
-  private final TopicsController topicsController  = new TopicsController(topicsService, clusterMapper);
+  private final TopicsController topicsController  = new TopicsController(
+      topicsService, mock(TopicAnalysisService.class), clusterMapper);
 
   private void init(Map<String, InternalTopic> topicsInCache) {
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/mapper/ClusterMapper.java
Patch:
@@ -126,7 +126,7 @@ default InternalSchemaRegistry setSchemaRegistry(ClustersProperties.Cluster clus
   CompatibilityCheckResponseDTO toCompatibilityCheckResponse(InternalCompatibilityCheck dto);
 
   @Mapping(target = "compatibility", source = "compatibilityLevel")
-  CompatibilityLevelDTO toCompatibilityLevel(InternalCompatibilityLevel dto);
+  CompatibilityLevelDTO toCompatibilityLevelDto(InternalCompatibilityLevel dto);
 
   default List<PartitionDTO> map(Map<Integer, InternalPartition> map) {
     return map.values().stream().map(this::toPartition).collect(Collectors.toList());

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/service/SchemaRegistryPaginationTest.java
Patch:
@@ -7,6 +7,7 @@
 import static org.mockito.Mockito.when;
 
 import com.provectus.kafka.ui.controller.SchemasController;
+import com.provectus.kafka.ui.mapper.ClusterMapper;
 import com.provectus.kafka.ui.model.InternalSchemaRegistry;
 import com.provectus.kafka.ui.model.KafkaCluster;
 import com.provectus.kafka.ui.model.SchemaSubjectDTO;
@@ -22,8 +23,9 @@ public class SchemaRegistryPaginationTest {
 
   private final SchemaRegistryService schemaRegistryService = mock(SchemaRegistryService.class);
   private final ClustersStorage clustersStorage = mock(ClustersStorage.class);
+  private final ClusterMapper clusterMapper = mock(ClusterMapper.class);
 
-  private SchemasController controller;
+  private final SchemasController controller = new SchemasController(clusterMapper, schemaRegistryService);
 
   private void init(String[] subjects) {
     when(schemaRegistryService.getAllSubjectNames(isA(KafkaCluster.class)))
@@ -34,7 +36,6 @@ private void init(String[] subjects) {
             .thenReturn(Optional.of(buildKafkaCluster(LOCAL_KAFKA_CLUSTER_NAME)));
     when(schemaRegistryService.getLatestSchemaVersionBySubject(isA(KafkaCluster.class), isA(String.class)))
             .thenAnswer(a -> Mono.just(new SchemaSubjectDTO().subject(a.getArgument(1))));
-    this.controller = new SchemasController(schemaRegistryService);
     this.controller.setClustersStorage(clustersStorage);
   }
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/config/CustomWebFilter.java
Patch:
@@ -16,7 +16,7 @@ public Mono<Void> filter(ServerWebExchange exchange, WebFilterChain chain) {
 
     final String path = exchange.getRequest().getPath().pathWithinApplication().value();
 
-    if (path.startsWith("/ui") || path.equals("/")) {
+    if (path.startsWith("/ui") || path.equals("") || path.equals("/")) {
       return chain.filter(
           exchange.mutate().request(
               exchange.getRequest().mutate()

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/config/ClustersProperties.java
Patch:
@@ -31,6 +31,8 @@ public static class Cluster {
     String protobufFile;
     String protobufMessageName;
     Map<String, String> protobufMessageNameByTopic;
+    String protobufMessageNameForKey;
+    Map<String, String> protobufMessageNameForKeyByTopic;
     List<ConnectCluster> kafkaConnect;
     int jmxPort;
     boolean jmxSsl;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/model/KafkaCluster.java
Patch:
@@ -28,6 +28,8 @@ public class KafkaCluster {
   private final Path protobufFile;
   private final String protobufMessageName;
   private final Map<String, String> protobufMessageNameByTopic;
+  private final String protobufMessageNameForKey;
+  private final Map<String, String> protobufMessageNameForKeyByTopic;
   private final Properties properties;
   private final boolean readOnly;
   private final boolean disableLogDirsCollection;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/serde/DeserializationService.java
Patch:
@@ -32,7 +32,8 @@ private RecordSerDe createRecordDeserializerForCluster(KafkaCluster cluster) {
       if (cluster.getProtobufFile() != null) {
         log.info("Using ProtobufFileRecordSerDe for cluster '{}'", cluster.getName());
         return new ProtobufFileRecordSerDe(cluster.getProtobufFile(),
-            cluster.getProtobufMessageNameByTopic(), cluster.getProtobufMessageName());
+            cluster.getProtobufMessageNameByTopic(), cluster.getProtobufMessageNameForKeyByTopic(),
+            cluster.getProtobufMessageName(), cluster.getProtobufMessageNameForKey());
       } else if (cluster.getSchemaRegistry() != null) {
         log.info("Using SchemaRegistryAwareRecordSerDe for cluster '{}'", cluster.getName());
         return new SchemaRegistryAwareRecordSerDe(cluster);

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/ConsumerGroupService.java
Patch:
@@ -210,6 +210,7 @@ public KafkaConsumer<Bytes, Bytes> createConsumer(KafkaCluster cluster,
     props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, BytesDeserializer.class);
     props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
     props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "false");
+    props.put(ConsumerConfig.ALLOW_AUTO_CREATE_TOPICS_CONFIG, "false");
     props.putAll(properties);
 
     return new KafkaConsumer<>(props);

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/config/ClustersProperties.java
Patch:
@@ -23,7 +23,6 @@ public class ClustersProperties {
   public static class Cluster {
     String name;
     String bootstrapServers;
-    String zookeeper;
     String schemaRegistry;
     SchemaRegistryAuth schemaRegistryAuth;
     String ksqldbServer;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/model/InternalClusterState.java
Patch:
@@ -2,7 +2,6 @@
 
 import com.google.common.base.Throwables;
 import com.provectus.kafka.ui.service.MetricsCache;
-import com.provectus.kafka.ui.util.ClusterUtil;
 import java.math.BigDecimal;
 import java.util.List;
 import java.util.Optional;
@@ -16,7 +15,6 @@ public class InternalClusterState {
   private MetricsCollectionErrorDTO lastError;
   private Integer topicCount;
   private Integer brokerCount;
-  private Integer zooKeeperStatus;
   private Integer activeControllers;
   private Integer onlinePartitionCount;
   private Integer offlinePartitionCount;
@@ -40,7 +38,6 @@ public InternalClusterState(KafkaCluster cluster, MetricsCache.Metrics metrics)
         .orElse(null);
     topicCount = metrics.getTopicDescriptions().size();
     brokerCount = metrics.getClusterDescription().getNodes().size();
-    zooKeeperStatus = ClusterUtil.convertToIntServerStatus(metrics.getZkStatus().getStatus());
     activeControllers = metrics.getClusterDescription().getController() != null ? 1 : 0;
     version = metrics.getVersion();
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/model/KafkaCluster.java
Patch:
@@ -20,7 +20,6 @@ public class KafkaCluster {
   private final String jmxUsername;
   private final String jmxPassword;
   private final String bootstrapServers;
-  private final String zookeeper;
   private final InternalSchemaRegistry schemaRegistry;
   private final String ksqldbServer;
   private final List<KafkaConnectCluster> kafkaConnect;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/MetricsCache.java
Patch:
@@ -27,7 +27,6 @@ public static class Metrics {
     Throwable lastKafkaException;
     String version;
     List<Feature> features;
-    ZookeeperService.ZkStatus zkStatus;
     ReactiveAdminClient.ClusterDescription clusterDescription;
     JmxClusterUtil.JmxMetrics jmxMetrics;
     InternalLogDirStats logDirInfo;
@@ -39,7 +38,6 @@ public static Metrics empty() {
           .status(ServerStatusDTO.OFFLINE)
           .version("Unknown")
           .features(List.of())
-          .zkStatus(new ZookeeperService.ZkStatus(ServerStatusDTO.OFFLINE, null))
           .clusterDescription(
               new ReactiveAdminClient.ClusterDescription(null, null, List.of(), Set.of()))
           .jmxMetrics(JmxClusterUtil.JmxMetrics.empty())

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/mapper/ClusterMapper.java
Patch:
@@ -12,6 +12,7 @@
 import com.provectus.kafka.ui.model.ConfigSourceDTO;
 import com.provectus.kafka.ui.model.ConfigSynonymDTO;
 import com.provectus.kafka.ui.model.ConnectDTO;
+import com.provectus.kafka.ui.model.FailoverUrlList;
 import com.provectus.kafka.ui.model.Feature;
 import com.provectus.kafka.ui.model.InternalBrokerConfig;
 import com.provectus.kafka.ui.model.InternalBrokerDiskUsage;
@@ -97,8 +98,8 @@ default InternalSchemaRegistry setSchemaRegistry(ClustersProperties.Cluster clus
 
     internalSchemaRegistry.url(
         clusterProperties.getSchemaRegistry() != null
-            ? Arrays.asList(clusterProperties.getSchemaRegistry().split(","))
-            : Collections.emptyList()
+            ? new FailoverUrlList(Arrays.asList(clusterProperties.getSchemaRegistry().split(",")))
+            : new FailoverUrlList(Collections.emptyList())
     );
 
     if (clusterProperties.getSchemaRegistryAuth() != null) {

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/exception/GlobalErrorWebExceptionHandler.java
Patch:
@@ -10,7 +10,7 @@
 import java.util.Set;
 import java.util.stream.Collectors;
 import java.util.stream.Stream;
-import org.springframework.boot.autoconfigure.web.ResourceProperties;
+import org.springframework.boot.autoconfigure.web.WebProperties;
 import org.springframework.boot.autoconfigure.web.reactive.error.AbstractErrorWebExceptionHandler;
 import org.springframework.boot.web.reactive.error.ErrorAttributes;
 import org.springframework.context.ApplicationContext;
@@ -36,10 +36,9 @@
 public class GlobalErrorWebExceptionHandler extends AbstractErrorWebExceptionHandler {
 
   public GlobalErrorWebExceptionHandler(ErrorAttributes errorAttributes,
-                                        ResourceProperties resourceProperties,
                                         ApplicationContext applicationContext,
                                         ServerCodecConfigurer codecConfigurer) {
-    super(errorAttributes, resourceProperties, applicationContext);
+    super(errorAttributes, new WebProperties.Resources(), applicationContext);
     this.setMessageWriters(codecConfigurer.getWriters());
   }
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/ReactiveAdminClient.java
Patch:
@@ -92,13 +92,13 @@ private static SupportedFeature getSupportedUpdateFeatureForVersion(String versi
 
   //TODO: discuss - maybe we should map kafka-library's exceptions to our exceptions here
   private static <T> Mono<T> toMono(KafkaFuture<T> future) {
-    return Mono.create(sink -> future.whenComplete((res, ex) -> {
+    return Mono.<T>create(sink -> future.whenComplete((res, ex) -> {
       if (ex != null) {
         sink.error(ex);
       } else {
         sink.success(res);
       }
-    }));
+    })).doOnCancel(() -> future.cancel(true));
   }
 
   //---------------------------------------------------------------------------------

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/config/auth/AbstractAuthSecurityConfig.java
Patch:
@@ -16,7 +16,8 @@ protected AbstractAuthSecurityConfig() {
       "/auth",
       "/login",
       "/logout",
-      "/oauth2/**"
+      "/oauth2/**",
+      "/static/**"
   };
 
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/MetricsService.java
Patch:
@@ -56,7 +56,7 @@ private Mono<MetricsCache.Metrics> getMetrics(KafkaCluster cluster) {
         .doOnError(e ->
             log.error("Failed to collect cluster {} info", cluster.getName(), e))
         .onErrorResume(
-            e -> Mono.just(MetricsCache.empty().toBuilder().lastKafkaException(e).build()));
+            e -> Mono.just(MetricsCache.Metrics.empty().toBuilder().lastKafkaException(e).build()));
   }
 
   private Mono<InternalLogDirStats> getLogDirInfo(KafkaCluster cluster, ReactiveAdminClient c) {

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/service/TopicsServicePaginationTest.java
Patch:
@@ -29,7 +29,7 @@ private void init(Collection<TopicDescription> topicsInCache) {
             .collect(Collectors.toSet())))
         .getMock();
 
-    MetricsCache.Metrics metricsCache = MetricsCache.empty().toBuilder()
+    MetricsCache.Metrics metricsCache = MetricsCache.Metrics.empty().toBuilder()
         .topicDescriptions(
             topicsInCache.stream().collect(Collectors.toMap(TopicDescription::name, d -> d)))
         .build();

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/client/RetryingKafkaConnectClient.java
Patch:
@@ -20,7 +20,6 @@
 import reactor.core.publisher.Flux;
 import reactor.core.publisher.Mono;
 import reactor.util.retry.Retry;
-import reactor.util.retry.RetryBackoffSpec;
 
 @Slf4j
 public class RetryingKafkaConnectClient extends KafkaConnectClientApi {
@@ -32,7 +31,7 @@ public RetryingKafkaConnectClient(String basePath) {
   }
 
   private static Retry conflictCodeRetry() {
-    return RetryBackoffSpec
+    return Retry
         .fixedDelay(MAX_RETRIES, RETRIES_DELAY)
         .filter(e -> e instanceof WebClientResponseException.Conflict)
         .onRetryExhaustedThrow((spec, signal) ->

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/config/ClustersProperties.java
Patch:
@@ -61,14 +61,14 @@ public void validateAndSetDefaults() {
 
   private void validateClusterNames() {
     // if only one cluster provided it is ok not to set name
-    if (clusters.size() == 1 && StringUtils.isEmpty(clusters.get(0).getName())) {
+    if (clusters.size() == 1 && !StringUtils.hasText(clusters.get(0).getName())) {
       clusters.get(0).setName("Default");
       return;
     }
 
     Set<String> clusterNames = new HashSet<>();
     for (Cluster clusterProperties : clusters) {
-      if (StringUtils.isEmpty(clusterProperties.getName())) {
+      if (!StringUtils.hasText(clusterProperties.getName())) {
         throw new IllegalStateException(
             "Application config isn't valid. "
                 + "Cluster names should be provided in case of multiple clusters present");
@@ -79,5 +79,4 @@ private void validateClusterNames() {
       }
     }
   }
-
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/config/Config.java
Patch:
@@ -1,6 +1,5 @@
 package com.provectus.kafka.ui.config;
 
-import com.fasterxml.jackson.databind.Module;
 import com.provectus.kafka.ui.model.JmxConnectionInfo;
 import com.provectus.kafka.ui.util.JmxPoolFactory;
 import java.util.Collections;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/config/ReadOnlyModeFilter.java
Patch:
@@ -45,7 +45,7 @@ public Mono<Void> filter(ServerWebExchange exchange, @NotNull WebFilterChain cha
             () -> new ClusterNotFoundException(
                 String.format("No cluster for name '%s'", clusterName)));
 
-    if (!kafkaCluster.getReadOnly()) {
+    if (!kafkaCluster.isReadOnly()) {
       return chain.filter(exchange);
     }
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/config/auth/LdapSecurityConfig.java
Patch:
@@ -43,11 +43,11 @@ public class LdapSecurityConfig extends AbstractAuthSecurityConfig {
   public ReactiveAuthenticationManager authenticationManager(BaseLdapPathContextSource contextSource) {
     BindAuthenticator ba = new BindAuthenticator(contextSource);
     if (ldapUserDnPattern != null) {
-      ba.setUserDnPatterns(new String[]{ldapUserDnPattern});
+      ba.setUserDnPatterns(new String[] {ldapUserDnPattern});
     }
     if (userFilterSearchFilter != null) {
       LdapUserSearch userSearch =
-              new FilterBasedLdapUserSearch(userFilterSearchBase, userFilterSearchFilter, contextSource);
+          new FilterBasedLdapUserSearch(userFilterSearchBase, userFilterSearchFilter, contextSource);
       ba.setUserSearch(userSearch);
     }
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/config/auth/OAuthSecurityConfig.java
Patch:
@@ -39,14 +39,14 @@ public SecurityWebFilterChain configure(ServerHttpSecurity http) {
         .authenticated();
 
     if (IS_OAUTH2_PRESENT && OAuth2ClasspathGuard.shouldConfigure(this.context)) {
-      OAuth2ClasspathGuard.configure(this.context, http);
+      OAuth2ClasspathGuard.configure(http);
     }
 
     return http.csrf().disable().build();
   }
 
   private static class OAuth2ClasspathGuard {
-    static void configure(ApplicationContext context, ServerHttpSecurity http) {
+    static void configure(ServerHttpSecurity http) {
       http
           .oauth2Login()
           .and()

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/controller/ClustersController.java
Patch:
@@ -21,15 +21,15 @@ public class ClustersController extends AbstractController implements ClustersAp
 
   @Override
   public Mono<ResponseEntity<ClusterMetricsDTO>> getClusterMetrics(String clusterName,
-                                                                ServerWebExchange exchange) {
+                                                                   ServerWebExchange exchange) {
     return clusterService.getClusterMetrics(getCluster(clusterName))
         .map(ResponseEntity::ok)
         .onErrorReturn(ResponseEntity.notFound().build());
   }
 
   @Override
   public Mono<ResponseEntity<ClusterStatsDTO>> getClusterStats(String clusterName,
-                                                            ServerWebExchange exchange) {
+                                                               ServerWebExchange exchange) {
     return clusterService.getClusterStats(getCluster(clusterName))
         .map(ResponseEntity::ok)
         .onErrorReturn(ResponseEntity.notFound().build());
@@ -42,7 +42,7 @@ public Mono<ResponseEntity<Flux<ClusterDTO>>> getClusters(ServerWebExchange exch
 
   @Override
   public Mono<ResponseEntity<ClusterDTO>> updateClusterInfo(String clusterName,
-                                                         ServerWebExchange exchange) {
+                                                            ServerWebExchange exchange) {
     return clusterService.updateCluster(getCluster(clusterName)).map(ResponseEntity::ok);
   }
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/controller/ConsumerGroupsController.java
Patch:
@@ -56,7 +56,7 @@ public Mono<ResponseEntity<ConsumerGroupDetailsDTO>> getConsumerGroup(
 
   @Override
   public Mono<ResponseEntity<Flux<ConsumerGroupDTO>>> getConsumerGroups(String clusterName,
-                                                                     ServerWebExchange exchange) {
+                                                                        ServerWebExchange exchange) {
     return consumerGroupService.getAllConsumerGroups(getCluster(clusterName))
         .map(Flux::fromIterable)
         .map(f -> f.map(ConsumerGroupMapper::toDto))
@@ -96,7 +96,7 @@ public Mono<ResponseEntity<ConsumerGroupsPageResponseDTO>> getConsumerGroupsPage
   }
 
   private ConsumerGroupsPageResponseDTO convertPage(ConsumerGroupService.ConsumerGroupsPage
-                                                    consumerGroupConsumerGroupsPage) {
+                                                        consumerGroupConsumerGroupsPage) {
     return new ConsumerGroupsPageResponseDTO()
         .pageCount(consumerGroupConsumerGroupsPage.getTotalPages())
         .consumerGroups(consumerGroupConsumerGroupsPage.getConsumerGroups()

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/controller/KsqlController.java
Patch:
@@ -27,9 +27,9 @@ public class KsqlController extends AbstractController implements KsqlApi {
 
   @Override
   public Mono<ResponseEntity<KsqlCommandResponseDTO>> executeKsqlCommand(String clusterName,
-                                                                      Mono<KsqlCommandDTO>
-                                                                          ksqlCommand,
-                                                                      ServerWebExchange exchange) {
+                                                                         Mono<KsqlCommandDTO>
+                                                                             ksqlCommand,
+                                                                         ServerWebExchange exchange) {
     return ksqlService.executeKsqlCommand(getCluster(clusterName), ksqlCommand)
         .map(ResponseEntity::ok);
   }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/controller/StaticController.java
Patch:
@@ -1,13 +1,11 @@
 package com.provectus.kafka.ui.controller;
 
 import com.provectus.kafka.ui.util.ResourceUtil;
-import java.util.Optional;
 import java.util.concurrent.atomic.AtomicReference;
 import lombok.RequiredArgsConstructor;
 import lombok.SneakyThrows;
 import lombok.extern.slf4j.Slf4j;
 import org.springframework.beans.factory.annotation.Value;
-import org.springframework.boot.autoconfigure.web.ServerProperties;
 import org.springframework.core.io.Resource;
 import org.springframework.http.ResponseEntity;
 import org.springframework.web.bind.annotation.GetMapping;
@@ -24,7 +22,7 @@ public class StaticController {
   private Resource indexFile;
   private final AtomicReference<String> renderedIndexFile = new AtomicReference<>();
 
-  @GetMapping(value = "/index.html", produces = { "text/html" })
+  @GetMapping(value = "/index.html", produces = {"text/html"})
   public Mono<ResponseEntity<String>> getIndex(ServerWebExchange exchange) {
     return Mono.just(ResponseEntity.ok(getRenderedIndexFile(exchange)));
   }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/emitter/BackwardRecordEmitter.java
Patch:
@@ -87,7 +87,7 @@ public void accept(FluxSink<TopicMessageEventDTO> sink) {
 
               // This is workaround for case when partition begin offset is less than
               // real minimal offset, usually appear in compcated topics
-              if (records.count() > 0  && partitionRecords.isEmpty()) {
+              if (records.count() > 0 && partitionRecords.isEmpty()) {
                 waitingOffsets.markPolled(entry.getKey().partition());
               }
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/emitter/MessageFilters.java
Patch:
@@ -18,6 +18,9 @@ public class MessageFilters {
 
   private static GroovyScriptEngineImpl GROOVY_ENGINE;
 
+  private MessageFilters() {
+  }
+
   public static Predicate<TopicMessageDTO> createMsgFilter(String query, MessageFilterTypeDTO type) {
     switch (type) {
       case STRING_CONTAINS:

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/exception/CustomBaseException.java
Patch:
@@ -19,7 +19,7 @@ protected CustomBaseException(Throwable cause) {
   }
 
   protected CustomBaseException(String message, Throwable cause, boolean enableSuppression,
-                             boolean writableStackTrace) {
+                                boolean writableStackTrace) {
     super(message, cause, enableSuppression, writableStackTrace);
   }
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/exception/ErrorCode.java
Patch:
@@ -34,7 +34,7 @@ public enum ErrorCode {
     for (ErrorCode value : ErrorCode.values()) {
       if (!codes.add(value.code())) {
         LoggerFactory.getLogger(ErrorCode.class)
-                .warn("Multiple {} values refer to code {}", ErrorCode.class, value.code);
+            .warn("Multiple {} values refer to code {}", ErrorCode.class, value.code);
       }
     }
   }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/exception/TopicRecreationException.java
Patch:
@@ -8,6 +8,6 @@ public ErrorCode getErrorCode() {
 
   public TopicRecreationException(String topicName, int seconds) {
     super(String.format("Can't create topic '%s' in %d seconds: "
-                + "topic deletion is still in progress", topicName, seconds));
+        + "topic deletion is still in progress", topicName, seconds));
   }
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/model/InternalClusterMetrics.java
Patch:
@@ -38,7 +38,7 @@ public static InternalClusterMetrics empty() {
 
   // zk stats
   @Deprecated //use 'zookeeperStatus' field with enum type instead
-  private final int zooKeeperStatus;
+  private final int zooKeeperStatusEnum;
   private final ServerStatusDTO zookeeperStatus;
   private final Throwable lastZookeeperException;
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/model/InternalClusterState.java
Patch:
@@ -67,7 +67,7 @@ public InternalClusterState(KafkaCluster cluster, MetricsCache.Metrics metrics)
     inSyncReplicasCount = partitionsStats.getInSyncReplicasCount();
     outOfSyncReplicasCount = partitionsStats.getOutOfSyncReplicasCount();
     underReplicatedPartitionCount = partitionsStats.getUnderReplicatedPartitionCount();
-    readOnly = cluster.getReadOnly();
+    readOnly = cluster.isReadOnly();
   }
 
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/model/InternalSchemaRegistry.java
Patch:
@@ -12,7 +12,7 @@ public class InternalSchemaRegistry {
   private final List<String> url;
 
   public String getFirstUrl() {
-    return url != null  && !url.isEmpty() ? url.iterator().next() : null;
+    return url != null && !url.isEmpty() ? url.iterator().next() : null;
   }
 
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/model/KafkaCluster.java
Patch:
@@ -30,6 +30,6 @@ public class KafkaCluster {
   private final String protobufMessageName;
   private final Map<String, String> protobufMessageNameByTopic;
   private final Properties properties;
-  private final Boolean readOnly;
-  private final Boolean disableLogDirsCollection;
+  private final boolean readOnly;
+  private final boolean disableLogDirsCollection;
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/model/schemaregistry/InternalNewSchema.java
Patch:
@@ -2,7 +2,6 @@
 
 import com.fasterxml.jackson.annotation.JsonInclude;
 import com.provectus.kafka.ui.model.SchemaTypeDTO;
-import com.provectus.kafka.ui.model.SchemaTypeDTO;
 import lombok.Data;
 
 @Data

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/serde/schemaregistry/AvroMessageReader.java
Patch:
@@ -6,8 +6,8 @@
 import io.confluent.kafka.schemaregistry.client.SchemaMetadata;
 import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;
 import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;
+import io.confluent.kafka.serializers.AbstractKafkaSchemaSerDeConfig;
 import io.confluent.kafka.serializers.KafkaAvroSerializer;
-import io.confluent.kafka.serializers.KafkaAvroSerializerConfig;
 import java.io.IOException;
 import java.util.Map;
 import org.apache.kafka.common.serialization.Serializer;
@@ -27,8 +27,8 @@ protected Serializer<Object> createSerializer(SchemaRegistryClient client) {
     serializer.configure(
         Map.of(
             "schema.registry.url", "wontbeused",
-            KafkaAvroSerializerConfig.AUTO_REGISTER_SCHEMAS, false,
-            KafkaAvroSerializerConfig.USE_LATEST_VERSION, true
+            AbstractKafkaSchemaSerDeConfig.AUTO_REGISTER_SCHEMAS, false,
+            AbstractKafkaSchemaSerDeConfig.USE_LATEST_VERSION, true
         ),
         isKey
     );

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/serde/schemaregistry/ProtobufMessageReader.java
Patch:
@@ -8,8 +8,8 @@
 import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;
 import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;
 import io.confluent.kafka.schemaregistry.protobuf.ProtobufSchema;
+import io.confluent.kafka.serializers.AbstractKafkaSchemaSerDeConfig;
 import io.confluent.kafka.serializers.protobuf.KafkaProtobufSerializer;
-import io.confluent.kafka.serializers.protobuf.KafkaProtobufSerializerConfig;
 import java.io.IOException;
 import java.util.Map;
 import org.apache.kafka.common.serialization.Serializer;
@@ -28,8 +28,8 @@ protected Serializer<Message> createSerializer(SchemaRegistryClient client) {
     serializer.configure(
         Map.of(
             "schema.registry.url", "wontbeused",
-            KafkaProtobufSerializerConfig.AUTO_REGISTER_SCHEMAS, false,
-            KafkaProtobufSerializerConfig.USE_LATEST_VERSION, true
+            AbstractKafkaSchemaSerDeConfig.AUTO_REGISTER_SCHEMAS, false,
+            AbstractKafkaSchemaSerDeConfig.USE_LATEST_VERSION, true
         ),
         isKey
     );

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/BrokerService.java
Patch:
@@ -105,7 +105,7 @@ private Mono<Void> updateBrokerLogDir(ReactiveAdminClient admin,
     Map<TopicPartitionReplica, String> req = Map.of(
         new TopicPartitionReplica(b.getTopic(), b.getPartition(), broker),
         b.getLogDir());
-    return  admin.alterReplicaLogDirs(req)
+    return admin.alterReplicaLogDirs(req)
         .onErrorResume(UnknownTopicOrPartitionException.class,
             e -> Mono.error(new TopicOrPartitionNotFoundException()))
         .onErrorResume(LogDirNotFoundException.class,

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/FeatureService.java
Patch:
@@ -44,7 +44,7 @@ public Mono<List<Feature>> getAvailableFeatures(KafkaCluster cluster, @Nullable
     if (controller != null) {
       features.add(
           isTopicDeletionEnabled(cluster, controller)
-              .flatMap(r -> r ? Mono.just(Feature.TOPIC_DELETION) : Mono.empty())
+              .flatMap(r -> Boolean.TRUE.equals(r) ? Mono.just(Feature.TOPIC_DELETION) : Mono.empty())
       );
     }
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/KafkaConfigSanitizer.java
Patch:
@@ -30,7 +30,7 @@ class KafkaConfigSanitizer extends Sanitizer {
       var keysToSanitize = new HashSet<>(
           patternsToSanitize.isEmpty() ? DEFAULT_PATTERNS_TO_SANITIZE : patternsToSanitize);
       keysToSanitize.addAll(kafkaConfigKeysToSanitize());
-      setKeysToSanitize(keysToSanitize.toArray(new String[]{}));
+      setKeysToSanitize(keysToSanitize.toArray(new String[] {}));
     }
   }
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/KsqlService.java
Patch:
@@ -20,7 +20,7 @@ public class KsqlService {
   private final List<BaseStrategy> ksqlStatementStrategies;
 
   public Mono<KsqlCommandResponseDTO> executeKsqlCommand(KafkaCluster cluster,
-                                                      Mono<KsqlCommandDTO> ksqlCommand) {
+                                                         Mono<KsqlCommandDTO> ksqlCommand) {
     return Mono.justOrEmpty(cluster)
         .map(KafkaCluster::getKsqldbServer)
         .onErrorResume(e -> {

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/MessagesService.java
Patch:
@@ -84,7 +84,7 @@ public Mono<RecordMetadata> sendMessage(KafkaCluster cluster, String topic,
                                           CreateTopicMessageDTO msg) {
     if (msg.getPartition() != null
         && msg.getPartition() > metricsCache.get(cluster).getTopicDescriptions()
-          .get(topic).partitions().size() - 1) {
+        .get(topic).partitions().size() - 1) {
       throw new ValidationException("Invalid partition");
     }
     RecordSerDe serde =

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/MetricsService.java
Patch:
@@ -60,7 +60,7 @@ private Mono<MetricsCache.Metrics> getMetrics(KafkaCluster cluster) {
   }
 
   private Mono<InternalLogDirStats> getLogDirInfo(KafkaCluster cluster, ReactiveAdminClient c) {
-    if (cluster.getDisableLogDirsCollection() == null || !cluster.getDisableLogDirsCollection()) {
+    if (!cluster.isDisableLogDirsCollection()) {
       return c.describeLogDirs().map(InternalLogDirStats::new);
     }
     return Mono.just(InternalLogDirStats.empty());

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/ZookeeperService.java
Patch:
@@ -15,7 +15,6 @@
 import org.springframework.stereotype.Service;
 import org.springframework.util.StringUtils;
 import reactor.core.publisher.Mono;
-import reactor.core.scheduler.Schedulers;
 
 @Service
 @RequiredArgsConstructor
@@ -82,10 +81,11 @@ private ZooKeeper getOrCreateZkClient(KafkaCluster cluster) {
 
   private ZooKeeper createClient(KafkaCluster cluster) {
     try {
-      return new ZooKeeper(cluster.getZookeeper(), 60 * 1000, watchedEvent -> {});
+      return new ZooKeeper(cluster.getZookeeper(), 60 * 1000, watchedEvent -> {
+      });
     } catch (IOException e) {
       log.error("Error while creating a zookeeper client for cluster [{}]",
-              cluster.getName());
+          cluster.getName());
       throw new ZooKeeperException(e);
     }
   }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/ksql/KsqlApiClient.java
Patch:
@@ -101,7 +101,7 @@ public Flux<KsqlResponseTable> execute(String ksql, Map<String, String> streamPr
     if (parsed.getStatements().size() > 1) {
       throw new ValidationException("Only single statement supported now");
     }
-    if (parsed.getStatements().size() == 0) {
+    if (parsed.getStatements().isEmpty()) {
       throw new ValidationException("No valid ksql statement found");
     }
     if (KsqlGrammar.isSelect(parsed.getStatements().get(0))) {

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/ksql/KsqlGrammar.java
Patch:
@@ -18,6 +18,9 @@
 
 class KsqlGrammar {
 
+  private KsqlGrammar() {
+  }
+
   @Value
   static class KsqlStatements {
     List<KsqlGrammarParser.SingleStatementContext> statements;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/ksql/response/DynamicParser.java
Patch:
@@ -12,6 +12,9 @@
 
 class DynamicParser {
 
+  private DynamicParser() {
+  }
+
   static KsqlResponseTable parseArray(String tableName, JsonNode array) {
     return parseArray(tableName, getFieldNamesFromArray(array), array);
   }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/ksql/response/ResponseParser.java
Patch:
@@ -14,6 +14,9 @@
 
 public class ResponseParser {
 
+  private ResponseParser() {
+  }
+
   public static Optional<KsqlApiClient.KsqlResponseTable> parseSelectResponse(JsonNode jsonNode) {
     // in response we getting either header record or row data
     if (arrayFieldNonEmpty(jsonNode, "header")) {

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/util/ClusterUtil.java
Patch:
@@ -18,6 +18,9 @@
 @Slf4j
 public class ClusterUtil {
 
+  private ClusterUtil() {
+  }
+
   private static final ZoneId UTC_ZONE_ID = ZoneId.of("UTC");
 
   public static int convertToIntServerStatus(ServerStatusDTO serverStatus) {

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/util/JmxPoolFactory.java
Patch:
@@ -21,7 +21,7 @@ public class JmxPoolFactory extends BaseKeyedPooledObjectFactory<JmxConnectionIn
   public JMXConnector create(JmxConnectionInfo info) throws Exception {
     Map<String, Object> env = new HashMap<>();
     if (StringUtils.isNotEmpty(info.getUsername()) && StringUtils.isNotEmpty(info.getPassword())) {
-      env.put("jmx.remote.credentials", new String[]{info.getUsername(), info.getPassword()});
+      env.put("jmx.remote.credentials", new String[] {info.getUsername(), info.getPassword()});
     }
 
     if (info.isSsl()) {

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/util/OffsetsSeek.java
Patch:
@@ -107,8 +107,8 @@ public WaitingOffsets(String topic, Consumer<?, ?> consumer,
           .collect(Collectors.toMap(Tuple2::getT1, Tuple2::getT2));
 
       this.beginOffsets = this.endOffsets.keySet().stream()
-         .map(p -> Tuples.of(p, allBeginningOffsets.get(new TopicPartition(topic, p))))
-         .collect(Collectors.toMap(Tuple2::getT1, Tuple2::getT2));
+          .map(p -> Tuples.of(p, allBeginningOffsets.get(new TopicPartition(topic, p))))
+          .collect(Collectors.toMap(Tuple2::getT1, Tuple2::getT2));
     }
 
     public List<TopicPartition> topicPartitions() {

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/util/OffsetsSeekBackward.java
Patch:
@@ -36,13 +36,13 @@ public int msgsPerPartition(long awaitingMessages, int partitionsSize) {
 
 
   protected Map<TopicPartition, Long> offsetsFromPositions(Consumer<Bytes, Bytes> consumer,
-                                        List<TopicPartition> partitions) {
+                                                           List<TopicPartition> partitions) {
 
     return findOffsetsInt(consumer, consumerPosition.getSeekTo(), partitions);
   }
 
   protected Map<TopicPartition, Long> offsetsFromBeginning(Consumer<Bytes, Bytes> consumer,
-                                            List<TopicPartition> partitions) {
+                                                           List<TopicPartition> partitions) {
     return findOffsets(consumer, Map.of(), partitions);
   }
 
@@ -51,7 +51,7 @@ protected Map<TopicPartition, Long> offsetsForTimestamp(Consumer<Bytes, Bytes> c
         consumerPosition.getSeekTo().entrySet().stream()
             .collect(Collectors.toMap(
                 Map.Entry::getKey,
-                e -> e.getValue()
+                Map.Entry::getValue
             ));
     Map<TopicPartition, Long> offsetsForTimestamps = consumer.offsetsForTimes(timestampsToSearch)
         .entrySet().stream()

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/util/OffsetsSeekForward.java
Patch:
@@ -19,7 +19,7 @@ public OffsetsSeekForward(String topic, ConsumerPosition consumerPosition) {
   }
 
   protected Map<TopicPartition, Long> offsetsFromPositions(Consumer<Bytes, Bytes> consumer,
-                                        List<TopicPartition> partitions) {
+                                                           List<TopicPartition> partitions) {
     final Map<TopicPartition, Long> offsets =
         offsetsFromBeginning(consumer, partitions);
 
@@ -54,7 +54,7 @@ protected Map<TopicPartition, Long> offsetsForTimestamp(Consumer<Bytes, Bytes> c
   }
 
   protected Map<TopicPartition, Long> offsetsFromBeginning(Consumer<Bytes, Bytes> consumer,
-                                            List<TopicPartition> partitions) {
+                                                           List<TopicPartition> partitions) {
     return consumer.beginningOffsets(partitions);
   }
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/util/jsonschema/JsonType.java
Patch:
@@ -8,7 +8,7 @@ public abstract class JsonType {
 
   protected final Type type;
 
-  public JsonType(Type type) {
+  protected JsonType(Type type) {
     this.type = type;
   }
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/util/jsonschema/ObjectFieldSchema.java
Patch:
@@ -2,10 +2,7 @@
 
 import com.fasterxml.jackson.databind.JsonNode;
 import com.fasterxml.jackson.databind.ObjectMapper;
-import com.fasterxml.jackson.databind.node.ArrayNode;
-import com.fasterxml.jackson.databind.node.BooleanNode;
 import com.fasterxml.jackson.databind.node.ObjectNode;
-import com.fasterxml.jackson.databind.node.TextNode;
 import java.util.List;
 import java.util.Map;
 import java.util.stream.Collectors;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/mapper/ConsumerGroupMapper.java
Patch:
@@ -93,7 +93,7 @@ private static <T extends ConsumerGroupDTO> T convertToConsumerGroup(
   }
 
   private static BrokerDTO mapCoordinator(Node node) {
-    return new BrokerDTO().host(node.host()).id(node.id());
+    return new BrokerDTO().host(node.host()).id(node.id()).port(node.port());
   }
 
   private static ConsumerGroupStateDTO mapConsumerGroupState(

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/BrokerService.java
Patch:
@@ -78,6 +78,7 @@ public Flux<BrokerDTO> getBrokers(KafkaCluster cluster) {
               BrokerDTO broker = new BrokerDTO();
               broker.setId(node.id());
               broker.setHost(node.host());
+              broker.setPort(node.port());
               return broker;
             }).collect(Collectors.toList()))
         .flatMapMany(Flux::fromIterable);

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/exception/ErrorCode.java
Patch:
@@ -11,7 +11,6 @@ public enum ErrorCode {
   KSQL_API_ERROR(5001, HttpStatus.INTERNAL_SERVER_ERROR),
   BINDING_FAIL(4001, HttpStatus.BAD_REQUEST),
   NOT_FOUND(404, HttpStatus.NOT_FOUND),
-  INVALID_ENTITY_STATE(4001, HttpStatus.BAD_REQUEST),
   VALIDATION_FAIL(4002, HttpStatus.BAD_REQUEST),
   READ_ONLY_MODE_ENABLE(4003, HttpStatus.METHOD_NOT_ALLOWED),
   CONNECT_CONFLICT_RESPONSE(4004, HttpStatus.CONFLICT),
@@ -26,7 +25,8 @@ public enum ErrorCode {
   TOPIC_OR_PARTITION_NOT_FOUND(4013, HttpStatus.BAD_REQUEST),
   INVALID_REQUEST(4014, HttpStatus.BAD_REQUEST),
   RECREATE_TOPIC_TIMEOUT(4015, HttpStatus.REQUEST_TIMEOUT),
-  SCHEMA_NOT_DELETED(4015, HttpStatus.INTERNAL_SERVER_ERROR);
+  INVALID_ENTITY_STATE(4016, HttpStatus.BAD_REQUEST),
+  SCHEMA_NOT_DELETED(4017, HttpStatus.INTERNAL_SERVER_ERROR);
 
   static {
     // codes uniqueness check

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/util/jsonschema/JsonSchema.java
Patch:
@@ -24,7 +24,8 @@ public class JsonSchema {
   private final Map<String, FieldSchema> definitions;
   private final List<String> required;
 
-  public String toJson(ObjectMapper mapper) {
+  public String toJson() {
+    final ObjectMapper mapper = new ObjectMapper();
     final ObjectNode objectNode = mapper.createObjectNode();
     objectNode.set("$id", new TextNode(id.toString()));
     objectNode.set("$schema", new TextNode(schema.toString()));

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/util/jsonschema/ProtobufSchemaConverterTest.java
Patch:
@@ -64,9 +64,7 @@ public void testSimpleProto() throws URISyntaxException, JsonProcessingException
     ObjectMapper om = new ObjectMapper();
     Assertions.assertEquals(
         om.readTree(expected),
-        om.readTree(
-            convert.toJson(om)
-        )
+        om.readTree(convert.toJson())
     );
   }
 }
\ No newline at end of file

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/controller/SchemasController.java
Patch:
@@ -69,7 +69,8 @@ public Mono<ResponseEntity<Void>> deleteLatestSchema(
   @Override
   public Mono<ResponseEntity<Void>> deleteSchema(
       String clusterName, String subjectName, ServerWebExchange exchange) {
-    return schemaRegistryService.deleteSchemaSubjectEntirely(getCluster(clusterName), subjectName);
+    return schemaRegistryService.deleteSchemaSubjectEntirely(getCluster(clusterName), subjectName)
+            .thenReturn(ResponseEntity.ok().build());
   }
 
   @Override

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/exception/ErrorCode.java
Patch:
@@ -25,15 +25,16 @@ public enum ErrorCode {
   DIR_NOT_FOUND(4012, HttpStatus.BAD_REQUEST),
   TOPIC_OR_PARTITION_NOT_FOUND(4013, HttpStatus.BAD_REQUEST),
   INVALID_REQUEST(4014, HttpStatus.BAD_REQUEST),
-  RECREATE_TOPIC_TIMEOUT(4015, HttpStatus.REQUEST_TIMEOUT);
+  RECREATE_TOPIC_TIMEOUT(4015, HttpStatus.REQUEST_TIMEOUT),
+  SCHEMA_NOT_DELETED(4015, HttpStatus.INTERNAL_SERVER_ERROR);
 
   static {
     // codes uniqueness check
     var codes = new HashSet<Integer>();
     for (ErrorCode value : ErrorCode.values()) {
       if (!codes.add(value.code())) {
         LoggerFactory.getLogger(ErrorCode.class)
-            .warn("Multiple {} values refer to code {}", ErrorCode.class, value.code);
+                .warn("Multiple {} values refer to code {}", ErrorCode.class, value.code);
       }
     }
   }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/exception/ErrorCode.java
Patch:
@@ -24,7 +24,8 @@ public enum ErrorCode {
   KSQLDB_NOT_FOUND(4011, HttpStatus.NOT_FOUND),
   DIR_NOT_FOUND(4012, HttpStatus.BAD_REQUEST),
   TOPIC_OR_PARTITION_NOT_FOUND(4013, HttpStatus.BAD_REQUEST),
-  INVALID_REQUEST(4014, HttpStatus.BAD_REQUEST);
+  INVALID_REQUEST(4014, HttpStatus.BAD_REQUEST),
+  RECREATE_TOPIC_TIMEOUT(4015, HttpStatus.REQUEST_TIMEOUT);
 
   static {
     // codes uniqueness check

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/ConsumerGroupService.java
Patch:
@@ -202,6 +202,7 @@ public KafkaConsumer<Bytes, Bytes> createConsumer(KafkaCluster cluster,
     props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, BytesDeserializer.class);
     props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, BytesDeserializer.class);
     props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
+    props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "false");
     props.putAll(properties);
 
     return new KafkaConsumer<>(props);

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/util/ResultSizeLimiter.java
Patch:
@@ -4,11 +4,11 @@
 import java.util.concurrent.atomic.AtomicInteger;
 import java.util.function.Predicate;
 
-public class FilterTopicMessageEvents implements Predicate<TopicMessageEventDTO> {
+public class ResultSizeLimiter implements Predicate<TopicMessageEventDTO> {
   private final AtomicInteger processed = new AtomicInteger();
   private final int limit;
 
-  public FilterTopicMessageEvents(int limit) {
+  public ResultSizeLimiter(int limit) {
     this.limit = limit;
   }
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/KafkaConnectService.java
Patch:
@@ -266,7 +266,8 @@ public Mono<Void> updateConnectorState(KafkaCluster cluster, String connectName,
     switch (action) {
       case RESTART:
         kafkaClientCall =
-            connect -> KafkaConnectClients.withBaseUrl(connect).restartConnector(connectorName);
+            connect -> KafkaConnectClients.withBaseUrl(connect)
+                .restartConnector(connectorName, true, false);
         break;
       case PAUSE:
         kafkaClientCall =

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/serde/DeserializationService.java
Patch:
@@ -39,7 +39,7 @@ private RecordSerDe createRecordDeserializerForCluster(KafkaCluster cluster) {
             objectMapper);
       } else {
         log.info("Using SchemaRegistryAwareRecordSerDe for cluster '{}'", cluster.getName());
-        return new SchemaRegistryAwareRecordSerDe(cluster);
+        return new SchemaRegistryAwareRecordSerDe(cluster, objectMapper);
       }
     } catch (Throwable e) {
       throw new RuntimeException("Can't init deserializer", e);

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/serde/SimpleRecordSerDe.java
Patch:
@@ -13,6 +13,8 @@
 
 public class SimpleRecordSerDe implements RecordSerDe {
 
+  private static final ObjectMapper objectMapper = new ObjectMapper();
+
   @Override
   public DeserializedKeyValue deserialize(ConsumerRecord<Bytes, Bytes> msg) {
     var builder = DeserializedKeyValue.builder();
@@ -45,7 +47,7 @@ public TopicMessageSchemaDTO getTopicSchema(String topic) {
     final MessageSchemaDTO schema = new MessageSchemaDTO()
         .name("unknown")
         .source(MessageSchemaDTO.SourceEnum.UNKNOWN)
-        .schema(JsonSchema.stringSchema().toJson(new ObjectMapper()));
+        .schema(JsonSchema.stringSchema().toJson(objectMapper));
     return new TopicMessageSchemaDTO()
         .key(schema)
         .value(schema);

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/serde/SchemaRegistryRecordDeserializerTest.java
Patch:
@@ -17,7 +17,7 @@ class SchemaRegistryRecordDeserializerTest {
       new SchemaRegistryAwareRecordSerDe(
           KafkaCluster.builder()
               .schemaNameTemplate("%s-value")
-              .build()
+              .build(), new ObjectMapper()
       );
 
   @Test

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/client/KsqlClient.java
Patch:
@@ -7,7 +7,7 @@
 import com.provectus.kafka.ui.strategy.ksql.statement.BaseStrategy;
 import lombok.RequiredArgsConstructor;
 import lombok.SneakyThrows;
-import lombok.extern.log4j.Log4j2;
+import lombok.extern.slf4j.Slf4j;
 import org.springframework.http.HttpStatus;
 import org.springframework.http.MediaType;
 import org.springframework.stereotype.Service;
@@ -18,7 +18,7 @@
 
 @Service
 @RequiredArgsConstructor
-@Log4j2
+@Slf4j
 public class KsqlClient {
   private final WebClient webClient;
   private final ObjectMapper mapper;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/client/RetryingKafkaConnectClient.java
Patch:
@@ -9,7 +9,7 @@
 import java.time.Duration;
 import java.util.List;
 import java.util.Map;
-import lombok.extern.log4j.Log4j2;
+import lombok.extern.slf4j.Slf4j;
 import org.springframework.core.ParameterizedTypeReference;
 import org.springframework.http.HttpHeaders;
 import org.springframework.http.HttpMethod;
@@ -22,7 +22,7 @@
 import reactor.util.retry.Retry;
 import reactor.util.retry.RetryBackoffSpec;
 
-@Log4j2
+@Slf4j
 public class RetryingKafkaConnectClient extends KafkaConnectClientApi {
   private static final int MAX_RETRIES = 5;
   private static final Duration RETRIES_DELAY = Duration.ofMillis(200);

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/controller/AuthController.java
Patch:
@@ -2,7 +2,7 @@
 
 import java.nio.charset.Charset;
 import lombok.RequiredArgsConstructor;
-import lombok.extern.log4j.Log4j2;
+import lombok.extern.slf4j.Slf4j;
 import org.springframework.security.web.server.csrf.CsrfToken;
 import org.springframework.util.MultiValueMap;
 import org.springframework.web.bind.annotation.GetMapping;
@@ -12,7 +12,7 @@
 
 @RestController
 @RequiredArgsConstructor
-@Log4j2
+@Slf4j
 public class AuthController {
 
   @GetMapping(value = "/auth", produces = { "text/html" })

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/controller/BrokersController.java
Patch:
@@ -10,7 +10,7 @@
 import com.provectus.kafka.ui.service.BrokerService;
 import java.util.List;
 import lombok.RequiredArgsConstructor;
-import lombok.extern.log4j.Log4j2;
+import lombok.extern.slf4j.Slf4j;
 import org.springframework.http.ResponseEntity;
 import org.springframework.web.bind.annotation.RestController;
 import org.springframework.web.server.ServerWebExchange;
@@ -19,7 +19,7 @@
 
 @RestController
 @RequiredArgsConstructor
-@Log4j2
+@Slf4j
 public class BrokersController extends AbstractController implements BrokersApi {
   private final BrokerService brokerService;
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/controller/ClustersController.java
Patch:
@@ -6,7 +6,7 @@
 import com.provectus.kafka.ui.model.ClusterStatsDTO;
 import com.provectus.kafka.ui.service.ClusterService;
 import lombok.RequiredArgsConstructor;
-import lombok.extern.log4j.Log4j2;
+import lombok.extern.slf4j.Slf4j;
 import org.springframework.http.ResponseEntity;
 import org.springframework.web.bind.annotation.RestController;
 import org.springframework.web.server.ServerWebExchange;
@@ -15,7 +15,7 @@
 
 @RestController
 @RequiredArgsConstructor
-@Log4j2
+@Slf4j
 public class ClustersController extends AbstractController implements ClustersApi {
   private final ClusterService clusterService;
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/controller/ConsumerGroupsController.java
Patch:
@@ -13,7 +13,7 @@
 import java.util.Map;
 import java.util.Optional;
 import lombok.RequiredArgsConstructor;
-import lombok.extern.log4j.Log4j2;
+import lombok.extern.slf4j.Slf4j;
 import org.springframework.http.ResponseEntity;
 import org.springframework.util.CollectionUtils;
 import org.springframework.web.bind.annotation.RestController;
@@ -23,7 +23,7 @@
 
 @RestController
 @RequiredArgsConstructor
-@Log4j2
+@Slf4j
 public class ConsumerGroupsController extends AbstractController implements ConsumerGroupsApi {
 
   private final ConsumerGroupService consumerGroupService;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/controller/KafkaConnectController.java
Patch:
@@ -13,7 +13,7 @@
 import java.util.Map;
 import javax.validation.Valid;
 import lombok.RequiredArgsConstructor;
-import lombok.extern.log4j.Log4j2;
+import lombok.extern.slf4j.Slf4j;
 import org.springframework.http.ResponseEntity;
 import org.springframework.web.bind.annotation.RestController;
 import org.springframework.web.server.ServerWebExchange;
@@ -22,7 +22,7 @@
 
 @RestController
 @RequiredArgsConstructor
-@Log4j2
+@Slf4j
 public class KafkaConnectController extends AbstractController implements KafkaConnectApi {
   private final KafkaConnectService kafkaConnectService;
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/controller/KsqlController.java
Patch:
@@ -11,7 +11,7 @@
 import java.util.Map;
 import java.util.Optional;
 import lombok.RequiredArgsConstructor;
-import lombok.extern.log4j.Log4j2;
+import lombok.extern.slf4j.Slf4j;
 import org.springframework.http.ResponseEntity;
 import org.springframework.web.bind.annotation.RestController;
 import org.springframework.web.server.ServerWebExchange;
@@ -21,7 +21,7 @@
 
 @RestController
 @RequiredArgsConstructor
-@Log4j2
+@Slf4j
 public class KsqlController extends AbstractController implements KsqlApi {
   private final KsqlService ksqlService;
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/controller/MessagesController.java
Patch:
@@ -15,7 +15,7 @@
 import java.util.function.Function;
 import javax.validation.Valid;
 import lombok.RequiredArgsConstructor;
-import lombok.extern.log4j.Log4j2;
+import lombok.extern.slf4j.Slf4j;
 import org.apache.commons.lang3.tuple.Pair;
 import org.apache.kafka.common.TopicPartition;
 import org.springframework.http.ResponseEntity;
@@ -26,7 +26,7 @@
 
 @RestController
 @RequiredArgsConstructor
-@Log4j2
+@Slf4j
 public class MessagesController extends AbstractController implements MessagesApi {
   private final MessagesService messagesService;
   private final TopicsService topicsService;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/controller/SchemasController.java
Patch:
@@ -10,7 +10,7 @@
 import com.provectus.kafka.ui.service.SchemaRegistryService;
 import javax.validation.Valid;
 import lombok.RequiredArgsConstructor;
-import lombok.extern.log4j.Log4j2;
+import lombok.extern.slf4j.Slf4j;
 import org.springframework.http.ResponseEntity;
 import org.springframework.web.bind.annotation.RestController;
 import org.springframework.web.server.ServerWebExchange;
@@ -19,7 +19,7 @@
 
 @RestController
 @RequiredArgsConstructor
-@Log4j2
+@Slf4j
 public class SchemasController extends AbstractController implements SchemasApi {
 
   private final SchemaRegistryService schemaRegistryService;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/controller/StaticController.java
Patch:
@@ -5,7 +5,7 @@
 import java.util.concurrent.atomic.AtomicReference;
 import lombok.RequiredArgsConstructor;
 import lombok.SneakyThrows;
-import lombok.extern.log4j.Log4j2;
+import lombok.extern.slf4j.Slf4j;
 import org.springframework.beans.factory.annotation.Value;
 import org.springframework.boot.autoconfigure.web.ServerProperties;
 import org.springframework.core.io.Resource;
@@ -17,7 +17,7 @@
 
 @RestController
 @RequiredArgsConstructor
-@Log4j2
+@Slf4j
 public class StaticController {
 
   @Value("classpath:static/index.html")

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/controller/TopicsController.java
Patch:
@@ -16,7 +16,7 @@
 import java.util.Optional;
 import javax.validation.Valid;
 import lombok.RequiredArgsConstructor;
-import lombok.extern.log4j.Log4j2;
+import lombok.extern.slf4j.Slf4j;
 import org.springframework.http.HttpStatus;
 import org.springframework.http.ResponseEntity;
 import org.springframework.web.bind.annotation.RestController;
@@ -26,7 +26,7 @@
 
 @RestController
 @RequiredArgsConstructor
-@Log4j2
+@Slf4j
 public class TopicsController extends AbstractController implements TopicsApi {
   private final TopicsService topicsService;
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/emitter/BackwardRecordEmitter.java
Patch:
@@ -11,7 +11,7 @@
 import java.util.TreeMap;
 import java.util.function.Function;
 import java.util.stream.Collectors;
-import lombok.extern.log4j.Log4j2;
+import lombok.extern.slf4j.Slf4j;
 import org.apache.kafka.clients.consumer.ConsumerConfig;
 import org.apache.kafka.clients.consumer.ConsumerRecord;
 import org.apache.kafka.clients.consumer.ConsumerRecords;
@@ -20,7 +20,7 @@
 import org.apache.kafka.common.utils.Bytes;
 import reactor.core.publisher.FluxSink;
 
-@Log4j2
+@Slf4j
 public class BackwardRecordEmitter
     extends AbstractEmitter
     implements java.util.function.Consumer<FluxSink<TopicMessageEventDTO>> {

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/emitter/ForwardRecordEmitter.java
Patch:
@@ -6,14 +6,14 @@
 import java.time.Duration;
 import java.time.Instant;
 import java.util.function.Supplier;
-import lombok.extern.log4j.Log4j2;
+import lombok.extern.slf4j.Slf4j;
 import org.apache.kafka.clients.consumer.ConsumerRecord;
 import org.apache.kafka.clients.consumer.ConsumerRecords;
 import org.apache.kafka.clients.consumer.KafkaConsumer;
 import org.apache.kafka.common.utils.Bytes;
 import reactor.core.publisher.FluxSink;
 
-@Log4j2
+@Slf4j
 public class ForwardRecordEmitter
     extends AbstractEmitter
     implements java.util.function.Consumer<FluxSink<TopicMessageEventDTO>> {

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/exception/ErrorCode.java
Patch:
@@ -1,7 +1,7 @@
 package com.provectus.kafka.ui.exception;
 
 import java.util.HashSet;
-import org.apache.logging.log4j.LogManager;
+import org.slf4j.LoggerFactory;
 import org.springframework.http.HttpStatus;
 
 
@@ -31,7 +31,7 @@ public enum ErrorCode {
     var codes = new HashSet<Integer>();
     for (ErrorCode value : ErrorCode.values()) {
       if (!codes.add(value.code())) {
-        LogManager.getLogger()
+        LoggerFactory.getLogger(ErrorCode.class)
             .warn("Multiple {} values refer to code {}", ErrorCode.class, value.code);
       }
     }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/serde/DeserializationService.java
Patch:
@@ -8,10 +8,10 @@
 import java.util.stream.Collectors;
 import javax.annotation.PostConstruct;
 import lombok.RequiredArgsConstructor;
-import lombok.extern.log4j.Log4j2;
+import lombok.extern.slf4j.Slf4j;
 import org.springframework.stereotype.Component;
 
-@Log4j2
+@Slf4j
 @Component
 @RequiredArgsConstructor
 public class DeserializationService {

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/serde/schemaregistry/SchemaRegistryAwareRecordSerDe.java
Patch:
@@ -37,12 +37,12 @@
 import java.util.concurrent.ConcurrentHashMap;
 import javax.annotation.Nullable;
 import lombok.SneakyThrows;
-import lombok.extern.log4j.Log4j2;
+import lombok.extern.slf4j.Slf4j;
 import org.apache.kafka.clients.consumer.ConsumerRecord;
 import org.apache.kafka.clients.producer.ProducerRecord;
 import org.apache.kafka.common.utils.Bytes;
 
-@Log4j2
+@Slf4j
 public class SchemaRegistryAwareRecordSerDe implements RecordSerDe {
 
   private static final int CLIENT_IDENTITY_MAP_CAPACITY = 100;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/AdminClientServiceImpl.java
Patch:
@@ -7,7 +7,7 @@
 import java.util.concurrent.ConcurrentHashMap;
 import lombok.RequiredArgsConstructor;
 import lombok.Setter;
-import lombok.extern.log4j.Log4j2;
+import lombok.extern.slf4j.Slf4j;
 import org.apache.kafka.clients.admin.AdminClient;
 import org.apache.kafka.clients.admin.AdminClientConfig;
 import org.springframework.beans.factory.annotation.Value;
@@ -16,7 +16,7 @@
 
 @Service
 @RequiredArgsConstructor
-@Log4j2
+@Slf4j
 public class AdminClientServiceImpl implements AdminClientService, Closeable {
   private final Map<String, ReactiveAdminClient> adminClientCache = new ConcurrentHashMap<>();
   @Setter // used in tests

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/ClusterService.java
Patch:
@@ -9,13 +9,13 @@
 import java.util.List;
 import java.util.stream.Collectors;
 import lombok.RequiredArgsConstructor;
-import lombok.extern.log4j.Log4j2;
+import lombok.extern.slf4j.Slf4j;
 import org.springframework.stereotype.Service;
 import reactor.core.publisher.Mono;
 
 @Service
 @RequiredArgsConstructor
-@Log4j2
+@Slf4j
 public class ClusterService {
 
   private final MetricsCache metricsCache;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/ClustersMetricsScheduler.java
Patch:
@@ -1,15 +1,15 @@
 package com.provectus.kafka.ui.service;
 
 import lombok.RequiredArgsConstructor;
-import lombok.extern.log4j.Log4j2;
+import lombok.extern.slf4j.Slf4j;
 import org.springframework.scheduling.annotation.Scheduled;
 import org.springframework.stereotype.Component;
 import reactor.core.publisher.Flux;
 import reactor.core.scheduler.Schedulers;
 
 @Component
 @RequiredArgsConstructor
-@Log4j2
+@Slf4j
 public class ClustersMetricsScheduler {
 
   private final ClustersStorage clustersStorage;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/FeatureService.java
Patch:
@@ -9,15 +9,15 @@
 import java.util.function.Predicate;
 import javax.annotation.Nullable;
 import lombok.RequiredArgsConstructor;
-import lombok.extern.log4j.Log4j2;
+import lombok.extern.slf4j.Slf4j;
 import org.apache.kafka.common.Node;
 import org.springframework.stereotype.Service;
 import reactor.core.publisher.Flux;
 import reactor.core.publisher.Mono;
 
 @Service
 @RequiredArgsConstructor
-@Log4j2
+@Slf4j
 public class FeatureService {
 
   private static final String DELETE_TOPIC_ENABLED_SERVER_PROPERTY = "delete.topic.enable";

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/KafkaConnectService.java
Patch:
@@ -32,7 +32,7 @@
 import java.util.stream.Stream;
 import lombok.RequiredArgsConstructor;
 import lombok.SneakyThrows;
-import lombok.extern.log4j.Log4j2;
+import lombok.extern.slf4j.Slf4j;
 import org.apache.commons.lang3.StringUtils;
 import org.springframework.stereotype.Service;
 import org.springframework.web.reactive.function.client.WebClientResponseException;
@@ -42,7 +42,7 @@
 import reactor.util.function.Tuples;
 
 @Service
-@Log4j2
+@Slf4j
 @RequiredArgsConstructor
 public class KafkaConnectService {
   private final ClusterMapper clusterMapper;
@@ -146,7 +146,7 @@ public Flux<String> getConnectors(KafkaCluster cluster, String connectName) {
     return getConnectAddress(cluster, connectName)
         .flatMapMany(connect ->
             KafkaConnectClients.withBaseUrl(connect).getConnectors(null)
-                .doOnError(log::error)
+                .doOnError(e -> log.error("Unexpected error upon getting connectors", e))
         );
   }
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/MessagesService.java
Patch:
@@ -23,7 +23,7 @@
 import java.util.stream.Collectors;
 import javax.annotation.Nullable;
 import lombok.RequiredArgsConstructor;
-import lombok.extern.log4j.Log4j2;
+import lombok.extern.slf4j.Slf4j;
 import org.apache.kafka.clients.admin.OffsetSpec;
 import org.apache.kafka.clients.producer.KafkaProducer;
 import org.apache.kafka.clients.producer.ProducerConfig;
@@ -43,7 +43,7 @@
 
 @Service
 @RequiredArgsConstructor
-@Log4j2
+@Slf4j
 public class MessagesService {
 
   private static final int MAX_LOAD_RECORD_LIMIT = 100;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/MetricsService.java
Patch:
@@ -8,15 +8,15 @@
 import java.util.List;
 import java.util.Map;
 import lombok.RequiredArgsConstructor;
-import lombok.extern.log4j.Log4j2;
+import lombok.extern.slf4j.Slf4j;
 import org.apache.kafka.clients.admin.ConfigEntry;
 import org.apache.kafka.clients.admin.TopicDescription;
 import org.springframework.stereotype.Service;
 import reactor.core.publisher.Mono;
 
 @Service
 @RequiredArgsConstructor
-@Log4j2
+@Slf4j
 public class MetricsService {
 
   private final ZookeeperService zookeeperService;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/OffsetsResetService.java
Patch:
@@ -16,7 +16,7 @@
 import java.util.Set;
 import javax.annotation.Nullable;
 import lombok.RequiredArgsConstructor;
-import lombok.extern.log4j.Log4j2;
+import lombok.extern.slf4j.Slf4j;
 import org.apache.kafka.clients.admin.OffsetSpec;
 import org.apache.kafka.common.TopicPartition;
 import org.springframework.stereotype.Component;
@@ -27,7 +27,7 @@
  * to works like "kafka-consumer-groups --reset-offsets" console command
  * (see kafka.admin.ConsumerGroupCommand)
  */
-@Log4j2
+@Slf4j
 @Component
 @RequiredArgsConstructor
 public class OffsetsResetService {

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/ReactiveAdminClient.java
Patch:
@@ -23,7 +23,7 @@
 import javax.annotation.Nullable;
 import lombok.RequiredArgsConstructor;
 import lombok.Value;
-import lombok.extern.log4j.Log4j2;
+import lombok.extern.slf4j.Slf4j;
 import org.apache.kafka.clients.admin.AdminClient;
 import org.apache.kafka.clients.admin.AlterConfigOp;
 import org.apache.kafka.clients.admin.Config;
@@ -55,7 +55,7 @@
 import reactor.util.function.Tuples;
 
 
-@Log4j2
+@Slf4j
 @RequiredArgsConstructor
 public class ReactiveAdminClient implements Closeable {
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/SchemaRegistryService.java
Patch:
@@ -25,7 +25,7 @@
 import java.util.Optional;
 import java.util.function.Function;
 import lombok.RequiredArgsConstructor;
-import lombok.extern.log4j.Log4j2;
+import lombok.extern.slf4j.Slf4j;
 import org.jetbrains.annotations.NotNull;
 import org.springframework.http.HttpHeaders;
 import org.springframework.http.HttpMethod;
@@ -39,7 +39,7 @@
 import reactor.core.publisher.Mono;
 
 @Service
-@Log4j2
+@Slf4j
 @RequiredArgsConstructor
 public class SchemaRegistryService {
   public static final String NO_SUCH_SCHEMA_VERSION = "No such schema %s with version %s";
@@ -68,7 +68,7 @@ public Mono<String[]> getAllSubjectNames(KafkaCluster cluster) {
         URL_SUBJECTS)
         .retrieve()
         .bodyToMono(String[].class)
-        .doOnError(log::error);
+        .doOnError(e -> log.error("Unexpected error", e));
   }
 
   public Flux<SchemaSubjectDTO> getAllVersionsBySubject(KafkaCluster cluster, String subject) {

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/ZookeeperService.java
Patch:
@@ -9,7 +9,7 @@
 import javax.annotation.Nullable;
 import lombok.RequiredArgsConstructor;
 import lombok.Value;
-import lombok.extern.log4j.Log4j2;
+import lombok.extern.slf4j.Slf4j;
 import org.apache.zookeeper.KeeperException;
 import org.apache.zookeeper.ZooKeeper;
 import org.springframework.stereotype.Service;
@@ -19,7 +19,7 @@
 
 @Service
 @RequiredArgsConstructor
-@Log4j2
+@Slf4j
 public class ZookeeperService {
 
   private final Map<String, ZooKeeper> cachedZkClient = new ConcurrentHashMap<>();

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/util/ClusterUtil.java
Patch:
@@ -21,7 +21,7 @@
 import java.util.Set;
 import java.util.stream.Collectors;
 import java.util.stream.Stream;
-import lombok.extern.log4j.Log4j2;
+import lombok.extern.slf4j.Slf4j;
 import org.apache.kafka.clients.admin.ConsumerGroupDescription;
 import org.apache.kafka.clients.consumer.ConsumerRecord;
 import org.apache.kafka.clients.consumer.OffsetAndMetadata;
@@ -31,7 +31,7 @@
 import org.apache.kafka.common.utils.Bytes;
 
 
-@Log4j2
+@Slf4j
 public class ClusterUtil {
 
   private static final ZoneId UTC_ZONE_ID = ZoneId.of("UTC");

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/util/JmxClusterUtil.java
Patch:
@@ -28,7 +28,7 @@
 import lombok.RequiredArgsConstructor;
 import lombok.SneakyThrows;
 import lombok.Value;
-import lombok.extern.log4j.Log4j2;
+import lombok.extern.slf4j.Slf4j;
 import org.apache.commons.pool2.KeyedObjectPool;
 import org.apache.kafka.common.Node;
 import org.jetbrains.annotations.Nullable;
@@ -39,7 +39,7 @@
 import reactor.util.function.Tuples;
 
 @Component
-@Log4j2
+@Slf4j
 @RequiredArgsConstructor
 public class JmxClusterUtil {
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/util/JmxPoolFactory.java
Patch:
@@ -8,13 +8,13 @@
 import javax.management.remote.JMXConnectorFactory;
 import javax.management.remote.JMXServiceURL;
 import javax.rmi.ssl.SslRMIClientSocketFactory;
-import lombok.extern.log4j.Log4j2;
+import lombok.extern.slf4j.Slf4j;
 import org.apache.commons.lang3.StringUtils;
 import org.apache.commons.pool2.BaseKeyedPooledObjectFactory;
 import org.apache.commons.pool2.PooledObject;
 import org.apache.commons.pool2.impl.DefaultPooledObject;
 
-@Log4j2
+@Slf4j
 public class JmxPoolFactory extends BaseKeyedPooledObjectFactory<JmxConnectionInfo, JMXConnector> {
 
   @Override

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/util/NumberUtil.java
Patch:
@@ -1,9 +1,9 @@
 package com.provectus.kafka.ui.util;
 
-import lombok.extern.log4j.Log4j2;
+import lombok.extern.slf4j.Slf4j;
 import org.apache.commons.lang3.math.NumberUtils;
 
-@Log4j2
+@Slf4j
 public class NumberUtil {
 
   private NumberUtil() {

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/util/OffsetsSeek.java
Patch:
@@ -6,15 +6,15 @@
 import java.util.List;
 import java.util.Map;
 import java.util.stream.Collectors;
-import lombok.extern.log4j.Log4j2;
+import lombok.extern.slf4j.Slf4j;
 import org.apache.kafka.clients.consumer.Consumer;
 import org.apache.kafka.clients.consumer.ConsumerRecord;
 import org.apache.kafka.common.TopicPartition;
 import org.apache.kafka.common.utils.Bytes;
 import reactor.util.function.Tuple2;
 import reactor.util.function.Tuples;
 
-@Log4j2
+@Slf4j
 public abstract class OffsetsSeek {
   protected final String topic;
   protected final ConsumerPosition consumerPosition;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/util/OffsetsSeekBackward.java
Patch:
@@ -8,14 +8,14 @@
 import java.util.Map;
 import java.util.Set;
 import java.util.stream.Collectors;
-import lombok.extern.log4j.Log4j2;
+import lombok.extern.slf4j.Slf4j;
 import org.apache.kafka.clients.consumer.Consumer;
 import org.apache.kafka.common.TopicPartition;
 import org.apache.kafka.common.utils.Bytes;
 import reactor.util.function.Tuple2;
 import reactor.util.function.Tuples;
 
-@Log4j2
+@Slf4j
 public class OffsetsSeekBackward extends OffsetsSeek {
 
   private final int maxMessages;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/util/OffsetsSeekForward.java
Patch:
@@ -6,12 +6,12 @@
 import java.util.Map;
 import java.util.Set;
 import java.util.stream.Collectors;
-import lombok.extern.log4j.Log4j2;
+import lombok.extern.slf4j.Slf4j;
 import org.apache.kafka.clients.consumer.Consumer;
 import org.apache.kafka.common.TopicPartition;
 import org.apache.kafka.common.utils.Bytes;
 
-@Log4j2
+@Slf4j
 public class OffsetsSeekForward extends OffsetsSeek {
 
   public OffsetsSeekForward(String topic, ConsumerPosition consumerPosition) {

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/KafkaConnectServiceTests.java
Patch:
@@ -16,7 +16,7 @@
 import java.util.List;
 import java.util.Map;
 import java.util.UUID;
-import lombok.extern.log4j.Log4j2;
+import lombok.extern.slf4j.Slf4j;
 import org.junit.jupiter.api.AfterEach;
 import org.junit.jupiter.api.BeforeEach;
 import org.junit.jupiter.api.Test;
@@ -27,7 +27,7 @@
 import org.springframework.test.web.reactive.server.WebTestClient;
 
 @ContextConfiguration(initializers = {AbstractBaseTest.Initializer.class})
-@Log4j2
+@Slf4j
 @AutoConfigureWebTestClient(timeout = "60000")
 public class KafkaConnectServiceTests extends AbstractBaseTest {
   private final String connectName = "kafka-connect";

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/KafkaConsumerGroupTests.java
Patch:
@@ -4,7 +4,7 @@
 import java.util.List;
 import java.util.Properties;
 import java.util.UUID;
-import lombok.extern.log4j.Log4j2;
+import lombok.extern.slf4j.Slf4j;
 import lombok.val;
 import org.apache.kafka.clients.admin.NewTopic;
 import org.apache.kafka.clients.consumer.ConsumerConfig;
@@ -18,7 +18,7 @@
 import org.springframework.test.web.reactive.server.WebTestClient;
 
 @ContextConfiguration(initializers = {AbstractBaseTest.Initializer.class})
-@Log4j2
+@Slf4j
 @AutoConfigureWebTestClient(timeout = "10000")
 public class KafkaConsumerGroupTests extends AbstractBaseTest {
   @Autowired

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/KafkaConsumerTests.java
Patch:
@@ -15,7 +15,7 @@
 import java.util.Map;
 import java.util.UUID;
 import java.util.stream.Stream;
-import lombok.extern.log4j.Log4j2;
+import lombok.extern.slf4j.Slf4j;
 import org.junit.jupiter.api.Assertions;
 import org.junit.jupiter.api.Test;
 import org.springframework.beans.factory.annotation.Autowired;
@@ -24,7 +24,7 @@
 import org.springframework.test.web.reactive.server.WebTestClient;
 
 @ContextConfiguration(initializers = {AbstractBaseTest.Initializer.class})
-@Log4j2
+@Slf4j
 @AutoConfigureWebTestClient(timeout = "60000")
 public class KafkaConsumerTests extends AbstractBaseTest {
 

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/KafkaTopicCreateTests.java
Patch:
@@ -2,7 +2,7 @@
 
 import com.provectus.kafka.ui.model.TopicCreationDTO;
 import java.util.UUID;
-import lombok.extern.log4j.Log4j2;
+import lombok.extern.slf4j.Slf4j;
 import org.junit.jupiter.api.BeforeEach;
 import org.junit.jupiter.api.Test;
 import org.springframework.beans.factory.annotation.Autowired;
@@ -11,7 +11,7 @@
 import org.springframework.test.web.reactive.server.WebTestClient;
 
 @ContextConfiguration(initializers = {AbstractBaseTest.Initializer.class})
-@Log4j2
+@Slf4j
 @AutoConfigureWebTestClient(timeout = "10000")
 public class KafkaTopicCreateTests extends AbstractBaseTest {
   @Autowired

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/ReadOnlyModeTests.java
Patch:
@@ -4,7 +4,7 @@
 import com.provectus.kafka.ui.model.TopicUpdateDTO;
 import java.util.Map;
 import java.util.UUID;
-import lombok.extern.log4j.Log4j2;
+import lombok.extern.slf4j.Slf4j;
 import org.junit.jupiter.api.Test;
 import org.springframework.beans.factory.annotation.Autowired;
 import org.springframework.boot.test.autoconfigure.web.reactive.AutoConfigureWebTestClient;
@@ -13,7 +13,7 @@
 import org.springframework.test.web.reactive.server.WebTestClient;
 
 @ContextConfiguration(initializers = {AbstractBaseTest.Initializer.class})
-@Log4j2
+@Slf4j
 @AutoConfigureWebTestClient(timeout = "60000")
 public class ReadOnlyModeTests extends AbstractBaseTest {
 

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/SchemaRegistryServiceTests.java
Patch:
@@ -6,7 +6,7 @@
 import com.provectus.kafka.ui.model.SchemaTypeDTO;
 import java.util.List;
 import java.util.UUID;
-import lombok.extern.log4j.Log4j2;
+import lombok.extern.slf4j.Slf4j;
 import lombok.val;
 import org.junit.jupiter.api.Assertions;
 import org.junit.jupiter.api.BeforeEach;
@@ -22,7 +22,7 @@
 import reactor.core.publisher.Mono;
 
 @ContextConfiguration(initializers = {AbstractBaseTest.Initializer.class})
-@Log4j2
+@Slf4j
 @AutoConfigureWebTestClient(timeout = "10000")
 class SchemaRegistryServiceTests extends AbstractBaseTest {
   @Autowired

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/service/RecordEmitterTest.java
Patch:
@@ -26,7 +26,7 @@
 import java.util.concurrent.ThreadLocalRandom;
 import java.util.stream.Collectors;
 import lombok.Value;
-import lombok.extern.log4j.Log4j2;
+import lombok.extern.slf4j.Slf4j;
 import org.apache.kafka.clients.admin.NewTopic;
 import org.apache.kafka.clients.consumer.ConsumerConfig;
 import org.apache.kafka.clients.consumer.KafkaConsumer;
@@ -41,7 +41,7 @@
 import org.springframework.test.context.ContextConfiguration;
 import reactor.core.publisher.Flux;
 
-@Log4j2
+@Slf4j
 @ContextConfiguration(initializers = {AbstractBaseTest.Initializer.class})
 class RecordEmitterTest extends AbstractBaseTest {
 

File: kafka-ui-e2e-checks/src/test/java/com/provectus/kafka/ui/base/BaseTest.java
Patch:
@@ -9,7 +9,7 @@
 import io.github.cdimascio.dotenv.Dotenv;
 import io.qameta.allure.selenide.AllureSelenide;
 import lombok.SneakyThrows;
-import lombok.extern.log4j.Log4j2;
+import lombok.extern.slf4j.Slf4j;
 import org.apache.commons.io.FileUtils;
 import org.junit.jupiter.api.AfterAll;
 import org.junit.jupiter.api.DisplayNameGeneration;
@@ -22,7 +22,7 @@
 import java.io.IOException;
 import java.util.Arrays;
 
-@Log4j2
+@Slf4j
 @DisplayNameGeneration(CamelCaseToSpacedDisplayNameGenerator.class)
 public class BaseTest {
 

File: kafka-ui-e2e-checks/src/test/java/com/provectus/kafka/ui/tests/TopicTests.java
Patch:
@@ -1,6 +1,5 @@
-package com.provectus.kafka.ui.topics;
+package com.provectus.kafka.ui.tests;
 
-import com.codeborne.selenide.Selenide;
 import com.provectus.kafka.ui.base.BaseTest;
 import com.provectus.kafka.ui.helpers.Helpers;
 import com.provectus.kafka.ui.pages.MainPage;
@@ -63,7 +62,8 @@ void updateTopic() {
                 .changeTimeToRetainValue(UPDATED_TIME_TO_RETAIN_VALUE)
                 .changeMaxSizeOnDisk(UPDATED_MAX_SIZE_ON_DISK)
                 .changeMaxMessageBytes(UPDATED_MAX_MESSAGE_BYTES)
-                .submitSettingChanges();
+                .submitSettingChanges()
+                .isOnTopicViewPage();
         pages.openTopicView(SECOND_LOCAL, TOPIC_TO_UPDATE)
                 .openEditSettings()
         // Assertions

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/AdminClientServiceImpl.java
Patch:
@@ -20,7 +20,7 @@
 public class AdminClientServiceImpl implements AdminClientService, Closeable {
   private final Map<String, ReactiveAdminClient> adminClientCache = new ConcurrentHashMap<>();
   @Setter // used in tests
-  @Value("${kafka.admin-client-timeout}")
+  @Value("${kafka.admin-client-timeout:30000}")
   private int clientTimeout;
 
   @Override

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/MetricsCache.java
Patch:
@@ -8,6 +8,7 @@
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
+import java.util.Optional;
 import java.util.Set;
 import java.util.concurrent.ConcurrentHashMap;
 import lombok.Builder;
@@ -88,7 +89,7 @@ public synchronized void onTopicDelete(KafkaCluster c, String topic) {
   }
 
   public Metrics get(KafkaCluster c) {
-    return cache.get(c.getName());
+    return Optional.ofNullable(cache.get(c.getName())).orElseGet(MetricsCache::empty);
   }
 
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/ClustersMetricsScheduler.java
Patch:
@@ -1,6 +1,5 @@
 package com.provectus.kafka.ui.service;
 
-import java.util.Map;
 import javax.annotation.PostConstruct;
 import lombok.RequiredArgsConstructor;
 import lombok.extern.log4j.Log4j2;
@@ -24,10 +23,9 @@ public class ClustersMetricsScheduler {
       initialDelayString = "${kafka.update-metrics-rate-millis:30000}"
   )
   public void updateMetrics() {
-    Flux.fromIterable(clustersStorage.getKafkaClustersMap().entrySet())
+    Flux.fromIterable(clustersStorage.getKafkaClusters())
         .parallel()
         .runOn(Schedulers.parallel())
-        .map(Map.Entry::getValue)
         .flatMap(cluster -> {
           log.debug("Start getting metrics for kafkaCluster: {}", cluster.getName());
           return metricsService.updateCache(cluster)

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/model/JmxBrokerMetrics.java
Patch:
@@ -6,6 +6,6 @@
 
 @Data
 @Builder(toBuilder = true)
-public class InternalBrokerMetrics {
+public class JmxBrokerMetrics {
   private final List<MetricDTO> metrics;
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/BrokerService.java
Patch:
@@ -70,7 +70,7 @@ public Mono<Map<String, InternalBrokerConfig>> getBrokerConfigMap(KafkaCluster c
   }
 
   private Flux<InternalBrokerConfig> getBrokersConfig(KafkaCluster cluster, Integer brokerId) {
-    if (!cluster.getBrokers().contains(brokerId)) {
+    if (!cluster.getMetrics().getBrokers().contains(brokerId)) {
       return Flux.error(
           new NotFoundException(String.format("Broker with id %s not found", brokerId)));
     }
@@ -139,7 +139,7 @@ private Mono<Map<Integer, Map<String, DescribeLogDirsResponse.LogDirInfo>>> getC
       KafkaCluster cluster, List<Integer> reqBrokers) {
     return adminClientService.get(cluster)
         .flatMap(admin -> {
-          List<Integer> brokers = new ArrayList<>(cluster.getBrokers());
+          List<Integer> brokers = new ArrayList<>(cluster.getMetrics().getBrokers());
           if (reqBrokers != null && !reqBrokers.isEmpty()) {
             brokers.retainAll(reqBrokers);
           }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/FeatureService.java
Patch:
@@ -22,7 +22,7 @@ public class FeatureService {
 
   private final BrokerService brokerService;
 
-  public Flux<Feature> getAvailableFeatures(KafkaCluster cluster) {
+  public Mono<List<Feature>> getAvailableFeatures(KafkaCluster cluster) {
     List<Mono<Feature>> features = new ArrayList<>();
 
     if (Optional.ofNullable(cluster.getKafkaConnect())
@@ -44,7 +44,7 @@ public Flux<Feature> getAvailableFeatures(KafkaCluster cluster) {
             .flatMap(r -> r ? Mono.just(Feature.TOPIC_DELETION) : Mono.empty())
     );
 
-    return Flux.fromIterable(features).flatMap(m -> m);
+    return Flux.fromIterable(features).flatMap(m -> m).collectList();
   }
 
   private Mono<Boolean> isTopicDeletionEnabled(KafkaCluster cluster) {

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/MessagesService.java
Patch:
@@ -58,7 +58,7 @@ public class MessagesService {
 
   public Mono<Void> deleteTopicMessages(KafkaCluster cluster, String topicName,
                                         List<Integer> partitionsToInclude) {
-    if (!cluster.getTopics().containsKey(topicName)) {
+    if (!cluster.getMetrics().getTopics().containsKey(topicName)) {
       throw new TopicNotFoundException();
     }
     return offsetsForDeletion(cluster, topicName, partitionsToInclude)
@@ -84,7 +84,8 @@ public Mono<RecordMetadata> sendMessage(KafkaCluster cluster, String topic,
       throw new ValidationException("Invalid message: both key and value can't be null");
     }
     if (msg.getPartition() != null
-        && msg.getPartition() > cluster.getTopics().get(topic).getPartitionCount() - 1) {
+        && msg.getPartition() > cluster.getMetrics().getTopics()
+          .get(topic).getPartitionCount() - 1) {
       throw new ValidationException("Invalid partition");
     }
     RecordSerDe serde =

File: kafka-ui-e2e-checks/src/test/java/com/provectus/kafka/ui/extensions/WaitUtils.java
Patch:
@@ -25,7 +25,7 @@ public static void waitForSelectedValue(SelenideElement element, String selected
             refresh();
             i++;
             sleep(2000);
-        } while (!selectedValue.equals(element.getSelectedValue()) && i != 40);
+        } while (!selectedValue.equals(element.getSelectedValue()) && i != 60);
         Assertions.assertEquals(selectedValue, element.getSelectedValue()) ;
     }
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/ReactiveAdminClient.java
Patch:
@@ -249,7 +249,7 @@ public Mono<Map<TopicPartition, Long>> listOffsets(String topic,
     return topicPartitions(topic).flatMap(tps -> listOffsets(tps, offsetSpec));
   }
 
-  public Mono<Map<TopicPartition, Long>> listOffsets(Set<TopicPartition> partitions,
+  public Mono<Map<TopicPartition, Long>> listOffsets(Collection<TopicPartition> partitions,
                                                      OffsetSpec offsetSpec) {
     return toMono(
         client.listOffsets(partitions.stream().collect(toMap(tp -> tp, tp -> offsetSpec))).all())

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/client/KsqlClient.java
Patch:
@@ -3,7 +3,7 @@
 import com.fasterxml.jackson.databind.JsonNode;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.provectus.kafka.ui.exception.UnprocessableEntityException;
-import com.provectus.kafka.ui.model.KsqlCommandResponse;
+import com.provectus.kafka.ui.model.KsqlCommandResponseDTO;
 import com.provectus.kafka.ui.strategy.ksql.statement.BaseStrategy;
 import lombok.RequiredArgsConstructor;
 import lombok.SneakyThrows;
@@ -23,7 +23,7 @@ public class KsqlClient {
   private final WebClient webClient;
   private final ObjectMapper mapper;
 
-  public Mono<KsqlCommandResponse> execute(BaseStrategy ksqlStatement) {
+  public Mono<KsqlCommandResponseDTO> execute(BaseStrategy ksqlStatement) {
     return webClient.post()
         .uri(ksqlStatement.getUri())
         .accept(new MediaType("application", "vnd.ksql.v1+json"))

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/emitter/BackwardRecordEmitter.java
Patch:
@@ -1,6 +1,6 @@
 package com.provectus.kafka.ui.emitter;
 
-import com.provectus.kafka.ui.model.TopicMessageEvent;
+import com.provectus.kafka.ui.model.TopicMessageEventDTO;
 import com.provectus.kafka.ui.serde.RecordSerDe;
 import com.provectus.kafka.ui.util.OffsetsSeekBackward;
 import java.util.Collections;
@@ -23,7 +23,7 @@
 @Log4j2
 public class BackwardRecordEmitter
     extends AbstractEmitter
-    implements java.util.function.Consumer<FluxSink<TopicMessageEvent>> {
+    implements java.util.function.Consumer<FluxSink<TopicMessageEventDTO>> {
 
   private final Function<Map<String, Object>, KafkaConsumer<Bytes, Bytes>> consumerSupplier;
   private final OffsetsSeekBackward offsetsSeek;
@@ -38,7 +38,7 @@ public BackwardRecordEmitter(
   }
 
   @Override
-  public void accept(FluxSink<TopicMessageEvent> sink) {
+  public void accept(FluxSink<TopicMessageEventDTO> sink) {
     try (KafkaConsumer<Bytes, Bytes> configConsumer = consumerSupplier.apply(Map.of())) {
       final List<TopicPartition> requestedPartitions =
           offsetsSeek.getRequestedPartitions(configConsumer);

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/emitter/ForwardRecordEmitter.java
Patch:
@@ -1,6 +1,6 @@
 package com.provectus.kafka.ui.emitter;
 
-import com.provectus.kafka.ui.model.TopicMessageEvent;
+import com.provectus.kafka.ui.model.TopicMessageEventDTO;
 import com.provectus.kafka.ui.serde.RecordSerDe;
 import com.provectus.kafka.ui.util.OffsetsSeek;
 import java.time.Duration;
@@ -16,7 +16,7 @@
 @Log4j2
 public class ForwardRecordEmitter
     extends AbstractEmitter
-    implements java.util.function.Consumer<FluxSink<TopicMessageEvent>> {
+    implements java.util.function.Consumer<FluxSink<TopicMessageEventDTO>> {
 
   private static final Duration POLL_TIMEOUT_MS = Duration.ofMillis(1000L);
 
@@ -33,7 +33,7 @@ public ForwardRecordEmitter(
   }
 
   @Override
-  public void accept(FluxSink<TopicMessageEvent> sink) {
+  public void accept(FluxSink<TopicMessageEventDTO> sink) {
     try (KafkaConsumer<Bytes, Bytes> consumer = consumerSupplier.get()) {
       sendPhase(sink, "Assigning partitions");
       var waitingOffsets = offsetsSeek.assignAndSeek(consumer);

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/model/ConsumerPosition.java
Patch:
@@ -6,7 +6,7 @@
 
 @Value
 public class ConsumerPosition {
-  SeekType seekType;
+  SeekTypeDTO seekType;
   Map<TopicPartition, Long> seekTo;
-  SeekDirection seekDirection;
+  SeekDirectionDTO seekDirection;
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/model/InternalBrokerMetrics.java
Patch:
@@ -7,5 +7,5 @@
 @Data
 @Builder(toBuilder = true)
 public class InternalBrokerMetrics {
-  private final List<Metric> metrics;
+  private final List<MetricDTO> metrics;
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/model/InternalClusterMetrics.java
Patch:
@@ -25,7 +25,7 @@ public class InternalClusterMetrics {
   private final long segmentSize;
   private final Map<Integer, InternalBrokerDiskUsage> internalBrokerDiskUsage;
   private final Map<Integer, InternalBrokerMetrics> internalBrokerMetrics;
-  private final List<Metric> metrics;
+  private final List<MetricDTO> metrics;
   private final int zooKeeperStatus;
   private final String version;
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/model/KafkaCluster.java
Patch:
@@ -26,8 +26,8 @@ public class KafkaCluster {
   private final List<KafkaConnectCluster> kafkaConnect;
   private final String schemaNameTemplate;
   private final String keySchemaNameTemplate;
-  private final ServerStatus status;
-  private final ServerStatus zookeeperStatus;
+  private final ServerStatusDTO status;
+  private final ServerStatusDTO zookeeperStatus;
   private final InternalClusterMetrics metrics;
   private final Map<String, InternalTopic> topics;
   private final List<Integer> brokers;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/serde/RecordSerDe.java
Patch:
@@ -1,6 +1,6 @@
 package com.provectus.kafka.ui.serde;
 
-import com.provectus.kafka.ui.model.TopicMessageSchema;
+import com.provectus.kafka.ui.model.TopicMessageSchemaDTO;
 import com.provectus.kafka.ui.serde.schemaregistry.MessageFormat;
 import javax.annotation.Nullable;
 import lombok.Builder;
@@ -29,5 +29,5 @@ ProducerRecord<byte[], byte[]> serialize(String topic,
                                            @Nullable String data,
                                            @Nullable Integer partition);
 
-  TopicMessageSchema getTopicSchema(String topic);
+  TopicMessageSchemaDTO getTopicSchema(String topic);
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/BrokerService.java
Patch:
@@ -1,6 +1,6 @@
 package com.provectus.kafka.ui.service;
 
-import com.provectus.kafka.ui.model.Broker;
+import com.provectus.kafka.ui.model.BrokerDTO;
 import com.provectus.kafka.ui.model.InternalBrokerConfig;
 import com.provectus.kafka.ui.model.KafkaCluster;
 import java.util.Map;
@@ -34,7 +34,7 @@ Mono<Map<String, InternalBrokerConfig>> getBrokerConfigMap(KafkaCluster cluster,
    * @param cluster - cluster
    * @return Flux of Broker
    */
-  Flux<Broker> getBrokers(KafkaCluster cluster);
+  Flux<BrokerDTO> getBrokers(KafkaCluster cluster);
 
   /**
    * Get cluster controller node.

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/BrokerServiceImpl.java
Patch:
@@ -2,7 +2,7 @@
 
 import com.provectus.kafka.ui.exception.IllegalEntityStateException;
 import com.provectus.kafka.ui.exception.NotFoundException;
-import com.provectus.kafka.ui.model.Broker;
+import com.provectus.kafka.ui.model.BrokerDTO;
 import com.provectus.kafka.ui.model.ExtendedAdminClient;
 import com.provectus.kafka.ui.model.InternalBrokerConfig;
 import com.provectus.kafka.ui.model.KafkaCluster;
@@ -85,12 +85,12 @@ public Flux<InternalBrokerConfig> getBrokersConfig(KafkaCluster cluster, Integer
   }
 
   @Override
-  public Flux<Broker> getBrokers(KafkaCluster cluster) {
+  public Flux<BrokerDTO> getBrokers(KafkaCluster cluster) {
     return adminClientService
         .getOrCreateAdminClient(cluster)
         .flatMap(client -> ClusterUtil.toMono(client.getAdminClient().describeCluster().nodes())
             .map(n -> n.stream().map(node -> {
-              Broker broker = new Broker();
+              BrokerDTO broker = new BrokerDTO();
               broker.setId(node.id());
               broker.setHost(node.host());
               return broker;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/strategy/ksql/statement/CreateStrategy.java
Patch:
@@ -1,15 +1,15 @@
 package com.provectus.kafka.ui.strategy.ksql.statement;
 
 import com.fasterxml.jackson.databind.JsonNode;
-import com.provectus.kafka.ui.model.KsqlCommandResponse;
+import com.provectus.kafka.ui.model.KsqlCommandResponseDTO;
 import org.springframework.stereotype.Component;
 
 @Component
 public class CreateStrategy extends BaseStrategy {
   private static final String RESPONSE_VALUE_KEY = "commandStatus";
 
   @Override
-  public KsqlCommandResponse serializeResponse(JsonNode response) {
+  public KsqlCommandResponseDTO serializeResponse(JsonNode response) {
     return serializeMessageResponse(response, RESPONSE_VALUE_KEY);
   }
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/strategy/ksql/statement/DescribeStrategy.java
Patch:
@@ -1,15 +1,15 @@
 package com.provectus.kafka.ui.strategy.ksql.statement;
 
 import com.fasterxml.jackson.databind.JsonNode;
-import com.provectus.kafka.ui.model.KsqlCommandResponse;
+import com.provectus.kafka.ui.model.KsqlCommandResponseDTO;
 import org.springframework.stereotype.Component;
 
 @Component
 public class DescribeStrategy extends BaseStrategy {
   private static final String RESPONSE_VALUE_KEY = "sourceDescription";
 
   @Override
-  public KsqlCommandResponse serializeResponse(JsonNode response) {
+  public KsqlCommandResponseDTO serializeResponse(JsonNode response) {
     return serializeTableResponse(response, RESPONSE_VALUE_KEY);
   }
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/strategy/ksql/statement/DropStrategy.java
Patch:
@@ -1,15 +1,15 @@
 package com.provectus.kafka.ui.strategy.ksql.statement;
 
 import com.fasterxml.jackson.databind.JsonNode;
-import com.provectus.kafka.ui.model.KsqlCommandResponse;
+import com.provectus.kafka.ui.model.KsqlCommandResponseDTO;
 import org.springframework.stereotype.Component;
 
 @Component
 public class DropStrategy extends BaseStrategy {
   private static final String RESPONSE_VALUE_KEY = "commandStatus";
 
   @Override
-  public KsqlCommandResponse serializeResponse(JsonNode response) {
+  public KsqlCommandResponseDTO serializeResponse(JsonNode response) {
     return serializeMessageResponse(response, RESPONSE_VALUE_KEY);
   }
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/strategy/ksql/statement/ExplainStrategy.java
Patch:
@@ -1,15 +1,15 @@
 package com.provectus.kafka.ui.strategy.ksql.statement;
 
 import com.fasterxml.jackson.databind.JsonNode;
-import com.provectus.kafka.ui.model.KsqlCommandResponse;
+import com.provectus.kafka.ui.model.KsqlCommandResponseDTO;
 import org.springframework.stereotype.Component;
 
 @Component
 public class ExplainStrategy extends BaseStrategy {
   private static final String RESPONSE_VALUE_KEY = "queryDescription";
 
   @Override
-  public KsqlCommandResponse serializeResponse(JsonNode response) {
+  public KsqlCommandResponseDTO serializeResponse(JsonNode response) {
     return serializeTableResponse(response, RESPONSE_VALUE_KEY);
   }
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/strategy/ksql/statement/SelectStrategy.java
Patch:
@@ -1,14 +1,14 @@
 package com.provectus.kafka.ui.strategy.ksql.statement;
 
 import com.fasterxml.jackson.databind.JsonNode;
-import com.provectus.kafka.ui.model.KsqlCommandResponse;
+import com.provectus.kafka.ui.model.KsqlCommandResponseDTO;
 import org.springframework.stereotype.Component;
 
 @Component
 public class SelectStrategy extends BaseStrategy {
 
   @Override
-  public KsqlCommandResponse serializeResponse(JsonNode response) {
+  public KsqlCommandResponseDTO serializeResponse(JsonNode response) {
     return serializeQueryResponse(response);
   }
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/strategy/ksql/statement/TerminateStrategy.java
Patch:
@@ -1,15 +1,15 @@
 package com.provectus.kafka.ui.strategy.ksql.statement;
 
 import com.fasterxml.jackson.databind.JsonNode;
-import com.provectus.kafka.ui.model.KsqlCommandResponse;
+import com.provectus.kafka.ui.model.KsqlCommandResponseDTO;
 import org.springframework.stereotype.Component;
 
 @Component
 public class TerminateStrategy extends BaseStrategy {
   private static final String RESPONSE_VALUE_KEY = "commandStatus";
 
   @Override
-  public KsqlCommandResponse serializeResponse(JsonNode response) {
+  public KsqlCommandResponseDTO serializeResponse(JsonNode response) {
     return serializeMessageResponse(response, RESPONSE_VALUE_KEY);
   }
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/util/OffsetsSeek.java
Patch:
@@ -1,7 +1,7 @@
 package com.provectus.kafka.ui.util;
 
 import com.provectus.kafka.ui.model.ConsumerPosition;
-import com.provectus.kafka.ui.model.SeekType;
+import com.provectus.kafka.ui.model.SeekTypeDTO;
 import java.util.Collection;
 import java.util.List;
 import java.util.Map;
@@ -29,7 +29,7 @@ public ConsumerPosition getConsumerPosition() {
   }
 
   public Map<TopicPartition, Long> getPartitionsOffsets(Consumer<Bytes, Bytes> consumer) {
-    SeekType seekType = consumerPosition.getSeekType();
+    SeekTypeDTO seekType = consumerPosition.getSeekType();
     List<TopicPartition> partitions = getRequestedPartitions(consumer);
     log.info("Positioning consumer for topic {} with {}", topic, consumerPosition);
     Map<TopicPartition, Long> offsets;

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/strategy/ksql/statement/CreateStrategyTest.java
Patch:
@@ -8,7 +8,7 @@
 import com.fasterxml.jackson.databind.JsonNode;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.provectus.kafka.ui.exception.UnprocessableEntityException;
-import com.provectus.kafka.ui.model.KsqlCommandResponse;
+import com.provectus.kafka.ui.model.KsqlCommandResponseDTO;
 import lombok.SneakyThrows;
 import org.junit.jupiter.api.BeforeEach;
 import org.junit.jupiter.api.Test;
@@ -59,7 +59,7 @@ void shouldReturnFalseInTest() {
   void shouldSerializeResponse() {
     String message = "updated successful";
     JsonNode node = getResponseWithMessage(message);
-    KsqlCommandResponse serializedResponse = strategy.serializeResponse(node);
+    KsqlCommandResponseDTO serializedResponse = strategy.serializeResponse(node);
     assertThat(serializedResponse.getMessage()).isEqualTo(message);
 
   }

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/strategy/ksql/statement/DropStrategyTest.java
Patch:
@@ -8,7 +8,7 @@
 import com.fasterxml.jackson.databind.JsonNode;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.provectus.kafka.ui.exception.UnprocessableEntityException;
-import com.provectus.kafka.ui.model.KsqlCommandResponse;
+import com.provectus.kafka.ui.model.KsqlCommandResponseDTO;
 import lombok.SneakyThrows;
 import org.junit.jupiter.api.BeforeEach;
 import org.junit.jupiter.api.Test;
@@ -49,7 +49,7 @@ void shouldReturnFalseInTest() {
   void shouldSerializeResponse() {
     String message = "updated successful";
     JsonNode node = getResponseWithMessage(message);
-    KsqlCommandResponse serializedResponse = strategy.serializeResponse(node);
+    KsqlCommandResponseDTO serializedResponse = strategy.serializeResponse(node);
     assertThat(serializedResponse.getMessage()).isEqualTo(message);
 
   }

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/strategy/ksql/statement/TerminateStrategyTest.java
Patch:
@@ -8,7 +8,7 @@
 import com.fasterxml.jackson.databind.JsonNode;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.provectus.kafka.ui.exception.UnprocessableEntityException;
-import com.provectus.kafka.ui.model.KsqlCommandResponse;
+import com.provectus.kafka.ui.model.KsqlCommandResponseDTO;
 import lombok.SneakyThrows;
 import org.junit.jupiter.api.BeforeEach;
 import org.junit.jupiter.api.Test;
@@ -46,7 +46,7 @@ void shouldReturnFalseInTest() {
   void shouldSerializeResponse() {
     String message = "query terminated.";
     JsonNode node = getResponseWithMessage(message);
-    KsqlCommandResponse serializedResponse = strategy.serializeResponse(node);
+    KsqlCommandResponseDTO serializedResponse = strategy.serializeResponse(node);
     assertThat(serializedResponse.getMessage()).isEqualTo(message);
 
   }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/util/ClusterUtil.java
Patch:
@@ -32,7 +32,7 @@
 import java.util.Set;
 import java.util.stream.Collectors;
 import java.util.stream.Stream;
-import lombok.extern.slf4j.Slf4j;
+import lombok.extern.log4j.Log4j2;
 import org.apache.kafka.clients.admin.AdminClient;
 import org.apache.kafka.clients.admin.Config;
 import org.apache.kafka.clients.admin.ConfigEntry;
@@ -49,7 +49,7 @@
 import reactor.core.publisher.Mono;
 import reactor.util.function.Tuple2;
 
-@Slf4j
+@Log4j2
 public class ClusterUtil {
 
   private static final String CLUSTER_VERSION_PARAM_KEY = "inter.broker.protocol.version";

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/util/JmxClusterUtil.java
Patch:
@@ -19,13 +19,13 @@
 import javax.management.remote.JMXConnector;
 import lombok.RequiredArgsConstructor;
 import lombok.SneakyThrows;
-import lombok.extern.slf4j.Slf4j;
+import lombok.extern.log4j.Log4j2;
 import org.apache.commons.pool2.KeyedObjectPool;
 import org.jetbrains.annotations.Nullable;
 import org.springframework.stereotype.Component;
 
 @Component
-@Slf4j
+@Log4j2
 @RequiredArgsConstructor
 public class JmxClusterUtil {
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/util/JmxPoolFactory.java
Patch:
@@ -8,13 +8,13 @@
 import javax.management.remote.JMXConnectorFactory;
 import javax.management.remote.JMXServiceURL;
 import javax.rmi.ssl.SslRMIClientSocketFactory;
-import lombok.extern.slf4j.Slf4j;
+import lombok.extern.log4j.Log4j2;
 import org.apache.commons.lang3.StringUtils;
 import org.apache.commons.pool2.BaseKeyedPooledObjectFactory;
 import org.apache.commons.pool2.PooledObject;
 import org.apache.commons.pool2.impl.DefaultPooledObject;
 
-@Slf4j
+@Log4j2
 public class JmxPoolFactory extends BaseKeyedPooledObjectFactory<JmxConnectionInfo, JMXConnector> {
 
   @Override

File: kafka-ui-e2e-checks/src/test/java/com/provectus/kafka/ui/base/BaseTest.java
Patch:
@@ -9,7 +9,7 @@
 import io.github.cdimascio.dotenv.Dotenv;
 import io.qameta.allure.selenide.AllureSelenide;
 import lombok.SneakyThrows;
-import lombok.extern.slf4j.Slf4j;
+import lombok.extern.log4j.Log4j2;
 import org.apache.commons.io.FileUtils;
 import org.junit.jupiter.api.AfterAll;
 import org.junit.jupiter.api.DisplayNameGeneration;
@@ -22,7 +22,7 @@
 import java.io.IOException;
 import java.util.Arrays;
 
-@Slf4j
+@Log4j2
 @DisplayNameGeneration(CamelCaseToSpacedDisplayNameGenerator.class)
 public class BaseTest {
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/config/ReadOnlyModeFilter.java
Patch:
@@ -31,7 +31,7 @@ public Mono<Void> filter(ServerWebExchange exchange, @NotNull WebFilterChain cha
       return chain.filter(exchange);
     }
 
-    var path = exchange.getRequest().getURI().getPath();
+    var path = exchange.getRequest().getPath().pathWithinApplication().value();
     var matcher = CLUSTER_NAME_REGEX.matcher(path);
     if (!matcher.find()) {
       return chain.filter(exchange);

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/serde/schemaregistry/SchemaRegistryAwareRecordSerDe.java
Patch:
@@ -90,7 +90,7 @@ private static SchemaRegistryClient createSchemaRegistryClient(KafkaCluster clus
           "You specified password but do not specified username");
     }
     return new CachedSchemaRegistryClient(
-        Collections.singletonList(cluster.getSchemaRegistry().getUrl()),
+        cluster.getSchemaRegistry().getUrl(),
         CLIENT_IDENTITY_MAP_CAPACITY,
         schemaProviders,
         configs
@@ -218,7 +218,7 @@ public TopicMessageSchema getTopicSchema(String topic) {
   private String convertSchema(SchemaMetadata schema) {
 
     String jsonSchema;
-    URI basePath = new URI(cluster.getSchemaRegistry().getUrl())
+    URI basePath = new URI(cluster.getSchemaRegistry().getFirstUrl())
         .resolve(Integer.toString(schema.getId()));
     final ParsedSchema schemaById = Objects.requireNonNull(schemaRegistryClient)
         .getSchemaById(schema.getId());

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/SchemaRegistryService.java
Patch:
@@ -343,8 +343,9 @@ private WebClient.RequestBodySpec configuredWebClient(KafkaCluster cluster, Http
   private WebClient.RequestBodySpec configuredWebClient(InternalSchemaRegistry schemaRegistry,
                                                         HttpMethod method, String uri,
                                                         Object... params) {
-    return webClient.method(method)
-        .uri(schemaRegistry.getUrl() + uri, params)
+    return webClient
+        .method(method)
+        .uri(schemaRegistry.getFirstUrl() + uri, params)
         .headers(headers -> setBasicAuthIfEnabled(schemaRegistry, headers));
   }
 }

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/util/jsonschema/AvroJsonSchemaConverterTest.java
Patch:
@@ -87,7 +87,7 @@ public void avroConvertTest() throws URISyntaxException, JsonProcessingException
         + "{\"$ref\":\"#/definitions/RecordInnerMessage\"}},"
         + "\"required\":[\"record\"],\"definitions\":"
         + "{\"RecordInnerMessage\":{\"type\":\"object\",\""
-        + "properties\":{\"long_text\":{\"type\":\"object\","
+        + "properties\":{\"long_text\":{\"type\":[\"object\", \"null\"],"
         + "\"properties\":{\"string\":{\"type\":\"string\"}}},"
         + "\"array\":{\"type\":\"array\",\"items\":{\"type\":\"string\"}},"
         + "\"id\":{\"type\":\"integer\"},\"text\":{\"type\":\"string\"},"
@@ -151,8 +151,8 @@ public void testNullableUnions() throws URISyntaxException, IOException, Process
         "{\"$id\":\"http://example.com/Message\","
         + "\"$schema\":\"https://json-schema.org/draft/2020-12/schema\","
         + "\"type\":\"object\",\"properties\":{\"text\":"
-        + "{\"type\":\"object\",\"properties\":{\"string\":"
-        + "{\"type\":\"string\"}}},\"value\":{\"type\":\"object\","
+        + "{\"type\":[\"object\", \"null\"],\"properties\":{\"string\":"
+        + "{\"type\":\"string\"}}},\"value\":{\"type\":[\"object\", \"null\"],"
         + "\"properties\":{\"string\":{\"type\":\"string\"},"
         + "\"long\":{\"type\":\"integer\"}}}}}";
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/config/ClustersProperties.java
Patch:
@@ -2,6 +2,7 @@
 
 import java.util.ArrayList;
 import java.util.List;
+import java.util.Map;
 import java.util.Properties;
 import lombok.Data;
 import org.springframework.boot.context.properties.ConfigurationProperties;
@@ -26,6 +27,7 @@ public static class Cluster {
     String keySchemaNameTemplate = "%s-key";
     String protobufFile;
     String protobufMessageName;
+    Map<String, String> protobufMessageNameByTopic;
     List<ConnectCluster> kafkaConnect;
     int jmxPort;
     boolean jmxSsl;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/model/KafkaCluster.java
Patch:
@@ -35,6 +35,7 @@ public class KafkaCluster {
   private final Throwable lastZookeeperException;
   private final Path protobufFile;
   private final String protobufMessageName;
+  private final Map<String, String> protobufMessageNameByTopic;
   private final Properties properties;
   private final Boolean readOnly;
   private final Boolean disableLogDirsCollection;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/serde/DeserializationService.java
Patch:
@@ -35,7 +35,8 @@ private RecordSerDe createRecordDeserializerForCluster(KafkaCluster cluster) {
       if (cluster.getProtobufFile() != null) {
         log.info("Using ProtobufFileRecordSerDe for cluster '{}'", cluster.getName());
         return new ProtobufFileRecordSerDe(cluster.getProtobufFile(),
-            cluster.getProtobufMessageName(), objectMapper);
+            cluster.getProtobufMessageNameByTopic(), cluster.getProtobufMessageName(),
+            objectMapper);
       } else {
         log.info("Using SchemaRegistryAwareRecordSerDe for cluster '{}'", cluster.getName());
         return new SchemaRegistryAwareRecordSerDe(cluster);

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/KafkaService.java
Patch:
@@ -31,6 +31,7 @@
 import com.provectus.kafka.ui.util.JmxClusterUtil;
 import com.provectus.kafka.ui.util.JmxMetricsName;
 import com.provectus.kafka.ui.util.JmxMetricsValueName;
+import com.provectus.kafka.ui.util.MapUtil;
 import java.math.BigDecimal;
 import java.util.ArrayList;
 import java.util.Collection;
@@ -396,7 +397,7 @@ public Mono<Map<TopicPartition, OffsetAndMetadata>> groupMetadata(KafkaCluster c
         ac.getAdminClient()
             .listConsumerGroupOffsets(consumerGroupId)
             .partitionsToOffsetAndMetadata()
-    ).flatMap(ClusterUtil::toMono);
+    ).flatMap(ClusterUtil::toMono).map(MapUtil::removeNullValues);
   }
 
   public Map<TopicPartition, Long> topicPartitionsEndOffsets(

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/ClustersStorage.java
Patch:
@@ -22,7 +22,6 @@ public class ClustersStorage {
   private final ClustersProperties clusterProperties;
 
   private final ClusterMapper clusterMapper = Mappers.getMapper(ClusterMapper.class);
-  private final FeatureService featureService;
 
   @PostConstruct
   public void init() {
@@ -36,7 +35,6 @@ public void init() {
           clusterProperties.getName(),
           cluster.toBuilder()
               .topics(new HashMap<>())
-              .features(featureService.getAvailableFeatures(cluster))
               .build()
       );
     }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/FeatureService.java
Patch:
@@ -2,7 +2,7 @@
 
 import com.provectus.kafka.ui.model.Feature;
 import com.provectus.kafka.ui.model.KafkaCluster;
-import java.util.List;
+import reactor.core.publisher.Flux;
 
 public interface FeatureService {
   /**
@@ -11,5 +11,5 @@ public interface FeatureService {
    * @param cluster - cluster
    * @return List of Feature
    */
-  List<Feature> getAvailableFeatures(KafkaCluster cluster);
+  Flux<Feature> getAvailableFeatures(KafkaCluster cluster);
 }

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/service/OffsetsResetServiceTest.java
Patch:
@@ -52,8 +52,10 @@ public class OffsetsResetServiceTest extends AbstractBaseTest {
   @BeforeEach
   void init() {
     AdminClientServiceImpl adminClientService = new AdminClientServiceImpl();
+    BrokerService brokerService = new BrokerServiceImpl(adminClientService);
+    FeatureService featureService = new FeatureServiceImpl(brokerService);
     adminClientService.setClientTimeout(5_000);
-    kafkaService = new KafkaService(null, null, null, null, adminClientService);
+    kafkaService = new KafkaService(null, null, null, null, adminClientService, featureService);
     offsetsResetService = new OffsetsResetService(kafkaService);
 
     createTopic(new NewTopic(topic, PARTITIONS, (short) 1));

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/model/CleanupPolicy.java
Patch:
@@ -6,7 +6,7 @@
 public enum CleanupPolicy {
   DELETE("delete"),
   COMPACT("compact"),
-  COMPACT_DELETE("compact, delete"),
+  COMPACT_DELETE("compact,delete"),
   UNKNOWN("unknown");
 
   private final String cleanUpPolicy;
@@ -21,7 +21,7 @@ public String getCleanUpPolicy() {
 
   public static CleanupPolicy fromString(String string) {
     return Arrays.stream(CleanupPolicy.values())
-        .filter(v -> v.cleanUpPolicy.equals(string))
+        .filter(v -> v.cleanUpPolicy.equals(string.replace(" ", "")))
         .findFirst()
         .orElseThrow(() ->
             new IllegalEntityStateException("Unknown cleanup policy value: " + string));

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/config/ClustersProperties.java
Patch:
@@ -30,6 +30,7 @@ public static class Cluster {
     int jmxPort;
     Properties properties;
     boolean readOnly = false;
+    boolean disableLogDirsCollection = false;
   }
 
   @Data

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/model/KafkaCluster.java
Patch:
@@ -31,5 +31,6 @@ public class KafkaCluster {
   private final String protobufMessageName;
   private final Properties properties;
   private final Boolean readOnly;
+  private final Boolean disableLogDirsCollection;
   private final List<Feature> features;
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/ClustersStorage.java
Patch:
@@ -2,7 +2,6 @@
 
 import com.provectus.kafka.ui.config.ClustersProperties;
 import com.provectus.kafka.ui.mapper.ClusterMapper;
-import com.provectus.kafka.ui.model.Feature;
 import com.provectus.kafka.ui.model.KafkaCluster;
 import java.util.Collection;
 import java.util.HashMap;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/ClustersStorage.java
Patch:
@@ -23,6 +23,7 @@ public class ClustersStorage {
   private final ClustersProperties clusterProperties;
 
   private final ClusterMapper clusterMapper = Mappers.getMapper(ClusterMapper.class);
+  private final FeatureService featureService;
 
   @PostConstruct
   public void init() {
@@ -36,7 +37,7 @@ public void init() {
           clusterProperties.getName(),
           cluster.toBuilder()
               .topics(new HashMap<>())
-              .features(Feature.getEnabledFeatures(cluster))
+              .features(featureService.getAvailableFeatures(cluster))
               .build()
       );
     }

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/service/OffsetsResetServiceTest.java
Patch:
@@ -51,8 +51,9 @@ public class OffsetsResetServiceTest extends AbstractBaseTest {
 
   @BeforeEach
   void init() {
-    kafkaService = new KafkaService(null, null, null, null);
-    kafkaService.setClientTimeout(5_000);
+    AdminClientServiceImpl adminClientService = new AdminClientServiceImpl();
+    adminClientService.setClientTimeout(5_000);
+    kafkaService = new KafkaService(null, null, null, null, adminClientService);
     offsetsResetService = new OffsetsResetService(kafkaService);
 
     createTopic(new NewTopic(topic, PARTITIONS, (short) 1));

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/util/ClusterUtil.java
Patch:
@@ -365,7 +365,7 @@ public static String getClusterVersion(Map<ConfigResource, Config> configs) {
         .map(Config::entries)
         .flatMap(Collection::stream)
         .filter(entry -> entry.name().contains(CLUSTER_VERSION_PARAM_KEY))
-        .findFirst().orElseThrow().value();
+        .findFirst().map(ConfigEntry::value).orElse("1.0-UNKNOWN");
   }
 
 

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/KafkaConnectServiceTests.java
Patch:
@@ -8,8 +8,8 @@
 import com.provectus.kafka.ui.model.ConnectorPluginConfig;
 import com.provectus.kafka.ui.model.ConnectorPluginConfigValidationResponse;
 import com.provectus.kafka.ui.model.ConnectorPluginConfigValue;
+import com.provectus.kafka.ui.model.ConnectorState;
 import com.provectus.kafka.ui.model.ConnectorStatus;
-import com.provectus.kafka.ui.model.ConnectorTaskStatus;
 import com.provectus.kafka.ui.model.ConnectorType;
 import com.provectus.kafka.ui.model.NewConnector;
 import com.provectus.kafka.ui.model.TaskId;
@@ -171,7 +171,7 @@ public void shouldRetrieveConnector() {
     Connector expected = (Connector) new Connector()
         .connect(connectName)
         .status(new ConnectorStatus()
-            .state(ConnectorTaskStatus.RUNNING)
+            .state(ConnectorState.RUNNING)
             .workerId("kafka-connect:8083"))
         .tasks(List.of(new TaskId()
             .connector(connectorName)

File: kafka-ui-e2e-checks/src/test/java/com/provectus/kafka/ui/topics/TopicTests.java
Patch:
@@ -57,7 +57,6 @@ void updateTopic() {
         pages.openTopicsList(SECOND_LOCAL)
                 .isOnPage()
                 .openTopic(TOPIC_TO_UPDATE);
-        Selenide.refresh();
         pages.openTopicView(SECOND_LOCAL, TOPIC_TO_UPDATE)
                 .openEditSettings()
                 .changeCleanupPolicy(COMPACT_POLICY_VALUE)

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/serde/schemaregistry/MessageFormat.java
Patch:
@@ -3,5 +3,6 @@
 public enum MessageFormat {
   AVRO,
   JSON,
-  PROTOBUF
+  PROTOBUF,
+  UNKNOWN
 }

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/service/OffsetsResetServiceTest.java
Patch:
@@ -29,7 +29,9 @@
 import org.junit.jupiter.api.AfterEach;
 import org.junit.jupiter.api.BeforeEach;
 import org.junit.jupiter.api.Test;
+import org.springframework.test.context.ContextConfiguration;
 
+@ContextConfiguration(initializers = {AbstractBaseTest.Initializer.class})
 public class OffsetsResetServiceTest extends AbstractBaseTest {
 
   private static final int PARTITIONS = 5;

File: kafka-ui-e2e-checks/src/test/java/com/provectus/kafka/ui/topics/TopicTests.java
Patch:
@@ -84,7 +84,6 @@ void deleteTopic() {
         pages.openTopicsList(SECOND_LOCAL)
                 .isOnPage()
                 .openTopic(TOPIC_TO_DELETE);
-        Selenide.refresh();
         pages.openTopicView(SECOND_LOCAL, TOPIC_TO_DELETE).clickDeleteTopicButton();
         pages.openTopicsList(SECOND_LOCAL).isNotVisible(TOPIC_TO_DELETE);
         

File: kafka-ui-e2e-checks/src/test/java/com/provectus/kafka/ui/topics/TopicTests.java
Patch:
@@ -84,6 +84,7 @@ void deleteTopic() {
         pages.openTopicsList(SECOND_LOCAL)
                 .isOnPage()
                 .openTopic(TOPIC_TO_DELETE);
+        Selenide.refresh();
         pages.openTopicView(SECOND_LOCAL, TOPIC_TO_DELETE).clickDeleteTopicButton();
         pages.openTopicsList(SECOND_LOCAL).isNotVisible(TOPIC_TO_DELETE);
         

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/mapper/ClusterMapper.java
Patch:
@@ -123,6 +123,8 @@ default TopicDetails toTopicDetails(InternalTopic topic, InternalClusterMetrics
     return result;
   }
 
+  @Mapping(target = "isReadOnly", source = "readOnly")
+  @Mapping(target = "isSensitive", source = "sensitive")
   TopicConfig toTopicConfig(InternalTopicConfig topic);
 
   Replica toReplica(InternalReplica replica);

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/KafkaService.java
Patch:
@@ -159,8 +159,9 @@ private KafkaCluster buildFromData(KafkaCluster currentCluster,
     ServerStatus zookeeperStatus = ServerStatus.OFFLINE;
     Throwable zookeeperException = null;
     try {
-      zookeeperStatus = zookeeperService.isZookeeperOnline(currentCluster) ? ServerStatus.ONLINE :
-          ServerStatus.OFFLINE;
+      zookeeperStatus = zookeeperService.isZookeeperOnline(currentCluster)
+              ? ServerStatus.ONLINE
+              : ServerStatus.OFFLINE;
     } catch (Throwable e) {
       zookeeperException = e;
     }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/mapper/ClusterMapper.java
Patch:
@@ -35,6 +35,7 @@
 import java.nio.file.Path;
 import java.util.List;
 import java.util.Map;
+import java.util.Objects;
 import java.util.Properties;
 import java.util.stream.Collectors;
 import org.apache.kafka.clients.admin.ConfigEntry;
@@ -91,7 +92,8 @@ default ConfigSynonym toConfigSynonym(ConfigEntry.ConfigSynonym config) {
   Partition toPartition(InternalPartition topic);
 
   default InternalSchemaRegistry setSchemaRegistry(ClustersProperties.Cluster clusterProperties) {
-    if (clusterProperties == null) {
+    if (clusterProperties == null
+        || clusterProperties.getSchemaRegistry() == null) {
       return null;
     }
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/serde/schemaregistry/SchemaRegistryAwareRecordSerDe.java
Patch:
@@ -68,8 +68,9 @@ public class SchemaRegistryAwareRecordSerDe implements RecordSerDe {
   private final ObjectMapper objectMapper = new ObjectMapper();
 
   private static SchemaRegistryClient createSchemaRegistryClient(KafkaCluster cluster) {
-    Objects.requireNonNull(cluster.getSchemaRegistry());
-    Objects.requireNonNull(cluster.getSchemaRegistry().getUrl());
+    if (cluster.getSchemaRegistry() == null) {
+      throw new ValidationException("schemaRegistry is not specified");
+    }
     List<SchemaProvider> schemaProviders =
         List.of(new AvroSchemaProvider(), new ProtobufSchemaProvider(), new JsonSchemaProvider());
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/ClustersStorage.java
Patch:
@@ -5,6 +5,7 @@
 import com.provectus.kafka.ui.model.Feature;
 import com.provectus.kafka.ui.model.KafkaCluster;
 import java.util.Collection;
+import java.util.HashMap;
 import java.util.Map;
 import java.util.Optional;
 import java.util.concurrent.ConcurrentHashMap;
@@ -34,6 +35,7 @@ public void init() {
       kafkaClusters.put(
           clusterProperties.getName(),
           cluster.toBuilder()
+              .topics(new HashMap<>())
               .features(Feature.getEnabledFeatures(cluster))
               .build()
       );

File: kafka-ui-e2e-checks/src/test/java/com/provectus/kafka/ui/extensions/WaitUtils.java
Patch:
@@ -26,6 +26,6 @@ public static void waitForSelectedValue(SelenideElement element, String selected
             i++;
             sleep(2000);
         } while (!selectedValue.equals(element.getSelectedValue()) && i != 20);
-        Assertions.assertEquals(element.getSelectedValue(), selectedValue);
+        Assertions.assertEquals(selectedValue, element.getSelectedValue()) ;
     }
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/config/ClustersProperties.java
Patch:
@@ -21,6 +21,7 @@ public static class Cluster {
     String zookeeper;
     String schemaRegistry;
     SchemaRegistryAuth schemaRegistryAuth;
+    String ksqldbServer;
     String schemaNameTemplate = "%s-value";
     String keySchemaNameTemplate = "%s-key";
     String protobufFile;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/exception/ErrorCode.java
Patch:
@@ -19,7 +19,8 @@ public enum ErrorCode {
   CLUSTER_NOT_FOUND(4007, HttpStatus.NOT_FOUND),
   TOPIC_NOT_FOUND(4008, HttpStatus.NOT_FOUND),
   SCHEMA_NOT_FOUND(4009, HttpStatus.NOT_FOUND),
-  CONNECT_NOT_FOUND(4010, HttpStatus.NOT_FOUND);
+  CONNECT_NOT_FOUND(4010, HttpStatus.NOT_FOUND),
+  KSQLDB_NOT_FOUND(4011, HttpStatus.NOT_FOUND);
 
   static {
     // codes uniqueness check

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/model/Feature.java
Patch:
@@ -11,6 +11,7 @@ public enum Feature {
       .filter(Predicate.not(List::isEmpty))
       .isPresent()
   ),
+  KSQL_DB(cluster -> cluster.getKsqldbServer() != null),
   SCHEMA_REGISTRY(cluster -> cluster.getSchemaRegistry() != null);
 
   private final Predicate<KafkaCluster> isEnabled;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/model/KafkaCluster.java
Patch:
@@ -16,6 +16,7 @@ public class KafkaCluster {
   private final String bootstrapServers;
   private final String zookeeper;
   private final InternalSchemaRegistry schemaRegistry;
+  private final String ksqldbServer;
   private final List<KafkaConnectCluster> kafkaConnect;
   private final String schemaNameTemplate;
   private final String keySchemaNameTemplate;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/config/Config.java
Patch:
@@ -22,15 +22,15 @@ public KeyedObjectPool<String, JMXConnector> pool() {
   }
 
   private GenericKeyedObjectPoolConfig poolConfig() {
-    GenericKeyedObjectPoolConfig poolConfig = new GenericKeyedObjectPoolConfig();
+    final var poolConfig = new GenericKeyedObjectPoolConfig();
     poolConfig.setMaxIdlePerKey(3);
     poolConfig.setMaxTotalPerKey(3);
     return poolConfig;
   }
 
   @Bean
   public MBeanExporter exporter() {
-    final MBeanExporter exporter = new MBeanExporter();
+    final var exporter = new MBeanExporter();
     exporter.setAutodetect(true);
     exporter.setExcludedBeans("pool");
     return exporter;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/exception/ErrorCode.java
Patch:
@@ -9,6 +9,8 @@ public enum ErrorCode {
 
   UNEXPECTED(5000, HttpStatus.INTERNAL_SERVER_ERROR),
   BINDING_FAIL(4001, HttpStatus.BAD_REQUEST),
+  NOT_FOUND(404, HttpStatus.NOT_FOUND),
+  INVALID_ENTITY_STATE(4001, HttpStatus.BAD_REQUEST),
   VALIDATION_FAIL(4002, HttpStatus.BAD_REQUEST),
   READ_ONLY_MODE_ENABLE(4003, HttpStatus.METHOD_NOT_ALLOWED),
   REBALANCE_IN_PROGRESS(4004, HttpStatus.CONFLICT),

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/model/ExtendedAdminClient.java
Patch:
@@ -15,6 +15,7 @@ public class ExtendedAdminClient {
   private final Set<SupportedFeature> supportedFeatures;
 
   public static Mono<ExtendedAdminClient> extendedAdminClient(AdminClient adminClient) {
+
     return ClusterUtil.getSupportedFeatures(adminClient)
         .map(s -> new ExtendedAdminClient(adminClient, s));
   }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/model/InternalClusterMetrics.java
Patch:
@@ -27,4 +27,5 @@ public class InternalClusterMetrics {
   private final Map<Integer, InternalBrokerMetrics> internalBrokerMetrics;
   private final List<Metric> metrics;
   private final int zooKeeperStatus;
+  private final String version;
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/model/InternalTopic.java
Patch:
@@ -14,6 +14,7 @@ public class InternalTopic {
   private final Map<Integer, InternalPartition> partitions;
   private final List<InternalTopicConfig> topicConfigs;
 
+  private final CleanupPolicy cleanUpPolicy;
   private final int replicas;
   private final int partitionCount;
   private final int inSyncReplicas;

File: kafka-ui-e2e-checks/src/test/java/com/provectus/kafka/ui/SmokeTests.java
Patch:
@@ -13,7 +13,7 @@ public class SmokeTests extends BaseTest {
     @DisplayName("main page should load")
     @Issue("380")
     void mainPageLoads() {
-        pages.goTo("")
+        pages.open()
             .mainPage.shouldBeOnPage();
         compareScreenshots("main");
     }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/model/KafkaCluster.java
Patch:
@@ -15,7 +15,7 @@ public class KafkaCluster {
   private final Integer jmxPort;
   private final String bootstrapServers;
   private final String zookeeper;
-  private final String schemaRegistry;
+  private final InternalSchemaRegistry schemaRegistry;
   private final List<KafkaConnectCluster> kafkaConnect;
   private final String schemaNameTemplate;
   private final String keySchemaNameTemplate;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/client/KsqlClient.java
Patch:
@@ -4,7 +4,7 @@
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.provectus.kafka.ui.exception.UnprocessableEntityException;
 import com.provectus.kafka.ui.model.KsqlCommandResponse;
-import com.provectus.kafka.ui.strategy.ksql.statement.KsqlStatementStrategy;
+import com.provectus.kafka.ui.strategy.ksql.statement.BaseStrategy;
 import lombok.RequiredArgsConstructor;
 import lombok.SneakyThrows;
 import lombok.extern.log4j.Log4j2;
@@ -23,7 +23,7 @@ public class KsqlClient {
   private final WebClient webClient;
   private final ObjectMapper mapper;
 
-  public Mono<KsqlCommandResponse> execute(KsqlStatementStrategy ksqlStatement) {
+  public Mono<KsqlCommandResponse> execute(BaseStrategy ksqlStatement) {
     return webClient.post()
         .uri(ksqlStatement.getUri())
         .accept(new MediaType("application", "vnd.ksql.v1+json"))

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/KsqlService.java
Patch:
@@ -7,7 +7,7 @@
 import com.provectus.kafka.ui.model.KafkaCluster;
 import com.provectus.kafka.ui.model.KsqlCommand;
 import com.provectus.kafka.ui.model.KsqlCommandResponse;
-import com.provectus.kafka.ui.strategy.ksql.statement.KsqlStatementStrategy;
+import com.provectus.kafka.ui.strategy.ksql.statement.BaseStrategy;
 import java.util.List;
 import lombok.RequiredArgsConstructor;
 import org.springframework.stereotype.Service;
@@ -18,7 +18,7 @@
 public class KsqlService {
   private final KsqlClient ksqlClient;
   private final ClustersStorage clustersStorage;
-  private final List<KsqlStatementStrategy> ksqlStatementStrategies;
+  private final List<BaseStrategy> ksqlStatementStrategies;
 
   public Mono<KsqlCommandResponse> executeKsqlCommand(String clusterName,
                                                       Mono<KsqlCommand> ksqlCommand) {
@@ -36,7 +36,7 @@ public Mono<KsqlCommandResponse> executeKsqlCommand(String clusterName,
         .flatMap(ksqlClient::execute);
   }
 
-  private Mono<KsqlStatementStrategy> getStatementStrategyForKsqlCommand(
+  private Mono<BaseStrategy> getStatementStrategyForKsqlCommand(
       Mono<KsqlCommand> ksqlCommand) {
     return ksqlCommand
         .map(command -> ksqlStatementStrategies.stream()

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/strategy/ksql/statement/SelectStrategy.java
Patch:
@@ -5,8 +5,7 @@
 import org.springframework.stereotype.Component;
 
 @Component
-public class SelectStrategy extends KsqlStatementStrategy {
-  private final String requestPath = "/query";
+public class SelectStrategy extends BaseStrategy {
 
   @Override
   public KsqlCommandResponse serializeResponse(JsonNode response) {
@@ -15,7 +14,7 @@ public KsqlCommandResponse serializeResponse(JsonNode response) {
 
   @Override
   protected String getRequestPath() {
-    return requestPath;
+    return BaseStrategy.queryRequestPath;
   }
 
   @Override

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/client/KsqlClient.java
Patch:
@@ -4,7 +4,7 @@
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.provectus.kafka.ui.exception.UnprocessableEntityException;
 import com.provectus.kafka.ui.model.KsqlCommandResponse;
-import com.provectus.kafka.ui.strategy.ksqlStatement.KsqlStatementStrategy;
+import com.provectus.kafka.ui.strategy.ksql.statement.KsqlStatementStrategy;
 import lombok.RequiredArgsConstructor;
 import lombok.SneakyThrows;
 import lombok.extern.log4j.Log4j2;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/controller/KsqlController.java
Patch:
@@ -2,7 +2,6 @@
 
 import com.provectus.kafka.ui.api.KsqlApi;
 import com.provectus.kafka.ui.model.KsqlCommand;
-
 import com.provectus.kafka.ui.model.KsqlCommandResponse;
 import com.provectus.kafka.ui.service.KsqlService;
 import lombok.RequiredArgsConstructor;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/KsqlService.java
Patch:
@@ -7,13 +7,12 @@
 import com.provectus.kafka.ui.model.KafkaCluster;
 import com.provectus.kafka.ui.model.KsqlCommand;
 import com.provectus.kafka.ui.model.KsqlCommandResponse;
-import com.provectus.kafka.ui.strategy.ksqlStatement.KsqlStatementStrategy;
+import com.provectus.kafka.ui.strategy.ksql.statement.KsqlStatementStrategy;
+import java.util.List;
 import lombok.RequiredArgsConstructor;
 import org.springframework.stereotype.Service;
 import reactor.core.publisher.Mono;
 
-import java.util.List;
-
 @Service
 @RequiredArgsConstructor
 public class KsqlService {

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/strategy/ksql/statement/CreateStrategy.java
Patch:
@@ -1,4 +1,4 @@
-package com.provectus.kafka.ui.strategy.ksqlStatement;
+package com.provectus.kafka.ui.strategy.ksql.statement;
 
 import com.fasterxml.jackson.databind.JsonNode;
 import com.provectus.kafka.ui.model.KsqlCommandResponse;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/strategy/ksql/statement/DescribeStrategy.java
Patch:
@@ -1,4 +1,4 @@
-package com.provectus.kafka.ui.strategy.ksqlStatement;
+package com.provectus.kafka.ui.strategy.ksql.statement;
 
 import com.fasterxml.jackson.databind.JsonNode;
 import com.provectus.kafka.ui.model.KsqlCommandResponse;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/strategy/ksql/statement/DropStrategy.java
Patch:
@@ -1,4 +1,4 @@
-package com.provectus.kafka.ui.strategy.ksqlStatement;
+package com.provectus.kafka.ui.strategy.ksql.statement;
 
 import com.fasterxml.jackson.databind.JsonNode;
 import com.provectus.kafka.ui.model.KsqlCommandResponse;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/strategy/ksql/statement/ExplainStrategy.java
Patch:
@@ -1,4 +1,4 @@
-package com.provectus.kafka.ui.strategy.ksqlStatement;
+package com.provectus.kafka.ui.strategy.ksql.statement;
 
 import com.fasterxml.jackson.databind.JsonNode;
 import com.provectus.kafka.ui.model.KsqlCommandResponse;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/strategy/ksql/statement/ListStrategy.java
Patch:
@@ -1,11 +1,10 @@
-package com.provectus.kafka.ui.strategy.ksqlStatement;
+package com.provectus.kafka.ui.strategy.ksql.statement;
 
 import com.fasterxml.jackson.databind.JsonNode;
 import com.provectus.kafka.ui.model.KsqlCommandResponse;
-import org.springframework.stereotype.Component;
-
 import java.util.List;
 import java.util.Optional;
+import org.springframework.stereotype.Component;
 
 @Component
 public class ListStrategy extends KsqlStatementStrategy {

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/strategy/ksql/statement/SelectStrategy.java
Patch:
@@ -1,4 +1,4 @@
-package com.provectus.kafka.ui.strategy.ksqlStatement;
+package com.provectus.kafka.ui.strategy.ksql.statement;
 
 import com.fasterxml.jackson.databind.JsonNode;
 import com.provectus.kafka.ui.model.KsqlCommandResponse;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/strategy/ksql/statement/TerminateStrategy.java
Patch:
@@ -1,4 +1,4 @@
-package com.provectus.kafka.ui.strategy.ksqlStatement;
+package com.provectus.kafka.ui.strategy.ksql.statement;
 
 import com.fasterxml.jackson.databind.JsonNode;
 import com.provectus.kafka.ui.model.KsqlCommandResponse;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/model/InternalTopic.java
Patch:
@@ -14,6 +14,7 @@ public class InternalTopic {
   private final Map<Integer, InternalPartition> partitions;
   private final List<InternalTopicConfig> topicConfigs;
 
+  private final CleanupPolicy cleanUpPolicy;
   private final int replicas;
   private final int partitionCount;
   private final int inSyncReplicas;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/client/KsqlClient.java
Patch:
@@ -15,7 +15,6 @@
 @Log4j2
 public final class KsqlClient {
   private final WebClient webClient;
-  private ObjectMapper objectMapper;
 
   public Mono<Object> execute(KsqlStatementStrategy ksqlStatement) {
     return webClient.post()

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/controller/KsqlController.java
Patch:
@@ -21,6 +21,6 @@ public class KsqlController implements KsqlApi {
   public Mono<ResponseEntity<Object>> executeKsqlCommand(String clusterName,
                                                                Mono<KsqlCommand> ksqlCommand,
                                                                ServerWebExchange exchange) {
-    return Mono.just(ResponseEntity.ok(ksqlService.getListStreams(clusterName, ksqlCommand)));
+    return Mono.just(ResponseEntity.ok(ksqlService.executeKsqlCommand(clusterName, ksqlCommand)));
   }
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/KsqlService.java
Patch:
@@ -20,8 +20,8 @@ public class KsqlService {
   private final ClustersStorage clustersStorage;
   private final List<KsqlStatementStrategy> commandParamsStrategies;
 
-  public Mono<Object> getListStreams(String name, Mono<KsqlCommand> ksqlCommand) {
-    return Mono.justOrEmpty(clustersStorage.getClusterByName(name))
+  public Mono<Object> executeKsqlCommand(String clusterName, Mono<KsqlCommand> ksqlCommand) {
+    return Mono.justOrEmpty(clustersStorage.getClusterByName(clusterName))
             .switchIfEmpty(Mono.error(ClusterNotFoundException::new))
             .map(KafkaCluster::getKsqldbServer)
             .switchIfEmpty(Mono.error(KsqlDbNotFoundException::new))
@@ -38,7 +38,7 @@ private Mono<KsqlStatementStrategy> getStatementStrategyForKsqlCommand(Mono<Ksql
                     .map(s -> s.ksqlCommand(command))
                     .findFirst())
             .flatMap(Mono::justOrEmpty)
-// TODO: how to handle not parsed statements?
+            // TODO: handle not parsed statements?
             .switchIfEmpty(Mono.error(new UnprocessableEntityException("Invalid sql")));
   }
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/strategy/ksqlStatement/ListStreamsStrategy.java
Patch:
@@ -19,7 +19,7 @@ public Object serializeResponse(String response) {
 
     @Override
     public boolean test(String sql) {
-        return sql.trim().toLowerCase().matches("list streams;");
+        return sql.trim().toLowerCase().matches("(list|show) streams;");
     }
 
     @Override

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/strategy/ksqlStatement/ListTopicsStrategy.java
Patch:
@@ -16,7 +16,7 @@ public Object serializeResponse(String response) {
 
     @Override
     public boolean test(String sql) {
-        return sql.trim().toLowerCase().matches("list topics;");
+        return sql.trim().toLowerCase().matches("(list|show) topics;");
     }
 
     @Override

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/config/ClustersProperties.java
Patch:
@@ -20,6 +20,7 @@ public static class Cluster {
     String bootstrapServers;
     String zookeeper;
     String schemaRegistry;
+    String ksqldbServer;
     String schemaNameTemplate = "%s-value";
     String protobufFile;
     String protobufMessageName;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/exception/ErrorCode.java
Patch:
@@ -17,7 +17,8 @@ public enum ErrorCode {
   CLUSTER_NOT_FOUND(4007, HttpStatus.NOT_FOUND),
   TOPIC_NOT_FOUND(4008, HttpStatus.NOT_FOUND),
   SCHEMA_NOT_FOUND(4009, HttpStatus.NOT_FOUND),
-  CONNECT_NOT_FOUND(4010, HttpStatus.NOT_FOUND);
+  CONNECT_NOT_FOUND(4010, HttpStatus.NOT_FOUND),
+  KSQLDB_NOT_FOUND(4011, HttpStatus.NOT_FOUND);
 
   static {
     // codes uniqueness check

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/model/KafkaCluster.java
Patch:
@@ -14,6 +14,7 @@ public class KafkaCluster {
   private final Integer jmxPort;
   private final String bootstrapServers;
   private final String zookeeper;
+  private final String ksqldbServer;
   private final String schemaRegistry;
   private final List<KafkaConnectCluster> kafkaConnect;
   private final String schemaNameTemplate;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/serde/schemaregistry/MessageFormatter.java
Patch:
@@ -1,5 +1,5 @@
 package com.provectus.kafka.ui.serde.schemaregistry;
 
 public interface MessageFormatter {
-  Object format(String topic, byte[] value);
+  String format(String topic, byte[] value);
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/serde/schemaregistry/MessageReader.java
Patch:
@@ -23,10 +23,10 @@ protected MessageReader(String topic, boolean isKey, SchemaRegistryClient client
 
   protected abstract Serializer<T> createSerializer(SchemaRegistryClient client);
 
-  public byte[] read(byte[] value) {
+  public byte[] read(String value) {
     final T read = this.read(value, schema);
     return this.serializer.serialize(topic, read);
   }
 
-  protected abstract T read(byte[] value, ParsedSchema schema);
+  protected abstract T read(String value, ParsedSchema schema);
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/util/ClusterUtil.java
Patch:
@@ -215,9 +215,9 @@ public static TopicMessage mapToTopicMessage(ConsumerRecord<Bytes, Bytes> consum
     topicMessage.setTimestampType(timestampType);
 
     topicMessage.setHeaders(headers);
-    Tuple2<String, Object> parsed = recordDeserializer.deserialize(consumerRecord);
-    topicMessage.setKey(parsed.getT1());
-    topicMessage.setContent(parsed.getT2());
+    var parsed = recordDeserializer.deserialize(consumerRecord);
+    topicMessage.setKey(parsed.getKey());
+    topicMessage.setContent(parsed.getValue());
 
     return topicMessage;
   }

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/AbstractBaseTest.java
Patch:
@@ -56,14 +56,14 @@ public static class Initializer
     public void initialize(@NotNull ConfigurableApplicationContext context) {
       System.setProperty("kafka.clusters.0.name", LOCAL);
       System.setProperty("kafka.clusters.0.bootstrapServers", kafka.getBootstrapServers());
-      System.setProperty("kafka.clusters.0.schemaRegistry", schemaRegistry.getTarget());
+      System.setProperty("kafka.clusters.0.schemaRegistry", schemaRegistry.getUrl());
       System.setProperty("kafka.clusters.0.kafkaConnect.0.name", "kafka-connect");
       System.setProperty("kafka.clusters.0.kafkaConnect.0.address", kafkaConnect.getTarget());
 
       System.setProperty("kafka.clusters.1.name", SECOND_LOCAL);
       System.setProperty("kafka.clusters.1.readOnly", "true");
       System.setProperty("kafka.clusters.1.bootstrapServers", kafka.getBootstrapServers());
-      System.setProperty("kafka.clusters.1.schemaRegistry", schemaRegistry.getTarget());
+      System.setProperty("kafka.clusters.1.schemaRegistry", schemaRegistry.getUrl());
       System.setProperty("kafka.clusters.1.kafkaConnect.0.name", "kafka-connect");
       System.setProperty("kafka.clusters.1.kafkaConnect.0.address", kafkaConnect.getTarget());
     }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/model/KafkaCluster.java
Patch:
@@ -23,6 +23,7 @@ public class KafkaCluster {
   private final ServerStatus zookeeperStatus;
   private final InternalClusterMetrics metrics;
   private final Map<String, InternalTopic> topics;
+  private final List<Integer> brokers;
   private final Throwable lastKafkaException;
   private final Throwable lastZookeeperException;
   private final Path protobufFile;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/KafkaService.java
Patch:
@@ -40,6 +40,7 @@
 import java.util.stream.Collectors;
 import java.util.stream.Stream;
 import lombok.RequiredArgsConstructor;
+import lombok.Setter;
 import lombok.SneakyThrows;
 import lombok.extern.log4j.Log4j2;
 import org.apache.kafka.clients.admin.AdminClient;
@@ -85,6 +86,7 @@ public class KafkaService {
   private final JmxClusterUtil jmxClusterUtil;
   private final ClustersStorage clustersStorage;
   private final DeserializationService deserializationService;
+  @Setter // used in tests
   @Value("${kafka.admin-client-timeout}")
   private int clientTimeout;
 
@@ -393,7 +395,7 @@ public KafkaConsumer<Bytes, Bytes> createConsumer(KafkaCluster cluster,
                                                     Map<String, Object> properties) {
     Properties props = new Properties();
     props.putAll(cluster.getProperties());
-    props.put(ConsumerConfig.CLIENT_ID_CONFIG, "kafka-ui-" + UUID.randomUUID().toString());
+    props.put(ConsumerConfig.CLIENT_ID_CONFIG, "kafka-ui-" + UUID.randomUUID());
     props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, cluster.getBootstrapServers());
     props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, BytesDeserializer.class);
     props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, BytesDeserializer.class);

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/util/jsonschema/AvroJsonSchemaConverterTest.java
Patch:
@@ -4,8 +4,8 @@
 import java.net.URI;
 import java.net.URISyntaxException;
 import org.apache.avro.Schema;
-import org.junit.Test;
 import org.junit.jupiter.api.Assertions;
+import org.junit.jupiter.api.Test;
 
 public class AvroJsonSchemaConverterTest {
   @Test

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/util/jsonschema/ProtobufSchemaConverterTest.java
Patch:
@@ -4,8 +4,8 @@
 import io.confluent.kafka.schemaregistry.protobuf.ProtobufSchema;
 import java.net.URI;
 import java.net.URISyntaxException;
-import org.junit.Test;
 import org.junit.jupiter.api.Assertions;
+import org.junit.jupiter.api.Test;
 
 
 public class ProtobufSchemaConverterTest {

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/service/ClusterServiceTest.java
Patch:
@@ -202,4 +202,5 @@ public void shouldListTopicsOrderedByPartitionsCount() {
     assertThat(topics.getTopics()).hasSize(25);
     assertThat(topics.getTopics()).map(Topic::getPartitionCount).isSorted();
   }
+
 }

File: kafka-ui-e2e-checks/src/test/java/com/provectus/kafka/ui/topics/TopicTests.java
Patch:
@@ -4,9 +4,7 @@
 import com.provectus.kafka.ui.pages.MainPage;
 import com.provectus.kafka.ui.steps.kafka.KafkaSteps;
 import lombok.SneakyThrows;
-import org.junit.jupiter.api.AfterEach;
-import org.junit.jupiter.api.DisplayName;
-import org.junit.jupiter.api.Test;
+import org.junit.jupiter.api.*;
 
 public class TopicTests extends BaseTest {
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/config/CustomWebFilter.java
Patch:
@@ -1,6 +1,5 @@
 package com.provectus.kafka.ui.config;
 
-import java.util.Optional;
 import org.springframework.boot.autoconfigure.web.ServerProperties;
 import org.springframework.stereotype.Component;
 import org.springframework.web.server.ServerWebExchange;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/serde/schemaregistry/JsonMessageReader.java
Patch:
@@ -5,7 +5,6 @@
 import io.confluent.kafka.schemaregistry.ParsedSchema;
 import io.confluent.kafka.schemaregistry.client.SchemaMetadata;
 import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;
-import io.confluent.kafka.schemaregistry.client.rest.entities.Schema;
 import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;
 import java.io.IOException;
 import lombok.SneakyThrows;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/serde/schemaregistry/MessageReader.java
Patch:
@@ -3,7 +3,6 @@
 import io.confluent.kafka.schemaregistry.ParsedSchema;
 import io.confluent.kafka.schemaregistry.client.SchemaMetadata;
 import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;
-import io.confluent.kafka.schemaregistry.client.rest.entities.Schema;
 import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;
 import java.io.IOException;
 import org.apache.kafka.common.serialization.Serializer;
@@ -12,8 +11,7 @@ public abstract class MessageReader<T> {
   protected final Serializer<T> serializer;
   protected final String topic;
   protected final boolean isKey;
-
-  private ParsedSchema schema;
+  private final ParsedSchema schema;
 
   protected MessageReader(String topic, boolean isKey, SchemaRegistryClient client,
                           SchemaMetadata schema) throws IOException, RestClientException {

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/serde/schemaregistry/ProtobufMessageReader.java
Patch:
@@ -6,7 +6,6 @@
 import io.confluent.kafka.schemaregistry.ParsedSchema;
 import io.confluent.kafka.schemaregistry.client.SchemaMetadata;
 import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;
-import io.confluent.kafka.schemaregistry.client.rest.entities.Schema;
 import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;
 import io.confluent.kafka.schemaregistry.protobuf.ProtobufSchema;
 import io.confluent.kafka.serializers.protobuf.KafkaProtobufSerializer;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/util/jsonschema/EnumJsonType.java
Patch:
@@ -7,7 +7,7 @@
 
 
 public class EnumJsonType extends JsonType {
-  private List<String> values;
+  private final List<String> values;
 
   public EnumJsonType(List<String> values) {
     super(Type.ENUM);

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/util/jsonschema/JsonSchema.java
Patch:
@@ -4,7 +4,6 @@
 import com.fasterxml.jackson.databind.node.ObjectNode;
 import com.fasterxml.jackson.databind.node.TextNode;
 import java.net.URI;
-import java.net.URISyntaxException;
 import java.util.List;
 import java.util.Map;
 import java.util.stream.Collectors;

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/AbstractBaseTest.java
Patch:
@@ -24,8 +24,8 @@
 @SpringBootTest
 @ActiveProfiles("test")
 public abstract class AbstractBaseTest {
-  public static String LOCAL = "local";
-  public static String SECOND_LOCAL = "secondLocal";
+  public static final String LOCAL = "local";
+  public static final String SECOND_LOCAL = "secondLocal";
 
   private static final String CONFLUENT_PLATFORM_VERSION = "5.5.0";
 

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/KafkaConsumerGroupTests.java
Patch:
@@ -20,7 +20,7 @@
 @ContextConfiguration(initializers = {AbstractBaseTest.Initializer.class})
 @Log4j2
 @AutoConfigureWebTestClient(timeout = "10000")
-public class KakfaConsumerGroupTests extends AbstractBaseTest {
+public class KafkaConsumerGroupTests extends AbstractBaseTest {
   @Autowired
   WebTestClient webTestClient;
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/config/ClustersProperties.java
Patch:
@@ -21,6 +21,7 @@ public static class Cluster {
     String zookeeper;
     String schemaRegistry;
     String schemaNameTemplate = "%s-value";
+    String keySchemaNameTemplate = "%s-key";
     String protobufFile;
     String protobufMessageName;
     List<ConnectCluster> kafkaConnect;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/model/KafkaCluster.java
Patch:
@@ -17,6 +17,7 @@ public class KafkaCluster {
   private final String schemaRegistry;
   private final List<KafkaConnectCluster> kafkaConnect;
   private final String schemaNameTemplate;
+  private final String keySchemaNameTemplate;
   private final ServerStatus status;
   private final ServerStatus zookeeperStatus;
   private final InternalClusterMetrics metrics;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/util/OffsetsSeek.java
Patch:
@@ -18,7 +18,7 @@ public abstract class OffsetsSeek {
   protected final String topic;
   protected final ConsumerPosition consumerPosition;
 
-  public OffsetsSeek(String topic, ConsumerPosition consumerPosition) {
+  protected OffsetsSeek(String topic, ConsumerPosition consumerPosition) {
     this.topic = topic;
     this.consumerPosition = consumerPosition;
   }

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/util/OffsetsSeekTest.java
Patch:
@@ -147,7 +147,7 @@ void backwardSeekToOffset() {
     assertThat(consumer.assignment()).containsExactlyInAnyOrder(tp0, tp1, tp2);
     assertThat(consumer.position(tp0)).isZero();
     assertThat(consumer.position(tp1)).isEqualTo(1L);
-    assertThat(consumer.position(tp2)).isEqualTo(0L);
+    assertThat(consumer.position(tp2)).isZero();
   }
 
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/exception/ErrorCode.java
Patch:
@@ -9,6 +9,8 @@ public enum ErrorCode {
 
   UNEXPECTED(5000, HttpStatus.INTERNAL_SERVER_ERROR),
   BINDING_FAIL(4001, HttpStatus.BAD_REQUEST),
+  NOT_FOUND(404, HttpStatus.NOT_FOUND),
+  INVALID_ENTITY_STATE(4001, HttpStatus.BAD_REQUEST),
   VALIDATION_FAIL(4002, HttpStatus.BAD_REQUEST),
   READ_ONLY_MODE_ENABLE(4003, HttpStatus.METHOD_NOT_ALLOWED),
   REBALANCE_IN_PROGRESS(4004, HttpStatus.CONFLICT),

File: kafka-ui-e2e-checks/src/test/java/com/provectus/kafka/ui/topics/TopicTests.java
Patch:
@@ -28,7 +28,7 @@ void createTopic(){
         helpers.apiHelper.createTopic("secondLocal","new-topic");
         pages.open()
                 .mainPage.shouldBeOnPage()
-        .goToSideMenu(KafkaSteps.Cluster.SECOND_LOCAL.getName(), MainPage.SideMenuOptions.TOPICS)
+        .goToSideMenu("secondLocal", MainPage.SideMenuOptions.TOPICS)
         .shouldBeTopic(NEW_TOPIC);
     }
 }

File: kafka-ui-e2e-checks/src/test/java/com/provectus/kafka/ui/helpers/ApiHelper.java
Patch:
@@ -18,9 +18,10 @@ public class ApiHelper {
     int partitions = 1;
     short replicationFactor = 1;
     String newTopic = "new-topic";
+    String baseURL = "http://localhost:8080/";
     @SneakyThrows
     private void sendRequest(String method, String additionalURL, JSONObject jsonObject){
-        URL url = new URL(TestConfiguration.BASE_URL+additionalURL);
+        URL url = new URL(baseURL+additionalURL);
         HttpURLConnection con = (HttpURLConnection) url.openConnection();
         con.setDoOutput(true);
         con.setDoInput(true);

File: kafka-ui-e2e-checks/src/test/java/com/provectus/kafka/ui/topics/TopicTests.java
Patch:
@@ -6,6 +6,7 @@
 import com.provectus.kafka.ui.helpers.ApiHelper;
 import lombok.SneakyThrows;
 import org.junit.jupiter.api.AfterEach;
+import org.junit.jupiter.api.Disabled;
 import org.junit.jupiter.api.DisplayName;
 import org.junit.jupiter.api.Test;
 

File: kafka-ui-e2e-checks/src/test/java/com/provectus/kafka/ui/SmokeTests.java
Patch:
@@ -13,7 +13,7 @@ public class SmokeTests extends BaseTest {
     @DisplayName("main page should load")
     @Issue("380")
     void mainPageLoads() {
-        pages.goTo("")
+        pages.open()
             .mainPage.shouldBeOnPage();
         compareScreenshots("main");
     }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/controller/StaticController.java
Patch:
@@ -50,6 +50,6 @@ private String buildIndexFile() {
     return ResourceUtil.readAsString(indexFile)
         .replace("href=\"./static", "href=\"" + staticPath)
         .replace("src=\"./static", "src=\"" + staticPath)
-        .replace("window.basePath=\"/\"", "window.basePath=\"" + contextPath + "\"");
+        .replace("window.basePath=\"\"", "window.basePath=\"" + contextPath + "\"");
   }
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/ClusterService.java
Patch:
@@ -43,7 +43,7 @@
 @Service
 @RequiredArgsConstructor
 public class ClusterService {
-  private static final Integer DEFAULT_PAGE_SIZE = 20;
+  private static final Integer DEFAULT_PAGE_SIZE = 25;
 
   private final ClustersStorage clustersStorage;
   private final ClusterMapper clusterMapper;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/config/Config.java
Patch:
@@ -22,15 +22,15 @@ public KeyedObjectPool<String, JMXConnector> pool() {
   }
 
   private GenericKeyedObjectPoolConfig poolConfig() {
-    GenericKeyedObjectPoolConfig poolConfig = new GenericKeyedObjectPoolConfig();
+    final var poolConfig = new GenericKeyedObjectPoolConfig();
     poolConfig.setMaxIdlePerKey(3);
     poolConfig.setMaxTotalPerKey(3);
     return poolConfig;
   }
 
   @Bean
   public MBeanExporter exporter() {
-    final MBeanExporter exporter = new MBeanExporter();
+    final var exporter = new MBeanExporter();
     exporter.setAutodetect(true);
     exporter.setExcludedBeans("pool");
     return exporter;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/deserialization/RecordDeserializer.java
Patch:
@@ -5,5 +5,5 @@
 
 public interface RecordDeserializer {
 
-  Object deserialize(ConsumerRecord<Bytes, Bytes> record);
+  Object deserialize(ConsumerRecord<Bytes, Bytes> msg);
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/deserialization/SimpleRecordDeserializer.java
Patch:
@@ -9,9 +9,9 @@ public class SimpleRecordDeserializer implements RecordDeserializer {
   private final StringDeserializer stringDeserializer = new StringDeserializer();
 
   @Override
-  public Object deserialize(ConsumerRecord<Bytes, Bytes> record) {
-    if (record.value() != null) {
-      return stringDeserializer.deserialize(record.topic(), record.value().get());
+  public Object deserialize(ConsumerRecord<Bytes, Bytes> msg) {
+    if (msg.value() != null) {
+      return stringDeserializer.deserialize(msg.topic(), msg.value().get());
     } else {
       return "empty";
     }

File: kafka-ui-e2e-checks/src/test/java/com/provectus/kafka/ui/SmokeTests.java
Patch:
@@ -17,4 +17,5 @@ void mainPageLoads() {
             .mainPage.shouldBeOnPage();
         compareScreenshots("main");
     }
+
 }

File: kafka-ui-e2e-checks/src/test/java/com/provectus/kafka/ui/SmokeTests.java
Patch:
@@ -8,8 +8,7 @@
 import org.junit.jupiter.api.Test;
 
 public class SmokeTests extends BaseTest {
-
-    @Disabled("till we get tests in ci run")
+    @Test
     @SneakyThrows
     @DisplayName("main page should load")
     @Issue("380")

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/util/ClusterUtil.java
Patch:
@@ -139,7 +139,9 @@ public static InternalTopicConfig mapToInternalTopicConfig(ConfigEntry configEnt
 
   public static InternalTopic mapToInternalTopic(TopicDescription topicDescription) {
     var topic = InternalTopic.builder();
-    topic.internal(topicDescription.isInternal());
+    topic.internal(
+        topicDescription.isInternal() || topicDescription.name().startsWith("_")
+    );
     topic.name(topicDescription.name());
 
     List<InternalPartition> partitions = topicDescription.partitions().stream().map(

File: kafka-ui-e2e-checks/src/test/java/com/provectus/kafka/ui/SmokeTests.java
Patch:
@@ -3,12 +3,13 @@
 import com.provectus.kafka.ui.base.BaseTest;
 import io.qameta.allure.Issue;
 import lombok.SneakyThrows;
+import org.junit.jupiter.api.Disabled;
 import org.junit.jupiter.api.DisplayName;
 import org.junit.jupiter.api.Test;
 
 public class SmokeTests extends BaseTest {
 
-    @Test
+    @Disabled("till we get tests in ci run")
     @SneakyThrows
     @DisplayName("main page should load")
     @Issue("380")

File: kafka-ui-e2e-checks/src/test/java/com/provectus/kafka/ui/screenshots/Screenshooter.java
Patch:
@@ -56,6 +56,7 @@ public void compareScreenshots(String name) {
     compareScreenshots(name, false);
   }
 
+  @SneakyThrows
   public void compareScreenshots(String name, boolean shouldUpdateScreenshotIfDiffer) {
     if (TURN_OFF_SCREENSHOTS) {
       return;
@@ -64,7 +65,7 @@ public void compareScreenshots(String name, boolean shouldUpdateScreenshotIfDiff
       if (SHOULD_SAVE_SCREENSHOTS_IF_NOT_EXIST) {
         updateActualScreenshot(name);
       } else {
-        fail("no reference screenshot found for %s".formatted(name));
+        throw new NoReferenceScreenshotFoundException(name);
       }
     } else {
       makeImageDiff(name, shouldUpdateScreenshotIfDiffer);
@@ -124,7 +125,7 @@ private void makeImageDiff(String expectedName, boolean shouldUpdateScreenshotIf
   @SneakyThrows
   private byte[] imgToBytes(String filename) {
     BufferedImage bImage2 = ImageIO.read(new File(filename));
-    ByteArrayOutputStream bos2 = new ByteArrayOutputStream();
+    var bos2 = new ByteArrayOutputStream();
     ImageIO.write(bImage2, "png", bos2);
     return bos2.toByteArray();
   }

File: kafka-ui-e2e-checks/src/test/java/com/provectus/kafka/ui/SmokeTests.java
Patch:
@@ -9,7 +9,7 @@
 
 public class SmokeTests extends BaseTest {
 
-    @Test
+    @Disabled("till we get tests in ci run")
     @SneakyThrows
     @DisplayName("main page should load")
     @Issue("380")

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/screenshots/Screenshooter.java
Patch:
@@ -125,7 +125,7 @@ private void makeImageDiff(String expectedName, boolean shouldUpdateScreenshotIf
   @SneakyThrows
   private byte[] imgToBytes(String filename) {
     BufferedImage bImage2 = ImageIO.read(new File(filename));
-    ByteArrayOutputStream bos2 = new ByteArrayOutputStream();
+    var bos2 = new ByteArrayOutputStream();
     ImageIO.write(bImage2, "png", bos2);
     return bos2.toByteArray();
   }

File: kafka-ui-e2e-checks/src/test/java/com/provectus/kafka/ui/SmokeTests.java
Patch:
@@ -9,7 +9,7 @@
 
 public class SmokeTests extends BaseTest {
 
-    @Disabled
+    @Test
     @SneakyThrows
     @DisplayName("main page should load")
     @Issue("380")

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/screenshots/Screenshooter.java
Patch:
@@ -56,6 +56,7 @@ public void compareScreenshots(String name) {
     compareScreenshots(name, false);
   }
 
+  @SneakyThrows
   public void compareScreenshots(String name, boolean shouldUpdateScreenshotIfDiffer) {
     if (TURN_OFF_SCREENSHOTS) {
       return;
@@ -64,7 +65,7 @@ public void compareScreenshots(String name, boolean shouldUpdateScreenshotIfDiff
       if (SHOULD_SAVE_SCREENSHOTS_IF_NOT_EXIST) {
         updateActualScreenshot(name);
       } else {
-        fail("no reference screenshot found for %s".formatted(name));
+        throw new NoReferenceScreenshotFoundException(name);
       }
     } else {
       makeImageDiff(name, shouldUpdateScreenshotIfDiffer);

File: kafka-ui-e2e-checks/src/test/java/com/provectus/kafka/ui/SmokeTests.java
Patch:
@@ -3,12 +3,13 @@
 import com.provectus.kafka.ui.base.BaseTest;
 import io.qameta.allure.Issue;
 import lombok.SneakyThrows;
+import org.junit.jupiter.api.Disabled;
 import org.junit.jupiter.api.DisplayName;
 import org.junit.jupiter.api.Test;
 
 public class SmokeTests extends BaseTest {
 
-    @Test
+    @Disabled
     @SneakyThrows
     @DisplayName("main page should load")
     @Issue("380")

File: kafka-ui-e2e-checks/src/test/java/com/provectus/kafka/ui/SmokeTests.java
Patch:
@@ -17,6 +17,4 @@ void mainPageLoads() {
             .mainPage.shouldBeOnPage();
         compareScreenshots("main");
     }
-
-
 }

File: kafka-ui-e2e-checks/src/main/java/com/provectus/kafka/ui/base/TestConfiguration.java
Patch:
@@ -7,7 +7,7 @@ public class TestConfiguration {
   public static boolean SHOULD_START_SELENOID =
       Boolean.parseBoolean(System.getProperty("SHOULD_START_SELENOID", "false"));
 
-  public static String BASE_URL = System.getProperty("BASE_URL", "http://192.168.1.2:8080/");
+  public static String BASE_URL = System.getProperty("BASE_URL", "http://localhost:8080/");
 
   public static boolean USE_LOCAL_BROWSER =
       Boolean.parseBoolean(System.getProperty("USE_LOCAL_BROWSER", "true"));

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/SchemaRegistryService.java
Patch:
@@ -80,7 +80,7 @@ private Flux<Integer> getSubjectVersions(String clusterName, String schemaName)
             .uri(cluster.getSchemaRegistry() + URL_SUBJECT_VERSIONS, schemaName)
             .retrieve()
             .onStatus(NOT_FOUND::equals,
-                throwIfNotFoundStatus(formatted(NO_SUCH_SCHEMA))
+                throwIfNotFoundStatus(formatted(NO_SUCH_SCHEMA, schemaName))
             ).bodyToFlux(Integer.class)
         ).orElse(Flux.error(ClusterNotFoundException::new));
   }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/exception/GlobalErrorWebExceptionHandler.java
Patch:
@@ -71,7 +71,7 @@ private Mono<ServerResponse> renderErrorResponse(ServerRequest request) {
   private Mono<ServerResponse> renderDefault(Throwable throwable, ServerRequest request) {
     var response = new ErrorResponse()
         .code(ErrorCode.UNEXPECTED.code())
-        .message(throwable.getMessage())
+        .message(coalesce(throwable.getMessage(), "Unexpected internal error"))
         .requestId(requestId(request))
         .timestamp(currentTimestamp());
     return ServerResponse
@@ -84,7 +84,7 @@ private Mono<ServerResponse> render(CustomBaseException baseException, ServerReq
     ErrorCode errorCode = baseException.getErrorCode();
     var response = new ErrorResponse()
         .code(errorCode.code())
-        .message(baseException.getMessage())
+        .message(coalesce(baseException.getMessage(), "Internal error"))
         .requestId(requestId(request))
         .timestamp(currentTimestamp());
     return ServerResponse

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/KafkaConsumerTests.java
Patch:
@@ -1,6 +1,6 @@
 package com.provectus.kafka.ui;
 
-import com.provectus.kafka.ui.model.TopicFormData;
+import com.provectus.kafka.ui.model.TopicCreation;
 import com.provectus.kafka.ui.model.TopicMessage;
 import com.provectus.kafka.ui.producer.KafkaTestProducer;
 import java.util.Map;
@@ -27,7 +27,7 @@ public void shouldDeleteRecords() {
     var topicName = UUID.randomUUID().toString();
     webTestClient.post()
         .uri("/api/clusters/{clusterName}/topics", LOCAL)
-        .bodyValue(new TopicFormData()
+        .bodyValue(new TopicCreation()
             .name(topicName)
             .partitions(1)
             .replicationFactor(1)

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/config/ReadOnlyModeFilter.java
Patch:
@@ -1,6 +1,6 @@
 package com.provectus.kafka.ui.config;
 
-import com.provectus.kafka.ui.exception.NotFoundException;
+import com.provectus.kafka.ui.exception.ClusterNotFoundException;
 import com.provectus.kafka.ui.exception.ReadOnlyModeException;
 import com.provectus.kafka.ui.service.ClustersStorage;
 import java.util.regex.Pattern;
@@ -39,7 +39,8 @@ public Mono<Void> filter(ServerWebExchange exchange, @NotNull WebFilterChain cha
     var clusterName = matcher.group("clusterName");
     var kafkaCluster = clustersStorage.getClusterByName(clusterName)
         .orElseThrow(
-            () -> new NotFoundException(String.format("No cluster for name '%s'", clusterName)));
+            () -> new ClusterNotFoundException(
+                String.format("No cluster for name '%s'", clusterName)));
 
     if (!kafkaCluster.getReadOnly()) {
       return chain.filter(exchange);

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/controller/ConsumerGroupsController.java
Patch:
@@ -32,7 +32,6 @@ public Mono<ResponseEntity<Flux<ConsumerGroup>>> getConsumerGroups(String cluste
     return clusterService.getConsumerGroups(clusterName)
         .map(Flux::fromIterable)
         .map(ResponseEntity::ok)
-        .switchIfEmpty(Mono.just(ResponseEntity.notFound()
-            .build())); // TODO: check behaviour on cluster not found and empty groups list
+        .switchIfEmpty(Mono.just(ResponseEntity.notFound().build()));
   }
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/KafkaService.java
Patch:
@@ -351,6 +351,7 @@ private Mono<String> incrementalAlterConfig(TopicFormData topicFormData, ConfigR
             .all(), topicCr.name());
   }
 
+  @SuppressWarnings("deprecation")
   private Mono<String> alterConfig(TopicFormData topicFormData, ConfigResource topicCr,
                                    ExtendedAdminClient ac) {
     List<ConfigEntry> configEntries = topicFormData.getConfigs().entrySet().stream()
@@ -359,7 +360,6 @@ private Mono<String> alterConfig(TopicFormData topicFormData, ConfigResource top
     Config config = new Config(configEntries);
     Map<ConfigResource, Config> map = Collections.singletonMap(topicCr, config);
     return ClusterUtil.toMono(ac.getAdminClient().alterConfigs(map).all(), topicCr.name());
-
   }
 
   private InternalTopic mergeWithStats(InternalTopic topic,

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/util/ClusterUtil.java
Patch:
@@ -158,9 +158,9 @@ public static InternalTopic mapToInternalTopic(TopicDescription topicDescription
     topic.inSyncReplicas(inSyncReplicasCount);
 
     topic.replicationFactor(
-        topicDescription.partitions().size() > 0
-            ? topicDescription.partitions().get(0).replicas().size()
-            : 0
+        topicDescription.partitions().isEmpty()
+            ? 0
+            : topicDescription.partitions().get(0).replicas().size()
     );
 
     topic.underReplicatedPartitions(urpCount);

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/config/ReadOnlyModeFilter.java
Patch:
@@ -1,7 +1,7 @@
 package com.provectus.kafka.ui.config;
 
 import com.provectus.kafka.ui.exception.NotFoundException;
-import com.provectus.kafka.ui.exception.ReadOnlyException;
+import com.provectus.kafka.ui.exception.ReadOnlyModeException;
 import com.provectus.kafka.ui.service.ClustersStorage;
 import java.util.regex.Pattern;
 import lombok.RequiredArgsConstructor;
@@ -45,6 +45,6 @@ public Mono<Void> filter(ServerWebExchange exchange, @NotNull WebFilterChain cha
       return chain.filter(exchange);
     }
 
-    return Mono.error(ReadOnlyException::new);
+    return Mono.error(ReadOnlyModeException::new);
   }
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/exception/CustomBaseException.java
Patch:
@@ -1,6 +1,5 @@
 package com.provectus.kafka.ui.exception;
 
-import org.springframework.http.HttpStatus;
 
 public abstract class CustomBaseException extends RuntimeException {
   public CustomBaseException() {
@@ -23,5 +22,5 @@ public CustomBaseException(String message, Throwable cause, boolean enableSuppre
     super(message, cause, enableSuppression, writableStackTrace);
   }
 
-  public abstract HttpStatus getResponseStatusCode();
+  public abstract ErrorCode getErrorCode();
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/exception/NotFoundException.java
Patch:
@@ -1,6 +1,5 @@
 package com.provectus.kafka.ui.exception;
 
-import org.springframework.http.HttpStatus;
 
 public class NotFoundException extends CustomBaseException {
 
@@ -9,7 +8,7 @@ public NotFoundException(String message) {
   }
 
   @Override
-  public HttpStatus getResponseStatusCode() {
-    return HttpStatus.NOT_FOUND;
+  public ErrorCode getErrorCode() {
+    return ErrorCode.ENTITY_NOT_FOUND;
   }
 }
\ No newline at end of file

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/exception/RebalanceInProgressException.java
Patch:
@@ -1,6 +1,5 @@
 package com.provectus.kafka.ui.exception;
 
-import org.springframework.http.HttpStatus;
 
 public class RebalanceInProgressException extends CustomBaseException {
 
@@ -9,7 +8,7 @@ public RebalanceInProgressException() {
   }
 
   @Override
-  public HttpStatus getResponseStatusCode() {
-    return HttpStatus.CONFLICT;
+  public ErrorCode getErrorCode() {
+    return ErrorCode.REBALANCE_IN_PROGRESS;
   }
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/exception/UnprocessableEntityException.java
Patch:
@@ -1,6 +1,5 @@
 package com.provectus.kafka.ui.exception;
 
-import org.springframework.http.HttpStatus;
 
 public class UnprocessableEntityException extends CustomBaseException {
 
@@ -9,7 +8,7 @@ public UnprocessableEntityException(String message) {
   }
 
   @Override
-  public HttpStatus getResponseStatusCode() {
-    return HttpStatus.UNPROCESSABLE_ENTITY;
+  public ErrorCode getErrorCode() {
+    return ErrorCode.UNPROCESSABLE_ENTITY;
   }
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/exception/ValidationException.java
Patch:
@@ -1,14 +1,13 @@
 package com.provectus.kafka.ui.exception;
 
-import org.springframework.http.HttpStatus;
 
 public class ValidationException extends CustomBaseException {
   public ValidationException(String message) {
     super(message);
   }
 
   @Override
-  public HttpStatus getResponseStatusCode() {
-    return HttpStatus.BAD_REQUEST;
+  public ErrorCode getErrorCode() {
+    return ErrorCode.VALIDATION_FAIL;
   }
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/ClusterService.java
Patch:
@@ -247,7 +247,7 @@ public Mono<Void> deleteTopicMessages(String clusterName, String topicName,
     if (!cluster.getTopics().containsKey(topicName)) {
       throw new NotFoundException("No such topic");
     }
-    return consumingService.loadOffsets(cluster, topicName, partitions)
+    return consumingService.offsetsForDeletion(cluster, topicName, partitions)
         .flatMap(offsets -> kafkaService.deleteTopicMessages(cluster, offsets));
   }
-}
+}
\ No newline at end of file

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/AbstractBaseTest.java
Patch:
@@ -1,5 +1,7 @@
 package com.provectus.kafka.ui;
 
+import com.provectus.kafka.ui.container.KafkaConnectContainer;
+import com.provectus.kafka.ui.container.SchemaRegistryContainer;
 import org.jetbrains.annotations.NotNull;
 import org.junit.jupiter.api.extension.ExtendWith;
 import org.springframework.boot.test.context.SpringBootTest;

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/container/KafkaConnectContainer.java
Patch:
@@ -1,4 +1,4 @@
-package com.provectus.kafka.ui;
+package com.provectus.kafka.ui.container;
 
 import org.testcontainers.containers.GenericContainer;
 import org.testcontainers.containers.KafkaContainer;

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/container/SchemaRegistryContainer.java
Patch:
@@ -1,4 +1,4 @@
-package com.provectus.kafka.ui;
+package com.provectus.kafka.ui.container;
 
 import org.testcontainers.containers.GenericContainer;
 import org.testcontainers.containers.KafkaContainer;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/cluster/mapper/ClusterMapper.java
Patch:
@@ -2,7 +2,8 @@
 
 import com.provectus.kafka.ui.cluster.config.ClustersProperties;
 import com.provectus.kafka.ui.cluster.model.*;
-import com.provectus.kafka.ui.cluster.model.InternalCompatibilityCheck;
+import com.provectus.kafka.ui.cluster.model.schemaregistry.InternalCompatibilityCheck;
+import com.provectus.kafka.ui.cluster.model.schemaregistry.InternalCompatibilityLevel;
 import com.provectus.kafka.ui.model.*;
 import java.util.Properties;
 import org.mapstruct.Mapper;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/cluster/model/schemaregistry/InternalCompatibilityCheck.java
Patch:
@@ -1,4 +1,4 @@
-package com.provectus.kafka.ui.cluster.model;
+package com.provectus.kafka.ui.cluster.model.schemaregistry;
 
 import com.fasterxml.jackson.annotation.JsonProperty;
 import lombok.Data;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/cluster/model/schemaregistry/InternalCompatibilityLevel.java
Patch:
@@ -1,4 +1,4 @@
-package com.provectus.kafka.ui.cluster.model;
+package com.provectus.kafka.ui.cluster.model.schemaregistry;
 
 import lombok.Data;
 

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/AbstractBaseTest.java
Patch:
@@ -18,7 +18,7 @@ public abstract class AbstractBaseTest {
     public static String LOCAL = "local";
     public static String SECOND_LOCAL = "secondLocal";
 
-    private static final String CONFLUENT_PLATFORM_VERSION = "5.2.1";
+    private static final String CONFLUENT_PLATFORM_VERSION = "5.5.0";
 
     public static final KafkaContainer kafka = new KafkaContainer(DockerImageName.parse("confluentinc/cp-kafka").withTag(CONFLUENT_PLATFORM_VERSION))
             .withNetwork(Network.SHARED);

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/KafkaConnectServiceTests.java
Patch:
@@ -249,7 +249,7 @@ public void shouldRetrieveConnectorPlugins() {
                 .exchange()
                 .expectStatus().isOk()
                 .expectBodyList(ConnectorPlugin.class)
-                .value(plugins -> assertEquals(13, plugins.size()));
+                .value(plugins -> assertEquals(14, plugins.size()));
     }
 
     @Test

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/cluster/mapper/ClusterMapper.java
Patch:
@@ -36,6 +36,7 @@ public interface ClusterMapper {
     TopicDetails toTopicDetails(InternalTopic topic);
     TopicConfig toTopicConfig(InternalTopicConfig topic);
     Replica toReplica(InternalReplica replica);
+    Connect toKafkaConnect(KafkaConnectCluster connect);
 
     @Mapping(target = "isCompatible", source = "compatible")
     CompatibilityCheckResponse toCompatibilityCheckResponse(InternalCompatibilityCheck dto);

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/cluster/model/KafkaCluster.java
Patch:
@@ -1,6 +1,8 @@
 package com.provectus.kafka.ui.cluster.model;
 
 import com.provectus.kafka.ui.model.ServerStatus;
+
+import java.util.List;
 import java.util.Properties;
 import lombok.Builder;
 import lombok.Data;
@@ -16,6 +18,7 @@ public class KafkaCluster {
     private final String bootstrapServers;
     private final String zookeeper;
     private final String schemaRegistry;
+    private final List<KafkaConnectCluster> kafkaConnect;
     private final String schemaNameTemplate;
     private final ServerStatus status;
     private final ServerStatus zookeeperStatus;

File: kafka-ui-api/src/test/java/com/provectus/kafka/ui/SchemaRegistryServiceTests.java
Patch:
@@ -41,7 +41,7 @@ public void should404WhenGetAllSchemasForUnknownCluster() {
     }
 
     @Test
-    void shouldReturn404WhenGetLatestSchemaByNonExistingSubject() {
+    public void shouldReturn404WhenGetLatestSchemaByNonExistingSubject() {
         String unknownSchema = "unknown-schema";
         webTestClient
                 .get()
@@ -51,7 +51,7 @@ void shouldReturn404WhenGetLatestSchemaByNonExistingSubject() {
     }
 
     @Test
-    void shouldReturnBackwardAsGlobalCompatibilityLevelByDefault() {
+    public void shouldReturnBackwardAsGlobalCompatibilityLevelByDefault() {
         webTestClient
                 .get()
                 .uri("http://localhost:8080/api/clusters/local/schemas/compatibility")

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/cluster/exception/GlobalErrorWebExceptionHandler.java
Patch:
@@ -38,7 +38,7 @@ protected RouterFunction<ServerResponse> getRoutingFunction(ErrorAttributes erro
     private Mono<ServerResponse> renderErrorResponse(ServerRequest request) {
         Map<String, Object> errorAttributes = getErrorAttributes(request, false);
         HttpStatus statusCode = Optional.ofNullable(errorAttributes.get(GlobalErrorAttributes.STATUS))
-                .map(code -> (HttpStatus) code)
+                .map(code -> code instanceof Integer ? HttpStatus.valueOf((Integer) code) : (HttpStatus) code)
                 .orElse(HttpStatus.BAD_REQUEST);
         return ServerResponse
                 .status(statusCode)

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/cluster/service/ClusterService.java
Patch:
@@ -68,6 +68,7 @@ public List<Topic> getTopics(String name) {
                 .map(c ->
                         c.getTopics().values().stream()
                                 .map(clusterMapper::toTopic)
+                                .sorted(Comparator.comparing(Topic::getName))
                                 .collect(Collectors.toList())
                 ).orElse(Collections.emptyList());
     }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/cluster/mapper/ClusterMapper.java
Patch:
@@ -64,7 +64,7 @@ default BrokerDiskUsage map(Integer id, InternalBrokerDiskUsage internalBrokerDi
      }
 
      default BigDecimal sumMetrics(Map<String, BigDecimal> metrics) {
-         if (metrics == null) {
+         if (metrics != null) {
            return metrics.values().stream().reduce(BigDecimal.ZERO, BigDecimal::add);
          } else {
            return BigDecimal.ZERO;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/cluster/service/ConsumingService.java
Patch:
@@ -104,7 +104,7 @@ public void emit(FluxSink<ConsumerRecord<Bytes, Bytes>> sink) {
 				while (!sink.isCancelled()) {
 					ConsumerRecords<Bytes, Bytes> records = consumer.poll(POLL_TIMEOUT_MS);
 					log.info("{} records polled", records.count());
-					if (records.count() == 0 && emptyPollsCount < MAX_EMPTY_POLLS_COUNT) {
+					if (records.count() == 0 && emptyPollsCount > MAX_EMPTY_POLLS_COUNT) {
 						break;
 					} else {
 						emptyPollsCount++;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/cluster/config/ClustersProperties.java
Patch:
@@ -1,5 +1,6 @@
 package com.provectus.kafka.ui.cluster.config;
 
+import java.util.Properties;
 import lombok.Data;
 import org.springframework.boot.context.properties.ConfigurationProperties;
 import org.springframework.context.annotation.Configuration;
@@ -24,5 +25,6 @@ public static class Cluster {
         String protobufFile;
         String protobufMessageName;
         int jmxPort;
+        Properties properties;
     }
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/cluster/deserialization/SchemaRegistryRecordDeserializer.java
Patch:
@@ -97,7 +97,8 @@ private MessageFormat detectFormat(ConsumerRecord<Bytes, Bytes> record) {
 				final List<Integer> versions = schemaRegistryClient.getAllVersions(schemaName);
 				if (!versions.isEmpty()) {
 					final Integer version = versions.iterator().next();
-					final Schema schema = schemaRegistryClient.getByVersion(record.topic(), version, false);
+					final String subjectName = String.format(cluster.getSchemaNameTemplate(), record.topic());
+					final Schema schema = schemaRegistryClient.getByVersion(subjectName, version, false);
 					if (schema.getSchemaType().equals(MessageFormat.PROTOBUF.name())) {
 						try {
 							protobufDeserializer.deserialize(record.topic(), record.value().get());

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/cluster/model/KafkaCluster.java
Patch:
@@ -1,6 +1,7 @@
 package com.provectus.kafka.ui.cluster.model;
 
 import com.provectus.kafka.ui.model.ServerStatus;
+import java.util.Properties;
 import lombok.Builder;
 import lombok.Data;
 
@@ -24,4 +25,5 @@ public class KafkaCluster {
     private final Throwable lastZookeeperException;
     private final Path protobufFile;
     private final String protobufMessageName;
+    private final Properties properties;
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/cluster/service/ClusterService.java
Patch:
@@ -128,6 +128,7 @@ public Mono<Map<TopicPartition, OffsetAndMetadata>> groupMetadata(KafkaCluster c
 
     public Map<TopicPartition, Long> topicPartitionsEndOffsets(KafkaCluster cluster, Collection<TopicPartition> topicPartitions) {
         Properties properties = new Properties();
+        properties.putAll(cluster.getProperties());
         properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, cluster.getBootstrapServers());
         properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
         properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/kafka/KafkaService.java
Patch:
@@ -209,6 +209,7 @@ public Mono<ExtendedAdminClient> getOrCreateAdminClient(KafkaCluster cluster) {
 
     public Mono<ExtendedAdminClient> createAdminClient(KafkaCluster kafkaCluster) {
         Properties properties = new Properties();
+        properties.putAll(kafkaCluster.getProperties());
         properties.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaCluster.getBootstrapServers());
         properties.put(AdminClientConfig.REQUEST_TIMEOUT_MS_CONFIG, clientTimeout);
         AdminClient adminClient = AdminClient.create(properties);
@@ -245,10 +246,12 @@ public Mono<List<ConsumerGroup>> getConsumerGroups(KafkaCluster cluster) {
 
     public KafkaConsumer<Bytes, Bytes> createConsumer(KafkaCluster cluster) {
         Properties props = new Properties();
+        props.putAll(cluster.getProperties());
         props.put(ConsumerConfig.CLIENT_ID_CONFIG, "kafka-ui");
         props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, cluster.getBootstrapServers());
         props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, BytesDeserializer.class);
         props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, BytesDeserializer.class);
+        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
 
         return new KafkaConsumer<>(props);
     }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/cluster/service/ClusterService.java
Patch:
@@ -150,7 +150,7 @@ public Flux<Broker> getBrokers (String clusterName) {
                 .flatMap(client -> ClusterUtil.toMono(client.getAdminClient().describeCluster().nodes())
                     .map(n -> n.stream().map(node -> {
                         Broker broker = new Broker();
-                        broker.setId(node.idString());
+                        broker.setId(node.id());
                         broker.setHost(node.host());
                         return broker;
                     }).collect(Collectors.toList())))

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/cluster/config/ClustersProperties.java
Patch:
@@ -21,6 +21,8 @@ public static class Cluster {
         String zookeeper;
         String schemaRegistry;
         String schemaNameTemplate = "%s-value";
+        String protobufFile;
+        String protobufMessageName;
         int jmxPort;
     }
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/cluster/model/KafkaCluster.java
Patch:
@@ -4,12 +4,12 @@
 import lombok.Builder;
 import lombok.Data;
 
+import java.nio.file.Path;
 import java.util.Map;
 
 @Data
 @Builder(toBuilder = true)
 public class KafkaCluster {
-
     private final String name;
     private final Integer jmxPort;
     private final String bootstrapServers;
@@ -22,4 +22,6 @@ public class KafkaCluster {
     private final Map<String, InternalTopic> topics;
     private final Throwable lastKafkaException;
     private final Throwable lastZookeeperException;
+    private final Path protobufFile;
+    private final String protobufMessageName;
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/kafka/KafkaConstants.java
Patch:
@@ -10,7 +10,7 @@ public final class KafkaConstants {
     private KafkaConstants() {
     }
 
-    public static Map<String, String> TOPIC_DEFAULT_CONFIGS = Map.ofEntries(
+    public static final Map<String, String> TOPIC_DEFAULT_CONFIGS = Map.ofEntries(
             new AbstractMap.SimpleEntry<>(CLEANUP_POLICY_CONFIG, CLEANUP_POLICY_DELETE),
             new AbstractMap.SimpleEntry<>(COMPRESSION_TYPE_CONFIG, "producer"),
             new AbstractMap.SimpleEntry<>(DELETE_RETENTION_MS_CONFIG, "86400000"),

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/cluster/model/InternalBrokerMetrics.java
Patch:
@@ -9,6 +9,5 @@
 @Data
 @Builder(toBuilder = true)
 public class InternalBrokerMetrics {
-    private final Long segmentSize;
     private final List<Metric> metrics;
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/cluster/model/InternalTopic.java
Patch:
@@ -23,5 +23,4 @@ public class InternalTopic {
     private final int underReplicatedPartitions;
     private final long segmentSize;
     private final long segmentCount;
-//    private final Map<TopicPartition, Long> partitionSegmentSize;
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/cluster/model/MetricDto.java
Patch:
@@ -4,11 +4,13 @@
 import lombok.Getter;
 
 import java.math.BigDecimal;
+import java.util.Map;
 
 @Getter
 @AllArgsConstructor
 public class MetricDto {
     private String canonicalName;
     private String metricName;
+    private Map<String,String> params;
     private BigDecimal value;
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/cluster/util/JmxMetricsName.java
Patch:
@@ -1,6 +1,6 @@
 package com.provectus.kafka.ui.cluster.util;
 
-public enum JmxMetricsNames {
+public enum JmxMetricsName {
     MessagesInPerSec,
     BytesInPerSec,
     ReplicationBytesInPerSec,

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/cluster/model/InternalBrokerMetrics.java
Patch:
@@ -10,5 +10,5 @@
 @Builder(toBuilder = true)
 public class InternalBrokerMetrics {
     private final Long segmentSize;
-    private final List<Metric> jmxMetrics;
+    private final List<Metric> metrics;
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/cluster/model/InternalPartition.java
Patch:
@@ -6,11 +6,13 @@
 import java.util.List;
 
 @Data
-@Builder
+@Builder(toBuilder = true)
 public class InternalPartition {
     private final int partition;
     private final Integer leader;
     private final List<InternalReplica> replicas;
     private final int inSyncReplicasCount;
     private final int replicasCount;
+    private final long offsetMin;
+    private final long offsetMax;
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/cluster/model/InternalTopic.java
Patch:
@@ -1,6 +1,5 @@
 package com.provectus.kafka.ui.cluster.model;
 
-import com.provectus.kafka.ui.model.TopicPartitionDto;
 import lombok.Builder;
 import lombok.Data;
 import org.apache.kafka.common.TopicPartition;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/cluster/model/KafkaCluster.java
Patch:
@@ -11,7 +11,7 @@
 public class KafkaCluster {
 
     private final String name;
-    private final int jmxPort;
+    private final Integer jmxPort;
     private final String bootstrapServers;
     private final String zookeeper;
     private final String schemaRegistry;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/cluster/util/JmxClusterUtil.java
Patch:
@@ -100,7 +100,7 @@ private void closeConnectionExceptionally(String url, JMXConnector srv) {
     public List<MetricDto> convertToMetricDto(InternalClusterMetrics internalClusterMetrics) {
         return internalClusterMetrics.getInternalBrokerMetrics().values().stream()
                 .map(c ->
-                        c.getJmxMetrics().stream()
+                        c.getMetrics().stream()
                                 .filter(j -> isSameMetric(j.getCanonicalName()))
                                 .map(j -> j.getValue().entrySet().stream()
                                         .map(e -> new MetricDto(j.getCanonicalName(), e.getKey(), e.getValue()))))

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/cluster/model/InternalClusterMetrics.java
Patch:
@@ -1,9 +1,10 @@
 package com.provectus.kafka.ui.cluster.model;
 
+import com.provectus.kafka.ui.model.Metric;
 import lombok.Builder;
 import lombok.Data;
 
-import java.math.BigDecimal;
+import java.util.List;
 import java.util.Map;
 
 
@@ -24,5 +25,6 @@ public class InternalClusterMetrics {
     private final int segmentCount;
     private final long segmentSize;
     private final Map<Integer, InternalBrokerMetrics> internalBrokerMetrics;
+    private final List<Metric> metrics;
     private final int zooKeeperStatus;
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/cluster/model/InternalTopic.java
Patch:
@@ -22,7 +22,6 @@ public class InternalTopic {
     private final int inSyncReplicas;
     private final int replicationFactor;
     private final int underReplicatedPartitions;
-    //TODO: find way to fill
     private final long segmentSize;
     private final int segmentCount;
     private final Map<TopicPartition, Long> partitionSegmentSize;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/zookeeper/ZookeeperService.java
Patch:
@@ -1,6 +1,5 @@
 package com.provectus.kafka.ui.zookeeper;
 
-import com.provectus.kafka.ui.cluster.model.ClustersStorage;
 import com.provectus.kafka.ui.cluster.model.KafkaCluster;
 import lombok.RequiredArgsConstructor;
 import lombok.extern.log4j.Log4j2;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/cluster/model/InternalTopic.java
Patch:
@@ -1,5 +1,6 @@
 package com.provectus.kafka.ui.cluster.model;
 
+import com.provectus.kafka.ui.model.TopicPartitionDto;
 import lombok.Builder;
 import lombok.Data;
 import org.apache.kafka.common.TopicPartition;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/rest/MetricsRestController.java
Patch:
@@ -5,7 +5,6 @@
 import com.provectus.kafka.ui.cluster.service.ClusterService;
 import com.provectus.kafka.ui.model.*;
 import lombok.RequiredArgsConstructor;
-
 import org.apache.commons.lang3.tuple.Pair;
 import org.springframework.http.HttpStatus;
 import org.springframework.http.ResponseEntity;
@@ -14,12 +13,11 @@
 import reactor.core.publisher.Flux;
 import reactor.core.publisher.Mono;
 
+import javax.validation.Valid;
 import java.util.Collections;
 import java.util.List;
 import java.util.function.Function;
 
-import javax.validation.Valid;
-
 @RestController
 @RequiredArgsConstructor
 public class MetricsRestController implements ApiClustersApi {

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/cluster/model/InternalClusterMetrics.java
Patch:
@@ -19,8 +19,8 @@ public class InternalClusterMetrics {
     private final int offlinePartitionCount;
     private final int inSyncReplicasCount;
     private final int outOfSyncReplicasCount;
-    private final Map<String, BigDecimal> bytesInPerSec;
-    private final Map<String, BigDecimal> bytesOutPerSec;
+    private final Map<String, Number> bytesInPerSec;
+    private final Map<String, Number> bytesOutPerSec;
     private final int segmentCount;
     private final long segmentSize;
     private final Map<Integer, InternalBrokerMetrics> internalBrokerMetrics;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/kafka/KafkaService.java
Patch:
@@ -166,8 +166,8 @@ private Mono<InternalClusterMetrics> getClusterMetrics(KafkaCluster cluster, Adm
                         c -> {
                             InternalClusterMetrics.InternalClusterMetricsBuilder metricsBuilder = InternalClusterMetrics.builder();
                             metricsBuilder.brokerCount(brokers.size()).activeControllers(c != null ? 1 : 0);
-                            Map<String, BigDecimal> bytesInPerSec = jmxClusterUtil.getJmxTrafficMetrics(cluster.getJmxPort(), c.host(), JmxClusterUtil.BYTES_IN_PER_SEC);
-                            Map<String, BigDecimal> bytesOutPerSec = jmxClusterUtil.getJmxTrafficMetrics(cluster.getJmxPort(), c.host(), JmxClusterUtil.BYTES_OUT_PER_SEC);
+                            Map<String, Number> bytesInPerSec = jmxClusterUtil.getJmxTrafficMetrics(cluster.getJmxPort(), c.host(), JmxClusterUtil.BYTES_IN_PER_SEC);
+                            Map<String, Number> bytesOutPerSec = jmxClusterUtil.getJmxTrafficMetrics(cluster.getJmxPort(), c.host(), JmxClusterUtil.BYTES_OUT_PER_SEC);
                             metricsBuilder
                                     .internalBrokerMetrics((brokers.stream().map(Node::id).collect(Collectors.toMap(k -> k, v -> InternalBrokerMetrics.builder().build()))))
                                     .bytesOutPerSec(bytesOutPerSec)

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/cluster/config/ClustersProperties.java
Patch:
@@ -21,5 +21,6 @@ public static class Cluster {
         String zookeeper;
         String schemaRegistry;
         String schemaNameTemplate = "%s-value";
+        int jmxPort;
     }
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/cluster/model/KafkaCluster.java
Patch:
@@ -11,8 +11,7 @@
 public class KafkaCluster {
 
     private final String name;
-    private final String jmxHost;
-    private final String jmxPort;
+    private final int jmxPort;
     private final String bootstrapServers;
     private final String zookeeper;
     private final String schemaRegistry;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/cluster/service/ClusterService.java
Patch:
@@ -9,8 +9,6 @@
 import com.provectus.kafka.ui.model.*;
 import lombok.RequiredArgsConstructor;
 import lombok.SneakyThrows;
-
-import org.apache.kafka.clients.admin.ConsumerGroupListing;
 import org.apache.kafka.clients.consumer.ConsumerConfig;
 import org.apache.kafka.clients.consumer.KafkaConsumer;
 import org.apache.kafka.clients.consumer.OffsetAndMetadata;
@@ -20,7 +18,6 @@
 import reactor.core.publisher.Flux;
 import reactor.core.publisher.Mono;
 
-import java.time.OffsetDateTime;
 import java.util.*;
 import java.util.stream.Collectors;
 import java.util.stream.Stream;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/cluster/service/ConsumingService.java
Patch:
@@ -112,7 +112,7 @@ private void seekOffsets(KafkaConsumer<Bytes, Bytes> consumer) {
 							.collect(Collectors.toMap(
 									partitionPosition -> new TopicPartition(topic, partitionPosition.getKey()),
 									Map.Entry::getValue
-							));  
+							));
 					consumer.offsetsForTimes(timestampsToSearch)
 							.forEach((topicPartition, offsetAndTimestamp) ->
 									consumer.seek(topicPartition, offsetAndTimestamp.offset())

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/cluster/service/MetricsUpdateService.java
Patch:
@@ -2,7 +2,6 @@
 
 import com.provectus.kafka.ui.cluster.model.KafkaCluster;
 import com.provectus.kafka.ui.kafka.KafkaService;
-import com.provectus.kafka.ui.zookeeper.ZookeeperService;
 import lombok.RequiredArgsConstructor;
 import lombok.extern.log4j.Log4j2;
 import org.springframework.stereotype.Service;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/cluster/service/ConsumingService.java
Patch:
@@ -68,12 +68,13 @@ public void emit(FluxSink<ConsumerRecord<Bytes, Bytes>> sink) {
 				assignPartitions(consumer);
 				seekOffsets(consumer);
 				int pollsCount = 0;
-				while (!sink.isCancelled() || ++pollsCount > MAX_POLLS_COUNT) {
+				while (!sink.isCancelled() && ++pollsCount < MAX_POLLS_COUNT) {
 					ConsumerRecords<Bytes, Bytes> records = consumer.poll(POLL_TIMEOUT_MS);
 					log.info("{} records polled", records.count());
 					records.iterator()
 							.forEachRemaining(sink::next);
 				}
+				sink.complete();
 			} catch (Exception e) {
 				log.error("Error occurred while consuming records", e);
 				throw new RuntimeException(e);

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/cluster/config/ClustersProperties.java
Patch:
@@ -19,5 +19,7 @@ public static class Cluster {
         String name;
         String bootstrapServers;
         String zookeeper;
+        String schemaRegistry;
+        String schemaNameTemplate = "%s-value";
     }
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/cluster/model/KafkaCluster.java
Patch:
@@ -15,11 +15,12 @@ public class KafkaCluster {
     private final String jmxPort;
     private final String bootstrapServers;
     private final String zookeeper;
+    private final String schemaRegistry;
+    private final String schemaNameTemplate;
     private final ServerStatus status;
     private final ServerStatus zookeeperStatus;
     private final InternalClusterMetrics metrics;
     private final Map<String, InternalTopic> topics;
     private final Throwable lastKafkaException;
     private final Throwable lastZookeeperException;
-
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/kafka/KafkaService.java
Patch:
@@ -53,7 +53,7 @@ public Mono<KafkaCluster> getUpdatedCluster(KafkaCluster cluster) {
                             getTopicsData(ac.getAdminClient()).flatMap( topics ->
                                 loadTopicsConfig(ac.getAdminClient(), topics.stream().map(InternalTopic::getName).collect(Collectors.toList()))
                                         .map( configs -> mergeWithConfigs(topics, configs))
-                                    .flatMap(it -> updateSegmentMetrics(ac, clusterMetrics, it))
+                                    .flatMap(it -> updateSegmentMetrics(ac.getAdminClient(), clusterMetrics, it))
                             ).map( segmentSizeDto -> buildFromData(cluster, segmentSizeDto))
                         )
         ).onErrorResume(

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/cluster/mapper/ClusterMapper.java
Patch:
@@ -8,6 +8,7 @@
 import com.provectus.kafka.ui.model.*;
 import org.mapstruct.Mapper;
 import org.mapstruct.Mapping;
+import org.mapstruct.ValueMapping;
 
 @Mapper(componentModel = "spring")
 public interface ClusterMapper {

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/cluster/model/InternalClusterMetrics.java
Patch:
@@ -1,5 +1,6 @@
 package com.provectus.kafka.ui.cluster.model;
 
+import com.provectus.kafka.ui.model.ServerStatus;
 import lombok.Builder;
 import lombok.Data;
 
@@ -22,4 +23,5 @@ public class InternalClusterMetrics {
     //TODO: find way to fill
     private final int segmentSize;
     private final int segmentCount;
+    private final int zooKeeperStatus;
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/cluster/ClustersMetricsScheduler.java
Patch:
@@ -26,7 +26,7 @@ public void updateMetrics() {
                 .subscribeOn(Schedulers.parallel())
                 .map(Map.Entry::getValue)
                 .flatMap(metricsUpdateService::updateMetrics)
-                .doOnNext(s -> clustersStorage.setKafkaCluster(s.getId(), s))
+                .doOnNext(s -> clustersStorage.setKafkaCluster(s.getName(), s))
                 .subscribe();
     }
 }

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/cluster/model/KafkaCluster.java
Patch:
@@ -10,7 +10,6 @@
 @Builder(toBuilder = true)
 public class KafkaCluster {
 
-    private final String id = "";
     private final String name;
     private final String jmxHost;
     private final String jmxPort;

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/cluster/util/ClusterUtil.java
Patch:
@@ -28,7 +28,6 @@ public static <T> Mono<T> toMono(KafkaFuture<T> future){
 
     public static ConsumerGroup convertToConsumerGroup(ConsumerGroupDescription c, KafkaCluster cluster) {
         ConsumerGroup consumerGroup = new ConsumerGroup();
-        consumerGroup.setClusterId(cluster.getId());
         consumerGroup.setConsumerGroupId(c.groupId());
         consumerGroup.setNumConsumers(c.members().size());
         int numTopics = c.members().stream().mapToInt( m -> m.assignment().topicPartitions().size()).sum();

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/kafka/KafkaService.java
Patch:
@@ -179,7 +179,7 @@ private Mono<String> getClusterId(AdminClient adminClient) {
 
     public Mono<AdminClient> getOrCreateAdminClient(KafkaCluster cluster) {
         AdminClient adminClient = adminClientCache.computeIfAbsent(
-                cluster.getId(),
+                cluster.getName(),
                 (id) -> createAdminClient(cluster)
         );
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/rest/config/CustomWebFilter.java
Patch:
@@ -10,7 +10,7 @@
 public class CustomWebFilter implements WebFilter {
     @Override
     public Mono<Void> filter(ServerWebExchange exchange, WebFilterChain chain) {
-        if (exchange.getRequest().getURI().getPath().equals("/")) {
+        if (exchange.getRequest().getURI().getPath().equals("/") || exchange.getRequest().getURI().getPath().startsWith("/ui")) {
             return chain.filter(exchange.mutate().request(exchange.getRequest().mutate().path("/index.html").build()).build());
         }
 

File: kafka-ui-api/src/main/java/com/provectus/kafka/ui/kafka/KafkaService.java
Patch:
@@ -84,6 +84,7 @@ private boolean createAdminClient(KafkaCluster kafkaCluster) {
             properties.put(AdminClientConfig.REQUEST_TIMEOUT_MS_CONFIG, 5000);
             kafkaCluster.setAdminClient(AdminClient.create(properties));
             kafkaCluster.setId(getClusterId(kafkaCluster));
+            kafkaCluster.getCluster().setId(kafkaCluster.getId());
 
             return true;
         } catch (Exception e) {

