File: metadata-io/src/main/java/com/linkedin/metadata/search/elasticsearch/query/ESBrowseDAO.java
Patch:
@@ -642,7 +642,7 @@ private QueryBuilder buildQueryStringV2(
     EntitySpec entitySpec = opContext.getEntityRegistry().getEntitySpec(entityName);
     QueryBuilder query =
         SearchRequestHandler.getBuilder(
-                opContext.getEntityRegistry(),
+                opContext,
                 entitySpec,
                 searchConfiguration,
                 customSearchConfiguration,
@@ -683,7 +683,7 @@ private QueryBuilder buildQueryStringBrowseAcrossEntities(
 
     QueryBuilder query =
         SearchRequestHandler.getBuilder(
-                finalOpContext.getEntityRegistry(),
+                finalOpContext,
                 entitySpecs,
                 searchConfiguration,
                 customSearchConfiguration,

File: metadata-service/openapi-servlet/src/main/java/io/datahubproject/openapi/config/SpringWebConfig.java
Patch:
@@ -28,7 +28,7 @@
 @EnableWebMvc
 @OpenAPIDefinition(
     info = @Info(title = "DataHub OpenAPI", version = "2.0.0"),
-    servers = {@Server(url = "/openapi/", description = "Default Server URL")})
+    servers = {@Server(url = "/", description = "Default Server URL")})
 @Order(2)
 @Configuration
 public class SpringWebConfig implements WebMvcConfigurer {

File: metadata-service/openapi-servlet/src/main/java/io/datahubproject/openapi/openlineage/controller/LineageApiImpl.java
Patch:
@@ -27,7 +27,7 @@
 import org.springframework.web.bind.annotation.RestController;
 
 @RestController
-@RequestMapping("/openlineage/api/v1")
+@RequestMapping("/openapi/openlineage/api/v1")
 @Slf4j
 public class LineageApiImpl implements LineageApi {
   private static final ObjectMapper OBJECT_MAPPER = OpenLineageClientUtils.newObjectMapper();

File: metadata-service/openapi-servlet/src/main/java/io/datahubproject/openapi/v2/controller/PlatformEntitiesController.java
Patch:
@@ -39,7 +39,7 @@
 
 @RestController
 @RequiredArgsConstructor
-@RequestMapping("/openapiv2/platform/entities/v1")
+@RequestMapping("/openapi/v2/platform/entities/v1")
 @Slf4j
 @Tag(
     name = "Platform Entities",

File: metadata-io/src/main/java/com/linkedin/metadata/search/utils/ESUtils.java
Patch:
@@ -503,6 +503,8 @@ public static String toKeywordField(
 
     return skipKeywordSuffix
             || KEYWORD_FIELDS.contains(fieldName)
+            || KEYWORD_FIELDS.stream()
+                .anyMatch(nestedField -> fieldName.endsWith("." + nestedField))
             || PATH_HIERARCHY_FIELDS.contains(fieldName)
             || SUBFIELDS.stream().anyMatch(subfield -> fieldName.endsWith("." + subfield))
         ? fieldName

File: metadata-io/src/main/java/com/linkedin/metadata/search/utils/ESUtils.java
Patch:
@@ -874,7 +874,7 @@ private static void filterSoftDeletedByDefault(
                                         || criterion.getField().equals(REMOVED + KEYWORD_SUFFIX)));
       }
       if (!removedInOrFilter) {
-        filterQuery.mustNot(QueryBuilders.matchQuery(REMOVED, true));
+        filterQuery.mustNot(QueryBuilders.termQuery(REMOVED, true));
       }
     }
   }

File: metadata-io/src/test/java/com/linkedin/metadata/search/query/request/AutocompleteRequestHandlerTest.java
Patch:
@@ -158,7 +158,7 @@ public void testDefaultAutocompleteRequest() {
     assertEquals("keyPart1.delimited", prefixQuery.fieldName());
 
     assertEquals(wrapper.mustNot().size(), 1);
-    MatchQueryBuilder removedFilter = (MatchQueryBuilder) wrapper.mustNot().get(0);
+    TermQueryBuilder removedFilter = (TermQueryBuilder) wrapper.mustNot().get(0);
     assertEquals(removedFilter.fieldName(), "removed");
     assertEquals(removedFilter.value(), true);
     HighlightBuilder highlightBuilder = sourceBuilder.highlighter();
@@ -203,7 +203,7 @@ public void testAutocompleteRequestWithField() {
         (MatchPhrasePrefixQueryBuilder) query.should().get(1);
     assertEquals("field.delimited", prefixQuery.fieldName());
 
-    MatchQueryBuilder removedFilter = (MatchQueryBuilder) wrapper.mustNot().get(0);
+    TermQueryBuilder removedFilter = (TermQueryBuilder) wrapper.mustNot().get(0);
     assertEquals(removedFilter.fieldName(), "removed");
     assertEquals(removedFilter.value(), true);
     HighlightBuilder highlightBuilder = sourceBuilder.highlighter();

File: metadata-io/src/test/java/com/linkedin/metadata/search/query/request/SearchRequestHandlerTest.java
Patch:
@@ -436,7 +436,7 @@ private BoolQueryBuilder constructFilterQuery(
   }
 
   private void testFilterQuery(BoolQueryBuilder testQuery) {
-    Optional<MatchQueryBuilder> mustNotHaveRemovedCondition =
+    Optional<TermQueryBuilder> mustNotHaveRemovedCondition =
         testQuery.filter().stream()
             .filter(or -> or instanceof BoolQueryBuilder)
             .map(or -> (BoolQueryBuilder) or)
@@ -445,8 +445,8 @@ private void testFilterQuery(BoolQueryBuilder testQuery) {
                   System.out.println("processing: " + or.mustNot());
                   return or.mustNot().stream();
                 })
-            .filter(and -> and instanceof MatchQueryBuilder)
-            .map(and -> (MatchQueryBuilder) and)
+            .filter(and -> and instanceof TermQueryBuilder)
+            .map(and -> (TermQueryBuilder) and)
             .filter(match -> match.fieldName().equals("removed"))
             .findAny();
 

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/entitytype/EntityTypeUrnMapper.java
Patch:
@@ -77,9 +77,6 @@ public class EntityTypeUrnMapper {
           .put(
               Constants.BUSINESS_ATTRIBUTE_ENTITY_NAME,
               "urn:li:entityType:datahub.businessAttribute")
-          .put(
-              Constants.DATA_PROCESS_INSTANCE_ENTITY_NAME,
-              "urn:li:entityType:datahub.dataProcessInstance")
           .build();
 
   private static final Map<String, String> ENTITY_TYPE_URN_TO_NAME =

File: metadata-io/src/main/java/com/linkedin/metadata/client/JavaEntityClient.java
Patch:
@@ -775,7 +775,8 @@ public List<String> batchIngestProposals(
 
     List<String> updatedUrns = new ArrayList<>();
     Iterators.partition(
-            metadataChangeProposals.iterator(), Math.max(1, entityClientConfig.getBatchGetV2Size()))
+            metadataChangeProposals.iterator(),
+            Math.max(1, entityClientConfig.getBatchIngestSize()))
         .forEachRemaining(
             batch -> {
               AspectsBatch aspectsBatch =

File: metadata-integration/java/acryl-spark-lineage/src/main/java/datahub/spark/conf/SparkLineageConf.java
Patch:
@@ -17,6 +17,7 @@ public class SparkLineageConf {
   final DatahubOpenlineageConfig openLineageConf;
   @Builder.Default final boolean coalesceEnabled = true;
   @Builder.Default final boolean emitCoalescePeriodically = false;
+  @Builder.Default final boolean logMcps = true;
   final SparkAppContext sparkAppContext;
   final DatahubEmitterConfig datahubEmitterConfig;
   @Builder.Default final List<String> tags = new LinkedList<>();
@@ -32,6 +33,7 @@ public static SparkLineageConf toSparkLineageConf(
         SparkConfigParser.sparkConfigToDatahubOpenlineageConf(sparkConfig, sparkAppContext);
     builder.openLineageConf(datahubOpenlineageConfig);
     builder.coalesceEnabled(SparkConfigParser.isCoalesceEnabled(sparkConfig));
+    builder.logMcps(SparkConfigParser.isLogMcps(sparkConfig));
     if (SparkConfigParser.getTags(sparkConfig) != null) {
       builder.tags(Arrays.asList(Objects.requireNonNull(SparkConfigParser.getTags(sparkConfig))));
     }

File: metadata-integration/java/openlineage-converter/src/main/java/io/datahubproject/openlineage/config/DatahubOpenlineageConfig.java
Patch:
@@ -16,6 +16,7 @@
 @Getter
 @ToString
 public class DatahubOpenlineageConfig {
+  @Builder.Default private final boolean isSpark = false;
   @Builder.Default private final boolean isStreaming = false;
   @Builder.Default private final String pipelineName = null;
   private final String platformInstance;
@@ -34,6 +35,7 @@ public class DatahubOpenlineageConfig {
   @Builder.Default private Map<String, String> urnAliases = new HashMap<>();
   @Builder.Default private final boolean disableSymlinkResolution = false;
   @Builder.Default private final boolean lowerCaseDatasetUrns = false;
+  @Builder.Default private final boolean removeLegacyLineage = false;
 
   public List<PathSpec> getPathSpecsForPlatform(String platform) {
     if ((pathSpecs == null) || (pathSpecs.isEmpty())) {

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/analytics/resolver/GetChartsResolver.java
Patch:
@@ -125,7 +125,7 @@ private AnalyticsChart getTopUsersChart(OperationContext opContext) {
       final DateRange trailingMonthDateRange = dateUtil.getTrailingMonthDateRange();
       final List<String> columns = ImmutableList.of("Name", "Title", "Email");
 
-      final String topUsersTitle = "Top Users";
+      final String topUsersTitle = "Top Users (Last 30 Days)";
       final List<Row> topUserRows =
           _analyticsService.getTopNTableChart(
               _analyticsService.getUsageIndexName(),
@@ -198,7 +198,7 @@ private Row buildNewUsersRow(@Nonnull final SearchEntity entity) {
   private AnalyticsChart getNewUsersChart(OperationContext opContext) {
     try {
       final List<String> columns = ImmutableList.of("Name", "Title", "Email");
-      final String newUsersTitle = "New Users";
+      final String newUsersTitle = "Active Users (Last 30 Days)";
       final SearchResult result = searchForNewUsers(opContext);
       final List<Row> newUserRows = new ArrayList<>();
       for (SearchEntity entity : result.getEntities()) {

File: metadata-service/configuration/src/main/java/com/linkedin/metadata/config/EbeanConfiguration.java
Patch:
@@ -23,6 +23,7 @@ public class EbeanConfiguration {
   private boolean autoCreateDdl;
   private boolean postgresUseIamAuth;
   private LockingConfiguration locking;
+  private String batchGetMethod;
 
   public static final EbeanConfiguration testDefault =
       EbeanConfiguration.builder().locking(LockingConfiguration.testDefault).build();

File: entity-registry/src/main/java/com/linkedin/metadata/aspect/patch/builder/UpstreamLineagePatchBuilder.java
Patch:
@@ -142,7 +142,7 @@ public UpstreamLineagePatchBuilder removeFineGrainedUpstreamField(
             FINE_GRAINED_PATH_START
                 + transformationOperation
                 + "/"
-                + downstreamSchemaField
+                + encodeValueUrn(downstreamSchemaField)
                 + "/"
                 + finalQueryUrn
                 + "/"

File: entity-registry/src/main/java/com/linkedin/metadata/aspect/patch/template/TemplateUtil.java
Patch:
@@ -84,7 +84,7 @@ public static JsonNode populateTopLevelKeys(JsonNode transformedNode, JsonPatch
       // Skip first as it will always be blank due to path starting with /
       for (int i = 1; i < endIdx; i++) {
         String decodedKey = decodeValue(keys[i]);
-        if (parent.get(keys[i]) == null) {
+        if (parent.get(decodedKey) == null) {
           ((ObjectNode) parent).set(decodedKey, instance.objectNode());
         }
         parent = parent.get(decodedKey);

File: entity-registry/src/testFixtures/java/com/linkedin/test/metadata/aspect/batch/TestMCP.java
Patch:
@@ -21,6 +21,7 @@
 import com.linkedin.test.metadata.aspect.TestEntityRegistry;
 import java.net.URISyntaxException;
 import java.util.Collection;
+import java.util.Collections;
 import java.util.Map;
 import java.util.Objects;
 import java.util.Optional;
@@ -140,7 +141,7 @@ public Map<String, String> getHeaders() {
             mcp ->
                 mcp.getHeaders().entrySet().stream()
                     .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue)))
-        .orElse(headers);
+        .orElse(headers != null ? headers : Collections.emptyMap());
   }
 
   @Override

File: metadata-io/src/main/java/com/linkedin/metadata/entity/AspectDao.java
Patch:
@@ -43,7 +43,7 @@ EntityAspect getAspect(
 
   @Nonnull
   Map<EntityAspectIdentifier, EntityAspect> batchGet(
-      @Nonnull final Set<EntityAspectIdentifier> keys);
+      @Nonnull final Set<EntityAspectIdentifier> keys, boolean forUpdate);
 
   @Nonnull
   List<EntityAspect> getAspectsInRange(

File: metadata-io/src/main/java/com/linkedin/metadata/entity/cassandra/CassandraAspectDao.java
Patch:
@@ -198,7 +198,7 @@ public void saveAspect(
   @Override
   @Nonnull
   public Map<EntityAspectIdentifier, EntityAspect> batchGet(
-      @Nonnull final Set<EntityAspectIdentifier> keys) {
+      @Nonnull final Set<EntityAspectIdentifier> keys, boolean forUpdate) {
     validateConnection();
     return keys.stream()
         .map(this::getAspect)

File: metadata-io/src/test/java/com/linkedin/metadata/entity/DeleteEntityServiceTest.java
Patch:
@@ -113,7 +113,7 @@ public void testDeleteUniqueRefGeneratesValidMCP() {
     dbValue.setCreatedOn(new Timestamp(auditStamp.getTime()));
 
     final Map<EntityAspectIdentifier, EntityAspect> dbEntries = Map.of(dbKey, dbValue);
-    Mockito.when(_aspectDao.batchGet(Mockito.any())).thenReturn(dbEntries);
+    Mockito.when(_aspectDao.batchGet(Mockito.any(), Mockito.anyBoolean())).thenReturn(dbEntries);
 
     RollbackResult result =
         new RollbackResult(

File: metadata-io/src/test/java/com/linkedin/metadata/timeline/TimelineServiceTest.java
Patch:
@@ -99,7 +99,7 @@ public void testGetTimeline() throws Exception {
 
     Map<String, RecordTemplate> latestAspects =
         _entityServiceImpl.getLatestAspectsForUrn(
-            opContext, entityUrn, new HashSet<>(Arrays.asList(aspectName)));
+            opContext, entityUrn, new HashSet<>(Arrays.asList(aspectName)), false);
 
     Set<ChangeCategory> elements = new HashSet<>();
     elements.add(ChangeCategory.TECHNICAL_SCHEMA);

File: metadata-io/src/test/java/io/datahubproject/test/fixtures/search/SampleDataFixtureConfiguration.java
Patch:
@@ -2,6 +2,7 @@
 
 import static com.linkedin.metadata.Constants.*;
 import static io.datahubproject.test.search.config.SearchTestContainerConfiguration.REFRESH_INTERVAL_SECONDS;
+import static org.mockito.ArgumentMatchers.anyBoolean;
 import static org.mockito.ArgumentMatchers.anySet;
 import static org.mockito.Mockito.mock;
 import static org.mockito.Mockito.when;
@@ -303,7 +304,7 @@ private EntityClient entityClientHelper(
             new ConcurrentMapCacheManager(), entitySearchService, 1, false);
 
     AspectDao mockAspectDao = mock(AspectDao.class);
-    when(mockAspectDao.batchGet(anySet()))
+    when(mockAspectDao.batchGet(anySet(), anyBoolean()))
         .thenAnswer(
             args -> {
               Set<EntityAspectIdentifier> ids = args.getArgument(0);

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/GmsGraphQLEngine.java
Patch:
@@ -1318,7 +1318,8 @@ private void configureMutationResolvers(final RuntimeWiring.Builder builder) {
               .dataFetcher("updateQuery", new UpdateQueryResolver(this.queryService))
               .dataFetcher("deleteQuery", new DeleteQueryResolver(this.queryService))
               .dataFetcher(
-                  "createDataProduct", new CreateDataProductResolver(this.dataProductService))
+                  "createDataProduct",
+                  new CreateDataProductResolver(this.dataProductService, this.entityService))
               .dataFetcher(
                   "updateDataProduct", new UpdateDataProductResolver(this.dataProductService))
               .dataFetcher(

File: metadata-service/openapi-servlet/src/main/java/io/datahubproject/openapi/v3/controller/EntityController.java
Patch:
@@ -146,6 +146,8 @@ public ResponseEntity<GenericEntityScrollResultV3> scrollEntities(
           Boolean skipCache,
       @RequestParam(value = "includeSoftDelete", required = false, defaultValue = "false")
           Boolean includeSoftDelete,
+      @RequestParam(value = "pitKeepAlive", required = false, defaultValue = "5m")
+          String pitKeepALive,
       @RequestBody @Nonnull GenericEntityAspectsBodyV3 entityAspectsBody)
       throws URISyntaxException {
 
@@ -202,7 +204,7 @@ public ResponseEntity<GenericEntityScrollResultV3> scrollEntities(
             null,
             sortCriteria,
             scrollId,
-            null,
+            pitKeepALive,
             count);
 
     if (!AuthUtil.isAPIAuthorizedResult(opContext, result)) {

File: metadata-io/src/test/java/com/linkedin/metadata/entity/EntityServiceTest.java
Patch:
@@ -627,7 +627,7 @@ public void testReingestLineageAspect() throws Exception {
     restateChangeLog.setSystemMetadata(futureSystemMetadata);
     restateChangeLog.setPreviousAspectValue(aspect);
     restateChangeLog.setPreviousSystemMetadata(
-        simulatePullFromDB(futureSystemMetadata, SystemMetadata.class));
+        simulatePullFromDB(initialSystemMetadata, SystemMetadata.class));
     restateChangeLog.setEntityKeyAspect(
         GenericRecordUtils.serializeAspect(
             EntityKeyUtils.convertUrnToEntityKey(
@@ -705,8 +705,7 @@ public void testReingestLineageProposal() throws Exception {
     restateChangeLog.setAspect(genericAspect);
     restateChangeLog.setSystemMetadata(futureSystemMetadata);
     restateChangeLog.setPreviousAspectValue(genericAspect);
-    restateChangeLog.setPreviousSystemMetadata(
-        simulatePullFromDB(futureSystemMetadata, SystemMetadata.class));
+    restateChangeLog.setPreviousSystemMetadata(simulatePullFromDB(metadata1, SystemMetadata.class));
 
     Map<String, RecordTemplate> latestAspects =
         _entityServiceImpl.getLatestAspectsForUrn(

File: metadata-io/src/main/java/com/linkedin/metadata/entity/cassandra/CassandraAspectDao.java
Patch:
@@ -590,7 +590,7 @@ public long saveLatestAspect(
     // Save oldValue as the largest version + 1
     long largestVersion = ASPECT_LATEST_VERSION;
     BatchStatement batch = BatchStatement.newInstance(BatchType.UNLOGGED);
-    if (oldAspectMetadata != null && oldTime != null) {
+    if (!ASPECT_LATEST_VERSION.equals(nextVersion) && oldTime != null) {
       largestVersion = nextVersion;
       final EntityAspect aspect =
           new EntityAspect(
@@ -616,7 +616,7 @@ public long saveLatestAspect(
             newTime,
             newActor,
             newImpersonator);
-    batch = batch.add(generateSaveStatement(aspect, oldAspectMetadata == null));
+    batch = batch.add(generateSaveStatement(aspect, ASPECT_LATEST_VERSION.equals(nextVersion)));
     _cqlSession.execute(batch);
     return largestVersion;
   }

File: metadata-io/src/main/java/com/linkedin/metadata/entity/ebean/EbeanAspectDao.java
Patch:
@@ -165,7 +165,7 @@ public long saveLatestAspect(
     }
     // Save oldValue as the largest version + 1
     long largestVersion = ASPECT_LATEST_VERSION;
-    if (oldAspectMetadata != null && oldTime != null) {
+    if (!ASPECT_LATEST_VERSION.equals(nextVersion) && oldTime != null) {
       largestVersion = nextVersion;
       saveAspect(
           txContext,
@@ -191,7 +191,7 @@ public long saveLatestAspect(
         newTime,
         newSystemMetadata,
         ASPECT_LATEST_VERSION,
-        oldAspectMetadata == null);
+        ASPECT_LATEST_VERSION.equals(nextVersion));
 
     return largestVersion;
   }

File: metadata-io/src/test/java/com/linkedin/metadata/AspectGenerationUtils.java
Patch:
@@ -34,19 +34,19 @@ public static SystemMetadata createSystemMetadata() {
   }
 
   @Nonnull
-  public static SystemMetadata createSystemMetadata(long nextAspectVersion) {
+  public static SystemMetadata createSystemMetadata(int nextAspectVersion) {
     return createSystemMetadata(
         1625792689, "run-123", "run-123", String.valueOf(nextAspectVersion));
   }
 
   @Nonnull
-  public static SystemMetadata createSystemMetadata(long lastObserved, @Nonnull String runId) {
+  public static SystemMetadata createSystemMetadata(int lastObserved, @Nonnull String runId) {
     return createSystemMetadata(lastObserved, runId, runId, null);
   }
 
   @Nonnull
   public static SystemMetadata createSystemMetadata(
-      long lastObserved,
+      int lastObserved, // for test comparison must be int
       @Nonnull String runId,
       @Nonnull String lastRunId,
       @Nullable String version) {

File: entity-registry/src/main/java/com/linkedin/metadata/models/registry/PluginEntityRegistryLoader.java
Patch:
@@ -6,7 +6,6 @@
 import com.linkedin.metadata.models.registry.config.LoadStatus;
 import com.linkedin.util.Pair;
 import java.io.File;
-import java.io.IOException;
 import java.io.PrintWriter;
 import java.io.StringWriter;
 import java.nio.file.Files;
@@ -204,8 +203,8 @@ private void loadOneRegistry(
       loadResultBuilder.plugins(entityRegistry.getPluginFactory().getPluginLoadResult());
 
       log.info("Loaded registry {} successfully", entityRegistry);
-    } catch (RuntimeException | EntityRegistryException | IOException e) {
-      log.debug("{}: Failed to load registry {} with {}", this, registryName, e.getMessage());
+    } catch (Exception | EntityRegistryException e) {
+      log.error("{}: Failed to load registry {} with {}", this, registryName, e.getMessage(), e);
       StringWriter sw = new StringWriter();
       PrintWriter pw = new PrintWriter(sw);
       e.printStackTrace(pw);

File: metadata-service/restli-servlet-impl/src/main/java/com/linkedin/metadata/resources/entity/AspectResource.java
Patch:
@@ -281,12 +281,13 @@ private Task<String> ingestProposals(
           boolean asyncBool)
   throws URISyntaxException {
     Authentication authentication = AuthenticationContext.getAuthentication();
+    String actorUrnStr = authentication.getActor().toUrnStr();
 
     Set<String> entityTypes = metadataChangeProposals.stream()
                                                      .map(MetadataChangeProposal::getEntityType)
                                                      .collect(Collectors.toSet());
     final OperationContext opContext = OperationContext.asSession(
-              systemOperationContext, RequestContext.builder().buildRestli(authentication.getActor().toUrnStr(), getContext(),
+              systemOperationContext, RequestContext.builder().buildRestli(actorUrnStr, getContext(),
                     ACTION_INGEST_PROPOSAL, entityTypes), _authorizer, authentication, true);
 
     // Ingest Authorization Checks
@@ -299,9 +300,8 @@ private Task<String> ingestProposals(
                  .map(ex -> String.format("HttpStatus: %s Urn: %s", ex.getSecond(), ex.getFirst().getEntityUrn()))
                  .collect(Collectors.joining(", "));
         throw new RestLiServiceException(
-                 HttpStatus.S_403_FORBIDDEN, "User is unauthorized to modify entity: " + errorMessages);
+                 HttpStatus.S_403_FORBIDDEN, "User " + actorUrnStr + " is unauthorized to modify entity: " + errorMessages);
     }
-    String actorUrnStr = authentication.getActor().toUrnStr();
     final AuditStamp auditStamp =
         new AuditStamp().setTime(_clock.millis()).setActor(Urn.createFromString(actorUrnStr));
 

File: metadata-service/restli-servlet-impl/src/main/java/com/linkedin/metadata/resources/usage/UsageStats.java
Patch:
@@ -104,9 +104,10 @@ public Task<Void> batchIngest(@ActionParam(PARAM_BUCKETS) @Nonnull UsageAggregat
         () -> {
 
           final Authentication auth = AuthenticationContext.getAuthentication();
+          String actorUrnStr = auth.getActor().toUrnStr();
           Set<Urn> urns = Arrays.stream(buckets).sequential().map(UsageAggregation::getResource).collect(Collectors.toSet());
           final OperationContext opContext = OperationContext.asSession(
-                  systemOperationContext, RequestContext.builder().buildRestli(auth.getActor().toUrnStr(), getContext(),
+                  systemOperationContext, RequestContext.builder().buildRestli(actorUrnStr, getContext(),
                           ACTION_BATCH_INGEST, urns.stream().map(Urn::getEntityType).collect(Collectors.toList())), _authorizer,
                   auth, true);
 
@@ -115,7 +116,7 @@ public Task<Void> batchIngest(@ActionParam(PARAM_BUCKETS) @Nonnull UsageAggregat
                   UPDATE,
                   urns)) {
             throw new RestLiServiceException(
-                HttpStatus.S_403_FORBIDDEN, "User is unauthorized to edit entities.");
+                HttpStatus.S_403_FORBIDDEN, "User " + actorUrnStr + " is unauthorized to edit entities.");
           }
 
           for (UsageAggregation agg : buckets) {

File: li-utils/src/main/java/com/linkedin/metadata/Constants.java
Patch:
@@ -51,6 +51,7 @@ public class Constants {
   // App sources
   public static final String UI_SOURCE = "ui";
   public static final String SYSTEM_UPDATE_SOURCE = "systemUpdate";
+  public static final String METADATA_TESTS_SOURCE = "metadataTests";
 
   /** Entities */
   public static final String CORP_USER_ENTITY_NAME = "corpuser";

File: entity-registry/src/main/java/com/linkedin/metadata/models/StructuredPropertyUtils.java
Patch:
@@ -178,7 +178,7 @@ public static String toElasticsearchFieldName(
   /**
    * Return an elasticsearch type from structured property type
    *
-   * @param fieldName filter or facet field name
+   * @param fieldName filter or facet field name - must match actual FQN of structured prop
    * @param aspectRetriever aspect retriever
    * @return elasticsearch type
    */

File: metadata-integration/java/datahub-schematron/lib/src/main/java/io/datahubproject/schematron/models/FieldPath.java
Patch:
@@ -2,6 +2,7 @@
 
 import com.linkedin.schema.*;
 import java.util.ArrayList;
+import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 import java.util.Objects;
@@ -117,8 +118,8 @@ public FieldPath expandType(String type, Object typeSchema) {
           .getPath()
           .add(
               new FieldElement(
-                  new ArrayList<>(List.of(type)),
-                  new ArrayList<>(List.of(typeSchema.toString())),
+                  new ArrayList<>(Collections.singletonList(type)),
+                  new ArrayList<>(Collections.singletonList(typeSchema.toString())),
                   null,
                   null));
     }

File: entity-registry/src/main/java/com/linkedin/metadata/aspect/patch/template/TemplateUtil.java
Patch:
@@ -81,14 +81,13 @@ public static JsonNode populateTopLevelKeys(JsonNode transformedNode, JsonPatch
           PatchOperationType.REMOVE.equals(operationPath.getFirst())
               ? keys.length
               : keys.length - 1;
-
       // Skip first as it will always be blank due to path starting with /
       for (int i = 1; i < endIdx; i++) {
+        String decodedKey = decodeValue(keys[i]);
         if (parent.get(keys[i]) == null) {
-          String decodedKey = decodeValue(keys[i]);
           ((ObjectNode) parent).set(decodedKey, instance.objectNode());
         }
-        parent = parent.get(keys[i]);
+        parent = parent.get(decodedKey);
       }
     }
 

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/plugins/SpringStandardPluginConfiguration.java
Patch:
@@ -98,8 +98,7 @@ public MCPSideEffect dataProductUnsetSideEffect() {
         AspectPluginConfig.builder()
             .enabled(true)
             .className(DataProductUnsetSideEffect.class.getName())
-            .supportedOperations(
-                List.of("CREATE", "CREATE_ENTITY", "UPSERT", "RESTATE", "DELETE", "PATCH"))
+            .supportedOperations(List.of("CREATE", "CREATE_ENTITY", "UPSERT", "RESTATE"))
             .supportedEntityAspectNames(
                 List.of(
                     AspectPluginConfig.EntityAspectName.builder()

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/common/mappers/InstitutionalMemoryMetadataMapper.java
Patch:
@@ -28,6 +28,7 @@ public InstitutionalMemoryMetadata apply(
     result.setDescription(input.getDescription()); // deprecated field
     result.setLabel(input.getDescription());
     result.setAuthor(getAuthor(input.getCreateStamp().getActor().toString()));
+    result.setActor(ResolvedActorMapper.map(input.getCreateStamp().getActor()));
     result.setCreated(AuditStampMapper.map(context, input.getCreateStamp()));
     result.setAssociatedUrn(entityUrn.toString());
     return result;

File: metadata-io/src/test/java/com/linkedin/metadata/entity/EntityServiceTest.java
Patch:
@@ -1836,7 +1836,7 @@ public void testValidateUrn() throws Exception {
       ValidationApiUtils.validateUrn(opContext.getEntityRegistry(), urnWithMismatchedParens);
       Assert.fail("Should have raised IllegalArgumentException for URN with mismatched parens");
     } catch (IllegalArgumentException e) {
-      assertTrue(e.getMessage().contains("mismatched paren nesting"));
+      assertTrue(e.getMessage().contains("[test(Key]"));
     }
 
     Urn invalidType = new Urn("li", "fakeMadeUpType", new TupleKey("testKey"));

File: li-utils/src/main/java/com/linkedin/metadata/Constants.java
Patch:
@@ -10,6 +10,7 @@ public class Constants {
   public static final String INTERNAL_DELEGATED_FOR_ACTOR_HEADER_NAME = "X-DataHub-Delegated-For";
   public static final String INTERNAL_DELEGATED_FOR_ACTOR_TYPE = "X-DataHub-Delegated-For-";
 
+  public static final String URN_LI_PREFIX = "urn:li:";
   public static final String DATAHUB_ACTOR = "urn:li:corpuser:datahub"; // Super user.
   public static final String SYSTEM_ACTOR =
       "urn:li:corpuser:__datahub_system"; // DataHub internal service principal.

File: metadata-service/services/src/main/java/com/linkedin/metadata/entity/EntityService.java
Patch:
@@ -363,7 +363,9 @@ List<UpdateAspectResult> ingestAspects(
    * @param auditStamp an {@link AuditStamp} containing metadata about the writer & current time
    * @param systemMetadata
    * @return the {@link RecordTemplate} representation of the written aspect object
+   * @deprecated See Conditional Write ChangeType CREATE
    */
+  @Deprecated
   RecordTemplate ingestAspectIfNotPresent(
       @Nonnull OperationContext opContext,
       @Nonnull Urn urn,

File: datahub-frontend/app/auth/sso/oidc/OidcCallbackLogic.java
Patch:
@@ -130,8 +130,6 @@ public Object perform(
     CallContext ctx = ctxResult.getFirst();
     Result result = (Result) ctxResult.getSecond();
 
-    setContextRedirectUrl(ctx);
-
     // Handle OIDC authentication errors.
     if (OidcResponseErrorHandler.isError(ctx)) {
       return OidcResponseErrorHandler.handleError(ctx);
@@ -192,6 +190,9 @@ private Pair<CallContext, Object> superPerform(
           }
         }
 
+        // Set the redirect url from cookie before creating action
+        setContextRedirectUrl(ctx);
+
         action = this.redirectToOriginallyRequestedUrl(ctx, defaultUrl);
       }
     } catch (RuntimeException var20) {

File: datahub-graphql-core/src/test/java/com/linkedin/datahub/graphql/resolvers/ingest/source/ListIngestionSourceResolverTest.java
Patch:
@@ -28,7 +28,7 @@
 public class ListIngestionSourceResolverTest {
 
   private static final ListIngestionSourcesInput TEST_INPUT =
-      new ListIngestionSourcesInput(0, 20, null, null);
+      new ListIngestionSourcesInput(0, 20, null, null, null);
 
   @Test
   public void testGetSuccess() throws Exception {

File: metadata-service/war/src/main/java/com/linkedin/gms/CommonApplicationConfig.java
Patch:
@@ -40,7 +40,8 @@
       "com.linkedin.gms.factory.plugins",
       "com.linkedin.gms.factory.change",
       "com.datahub.event.hook",
-      "com.linkedin.gms.factory.notifications"
+      "com.linkedin.gms.factory.notifications",
+      "com.linkedin.gms.factory.telemetry"
     })
 @PropertySource(value = "classpath:/application.yaml", factory = YamlPropertySourceFactory.class)
 @Configuration

File: metadata-io/src/main/java/com/linkedin/metadata/search/utils/ESUtils.java
Patch:
@@ -100,7 +100,7 @@ public class ESUtils {
   // top-level properties
   // to field level properties
   public static final Map<String, List<String>> FIELDS_TO_EXPANDED_FIELDS_LIST =
-      new HashMap<String, List<String>>() {
+      new HashMap<>() {
         {
           put("tags", ImmutableList.of("tags", "fieldTags", "editedFieldTags"));
           put(
@@ -117,6 +117,8 @@ public class ESUtils {
           put(
               "businessAttribute",
               ImmutableList.of("businessAttributeRef", "businessAttributeRef.urn"));
+          put("origin", ImmutableList.of("origin", "env"));
+          put("env", ImmutableList.of("env", "origin"));
         }
       };
 

File: datahub-upgrade/src/main/java/com/linkedin/datahub/upgrade/config/restoreindices/ReindexDomainDescriptionConfig.java
Patch:
@@ -1,7 +1,8 @@
-package com.linkedin.datahub.upgrade.config;
+package com.linkedin.datahub.upgrade.config.restoreindices;
 
+import com.linkedin.datahub.upgrade.config.SystemUpdateCondition;
 import com.linkedin.datahub.upgrade.system.NonBlockingSystemUpgrade;
-import com.linkedin.datahub.upgrade.system.domaindescription.ReindexDomainDescription;
+import com.linkedin.datahub.upgrade.system.restoreindices.domaindescription.ReindexDomainDescription;
 import com.linkedin.metadata.entity.AspectDao;
 import com.linkedin.metadata.entity.EntityService;
 import io.datahubproject.metadata.context.OperationContext;

File: datahub-upgrade/src/main/java/com/linkedin/datahub/upgrade/config/restoreindices/graph/ReindexDataJobViaNodesCLLConfig.java
Patch:
@@ -1,8 +1,8 @@
-package com.linkedin.datahub.upgrade.config.graph;
+package com.linkedin.datahub.upgrade.config.restoreindices.graph;
 
 import com.linkedin.datahub.upgrade.config.SystemUpdateCondition;
 import com.linkedin.datahub.upgrade.system.NonBlockingSystemUpgrade;
-import com.linkedin.datahub.upgrade.system.graph.vianodes.ReindexDataJobViaNodesCLL;
+import com.linkedin.datahub.upgrade.system.restoreindices.graph.vianodes.ReindexDataJobViaNodesCLL;
 import com.linkedin.metadata.entity.AspectDao;
 import com.linkedin.metadata.entity.EntityService;
 import io.datahubproject.metadata.context.OperationContext;

File: datahub-upgrade/src/main/java/com/linkedin/datahub/upgrade/config/restoreindices/graph/ReindexEdgeStatusConfig.java
Patch:
@@ -1,8 +1,8 @@
-package com.linkedin.datahub.upgrade.config.graph;
+package com.linkedin.datahub.upgrade.config.restoreindices.graph;
 
 import com.linkedin.datahub.upgrade.config.SystemUpdateCondition;
 import com.linkedin.datahub.upgrade.system.NonBlockingSystemUpgrade;
-import com.linkedin.datahub.upgrade.system.graph.edgestatus.ReindexEdgeStatus;
+import com.linkedin.datahub.upgrade.system.restoreindices.graph.edgestatus.ReindexEdgeStatus;
 import com.linkedin.metadata.entity.AspectDao;
 import com.linkedin.metadata.entity.EntityService;
 import io.datahubproject.metadata.context.OperationContext;

File: datahub-upgrade/src/main/java/com/linkedin/datahub/upgrade/system/restoreindices/domaindescription/ReindexDomainDescription.java
Patch:
@@ -1,4 +1,4 @@
-package com.linkedin.datahub.upgrade.system.domaindescription;
+package com.linkedin.datahub.upgrade.system.restoreindices.domaindescription;
 
 import com.google.common.collect.ImmutableList;
 import com.linkedin.datahub.upgrade.UpgradeStep;

File: datahub-upgrade/src/main/java/com/linkedin/datahub/upgrade/system/restoreindices/domaindescription/ReindexDomainDescriptionStep.java
Patch:
@@ -1,4 +1,4 @@
-package com.linkedin.datahub.upgrade.system.domaindescription;
+package com.linkedin.datahub.upgrade.system.restoreindices.domaindescription;
 
 import static com.linkedin.metadata.Constants.*;
 

File: datahub-upgrade/src/main/java/com/linkedin/datahub/upgrade/system/restoreindices/graph/edgestatus/ReindexEdgeStatus.java
Patch:
@@ -1,4 +1,4 @@
-package com.linkedin.datahub.upgrade.system.graph.edgestatus;
+package com.linkedin.datahub.upgrade.system.restoreindices.graph.edgestatus;
 
 import com.google.common.collect.ImmutableList;
 import com.linkedin.datahub.upgrade.UpgradeStep;

File: datahub-upgrade/src/main/java/com/linkedin/datahub/upgrade/system/restoreindices/graph/edgestatus/ReindexReindexEdgeStatusStep.java
Patch:
@@ -1,4 +1,4 @@
-package com.linkedin.datahub.upgrade.system.graph.edgestatus;
+package com.linkedin.datahub.upgrade.system.restoreindices.graph.edgestatus;
 
 import static com.linkedin.metadata.Constants.STATUS_ASPECT_NAME;
 

File: datahub-upgrade/src/main/java/com/linkedin/datahub/upgrade/system/restoreindices/graph/vianodes/ReindexDataJobViaNodesCLL.java
Patch:
@@ -1,4 +1,4 @@
-package com.linkedin.datahub.upgrade.system.graph.vianodes;
+package com.linkedin.datahub.upgrade.system.restoreindices.graph.vianodes;
 
 import com.google.common.collect.ImmutableList;
 import com.linkedin.datahub.upgrade.UpgradeStep;

File: datahub-upgrade/src/main/java/com/linkedin/datahub/upgrade/system/restoreindices/graph/vianodes/ReindexDataJobViaNodesCLLStep.java
Patch:
@@ -1,4 +1,4 @@
-package com.linkedin.datahub.upgrade.system.graph.vianodes;
+package com.linkedin.datahub.upgrade.system.restoreindices.graph.vianodes;
 
 import static com.linkedin.metadata.Constants.*;
 

File: datahub-upgrade/src/test/java/com/linkedin/datahub/upgrade/DatahubUpgradeNonBlockingTest.java
Patch:
@@ -12,7 +12,7 @@
 import com.linkedin.datahub.upgrade.impl.DefaultUpgradeManager;
 import com.linkedin.datahub.upgrade.system.SystemUpdateNonBlocking;
 import com.linkedin.datahub.upgrade.system.bootstrapmcps.BootstrapMCPStep;
-import com.linkedin.datahub.upgrade.system.graph.vianodes.ReindexDataJobViaNodesCLL;
+import com.linkedin.datahub.upgrade.system.restoreindices.graph.vianodes.ReindexDataJobViaNodesCLL;
 import com.linkedin.metadata.boot.kafka.MockSystemUpdateDeserializer;
 import com.linkedin.metadata.boot.kafka.MockSystemUpdateSerializer;
 import com.linkedin.metadata.config.kafka.KafkaConfiguration;

File: metadata-io/src/main/java/com/linkedin/metadata/search/elasticsearch/ElasticSearchService.java
Patch:
@@ -112,7 +112,7 @@ public void appendRunId(
       @Nullable String runId) {
     final String docId = indexBuilders.getIndexConvention().getEntityDocumentId(urn);
 
-    log.info(
+    log.debug(
         "Appending run id for entity name: {}, doc id: {}, run id: {}", entityName, docId, runId);
     esWriteDAO.applyScriptUpdate(
         opContext,

File: metadata-jobs/mae-consumer/src/main/java/com/linkedin/metadata/kafka/MCLKafkaListener.java
Patch:
@@ -95,7 +95,7 @@ public void consume(final ConsumerRecord<String, GenericRecord> consumerRecord)
 
       // Here - plug in additional "custom processor hooks"
       for (MetadataChangeLogHook hook : this.hooks) {
-        log.info(
+        log.debug(
             "Invoking MCL hook {} for urn: {}",
             hook.getClass().getSimpleName(),
             event.getEntityUrn());

File: li-utils/src/main/java/com/linkedin/metadata/Constants.java
Patch:
@@ -460,7 +460,7 @@ public class Constants {
 
   // Logging MDC
   public static final String MDC_ENTITY_URN = "entityUrn";
-  public static final String MDC_ASPECT_NAME = "";
+  public static final String MDC_ASPECT_NAME = "aspectName";
   public static final String MDC_ENTITY_TYPE = "entityType";
   public static final String MDC_CHANGE_TYPE = "changeType";
 

File: metadata-io/src/main/java/com/linkedin/metadata/service/UpdateGraphIndicesService.java
Patch:
@@ -388,24 +388,24 @@ private void updateGraphServiceDiff(
     // Remove any old edges that no longer exist first
     if (!subtractiveDifference.isEmpty()) {
       log.debug("Removing edges: {}", subtractiveDifference);
+      subtractiveDifference.forEach(graphService::removeEdge);
       MetricUtils.counter(this.getClass(), GRAPH_DIFF_MODE_REMOVE_METRIC)
           .inc(subtractiveDifference.size());
-      subtractiveDifference.forEach(graphService::removeEdge);
     }
 
     // Then add new edges
     if (!additiveDifference.isEmpty()) {
       log.debug("Adding edges: {}", additiveDifference);
+      additiveDifference.forEach(graphService::addEdge);
       MetricUtils.counter(this.getClass(), GRAPH_DIFF_MODE_ADD_METRIC)
           .inc(additiveDifference.size());
-      additiveDifference.forEach(graphService::addEdge);
     }
 
     // Then update existing edges
     if (!mergedEdges.isEmpty()) {
       log.debug("Updating edges: {}", mergedEdges);
-      MetricUtils.counter(this.getClass(), GRAPH_DIFF_MODE_UPDATE_METRIC).inc(mergedEdges.size());
       mergedEdges.forEach(graphService::upsertEdge);
+      MetricUtils.counter(this.getClass(), GRAPH_DIFF_MODE_UPDATE_METRIC).inc(mergedEdges.size());
     }
   }
 

File: datahub-frontend/app/auth/AuthModule.java
Patch:
@@ -21,6 +21,7 @@
 import com.linkedin.util.Configuration;
 import config.ConfigurationProvider;
 import controllers.SsoCallbackController;
+import io.datahubproject.metadata.context.ValidationContext;
 import java.nio.charset.StandardCharsets;
 import java.util.Collections;
 
@@ -187,6 +188,7 @@ protected OperationContext provideOperationContext(
             .authorizationContext(AuthorizationContext.builder().authorizer(Authorizer.EMPTY).build())
             .searchContext(SearchContext.EMPTY)
             .entityRegistryContext(EntityRegistryContext.builder().build(EmptyEntityRegistry.EMPTY))
+            .validationContext(ValidationContext.builder().alternateValidation(false).build())
             .build(systemAuthentication);
   }
 

File: metadata-io/metadata-io-api/src/main/java/com/linkedin/metadata/entity/ebean/batch/PatchItemImpl.java
Patch:
@@ -203,7 +203,7 @@ public static PatchItemImpl build(
           .build(entityRegistry);
     }
 
-    private static JsonPatch convertToJsonPatch(MetadataChangeProposal mcp) {
+    public static JsonPatch convertToJsonPatch(MetadataChangeProposal mcp) {
       JsonNode json;
       try {
         return Json.createPatch(

File: metadata-io/src/test/java/com/linkedin/metadata/entity/EbeanEntityServiceTest.java
Patch:
@@ -104,7 +104,8 @@ public void setupTest() {
             null,
             opContext ->
                 ((EntityServiceAspectRetriever) opContext.getAspectRetrieverOpt().get())
-                    .setSystemOperationContext(opContext));
+                    .setSystemOperationContext(opContext),
+            null);
   }
 
   /**

File: metadata-io/src/test/java/com/linkedin/metadata/entity/cassandra/CassandraEntityServiceTest.java
Patch:
@@ -99,7 +99,8 @@ private void configureComponents() {
             null,
             opContext ->
                 ((EntityServiceAspectRetriever) opContext.getAspectRetrieverOpt().get())
-                    .setSystemOperationContext(opContext));
+                    .setSystemOperationContext(opContext),
+            null);
   }
 
   /**

File: metadata-jobs/mae-consumer/src/test/java/com/linkedin/metadata/kafka/hook/spring/MCLSpringCommonTestConfiguration.java
Patch:
@@ -21,6 +21,7 @@
 import io.datahubproject.metadata.context.OperationContextConfig;
 import io.datahubproject.metadata.context.RetrieverContext;
 import io.datahubproject.metadata.context.ServicesRegistryContext;
+import io.datahubproject.metadata.context.ValidationContext;
 import io.datahubproject.test.metadata.context.TestOperationContexts;
 import org.apache.avro.generic.GenericRecord;
 import org.springframework.beans.factory.annotation.Qualifier;
@@ -88,7 +89,8 @@ public OperationContext operationContext(
         entityRegistry,
         mock(ServicesRegistryContext.class),
         indexConvention,
-        mock(RetrieverContext.class));
+        mock(RetrieverContext.class),
+        mock(ValidationContext.class));
   }
 
   @MockBean SpringStandardPluginConfiguration springStandardPluginConfiguration;

File: metadata-operation-context/src/test/java/io/datahubproject/metadata/context/OperationContextTest.java
Patch:
@@ -25,7 +25,8 @@ public void testSystemPrivilegeEscalation() {
             mock(EntityRegistry.class),
             mock(ServicesRegistryContext.class),
             null,
-            mock(RetrieverContext.class));
+            mock(RetrieverContext.class),
+            mock(ValidationContext.class));
 
     OperationContext opContext =
         systemOpContext.asSession(RequestContext.TEST, Authorizer.EMPTY, userAuth);

File: metadata-service/auth-impl/src/test/java/com/datahub/authorization/DataHubAuthorizerTest.java
Patch:
@@ -54,6 +54,7 @@
 import io.datahubproject.metadata.context.OperationContextConfig;
 import io.datahubproject.metadata.context.RetrieverContext;
 import io.datahubproject.metadata.context.ServicesRegistryContext;
+import io.datahubproject.metadata.context.ValidationContext;
 import java.util.Collections;
 import java.util.HashMap;
 import java.util.HashSet;
@@ -318,7 +319,8 @@ public void setupTest() throws Exception {
             mock(EntityRegistry.class),
             mock(ServicesRegistryContext.class),
             mock(IndexConvention.class),
-            mock(RetrieverContext.class));
+            mock(RetrieverContext.class),
+            mock(ValidationContext.class));
 
     _dataHubAuthorizer =
         new DataHubAuthorizer(
@@ -598,7 +600,6 @@ private DataHubPolicyInfo createDataHubPolicyInfoFor(
     dataHubPolicyInfo.setDisplayName("My Test Display");
     dataHubPolicyInfo.setDescription("My test display!");
     dataHubPolicyInfo.setEditable(true);
-
     dataHubPolicyInfo.setActors(actorFilter);
 
     final DataHubResourceFilter resourceFilter = new DataHubResourceFilter();

File: metadata-service/configuration/src/main/java/com/linkedin/datahub/graphql/featureflags/FeatureFlags.java
Patch:
@@ -23,4 +23,5 @@ public class FeatureFlags {
   private boolean dataContractsEnabled = false;
   private boolean editableDatasetNameEnabled = false;
   private boolean showSeparateSiblings = false;
+  private boolean alternateMCPValidation = false;
 }

File: metadata-service/configuration/src/main/java/com/linkedin/metadata/config/MetadataChangeProposalConfig.java
Patch:
@@ -8,6 +8,7 @@
 public class MetadataChangeProposalConfig {
 
   ThrottlesConfig throttle;
+  MCPValidationConfig validation;
   SideEffectsConfig sideEffects;
 
   @Data

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/plugins/SpringStandardPluginConfiguration.java
Patch:
@@ -45,7 +45,7 @@ public MutationHook ignoreUnknownMutator() {
             AspectPluginConfig.builder()
                 .className(IgnoreUnknownMutator.class.getName())
                 .enabled(ignoreUnknownEnabled && !extensionsEnabled)
-                .supportedOperations(List.of("CREATE", "CREATE_ENTITY", "UPSERT"))
+                .supportedOperations(List.of("*"))
                 .supportedEntityAspectNames(
                     List.of(
                         AspectPluginConfig.EntityAspectName.builder()

File: metadata-service/restli-servlet-impl/src/main/java/com/linkedin/metadata/resources/entity/EntityResource.java
Patch:
@@ -927,7 +927,7 @@ public Task<DeleteEntityResponse> deleteEntity(
                   opContext.getEntityRegistry(), urn.getEntityType());
           if (aspectName != null && !timeseriesAspectNames.contains(aspectName)) {
             throw new UnsupportedOperationException(
-                String.format("Not supported for non-timeseries aspect '{}'.", aspectName));
+                String.format("Not supported for non-timeseries aspect %s.", aspectName));
           }
           List<String> timeseriesAspectsToDelete =
               (aspectName == null) ? timeseriesAspectNames : ImmutableList.of(aspectName);

File: metadata-service/services/src/main/java/com/linkedin/metadata/entity/EntityService.java
Patch:
@@ -487,7 +487,7 @@ RollbackRunResult rollbackWithConditions(
       Map<String, String> conditions,
       boolean hardDelete);
 
-  Set<IngestResult> ingestProposal(
+  List<IngestResult> ingestProposal(
       @Nonnull OperationContext opContext, AspectsBatch aspectsBatch, final boolean async);
 
   /**

File: metadata-io/src/main/java/com/linkedin/metadata/search/elasticsearch/query/ESSearchDAO.java
Patch:
@@ -370,7 +370,7 @@ public AutoCompleteResult autoComplete(
       IndexConvention indexConvention = opContext.getSearchContext().getIndexConvention();
       AutocompleteRequestHandler builder =
           AutocompleteRequestHandler.getBuilder(
-              entitySpec, customSearchConfiguration, queryFilterRewriteChain);
+              entitySpec, customSearchConfiguration, queryFilterRewriteChain, searchConfiguration);
       SearchRequest req =
           builder.getSearchRequest(
               opContext,

File: metadata-io/src/test/java/com/linkedin/metadata/entity/ebean/EbeanAspectDaoTest.java
Patch:
@@ -56,7 +56,8 @@ public void testGetLatestAspectsForUpdate() throws JsonProcessingException {
 
     testDao.runInTransactionWithRetryUnlocked(
         (txContext) -> {
-          testDao.getLatestAspects(Map.of("urn:li:corpuser:test", Set.of("status")), true);
+          testDao.getLatestAspects(
+              Map.of("urn:li:corpuser:testGetLatestAspectsForUpdate", Set.of("status")), true);
           return "";
         },
         mock(AspectsBatch.class),
@@ -65,7 +66,7 @@ public void testGetLatestAspectsForUpdate() throws JsonProcessingException {
     // Get the captured SQL statements
     List<String> sql =
         LoggedSql.stop().stream()
-            .filter(str -> str.contains("(t0.urn,t0.aspect,t0.version)"))
+            .filter(str -> str.contains("testGetLatestAspectsForUpdate"))
             .toList();
     assertEquals(
         sql.size(), 1, String.format("Found: %s", new ObjectMapper().writeValueAsString(sql)));

File: metadata-io/src/main/java/com/linkedin/metadata/search/utils/ESUtils.java
Patch:
@@ -759,7 +759,7 @@ private static Set<String> getFieldTypes(
           StructuredPropertyUtils.toElasticsearchFieldType(fieldName, aspectRetriever);
     } else {
       Set<SearchableAnnotation.FieldType> fieldTypes =
-          searchableFields.getOrDefault(fieldName, Collections.emptySet());
+          searchableFields.getOrDefault(fieldName.split("\\.")[0], Collections.emptySet());
       finalFieldTypes =
           fieldTypes.stream().map(ESUtils::getElasticTypeForFieldType).collect(Collectors.toSet());
     }
@@ -785,6 +785,7 @@ private static RangeQueryBuilder buildRangeQueryFromCriterion(
     // Determine criterion value, range query only accepts single value so take first value in
     // values if multiple
     String criterionValueString = criterion.getValues().get(0).trim();
+
     Object criterionValue;
     String documentFieldName;
     if (fieldTypes.contains(BOOLEAN_FIELD_TYPE)) {

File: metadata-io/src/test/java/com/linkedin/metadata/entity/cassandra/CassandraAspectMigrationsDaoTest.java
Patch:
@@ -1,12 +1,12 @@
-package com.linkedin.metadata.entity;
+package com.linkedin.metadata.entity.cassandra;
 
 import static org.mockito.Mockito.*;
 
 import com.datastax.oss.driver.api.core.CqlSession;
 import com.linkedin.metadata.CassandraTestUtils;
 import com.linkedin.metadata.config.PreProcessHooks;
-import com.linkedin.metadata.entity.cassandra.CassandraAspectDao;
-import com.linkedin.metadata.entity.cassandra.CassandraRetentionService;
+import com.linkedin.metadata.entity.AspectMigrationsDaoTest;
+import com.linkedin.metadata.entity.EntityServiceImpl;
 import com.linkedin.metadata.event.EventProducer;
 import com.linkedin.metadata.models.registry.EntityRegistryException;
 import com.linkedin.metadata.service.UpdateIndicesService;

File: metadata-io/src/test/java/com/linkedin/metadata/timeline/cassandra/CassandraTimelineServiceTest.java
Patch:
@@ -1,4 +1,4 @@
-package com.linkedin.metadata.timeline;
+package com.linkedin.metadata.timeline.cassandra;
 
 import static org.mockito.Mockito.mock;
 
@@ -9,6 +9,8 @@
 import com.linkedin.metadata.entity.cassandra.CassandraAspectDao;
 import com.linkedin.metadata.event.EventProducer;
 import com.linkedin.metadata.models.registry.EntityRegistryException;
+import com.linkedin.metadata.timeline.TimelineServiceImpl;
+import com.linkedin.metadata.timeline.TimelineServiceTest;
 import org.testcontainers.containers.CassandraContainer;
 import org.testng.Assert;
 import org.testng.annotations.AfterClass;

File: metadata-io/src/test/java/com/linkedin/metadata/search/fixtures/GoldenTestBase.java
Patch:
@@ -4,7 +4,6 @@
 import static com.linkedin.metadata.utils.CriterionUtils.buildCriterion;
 import static io.datahubproject.test.search.SearchTestUtils.searchAcrossEntities;
 import static org.testng.Assert.*;
-import static org.testng.AssertJUnit.assertNotNull;
 
 import com.google.common.collect.ImmutableList;
 import com.linkedin.common.urn.Urn;

File: metadata-io/src/main/java/com/linkedin/metadata/search/elasticsearch/ElasticSearchService.java
Patch:
@@ -2,6 +2,7 @@
 
 import static com.linkedin.metadata.search.utils.SearchUtils.applyDefaultSearchFlags;
 
+import com.google.common.annotations.VisibleForTesting;
 import com.linkedin.common.urn.Urn;
 import com.linkedin.metadata.browse.BrowseResult;
 import com.linkedin.metadata.browse.BrowseResultV2;
@@ -30,6 +31,7 @@
 import java.util.Optional;
 import javax.annotation.Nonnull;
 import javax.annotation.Nullable;
+import lombok.Getter;
 import lombok.RequiredArgsConstructor;
 import lombok.extern.slf4j.Slf4j;
 import org.opensearch.action.explain.ExplainResponse;
@@ -51,7 +53,7 @@ public class ElasticSearchService implements EntitySearchService, ElasticSearchI
 
   private static final int MAX_RUN_IDS_INDEXED = 25; // Save the previous 25 run ids in the index.
   private final EntityIndexBuilders indexBuilders;
-  private final ESSearchDAO esSearchDAO;
+  @VisibleForTesting @Getter private final ESSearchDAO esSearchDAO;
   private final ESBrowseDAO esBrowseDAO;
   private final ESWriteDAO esWriteDAO;
 

File: metadata-io/src/main/java/com/linkedin/metadata/search/elasticsearch/query/request/SearchRequestHandler.java
Patch:
@@ -58,6 +58,7 @@
 import java.util.stream.Stream;
 import javax.annotation.Nonnull;
 import javax.annotation.Nullable;
+import lombok.Getter;
 import lombok.extern.slf4j.Slf4j;
 import org.apache.commons.collections.CollectionUtils;
 import org.opensearch.action.search.SearchRequest;
@@ -80,7 +81,7 @@ public class SearchRequestHandler {
   private static final Map<List<EntitySpec>, SearchRequestHandler> REQUEST_HANDLER_BY_ENTITY_NAME =
       new ConcurrentHashMap<>();
   private final List<EntitySpec> entitySpecs;
-  private final Set<String> defaultQueryFieldNames;
+  @Getter private final Set<String> defaultQueryFieldNames;
   @Nonnull private final HighlightBuilder highlights;
 
   private final SearchConfiguration configs;

File: metadata-io/src/test/java/com/linkedin/metadata/entity/DeleteEntityServiceTest.java
Patch:
@@ -2,7 +2,9 @@
 
 import static com.linkedin.metadata.search.utils.QueryUtils.*;
 import static org.mockito.Mockito.*;
-import static org.testng.AssertJUnit.*;
+import static org.testng.Assert.assertEquals;
+import static org.testng.Assert.assertFalse;
+import static org.testng.Assert.assertTrue;
 
 import com.datahub.util.RecordUtils;
 import com.google.common.collect.ImmutableList;

File: metadata-io/src/test/java/com/linkedin/metadata/extractor/AspectExtractorTest.java
Patch:
@@ -1,6 +1,6 @@
 package com.linkedin.metadata.extractor;
 
-import static org.testng.AssertJUnit.assertEquals;
+import static org.testng.Assert.assertEquals;
 
 import com.datahub.test.TestEntityAspect;
 import com.datahub.test.TestEntityAspectArray;

File: metadata-io/src/test/java/com/linkedin/metadata/graph/search/elasticsearch/SearchGraphServiceElasticSearchTest.java
Patch:
@@ -1,5 +1,7 @@
 package com.linkedin.metadata.graph.search.elasticsearch;
 
+import static org.testng.Assert.assertNotNull;
+
 import com.linkedin.metadata.graph.search.SearchGraphServiceTestBase;
 import com.linkedin.metadata.search.elasticsearch.ElasticSearchSuite;
 import com.linkedin.metadata.search.elasticsearch.indexbuilder.ESIndexBuilder;
@@ -9,7 +11,6 @@
 import org.opensearch.client.RestHighLevelClient;
 import org.springframework.beans.factory.annotation.Autowired;
 import org.springframework.context.annotation.Import;
-import org.testng.AssertJUnit;
 import org.testng.annotations.Test;
 
 @Import({ElasticSearchSuite.class, SearchTestContainerConfiguration.class})
@@ -39,6 +40,6 @@ protected ESIndexBuilder getIndexBuilder() {
 
   @Test
   public void initTest() {
-    AssertJUnit.assertNotNull(_searchClient);
+    assertNotNull(_searchClient);
   }
 }

File: metadata-io/src/test/java/com/linkedin/metadata/graph/search/opensearch/SearchGraphServiceOpenSearchTest.java
Patch:
@@ -1,5 +1,7 @@
 package com.linkedin.metadata.graph.search.opensearch;
 
+import static org.testng.Assert.assertNotNull;
+
 import com.linkedin.metadata.graph.search.SearchGraphServiceTestBase;
 import com.linkedin.metadata.search.elasticsearch.indexbuilder.ESIndexBuilder;
 import com.linkedin.metadata.search.elasticsearch.update.ESBulkProcessor;
@@ -9,7 +11,6 @@
 import org.opensearch.client.RestHighLevelClient;
 import org.springframework.beans.factory.annotation.Autowired;
 import org.springframework.context.annotation.Import;
-import org.testng.AssertJUnit;
 import org.testng.annotations.Test;
 
 @Import({OpenSearchSuite.class, SearchTestContainerConfiguration.class})
@@ -39,6 +40,6 @@ protected ESIndexBuilder getIndexBuilder() {
 
   @Test
   public void initTest() {
-    AssertJUnit.assertNotNull(_searchClient);
+    assertNotNull(_searchClient);
   }
 }

File: metadata-io/src/test/java/com/linkedin/metadata/search/LineageSearchResultCacheKeyTest.java
Patch:
@@ -1,7 +1,7 @@
 package com.linkedin.metadata.search;
 
-import static org.testng.AssertJUnit.assertEquals;
-import static org.testng.AssertJUnit.assertNotSame;
+import static org.testng.Assert.assertEquals;
+import static org.testng.Assert.assertNotSame;
 
 import org.springframework.test.context.testng.AbstractTestNGSpringContextTests;
 import org.testng.annotations.Test;

File: metadata-io/src/test/java/com/linkedin/metadata/search/elasticsearch/GoldenElasticSearchTest.java
Patch:
@@ -1,6 +1,6 @@
 package com.linkedin.metadata.search.elasticsearch;
 
-import static org.testng.AssertJUnit.assertNotNull;
+import static org.testng.Assert.assertNotNull;
 
 import com.linkedin.metadata.search.SearchService;
 import com.linkedin.metadata.search.fixtures.GoldenTestBase;

File: metadata-io/src/test/java/com/linkedin/metadata/search/elasticsearch/IndexBuilderElasticSearchTest.java
Patch:
@@ -1,6 +1,6 @@
 package com.linkedin.metadata.search.elasticsearch;
 
-import static org.testng.AssertJUnit.assertNotNull;
+import static org.testng.Assert.assertNotNull;
 
 import com.linkedin.metadata.search.indexbuilder.IndexBuilderTestBase;
 import io.datahubproject.test.search.config.SearchTestContainerConfiguration;

File: metadata-io/src/test/java/com/linkedin/metadata/search/elasticsearch/LineageDataFixtureElasticSearchTest.java
Patch:
@@ -1,5 +1,7 @@
 package com.linkedin.metadata.search.elasticsearch;
 
+import static org.testng.Assert.assertNotNull;
+
 import com.linkedin.metadata.search.LineageSearchService;
 import com.linkedin.metadata.search.SearchService;
 import com.linkedin.metadata.search.fixtures.LineageDataFixtureTestBase;
@@ -10,7 +12,6 @@
 import org.springframework.beans.factory.annotation.Autowired;
 import org.springframework.beans.factory.annotation.Qualifier;
 import org.springframework.context.annotation.Import;
-import org.testng.AssertJUnit;
 import org.testng.annotations.Test;
 
 @Getter
@@ -35,6 +36,6 @@ public class LineageDataFixtureElasticSearchTest extends LineageDataFixtureTestB
 
   @Test
   public void initTest() {
-    AssertJUnit.assertNotNull(lineageService);
+    assertNotNull(lineageService);
   }
 }

File: metadata-io/src/test/java/com/linkedin/metadata/search/elasticsearch/SystemMetadataServiceElasticSearchTest.java
Patch:
@@ -1,5 +1,7 @@
 package com.linkedin.metadata.search.elasticsearch;
 
+import static org.testng.Assert.assertNotNull;
+
 import com.linkedin.metadata.search.elasticsearch.indexbuilder.ESIndexBuilder;
 import com.linkedin.metadata.search.elasticsearch.update.ESBulkProcessor;
 import com.linkedin.metadata.systemmetadata.SystemMetadataServiceTestBase;
@@ -8,7 +10,6 @@
 import org.opensearch.client.RestHighLevelClient;
 import org.springframework.beans.factory.annotation.Autowired;
 import org.springframework.context.annotation.Import;
-import org.testng.AssertJUnit;
 import org.testng.annotations.Test;
 
 @Import({ElasticSearchSuite.class, SearchTestContainerConfiguration.class})
@@ -38,6 +39,6 @@ protected ESIndexBuilder getIndexBuilder() {
 
   @Test
   public void initTest() {
-    AssertJUnit.assertNotNull(_searchClient);
+    assertNotNull(_searchClient);
   }
 }

File: metadata-io/src/test/java/com/linkedin/metadata/search/elasticsearch/TimeseriesAspectServiceElasticSearchTest.java
Patch:
@@ -1,5 +1,7 @@
 package com.linkedin.metadata.search.elasticsearch;
 
+import static org.testng.Assert.assertNotNull;
+
 import com.linkedin.metadata.search.elasticsearch.indexbuilder.ESIndexBuilder;
 import com.linkedin.metadata.search.elasticsearch.update.ESBulkProcessor;
 import com.linkedin.metadata.timeseries.search.TimeseriesAspectServiceTestBase;
@@ -9,7 +11,6 @@
 import org.springframework.beans.factory.annotation.Autowired;
 import org.springframework.beans.factory.annotation.Qualifier;
 import org.springframework.context.annotation.Import;
-import org.testng.AssertJUnit;
 import org.testng.annotations.Test;
 
 @Import({ElasticSearchSuite.class, SearchTestContainerConfiguration.class})
@@ -42,6 +43,6 @@ protected ESIndexBuilder getIndexBuilder() {
 
   @Test
   public void initTest() {
-    AssertJUnit.assertNotNull(_searchClient);
+    assertNotNull(_searchClient);
   }
 }

File: metadata-io/src/test/java/com/linkedin/metadata/search/opensearch/GoldenOpenSearchTest.java
Patch:
@@ -1,6 +1,6 @@
 package com.linkedin.metadata.search.opensearch;
 
-import static org.testng.AssertJUnit.assertNotNull;
+import static org.testng.Assert.assertNotNull;
 
 import com.linkedin.metadata.search.SearchService;
 import com.linkedin.metadata.search.fixtures.GoldenTestBase;

File: metadata-io/src/test/java/com/linkedin/metadata/search/opensearch/IndexBuilderOpenSearchTest.java
Patch:
@@ -1,6 +1,6 @@
 package com.linkedin.metadata.search.opensearch;
 
-import static org.testng.AssertJUnit.assertNotNull;
+import static org.testng.Assert.assertNotNull;
 
 import com.linkedin.metadata.search.indexbuilder.IndexBuilderTestBase;
 import io.datahubproject.test.search.config.SearchTestContainerConfiguration;

File: metadata-io/src/test/java/com/linkedin/metadata/search/opensearch/LineageDataFixtureOpenSearchTest.java
Patch:
@@ -1,5 +1,7 @@
 package com.linkedin.metadata.search.opensearch;
 
+import static org.testng.Assert.assertNotNull;
+
 import com.linkedin.metadata.search.LineageSearchService;
 import com.linkedin.metadata.search.SearchService;
 import com.linkedin.metadata.search.fixtures.LineageDataFixtureTestBase;
@@ -10,7 +12,6 @@
 import org.springframework.beans.factory.annotation.Autowired;
 import org.springframework.beans.factory.annotation.Qualifier;
 import org.springframework.context.annotation.Import;
-import org.testng.AssertJUnit;
 import org.testng.annotations.Test;
 
 @Getter
@@ -35,6 +36,6 @@ public class LineageDataFixtureOpenSearchTest extends LineageDataFixtureTestBase
 
   @Test
   public void initTest() {
-    AssertJUnit.assertNotNull(lineageService);
+    assertNotNull(lineageService);
   }
 }

File: metadata-io/src/test/java/com/linkedin/metadata/search/opensearch/SystemMetadataServiceOpenSearchTest.java
Patch:
@@ -1,5 +1,7 @@
 package com.linkedin.metadata.search.opensearch;
 
+import static org.testng.Assert.assertNotNull;
+
 import com.linkedin.metadata.search.elasticsearch.indexbuilder.ESIndexBuilder;
 import com.linkedin.metadata.search.elasticsearch.update.ESBulkProcessor;
 import com.linkedin.metadata.systemmetadata.SystemMetadataServiceTestBase;
@@ -8,7 +10,6 @@
 import org.opensearch.client.RestHighLevelClient;
 import org.springframework.beans.factory.annotation.Autowired;
 import org.springframework.context.annotation.Import;
-import org.testng.AssertJUnit;
 import org.testng.annotations.Test;
 
 @Import({OpenSearchSuite.class, SearchTestContainerConfiguration.class})
@@ -38,6 +39,6 @@ protected ESIndexBuilder getIndexBuilder() {
 
   @Test
   public void initTest() {
-    AssertJUnit.assertNotNull(_searchClient);
+    assertNotNull(_searchClient);
   }
 }

File: metadata-io/src/test/java/com/linkedin/metadata/search/opensearch/TimeseriesAspectServiceOpenSearchTest.java
Patch:
@@ -1,5 +1,7 @@
 package com.linkedin.metadata.search.opensearch;
 
+import static org.testng.Assert.assertNotNull;
+
 import com.linkedin.metadata.search.elasticsearch.indexbuilder.ESIndexBuilder;
 import com.linkedin.metadata.search.elasticsearch.update.ESBulkProcessor;
 import com.linkedin.metadata.timeseries.search.TimeseriesAspectServiceTestBase;
@@ -9,7 +11,6 @@
 import org.springframework.beans.factory.annotation.Autowired;
 import org.springframework.beans.factory.annotation.Qualifier;
 import org.springframework.context.annotation.Import;
-import org.testng.AssertJUnit;
 import org.testng.annotations.Test;
 
 @Import({OpenSearchSuite.class, SearchTestContainerConfiguration.class})
@@ -42,6 +43,6 @@ protected ESIndexBuilder getIndexBuilder() {
 
   @Test
   public void initTest() {
-    AssertJUnit.assertNotNull(_searchClient);
+    assertNotNull(_searchClient);
   }
 }

File: metadata-io/src/test/java/com/linkedin/metadata/search/query/request/AggregationQueryBuilderTest.java
Patch:
@@ -41,7 +41,7 @@ public class AggregationQueryBuilderTest {
   private static AspectRetriever aspectRetrieverV1;
 
   @BeforeClass
-  public static void setup() throws RemoteInvocationException, URISyntaxException {
+  public void setup() throws RemoteInvocationException, URISyntaxException {
     Urn helloUrn = Urn.createFromString("urn:li:structuredProperty:hello");
     Urn abFghTenUrn = Urn.createFromString("urn:li:structuredProperty:ab.fgh.ten");
 

File: metadata-io/src/test/java/com/linkedin/metadata/search/utils/ESUtilsTest.java
Patch:
@@ -38,7 +38,7 @@ public class ESUtilsTest {
   private static AspectRetriever aspectRetrieverV1;
 
   @BeforeClass
-  public static void setup() throws RemoteInvocationException, URISyntaxException {
+  public void setup() throws RemoteInvocationException, URISyntaxException {
     Urn abFghTenUrn = Urn.createFromString("urn:li:structuredProperty:ab.fgh.ten");
 
     // legacy

File: metadata-io/src/test/java/com/linkedin/metadata/structuredproperties/validators/PropertyDefinitionValidatorTest.java
Patch:
@@ -2,7 +2,7 @@
 
 import static org.mockito.Mockito.mock;
 import static org.mockito.Mockito.when;
-import static org.testng.AssertJUnit.assertEquals;
+import static org.testng.Assert.assertEquals;
 
 import com.linkedin.common.UrnArray;
 import com.linkedin.common.urn.Urn;

File: metadata-io/src/test/java/com/linkedin/metadata/timeline/eventgenerator/SchemaMetadataChangeEventGeneratorTest.java
Patch:
@@ -1,6 +1,6 @@
 package com.linkedin.metadata.timeline.eventgenerator;
 
-import static org.testng.AssertJUnit.assertEquals;
+import static org.testng.Assert.assertEquals;
 
 import com.linkedin.common.AuditStamp;
 import com.linkedin.common.urn.Urn;

File: datahub-frontend/app/auth/sso/oidc/OidcConfigs.java
Patch:
@@ -243,6 +243,9 @@ public Builder from(final com.typesafe.config.Config configs, final String ssoSe
             Optional.ofNullable(getOptional(configs, OIDC_PREFERRED_JWS_ALGORITHM, null));
       }
 
+      grantType = Optional.ofNullable(getOptional(configs, OIDC_GRANT_TYPE, null));
+      acrValues = Optional.ofNullable(getOptional(configs, OIDC_ACR_VALUES, null));
+
       return this;
     }
 

File: datahub-frontend/app/auth/sso/oidc/custom/CustomOidcClient.java
Patch:
@@ -18,7 +18,7 @@ public CustomOidcClient(final OidcConfiguration configuration) {
   protected void clientInit() {
     CommonHelper.assertNotNull("configuration", getConfiguration());
     getConfiguration().init();
-    defaultRedirectionActionBuilder(new OidcRedirectionActionBuilder(getConfiguration(), this));
+    defaultRedirectionActionBuilder(new CustomOidcRedirectionActionBuilder(getConfiguration(), this));
     defaultCredentialsExtractor(new OidcExtractor(getConfiguration(), this));
     defaultAuthenticator(new CustomOidcAuthenticator(this));
     defaultProfileCreator(new OidcProfileCreator<>(getConfiguration(), this));

File: metadata-service/configuration/src/main/java/com/linkedin/metadata/config/search/custom/CustomSearchConfiguration.java
Patch:
@@ -7,10 +7,12 @@
 import lombok.Builder;
 import lombok.EqualsAndHashCode;
 import lombok.Getter;
+import lombok.ToString;
 
 @Builder(toBuilder = true)
 @Getter
 @EqualsAndHashCode
+@ToString
 @JsonDeserialize(builder = CustomSearchConfiguration.CustomSearchConfigurationBuilder.class)
 public class CustomSearchConfiguration {
 

File: datahub-graphql-core/src/test/java/com/linkedin/datahub/graphql/resolvers/structuredproperties/UpdateStructuredPropertyResolverTest.java
Patch:
@@ -89,8 +89,8 @@ public void testGetFailure() throws Exception {
 
     assertThrows(CompletionException.class, () -> resolver.get(mockEnv).join());
 
-    // Validate that ingest was called, but that caused a failure
-    Mockito.verify(mockEntityClient, Mockito.times(1))
+    // Validate that ingest was not called since there was a get failure before ingesting
+    Mockito.verify(mockEntityClient, Mockito.times(0))
         .ingestProposal(any(), any(MetadataChangeProposal.class), Mockito.eq(false));
   }
 

File: metadata-io/src/main/java/com/linkedin/metadata/search/elasticsearch/query/request/SearchQueryBuilder.java
Patch:
@@ -78,7 +78,7 @@ public QueryBuilder buildQuery(
 
     final QueryBuilder queryBuilder =
         buildInternalQuery(opContext, customQueryConfig, entitySpecs, query, fulltext);
-    return buildScoreFunctions(opContext, customQueryConfig, entitySpecs, queryBuilder);
+    return buildScoreFunctions(opContext, customQueryConfig, entitySpecs, query, queryBuilder);
   }
 
   /**
@@ -485,12 +485,13 @@ static FunctionScoreQueryBuilder buildScoreFunctions(
       @Nonnull OperationContext opContext,
       @Nullable QueryConfiguration customQueryConfig,
       @Nonnull List<EntitySpec> entitySpecs,
+      String query,
       @Nonnull QueryBuilder queryBuilder) {
 
     if (customQueryConfig != null) {
       // Prefer configuration function scoring over annotation scoring
       return CustomizedQueryHandler.functionScoreQueryBuilder(
-          opContext.getObjectMapper(), customQueryConfig, queryBuilder);
+          opContext.getObjectMapper(), customQueryConfig, queryBuilder, query);
     } else {
       return QueryBuilders.functionScoreQuery(
               queryBuilder, buildAnnotationScoreFunctions(entitySpecs))

File: metadata-io/src/test/java/com/linkedin/metadata/search/query/request/CustomizedQueryHandlerTest.java
Patch:
@@ -179,7 +179,7 @@ public void functionScoreQueryBuilderTest() {
      */
     FunctionScoreQueryBuilder selectStarTest =
         CustomizedQueryHandler.functionScoreQueryBuilder(
-            new ObjectMapper(), test.lookupQueryConfig("*").get(), inputQuery);
+            new ObjectMapper(), test.lookupQueryConfig("*").get(), inputQuery, "*");
 
     FunctionScoreQueryBuilder.FilterFunctionBuilder[] expectedSelectStarScoreFunctions = {
       new FunctionScoreQueryBuilder.FilterFunctionBuilder(
@@ -203,7 +203,7 @@ public void functionScoreQueryBuilderTest() {
      */
     FunctionScoreQueryBuilder defaultTest =
         CustomizedQueryHandler.functionScoreQueryBuilder(
-            new ObjectMapper(), test.lookupQueryConfig("foobar").get(), inputQuery);
+            new ObjectMapper(), test.lookupQueryConfig("foobar").get(), inputQuery, "foobar");
 
     FunctionScoreQueryBuilder.FilterFunctionBuilder[] expectedDefaultScoreFunctions = {
       new FunctionScoreQueryBuilder.FilterFunctionBuilder(

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/assertion/AssertionRunEventResolver.java
Patch:
@@ -12,6 +12,7 @@
 import com.linkedin.datahub.graphql.generated.AssertionRunStatus;
 import com.linkedin.datahub.graphql.generated.FacetFilterInput;
 import com.linkedin.datahub.graphql.generated.FilterInput;
+import com.linkedin.datahub.graphql.resolvers.ResolverUtils;
 import com.linkedin.datahub.graphql.types.dataset.mappers.AssertionRunEventMapper;
 import com.linkedin.entity.client.EntityClient;
 import com.linkedin.metadata.Constants;
@@ -147,7 +148,7 @@ public static Filter buildFilter(
                     .setAnd(
                         new CriterionArray(
                             facetFilters.stream()
-                                .map(filter -> criterionFromFilter(filter, true, aspectRetriever))
+                                .map(ResolverUtils::criterionFromFilter)
                                 .collect(Collectors.toList())))));
   }
 }

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/chart/BrowseV2Resolver.java
Patch:
@@ -74,9 +74,7 @@ public CompletableFuture<BrowseResultsV2> get(DataFetchingEnvironment environmen
                     ? BROWSE_PATH_V2_DELIMITER
                         + String.join(BROWSE_PATH_V2_DELIMITER, input.getPath())
                     : "";
-            final Filter inputFilter =
-                ResolverUtils.buildFilter(
-                    null, input.getOrFilters(), context.getOperationContext().getAspectRetriever());
+            final Filter inputFilter = ResolverUtils.buildFilter(null, input.getOrFilters());
 
             BrowseResultV2 browseResults =
                 _entityClient.browseV2(

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/domain/DomainEntitiesResolver.java
Patch:
@@ -79,9 +79,7 @@ public CompletableFuture<SearchResults> get(final DataFetchingEnvironment enviro
                   .getFilters()
                   .forEach(
                       filter -> {
-                        criteria.add(
-                            criterionFromFilter(
-                                filter, true, context.getOperationContext().getAspectRetriever()));
+                        criteria.add(criterionFromFilter(filter));
                       });
             }
 

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/form/CreateDynamicFormAssignmentResolver.java
Patch:
@@ -33,9 +33,7 @@ public CompletableFuture<Boolean> get(final DataFetchingEnvironment environment)
     final CreateDynamicFormAssignmentInput input =
         bindArgument(environment.getArgument("input"), CreateDynamicFormAssignmentInput.class);
     final Urn formUrn = UrnUtils.getUrn(input.getFormUrn());
-    final DynamicFormAssignment formAssignment =
-        FormUtils.mapDynamicFormAssignment(
-            input, context.getOperationContext().getAspectRetriever());
+    final DynamicFormAssignment formAssignment = FormUtils.mapDynamicFormAssignment(input);
 
     return GraphQLConcurrencyUtils.supplyAsync(
         () -> {

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/recommendation/ListRecommendationsResolver.java
Patch:
@@ -17,6 +17,7 @@
 import com.linkedin.datahub.graphql.generated.RecommendationRenderType;
 import com.linkedin.datahub.graphql.generated.RecommendationRequestContext;
 import com.linkedin.datahub.graphql.generated.SearchParams;
+import com.linkedin.datahub.graphql.resolvers.ResolverUtils;
 import com.linkedin.datahub.graphql.types.common.mappers.UrnToEntityMapper;
 import com.linkedin.datahub.graphql.types.entitytype.EntityTypeMapper;
 import com.linkedin.metadata.query.filter.CriterionArray;
@@ -105,9 +106,7 @@ private com.linkedin.metadata.recommendation.RecommendationRequestContext mapReq
         searchRequestContext.setFilters(
             new CriterionArray(
                 requestContext.getSearchRequestContext().getFilters().stream()
-                    .map(
-                        facetField ->
-                            criterionFromFilter(facetField, opContext.getAspectRetriever()))
+                    .map(ResolverUtils::criterionFromFilter)
                     .collect(Collectors.toList())));
       }
       mappedRequestContext.setSearchRequestContext(searchRequestContext);

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/search/AggregateAcrossEntitiesResolver.java
Patch:
@@ -64,9 +64,7 @@ public CompletableFuture<AggregateResults> get(DataFetchingEnvironment environme
                       UrnUtils.getUrn(input.getViewUrn()))
                   : null;
 
-          final Filter inputFilter =
-              ResolverUtils.buildFilter(
-                  null, input.getOrFilters(), context.getOperationContext().getAspectRetriever());
+          final Filter inputFilter = ResolverUtils.buildFilter(null, input.getOrFilters());
 
           final SearchFlags searchFlags = mapInputFlags(context, input.getSearchFlags());
 

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/search/ScrollAcrossEntitiesResolver.java
Patch:
@@ -72,9 +72,7 @@ public CompletableFuture<ScrollResults> get(DataFetchingEnvironment environment)
                       UrnUtils.getUrn(input.getViewUrn()))
                   : null;
 
-          final Filter baseFilter =
-              ResolverUtils.buildFilter(
-                  null, input.getOrFilters(), context.getOperationContext().getAspectRetriever());
+          final Filter baseFilter = ResolverUtils.buildFilter(null, input.getOrFilters());
           final SearchFlags searchFlags;
           com.linkedin.datahub.graphql.generated.SearchFlags inputFlags = input.getSearchFlags();
           if (inputFlags != null) {

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/view/ListMyViewsResolver.java
Patch:
@@ -132,6 +132,6 @@ private Filter buildFilters(
     filterCriteria.setAnd(andConditions);
 
     // Currently, there is no way to fetch the views belonging to another user.
-    return buildFilter(Collections.emptyList(), ImmutableList.of(filterCriteria), aspectRetriever);
+    return buildFilter(Collections.emptyList(), ImmutableList.of(filterCriteria));
   }
 }

File: datahub-graphql-core/src/test/java/com/linkedin/datahub/graphql/resolvers/auth/ListAccessTokensResolverTest.java
Patch:
@@ -46,7 +46,7 @@ public void testGetSuccess() throws Exception {
                 any(),
                 Mockito.eq(Constants.ACCESS_TOKEN_ENTITY_NAME),
                 Mockito.eq(""),
-                Mockito.eq(buildFilter(filters, Collections.emptyList(), null)),
+                Mockito.eq(buildFilter(filters, Collections.emptyList())),
                 Mockito.any(List.class),
                 Mockito.eq(input.getStart()),
                 Mockito.eq(input.getCount())))

File: datahub-graphql-core/src/test/java/com/linkedin/datahub/graphql/resolvers/browse/BrowseV2ResolverTest.java
Patch:
@@ -2,7 +2,6 @@
 
 import static com.linkedin.datahub.graphql.TestUtils.getMockAllowContext;
 import static org.mockito.ArgumentMatchers.any;
-import static org.mockito.Mockito.mock;
 
 import com.google.common.collect.ImmutableList;
 import com.linkedin.common.AuditStamp;
@@ -18,7 +17,6 @@
 import com.linkedin.datahub.graphql.resolvers.ResolverUtils;
 import com.linkedin.datahub.graphql.resolvers.chart.BrowseV2Resolver;
 import com.linkedin.entity.client.EntityClient;
-import com.linkedin.metadata.aspect.AspectRetriever;
 import com.linkedin.metadata.browse.BrowseResultGroupV2;
 import com.linkedin.metadata.browse.BrowseResultGroupV2Array;
 import com.linkedin.metadata.browse.BrowseResultMetadata;
@@ -102,7 +100,7 @@ public static void testBrowseV2SuccessWithQueryAndFilter() throws Exception {
     facetFilterInput.setValues(ImmutableList.of("urn:li:corpuser:test"));
     andFilterInput.setAnd(ImmutableList.of(facetFilterInput));
     orFilters.add(andFilterInput);
-    Filter filter = ResolverUtils.buildFilter(null, orFilters, mock(AspectRetriever.class));
+    Filter filter = ResolverUtils.buildFilter(null, orFilters);
 
     EntityClient mockClient =
         initMockEntityClient(

File: datahub-graphql-core/src/test/java/com/linkedin/datahub/graphql/resolvers/query/ListQueriesResolverTest.java
Patch:
@@ -170,6 +170,6 @@ private Filter buildFilter(@Nullable QuerySource source, @Nullable String entity
               FilterOperator.EQUAL));
     }
     criteria.setAnd(andConditions);
-    return ResolverUtils.buildFilter(Collections.emptyList(), ImmutableList.of(criteria), null);
+    return ResolverUtils.buildFilter(Collections.emptyList(), ImmutableList.of(criteria));
   }
 }

File: datahub-graphql-core/src/test/java/com/linkedin/datahub/graphql/resolvers/search/AggregateAcrossEntitiesResolverTest.java
Patch:
@@ -107,7 +107,7 @@ public static void testApplyViewBaseFilter() throws Exception {
     FormService mockFormService = Mockito.mock(FormService.class);
     ViewService mockService = initMockViewService(TEST_VIEW_URN, info);
 
-    Filter baseFilter = createFilter("baseField.keyword", "baseTest");
+    Filter baseFilter = createFilter("baseField", "baseTest");
 
     EntityClient mockClient =
         initMockEntityClient(

File: datahub-graphql-core/src/test/java/com/linkedin/datahub/graphql/resolvers/search/SearchAcrossEntitiesResolverTest.java
Patch:
@@ -164,7 +164,7 @@ public static void testApplyViewBaseFilter() throws Exception {
                             new CriterionArray(
                                 ImmutableList.of(
                                     new Criterion()
-                                        .setField("baseField.keyword")
+                                        .setField("baseField")
                                         .setValue("baseTest")
                                         .setCondition(Condition.EQUAL)
                                         .setNegated(false)

File: datahub-graphql-core/src/test/java/com/linkedin/datahub/graphql/resolvers/view/CreateViewResolverTest.java
Patch:
@@ -110,7 +110,7 @@ public void testGetSuccess() throws Exception {
                                                     ImmutableList.of(
                                                         new Criterion()
                                                             .setCondition(Condition.EQUAL)
-                                                            .setField("test1.keyword")
+                                                            .setField("test1")
                                                             .setValue(
                                                                 "value1") // Unfortunate --- For
                                                             // backwards compat.
@@ -121,7 +121,7 @@ public void testGetSuccess() throws Exception {
                                                             .setNegated(false),
                                                         new Criterion()
                                                             .setCondition(Condition.IN)
-                                                            .setField("test2.keyword")
+                                                            .setField("test2")
                                                             .setValue(
                                                                 "value1") // Unfortunate --- For
                                                             // backwards compat.

File: datahub-graphql-core/src/test/java/com/linkedin/datahub/graphql/resolvers/view/ListGlobalViewsResolverTest.java
Patch:
@@ -56,7 +56,7 @@ public void testGetSuccessInput() throws Exception {
                                             new CriterionArray(
                                                 ImmutableList.of(
                                                     new Criterion()
-                                                        .setField("type.keyword")
+                                                        .setField("type")
                                                         .setValue(DataHubViewType.GLOBAL.toString())
                                                         .setValues(
                                                             new StringArray(

File: datahub-graphql-core/src/test/java/com/linkedin/datahub/graphql/resolvers/view/ListMyViewsResolverTest.java
Patch:
@@ -58,7 +58,7 @@ public void testGetSuccessInput1() throws Exception {
                                             new CriterionArray(
                                                 ImmutableList.of(
                                                     new Criterion()
-                                                        .setField("createdBy.keyword")
+                                                        .setField("createdBy")
                                                         .setValue(TEST_USER.toString())
                                                         .setValues(
                                                             new StringArray(
@@ -67,7 +67,7 @@ public void testGetSuccessInput1() throws Exception {
                                                         .setCondition(Condition.EQUAL)
                                                         .setNegated(false),
                                                     new Criterion()
-                                                        .setField("type.keyword")
+                                                        .setField("type")
                                                         .setValue(DataHubViewType.GLOBAL.toString())
                                                         .setValues(
                                                             new StringArray(
@@ -124,7 +124,7 @@ public void testGetSuccessInput2() throws Exception {
                                             new CriterionArray(
                                                 ImmutableList.of(
                                                     new Criterion()
-                                                        .setField("createdBy.keyword")
+                                                        .setField("createdBy")
                                                         .setValue(TEST_USER.toString())
                                                         .setValues(
                                                             new StringArray(

File: metadata-io/src/test/java/io/datahubproject/test/search/SearchTestUtils.java
Patch:
@@ -2,7 +2,6 @@
 
 import static com.linkedin.datahub.graphql.resolvers.search.SearchUtils.AUTO_COMPLETE_ENTITY_TYPES;
 import static com.linkedin.datahub.graphql.resolvers.search.SearchUtils.SEARCHABLE_ENTITY_TYPES;
-import static org.mockito.Mockito.mock;
 
 import com.datahub.authentication.Authentication;
 import com.datahub.plugins.auth.authorization.Authorizer;
@@ -14,7 +13,6 @@
 import com.linkedin.datahub.graphql.resolvers.ResolverUtils;
 import com.linkedin.datahub.graphql.types.SearchableEntityType;
 import com.linkedin.datahub.graphql.types.entitytype.EntityTypeMapper;
-import com.linkedin.metadata.aspect.AspectRetriever;
 import com.linkedin.metadata.config.DataHubAppConfiguration;
 import com.linkedin.metadata.config.search.GraphQueryConfiguration;
 import com.linkedin.metadata.graph.LineageDirection;
@@ -188,7 +186,7 @@ public static LineageSearchResult lineage(
             .collect(Collectors.toList()),
         "*",
         hops,
-        ResolverUtils.buildFilter(filters, List.of(), mock(AspectRetriever.class)),
+        ResolverUtils.buildFilter(filters, List.of()),
         null,
         0,
         100);

File: entity-registry/src/test/java/com/linkedin/metadata/models/EntitySpecBuilderTest.java
Patch:
@@ -189,7 +189,7 @@ private void validateTestEntityInfo(final AspectSpec testEntityInfo) {
         testEntityInfo.getPegasusSchema().getFullName());
 
     // Assert on Searchable Fields
-    assertEquals(testEntityInfo.getSearchableFieldSpecs().size(), 12);
+    assertEquals(testEntityInfo.getSearchableFieldSpecs().size(), 17);
     assertEquals(
         "customProperties",
         testEntityInfo

File: metadata-io/src/test/java/com/linkedin/metadata/search/indexbuilder/MappingsBuilderTest.java
Patch:
@@ -31,7 +31,7 @@ public void testMappingsBuilder() {
     Map<String, Object> result = MappingsBuilder.getMappings(TestEntitySpecBuilder.getSpec());
     assertEquals(result.size(), 1);
     Map<String, Object> properties = (Map<String, Object>) result.get("properties");
-    assertEquals(properties.size(), 22);
+    assertEquals(properties.size(), 27);
     assertEquals(
         properties.get("urn"),
         ImmutableMap.of(

File: datahub-upgrade/src/main/java/com/linkedin/datahub/upgrade/nocode/CreateAspectTableStep.java
Patch:
@@ -77,7 +77,7 @@ public Function<UpgradeContext, UpgradeStepResult> executable() {
       }
 
       try {
-        _server.execute(_server.createSqlUpdate(sqlUpdateStr));
+        _server.execute(_server.sqlUpdate(sqlUpdateStr));
       } catch (Exception e) {
         context.report().addLine("Failed to create table metadata_aspect_v2", e);
         return new DefaultUpgradeStepResult(id(), DataHubUpgradeState.FAILED);

File: metadata-io/src/main/java/com/linkedin/metadata/entity/EntityServiceImpl.java
Patch:
@@ -85,6 +85,7 @@
 import com.linkedin.util.Pair;
 import io.datahubproject.metadata.context.OperationContext;
 import io.opentelemetry.extension.annotations.WithSpan;
+import jakarta.persistence.EntityNotFoundException;
 import java.net.URISyntaxException;
 import java.nio.charset.StandardCharsets;
 import java.sql.Timestamp;
@@ -111,7 +112,6 @@
 import java.util.stream.StreamSupport;
 import javax.annotation.Nonnull;
 import javax.annotation.Nullable;
-import javax.persistence.EntityNotFoundException;
 import lombok.Getter;
 import lombok.extern.slf4j.Slf4j;
 

File: metadata-jobs/mce-consumer-job/src/main/java/com/linkedin/metadata/restli/RestliServletConfig.java
Patch:
@@ -2,6 +2,7 @@
 
 import com.datahub.auth.authentication.filter.AuthenticationFilter;
 import com.linkedin.gms.factory.auth.SystemAuthenticationFactory;
+import com.linkedin.r2.transport.http.server.RAPJakartaServlet;
 import com.linkedin.restli.server.RestliHandlerServlet;
 import java.util.Collections;
 import org.springframework.beans.factory.annotation.Qualifier;
@@ -36,8 +37,8 @@ public ServletRegistrationBean<RestliHandlerServlet> restliServletRegistration(
   }
 
   @Bean("restliHandlerServlet")
-  public RestliHandlerServlet restliHandlerServlet() {
-    return new RestliHandlerServlet();
+  public RestliHandlerServlet restliHandlerServlet(final RAPJakartaServlet r2Servlet) {
+    return new RestliHandlerServlet(r2Servlet);
   }
 
   @Bean

File: metadata-service/auth-filter/src/main/java/com/datahub/auth/authentication/filter/AuthenticationFilter.java
Patch:
@@ -77,6 +77,7 @@ public class AuthenticationFilter implements Filter {
   public void init(FilterConfig filterConfig) throws ServletException {
     SpringBeanAutowiringSupport.processInjectionBasedOnCurrentContext(this);
     buildAuthenticatorChain();
+    log.info("AuthenticationFilter initialized.");
   }
 
   @Override

File: metadata-service/war/src/main/java/com/linkedin/gms/servlet/RestliServletConfig.java
Patch:
@@ -7,14 +7,14 @@
 import org.springframework.context.annotation.ComponentScan;
 import org.springframework.context.annotation.Configuration;
 import org.springframework.context.annotation.PropertySource;
-import org.springframework.web.HttpRequestHandler;
+import org.springframework.web.context.support.HttpRequestHandlerServlet;
 
 @ComponentScan(basePackages = {"com.linkedin.restli.server"})
 @PropertySource(value = "classpath:/application.yaml", factory = YamlPropertySourceFactory.class)
 @Configuration
 public class RestliServletConfig {
   @Bean("restliRequestHandler")
-  public HttpRequestHandler restliHandlerServlet(final RAPJakartaServlet r2Servlet) {
+  public HttpRequestHandlerServlet restliHandlerServlet(final RAPJakartaServlet r2Servlet) {
     return new RestliHandlerServlet(r2Servlet);
   }
 }

File: metadata-io/src/main/java/com/linkedin/metadata/entity/EntityServiceImpl.java
Patch:
@@ -1041,6 +1041,7 @@ private List<UpdateAspectResult> ingestAspectsToLocalDB(
             inputBatch,
             DEFAULT_MAX_TRANSACTION_RETRY)
         .stream()
+        .filter(Objects::nonNull)
         .flatMap(List::stream)
         .collect(Collectors.toList());
   }

File: metadata-io/src/test/java/com/linkedin/metadata/TestEntityUtil.java
Patch:
@@ -65,6 +65,8 @@ public static TestEntityInfo getTestEntityInfo(Urn urn) {
                 "value1",
                 "key2",
                 "value2",
+                "key3",
+                "",
                 "shortValue",
                 "123",
                 "longValue",

File: metadata-io/src/main/java/com/linkedin/metadata/search/elasticsearch/query/ESBrowseDAO.java
Patch:
@@ -87,7 +87,7 @@ public class ESBrowseDAO {
 
   private static final SearchFlags DEFAULT_BROWSE_SEARCH_FLAGS =
       new SearchFlags()
-          .setFulltext(false)
+          .setFulltext(true)
           .setSkipHighlighting(true)
           .setGetSuggestions(false)
           .setIncludeSoftDeleted(false)

File: metadata-io/src/main/java/com/linkedin/metadata/timeline/eventgenerator/SchemaMetadataChangeEventGenerator.java
Patch:
@@ -15,6 +15,7 @@
 import com.linkedin.metadata.timeline.data.ChangeTransaction;
 import com.linkedin.metadata.timeline.data.SemanticChangeType;
 import com.linkedin.metadata.timeline.data.dataset.DatasetSchemaFieldChangeEvent;
+import com.linkedin.metadata.timeline.data.dataset.SchemaFieldModificationCategory;
 import com.linkedin.schema.SchemaField;
 import com.linkedin.schema.SchemaFieldArray;
 import com.linkedin.schema.SchemaMetadata;
@@ -246,6 +247,7 @@ private static void processFieldPathDataTypeChange(
                 .fieldPath(curBaseField.getFieldPath())
                 .fieldUrn(getSchemaFieldUrn(datasetUrn, curBaseField))
                 .nullable(curBaseField.isNullable())
+                .modificationCategory(SchemaFieldModificationCategory.TYPE_CHANGE)
                 .auditStamp(auditStamp)
                 .build());
       }
@@ -483,6 +485,7 @@ private static ChangeEvent generateRenameEvent(
         .fieldPath(curBaseField.getFieldPath())
         .fieldUrn(getSchemaFieldUrn(datasetUrn, curBaseField))
         .nullable(curBaseField.isNullable())
+        .modificationCategory(SchemaFieldModificationCategory.RENAME)
         .auditStamp(auditStamp)
         .build();
   }

File: metadata-io/src/main/java/com/linkedin/metadata/search/elasticsearch/query/request/SearchQueryBuilder.java
Patch:
@@ -52,7 +52,7 @@
 
 @Slf4j
 public class SearchQueryBuilder {
-  public static final String STRUCTURED_QUERY_PREFIX = "\\\\/q ";
+  public static final String STRUCTURED_QUERY_PREFIX = "\\/q ";
   private final ExactMatchConfiguration exactMatchConfiguration;
   private final PartialConfiguration partialConfiguration;
   private final WordGramConfiguration wordGramConfiguration;

File: datahub-frontend/app/utils/SearchUtil.java
Patch:
@@ -20,7 +20,7 @@ private SearchUtil() {
   @Nonnull
   public static String escapeForwardSlash(@Nonnull String input) {
     if (input.contains("/")) {
-      input = input.replace("/", "\\\\/");
+      input = input.replace("/", "\\/");
     }
     return input;
   }

File: datahub-frontend/test/utils/SearchUtilTest.java
Patch:
@@ -8,10 +8,10 @@ public class SearchUtilTest {
   @Test
   public void testEscapeForwardSlash() {
     // escape "/"
-    assertEquals("\\\\/foo\\\\/bar", SearchUtil.escapeForwardSlash("/foo/bar"));
+    assertEquals("\\/foo\\/bar", SearchUtil.escapeForwardSlash("/foo/bar"));
     // "/" is escaped but "*" is not escaped and is treated as regex. Since currently we want to
     // retain the regex behaviour with "*"
-    assertEquals("\\\\/foo\\\\/bar\\\\/*", SearchUtil.escapeForwardSlash("/foo/bar/*"));
+    assertEquals("\\/foo\\/bar\\/*", SearchUtil.escapeForwardSlash("/foo/bar/*"));
     assertEquals("", "");
     assertEquals("foo", "foo");
   }

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/ResolverUtils.java
Patch:
@@ -70,7 +70,7 @@ public static <T> T bindArgument(Object argument, Class<T> clazz) {
   @Nonnull
   public static String escapeForwardSlash(@Nonnull String input) {
     if (input.contains("/")) {
-      input = input.replace("/", "\\\\/");
+      input = input.replace("/", "\\/");
     }
     return input;
   }

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/mutate/util/FormUtils.java
Patch:
@@ -202,7 +202,7 @@ public static FormActorAssignment mapFormActorAssignment(
     if (input.getGroups() != null) {
       UrnArray groupUrns = new UrnArray();
       input.getGroups().forEach(group -> groupUrns.add(UrnUtils.getUrn(group)));
-      result.setUsers(groupUrns);
+      result.setGroups(groupUrns);
     }
 
     return result;

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/mutate/util/OwnerUtils.java
Patch:
@@ -171,7 +171,7 @@ public static boolean isOwnerEqual(
     if (!owner.getOwner().equals(ownerUrn)) {
       return false;
     }
-    if (owner.getTypeUrn() != null) {
+    if (owner.getTypeUrn() != null && ownershipTypeUrn != null) {
       return owner.getTypeUrn().equals(ownershipTypeUrn);
     }
     if (ownershipTypeUrn == null) {

File: entity-registry/src/main/java/com/linkedin/metadata/aspect/models/graph/Edge.java
Patch:
@@ -13,6 +13,7 @@
 import java.util.Map;
 import java.util.Optional;
 import java.util.stream.Collectors;
+import javax.annotation.Nonnull;
 import lombok.AllArgsConstructor;
 import lombok.Data;
 import lombok.EqualsAndHashCode;
@@ -59,7 +60,7 @@ public Edge(
         null);
   }
 
-  public String toDocId() {
+  public String toDocId(@Nonnull String idHashAlgo) {
     StringBuilder rawDocId = new StringBuilder();
     rawDocId
         .append(getSource().toString())
@@ -72,9 +73,8 @@ public String toDocId() {
     }
 
     try {
-      String hashAlgo = System.getenv("ELASTIC_ID_HASH_ALGO");
       byte[] bytesOfRawDocID = rawDocId.toString().getBytes(StandardCharsets.UTF_8);
-      MessageDigest md = MessageDigest.getInstance(hashAlgo);
+      MessageDigest md = MessageDigest.getInstance(idHashAlgo);
       byte[] thedigest = md.digest(bytesOfRawDocID);
       return Base64.getEncoder().encodeToString(thedigest);
     } catch (NoSuchAlgorithmException e) {

File: metadata-io/src/main/java/com/linkedin/metadata/graph/elastic/ElasticSearchGraphService.java
Patch:
@@ -64,6 +64,7 @@ public class ElasticSearchGraphService implements GraphService, ElasticSearchInd
   private final ESGraphWriteDAO _graphWriteDAO;
   private final ESGraphQueryDAO _graphReadDAO;
   private final ESIndexBuilder _indexBuilder;
+  private final String idHashAlgo;
   public static final String INDEX_NAME = "graph_service_v1";
   private static final Map<String, Object> EMPTY_HASH = new HashMap<>();
 
@@ -125,7 +126,7 @@ public LineageRegistry getLineageRegistry() {
 
   @Override
   public void addEdge(@Nonnull final Edge edge) {
-    String docId = edge.toDocId();
+    String docId = edge.toDocId(idHashAlgo);
     String edgeDocument = toDocument(edge);
     _graphWriteDAO.upsertDocument(docId, edgeDocument);
   }
@@ -137,7 +138,7 @@ public void upsertEdge(@Nonnull final Edge edge) {
 
   @Override
   public void removeEdge(@Nonnull final Edge edge) {
-    String docId = edge.toDocId();
+    String docId = edge.toDocId(idHashAlgo);
     _graphWriteDAO.deleteDocument(docId);
   }
 

File: metadata-io/src/test/java/com/linkedin/metadata/graph/search/SearchGraphServiceTestBase.java
Patch:
@@ -62,7 +62,7 @@ public abstract class SearchGraphServiceTestBase extends GraphServiceTestBase {
   @Nonnull
   protected abstract ESIndexBuilder getIndexBuilder();
 
-  private final IndexConvention _indexConvention = IndexConventionImpl.NO_PREFIX;
+  private final IndexConvention _indexConvention = IndexConventionImpl.noPrefix("MD5");
   private final String _indexName = _indexConvention.getIndexName(INDEX_NAME);
   private ElasticSearchGraphService _client;
 
@@ -108,7 +108,8 @@ private ElasticSearchGraphService buildService(boolean enableMultiPathSearch) {
         _indexConvention,
         writeDAO,
         readDAO,
-        getIndexBuilder());
+        getIndexBuilder(),
+        "MD5");
   }
 
   @Override

File: metadata-io/src/test/java/com/linkedin/metadata/search/LineageServiceTestBase.java
Patch:
@@ -122,7 +122,7 @@ public void setup() throws RemoteInvocationException, URISyntaxException {
     operationContext =
         TestOperationContexts.systemContextNoSearchAuthorization(
                 new SnapshotEntityRegistry(new Snapshot()),
-                new IndexConventionImpl("lineage_search_service_test"))
+                new IndexConventionImpl("lineage_search_service_test", "MD5"))
             .asSession(RequestContext.TEST, Authorizer.EMPTY, TestOperationContexts.TEST_USER_AUTH);
     settingsBuilder = new SettingsBuilder(null);
     elasticSearchService = buildEntitySearchService();

File: metadata-io/src/test/java/com/linkedin/metadata/search/SearchServiceTestBase.java
Patch:
@@ -79,7 +79,7 @@ public void setup() throws RemoteInvocationException, URISyntaxException {
     operationContext =
         TestOperationContexts.systemContextNoSearchAuthorization(
                 new SnapshotEntityRegistry(new Snapshot()),
-                new IndexConventionImpl("search_service_test"))
+                new IndexConventionImpl("search_service_test", "MD5"))
             .asSession(RequestContext.TEST, Authorizer.EMPTY, TestOperationContexts.TEST_USER_AUTH);
 
     settingsBuilder = new SettingsBuilder(null);

File: metadata-io/src/test/java/com/linkedin/metadata/search/TestEntityTestBase.java
Patch:
@@ -62,7 +62,8 @@ public abstract class TestEntityTestBase extends AbstractTestNGSpringContextTest
   public void setup() {
     opContext =
         TestOperationContexts.systemContextNoSearchAuthorization(
-            new SnapshotEntityRegistry(new Snapshot()), new IndexConventionImpl("es_service_test"));
+            new SnapshotEntityRegistry(new Snapshot()),
+            new IndexConventionImpl("es_service_test", "MD5"));
     settingsBuilder = new SettingsBuilder(null);
     elasticSearchService = buildService();
     elasticSearchService.reindexAll(Collections.emptySet());

File: metadata-io/src/test/java/com/linkedin/metadata/search/query/BrowseDAOTest.java
Patch:
@@ -45,7 +45,7 @@ public void setup() throws RemoteInvocationException, URISyntaxException {
     mockClient = mock(RestHighLevelClient.class);
     opContext =
         TestOperationContexts.systemContextNoSearchAuthorization(
-            new IndexConventionImpl("es_browse_dao_test"));
+            new IndexConventionImpl("es_browse_dao_test", "MD5"));
     browseDAO = new ESBrowseDAO(mockClient, searchConfiguration, customSearchConfiguration);
   }
 

File: metadata-io/src/test/java/com/linkedin/metadata/systemmetadata/SystemMetadataServiceTestBase.java
Patch:
@@ -32,7 +32,7 @@ public abstract class SystemMetadataServiceTestBase extends AbstractTestNGSpring
   protected abstract ESIndexBuilder getIndexBuilder();
 
   private final IndexConvention _indexConvention =
-      new IndexConventionImpl("es_system_metadata_service_test");
+      new IndexConventionImpl("es_system_metadata_service_test", "MD5");
 
   private ElasticSearchSystemMetadataService _client;
 

File: metadata-io/src/test/java/com/linkedin/metadata/timeseries/search/TimeseriesAspectServiceTestBase.java
Patch:
@@ -126,7 +126,7 @@ public void setup() throws RemoteInvocationException, URISyntaxException {
 
     opContext =
         TestOperationContexts.systemContextNoSearchAuthorization(
-            entityRegistry, new IndexConventionImpl("es_timeseries_aspect_service_test"));
+            entityRegistry, new IndexConventionImpl("es_timeseries_aspect_service_test", "MD5"));
 
     elasticSearchTimeseriesAspectService = buildService();
     elasticSearchTimeseriesAspectService.reindexAll(Collections.emptySet());
@@ -152,7 +152,7 @@ private ElasticSearchTimeseriesAspectService buildService() {
 
   private void upsertDocument(TestEntityProfile dp, Urn urn) throws JsonProcessingException {
     Map<String, JsonNode> documents =
-        TimeseriesAspectTransformer.transform(urn, dp, aspectSpec, null);
+        TimeseriesAspectTransformer.transform(urn, dp, aspectSpec, null, "MD5");
     assertEquals(documents.size(), 3);
     documents.forEach(
         (key, value) ->

File: metadata-io/src/test/java/io/datahubproject/test/fixtures/search/SampleDataFixtureConfiguration.java
Patch:
@@ -86,12 +86,12 @@ protected String longTailIndexPrefix() {
 
   @Bean(name = "sampleDataIndexConvention")
   protected IndexConvention indexConvention(@Qualifier("sampleDataPrefix") String prefix) {
-    return new IndexConventionImpl(prefix);
+    return new IndexConventionImpl(prefix, "MD5");
   }
 
   @Bean(name = "longTailIndexConvention")
   protected IndexConvention longTailIndexConvention(@Qualifier("longTailPrefix") String prefix) {
-    return new IndexConventionImpl(prefix);
+    return new IndexConventionImpl(prefix, "MD5");
   }
 
   @Bean(name = "sampleDataFixtureName")

File: metadata-io/src/test/java/io/datahubproject/test/fixtures/search/SearchLineageFixtureConfiguration.java
Patch:
@@ -71,7 +71,7 @@ protected String indexPrefix() {
 
   @Bean(name = "searchLineageIndexConvention")
   protected IndexConvention indexConvention(@Qualifier("searchLineagePrefix") String prefix) {
-    return new IndexConventionImpl(prefix);
+    return new IndexConventionImpl(prefix, "MD5");
   }
 
   @Bean(name = "searchLineageFixtureName")
@@ -173,7 +173,8 @@ protected ElasticSearchGraphService graphService(
             new ESGraphWriteDAO(indexConvention, bulkProcessor, 1),
             new ESGraphQueryDAO(
                 searchClient, lineageRegistry, indexConvention, getGraphQueryConfiguration()),
-            indexBuilder);
+            indexBuilder,
+            indexConvention.getIdHashAlgo());
     graphService.reindexAll(Collections.emptySet());
     return graphService;
   }

File: metadata-operation-context/src/main/java/io/datahubproject/metadata/context/SearchContext.java
Patch:
@@ -21,7 +21,7 @@
 public class SearchContext implements ContextInterface {
 
   public static SearchContext EMPTY =
-      SearchContext.builder().indexConvention(IndexConventionImpl.NO_PREFIX).build();
+      SearchContext.builder().indexConvention(IndexConventionImpl.noPrefix("")).build();
 
   public static SearchContext withFlagDefaults(
       @Nonnull SearchContext searchContext,

File: metadata-operation-context/src/main/java/io/datahubproject/test/metadata/context/TestOperationContexts.java
Patch:
@@ -191,7 +191,7 @@ public static OperationContext systemContext(
     IndexConvention indexConvention =
         Optional.ofNullable(indexConventionSupplier)
             .map(Supplier::get)
-            .orElse(IndexConventionImpl.NO_PREFIX);
+            .orElse(IndexConventionImpl.noPrefix("MD5"));
 
     ServicesRegistryContext servicesRegistryContext =
         Optional.ofNullable(servicesRegistrySupplier).orElse(() -> null).get();

File: metadata-service/configuration/src/main/java/com/linkedin/metadata/config/search/ElasticSearchConfiguration.java
Patch:
@@ -8,4 +8,5 @@ public class ElasticSearchConfiguration {
   private BuildIndicesConfiguration buildIndices;
   public String implementation;
   private SearchConfiguration search;
+  private String idHashAlgo;
 }

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/common/IndexConventionFactory.java
Patch:
@@ -19,7 +19,8 @@ public class IndexConventionFactory {
   private String indexPrefix;
 
   @Bean(name = INDEX_CONVENTION_BEAN)
-  protected IndexConvention createInstance() {
-    return new IndexConventionImpl(indexPrefix);
+  protected IndexConvention createInstance(
+      @Value("${elasticsearch.idHashAlgo}") final String isHashAlgo) {
+    return new IndexConventionImpl(indexPrefix, isHashAlgo);
   }
 }

File: metadata-utils/src/main/java/com/linkedin/metadata/utils/elasticsearch/IndexConvention.java
Patch:
@@ -47,4 +47,7 @@ public interface IndexConvention {
    *     if one cannot be extracted
    */
   Optional<Pair<String, String>> getEntityAndAspectName(String timeseriesAspectIndexName);
+
+  @Nonnull
+  String getIdHashAlgo();
 }

File: metadata-io/src/main/java/com/linkedin/metadata/systemmetadata/ElasticSearchSystemMetadataService.java
Patch:
@@ -52,6 +52,7 @@ public class ElasticSearchSystemMetadataService
   private final IndexConvention _indexConvention;
   private final ESSystemMetadataDAO _esDAO;
   private final ESIndexBuilder _indexBuilder;
+  @Nonnull private final String elasticIdHashAlgo;
 
   private static final String DOC_DELIMETER = "--";
   public static final String INDEX_NAME = "system_metadata_service_v1";
@@ -86,10 +87,9 @@ private String toDocument(SystemMetadata systemMetadata, String urn, String aspe
 
   private String toDocId(@Nonnull final String urn, @Nonnull final String aspect) {
     String rawDocId = urn + DOC_DELIMETER + aspect;
-    String hashAlgo = System.getenv("ELASTIC_ID_HASH_ALGO");
     try {
       byte[] bytesOfRawDocID = rawDocId.getBytes(StandardCharsets.UTF_8);
-      MessageDigest md = MessageDigest.getInstance(hashAlgo);
+      MessageDigest md = MessageDigest.getInstance(elasticIdHashAlgo);
       byte[] thedigest = md.digest(bytesOfRawDocID);
       return Base64.getEncoder().encodeToString(thedigest);
     } catch (NoSuchAlgorithmException e) {

File: metadata-io/src/test/java/com/linkedin/metadata/systemmetadata/SystemMetadataServiceTestBase.java
Patch:
@@ -54,7 +54,7 @@ private ElasticSearchSystemMetadataService buildService() {
     ESSystemMetadataDAO dao =
         new ESSystemMetadataDAO(getSearchClient(), _indexConvention, getBulkProcessor(), 1);
     return new ElasticSearchSystemMetadataService(
-        getBulkProcessor(), _indexConvention, dao, getIndexBuilder());
+        getBulkProcessor(), _indexConvention, dao, getIndexBuilder(), "MD5");
   }
 
   @Test

File: metadata-service/services/src/main/java/com/linkedin/metadata/entity/DeleteEntityService.java
Patch:
@@ -729,11 +729,11 @@ private MetadataChangeProposal updateFormsAspect(
             .collect(Collectors.toList());
     List<FormAssociation> completedForms =
         formsAspect.getCompletedForms().stream()
-            .filter(completedForm -> completedForm.getUrn() != deletedUrn)
+            .filter(completedForm -> !completedForm.getUrn().equals(deletedUrn))
             .collect(Collectors.toList());
     final List<FormVerificationAssociation> verifications =
         formsAspect.getVerifications().stream()
-            .filter(verification -> verification.getForm() != deletedUrn)
+            .filter(verification -> !verification.getForm().equals(deletedUrn))
             .collect(Collectors.toList());
 
     updatedAspect.get().setIncompleteForms(new FormAssociationArray(incompleteForms));

File: metadata-jobs/mae-consumer/src/main/java/com/linkedin/metadata/kafka/hook/MetadataChangeLogHook.java
Patch:
@@ -30,9 +30,7 @@ default MetadataChangeLogHook init(@Nonnull OperationContext systemOperationCont
    * Return whether the hook is enabled or not. If not enabled, the below invoke method is not
    * triggered
    */
-  default boolean isEnabled() {
-    return true;
-  }
+  boolean isEnabled();
 
   /** Invoke the hook when a MetadataChangeLog is received */
   void invoke(@Nonnull MetadataChangeLog log) throws Exception;

File: metadata-jobs/pe-consumer/src/main/java/com/datahub/event/hook/PlatformEventHook.java
Patch:
@@ -20,9 +20,7 @@ default void init() {}
    * Return whether the hook is enabled or not. If not enabled, the below invoke method is not
    * triggered
    */
-  default boolean isEnabled() {
-    return true;
-  }
+  boolean isEnabled();
 
   /** Invoke the hook when a PlatformEvent is received */
   void invoke(@Nonnull OperationContext opContext, @Nonnull PlatformEvent event);

File: entity-registry/src/main/java/com/linkedin/metadata/models/registry/ConfigEntityRegistry.java
Patch:
@@ -52,7 +52,7 @@ public class ConfigEntityRegistry implements EntityRegistry {
   private final DataSchemaFactory dataSchemaFactory;
   @Getter private final PluginFactory pluginFactory;
 
-  @Nullable
+  @Getter @Nullable
   private BiFunction<PluginConfiguration, List<ClassLoader>, PluginFactory> pluginFactoryProvider;
 
   private final Map<String, EntitySpec> entityNameToSpec;

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/mutate/MutationUtils.java
Patch:
@@ -1,6 +1,7 @@
 package com.linkedin.datahub.graphql.resolvers.mutate;
 
 import static com.linkedin.metadata.Constants.*;
+import static com.linkedin.metadata.utils.SystemMetadataUtils.createDefaultSystemMetadata;
 
 import com.linkedin.common.urn.Urn;
 import com.linkedin.data.template.RecordTemplate;
@@ -84,7 +85,7 @@ private static MetadataChangeProposal setProposalProperties(
     proposal.setChangeType(ChangeType.UPSERT);
 
     // Assumes proposal is generated first from the builder methods above so SystemMetadata is empty
-    SystemMetadata systemMetadata = new SystemMetadata();
+    SystemMetadata systemMetadata = createDefaultSystemMetadata();
     StringMap properties = new StringMap();
     properties.put(APP_SOURCE, UI_SOURCE);
     systemMetadata.setProperties(properties);

File: datahub-graphql-core/src/test/java/com/linkedin/datahub/graphql/resolvers/businessattribute/AddBusinessAttributeResolverTest.java
Patch:
@@ -29,7 +29,7 @@ public class AddBusinessAttributeResolverTest {
       "urn:li:businessAttribute:7d0c4283-de02-4043-aaf2-698b04274658";
   private static final String RESOURCE_URN =
       "urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:hive,SampleCypressHiveDataset,PROD),field_bar)";
-  private EntityService mockService;
+  private EntityService<?> mockService;
   private QueryContext mockContext;
   private DataFetchingEnvironment mockEnv;
 

File: datahub-graphql-core/src/test/java/com/linkedin/datahub/graphql/resolvers/businessattribute/CreateBusinessAttributeResolverTest.java
Patch:
@@ -56,7 +56,7 @@ public class CreateBusinessAttributeResolverTest {
           TEST_BUSINESS_ATTRIBUTE_DESCRIPTION,
           SchemaFieldDataType.BOOLEAN);
   private EntityClient mockClient;
-  private EntityService mockService;
+  private EntityService<?> mockService;
   private QueryContext mockContext;
   private DataFetchingEnvironment mockEnv;
   private BusinessAttributeService businessAttributeService;

File: datahub-graphql-core/src/test/java/com/linkedin/datahub/graphql/resolvers/businessattribute/RemoveBusinessAttributeResolverTest.java
Patch:
@@ -31,7 +31,7 @@ public class RemoveBusinessAttributeResolverTest {
       "urn:li:businessAttribute:7d0c4283-de02-4043-aaf2-698b04274658";
   private static final String RESOURCE_URN =
       "urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:hive,SampleCypressHiveDataset,PROD),field_bar)";
-  private EntityService mockService;
+  private EntityService<?> mockService;
   private QueryContext mockContext;
   private DataFetchingEnvironment mockEnv;
 

File: datahub-graphql-core/src/test/java/com/linkedin/datahub/graphql/resolvers/glossary/DeleteGlossaryEntityResolverTest.java
Patch:
@@ -25,7 +25,7 @@ public class DeleteGlossaryEntityResolverTest {
   @Test
   public void testGetSuccess() throws Exception {
     EntityClient mockClient = Mockito.mock(EntityClient.class);
-    EntityService mockService = getMockEntityService();
+    EntityService<?> mockService = getMockEntityService();
 
     Mockito.when(mockService.exists(any(), eq(Urn.createFromString(TEST_TERM_URN)), eq(true)))
         .thenReturn(true);
@@ -50,7 +50,7 @@ public void testGetEntityClientException() throws Exception {
         .when(mockClient)
         .deleteEntity(any(), Mockito.any());
 
-    EntityService mockService = getMockEntityService();
+    EntityService<?> mockService = getMockEntityService();
     Mockito.when(mockService.exists(any(), eq(Urn.createFromString(TEST_TERM_URN)), eq(true)))
         .thenReturn(true);
 

File: datahub-graphql-core/src/test/java/com/linkedin/datahub/graphql/resolvers/mutate/SiblingsUtilsTest.java
Patch:
@@ -31,7 +31,7 @@ public class SiblingsUtilsTest {
   public void testGetSiblingUrns() {
     UrnArray siblingUrns =
         new UrnArray(UrnUtils.getUrn(TEST_DATASET_URN2), UrnUtils.getUrn(TEST_DATASET_URN3));
-    EntityService mockService = mock(EntityService.class);
+    EntityService<?> mockService = mock(EntityService.class);
     Mockito.when(
             mockService.getLatestAspect(
                 any(), eq(UrnUtils.getUrn(TEST_DATASET_URN1)), eq(SIBLINGS_ASPECT_NAME)))
@@ -45,7 +45,7 @@ public void testGetSiblingUrns() {
 
   @Test
   public void testGetSiblingUrnsWithoutSiblings() {
-    EntityService mockService = mock(EntityService.class);
+    EntityService<?> mockService = mock(EntityService.class);
     Mockito.when(
             mockService.getLatestAspect(
                 any(), eq(UrnUtils.getUrn(TEST_DATASET_URN1)), eq(SIBLINGS_ASPECT_NAME)))
@@ -59,7 +59,7 @@ public void testGetSiblingUrnsWithoutSiblings() {
 
   @Test
   public void testGetSiblingUrnsWithSiblingsAspect() {
-    EntityService mockService = mock(EntityService.class);
+    EntityService<?> mockService = mock(EntityService.class);
     Mockito.when(
             mockService.getLatestAspect(
                 any(), eq(UrnUtils.getUrn(TEST_DATASET_URN1)), eq(SIBLINGS_ASPECT_NAME)))

File: datahub-graphql-core/src/test/java/com/linkedin/datahub/graphql/resolvers/mutate/UpdateUserSettingResolverTest.java
Patch:
@@ -23,7 +23,7 @@ public class UpdateUserSettingResolverTest {
 
   @Test
   public void testWriteCorpUserSettings() throws Exception {
-    EntityService mockService = getMockEntityService();
+    EntityService<?> mockService = getMockEntityService();
     Mockito.when(mockService.exists(any(), eq(Urn.createFromString(TEST_USER_URN)), eq(true)))
         .thenReturn(true);
 

File: datahub-graphql-core/src/test/java/com/linkedin/datahub/graphql/resolvers/operation/ReportOperationResolverTest.java
Patch:
@@ -57,8 +57,7 @@ public void testGetSuccess() throws Exception {
     Mockito.when(mockEnv.getContext()).thenReturn(mockContext);
     resolver.get(mockEnv).get();
 
-    Mockito.verify(mockClient, Mockito.times(1))
-        .ingestProposal(any(), Mockito.eq(expectedProposal), Mockito.eq(false));
+    verifyIngestProposal(mockClient, 1, expectedProposal);
   }
 
   @Test

File: datahub-upgrade/src/main/java/com/linkedin/datahub/upgrade/system/browsepaths/BackfillBrowsePathsV2Step.java
Patch:
@@ -1,6 +1,7 @@
 package com.linkedin.datahub.upgrade.system.browsepaths;
 
 import static com.linkedin.metadata.Constants.*;
+import static com.linkedin.metadata.utils.SystemMetadataUtils.createDefaultSystemMetadata;
 
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableSet;
@@ -29,7 +30,6 @@
 import com.linkedin.metadata.search.SearchService;
 import com.linkedin.metadata.utils.GenericRecordUtils;
 import com.linkedin.mxe.MetadataChangeProposal;
-import com.linkedin.mxe.SystemMetadata;
 import io.datahubproject.metadata.context.OperationContext;
 import java.util.Set;
 import java.util.function.Function;
@@ -208,8 +208,7 @@ private void ingestBrowsePathsV2(
     proposal.setEntityType(urn.getEntityType());
     proposal.setAspectName(Constants.BROWSE_PATHS_V2_ASPECT_NAME);
     proposal.setChangeType(ChangeType.UPSERT);
-    proposal.setSystemMetadata(
-        new SystemMetadata().setRunId(DEFAULT_RUN_ID).setLastObserved(System.currentTimeMillis()));
+    proposal.setSystemMetadata(createDefaultSystemMetadata());
     proposal.setAspect(GenericRecordUtils.serializeAspect(browsePathsV2));
     entityService.ingestProposal(opContext, proposal, auditStamp, true);
   }

File: metadata-io/src/test/java/com/linkedin/metadata/entity/EntityServiceTest.java
Patch:
@@ -602,8 +602,9 @@ public void testReingestLineageAspect() throws Exception {
 
     GenericAspect aspect = GenericRecordUtils.serializeAspect(pairToIngest.get(0).getSecond());
 
+    SystemMetadata initialSystemMetadata = AspectGenerationUtils.createSystemMetadata(1);
     initialChangeLog.setAspect(aspect);
-    initialChangeLog.setSystemMetadata(AspectGenerationUtils.createSystemMetadata(1));
+    initialChangeLog.setSystemMetadata(initialSystemMetadata);
     initialChangeLog.setEntityKeyAspect(
         GenericRecordUtils.serializeAspect(
             EntityKeyUtils.convertUrnToEntityKey(
@@ -620,7 +621,7 @@ public void testReingestLineageAspect() throws Exception {
     restateChangeLog.setSystemMetadata(AspectGenerationUtils.createSystemMetadata(1));
     restateChangeLog.setPreviousAspectValue(aspect);
     restateChangeLog.setPreviousSystemMetadata(
-        simulatePullFromDB(AspectGenerationUtils.createSystemMetadata(1), SystemMetadata.class));
+        simulatePullFromDB(initialSystemMetadata, SystemMetadata.class));
     restateChangeLog.setEntityKeyAspect(
         GenericRecordUtils.serializeAspect(
             EntityKeyUtils.convertUrnToEntityKey(

File: metadata-service/factories/src/main/java/com/linkedin/metadata/boot/steps/BackfillBrowsePathsV2Step.java
Patch:
@@ -1,6 +1,7 @@
 package com.linkedin.metadata.boot.steps;
 
 import static com.linkedin.metadata.Constants.*;
+import static com.linkedin.metadata.utils.SystemMetadataUtils.createDefaultSystemMetadata;
 
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableSet;
@@ -23,7 +24,6 @@
 import com.linkedin.metadata.search.SearchService;
 import com.linkedin.metadata.utils.GenericRecordUtils;
 import com.linkedin.mxe.MetadataChangeProposal;
-import com.linkedin.mxe.SystemMetadata;
 import io.datahubproject.metadata.context.OperationContext;
 import java.util.Set;
 import javax.annotation.Nonnull;
@@ -152,8 +152,7 @@ private void ingestBrowsePathsV2(
     proposal.setEntityType(urn.getEntityType());
     proposal.setAspectName(Constants.BROWSE_PATHS_V2_ASPECT_NAME);
     proposal.setChangeType(ChangeType.UPSERT);
-    proposal.setSystemMetadata(
-        new SystemMetadata().setRunId(DEFAULT_RUN_ID).setLastObserved(System.currentTimeMillis()));
+    proposal.setSystemMetadata(createDefaultSystemMetadata());
     proposal.setAspect(GenericRecordUtils.serializeAspect(browsePathsV2));
     entityService.ingestProposal(systemOperationContext, proposal, auditStamp, false);
   }

File: metadata-service/factories/src/main/java/com/linkedin/metadata/boot/steps/UpgradeDefaultBrowsePathsStep.java
Patch:
@@ -1,6 +1,7 @@
 package com.linkedin.metadata.boot.steps;
 
 import static com.linkedin.metadata.Constants.*;
+import static com.linkedin.metadata.utils.SystemMetadataUtils.createDefaultSystemMetadata;
 
 import com.google.common.collect.ImmutableSet;
 import com.linkedin.common.AuditStamp;
@@ -17,7 +18,6 @@
 import com.linkedin.metadata.search.utils.BrowsePathUtils;
 import com.linkedin.metadata.utils.GenericRecordUtils;
 import com.linkedin.mxe.MetadataChangeProposal;
-import com.linkedin.mxe.SystemMetadata;
 import io.datahubproject.metadata.context.OperationContext;
 import java.util.Set;
 import javax.annotation.Nonnull;
@@ -138,8 +138,7 @@ private void migrateBrowsePath(
     proposal.setEntityType(urn.getEntityType());
     proposal.setAspectName(Constants.BROWSE_PATHS_ASPECT_NAME);
     proposal.setChangeType(ChangeType.UPSERT);
-    proposal.setSystemMetadata(
-        new SystemMetadata().setRunId(DEFAULT_RUN_ID).setLastObserved(System.currentTimeMillis()));
+    proposal.setSystemMetadata(createDefaultSystemMetadata());
     proposal.setAspect(GenericRecordUtils.serializeAspect(newPaths));
     entityService.ingestProposal(opContext, proposal, auditStamp, false);
   }

File: metadata-integration/java/datahub-client/src/main/java/datahub/client/kafka/KafkaEmitter.java
Patch:
@@ -160,7 +160,7 @@ private static MetadataWriteResponse mapResponse(RecordMetadata metadata, Except
     return builder.build();
   }
 
-  public Properties getKafkaConfgiProperties() {
+  public Properties getKafkaConfigProperties() {
     return kafkaConfigProperties;
   }
 }

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/GmsGraphQLEngine.java
Patch:
@@ -1032,6 +1032,8 @@ private void configureQueryResolvers(final RuntimeWiring.Builder builder) {
                 .dataFetcher("mlModel", getResolver(mlModelType))
                 .dataFetcher("mlModelGroup", getResolver(mlModelGroupType))
                 .dataFetcher("assertion", getResolver(assertionType))
+                .dataFetcher("form", getResolver(formType))
+                .dataFetcher("view", getResolver(dataHubViewType))
                 .dataFetcher("listPolicies", new ListPoliciesResolver(this.entityClient))
                 .dataFetcher("getGrantedPrivileges", new GetGrantedPrivilegesResolver())
                 .dataFetcher("listUsers", new ListUsersResolver(this.entityClient))

File: entity-registry/src/main/java/com/linkedin/metadata/models/extractor/FieldExtractor.java
Patch:
@@ -63,7 +63,7 @@ public static <T extends FieldSpec> Map<T, List<Object>> extractFields(
         } else {
           List<Object> valueList = (List<Object>) value.get();
           // If the field is a nested list of values, flatten it
-          for (int i = 0; i < numArrayWildcards - 1; i++) {
+          for (long i = 0; i < numArrayWildcards - 1; i++) {
             valueList =
                 valueList.stream()
                     .flatMap(v -> ((List<Object>) v).stream())

File: metadata-service/factories/src/main/java/com/linkedin/metadata/boot/steps/IngestDataPlatformInstancesStep.java
Patch:
@@ -61,7 +61,7 @@ public void execute(@Nonnull OperationContext systemOperationContext) throws Exc
     long numEntities = _migrationsDao.countEntities();
     int start = 0;
 
-    while (start < numEntities) {
+    while (start < (int) numEntities) {
       log.info(
           "Reading urns {} to {} from the aspects table to generate dataplatform instance aspects",
           start,

File: metadata-service/services/src/main/java/com/linkedin/metadata/service/RollbackService.java
Patch:
@@ -173,7 +173,7 @@ public RollbackResponse rollbackIngestion(
     // Rollback timeseries aspects
     DeleteAspectValuesResult timeseriesRollbackResult =
         timeseriesAspectService.rollbackTimeseriesAspects(opContext, runId);
-    rowsDeletedFromEntityDeletion += timeseriesRollbackResult.getNumDocsDeleted();
+    rowsDeletedFromEntityDeletion += timeseriesRollbackResult.getNumDocsDeleted().intValue();
 
     log.info("finished deleting {} rows", deletedRows.size());
     int aspectsReverted = deletedRows.size() + rowsDeletedFromEntityDeletion;

File: metadata-integration/java/acryl-spark-lineage/src/main/java/datahub/spark/DatahubSparkListener.java
Patch:
@@ -257,7 +257,6 @@ private synchronized SparkLineageConf loadDatahubConfig(
       this.appContext.setDatabricksTags(databricksTags.orElse(null));
     }
 
-    log.info("Datahub configuration: {}", datahubConf.root().render());
     Optional<DatahubEmitterConfig> emitterConfig = initializeEmitter(datahubConf);
     SparkLineageConf sparkLineageConf =
         SparkLineageConf.toSparkLineageConf(datahubConf, appContext, emitterConfig.orElse(null));

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/GmsGraphQLEngine.java
Patch:
@@ -1869,7 +1869,9 @@ private void configureCorpGroupResolvers(final RuntimeWiring.Builder builder) {
         "CorpGroup",
         typeWiring ->
             typeWiring
-                .dataFetcher("relationships", new EntityRelationshipsResultResolver(graphClient))
+                .dataFetcher(
+                    "relationships",
+                    new EntityRelationshipsResultResolver(graphClient, entityService))
                 .dataFetcher("privileges", new EntityPrivilegesResolver(entityClient))
                 .dataFetcher(
                     "aspects", new WeaklyTypedAspectsResolver(entityClient, entityRegistry))

File: metadata-service/openapi-servlet/src/test/java/io/datahubproject/openapi/v3/OpenAPIV3GeneratorTest.java
Patch:
@@ -61,12 +61,12 @@ public void testOpenApiSpecBuilder() throws Exception {
     assertFalse(requiredNames.contains("name"));
     assertTrue(name.getNullable());
 
-    // Assert non-required $ref properties are replaced by nullable { allOf: [ $ref ] } objects
+    // Assert non-required $ref properties are replaced by nullable { anyOf: [ $ref ] } objects
     Schema created = properties.get("created");
     assertFalse(requiredNames.contains("created"));
     assertEquals("object", created.getType());
     assertNull(created.get$ref());
-    assertEquals(List.of(new Schema().$ref("#/components/schemas/TimeStamp")), created.getAllOf());
+    assertEquals(List.of(new Schema().$ref("#/components/schemas/TimeStamp")), created.getAnyOf());
     assertTrue(created.getNullable());
 
     // Assert systemMetadata property on response schema is optional.
@@ -81,7 +81,7 @@ public void testOpenApiSpecBuilder() throws Exception {
     assertNull(systemMetadata.get$ref());
     assertEquals(
         List.of(new Schema().$ref("#/components/schemas/SystemMetadata")),
-        systemMetadata.getAllOf());
+        systemMetadata.getAnyOf());
     assertTrue(systemMetadata.getNullable());
 
     // Assert enum property is string.

File: metadata-io/src/main/java/com/linkedin/metadata/structuredproperties/validation/StructuredPropertiesValidator.java
Patch:
@@ -356,7 +356,7 @@ private static Optional<AspectValidationException> validateType(
                 throw new RuntimeException(e);
               }
               String allowedEntityName = getValueTypeId(typeUrn);
-              if (typeValue.getEntityType().equals(allowedEntityName)) {
+              if (typeValue.getEntityType().equalsIgnoreCase(allowedEntityName)) {
                 matchedAny = true;
               }
             }

File: metadata-io/src/main/java/com/linkedin/metadata/entity/EntityServiceImpl.java
Patch:
@@ -2176,7 +2176,8 @@ public Set<Urn> exists(
               urn ->
                   // key aspect is always returned, make sure to only consider the status aspect
                   statusResult.getOrDefault(urn, List.of()).stream()
-                      .filter(aspect -> STATUS_ASPECT_NAME.equals(aspect.schema().getName()))
+                      .filter(
+                          aspect -> STATUS_ASPECT_NAME.equalsIgnoreCase(aspect.schema().getName()))
                       .noneMatch(aspect -> ((Status) aspect).isRemoved()))
           .collect(Collectors.toSet());
     }

File: metadata-service/openapi-servlet/src/main/java/io/datahubproject/openapi/v3/controller/EntityController.java
Patch:
@@ -37,6 +37,7 @@
 import io.datahubproject.openapi.v3.models.GenericAspectV3;
 import io.datahubproject.openapi.v3.models.GenericEntityScrollResultV3;
 import io.datahubproject.openapi.v3.models.GenericEntityV3;
+import io.swagger.v3.oas.annotations.Hidden;
 import io.swagger.v3.oas.annotations.Operation;
 import io.swagger.v3.oas.annotations.tags.Tag;
 import jakarta.servlet.http.HttpServletRequest;
@@ -67,6 +68,7 @@
 @RequiredArgsConstructor
 @RequestMapping("/v3/entity")
 @Slf4j
+@Hidden
 public class EntityController
     extends GenericEntitiesController<
         GenericAspectV3, GenericEntityV3, GenericEntityScrollResultV3> {

File: metadata-integration/java/datahub-client/src/main/java/datahub/client/MetadataResponseFuture.java
Patch:
@@ -69,7 +69,7 @@ public MetadataWriteResponse get(long timeout, TimeUnit unit)
       return mapper.map(response);
     } else {
       // We wait for the callback to fill this out
-      responseLatch.await();
+      responseLatch.await(timeout, unit);
       return responseReference.get();
     }
   }

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/search/ScrollAcrossLineageResolver.java
Patch:
@@ -78,7 +78,8 @@ public CompletableFuture<ScrollAcrossLineageResults> get(DataFetchingEnvironment
     @Nullable
     Long startTimeMillis = input.getStartTimeMillis() == null ? null : input.getStartTimeMillis();
     @Nullable
-    Long endTimeMillis = input.getEndTimeMillis() == null ? null : input.getEndTimeMillis();
+    Long endTimeMillis =
+        ResolverUtils.getLineageEndTimeMillis(input.getStartTimeMillis(), input.getEndTimeMillis());
 
     final LineageFlags lineageFlags = LineageFlagsInputMapper.map(context, input.getLineageFlags());
     if (lineageFlags.getStartTimeMillis() == null && startTimeMillis != null) {

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/search/SearchAcrossLineageResolver.java
Patch:
@@ -111,7 +111,8 @@ public CompletableFuture<SearchAcrossLineageResults> get(DataFetchingEnvironment
     @Nullable
     Long startTimeMillis = input.getStartTimeMillis() == null ? null : input.getStartTimeMillis();
     @Nullable
-    Long endTimeMillis = input.getEndTimeMillis() == null ? null : input.getEndTimeMillis();
+    Long endTimeMillis =
+        ResolverUtils.getLineageEndTimeMillis(input.getStartTimeMillis(), input.getEndTimeMillis());
 
     final LineageFlags lineageFlags = LineageFlagsInputMapper.map(context, input.getLineageFlags());
     if (lineageFlags.getStartTimeMillis() == null && startTimeMillis != null) {

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/entitytype/EntityTypeMapper.java
Patch:
@@ -48,6 +48,7 @@ public class EntityTypeMapper {
           .put(EntityType.BUSINESS_ATTRIBUTE, Constants.BUSINESS_ATTRIBUTE_ENTITY_NAME)
           .put(EntityType.QUERY, Constants.QUERY_ENTITY_NAME)
           .put(EntityType.POST, Constants.POST_ENTITY_NAME)
+          .put(EntityType.FORM, Constants.FORM_ENTITY_NAME)
           .build();
 
   private static final Map<String, EntityType> ENTITY_NAME_TO_TYPE =

File: entity-registry/src/main/java/com/linkedin/metadata/aspect/validation/ConditionalWriteValidator.java
Patch:
@@ -42,7 +42,7 @@
 @Getter
 @Accessors(chain = true)
 public class ConditionalWriteValidator extends AspectPayloadValidator {
-  public static final String DEFAULT_ASPECT_VERSION = "1";
+  public static final String UNVERSIONED_ASPECT_VERSION = "-1";
   public static final long DEFAULT_LAST_MODIFIED_TIME = Long.MIN_VALUE;
   public static final String HTTP_HEADER_IF_VERSION_MATCH = "If-Version-Match";
   public static final Set<ChangeType> CREATE_CHANGE_TYPES =
@@ -130,7 +130,7 @@ private static Optional<AspectValidationException> validateVersionPrecondition(
     switch (item.getChangeType()) {
       case CREATE:
       case CREATE_ENTITY:
-        actualAspectVersion = DEFAULT_ASPECT_VERSION;
+        actualAspectVersion = UNVERSIONED_ASPECT_VERSION;
         break;
       default:
         actualAspectVersion =
@@ -143,7 +143,7 @@ private static Optional<AspectValidationException> validateVersionPrecondition(
                         return String.valueOf(Math.max(1, prevSystemAspect.getVersion()));
                       }
                     })
-                .orElse(DEFAULT_ASPECT_VERSION);
+                .orElse(UNVERSIONED_ASPECT_VERSION);
         break;
     }
 

File: metadata-service/openapi-servlet/src/main/java/io/datahubproject/openapi/v2/controller/EntityController.java
Patch:
@@ -45,6 +45,7 @@
 import java.util.ArrayList;
 import java.util.HashSet;
 import java.util.Iterator;
+import java.util.LinkedHashMap;
 import java.util.LinkedList;
 import java.util.List;
 import java.util.Map;
@@ -187,12 +188,12 @@ protected AspectsBatch toMCPBatch(
   @Override
   protected List<GenericEntityV2> buildEntityVersionedAspectList(
       @Nonnull OperationContext opContext,
-      Map<Urn, Map<String, Long>> urnAspectVersions,
+      LinkedHashMap<Urn, Map<String, Long>> urnAspectVersions,
       boolean withSystemMetadata)
       throws URISyntaxException {
     Map<Urn, List<EnvelopedAspect>> aspects =
         entityService.getEnvelopedVersionedAspects(
-            opContext, resolveAspectNames(urnAspectVersions), true);
+            opContext, resolveAspectNames(urnAspectVersions, 0L), true);
 
     return urnAspectVersions.keySet().stream()
         .map(

File: metadata-integration/java/openlineage-converter/src/main/java/io/datahubproject/openlineage/config/DatahubOpenlineageConfig.java
Patch:
@@ -33,6 +33,7 @@ public class DatahubOpenlineageConfig {
   @Builder.Default private String hivePlatformAlias = "hive";
   @Builder.Default private Map<String, String> urnAliases = new HashMap<>();
   @Builder.Default private final boolean disableSymlinkResolution = false;
+  @Builder.Default private final boolean lowerCaseDatasetUrns = false;
 
   public List<PathSpec> getPathSpecsForPlatform(String platform) {
     if ((pathSpecs == null) || (pathSpecs.isEmpty())) {

File: metadata-integration/java/openlineage-converter/src/main/java/io/datahubproject/openlineage/dataset/DatahubJob.java
Patch:
@@ -280,11 +280,11 @@ private Pair<UrnArray, EdgeArray> processDownstreams(
                     for (Urn downstream :
                         Objects.requireNonNull(fineGrainedLineage.getDownstreams())) {
                       upstreamLineagePatchBuilder.addFineGrainedUpstreamField(
-                          downstream,
+                          upstream,
                           fineGrainedLineage.getConfidenceScore(),
                           StringUtils.defaultIfEmpty(
                               fineGrainedLineage.getTransformOperation(), "TRANSFORM"),
-                          upstream,
+                          downstream,
                           null);
                     }
                   }

File: metadata-integration/java/openlineage-converter/src/main/java/io/datahubproject/openlineage/dataset/PathSpec.java
Patch:
@@ -14,7 +14,7 @@
 public class PathSpec {
   final String alias;
   final String platform;
-  @Builder.Default final String env = "PROD";
+  @Builder.Default final Optional<String> env = Optional.empty();
   final List<String> pathSpecList;
   @Builder.Default final Optional<String> platformInstance = Optional.empty();
 }

File: metadata-jobs/mae-consumer/src/main/java/com/linkedin/metadata/kafka/hook/form/FormAssignmentHook.java
Patch:
@@ -38,9 +38,6 @@
  * <p>3. When a form is hard deleted, any automations used for assigning the form, or validating
  * prompts, are automatically deleted.
  *
- * <p>Note that currently, Datasets, Dashboards, Charts, Data Jobs, Data Flows, Containers, are the
- * only asset types supported for this hook.
- *
  * <p>TODO: In the future, let's decide whether we want to support automations to auto-mark form
  * prompts as "completed" when they do in fact have the correct metadata. (Without user needing to
  * explicitly fill out a form prompt response)

File: metadata-integration/java/openlineage-converter/src/main/java/io/datahubproject/openlineage/converter/OpenLineageToDataHub.java
Patch:
@@ -102,7 +102,7 @@ public static Optional<DatasetUrn> convertOpenlineageDatasetToDatasetUrn(
     String namespace = dataset.getNamespace();
     String datasetName = dataset.getName();
     Optional<DatasetUrn> datahubUrn;
-    if (dataset.getFacets() != null && dataset.getFacets().getSymlinks() != null) {
+    if (dataset.getFacets() != null && dataset.getFacets().getSymlinks() != null && !mappingConfig.isDisableSymlinkResolution()) {
       Optional<DatasetUrn> originalUrn =
           getDatasetUrnFromOlDataset(namespace, datasetName, mappingConfig);
       for (OpenLineage.SymlinksDatasetFacetIdentifiers symlink :
@@ -581,6 +581,7 @@ private static void convertJobToDataJob(
     OpenLineage.Job job = event.getJob();
     DataJobInfo dji = new DataJobInfo();
 
+    log.debug("Datahub Config: {}", datahubConf);
     if (job.getName().contains(".")) {
 
       String jobName = job.getName().substring(job.getName().indexOf(".") + 1);

File: datahub-frontend/app/controllers/Application.java
Patch:
@@ -155,7 +155,7 @@ AuthenticationConstants.LEGACY_X_DATAHUB_ACTOR_HEADER, getDataHubActorHeader(req
         .setBody(
             new InMemoryBodyWritable(
                 ByteString.fromByteBuffer(request.body().asBytes().asByteBuffer()),
-                "application/json"))
+                request.contentType().orElse("application/json")))
         .setRequestTimeout(Duration.ofSeconds(120))
         .execute()
         .thenApply(

File: metadata-service/openapi-servlet/src/main/java/io/datahubproject/openapi/controller/GenericEntitiesController.java
Patch:
@@ -490,7 +490,7 @@ public ResponseEntity<E> createAspect(
   @Tag(name = "Generic Aspects")
   @PatchMapping(
       value = "/{entityName}/{entityUrn:urn:li:.+}/{aspectName}",
-      consumes = "application/json-patch+json",
+      consumes = {"application/json-patch+json", MediaType.APPLICATION_JSON_VALUE},
       produces = MediaType.APPLICATION_JSON_VALUE)
   @Operation(summary = "Patch an entity aspect. (Experimental)")
   public ResponseEntity<E> patchAspect(

File: metadata-io/src/main/java/com/linkedin/metadata/search/elasticsearch/query/request/SearchQueryBuilder.java
Patch:
@@ -455,7 +455,7 @@ private Optional<QueryBuilder> getStructuredQuery(
     if (customQueryConfig != null) {
       executeStructuredQuery = customQueryConfig.isStructuredQuery();
     } else {
-      executeStructuredQuery = !(isQuoted(sanitizedQuery) && exactMatchConfiguration.isExclusive());
+      executeStructuredQuery = true;
     }
 
     if (executeStructuredQuery) {

File: metadata-service/openapi-servlet/models/src/main/java/io/datahubproject/openapi/models/GenericEntityScrollResult.java
Patch:
@@ -0,0 +1,3 @@
+package io.datahubproject.openapi.models;
+
+public interface GenericEntityScrollResult<T extends GenericEntity> {}

File: metadata-service/openapi-servlet/models/src/main/java/io/datahubproject/openapi/models/GenericScrollResult.java
Patch:
@@ -1,4 +1,4 @@
-package io.datahubproject.openapi.v2.models;
+package io.datahubproject.openapi.models;
 
 import java.util.List;
 import lombok.Builder;

File: metadata-service/openapi-servlet/models/src/main/java/io/datahubproject/openapi/v2/models/BatchGetUrnResponse.java
Patch:
@@ -16,5 +16,5 @@
 public class BatchGetUrnResponse implements Serializable {
   @JsonProperty("entities")
   @Schema(description = "List of entity responses")
-  List<GenericEntity> entities;
+  List<GenericEntityV2> entities;
 }

File: metadata-service/openapi-servlet/src/main/java/io/datahubproject/openapi/v2/controller/RelationshipController.java
Patch:
@@ -18,8 +18,8 @@
 import com.linkedin.metadata.query.filter.RelationshipFilter;
 import com.linkedin.metadata.search.utils.QueryUtils;
 import io.datahubproject.openapi.exception.UnauthorizedException;
+import io.datahubproject.openapi.models.GenericScrollResult;
 import io.datahubproject.openapi.v2.models.GenericRelationship;
-import io.datahubproject.openapi.v2.models.GenericScrollResult;
 import io.swagger.v3.oas.annotations.Operation;
 import io.swagger.v3.oas.annotations.tags.Tag;
 import java.util.Arrays;

File: metadata-service/openapi-servlet/src/main/java/io/datahubproject/openapi/v2/controller/TimeseriesController.java
Patch:
@@ -19,7 +19,7 @@
 import io.datahubproject.metadata.context.OperationContext;
 import io.datahubproject.metadata.context.RequestContext;
 import io.datahubproject.openapi.exception.UnauthorizedException;
-import io.datahubproject.openapi.v2.models.GenericScrollResult;
+import io.datahubproject.openapi.models.GenericScrollResult;
 import io.datahubproject.openapi.v2.models.GenericTimeseriesAspect;
 import io.swagger.v3.oas.annotations.tags.Tag;
 import java.net.URISyntaxException;

File: datahub-frontend/app/auth/AuthModule.java
Patch:
@@ -63,6 +63,7 @@ public class AuthModule extends AbstractModule {
   private static final String ENTITY_CLIENT_RETRY_INTERVAL = "entityClient.retryInterval";
   private static final String ENTITY_CLIENT_NUM_RETRIES = "entityClient.numRetries";
   private static final String ENTITY_CLIENT_RESTLI_GET_BATCH_SIZE = "entityClient.restli.get.batchSize";
+  private static final String ENTITY_CLIENT_RESTLI_GET_BATCH_CONCURRENCY = "entityClient.restli.get.batchConcurrency";
   private static final String GET_SSO_SETTINGS_ENDPOINT = "auth/getSsoSettings";
 
   private final com.typesafe.config.Config _configs;
@@ -208,7 +209,8 @@ protected SystemEntityClient provideEntityClient(
         new ExponentialBackoff(_configs.getInt(ENTITY_CLIENT_RETRY_INTERVAL)),
         _configs.getInt(ENTITY_CLIENT_NUM_RETRIES),
         configurationProvider.getCache().getClient().getEntityClient(),
-        Math.max(1, _configs.getInt(ENTITY_CLIENT_RESTLI_GET_BATCH_SIZE)));
+        Math.max(1, _configs.getInt(ENTITY_CLIENT_RESTLI_GET_BATCH_SIZE)),
+        Math.max(1, _configs.getInt(ENTITY_CLIENT_RESTLI_GET_BATCH_CONCURRENCY)));
   }
 
   @Provides

File: metadata-jobs/mce-consumer-job/src/test/java/com/linkedin/metadata/kafka/MceConsumerApplicationTestConfiguration.java
Patch:
@@ -47,7 +47,8 @@ public SystemEntityClient systemEntityClient(
         new ExponentialBackoff(1),
         1,
         configurationProvider.getCache().getClient().getEntityClient(),
-        1);
+        1,
+        2);
   }
 
   @MockBean public Database ebeanServer;

File: metadata-service/restli-client/src/main/java/com/linkedin/entity/client/SystemRestliEntityClient.java
Patch:
@@ -27,8 +27,9 @@ public SystemRestliEntityClient(
       @Nonnull final BackoffPolicy backoffPolicy,
       int retryCount,
       EntityClientCacheConfig cacheConfig,
-      int batchGetV2Size) {
-    super(restliClient, backoffPolicy, retryCount, batchGetV2Size);
+      int batchGetV2Size,
+      int batchGetV2Concurrency) {
+    super(restliClient, backoffPolicy, retryCount, batchGetV2Size, batchGetV2Concurrency);
     this.operationContextMap = CacheBuilder.newBuilder().maximumSize(500).build();
     this.entityClientCache = buildEntityClientCache(SystemRestliEntityClient.class, cacheConfig);
   }

File: metadata-service/restli-client/src/test/java/com/linkedin/common/client/BaseClientTest.java
Patch:
@@ -37,7 +37,7 @@ public void testZeroRetry() throws RemoteInvocationException {
     when(mockRestliClient.sendRequest(any(ActionRequest.class))).thenReturn(mockFuture);
 
     RestliEntityClient testClient =
-        new RestliEntityClient(mockRestliClient, new ExponentialBackoff(1), 0, 10);
+        new RestliEntityClient(mockRestliClient, new ExponentialBackoff(1), 0, 10, 2);
     testClient.sendClientRequest(testRequestBuilder, AUTH);
     // Expected 1 actual try and 0 retries
     verify(mockRestliClient).sendRequest(any(ActionRequest.class));
@@ -56,7 +56,7 @@ public void testMultipleRetries() throws RemoteInvocationException {
         .thenReturn(mockFuture);
 
     RestliEntityClient testClient =
-        new RestliEntityClient(mockRestliClient, new ExponentialBackoff(1), 1, 10);
+        new RestliEntityClient(mockRestliClient, new ExponentialBackoff(1), 1, 10, 2);
     testClient.sendClientRequest(testRequestBuilder, AUTH);
     // Expected 1 actual try and 1 retries
     verify(mockRestliClient, times(2)).sendRequest(any(ActionRequest.class));
@@ -73,7 +73,7 @@ public void testNonRetry() {
         .thenThrow(new RuntimeException(new RequiredFieldNotPresentException("value")));
 
     RestliEntityClient testClient =
-        new RestliEntityClient(mockRestliClient, new ExponentialBackoff(1), 1, 10);
+        new RestliEntityClient(mockRestliClient, new ExponentialBackoff(1), 1, 10, 2);
     assertThrows(
         RuntimeException.class, () -> testClient.sendClientRequest(testRequestBuilder, AUTH));
   }

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/ResolverUtils.java
Patch:
@@ -239,6 +239,9 @@ public static Filter viewFilter(
       return null;
     }
     DataHubViewInfo viewInfo = resolveView(opContext, viewService, UrnUtils.getUrn(viewUrn));
+    if (viewInfo == null) {
+      return null;
+    }
     Filter result = SearchUtils.combineFilters(null, viewInfo.getDefinition().getFilter());
     return result;
   }

File: metadata-integration/java/openlineage-converter/src/main/java/io/datahubproject/openlineage/config/DatahubOpenlineageConfig.java
Patch:
@@ -27,7 +27,7 @@ public class DatahubOpenlineageConfig {
   @Builder.Default private final boolean captureColumnLevelLineage = true;
   @Builder.Default private final DataJobUrn parentJobUrn = null;
   // This is disabled until column level patch support won't be fixed in GMS
-  @Builder.Default private final boolean usePatch = false;
+  @Builder.Default private final boolean usePatch = true;
 
   public List<PathSpec> getPathSpecsForPlatform(String platform) {
     if ((pathSpecs == null) || (pathSpecs.isEmpty())) {

File: metadata-integration/java/spark-lineage-beta/src/main/java/datahub/spark/conf/SparkConfigParser.java
Patch:
@@ -307,7 +307,7 @@ public static boolean isCoalesceEnabled(Config datahubConfig) {
 
   public static boolean isPatchEnabled(Config datahubConfig) {
     if (!datahubConfig.hasPath(PATCH_ENABLED)) {
-      return false;
+      return true;
     }
     return datahubConfig.hasPath(PATCH_ENABLED) && datahubConfig.getBoolean(PATCH_ENABLED);
   }

File: metadata-integration/java/spark-lineage-beta/src/main/java/io/openlineage/spark/agent/util/RemovePathPatternUtils.java
Patch:
@@ -22,7 +22,7 @@
 import java.util.regex.Pattern;
 import java.util.stream.Collectors;
 import lombok.extern.slf4j.Slf4j;
-import org.apache.commons.lang.StringUtils;
+import org.apache.commons.lang3.StringUtils;
 import org.apache.spark.SparkConf;
 import org.apache.spark.sql.SparkSession;
 

File: entity-registry/src/main/java/com/linkedin/metadata/models/SearchableRefFieldSpecExtractor.java
Patch:
@@ -115,6 +115,7 @@ private void extractSearchableRefAnnotation(
             new SearchableRefAnnotation(
                 pathName,
                 annotation.getFieldType(),
+                annotation.isQueryByDefault(),
                 annotation.getBoostScore(),
                 annotation.getDepth(),
                 annotation.getRefType(),

File: metadata-jobs/mae-consumer/src/main/java/com/linkedin/metadata/kafka/hook/UpdateIndicesHook.java
Patch:
@@ -60,7 +60,8 @@ public UpdateIndicesHook init(@javax.annotation.Nonnull OperationContext systemO
   public void invoke(@Nonnull final MetadataChangeLog event) {
     if (event.getSystemMetadata() != null) {
       if (event.getSystemMetadata().getProperties() != null) {
-        if (UI_SOURCE.equals(event.getSystemMetadata().getProperties().get(APP_SOURCE))
+        if (!Boolean.parseBoolean(event.getSystemMetadata().getProperties().get(FORCE_INDEXING_KEY))
+            && UI_SOURCE.equals(event.getSystemMetadata().getProperties().get(APP_SOURCE))
             && !reprocessUIEvents) {
           // If coming from the UI, we pre-process the Update Indices hook as a fast path to avoid
           // Kafka lag

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/GmsGraphQLEngine.java
Patch:
@@ -830,10 +830,10 @@ public GraphQLEngine.Builder builder() {
     builder
         .addDataLoaders(loaderSuppliers(loadableTypes))
         .addDataLoader("Aspect", context -> createDataLoader(aspectType, context))
-        .configureRuntimeWiring(this::configureRuntimeWiring)
         .setGraphQLQueryComplexityLimit(graphQLQueryComplexityLimit)
         .setGraphQLQueryDepthLimit(graphQLQueryDepthLimit)
-        .setGraphQLQueryIntrospectionEnabled(graphQLQueryIntrospectionEnabled);
+        .setGraphQLQueryIntrospectionEnabled(graphQLQueryIntrospectionEnabled)
+        .configureRuntimeWiring(this::configureRuntimeWiring);
     return builder;
   }
 

File: li-utils/src/main/javaPegasus/com/linkedin/common/urn/UrnUtils.java
Patch:
@@ -23,7 +23,8 @@ private UrnUtils() {}
   @Nonnull
   public static DatasetUrn toDatasetUrn(
       @Nonnull String platformName, @Nonnull String datasetName, @Nonnull String origin) {
-    return new DatasetUrn(new DataPlatformUrn(platformName), datasetName, FabricType.valueOf(origin.toUpperCase()));
+    return new DatasetUrn(
+        new DataPlatformUrn(platformName), datasetName, FabricType.valueOf(origin.toUpperCase()));
   }
 
   /**

File: li-utils/src/main/javaPegasus/com/linkedin/common/urn/UrnUtils.java
Patch:
@@ -23,7 +23,7 @@ private UrnUtils() {}
   @Nonnull
   public static DatasetUrn toDatasetUrn(
       @Nonnull String platformName, @Nonnull String datasetName, @Nonnull String origin) {
-    return new DatasetUrn(new DataPlatformUrn(platformName), datasetName, toFabricType(origin));
+    return new DatasetUrn(new DataPlatformUrn(platformName), datasetName, FabricType.valueOf(origin.toUpperCase()));
   }
 
   /**

File: metadata-service/configuration/src/main/java/com/linkedin/metadata/config/GraphQLQueryConfiguration.java
Patch:
@@ -6,4 +6,5 @@
 public class GraphQLQueryConfiguration {
   private int complexityLimit;
   private int depthLimit;
+  private boolean introspectionEnabled;
 }

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/graphql/GraphQLEngineFactory.java
Patch:
@@ -236,6 +236,8 @@ protected GraphQLEngine graphQLEngine(
     args.setDataProductService(dataProductService);
     args.setGraphQLQueryComplexityLimit(
         configProvider.getGraphQL().getQuery().getComplexityLimit());
+    args.setGraphQLQueryIntrospectionEnabled(
+        configProvider.getGraphQL().getQuery().isIntrospectionEnabled());
     args.setGraphQLQueryDepthLimit(configProvider.getGraphQL().getQuery().getDepthLimit());
     args.setBusinessAttributeService(businessAttributeService);
     args.setConnectionService(_connectionService);

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/Constants.java
Patch:
@@ -21,6 +21,7 @@ private Constants() {}
   public static final String PROPERTIES_SCHEMA_FILE = "properties.graphql";
   public static final String FORMS_SCHEMA_FILE = "forms.graphql";
   public static final String INCIDENTS_SCHEMA_FILE = "incident.graphql";
+  public static final String CONNECTIONS_SCHEMA_FILE = "connection.graphql";
   public static final String BROWSE_PATH_DELIMITER = "/";
   public static final String BROWSE_PATH_V2_DELIMITER = "";
   public static final String VERSION_STAMP_FIELD_NAME = "versionStamp";

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/GmsGraphQLEngineArgs.java
Patch:
@@ -19,6 +19,7 @@
 import com.linkedin.metadata.config.ViewsConfiguration;
 import com.linkedin.metadata.config.VisualConfiguration;
 import com.linkedin.metadata.config.telemetry.TelemetryConfiguration;
+import com.linkedin.metadata.connection.ConnectionService;
 import com.linkedin.metadata.entity.EntityService;
 import com.linkedin.metadata.graph.GraphClient;
 import com.linkedin.metadata.graph.SiblingGraphService;
@@ -84,6 +85,7 @@ public class GmsGraphQLEngineArgs {
   int graphQLQueryDepthLimit;
   boolean graphQLQueryIntrospectionEnabled;
   BusinessAttributeService businessAttributeService;
+  ConnectionService connectionService;
 
   // any fork specific args should go below this line
 }

File: metadata-service/war/src/main/java/com/linkedin/gms/servlet/GraphQLServletConfig.java
Patch:
@@ -18,7 +18,8 @@
       "com.linkedin.gms.factory.query",
       "com.linkedin.gms.factory.ermodelrelation",
       "com.linkedin.gms.factory.dataproduct",
-      "com.linkedin.gms.factory.businessattribute"
+      "com.linkedin.gms.factory.businessattribute",
+      "com.linkedin.gms.factory.connection"
     })
 @Configuration
 public class GraphQLServletConfig {}

File: datahub-upgrade/src/main/java/com/linkedin/datahub/upgrade/restoreindices/SendMAEStep.java
Patch:
@@ -49,7 +49,8 @@ public KafkaJob(UpgradeContext context, RestoreIndicesArgs args) {
     @Override
     public RestoreIndicesResult call() {
       return _entityService
-          .streamRestoreIndices(context.opContext(), args, context.report()::addLine)
+          .restoreIndices(context.opContext(), args, context.report()::addLine)
+          .stream()
           .findFirst()
           .get();
     }

File: metadata-io/src/main/java/com/linkedin/metadata/entity/AspectDao.java
Patch:
@@ -3,6 +3,7 @@
 import com.linkedin.common.urn.Urn;
 import com.linkedin.metadata.aspect.batch.AspectsBatch;
 import com.linkedin.metadata.entity.ebean.EbeanAspectV2;
+import com.linkedin.metadata.entity.ebean.PartitionedStream;
 import com.linkedin.metadata.entity.restoreindices.RestoreIndicesArgs;
 import com.linkedin.metadata.utils.metrics.MetricUtils;
 import io.ebean.Transaction;
@@ -105,7 +106,7 @@ ListResult<String> listUrns(
   Integer countAspect(@Nonnull final String aspectName, @Nullable String urnLike);
 
   @Nonnull
-  Stream<Stream<EbeanAspectV2>> streamAspectBatches(final RestoreIndicesArgs args);
+  PartitionedStream<EbeanAspectV2> streamAspectBatches(final RestoreIndicesArgs args);
 
   @Nonnull
   Stream<EntityAspect> streamAspects(String entityName, String aspectName);

File: metadata-io/src/main/java/com/linkedin/metadata/entity/cassandra/CassandraAspectDao.java
Patch:
@@ -30,6 +30,7 @@
 import com.linkedin.metadata.entity.EntityAspectIdentifier;
 import com.linkedin.metadata.entity.ListResult;
 import com.linkedin.metadata.entity.ebean.EbeanAspectV2;
+import com.linkedin.metadata.entity.ebean.PartitionedStream;
 import com.linkedin.metadata.entity.restoreindices.RestoreIndicesArgs;
 import com.linkedin.metadata.query.ExtraInfo;
 import com.linkedin.metadata.query.ExtraInfoArray;
@@ -491,7 +492,7 @@ public Integer countAspect(@Nonnull String aspectName, @Nullable String urnLike)
   }
 
   @Nonnull
-  public Stream<Stream<EbeanAspectV2>> streamAspectBatches(final RestoreIndicesArgs args) {
+  public PartitionedStream<EbeanAspectV2> streamAspectBatches(final RestoreIndicesArgs args) {
     // Not implemented
     return null;
   }

File: metadata-jobs/mce-consumer-job/src/test/java/com/linkedin/metadata/kafka/MceConsumerApplicationTest.java
Patch:
@@ -8,7 +8,7 @@
 import com.linkedin.metadata.entity.restoreindices.RestoreIndicesResult;
 import io.datahubproject.metadata.context.OperationContext;
 import io.datahubproject.metadata.jobs.common.health.kafka.KafkaHealthIndicator;
-import java.util.stream.Stream;
+import java.util.List;
 import org.springframework.beans.factory.annotation.Autowired;
 import org.springframework.boot.test.context.SpringBootTest;
 import org.springframework.boot.test.web.client.TestRestTemplate;
@@ -32,8 +32,8 @@ public class MceConsumerApplicationTest extends AbstractTestNGSpringContextTests
   public void testRestliServletConfig() {
     RestoreIndicesResult mockResult = new RestoreIndicesResult();
     mockResult.setRowsMigrated(100);
-    when(_mockEntityService.streamRestoreIndices(any(OperationContext.class), any(), any()))
-        .thenReturn(Stream.of(mockResult));
+    when(_mockEntityService.restoreIndices(any(OperationContext.class), any(), any()))
+        .thenReturn(List.of(mockResult));
 
     String response =
         this.restTemplate.postForObject(

File: metadata-service/restli-servlet-impl/src/main/java/com/linkedin/metadata/resources/operations/Utils.java
Patch:
@@ -74,7 +74,8 @@ public static String restoreIndices(
     Map<String, Object> result = new HashMap<>();
     result.put("args", args);
     result.put("result", entityService
-            .streamRestoreIndices(opContext, args, log::info)
+            .restoreIndices(opContext, args, log::info)
+            .stream()
             .map(RestoreIndicesResult::toString)
             .collect(Collectors.joining("\n")));
     return result.toString();

File: metadata-service/services/src/main/java/com/linkedin/metadata/entity/EntityService.java
Patch:
@@ -29,7 +29,6 @@
 import java.util.Set;
 import java.util.concurrent.Future;
 import java.util.function.Consumer;
-import java.util.stream.Stream;
 import javax.annotation.Nonnull;
 import javax.annotation.Nullable;
 
@@ -298,7 +297,7 @@ Integer getCountAspect(
       @Nonnull OperationContext opContext, @Nonnull String aspectName, @Nullable String urnLike);
 
   // TODO: Extract this to a different service, doesn't need to be here
-  Stream<RestoreIndicesResult> streamRestoreIndices(
+  List<RestoreIndicesResult> restoreIndices(
       @Nonnull OperationContext opContext,
       @Nonnull RestoreIndicesArgs args,
       @Nonnull Consumer<String> logger);

File: li-utils/src/main/java/com/datahub/util/RecordUtils.java
Patch:
@@ -463,7 +463,7 @@ private static Object invokeMethod(@Nonnull RecordTemplate record, @Nonnull Stri
     METHOD_CACHE.putIfAbsent(record.getClass(), getMethodsFromRecordTemplate(record));
     try {
       return METHOD_CACHE.get(record.getClass()).get(fieldName).invoke(record);
-    } catch (IllegalAccessException | InvocationTargetException e) {
+    } catch (NullPointerException | IllegalAccessException | InvocationTargetException e) {
       throw new RuntimeException(
           String.format(
               "Failed to execute method for class [%s], field [%s]",

File: metadata-io/src/test/java/com/linkedin/metadata/client/JavaEntityClientTest.java
Patch:
@@ -70,7 +70,8 @@ private JavaEntityClient getJavaEntityClient() {
         _lineageSearchService,
         _timeseriesAspectService,
         rollbackService,
-        _eventProducer);
+        _eventProducer,
+        1);
   }
 
   @Test

File: metadata-io/src/test/java/io/datahubproject/test/fixtures/search/SampleDataFixtureConfiguration.java
Patch:
@@ -315,6 +315,7 @@ private EntityClient entityClientHelper(
         null,
         null,
         null,
-        null);
+        null,
+        1);
   }
 }

File: metadata-io/src/test/java/io/datahubproject/test/fixtures/search/SearchLineageFixtureConfiguration.java
Patch:
@@ -250,6 +250,7 @@ protected EntityClient entityClient(
         null,
         null,
         null,
-        null);
+        null,
+        1);
   }
 }

File: metadata-jobs/mce-consumer-job/src/test/java/com/linkedin/metadata/kafka/MceConsumerApplicationTestConfiguration.java
Patch:
@@ -46,7 +46,8 @@ public SystemEntityClient systemEntityClient(
         restClient,
         new ExponentialBackoff(1),
         1,
-        configurationProvider.getCache().getClient().getEntityClient());
+        configurationProvider.getCache().getClient().getEntityClient(),
+        1);
   }
 
   @MockBean public Database ebeanServer;

File: metadata-service/restli-client/src/main/java/com/linkedin/entity/client/SystemRestliEntityClient.java
Patch:
@@ -26,8 +26,9 @@ public SystemRestliEntityClient(
       @Nonnull final Client restliClient,
       @Nonnull final BackoffPolicy backoffPolicy,
       int retryCount,
-      EntityClientCacheConfig cacheConfig) {
-    super(restliClient, backoffPolicy, retryCount);
+      EntityClientCacheConfig cacheConfig,
+      int batchGetV2Size) {
+    super(restliClient, backoffPolicy, retryCount, batchGetV2Size);
     this.operationContextMap = CacheBuilder.newBuilder().maximumSize(500).build();
     this.entityClientCache = buildEntityClientCache(SystemRestliEntityClient.class, cacheConfig);
   }

File: metadata-service/restli-client/src/test/java/com/linkedin/common/client/BaseClientTest.java
Patch:
@@ -37,7 +37,7 @@ public void testZeroRetry() throws RemoteInvocationException {
     when(mockRestliClient.sendRequest(any(ActionRequest.class))).thenReturn(mockFuture);
 
     RestliEntityClient testClient =
-        new RestliEntityClient(mockRestliClient, new ExponentialBackoff(1), 0);
+        new RestliEntityClient(mockRestliClient, new ExponentialBackoff(1), 0, 10);
     testClient.sendClientRequest(testRequestBuilder, AUTH);
     // Expected 1 actual try and 0 retries
     verify(mockRestliClient).sendRequest(any(ActionRequest.class));
@@ -56,7 +56,7 @@ public void testMultipleRetries() throws RemoteInvocationException {
         .thenReturn(mockFuture);
 
     RestliEntityClient testClient =
-        new RestliEntityClient(mockRestliClient, new ExponentialBackoff(1), 1);
+        new RestliEntityClient(mockRestliClient, new ExponentialBackoff(1), 1, 10);
     testClient.sendClientRequest(testRequestBuilder, AUTH);
     // Expected 1 actual try and 1 retries
     verify(mockRestliClient, times(2)).sendRequest(any(ActionRequest.class));
@@ -73,7 +73,7 @@ public void testNonRetry() {
         .thenThrow(new RuntimeException(new RequiredFieldNotPresentException("value")));
 
     RestliEntityClient testClient =
-        new RestliEntityClient(mockRestliClient, new ExponentialBackoff(1), 1);
+        new RestliEntityClient(mockRestliClient, new ExponentialBackoff(1), 1, 10);
     assertThrows(
         RuntimeException.class, () -> testClient.sendClientRequest(testRequestBuilder, AUTH));
   }

File: entity-registry/src/main/java/com/linkedin/metadata/aspect/plugins/PluginFactory.java
Patch:
@@ -157,10 +157,10 @@ protected static <T extends PluginSpec> List<T> initPlugins(
                                     "Error constructing entity registry plugin class: {}",
                                     config.getClassName(),
                                     e);
-                                return Stream.<T>empty();
+                                return (T) null;
                               }
                             })
-                        .map(plugin -> (T) plugin)
+                        .filter(Objects::nonNull)
                         .filter(PluginSpec::enabled)
                         .collect(Collectors.toList());
                   }

File: metadata-service/war/src/main/java/com/linkedin/gms/CommonApplicationConfig.java
Patch:
@@ -14,6 +14,8 @@
 @ComponentScan(
     basePackages = {
       "com.linkedin.metadata.boot",
+      "com.linkedin.metadata.service",
+      "com.datahub.event",
       "com.linkedin.gms.factory.config",
       "com.linkedin.gms.factory.entityregistry",
       "com.linkedin.gms.factory.common",
@@ -34,7 +36,7 @@
       "com.linkedin.gms.factory.auth",
       "com.linkedin.gms.factory.search",
       "com.linkedin.gms.factory.secret",
-      "com.linkedin.gms.factory.timeseries"
+      "com.linkedin.gms.factory.timeseries",
     })
 @PropertySource(value = "classpath:/application.yaml", factory = YamlPropertySourceFactory.class)
 @Configuration

File: entity-registry/src/main/java/com/linkedin/metadata/aspect/batch/PatchMCP.java
Patch:
@@ -1,8 +1,8 @@
 package com.linkedin.metadata.aspect.batch;
 
-import com.github.fge.jsonpatch.Patch;
 import com.linkedin.data.template.RecordTemplate;
 import com.linkedin.metadata.aspect.AspectRetriever;
+import jakarta.json.JsonPatch;
 
 /**
  * A change proposal represented as a patch to an exiting stored object in the primary data store.
@@ -17,5 +17,5 @@ public interface PatchMCP extends MCPItem {
    */
   ChangeMCP applyPatch(RecordTemplate recordTemplate, AspectRetriever aspectRetriever);
 
-  Patch getPatch();
+  JsonPatch getPatch();
 }

File: entity-registry/src/main/java/com/linkedin/metadata/aspect/patch/template/common/GenericPatchTemplate.java
Patch:
@@ -1,7 +1,6 @@
 package com.linkedin.metadata.aspect.patch.template.common;
 
 import com.fasterxml.jackson.databind.JsonNode;
-import com.github.fge.jsonpatch.JsonPatchException;
 import com.linkedin.data.template.RecordTemplate;
 import com.linkedin.metadata.aspect.patch.GenericJsonPatch;
 import com.linkedin.metadata.aspect.patch.template.CompoundKeyTemplate;
@@ -53,7 +52,7 @@ public JsonNode rebaseFields(JsonNode patched) {
     return transformedNode;
   }
 
-  public T applyPatch(RecordTemplate recordTemplate) throws IOException, JsonPatchException {
+  public T applyPatch(RecordTemplate recordTemplate) throws IOException {
     return super.applyPatch(recordTemplate, genericJsonPatch.getJsonPatch());
   }
 }

File: metadata-io/src/main/java/com/linkedin/metadata/entity/ebean/batch/ChangeItemImpl.java
Patch:
@@ -3,7 +3,6 @@
 import static com.linkedin.metadata.entity.AspectUtils.validateAspect;
 
 import com.datahub.util.exception.ModelConversionException;
-import com.github.fge.jsonpatch.JsonPatchException;
 import com.linkedin.common.AuditStamp;
 import com.linkedin.common.urn.Urn;
 import com.linkedin.data.template.RecordTemplate;
@@ -54,7 +53,7 @@ public static ChangeItemImpl fromPatch(
 
     try {
       builder.recordTemplate(genericPatchTemplate.applyPatch(currentValue));
-    } catch (JsonPatchException | IOException e) {
+    } catch (IOException e) {
       throw new RuntimeException(e);
     }
 

File: metadata-service/openapi-servlet/src/main/java/io/datahubproject/openapi/v2/controller/EntityController.java
Patch:
@@ -677,8 +677,7 @@ private ChangeMCP toUpsertItem(
       @Nonnull AspectSpec aspectSpec,
       @Nullable RecordTemplate currentValue,
       @Nonnull GenericPatchTemplate<? extends RecordTemplate> genericPatchTemplate,
-      @Nonnull Actor actor)
-      throws URISyntaxException {
+      @Nonnull Actor actor) {
     return ChangeItemImpl.fromPatch(
         urn,
         aspectSpec,

File: metadata-io/src/main/java/com/linkedin/metadata/search/utils/BrowsePathV2Utils.java
Patch:
@@ -67,8 +67,7 @@ public static BrowsePathsV2 getDefaultBrowsePathV2(
           BrowsePathEntryArray defaultDatasetPathEntries =
               getDefaultDatasetPathEntries(dsKey.getName(), dataPlatformDelimiter);
           if (defaultDatasetPathEntries.size() > 0) {
-            browsePathEntries.addAll(
-                getDefaultDatasetPathEntries(dsKey.getName().toLowerCase(), dataPlatformDelimiter));
+            browsePathEntries.addAll(defaultDatasetPathEntries);
           } else {
             browsePathEntries.add(createBrowsePathEntry(DEFAULT_FOLDER_NAME, null));
           }

File: metadata-io/src/test/java/com/linkedin/metadata/search/utils/BrowsePathV2UtilsTest.java
Patch:
@@ -146,8 +146,8 @@ public void testGetDefaultBrowsePathV2WithoutContainers() throws URISyntaxExcept
         BrowsePathV2Utils.getDefaultBrowsePathV2(
             mock(OperationContext.class), datasetUrn, this.registry, '.', mockService, true);
     BrowsePathEntryArray expectedPath = new BrowsePathEntryArray();
-    BrowsePathEntry entry1 = new BrowsePathEntry().setId("test");
-    BrowsePathEntry entry2 = new BrowsePathEntry().setId("a");
+    BrowsePathEntry entry1 = new BrowsePathEntry().setId("Test");
+    BrowsePathEntry entry2 = new BrowsePathEntry().setId("A");
     expectedPath.add(entry1);
     expectedPath.add(entry2);
     Assert.assertEquals(browsePathsV2.getPath(), expectedPath);

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/common/CacheConfig.java
Patch:
@@ -63,6 +63,9 @@ public CacheManager hazelcastCacheManager() {
     mapConfig.setName("default");
     config.addMapConfig(mapConfig);
 
+    // Force classloader to load from application code
+    config.setClassLoader(this.getClass().getClassLoader());
+
     config.getNetworkConfig().getJoin().getMulticastConfig().setEnabled(false);
     config
         .getNetworkConfig()

File: datahub-frontend/app/auth/sso/oidc/OidcConfigs.java
Patch:
@@ -226,8 +226,8 @@ public Builder from(final com.typesafe.config.Config configs, final String ssoSe
         extractJwtAccessTokenClaims =
             Optional.of(jsonNode.get(EXTRACT_JWT_ACCESS_TOKEN_CLAIMS).asBoolean());
       }
-      if (jsonNode.has(PREFERRED_JWS_ALGORITHM_2)) {
-        preferredJwsAlgorithm = Optional.of(jsonNode.get(PREFERRED_JWS_ALGORITHM_2).asText());
+      if (jsonNode.has(PREFERRED_JWS_ALGORITHM)) {
+        preferredJwsAlgorithm = Optional.of(jsonNode.get(PREFERRED_JWS_ALGORITHM).asText());
       } else {
         preferredJwsAlgorithm =
             Optional.ofNullable(getOptional(configs, OIDC_PREFERRED_JWS_ALGORITHM, null));

File: datahub-frontend/test/security/OidcConfigurationTest.java
Patch:
@@ -322,7 +322,7 @@ public void readTimeoutPropagation() {
 
   @Test
   public void readPreferredJwsAlgorithmPropagationFromConfig() {
-    final String SSO_SETTINGS_JSON_STR = new JSONObject().put(PREFERRED_JWS_ALGORITHM, "HS256").toString();
+    final String SSO_SETTINGS_JSON_STR = new JSONObject().toString();
     CONFIG.withValue(OIDC_PREFERRED_JWS_ALGORITHM, ConfigValueFactory.fromAnyRef("RS256"));
     OidcConfigs.Builder oidcConfigsBuilder = new OidcConfigs.Builder();
     oidcConfigsBuilder.from(CONFIG, SSO_SETTINGS_JSON_STR);
@@ -333,7 +333,7 @@ public void readPreferredJwsAlgorithmPropagationFromConfig() {
 
   @Test
   public void readPreferredJwsAlgorithmPropagationFromJSON() {
-    final String SSO_SETTINGS_JSON_STR = new JSONObject().put(PREFERRED_JWS_ALGORITHM, "Unused").put(PREFERRED_JWS_ALGORITHM_2, "HS256").toString();
+    final String SSO_SETTINGS_JSON_STR = new JSONObject().put(PREFERRED_JWS_ALGORITHM, "HS256").toString();
     CONFIG.withValue(OIDC_PREFERRED_JWS_ALGORITHM, ConfigValueFactory.fromAnyRef("RS256"));
     OidcConfigs.Builder oidcConfigsBuilder = new OidcConfigs.Builder();
     oidcConfigsBuilder.from(CONFIG, SSO_SETTINGS_JSON_STR);

File: datahub-upgrade/src/main/java/com/linkedin/datahub/upgrade/UpgradeCliApplication.java
Patch:
@@ -5,7 +5,6 @@
 import com.linkedin.gms.factory.graphql.GraphQLEngineFactory;
 import com.linkedin.gms.factory.kafka.KafkaEventConsumerFactory;
 import com.linkedin.gms.factory.kafka.SimpleKafkaConsumerFactory;
-import com.linkedin.gms.factory.kafka.schemaregistry.InternalSchemaRegistryFactory;
 import com.linkedin.gms.factory.telemetry.ScheduledAnalyticsFactory;
 import org.springframework.boot.WebApplicationType;
 import org.springframework.boot.autoconfigure.SpringBootApplication;
@@ -31,7 +30,6 @@
             DataHubAuthorizerFactory.class,
             SimpleKafkaConsumerFactory.class,
             KafkaEventConsumerFactory.class,
-            InternalSchemaRegistryFactory.class,
             GraphQLEngineFactory.class
           })
     })

File: metadata-events/mxe-utils-avro/src/main/java/com/linkedin/metadata/EventUtils.java
Patch:
@@ -57,7 +57,7 @@ public class EventUtils {
   private static final Schema ORIGINAL_MCP_AVRO_SCHEMA =
       getAvroSchemaFromResource("avro/com/linkedin/mxe/MetadataChangeProposal.avsc");
 
-  public static final Schema ORIGINAL_MCL_AVRO_SCHEMA =
+  private static final Schema ORIGINAL_MCL_AVRO_SCHEMA =
       getAvroSchemaFromResource("avro/com/linkedin/mxe/MetadataChangeLog.avsc");
 
   private static final Schema ORIGINAL_FMCL_AVRO_SCHEMA =
@@ -84,7 +84,7 @@ public class EventUtils {
   private static final Schema RENAMED_MCP_AVRO_SCHEMA =
       com.linkedin.pegasus2avro.mxe.MetadataChangeProposal.SCHEMA$;
 
-  private static final Schema RENAMED_MCL_AVRO_SCHEMA =
+  public static final Schema RENAMED_MCL_AVRO_SCHEMA =
       com.linkedin.pegasus2avro.mxe.MetadataChangeLog.SCHEMA$;
 
   private static final Schema RENAMED_FMCP_AVRO_SCHEMA =

File: metadata-jobs/mae-consumer-job/src/main/java/com/linkedin/metadata/kafka/MaeConsumerApplication.java
Patch:
@@ -14,7 +14,10 @@
     exclude = {ElasticsearchRestClientAutoConfiguration.class, CassandraAutoConfiguration.class})
 @ComponentScan(
     basePackages = {
+      "com.linkedin.gms.factory.common",
       "com.linkedin.gms.factory.kafka",
+      "com.linkedin.gms.factory.kafka.common",
+      "com.linkedin.gms.factory.kafka.schemaregistry",
       "com.linkedin.metadata.boot.kafka",
       "com.linkedin.metadata.kafka",
       "com.linkedin.metadata.dao.producer",

File: metadata-jobs/mae-consumer/src/main/java/com/linkedin/metadata/kafka/hook/UpdateIndicesHook.java
Patch:
@@ -11,7 +11,7 @@
 import com.linkedin.metadata.service.UpdateIndicesService;
 import com.linkedin.mxe.MetadataChangeLog;
 import io.datahubproject.metadata.context.OperationContext;
-import jakarta.annotation.Nonnull;
+import javax.annotation.Nonnull;
 import lombok.extern.slf4j.Slf4j;
 import org.springframework.beans.factory.annotation.Value;
 import org.springframework.context.annotation.Import;

File: metadata-jobs/mae-consumer/src/test/java/com/linkedin/metadata/kafka/hook/spring/MCLSpringTest.java
Patch:
@@ -24,7 +24,7 @@
       "kafka.schemaRegistry.type=INTERNAL"
     })
 @TestPropertySource(
-    locations = "classpath:/application.yml",
+    locations = "classpath:/application.yaml",
     properties = {"MCL_CONSUMER_ENABLED=true"})
 @EnableAutoConfiguration(exclude = {CassandraAutoConfiguration.class})
 public class MCLSpringTest extends AbstractTestNGSpringContextTests {

File: metadata-jobs/mce-consumer-job/src/main/java/com/linkedin/metadata/kafka/MceConsumerApplication.java
Patch:
@@ -40,7 +40,7 @@
           type = FilterType.ASSIGNABLE_TYPE,
           classes = {ScheduledAnalyticsFactory.class})
     })
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
+@PropertySource(value = "classpath:/application.yaml", factory = YamlPropertySourceFactory.class)
 public class MceConsumerApplication {
 
   public static void main(String[] args) {

File: metadata-jobs/mce-consumer-job/src/main/java/com/linkedin/metadata/restli/EbeanServerConfig.java
Patch:
@@ -1,6 +1,6 @@
 package com.linkedin.metadata.restli;
 
-import static com.linkedin.gms.factory.common.LocalEbeanServerConfigFactory.getListenerToTrackCounts;
+import static com.linkedin.gms.factory.common.LocalEbeanConfigFactory.getListenerToTrackCounts;
 
 import io.ebean.datasource.DataSourceConfig;
 import java.util.HashMap;

File: metadata-service/auth-config/src/main/java/com/datahub/authentication/AuthenticationConfiguration.java
Patch:
@@ -3,7 +3,7 @@
 import java.util.List;
 import lombok.Data;
 
-/** POJO representing the "authentication" configuration block in application.yml. */
+/** POJO representing the "authentication" configuration block in application.yaml. */
 @Data
 public class AuthenticationConfiguration {
   /** Whether authentication is enabled */

File: metadata-service/auth-config/src/main/java/com/datahub/authentication/AuthenticatorConfiguration.java
Patch:
@@ -5,7 +5,7 @@
 
 /**
  * POJO representing {@link com.datahub.plugins.auth.authentication.Authenticator} configurations
- * provided in the application.yml.
+ * provided in the application.yaml.
  */
 @Data
 public class AuthenticatorConfiguration {

File: metadata-service/auth-config/src/main/java/com/datahub/authorization/AuthorizationConfiguration.java
Patch:
@@ -5,7 +5,7 @@
 import java.util.List;
 import lombok.Data;
 
-/** POJO representing the "authentication" configuration block in application.yml. */
+/** POJO representing the "authentication" configuration block in application.yaml. */
 @Data
 public class AuthorizationConfiguration {
   /** Configuration for the default DataHub Policies-based authorizer. */

File: metadata-service/auth-config/src/main/java/com/datahub/authorization/AuthorizerConfiguration.java
Patch:
@@ -4,7 +4,7 @@
 import java.util.Map;
 import lombok.Data;
 
-/** POJO representing {@link Authorizer} configurations provided in the application.yml. */
+/** POJO representing {@link Authorizer} configurations provided in the application.yaml. */
 @Data
 public class AuthorizerConfiguration {
   /** Whether to enable this authorizer */

File: metadata-service/configuration/src/main/java/com/linkedin/metadata/config/DataHubConfiguration.java
Patch:
@@ -2,7 +2,7 @@
 
 import lombok.Data;
 
-/** POJO representing the "datahub" configuration block in application.yml. */
+/** POJO representing the "datahub" configuration block in application.yaml. */
 @Data
 public class DataHubConfiguration {
   /**

File: metadata-service/configuration/src/main/java/com/linkedin/metadata/config/IngestionConfiguration.java
Patch:
@@ -2,7 +2,7 @@
 
 import lombok.Data;
 
-/** POJO representing the "ingestion" configuration block in application.yml. */
+/** POJO representing the "ingestion" configuration block in application.yaml. */
 @Data
 public class IngestionConfiguration {
   /** Whether managed ingestion is enabled */

File: metadata-service/configuration/src/main/java/com/linkedin/metadata/config/TestsConfiguration.java
Patch:
@@ -2,7 +2,7 @@
 
 import lombok.Data;
 
-/** POJO representing the "tests" configuration block in application.yml.on.yml */
+/** POJO representing the "tests" configuration block in application.yaml.on.yml */
 @Data
 public class TestsConfiguration {
   /** Whether tests are enabled */

File: metadata-service/configuration/src/main/java/com/linkedin/metadata/config/ViewsConfiguration.java
Patch:
@@ -2,7 +2,7 @@
 
 import lombok.Data;
 
-/** POJO representing the "views" configuration block in application.yml.on.yml */
+/** POJO representing the "views" configuration block in application.yaml.on.yml */
 @Data
 public class ViewsConfiguration {
   /** Whether Views are enabled */

File: metadata-service/configuration/src/main/java/com/linkedin/metadata/config/VisualConfiguration.java
Patch:
@@ -2,7 +2,7 @@
 
 import lombok.Data;
 
-/** POJO representing visualConfig block in the application.yml. */
+/** POJO representing visualConfig block in the application.yaml. */
 @Data
 public class VisualConfiguration {
   /** Asset related configurations */

File: metadata-service/configuration/src/main/java/com/linkedin/metadata/config/search/CustomConfiguration.java
Patch:
@@ -17,7 +17,7 @@ public class CustomConfiguration {
   private String file;
 
   /**
-   * Materialize the search configuration from a location external to main application.yml
+   * Materialize the search configuration from a location external to main application.yaml
    *
    * @param mapper yaml enabled jackson mapper
    * @return search configuration class

File: metadata-service/configuration/src/main/java/com/linkedin/metadata/config/telemetry/TelemetryConfiguration.java
Patch:
@@ -2,7 +2,7 @@
 
 import lombok.Data;
 
-/** POJO representing the "telemetry" configuration block in application.yml. */
+/** POJO representing the "telemetry" configuration block in application.yaml. */
 @Data
 public class TelemetryConfiguration {
   /** Whether cli telemetry is enabled */

File: metadata-service/configuration/src/main/java/com/linkedin/metadata/spring/YamlPropertySourceFactory.java
Patch:
@@ -8,7 +8,7 @@
 import org.springframework.core.io.support.EncodedResource;
 import org.springframework.core.io.support.PropertySourceFactory;
 
-/** Required for Spring to parse the application.yml provided by this module */
+/** Required for Spring to parse the application.yaml provided by this module */
 public class YamlPropertySourceFactory implements PropertySourceFactory {
 
   @Override

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/auth/AuthorizerChainFactory.java
Patch:
@@ -19,7 +19,6 @@
 import com.google.common.collect.ImmutableMap;
 import com.linkedin.entity.client.SystemEntityClient;
 import com.linkedin.gms.factory.config.ConfigurationProvider;
-import com.linkedin.metadata.spring.YamlPropertySourceFactory;
 import io.datahubproject.metadata.context.OperationContext;
 import jakarta.annotation.Nonnull;
 import java.nio.file.Path;
@@ -35,12 +34,10 @@
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
 import org.springframework.context.annotation.Import;
-import org.springframework.context.annotation.PropertySource;
 import org.springframework.context.annotation.Scope;
 
 @Slf4j
 @Configuration
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 @Import({DataHubAuthorizerFactory.class})
 public class AuthorizerChainFactory {
   @Autowired

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/auth/DataHubAuthorizerFactory.java
Patch:
@@ -2,18 +2,15 @@
 
 import com.datahub.authorization.DataHubAuthorizer;
 import com.linkedin.entity.client.SystemEntityClient;
-import com.linkedin.metadata.spring.YamlPropertySourceFactory;
 import io.datahubproject.metadata.context.OperationContext;
 import javax.annotation.Nonnull;
 import org.springframework.beans.factory.annotation.Qualifier;
 import org.springframework.beans.factory.annotation.Value;
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
-import org.springframework.context.annotation.PropertySource;
 import org.springframework.context.annotation.Scope;
 
 @Configuration
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 public class DataHubAuthorizerFactory {
 
   @Value("${authorization.defaultAuthorizer.cacheRefreshIntervalSecs}")

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/auth/DataHubTokenServiceFactory.java
Patch:
@@ -2,19 +2,16 @@
 
 import com.datahub.authentication.token.StatefulTokenService;
 import com.linkedin.metadata.entity.EntityService;
-import com.linkedin.metadata.spring.YamlPropertySourceFactory;
 import io.datahubproject.metadata.context.OperationContext;
 import javax.annotation.Nonnull;
 import org.springframework.beans.factory.annotation.Autowired;
 import org.springframework.beans.factory.annotation.Qualifier;
 import org.springframework.beans.factory.annotation.Value;
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
-import org.springframework.context.annotation.PropertySource;
 import org.springframework.context.annotation.Scope;
 
 @Configuration
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 public class DataHubTokenServiceFactory {
 
   @Value("${authentication.tokenService.signingKey:}")

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/auth/GroupServiceFactory.java
Patch:
@@ -4,17 +4,14 @@
 import com.linkedin.entity.client.EntityClient;
 import com.linkedin.metadata.entity.EntityService;
 import com.linkedin.metadata.graph.GraphClient;
-import com.linkedin.metadata.spring.YamlPropertySourceFactory;
 import javax.annotation.Nonnull;
 import org.springframework.beans.factory.annotation.Autowired;
 import org.springframework.beans.factory.annotation.Qualifier;
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
-import org.springframework.context.annotation.PropertySource;
 import org.springframework.context.annotation.Scope;
 
 @Configuration
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 public class GroupServiceFactory {
   @Autowired
   @Qualifier("entityService")

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/auth/InviteTokenServiceFactory.java
Patch:
@@ -2,18 +2,15 @@
 
 import com.datahub.authentication.invite.InviteTokenService;
 import com.linkedin.entity.client.EntityClient;
-import com.linkedin.metadata.spring.YamlPropertySourceFactory;
 import io.datahubproject.metadata.services.SecretService;
 import javax.annotation.Nonnull;
 import org.springframework.beans.factory.annotation.Autowired;
 import org.springframework.beans.factory.annotation.Qualifier;
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
-import org.springframework.context.annotation.PropertySource;
 import org.springframework.context.annotation.Scope;
 
 @Configuration
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 public class InviteTokenServiceFactory {
 
   @Autowired

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/auth/NativeUserServiceFactory.java
Patch:
@@ -4,18 +4,15 @@
 import com.linkedin.entity.client.SystemEntityClient;
 import com.linkedin.gms.factory.config.ConfigurationProvider;
 import com.linkedin.metadata.entity.EntityService;
-import com.linkedin.metadata.spring.YamlPropertySourceFactory;
 import io.datahubproject.metadata.services.SecretService;
 import javax.annotation.Nonnull;
 import org.springframework.beans.factory.annotation.Autowired;
 import org.springframework.beans.factory.annotation.Qualifier;
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
-import org.springframework.context.annotation.PropertySource;
 import org.springframework.context.annotation.Scope;
 
 @Configuration
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 public class NativeUserServiceFactory {
   @Autowired
   @Qualifier("entityService")

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/auth/PostServiceFactory.java
Patch:
@@ -2,16 +2,13 @@
 
 import com.datahub.authentication.post.PostService;
 import com.linkedin.entity.client.EntityClient;
-import com.linkedin.metadata.spring.YamlPropertySourceFactory;
 import javax.annotation.Nonnull;
 import org.springframework.beans.factory.annotation.Qualifier;
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
-import org.springframework.context.annotation.PropertySource;
 import org.springframework.context.annotation.Scope;
 
 @Configuration
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 public class PostServiceFactory {
 
   @Bean(name = "postService")

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/auth/RoleServiceFactory.java
Patch:
@@ -2,16 +2,13 @@
 
 import com.datahub.authorization.role.RoleService;
 import com.linkedin.entity.client.EntityClient;
-import com.linkedin.metadata.spring.YamlPropertySourceFactory;
 import javax.annotation.Nonnull;
 import org.springframework.beans.factory.annotation.Qualifier;
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
-import org.springframework.context.annotation.PropertySource;
 import org.springframework.context.annotation.Scope;
 
 @Configuration
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 public class RoleServiceFactory {
 
   @Bean(name = "roleService")

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/auth/SystemAuthenticationFactory.java
Patch:
@@ -3,14 +3,12 @@
 import com.datahub.authentication.Actor;
 import com.datahub.authentication.ActorType;
 import com.datahub.authentication.Authentication;
-import com.linkedin.metadata.spring.YamlPropertySourceFactory;
 import javax.annotation.Nonnull;
 import lombok.Data;
 import org.springframework.beans.factory.annotation.Value;
 import org.springframework.boot.context.properties.ConfigurationProperties;
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
-import org.springframework.context.annotation.PropertySource;
 import org.springframework.context.annotation.Scope;
 
 /**
@@ -19,7 +17,6 @@
  */
 @Configuration
 @ConfigurationProperties
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 @Data
 public class SystemAuthenticationFactory {
 

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/common/ElasticSearchGraphServiceFactory.java
Patch:
@@ -8,17 +8,14 @@
 import com.linkedin.metadata.graph.elastic.ElasticSearchGraphService;
 import com.linkedin.metadata.models.registry.EntityRegistry;
 import com.linkedin.metadata.models.registry.LineageRegistry;
-import com.linkedin.metadata.spring.YamlPropertySourceFactory;
 import javax.annotation.Nonnull;
 import org.springframework.beans.factory.annotation.Autowired;
 import org.springframework.beans.factory.annotation.Qualifier;
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
 import org.springframework.context.annotation.Import;
-import org.springframework.context.annotation.PropertySource;
 
 @Configuration
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 @Import({BaseElasticSearchComponentsFactory.class, EntityRegistryFactory.class})
 public class ElasticSearchGraphServiceFactory {
   @Autowired

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/common/ElasticSearchSystemMetadataServiceFactory.java
Patch:
@@ -1,7 +1,6 @@
 package com.linkedin.gms.factory.common;
 
 import com.linkedin.gms.factory.search.BaseElasticSearchComponentsFactory;
-import com.linkedin.metadata.spring.YamlPropertySourceFactory;
 import com.linkedin.metadata.systemmetadata.ESSystemMetadataDAO;
 import com.linkedin.metadata.systemmetadata.ElasticSearchSystemMetadataService;
 import javax.annotation.Nonnull;
@@ -10,10 +9,8 @@
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
 import org.springframework.context.annotation.Import;
-import org.springframework.context.annotation.PropertySource;
 
 @Configuration
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 @Import({BaseElasticSearchComponentsFactory.class})
 public class ElasticSearchSystemMetadataServiceFactory {
   @Autowired

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/common/ElasticsearchSSLContextFactory.java
Patch:
@@ -1,6 +1,5 @@
 package com.linkedin.gms.factory.common;
 
-import com.linkedin.metadata.spring.YamlPropertySourceFactory;
 import java.io.FileInputStream;
 import java.io.IOException;
 import java.io.InputStream;
@@ -17,10 +16,8 @@
 import org.springframework.beans.factory.annotation.Value;
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
-import org.springframework.context.annotation.PropertySource;
 
 @Configuration
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 public class ElasticsearchSSLContextFactory {
 
   @Value("${elasticsearch.sslContext.protocol}")

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/common/GraphServiceFactory.java
Patch:
@@ -3,7 +3,6 @@
 import com.linkedin.metadata.graph.GraphService;
 import com.linkedin.metadata.graph.elastic.ElasticSearchGraphService;
 import com.linkedin.metadata.graph.neo4j.Neo4jGraphService;
-import com.linkedin.metadata.spring.YamlPropertySourceFactory;
 import javax.annotation.Nonnull;
 import org.springframework.beans.factory.annotation.Autowired;
 import org.springframework.beans.factory.annotation.Qualifier;
@@ -13,10 +12,8 @@
 import org.springframework.context.annotation.DependsOn;
 import org.springframework.context.annotation.Import;
 import org.springframework.context.annotation.Primary;
-import org.springframework.context.annotation.PropertySource;
 
 @Configuration
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 @Import({Neo4jGraphServiceFactory.class, ElasticSearchGraphServiceFactory.class})
 public class GraphServiceFactory {
   @Autowired

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/common/IndexConventionFactory.java
Patch:
@@ -1,20 +1,17 @@
 package com.linkedin.gms.factory.common;
 
-import com.linkedin.metadata.spring.YamlPropertySourceFactory;
 import com.linkedin.metadata.utils.elasticsearch.IndexConvention;
 import com.linkedin.metadata.utils.elasticsearch.IndexConventionImpl;
 import org.springframework.beans.factory.annotation.Value;
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
-import org.springframework.context.annotation.PropertySource;
 
 /**
  * Creates a {@link IndexConvention} to generate search index names.
  *
  * <p>This allows you to easily add prefixes to the index names.
  */
 @Configuration
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 public class IndexConventionFactory {
   public static final String INDEX_CONVENTION_BEAN = "searchIndexConvention";
 

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/common/Neo4jDriverFactory.java
Patch:
@@ -1,6 +1,5 @@
 package com.linkedin.gms.factory.common;
 
-import com.linkedin.metadata.spring.YamlPropertySourceFactory;
 import java.util.concurrent.TimeUnit;
 import org.neo4j.driver.AuthTokens;
 import org.neo4j.driver.Config;
@@ -9,10 +8,8 @@
 import org.springframework.beans.factory.annotation.Value;
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
-import org.springframework.context.annotation.PropertySource;
 
 @Configuration
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 public class Neo4jDriverFactory {
   @Value("${neo4j.username}")
   private String username;

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/common/RestHighLevelClientFactory.java
Patch:
@@ -1,7 +1,6 @@
 package com.linkedin.gms.factory.common;
 
 import com.linkedin.gms.factory.auth.AwsRequestSigningApacheInterceptor;
-import com.linkedin.metadata.spring.YamlPropertySourceFactory;
 import java.io.IOException;
 import javax.annotation.Nonnull;
 import javax.net.ssl.HostnameVerifier;
@@ -38,13 +37,11 @@
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
 import org.springframework.context.annotation.Import;
-import org.springframework.context.annotation.PropertySource;
 import software.amazon.awssdk.auth.credentials.DefaultCredentialsProvider;
 import software.amazon.awssdk.auth.signer.Aws4Signer;
 
 @Slf4j
 @Configuration
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 @Import({ElasticsearchSSLContextFactory.class})
 public class RestHighLevelClientFactory {
 

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/context/services/RestrictedServiceFactory.java
Patch:
@@ -1,18 +1,15 @@
 package com.linkedin.gms.factory.context.services;
 
-import com.linkedin.metadata.spring.YamlPropertySourceFactory;
 import io.datahubproject.metadata.services.RestrictedService;
 import io.datahubproject.metadata.services.SecretService;
 import javax.annotation.Nonnull;
 import org.springframework.beans.factory.annotation.Autowired;
 import org.springframework.beans.factory.annotation.Qualifier;
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
-import org.springframework.context.annotation.PropertySource;
 import org.springframework.context.annotation.Scope;
 
 @Configuration
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 public class RestrictedServiceFactory {
 
   @Autowired

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/dataproduct/DataProductServiceFactory.java
Patch:
@@ -3,17 +3,14 @@
 import com.linkedin.entity.client.EntityClient;
 import com.linkedin.metadata.graph.GraphClient;
 import com.linkedin.metadata.service.DataProductService;
-import com.linkedin.metadata.spring.YamlPropertySourceFactory;
 import javax.annotation.Nonnull;
 import org.springframework.beans.factory.annotation.Autowired;
 import org.springframework.beans.factory.annotation.Qualifier;
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
-import org.springframework.context.annotation.PropertySource;
 import org.springframework.context.annotation.Scope;
 
 @Configuration
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 public class DataProductServiceFactory {
 
   @Autowired

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/entity/EntityAspectDaoFactory.java
Patch:
@@ -7,6 +7,7 @@
 import com.linkedin.metadata.entity.ebean.EbeanAspectDao;
 import io.ebean.Database;
 import javax.annotation.Nonnull;
+import org.springframework.beans.factory.annotation.Qualifier;
 import org.springframework.boot.autoconfigure.condition.ConditionalOnProperty;
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
@@ -16,11 +17,11 @@
 public class EntityAspectDaoFactory {
 
   @Bean(name = "entityAspectDao")
-  @DependsOn({"gmsEbeanServiceConfig"})
   @ConditionalOnProperty(name = "entityService.impl", havingValue = "ebean", matchIfMissing = true)
   @Nonnull
   protected AspectDao createEbeanInstance(
-      Database server, final ConfigurationProvider configurationProvider) {
+      @Qualifier("ebeanServer") final Database server,
+      final ConfigurationProvider configurationProvider) {
     return new EbeanAspectDao(server, configurationProvider.getEbean());
   }
 

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/entity/EntityAspectMigrationsDaoFactory.java
Patch:
@@ -7,6 +7,7 @@
 import com.linkedin.metadata.entity.ebean.EbeanAspectDao;
 import io.ebean.Database;
 import javax.annotation.Nonnull;
+import org.springframework.beans.factory.annotation.Qualifier;
 import org.springframework.boot.autoconfigure.condition.ConditionalOnProperty;
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
@@ -16,11 +17,11 @@
 public class EntityAspectMigrationsDaoFactory {
 
   @Bean(name = "entityAspectMigrationsDao")
-  @DependsOn({"gmsEbeanServiceConfig"})
   @ConditionalOnProperty(name = "entityService.impl", havingValue = "ebean", matchIfMissing = true)
   @Nonnull
   protected AspectMigrationsDao createEbeanInstance(
-      Database server, final ConfigurationProvider configurationProvider) {
+      @Qualifier("ebeanServer") final Database server,
+      final ConfigurationProvider configurationProvider) {
     return new EbeanAspectDao(server, configurationProvider.getEbean());
   }
 

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/entityclient/EntityClientConfigFactory.java
Patch:
@@ -2,14 +2,11 @@
 
 import com.linkedin.gms.factory.config.ConfigurationProvider;
 import com.linkedin.metadata.config.cache.client.EntityClientCacheConfig;
-import com.linkedin.metadata.spring.YamlPropertySourceFactory;
 import org.springframework.beans.factory.annotation.Qualifier;
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
-import org.springframework.context.annotation.PropertySource;
 
 @Configuration
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 public class EntityClientConfigFactory {
 
   @Bean

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/entityclient/JavaEntityClientFactory.java
Patch:
@@ -13,18 +13,15 @@
 import com.linkedin.metadata.search.SearchService;
 import com.linkedin.metadata.search.client.CachingEntitySearchService;
 import com.linkedin.metadata.service.RollbackService;
-import com.linkedin.metadata.spring.YamlPropertySourceFactory;
 import com.linkedin.metadata.timeseries.TimeseriesAspectService;
 import javax.inject.Singleton;
 import org.springframework.beans.factory.annotation.Qualifier;
 import org.springframework.boot.autoconfigure.condition.ConditionalOnProperty;
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
-import org.springframework.context.annotation.PropertySource;
 
 /** The *Java* Entity Client should be preferred if executing within the GMS service. */
 @Configuration
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 @ConditionalOnProperty(name = "entityClient.impl", havingValue = "java", matchIfMissing = true)
 public class JavaEntityClientFactory {
 

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/entityclient/RestliEntityClientFactory.java
Patch:
@@ -6,7 +6,6 @@
 import com.linkedin.entity.client.SystemRestliEntityClient;
 import com.linkedin.metadata.config.cache.client.EntityClientCacheConfig;
 import com.linkedin.metadata.restli.DefaultRestliClientFactory;
-import com.linkedin.metadata.spring.YamlPropertySourceFactory;
 import com.linkedin.parseq.retry.backoff.ExponentialBackoff;
 import com.linkedin.restli.client.Client;
 import java.net.URI;
@@ -15,11 +14,9 @@
 import org.springframework.boot.autoconfigure.condition.ConditionalOnProperty;
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
-import org.springframework.context.annotation.PropertySource;
 
 /** The Java Entity Client should be preferred if executing within the GMS service. */
 @Configuration
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 @ConditionalOnProperty(name = "entityClient.impl", havingValue = "restli")
 public class RestliEntityClientFactory {
 

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/entityregistry/ConfigEntityRegistryFactory.java
Patch:
@@ -2,17 +2,14 @@
 
 import com.linkedin.metadata.models.registry.ConfigEntityRegistry;
 import com.linkedin.metadata.models.registry.EntityRegistryException;
-import com.linkedin.metadata.spring.YamlPropertySourceFactory;
 import java.io.IOException;
 import javax.annotation.Nonnull;
 import org.springframework.beans.factory.annotation.Value;
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
-import org.springframework.context.annotation.PropertySource;
 import org.springframework.core.io.Resource;
 
 @Configuration
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 public class ConfigEntityRegistryFactory {
 
   @Value("${configEntityRegistry.path}")

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/ermodelrelation/ERModelRelationshipServiceFactory.java
Patch:
@@ -2,16 +2,13 @@
 
 import com.linkedin.entity.client.SystemEntityClient;
 import com.linkedin.metadata.service.ERModelRelationshipService;
-import com.linkedin.metadata.spring.YamlPropertySourceFactory;
 import javax.annotation.Nonnull;
 import org.springframework.beans.factory.annotation.Qualifier;
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
-import org.springframework.context.annotation.PropertySource;
 import org.springframework.context.annotation.Scope;
 
 @Configuration
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 public class ERModelRelationshipServiceFactory {
   @Bean(name = "erModelRelationshipService")
   @Scope("singleton")

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/form/FormServiceFactory.java
Patch:
@@ -2,15 +2,12 @@
 
 import com.linkedin.entity.client.SystemEntityClient;
 import com.linkedin.metadata.service.FormService;
-import com.linkedin.metadata.spring.YamlPropertySourceFactory;
 import javax.annotation.Nonnull;
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
-import org.springframework.context.annotation.PropertySource;
 import org.springframework.context.annotation.Scope;
 
 @Configuration
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 public class FormServiceFactory {
   @Bean(name = "formService")
   @Scope("singleton")

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/incident/IncidentServiceFactory.java
Patch:
@@ -3,16 +3,13 @@
 import com.linkedin.entity.client.SystemEntityClient;
 import com.linkedin.gms.factory.auth.SystemAuthenticationFactory;
 import com.linkedin.metadata.service.IncidentService;
-import com.linkedin.metadata.spring.YamlPropertySourceFactory;
 import javax.annotation.Nonnull;
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
 import org.springframework.context.annotation.Import;
-import org.springframework.context.annotation.PropertySource;
 import org.springframework.context.annotation.Scope;
 
 @Configuration
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 @Import({SystemAuthenticationFactory.class})
 public class IncidentServiceFactory {
   @Bean(name = "incidentService")

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/ingestion/IngestionSchedulerFactory.java
Patch:
@@ -4,19 +4,16 @@
 import com.linkedin.entity.client.SystemEntityClient;
 import com.linkedin.gms.factory.auth.SystemAuthenticationFactory;
 import com.linkedin.gms.factory.config.ConfigurationProvider;
-import com.linkedin.metadata.spring.YamlPropertySourceFactory;
 import io.datahubproject.metadata.context.OperationContext;
 import javax.annotation.Nonnull;
 import org.springframework.beans.factory.annotation.Autowired;
 import org.springframework.beans.factory.annotation.Qualifier;
 import org.springframework.beans.factory.annotation.Value;
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Import;
-import org.springframework.context.annotation.PropertySource;
 import org.springframework.context.annotation.Scope;
 
 @Import({SystemAuthenticationFactory.class})
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 public class IngestionSchedulerFactory {
 
   @Autowired

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/kafka/SimpleKafkaConsumerFactory.java
Patch:
@@ -11,16 +11,16 @@
 import org.apache.kafka.common.serialization.StringDeserializer;
 import org.springframework.beans.factory.annotation.Qualifier;
 import org.springframework.boot.autoconfigure.kafka.KafkaProperties;
-import org.springframework.boot.context.properties.EnableConfigurationProperties;
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
+import org.springframework.context.annotation.DependsOn;
 import org.springframework.kafka.config.ConcurrentKafkaListenerContainerFactory;
 import org.springframework.kafka.config.KafkaListenerContainerFactory;
 import org.springframework.kafka.core.DefaultKafkaConsumerFactory;
 
 @Slf4j
 @Configuration
-@EnableConfigurationProperties({KafkaProperties.class})
+@DependsOn("configurationProvider")
 public class SimpleKafkaConsumerFactory {
 
   @Bean(name = "simpleKafkaConsumer")

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/kafka/common/TopicConventionFactory.java
Patch:
@@ -1,4 +1,4 @@
-package com.linkedin.gms.factory.common;
+package com.linkedin.gms.factory.kafka.common;
 
 import com.linkedin.mxe.TopicConvention;
 import com.linkedin.mxe.TopicConventionImpl;

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/kafka/schemaregistry/AwsGlueSchemaRegistryFactory.java
Patch:
@@ -5,7 +5,6 @@
 import com.amazonaws.services.schemaregistry.utils.AWSSchemaRegistryConstants;
 import com.amazonaws.services.schemaregistry.utils.AvroRecordType;
 import com.linkedin.gms.factory.config.ConfigurationProvider;
-import com.linkedin.metadata.spring.YamlPropertySourceFactory;
 import java.util.HashMap;
 import java.util.Map;
 import java.util.Optional;
@@ -15,11 +14,9 @@
 import org.springframework.boot.autoconfigure.condition.ConditionalOnProperty;
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
-import org.springframework.context.annotation.PropertySource;
 
 @Slf4j
 @Configuration
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 @ConditionalOnProperty(
     name = "kafka.schemaRegistry.type",
     havingValue = AwsGlueSchemaRegistryFactory.TYPE)

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/kafka/schemaregistry/SchemaRegistryServiceFactory.java
Patch:
@@ -1,6 +1,6 @@
 package com.linkedin.gms.factory.kafka.schemaregistry;
 
-import com.linkedin.gms.factory.common.TopicConventionFactory;
+import com.linkedin.gms.factory.kafka.common.TopicConventionFactory;
 import com.linkedin.metadata.registry.SchemaRegistryService;
 import com.linkedin.metadata.registry.SchemaRegistryServiceImpl;
 import com.linkedin.mxe.TopicConvention;

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/lineage/LineageServiceFactory.java
Patch:
@@ -2,16 +2,13 @@
 
 import com.linkedin.entity.client.SystemEntityClient;
 import com.linkedin.metadata.service.LineageService;
-import com.linkedin.metadata.spring.YamlPropertySourceFactory;
 import javax.annotation.Nonnull;
 import org.springframework.beans.factory.annotation.Qualifier;
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
-import org.springframework.context.annotation.PropertySource;
 import org.springframework.context.annotation.Scope;
 
 @Configuration
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 public class LineageServiceFactory {
 
   @Bean(name = "lineageService")

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/ownership/OwnershipTypeServiceFactory.java
Patch:
@@ -2,15 +2,12 @@
 
 import com.linkedin.entity.client.SystemEntityClient;
 import com.linkedin.metadata.service.OwnershipTypeService;
-import com.linkedin.metadata.spring.YamlPropertySourceFactory;
 import javax.annotation.Nonnull;
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
-import org.springframework.context.annotation.PropertySource;
 import org.springframework.context.annotation.Scope;
 
 @Configuration
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 public class OwnershipTypeServiceFactory {
 
   @Bean(name = "ownerShipTypeService")

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/query/QueryServiceFactory.java
Patch:
@@ -2,15 +2,12 @@
 
 import com.linkedin.entity.client.SystemEntityClient;
 import com.linkedin.metadata.service.QueryService;
-import com.linkedin.metadata.spring.YamlPropertySourceFactory;
 import javax.annotation.Nonnull;
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
-import org.springframework.context.annotation.PropertySource;
 import org.springframework.context.annotation.Scope;
 
 @Configuration
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 public class QueryServiceFactory {
 
   @Bean(name = "queryService")

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/search/BaseElasticSearchComponentsFactory.java
Patch:
@@ -4,7 +4,6 @@
 import com.linkedin.gms.factory.common.RestHighLevelClientFactory;
 import com.linkedin.metadata.search.elasticsearch.indexbuilder.ESIndexBuilder;
 import com.linkedin.metadata.search.elasticsearch.update.ESBulkProcessor;
-import com.linkedin.metadata.spring.YamlPropertySourceFactory;
 import com.linkedin.metadata.utils.elasticsearch.IndexConvention;
 import javax.annotation.Nonnull;
 import org.opensearch.client.RestHighLevelClient;
@@ -14,7 +13,6 @@
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
 import org.springframework.context.annotation.Import;
-import org.springframework.context.annotation.PropertySource;
 
 /** Factory for components required for any services using elasticsearch */
 @Configuration
@@ -24,7 +22,6 @@
   ElasticSearchBulkProcessorFactory.class,
   ElasticSearchIndexBuilderFactory.class
 })
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 public class BaseElasticSearchComponentsFactory {
   @lombok.Value
   public static class BaseElasticSearchComponents {

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/search/CachingEntitySearchServiceFactory.java
Patch:
@@ -2,7 +2,6 @@
 
 import com.linkedin.metadata.search.EntitySearchService;
 import com.linkedin.metadata.search.client.CachingEntitySearchService;
-import com.linkedin.metadata.spring.YamlPropertySourceFactory;
 import javax.annotation.Nonnull;
 import org.springframework.beans.factory.annotation.Autowired;
 import org.springframework.beans.factory.annotation.Qualifier;
@@ -11,10 +10,8 @@
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
 import org.springframework.context.annotation.Primary;
-import org.springframework.context.annotation.PropertySource;
 
 @Configuration
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 public class CachingEntitySearchServiceFactory {
 
   @Autowired

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/search/ElasticSearchBulkProcessorFactory.java
Patch:
@@ -2,7 +2,6 @@
 
 import com.linkedin.gms.factory.common.RestHighLevelClientFactory;
 import com.linkedin.metadata.search.elasticsearch.update.ESBulkProcessor;
-import com.linkedin.metadata.spring.YamlPropertySourceFactory;
 import javax.annotation.Nonnull;
 import lombok.extern.slf4j.Slf4j;
 import org.opensearch.action.support.WriteRequest;
@@ -13,12 +12,10 @@
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
 import org.springframework.context.annotation.Import;
-import org.springframework.context.annotation.PropertySource;
 
 @Slf4j
 @Configuration
 @Import({RestHighLevelClientFactory.class})
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 public class ElasticSearchBulkProcessorFactory {
   @Autowired
   @Qualifier("elasticSearchRestHighLevelClient")

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/search/ElasticSearchIndexBuilderFactory.java
Patch:
@@ -9,7 +9,6 @@
 import com.linkedin.gms.factory.common.RestHighLevelClientFactory;
 import com.linkedin.gms.factory.config.ConfigurationProvider;
 import com.linkedin.metadata.search.elasticsearch.indexbuilder.ESIndexBuilder;
-import com.linkedin.metadata.spring.YamlPropertySourceFactory;
 import com.linkedin.metadata.utils.elasticsearch.IndexConvention;
 import com.linkedin.metadata.version.GitVersion;
 import jakarta.annotation.Nonnull;
@@ -25,11 +24,9 @@
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
 import org.springframework.context.annotation.Import;
-import org.springframework.context.annotation.PropertySource;
 
 @Configuration
 @Import({RestHighLevelClientFactory.class, IndexConventionFactory.class, GitVersionFactory.class})
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 public class ElasticSearchIndexBuilderFactory {
 
   @Autowired

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/search/ElasticSearchServiceFactory.java
Patch:
@@ -16,7 +16,6 @@
 import com.linkedin.metadata.search.elasticsearch.query.ESBrowseDAO;
 import com.linkedin.metadata.search.elasticsearch.query.ESSearchDAO;
 import com.linkedin.metadata.search.elasticsearch.update.ESWriteDAO;
-import com.linkedin.metadata.spring.YamlPropertySourceFactory;
 import java.io.IOException;
 import javax.annotation.Nonnull;
 import lombok.extern.slf4j.Slf4j;
@@ -25,11 +24,9 @@
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
 import org.springframework.context.annotation.Import;
-import org.springframework.context.annotation.PropertySource;
 
 @Slf4j
 @Configuration
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 @Import({EntityRegistryFactory.class, SettingsBuilderFactory.class})
 public class ElasticSearchServiceFactory {
   private static final ObjectMapper YAML_MAPPER = new YAMLMapper();

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/search/EntityIndexBuildersFactory.java
Patch:
@@ -3,15 +3,12 @@
 import com.linkedin.metadata.models.registry.EntityRegistry;
 import com.linkedin.metadata.search.elasticsearch.indexbuilder.EntityIndexBuilders;
 import com.linkedin.metadata.search.elasticsearch.indexbuilder.SettingsBuilder;
-import com.linkedin.metadata.spring.YamlPropertySourceFactory;
 import org.springframework.beans.factory.annotation.Autowired;
 import org.springframework.beans.factory.annotation.Qualifier;
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
-import org.springframework.context.annotation.PropertySource;
 
 @Configuration
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 public class EntityIndexBuildersFactory {
 
   @Autowired

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/search/LineageSearchServiceFactory.java
Patch:
@@ -5,18 +5,15 @@
 import com.linkedin.metadata.graph.GraphService;
 import com.linkedin.metadata.search.LineageSearchService;
 import com.linkedin.metadata.search.SearchService;
-import com.linkedin.metadata.spring.YamlPropertySourceFactory;
 import javax.annotation.Nonnull;
 import org.springframework.cache.CacheManager;
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
 import org.springframework.context.annotation.Import;
 import org.springframework.context.annotation.Primary;
-import org.springframework.context.annotation.PropertySource;
 
 @Configuration
 @Import({GraphServiceFactory.class})
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 public class LineageSearchServiceFactory {
 
   public static final String LINEAGE_SEARCH_SERVICE_CACHE_NAME = "relationshipSearchService";

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/search/SearchDocumentTransformerFactory.java
Patch:
@@ -1,14 +1,11 @@
 package com.linkedin.gms.factory.search;
 
 import com.linkedin.metadata.search.transformer.SearchDocumentTransformer;
-import com.linkedin.metadata.spring.YamlPropertySourceFactory;
 import org.springframework.beans.factory.annotation.Value;
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
-import org.springframework.context.annotation.PropertySource;
 
 @Configuration
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 public class SearchDocumentTransformerFactory {
   @Value("${elasticsearch.index.maxArrayLength}")
   private int maxArrayLength;

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/search/SearchServiceFactory.java
Patch:
@@ -7,17 +7,14 @@
 import com.linkedin.metadata.search.cache.EntityDocCountCache;
 import com.linkedin.metadata.search.client.CachingEntitySearchService;
 import com.linkedin.metadata.search.ranker.SearchRanker;
-import com.linkedin.metadata.spring.YamlPropertySourceFactory;
 import javax.annotation.Nonnull;
 import org.springframework.beans.factory.annotation.Autowired;
 import org.springframework.beans.factory.annotation.Qualifier;
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
 import org.springframework.context.annotation.Primary;
-import org.springframework.context.annotation.PropertySource;
 
 @Configuration
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 public class SearchServiceFactory {
 
   @Autowired

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/search/SettingsBuilderFactory.java
Patch:
@@ -3,18 +3,15 @@
 import com.linkedin.gms.factory.entityregistry.EntityRegistryFactory;
 import com.linkedin.metadata.models.registry.EntityRegistry;
 import com.linkedin.metadata.search.elasticsearch.indexbuilder.SettingsBuilder;
-import com.linkedin.metadata.spring.YamlPropertySourceFactory;
 import org.springframework.beans.factory.annotation.Autowired;
 import org.springframework.beans.factory.annotation.Qualifier;
 import org.springframework.beans.factory.annotation.Value;
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
 import org.springframework.context.annotation.Import;
-import org.springframework.context.annotation.PropertySource;
 
 @Configuration
 @Import(EntityRegistryFactory.class)
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 public class SettingsBuilderFactory {
   @Autowired
   @Qualifier("entityRegistry")

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/search/views/ViewServiceFactory.java
Patch:
@@ -2,15 +2,12 @@
 
 import com.linkedin.entity.client.SystemEntityClient;
 import com.linkedin.metadata.service.ViewService;
-import com.linkedin.metadata.spring.YamlPropertySourceFactory;
 import javax.annotation.Nonnull;
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
-import org.springframework.context.annotation.PropertySource;
 import org.springframework.context.annotation.Scope;
 
 @Configuration
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 public class ViewServiceFactory {
 
   @Bean(name = "viewService")

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/settings/SettingsServiceFactory.java
Patch:
@@ -2,15 +2,12 @@
 
 import com.linkedin.entity.client.SystemEntityClient;
 import com.linkedin.metadata.service.SettingsService;
-import com.linkedin.metadata.spring.YamlPropertySourceFactory;
 import javax.annotation.Nonnull;
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
-import org.springframework.context.annotation.PropertySource;
 import org.springframework.context.annotation.Scope;
 
 @Configuration
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 public class SettingsServiceFactory {
   @Bean(name = "settingsService")
   @Scope("singleton")

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/telemetry/MixpanelApiFactory.java
Patch:
@@ -1,15 +1,12 @@
 package com.linkedin.gms.factory.telemetry;
 
-import com.linkedin.metadata.spring.YamlPropertySourceFactory;
 import com.mixpanel.mixpanelapi.MixpanelAPI;
 import org.springframework.boot.autoconfigure.condition.ConditionalOnProperty;
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
-import org.springframework.context.annotation.PropertySource;
 import org.springframework.context.annotation.Scope;
 
 @Configuration
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 public class MixpanelApiFactory {
   private static final String EVENTS_ENDPOINT = "https://track.datahubproject.io/mp/track";
   private static final String PEOPLE_ENDPOINT = "https://track.datahubproject.io/mp/engage";

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/telemetry/MixpanelMessageBuilderFactory.java
Patch:
@@ -1,15 +1,12 @@
 package com.linkedin.gms.factory.telemetry;
 
-import com.linkedin.metadata.spring.YamlPropertySourceFactory;
 import com.mixpanel.mixpanelapi.MessageBuilder;
 import org.springframework.boot.autoconfigure.condition.ConditionalOnProperty;
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
-import org.springframework.context.annotation.PropertySource;
 import org.springframework.context.annotation.Scope;
 
 @Configuration
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 public class MixpanelMessageBuilderFactory {
   private static final String MIXPANEL_TOKEN = "5ee83d940754d63cacbf7d34daa6f44a";
 

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/telemetry/TrackingServiceFactory.java
Patch:
@@ -2,7 +2,6 @@
 
 import com.datahub.telemetry.TrackingService;
 import com.linkedin.metadata.entity.EntityService;
-import com.linkedin.metadata.spring.YamlPropertySourceFactory;
 import com.linkedin.metadata.version.GitVersion;
 import com.mixpanel.mixpanelapi.MessageBuilder;
 import com.mixpanel.mixpanelapi.MixpanelAPI;
@@ -12,11 +11,9 @@
 import org.springframework.boot.autoconfigure.condition.ConditionalOnProperty;
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
-import org.springframework.context.annotation.PropertySource;
 import org.springframework.context.annotation.Scope;
 
 @Configuration
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 public class TrackingServiceFactory {
   @Autowired(required = false)
   @Qualifier("mixpanelApi")

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/timeline/TimelineServiceFactory.java
Patch:
@@ -2,18 +2,15 @@
 
 import com.linkedin.metadata.entity.AspectDao;
 import com.linkedin.metadata.models.registry.EntityRegistry;
-import com.linkedin.metadata.spring.YamlPropertySourceFactory;
 import com.linkedin.metadata.timeline.TimelineService;
 import com.linkedin.metadata.timeline.TimelineServiceImpl;
 import javax.annotation.Nonnull;
 import org.springframework.beans.factory.annotation.Qualifier;
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
 import org.springframework.context.annotation.DependsOn;
-import org.springframework.context.annotation.PropertySource;
 
 @Configuration
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 public class TimelineServiceFactory {
 
   @Bean(name = "timelineService")

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/timeseries/ElasticSearchTimeseriesAspectServiceFactory.java
Patch:
@@ -3,7 +3,6 @@
 import com.linkedin.gms.factory.entityregistry.EntityRegistryFactory;
 import com.linkedin.gms.factory.search.BaseElasticSearchComponentsFactory;
 import com.linkedin.metadata.models.registry.EntityRegistry;
-import com.linkedin.metadata.spring.YamlPropertySourceFactory;
 import com.linkedin.metadata.timeseries.elastic.ElasticSearchTimeseriesAspectService;
 import com.linkedin.metadata.timeseries.elastic.indexbuilder.TimeseriesAspectIndexBuilders;
 import javax.annotation.Nonnull;
@@ -12,10 +11,8 @@
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
 import org.springframework.context.annotation.Import;
-import org.springframework.context.annotation.PropertySource;
 
 @Configuration
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 @Import({BaseElasticSearchComponentsFactory.class, EntityRegistryFactory.class})
 public class ElasticSearchTimeseriesAspectServiceFactory {
   @Autowired

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/usage/UsageClientFactory.java
Patch:
@@ -2,7 +2,6 @@
 
 import com.linkedin.gms.factory.config.ConfigurationProvider;
 import com.linkedin.metadata.restli.DefaultRestliClientFactory;
-import com.linkedin.metadata.spring.YamlPropertySourceFactory;
 import com.linkedin.parseq.retry.backoff.ExponentialBackoff;
 import com.linkedin.r2.transport.http.client.HttpClientFactory;
 import com.linkedin.restli.client.Client;
@@ -14,10 +13,8 @@
 import org.springframework.beans.factory.annotation.Value;
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
-import org.springframework.context.annotation.PropertySource;
 
 @Configuration
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 public class UsageClientFactory {
 
   @Value("${DATAHUB_GMS_HOST:localhost}")

File: metadata-service/factories/src/main/java/com/linkedin/metadata/boot/factories/IngestRetentionPoliciesStepFactory.java
Patch:
@@ -4,20 +4,17 @@
 import com.linkedin.metadata.boot.steps.IngestRetentionPoliciesStep;
 import com.linkedin.metadata.entity.EntityService;
 import com.linkedin.metadata.entity.RetentionService;
-import com.linkedin.metadata.spring.YamlPropertySourceFactory;
 import javax.annotation.Nonnull;
 import org.springframework.beans.factory.annotation.Autowired;
 import org.springframework.beans.factory.annotation.Qualifier;
 import org.springframework.beans.factory.annotation.Value;
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
 import org.springframework.context.annotation.Import;
-import org.springframework.context.annotation.PropertySource;
 import org.springframework.context.annotation.Scope;
 
 @Configuration
 @Import({RetentionServiceFactory.class})
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 public class IngestRetentionPoliciesStepFactory {
 
   @Autowired

File: metadata-service/factories/src/main/java/com/linkedin/metadata/boot/kafka/MockSystemUpdateDeserializer.java
Patch:
@@ -3,6 +3,7 @@
 import static com.linkedin.gms.factory.kafka.schemaregistry.SystemUpdateSchemaRegistryFactory.DUHE_SCHEMA_REGISTRY_TOPIC_KEY;
 import static com.linkedin.gms.factory.kafka.schemaregistry.SystemUpdateSchemaRegistryFactory.SYSTEM_UPDATE_TOPIC_KEY_ID_SUFFIX;
 import static com.linkedin.metadata.boot.kafka.MockSystemUpdateSerializer.topicToSubjectName;
+import static io.datahubproject.openapi.schema.registry.Constants.FIXED_SCHEMA_VERSION;
 
 import com.linkedin.metadata.EventUtils;
 import io.confluent.kafka.schemaregistry.ParsedSchema;
@@ -42,7 +43,7 @@ private MockSchemaRegistryClient buildMockSchemaRegistryClient() {
       schemaRegistry.register(
           topicToSubjectName(topicName),
           new AvroSchema(EventUtils.ORIGINAL_DUHE_AVRO_SCHEMA),
-          0,
+          FIXED_SCHEMA_VERSION,
           schemaId);
       return schemaRegistry;
     } catch (IOException | RestClientException e) {

File: metadata-service/factories/src/test/java/com/linkedin/gms/factory/search/ElasticSearchBulkProcessorFactoryTest.java
Patch:
@@ -13,7 +13,7 @@
 import org.springframework.test.context.testng.AbstractTestNGSpringContextTests;
 import org.testng.annotations.Test;
 
-@TestPropertySource(locations = "classpath:/application.yml")
+@TestPropertySource(locations = "classpath:/application.yaml")
 @SpringBootTest(classes = {ElasticSearchBulkProcessorFactory.class})
 @EnableConfigurationProperties(ConfigurationProvider.class)
 public class ElasticSearchBulkProcessorFactoryTest extends AbstractTestNGSpringContextTests {

File: metadata-service/factories/src/test/java/com/linkedin/gms/factory/search/ElasticSearchIndexBuilderFactoryDefaultsTest.java
Patch:
@@ -13,7 +13,7 @@
 import org.springframework.test.context.testng.AbstractTestNGSpringContextTests;
 import org.testng.annotations.Test;
 
-@TestPropertySource(locations = "classpath:/application.yml")
+@TestPropertySource(locations = "classpath:/application.yaml")
 @SpringBootTest(classes = {ElasticSearchIndexBuilderFactory.class})
 @EnableConfigurationProperties(ConfigurationProvider.class)
 public class ElasticSearchIndexBuilderFactoryDefaultsTest extends AbstractTestNGSpringContextTests {

File: metadata-service/factories/src/test/java/com/linkedin/gms/factory/search/ElasticSearchIndexBuilderFactoryTest.java
Patch:
@@ -13,7 +13,7 @@
 import org.springframework.test.context.testng.AbstractTestNGSpringContextTests;
 import org.testng.annotations.Test;
 
-@TestPropertySource(locations = "classpath:/application.yml")
+@TestPropertySource(locations = "classpath:/application.yaml")
 @SpringBootTest(classes = {ElasticSearchIndexBuilderFactory.class})
 @EnableConfigurationProperties(ConfigurationProvider.class)
 public class ElasticSearchIndexBuilderFactoryTest extends AbstractTestNGSpringContextTests {

File: metadata-service/factories/src/test/java/com/linkedin/gms/factory/secret/SecretServiceFactoryTest.java
Patch:
@@ -16,7 +16,7 @@
 import org.springframework.test.context.testng.AbstractTestNGSpringContextTests;
 import org.testng.annotations.Test;
 
-@TestPropertySource(locations = "classpath:/application.yml")
+@TestPropertySource(locations = "classpath:/application.yaml")
 @SpringBootTest(classes = {SecretServiceFactory.class})
 @EnableConfigurationProperties(ConfigurationProvider.class)
 public class SecretServiceFactoryTest extends AbstractTestNGSpringContextTests {

File: metadata-service/openapi-entity-servlet/src/test/java/io/datahubproject/openapi/util/OpenApiEntitiesUtilTest.java
Patch:
@@ -5,7 +5,6 @@
 
 import com.linkedin.data.schema.annotation.PathSpecBasedSchemaAnnotationVisitor;
 import com.linkedin.metadata.models.registry.EntityRegistry;
-import com.linkedin.metadata.spring.YamlPropertySourceFactory;
 import io.datahubproject.openapi.config.OpenAPIEntityTestConfiguration;
 import io.datahubproject.openapi.dto.UpsertAspectRequest;
 import io.datahubproject.openapi.generated.ContainerEntityRequestV2;
@@ -14,13 +13,11 @@
 import java.util.List;
 import org.springframework.beans.factory.annotation.Autowired;
 import org.springframework.context.annotation.Import;
-import org.springframework.context.annotation.PropertySource;
 import org.springframework.test.context.testng.AbstractTestNGSpringContextTests;
 import org.testng.annotations.BeforeTest;
 import org.testng.annotations.Test;
 
 @Import({OpenAPIEntityTestConfiguration.class})
-@PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 public class OpenApiEntitiesUtilTest extends AbstractTestNGSpringContextTests {
   @Autowired private EntityRegistry entityRegistry;
 

File: metadata-service/openapi-entity-servlet/src/test/java/io/datahubproject/openapi/v2/delegates/EntityApiDelegateImplTest.java
Patch:
@@ -47,12 +47,14 @@
 import org.springframework.test.web.servlet.MockMvc;
 import org.springframework.test.web.servlet.request.MockMvcRequestBuilders;
 import org.springframework.test.web.servlet.result.MockMvcResultMatchers;
+import org.springframework.web.servlet.config.annotation.EnableWebMvc;
 import org.testng.annotations.BeforeTest;
 import org.testng.annotations.Test;
 
 @SpringBootTest(classes = {SpringWebConfig.class})
 @ComponentScan(basePackages = {"io.datahubproject.openapi.v2.generated.controller"})
 @Import({OpenAPIEntityTestConfiguration.class})
+@EnableWebMvc
 @AutoConfigureMockMvc
 public class EntityApiDelegateImplTest extends AbstractTestNGSpringContextTests {
   @BeforeTest

File: metadata-service/openapi-servlet/src/main/java/io/datahubproject/openapi/config/SpringWebConfig.java
Patch:
@@ -16,10 +16,8 @@
 import org.springframework.http.converter.HttpMessageConverter;
 import org.springframework.http.converter.StringHttpMessageConverter;
 import org.springframework.http.converter.json.MappingJackson2HttpMessageConverter;
-import org.springframework.web.servlet.config.annotation.EnableWebMvc;
 import org.springframework.web.servlet.config.annotation.WebMvcConfigurer;
 
-@EnableWebMvc
 @OpenAPIDefinition(
     info = @Info(title = "DataHub OpenAPI", version = "2.0.0"),
     servers = {@Server(url = "/openapi/", description = "Default Server URL")})

File: metadata-service/schema-registry-servlet/src/main/java/io/datahubproject/openapi/schema/registry/config/SpringWebSchemaRegistryConfig.java
Patch:
@@ -10,10 +10,8 @@
 import org.springframework.http.converter.HttpMessageConverter;
 import org.springframework.http.converter.StringHttpMessageConverter;
 import org.springframework.http.converter.json.MappingJackson2HttpMessageConverter;
-import org.springframework.web.servlet.config.annotation.EnableWebMvc;
 import org.springframework.web.servlet.config.annotation.WebMvcConfigurer;
 
-@EnableWebMvc
 @OpenAPIDefinition(
     info = @Info(title = "DataHub OpenAPI", version = "1.0.0"),
     servers = {@Server(url = "/schema-registry/", description = "Schema Registry Server URL")})

File: ingestion-scheduler/src/main/java/com/datahub/metadata/ingestion/IngestionScheduler.java
Patch:
@@ -413,6 +413,9 @@ public void run() {
         if (ingestionSourceInfo.getConfig().hasDebugMode()) {
           debugMode = ingestionSourceInfo.getConfig().isDebugMode() ? "true" : "false";
         }
+        if (ingestionSourceInfo.getConfig().hasExtraArgs()) {
+          arguments.putAll(ingestionSourceInfo.getConfig().getExtraArgs());
+        }
         arguments.put(DEBUG_MODE_ARG_NAME, debugMode);
         input.setArgs(new StringMap(arguments));
 

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/GmsGraphQLEngine.java
Patch:
@@ -975,6 +975,7 @@ private void configureQueryResolvers(final RuntimeWiring.Builder builder) {
                 .dataFetcher(
                     "getAccessTokenMetadata",
                     new GetAccessTokenMetadataResolver(statefulTokenService, this.entityClient))
+                .dataFetcher("debugAccess", new DebugAccessResolver(this.entityClient, graphClient))
                 .dataFetcher("container", getResolver(containerType))
                 .dataFetcher("listDomains", new ListDomainsResolver(this.entityClient))
                 .dataFetcher("listSecrets", new ListSecretsResolver(this.entityClient))

File: metadata-io/src/main/java/com/linkedin/metadata/recommendation/candidatesource/RecentlyEditedSource.java
Patch:
@@ -99,7 +99,8 @@ public boolean isEligible(
   @WithSpan
   public List<RecommendationContent> getRecommendations(
       @Nonnull OperationContext opContext, @Nonnull RecommendationRequestContext requestContext) {
-    SearchRequest searchRequest = buildSearchRequest(opContext.getActorContext().getActorUrn());
+    SearchRequest searchRequest =
+        buildSearchRequest(opContext.getSessionActorContext().getActorUrn());
     try (Timer.Context ignored = MetricUtils.timer(this.getClass(), "getRecentlyEdited").time()) {
       final SearchResponse searchResponse =
           _searchClient.search(searchRequest, RequestOptions.DEFAULT);

File: metadata-io/src/main/java/com/linkedin/metadata/recommendation/candidatesource/RecentlyViewedSource.java
Patch:
@@ -99,7 +99,8 @@ public boolean isEligible(
   @WithSpan
   public List<RecommendationContent> getRecommendations(
       @Nonnull OperationContext opContext, @Nonnull RecommendationRequestContext requestContext) {
-    SearchRequest searchRequest = buildSearchRequest(opContext.getActorContext().getActorUrn());
+    SearchRequest searchRequest =
+        buildSearchRequest(opContext.getSessionActorContext().getActorUrn());
     try (Timer.Context ignored = MetricUtils.timer(this.getClass(), "getRecentlyViewed").time()) {
       final SearchResponse searchResponse =
           _searchClient.search(searchRequest, RequestOptions.DEFAULT);

File: metadata-service/services/src/main/java/com/linkedin/metadata/recommendation/candidatesource/RecentlySearchedSource.java
Patch:
@@ -79,7 +79,8 @@ public boolean isEligible(
   @Override
   public List<RecommendationContent> getRecommendations(
       @Nonnull OperationContext opContext, @Nonnull RecommendationRequestContext requestContext) {
-    SearchRequest searchRequest = buildSearchRequest(opContext.getActorContext().getActorUrn());
+    SearchRequest searchRequest =
+        buildSearchRequest(opContext.getSessionActorContext().getActorUrn());
     try (Timer.Context ignored = MetricUtils.timer(this.getClass(), "getRecentlySearched").time()) {
       final SearchResponse searchResponse =
           _searchClient.search(searchRequest, RequestOptions.DEFAULT);

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/search/SearchUtils.java
Patch:
@@ -90,7 +90,8 @@ private SearchUtils() {}
           EntityType.CORP_USER,
           EntityType.CORP_GROUP,
           EntityType.NOTEBOOK,
-          EntityType.DATA_PRODUCT);
+          EntityType.DATA_PRODUCT,
+          EntityType.DOMAIN);
 
   /** Entities that are part of browse by default */
   public static final List<EntityType> BROWSE_ENTITY_TYPES =

File: entity-registry/src/main/java/com/linkedin/metadata/aspect/batch/AspectsBatch.java
Patch:
@@ -179,4 +179,6 @@ static <T> Map<String, Map<String, T>> merge(
                 Collectors.mapping(
                     Pair::getValue, Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue))));
   }
+
+  String toAbbreviatedString(int maxWidth);
 }

File: metadata-io/src/main/java/com/linkedin/metadata/entity/EntityServiceImpl.java
Patch:
@@ -631,7 +631,7 @@ public List<UpdateAspectResult> ingestAspects(
       return Collections.emptyList();
     }
 
-    log.info("Ingesting aspects batch to database, items: {}", aspectsBatch.getItems());
+    log.info("Ingesting aspects batch to database: {}", aspectsBatch.toAbbreviatedString(2048));
     Timer.Context ingestToLocalDBTimer =
         MetricUtils.timer(this.getClass(), "ingestAspectsToLocalDB").time();
     List<UpdateAspectResult> ingestResults = ingestAspectsToLocalDB(aspectsBatch, overwrite);

File: metadata-io/src/main/java/com/linkedin/metadata/entity/cassandra/CassandraRetentionService.java
Patch:
@@ -195,7 +195,7 @@ private void applyTimeBasedRetention(
       @Nonnull final Urn urn,
       @Nonnull final String aspectName,
       @Nonnull final TimeBasedRetention retention) {
-    Timestamp threshold = new Timestamp(_clock.millis() - retention.getMaxAgeInSeconds() * 1000);
+    Timestamp threshold = new Timestamp(_clock.millis() - retention.getMaxAgeInSeconds() * 1000L);
     SimpleStatement ss =
         deleteFrom(CassandraAspect.TABLE_NAME)
             .whereColumn(CassandraAspect.URN_COLUMN)

File: metadata-io/src/main/java/com/linkedin/metadata/entity/ebean/EbeanRetentionService.java
Patch:
@@ -156,7 +156,7 @@ private Expression getTimeBasedRetentionQuery(@Nonnull final TimeBasedRetention
     return new SimpleExpression(
         EbeanAspectV2.CREATED_ON_COLUMN,
         Op.LT,
-        new Timestamp(_clock.millis() - retention.getMaxAgeInSeconds() * 1000));
+        new Timestamp(_clock.millis() - retention.getMaxAgeInSeconds() * 1000L));
   }
 
   private void applyRetention(

File: metadata-io/src/main/java/com/linkedin/metadata/entity/EntityServiceImpl.java
Patch:
@@ -930,7 +930,7 @@ public IngestResult ingestProposal(
             AspectsBatchImpl.builder().mcps(List.of(proposal), auditStamp, this).build(), async)
         .stream()
         .findFirst()
-        .get();
+        .orElse(null);
   }
 
   /**

File: metadata-service/services/src/main/java/com/linkedin/metadata/entity/DeleteEntityService.java
Patch:
@@ -349,7 +349,7 @@ private void updateAspect(
     final IngestResult ingestProposalResult =
         _entityService.ingestProposal(proposal, auditStamp, false);
 
-    if (!ingestProposalResult.isSqlCommitted()) {
+    if (ingestProposalResult != null && !ingestProposalResult.isSqlCommitted()) {
       log.error(
           "Failed to ingest aspect with references removed. Before {}, after: {}, please check MCP processor"
               + " logs for more information",

File: metadata-io/src/main/java/com/linkedin/metadata/entity/AspectDao.java
Patch:
@@ -5,7 +5,6 @@
 import com.linkedin.metadata.entity.ebean.EbeanAspectV2;
 import com.linkedin.metadata.entity.restoreindices.RestoreIndicesArgs;
 import com.linkedin.metadata.utils.metrics.MetricUtils;
-import io.ebean.PagedList;
 import io.ebean.Transaction;
 import java.sql.Timestamp;
 import java.util.List;
@@ -106,7 +105,7 @@ ListResult<String> listUrns(
   Integer countAspect(@Nonnull final String aspectName, @Nullable String urnLike);
 
   @Nonnull
-  PagedList<EbeanAspectV2> getPagedAspects(final RestoreIndicesArgs args);
+  Stream<Stream<EbeanAspectV2>> streamAspectBatches(final RestoreIndicesArgs args);
 
   @Nonnull
   Stream<EntityAspect> streamAspects(String entityName, String aspectName);

File: metadata-io/src/main/java/com/linkedin/metadata/entity/cassandra/CassandraAspectDao.java
Patch:
@@ -34,7 +34,6 @@
 import com.linkedin.metadata.query.ExtraInfo;
 import com.linkedin.metadata.query.ExtraInfoArray;
 import com.linkedin.metadata.query.ListResultMetadata;
-import io.ebean.PagedList;
 import io.ebean.Transaction;
 import java.net.URISyntaxException;
 import java.nio.charset.StandardCharsets;
@@ -492,7 +491,7 @@ public Integer countAspect(@Nonnull String aspectName, @Nullable String urnLike)
   }
 
   @Nonnull
-  public PagedList<EbeanAspectV2> getPagedAspects(final RestoreIndicesArgs args) {
+  public Stream<Stream<EbeanAspectV2>> streamAspectBatches(final RestoreIndicesArgs args) {
     // Not implemented
     return null;
   }

File: metadata-jobs/mce-consumer-job/src/test/java/com/linkedin/metadata/kafka/MceConsumerApplicationTest.java
Patch:
@@ -7,6 +7,7 @@
 import com.linkedin.metadata.entity.EntityService;
 import com.linkedin.metadata.entity.restoreindices.RestoreIndicesResult;
 import io.datahubproject.metadata.jobs.common.health.kafka.KafkaHealthIndicator;
+import java.util.stream.Stream;
 import org.springframework.beans.factory.annotation.Autowired;
 import org.springframework.boot.test.context.SpringBootTest;
 import org.springframework.boot.test.web.client.TestRestTemplate;
@@ -30,7 +31,7 @@ public class MceConsumerApplicationTest extends AbstractTestNGSpringContextTests
   public void testRestliServletConfig() {
     RestoreIndicesResult mockResult = new RestoreIndicesResult();
     mockResult.setRowsMigrated(100);
-    when(_mockEntityService.restoreIndices(any(), any())).thenReturn(mockResult);
+    when(_mockEntityService.streamRestoreIndices(any(), any())).thenReturn(Stream.of(mockResult));
 
     String response =
         this.restTemplate.postForObject(

File: metadata-service/services/src/main/java/com/linkedin/metadata/entity/EntityService.java
Patch:
@@ -31,6 +31,7 @@
 import java.util.Set;
 import java.util.concurrent.Future;
 import java.util.function.Consumer;
+import java.util.stream.Stream;
 import javax.annotation.Nonnull;
 import javax.annotation.Nullable;
 
@@ -244,7 +245,7 @@ String batchApplyRetention(
   Integer getCountAspect(@Nonnull String aspectName, @Nullable String urnLike);
 
   // TODO: Extract this to a different service, doesn't need to be here
-  RestoreIndicesResult restoreIndices(
+  Stream<RestoreIndicesResult> streamRestoreIndices(
       @Nonnull RestoreIndicesArgs args, @Nonnull Consumer<String> logger);
 
   // Restore indices from list using key lookups (no scans)

File: metadata-io/src/main/java/com/linkedin/metadata/graph/elastic/ESGraphQueryDAO.java
Patch:
@@ -1,7 +1,7 @@
 package com.linkedin.metadata.graph.elastic;
 
+import static com.linkedin.metadata.graph.Edge.*;
 import static com.linkedin.metadata.graph.elastic.ElasticSearchGraphService.*;
-import static com.linkedin.metadata.graph.elastic.GraphRelationshipMappingsBuilder.*;
 
 import com.codahale.metrics.Timer;
 import com.datahub.util.exception.ESQueryException;

File: metadata-io/src/main/java/com/linkedin/metadata/graph/elastic/ElasticSearchGraphService.java
Patch:
@@ -1,6 +1,6 @@
 package com.linkedin.metadata.graph.elastic;
 
-import static com.linkedin.metadata.graph.elastic.GraphRelationshipMappingsBuilder.*;
+import static com.linkedin.metadata.graph.Edge.*;
 
 import com.fasterxml.jackson.databind.node.JsonNodeFactory;
 import com.fasterxml.jackson.databind.node.ObjectNode;

File: metadata-jobs/mae-consumer/src/main/java/com/linkedin/metadata/kafka/hydrator/EntityHydrator.java
Patch:
@@ -32,7 +32,7 @@ public class EntityHydrator {
   private final DataJobHydrator _dataJobHydrator = new DataJobHydrator();
   private final DatasetHydrator _datasetHydrator = new DatasetHydrator();
 
-  public Optional<ObjectNode> getHydratedEntity(String entityTypeName, String urn) {
+  public Optional<ObjectNode> getHydratedEntity(String urn) {
     final ObjectNode document = JsonNodeFactory.instance.objectNode();
     // Hydrate fields from urn
     Urn urnObj;

File: metadata-service/restli-client-api/src/main/java/com/linkedin/entity/client/SystemEntityClient.java
Patch:
@@ -41,7 +41,7 @@ default EntityClientCache buildEntityClientCache(
       Class<?> metricClazz, EntityClientCacheConfig cacheConfig) {
     return EntityClientCache.builder()
         .config(cacheConfig)
-        .loadFunction(
+        .build(
             (EntityClientCache.CollectionKey collectionKey) -> {
               try {
                 String entityName =
@@ -61,8 +61,8 @@ default EntityClientCache buildEntityClientCache(
               } catch (RemoteInvocationException | URISyntaxException e) {
                 throw new RuntimeException(e);
               }
-            })
-        .build(metricClazz);
+            },
+            metricClazz);
   }
 
   /**

File: metadata-service/restli-servlet-impl/src/main/java/com/linkedin/metadata/resources/entity/AspectResource.java
Patch:
@@ -253,11 +253,11 @@ public Task<String> ingestProposal(
         Set<IngestResult> results =
                 _entityService.ingestProposal(batch, asyncBool);
 
-            IngestResult one = results.stream().findFirst().get();
+            java.util.Optional<IngestResult> one = results.stream().findFirst();
 
             // Update runIds, only works for existing documents, so ES document must exist
-            Urn resultUrn = one.getUrn();
-            if (one.isProcessedMCL() || one.isUpdate()) {
+            Urn resultUrn = one.map(IngestResult::getUrn).orElse(metadataChangeProposal.getEntityUrn());
+            if (one.map(result -> result.isProcessedMCL() || result.isUpdate()).orElse(false)) {
               tryIndexRunId(
                   resultUrn, metadataChangeProposal.getSystemMetadata(), entitySearchService);
             }

File: metadata-io/src/main/java/com/linkedin/metadata/search/utils/ESUtils.java
Patch:
@@ -188,7 +188,7 @@ public static BoolQueryBuilder buildConjunctiveFilterQuery(
         .forEach(
             criterion -> {
               if (Set.of(Condition.EXISTS, Condition.IS_NULL).contains(criterion.getCondition())
-                  || !criterion.getValue().trim().isEmpty()
+                  || (criterion.hasValue() && !criterion.getValue().trim().isEmpty())
                   || criterion.hasValues()) {
                 if (!criterion.isNegated()) {
                   // `filter` instead of `must` (enables caching and bypasses scoring)
@@ -646,6 +646,7 @@ private static RangeQueryBuilder buildRangeQueryFromCriterion(
    * <p>For all new code, we should be using the new 'values' field for performing multi-match. This
    * is simply retained for backwards compatibility of the search API.
    */
+  @Deprecated
   private static QueryBuilder buildEqualsFromCriterionWithValue(
       @Nonnull final String fieldName,
       @Nonnull final Criterion criterion,

File: metadata-service/restli-servlet-impl/src/main/java/com/linkedin/metadata/resources/usage/UsageStats.java
Patch:
@@ -386,22 +386,22 @@ public Task<UsageQueryResult> query(
           Filter filter = new Filter();
           ArrayList<Criterion> criteria = new ArrayList<>();
           Criterion hasUrnCriterion =
-              new Criterion().setField("urn").setCondition(Condition.EQUAL).setValue(resource);
+              new Criterion().setField("urn").setCondition(Condition.EQUAL).setValues(new StringArray(resource));
           criteria.add(hasUrnCriterion);
           if (startTime != null) {
             Criterion startTimeCriterion =
                 new Criterion()
                     .setField(ES_FIELD_TIMESTAMP)
                     .setCondition(Condition.GREATER_THAN_OR_EQUAL_TO)
-                    .setValue(startTime.toString());
+                    .setValues(new StringArray(startTime.toString()));
             criteria.add(startTimeCriterion);
           }
           if (endTime != null) {
             Criterion endTimeCriterion =
                 new Criterion()
                     .setField(ES_FIELD_TIMESTAMP)
                     .setCondition(Condition.LESS_THAN_OR_EQUAL_TO)
-                    .setValue(endTime.toString());
+                    .setValues(new StringArray(endTime.toString()));
             criteria.add(endTimeCriterion);
           }
 

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/GmsGraphQLEngine.java
Patch:
@@ -1726,7 +1726,8 @@ private void configureCorpUserResolvers(final RuntimeWiring.Builder builder) {
                 .dataFetcher("relationships", new EntityRelationshipsResultResolver(graphClient))
                 .dataFetcher("privileges", new EntityPrivilegesResolver(entityClient))
                 .dataFetcher(
-                    "aspects", new WeaklyTypedAspectsResolver(entityClient, entityRegistry)));
+                    "aspects", new WeaklyTypedAspectsResolver(entityClient, entityRegistry))
+                .dataFetcher("exists", new EntityExistsResolver(entityService)));
     builder.type(
         "CorpUserInfo",
         typeWiring ->

File: metadata-service/openapi-servlet/src/main/java/io/datahubproject/openapi/config/SpringWebConfig.java
Patch:
@@ -31,7 +31,8 @@ public class SpringWebConfig implements WebMvcConfigurer {
   private static final Set<String> SCHEMA_REGISTRY_PACKAGES =
       Set.of("io.datahubproject.openapi.schema.registry");
 
-  private static final Set<String> OPENLINEAGE_PACKAGES = Set.of("io.datahubproject.openlineage");
+  private static final Set<String> OPENLINEAGE_PACKAGES =
+      Set.of("io.datahubproject.openapi.openlineage");
 
   public static final Set<String> NONDEFAULT_OPENAPI_PACKAGES;
 

File: metadata-service/auth-impl/src/main/java/com/datahub/authorization/DataHubAuthorizer.java
Patch:
@@ -278,7 +278,7 @@ public void run() {
         while (total == null || scrollId != null) {
           try {
             final PolicyFetcher.PolicyFetchResult policyFetchResult =
-                _policyFetcher.fetchPolicies(count, scrollId, _systemAuthentication);
+                _policyFetcher.fetchPolicies(count, scrollId, null, _systemAuthentication);
 
             addPoliciesToCache(newCache, policyFetchResult.getPolicies());
 

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/chart/ChartType.java
Patch:
@@ -82,7 +82,8 @@ public class ChartType
           DATA_PRODUCTS_ASPECT_NAME,
           BROWSE_PATHS_V2_ASPECT_NAME,
           SUB_TYPES_ASPECT_NAME,
-          STRUCTURED_PROPERTIES_ASPECT_NAME);
+          STRUCTURED_PROPERTIES_ASPECT_NAME,
+          FORMS_ASPECT_NAME);
   private static final Set<String> FACET_FIELDS =
       ImmutableSet.of("access", "queryType", "tool", "type");
 

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/container/ContainerType.java
Patch:
@@ -52,7 +52,8 @@ public class ContainerType
           Constants.DOMAINS_ASPECT_NAME,
           Constants.DEPRECATION_ASPECT_NAME,
           Constants.DATA_PRODUCTS_ASPECT_NAME,
-          Constants.STRUCTURED_PROPERTIES_ASPECT_NAME);
+          Constants.STRUCTURED_PROPERTIES_ASPECT_NAME,
+          Constants.FORMS_ASPECT_NAME);
 
   private static final Set<String> FACET_FIELDS = ImmutableSet.of("origin", "platform");
   private static final String ENTITY_NAME = "container";

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/dashboard/DashboardType.java
Patch:
@@ -82,7 +82,8 @@ public class DashboardType
           EMBED_ASPECT_NAME,
           DATA_PRODUCTS_ASPECT_NAME,
           BROWSE_PATHS_V2_ASPECT_NAME,
-          STRUCTURED_PROPERTIES_ASPECT_NAME);
+          STRUCTURED_PROPERTIES_ASPECT_NAME,
+          FORMS_ASPECT_NAME);
   private static final Set<String> FACET_FIELDS = ImmutableSet.of("access", "tool");
 
   private final EntityClient _entityClient;

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/dataflow/DataFlowType.java
Patch:
@@ -77,7 +77,8 @@ public class DataFlowType
           DATA_PLATFORM_INSTANCE_ASPECT_NAME,
           DATA_PRODUCTS_ASPECT_NAME,
           BROWSE_PATHS_V2_ASPECT_NAME,
-          STRUCTURED_PROPERTIES_ASPECT_NAME);
+          STRUCTURED_PROPERTIES_ASPECT_NAME,
+          FORMS_ASPECT_NAME);
   private static final Set<String> FACET_FIELDS = ImmutableSet.of("orchestrator", "cluster");
   private final EntityClient _entityClient;
 

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/datajob/DataJobType.java
Patch:
@@ -79,7 +79,8 @@ public class DataJobType
           DATA_PRODUCTS_ASPECT_NAME,
           BROWSE_PATHS_V2_ASPECT_NAME,
           SUB_TYPES_ASPECT_NAME,
-          STRUCTURED_PROPERTIES_ASPECT_NAME);
+          STRUCTURED_PROPERTIES_ASPECT_NAME,
+          FORMS_ASPECT_NAME);
   private static final Set<String> FACET_FIELDS = ImmutableSet.of("flow");
   private final EntityClient _entityClient;
 

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/dataproduct/DataProductType.java
Patch:
@@ -3,6 +3,7 @@
 import static com.linkedin.metadata.Constants.DATA_PRODUCT_ENTITY_NAME;
 import static com.linkedin.metadata.Constants.DATA_PRODUCT_PROPERTIES_ASPECT_NAME;
 import static com.linkedin.metadata.Constants.DOMAINS_ASPECT_NAME;
+import static com.linkedin.metadata.Constants.FORMS_ASPECT_NAME;
 import static com.linkedin.metadata.Constants.GLOBAL_TAGS_ASPECT_NAME;
 import static com.linkedin.metadata.Constants.GLOSSARY_TERMS_ASPECT_NAME;
 import static com.linkedin.metadata.Constants.INSTITUTIONAL_MEMORY_ASPECT_NAME;
@@ -51,7 +52,8 @@ public class DataProductType
           GLOSSARY_TERMS_ASPECT_NAME,
           DOMAINS_ASPECT_NAME,
           INSTITUTIONAL_MEMORY_ASPECT_NAME,
-          STRUCTURED_PROPERTIES_ASPECT_NAME);
+          STRUCTURED_PROPERTIES_ASPECT_NAME,
+          FORMS_ASPECT_NAME);
   private final EntityClient _entityClient;
 
   @Override

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/domain/DomainType.java
Patch:
@@ -39,7 +39,8 @@ public class DomainType
           Constants.DOMAIN_PROPERTIES_ASPECT_NAME,
           Constants.OWNERSHIP_ASPECT_NAME,
           Constants.INSTITUTIONAL_MEMORY_ASPECT_NAME,
-          Constants.STRUCTURED_PROPERTIES_ASPECT_NAME);
+          Constants.STRUCTURED_PROPERTIES_ASPECT_NAME,
+          Constants.FORMS_ASPECT_NAME);
   private final EntityClient _entityClient;
 
   public DomainType(final EntityClient entityClient) {

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/glossary/GlossaryNodeType.java
Patch:
@@ -1,5 +1,6 @@
 package com.linkedin.datahub.graphql.types.glossary;
 
+import static com.linkedin.metadata.Constants.FORMS_ASPECT_NAME;
 import static com.linkedin.metadata.Constants.GLOSSARY_NODE_ENTITY_NAME;
 import static com.linkedin.metadata.Constants.GLOSSARY_NODE_INFO_ASPECT_NAME;
 import static com.linkedin.metadata.Constants.GLOSSARY_NODE_KEY_ASPECT_NAME;
@@ -33,7 +34,8 @@ public class GlossaryNodeType
           GLOSSARY_NODE_KEY_ASPECT_NAME,
           GLOSSARY_NODE_INFO_ASPECT_NAME,
           OWNERSHIP_ASPECT_NAME,
-          STRUCTURED_PROPERTIES_ASPECT_NAME);
+          STRUCTURED_PROPERTIES_ASPECT_NAME,
+          FORMS_ASPECT_NAME);
 
   private final EntityClient _entityClient;
 

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/glossary/GlossaryTermType.java
Patch:
@@ -59,7 +59,8 @@ public class GlossaryTermType
           BROWSE_PATHS_ASPECT_NAME,
           DOMAINS_ASPECT_NAME,
           DEPRECATION_ASPECT_NAME,
-          STRUCTURED_PROPERTIES_ASPECT_NAME);
+          STRUCTURED_PROPERTIES_ASPECT_NAME,
+          FORMS_ASPECT_NAME);
 
   private final EntityClient _entityClient;
 

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/chart/ChartType.java
Patch:
@@ -81,7 +81,8 @@ public class ChartType
           EMBED_ASPECT_NAME,
           DATA_PRODUCTS_ASPECT_NAME,
           BROWSE_PATHS_V2_ASPECT_NAME,
-          SUB_TYPES_ASPECT_NAME);
+          SUB_TYPES_ASPECT_NAME,
+          STRUCTURED_PROPERTIES_ASPECT_NAME);
   private static final Set<String> FACET_FIELDS =
       ImmutableSet.of("access", "queryType", "tool", "type");
 

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/container/ContainerType.java
Patch:
@@ -51,7 +51,8 @@ public class ContainerType
           Constants.CONTAINER_ASPECT_NAME,
           Constants.DOMAINS_ASPECT_NAME,
           Constants.DEPRECATION_ASPECT_NAME,
-          Constants.DATA_PRODUCTS_ASPECT_NAME);
+          Constants.DATA_PRODUCTS_ASPECT_NAME,
+          Constants.STRUCTURED_PROPERTIES_ASPECT_NAME);
 
   private static final Set<String> FACET_FIELDS = ImmutableSet.of("origin", "platform");
   private static final String ENTITY_NAME = "container";

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/dashboard/DashboardType.java
Patch:
@@ -81,7 +81,8 @@ public class DashboardType
           SUB_TYPES_ASPECT_NAME,
           EMBED_ASPECT_NAME,
           DATA_PRODUCTS_ASPECT_NAME,
-          BROWSE_PATHS_V2_ASPECT_NAME);
+          BROWSE_PATHS_V2_ASPECT_NAME,
+          STRUCTURED_PROPERTIES_ASPECT_NAME);
   private static final Set<String> FACET_FIELDS = ImmutableSet.of("access", "tool");
 
   private final EntityClient _entityClient;

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/dataflow/DataFlowType.java
Patch:
@@ -76,7 +76,8 @@ public class DataFlowType
           DEPRECATION_ASPECT_NAME,
           DATA_PLATFORM_INSTANCE_ASPECT_NAME,
           DATA_PRODUCTS_ASPECT_NAME,
-          BROWSE_PATHS_V2_ASPECT_NAME);
+          BROWSE_PATHS_V2_ASPECT_NAME,
+          STRUCTURED_PROPERTIES_ASPECT_NAME);
   private static final Set<String> FACET_FIELDS = ImmutableSet.of("orchestrator", "cluster");
   private final EntityClient _entityClient;
 

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/datajob/DataJobType.java
Patch:
@@ -78,7 +78,8 @@ public class DataJobType
           DATA_PLATFORM_INSTANCE_ASPECT_NAME,
           DATA_PRODUCTS_ASPECT_NAME,
           BROWSE_PATHS_V2_ASPECT_NAME,
-          SUB_TYPES_ASPECT_NAME);
+          SUB_TYPES_ASPECT_NAME,
+          STRUCTURED_PROPERTIES_ASPECT_NAME);
   private static final Set<String> FACET_FIELDS = ImmutableSet.of("flow");
   private final EntityClient _entityClient;
 

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/dataproduct/DataProductType.java
Patch:
@@ -7,6 +7,7 @@
 import static com.linkedin.metadata.Constants.GLOSSARY_TERMS_ASPECT_NAME;
 import static com.linkedin.metadata.Constants.INSTITUTIONAL_MEMORY_ASPECT_NAME;
 import static com.linkedin.metadata.Constants.OWNERSHIP_ASPECT_NAME;
+import static com.linkedin.metadata.Constants.STRUCTURED_PROPERTIES_ASPECT_NAME;
 
 import com.google.common.collect.ImmutableSet;
 import com.linkedin.common.urn.Urn;
@@ -49,7 +50,8 @@ public class DataProductType
           GLOBAL_TAGS_ASPECT_NAME,
           GLOSSARY_TERMS_ASPECT_NAME,
           DOMAINS_ASPECT_NAME,
-          INSTITUTIONAL_MEMORY_ASPECT_NAME);
+          INSTITUTIONAL_MEMORY_ASPECT_NAME,
+          STRUCTURED_PROPERTIES_ASPECT_NAME);
   private final EntityClient _entityClient;
 
   @Override

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/dataset/mappers/DatasetMapper.java
Patch:
@@ -157,8 +157,8 @@ public Dataset apply(@Nonnull final EntityResponse entityResponse) {
             dataset.setAccess(AccessMapper.map(new Access(dataMap), entityUrn))));
     mappingHelper.mapToResult(
         STRUCTURED_PROPERTIES_ASPECT_NAME,
-        ((dataset, dataMap) ->
-            dataset.setStructuredProperties(
+        ((entity, dataMap) ->
+            entity.setStructuredProperties(
                 StructuredPropertiesMapper.map(new StructuredProperties(dataMap)))));
     mappingHelper.mapToResult(
         FORMS_ASPECT_NAME,

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/domain/DomainType.java
Patch:
@@ -38,7 +38,8 @@ public class DomainType
           Constants.DOMAIN_KEY_ASPECT_NAME,
           Constants.DOMAIN_PROPERTIES_ASPECT_NAME,
           Constants.OWNERSHIP_ASPECT_NAME,
-          Constants.INSTITUTIONAL_MEMORY_ASPECT_NAME);
+          Constants.INSTITUTIONAL_MEMORY_ASPECT_NAME,
+          Constants.STRUCTURED_PROPERTIES_ASPECT_NAME);
   private final EntityClient _entityClient;
 
   public DomainType(final EntityClient entityClient) {

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/glossary/GlossaryTermType.java
Patch:
@@ -58,7 +58,8 @@ public class GlossaryTermType
           STATUS_ASPECT_NAME,
           BROWSE_PATHS_ASPECT_NAME,
           DOMAINS_ASPECT_NAME,
-          DEPRECATION_ASPECT_NAME);
+          DEPRECATION_ASPECT_NAME,
+          STRUCTURED_PROPERTIES_ASPECT_NAME);
 
   private final EntityClient _entityClient;
 

File: metadata-events/mxe-utils-avro/src/main/java/com/linkedin/metadata/EventUtils.java
Patch:
@@ -57,7 +57,7 @@ public class EventUtils {
   private static final Schema ORIGINAL_MCP_AVRO_SCHEMA =
       getAvroSchemaFromResource("avro/com/linkedin/mxe/MetadataChangeProposal.avsc");
 
-  private static final Schema ORIGINAL_MCL_AVRO_SCHEMA =
+  public static final Schema ORIGINAL_MCL_AVRO_SCHEMA =
       getAvroSchemaFromResource("avro/com/linkedin/mxe/MetadataChangeLog.avsc");
 
   private static final Schema ORIGINAL_FMCL_AVRO_SCHEMA =

File: metadata-service/restli-servlet-impl/src/test/java/com/linkedin/metadata/resources/entity/AspectResourceTest.java
Patch:
@@ -122,7 +122,7 @@ public void testAsyncDefaultAspects() throws URISyntaxException {
                     .request(req)
                     .build())));
     _aspectResource.ingestProposal(mcp, "false");
-    verify(_producer, times(5))
+    verify(_producer, times(10))
         .produceMetadataChangeLog(eq(urn), any(AspectSpec.class), any(MetadataChangeLog.class));
     verifyNoMoreInteractions(_producer);
   }

File: datahub-graphql-core/src/test/java/com/linkedin/datahub/graphql/resolvers/browse/BrowseV2ResolverTest.java
Patch:
@@ -21,6 +21,7 @@
 import com.linkedin.metadata.browse.BrowseResultGroupV2Array;
 import com.linkedin.metadata.browse.BrowseResultMetadata;
 import com.linkedin.metadata.browse.BrowseResultV2;
+import com.linkedin.metadata.query.SearchFlags;
 import com.linkedin.metadata.query.filter.ConjunctiveCriterion;
 import com.linkedin.metadata.query.filter.ConjunctiveCriterionArray;
 import com.linkedin.metadata.query.filter.Criterion;
@@ -262,7 +263,8 @@ private static EntityClient initMockEntityClient(
                 Mockito.eq(query),
                 Mockito.eq(start),
                 Mockito.eq(limit),
-                Mockito.any(Authentication.class)))
+                Mockito.any(Authentication.class),
+                Mockito.nullable(SearchFlags.class)))
         .thenReturn(result);
     return client;
   }

File: metadata-io/src/main/java/com/linkedin/metadata/search/utils/ESUtils.java
Patch:
@@ -261,6 +261,8 @@ public static String getElasticTypeForFieldType(SearchableAnnotation.FieldType f
       return DATE_FIELD_TYPE;
     } else if (fieldType == SearchableAnnotation.FieldType.OBJECT) {
       return OBJECT_FIELD_TYPE;
+    } else if (fieldType == SearchableAnnotation.FieldType.DOUBLE) {
+      return DOUBLE_FIELD_TYPE;
     } else {
       log.warn("FieldType {} has no mappings implemented", fieldType);
       return null;

File: entity-registry/src/main/java/com/linkedin/metadata/aspect/patch/builder/GlobalTagsPatchBuilder.java
Patch:
@@ -4,7 +4,7 @@
 import static com.linkedin.metadata.Constants.GLOBAL_TAGS_ASPECT_NAME;
 
 import com.fasterxml.jackson.databind.node.ObjectNode;
-import com.linkedin.common.TagUrn;
+import com.linkedin.common.urn.TagUrn;
 import com.linkedin.metadata.aspect.patch.PatchOperationType;
 import javax.annotation.Nonnull;
 import javax.annotation.Nullable;

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/recommendation/candidatesource/DomainsCandidateSourceFactory.java
Patch:
@@ -1,6 +1,7 @@
 package com.linkedin.gms.factory.recommendation.candidatesource;
 
 import com.linkedin.gms.factory.search.EntitySearchServiceFactory;
+import com.linkedin.metadata.models.registry.EntityRegistry;
 import com.linkedin.metadata.recommendation.candidatesource.DomainsCandidateSource;
 import com.linkedin.metadata.search.EntitySearchService;
 import javax.annotation.Nonnull;
@@ -20,7 +21,7 @@ public class DomainsCandidateSourceFactory {
 
   @Bean(name = "domainsCandidateSource")
   @Nonnull
-  protected DomainsCandidateSource getInstance() {
-    return new DomainsCandidateSource(entitySearchService);
+  protected DomainsCandidateSource getInstance(final EntityRegistry entityRegistry) {
+    return new DomainsCandidateSource(entitySearchService, entityRegistry);
   }
 }

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/recommendation/candidatesource/TopTagsCandidateSourceFactory.java
Patch:
@@ -1,6 +1,7 @@
 package com.linkedin.gms.factory.recommendation.candidatesource;
 
 import com.linkedin.gms.factory.search.EntitySearchServiceFactory;
+import com.linkedin.metadata.entity.EntityService;
 import com.linkedin.metadata.recommendation.candidatesource.TopTagsSource;
 import com.linkedin.metadata.search.EntitySearchService;
 import javax.annotation.Nonnull;
@@ -20,7 +21,7 @@ public class TopTagsCandidateSourceFactory {
 
   @Bean(name = "topTagsCandidateSource")
   @Nonnull
-  protected TopTagsSource getInstance() {
-    return new TopTagsSource(entitySearchService);
+  protected TopTagsSource getInstance(final EntityService<?> entityService) {
+    return new TopTagsSource(entitySearchService, entityService);
   }
 }

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/recommendation/candidatesource/TopTermsCandidateSourceFactory.java
Patch:
@@ -1,6 +1,7 @@
 package com.linkedin.gms.factory.recommendation.candidatesource;
 
 import com.linkedin.gms.factory.search.EntitySearchServiceFactory;
+import com.linkedin.metadata.entity.EntityService;
 import com.linkedin.metadata.recommendation.candidatesource.TopTermsSource;
 import com.linkedin.metadata.search.EntitySearchService;
 import javax.annotation.Nonnull;
@@ -20,7 +21,7 @@ public class TopTermsCandidateSourceFactory {
 
   @Bean(name = "topTermsCandidateSource")
   @Nonnull
-  protected TopTermsSource getInstance() {
-    return new TopTermsSource(entitySearchService);
+  protected TopTermsSource getInstance(final EntityService<?> entityService) {
+    return new TopTermsSource(entitySearchService, entityService);
   }
 }

File: metadata-service/services/src/main/java/com/linkedin/metadata/recommendation/candidatesource/TopTagsSource.java
Patch:
@@ -1,6 +1,7 @@
 package com.linkedin.metadata.recommendation.candidatesource;
 
 import com.linkedin.common.urn.Urn;
+import com.linkedin.metadata.entity.EntityService;
 import com.linkedin.metadata.recommendation.RecommendationRenderType;
 import com.linkedin.metadata.recommendation.RecommendationRequestContext;
 import com.linkedin.metadata.recommendation.ScenarioType;
@@ -13,8 +14,8 @@ public class TopTagsSource extends EntitySearchAggregationSource {
 
   private static final String TAGS = "tags";
 
-  public TopTagsSource(EntitySearchService entitySearchService) {
-    super(entitySearchService);
+  public TopTagsSource(EntitySearchService entitySearchService, EntityService<?> entityService) {
+    super(entitySearchService, entityService.getEntityRegistry());
   }
 
   @Override

File: metadata-service/services/src/main/java/com/linkedin/metadata/recommendation/candidatesource/TopTermsSource.java
Patch:
@@ -1,6 +1,7 @@
 package com.linkedin.metadata.recommendation.candidatesource;
 
 import com.linkedin.common.urn.Urn;
+import com.linkedin.metadata.entity.EntityService;
 import com.linkedin.metadata.recommendation.RecommendationRenderType;
 import com.linkedin.metadata.recommendation.RecommendationRequestContext;
 import com.linkedin.metadata.recommendation.ScenarioType;
@@ -13,8 +14,8 @@ public class TopTermsSource extends EntitySearchAggregationSource {
 
   private static final String TERMS = "glossaryTerms";
 
-  public TopTermsSource(EntitySearchService entitySearchService) {
-    super(entitySearchService);
+  public TopTermsSource(EntitySearchService entitySearchService, EntityService<?> entityService) {
+    super(entitySearchService, entityService.getEntityRegistry());
   }
 
   @Override

File: datahub-upgrade/src/main/java/com/linkedin/datahub/upgrade/nocode/DataMigrationStep.java
Patch:
@@ -10,6 +10,7 @@
 import com.linkedin.datahub.upgrade.UpgradeStepResult;
 import com.linkedin.datahub.upgrade.impl.DefaultUpgradeStepResult;
 import com.linkedin.metadata.Constants;
+import com.linkedin.metadata.aspect.utils.DefaultAspectsUtil;
 import com.linkedin.metadata.entity.EntityService;
 import com.linkedin.metadata.entity.ebean.EbeanAspectV1;
 import com.linkedin.metadata.entity.ebean.EbeanAspectV2;
@@ -170,7 +171,7 @@ public Function<UpgradeContext, UpgradeStepResult> executable() {
             // Emit a browse path aspect.
             final BrowsePaths browsePaths;
             try {
-              browsePaths = _entityService.buildDefaultBrowsePath(urn);
+              browsePaths = DefaultAspectsUtil.buildDefaultBrowsePath(urn, _entityService);
 
               final AuditStamp browsePathsStamp = new AuditStamp();
               browsePathsStamp.setActor(Urn.createFromString(Constants.SYSTEM_ACTOR));

File: datahub-upgrade/src/main/java/com/linkedin/datahub/upgrade/system/entity/steps/BackfillBrowsePathsV2Step.java
Patch:
@@ -15,6 +15,7 @@
 import com.linkedin.datahub.upgrade.impl.DefaultUpgradeStepResult;
 import com.linkedin.events.metadata.ChangeType;
 import com.linkedin.metadata.Constants;
+import com.linkedin.metadata.aspect.utils.DefaultAspectsUtil;
 import com.linkedin.metadata.entity.EntityService;
 import com.linkedin.metadata.query.SearchFlags;
 import com.linkedin.metadata.query.filter.Condition;
@@ -181,7 +182,8 @@ private Filter backfillDefaultBrowsePathsV2Filter() {
   }
 
   private void ingestBrowsePathsV2(Urn urn, AuditStamp auditStamp) throws Exception {
-    BrowsePathsV2 browsePathsV2 = _entityService.buildDefaultBrowsePathV2(urn, true);
+    BrowsePathsV2 browsePathsV2 =
+        DefaultAspectsUtil.buildDefaultBrowsePathV2(urn, true, _entityService);
     log.debug(String.format("Adding browse path v2 for urn %s with value %s", urn, browsePathsV2));
     MetadataChangeProposal proposal = new MetadataChangeProposal();
     proposal.setEntityUrn(urn);

File: entity-registry/src/main/java/com/linkedin/metadata/aspect/batch/AspectsBatch.java
Patch:
@@ -3,6 +3,7 @@
 import com.linkedin.metadata.aspect.plugins.validation.AspectRetriever;
 import com.linkedin.mxe.SystemMetadata;
 import com.linkedin.util.Pair;
+import java.util.Collection;
 import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
@@ -17,14 +18,14 @@
  * SystemMetadata} and record/message created time
  */
 public interface AspectsBatch {
-  List<? extends BatchItem> getItems();
+  Collection<? extends BatchItem> getItems();
 
   /**
    * Returns MCP items. Can be patch, upsert, etc.
    *
    * @return batch items
    */
-  default List<? extends MCPBatchItem> getMCPItems() {
+  default Collection<? extends MCPBatchItem> getMCPItems() {
     return getItems().stream()
         .filter(item -> item instanceof MCPBatchItem)
         .map(item -> (MCPBatchItem) item)

File: entity-registry/src/main/java/com/linkedin/metadata/aspect/batch/UpsertItem.java
Patch:
@@ -11,8 +11,6 @@
  * related data stored along with the aspect
  */
 public abstract class UpsertItem extends MCPBatchItem {
-  public abstract RecordTemplate getAspect();
-
   public abstract SystemAspect toLatestEntityAspect();
 
   public abstract void validatePreCommit(

File: metadata-integration/java/spark-lineage/src/test/java/datahub/spark/TestCoalesceJobLineage.java
Patch:
@@ -99,7 +99,9 @@ public static void resetBaseExpectations() {
 
   @BeforeClass
   public static void initMockServer() {
-    mockServer = startClientAndServer(GMS_PORT);
+    if (mockServer == null) {
+      mockServer = startClientAndServer(GMS_PORT);
+    }
     resetBaseExpectations();
   }
 

File: metadata-integration/java/spark-lineage/src/test/java/datahub/spark/TestSparkJobsLineage.java
Patch:
@@ -138,7 +138,9 @@ public static void resetBaseExpectations() {
 
   @BeforeClass
   public static void init() {
-    mockServer = startClientAndServer(GMS_PORT);
+    if (mockServer == null) {
+      mockServer = startClientAndServer(GMS_PORT);
+    }
     resetBaseExpectations();
   }
 

File: metadata-io/src/main/java/com/linkedin/metadata/entity/AspectDao.java
Patch:
@@ -148,11 +148,11 @@ <T> T runInTransactionWithRetry(
       @Nonnull final Function<Transaction, T> block, final int maxTransactionRetry);
 
   @Nonnull
-  default <T> T runInTransactionWithRetry(
+  default <T> List<T> runInTransactionWithRetry(
       @Nonnull final Function<Transaction, T> block,
       AspectsBatch batch,
       final int maxTransactionRetry) {
-    return runInTransactionWithRetry(block, maxTransactionRetry);
+    return List.of(runInTransactionWithRetry(block, maxTransactionRetry));
   }
 
   default void incrementWriteMetrics(String aspectName, long count, long bytes) {

File: metadata-io/src/main/java/com/linkedin/metadata/entity/ebean/batch/AspectsBatchImpl.java
Patch:
@@ -11,6 +11,7 @@
 import com.linkedin.mxe.MetadataChangeProposal;
 import com.linkedin.mxe.SystemMetadata;
 import com.linkedin.util.Pair;
+import java.util.Collection;
 import java.util.LinkedList;
 import java.util.List;
 import java.util.Map;
@@ -26,7 +27,7 @@
 @Builder(toBuilder = true)
 public class AspectsBatchImpl implements AspectsBatch {
 
-  private final List<? extends BatchItem> items;
+  private final Collection<? extends BatchItem> items;
 
   /**
    * Convert patches to upserts, apply hooks at the aspect and batch level.

File: metadata-io/src/main/java/com/linkedin/metadata/entity/ebean/batch/MCLBatchItemImpl.java
Patch:
@@ -30,9 +30,9 @@ public class MCLBatchItemImpl implements MCLBatchItem {
 
   @Nonnull private final MetadataChangeLog metadataChangeLog;
 
-  @Nullable private final RecordTemplate aspect;
+  @Nullable private final RecordTemplate recordTemplate;
 
-  @Nullable private final RecordTemplate previousAspect;
+  @Nullable private final RecordTemplate previousRecordTemplate;
 
   // derived
   private final EntitySpec entitySpec;

File: metadata-io/src/main/java/com/linkedin/metadata/search/utils/BrowsePathUtils.java
Patch:
@@ -29,8 +29,7 @@ public class BrowsePathUtils {
   public static String getDefaultBrowsePath(
       @Nonnull Urn urn,
       @Nonnull EntityRegistry entityRegistry,
-      @Nonnull Character dataPlatformDelimiter)
-      throws URISyntaxException {
+      @Nonnull Character dataPlatformDelimiter) {
 
     switch (urn.getEntityType()) {
       case Constants.DATASET_ENTITY_NAME:

File: metadata-io/src/main/java/com/linkedin/metadata/service/UpdateIndicesService.java
Patch:
@@ -160,8 +160,8 @@ private void handleUpdateChangeEvent(@Nonnull final MCLBatchItem event) throws I
     final AspectSpec aspectSpec = event.getAspectSpec();
     final Urn urn = event.getUrn();
 
-    RecordTemplate aspect = event.getAspect();
-    RecordTemplate previousAspect = event.getPreviousAspect();
+    RecordTemplate aspect = event.getRecordTemplate();
+    RecordTemplate previousAspect = event.getPreviousRecordTemplate();
 
     // Step 0. If the aspect is timeseries, add to its timeseries index.
     if (aspectSpec.isTimeseries()) {
@@ -264,7 +264,7 @@ private void handleDeleteChangeEvent(@Nonnull final MCLBatchItem event) {
               urn.getEntityType(), event.getAspectName()));
     }
 
-    RecordTemplate aspect = event.getAspect();
+    RecordTemplate aspect = event.getRecordTemplate();
     Boolean isDeletingKey = event.getAspectName().equals(entitySpec.getKeyAspectName());
 
     if (!aspectSpec.isTimeseries()) {

File: metadata-io/src/test/java/com/linkedin/metadata/AspectIngestionUtils.java
Patch:
@@ -38,7 +38,7 @@ public static Map<Urn, CorpUserKey> ingestCorpUserKeyAspects(
           MCPUpsertBatchItem.builder()
               .urn(urn)
               .aspectName(aspectName)
-              .aspect(aspect)
+              .recordTemplate(aspect)
               .auditStamp(AspectGenerationUtils.createAuditStamp())
               .systemMetadata(AspectGenerationUtils.createSystemMetadata())
               .build(entityService));
@@ -68,7 +68,7 @@ public static Map<Urn, CorpUserInfo> ingestCorpUserInfoAspects(
           MCPUpsertBatchItem.builder()
               .urn(urn)
               .aspectName(aspectName)
-              .aspect(aspect)
+              .recordTemplate(aspect)
               .auditStamp(AspectGenerationUtils.createAuditStamp())
               .systemMetadata(AspectGenerationUtils.createSystemMetadata())
               .build(entityService));
@@ -99,7 +99,7 @@ public static Map<Urn, ChartInfo> ingestChartInfoAspects(
           MCPUpsertBatchItem.builder()
               .urn(urn)
               .aspectName(aspectName)
-              .aspect(aspect)
+              .recordTemplate(aspect)
               .auditStamp(AspectGenerationUtils.createAuditStamp())
               .systemMetadata(AspectGenerationUtils.createSystemMetadata())
               .build(entityService));

File: metadata-io/src/test/java/com/linkedin/metadata/entity/CassandraAspectMigrationsDaoTest.java
Patch:
@@ -54,7 +54,8 @@ private void configureComponents() {
             _testEntityRegistry,
             true,
             _mockUpdateIndicesService,
-            preProcessHooks);
+            preProcessHooks,
+            true);
     _retentionService = new CassandraRetentionService(_entityServiceImpl, session, 1000);
     _entityServiceImpl.setRetentionService(_retentionService);
 

File: metadata-io/src/test/java/com/linkedin/metadata/entity/CassandraEntityServiceTest.java
Patch:
@@ -77,7 +77,8 @@ private void configureComponents() {
             _testEntityRegistry,
             false,
             _mockUpdateIndicesService,
-            preProcessHooks);
+            preProcessHooks,
+            true);
     _retentionService = new CassandraRetentionService(_entityServiceImpl, session, 1000);
     _entityServiceImpl.setRetentionService(_retentionService);
   }

File: metadata-io/src/test/java/com/linkedin/metadata/entity/DeleteEntityServiceTest.java
Patch:
@@ -59,7 +59,8 @@ public DeleteEntityServiceTest() {
             _entityRegistry,
             true,
             _mockUpdateIndicesService,
-            preProcessHooks);
+            preProcessHooks,
+            true);
     _deleteEntityService = new DeleteEntityService(_entityServiceImpl, _graphService);
   }
 

File: metadata-io/src/test/java/com/linkedin/metadata/timeline/CassandraTimelineServiceTest.java
Patch:
@@ -61,7 +61,8 @@ private void configureComponents() {
             _testEntityRegistry,
             true,
             _mockUpdateIndicesService,
-            preProcessHooks);
+            preProcessHooks,
+            true);
   }
 
   /**

File: metadata-io/src/test/java/io/datahubproject/test/fixtures/search/SampleDataFixtureConfiguration.java
Patch:
@@ -296,7 +296,8 @@ private EntityClient entityClientHelper(
     PreProcessHooks preProcessHooks = new PreProcessHooks();
     preProcessHooks.setUiEnabled(true);
     return new JavaEntityClient(
-        new EntityServiceImpl(mockAspectDao, null, entityRegistry, true, null, preProcessHooks),
+        new EntityServiceImpl(
+            mockAspectDao, null, entityRegistry, true, null, preProcessHooks, true),
         null,
         entitySearchService,
         cachingEntitySearchService,

File: metadata-io/src/test/java/io/datahubproject/test/fixtures/search/SearchLineageFixtureConfiguration.java
Patch:
@@ -234,7 +234,7 @@ protected EntityClient entityClient(
     PreProcessHooks preProcessHooks = new PreProcessHooks();
     preProcessHooks.setUiEnabled(true);
     return new JavaEntityClient(
-        new EntityServiceImpl(null, null, entityRegistry, true, null, preProcessHooks),
+        new EntityServiceImpl(null, null, entityRegistry, true, null, preProcessHooks, true),
         null,
         entitySearchService,
         cachingEntitySearchService,

File: metadata-models-custom/src/main/java/com/linkedin/metadata/aspect/plugins/hooks/CustomDataQualityRulesMCPSideEffect.java
Patch:
@@ -24,7 +24,7 @@ protected Stream<UpsertItem> applyMCPSideEffect(
         MCPUpsertBatchItem.builder()
             .urn(mirror)
             .aspectName(input.getAspectName())
-            .aspect(input.getAspect())
+            .recordTemplate(input.getRecordTemplate())
             .auditStamp(input.getAuditStamp())
             .systemMetadata(input.getSystemMetadata())
             .build(aspectRetriever));

File: metadata-service/factories/src/main/java/com/linkedin/metadata/boot/steps/BackfillBrowsePathsV2Step.java
Patch:
@@ -9,6 +9,7 @@
 import com.linkedin.common.urn.Urn;
 import com.linkedin.events.metadata.ChangeType;
 import com.linkedin.metadata.Constants;
+import com.linkedin.metadata.aspect.utils.DefaultAspectsUtil;
 import com.linkedin.metadata.boot.UpgradeStep;
 import com.linkedin.metadata.entity.EntityService;
 import com.linkedin.metadata.query.filter.Condition;
@@ -128,7 +129,8 @@ private String backfillBrowsePathsV2(String entityType, AuditStamp auditStamp, S
   }
 
   private void ingestBrowsePathsV2(Urn urn, AuditStamp auditStamp) throws Exception {
-    BrowsePathsV2 browsePathsV2 = _entityService.buildDefaultBrowsePathV2(urn, true);
+    BrowsePathsV2 browsePathsV2 =
+        DefaultAspectsUtil.buildDefaultBrowsePathV2(urn, true, _entityService);
     log.debug(String.format("Adding browse path v2 for urn %s with value %s", urn, browsePathsV2));
     MetadataChangeProposal proposal = new MetadataChangeProposal();
     proposal.setEntityUrn(urn);

File: metadata-service/factories/src/main/java/com/linkedin/metadata/boot/steps/IngestDataPlatformInstancesStep.java
Patch:
@@ -79,7 +79,7 @@ public void execute() throws Exception {
               MCPUpsertBatchItem.builder()
                   .urn(urn)
                   .aspectName(DATA_PLATFORM_INSTANCE_ASPECT_NAME)
-                  .aspect(dataPlatformInstance.get())
+                  .recordTemplate(dataPlatformInstance.get())
                   .auditStamp(aspectAuditStamp)
                   .build(_entityService));
         }

File: metadata-service/factories/src/main/java/com/linkedin/metadata/boot/steps/IngestDataPlatformsStep.java
Patch:
@@ -86,7 +86,7 @@ public void execute() throws IOException, URISyntaxException {
                     return MCPUpsertBatchItem.builder()
                         .urn(urn)
                         .aspectName(PLATFORM_ASPECT_NAME)
-                        .aspect(info)
+                        .recordTemplate(info)
                         .auditStamp(
                             new AuditStamp()
                                 .setActor(Urn.createFromString(Constants.SYSTEM_ACTOR))

File: metadata-service/factories/src/main/java/com/linkedin/metadata/boot/steps/UpgradeDefaultBrowsePathsStep.java
Patch:
@@ -9,6 +9,7 @@
 import com.linkedin.data.template.RecordTemplate;
 import com.linkedin.events.metadata.ChangeType;
 import com.linkedin.metadata.Constants;
+import com.linkedin.metadata.aspect.utils.DefaultAspectsUtil;
 import com.linkedin.metadata.boot.UpgradeStep;
 import com.linkedin.metadata.entity.EntityService;
 import com.linkedin.metadata.entity.ListResult;
@@ -126,7 +127,7 @@ private int getAndMigrateBrowsePaths(String entityType, int start, AuditStamp au
   }
 
   private void migrateBrowsePath(Urn urn, AuditStamp auditStamp) throws Exception {
-    BrowsePaths newPaths = _entityService.buildDefaultBrowsePath(urn);
+    BrowsePaths newPaths = DefaultAspectsUtil.buildDefaultBrowsePath(urn, _entityService);
     log.debug(String.format("Updating browse path for urn %s to value %s", urn, newPaths));
     MetadataChangeProposal proposal = new MetadataChangeProposal();
     proposal.setEntityUrn(urn);

File: metadata-service/factories/src/test/java/com/linkedin/metadata/boot/steps/IngestDataPlatformInstancesStepTest.java
Patch:
@@ -122,7 +122,7 @@ public void testExecuteWhenSomeEntitiesShouldReceiveDataPlatformInstance() throw
                                 item.getUrn().getEntityType().equals("chart")
                                     && item.getAspectName()
                                         .equals(DATA_PLATFORM_INSTANCE_ASPECT_NAME)
-                                    && ((MCPUpsertBatchItem) item).getAspect()
+                                    && ((MCPUpsertBatchItem) item).getRecordTemplate()
                                         instanceof DataPlatformInstance)),
             anyBoolean(),
             anyBoolean());
@@ -136,7 +136,7 @@ public void testExecuteWhenSomeEntitiesShouldReceiveDataPlatformInstance() throw
                                 item.getUrn().getEntityType().equals("chart")
                                     && item.getAspectName()
                                         .equals(DATA_PLATFORM_INSTANCE_ASPECT_NAME)
-                                    && ((MCPUpsertBatchItem) item).getAspect()
+                                    && ((MCPUpsertBatchItem) item).getRecordTemplate()
                                         instanceof DataPlatformInstance)),
             anyBoolean(),
             anyBoolean());

File: metadata-service/openapi-servlet/src/main/java/io/datahubproject/openapi/v2/controller/EntityController.java
Patch:
@@ -481,7 +481,7 @@ private UpsertItem toUpsertItem(
         .urn(entityUrn)
         .aspectName(aspectSpec.getName())
         .auditStamp(AuditStampUtils.createAuditStamp(actor.toUrnStr()))
-        .aspect(
+        .recordTemplate(
             GenericRecordUtils.deserializeAspect(
                 ByteString.copyString(jsonAspect, StandardCharsets.UTF_8),
                 GenericRecordUtils.JSON,

File: metadata-service/openapi-servlet/src/test/java/mock/MockEntityService.java
Patch:
@@ -59,7 +59,7 @@ public MockEntityService(
       @Nonnull EntityRegistry entityRegistry,
       @Nonnull UpdateIndicesService updateIndicesService,
       PreProcessHooks preProcessHooks) {
-    super(aspectDao, producer, entityRegistry, true, updateIndicesService, preProcessHooks);
+    super(aspectDao, producer, entityRegistry, true, updateIndicesService, preProcessHooks, true);
   }
 
   @Override

File: metadata-service/auth-impl/src/main/java/com/datahub/authentication/authenticator/HealthStatusAuthenticator.java
Patch:
@@ -24,7 +24,8 @@
  */
 @Slf4j
 public class HealthStatusAuthenticator implements Authenticator {
-  private static final Set<String> HEALTH_ENDPOINTS = Set.of("/openapi/check/", "/openapi/up/");
+  private static final Set<String> HEALTH_ENDPOINTS =
+      Set.of("/openapi/check/", "/openapi/up/", "/actuator/health", "/health");
   private String systemClientId;
 
   @Override

File: metadata-io/src/main/java/com/linkedin/metadata/search/elasticsearch/query/ESSearchDAO.java
Patch:
@@ -78,8 +78,7 @@ public long docCount(@Nonnull String entityName) {
     EntitySpec entitySpec = entityRegistry.getEntitySpec(entityName);
     CountRequest countRequest =
         new CountRequest(indexConvention.getIndexName(entitySpec))
-            .query(
-                SearchRequestHandler.getFilterQuery(null, entitySpec.getSearchableFieldSpecMap()));
+            .query(SearchRequestHandler.getFilterQuery(null, entitySpec.getSearchableFieldTypes()));
     try (Timer.Context ignored = MetricUtils.timer(this.getClass(), "docCount").time()) {
       return client.count(countRequest, RequestOptions.DEFAULT).getCount();
     } catch (IOException e) {

File: metadata-io/src/main/java/com/linkedin/metadata/timeseries/elastic/query/ESAggregatedStatsDAO.java
Patch:
@@ -379,7 +379,7 @@ public GenericTable getAggregatedStats(
     // Setup the filter query builder using the input filter provided.
     final BoolQueryBuilder filterQueryBuilder =
         ESUtils.buildFilterQuery(
-            filter, true, _entityRegistry.getEntitySpec(entityName).getSearchableFieldSpecMap());
+            filter, true, _entityRegistry.getEntitySpec(entityName).getSearchableFieldTypes());
 
     AspectSpec aspectSpec = getTimeseriesAspectSpec(entityName, aspectName);
     // Build and attach the grouping aggregations

File: entity-registry/src/main/java/com/linkedin/metadata/models/EntitySpecBuilder.java
Patch:
@@ -248,9 +248,9 @@ public AspectSpec buildAspectSpec(
       // Extract SearchScore Field Specs
       final SearchScoreFieldSpecExtractor searchScoreFieldSpecExtractor =
           new SearchScoreFieldSpecExtractor();
-      final DataSchemaRichContextTraverser searcScoreFieldSpecTraverser =
+      final DataSchemaRichContextTraverser searchScoreFieldSpecTraverser =
           new DataSchemaRichContextTraverser(searchScoreFieldSpecExtractor);
-      searcScoreFieldSpecTraverser.traverse(processedSearchScoreResult.getResultSchema());
+      searchScoreFieldSpecTraverser.traverse(processedSearchScoreResult.getResultSchema());
 
       final SchemaAnnotationProcessor.SchemaAnnotationProcessResult processedRelationshipResult =
           SchemaAnnotationProcessor.process(

File: entity-registry/src/main/java/com/linkedin/metadata/models/registry/ConfigEntityRegistry.java
Patch:
@@ -91,7 +91,7 @@ private static Pair<Path, Path> getFileAndClassPath(String entityRegistryRoot)
               .filter(Files::isRegularFile)
               .filter(f -> f.endsWith("entity-registry.yml") || f.endsWith("entity-registry.yaml"))
               .collect(Collectors.toList());
-      if (yamlFiles.size() == 0) {
+      if (yamlFiles.isEmpty()) {
         throw new EntityRegistryException(
             String.format(
                 "Did not find an entity registry (entity_registry.yaml/yml) under %s",

File: entity-registry/src/main/java/com/linkedin/metadata/models/registry/SnapshotEntityRegistry.java
Patch:
@@ -120,7 +120,7 @@ public AspectTemplateEngine getAspectTemplateEngine() {
   }
 
   @Override
-  public EventSpec getEventSpec(final String ignored) {
+  public EventSpec getEventSpec(@Nonnull final String ignored) {
     return null;
   }
 

File: metadata-io/src/main/java/com/linkedin/metadata/timeseries/elastic/query/ESAggregatedStatsDAO.java
Patch:
@@ -377,7 +377,9 @@ public GenericTable getAggregatedStats(
       @Nullable GroupingBucket[] groupingBuckets) {
 
     // Setup the filter query builder using the input filter provided.
-    final BoolQueryBuilder filterQueryBuilder = ESUtils.buildFilterQuery(filter, true);
+    final BoolQueryBuilder filterQueryBuilder =
+        ESUtils.buildFilterQuery(
+            filter, true, _entityRegistry.getEntitySpec(entityName).getSearchableFieldSpecMap());
 
     AspectSpec aspectSpec = getTimeseriesAspectSpec(entityName, aspectName);
     // Build and attach the grouping aggregations

File: metadata-io/src/test/java/com/linkedin/metadata/search/indexbuilder/MappingsBuilderTest.java
Patch:
@@ -21,7 +21,7 @@ public void testMappingsBuilder() {
     Map<String, Object> result = MappingsBuilder.getMappings(TestEntitySpecBuilder.getSpec());
     assertEquals(result.size(), 1);
     Map<String, Object> properties = (Map<String, Object>) result.get("properties");
-    assertEquals(properties.size(), 20);
+    assertEquals(properties.size(), 21);
     assertEquals(
         properties.get("urn"),
         ImmutableMap.of(
@@ -52,6 +52,7 @@ public void testMappingsBuilder() {
     assertEquals(properties.get("runId"), ImmutableMap.of("type", "keyword"));
     assertTrue(properties.containsKey("browsePaths"));
     assertTrue(properties.containsKey("browsePathV2"));
+    assertTrue(properties.containsKey("removed"));
     // KEYWORD
     Map<String, Object> keyPart3Field = (Map<String, Object>) properties.get("keyPart3");
     assertEquals(keyPart3Field.get("type"), "keyword");

File: metadata-io/src/test/java/com/linkedin/metadata/search/query/request/SearchRequestHandlerTest.java
Patch:
@@ -614,7 +614,7 @@ public void testBrowsePathQueryFilter() {
     Filter filter = new Filter();
     filter.setOr(conjunctiveCriterionArray);
 
-    BoolQueryBuilder test = SearchRequestHandler.getFilterQuery(filter);
+    BoolQueryBuilder test = SearchRequestHandler.getFilterQuery(filter, new HashMap<>());
 
     assertEquals(test.should().size(), 1);
 

File: datahub-frontend/app/auth/AuthUtils.java
Patch:
@@ -76,6 +76,9 @@ public class AuthUtils {
   public static final String USE_NONCE = "useNonce";
   public static final String READ_TIMEOUT = "readTimeout";
   public static final String EXTRACT_JWT_ACCESS_TOKEN_CLAIMS = "extractJwtAccessTokenClaims";
+  // Retained for backwards compatibility
+  public static final String PREFERRED_JWS_ALGORITHM = "preferredJwsAlgorithm";
+  public static final String PREFERRED_JWS_ALGORITHM_2 = "preferredJwsAlgorithm2";
 
   /**
    * Determines whether the inbound request should be forward to downstream Metadata Service. Today,

File: datahub-frontend/app/auth/sso/oidc/OidcConfigs.java
Patch:
@@ -226,8 +226,8 @@ public Builder from(final com.typesafe.config.Config configs, final String ssoSe
         extractJwtAccessTokenClaims =
             Optional.of(jsonNode.get(EXTRACT_JWT_ACCESS_TOKEN_CLAIMS).asBoolean());
       }
-      if (jsonNode.has(OIDC_PREFERRED_JWS_ALGORITHM)) {
-        preferredJwsAlgorithm = Optional.of(jsonNode.get(OIDC_PREFERRED_JWS_ALGORITHM).asText());
+      if (jsonNode.has(PREFERRED_JWS_ALGORITHM_2)) {
+        preferredJwsAlgorithm = Optional.of(jsonNode.get(PREFERRED_JWS_ALGORITHM_2).asText());
       } else {
         preferredJwsAlgorithm =
             Optional.ofNullable(getOptional(configs, OIDC_PREFERRED_JWS_ALGORITHM, null));

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/search/SearchUtils.java
Patch:
@@ -88,7 +88,6 @@ private SearchUtils() {}
           EntityType.TAG,
           EntityType.CORP_USER,
           EntityType.CORP_GROUP,
-          EntityType.ROLE,
           EntityType.NOTEBOOK,
           EntityType.DATA_PRODUCT);
 

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/dataset/mappers/SchemaFieldMapper.java
Patch:
@@ -36,6 +36,7 @@ public SchemaField apply(
     }
     result.setIsPartOfKey(input.isIsPartOfKey());
     result.setIsPartitioningKey(input.isIsPartitioningKey());
+    result.setJsonProps(input.getJsonProps());
     return result;
   }
 

File: metadata-service/configuration/src/main/java/com/linkedin/metadata/config/kafka/ConsumerConfiguration.java
Patch:
@@ -6,4 +6,5 @@
 public class ConsumerConfiguration {
 
   private int maxPartitionFetchBytes;
+  private boolean stopOnDeserializationError;
 }

File: entity-registry/src/main/java/com/linkedin/metadata/models/annotation/SearchableAnnotation.java
Patch:
@@ -66,7 +66,8 @@ public enum FieldType {
     DATETIME,
     OBJECT,
     BROWSE_PATH_V2,
-    WORD_GRAM
+    WORD_GRAM,
+    DOUBLE
   }
 
   @Nonnull

File: metadata-io/src/main/java/com/linkedin/metadata/search/elasticsearch/indexbuilder/MappingsBuilder.java
Patch:
@@ -134,6 +134,8 @@ private static Map<String, Object> getMappingsForField(@Nonnull final Searchable
       mappingForField.put(TYPE, ESUtils.DATE_FIELD_TYPE);
     } else if (fieldType == FieldType.OBJECT) {
       mappingForField.put(TYPE, ESUtils.OBJECT_FIELD_TYPE);
+    } else if (fieldType == FieldType.DOUBLE) {
+      mappingForField.put(TYPE, ESUtils.DOUBLE_FIELD_TYPE);
     } else {
       log.info("FieldType {} has no mappings implemented", fieldType);
     }

File: metadata-jobs/mae-consumer/src/main/java/com/linkedin/metadata/kafka/hook/siblings/SiblingAssociationHook.java
Patch:
@@ -205,9 +205,8 @@ private void handleSourceDatasetEvent(MetadataChangeLog event, DatasetUrn source
         // We're assuming a data asset (eg. snowflake table) will only ever be downstream of 1 dbt model
         if (dbtUpstreams.size() == 1) {
           setSiblingsAndSoftDeleteSibling(dbtUpstreams.get(0).getDataset(), sourceUrn);
-        } else {
+        } else if (dbtUpstreams.size() > 1) {
           log.error("{} has an unexpected number of dbt upstreams: {}. Not adding any as siblings.", sourceUrn.toString(), dbtUpstreams.size());
- 
         }
       }
     }

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/chart/ChartType.java
Patch:
@@ -77,7 +77,8 @@ public class ChartType implements SearchableEntityType<Chart, String>, Browsable
         INPUT_FIELDS_ASPECT_NAME,
         EMBED_ASPECT_NAME,
         DATA_PRODUCTS_ASPECT_NAME,
-        BROWSE_PATHS_V2_ASPECT_NAME
+        BROWSE_PATHS_V2_ASPECT_NAME,
+        SUB_TYPES_ASPECT_NAME
     );
     private static final Set<String> FACET_FIELDS = ImmutableSet.of("access", "queryType", "tool", "type");
 

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/datajob/DataJobType.java
Patch:
@@ -75,7 +75,8 @@ public class DataJobType implements SearchableEntityType<DataJob, String>, Brows
         DEPRECATION_ASPECT_NAME,
         DATA_PLATFORM_INSTANCE_ASPECT_NAME,
         DATA_PRODUCTS_ASPECT_NAME,
-        BROWSE_PATHS_V2_ASPECT_NAME
+        BROWSE_PATHS_V2_ASPECT_NAME,
+        SUB_TYPES_ASPECT_NAME
     );
     private static final Set<String> FACET_FIELDS = ImmutableSet.of("flow");
     private final EntityClient _entityClient;

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/dataset/DatasetType.java
Patch:
@@ -86,7 +86,8 @@ public class DatasetType implements SearchableEntityType<Dataset, String>, Brows
         EMBED_ASPECT_NAME,
         DATA_PRODUCTS_ASPECT_NAME,
         BROWSE_PATHS_V2_ASPECT_NAME,
-        ACCESS_DATASET_ASPECT_NAME
+        ACCESS_DATASET_ASPECT_NAME,
+        SUB_TYPES_ASPECT_NAME
     );
 
     private static final Set<String> FACET_FIELDS = ImmutableSet.of("origin", "platform");

File: metadata-io/src/main/java/com/linkedin/metadata/search/elasticsearch/ElasticSearchService.java
Patch:
@@ -144,7 +144,7 @@ public AutoCompleteResult autoComplete(@Nonnull String entityName, @Nonnull Stri
   @Override
   public Map<String, Long> aggregateByValue(@Nullable List<String> entityNames, @Nonnull String field,
       @Nullable Filter requestParams, int limit) {
-    log.debug("Aggregating by value: {}, field: {}, requestParams: {}, limit: {}", entityNames.toString(), field,
+    log.debug("Aggregating by value: {}, field: {}, requestParams: {}, limit: {}", entityNames != null ? entityNames.toString() : null, field,
         requestParams, limit);
     return esSearchDAO.aggregateByValue(entityNames, field, requestParams, limit);
   }

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/kafka/SimpleKafkaConsumerFactory.java
Patch:
@@ -43,7 +43,7 @@ protected KafkaListenerContainerFactory<?> createInstance(@Qualifier("configurat
       consumerProps.setBootstrapServers(Arrays.asList(kafkaConfiguration.getBootstrapServers().split(",")));
     } // else we rely on KafkaProperties which defaults to localhost:9092
 
-    Map<String, Object> customizedProperties = consumerProps.buildProperties();
+    Map<String, Object> customizedProperties = properties.buildConsumerProperties();
     customizedProperties.put(ConsumerConfig.MAX_PARTITION_FETCH_BYTES_CONFIG,
             kafkaConfiguration.getConsumer().getMaxPartitionFetchBytes());
 

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/dataproduct/ListDataProductAssetsResolver.java
Patch:
@@ -79,11 +79,11 @@ public CompletableFuture<SearchResults> get(DataFetchingEnvironment environment)
     }
 
     // 2. Get list of entities that we should query based on filters or assets from aspect.
-    List<String> entitiesToQuery = assetUrns.stream().map(Urn::getEntityType).collect(Collectors.toList());
+    List<String> entitiesToQuery = assetUrns.stream().map(Urn::getEntityType).distinct().collect(Collectors.toList());
 
 
     final List<EntityType> inputEntityTypes = (input.getTypes() == null || input.getTypes().isEmpty()) ? ImmutableList.of() : input.getTypes();
-    final List<String> inputEntityNames = inputEntityTypes.stream().map(EntityTypeMapper::getName).collect(Collectors.toList());
+    final List<String> inputEntityNames = inputEntityTypes.stream().map(EntityTypeMapper::getName).distinct().collect(Collectors.toList());
 
     final List<String> finalEntityNames = inputEntityNames.size() > 0 ? inputEntityNames : entitiesToQuery;
 

File: metadata-service/openapi-servlet/src/main/java/io/datahubproject/openapi/config/SpringWebConfig.java
Patch:
@@ -44,7 +44,6 @@ public GroupedOpenApi defaultOpenApiGroup() {
             .group("default")
             .packagesToExclude(
                     "io.datahubproject.openapi.operations",
-                    "com.datahub.health",
                     "io.datahubproject.openapi.health"
             ).build();
   }
@@ -55,7 +54,6 @@ public GroupedOpenApi operationsOpenApiGroup() {
             .group("operations")
             .packagesToScan(
                     "io.datahubproject.openapi.operations",
-                    "com.datahub.health",
                     "io.datahubproject.openapi.health"
             ).build();
   }

File: metadata-integration/java/datahub-protobuf/src/main/java/datahub/protobuf/model/ProtobufField.java
Patch:
@@ -259,7 +259,9 @@ private FieldDescriptorProto getNestedTypeFields(List<Integer> pathList, Descrip
             messageType = messageType.getNestedType(value);
         }
 
-        if (pathList.get(pathSize - 2) == DescriptorProto.FIELD_FIELD_NUMBER) {
+        if (pathList.get(pathSize - 2) == DescriptorProto.FIELD_FIELD_NUMBER
+                && pathList.get(pathSize - 1) != DescriptorProto.RESERVED_RANGE_FIELD_NUMBER
+                && pathList.get(pathSize - 1) != DescriptorProto.RESERVED_NAME_FIELD_NUMBER) {
             return messageType.getField(pathList.get(pathSize - 1));
         } else {
             return null;

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/ingest/execution/CreateIngestionExecutionRequestResolver.java
Patch:
@@ -117,6 +117,9 @@ public CompletableFuture<String> get(final DataFetchingEnvironment environment)
           if (ingestionSourceInfo.getConfig().hasDebugMode()) {
             debugMode = ingestionSourceInfo.getConfig().isDebugMode() ? "true" : "false";
           }
+          if (ingestionSourceInfo.getConfig().hasExtraArgs()) {
+            arguments.putAll(ingestionSourceInfo.getConfig().getExtraArgs());
+          }
           arguments.put(DEBUG_MODE_ARG_NAME, debugMode);
           execInput.setArgs(new StringMap(arguments));
 

File: datahub-graphql-core/src/test/java/com/linkedin/datahub/graphql/resolvers/ingest/source/UpsertIngestionSourceResolverTest.java
Patch:
@@ -26,7 +26,7 @@ public class UpsertIngestionSourceResolverTest {
       "Test source",
       "mysql", "Test source description",
       new UpdateIngestionSourceScheduleInput("* * * * *", "UTC"),
-      new UpdateIngestionSourceConfigInput("my test recipe", "0.8.18", "executor id", false)
+      new UpdateIngestionSourceConfigInput("my test recipe", "0.8.18", "executor id", false, null)
   );
 
   @Test

File: metadata-io/src/test/java/io/datahubproject/test/search/SearchTestContainer.java
Patch:
@@ -5,7 +5,9 @@
 import java.time.Duration;
 
 public interface SearchTestContainer {
+
     String SEARCH_JAVA_OPTS = "-Xms446m -Xmx446m -XX:MaxDirectMemorySize=368435456";
+
     Duration STARTUP_TIMEOUT = Duration.ofMinutes(5); // usually < 1min
 
     GenericContainer<?> startContainer();

File: metadata-io/src/main/java/com/linkedin/metadata/search/elasticsearch/indexbuilder/MappingsBuilder.java
Patch:
@@ -133,7 +133,7 @@ private static Map<String, Object> getMappingsForField(@Nonnull final Searchable
     } else if (fieldType == FieldType.DATETIME) {
       mappingForField.put(TYPE, ESUtils.DATE_FIELD_TYPE);
     } else if (fieldType == FieldType.OBJECT) {
-      mappingForField.put(TYPE, ESUtils.DATE_FIELD_TYPE);
+      mappingForField.put(TYPE, ESUtils.OBJECT_FIELD_TYPE);
     } else {
       log.info("FieldType {} has no mappings implemented", fieldType);
     }

File: metadata-io/src/test/java/io/datahubproject/test/search/SearchTestContainer.java
Patch:
@@ -5,7 +5,7 @@
 import java.time.Duration;
 
 public interface SearchTestContainer {
-    String SEARCH_JAVA_OPTS = "-Xms64m -Xmx384m -XX:MaxDirectMemorySize=368435456";
+    String SEARCH_JAVA_OPTS = "-Xms446m -Xmx446m -XX:MaxDirectMemorySize=368435456";
     Duration STARTUP_TIMEOUT = Duration.ofMinutes(5); // usually < 1min
 
     GenericContainer<?> startContainer();

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/mutate/AddOwnersResolver.java
Patch:
@@ -39,7 +39,7 @@ public CompletableFuture<Boolean> get(DataFetchingEnvironment environment) throw
         throw new AuthorizationException("Unauthorized to perform this action. Please contact your DataHub administrator.");
       }
 
-      OwnerUtils.validateAddInput(
+      OwnerUtils.validateAddOwnerInput(
           owners,
           targetUrn,
           _entityService

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/mutate/BatchAddOwnersResolver.java
Patch:
@@ -53,8 +53,7 @@ public CompletableFuture<Boolean> get(DataFetchingEnvironment environment) throw
 
   private void validateOwners(List<OwnerInput> owners) {
     for (OwnerInput ownerInput : owners) {
-      OwnerUtils.validateOwner(UrnUtils.getUrn(ownerInput.getOwnerUrn()), ownerInput.getOwnerEntityType(),
-          UrnUtils.getUrn(ownerInput.getOwnershipTypeUrn()), _entityService);
+      OwnerUtils.validateOwner(ownerInput, _entityService);
     }
   }
 

File: metadata-io/src/main/java/com/linkedin/metadata/search/elasticsearch/query/request/SearchRequestHandler.java
Patch:
@@ -242,7 +242,9 @@ public SearchRequest getSearchRequest(@Nonnull String input, @Nullable Filter fi
     BoolQueryBuilder filterQuery = getFilterQuery(filter);
     searchSourceBuilder.query(QueryBuilders.boolQuery().must(getQuery(input, finalSearchFlags.isFulltext())).filter(filterQuery));
     _aggregationQueryBuilder.getAggregations().forEach(searchSourceBuilder::aggregation);
-    searchSourceBuilder.highlighter(getHighlights());
+    if (!finalSearchFlags.isSkipHighlighting()) {
+      searchSourceBuilder.highlighter(_highlights);
+    }
     ESUtils.buildSortOrder(searchSourceBuilder, sortCriterion, _entitySpecs);
     searchRequest.source(searchSourceBuilder);
     log.debug("Search request is: " + searchRequest);

File: metadata-io/src/main/java/com/linkedin/metadata/timeseries/elastic/ElasticSearchTimeseriesAspectService.java
Patch:
@@ -169,7 +169,7 @@ public List<TimeseriesIndexSizeResult> getIndexSizes() {
     List<TimeseriesIndexSizeResult> res = new ArrayList<>();
     try {
       String indicesPattern = _indexConvention.getAllTimeseriesAspectIndicesPattern();
-      Response r = _searchClient.getLowLevelClient().performRequest(new Request("GET", indicesPattern + "/_stats"));
+      Response r = _searchClient.getLowLevelClient().performRequest(new Request("GET", "/" + indicesPattern + "/_stats"));
       JsonNode body = new ObjectMapper().readTree(r.getEntity().getContent());
       body.get("indices").fields().forEachRemaining(entry -> {
         TimeseriesIndexSizeResult elemResult = new TimeseriesIndexSizeResult();

File: datahub-frontend/app/auth/AuthModule.java
Patch:
@@ -56,7 +56,7 @@ public class AuthModule extends AbstractModule {
      * Pac4j Stores Session State in a browser-side cookie in encrypted fashion. This configuration
      * value provides a stable encryption base from which to derive the encryption key.
      *
-     * We hash this value (SHA1), then take the first 16 bytes as the AES key.
+     * We hash this value (SHA256), then take the first 16 bytes as the AES key.
      */
     private static final String PAC4J_AES_KEY_BASE_CONF = "play.http.secret.key";
     private static final String PAC4J_SESSIONSTORE_PROVIDER_CONF = "pac4j.sessionStore.provider";
@@ -93,7 +93,7 @@ protected void configure() {
                 // it to hex and slice the first 16 bytes, because AES key length must strictly
                 // have a specific length.
                 final String aesKeyBase = _configs.getString(PAC4J_AES_KEY_BASE_CONF);
-                final String aesKeyHash = DigestUtils.sha1Hex(aesKeyBase.getBytes(StandardCharsets.UTF_8));
+                final String aesKeyHash = DigestUtils.sha256Hex(aesKeyBase.getBytes(StandardCharsets.UTF_8));
                 final String aesEncryptionKey = aesKeyHash.substring(0, 16);
                 playCacheCookieStore = new PlayCookieSessionStore(
                         new ShiroAesDataEncrypter(aesEncryptionKey.getBytes()));

File: metadata-service/auth-config/src/main/java/com/datahub/authentication/AuthenticationConfiguration.java
Patch:
@@ -29,4 +29,6 @@ public class AuthenticationConfiguration {
    * The lifespan of a UI session token.
    */
   private long sessionTokenDurationMs;
+
+  private TokenServiceConfiguration tokenService;
 }

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/auth/DataHubTokenServiceFactory.java
Patch:
@@ -23,10 +23,10 @@ public class DataHubTokenServiceFactory {
   @Value("${authentication.tokenService.salt:}")
   private String saltingKey;
 
-  @Value("${elasticsearch.tokenService.signingAlgorithm:HS256}")
+  @Value("${authentication.tokenService.signingAlgorithm:HS256}")
   private String signingAlgorithm;
 
-  @Value("${elasticsearch.tokenService.issuer:datahub-metadata-service}")
+  @Value("${authentication.tokenService.issuer:datahub-metadata-service}")
   private String issuer;
 
   /**

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/GmsGraphQLEngine.java
Patch:
@@ -1292,7 +1292,8 @@ private void configureCorpUserResolvers(final RuntimeWiring.Builder builder) {
      */
     private void configureCorpGroupResolvers(final RuntimeWiring.Builder builder) {
         builder.type("CorpGroup", typeWiring -> typeWiring
-            .dataFetcher("relationships", new EntityRelationshipsResultResolver(graphClient)));
+            .dataFetcher("relationships", new EntityRelationshipsResultResolver(graphClient))
+            .dataFetcher("exists", new EntityExistsResolver(entityService)));
         builder.type("CorpGroupInfo", typeWiring -> typeWiring
                 .dataFetcher("admins",
                     new LoadableTypeBatchResolver<>(corpUserType,

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/EntityTypeMapper.java
Patch:
@@ -17,6 +17,7 @@ public class EntityTypeMapper {
       ImmutableMap.<EntityType, String>builder()
           .put(EntityType.DATASET, "dataset")
           .put(EntityType.ROLE, "role")
+          .put(EntityType.ASSERTION, Constants.ASSERTION_ENTITY_NAME)
           .put(EntityType.CORP_USER, "corpuser")
           .put(EntityType.CORP_GROUP, "corpGroup")
           .put(EntityType.DATA_PLATFORM, "dataPlatform")
@@ -25,6 +26,7 @@ public class EntityTypeMapper {
           .put(EntityType.TAG, "tag")
           .put(EntityType.DATA_FLOW, "dataFlow")
           .put(EntityType.DATA_JOB, "dataJob")
+          .put(EntityType.DATA_PROCESS_INSTANCE, Constants.DATA_PROCESS_INSTANCE_ENTITY_NAME)
           .put(EntityType.GLOSSARY_TERM, "glossaryTerm")
           .put(EntityType.GLOSSARY_NODE, "glossaryNode")
           .put(EntityType.MLMODEL, "mlModel")

File: datahub-upgrade/src/main/java/com/linkedin/datahub/upgrade/system/elasticsearch/util/IndexUtils.java
Patch:
@@ -31,7 +31,7 @@ public static List<ReindexConfig> getAllReindexConfigs(List<ElasticSearchIndexed
     List<ReindexConfig> reindexConfigs = new ArrayList<>(_reindexConfigs);
     if (reindexConfigs.isEmpty()) {
       for (ElasticSearchIndexed elasticSearchIndexed : elasticSearchIndexedList) {
-        reindexConfigs.addAll(elasticSearchIndexed.getReindexConfigs());
+        reindexConfigs.addAll(elasticSearchIndexed.buildReindexConfigs());
       }
       _reindexConfigs = new ArrayList<>(reindexConfigs);
     }

File: metadata-io/src/main/java/com/linkedin/metadata/graph/elastic/ElasticSearchGraphService.java
Patch:
@@ -318,7 +318,7 @@ public void removeEdgesFromNode(
   public void configure() {
     log.info("Setting up elastic graph index");
     try {
-      for (ReindexConfig config : getReindexConfigs()) {
+      for (ReindexConfig config : buildReindexConfigs()) {
         _indexBuilder.buildIndex(config);
       }
     } catch (IOException e) {
@@ -327,7 +327,7 @@ public void configure() {
   }
 
   @Override
-  public List<ReindexConfig> getReindexConfigs() throws IOException {
+  public List<ReindexConfig> buildReindexConfigs() throws IOException {
     return List.of(_indexBuilder.buildReindexState(_indexConvention.getIndexName(INDEX_NAME),
             GraphRelationshipMappingsBuilder.getMappings(), Collections.emptyMap()));
   }

File: metadata-io/src/main/java/com/linkedin/metadata/search/elasticsearch/ElasticSearchService.java
Patch:
@@ -46,8 +46,8 @@ public void configure() {
   }
 
   @Override
-  public List<ReindexConfig> getReindexConfigs() {
-    return indexBuilders.getReindexConfigs();
+  public List<ReindexConfig> buildReindexConfigs() {
+    return indexBuilders.buildReindexConfigs();
   }
 
   @Override

File: metadata-io/src/main/java/com/linkedin/metadata/shared/ElasticSearchIndexed.java
Patch:
@@ -11,7 +11,7 @@ public interface ElasticSearchIndexed {
      * The index configurations for the given service.
      * @return List of reindex configurations
      */
-    List<ReindexConfig> getReindexConfigs() throws IOException;
+    List<ReindexConfig> buildReindexConfigs() throws IOException;
 
     /**
      * Mirrors the service's functions which

File: metadata-io/src/main/java/com/linkedin/metadata/systemmetadata/ElasticSearchSystemMetadataService.java
Patch:
@@ -205,7 +205,7 @@ public List<IngestionRunSummary> listRuns(Integer pageOffset, Integer pageSize,
   public void configure() {
     log.info("Setting up system metadata index");
     try {
-      for (ReindexConfig config : getReindexConfigs()) {
+      for (ReindexConfig config : buildReindexConfigs()) {
         _indexBuilder.buildIndex(config);
       }
     } catch (IOException ie) {
@@ -214,7 +214,7 @@ public void configure() {
   }
 
   @Override
-  public List<ReindexConfig> getReindexConfigs() throws IOException {
+  public List<ReindexConfig> buildReindexConfigs() throws IOException {
     return List.of(_indexBuilder.buildReindexState(_indexConvention.getIndexName(INDEX_NAME),
             SystemMetadataMappingsBuilder.getMappings(), Collections.emptyMap()));
   }

File: metadata-io/src/main/java/com/linkedin/metadata/timeseries/elastic/ElasticSearchTimeseriesAspectService.java
Patch:
@@ -137,9 +137,10 @@ public void configure() {
   }
 
   @Override
-  public List<ReindexConfig> getReindexConfigs() {
-    return _indexBuilders.getReindexConfigs();
+  public List<ReindexConfig> buildReindexConfigs() {
+    return _indexBuilders.buildReindexConfigs();
   }
+
   public String reindexAsync(String index, @Nullable QueryBuilder filterQuery, BatchWriteOperationsOptions options)
       throws Exception {
     return _indexBuilders.reindexAsync(index, filterQuery, options);

File: metadata-io/src/main/java/com/linkedin/metadata/timeseries/elastic/indexbuilder/TimeseriesAspectIndexBuilders.java
Patch:
@@ -29,7 +29,7 @@ public class TimeseriesAspectIndexBuilders implements ElasticSearchIndexed {
 
   @Override
   public void reindexAll() {
-    for (ReindexConfig config : getReindexConfigs()) {
+    for (ReindexConfig config : buildReindexConfigs()) {
       try {
         _indexBuilder.buildIndex(config);
       } catch (IOException e) {
@@ -63,7 +63,7 @@ public String reindexAsync(String index, @Nullable QueryBuilder filterQuery, Bat
   }
 
   @Override
-  public List<ReindexConfig> getReindexConfigs() {
+  public List<ReindexConfig> buildReindexConfigs() {
     return _entityRegistry.getEntitySpecs().values().stream()
             .flatMap(entitySpec -> entitySpec.getAspectSpecs().stream()
                     .map(aspectSpec -> Pair.of(entitySpec, aspectSpec)))
@@ -80,4 +80,5 @@ public List<ReindexConfig> getReindexConfigs() {
               }
             }).collect(Collectors.toList());
   }
+
 }

File: metadata-jobs/mae-consumer/src/main/java/com/linkedin/metadata/kafka/hook/UpdateIndicesHook.java
Patch:
@@ -24,7 +24,7 @@
     EntityRegistryFactory.class, SystemMetadataServiceFactory.class, SearchDocumentTransformerFactory.class})
 public class UpdateIndicesHook implements MetadataChangeLogHook {
 
-  private final UpdateIndicesService _updateIndicesService;
+  protected final UpdateIndicesService _updateIndicesService;
   private final boolean _isEnabled;
 
   public UpdateIndicesHook(

File: metadata-service/restli-servlet-impl/src/main/java/com/linkedin/metadata/resources/usage/UsageStats.java
Patch:
@@ -380,8 +380,10 @@ public Task<UsageQueryResult> query(@ActionParam(PARAM_RESOURCE) @Nonnull String
   public Task<UsageQueryResult> queryRange(@ActionParam(PARAM_RESOURCE) @Nonnull String resource,
       @ActionParam(PARAM_DURATION) @Nonnull WindowDuration duration, @ActionParam(PARAM_RANGE) UsageTimeRange range) {
     Authentication auth = AuthenticationContext.getAuthentication();
+    Urn resourceUrn = UrnUtils.getUrn(resource);
     if (Boolean.parseBoolean(System.getenv(REST_API_AUTHORIZATION_ENABLED_ENV))
-        && !isAuthorized(auth, _authorizer, ImmutableList.of(PoliciesConfig.VIEW_DATASET_USAGE_PRIVILEGE), (ResourceSpec) null)) {
+        && !isAuthorized(auth, _authorizer, ImmutableList.of(PoliciesConfig.VIEW_DATASET_USAGE_PRIVILEGE),
+            new ResourceSpec(resourceUrn.getEntityType(), resourceUrn.toString()))) {
       throw new RestLiServiceException(HttpStatus.S_401_UNAUTHORIZED,
           "User is unauthorized to query usage.");
     }

File: metadata-service/auth-impl/src/main/java/com/datahub/telemetry/TrackingService.java
Patch:
@@ -102,7 +102,8 @@ public void emitAnalyticsEvent(@Nonnull final JsonNode event) {
     try {
       _mixpanelAPI.sendMessage(_mixpanelMessageBuilder.event(getClientId(), eventType, sanitizedEvent));
     } catch (IOException e) {
-      log.error("Failed to send event to Mixpanel", e);
+      log.info("Failed to send event to Mixpanel; this does not affect the functionality of the application");
+      log.debug("Failed to send event to Mixpanel", e);
     }
   }
 

File: metadata-io/src/main/java/com/linkedin/metadata/search/elasticsearch/indexbuilder/MappingsBuilder.java
Patch:
@@ -121,8 +121,7 @@ private static Map<String, Object> getMappingsForField(@Nonnull final Searchable
             String analyzerName = entry.getValue();
             subFields.put(fieldName, ImmutableMap.of(
                 TYPE, TEXT,
-                ANALYZER, analyzerName,
-                SEARCH_ANALYZER, analyzerName
+                ANALYZER, analyzerName
             ));
           }
         }

File: metadata-io/src/main/java/com/linkedin/metadata/recommendation/candidatesource/RecentlyViewedSource.java
Patch:
@@ -122,7 +122,7 @@ private SearchRequest buildSearchRequest(@Nonnull Urn userUrn) {
     BoolQueryBuilder query = QueryBuilders.boolQuery();
     // Filter for the entity view events of the user requesting recommendation
     query.must(
-        QueryBuilders.termQuery(ESUtils.toKeywordField(DataHubUsageEventConstants.ACTOR_URN, true), userUrn.toString()));
+        QueryBuilders.termQuery(ESUtils.toKeywordField(DataHubUsageEventConstants.ACTOR_URN, false), userUrn.toString()));
     query.must(
         QueryBuilders.termQuery(DataHubUsageEventConstants.TYPE, DataHubUsageEventType.ENTITY_VIEW_EVENT.getType()));
     source.query(query);

File: metadata-io/src/main/java/com/linkedin/metadata/search/elasticsearch/query/request/SearchQueryBuilder.java
Patch:
@@ -437,7 +437,8 @@ public Optional<BoolQueryBuilder> boolQueryBuilder(QueryConfiguration customQuer
   private BoolQueryBuilder toBoolQueryBuilder(String query, BoolQueryConfiguration boolQuery) {
     try {
       String jsonFragment = OBJECT_MAPPER.writeValueAsString(boolQuery)
-          .replace("\"{{query_string}}\"", OBJECT_MAPPER.writeValueAsString(query));
+          .replace("\"{{query_string}}\"", OBJECT_MAPPER.writeValueAsString(query))
+          .replace("\"{{unquoted_query_string}}\"", OBJECT_MAPPER.writeValueAsString(unquote(query)));
       XContentParser parser = XContentType.JSON.xContent().createParser(X_CONTENT_REGISTRY,
           LoggingDeprecationHandler.INSTANCE, jsonFragment);
       return BoolQueryBuilder.fromXContent(parser);

File: metadata-service/auth-impl/src/main/java/com/datahub/authorization/DataHubAuthorizer.java
Patch:
@@ -250,11 +250,11 @@ private void addPoliciesToCache(final Map<String, List<DataHubPolicyInfo>> cache
     private void addPolicyToCache(final Map<String, List<DataHubPolicyInfo>> cache, final DataHubPolicyInfo policy) {
       final List<String> privileges = policy.getPrivileges();
       for (String privilege : privileges) {
-        List<DataHubPolicyInfo> existingPolicies = cache.getOrDefault(privilege, new ArrayList<>());
+        List<DataHubPolicyInfo> existingPolicies = cache.containsKey(privilege) ? new ArrayList<>(cache.get(privilege)) : new ArrayList<>();
         existingPolicies.add(policy);
         cache.put(privilege, existingPolicies);
       }
-      List<DataHubPolicyInfo> existingPolicies = cache.getOrDefault(ALL, new ArrayList<>());
+      List<DataHubPolicyInfo> existingPolicies = cache.containsKey(ALL) ? new ArrayList<>(cache.get(ALL)) : new ArrayList<>();
       existingPolicies.add(policy);
       cache.put(ALL, existingPolicies);
     }

File: entity-registry/src/main/java/com/linkedin/metadata/models/SearchableFieldSpecExtractor.java
Patch:
@@ -155,7 +155,8 @@ private void extractSearchableAnnotation(final Object annotationObj, final DataS
             annotation.getBoostScore(),
             annotation.getHasValuesFieldName(),
             annotation.getNumValuesFieldName(),
-            annotation.getWeightsPerFieldValue());
+            annotation.getWeightsPerFieldValue(),
+            annotation.getFieldNameAliases());
       }
     }
     log.debug("Searchable annotation for field: {} : {}", schemaPathSpec, annotation);

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/search/SearchUtils.java
Patch:
@@ -73,7 +73,6 @@ private SearchUtils() {
           EntityType.CONTAINER,
           EntityType.DOMAIN,
           EntityType.DATA_PRODUCT,
-          EntityType.ROLE,
           EntityType.NOTEBOOK);
 
 

File: metadata-io/src/test/java/com/linkedin/metadata/search/elasticsearch/fixtures/ElasticSearchGoldenTest.java
Patch:
@@ -15,6 +15,7 @@
 import org.springframework.beans.factory.annotation.Qualifier;
 import org.springframework.context.annotation.Import;
 import org.springframework.test.context.testng.AbstractTestNGSpringContextTests;
+import org.testng.annotations.Ignore;
 import org.testng.annotations.Test;
 
 import java.util.List;
@@ -96,6 +97,7 @@ public void testNameMatchMemberInWorkspace() {
     }
 
     @Test
+    @Ignore("unstable")
     public void testGlossaryTerms() {
         /*
           Searching for "ReturnRate" should return all tables that have the glossary term applied before

File: metadata-io/src/main/java/com/linkedin/metadata/timeseries/elastic/ElasticSearchTimeseriesAspectService.java
Patch:
@@ -195,9 +195,11 @@ public long countByFilter(
       @Nonnull final String aspectName,
       @Nullable final Filter filter
   ) {
+    final String indexName = _indexConvention.getTimeseriesAspectIndexName(entityName, aspectName);
     final BoolQueryBuilder filterQueryBuilder = QueryBuilders.boolQuery().must(ESUtils.buildFilterQuery(filter, true));
     CountRequest countRequest = new CountRequest();
     countRequest.query(filterQueryBuilder);
+    countRequest.indices(indexName);
     try {
       CountResponse resp = _searchClient.count(countRequest, RequestOptions.DEFAULT);
       return resp.getCount();

File: metadata-io/src/main/java/com/linkedin/metadata/search/utils/SearchUtils.java
Patch:
@@ -120,9 +120,8 @@ public static String readResourceFile(@Nonnull Class clazz, @Nonnull String file
     }
   }
 
-  @Nonnull
-  public static Filter removeCriteria(@Nonnull Filter originalFilter, Predicate<Criterion> shouldRemove) {
-    if (originalFilter.getOr() != null) {
+  public static Filter removeCriteria(@Nullable Filter originalFilter, Predicate<Criterion> shouldRemove) {
+    if (originalFilter != null && originalFilter.getOr() != null) {
       return new Filter().setOr(new ConjunctiveCriterionArray(originalFilter.getOr()
           .stream()
           .map(criteria -> removeCriteria(criteria, shouldRemove))

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/chart/BrowseV2Resolver.java
Patch:
@@ -49,6 +49,8 @@ public CompletableFuture<BrowseResultsV2> get(DataFetchingEnvironment environmen
     final int start = input.getStart() != null ? input.getStart() : DEFAULT_START;
     final int count = input.getCount() != null ? input.getCount() : DEFAULT_COUNT;
     final String query = input.getQuery() != null ? input.getQuery() : "*";
+    // escape forward slash since it is a reserved character in Elasticsearch
+    final String sanitizedQuery = ResolverUtils.escapeForwardSlash(query);
 
     return CompletableFuture.supplyAsync(() -> {
       try {
@@ -64,7 +66,7 @@ public CompletableFuture<BrowseResultsV2> get(DataFetchingEnvironment environmen
             maybeResolvedView != null
             ? SearchUtils.combineFilters(filter, maybeResolvedView.getDefinition().getFilter())
             : filter,
-            query,
+            sanitizedQuery,
             start,
             count,
             context.getAuthentication()

File: metadata-io/src/main/java/com/linkedin/metadata/search/client/CachingEntitySearchService.java
Patch:
@@ -154,8 +154,8 @@ public SearchResult getCachedSearchResults(
         batchSize,
         querySize -> getRawSearchResults(entityName, query, filters, sortCriterion, querySize.getFrom(),
                 querySize.getSize(), flags, facets),
-        querySize -> Quintet.with(entityName, query, filters != null ? toJsonString(filters) : null,
-            sortCriterion != null ? toJsonString(sortCriterion) : null, querySize), flags, enableCache).getSearchResults(from, size);
+        querySize -> Sextet.with(entityName, query, filters != null ? toJsonString(filters) : null,
+            sortCriterion != null ? toJsonString(sortCriterion) : null, facets, querySize), flags, enableCache).getSearchResults(from, size);
   }
 
 

File: metadata-service/factories/src/test/java/com/linkedin/metadata/boot/steps/BackfillBrowsePathsV2StepTest.java
Patch:
@@ -32,7 +32,7 @@
 import static com.linkedin.metadata.Constants.CONTAINER_ASPECT_NAME;
 
 public class BackfillBrowsePathsV2StepTest {
-  private static final String VERSION_1 = "1";
+  private static final String VERSION = "2";
   private static final String UPGRADE_URN = String.format(
       "urn:li:%s:%s",
       Constants.DATA_HUB_UPGRADE_ENTITY_NAME,
@@ -110,7 +110,7 @@ public void testDoesNotRunWhenAlreadyExecuted() throws Exception {
     final SearchService mockSearchService = initMockSearchService();
 
     final Urn upgradeEntityUrn = Urn.createFromString(UPGRADE_URN);
-    com.linkedin.upgrade.DataHubUpgradeRequest upgradeRequest = new com.linkedin.upgrade.DataHubUpgradeRequest().setVersion(VERSION_1);
+    com.linkedin.upgrade.DataHubUpgradeRequest upgradeRequest = new com.linkedin.upgrade.DataHubUpgradeRequest().setVersion(VERSION);
     Map<String, EnvelopedAspect> upgradeRequestAspects = new HashMap<>();
     upgradeRequestAspects.put(Constants.DATA_HUB_UPGRADE_REQUEST_ASPECT_NAME,
         new EnvelopedAspect().setValue(new Aspect(upgradeRequest.data())));

File: datahub-upgrade/src/test/java/com/linkedin/datahub/upgrade/DatahubUpgradeNoSchemaRegistryTest.java
Patch:
@@ -19,7 +19,8 @@
 @ActiveProfiles("test")
 @SpringBootTest(classes = {UpgradeCliApplication.class, UpgradeCliApplicationTestConfiguration.class},
         properties = {
-                "kafka.schemaRegistry.type=INTERNAL"
+                "kafka.schemaRegistry.type=INTERNAL",
+                "DATAHUB_UPGRADE_HISTORY_TOPIC_NAME=test_due_topic"
         })
 public class DatahubUpgradeNoSchemaRegistryTest extends AbstractTestNGSpringContextTests {
 

File: metadata-service/factories/src/main/java/com/linkedin/metadata/boot/kafka/DataHubUpgradeKafkaListener.java
Patch:
@@ -38,7 +38,7 @@ public class DataHubUpgradeKafkaListener implements ConsumerSeekAware, Bootstrap
 
   private static final String CONSUMER_GROUP = "${DATAHUB_UPGRADE_HISTORY_KAFKA_CONSUMER_GROUP_ID:generic-duhe-consumer-job-client}";
   private static final String SUFFIX = "temp";
-  private static final String TOPIC_NAME = "${DATAHUB_UPGRADE_HISTORY_TOPIC_NAME:" + Topics.DATAHUB_UPGRADE_HISTORY_TOPIC_NAME  + "}";
+  public static final String TOPIC_NAME = "${DATAHUB_UPGRADE_HISTORY_TOPIC_NAME:" + Topics.DATAHUB_UPGRADE_HISTORY_TOPIC_NAME  + "}";
 
   private final DefaultKafkaConsumerFactory<String, GenericRecord> _defaultKafkaConsumerFactory;
 

File: metadata-service/restli-servlet-impl/src/main/java/com/linkedin/metadata/resources/operations/OperationsResource.java
Patch:
@@ -99,7 +99,7 @@ public Task<String> restoreIndices(@ActionParam(PARAM_ASPECT) @Optional @Nonnull
 
   @VisibleForTesting
   static boolean isTaskIdValid(String task) {
-    if (task.matches("^[a-zA-Z0-9]+:[0-9]+$")) {
+    if (task.matches("^[a-zA-Z0-9-_]+:[0-9]+$")) {
       try {
         return Long.parseLong(task.split(":")[1]) != 0;
       } catch (NumberFormatException e) {

File: metadata-service/restli-servlet-impl/src/test/java/com/linkedin/metadata/resources/operations/OperationsResourceTest.java
Patch:
@@ -31,5 +31,8 @@ public void testIsTaskIdValid() {
     assertTrue(OperationsResource.isTaskIdValid("aB1cdEf2GHIJKLMnoPQr3S:123456"));
     assertFalse(OperationsResource.isTaskIdValid("123456:aB1cdEf2GHIJKLMnoPQr3S"));
     assertFalse(OperationsResource.isTaskIdValid(":123"));
+    // node can have a - in it
+    assertTrue(OperationsResource.isTaskIdValid("qhxGdzytQS-pQek8CwBCZg:54654"));
+    assertTrue(OperationsResource.isTaskIdValid("qhxGdzytQSpQek8CwBCZg_:54654"));
   }
 }
\ No newline at end of file

File: metadata-io/src/main/java/com/linkedin/metadata/search/cache/CachingAllEntitiesSearchAggregator.java
Patch:
@@ -9,7 +9,7 @@
 import javax.annotation.Nonnull;
 import javax.annotation.Nullable;
 import lombok.RequiredArgsConstructor;
-import org.javatuples.Quintet;
+import org.javatuples.Sextet;
 import org.springframework.cache.CacheManager;
 
 import static com.datahub.util.RecordUtils.*;
@@ -29,8 +29,8 @@ public SearchResult getSearchResults(List<String> entities, @Nonnull String inpu
     return new CacheableSearcher<>(cacheManager.getCache(ALL_ENTITIES_SEARCH_AGGREGATOR_CACHE_NAME), batchSize,
         querySize -> aggregator.search(entities, input, postFilters, sortCriterion, querySize.getFrom(),
             querySize.getSize(), searchFlags, facets),
-        querySize -> Quintet.with(entities, input, postFilters != null ? toJsonString(postFilters) : null,
-            sortCriterion != null ? toJsonString(sortCriterion) : null, querySize), searchFlags, enableCache)
+        querySize -> Sextet.with(entities, input, postFilters != null ? toJsonString(postFilters) : null,
+            sortCriterion != null ? toJsonString(sortCriterion) : null, facets, querySize), searchFlags, enableCache)
         .getSearchResults(from, size);
   }
 }

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/chart/mappers/ChartMapper.java
Patch:
@@ -82,7 +82,7 @@ public Chart apply(@Nonnull final EntityResponse entityResponse) {
             chart.setStatus(StatusMapper.map(new Status(dataMap))));
         mappingHelper.mapToResult(GLOBAL_TAGS_ASPECT_NAME, (dataset, dataMap) -> this.mapGlobalTags(dataset, dataMap, entityUrn));
         mappingHelper.mapToResult(INSTITUTIONAL_MEMORY_ASPECT_NAME, (chart, dataMap) ->
-            chart.setInstitutionalMemory(InstitutionalMemoryMapper.map(new InstitutionalMemory(dataMap))));
+            chart.setInstitutionalMemory(InstitutionalMemoryMapper.map(new InstitutionalMemory(dataMap), entityUrn)));
         mappingHelper.mapToResult(GLOSSARY_TERMS_ASPECT_NAME, (chart, dataMap) ->
             chart.setGlossaryTerms(GlossaryTermsMapper.map(new GlossaryTerms(dataMap), entityUrn)));
         mappingHelper.mapToResult(CONTAINER_ASPECT_NAME, this::mapContainers);

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/container/mappers/ContainerMapper.java
Patch:
@@ -87,7 +87,7 @@ public static Container map(final EntityResponse entityResponse) {
 
     final EnvelopedAspect envelopedInstitutionalMemory = aspects.get(Constants.INSTITUTIONAL_MEMORY_ASPECT_NAME);
     if (envelopedInstitutionalMemory != null) {
-      result.setInstitutionalMemory(InstitutionalMemoryMapper.map(new InstitutionalMemory(envelopedInstitutionalMemory.getValue().data())));
+      result.setInstitutionalMemory(InstitutionalMemoryMapper.map(new InstitutionalMemory(envelopedInstitutionalMemory.getValue().data()), entityUrn));
     }
 
     final EnvelopedAspect statusAspect = aspects.get(Constants.STATUS_ASPECT_NAME);

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/dashboard/mappers/DashboardMapper.java
Patch:
@@ -79,7 +79,7 @@ public Dashboard apply(@Nonnull final EntityResponse entityResponse) {
         mappingHelper.mapToResult(STATUS_ASPECT_NAME, (dashboard, dataMap) ->
             dashboard.setStatus(StatusMapper.map(new Status(dataMap))));
         mappingHelper.mapToResult(INSTITUTIONAL_MEMORY_ASPECT_NAME, (dashboard, dataMap) ->
-            dashboard.setInstitutionalMemory(InstitutionalMemoryMapper.map(new InstitutionalMemory(dataMap))));
+            dashboard.setInstitutionalMemory(InstitutionalMemoryMapper.map(new InstitutionalMemory(dataMap), entityUrn)));
         mappingHelper.mapToResult(GLOSSARY_TERMS_ASPECT_NAME, (dashboard, dataMap) ->
             dashboard.setGlossaryTerms(GlossaryTermsMapper.map(new GlossaryTerms(dataMap), entityUrn)));
         mappingHelper.mapToResult(CONTAINER_ASPECT_NAME, this::mapContainers);

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/dataflow/mappers/DataFlowMapper.java
Patch:
@@ -70,7 +70,7 @@ public DataFlow apply(@Nonnull final EntityResponse entityResponse) {
             dataFlow.setStatus(StatusMapper.map(new Status(dataMap))));
         mappingHelper.mapToResult(GLOBAL_TAGS_ASPECT_NAME, (dataFlow, dataMap) -> this.mapGlobalTags(dataFlow, dataMap, entityUrn));
         mappingHelper.mapToResult(INSTITUTIONAL_MEMORY_ASPECT_NAME, (dataFlow, dataMap) ->
-            dataFlow.setInstitutionalMemory(InstitutionalMemoryMapper.map(new InstitutionalMemory(dataMap))));
+            dataFlow.setInstitutionalMemory(InstitutionalMemoryMapper.map(new InstitutionalMemory(dataMap), entityUrn)));
         mappingHelper.mapToResult(GLOSSARY_TERMS_ASPECT_NAME, (dataFlow, dataMap) ->
             dataFlow.setGlossaryTerms(GlossaryTermsMapper.map(new GlossaryTerms(dataMap), entityUrn)));
         mappingHelper.mapToResult(DOMAINS_ASPECT_NAME, this::mapDomains);

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/datajob/mappers/DataJobMapper.java
Patch:
@@ -90,7 +90,7 @@ public DataJob apply(@Nonnull final EntityResponse entityResponse) {
                 result.setGlobalTags(globalTags);
                 result.setTags(globalTags);
             } else if (INSTITUTIONAL_MEMORY_ASPECT_NAME.equals(name)) {
-                result.setInstitutionalMemory(InstitutionalMemoryMapper.map(new InstitutionalMemory(data)));
+                result.setInstitutionalMemory(InstitutionalMemoryMapper.map(new InstitutionalMemory(data), entityUrn));
             } else if (GLOSSARY_TERMS_ASPECT_NAME.equals(name)) {
                 result.setGlossaryTerms(GlossaryTermsMapper.map(new GlossaryTerms(data), entityUrn));
             } else if (DOMAINS_ASPECT_NAME.equals(name)) {

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/dataplatforminstance/mappers/DataPlatformInstanceMapper.java
Patch:
@@ -57,7 +57,7 @@ public DataPlatformInstance apply(@Nonnull final EntityResponse entityResponse)
     );
     mappingHelper.mapToResult(Constants.INSTITUTIONAL_MEMORY_ASPECT_NAME,
             (dataPlatformInstance, dataMap) ->
-                    dataPlatformInstance.setInstitutionalMemory(InstitutionalMemoryMapper.map(new InstitutionalMemory(dataMap)))
+                    dataPlatformInstance.setInstitutionalMemory(InstitutionalMemoryMapper.map(new InstitutionalMemory(dataMap), entityUrn))
     );
     mappingHelper.mapToResult(Constants.STATUS_ASPECT_NAME,
             (dataPlatformInstance, dataMap) ->

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/dataproduct/mappers/DataProductMapper.java
Patch:
@@ -60,7 +60,7 @@ public DataProduct apply(@Nonnull final EntityResponse entityResponse) {
     mappingHelper.mapToResult(OWNERSHIP_ASPECT_NAME, (dataProduct, dataMap) ->
         dataProduct.setOwnership(OwnershipMapper.map(new Ownership(dataMap), entityUrn)));
     mappingHelper.mapToResult(INSTITUTIONAL_MEMORY_ASPECT_NAME, (dataProduct, dataMap) ->
-        dataProduct.setInstitutionalMemory(InstitutionalMemoryMapper.map(new InstitutionalMemory(dataMap))));
+        dataProduct.setInstitutionalMemory(InstitutionalMemoryMapper.map(new InstitutionalMemory(dataMap), entityUrn)));
 
     return result;
   }

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/dataset/mappers/DatasetMapper.java
Patch:
@@ -86,7 +86,7 @@ public Dataset apply(@Nonnull final EntityResponse entityResponse) {
         mappingHelper.mapToResult(EDITABLE_DATASET_PROPERTIES_ASPECT_NAME, this::mapEditableDatasetProperties);
         mappingHelper.mapToResult(VIEW_PROPERTIES_ASPECT_NAME, this::mapViewProperties);
         mappingHelper.mapToResult(INSTITUTIONAL_MEMORY_ASPECT_NAME, (dataset, dataMap) ->
-            dataset.setInstitutionalMemory(InstitutionalMemoryMapper.map(new InstitutionalMemory(dataMap))));
+            dataset.setInstitutionalMemory(InstitutionalMemoryMapper.map(new InstitutionalMemory(dataMap), entityUrn)));
         mappingHelper.mapToResult(OWNERSHIP_ASPECT_NAME, (dataset, dataMap) ->
             dataset.setOwnership(OwnershipMapper.map(new Ownership(dataMap), entityUrn)));
         mappingHelper.mapToResult(STATUS_ASPECT_NAME, (dataset, dataMap) ->

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/dataset/mappers/VersionedDatasetMapper.java
Patch:
@@ -75,7 +75,7 @@ public VersionedDataset apply(@Nonnull final EntityResponse entityResponse) {
     mappingHelper.mapToResult(EDITABLE_DATASET_PROPERTIES_ASPECT_NAME, this::mapEditableDatasetProperties);
     mappingHelper.mapToResult(VIEW_PROPERTIES_ASPECT_NAME, this::mapViewProperties);
     mappingHelper.mapToResult(INSTITUTIONAL_MEMORY_ASPECT_NAME, (dataset, dataMap) ->
-        dataset.setInstitutionalMemory(InstitutionalMemoryMapper.map(new InstitutionalMemory(dataMap))));
+        dataset.setInstitutionalMemory(InstitutionalMemoryMapper.map(new InstitutionalMemory(dataMap), entityUrn)));
     mappingHelper.mapToResult(OWNERSHIP_ASPECT_NAME, (dataset, dataMap) ->
         dataset.setOwnership(OwnershipMapper.map(new Ownership(dataMap), entityUrn)));
     mappingHelper.mapToResult(STATUS_ASPECT_NAME, (dataset, dataMap) ->

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/domain/DomainMapper.java
Patch:
@@ -45,7 +45,7 @@ public static Domain map(final EntityResponse entityResponse) {
 
     final EnvelopedAspect envelopedInstitutionalMemory = aspects.get(Constants.INSTITUTIONAL_MEMORY_ASPECT_NAME);
     if (envelopedInstitutionalMemory != null) {
-      result.setInstitutionalMemory(InstitutionalMemoryMapper.map(new InstitutionalMemory(envelopedInstitutionalMemory.getValue().data())));
+      result.setInstitutionalMemory(InstitutionalMemoryMapper.map(new InstitutionalMemory(envelopedInstitutionalMemory.getValue().data()), entityUrn));
     }
 
     return result;

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/glossary/mappers/GlossaryTermMapper.java
Patch:
@@ -60,7 +60,7 @@ public GlossaryTerm apply(@Nonnull final EntityResponse entityResponse) {
       mappingHelper.mapToResult(DEPRECATION_ASPECT_NAME, (glossaryTerm, dataMap) ->
         glossaryTerm.setDeprecation(DeprecationMapper.map(new Deprecation(dataMap))));
       mappingHelper.mapToResult(INSTITUTIONAL_MEMORY_ASPECT_NAME, (dataset, dataMap) ->
-          dataset.setInstitutionalMemory(InstitutionalMemoryMapper.map(new InstitutionalMemory(dataMap))));
+          dataset.setInstitutionalMemory(InstitutionalMemoryMapper.map(new InstitutionalMemory(dataMap), entityUrn)));
 
       // If there's no name property, resort to the legacy name computation.
       if (result.getGlossaryTermInfo() != null && result.getGlossaryTermInfo().getName() == null) {

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/mlmodel/mappers/MLFeatureMapper.java
Patch:
@@ -69,7 +69,7 @@ public MLFeature apply(@Nonnull final EntityResponse entityResponse) {
             mlFeature.setOwnership(OwnershipMapper.map(new Ownership(dataMap), entityUrn)));
         mappingHelper.mapToResult(ML_FEATURE_PROPERTIES_ASPECT_NAME, this::mapMLFeatureProperties);
         mappingHelper.mapToResult(INSTITUTIONAL_MEMORY_ASPECT_NAME, (mlFeature, dataMap) ->
-            mlFeature.setInstitutionalMemory(InstitutionalMemoryMapper.map(new InstitutionalMemory(dataMap))));
+            mlFeature.setInstitutionalMemory(InstitutionalMemoryMapper.map(new InstitutionalMemory(dataMap), entityUrn)));
         mappingHelper.mapToResult(STATUS_ASPECT_NAME, (mlFeature, dataMap) ->
             mlFeature.setStatus(StatusMapper.map(new Status(dataMap))));
         mappingHelper.mapToResult(DEPRECATION_ASPECT_NAME, (mlFeature, dataMap) ->

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/mlmodel/mappers/MLFeatureTableMapper.java
Patch:
@@ -68,7 +68,7 @@ public MLFeatureTable apply(@Nonnull final EntityResponse entityResponse) {
         mappingHelper.mapToResult(ML_FEATURE_TABLE_KEY_ASPECT_NAME, this::mapMLFeatureTableKey);
         mappingHelper.mapToResult(ML_FEATURE_TABLE_PROPERTIES_ASPECT_NAME, (entity, dataMap) -> this.mapMLFeatureTableProperties(entity, dataMap, entityUrn));
         mappingHelper.mapToResult(INSTITUTIONAL_MEMORY_ASPECT_NAME, (mlFeatureTable, dataMap) ->
-            mlFeatureTable.setInstitutionalMemory(InstitutionalMemoryMapper.map(new InstitutionalMemory(dataMap))));
+            mlFeatureTable.setInstitutionalMemory(InstitutionalMemoryMapper.map(new InstitutionalMemory(dataMap), entityUrn)));
         mappingHelper.mapToResult(STATUS_ASPECT_NAME, (mlFeatureTable, dataMap) ->
             mlFeatureTable.setStatus(StatusMapper.map(new Status(dataMap))));
         mappingHelper.mapToResult(DEPRECATION_ASPECT_NAME, (mlFeatureTable, dataMap) ->

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/mlmodel/mappers/MLModelMapper.java
Patch:
@@ -101,7 +101,7 @@ public MLModel apply(@Nonnull final EntityResponse entityResponse) {
         mappingHelper.mapToResult(CAVEATS_AND_RECOMMENDATIONS_ASPECT_NAME, (mlModel, dataMap) ->
             mlModel.setCaveatsAndRecommendations(CaveatsAndRecommendationsMapper.map(new CaveatsAndRecommendations(dataMap))));
         mappingHelper.mapToResult(INSTITUTIONAL_MEMORY_ASPECT_NAME, (mlModel, dataMap) ->
-            mlModel.setInstitutionalMemory(InstitutionalMemoryMapper.map(new InstitutionalMemory(dataMap))));
+            mlModel.setInstitutionalMemory(InstitutionalMemoryMapper.map(new InstitutionalMemory(dataMap), entityUrn)));
         mappingHelper.mapToResult(SOURCE_CODE_ASPECT_NAME, this::mapSourceCode);
         mappingHelper.mapToResult(STATUS_ASPECT_NAME, (mlModel, dataMap) ->
             mlModel.setStatus(StatusMapper.map(new Status(dataMap))));

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/mlmodel/mappers/MLPrimaryKeyMapper.java
Patch:
@@ -65,7 +65,7 @@ public MLPrimaryKey apply(@Nonnull final EntityResponse entityResponse) {
         mappingHelper.mapToResult(ML_PRIMARY_KEY_KEY_ASPECT_NAME, this::mapMLPrimaryKeyKey);
         mappingHelper.mapToResult(ML_PRIMARY_KEY_PROPERTIES_ASPECT_NAME, this::mapMLPrimaryKeyProperties);
         mappingHelper.mapToResult(INSTITUTIONAL_MEMORY_ASPECT_NAME, (mlPrimaryKey, dataMap) ->
-            mlPrimaryKey.setInstitutionalMemory(InstitutionalMemoryMapper.map(new InstitutionalMemory(dataMap))));
+            mlPrimaryKey.setInstitutionalMemory(InstitutionalMemoryMapper.map(new InstitutionalMemory(dataMap), entityUrn)));
         mappingHelper.mapToResult(STATUS_ASPECT_NAME, (mlPrimaryKey, dataMap) ->
             mlPrimaryKey.setStatus(StatusMapper.map(new Status(dataMap))));
         mappingHelper.mapToResult(DEPRECATION_ASPECT_NAME, (mlPrimaryKey, dataMap) ->

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/notebook/mappers/NotebookMapper.java
Patch:
@@ -74,7 +74,7 @@ public Notebook apply(EntityResponse response) {
     mappingHelper.mapToResult(GLOBAL_TAGS_ASPECT_NAME, (notebook, dataMap) ->
         notebook.setTags(GlobalTagsMapper.map(new GlobalTags(dataMap), entityUrn)));
     mappingHelper.mapToResult(INSTITUTIONAL_MEMORY_ASPECT_NAME, (notebook, dataMap) -> 
-      notebook.setInstitutionalMemory(InstitutionalMemoryMapper.map(new InstitutionalMemory(dataMap))));
+      notebook.setInstitutionalMemory(InstitutionalMemoryMapper.map(new InstitutionalMemory(dataMap), entityUrn)));
     mappingHelper.mapToResult(DOMAINS_ASPECT_NAME, this::mapDomains);
     mappingHelper.mapToResult(SUB_TYPES_ASPECT_NAME, this::mapSubTypes);
     mappingHelper.mapToResult(GLOSSARY_TERMS_ASPECT_NAME, (notebook, dataMap) -> 

File: metadata-jobs/mae-consumer/src/test/java/com/linkedin/metadata/kafka/hook/GraphIndexUtilsTest.java
Patch:
@@ -17,7 +17,6 @@
 import com.linkedin.metadata.models.EntitySpec;
 import com.linkedin.metadata.models.RelationshipFieldSpec;
 import com.linkedin.metadata.models.extractor.FieldExtractor;
-import com.linkedin.metadata.models.registry.ConfigEntityRegistry;
 import com.linkedin.metadata.models.registry.EntityRegistry;
 import com.linkedin.metadata.utils.GenericRecordUtils;
 import com.linkedin.mxe.MetadataChangeLog;
@@ -31,6 +30,7 @@
 import org.testng.annotations.Test;
 
 import static com.linkedin.metadata.graph.GraphIndexUtils.*;
+import static com.linkedin.metadata.kafka.hook.EntityRegistryTestUtil.ENTITY_REGISTRY;
 import static org.testng.Assert.*;
 
 
@@ -57,8 +57,7 @@ public void setupTest() {
     _datasetUrn = UrnUtils.getUrn("urn:li:dataset:(urn:li:dataPlatform:bigquery,my-proj.jaffle_shop.customers,PROD)");
     _upstreamDataset1 = UrnUtils.toDatasetUrn("snowflake", "test", "DEV");
     _upstreamDataset2 = UrnUtils.toDatasetUrn("snowflake", "test2", "DEV");
-    _mockRegistry = new ConfigEntityRegistry(
-        UpdateIndicesHookTest.class.getClassLoader().getResourceAsStream("test-entity-registry.yml"));
+    _mockRegistry = ENTITY_REGISTRY;
   }
 
   @Test

File: metadata-auth/auth-api/src/main/java/com/datahub/authorization/ResolvedResourceSpec.java
Patch:
@@ -6,12 +6,14 @@
 import javax.annotation.Nullable;
 import lombok.Getter;
 import lombok.RequiredArgsConstructor;
+import lombok.ToString;
 
 
 /**
  * Wrapper around authorization request with field resolvers for lazily fetching the field values for each field type
  */
 @RequiredArgsConstructor
+@ToString
 public class ResolvedResourceSpec {
   @Getter
   private final ResourceSpec spec;

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/domain/CreateDomainResolver.java
Patch:
@@ -74,7 +74,7 @@ public CompletableFuture<String> get(DataFetchingEnvironment environment) throws
 
         String domainUrn = _entityClient.ingestProposal(proposal, context.getAuthentication());
         OwnershipType ownershipType = OwnershipType.TECHNICAL_OWNER;
-        if (!_entityService.exists(UrnUtils.getUrn(mapOwnershipTypeToEntity(ownershipType)))) {
+        if (!_entityService.exists(UrnUtils.getUrn(mapOwnershipTypeToEntity(ownershipType.name())))) {
           log.warn("Technical owner does not exist, defaulting to None ownership.");
           ownershipType = OwnershipType.NONE;
         }

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/glossary/CreateGlossaryNodeResolver.java
Patch:
@@ -69,7 +69,7 @@ public CompletableFuture<String> get(DataFetchingEnvironment environment) throws
           String glossaryNodeUrn = _entityClient.ingestProposal(proposal, context.getAuthentication());
 
           OwnershipType ownershipType = OwnershipType.TECHNICAL_OWNER;
-          if (!_entityService.exists(UrnUtils.getUrn(mapOwnershipTypeToEntity(ownershipType)))) {
+          if (!_entityService.exists(UrnUtils.getUrn(mapOwnershipTypeToEntity(ownershipType.name())))) {
             log.warn("Technical owner does not exist, defaulting to None ownership.");
             ownershipType = OwnershipType.NONE;
           }

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/glossary/CreateGlossaryTermResolver.java
Patch:
@@ -68,7 +68,7 @@ public CompletableFuture<String> get(DataFetchingEnvironment environment) throws
 
           String glossaryTermUrn = _entityClient.ingestProposal(proposal, context.getAuthentication());
           OwnershipType ownershipType = OwnershipType.TECHNICAL_OWNER;
-          if (!_entityService.exists(UrnUtils.getUrn(mapOwnershipTypeToEntity(ownershipType)))) {
+          if (!_entityService.exists(UrnUtils.getUrn(mapOwnershipTypeToEntity(ownershipType.name())))) {
             log.warn("Technical owner does not exist, defaulting to None ownership.");
             ownershipType = OwnershipType.NONE;
           }

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/mutate/AddOwnerResolver.java
Patch:
@@ -36,7 +36,7 @@ public CompletableFuture<Boolean> get(DataFetchingEnvironment environment) throw
     Urn ownerUrn = Urn.createFromString(input.getOwnerUrn());
     OwnerEntityType ownerEntityType = input.getOwnerEntityType();
     OwnershipType type = input.getType() == null ? OwnershipType.NONE : input.getType();
-    String ownershipUrn = input.getOwnershipTypeUrn() == null ? mapOwnershipTypeToEntity(type) : input.getOwnershipTypeUrn();
+    String ownershipUrn = input.getOwnershipTypeUrn() == null ? mapOwnershipTypeToEntity(type.name()) : input.getOwnershipTypeUrn();
     Urn targetUrn = Urn.createFromString(input.getResourceUrn());
 
     if (!OwnerUtils.isAuthorizedToUpdateOwners(environment.getContext(), targetUrn)) {

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/tag/CreateTagResolver.java
Patch:
@@ -73,7 +73,7 @@ public CompletableFuture<String> get(DataFetchingEnvironment environment) throws
 
         String tagUrn = _entityClient.ingestProposal(proposal, context.getAuthentication());
         OwnershipType ownershipType = OwnershipType.TECHNICAL_OWNER;
-        if (!_entityService.exists(UrnUtils.getUrn(mapOwnershipTypeToEntity(ownershipType)))) {
+        if (!_entityService.exists(UrnUtils.getUrn(mapOwnershipTypeToEntity(ownershipType.name())))) {
           log.warn("Technical owner does not exist, defaulting to None ownership.");
           ownershipType = OwnershipType.NONE;
         }

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/common/mappers/OwnerMapper.java
Patch:
@@ -33,7 +33,7 @@ public Owner apply(@Nonnull final com.linkedin.common.Owner owner, @Nonnull fina
 
         if (owner.getTypeUrn() == null) {
             OwnershipType ownershipType = OwnershipType.valueOf(owner.getType().toString());
-            owner.setTypeUrn(UrnUtils.getUrn(mapOwnershipTypeToEntity(ownershipType)));
+            owner.setTypeUrn(UrnUtils.getUrn(mapOwnershipTypeToEntity(ownershipType.name())));
         }
 
         if (owner.getTypeUrn() != null) {

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/tag/mappers/TagUpdateInputMapper.java
Patch:
@@ -8,6 +8,7 @@
 import com.linkedin.common.OwnershipSourceType;
 import com.linkedin.common.OwnershipType;
 import com.linkedin.common.urn.Urn;
+import com.linkedin.common.urn.UrnUtils;
 import com.linkedin.data.template.SetMode;
 import com.linkedin.datahub.graphql.generated.TagUpdateInput;
 import com.linkedin.datahub.graphql.types.common.mappers.util.UpdateMappingHelper;
@@ -18,6 +19,7 @@
 import java.util.Collection;
 import javax.annotation.Nonnull;
 
+import static com.linkedin.datahub.graphql.resolvers.mutate.util.OwnerUtils.*;
 import static com.linkedin.metadata.Constants.*;
 
 
@@ -47,6 +49,7 @@ public Collection<MetadataChangeProposal> apply(
     final Owner owner = new Owner();
     owner.setOwner(actor);
     owner.setType(OwnershipType.NONE);
+    owner.setTypeUrn(UrnUtils.getUrn(mapOwnershipTypeToEntity(OwnershipType.NONE.name())));
     owner.setSource(new OwnershipSource().setType(OwnershipSourceType.SERVICE));
     ownership.setOwners(new OwnerArray(owner));
     ownership.setLastModified(auditStamp);

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/group/EntityCountsResolver.java
Patch:
@@ -38,7 +38,7 @@ public CompletableFuture<EntityCountResults> get(final DataFetchingEnvironment e
         try {
           // First, get all counts
           Map<String, Long> gmsResult = _entityClient.batchGetTotalEntityCount(
-              input.getTypes().stream().map(type -> EntityTypeMapper.getName(type)).collect(Collectors.toList()), context.getAuthentication());
+              input.getTypes().stream().map(EntityTypeMapper::getName).collect(Collectors.toList()), context.getAuthentication());
 
           // bind to a result.
           List<EntityCountResult> resultList = gmsResult.entrySet().stream().map(entry -> {

File: metadata-io/src/main/java/com/linkedin/metadata/shared/ValidationUtils.java
Patch:
@@ -150,7 +150,8 @@ public static EntityLineageResult validateEntityLineageResult(@Nullable final En
         .collect(Collectors.toCollection(LineageRelationshipArray::new));
 
     validatedEntityLineageResult.setFiltered(
-        entityLineageResult.getRelationships().size() - validatedRelationships.size());
+        (entityLineageResult.hasFiltered() && entityLineageResult.getFiltered() != null ? entityLineageResult.getFiltered() : 0)
+            + entityLineageResult.getRelationships().size() - validatedRelationships.size());
     validatedEntityLineageResult.setRelationships(validatedRelationships);
 
     return validatedEntityLineageResult;

File: datahub-upgrade/src/main/java/com/linkedin/datahub/upgrade/common/steps/ClearGraphServiceStep.java
Patch:
@@ -49,7 +49,7 @@ public Function<UpgradeContext, UpgradeStepResult> executable() {
       try {
         _graphService.clear();
       } catch (Exception e) {
-        context.report().addLine(String.format("Failed to clear graph indices: %s", e.toString()));
+        context.report().addLine("Failed to clear graph indices", e);
         return new DefaultUpgradeStepResult(id(), UpgradeStepResult.Result.FAILED);
       }
       return new DefaultUpgradeStepResult(id(), UpgradeStepResult.Result.SUCCEEDED);

File: datahub-upgrade/src/main/java/com/linkedin/datahub/upgrade/common/steps/ClearSearchServiceStep.java
Patch:
@@ -47,7 +47,7 @@ public Function<UpgradeContext, UpgradeStepResult> executable() {
       try {
         _entitySearchService.clear();
       } catch (Exception e) {
-        context.report().addLine(String.format("Failed to clear search service: %s", e.toString()));
+        context.report().addLine("Failed to clear search service", e);
         return new DefaultUpgradeStepResult(id(), UpgradeStepResult.Result.FAILED);
       }
       return new DefaultUpgradeStepResult(id(), UpgradeStepResult.Result.SUCCEEDED);

File: datahub-upgrade/src/main/java/com/linkedin/datahub/upgrade/nocode/CreateAspectTableStep.java
Patch:
@@ -76,7 +76,7 @@ public Function<UpgradeContext, UpgradeStepResult> executable() {
       try {
         _server.execute(_server.createSqlUpdate(sqlUpdateStr));
       } catch (Exception e) {
-        context.report().addLine(String.format("Failed to create table metadata_aspect_v2: %s", e.toString()));
+        context.report().addLine("Failed to create table metadata_aspect_v2", e);
         return new DefaultUpgradeStepResult(
             id(),
             UpgradeStepResult.Result.FAILED);

File: datahub-upgrade/src/main/java/com/linkedin/datahub/upgrade/nocode/UpgradeQualificationStep.java
Patch:
@@ -45,7 +45,7 @@ public Function<UpgradeContext, UpgradeStepResult> executable() {
         context.report().addLine("Failed to qualify upgrade candidate. Aborting the upgrade...");
         return new DefaultUpgradeStepResult(id(), UpgradeStepResult.Result.SUCCEEDED, UpgradeStepResult.Action.ABORT);
       } catch (Exception e) {
-        context.report().addLine(String.format("Failed to check if metadata_aspect_v2 table exists: %s", e.toString()));
+        context.report().addLine("Failed to check if metadata_aspect_v2 table exists", e);
         return new DefaultUpgradeStepResult(id(), UpgradeStepResult.Result.FAILED);
       }
     };

File: datahub-upgrade/src/main/java/com/linkedin/datahub/upgrade/nocodecleanup/DeleteAspectTableStep.java
Patch:
@@ -33,7 +33,7 @@ public Function<UpgradeContext, UpgradeStepResult> executable() {
       try {
         _server.execute(_server.createSqlUpdate("DROP TABLE IF EXISTS metadata_aspect;"));
       } catch (Exception e) {
-        context.report().addLine(String.format("Failed to delete data from legacy table metadata_aspect: %s", e.toString()));
+        context.report().addLine("Failed to delete data from legacy table metadata_aspect", e);
         return new DefaultUpgradeStepResult(
             id(),
             UpgradeStepResult.Result.FAILED);

File: datahub-upgrade/src/main/java/com/linkedin/datahub/upgrade/nocodecleanup/DeleteLegacyGraphRelationshipsStep.java
Patch:
@@ -36,7 +36,7 @@ public Function<UpgradeContext, UpgradeStepResult> executable() {
       try {
         ((Neo4jGraphService) _graphClient).removeNodesMatchingLabel(deletePattern);
       } catch (Exception e) {
-        context.report().addLine(String.format("Failed to delete legacy data from graph: %s", e.toString()));
+        context.report().addLine("Failed to delete legacy data from graph", e);
         return new DefaultUpgradeStepResult(
             id(),
             UpgradeStepResult.Result.FAILED);

File: datahub-upgrade/src/main/java/com/linkedin/datahub/upgrade/nocodecleanup/DeleteLegacySearchIndicesStep.java
Patch:
@@ -42,7 +42,7 @@ public Function<UpgradeContext, UpgradeStepResult> executable() {
       try {
         _searchClient.indices().delete(request, RequestOptions.DEFAULT);
       } catch (Exception e) {
-        context.report().addLine(String.format("Failed to delete legacy search index: %s", e.toString()));
+        context.report().addLine("Failed to delete legacy search index: %s", e);
         return new DefaultUpgradeStepResult(id(), UpgradeStepResult.Result.FAILED);
       }
       return new DefaultUpgradeStepResult(id(), UpgradeStepResult.Result.SUCCEEDED);

File: datahub-upgrade/src/main/java/com/linkedin/datahub/upgrade/nocodecleanup/NoCodeUpgradeQualificationStep.java
Patch:
@@ -46,7 +46,7 @@ public Function<UpgradeContext, UpgradeStepResult> executable() {
               UpgradeStepResult.Result.SUCCEEDED);
         }
       } catch (Exception e) {
-        context.report().addLine(String.format("Failed to check if metadata_aspect_v2 table exists: %s", e.toString()));
+        context.report().addLine("Failed to check if metadata_aspect_v2 table exists: %s", e);
         return new DefaultUpgradeStepResult(
             id(),
             UpgradeStepResult.Result.FAILED);

File: metadata-io/src/main/java/com/linkedin/metadata/search/client/CachingEntitySearchService.java
Patch:
@@ -251,12 +251,13 @@ public ScrollResult getCachedScrollResults(
             filters != null ? toJsonString(filters) : null,
             sortCriterion != null ? toJsonString(sortCriterion) : null,
             scrollId, size);
-        result = cache.get(cacheKey, ScrollResult.class);
+        String json = cache.get(cacheKey, String.class);
+        result = json != null ? toRecordTemplate(ScrollResult.class, json) : null;
         cacheAccess.stop();
         if (result == null) {
           Timer.Context cacheMiss = MetricUtils.timer(this.getClass(), "scroll_cache_miss").time();
           result = getRawScrollResults(entities, query, filters, sortCriterion, scrollId, keepAlive, size, isFullText);
-          cache.put(cacheKey, result);
+          cache.put(cacheKey, toJsonString(result));
           cacheMiss.stop();
           MetricUtils.counter(this.getClass(), "scroll_cache_miss_count").inc();
         }

File: metadata-service/factories/src/test/java/com/linkedin/gms/factory/search/CacheTest.java
Patch:
@@ -19,6 +19,7 @@
 import com.linkedin.metadata.query.filter.Filter;
 import com.linkedin.metadata.query.filter.SortCriterion;
 import com.linkedin.metadata.search.EntityLineageResultCacheKey;
+import com.linkedin.metadata.search.ScrollResult;
 import com.linkedin.metadata.search.SearchEntity;
 import com.linkedin.metadata.search.SearchEntityArray;
 import com.linkedin.metadata.search.SearchResult;
@@ -92,10 +93,9 @@ public void hazelcastTest() {
     public void hazelcastTestScroll() {
         CorpuserUrn corpuserUrn = new CorpuserUrn("user");
         SearchEntity searchEntity = new SearchEntity().setEntity(corpuserUrn);
-        SearchResult searchResult = new SearchResult()
+        ScrollResult scrollResult = new ScrollResult()
             .setEntities(new SearchEntityArray(List.of(searchEntity)))
             .setNumEntities(1)
-            .setFrom(0)
             .setPageSize(1)
             .setMetadata(new SearchResultMetadata());
 
@@ -126,7 +126,7 @@ public void hazelcastTestScroll() {
         Cache cache2 = cacheManager2.getCache(ENTITY_SEARCH_SERVICE_SCROLL_CACHE_NAME);
 
         // Cache result
-        String json = toJsonString(searchResult);
+        String json = toJsonString(scrollResult);
         cache1.put(sextet, json);
         Assert.assertEquals(instance1.getMap(ENTITY_SEARCH_SERVICE_SCROLL_CACHE_NAME).get(sextet),
             instance2.getMap(ENTITY_SEARCH_SERVICE_SCROLL_CACHE_NAME).get(sextet));

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/Constants.java
Patch:
@@ -22,4 +22,7 @@ public class Constants {
     public static final String LINEAGE_SCHEMA_FILE = "lineage.graphql";
     public static final String BROWSE_PATH_DELIMITER = "/";
     public static final String VERSION_STAMP_FIELD_NAME = "versionStamp";
+
+    public static final String ENTITY_FILTER_NAME = "_entityType";
+
 }

File: metadata-io/src/main/java/com/linkedin/metadata/search/utils/FilterUtils.java
Patch:
@@ -15,7 +15,7 @@ private FilterUtils() {
   }
 
   private static final List<String> FILTER_RANKING =
-      ImmutableList.of("entity", "typeNames", "platform", "domains", "tags", "glossaryTerms", "container", "owners",
+      ImmutableList.of("_entityType", "typeNames", "platform", "domains", "tags", "glossaryTerms", "container", "owners",
           "origin");
 
   public static List<AggregationMetadata> rankFilterGroups(Map<String, AggregationMetadata> aggregations) {

File: metadata-io/src/test/java/com/linkedin/metadata/search/elasticsearch/fixtures/SampleDataFixtureTests.java
Patch:
@@ -713,9 +713,9 @@ public void testNestedAggregation() {
                         .map(AggregationMetadata::getName).collect(Collectors.toList())));
         });
 
-        expectedFacets = Set.of("platform", "typeNames", "_entityType");
+        expectedFacets = Set.of("platform", "typeNames", "_entityType", "entity");
         SearchResult testResult2 = search(searchService, "cypress", List.copyOf(expectedFacets));
-        assertEquals(testResult2.getMetadata().getAggregations().size(), 3);
+        assertEquals(testResult2.getMetadata().getAggregations().size(), 4);
         expectedFacets.forEach(facet -> {
             assertTrue(testResult2.getMetadata().getAggregations().stream().anyMatch(agg -> agg.getName().equals(facet)),
                 String.format("Failed to find facet `%s` in %s", facet,

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/GmsGraphQLEngineArgs.java
Patch:
@@ -24,6 +24,7 @@
 import com.linkedin.metadata.secret.SecretService;
 import com.linkedin.metadata.service.DataProductService;
 import com.linkedin.metadata.service.LineageService;
+import com.linkedin.metadata.service.OwnershipTypeService;
 import com.linkedin.metadata.service.QueryService;
 import com.linkedin.metadata.service.SettingsService;
 import com.linkedin.metadata.service.ViewService;
@@ -64,6 +65,7 @@ public class GmsGraphQLEngineArgs {
     InviteTokenService inviteTokenService;
     PostService postService;
     ViewService viewService;
+    OwnershipTypeService ownershipTypeService;
     SettingsService settingsService;
     LineageService lineageService;
     QueryService queryService;

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/MeResolver.java
Patch:
@@ -73,6 +73,7 @@ public CompletableFuture<AuthenticatedUser> get(DataFetchingEnvironment environm
         platformPrivileges.setCreateTags(AuthorizationUtils.canCreateTags(context));
         platformPrivileges.setManageTags(AuthorizationUtils.canManageTags(context));
         platformPrivileges.setManageGlobalViews(AuthorizationUtils.canManageGlobalViews(context));
+        platformPrivileges.setManageOwnershipTypes(AuthorizationUtils.canManageOwnershipTypes(context));
 
         // Construct and return authenticated user object.
         final AuthenticatedUser authUser = new AuthenticatedUser();

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/mutate/AddOwnersResolver.java
Patch:
@@ -46,7 +46,7 @@ public CompletableFuture<Boolean> get(DataFetchingEnvironment environment) throw
       );
       try {
 
-        log.debug("Adding Owners. input: {}", input.toString());
+        log.debug("Adding Owners. input: {}", input);
 
         Urn actor = CorpuserUrn.createFromString(((QueryContext) environment.getContext()).getActorUrn());
         OwnerUtils.addOwnersToResources(
@@ -57,8 +57,8 @@ public CompletableFuture<Boolean> get(DataFetchingEnvironment environment) throw
         );
         return true;
       } catch (Exception e) {
-        log.error("Failed to add owners to resource with input {}, {}", input.toString(), e.getMessage());
-        throw new RuntimeException(String.format("Failed to add owners to resource with input %s", input.toString()), e);
+        log.error("Failed to add owners to resource with input {}, {}", input, e.getMessage());
+        throw new RuntimeException(String.format("Failed to add owners to resource with input %s", input), e);
       }
     });
   }

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/mutate/BatchAddOwnersResolver.java
Patch:
@@ -46,14 +46,15 @@ public CompletableFuture<Boolean> get(DataFetchingEnvironment environment) throw
         return true;
       } catch (Exception e) {
         log.error("Failed to perform update against input {}, {}", input.toString(), e.getMessage());
-        throw new RuntimeException(String.format("Failed to perform update against input %s", input.toString()), e);
+        throw new RuntimeException(String.format("Failed to perform update against input %s", input), e);
       }
     });
   }
 
   private void validateOwners(List<OwnerInput> owners) {
     for (OwnerInput ownerInput : owners) {
-      OwnerUtils.validateOwner(UrnUtils.getUrn(ownerInput.getOwnerUrn()), ownerInput.getOwnerEntityType(), _entityService);
+      OwnerUtils.validateOwner(UrnUtils.getUrn(ownerInput.getOwnerUrn()), ownerInput.getOwnerEntityType(),
+          UrnUtils.getUrn(ownerInput.getOwnershipTypeUrn()), _entityService);
     }
   }
 

File: metadata-utils/src/main/java/com/linkedin/metadata/utils/elasticsearch/IndexConvention.java
Patch:
@@ -4,7 +4,6 @@
 import com.linkedin.metadata.models.EntitySpec;
 import java.util.Optional;
 import javax.annotation.Nonnull;
-import javax.annotation.Nullable;
 
 
 /**

File: metadata-utils/src/main/java/com/linkedin/metadata/utils/elasticsearch/IndexConventionImpl.java
Patch:
@@ -36,7 +36,7 @@ private String createIndexName(String baseName) {
   }
 
   private Optional<String> extractEntityName(String indexName) {
-    String prefixString =_prefix.map(prefix -> prefix + "_").orElse("") ;
+    String prefixString = _prefix.map(prefix -> prefix + "_").orElse("");
     if (!indexName.startsWith(prefixString)) {
       return Optional.empty();
     }
@@ -97,7 +97,6 @@ public String getAllTimeseriesAspectIndicesPattern() {
     return _getAllTimeseriesIndicesPattern;
   }
 
-  @Nullable
   @Override
   public Optional<String> getEntityName(String indexName) {
     return extractEntityName(indexName);

File: metadata-utils/src/test/java/com/linkedin/metadata/utils/elasticsearch/IndexConventionImplTest.java
Patch:
@@ -6,7 +6,7 @@
 import static org.testng.Assert.*;
 
 
-public class IndexConventionImplTest{
+public class IndexConventionImplTest {
 
   @Test
   public void testIndexConventionNoPrefix() {

File: metadata-io/src/main/java/com/linkedin/metadata/client/JavaEntityClient.java
Patch:
@@ -376,7 +376,7 @@ public LineageScrollResult scrollAcrossLineage(@Nonnull Urn sourceUrn, @Nonnull
         @Nullable Long startTimeMillis, @Nullable Long endTimeMillis, @Nullable SearchFlags searchFlags,
         @Nonnull final Authentication authentication)
         throws RemoteInvocationException {
-        final SearchFlags finalFlags = searchFlags != null ? searchFlags : new SearchFlags().setSkipCache(true);
+        final SearchFlags finalFlags = searchFlags != null ? searchFlags : new SearchFlags().setFulltext(true).setSkipCache(true);
         return ValidationUtils.validateLineageScrollResult(
             _lineageSearchService.scrollAcrossLineage(sourceUrn, direction, entities, input, maxHops, filter,
                 sortCriterion, scrollId, keepAlive, count, startTimeMillis, endTimeMillis, finalFlags), _entityService);

File: metadata-service/auth-impl/src/main/java/com/datahub/authentication/user/NativeUserService.java
Patch:
@@ -44,8 +44,8 @@ public void createNativeUser(@Nonnull String userUrnString, @Nonnull String full
     Objects.requireNonNull(password, "password must not be null!");
     Objects.requireNonNull(authentication, "authentication must not be null!");
 
-    Urn userUrn = Urn.createFromString(userUrnString);
-    if (_entityService.exists(userUrn)) {
+    final Urn userUrn = Urn.createFromString(userUrnString);
+    if (_entityService.exists(userUrn) || userUrn.toString().equals(SYSTEM_ACTOR)) {
       throw new RuntimeException("This user already exists! Cannot create a new user.");
     }
     updateCorpUserInfo(userUrn, fullName, email, title, authentication);

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/search/ElasticSearchServiceFactory.java
Patch:
@@ -58,7 +58,7 @@ protected ElasticSearchService getInstance(ConfigurationProvider configurationPr
     ElasticSearchConfiguration elasticSearchConfiguration = configurationProvider.getElasticSearch();
     SearchConfiguration searchConfiguration = elasticSearchConfiguration.getSearch();
     CustomSearchConfiguration customSearchConfiguration = searchConfiguration.getCustom() == null ? null
-            : searchConfiguration.getCustom().customSearchConfiguration(YAML_MAPPER);
+            : searchConfiguration.getCustom().resolve(YAML_MAPPER);
 
     ESSearchDAO esSearchDAO =
         new ESSearchDAO(entityRegistry, components.getSearchClient(), components.getIndexConvention(),

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/GmsGraphQLEngine.java
Patch:
@@ -819,13 +819,13 @@ private void configureMutationResolvers(final RuntimeWiring.Builder builder) {
             .dataFetcher("createPolicy", new UpsertPolicyResolver(this.entityClient))
             .dataFetcher("updatePolicy", new UpsertPolicyResolver(this.entityClient))
             .dataFetcher("deletePolicy", new DeletePolicyResolver(this.entityClient))
-            .dataFetcher("updateDescription", new UpdateDescriptionResolver(entityService))
+            .dataFetcher("updateDescription", new UpdateDescriptionResolver(entityService, this.entityClient))
             .dataFetcher("addOwner", new AddOwnerResolver(entityService))
             .dataFetcher("addOwners", new AddOwnersResolver(entityService))
             .dataFetcher("batchAddOwners", new BatchAddOwnersResolver(entityService))
             .dataFetcher("removeOwner", new RemoveOwnerResolver(entityService))
             .dataFetcher("batchRemoveOwners", new BatchRemoveOwnersResolver(entityService))
-            .dataFetcher("addLink", new AddLinkResolver(entityService))
+            .dataFetcher("addLink", new AddLinkResolver(entityService, this.entityClient))
             .dataFetcher("removeLink", new RemoveLinkResolver(entityService))
             .dataFetcher("addGroupMembers", new AddGroupMembersResolver(this.groupService))
             .dataFetcher("removeGroupMembers", new RemoveGroupMembersResolver(this.groupService))

File: metadata-io/src/main/java/com/linkedin/metadata/datahubusage/DataHubUsageEventType.java
Patch:
@@ -24,6 +24,8 @@ public enum DataHubUsageEventType {
   RECOMMENDATION_CLICK_EVENT("RecommendationClickEvent"),
   HOME_PAGE_RECOMMENDATION_CLICK_EVENT("HomePageRecommendationClickEvent"),
   HOME_PAGE_EXPLORE_ALL_CLICK_EVENT("HomePageExploreAllClickEvent"),
+  SEARCH_BAR_EXPLORE_ALL_CLICK_EVENT("SearchBarExploreAllClickEvent"),
+  SEARCH_RESULTS_EXPLORE_ALL_CLICK_EVENT("SearchResultsExploreAllClickEvent"),
   SEARCH_ACROSS_LINEAGE_EVENT("SearchAcrossLineageEvent"),
   SEARCH_ACROSS_LINEAGE_RESULTS_VIEW_EVENT("SearchAcrossLineageResultsViewEvent"),
   DOWNLOAD_AS_CSV_EVENT("DownloadAsCsvEvent"),

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/ResolverUtils.java
Patch:
@@ -31,7 +31,8 @@
 public class ResolverUtils {
 
     private static final Set<String> KEYWORD_EXCLUDED_FILTERS = ImmutableSet.of(
-        "runId"
+        "runId",
+        "_entityType"
     );
     private static final ObjectMapper MAPPER = new ObjectMapper();
 

File: metadata-io/src/main/java/com/linkedin/metadata/search/elasticsearch/query/ESSearchDAO.java
Patch:
@@ -296,7 +296,7 @@ private static ConjunctiveCriterion transformConjunctiveCriterion(ConjunctiveCri
       IndexConvention indexConvention) {
     return new ConjunctiveCriterion().setAnd(
         conjunctiveCriterion.getAnd().stream().map(
-                criterion -> criterion.getField().equals("_entityType")
+                criterion -> criterion.getField().equalsIgnoreCase("_entityType")
                     ? transformEntityTypeCriterion(criterion, indexConvention)
                     : criterion)
             .collect(Collectors.toCollection(CriterionArray::new)));

File: metadata-io/src/main/java/com/linkedin/metadata/search/elasticsearch/query/request/SearchFieldConfig.java
Patch:
@@ -21,7 +21,7 @@
 public class SearchFieldConfig {
     public static final float DEFAULT_BOOST = 1.0f;
 
-    public static final Set<String> KEYWORD_FIELDS = Set.of("urn", "runId");
+    public static final Set<String> KEYWORD_FIELDS = Set.of("urn", "runId", "_index");
 
     // These should not be used directly since there is a specific
     // order in which these rules need to be evaluated for exceptions to

File: metadata-jobs/mae-consumer/src/main/java/com/linkedin/metadata/kafka/elasticsearch/ElasticsearchConnector.java
Patch:
@@ -23,6 +23,9 @@ public ElasticsearchConnector(ESBulkProcessor bulkProcessor, int numRetries) {
     _numRetries = numRetries;
   }
 
+  /*
+    Be careful here, we are mixing `DataHub` change type semantics with `Elasticsearch` concepts.
+  */
   public void feedElasticEvent(@Nonnull ElasticEvent event) {
     if (event.getActionType().equals(ChangeType.DELETE)) {
       _bulkProcessor.add(createDeleteRequest(event));

File: metadata-service/factories/src/main/java/com/linkedin/metadata/boot/kafka/DataHubUpgradeKafkaListener.java
Patch:
@@ -84,6 +84,9 @@ public void checkSystemVersion(final ConsumerRecord<String, GenericRecord> consu
       log.info("Latest system update version: {}", event.getVersion());
       if (expectedVersion.equals(event.getVersion())) {
         IS_UPDATED.getAndSet(true);
+      } else if (!_configurationProvider.getSystemUpdate().isWaitForSystemUpdate()) {
+        log.warn("Wait for system update is disabled. Proceeding with startup.");
+        IS_UPDATED.getAndSet(true);
       } else {
         log.warn("System version is not up to date: {}. Waiting for datahub-upgrade to complete...", expectedVersion);
       }

File: metadata-io/src/test/java/com/linkedin/metadata/TestEntityUtil.java
Patch:
@@ -1,6 +1,6 @@
 package com.linkedin.metadata;
 
-import com.datahub.test.BrowsePaths;
+import com.datahub.test.TestBrowsePaths;
 import com.datahub.test.KeyPartEnum;
 import com.datahub.test.SearchFeatures;
 import com.datahub.test.SimpleNestedRecord1;
@@ -52,7 +52,7 @@ public static TestEntitySnapshot getSnapshot() {
     Urn urn = getTestEntityUrn();
     snapshot.setUrn(urn);
 
-    BrowsePaths browsePaths = new BrowsePaths().setPaths(new StringArray(ImmutableList.of("/a/b/c", "d/e/f")));
+    TestBrowsePaths browsePaths = new TestBrowsePaths().setPaths(new StringArray(ImmutableList.of("/a/b/c", "d/e/f")));
     SearchFeatures searchFeatures = new SearchFeatures().setFeature1(2).setFeature2(1);
 
     TestEntityAspectArray aspects = new TestEntityAspectArray(

File: metadata-io/src/main/java/com/linkedin/metadata/recommendation/candidatesource/RecentlySearchedSource.java
Patch:
@@ -46,7 +46,7 @@ public class RecentlySearchedSource implements RecommendationSource {
 
   @Override
   public String getTitle() {
-    return "Recent Searches";
+    return "Recent searches";
   }
 
   @Override

File: metadata-io/src/main/java/com/linkedin/metadata/entity/EntityService.java
Patch:
@@ -21,6 +21,7 @@
 import com.linkedin.common.urn.Urn;
 import com.linkedin.common.urn.UrnUtils;
 import com.linkedin.common.urn.VersionedUrnUtils;
+import com.linkedin.data.DataMap;
 import com.linkedin.data.schema.RecordDataSchema;
 import com.linkedin.data.schema.TyperefDataSchema;
 import com.linkedin.data.schema.validator.Validator;
@@ -1042,7 +1043,7 @@ private boolean emitChangeLog(@Nullable RecordTemplate oldAspect, @Nullable Syst
       log.debug("Producing MetadataChangeLog for ingested aspect {}, urn {}", mcp.getAspectName(), entityUrn);
 
       // Uses new data map to prevent side effects on original
-      final MetadataChangeLog metadataChangeLog = new MetadataChangeLog(mcp.data());
+      final MetadataChangeLog metadataChangeLog = new MetadataChangeLog(new DataMap(mcp.data()));
       metadataChangeLog.setEntityUrn(entityUrn);
       metadataChangeLog.setCreated(auditStamp);
       metadataChangeLog.setChangeType(isNoOp ? ChangeType.RESTATE : ChangeType.UPSERT);

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/assertion/AssertionRunEventResolver.java
Patch:
@@ -64,7 +64,6 @@ public CompletableFuture<AssertionRunEventsResult> get(DataFetchingEnvironment e
             maybeStartTimeMillis,
             maybeEndTimeMillis,
             maybeLimit,
-            false,
             buildFilter(maybeFilters, maybeStatus),
             context.getAuthentication());
 

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/dashboard/DashboardUsageStatsResolver.java
Patch:
@@ -87,7 +87,7 @@ private List<DashboardUsageMetrics> getDashboardUsageMetrics(String dashboardUrn
       List<EnvelopedAspect> aspects =
           timeseriesAspectService.getAspectValues(Urn.createFromString(dashboardUrn), Constants.DASHBOARD_ENTITY_NAME,
               Constants.DASHBOARD_USAGE_STATISTICS_ASPECT_NAME, maybeStartTimeMillis, maybeEndTimeMillis, maybeLimit,
-              null, filter);
+              filter);
       dashboardUsageMetrics = aspects.stream().map(DashboardUsageMetricMapper::map).collect(Collectors.toList());
     } catch (URISyntaxException e) {
       throw new IllegalArgumentException("Invalid resource", e);

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/dashboard/DashboardUsageStatsUtils.java
Patch:
@@ -56,7 +56,6 @@ public static List<DashboardUsageMetrics> getDashboardUsageMetrics(
         maybeStartTimeMillis,
         maybeEndTimeMillis,
         maybeLimit,
-        null,
         filter);
       dashboardUsageMetrics = aspects.stream().map(DashboardUsageMetricMapper::map).collect(Collectors.toList());
     } catch (URISyntaxException e) {

File: datahub-graphql-core/src/test/java/com/linkedin/datahub/graphql/resolvers/assertion/AssertionRunEventResolverTest.java
Patch:
@@ -50,7 +50,6 @@ public void testGetSuccess() throws Exception {
         Mockito.eq(0L),
         Mockito.eq(10L),
         Mockito.eq(5),
-        Mockito.eq(false),
         Mockito.eq(AssertionRunEventResolver.buildFilter(null, AssertionRunStatus.COMPLETE.toString())),
         Mockito.any(Authentication.class)
     )).thenReturn(
@@ -86,7 +85,6 @@ public void testGetSuccess() throws Exception {
         Mockito.eq(0L),
         Mockito.eq(10L),
         Mockito.eq(5),
-        Mockito.eq(false),
         Mockito.any(Filter.class),
         Mockito.any(Authentication.class)
     );

File: datahub-graphql-core/src/test/java/com/linkedin/datahub/graphql/resolvers/dashboard/DashboardStatsSummaryTest.java
Patch:
@@ -81,7 +81,6 @@ public void testGetSuccess() throws Exception {
         Mockito.eq(null),
         Mockito.eq(null),
         Mockito.eq(1),
-        Mockito.eq(null),
         Mockito.eq(filterForLatestStats)
     )).thenReturn(ImmutableList.of(newResult));
 
@@ -159,7 +158,6 @@ private TimeseriesAspectService initTestAspectService() {
         Mockito.eq(null),
         Mockito.eq(null),
         Mockito.eq(1),
-        Mockito.eq(null),
         Mockito.eq(filterForLatestStats)
     )).thenReturn(
         ImmutableList.of(envelopedLatestStats)

File: li-utils/src/main/java/com/linkedin/metadata/Constants.java
Patch:
@@ -75,6 +75,7 @@ public class Constants {
   public static final String SUB_TYPES_ASPECT_NAME = "subTypes";
   public static final String DEPRECATION_ASPECT_NAME = "deprecation";
   public static final String OPERATION_ASPECT_NAME = "operation";
+  public static final String OPERATION_EVENT_TIME_FIELD_NAME = "lastUpdatedTimestamp"; // :(
   public static final String SIBLINGS_ASPECT_NAME = "siblings";
   public static final String ORIGIN_ASPECT_NAME = "origin";
   public static final String INPUT_FIELDS_ASPECT_NAME = "inputFields";

File: metadata-io/src/test/java/com/linkedin/metadata/ESTestConfiguration.java
Patch:
@@ -56,7 +56,7 @@ public SearchConfiguration searchConfiguration() {
 
         PartialConfiguration partialConfiguration = new PartialConfiguration();
         partialConfiguration.setFactor(0.4f);
-        partialConfiguration.setUrnFactor(0.7f);
+        partialConfiguration.setUrnFactor(0.5f);
 
         searchConfiguration.setExactMatch(exactMatchConfiguration);
         searchConfiguration.setPartial(partialConfiguration);

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/GmsGraphQLEngine.java
Patch:
@@ -688,7 +688,7 @@ private void configureQueryResolvers(final RuntimeWiring.Builder builder) {
             .dataFetcher("searchAcrossLineage", new SearchAcrossLineageResolver(this.entityClient))
             .dataFetcher("scrollAcrossLineage", new ScrollAcrossLineageResolver(this.entityClient))
             .dataFetcher("autoComplete", new AutoCompleteResolver(searchableTypes))
-            .dataFetcher("autoCompleteForMultiple", new AutoCompleteForMultipleResolver(searchableTypes))
+            .dataFetcher("autoCompleteForMultiple", new AutoCompleteForMultipleResolver(searchableTypes, this.viewService))
             .dataFetcher("browse", new BrowseResolver(browsableTypes))
             .dataFetcher("browsePaths", new BrowsePathsResolver(browsableTypes))
             .dataFetcher("dataset", getResolver(datasetType))

File: metadata-io/src/main/java/com/linkedin/metadata/search/LineageSearchService.java
Patch:
@@ -122,7 +122,8 @@ public LineageSearchResult searchAcrossLineage(@Nonnull Urn sourceUrn, @Nonnull
     }
 
     // Cache multihop result for faster performance
-    final EntityLineageResultCacheKey cacheKey = new EntityLineageResultCacheKey(sourceUrn, direction, startTimeMillis, endTimeMillis, maxHops);
+    final EntityLineageResultCacheKey cacheKey = EntityLineageResultCacheKey.from(sourceUrn, direction, startTimeMillis,
+            endTimeMillis, maxHops);
     CachedEntityLineageResult cachedLineageResult = null;
 
     if (cacheEnabled) {

File: metadata-io/src/main/java/com/linkedin/metadata/entity/EntityService.java
Patch:
@@ -1084,7 +1084,7 @@ public RestoreIndicesResult restoreIndices(@Nonnull RestoreIndicesArgs args, @No
     logger.accept(String.format(
         "Reading rows %s through %s from the aspects table completed.", args.start, args.start + args.batchSize));
 
-    for (EbeanAspectV2 aspect : rows.getList()) {
+    for (EbeanAspectV2 aspect : rows != null ? rows.getList() : List.<EbeanAspectV2>of()) {
       // 1. Extract an Entity type from the entity Urn
       result.timeGetRowMs = System.currentTimeMillis() - startTime;
       startTime = System.currentTimeMillis();

File: metadata-io/src/main/java/com/linkedin/metadata/entity/RetentionService.java
Patch:
@@ -107,6 +107,7 @@ public boolean setRetention(@Nullable String entityName, @Nullable String aspect
     GenericAspect retentionAspect = GenericRecordUtils.serializeAspect(retentionConfig);
     aspectProposal.setAspect(retentionAspect);
     aspectProposal.setAspectName(Constants.DATAHUB_RETENTION_ASPECT);
+    aspectProposal.setChangeType(ChangeType.UPSERT);
     return getEntityService().ingestProposal(aspectProposal, auditStamp, false).isDidUpdate();
   }
 

File: metadata-io/src/test/java/com/linkedin/metadata/AspectGenerationUtils.java
Patch:
@@ -6,15 +6,13 @@
 import com.linkedin.common.urn.Urn;
 import com.linkedin.common.urn.UrnUtils;
 import com.linkedin.data.template.RecordTemplate;
-import com.linkedin.dataset.Upstream;
 import com.linkedin.dataset.UpstreamArray;
 import com.linkedin.dataset.UpstreamLineage;
 import com.linkedin.identity.CorpUserInfo;
 import com.linkedin.metadata.key.CorpUserKey;
 import com.linkedin.metadata.utils.EntityKeyUtils;
 import com.linkedin.metadata.utils.PegasusUtils;
 import com.linkedin.mxe.SystemMetadata;
-import java.util.Collections;
 import javax.annotation.Nonnull;
 
 

File: metadata-jobs/mae-consumer/src/main/java/com/linkedin/metadata/kafka/hook/UpdateIndicesHook.java
Patch:
@@ -22,7 +22,7 @@
 import com.linkedin.metadata.Constants;
 import com.linkedin.metadata.graph.Edge;
 import com.linkedin.metadata.graph.GraphService;
-import com.linkedin.metadata.graph.elastic.ElasticSearchGraphService;
+import com.linkedin.metadata.graph.dgraph.DgraphGraphService;
 import com.linkedin.metadata.key.SchemaFieldKey;
 import com.linkedin.metadata.models.AspectSpec;
 import com.linkedin.metadata.models.EntitySpec;
@@ -179,7 +179,7 @@ private void handleUpdateChangeEvent(@Nonnull final MetadataChangeLog event) {
 
     // Step 2. For all aspects, attempt to update Graph
     SystemMetadata systemMetadata = event.getSystemMetadata();
-    if (_graphDiffMode && !(_graphService instanceof DGraphGraphService)
+    if (_graphDiffMode && !(_graphService instanceof DgraphGraphService)
         && (systemMetadata == null || systemMetadata.getProperties() == null
         || !Boolean.parseBoolean(systemMetadata.getProperties().get(FORCE_INDEXING_KEY)))) {
       updateGraphServiceDiff(urn, aspectSpec, previousAspect, aspect, event);

File: metadata-jobs/mae-consumer/src/main/java/com/linkedin/metadata/kafka/hook/UpdateIndicesHook.java
Patch:
@@ -179,7 +179,7 @@ private void handleUpdateChangeEvent(@Nonnull final MetadataChangeLog event) {
 
     // Step 2. For all aspects, attempt to update Graph
     SystemMetadata systemMetadata = event.getSystemMetadata();
-    if (_graphDiffMode && _graphService instanceof ElasticSearchGraphService
+    if (_graphDiffMode && !(_graphService instanceof DGraphGraphService)
         && (systemMetadata == null || systemMetadata.getProperties() == null
         || !Boolean.parseBoolean(systemMetadata.getProperties().get(FORCE_INDEXING_KEY)))) {
       updateGraphServiceDiff(urn, aspectSpec, previousAspect, aspect, event);
@@ -564,4 +564,4 @@ private EntitySpec getEventEntitySpec(@Nonnull final MetadataChangeLog event) {
               event.getEntityType()));
     }
   }
-}
\ No newline at end of file
+}

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/entityregistry/EntityRegistryFactory.java
Patch:
@@ -33,7 +33,7 @@ public class EntityRegistryFactory {
   @Nonnull
   protected EntityRegistry getInstance() throws EntityRegistryException {
     MergedEntityRegistry baseEntityRegistry = new MergedEntityRegistry(SnapshotEntityRegistry.getInstance()).apply(configEntityRegistry);
-    pluginEntityRegistryLoader.withBaseRegistry(baseEntityRegistry).start(false);
+    pluginEntityRegistryLoader.withBaseRegistry(baseEntityRegistry).start(true);
     return baseEntityRegistry;
   }
 }

File: metadata-integration/java/datahub-client/src/test/java/datahub/client/kafka/containers/Utils.java
Patch:
@@ -4,7 +4,7 @@
 import java.net.ServerSocket;
 
 final class Utils {
-  public static final String CONFLUENT_PLATFORM_VERSION = "5.3.1";
+  public static final String CONFLUENT_PLATFORM_VERSION = "5.5.3";
 
   private Utils() {
   }

File: metadata-io/src/main/java/com/linkedin/metadata/config/search/SearchConfiguration.java
Patch:
@@ -8,5 +8,6 @@ public class SearchConfiguration {
 
   private int maxTermBucketSize;
   private ExactMatchConfiguration exactMatch;
+  private PartialConfiguration partial;
   private GraphQueryConfiguration graph;
 }

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/common/mappers/SearchFlagsInputMapper.java
Patch:
@@ -24,6 +24,8 @@ public com.linkedin.metadata.query.SearchFlags apply(@Nonnull final SearchFlags
     com.linkedin.metadata.query.SearchFlags result = new com.linkedin.metadata.query.SearchFlags();
     if (searchFlags.getFulltext() != null) {
       result.setFulltext(searchFlags.getFulltext());
+    } else {
+      result.setFulltext(true);
     }
     if (searchFlags.getSkipCache() != null) {
       result.setSkipCache(searchFlags.getSkipCache());

File: metadata-io/src/main/java/com/linkedin/metadata/search/elasticsearch/query/request/SearchRequestHandler.java
Patch:
@@ -186,7 +186,7 @@ public static BoolQueryBuilder getFilterQuery(@Nullable Filter filter) {
   @WithSpan
   public SearchRequest getSearchRequest(@Nonnull String input, @Nullable Filter filter,
                                         @Nullable SortCriterion sortCriterion, int from, int size,
-                                        @Nonnull SearchFlags searchFlags) {
+                                        @Nullable SearchFlags searchFlags) {
     SearchFlags finalSearchFlags = applyDefaultSearchFlags(searchFlags, input, DEFAULT_SERVICE_SEARCH_FLAGS);
 
     SearchRequest searchRequest = new SearchRequest();

File: metadata-io/src/main/java/com/linkedin/metadata/search/utils/SearchUtils.java
Patch:
@@ -170,9 +170,10 @@ public static ListResult toListResult(final SearchResult searchResult) {
     return listResult;
   }
 
+  @SneakyThrows
   public static SearchFlags applyDefaultSearchFlags(@Nullable SearchFlags inputFlags, @Nullable String query,
                                                     @Nonnull SearchFlags defaultFlags) {
-    SearchFlags finalSearchFlags = inputFlags != null ? inputFlags : defaultFlags;
+    SearchFlags finalSearchFlags = inputFlags != null ? inputFlags : defaultFlags.copy();
     if (!finalSearchFlags.hasFulltext() || finalSearchFlags.isFulltext() == null) {
       finalSearchFlags.setFulltext(defaultFlags.isFulltext());
     }

File: metadata-utils/src/main/java/com/linkedin/metadata/utils/elasticsearch/IndexConventionImpl.java
Patch:
@@ -2,17 +2,17 @@
 
 import com.linkedin.data.template.RecordTemplate;
 import com.linkedin.metadata.models.EntitySpec;
-import java.util.HashMap;
 import java.util.Map;
 import java.util.Optional;
+import java.util.concurrent.ConcurrentHashMap;
 import javax.annotation.Nonnull;
 import javax.annotation.Nullable;
 import org.apache.commons.lang3.StringUtils;
 
 
 // Default implementation of search index naming convention
 public class IndexConventionImpl implements IndexConvention {
-  private final Map<String, String> indexNameMapping = new HashMap<>();
+  private final Map<String, String> indexNameMapping = new ConcurrentHashMap<>();
   private final Optional<String> _prefix;
   private final String _getAllEntityIndicesPattern;
   private final String _getAllTimeseriesIndicesPattern;

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/load/TimeSeriesAspectResolver.java
Patch:
@@ -98,7 +98,7 @@ public CompletableFuture<List<TimeSeriesAspect>> get(DataFetchingEnvironment env
                 maybeLimit, null, buildFilters(maybeFilters), context.getAuthentication());
 
         // Step 2: Bind profiles into GraphQL strong types.
-        return aspects.stream().map(_aspectMapper::apply).collect(Collectors.toList());
+        return aspects.stream().map(_aspectMapper).collect(Collectors.toList());
       } catch (RemoteInvocationException e) {
         throw new RuntimeException("Failed to retrieve aspects from GMS", e);
       }

File: metadata-io/src/main/java/com/linkedin/metadata/client/JavaEntityClient.java
Patch:
@@ -459,7 +459,7 @@ public VersionedAspect getAspectOrNull(@Nonnull String urn, @Nonnull String aspe
     @Override
     public List<EnvelopedAspect> getTimeseriesAspectValues(@Nonnull String urn, @Nonnull String entity,
         @Nonnull String aspect, @Nullable Long startTimeMillis, @Nullable Long endTimeMillis, @Nullable Integer limit,
-        @Nonnull Boolean getLatestValue, @Nullable Filter filter, @Nonnull final Authentication authentication)
+        @Nullable Boolean getLatestValue, @Nullable Filter filter, @Nonnull final Authentication authentication)
         throws RemoteInvocationException {
       GetTimeseriesAspectValuesResponse response = new GetTimeseriesAspectValuesResponse();
       response.setEntityName(entity);

File: metadata-service/openapi-servlet/src/main/java/io/datahubproject/openapi/relationships/RelationshipsController.java
Patch:
@@ -143,7 +143,7 @@ public ResponseEntity<RelatedEntitiesResult> getRelationships(
       exceptionally = e;
       throw new RuntimeException(
           String.format("Failed to batch get relationships with urn: %s, relationshipTypes: %s", urn,
-              relationshipTypes), e);
+                  Arrays.toString(relationshipTypes)), e);
     } finally {
       if (exceptionally != null) {
         MetricUtils.counter(MetricRegistry.name("getRelationships", "failed")).inc();

File: metadata-service/openapi-servlet/src/test/java/mock/MockEntityService.java
Patch:
@@ -169,7 +169,7 @@ protected List<Pair<String, UpdateAspectResult>> ingestAspectsToLocalDB(@Nonnull
   @Nullable
   @Override
   public RecordTemplate ingestAspectIfNotPresent(@NotNull Urn urn, @NotNull String aspectName,
-      @NotNull RecordTemplate newValue, @NotNull AuditStamp auditStamp, SystemMetadata systemMetadata) {
+      @NotNull RecordTemplate newValue, @NotNull AuditStamp auditStamp, @Nullable SystemMetadata systemMetadata) {
     return null;
   }
 

File: metadata-service/restli-client/src/main/java/com/linkedin/entity/client/EntityClient.java
Patch:
@@ -359,7 +359,7 @@ public VersionedAspect getAspectOrNull(@Nonnull String urn, @Nonnull String aspe
 
   public List<EnvelopedAspect> getTimeseriesAspectValues(@Nonnull String urn, @Nonnull String entity,
       @Nonnull String aspect, @Nullable Long startTimeMillis, @Nullable Long endTimeMillis, @Nullable Integer limit,
-      @Nonnull Boolean getLatestValue, @Nullable Filter filter, @Nonnull Authentication authentication)
+      @Nullable Boolean getLatestValue, @Nullable Filter filter, @Nonnull Authentication authentication)
       throws RemoteInvocationException;
 
   @Deprecated

File: metadata-service/restli-client/src/main/java/com/linkedin/entity/client/RestliEntityClient.java
Patch:
@@ -714,7 +714,7 @@ public VersionedAspect getAspectOrNull(@Nonnull String urn, @Nonnull String aspe
   @Nonnull
   public List<EnvelopedAspect> getTimeseriesAspectValues(@Nonnull String urn, @Nonnull String entity,
       @Nonnull String aspect, @Nullable Long startTimeMillis, @Nullable Long endTimeMillis, @Nullable Integer limit,
-      @Nonnull Boolean getLatestValue, @Nullable Filter filter, @Nonnull final Authentication authentication)
+      @Nullable Boolean getLatestValue, @Nullable Filter filter, @Nonnull final Authentication authentication)
       throws RemoteInvocationException {
 
     AspectsDoGetTimeseriesAspectValuesRequestBuilder requestBuilder =

File: metadata-service/servlet/src/main/java/com/datahub/gms/servlet/Config.java
Patch:
@@ -32,7 +32,7 @@ public class Config extends HttpServlet {
     put("retention", "true");
     put("statefulIngestionCapable", true);
     put("patchCapable", true);
-    put("timeZone", ZoneId.systemDefault());
+    put("timeZone", ZoneId.systemDefault().toString());
   }};
   ObjectMapper objectMapper = new ObjectMapper().setSerializationInclusion(JsonInclude.Include.NON_NULL);
 

File: metadata-service/factories/src/main/java/com/linkedin/metadata/boot/kafka/DataHubUpgradeKafkaListener.java
Patch:
@@ -85,7 +85,7 @@ public void checkSystemVersion(final ConsumerRecord<String, GenericRecord> consu
       if (expectedVersion.equals(event.getVersion())) {
         IS_UPDATED.getAndSet(true);
       } else {
-        log.info("System version is not up to date: {}. Waiting for datahub-upgrade to complete...", expectedVersion);
+        log.warn("System version is not up to date: {}. Waiting for datahub-upgrade to complete...", expectedVersion);
       }
 
     } catch (Exception e) {

File: metadata-service/servlet/src/main/java/com/datahub/gms/servlet/Config.java
Patch:
@@ -14,6 +14,7 @@
 import java.io.PrintWriter;
 import java.util.HashMap;
 import java.util.Map;
+import java.time.ZoneId;
 import javax.servlet.ServletContext;
 import javax.servlet.http.HttpServlet;
 import javax.servlet.http.HttpServletRequest;
@@ -31,6 +32,7 @@ public class Config extends HttpServlet {
     put("retention", "true");
     put("statefulIngestionCapable", true);
     put("patchCapable", true);
+    put("timeZone", ZoneId.systemDefault());
   }};
   ObjectMapper objectMapper = new ObjectMapper().setSerializationInclusion(JsonInclude.Include.NON_NULL);
 

File: datahub-frontend/app/controllers/Application.java
Patch:
@@ -263,6 +263,7 @@ private StandaloneWSClient createWsClient() {
     Materializer materializer = ActorMaterializer.create(system);
     AsyncHttpClientConfig asyncHttpClientConfig =
         new DefaultAsyncHttpClientConfig.Builder()
+            .setDisableUrlEncodingForBoundRequests(true)
             .setMaxRequestRetry(0)
             .setShutdownQuietPeriod(0)
             .setShutdownTimeout(0)

File: metadata-service/factories/src/main/java/com/linkedin/metadata/boot/kafka/DataHubUpgradeKafkaListener.java
Patch:
@@ -85,7 +85,7 @@ public void checkSystemVersion(final ConsumerRecord<String, GenericRecord> consu
       if (expectedVersion.equals(event.getVersion())) {
         IS_UPDATED.getAndSet(true);
       } else {
-        log.debug("System version is not up to date: {}", expectedVersion);
+        log.info("System version is not up to date: {}. Waiting for datahub-upgrade to complete...", expectedVersion);
       }
 
     } catch (Exception e) {

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/ingest/execution/CreateIngestionExecutionRequestResolver.java
Patch:
@@ -108,7 +108,7 @@ public CompletableFuture<String> get(final DataFetchingEnvironment environment)
           Map<String, String> arguments = new HashMap<>();
           String recipe = ingestionSourceInfo.getConfig().getRecipe();
           recipe = injectRunId(recipe, executionRequestUrn.toString());
-          recipe = IngestionUtils.injectPipelineName(recipe, executionRequestUrn.toString());
+          recipe = IngestionUtils.injectPipelineName(recipe, ingestionSourceUrn.toString());
           arguments.put(RECIPE_ARG_NAME, recipe);
           arguments.put(VERSION_ARG_NAME, ingestionSourceInfo.getConfig().hasVersion()
               ? ingestionSourceInfo.getConfig().getVersion()

File: metadata-io/src/main/java/com/linkedin/metadata/search/utils/ESUtils.java
Patch:
@@ -25,6 +25,7 @@
 import org.elasticsearch.search.sort.ScoreSortBuilder;
 import org.elasticsearch.search.sort.SortOrder;
 
+import static com.linkedin.metadata.search.elasticsearch.query.request.SearchFieldConfig.KEYWORD_FIELDS;
 import static com.linkedin.metadata.search.utils.SearchUtils.isUrn;
 
 
@@ -263,8 +264,7 @@ public static String toFacetField(@Nonnull final String filterField) {
   @Nonnull
   public static String toKeywordField(@Nonnull final String filterField, @Nonnull final boolean skipKeywordSuffix) {
     return skipKeywordSuffix
-            || "urn".equals(filterField)
-            || "runId".equals(filterField)
+            || KEYWORD_FIELDS.contains(filterField)
             || filterField.contains(".") ? filterField : filterField + ESUtils.KEYWORD_SUFFIX;
   }
 

File: metadata-io/src/test/java/com/linkedin/metadata/search/elasticsearch/fixtures/SampleDataFixtureTests.java
Patch:
@@ -494,7 +494,7 @@ public void testSmokeTestQueries() {
         Map<String, Integer> expectedFulltextMinimums = Map.of(
                 "sample", 3,
                 "covid", 2,
-                "\"raw_orders\"", 1,
+                "\"raw_orders\"", 6,
                 STRUCTURED_QUERY_PREFIX + "sample", 1,
                 STRUCTURED_QUERY_PREFIX + "covid", 0,
                 STRUCTURED_QUERY_PREFIX + "\"raw_orders\"", 1
@@ -960,10 +960,10 @@ public void testPrefixVsExact() {
         assertTrue(result.getEntities().stream().noneMatch(e -> e.getMatchedFields().isEmpty()),
                 String.format("%s - Expected search results to include matched fields", query));
 
-        assertEquals(result.getEntities().size(), 2);
+        assertEquals(result.getEntities().size(), 10);
         assertEquals(result.getEntities().get(0).getEntity().toString(),
                 "urn:li:dataset:(urn:li:dataPlatform:dbt,cypress_project.jaffle_shop.customers,PROD)",
-                "Expected exact match");
+                "Expected exact match and 1st position");
     }
 
     private Stream<AnalyzeResponse.AnalyzeToken> getTokens(AnalyzeRequest request) throws IOException {

File: metadata-io/src/main/java/com/linkedin/metadata/search/utils/ESUtils.java
Patch:
@@ -264,6 +264,7 @@ public static String toFacetField(@Nonnull final String filterField) {
   public static String toKeywordField(@Nonnull final String filterField, @Nonnull final boolean skipKeywordSuffix) {
     return skipKeywordSuffix
             || "urn".equals(filterField)
+            || "runId".equals(filterField)
             || filterField.contains(".") ? filterField : filterField + ESUtils.KEYWORD_SUFFIX;
   }
 

File: metadata-io/src/main/java/com/linkedin/metadata/recommendation/candidatesource/MostPopularSource.java
Patch:
@@ -127,7 +127,7 @@ private SearchRequest buildSearchRequest(@Nonnull Urn userUrn) {
 
     // Find the entities with the most views
     AggregationBuilder aggregation = AggregationBuilders.terms(ENTITY_AGG_NAME)
-        .field(ESUtils.toKeywordField(DataHubUsageEventConstants.ENTITY_URN, true))
+        .field(ESUtils.toKeywordField(DataHubUsageEventConstants.ENTITY_URN, false))
         .size(MAX_CONTENT * 2);
     source.aggregation(aggregation);
     source.size(0);

File: metadata-io/src/main/java/com/linkedin/metadata/recommendation/candidatesource/RecentlyEditedSource.java
Patch:
@@ -128,7 +128,7 @@ private SearchRequest buildSearchRequest(@Nonnull Urn userUrn) {
     // Find the entity with the largest last viewed timestamp
     String lastViewed = "last_viewed";
     AggregationBuilder aggregation = AggregationBuilders.terms(ENTITY_AGG_NAME)
-        .field(ESUtils.toKeywordField(DataHubUsageEventConstants.ENTITY_URN, true))
+        .field(ESUtils.toKeywordField(DataHubUsageEventConstants.ENTITY_URN, false))
         .size(MAX_CONTENT)
         .order(BucketOrder.aggregation(lastViewed, false))
         .subAggregation(AggregationBuilders.max(lastViewed).field(DataHubUsageEventConstants.TIMESTAMP));

File: metadata-io/src/main/java/com/linkedin/metadata/recommendation/candidatesource/RecentlyViewedSource.java
Patch:
@@ -130,7 +130,7 @@ private SearchRequest buildSearchRequest(@Nonnull Urn userUrn) {
     // Find the entity with the largest last viewed timestamp
     String lastViewed = "last_viewed";
     AggregationBuilder aggregation = AggregationBuilders.terms(ENTITY_AGG_NAME)
-        .field(ESUtils.toKeywordField(DataHubUsageEventConstants.ENTITY_URN, true))
+        .field(ESUtils.toKeywordField(DataHubUsageEventConstants.ENTITY_URN, false))
         .size(MAX_CONTENT)
         .order(BucketOrder.aggregation(lastViewed, false))
         .subAggregation(AggregationBuilders.max(lastViewed).field(DataHubUsageEventConstants.TIMESTAMP));

File: metadata-io/src/test/java/com/linkedin/metadata/search/elasticsearch/query/request/SearchQueryBuilderTest.java
Patch:
@@ -54,7 +54,9 @@ public void testQueryBuilderFulltext() {
     }).collect(Collectors.toList());
 
     assertEquals(prefixFieldWeights, List.of(
-            Pair.of("keyPart1.delimited", 10.0f)
+            Pair.of("urn", 100.0f),
+            Pair.of("keyPart1.delimited", 10.0f),
+            Pair.of("keyPart1.keyword", 100.0f)
     ));
 
     // Validate scorer

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/ResolverUtils.java
Patch:
@@ -142,7 +142,7 @@ public static Criterion criterionFromFilter(final FacetFilterInput filter, final
         if (skipKeywordSuffix) {
             result.setField(filter.getField());
         } else {
-            result.setField(getFilterField(filter.getField()));
+            result.setField(getFilterField(filter.getField(), skipKeywordSuffix));
         }
 
         // `value` is deprecated in place of `values`- this is to support old query patterns. If values is provided,
@@ -176,10 +176,10 @@ public static Criterion criterionFromFilter(final FacetFilterInput filter, final
         return result;
     }
 
-    private static String getFilterField(final String originalField) {
+    private static String getFilterField(final String originalField, final boolean skipKeywordSuffix) {
         if (KEYWORD_EXCLUDED_FILTERS.contains(originalField)) {
             return originalField;
         }
-        return originalField + ESUtils.KEYWORD_SUFFIX;
+        return ESUtils.toKeywordField(originalField, skipKeywordSuffix);
     }
 }

File: metadata-io/src/main/java/com/linkedin/metadata/recommendation/candidatesource/MostPopularSource.java
Patch:
@@ -127,7 +127,7 @@ private SearchRequest buildSearchRequest(@Nonnull Urn userUrn) {
 
     // Find the entities with the most views
     AggregationBuilder aggregation = AggregationBuilders.terms(ENTITY_AGG_NAME)
-        .field(DataHubUsageEventConstants.ENTITY_URN + ESUtils.KEYWORD_SUFFIX)
+        .field(ESUtils.toKeywordField(DataHubUsageEventConstants.ENTITY_URN, true))
         .size(MAX_CONTENT * 2);
     source.aggregation(aggregation);
     source.size(0);

File: metadata-io/src/main/java/com/linkedin/metadata/recommendation/candidatesource/RecentlyEditedSource.java
Patch:
@@ -128,7 +128,7 @@ private SearchRequest buildSearchRequest(@Nonnull Urn userUrn) {
     // Find the entity with the largest last viewed timestamp
     String lastViewed = "last_viewed";
     AggregationBuilder aggregation = AggregationBuilders.terms(ENTITY_AGG_NAME)
-        .field(DataHubUsageEventConstants.ENTITY_URN + ESUtils.KEYWORD_SUFFIX)
+        .field(ESUtils.toKeywordField(DataHubUsageEventConstants.ENTITY_URN, true))
         .size(MAX_CONTENT)
         .order(BucketOrder.aggregation(lastViewed, false))
         .subAggregation(AggregationBuilders.max(lastViewed).field(DataHubUsageEventConstants.TIMESTAMP));

File: metadata-io/src/main/java/com/linkedin/metadata/recommendation/candidatesource/RecentlyViewedSource.java
Patch:
@@ -122,15 +122,15 @@ private SearchRequest buildSearchRequest(@Nonnull Urn userUrn) {
     BoolQueryBuilder query = QueryBuilders.boolQuery();
     // Filter for the entity view events of the user requesting recommendation
     query.must(
-        QueryBuilders.termQuery(DataHubUsageEventConstants.ACTOR_URN + ESUtils.KEYWORD_SUFFIX, userUrn.toString()));
+        QueryBuilders.termQuery(ESUtils.toKeywordField(DataHubUsageEventConstants.ACTOR_URN, true), userUrn.toString()));
     query.must(
         QueryBuilders.termQuery(DataHubUsageEventConstants.TYPE, DataHubUsageEventType.ENTITY_VIEW_EVENT.getType()));
     source.query(query);
 
     // Find the entity with the largest last viewed timestamp
     String lastViewed = "last_viewed";
     AggregationBuilder aggregation = AggregationBuilders.terms(ENTITY_AGG_NAME)
-        .field(DataHubUsageEventConstants.ENTITY_URN + ESUtils.KEYWORD_SUFFIX)
+        .field(ESUtils.toKeywordField(DataHubUsageEventConstants.ENTITY_URN, true))
         .size(MAX_CONTENT)
         .order(BucketOrder.aggregation(lastViewed, false))
         .subAggregation(AggregationBuilders.max(lastViewed).field(DataHubUsageEventConstants.TIMESTAMP));

File: metadata-io/src/main/java/com/linkedin/metadata/search/elasticsearch/query/request/AutocompleteRequestHandler.java
Patch:
@@ -65,7 +65,7 @@ public SearchRequest getSearchRequest(@Nonnull String input, @Nullable String fi
     SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder();
     searchSourceBuilder.size(limit);
     searchSourceBuilder.query(getQuery(input, field));
-    searchSourceBuilder.postFilter(ESUtils.buildFilterQuery(filter));
+    searchSourceBuilder.postFilter(ESUtils.buildFilterQuery(filter, false));
     searchSourceBuilder.highlighter(getHighlights(field));
     searchRequest.source(searchSourceBuilder);
     return searchRequest;

File: metadata-io/src/main/java/com/linkedin/metadata/search/elasticsearch/query/request/SearchRequestHandler.java
Patch:
@@ -133,7 +133,7 @@ private Set<String> getDefaultQueryFieldNames() {
   }
 
   public static BoolQueryBuilder getFilterQuery(@Nullable Filter filter) {
-    BoolQueryBuilder filterQuery = ESUtils.buildFilterQuery(filter);
+    BoolQueryBuilder filterQuery = ESUtils.buildFilterQuery(filter, false);
 
     boolean removedInOrFilter = false;
     if (filter != null) {
@@ -226,7 +226,7 @@ public static SearchRequest getAggregationRequest(@Nonnull String field, @Nullab
     final SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder();
     searchSourceBuilder.query(filterQuery);
     searchSourceBuilder.size(0);
-    searchSourceBuilder.aggregation(AggregationBuilders.terms(field).field(field + ESUtils.KEYWORD_SUFFIX).size(limit));
+    searchSourceBuilder.aggregation(AggregationBuilders.terms(field).field(ESUtils.toKeywordField(field, false)).size(limit));
     searchRequest.source(searchSourceBuilder);
 
     return searchRequest;
@@ -241,7 +241,7 @@ private List<AggregationBuilder> getAggregations() {
     for (String facet : _facetFields) {
       // All facet fields must have subField keyword
       AggregationBuilder aggBuilder =
-          AggregationBuilders.terms(facet).field(facet + ESUtils.KEYWORD_SUFFIX).size(_configs.getMaxTermBucketSize());
+          AggregationBuilders.terms(facet).field(ESUtils.toKeywordField(facet, false)).size(_configs.getMaxTermBucketSize());
       aggregationBuilders.add(aggBuilder);
     }
     return aggregationBuilders;

File: metadata-io/src/main/java/com/linkedin/metadata/timeseries/elastic/query/ESAggregatedStatsDAO.java
Patch:
@@ -342,7 +342,7 @@ public GenericTable getAggregatedStats(@Nonnull String entityName, @Nonnull Stri
       @Nullable GroupingBucket[] groupingBuckets) {
 
     // Setup the filter query builder using the input filter provided.
-    final BoolQueryBuilder filterQueryBuilder = ESUtils.buildFilterQuery(filter);
+    final BoolQueryBuilder filterQueryBuilder = ESUtils.buildFilterQuery(filter, true);
     // Create the high-level aggregation builder with the filter.
     final AggregationBuilder filteredAggBuilder = AggregationBuilders.filter(ES_FILTERED_STATS, filterQueryBuilder);
 
@@ -428,7 +428,8 @@ private AggregationBuilder makeGroupingAggregationBuilder(AspectSpec aspectSpec,
             .calendarInterval(getHistogramInterval(curGroupingBucket.getTimeWindowSize()));
       } else if (curGroupingBucket.getType() == GroupingBucketType.STRING_GROUPING_BUCKET) {
         // Process the string grouping bucket using the 'terms' aggregation.
-        String fieldName = curGroupingBucket.getKey();
+        // The field can be Keyword, Numeric, ip, boolean, or binary.
+        String fieldName = ESUtils.toKeywordField(curGroupingBucket.getKey(), true);
         DataSchema.Type fieldType = getGroupingBucketKeyType(aspectSpec, curGroupingBucket);
         curAggregationBuilder = AggregationBuilders.terms(getGroupingBucketAggName(curGroupingBucket))
             .field(fieldName)

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/load/EntityLineageResultResolver.java
Patch:
@@ -79,6 +79,7 @@ private EntityLineageResult mapEntityRelationships(
     result.setStart(entityLineageResult.getStart());
     result.setCount(entityLineageResult.getCount());
     result.setTotal(entityLineageResult.getTotal());
+    result.setFiltered(entityLineageResult.getFiltered());
     result.setRelationships(entityLineageResult.getRelationships()
         .stream()
         .map(this::mapEntityRelationship)

File: metadata-io/src/main/java/com/linkedin/metadata/shared/ValidationUtils.java
Patch:
@@ -119,8 +119,11 @@ public static EntityLineageResult validateEntityLineageResult(@Nullable final En
 
     final LineageRelationshipArray validatedRelationships = entityLineageResult.getRelationships().stream()
         .filter(relationship -> entityService.exists(relationship.getEntity()))
+        .filter(relationship -> !entityService.isSoftDeleted(relationship.getEntity()))
         .collect(Collectors.toCollection(LineageRelationshipArray::new));
 
+    validatedEntityLineageResult.setFiltered(
+        entityLineageResult.getRelationships().size() - validatedRelationships.size());
     validatedEntityLineageResult.setRelationships(validatedRelationships);
 
     return validatedEntityLineageResult;

File: metadata-io/src/main/java/com/linkedin/metadata/search/EntityLineageResultCacheKey.java
Patch:
@@ -11,4 +11,5 @@ public class EntityLineageResultCacheKey {
   private final LineageDirection direction;
   private final Long startTimeMillis;
   private final Long endTimeMillis;
+  private final Integer maxHops;
 }

File: metadata-io/src/main/java/com/linkedin/metadata/search/LineageSearchService.java
Patch:
@@ -80,8 +80,7 @@ public LineageSearchResult searchAcrossLineage(@Nonnull Urn sourceUrn, @Nonnull
       @Nullable SortCriterion sortCriterion, int from, int size, @Nullable Long startTimeMillis,
       @Nullable Long endTimeMillis, @Nonnull SearchFlags searchFlags) {
     // Cache multihop result for faster performance
-    final EntityLineageResultCacheKey cacheKey =
-        new EntityLineageResultCacheKey(sourceUrn, direction, startTimeMillis, endTimeMillis);
+    final EntityLineageResultCacheKey cacheKey = new EntityLineageResultCacheKey(sourceUrn, direction, startTimeMillis, endTimeMillis, maxHops);
     CachedEntityLineageResult cachedLineageResult = cacheEnabled
         ? cache.get(cacheKey, CachedEntityLineageResult.class) : null;
     EntityLineageResult lineageResult;

File: metadata-jobs/mae-consumer-job/src/main/java/com/linkedin/metadata/kafka/MaeConsumerApplication.java
Patch:
@@ -14,8 +14,10 @@
 @SpringBootApplication(exclude = {ElasticsearchRestClientAutoConfiguration.class, CassandraAutoConfiguration.class,
     SolrHealthContributorAutoConfiguration.class})
 @ComponentScan(basePackages = {
+        "com.linkedin.gms.factory.kafka",
         "com.linkedin.metadata.boot.kafka",
-        "com.linkedin.metadata.kafka"
+        "com.linkedin.metadata.kafka",
+        "com.linkedin.metadata.dao.producer"
 }, excludeFilters = {
     @ComponentScan.Filter(type = FilterType.ASSIGNABLE_TYPE, classes = ScheduledAnalyticsFactory.class)})
 public class MaeConsumerApplication {

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/analytics/resolver/GetMetadataAnalyticsResolver.java
Patch:
@@ -77,7 +77,8 @@ private List<AnalyticsChart> getCharts(MetadataAnalyticsInput input, Authenticat
       filter = QueryUtils.newFilter("domains.keyword", input.getDomain());
     }
 
-    SearchResult searchResult = _entityClient.searchAcrossEntities(entities, query, filter, 0, 0, authentication);
+    SearchResult searchResult = _entityClient.searchAcrossEntities(entities, query, filter, 0, 0,
+        null, authentication);
 
     List<AggregationMetadata> aggregationMetadataList = searchResult.getMetadata().getAggregations();
 

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/auth/ListAccessTokensResolver.java
Patch:
@@ -54,10 +54,9 @@ public CompletableFuture<ListAccessTokenResult> get(DataFetchingEnvironment envi
         try {
           final SortCriterion sortCriterion =
               new SortCriterion().setField(EXPIRES_AT_FIELD_NAME).setOrder(SortOrder.DESCENDING);
-
           final SearchResult searchResult = _entityClient.search(Constants.ACCESS_TOKEN_ENTITY_NAME, "",
               buildFilter(filters, Collections.emptyList()), sortCriterion, start, count,
-              getAuthentication(environment), true);
+              getAuthentication(environment), true, null);
 
           final List<AccessTokenMetadata> tokens = searchResult.getEntities().stream().map(entity -> {
             final AccessTokenMetadata metadata = new AccessTokenMetadata();

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/container/ContainerEntitiesResolver.java
Patch:
@@ -85,6 +85,7 @@ public CompletableFuture<SearchResults> get(final DataFetchingEnvironment enviro
             )),
             start,
             count,
+            null,
             context.getAuthentication()
         ));
 

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/domain/DomainEntitiesResolver.java
Patch:
@@ -80,6 +80,7 @@ public CompletableFuture<SearchResults> get(final DataFetchingEnvironment enviro
             )),
             start,
             count,
+            null,
             context.getAuthentication()
         ));
 

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/domain/ListDomainsResolver.java
Patch:
@@ -63,7 +63,8 @@ public CompletableFuture<ListDomainsResult> get(final DataFetchingEnvironment en
                   start,
                   count,
                   context.getAuthentication(),
-                  true);
+                  true,
+              null);
 
           // Now that we have entities we can bind this to a result.
           final ListDomainsResult result = new ListDomainsResult();

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/group/ListGroupsResolver.java
Patch:
@@ -59,7 +59,7 @@ public CompletableFuture<ListGroupsResult> get(final DataFetchingEnvironment env
                       null,
                       new SortCriterion().setField(CORP_GROUP_CREATED_TIME_INDEX_FIELD_NAME).setOrder(SortOrder.DESCENDING),
                       start, count, context.getAuthentication(),
-                      true);
+                      true, null);
 
           // Then, get hydrate all groups.
           final Map<Urn, EntityResponse> entities = _entityClient.batchGetV2(CORP_GROUP_ENTITY_NAME,

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/ingest/secret/ListSecretsResolver.java
Patch:
@@ -72,7 +72,8 @@ public CompletableFuture<ListSecretsResult> get(final DataFetchingEnvironment en
                   start,
                   count,
                   context.getAuthentication(),
-                  true);
+                  true,
+              null);
 
           // Then, resolve all secrets
           final Map<Urn, EntityResponse> entities = _entityClient.batchGetV2(

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/ingest/source/ListIngestionSourcesResolver.java
Patch:
@@ -61,7 +61,8 @@ public CompletableFuture<ListIngestionSourcesResult> get(final DataFetchingEnvir
               start,
               count,
               context.getAuthentication(),
-                  true);
+              true,
+              null);
 
           // Then, resolve all ingestion sources
           final Map<Urn, EntityResponse> entities = _entityClient.batchGetV2(

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/post/ListPostsResolver.java
Patch:
@@ -51,7 +51,7 @@ public CompletableFuture<ListPostsResult> get(final DataFetchingEnvironment envi
 
         // First, get all Post Urns.
         final SearchResult gmsResult = _entityClient.search(POST_ENTITY_NAME, query, null, sortCriterion, start, count,
-            context.getAuthentication(), true);
+            context.getAuthentication(), true, null);
 
         // Then, get and hydrate all Posts.
         final Map<Urn, EntityResponse> entities = _entityClient.batchGetV2(POST_ENTITY_NAME,

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/role/ListRolesResolver.java
Patch:
@@ -50,7 +50,7 @@ public CompletableFuture<ListRolesResult> get(final DataFetchingEnvironment envi
         // First, get all role Urns.
         final SearchResult gmsResult =
             _entityClient.search(DATAHUB_ROLE_ENTITY_NAME, query, Collections.emptyMap(), start, count,
-                context.getAuthentication(), true);
+                context.getAuthentication(), true, null);
 
         // Then, get and hydrate all users.
         final Map<Urn, EntityResponse> entities = _entityClient.batchGetV2(DATAHUB_ROLE_ENTITY_NAME,

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/test/ListTestsResolver.java
Patch:
@@ -59,7 +59,8 @@ public CompletableFuture<ListTestsResult> get(final DataFetchingEnvironment envi
               start,
               count,
               context.getAuthentication(),
-                  true);
+                  true,
+              null);
 
           // Now that we have entities we can bind this to a result.
           final ListTestsResult result = new ListTestsResult();

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/user/ListUsersResolver.java
Patch:
@@ -54,7 +54,7 @@ public CompletableFuture<ListUsersResult> get(final DataFetchingEnvironment envi
           // First, get all policy Urns.
           final SearchResult gmsResult =
               _entityClient.search(CORP_USER_ENTITY_NAME, query, Collections.emptyMap(), start, count,
-                      context.getAuthentication(), true);
+                      context.getAuthentication(), true, null);
 
           // Then, get hydrate all users.
           final Map<Urn, EntityResponse> entities = _entityClient.batchGetV2(CORP_USER_ENTITY_NAME,

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/view/ListGlobalViewsResolver.java
Patch:
@@ -74,7 +74,8 @@ public CompletableFuture<ListViewsResult> get(final DataFetchingEnvironment envi
                 start,
                 count,
                 context.getAuthentication(),
-                true);
+                true,
+            null);
 
         final ListViewsResult result = new ListViewsResult();
         result.setStart(gmsResult.getFrom());

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/view/ListMyViewsResolver.java
Patch:
@@ -76,7 +76,8 @@ public CompletableFuture<ListViewsResult> get(final DataFetchingEnvironment envi
                 start,
                 count,
                 context.getAuthentication(),
-                true);
+                true,
+            null);
 
         final ListViewsResult result = new ListViewsResult();
         result.setStart(gmsResult.getFrom());

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/chart/ChartType.java
Patch:
@@ -144,8 +144,8 @@ public SearchResults search(@Nonnull String query,
                 start,
                 count,
                 context.getAuthentication(),
-                true
-        );
+                true,
+            null);
         return UrnSearchResultsMapper.map(searchResult);
     }
 

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/container/ContainerType.java
Patch:
@@ -118,7 +118,7 @@ public SearchResults search(@Nonnull String query,
                               @Nonnull final QueryContext context) throws Exception {
     final Map<String, String> facetFilters = ResolverUtils.buildFacetFilters(filters, FACET_FIELDS);
     final SearchResult searchResult = _entityClient.search(ENTITY_NAME, query, facetFilters, start, count,
-            context.getAuthentication(), true);
+            context.getAuthentication(), true, null);
     return UrnSearchResultsMapper.map(searchResult);
   }
 

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/corpgroup/CorpGroupType.java
Patch:
@@ -103,7 +103,7 @@ public SearchResults search(@Nonnull String query,
                                 @Nonnull final QueryContext context) throws Exception {
         final SearchResult
             searchResult = _entityClient.search("corpGroup", query, Collections.emptyMap(), start, count,
-            context.getAuthentication(), true);
+            context.getAuthentication(), true, null);
         return UrnSearchResultsMapper.map(searchResult);
     }
 

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/corpuser/CorpUserType.java
Patch:
@@ -106,7 +106,7 @@ public SearchResults search(@Nonnull String query,
                                 int count,
                                 @Nonnull final QueryContext context) throws Exception {
         final SearchResult searchResult = _entityClient.search("corpuser", query, Collections.emptyMap(), start, count,
-            context.getAuthentication(), true);
+            context.getAuthentication(), true, null);
         return UrnSearchResultsMapper.map(searchResult);
     }
 

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/dashboard/DashboardType.java
Patch:
@@ -140,7 +140,7 @@ public SearchResults search(@Nonnull String query,
                                 @Nonnull QueryContext context) throws Exception {
         final Map<String, String> facetFilters = ResolverUtils.buildFacetFilters(filters, FACET_FIELDS);
         final SearchResult searchResult = _entityClient.search("dashboard", query, facetFilters, start, count,
-                context.getAuthentication(), true);
+                context.getAuthentication(), true, null);
         return UrnSearchResultsMapper.map(searchResult);
     }
 

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/dataflow/DataFlowType.java
Patch:
@@ -135,7 +135,7 @@ public SearchResults search(@Nonnull String query,
                                 @Nonnull final QueryContext context) throws Exception {
         final Map<String, String> facetFilters = ResolverUtils.buildFacetFilters(filters, FACET_FIELDS);
         final SearchResult searchResult = _entityClient.search("dataFlow", query, facetFilters, start, count,
-                context.getAuthentication(), true);
+                context.getAuthentication(), true, null);
         return UrnSearchResultsMapper.map(searchResult);
     }
 

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/datajob/DataJobType.java
Patch:
@@ -135,7 +135,7 @@ public SearchResults search(@Nonnull String query,
                                 @Nonnull final QueryContext context) throws Exception {
         final Map<String, String> facetFilters = ResolverUtils.buildFacetFilters(filters, FACET_FIELDS);
         final SearchResult searchResult = _entityClient.search(
-            "dataJob", query, facetFilters, start, count, context.getAuthentication(), true);
+            "dataJob", query, facetFilters, start, count, context.getAuthentication(), true, null);
         return UrnSearchResultsMapper.map(searchResult);
     }
 

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/dataset/DatasetType.java
Patch:
@@ -155,7 +155,7 @@ public SearchResults search(@Nonnull String query,
                                 @Nonnull final QueryContext context) throws Exception {
         final Map<String, String> facetFilters = ResolverUtils.buildFacetFilters(filters, FACET_FIELDS);
         final SearchResult searchResult = _entityClient.search(ENTITY_NAME, query, facetFilters, start, count,
-                context.getAuthentication(), true);
+                context.getAuthentication(), true, null);
         return UrnSearchResultsMapper.map(searchResult);
     }
 

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/glossary/GlossaryTermType.java
Patch:
@@ -112,7 +112,7 @@ public SearchResults search(@Nonnull String query,
                                 @Nonnull final QueryContext context) throws Exception {
         final Map<String, String> facetFilters = ResolverUtils.buildFacetFilters(filters, FACET_FIELDS);
         final SearchResult searchResult = _entityClient.search(
-            "glossaryTerm", query, facetFilters, start, count, context.getAuthentication(), true);
+            "glossaryTerm", query, facetFilters, start, count, context.getAuthentication(), true, null);
         return UrnSearchResultsMapper.map(searchResult);
     }
 

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/mlmodel/MLFeatureTableType.java
Patch:
@@ -98,7 +98,7 @@ public SearchResults search(@Nonnull String query,
                                 @Nonnull final QueryContext context) throws Exception {
         final Map<String, String> facetFilters = ResolverUtils.buildFacetFilters(filters, FACET_FIELDS);
         final SearchResult searchResult = _entityClient.search("mlFeatureTable", query, facetFilters, start, count,
-                context.getAuthentication(), true);
+                context.getAuthentication(), true, null);
         return UrnSearchResultsMapper.map(searchResult);
     }
 

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/mlmodel/MLFeatureType.java
Patch:
@@ -90,7 +90,7 @@ public SearchResults search(@Nonnull String query,
                                 @Nonnull final QueryContext context) throws Exception {
         final Map<String, String> facetFilters = ResolverUtils.buildFacetFilters(filters, FACET_FIELDS);
         final SearchResult searchResult = _entityClient.search("mlFeature", query, facetFilters, start, count,
-                context.getAuthentication(), true);
+                context.getAuthentication(), true, null);
         return UrnSearchResultsMapper.map(searchResult);
     }
 

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/mlmodel/MLModelGroupType.java
Patch:
@@ -99,7 +99,7 @@ public SearchResults search(@Nonnull String query,
                                 @Nonnull final QueryContext context) throws Exception {
         final Map<String, String> facetFilters = ResolverUtils.buildFacetFilters(filters, FACET_FIELDS);
         final SearchResult searchResult = _entityClient.search("mlModelGroup", query, facetFilters, start, count,
-                context.getAuthentication(), true);
+                context.getAuthentication(), true, null);
         return UrnSearchResultsMapper.map(searchResult);
     }
 

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/mlmodel/MLModelType.java
Patch:
@@ -97,7 +97,7 @@ public SearchResults search(@Nonnull String query,
                                 @Nonnull final QueryContext context) throws Exception {
         final Map<String, String> facetFilters = ResolverUtils.buildFacetFilters(filters, FACET_FIELDS);
         final SearchResult searchResult = _entityClient.search("mlModel", query, facetFilters, start, count,
-                context.getAuthentication(), true);
+                context.getAuthentication(), true, null);
         return UrnSearchResultsMapper.map(searchResult);
     }
 

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/mlmodel/MLPrimaryKeyType.java
Patch:
@@ -90,7 +90,7 @@ public SearchResults search(@Nonnull String query,
                                 @Nonnull final QueryContext context) throws Exception {
         final Map<String, String> facetFilters = ResolverUtils.buildFacetFilters(filters, FACET_FIELDS);
         final SearchResult searchResult = _entityClient.search("mlPrimaryKey", query, facetFilters, start, count,
-                context.getAuthentication(), true);
+                context.getAuthentication(), true, null);
         return UrnSearchResultsMapper.map(searchResult);
     }
 

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/notebook/NotebookType.java
Patch:
@@ -86,7 +86,7 @@ public SearchResults search(@Nonnull String query,
     // https://datahubspace.slack.com/archives/C029A3M079U/p1646288772126639
     final Map<String, String> facetFilters = Collections.emptyMap();
     final SearchResult searchResult = _entityClient.search(NOTEBOOK_ENTITY_NAME, query, facetFilters, start, count,
-            context.getAuthentication(), true);
+            context.getAuthentication(), true, null);
     return UrnSearchResultsMapper.map(searchResult);
   }
 

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/tag/TagType.java
Patch:
@@ -110,7 +110,7 @@ public SearchResults search(@Nonnull String query,
                                 @Nonnull QueryContext context) throws Exception {
         final Map<String, String> facetFilters = ResolverUtils.buildFacetFilters(filters, FACET_FIELDS);
         final SearchResult searchResult = _entityClient.search("tag", query, facetFilters, start, count,
-                context.getAuthentication(), true);
+                context.getAuthentication(), true, null);
         return UrnSearchResultsMapper.map(searchResult);
     }
 

File: datahub-graphql-core/src/test/java/com/linkedin/datahub/graphql/resolvers/container/ContainerEntitiesResolverTest.java
Patch:
@@ -59,6 +59,7 @@ public void testGetSuccess() throws Exception {
         ),
         Mockito.eq(0),
         Mockito.eq(20),
+        Mockito.eq(null),
         Mockito.any(Authentication.class)
     )).thenReturn(
         new SearchResult()

File: datahub-graphql-core/src/test/java/com/linkedin/datahub/graphql/resolvers/domain/DomainEntitiesResolverTest.java
Patch:
@@ -62,6 +62,7 @@ public void testGetSuccess() throws Exception {
         ),
         Mockito.eq(0),
         Mockito.eq(20),
+        Mockito.eq(null),
         Mockito.any(Authentication.class)
     )).thenReturn(
         new SearchResult()

File: datahub-graphql-core/src/test/java/com/linkedin/datahub/graphql/resolvers/post/ListPostsResolverTest.java
Patch:
@@ -110,7 +110,7 @@ public void testListPosts() throws Exception {
         new SearchEntityArray(ImmutableList.of(new SearchEntity().setEntity(Urn.createFromString(POST_URN_STRING)))));
 
     when(_entityClient.search(eq(POST_ENTITY_NAME), any(), eq(null), any(), anyInt(), anyInt(),
-        eq(_authentication), Mockito.eq(Boolean.TRUE))).thenReturn(roleSearchResult);
+        eq(_authentication), Mockito.eq(Boolean.TRUE), Mockito.eq(null))).thenReturn(roleSearchResult);
     when(_entityClient.batchGetV2(eq(POST_ENTITY_NAME), any(), any(), any())).thenReturn(_entityResponseMap);
 
     ListPostsResult result = _resolver.get(_dataFetchingEnvironment).join();

File: datahub-graphql-core/src/test/java/com/linkedin/datahub/graphql/resolvers/role/ListRolesResolverTest.java
Patch:
@@ -19,6 +19,7 @@
 import com.linkedin.policy.DataHubRoleInfo;
 import graphql.schema.DataFetchingEnvironment;
 import java.util.Map;
+import org.mockito.Mockito;
 import org.testng.annotations.BeforeMethod;
 import org.testng.annotations.Test;
 
@@ -87,8 +88,8 @@ public void testListRoles() throws Exception {
         ImmutableList.of(new SearchEntity().setEntity(Urn.createFromString(ADMIN_ROLE_URN_STRING)),
             new SearchEntity().setEntity(Urn.createFromString(EDITOR_ROLE_URN_STRING)))));
 
-    when(_entityClient.search(eq(DATAHUB_ROLE_ENTITY_NAME), any(), any(), anyInt(), anyInt(), any(), eq(Boolean.TRUE))).thenReturn(
-        roleSearchResult);
+    when(_entityClient.search(eq(DATAHUB_ROLE_ENTITY_NAME), any(), any(), anyInt(), anyInt(), any(), eq(Boolean.TRUE),
+        Mockito.eq(null))).thenReturn(roleSearchResult);
     when(_entityClient.batchGetV2(eq(DATAHUB_ROLE_ENTITY_NAME), any(), any(), any())).thenReturn(_entityResponseMap);
 
     ListRolesResult result = _resolver.get(_dataFetchingEnvironment).join();

File: datahub-graphql-core/src/test/java/com/linkedin/datahub/graphql/resolvers/search/SearchAcrossLineageResolverTest.java
Patch:
@@ -97,6 +97,7 @@ public void testSearchAcrossLineage() throws Exception {
         eq(COUNT),
         eq(START_TIMESTAMP_MILLIS),
         eq(END_TIMESTAMP_MILLIS),
+        eq(null),
         eq(_authentication))).thenReturn(lineageSearchResult);
 
     final SearchAcrossLineageResults results = _resolver.get(_dataFetchingEnvironment).join();

File: metadata-io/src/test/java/com/linkedin/metadata/ESTestUtils.java
Patch:
@@ -87,7 +87,8 @@ public static LineageSearchResult lineage(LineageSearchService lineageSearchServ
 
         return lineageSearchService.searchAcrossLineage(root, LineageDirection.DOWNSTREAM,
             SEARCHABLE_ENTITY_TYPES.stream().map(EntityTypeMapper::getName).collect(Collectors.toList()),
-            "*", hops, ResolverUtils.buildFilter(filters, List.of()), null, 0, 100, null, null);
+            "*", hops, ResolverUtils.buildFilter(filters, List.of()), null, 0, 100, null,
+            null, new SearchFlags().setSkipCache(true));
     }
 
     public static AutoCompleteResults autocomplete(SearchableEntityType<?, String> searchableEntityType, String query) throws Exception {

File: metadata-service/auth-impl/src/main/java/com/datahub/authorization/PolicyFetcher.java
Patch:
@@ -47,7 +47,8 @@ public PolicyFetchResult fetchPolicies(int start, int count, String query, Authe
     log.debug(String.format("Batch fetching policies. start: %s, count: %s ", start, count));
     // First fetch all policy urns from start - start + count
     SearchResult result =
-        _entityClient.search(POLICY_ENTITY_NAME, query, null, POLICY_SORT_CRITERION, start, count, authentication, true);
+        _entityClient.search(POLICY_ENTITY_NAME, query, null, POLICY_SORT_CRITERION, start, count, authentication,
+            true, null);
     List<Urn> policyUrns = result.getEntities().stream().map(SearchEntity::getEntity).collect(Collectors.toList());
 
     if (policyUrns.isEmpty()) {

File: metadata-service/restli-servlet-impl/src/main/java/com/linkedin/metadata/resources/restli/RestliConstants.java
Patch:
@@ -37,5 +37,6 @@ private RestliConstants() { }
   public static final String PARAM_MODE = "mode";
   public static final String PARAM_DIRECTION = "direction";
   public static final String PARAM_ENTITY_TYPE = "entityType";
+  public static final String PARAM_SEARCH_FLAGS = "searchFlags";
   public static final String PARAM_VERSIONED_URN_PAIRS = "versionedUrns";
 }

File: datahub-frontend/app/auth/AuthModule.java
Patch:
@@ -105,9 +105,10 @@ protected void configure() {
                 SsoManager.class,
                 Authentication.class,
                 EntityClient.class,
-                AuthServiceClient.class));
+                AuthServiceClient.class,
+                com.typesafe.config.Config.class));
         } catch (NoSuchMethodException | SecurityException e) {
-            throw new RuntimeException("Failed to bind to SsoCallbackController. Cannot find constructor, e");
+            throw new RuntimeException("Failed to bind to SsoCallbackController. Cannot find constructor", e);
         }
         // logout
         final LogoutController logoutController = new LogoutController();

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/common/CacheConfig.java
Patch:
@@ -21,10 +21,10 @@
 @Configuration
 public class CacheConfig {
 
-  @Value("${CACHE_TTL_SECONDS:600}")
+  @Value("${cache.primary.ttlSeconds:600}")
   private int cacheTtlSeconds;
 
-  @Value("${CACHE_MAX_SIZE:10000}")
+  @Value("${cache.primary.maxSize:10000}")
   private int cacheMaxSize;
 
   @Value("${searchService.cache.hazelcast.serviceName:hazelcast-service}")

File: metadata-service/auth-impl/src/main/java/com/datahub/telemetry/TrackingService.java
Patch:
@@ -36,7 +36,6 @@ public class TrackingService {
   private static final String APP_VERSION_FIELD = "appVersion";
   private static final String EVENT_TYPE_FIELD = "type";
   private static final String FAILED_EVENT_NANE = "FailedEvent";
-  private static final String SIGN_UP_TITLE_FIELD = "title";
   private static final String ENTITY_TYPE_FIELD = "entityType";
   private static final String ENTITY_TYPE_FILTER_FIELD = "entityTypeFilter";
   private static final String PAGE_NUMBER_FIELD = "pageNumber";
@@ -58,7 +57,7 @@ public class TrackingService {
   private static final String VIEW_TYPE_FIELD = "viewType";
 
   private static final Set<String> ALLOWED_EVENT_FIELDS = new HashSet<>(
-      ImmutableList.of(EVENT_TYPE_FIELD, SIGN_UP_TITLE_FIELD, ENTITY_TYPE_FIELD, ENTITY_TYPE_FILTER_FIELD,
+      ImmutableList.of(EVENT_TYPE_FIELD, ENTITY_TYPE_FIELD, ENTITY_TYPE_FILTER_FIELD,
           PAGE_NUMBER_FIELD, PAGE_FIELD, TOTAL_FIELD, INDEX_FIELD, RESULT_TYPE_FIELD, RENDER_ID_FIELD, MODULE_ID_FIELD,
           RENDER_TYPE_FIELD, SCENARIO_TYPE_FIELD, SECTION_FIELD, ACCESS_TOKEN_TYPE_FIELD, DURATION_FIELD,
           ROLE_URN_FIELD, POLICY_URN_FIELD, SOURCE_TYPE_FIELD, INTERVAL_FIELD, VIEW_TYPE_FIELD));

File: metadata-io/src/main/java/com/linkedin/metadata/search/utils/ESUtils.java
Patch:
@@ -143,6 +143,7 @@ public static QueryBuilder getQueryBuilderFromCriterion(@Nonnull Criterion crite
         criterionToQuery.setCondition(criterion.getCondition());
         criterionToQuery.setNegated(criterion.isNegated());
         criterionToQuery.setValue(criterion.getValue());
+        criterionToQuery.setValues(criterion.getValues());
         criterionToQuery.setField(field + KEYWORD_SUFFIX);
         orQueryBuilder.should(getQueryBuilderFromCriterionForSingleField(criterionToQuery));
       }

File: metadata-jobs/mae-consumer/src/main/java/com/linkedin/metadata/kafka/boot/ApplicationStartupListener.java
Patch:
@@ -2,6 +2,7 @@
 
 import com.linkedin.gms.factory.config.ConfigurationProvider;
 import com.linkedin.metadata.boot.BootstrapManager;
+import com.linkedin.metadata.boot.kafka.DataHubUpgradeKafkaListener;
 import com.linkedin.metadata.kafka.config.MetadataChangeLogProcessorCondition;
 
 import javax.annotation.Nonnull;

File: metadata-jobs/mae-consumer/src/test/java/com/linkedin/metadata/kafka/hook/UpdateIndicesHookTest.java
Patch:
@@ -23,7 +23,7 @@
 import com.linkedin.metadata.config.SystemUpdateConfiguration;
 import com.linkedin.metadata.graph.Edge;
 import com.linkedin.metadata.graph.GraphService;
-import com.linkedin.metadata.kafka.boot.DataHubUpgradeKafkaListener;
+import com.linkedin.metadata.boot.kafka.DataHubUpgradeKafkaListener;
 import com.linkedin.metadata.key.ChartKey;
 import com.linkedin.metadata.models.AspectSpec;
 import com.linkedin.metadata.models.EntitySpec;

File: metadata-jobs/mce-consumer-job/src/main/java/com/linkedin/metadata/kafka/MceConsumerApplication.java
Patch:
@@ -20,6 +20,7 @@
         SolrHealthContributorAutoConfiguration.class
 })
 @ComponentScan(basePackages = {
+        "com.linkedin.metadata.boot.kafka",
         "com.linkedin.gms.factory.auth",
         "com.linkedin.gms.factory.common",
         "com.linkedin.gms.factory.config",

File: metadata-jobs/mce-consumer/src/main/java/com/linkedin/metadata/kafka/boot/ApplicationStartupListener.java
Patch:
@@ -2,6 +2,7 @@
 
 import com.linkedin.gms.factory.config.ConfigurationProvider;
 import com.linkedin.metadata.boot.BootstrapManager;
+import com.linkedin.metadata.boot.kafka.DataHubUpgradeKafkaListener;
 import com.linkedin.metadata.kafka.config.MetadataChangeProposalProcessorCondition;
 import lombok.extern.slf4j.Slf4j;
 import org.springframework.beans.factory.annotation.Qualifier;

File: metadata-io/src/main/java/com/linkedin/metadata/entity/EntityService.java
Patch:
@@ -877,7 +877,7 @@ public IngestProposalResult ingestProposal(@Nonnull MetadataChangeProposal mcp,
       } else {
         // When async is turned on, we write to proposal log and return without waiting
         _producer.produceMetadataChangeProposal(entityUrn, mcp);
-        return new IngestProposalResult(mcp.getEntityUrn(), false, true);
+        return new IngestProposalResult(entityUrn, false, true);
       }
     } else {
       // For timeseries aspects

File: metadata-service/graphql-servlet-impl/src/main/java/com/datahub/graphql/GraphQLController.java
Patch:
@@ -36,7 +36,8 @@
 public class GraphQLController {
 
   public GraphQLController() {
-
+    MetricUtils.get().counter(MetricRegistry.name(this.getClass(), "error"));
+    MetricUtils.get().counter(MetricRegistry.name(this.getClass(), "call"));
   }
 
   @Inject
@@ -148,6 +149,7 @@ private void observeErrors(ExecutionResult executionResult) {
   private void submitMetrics(ExecutionResult executionResult) {
     try {
       observeErrors(executionResult);
+      MetricUtils.get().counter(MetricRegistry.name(this.getClass(), "call")).inc();
       Object tracingInstrumentation = executionResult.getExtensions().get("tracing");
       if (tracingInstrumentation instanceof Map) {
         Map<String, Object> tracingMap = (Map<String, Object>) tracingInstrumentation;

File: datahub-frontend/app/security/AuthenticationManager.java
Patch:
@@ -58,8 +58,6 @@ public void handle(@Nonnull Callback[] callbacks) {
         } else if (callback instanceof PasswordCallback) {
           pc = (PasswordCallback) callback;
           pc.setPassword(this.password.toCharArray());
-        } else {
-          Logger.warn("The submitted callback is unsupported! type: " + callback.getClass(), callback);
         }
       }
     }

File: metadata-io/src/main/java/com/linkedin/metadata/search/elasticsearch/indexbuilder/ESIndexBuilder.java
Patch:
@@ -439,7 +439,7 @@ private static List<String> getOrphanedIndices(RestHighLevelClient searchClient,
               .minus(Duration.of(esConfig.getBuildIndices().getRetentionValue(),
                       ChronoUnit.valueOf(esConfig.getBuildIndices().getRetentionUnit()))));
 
-      GetIndexResponse response = searchClient.indices().get(new GetIndexRequest(indexState.indexPattern()), RequestOptions.DEFAULT);
+      GetIndexResponse response = searchClient.indices().get(new GetIndexRequest(indexState.indexCleanPattern()), RequestOptions.DEFAULT);
 
       for (String index : response.getIndices()) {
         var creationDateStr = response.getSetting(index, "index.creation_date");
@@ -456,7 +456,7 @@ private static List<String> getOrphanedIndices(RestHighLevelClient searchClient,
         }
       }
     } catch (Exception e) {
-      log.info("Failed to get orphaned indices with pattern {}: Exception {}", indexState.indexPattern(), e.toString());
+      log.info("Failed to get orphaned indices with pattern {}: Exception {}", indexState.indexCleanPattern(), e.toString());
     }
     return orphanedIndices;
   }

File: metadata-service/restli-servlet-impl/src/main/java/com/linkedin/metadata/resources/entity/AspectResource.java
Patch:
@@ -97,7 +97,7 @@ public Task<AnyRecord> get(@Nonnull String urnStr, @QueryParam("aspect") @Option
     return RestliUtil.toTask(() -> {
       final VersionedAspect aspect = _entityService.getVersionedAspect(urn, aspectName, version);
       if (aspect == null) {
-        throw RestliUtil.resourceNotFoundException();
+        throw RestliUtil.resourceNotFoundException(String.format("Did not find urn: %s aspect: %s version: %s", urn, aspectName, version));
       }
       return new AnyRecord(aspect.data());
     }, MetricRegistry.name(this.getClass(), "get"));

File: metadata-service/restli-servlet-impl/src/main/java/com/linkedin/metadata/resources/entity/EntityResource.java
Patch:
@@ -162,7 +162,7 @@ public Task<AnyRecord> get(@Nonnull String urnStr,
           aspectNames == null ? Collections.emptySet() : new HashSet<>(Arrays.asList(aspectNames));
       final Entity entity = _entityService.getEntity(urn, projectedAspects);
       if (entity == null) {
-        throw RestliUtil.resourceNotFoundException();
+        throw RestliUtil.resourceNotFoundException(String.format("Did not find %s", urnStr));
       }
       return new AnyRecord(entity.data());
     }, MetricRegistry.name(this.getClass(), "get"));

File: metadata-io/src/main/java/com/linkedin/metadata/entity/EntityService.java
Patch:
@@ -882,7 +882,7 @@ public IngestProposalResult ingestProposal(@Nonnull MetadataChangeProposal mcp,
         newSystemMetadata = result != null ? result.getNewSystemMetadata() : null;
       } else {
         // When async is turned on, we write to proposal log and return without waiting
-        _producer.produceMetadataChangeProposal(mcp);
+        _producer.produceMetadataChangeProposal(entityUrn, mcp);
         return new IngestProposalResult(mcp.getEntityUrn(), false, true);
       }
     } else {

File: metadata-io/src/main/java/com/linkedin/metadata/event/EventProducer.java
Patch:
@@ -63,7 +63,8 @@ void produceMetadataChangeLog(
    * @param metadataChangeProposal metadata change proposal to push into MCP kafka topic
    */
   @WithSpan
-  void produceMetadataChangeProposal(@Nonnull MetadataChangeProposal metadataChangeProposal);
+  void produceMetadataChangeProposal(@Nonnull final Urn urn, @Nonnull final MetadataChangeProposal
+      metadataChangeProposal);
 
   /**
    * Produces a generic platform "event".

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/common/mappers/UrnToEntityMapper.java
Patch:
@@ -152,8 +152,8 @@ public Entity apply(Urn input) {
     }
     if (input.getEntityType().equals("test")) {
       partialEntity = new Test();
-      ((Assertion) partialEntity).setUrn(input.toString());
-      ((Assertion) partialEntity).setType(EntityType.TEST);
+      ((Test) partialEntity).setUrn(input.toString());
+      ((Test) partialEntity).setType(EntityType.TEST);
     }
     if (input.getEntityType().equals(DATAHUB_ROLE_ENTITY_NAME)) {
       partialEntity = new DataHubRole();

File: metadata-integration/java/datahub-protobuf/src/main/java/datahub/protobuf/model/ProtobufElement.java
Patch:
@@ -24,7 +24,8 @@ default Stream<SourceCodeInfo.Location> messageLocations() {
         List<SourceCodeInfo.Location> fileLocations = fileProto().getSourceCodeInfo().getLocationList();
         return fileLocations.stream()
                 .filter(loc -> loc.getPathCount() > 1
-                        && loc.getPath(0) == FileDescriptorProto.MESSAGE_TYPE_FIELD_NUMBER);
+                        && loc.getPath(0) == FileDescriptorProto.MESSAGE_TYPE_FIELD_NUMBER
+                        && messageProto() == fileProto().getMessageType(loc.getPath(1)));
     }
 
     <T> Stream<T> accept(ProtobufModelVisitor<T> v, VisitContext context);

File: metadata-integration/java/datahub-protobuf/src/test/java/datahub/protobuf/visitors/dataset/DescriptionVisitorTest.java
Patch:
@@ -16,11 +16,11 @@ public class DescriptionVisitorTest {
 
     @Test
     public void visitorTest() throws IOException {
-        ProtobufGraph graph = getTestProtobufGraph("protobuf", "messageB");
+        ProtobufGraph graph = getTestProtobufGraph("protobuf", "messageC2", "protobuf.MessageC2");
 
         DescriptionVisitor test = new DescriptionVisitor();
 
-        assertEquals(Set.of("This contains nested types.\n\nOwned by TeamB"),
-                graph.accept(getVisitContextBuilder("protobuf.MessageB"), List.of(test)).collect(Collectors.toSet()));
+        assertEquals(Set.of("This contains nested type\n\nDescription for MessageC2"),
+                graph.accept(getVisitContextBuilder("protobuf.MessageC2"), List.of(test)).collect(Collectors.toSet()));
     }
 }

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/analytics/resolver/GetChartsResolver.java
Patch:
@@ -180,7 +180,7 @@ private List<AnalyticsChart> getGlobalMetadataAnalyticsCharts(Authentication aut
             ImmutableList.of("glossaryTerms.keyword"), Collections.emptyMap(),
             ImmutableMap.of("removed", ImmutableList.of("true")), Optional.empty(), false);
     AnalyticsUtil.hydrateDisplayNameForBars(_entityClient, entitiesPerTerm, Constants.GLOSSARY_TERM_ENTITY_NAME,
-        ImmutableSet.of(Constants.GLOSSARY_TERM_KEY_ASPECT_NAME), AnalyticsUtil::getTermName, authentication);
+        ImmutableSet.of(Constants.GLOSSARY_TERM_KEY_ASPECT_NAME, Constants.GLOSSARY_TERM_INFO_ASPECT_NAME), AnalyticsUtil::getTermName, authentication);
     if (!entitiesPerTerm.isEmpty()) {
       charts.add(BarChart.builder().setTitle("Entities per Term").setBars(entitiesPerTerm).build());
     }

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/analytics/resolver/GetMetadataAnalyticsResolver.java
Patch:
@@ -107,7 +107,7 @@ private List<AnalyticsChart> getCharts(MetadataAnalyticsInput input, Authenticat
     if (termAggregation.isPresent()) {
       List<NamedBar> termChart = buildBarChart(termAggregation.get());
       AnalyticsUtil.hydrateDisplayNameForBars(_entityClient, termChart, Constants.GLOSSARY_TERM_ENTITY_NAME,
-          ImmutableSet.of(Constants.GLOSSARY_TERM_KEY_ASPECT_NAME), AnalyticsUtil::getTermName, authentication);
+          ImmutableSet.of(Constants.GLOSSARY_TERM_KEY_ASPECT_NAME, Constants.GLOSSARY_TERM_INFO_ASPECT_NAME), AnalyticsUtil::getTermName, authentication);
       charts.add(BarChart.builder().setTitle("Entities by Term").setBars(termChart).build());
     }
 

File: metadata-jobs/mae-consumer/src/main/java/com/linkedin/metadata/kafka/hook/UpdateIndicesHook.java
Patch:
@@ -21,6 +21,7 @@
 import com.linkedin.metadata.Constants;
 import com.linkedin.metadata.graph.Edge;
 import com.linkedin.metadata.graph.GraphService;
+import com.linkedin.metadata.graph.elastic.ElasticSearchGraphService;
 import com.linkedin.metadata.key.SchemaFieldKey;
 import com.linkedin.metadata.models.AspectSpec;
 import com.linkedin.metadata.models.EntitySpec;
@@ -168,7 +169,7 @@ private void handleUpdateChangeEvent(@Nonnull final MetadataChangeLog event) {
         event.hasSystemMetadata() ? event.getSystemMetadata().getRunId() : null);
 
     // Step 2. For all aspects, attempt to update Graph
-    if (_diffMode) {
+    if (_diffMode && _graphService instanceof ElasticSearchGraphService) {
       updateGraphServiceDiff(urn, aspectSpec, previousAspect, aspect, event);
     } else {
       updateGraphService(urn, aspectSpec, aspect, event);

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/dataset/mappers/DatasetMapper.java
Patch:
@@ -136,9 +136,6 @@ private void mapDatasetProperties(@Nonnull Dataset dataset, @Nonnull DataMap dat
         properties.setQualifiedName(gmsProperties.getQualifiedName());
         dataset.setProperties(properties);
         dataset.setDescription(properties.getDescription());
-        if (gmsProperties.getUri() != null) {
-            dataset.setUri(gmsProperties.getUri().toString());
-        }
     }
 
     private void mapEditableDatasetProperties(@Nonnull Dataset dataset, @Nonnull DataMap dataMap) {

File: datahub-frontend/app/controllers/CentralLogoutController.java
Patch:
@@ -43,10 +43,11 @@ public Result executeLogout(Http.Request request) {
         return redirect(
             String.format("/login?error_msg=%s",
                 URLEncoder.encode("Failed to sign out using Single Sign-On provider. Please contact your DataHub Administrator, "
-                    + "or refer to server logs for more information.", StandardCharsets.UTF_8)));
+                    + "or refer to server logs for more information.", StandardCharsets.UTF_8)))
+        .withNewSession();
       }
     }
     return Results.redirect(DEFAULT_BASE_URL_PATH)
             .withNewSession();
   }
-}
\ No newline at end of file
+}

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/corpuser/mappers/CorpUserPropertiesMapper.java
Patch:
@@ -21,8 +21,6 @@ public static CorpUserProperties map(@Nonnull final com.linkedin.identity.CorpUs
   @Override
   public CorpUserProperties apply(@Nonnull final com.linkedin.identity.CorpUserInfo info) {
     final CorpUserProperties result = new CorpUserProperties();
-    result.setDisplayName(info.getDisplayName());
-    result.setTitle(info.getTitle());
     result.setActive(info.isActive());
     result.setCountryCode(info.getCountryCode());
     result.setDepartmentId(info.getDepartmentId());

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/WeaklyTypedAspectsResolver.java
Patch:
@@ -65,7 +65,7 @@ public CompletableFuture<List<RawAspect>> get(DataFetchingEnvironment environmen
                     }
 
                     DataMap resolvedAspect = entityResponse.getAspects().get(aspectSpec.getName()).getValue().data();
-                    if (resolvedAspect == null || resolvedAspect.keySet().size() != 1) {
+                    if (resolvedAspect == null) {
                         return;
                     }
 

File: datahub-graphql-core/src/test/java/com/linkedin/datahub/graphql/resolvers/ingest/execution/CreateTestConnectionRequestResolverTest.java
Patch:
@@ -17,7 +17,7 @@
 public class CreateTestConnectionRequestResolverTest {
 
   private static final CreateTestConnectionRequestInput TEST_INPUT = new CreateTestConnectionRequestInput(
-      "test recipe",
+      "{}",
       "0.8.44"
   );
 

File: metadata-jobs/mae-consumer/src/main/java/com/linkedin/metadata/kafka/hook/GraphIndexUtils.java
Patch:
@@ -56,11 +56,9 @@ private static List<Map<String, Object>> getPropertiesList(@Nullable final Strin
   @Nullable
   private static boolean isValueListValid(@Nullable final List<?> entryList, final int valueListSize) {
     if (entryList == null) {
-      log.warn("Unable to get entry as entryList is null");
       return false;
     }
     if (valueListSize != entryList.size()) {
-      log.warn("Unable to get entry for graph edge as values list and entry list have differing sizes");
       return false;
     }
     return true;

File: metadata-utils/src/main/java/com/linkedin/metadata/utils/EntityKeyUtils.java
Patch:
@@ -93,7 +93,7 @@ public static RecordTemplate convertUrnToEntityKey(@Nonnull final Urn urn, @Nonn
     // #2. Bind fields into a DataMap
     if (urn.getEntityKey().getParts().size() != keySchema.getFields().size()) {
       throw new IllegalArgumentException(
-          "Failed to convert urn to entity key: urns parts and key fields do not have same length");
+          "Failed to convert urn to entity key: urns parts and key fields do not have same length for " + urn);
     }
     final DataMap dataMap = new DataMap();
     for (int i = 0; i < urn.getEntityKey().getParts().size(); i++) {

File: datahub-frontend/app/client/KafkaTrackingProducer.java
Patch:
@@ -94,6 +94,7 @@ private static KafkaProducer createKafkaProducer(Config config) {
                 setConfig(config, props, SaslConfigs.SASL_JAAS_CONFIG, "analytics.kafka.sasl.jaas.config");
                 setConfig(config, props, SaslConfigs.SASL_KERBEROS_SERVICE_NAME, "analytics.kafka.sasl.kerberos.service.name");
                 setConfig(config, props, SaslConfigs.SASL_LOGIN_CALLBACK_HANDLER_CLASS, "analytics.kafka.sasl.login.callback.handler.class");
+                setConfig(config, props, SaslConfigs.SASL_CLIENT_CALLBACK_HANDLER_CLASS, "analytics.kafka.sasl.client.callback.handler.class");
             }
         }
 

File: metadata-io/src/main/java/com/linkedin/metadata/entity/EntityService.java
Patch:
@@ -147,6 +147,7 @@ public static class UpdateAspectResult {
   public static class IngestProposalResult {
     Urn urn;
     boolean didUpdate;
+    boolean queued;
   }
 
   private static final int DEFAULT_MAX_TRANSACTION_RETRY = 3;
@@ -881,7 +882,7 @@ public IngestProposalResult ingestProposal(@Nonnull MetadataChangeProposal mcp,
       } else {
         // When async is turned on, we write to proposal log and return without waiting
         _producer.produceMetadataChangeProposal(mcp);
-        return new IngestProposalResult(mcp.getEntityUrn(), false);
+        return new IngestProposalResult(mcp.getEntityUrn(), false, true);
       }
     } else {
       // For timeseries aspects
@@ -893,7 +894,7 @@ public IngestProposalResult ingestProposal(@Nonnull MetadataChangeProposal mcp,
         emitChangeLog(oldAspect, oldSystemMetadata, newAspect, newSystemMetadata, mcp, entityUrn, auditStamp,
             aspectSpec);
 
-    return new IngestProposalResult(entityUrn, didUpdate);
+    return new IngestProposalResult(entityUrn, didUpdate, false);
   }
 
   private AspectSpec validateAspect(MetadataChangeProposal mcp, EntitySpec entitySpec) {

File: metadata-service/restli-servlet-impl/src/main/java/com/linkedin/metadata/resources/entity/AspectResource.java
Patch:
@@ -164,7 +164,7 @@ public Task<String> ingestProposal(
         AspectUtils.getAdditionalChanges(metadataChangeProposal, _entityService)
                 .forEach(proposal -> _entityService.ingestProposal(proposal, auditStamp, asyncBool));
 
-        if (!result.isDidUpdate()) {
+        if (!result.isQueued()) {
           tryIndexRunId(urn, metadataChangeProposal.getSystemMetadata(), _entitySearchService);
         }
         return urn.toString();

File: metadata-io/src/main/java/com/linkedin/metadata/entity/EntityService.java
Patch:
@@ -883,7 +883,7 @@ public IngestProposalResult ingestProposal(@Nonnull MetadataChangeProposal mcp,
         _producer.produceMetadataChangeProposal(mcp);
         return new IngestProposalResult(mcp.getEntityUrn(), false);
       }
-  } else {
+    } else {
       // For timeseries aspects
       newAspect = convertToRecordTemplate(mcp, aspectSpec);
       newSystemMetadata = mcp.getSystemMetadata();

File: metadata-io/src/test/java/com/linkedin/metadata/systemmetadata/ElasticSearchSystemMetadataServiceTest.java
Patch:
@@ -21,7 +21,6 @@
 import java.util.List;
 
 import static com.linkedin.metadata.ElasticSearchTestConfiguration.syncAfterWrite;
-import static com.linkedin.metadata.systemmetadata.ElasticSearchSystemMetadataService.INDEX_NAME;
 import static org.testng.Assert.assertEquals;
 
 @Import(ElasticSearchTestConfiguration.class)
@@ -34,7 +33,7 @@ public class ElasticSearchSystemMetadataServiceTest extends AbstractTestNGSpring
   @Autowired
   private ESIndexBuilder _esIndexBuilder;
   private final IndexConvention _indexConvention = new IndexConventionImpl("es_system_metadata_service_test");
-  private final String _indexName = _indexConvention.getIndexName(INDEX_NAME);
+
   private ElasticSearchSystemMetadataService _client;
 
   @BeforeClass

File: metadata-service/restli-client/src/main/java/com/linkedin/usage/UsageClient.java
Patch:
@@ -5,6 +5,7 @@
 
 import com.linkedin.common.WindowDuration;
 import com.linkedin.common.client.BaseClient;
+import com.linkedin.parseq.retry.backoff.BackoffPolicy;
 import com.linkedin.r2.RemoteInvocationException;
 import com.linkedin.restli.client.Client;
 import java.net.URISyntaxException;
@@ -16,8 +17,8 @@ public class UsageClient extends BaseClient {
     private static final UsageStatsRequestBuilders USAGE_STATS_REQUEST_BUILDERS =
         new UsageStatsRequestBuilders();
 
-    public UsageClient(@Nonnull final Client restliClient) {
-        super(restliClient);
+    public UsageClient(@Nonnull final Client restliClient, @Nonnull final BackoffPolicy backoffPolicy, int retryCount) {
+        super(restliClient, backoffPolicy, retryCount);
     }
 
     /**

File: metadata-jobs/mce-consumer/src/main/java/com/linkedin/metadata/kafka/MetadataChangeEventsProcessor.java
Patch:
@@ -19,6 +19,8 @@
 import com.linkedin.r2.RemoteInvocationException;
 import java.io.IOException;
 import javax.annotation.Nonnull;
+
+import lombok.NonNull;
 import lombok.RequiredArgsConstructor;
 import lombok.extern.slf4j.Slf4j;
 import org.apache.avro.generic.GenericRecord;
@@ -44,6 +46,7 @@
 @RequiredArgsConstructor
 public class MetadataChangeEventsProcessor {
 
+  @NonNull
   private final Authentication systemAuthentication;
   private final RestliEntityClient entityClient;
   private final Producer<String, IndexedRecord> kafkaProducer;

File: metadata-jobs/mce-consumer/src/main/java/com/linkedin/metadata/kafka/MetadataChangeProposalsProcessor.java
Patch:
@@ -16,6 +16,7 @@
 import com.linkedin.mxe.Topics;
 import java.io.IOException;
 import javax.annotation.Nonnull;
+
 import lombok.RequiredArgsConstructor;
 import lombok.extern.slf4j.Slf4j;
 import org.apache.avro.generic.GenericRecord;

File: ingestion-scheduler/src/main/java/com/datahub/metadata/ingestion/IngestionScheduler.java
Patch:
@@ -21,6 +21,7 @@
 import com.linkedin.metadata.key.ExecutionRequestKey;
 import com.linkedin.metadata.query.ListResult;
 import com.linkedin.metadata.utils.GenericRecordUtils;
+import com.linkedin.metadata.utils.IngestionUtils;
 import com.linkedin.mxe.MetadataChangeProposal;
 import com.linkedin.r2.RemoteInvocationException;
 import java.util.ArrayList;
@@ -347,7 +348,8 @@ public void run() {
         input.setRequestedAt(System.currentTimeMillis());
 
         Map<String, String> arguments = new HashMap<>();
-        arguments.put(RECIPE_ARGUMENT_NAME, _ingestionSourceInfo.getConfig().getRecipe());
+        String recipe = IngestionUtils.injectPipelineName(_ingestionSourceInfo.getConfig().getRecipe(), _ingestionSourceUrn.toString());
+        arguments.put(RECIPE_ARGUMENT_NAME, recipe);
         arguments.put(VERSION_ARGUMENT_NAME, _ingestionSourceInfo.getConfig().hasVersion()
             ? _ingestionSourceInfo.getConfig().getVersion()
             : _ingestionConfiguration.getDefaultCliVersion());

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/load/EntityLineageResultResolver.java
Patch:
@@ -11,6 +11,7 @@
 import graphql.schema.DataFetcher;
 import graphql.schema.DataFetchingEnvironment;
 import java.net.URISyntaxException;
+import java.util.HashSet;
 import java.util.concurrent.CompletableFuture;
 import java.util.stream.Collectors;
 import javax.annotation.Nullable;
@@ -57,7 +58,8 @@ public CompletableFuture<EntityLineageResult> get(DataFetchingEnvironment enviro
                 start != null ? start : 0,
                 count != null ? count : 100,
                 1,
-                separateSiblings != null ? input.getSeparateSiblings() : false
+                separateSiblings != null ? input.getSeparateSiblings() : false,
+                new HashSet<>()
             ));
       } catch (URISyntaxException e) {
         log.error("Failed to fetch lineage for {}", urn);

File: metadata-integration/java/spark-lineage/src/main/java/datahub/spark/DatasetExtractor.java
Patch:
@@ -143,7 +143,7 @@ Optional<? extends Collection<SparkDataset>> fromSparkPlanNode(SparkPlan plan, S
       SaveIntoDataSourceCommand cmd = (SaveIntoDataSourceCommand) p;
 
       Map<String, String> options = JavaConversions.mapAsJavaMap(cmd.options());
-      String url = options.get("url"); // e.g. jdbc:postgresql://localhost:5432/sparktestdb
+      String url = options.getOrDefault("url", ""); // e.g. jdbc:postgresql://localhost:5432/sparktestdb
       if (!url.contains("jdbc")) {
         return Optional.empty();
       }

File: datahub-frontend/app/auth/AuthModule.java
Patch:
@@ -89,7 +89,7 @@ protected void configure() {
                 final String aesKeyHash = DigestUtils.sha1Hex(aesKeyBase.getBytes(StandardCharsets.UTF_8));
                 final String aesEncryptionKey = aesKeyHash.substring(0, 16);
                 playCacheCookieStore = new PlayCookieSessionStore(
-                        new ShiroAesDataEncrypter(aesEncryptionKey));
+                        new ShiroAesDataEncrypter(aesEncryptionKey.getBytes()));
             } catch (Exception e) {
                 throw new RuntimeException("Failed to instantiate Pac4j cookie session store!", e);
             }

File: datahub-frontend/app/auth/sso/SsoProvider.java
Patch:
@@ -1,6 +1,7 @@
 package auth.sso;
 
 import org.pac4j.core.client.Client;
+import org.pac4j.core.credentials.Credentials;
 
 /**
  * A thin interface over a Pac4j {@link Client} object and its
@@ -40,6 +41,6 @@ public String getCommonName() {
   /**
    * Retrieves an initialized Pac4j {@link Client}.
    */
-  Client<?, ?> client();
+  Client<? extends Credentials> client();
 
 }

File: datahub-frontend/app/controllers/TrackingController.java
Patch:
@@ -74,7 +74,7 @@ public Result track(Http.Request request) throws Exception {
         } catch (Exception e) {
             return badRequest();
         }
-        final String actor = ctx().session().get(ACTOR);
+        final String actor = request.session().data().get(ACTOR);
         try {
             _logger.debug(String.format("Emitting product analytics event. actor: %s, event: %s", actor, event));
             final ProducerRecord<String, String> record = new ProducerRecord<>(

File: datahub-frontend/test/security/OidcConfigurationTest.java
Patch:
@@ -19,11 +19,12 @@
 import java.util.Map;
 import java.util.Set;
 import java.util.concurrent.TimeUnit;
+
+import org.junit.jupiter.api.Test;
 import org.pac4j.oidc.client.OidcClient;
-import org.testng.annotations.Test;
 
 import static auth.sso.oidc.OidcConfigs.*;
-import static org.junit.Assert.*;
+import static org.junit.jupiter.api.Assertions.assertEquals;
 
 
 public class OidcConfigurationTest {

File: datahub-frontend/test/utils/SearchUtilTest.java
Patch:
@@ -1,8 +1,8 @@
 package utils;
 
-import org.testng.annotations.Test;
+import org.junit.jupiter.api.Test;
 
-import static org.junit.Assert.*;
+import static org.junit.jupiter.api.Assertions.assertEquals;
 
 public class SearchUtilTest {
     @Test

File: metadata-auth/auth-api/src/main/java/com/datahub/authentication/AuthenticatorContext.java
Patch:
@@ -23,7 +23,7 @@ public AuthenticatorContext(@Nonnull final Map<String, Object> context) {
   /**
    *
    * @return contextMap   The contextMap contains below key and value
-   *                      {@link com.datahub.plugins.PluginConstant::PLUGIN_HOME}: Directory path where plugin is installed
+   *                      {@link com.datahub.plugins.PluginConstant#PLUGIN_HOME PLUGIN_HOME}: Directory path where plugin is installed
    *
    */
   @Nonnull

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/mutate/util/OwnerUtils.java
Patch:
@@ -104,7 +104,7 @@ private static void addOwner(Ownership ownershipAspect, Urn ownerUrn, OwnershipT
 
     final OwnerArray ownerArray = new OwnerArray(ownershipAspect.getOwners()
         .stream()
-        .filter(owner ->  !owner.getOwner().equals(ownerUrn))
+        .filter(owner ->  !(owner.getOwner().equals(ownerUrn) && owner.getType().equals(type)))
         .collect(Collectors.toList()));
 
     Owner newOwner = new Owner();

File: datahub-frontend/app/security/AuthenticationManager.java
Patch:
@@ -51,15 +51,15 @@ public void handle(@Nonnull Callback[] callbacks) {
       NameCallback nc = null;
       PasswordCallback pc = null;
       for (Callback callback : callbacks) {
-        Logger.error("The submitted callback is of type: " + callback.getClass() + " : " + callback);
+        Logger.debug("The submitted callback is of type: " + callback.getClass() + " : " + callback);
         if (callback instanceof NameCallback) {
           nc = (NameCallback) callback;
           nc.setName(this.username);
         } else if (callback instanceof PasswordCallback) {
           pc = (PasswordCallback) callback;
           pc.setPassword(this.password.toCharArray());
         } else {
-          Logger.warn("The submitted callback is unsupported! ", callback);
+          Logger.warn("The submitted callback is unsupported! type: " + callback.getClass(), callback);
         }
       }
     }

File: metadata-service/restli-client/src/main/java/com/linkedin/entity/client/RestliEntityClient.java
Patch:
@@ -628,7 +628,7 @@ public List<EnvelopedAspect> getTimeseriesAspectValues(@Nonnull String urn, @Non
   public String ingestProposal(@Nonnull final MetadataChangeProposal metadataChangeProposal,
       @Nonnull final Authentication authentication, final boolean async) throws RemoteInvocationException {
     final AspectsDoIngestProposalRequestBuilder requestBuilder =
-        ASPECTS_REQUEST_BUILDERS.actionIngestProposal().proposalParam(metadataChangeProposal);
+        ASPECTS_REQUEST_BUILDERS.actionIngestProposal().proposalParam(metadataChangeProposal).asyncParam(String.valueOf(async));
     return sendClientRequest(requestBuilder, authentication).getEntity();
   }
 

File: metadata-io/src/test/java/com/linkedin/metadata/graph/sibling/SiblingGraphServiceTest.java
Patch:
@@ -259,7 +259,7 @@ public void testSiblingInResult() throws Exception {
     EntityLineageResult expectedResult = mockResult.clone();
     expectedResult.setTotal(3);
     expectedResult.setCount(2);
-    expectedResult.setRelationships(new LineageRelationshipArray(relationship2, relationship1));
+    expectedResult.setRelationships(new LineageRelationshipArray(relationship1, relationship2));
 
     EntityLineageResult upstreamLineage = service.getLineage(datasetFourUrn, LineageDirection.UPSTREAM, 0, 100, 1);
 
@@ -302,7 +302,7 @@ public void testCombineSiblingResult() throws Exception {
 
     expectedRelationships.add(relationship2);
     expectedRelationships.add(relationship4);
-    expectedRelationships.add(relationship1);
+    expectedRelationships.add(relationship1); // expect just one relationship1 despite duplicates in sibling lineage
 
     expectedResult.setCount(3);
     expectedResult.setStart(0);
@@ -316,6 +316,7 @@ public void testCombineSiblingResult() throws Exception {
 
     siblingRelationships.add(relationship2);
     siblingRelationships.add(relationship4);
+    siblingRelationships.add(relationship1); // duplicate from sibling's lineage, we should not see duplicates in result
     siblingMockResult.setStart(0);
     siblingMockResult.setTotal(2);
     siblingMockResult.setCount(2);

File: metadata-service/auth-impl/src/main/java/com/datahub/authentication/invite/InviteTokenService.java
Patch:
@@ -50,7 +50,7 @@ public boolean isInviteTokenValid(@Nonnull final Urn inviteTokenUrn, @Nonnull fi
   @Nullable
   public Urn getInviteTokenRole(@Nonnull final Urn inviteTokenUrn, @Nonnull final Authentication authentication)
       throws URISyntaxException, RemoteInvocationException {
-    com.linkedin.identity.InviteToken inviteToken = getInviteTokenEntity(inviteTokenUrn, authentication);
+    final com.linkedin.identity.InviteToken inviteToken = getInviteTokenEntity(inviteTokenUrn, authentication);
     return inviteToken.hasRole() ? inviteToken.getRole() : null;
   }
 

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/mlmodel/mappers/MLModelPropertiesMapper.java
Patch:
@@ -23,6 +23,9 @@ public MLModelProperties apply(@NonNull final com.linkedin.ml.metadata.MLModelPr
 
         result.setDate(mlModelProperties.getDate());
         result.setDescription(mlModelProperties.getDescription());
+        if (mlModelProperties.getExternalUrl() != null) {
+            result.setExternalUrl(mlModelProperties.getExternalUrl().toString());
+        }        
         if (mlModelProperties.getVersion() != null) {
             result.setVersion(mlModelProperties.getVersion().getVersionTag());
         }

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/timeline/EntityChangeEventGeneratorRegistryFactory.java
Patch:
@@ -33,7 +33,7 @@ public class EntityChangeEventGeneratorRegistryFactory {
   ApplicationContext applicationContext;
 
   @Bean(name = "entityChangeEventGeneratorRegistry")
-  @DependsOn({"entityAspectDao", "entityService", "entityRegistry", "restliEntityClient", "systemAuthentication"})
+  @DependsOn({"restliEntityClient", "systemAuthentication"})
   @Singleton
   @Nonnull
   protected com.linkedin.metadata.timeline.eventgenerator.EntityChangeEventGeneratorRegistry entityChangeEventGeneratorRegistry() {

File: metadata-io/src/main/java/com/linkedin/metadata/search/utils/ESUtils.java
Patch:
@@ -33,7 +33,7 @@ public class ESUtils {
 
   // we use this to make sure we filter for editable & non-editable fields
   public static final String[][] EDITABLE_FIELD_TO_QUERY_PAIRS = {
-      {"fieldGlossaryTags", "editedFieldGlossaryTags"},
+      {"fieldTags", "editedFieldTags"},
       {"fieldGlossaryTerms", "editedFieldGlossaryTerms"},
       {"fieldDescriptions", "editedFieldDescriptions"},
       {"description", "editedDescription"},

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/assertion/AssertionRunEventResolver.java
Patch:
@@ -94,7 +94,7 @@ public CompletableFuture<AssertionRunEventsResult> get(DataFetchingEnvironment e
   }
 
   @Nullable
-  private Filter buildFilter(@Nullable FilterInput filtersInput, @Nullable final String status) {
+  public static Filter buildFilter(@Nullable FilterInput filtersInput, @Nullable final String status) {
     if (filtersInput == null && status == null) {
       return null;
     }
@@ -109,7 +109,7 @@ private Filter buildFilter(@Nullable FilterInput filtersInput, @Nullable final S
       facetFilters.addAll(filtersInput.getAnd());
     }
     return new Filter().setOr(new ConjunctiveCriterionArray(new ConjunctiveCriterion().setAnd(new CriterionArray(facetFilters.stream()
-        .map(filter -> criterionFromFilter(filter))
+        .map(filter -> criterionFromFilter(filter, true))
         .collect(Collectors.toList())))));
   }
 }

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/load/TimeSeriesAspectResolver.java
Patch:
@@ -110,7 +110,7 @@ private Filter buildFilters(@Nullable FilterInput maybeFilters) {
       return null;
     }
     return new Filter().setOr(new ConjunctiveCriterionArray(new ConjunctiveCriterion().setAnd(new CriterionArray(maybeFilters.getAnd().stream()
-        .map(filter -> criterionFromFilter(filter))
+        .map(filter -> criterionFromFilter(filter, true))
         .collect(Collectors.toList())))));
   }
 }

File: datahub-graphql-core/src/test/java/com/linkedin/datahub/graphql/resolvers/assertion/AssertionRunEventResolverTest.java
Patch:
@@ -51,7 +51,7 @@ public void testGetSuccess() throws Exception {
         Mockito.eq(10L),
         Mockito.eq(5),
         Mockito.eq(false),
-        Mockito.any(Filter.class),
+        Mockito.eq(AssertionRunEventResolver.buildFilter(null, AssertionRunStatus.COMPLETE.toString())),
         Mockito.any(Authentication.class)
     )).thenReturn(
         ImmutableList.of(
@@ -108,4 +108,4 @@ public void testGetSuccess() throws Exception {
     assertEquals((long) graphqlRunEvent.getResult().getUnexpectedCount(), 2L);
     assertEquals(graphqlRunEvent.getResult().getType(), com.linkedin.datahub.graphql.generated.AssertionResultType.SUCCESS);
   }
-}
\ No newline at end of file
+}

File: metadata-io/src/test/java/com/linkedin/metadata/graph/sibling/SiblingGraphServiceTest.java
Patch:
@@ -259,7 +259,7 @@ public void testSiblingInResult() throws Exception {
     EntityLineageResult expectedResult = mockResult.clone();
     expectedResult.setTotal(3);
     expectedResult.setCount(2);
-    expectedResult.setRelationships(new LineageRelationshipArray(relationship1, relationship2));
+    expectedResult.setRelationships(new LineageRelationshipArray(relationship2, relationship1));
 
     EntityLineageResult upstreamLineage = service.getLineage(datasetFourUrn, LineageDirection.UPSTREAM, 0, 100, 1);
 

File: datahub-frontend/app/auth/sso/oidc/OidcConfigs.java
Patch:
@@ -42,7 +42,7 @@ public class OidcConfigs extends SsoConfigs {
     /**
      * Default values
      */
-    private static final String DEFAULT_OIDC_USERNAME_CLAIM = "preferred_username";
+    private static final String DEFAULT_OIDC_USERNAME_CLAIM = "email";
     private static final String DEFAULT_OIDC_USERNAME_CLAIM_REGEX = "(.*)";
     private static final String DEFAULT_OIDC_SCOPE = "openid profile email"; // Often "group" must be included for groups.
     private static final String DEFAULT_OIDC_CLIENT_NAME = "oidc";

File: datahub-frontend/app/controllers/SsoCallbackController.java
Patch:
@@ -40,6 +40,7 @@ public SsoCallbackController(
       @Nonnull AuthServiceClient authClient) {
     _ssoManager = ssoManager;
     setDefaultUrl("/"); // By default, redirects to Home Page on log in.
+    setSaveInSession(false);
     setCallbackLogic(new SsoCallbackLogic(ssoManager, systemAuthentication, entityClient, authClient));
   }
 

File: ingestion-scheduler/src/main/java/com/datahub/metadata/ingestion/IngestionScheduler.java
Patch:
@@ -204,6 +204,9 @@ public BatchRefreshSchedulesRunnable(
     public void run() {
       try {
 
+        // First un-schedule all currently scheduled runs (to make sure consistency is maintained)
+        _unscheduleAll.run();
+
         int start = 0;
         int count = 30;
         int total = 30;
@@ -230,9 +233,6 @@ public void run() {
             // 3. Reschedule ingestion sources based on the fetched schedules (inside "info")
             log.debug("Received batch of Ingestion Source Info aspects. Attempting to re-schedule execution requests.");
 
-            // First unschedule all currently scheduled runs (to make sure consistency is maintained)
-            _unscheduleAll.run();
-
             // Then schedule the next ingestion runs
             scheduleNextIngestionRuns(new ArrayList<>(ingestionSources.values()));
 

File: metadata-io/src/main/java/com/linkedin/metadata/search/LineageSearchService.java
Patch:
@@ -119,7 +119,7 @@ private LineageSearchResult getSearchResultInBatches(List<LineageRelationship> l
           .distinct()
           .collect(Collectors.toList());
       Map<Urn, LineageRelationship> urnToRelationship =
-          lineageRelationships.stream().collect(Collectors.toMap(LineageRelationship::getEntity, Function.identity()));
+          batch.stream().collect(Collectors.toMap(LineageRelationship::getEntity, Function.identity()));
       Filter finalFilter = buildFilter(urnToRelationship.keySet(), inputFilters);
       LineageSearchResult resultForBatch = buildLineageSearchResult(
           _searchService.searchAcrossEntities(entitiesToQuery, input, finalFilter, sortCriterion, queryFrom, querySize,

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/auth/ListAccessTokensResolver.java
Patch:
@@ -1,5 +1,6 @@
 package com.linkedin.datahub.graphql.resolvers.auth;
 
+import com.google.common.collect.ImmutableList;
 import com.linkedin.datahub.graphql.QueryContext;
 import com.linkedin.datahub.graphql.authorization.AuthorizationUtils;
 import com.linkedin.datahub.graphql.exception.AuthorizationException;
@@ -55,7 +56,7 @@ public CompletableFuture<ListAccessTokenResult> get(DataFetchingEnvironment envi
               new SortCriterion().setField(EXPIRES_AT_FIELD_NAME).setOrder(SortOrder.DESCENDING);
 
           final SearchResult searchResult = _entityClient.search(Constants.ACCESS_TOKEN_ENTITY_NAME, "",
-              buildFilter(filters), sortCriterion, start, count,
+              buildFilter(filters, Collections.emptyList()), sortCriterion, start, count,
               getAuthentication(environment));
 
           final List<AccessTokenMetadata> tokens = searchResult.getEntities().stream().map(entity -> {
@@ -94,6 +95,6 @@ public CompletableFuture<ListAccessTokenResult> get(DataFetchingEnvironment envi
    */
   private boolean isListingSelfTokens(final List<FacetFilterInput> filters, final QueryContext context) {
     return AuthorizationUtils.canGeneratePersonalAccessToken(context) && filters.stream()
-        .anyMatch(filter -> filter.getField().equals("ownerUrn") && filter.getValue().equals(context.getActorUrn()));
+        .anyMatch(filter -> filter.getField().equals("ownerUrn") && filter.getValues().equals(ImmutableList.of(context.getActorUrn())));
   }
 }

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/load/TimeSeriesAspectResolver.java
Patch:
@@ -12,7 +12,6 @@
 import com.linkedin.metadata.authorization.PoliciesConfig;
 import com.linkedin.metadata.query.filter.ConjunctiveCriterion;
 import com.linkedin.metadata.query.filter.ConjunctiveCriterionArray;
-import com.linkedin.metadata.query.filter.Criterion;
 import com.linkedin.metadata.query.filter.CriterionArray;
 import com.linkedin.metadata.query.filter.Filter;
 import com.linkedin.r2.RemoteInvocationException;
@@ -111,7 +110,7 @@ private Filter buildFilters(@Nullable FilterInput maybeFilters) {
       return null;
     }
     return new Filter().setOr(new ConjunctiveCriterionArray(new ConjunctiveCriterion().setAnd(new CriterionArray(maybeFilters.getAnd().stream()
-        .map(filter -> new Criterion().setField(filter.getField()).setValue(filter.getValue()))
+        .map(filter -> criterionFromFilter(filter))
         .collect(Collectors.toList())))));
   }
 }

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/search/SearchAcrossEntitiesResolver.java
Patch:
@@ -52,7 +52,7 @@ public CompletableFuture<SearchResults> get(DataFetchingEnvironment environment)
             "Executing search for multiple entities: entity types {}, query {}, filters: {}, start: {}, count: {}",
             input.getTypes(), input.getQuery(), input.getFilters(), start, count);
         return UrnSearchResultsMapper.map(_entityClient.searchAcrossEntities(entityNames, sanitizedQuery,
-            ResolverUtils.buildFilter(input.getFilters()), start, count, ResolverUtils.getAuthentication(environment)));
+            ResolverUtils.buildFilter(input.getFilters(), input.getOrFilters()), start, count, ResolverUtils.getAuthentication(environment)));
       } catch (Exception e) {
         log.error(
             "Failed to execute search for multiple entities: entity types {}, query {}, filters: {}, start: {}, count: {}",

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/search/SearchAcrossLineageResolver.java
Patch:
@@ -72,7 +72,7 @@ public CompletableFuture<SearchAcrossLineageResults> get(DataFetchingEnvironment
             urn, resolvedDirection, input.getTypes(), input.getQuery(), filters, start, count);
         return UrnSearchAcrossLineageResultsMapper.map(
             _entityClient.searchAcrossLineage(urn, resolvedDirection, entityNames, sanitizedQuery,
-                maxHops, ResolverUtils.buildFilter(filters), null, start, count,
+                maxHops, ResolverUtils.buildFilter(filters, input.getOrFilters()), null, start, count,
                 ResolverUtils.getAuthentication(environment)));
       } catch (RemoteInvocationException e) {
         log.error(
@@ -89,7 +89,7 @@ public CompletableFuture<SearchAcrossLineageResults> get(DataFetchingEnvironment
   private Integer getMaxHops(List<FacetFilterInput> filters) {
     Set<String> degreeFilterValues = filters.stream()
         .filter(filter -> filter.getField().equals("degree"))
-        .map(FacetFilterInput::getValue)
+        .flatMap(filter -> filter.getValues().stream())
         .collect(Collectors.toSet());
     Integer maxHops = null;
     if (!degreeFilterValues.contains("3+")) {

File: metadata-io/src/main/java/com/linkedin/metadata/search/LineageSearchService.java
Patch:
@@ -182,7 +182,7 @@ private List<LineageRelationship> filterRelationships(@Nonnull EntityLineageResu
         List<String> degreeFilter = conjunctiveCriterion.getAnd()
             .stream()
             .filter(criterion -> criterion.getField().equals(DEGREE_FILTER_INPUT))
-            .map(Criterion::getValue)
+            .flatMap(c -> c.getValues().stream())
             .collect(Collectors.toList());
         if (!degreeFilter.isEmpty()) {
           Predicate<Integer> degreePredicate = convertFilterToPredicate(degreeFilter);

File: metadata-io/src/main/java/com/linkedin/metadata/search/aggregator/AllEntitiesSearchAggregator.java
Patch:
@@ -171,7 +171,8 @@ private Map<String, AggregationMetadata> trimMergedAggregations(Map<String, Aggr
         entry -> Pair.of(entry.getKey(), new AggregationMetadata()
             .setName(entry.getValue().getName())
             .setDisplayName(entry.getValue().getDisplayName(GetMode.NULL))
-            .setAggregations(entry.getValue().getAggregations())
+            .setAggregations(
+                entry.getValue().getAggregations())
             .setFilterValues(
                 trimFilterValues(entry.getValue().getFilterValues()))
         )

File: metadata-io/src/main/java/com/linkedin/metadata/search/utils/QueryUtils.java
Patch:
@@ -3,6 +3,7 @@
 import com.datahub.util.ModelUtils;
 import com.google.common.collect.ImmutableList;
 import com.linkedin.data.template.RecordTemplate;
+import com.linkedin.data.template.StringArray;
 import com.linkedin.metadata.aspect.AspectVersion;
 import com.linkedin.metadata.dao.BaseReadDAO;
 import com.linkedin.metadata.query.filter.Condition;
@@ -39,7 +40,7 @@ public static Criterion newCriterion(@Nonnull String field, @Nonnull String valu
   // Creates new Criterion with field, value and condition.
   @Nonnull
   public static Criterion newCriterion(@Nonnull String field, @Nonnull String value, @Nonnull Condition condition) {
-    return new Criterion().setField(field).setValue(value).setCondition(condition);
+    return new Criterion().setField(field).setValue(value).setValues(new StringArray(ImmutableList.of(value))).setCondition(condition);
   }
 
   // Creates new Filter from a map of Criteria by removing null-valued Criteria and using EQUAL condition (default).

File: metadata-io/src/main/java/com/linkedin/metadata/search/utils/SearchUtils.java
Patch:
@@ -137,6 +137,7 @@ public static AggregationMetadata merge(AggregationMetadata one, AggregationMeta
         Stream.concat(one.getAggregations().entrySet().stream(), two.getAggregations().entrySet().stream())
             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue, Long::sum));
     return one.clone()
+        .setDisplayName(two.getDisplayName() != two.getName() ? two.getDisplayName() : one.getDisplayName())
         .setAggregations(new LongMap(mergedMap))
         .setFilterValues(new FilterValueArray(SearchUtil.convertToFilters(mergedMap)));
   }
@@ -153,4 +154,4 @@ public static ListResult toListResult(final SearchResult searchResult) {
         new UrnArray(searchResult.getEntities().stream().map(SearchEntity::getEntity).collect(Collectors.toList())));
     return listResult;
   }
-}
\ No newline at end of file
+}

File: metadata-io/src/main/java/com/linkedin/metadata/client/JavaEntityClient.java
Patch:
@@ -184,7 +184,7 @@ public BrowseResult browse(
         int limit,
         @Nonnull final Authentication authentication) throws RemoteInvocationException {
         return ValidationUtils.validateBrowseResult(
-            _entitySearchService.browse(entityType, path, newFilter(requestFilters), start, limit), _entityService);
+            _cachingEntitySearchService.browse(entityType, path, newFilter(requestFilters), start, limit, null), _entityService);
     }
 
     @SneakyThrows

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/timeline/mappers/SchemaBlameMapper.java
Patch:
@@ -22,7 +22,7 @@
 import java.util.stream.Collectors;
 import javax.annotation.Nullable;
 import lombok.extern.slf4j.Slf4j;
-import org.apache.parquet.SemanticVersion;
+import org.apache.maven.artifact.versioning.ComparableVersion;
 
 import static com.linkedin.datahub.graphql.types.timeline.utils.TimelineUtils.*;
 
@@ -45,12 +45,12 @@ public static GetSchemaBlameResult map(List<ChangeTransaction> changeTransaction
         truncateSemanticVersion(changeTransactions.get(changeTransactions.size() - 1).getSemVer());
 
     String semanticVersionFilterString = versionCutoff == null ? latestSemanticVersionString : versionCutoff;
-    Optional<SemanticVersion> semanticVersionFilterOptional = createSemanticVersion(semanticVersionFilterString);
+    Optional<ComparableVersion> semanticVersionFilterOptional = createSemanticVersion(semanticVersionFilterString);
     if (!semanticVersionFilterOptional.isPresent()) {
       return result;
     }
 
-    SemanticVersion semanticVersionFilter = semanticVersionFilterOptional.get();
+    ComparableVersion semanticVersionFilter = semanticVersionFilterOptional.get();
 
     List<ChangeTransaction> reversedChangeTransactions = changeTransactions.stream()
         .map(TimelineUtils::semanticVersionChangeTransactionPair)

File: datahub-frontend/app/controllers/AuthenticationController.java
Patch:
@@ -305,10 +305,10 @@ private boolean tryLogin(String username, String password) {
             try {
                 _logger.debug("Attempting jaas authentication");
                 AuthenticationManager.authenticateJaasUser(username, password);
+                _logger.debug("Jaas authentication successful. Login succeeded");
                 loginSucceeded = true;
-                _logger.debug("Jaas authentication successful");
             } catch (Exception e) {
-                _logger.debug("Jaas authentication error", e);
+                _logger.debug("Jaas authentication error. Login failed", e);
             }
         }
 

File: datahub-frontend/app/controllers/CentralLogoutController.java
Patch:
@@ -41,7 +41,7 @@ public CentralLogoutController(Config config) {
   public Result executeLogout() throws ExecutionException, InterruptedException {
     if (_isOidcEnabled) {
       try {
-        return logout().toCompletableFuture().get();
+        return logout().toCompletableFuture().get().withNewSession();
       } catch (Exception e) {
         log.error("Caught exception while attempting to perform SSO logout! It's likely that SSO integration is mis-configured.", e);
         return redirect(
@@ -50,6 +50,6 @@ public Result executeLogout() throws ExecutionException, InterruptedException {
                     + "or refer to server logs for more information.")));
       }
     }
-    return redirect("/");
+    return redirect("/").withNewSession();
   }
 }
\ No newline at end of file

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/dataplatform/mappers/DataPlatformMapper.java
Patch:
@@ -27,7 +27,7 @@ public static DataPlatform map(@Nonnull final EntityResponse platform) {
     @Override
     public DataPlatform apply(@Nonnull final EntityResponse entityResponse) {
         final DataPlatform result = new DataPlatform();
-        final DataPlatformKey dataPlatformKey = (DataPlatformKey) EntityKeyUtils.convertUrnToEntityKey(entityResponse.getUrn(),
+        final DataPlatformKey dataPlatformKey = (DataPlatformKey) EntityKeyUtils.convertUrnToEntityKeyInternal(entityResponse.getUrn(),
             new DataPlatformKey().schema());
         result.setType(EntityType.DATA_PLATFORM);
         Urn urn = entityResponse.getUrn();

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/timeline/mappers/SchemaBlameMapper.java
Patch:
@@ -83,7 +83,7 @@ public static GetSchemaBlameResult map(List<ChangeTransaction> changeTransaction
 
         SchemaFieldKey schemaFieldKey;
         try {
-          schemaFieldKey = (SchemaFieldKey) EntityKeyUtils.convertUrnToEntityKey(Urn.createFromString(schemaUrn),
+          schemaFieldKey = (SchemaFieldKey) EntityKeyUtils.convertUrnToEntityKeyInternal(Urn.createFromString(schemaUrn),
               new SchemaFieldKey().schema());
         } catch (Exception e) {
           log.debug(String.format("Could not generate schema urn for %s", schemaUrn));

File: metadata-io/src/main/java/com/linkedin/metadata/entity/EntityRegistryUrnValidator.java
Patch:
@@ -47,7 +47,7 @@ public void validate(ValidatorContext context) {
         Urn urn = Urn.createFromString(urnStr);
         EntitySpec entitySpec = _entityRegistry.getEntitySpec(urn.getEntityType());
         RecordTemplate entityKey = EntityKeyUtils.convertUrnToEntityKey(urn,
-            entitySpec.getKeyAspectSpec().getPegasusSchema());
+            entitySpec.getKeyAspectSpec());
         NamedDataSchema namedDataSchema = ((NamedDataSchema) context.dataElement().getSchema());
         Class<? extends Urn> urnClass;
         try {

File: metadata-io/src/main/java/com/linkedin/metadata/entity/EntityService.java
Patch:
@@ -1225,7 +1225,7 @@ protected RecordTemplate buildKeyAspect(@Nonnull final Urn urn) {
     final EntitySpec spec = _entityRegistry.getEntitySpec(urnToEntityName(urn));
     final AspectSpec keySpec = spec.getKeyAspectSpec();
     final RecordDataSchema keySchema = keySpec.getPegasusSchema();
-    return EntityKeyUtils.convertUrnToEntityKey(urn, keySchema);
+    return EntityKeyUtils.convertUrnToEntityKey(urn, keySpec);
   }
 
   public AspectSpec getKeyAspectSpec(@Nonnull final Urn urn) {
@@ -1641,7 +1641,7 @@ private EnvelopedAspect getKeyEnvelopedAspect(final Urn urn) {
     final AspectSpec keySpec = spec.getKeyAspectSpec();
     final RecordDataSchema keySchema = keySpec.getPegasusSchema();
     final com.linkedin.entity.Aspect aspect =
-        new com.linkedin.entity.Aspect(EntityKeyUtils.convertUrnToEntityKey(urn, keySchema).data());
+        new com.linkedin.entity.Aspect(EntityKeyUtils.convertUrnToEntityKey(urn, keySpec).data());
 
     final EnvelopedAspect envelopedAspect = new EnvelopedAspect();
     envelopedAspect.setName(keySpec.getName());

File: metadata-io/src/test/java/com/linkedin/metadata/AspectGenerationUtils.java
Patch:
@@ -39,7 +39,7 @@ public static SystemMetadata createSystemMetadata(long lastObserved, @Nonnull St
 
   @Nonnull
   public static CorpUserKey createCorpUserKey(Urn urn) {
-    return (CorpUserKey) EntityKeyUtils.convertUrnToEntityKey(urn, new CorpUserKey().schema());
+    return (CorpUserKey) EntityKeyUtils.convertUrnToEntityKeyInternal(urn, new CorpUserKey().schema());
   }
 
   @Nonnull

File: metadata-jobs/mae-consumer/src/main/java/com/linkedin/metadata/kafka/hydrator/DataJobHydrator.java
Patch:
@@ -36,7 +36,7 @@ protected void hydrateFromEntityResponse(ObjectNode document, EntityResponse ent
   private void mapKey(ObjectNode jsonNodes, DataMap dataMap) {
     DataJobKey dataJobKey = new DataJobKey(dataMap);
     DataFlowKey dataFlowKey = (DataFlowKey) EntityKeyUtils
-        .convertUrnToEntityKey(dataJobKey.getFlow(), new DataFlowKey().schema());
+        .convertUrnToEntityKeyInternal(dataJobKey.getFlow(), new DataFlowKey().schema());
     jsonNodes.put(ORCHESTRATOR, dataFlowKey.getOrchestrator());
   }
 }

File: metadata-service/factories/src/main/java/com/linkedin/metadata/boot/steps/IngestDataPlatformInstancesStep.java
Patch:
@@ -42,7 +42,7 @@ public ExecutionMode getExecutionMode() {
 
   private Optional<DataPlatformInstance> getDataPlatformInstance(Urn urn) {
     final AspectSpec keyAspectSpec = _entityService.getKeyAspectSpec(urn);
-    RecordTemplate keyAspect = EntityKeyUtils.convertUrnToEntityKey(urn, keyAspectSpec.getPegasusSchema());
+    RecordTemplate keyAspect = EntityKeyUtils.convertUrnToEntityKey(urn, keyAspectSpec);
     return DataPlatformInstanceUtils.buildDataPlatformInstance(urn.getEntityType(), keyAspect);
   }
 

File: metadata-service/factories/src/main/java/com/linkedin/metadata/boot/steps/IngestPoliciesStep.java
Patch:
@@ -159,7 +159,7 @@ private void ingestPolicy(final Urn urn, final DataHubPolicyInfo info) throws UR
     final MetadataChangeProposal keyAspectProposal = new MetadataChangeProposal();
     final AspectSpec keyAspectSpec = _entityService.getKeyAspectSpec(urn);
     GenericAspect aspect =
-        GenericRecordUtils.serializeAspect(EntityKeyUtils.convertUrnToEntityKey(urn, keyAspectSpec.getPegasusSchema()));
+        GenericRecordUtils.serializeAspect(EntityKeyUtils.convertUrnToEntityKey(urn, keyAspectSpec));
     keyAspectProposal.setAspect(aspect);
     keyAspectProposal.setAspectName(keyAspectSpec.getName());
     keyAspectProposal.setEntityType(POLICY_ENTITY_NAME);

File: metadata-service/factories/src/main/java/com/linkedin/metadata/boot/steps/IngestRolesStep.java
Patch:
@@ -79,7 +79,7 @@ private void ingestRole(final Urn urn, final DataHubRoleInfo info) throws URISyn
     final MetadataChangeProposal keyAspectProposal = new MetadataChangeProposal();
     final AspectSpec keyAspectSpec = _entityService.getKeyAspectSpec(urn);
     GenericAspect aspect =
-        GenericRecordUtils.serializeAspect(EntityKeyUtils.convertUrnToEntityKey(urn, keyAspectSpec.getPegasusSchema()));
+        GenericRecordUtils.serializeAspect(EntityKeyUtils.convertUrnToEntityKey(urn, keyAspectSpec));
     keyAspectProposal.setAspect(aspect);
     keyAspectProposal.setAspectName(keyAspectSpec.getName());
     keyAspectProposal.setEntityType(DATAHUB_ROLE_ENTITY_NAME);

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/dashboard/DashboardType.java
Patch:
@@ -73,7 +73,8 @@ public class DashboardType implements SearchableEntityType<Dashboard, String>, B
         DOMAINS_ASPECT_NAME,
         DEPRECATION_ASPECT_NAME,
         DATA_PLATFORM_INSTANCE_ASPECT_NAME,
-        INPUT_FIELDS_ASPECT_NAME
+        INPUT_FIELDS_ASPECT_NAME,
+        SUB_TYPES_ASPECT_NAME
     );
     private static final Set<String> FACET_FIELDS = ImmutableSet.of("access", "tool");
 

File: metadata-integration/java/datahub-protobuf/src/test/java/datahub/protobuf/ProtobufDatasetTest.java
Patch:
@@ -73,6 +73,8 @@ public void messageA() throws IOException {
         assertEquals(1, testMetadata.getVersion());
         assertEquals(9, testMetadata.getFields().size());
 
+        assertEquals("MessageA", extractAspect(test.getDatasetMCPs().get(0), "name"));
+        assertEquals("protobuf.MessageA", extractAspect(test.getDatasetMCPs().get(0), "qualifiedName"));
 
         assertEquals("platform.topic", extractCustomProperty(test.getDatasetMCPs().get(0), "kafka_topic"));
 

File: entity-registry/src/main/java/com/linkedin/metadata/models/annotation/SearchableAnnotation.java
Patch:
@@ -53,7 +53,8 @@ public enum FieldType {
     URN_PARTIAL,
     BOOLEAN,
     COUNT,
-    DATETIME
+    DATETIME,
+    OBJECT
   }
 
   @Nonnull

File: metadata-io/src/main/java/com/linkedin/metadata/search/elasticsearch/indexbuilder/MappingsBuilder.java
Patch:
@@ -82,6 +82,8 @@ private static Map<String, Object> getMappingsForField(@Nonnull final Searchable
       mappingForField.put("type", "long");
     } else if (fieldType == FieldType.DATETIME) {
       mappingForField.put("type", "date");
+    } else if (fieldType == FieldType.OBJECT) {
+      mappingForField.put("type", "object");
     } else {
       log.info("FieldType {} has no mappings implemented", fieldType);
     }

File: metadata-io/src/test/java/com/linkedin/metadata/TestEntityUtil.java
Patch:
@@ -41,6 +41,7 @@ public static TestEntityInfo getTestEntityInfo(Urn urn) {
             new SimpleNestedRecord2().setNestedArrayStringField("nestedArray2")
                 .setNestedArrayArrayField(new StringArray(ImmutableList.of("testNestedArray1", "testNestedArray2"))))));
     testEntityInfo.setCustomProperties(new StringMap(ImmutableMap.of("key1", "value1", "key2", "value2")));
+    testEntityInfo.setEsObjectField(new StringMap(ImmutableMap.of("key1", "value1", "key2", "value2")));
     return testEntityInfo;
   }
 

File: metadata-io/src/test/java/com/linkedin/metadata/extractor/FieldExtractorTest.java
Patch:
@@ -45,5 +45,6 @@ public void testExtractor() {
     assertEquals(result.get(nameToSpec.get("nestedArrayStringField")), ImmutableList.of("nestedArray1", "nestedArray2"));
     assertEquals(result.get(nameToSpec.get("nestedArrayArrayField")), ImmutableList.of("testNestedArray1", "testNestedArray2"));
     assertEquals(result.get(nameToSpec.get("customProperties")), ImmutableList.of("key1=value1", "key2=value2"));
+    assertEquals(result.get(nameToSpec.get("esObjectField")), ImmutableList.of("key1=value1", "key2=value2"));
   }
 }

File: metadata-io/src/test/java/com/linkedin/metadata/search/elasticsearch/query/request/SearchQueryBuilderTest.java
Patch:
@@ -25,11 +25,12 @@ public void testQueryBuilder() {
     assertEquals(keywordQuery.queryString(), "testQuery");
     assertEquals(keywordQuery.analyzer(), "custom_keyword");
     Map<String, Float> keywordFields = keywordQuery.fields();
-    assertEquals(keywordFields.size(), 8);
+    assertEquals(keywordFields.size(), 9);
     assertEquals(keywordFields.get("keyPart1").floatValue(), 10.0f);
     assertFalse(keywordFields.containsKey("keyPart3"));
     assertEquals(keywordFields.get("textFieldOverride").floatValue(), 1.0f);
     assertEquals(keywordFields.get("customProperties").floatValue(), 1.0f);
+    assertEquals(keywordFields.get("esObjectField").floatValue(), 1.0f);
     QueryStringQueryBuilder textQuery = (QueryStringQueryBuilder) shouldQueries.get(1);
     assertEquals(textQuery.queryString(), "testQuery");
     assertEquals(textQuery.analyzer(), "word_delimited");

File: metadata-io/src/test/java/com/linkedin/metadata/search/transformer/SearchDocumentTransformerTest.java
Patch:
@@ -22,7 +22,7 @@ public class SearchDocumentTransformerTest {
 
   @Test
   public void testTransform() throws IOException {
-    SearchDocumentTransformer searchDocumentTransformer = new SearchDocumentTransformer(1000);
+    SearchDocumentTransformer searchDocumentTransformer = new SearchDocumentTransformer(1000, 1000);
     TestEntitySnapshot snapshot = TestEntityUtil.getSnapshot();
     EntitySpec testEntitySpec = TestEntitySpecBuilder.getSpec();
     Optional<String> result = searchDocumentTransformer.transformSnapshot(snapshot, testEntitySpec, false);
@@ -54,7 +54,7 @@ public void testTransform() throws IOException {
 
   @Test
   public void testTransformForDelete() throws IOException {
-    SearchDocumentTransformer searchDocumentTransformer = new SearchDocumentTransformer(1000);
+    SearchDocumentTransformer searchDocumentTransformer = new SearchDocumentTransformer(1000, 1000);
     TestEntitySnapshot snapshot = TestEntityUtil.getSnapshot();
     EntitySpec testEntitySpec = TestEntitySpecBuilder.getSpec();
     Optional<String> result = searchDocumentTransformer.transformSnapshot(snapshot, testEntitySpec, true);

File: datahub-upgrade/src/main/java/com/linkedin/datahub/upgrade/nocode/DataMigrationStep.java
Patch:
@@ -17,7 +17,6 @@
 import com.linkedin.metadata.entity.ebean.EbeanAspectV1;
 import com.linkedin.metadata.entity.ebean.EbeanAspectV2;
 import com.linkedin.metadata.models.EntitySpec;
-import com.linkedin.metadata.search.utils.BrowsePathUtils;
 import io.ebean.EbeanServer;
 import io.ebean.PagedList;
 import java.net.URISyntaxException;
@@ -151,7 +150,7 @@ public Function<UpgradeContext, UpgradeStepResult> executable() {
             // Emit a browse path aspect.
             final BrowsePaths browsePaths;
             try {
-              browsePaths = BrowsePathUtils.buildBrowsePath(urn, _entityService.getEntityRegistry());
+              browsePaths = _entityService.buildDefaultBrowsePath(urn);
 
               final AuditStamp browsePathsStamp = new AuditStamp();
               browsePathsStamp.setActor(Urn.createFromString(Constants.SYSTEM_ACTOR));

File: metadata-jobs/mae-consumer-job/src/main/java/com/linkedin/metadata/kafka/MaeConsumerApplication.java
Patch:
@@ -2,6 +2,7 @@
 
 import com.linkedin.gms.factory.telemetry.ScheduledAnalyticsFactory;
 import org.springframework.boot.SpringApplication;
+import org.springframework.boot.actuate.autoconfigure.solr.SolrHealthContributorAutoConfiguration;
 import org.springframework.boot.autoconfigure.SpringBootApplication;
 import org.springframework.boot.autoconfigure.cassandra.CassandraAutoConfiguration;
 import org.springframework.boot.autoconfigure.elasticsearch.ElasticsearchRestClientAutoConfiguration;
@@ -10,7 +11,8 @@
 
 
 @SuppressWarnings("checkstyle:HideUtilityClassConstructor")
-@SpringBootApplication(exclude = {ElasticsearchRestClientAutoConfiguration.class, CassandraAutoConfiguration.class})
+@SpringBootApplication(exclude = {ElasticsearchRestClientAutoConfiguration.class, CassandraAutoConfiguration.class,
+    SolrHealthContributorAutoConfiguration.class})
 @ComponentScan(excludeFilters = {
     @ComponentScan.Filter(type = FilterType.ASSIGNABLE_TYPE, classes = ScheduledAnalyticsFactory.class)})
 public class MaeConsumerApplication {

File: metadata-jobs/mce-consumer-job/src/main/java/com/linkedin/metadata/kafka/MceConsumerApplication.java
Patch:
@@ -2,6 +2,7 @@
 
 import com.linkedin.gms.factory.telemetry.ScheduledAnalyticsFactory;
 import org.springframework.boot.SpringApplication;
+import org.springframework.boot.actuate.autoconfigure.solr.SolrHealthContributorAutoConfiguration;
 import org.springframework.boot.autoconfigure.SpringBootApplication;
 import org.springframework.boot.autoconfigure.cassandra.CassandraAutoConfiguration;
 import org.springframework.boot.autoconfigure.elasticsearch.ElasticsearchRestClientAutoConfiguration;
@@ -10,7 +11,8 @@
 
 
 @SuppressWarnings("checkstyle:HideUtilityClassConstructor")
-@SpringBootApplication(exclude = {ElasticsearchRestClientAutoConfiguration.class, CassandraAutoConfiguration.class})
+@SpringBootApplication(exclude = {ElasticsearchRestClientAutoConfiguration.class, CassandraAutoConfiguration.class,
+    SolrHealthContributorAutoConfiguration.class})
 @ComponentScan(excludeFilters = {
         @ComponentScan.Filter(type = FilterType.ASSIGNABLE_TYPE, classes = ScheduledAnalyticsFactory.class)})
 public class MceConsumerApplication {

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/config/AppConfigResolver.java
Patch:
@@ -165,6 +165,8 @@ private EntityType mapResourceTypeToEntityType(final String resourceType) {
       return EntityType.TAG;
     } else if (com.linkedin.metadata.authorization.PoliciesConfig.GLOSSARY_TERM_PRIVILEGES.getResourceType().equals(resourceType)) {
       return EntityType.GLOSSARY_TERM;
+    } else if (com.linkedin.metadata.authorization.PoliciesConfig.GLOSSARY_NODE_PRIVILEGES.getResourceType().equals(resourceType)) {
+      return EntityType.GLOSSARY_NODE;
     } else if (com.linkedin.metadata.authorization.PoliciesConfig.DOMAIN_PRIVILEGES.getResourceType().equals(resourceType)) {
       return EntityType.DOMAIN;
     } else if (com.linkedin.metadata.authorization.PoliciesConfig.CONTAINER_PRIVILEGES.getResourceType().equals(resourceType)) {

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/chart/ChartType.java
Patch:
@@ -71,7 +71,8 @@ public class ChartType implements SearchableEntityType<Chart, String>, Browsable
         CONTAINER_ASPECT_NAME,
         DOMAINS_ASPECT_NAME,
         DEPRECATION_ASPECT_NAME,
-        DATA_PLATFORM_INSTANCE_ASPECT_NAME
+        DATA_PLATFORM_INSTANCE_ASPECT_NAME,
+        INPUT_FIELDS_ASPECT_NAME
     );
     private static final Set<String> FACET_FIELDS = ImmutableSet.of("access", "queryType", "tool", "type");
 

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/chart/mappers/ChartMapper.java
Patch:
@@ -5,6 +5,7 @@
 import com.linkedin.common.Deprecation;
 import com.linkedin.common.GlobalTags;
 import com.linkedin.common.GlossaryTerms;
+import com.linkedin.common.InputFields;
 import com.linkedin.common.InstitutionalMemory;
 import com.linkedin.common.Ownership;
 import com.linkedin.common.Status;
@@ -86,6 +87,8 @@ public Chart apply(@Nonnull final EntityResponse entityResponse) {
             chart.setDeprecation(DeprecationMapper.map(new Deprecation(dataMap))));
         mappingHelper.mapToResult(DATA_PLATFORM_INSTANCE_ASPECT_NAME, (dataset, dataMap) ->
             dataset.setDataPlatformInstance(DataPlatformInstanceAspectMapper.map(new DataPlatformInstance(dataMap))));
+        mappingHelper.mapToResult(INPUT_FIELDS_ASPECT_NAME, (chart, dataMap) ->
+            chart.setInputFields(InputFieldsMapper.map(new InputFields(dataMap), entityUrn)));
 
         return mappingHelper.getResult();
     }

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/dashboard/DashboardType.java
Patch:
@@ -72,7 +72,8 @@ public class DashboardType implements SearchableEntityType<Dashboard, String>, B
         CONTAINER_ASPECT_NAME,
         DOMAINS_ASPECT_NAME,
         DEPRECATION_ASPECT_NAME,
-        DATA_PLATFORM_INSTANCE_ASPECT_NAME
+        DATA_PLATFORM_INSTANCE_ASPECT_NAME,
+        INPUT_FIELDS_ASPECT_NAME
     );
     private static final Set<String> FACET_FIELDS = ImmutableSet.of("access", "tool");
 

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/dataset/mappers/SchemaFieldMapper.java
Patch:
@@ -25,6 +25,7 @@ public SchemaField apply(@Nonnull final com.linkedin.schema.SchemaField input, @
         result.setNullable(input.isNullable());
         result.setNativeDataType(input.getNativeDataType());
         result.setType(mapSchemaFieldDataType(input.getType()));
+        result.setLabel(input.getLabel());
         if (input.hasGlobalTags()) {
             result.setGlobalTags(GlobalTagsMapper.map(input.getGlobalTags(), entityUrn));
             result.setTags(GlobalTagsMapper.map(input.getGlobalTags(), entityUrn));

File: metadata-utils/src/main/java/com/linkedin/metadata/Constants.java
Patch:
@@ -67,6 +67,7 @@ public class Constants {
   public static final String OPERATION_ASPECT_NAME = "operation";
   public static final String SIBLINGS_ASPECT_NAME = "siblings";
   public static final String ORIGIN_ASPECT_NAME = "origin";
+  public static final String INPUT_FIELDS_ASPECT_NAME = "inputFields";
 
   // User
   public static final String CORP_USER_KEY_ASPECT_NAME = "corpUserKey";

File: metadata-service/auth-impl/src/main/java/com/datahub/authentication/user/NativeUserService.java
Patch:
@@ -61,14 +61,15 @@ public NativeUserService(@Nonnull EntityService entityService, @Nonnull EntityCl
   }
 
   public void createNativeUser(@Nonnull String userUrnString, @Nonnull String fullName, @Nonnull String email,
-      @Nonnull String title, @Nonnull String password, @Nonnull String inviteToken, Authentication authentication)
+      @Nonnull String title, @Nonnull String password, @Nonnull String inviteToken, @Nonnull Authentication authentication)
       throws Exception {
     Objects.requireNonNull(userUrnString, "userUrnSting must not be null!");
     Objects.requireNonNull(fullName, "fullName must not be null!");
     Objects.requireNonNull(email, "email must not be null!");
     Objects.requireNonNull(title, "title must not be null!");
     Objects.requireNonNull(password, "password must not be null!");
     Objects.requireNonNull(inviteToken, "inviteToken must not be null!");
+    Objects.requireNonNull(inviteToken, "authentication must not be null!");
 
     InviteToken inviteTokenAspect =
         (InviteToken) _entityService.getLatestAspect(Urn.createFromString(GLOBAL_INVITE_TOKEN),
@@ -125,7 +126,7 @@ void updateCorpUserStatus(@Nonnull Urn userUrn, Authentication authentication) t
   }
 
   void updateCorpUserCredentials(@Nonnull Urn userUrn, @Nonnull String password,
-      Authentication authentication) throws Exception {
+      @Nonnull Authentication authentication) throws Exception {
     // Construct corpUserCredentials
     CorpUserCredentials corpUserCredentials = new CorpUserCredentials();
     final byte[] salt = getRandomBytes(SALT_TOKEN_LENGTH);

File: metadata-service/restli-client/src/main/java/com/linkedin/entity/client/JavaEntityClient.java
Patch:
@@ -437,8 +437,10 @@ public List<EnvelopedAspect> getTimeseriesAspectValues(@Nonnull String urn, @Non
     // TODO: Factor out ingest logic into a util that can be accessed by the java client and the resource
     @SneakyThrows
     @Override
-    public String ingestProposal(@Nonnull MetadataChangeProposal metadataChangeProposal,
+    public String ingestProposal(
+        @Nonnull final MetadataChangeProposal metadataChangeProposal,
         @Nonnull final Authentication authentication) throws RemoteInvocationException {
+
         String actorUrnStr = authentication.getActor() != null ? authentication.getActor().toUrnStr() : Constants.UNKNOWN_ACTOR;
         final AuditStamp auditStamp =
             new AuditStamp().setTime(_clock.millis()).setActor(Urn.createFromString(actorUrnStr));

File: metadata-service/restli-client/src/main/java/com/linkedin/entity/client/JavaEntityClient.java
Patch:
@@ -439,8 +439,9 @@ public List<EnvelopedAspect> getTimeseriesAspectValues(@Nonnull String urn, @Non
     @Override
     public String ingestProposal(@Nonnull MetadataChangeProposal metadataChangeProposal,
         @Nonnull final Authentication authentication) throws RemoteInvocationException {
+        String actorUrnStr = authentication.getActor() != null ? authentication.getActor().toUrnStr() : Constants.UNKNOWN_ACTOR;
         final AuditStamp auditStamp =
-            new AuditStamp().setTime(_clock.millis()).setActor(Urn.createFromString(Constants.UNKNOWN_ACTOR));
+            new AuditStamp().setTime(_clock.millis()).setActor(Urn.createFromString(actorUrnStr));
         final List<MetadataChangeProposal> additionalChanges =
             AspectUtils.getAdditionalChanges(metadataChangeProposal, _entityService);
 

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/GmsGraphQLEngine.java
Patch:
@@ -704,7 +704,7 @@ private void configureMutationResolvers(final RuntimeWiring.Builder builder) {
         builder.type("Mutation", typeWiring -> typeWiring
             .dataFetcher("updateDataset", new MutableTypeResolver<>(datasetType))
             .dataFetcher("updateDatasets", new MutableTypeBatchResolver<>(datasetType))
-            .dataFetcher("createTag", new CreateTagResolver(this.entityClient))
+            .dataFetcher("createTag", new CreateTagResolver(this.entityClient, this.entityService))
             .dataFetcher("updateTag", new MutableTypeResolver<>(tagType))
             .dataFetcher("setTagColor", new SetTagColorResolver(entityClient, entityService))
             .dataFetcher("deleteTag", new DeleteTagResolver(entityClient))

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/ingest/source/UpsertIngestionSourceResolver.java
Patch:
@@ -122,7 +122,9 @@ private DataHubIngestionSourceConfig mapConfig(final UpdateIngestionSourceConfig
     if (input.getExecutorId() != null) {
       result.setExecutorId(input.getExecutorId());
     }
-    result.setDebugMode(input.getDebugMode());
+    if (input.getDebugMode() != null) {
+      result.setDebugMode(input.getDebugMode());
+    }
     return result;
   }
 

File: metadata-utils/src/main/java/com/linkedin/metadata/Constants.java
Patch:
@@ -77,6 +77,8 @@ public class Constants {
   public static final String CORP_USER_STATUS_ASPECT_NAME = "corpUserStatus";
   public static final String CORP_USER_CREDENTIALS_ASPECT_NAME = "corpUserCredentials";
 
+  public static final String CORP_USER_SETTINGS_ASPECT_NAME = "corpUserSettings";
+
   // Group
   public static final String CORP_GROUP_KEY_ASPECT_NAME = "corpGroupKey";
   public static final String CORP_GROUP_INFO_ASPECT_NAME = "corpGroupInfo";

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/ingest/IngestionResolverUtils.java
Patch:
@@ -142,6 +142,7 @@ public static IngestionConfig mapIngestionSourceConfig(final DataHubIngestionSou
     result.setRecipe(config.getRecipe());
     result.setVersion(config.getVersion());
     result.setExecutorId(config.getExecutorId());
+    result.setDebugMode(config.isDebugMode());
     return result;
   }
 

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/ingest/source/UpsertIngestionSourceResolver.java
Patch:
@@ -122,6 +122,7 @@ private DataHubIngestionSourceConfig mapConfig(final UpdateIngestionSourceConfig
     if (input.getExecutorId() != null) {
       result.setExecutorId(input.getExecutorId());
     }
+    result.setDebugMode(input.getDebugMode());
     return result;
   }
 

File: datahub-graphql-core/src/test/java/com/linkedin/datahub/graphql/resolvers/ingest/source/UpsertIngestionSourceResolverTest.java
Patch:
@@ -28,7 +28,7 @@ public class UpsertIngestionSourceResolverTest {
       "Test source",
       "mysql", "Test source description",
       new UpdateIngestionSourceScheduleInput("* * * * *", "UTC"),
-      new UpdateIngestionSourceConfigInput("my test recipe", "0.8.18", "executor id")
+      new UpdateIngestionSourceConfigInput("my test recipe", "0.8.18", "executor id", false)
   );
 
   @Test

File: datahub-upgrade/src/main/java/com/linkedin/datahub/upgrade/restoreindices/RestoreIndices.java
Patch:
@@ -18,6 +18,9 @@
 public class RestoreIndices implements Upgrade {
   public static final String BATCH_SIZE_ARG_NAME = "batchSize";
   public static final String BATCH_DELAY_MS_ARG_NAME = "batchDelayMs";
+  public static final String NUM_THREADS_ARG_NAME = "numThreads";
+  public static final String ASPECT_NAME_ARG_NAME = "aspectName";
+  public static final String URN_ARG_NAME = "urn";
 
   private final List<UpgradeStep> _steps;
 

File: metadata-jobs/mae-consumer/src/main/java/com/linkedin/metadata/kafka/DataHubUsageEventsProcessor.java
Patch:
@@ -53,7 +53,7 @@ public void consume(final ConsumerRecord<String, String> consumerRecord) {
     Optional<DataHubUsageEventTransformer.TransformedDocument> eventDocument =
         dataHubUsageEventTransformer.transformDataHubUsageEvent(record);
     if (!eventDocument.isPresent()) {
-      log.info("failed transform: {}", record);
+      log.warn("Failed to apply usage events transform to record: {}", record);
       return;
     }
     JsonElasticEvent elasticEvent = new JsonElasticEvent(eventDocument.get().getDocument());

File: metadata-jobs/mae-consumer/src/main/java/com/linkedin/metadata/kafka/hook/event/EntityChangeEventGeneratorHook.java
Patch:
@@ -147,7 +147,7 @@ public void invoke(@Nonnull final MetadataChangeLog logEvent) throws Exception {
             platformEvent,
             String.format("%s-%s", Constants.CHANGE_EVENT_PLATFORM_EVENT_NAME, event.getEntityUrn())
         );
-        log.info("Successfully emitted change event. category: {}, operation: {}, entity urn: {}",
+        log.debug("Successfully emitted change event. category: {}, operation: {}, entity urn: {}",
             event.getCategory(),
             event.getOperation(),
             event.getEntityUrn());
@@ -204,7 +204,7 @@ private PlatformEvent buildPlatformEvent(final ChangeEvent rawChangeEvent) {
    */
   private RecordTemplate convertRawEventToChangeEvent(final ChangeEvent rawChangeEvent) {
     com.linkedin.platform.event.v1.EntityChangeEvent changeEvent = new com.linkedin.platform.event.v1.EntityChangeEvent();
-    log.info(String.format("Attempting to convert %s", rawChangeEvent));
+    log.debug(String.format("Attempting to convert %s", rawChangeEvent));
     try {
       Urn entityUrn = Urn.createFromString(rawChangeEvent.getEntityUrn());
       changeEvent.setEntityType(entityUrn.getEntityType());
@@ -220,7 +220,6 @@ private RecordTemplate convertRawEventToChangeEvent(final ChangeEvent rawChangeE
             new Parameters(new DataMap(rawChangeEvent.getParameters()))
         );
       }
-      System.out.println(changeEvent.toString());
       return changeEvent;
     } catch (Exception e) {
       throw new RuntimeException("Failed to convert raw change event into PDL change", e);

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/timeline/mappers/SchemaBlameMapper.java
Patch:
@@ -33,13 +33,13 @@
 public class SchemaBlameMapper {
 
   public static GetSchemaBlameResult map(List<ChangeTransaction> changeTransactions, @Nullable String versionCutoff) {
+    GetSchemaBlameResult result = new GetSchemaBlameResult();
     if (changeTransactions.isEmpty()) {
       log.debug("Change transactions are empty");
-      return null;
+      return result;
     }
 
     Map<String, SchemaFieldBlame> schemaBlameMap = new HashMap<>();
-    GetSchemaBlameResult result = new GetSchemaBlameResult();
 
     String latestSemanticVersionString =
         truncateSemanticVersion(changeTransactions.get(changeTransactions.size() - 1).getSemVer());

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/BrowsableEntityType.java
Patch:
@@ -27,6 +27,7 @@ public interface BrowsableEntityType<T extends Entity, K> extends EntityType<T,
      * @param count the number of results to retrieve
      * @param context the {@link QueryContext} corresponding to the request.
      */
+    @Nonnull
     BrowseResults browse(@Nonnull List<String> path,
                          @Nullable List<FacetFilterInput> filters,
                          int start,
@@ -39,6 +40,7 @@ BrowseResults browse(@Nonnull List<String> path,
      * @param urn the entity urn to fetch browse paths for
      * @param context the {@link QueryContext} corresponding to the request.
      */
+    @Nonnull
     List<BrowsePath> browsePaths(@Nonnull String urn, @Nonnull final QueryContext context) throws Exception;
 
 }

File: metadata-io/src/main/java/com/linkedin/metadata/graph/JavaGraphClient.java
Patch:
@@ -38,10 +38,9 @@ public EntityRelationships getRelatedEntities(String rawUrn, List<String> relati
     count = count == null ? DEFAULT_PAGE_SIZE : count;
 
     RelatedEntitiesResult relatedEntitiesResult =
-        _graphService.findRelatedEntities(
-            "",
+        _graphService.findRelatedEntities(null,
             QueryUtils.newFilter("urn", rawUrn),
-            "",
+            null,
             EMPTY_FILTER,
             relationshipTypes,
             QueryUtils.newRelationshipFilter(EMPTY_FILTER, direction),

File: metadata-io/src/main/java/com/linkedin/metadata/graph/SiblingGraphService.java
Patch:
@@ -40,8 +40,8 @@ public EntityLineageResult getLineage(@Nonnull Urn entityUrn, @Nonnull LineageDi
    * Unless overridden, it uses the lineage registry to fetch valid edge types and queries for them
    */
   @Nonnull
-  public EntityLineageResult getLineage(@Nonnull Urn entityUrn, @Nonnull LineageDirection direction, int offset,
-      int count, int maxHops, boolean separateSiblings) {
+  public EntityLineageResult getLineage(@Nonnull Urn entityUrn, @Nonnull LineageDirection direction,
+     int offset, int count, int maxHops, boolean separateSiblings) {
     if (separateSiblings) {
       return _graphService.getLineage(entityUrn, direction, offset, count, maxHops);
     }

File: metadata-service/restli-servlet-impl/src/main/java/com/linkedin/metadata/resources/lineage/Relationships.java
Patch:
@@ -67,7 +67,7 @@ private RelatedEntitiesResult getRelatedEntities(String rawUrn, List<String> rel
     start = start == null ? 0 : start;
     count = count == null ? MAX_DOWNSTREAM_CNT : count;
 
-    return _graphService.findRelatedEntities("", newFilter("urn", rawUrn), "", QueryUtils.EMPTY_FILTER,
+    return _graphService.findRelatedEntities(null, newFilter("urn", rawUrn), null, QueryUtils.EMPTY_FILTER,
         relationshipTypes, newRelationshipFilter(QueryUtils.EMPTY_FILTER, direction), start, count);
   }
 

File: metadata-io/src/main/java/com/linkedin/metadata/search/aggregator/AllEntitiesSearchAggregator.java
Patch:
@@ -158,7 +158,6 @@ private Map<String, SearchResult> getSearchResultsForEachEntity(@Nonnull List<St
       searchResults = ConcurrencyUtils.transformAndCollectAsync(entities, entity -> new Pair<>(entity,
           _cachingEntitySearchService.search(entity, input, postFilters, sortCriterion, queryFrom, querySize, searchFlags)))
           .stream()
-          .filter(pair -> pair.getValue().getNumEntities() > 0)
           .collect(Collectors.toMap(Pair::getKey, Pair::getValue));
     }
     return searchResults;

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/GmsGraphQLEngine.java
Patch:
@@ -144,6 +144,7 @@
 import com.linkedin.datahub.graphql.resolvers.mutate.AddTagsResolver;
 import com.linkedin.datahub.graphql.resolvers.mutate.AddTermResolver;
 import com.linkedin.datahub.graphql.resolvers.mutate.AddTermsResolver;
+import com.linkedin.datahub.graphql.resolvers.mutate.MutableTypeBatchResolver;
 import com.linkedin.datahub.graphql.resolvers.mutate.BatchAddOwnersResolver;
 import com.linkedin.datahub.graphql.resolvers.mutate.BatchAddTagsResolver;
 import com.linkedin.datahub.graphql.resolvers.mutate.BatchAddTermsResolver;
@@ -681,6 +682,7 @@ private String getUrnField(DataFetchingEnvironment env) {
     private void configureMutationResolvers(final RuntimeWiring.Builder builder) {
         builder.type("Mutation", typeWiring -> typeWiring
             .dataFetcher("updateDataset", new MutableTypeResolver<>(datasetType))
+            .dataFetcher("updateDatasets", new MutableTypeBatchResolver<>(datasetType))
             .dataFetcher("createTag", new CreateTagResolver(this.entityClient))
             .dataFetcher("updateTag", new MutableTypeResolver<>(tagType))
             .dataFetcher("setTagColor", new SetTagColorResolver(entityClient, entityService))

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/MutableType.java
Patch:
@@ -1,6 +1,7 @@
 package com.linkedin.datahub.graphql.types;
 
 import com.linkedin.datahub.graphql.QueryContext;
+
 import javax.annotation.Nonnull;
 
 /**
@@ -9,12 +10,11 @@
  * @param <I>: The input type corresponding to the write.
  */
 public interface MutableType<I, T> {
-
     /**
      * Returns generated GraphQL class associated with the input type
      */
-    Class<I> inputClass();
 
+    Class<I> inputClass();
 
     /**
      * Update an entity by urn

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/config/AppConfigResolver.java
Patch:
@@ -87,7 +87,7 @@ public CompletableFuture<AppConfig> get(final DataFetchingEnvironment environmen
     authConfig.setTokenAuthEnabled(_authenticationConfiguration.isEnabled());
 
     final PoliciesConfig policiesConfig = new PoliciesConfig();
-    policiesConfig.setEnabled(Boolean.TRUE.equals(_authorizationConfiguration.getDefaultAuthorizer().getEnabled()));
+    policiesConfig.setEnabled(_authorizationConfiguration.getDefaultAuthorizer().isEnabled());
 
     policiesConfig.setPlatformPrivileges(com.linkedin.metadata.authorization.PoliciesConfig.PLATFORM_PRIVILEGES
         .stream()

File: metadata-service/auth-api/src/main/java/com/datahub/authorization/AuthorizerConfiguration.java
Patch:
@@ -12,7 +12,7 @@ public class AuthorizerConfiguration {
   /**
    * Whether to enable this authorizer
    */
-  private Boolean enabled = false;
+  private boolean enabled;
   /**
    * A fully-qualified class name for the {@link Authorizer} implementation to be registered.
    */

File: metadata-service/auth-api/src/main/java/com/datahub/authorization/DefaultAuthorizerConfiguration.java
Patch:
@@ -8,7 +8,7 @@ public class DefaultAuthorizerConfiguration {
   /**
    * Whether authorization via DataHub policies is enabled.
    */
-  private Boolean enabled;
+  private boolean enabled;
   /**
    * The duration between policies cache refreshes.
    */

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/auth/AuthorizerChainFactory.java
Patch:
@@ -56,7 +56,7 @@ protected AuthorizerChain getInstance() {
     // Extract + initialize customer authorizers from application configs.
     final List<Authorizer> authorizers = new ArrayList<>(initCustomAuthorizers(ctx));
 
-    if (Boolean.TRUE.equals(configurationProvider.getAuthorization().getDefaultAuthorizer().getEnabled())) {
+    if (configurationProvider.getAuthorization().getDefaultAuthorizer().isEnabled()) {
       this.dataHubAuthorizer.init(Collections.emptyMap(), ctx);
       log.info("Default DataHubAuthorizer is enabled. Appending it to the authorization chain.");
       authorizers.add(this.dataHubAuthorizer);
@@ -81,8 +81,7 @@ private List<Authorizer> initCustomAuthorizers(AuthorizerContext ctx) {
       for (AuthorizerConfiguration authorizer : authorizerConfigurations) {
         final String type = authorizer.getType();
         // continue if authorizer is not enabled
-        // Boolean.FALSE.equals take care to map null to false and false to false
-        if (Boolean.FALSE.equals(authorizer.getEnabled())) {
+        if (!authorizer.isEnabled()) {
           log.info(String.format("Authorizer %s is not enabled", type));
           continue;
         }

File: metadata-integration/java/datahub-protobuf/src/main/java/datahub/protobuf/visitors/dataset/OwnershipVisitor.java
Patch:
@@ -37,14 +37,14 @@ public Stream<Owner> visitGraph(VisitContext context) {
                         try {
                             ownershipType = OwnershipType.valueOf(entry.getKey().getName().toUpperCase());
                         } catch (IllegalArgumentException e) {
-                            ownershipType = OwnershipType.PRODUCER;
+                            ownershipType = OwnershipType.TECHNICAL_OWNER;
                         }
 
                         String[] id = entry.getValue().toLowerCase().split(":", 2);
                         return new Owner()
                                 .setType(ownershipType)
                                 .setSource(new OwnershipSource().setType(OwnershipSourceType.MANUAL))
-                                .setOwner(new Urn(id.length > 1 ? id[0] : "corpgroup", id[id.length - 1]));
+                                .setOwner(new Urn(id.length > 1 ? id[0].replaceFirst("corpgroup", "corpGroup") : "corpGroup", id[id.length - 1]));
                     } catch (URISyntaxException e) {
                         System.err.println(e.getMessage());
                         return null;

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/aspect/AspectType.java
Patch:
@@ -62,7 +62,7 @@ public List<DataFetcherResult<Aspect>> batchLoad(@Nonnull List<VersionedAspectKe
             return DataFetcherResult.<Aspect>newResult().data(null).build();
           }
           final EnvelopedAspect aspect = entityResponse.getAspects().get(key.getAspectName());
-          return DataFetcherResult.<Aspect>newResult().data(AspectMapper.map(aspect)).build();
+          return DataFetcherResult.<Aspect>newResult().data(AspectMapper.map(aspect, entityUrn)).build();
         } catch (Exception e) {
           if (e instanceof RestLiResponseException) {
             // if no aspect is found, restli will return a 404 rather than null

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/assertion/AssertionMapper.java
Patch:
@@ -142,4 +142,4 @@ private static AssertionStdParameter mapParameter(final com.linkedin.assertion.A
 
   private AssertionMapper() {
   }
-}
\ No newline at end of file
+}

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/common/mappers/StringMapMapper.java
Patch:
@@ -1,13 +1,12 @@
 package com.linkedin.datahub.graphql.types.common.mappers;
 
-
 import com.linkedin.datahub.graphql.generated.StringMapEntry;
 import com.linkedin.datahub.graphql.types.mappers.ModelMapper;
-
-import javax.annotation.Nonnull;
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
+import javax.annotation.Nonnull;
+
 
 /**
  * Maps Pegasus {@link RecordTemplate} objects to objects conforming to the GQL schema.

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/domain/DomainMapper.java
Patch:
@@ -40,7 +40,7 @@ public static Domain map(final EntityResponse entityResponse) {
 
     final EnvelopedAspect envelopedOwnership = aspects.get(Constants.OWNERSHIP_ASPECT_NAME);
     if (envelopedOwnership != null) {
-      result.setOwnership(OwnershipMapper.map(new Ownership(envelopedOwnership.getValue().data())));
+      result.setOwnership(OwnershipMapper.map(new Ownership(envelopedOwnership.getValue().data()), entityUrn));
     }
 
     final EnvelopedAspect envelopedInstitutionalMemory = aspects.get(Constants.INSTITUTIONAL_MEMORY_ASPECT_NAME);
@@ -59,4 +59,4 @@ private static com.linkedin.datahub.graphql.generated.DomainProperties mapDomain
   }
 
   private DomainMapper() { }
-}
\ No newline at end of file
+}

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/glossary/mappers/GlossaryNodeMapper.java
Patch:
@@ -1,6 +1,7 @@
 package com.linkedin.datahub.graphql.types.glossary.mappers;
 
 import com.linkedin.common.Ownership;
+import com.linkedin.common.urn.Urn;
 import com.linkedin.data.DataMap;
 import com.linkedin.datahub.graphql.generated.EntityType;
 import com.linkedin.datahub.graphql.generated.GlossaryNode;
@@ -30,14 +31,15 @@ public GlossaryNode apply(@Nonnull final EntityResponse entityResponse) {
     GlossaryNode result = new GlossaryNode();
     result.setUrn(entityResponse.getUrn().toString());
     result.setType(EntityType.GLOSSARY_NODE);
+    Urn entityUrn = entityResponse.getUrn();
 
     EnvelopedAspectMap aspectMap = entityResponse.getAspects();
     MappingHelper<GlossaryNode> mappingHelper = new MappingHelper<>(aspectMap, result);
     mappingHelper.mapToResult(GLOSSARY_NODE_INFO_ASPECT_NAME, (glossaryNode, dataMap) ->
         glossaryNode.setProperties(mapGlossaryNodeProperties(dataMap)));
     mappingHelper.mapToResult(GLOSSARY_NODE_KEY_ASPECT_NAME, this::mapGlossaryNodeKey);
     mappingHelper.mapToResult(OWNERSHIP_ASPECT_NAME, (glossaryNode, dataMap) ->
-        glossaryNode.setOwnership(OwnershipMapper.map(new Ownership(dataMap))));
+        glossaryNode.setOwnership(OwnershipMapper.map(new Ownership(dataMap), entityUrn)));
 
     return mappingHelper.getResult();
   }

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/tag/mappers/TagMapper.java
Patch:
@@ -1,6 +1,7 @@
 package com.linkedin.datahub.graphql.types.tag.mappers;
 
 import com.linkedin.common.Ownership;
+import com.linkedin.common.urn.Urn;
 import com.linkedin.data.DataMap;
 import com.linkedin.data.template.GetMode;
 import com.linkedin.data.template.RecordTemplate;
@@ -34,6 +35,7 @@ public static Tag map(@Nonnull final EntityResponse entityResponse) {
     @Override
     public Tag apply(@Nonnull final EntityResponse entityResponse) {
         final Tag result = new Tag();
+        Urn entityUrn = entityResponse.getUrn();
         result.setUrn(entityResponse.getUrn().toString());
         result.setType(EntityType.TAG);
 
@@ -45,7 +47,7 @@ public Tag apply(@Nonnull final EntityResponse entityResponse) {
         mappingHelper.mapToResult(TAG_KEY_ASPECT_NAME, this::mapTagKey);
         mappingHelper.mapToResult(TAG_PROPERTIES_ASPECT_NAME, this::mapTagProperties);
         mappingHelper.mapToResult(OWNERSHIP_ASPECT_NAME, (tag, dataMap) ->
-            tag.setOwnership(OwnershipMapper.map(new Ownership(dataMap))));
+            tag.setOwnership(OwnershipMapper.map(new Ownership(dataMap), entityUrn)));
 
         if (result.getProperties() != null && result.getProperties().getName() == null) {
             result.getProperties().setName(legacyName);

File: metadata-service/auth-impl/src/main/java/com/datahub/authentication/token/StatefulTokenService.java
Patch:
@@ -161,7 +161,7 @@ public TokenClaims validateAccessToken(@Nonnull String accessToken) throws Token
       this.revokeAccessToken(hash(accessToken));
       throw e;
     } catch (final ExecutionException e) {
-      throw new TokenException("Failed to validate DataHub token: Unable to load token information from store");
+      throw new TokenException("Failed to validate DataHub token: Unable to load token information from store", e);
     }
   }
 
@@ -174,7 +174,7 @@ public void revokeAccessToken(@Nonnull String hashedToken) throws TokenException
         return;
       }
     } catch (ExecutionException e) {
-      throw new TokenException("Failed to validate DataHub token from cache");
+      throw new TokenException("Failed to validate DataHub token from cache", e);
     }
     throw new TokenException("Access token no longer exists");
   }

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/auth/DataHubAuthorizerFactory.java
Patch:
@@ -32,7 +32,7 @@ public class DataHubAuthorizerFactory {
   @Value("${authorization.defaultAuthorizer.cacheRefreshIntervalSecs}")
   private Integer policyCacheRefreshIntervalSeconds;
 
-  @Value("${authorization.defaultAuthorizer..enabled:true}")
+  @Value("${authorization.defaultAuthorizer.enabled:true}")
   private Boolean policiesEnabled;
 
   @Bean(name = "dataHubAuthorizer")

File: metadata-io/src/main/java/com/linkedin/metadata/entity/cassandra/AspectStorageValidationUtil.java
Patch:
@@ -16,8 +16,8 @@ private AspectStorageValidationUtil() {
    * @return {@code true} if table exists.
    */
   public static boolean checkTableExists(@Nonnull CqlSession session) {
-    String query = String.format("SELECT columnfamily_name\n "
-        + "FROM schema_columnfamilies WHERE keyspace_name='%s';",
+    String query = String.format("SELECT table_name \n "
+        + "FROM system_schema.tables where table_name = '%s' allow filtering;",
         CassandraAspect.TABLE_NAME);
     ResultSet rs = session.execute(query);
     return rs.all().size() > 0;

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/search/AutoCompleteForMultipleResolver.java
Patch:
@@ -57,7 +57,7 @@ public CompletableFuture<AutoCompleteMultipleResults> get(DataFetchingEnvironmen
                 environment);
         }
 
-        // By default, autocomplete only against the set of Searchable Entity Types.
+        // By default, autocomplete only against the Default Set of Autocomplete entities
         return AutocompleteUtils.batchGetAutocompleteResults(
             AUTO_COMPLETE_ENTITY_TYPES.stream().map(_typeToEntity::get).collect(Collectors.toList()),
             sanitizedQuery,

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/GmsGraphQLEngine.java
Patch:
@@ -675,7 +675,7 @@ private String getUrnField(DataFetchingEnvironment env) {
     private void configureMutationResolvers(final RuntimeWiring.Builder builder) {
         builder.type("Mutation", typeWiring -> typeWiring
             .dataFetcher("updateDataset", new MutableTypeResolver<>(datasetType))
-            .dataFetcher("createTag", new CreateTagResolver(entityService))
+            .dataFetcher("createTag", new CreateTagResolver(this.entityClient))
             .dataFetcher("updateTag", new MutableTypeResolver<>(tagType))
             .dataFetcher("setTagColor", new SetTagColorResolver(entityClient, entityService))
             .dataFetcher("deleteTag", new DeleteTagResolver(entityClient))
@@ -707,7 +707,7 @@ private void configureMutationResolvers(final RuntimeWiring.Builder builder) {
             .dataFetcher("removeUser", new RemoveUserResolver(this.entityClient))
             .dataFetcher("removeGroup", new RemoveGroupResolver(this.entityClient))
             .dataFetcher("updateUserStatus", new UpdateUserStatusResolver(this.entityClient))
-            .dataFetcher("createDomain", new CreateDomainResolver(this.entityService))
+            .dataFetcher("createDomain", new CreateDomainResolver(this.entityClient))
             .dataFetcher("deleteDomain", new DeleteDomainResolver(entityClient))
             .dataFetcher("setDomain", new SetDomainResolver(this.entityClient, this.entityService))
             .dataFetcher("updateDeprecation", new UpdateDeprecationResolver(this.entityClient, this.entityService))

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/domain/DeleteDomainResolver.java
Patch:
@@ -34,6 +34,7 @@ public CompletableFuture<Boolean> get(final DataFetchingEnvironment environment)
       if (AuthorizationUtils.canManageDomains(context) || AuthorizationUtils.canDeleteEntity(urn, context)) {
         try {
           _entityClient.deleteEntity(urn, context.getAuthentication());
+          log.info(String.format("I've successfully deleted the entity %s with urn", domainUrn));
 
           // Asynchronously Delete all references to the entity (to return quickly)
           CompletableFuture.runAsync(() -> {

File: metadata-service/restli-client/src/main/java/com/linkedin/entity/client/EntityClient.java
Patch:
@@ -296,4 +296,6 @@ public DataMap getRawAspect(@Nonnull String urn, @Nonnull String aspect, @Nonnul
 
   public void producePlatformEvent(@Nonnull String name, @Nullable String key, @Nonnull PlatformEvent event,
       @Nonnull Authentication authentication) throws Exception;
+
+  Boolean exists(Urn urn, @Nonnull Authentication authentication) throws RemoteInvocationException;
 }

File: datahub-frontend/app/auth/JAASConfigs.java
Patch:
@@ -11,9 +11,7 @@ public class JAASConfigs {
     private Boolean _isEnabled = true;
 
     public JAASConfigs(final com.typesafe.config.Config configs) {
-        if (configs.hasPath(JAAS_ENABLED_CONFIG_PATH)
-                && Boolean.FALSE.equals(
-                        Boolean.parseBoolean(configs.getValue(JAAS_ENABLED_CONFIG_PATH).toString()))) {
+        if (configs.hasPath(JAAS_ENABLED_CONFIG_PATH) && !configs.getBoolean(JAAS_ENABLED_CONFIG_PATH)) {
             _isEnabled = false;
         }
     }

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/auth/ListAccessTokensResolver.java
Patch:
@@ -14,7 +14,6 @@
 import com.linkedin.metadata.query.filter.SortCriterion;
 import com.linkedin.metadata.query.filter.SortOrder;
 import com.linkedin.metadata.search.SearchResult;
-import com.linkedin.metadata.search.utils.QueryUtils;
 import graphql.schema.DataFetcher;
 import graphql.schema.DataFetchingEnvironment;
 
@@ -62,7 +61,7 @@ public CompletableFuture<ListAccessTokenResult> get(DataFetchingEnvironment envi
               new SortCriterion().setField(EXPIRES_AT_FIELD_NAME).setOrder(SortOrder.DESCENDING);
 
           final SearchResult searchResult = _entityClient.search(Constants.ACCESS_TOKEN_ENTITY_NAME, "",
-              QueryUtils.newFilter(buildFacetFilters(filters, FACET_FIELDS)), sortCriterion, start, count,
+              buildFilter(filters), sortCriterion, start, count,
               getAuthentication(environment));
 
           final List<AccessTokenMetadata> tokens = searchResult.getEntities().stream().map(entity -> {

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/auth/ListAccessTokensResolver.java
Patch:
@@ -101,6 +101,6 @@ public CompletableFuture<ListAccessTokenResult> get(DataFetchingEnvironment envi
    */
   private boolean isListingSelfTokens(final List<FacetFilterInput> filters, final QueryContext context) {
     return AuthorizationUtils.canGeneratePersonalAccessToken(context) && filters.stream()
-        .anyMatch(filter -> filter.getField().equals("actorUrn") && filter.getValue().equals(context.getActorUrn()));
+        .anyMatch(filter -> filter.getField().equals("ownerUrn") && filter.getValue().equals(context.getActorUrn()));
   }
 }

File: metadata-service/auth-api/src/main/java/com/datahub/authentication/AuthenticationConstants.java
Patch:
@@ -30,6 +30,8 @@ public class AuthenticationConstants {
   public static final String SYSTEM_CLIENT_SECRET_CONFIG = "systemClientSecret";
 
   public static final String ENTITY_SERVICE = "entityService";
+  public static final String TOKEN_SERVICE = "tokenService";
+
 
   private AuthenticationConstants() { }
 }

File: metadata-service/auth-impl/src/main/java/com/datahub/authentication/authenticator/DataHubTokenAuthenticator.java
Patch:
@@ -62,9 +62,7 @@ public void init(@Nonnull final Map<String, Object> config, final AuthenticatorC
           "Unable to initialize DataHubTokenAuthenticator, entity service reference is not of type: "
               + "EntityService.class, found: " + entityService.getClass());
     }
-    this._statefulTokenService =
-        new StatefulTokenService(signingKey, signingAlgorithm, DEFAULT_ISSUER, (EntityService) entityService,
-            salt);
+    this._statefulTokenService = (StatefulTokenService) Objects.requireNonNull(context.data().get(TOKEN_SERVICE));
   }
 
   @Override

File: metadata-service/auth-impl/src/main/java/com/datahub/authentication/token/StatefulTokenService.java
Patch:
@@ -56,7 +56,7 @@ public StatefulTokenService(@Nonnull final String signingKey, @Nonnull final Str
     this._entityService = entityService;
     this._revokedTokenCache = CacheBuilder.newBuilder()
         .maximumSize(10000)
-        .expireAfterWrite(6, TimeUnit.HOURS)
+        .expireAfterWrite(5, TimeUnit.MINUTES)
         .build(new CacheLoader<String, Boolean>() {
           @Override
           public Boolean load(final String key) {

File: metadata-integration/java/datahub-client/src/main/java/datahub/client/Emitter.java
Patch:
@@ -5,7 +5,6 @@
 import datahub.event.UpsertAspectRequest;
 import java.io.Closeable;
 import java.io.IOException;
-import java.net.URISyntaxException;
 import java.util.List;
 import java.util.concurrent.ExecutionException;
 import java.util.concurrent.Future;
@@ -30,7 +29,6 @@ public interface Emitter extends Closeable {
    * @param callback if not null, is called from the IO thread. Should be a quick operation.
    * @return a {@link Future} for callers to inspect the result of the operation or block until one is available
    * @throws IOException
-   * @throws URISyntaxException 
    */
   Future<MetadataWriteResponse> emit(@Nonnull MetadataChangeProposalWrapper mcpw, Callback callback) throws IOException;
 

File: datahub-upgrade/src/main/java/com/linkedin/datahub/upgrade/restoreindices/SendMAEStep.java
Patch:
@@ -114,7 +114,7 @@ public Function<UpgradeContext, UpgradeStepResult> executable() {
           _entityService.produceMetadataChangeLog(urn, entityName, aspectName, aspectSpec, null, aspectRecord, null,
               latestSystemMetadata,
               new AuditStamp().setActor(UrnUtils.getUrn(SYSTEM_ACTOR)).setTime(System.currentTimeMillis()),
-              ChangeType.UPSERT);
+              ChangeType.RESTATE);
 
           totalRowsMigrated++;
         }

File: metadata-jobs/mae-consumer/src/main/java/com/linkedin/metadata/kafka/hook/UpdateIndicesHook.java
Patch:
@@ -95,7 +95,7 @@ public void invoke(@Nonnull MetadataChangeLog event) {
     }
     Urn urn = EntityKeyUtils.getUrnFromLog(event, entitySpec.getKeyAspectSpec());
 
-    if (event.getChangeType() == ChangeType.UPSERT) {
+    if (event.getChangeType() == ChangeType.UPSERT || event.getChangeType() == ChangeType.RESTATE) {
 
       if (!event.hasAspectName() || !event.hasAspect()) {
         log.error("Aspect or aspect name is missing");

File: metadata-io/src/main/java/com/linkedin/metadata/timeline/TimelineServiceImpl.java
Patch:
@@ -427,9 +427,9 @@ private List<ChangeTransaction> combineTransactionsByTimestamp(List<ChangeTransa
           ChangeTransaction element = transactionList.get(i);
           result.getChangeEvents().addAll(element.getChangeEvents());
           maxSemanticChangeType =
-              result.getSemVerChange().compareTo(element.getSemVerChange()) >= 0 ? result.getSemVerChange()
+              maxSemanticChangeType.compareTo(element.getSemVerChange()) >= 0 ? maxSemanticChangeType
                   : element.getSemVerChange();
-          maxSemVer = result.getSemVer().compareTo(element.getSemVer()) >= 0 ? result.getSemVer() : element.getSemVer();
+          maxSemVer = maxSemVer.compareTo(element.getSemVer()) >= 0 ? maxSemVer : element.getSemVer();
         }
         result.setSemVerChange(maxSemanticChangeType);
         result.setSemanticVersion(maxSemVer);

File: datahub-upgrade/src/main/java/com/linkedin/datahub/upgrade/UpgradeCliApplication.java
Patch:
@@ -1,13 +1,14 @@
 package com.linkedin.datahub.upgrade;
 
+import com.linkedin.gms.factory.telemetry.ScheduledAnalyticsFactory;
 import org.springframework.boot.WebApplicationType;
 import org.springframework.boot.autoconfigure.SpringBootApplication;
 import org.springframework.boot.autoconfigure.elasticsearch.ElasticsearchRestClientAutoConfiguration;
 import org.springframework.boot.builder.SpringApplicationBuilder;
 
 
 @SuppressWarnings("checkstyle:HideUtilityClassConstructor")
-@SpringBootApplication(exclude = {ElasticsearchRestClientAutoConfiguration.class}, scanBasePackages = {
+@SpringBootApplication(exclude = {ElasticsearchRestClientAutoConfiguration.class, ScheduledAnalyticsFactory.class}, scanBasePackages = {
     "com.linkedin.gms.factory", "com.linkedin.datahub.upgrade.config"})
 public class UpgradeCliApplication {
   public static void main(String[] args) {

File: metadata-jobs/mae-consumer-job/src/main/java/com/linkedin/metadata/kafka/MaeConsumerApplication.java
Patch:
@@ -1,12 +1,14 @@
 package com.linkedin.metadata.kafka;
 
+import com.linkedin.gms.factory.telemetry.ScheduledAnalyticsFactory;
 import org.springframework.boot.SpringApplication;
 import org.springframework.boot.autoconfigure.SpringBootApplication;
 import org.springframework.boot.autoconfigure.cassandra.CassandraAutoConfiguration;
 import org.springframework.boot.autoconfigure.elasticsearch.ElasticsearchRestClientAutoConfiguration;
 
 @SuppressWarnings("checkstyle:HideUtilityClassConstructor")
-@SpringBootApplication(exclude = {ElasticsearchRestClientAutoConfiguration.class, CassandraAutoConfiguration.class})
+@SpringBootApplication(exclude = {ElasticsearchRestClientAutoConfiguration.class, CassandraAutoConfiguration.class,
+    ScheduledAnalyticsFactory.class})
 public class MaeConsumerApplication {
 
   public static void main(String[] args) {

File: metadata-jobs/mce-consumer-job/src/main/java/com/linkedin/metadata/kafka/MceConsumerApplication.java
Patch:
@@ -1,13 +1,15 @@
 package com.linkedin.metadata.kafka;
 
+import com.linkedin.gms.factory.telemetry.ScheduledAnalyticsFactory;
 import org.springframework.boot.SpringApplication;
 import org.springframework.boot.autoconfigure.SpringBootApplication;
 import org.springframework.boot.autoconfigure.cassandra.CassandraAutoConfiguration;
 import org.springframework.boot.autoconfigure.elasticsearch.ElasticsearchRestClientAutoConfiguration;
 
 
 @SuppressWarnings("checkstyle:HideUtilityClassConstructor")
-@SpringBootApplication(exclude = {ElasticsearchRestClientAutoConfiguration.class, CassandraAutoConfiguration.class})
+@SpringBootApplication(exclude = {ElasticsearchRestClientAutoConfiguration.class, CassandraAutoConfiguration.class,
+    ScheduledAnalyticsFactory.class})
 public class MceConsumerApplication {
 
   public static void main(String[] args) {

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/auth/DataHubTokenServiceFactory.java
Patch:
@@ -19,7 +19,7 @@ public class DataHubTokenServiceFactory {
   @Value("${authentication.tokenService.signingKey:}")
   private String signingKey;
 
-  @Value("${authentication.tokenService.saltingKey:}")
+  @Value("${authentication.tokenService.salt:}")
   private String saltingKey;
 
   @Value("${elasticsearch.tokenService.signingAlgorithm:HS256}")

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/telemetry/TelemetryUtils.java
Patch:
@@ -15,7 +15,7 @@
 public final class TelemetryUtils {
 
     public static final String CLIENT_ID_URN = "urn:li:telemetry:clientId";
-    public static final String CLIENT_ID_ASPECT = "clientId";
+    public static final String CLIENT_ID_ASPECT = "telemetryClientId";
 
     private static String _clientId;
 

File: metadata-integration/java/datahub-client/src/main/java/datahub/client/Emitter.java
Patch:
@@ -5,6 +5,7 @@
 import datahub.event.UpsertAspectRequest;
 import java.io.Closeable;
 import java.io.IOException;
+import java.net.URISyntaxException;
 import java.util.List;
 import java.util.concurrent.ExecutionException;
 import java.util.concurrent.Future;
@@ -29,6 +30,7 @@ public interface Emitter extends Closeable {
    * @param callback if not null, is called from the IO thread. Should be a quick operation.
    * @return a {@link Future} for callers to inspect the result of the operation or block until one is available
    * @throws IOException
+   * @throws URISyntaxException 
    */
   Future<MetadataWriteResponse> emit(@Nonnull MetadataChangeProposalWrapper mcpw, Callback callback) throws IOException;
 

File: metadata-integration/java/datahub-client/src/main/java/datahub/event/EventFormatter.java
Patch:
@@ -7,6 +7,7 @@
 import com.linkedin.data.template.JacksonDataTemplateCodec;
 import com.linkedin.mxe.GenericAspect;
 import com.linkedin.mxe.MetadataChangeProposal;
+
 import java.io.IOException;
 import java.net.URISyntaxException;
 import java.nio.charset.StandardCharsets;
@@ -33,7 +34,8 @@ public EventFormatter() {
 
   @SneakyThrows(URISyntaxException.class)
   public MetadataChangeProposal convert(MetadataChangeProposalWrapper mcpw) throws IOException {
-    String serializedAspect = dataTemplateCodec.dataTemplateToString(mcpw.getAspect());
+    
+    String serializedAspect = StringEscapeUtils.escapeJava(dataTemplateCodec.dataTemplateToString(mcpw.getAspect()));
     MetadataChangeProposal mcp = new MetadataChangeProposal().setEntityType(mcpw.getEntityType())
         .setAspectName(mcpw.getAspectName())
         .setEntityUrn(Urn.createFromString(mcpw.getEntityUrn()))

File: metadata-integration/java/datahub-client/src/test/java/datahub/client/rest/RestEmitterTest.java
Patch:
@@ -90,7 +90,7 @@ public void testPost() throws URISyntaxException, IOException {
         + ",\"value\":\"{\\\"description\\\":\\\"Test Dataset\\\"}\"}}}";
     Assert.assertEquals(expectedContent, contentString);
   }
-
+  
   @Test
   public void testExceptions() throws URISyntaxException, IOException, ExecutionException, InterruptedException {
 

File: metadata-jobs/mae-consumer-job/src/main/java/com/linkedin/metadata/kafka/MaeConsumerApplication.java
Patch:
@@ -2,14 +2,15 @@
 
 import org.springframework.boot.SpringApplication;
 import org.springframework.boot.autoconfigure.SpringBootApplication;
+import org.springframework.boot.autoconfigure.cassandra.CassandraAutoConfiguration;
 import org.springframework.boot.autoconfigure.elasticsearch.ElasticsearchRestClientAutoConfiguration;
 
 @SuppressWarnings("checkstyle:HideUtilityClassConstructor")
-@SpringBootApplication(exclude = {ElasticsearchRestClientAutoConfiguration.class})
+@SpringBootApplication(exclude = {ElasticsearchRestClientAutoConfiguration.class, CassandraAutoConfiguration.class})
 public class MaeConsumerApplication {
 
   public static void main(String[] args) {
     Class<?>[] primarySources = {MaeConsumerApplication.class, MclConsumerConfig.class};
     SpringApplication.run(primarySources, args);
   }
-}
\ No newline at end of file
+}

File: metadata-jobs/mce-consumer-job/src/main/java/com/linkedin/metadata/kafka/MceConsumerApplication.java
Patch:
@@ -2,14 +2,15 @@
 
 import org.springframework.boot.SpringApplication;
 import org.springframework.boot.autoconfigure.SpringBootApplication;
+import org.springframework.boot.autoconfigure.cassandra.CassandraAutoConfiguration;
 import org.springframework.boot.autoconfigure.elasticsearch.ElasticsearchRestClientAutoConfiguration;
 
 
 @SuppressWarnings("checkstyle:HideUtilityClassConstructor")
-@SpringBootApplication(exclude = {ElasticsearchRestClientAutoConfiguration.class})
+@SpringBootApplication(exclude = {ElasticsearchRestClientAutoConfiguration.class, CassandraAutoConfiguration.class})
 public class MceConsumerApplication {
 
   public static void main(String[] args) {
     SpringApplication.run(MceConsumerApplication.class, args);
   }
-}
\ No newline at end of file
+}

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/timeline/mappers/SchemaFieldBlameMapper.java
Patch:
@@ -147,6 +147,7 @@ private static SchemaFieldChange getLastSchemaFieldChange(ChangeEvent changeEven
       String semanticVersion, String versionStamp) {
     SchemaFieldChange schemaFieldChange = new SchemaFieldChange();
     schemaFieldChange.setTimestampMillis(timestamp);
+    schemaFieldChange.setLastSemanticVersion(truncateSemanticVersion(semanticVersion));
     schemaFieldChange.setChangeType(
         ChangeOperationType.valueOf(ChangeOperationType.class, changeEvent.getOperation().toString()));
     schemaFieldChange.setVersionStamp(versionStamp);

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/ingestion/IngestionSchedulerFactory.java
Patch:
@@ -32,10 +32,10 @@ public class IngestionSchedulerFactory {
   @Autowired
   private ConfigurationProvider _configProvider;
 
-  @Value("${ingestion.scheduler.delayIntervalSeconds:30}")
+  @Value("${ingestion.scheduler.delayIntervalSeconds:45}") // Boot up ingestion source cache after waiting 45 seconds for startup.
   private Integer _delayIntervalSeconds;
 
-  @Value("${ingestion.scheduler.refreshIntervalSeconds:86400}")
+  @Value("${ingestion.scheduler.refreshIntervalSeconds:43200}") // By default, refresh ingestion sources 2 times per day.
   private Integer _refreshIntervalSeconds;
 
   @Bean(name = "ingestionScheduler")

File: metadata-utils/src/main/java/com/linkedin/metadata/Constants.java
Patch:
@@ -179,6 +179,7 @@ public class Constants {
   public static final String ASSERTION_RUN_EVENT_STATUS_COMPLETE = "COMPLETE";
 
   // DataHub Ingestion Source
+  public static final String INGESTION_SOURCE_KEY_ASPECT_NAME = "dataHubIngestionSourceKey";
   public static final String INGESTION_INFO_ASPECT_NAME = "dataHubIngestionSourceInfo";
 
   // DataHub Secret

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/policy/UpsertPolicyResolver.java
Patch:
@@ -60,6 +60,8 @@ public CompletableFuture<String> get(final DataFetchingEnvironment environment)
 
       // Create the policy info.
       final DataHubPolicyInfo info = PolicyUpdateInputInfoMapper.map(input);
+      info.setLastUpdatedTimestamp(System.currentTimeMillis());
+
       proposal.setEntityType(POLICY_ENTITY_NAME);
       proposal.setAspectName(POLICY_INFO_ASPECT_NAME);
       proposal.setAspect(GenericRecordUtils.serializeAspect(info));

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/search/SearchResolver.java
Patch:
@@ -44,7 +44,7 @@ public CompletableFuture<SearchResults> get(DataFetchingEnvironment environment)
         log.debug("Executing search. entity type {}, query {}, filters: {}, start: {}, count: {}", input.getType(),
             input.getQuery(), input.getFilters(), start, count);
         return UrnSearchResultsMapper.map(
-            _entityClient.search(entityName, sanitizedQuery, ResolverUtils.buildFilter(input.getFilters()), start,
+            _entityClient.search(entityName, sanitizedQuery, ResolverUtils.buildFilter(input.getFilters()), null, start,
                 count, ResolverUtils.getAuthentication(environment)));
       } catch (Exception e) {
         log.error("Failed to execute search: entity type {}, query {}, filters: {}, start: {}, count: {}",

File: metadata-service/restli-client/src/main/java/com/linkedin/entity/client/EntityClient.java
Patch:
@@ -146,14 +146,15 @@ public ListResult list(@Nonnull String entity, @Nullable Map<String, String> req
    *
    * @param input search query
    * @param filter search filters
+   * @param sortCriterion sort criterion
    * @param start start offset for search results
    * @param count max number of search results requested
    * @return Snapshot key
    * @throws RemoteInvocationException
    */
   @Nonnull
-  public SearchResult search(@Nonnull String entity, @Nonnull String input, @Nullable Filter filter, int start,
-      int count, @Nonnull Authentication authentication) throws RemoteInvocationException;
+  public SearchResult search(@Nonnull String entity, @Nonnull String input, @Nullable Filter filter,
+      SortCriterion sortCriterion, int start, int count, @Nonnull Authentication authentication) throws RemoteInvocationException;
 
   /**
    * Searches for entities matching to a given query and filters across multiple entity types

File: metadata-service/restli-client/src/main/java/com/linkedin/entity/client/JavaEntityClient.java
Patch:
@@ -248,6 +248,7 @@ public ListResult list(
      *
      * @param input search query
      * @param filter search filters
+     * @param sortCriterion sort criterion
      * @param start start offset for search results
      * @param count max number of search results requested
      * @return Snapshot key
@@ -258,11 +259,12 @@ public SearchResult search(
         @Nonnull String entity,
         @Nonnull String input,
         @Nullable Filter filter,
+        @Nullable SortCriterion sortCriterion,
         int start,
         int count,
         @Nonnull final Authentication authentication)
         throws RemoteInvocationException {
-        return _entitySearchService.search(entity, input, filter, null, start, count);
+        return _entitySearchService.search(entity, input, filter, sortCriterion, start, count);
     }
 
     /**

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/graphql/GraphQLEngineFactory.java
Patch:
@@ -116,6 +116,7 @@ protected GraphQLEngine getInstance() {
           _entityRegistry,
           _secretService,
           _configProvider.getIngestion(),
+          _configProvider.getAuthentication(),
           _configProvider.getAuthorization(),
           _gitVersion,
           _graphService.supportsMultiHop(),
@@ -134,6 +135,7 @@ protected GraphQLEngine getInstance() {
         _entityRegistry,
         _secretService,
         _configProvider.getIngestion(),
+        _configProvider.getAuthentication(),
         _configProvider.getAuthorization(),
         _gitVersion,
         _graphService.supportsMultiHop(),

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/policy/UpsertPolicyResolver.java
Patch:
@@ -60,8 +60,6 @@ public CompletableFuture<String> get(final DataFetchingEnvironment environment)
 
       // Create the policy info.
       final DataHubPolicyInfo info = PolicyUpdateInputInfoMapper.map(input);
-      info.setLastUpdatedTimestamp(System.currentTimeMillis());
-
       proposal.setEntityType(POLICY_ENTITY_NAME);
       proposal.setAspectName(POLICY_INFO_ASPECT_NAME);
       proposal.setAspect(GenericRecordUtils.serializeAspect(info));

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/search/SearchResolver.java
Patch:
@@ -44,7 +44,7 @@ public CompletableFuture<SearchResults> get(DataFetchingEnvironment environment)
         log.debug("Executing search. entity type {}, query {}, filters: {}, start: {}, count: {}", input.getType(),
             input.getQuery(), input.getFilters(), start, count);
         return UrnSearchResultsMapper.map(
-            _entityClient.search(entityName, sanitizedQuery, ResolverUtils.buildFilter(input.getFilters()), null, start,
+            _entityClient.search(entityName, sanitizedQuery, ResolverUtils.buildFilter(input.getFilters()), start,
                 count, ResolverUtils.getAuthentication(environment)));
       } catch (Exception e) {
         log.error("Failed to execute search: entity type {}, query {}, filters: {}, start: {}, count: {}",

File: metadata-service/restli-client/src/main/java/com/linkedin/entity/client/EntityClient.java
Patch:
@@ -146,15 +146,14 @@ public ListResult list(@Nonnull String entity, @Nullable Map<String, String> req
    *
    * @param input search query
    * @param filter search filters
-   * @param sortCriterion sort criterion
    * @param start start offset for search results
    * @param count max number of search results requested
    * @return Snapshot key
    * @throws RemoteInvocationException
    */
   @Nonnull
-  public SearchResult search(@Nonnull String entity, @Nonnull String input, @Nullable Filter filter,
-      SortCriterion sortCriterion, int start, int count, @Nonnull Authentication authentication) throws RemoteInvocationException;
+  public SearchResult search(@Nonnull String entity, @Nonnull String input, @Nullable Filter filter, int start,
+      int count, @Nonnull Authentication authentication) throws RemoteInvocationException;
 
   /**
    * Searches for entities matching to a given query and filters across multiple entity types

File: metadata-service/restli-client/src/main/java/com/linkedin/entity/client/JavaEntityClient.java
Patch:
@@ -248,7 +248,6 @@ public ListResult list(
      *
      * @param input search query
      * @param filter search filters
-     * @param sortCriterion sort criterion
      * @param start start offset for search results
      * @param count max number of search results requested
      * @return Snapshot key
@@ -259,12 +258,11 @@ public SearchResult search(
         @Nonnull String entity,
         @Nonnull String input,
         @Nullable Filter filter,
-        @Nullable SortCriterion sortCriterion,
         int start,
         int count,
         @Nonnull final Authentication authentication)
         throws RemoteInvocationException {
-        return _entitySearchService.search(entity, input, filter, sortCriterion, start, count);
+        return _entitySearchService.search(entity, input, filter, null, start, count);
     }
 
     /**

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/policy/UpsertPolicyResolver.java
Patch:
@@ -60,6 +60,8 @@ public CompletableFuture<String> get(final DataFetchingEnvironment environment)
 
       // Create the policy info.
       final DataHubPolicyInfo info = PolicyUpdateInputInfoMapper.map(input);
+      info.setLastUpdatedTimestamp(System.currentTimeMillis());
+
       proposal.setEntityType(POLICY_ENTITY_NAME);
       proposal.setAspectName(POLICY_INFO_ASPECT_NAME);
       proposal.setAspect(GenericRecordUtils.serializeAspect(info));

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/search/SearchResolver.java
Patch:
@@ -44,7 +44,7 @@ public CompletableFuture<SearchResults> get(DataFetchingEnvironment environment)
         log.debug("Executing search. entity type {}, query {}, filters: {}, start: {}, count: {}", input.getType(),
             input.getQuery(), input.getFilters(), start, count);
         return UrnSearchResultsMapper.map(
-            _entityClient.search(entityName, sanitizedQuery, ResolverUtils.buildFilter(input.getFilters()), start,
+            _entityClient.search(entityName, sanitizedQuery, ResolverUtils.buildFilter(input.getFilters()), null, start,
                 count, ResolverUtils.getAuthentication(environment)));
       } catch (Exception e) {
         log.error("Failed to execute search: entity type {}, query {}, filters: {}, start: {}, count: {}",

File: metadata-service/restli-client/src/main/java/com/linkedin/entity/client/EntityClient.java
Patch:
@@ -146,14 +146,15 @@ public ListResult list(@Nonnull String entity, @Nullable Map<String, String> req
    *
    * @param input search query
    * @param filter search filters
+   * @param sortCriterion sort criterion
    * @param start start offset for search results
    * @param count max number of search results requested
    * @return Snapshot key
    * @throws RemoteInvocationException
    */
   @Nonnull
-  public SearchResult search(@Nonnull String entity, @Nonnull String input, @Nullable Filter filter, int start,
-      int count, @Nonnull Authentication authentication) throws RemoteInvocationException;
+  public SearchResult search(@Nonnull String entity, @Nonnull String input, @Nullable Filter filter,
+      SortCriterion sortCriterion, int start, int count, @Nonnull Authentication authentication) throws RemoteInvocationException;
 
   /**
    * Searches for entities matching to a given query and filters across multiple entity types

File: metadata-service/restli-client/src/main/java/com/linkedin/entity/client/JavaEntityClient.java
Patch:
@@ -248,6 +248,7 @@ public ListResult list(
      *
      * @param input search query
      * @param filter search filters
+     * @param sortCriterion sort criterion
      * @param start start offset for search results
      * @param count max number of search results requested
      * @return Snapshot key
@@ -258,11 +259,12 @@ public SearchResult search(
         @Nonnull String entity,
         @Nonnull String input,
         @Nullable Filter filter,
+        @Nullable SortCriterion sortCriterion,
         int start,
         int count,
         @Nonnull final Authentication authentication)
         throws RemoteInvocationException {
-        return _entitySearchService.search(entity, input, filter, null, start, count);
+        return _entitySearchService.search(entity, input, filter, sortCriterion, start, count);
     }
 
     /**

File: metadata-io/src/main/java/com/linkedin/metadata/entity/ebean/EbeanAspectDao.java
Patch:
@@ -498,6 +498,8 @@ public Map<String, Long> getNextVersions(@Nonnull final String urn, @Nonnull fin
     if (exp == null) {
       return result;
     }
+    // Ensure Ordering of Aspect version
+    exp.orderBy("version");
     List<EbeanAspectV2.PrimaryKey> dbResults = exp.endOr().findIds();
 
     for (EbeanAspectV2.PrimaryKey key: dbResults) {

File: metadata-io/src/main/java/com/linkedin/metadata/search/elasticsearch/indexbuilder/SettingsBuilder.java
Patch:
@@ -65,7 +65,7 @@ private static Map<String, Object> buildTokenizers() {
 
     // Tokenize for urns
     tokenizers.put("urn_char_group",
-        ImmutableMap.<String, Object>builder().put("type", "pattern").put("pattern", "[:\\s(), ./]").build());
+        ImmutableMap.<String, Object>builder().put("type", "pattern").put("pattern", "[:\\s(),]").build());
 
     return tokenizers.build();
   }

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/search/SearchUtils.java
Patch:
@@ -20,6 +20,8 @@ private SearchUtils() {
           EntityType.MLMODEL,
           EntityType.MLMODEL_GROUP,
           EntityType.MLFEATURE_TABLE,
+          EntityType.MLFEATURE,
+          EntityType.MLPRIMARY_KEY,
           EntityType.DATA_FLOW,
           EntityType.DATA_JOB,
           EntityType.GLOSSARY_TERM,

File: metadata-io/src/main/java/com/linkedin/metadata/search/SearchService.java
Patch:
@@ -45,7 +45,7 @@ public SearchService(EntityRegistry entityRegistry, EntitySearchService entitySe
   public Map<String, Long> docCountPerEntity(@Nonnull List<String> entityNames) {
     return entityNames.stream()
         .collect(Collectors.toMap(Function.identity(),
-            entityName -> _entityDocCountCache.getEntityDocCount().getOrDefault(entityName, 0L)));
+            entityName -> _entityDocCountCache.getEntityDocCount().getOrDefault(entityName.toLowerCase(), 0L)));
   }
 
   /**

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/dashboard/mappers/DashboardMapper.java
Patch:
@@ -70,6 +70,7 @@ public Dashboard apply(@Nonnull final EntityResponse entityResponse) {
         mappingHelper.mapToResult(DOMAINS_ASPECT_NAME, this::mapDomains);
         mappingHelper.mapToResult(DEPRECATION_ASPECT_NAME, (dashboard, dataMap) ->
             dashboard.setDeprecation(DeprecationMapper.map(new Deprecation(dataMap))));
+        mappingHelper.mapToResult(GLOBAL_TAGS_ASPECT_NAME, this::mapGlobalTags);
 
         return mappingHelper.getResult();
     }

File: metadata-integration/java/datahub-protobuf/src/main/java/datahub/protobuf/model/ProtobufGraph.java
Patch:
@@ -306,7 +306,7 @@ private void flattenGoogleWrapped() {
                 .collect(Collectors.toSet());
         removeVertices.addAll(wrappedPrimitiveFields);
 
-        wrappedPrimitiveFields.forEach(primitiveField -> {
+        wrappedPrimitiveFields.stream().filter(fld -> fld.getNumber() == 1).forEach(primitiveField -> {
             // remove incoming old edges to primitive
             removeEdges.addAll(incomingEdgesOf(primitiveField));
 

File: metadata-io/src/main/java/com/linkedin/metadata/search/elasticsearch/query/request/SearchRequestHandler.java
Patch:
@@ -270,6 +270,7 @@ private Map<String, Double> extractFeatures(@Nonnull SearchHit searchHit) {
   private SearchEntity getResult(@Nonnull SearchHit hit) {
     return new SearchEntity().setEntity(getUrnFromSearchHit(hit))
         .setMatchedFields(new MatchedFieldArray(extractMatchedFields(hit.getHighlightFields())))
+        .setScore(hit.getScore())
         .setFeatures(new DoubleMap(extractFeatures(hit)));
   }
 

File: entity-registry/src/main/java/com/linkedin/metadata/models/annotation/SearchableAnnotation.java
Patch:
@@ -52,7 +52,8 @@ public enum FieldType {
     URN,
     URN_PARTIAL,
     BOOLEAN,
-    COUNT
+    COUNT,
+    DATETIME
   }
 
   @Nonnull

File: metadata-io/src/main/java/com/linkedin/metadata/search/elasticsearch/indexbuilder/MappingsBuilder.java
Patch:
@@ -77,6 +77,8 @@ private static Map<String, Object> getMappingsForField(@Nonnull final Searchable
       mappingForField.put("type", "boolean");
     } else if (fieldType == FieldType.COUNT) {
       mappingForField.put("type", "long");
+    } else if (fieldType == FieldType.DATETIME) {
+      mappingForField.put("type", "date");
     } else {
       log.info("FieldType {} has no mappings implemented", fieldType);
     }

File: metadata-integration/java/datahub-protobuf/src/main/java/datahub/protobuf/model/ProtobufGraph.java
Patch:
@@ -286,6 +286,7 @@ private void attachNestedMessageFields(Map<String, List<ProtobufField>> fieldMap
         });
     }
 
+    private static final Set<String> GOOGLE_WRAPPERS = Set.of("google/protobuf/wrappers.proto", "google/protobuf/timestamp.proto");
     private void flattenGoogleWrapped() {
         HashSet<ProtobufElement> removeVertices = new HashSet<>();
         HashSet<FieldTypeEdge> removeEdges = new HashSet<>();
@@ -294,7 +295,7 @@ private void flattenGoogleWrapped() {
 
         Set<ProtobufElement> googleWrapped = vertexSet().stream()
                 .filter(v -> v instanceof ProtobufMessage
-                        && "google/protobuf/wrappers.proto".equals(v.fileProto().getName()))
+                        && GOOGLE_WRAPPERS.contains(v.fileProto().getName()))
                 .collect(Collectors.toSet());
         removeVertices.addAll(googleWrapped);
 

File: metadata-integration/java/datahub-protobuf/src/test/java/datahub/protobuf/model/ProtobufGraphTest.java
Patch:
@@ -20,10 +20,10 @@ public void autodetectRootMessageTest() throws IOException {
         ProtobufGraph test = getTestProtobufGraph("protobuf", "messageB");
 
         assertEquals("MessageB", test.autodetectRootMessage(
-                fileset.getFile(2)).get().messageProto().getName());
+                fileset.getFileList().stream().filter(f -> f.getName().equals("protobuf/messageB.proto")).findFirst().get()).get().messageProto().getName());
 
         assertEquals("MessageA", test.autodetectRootMessage(
-                fileset.getFile(1)).get().messageProto().getName());
+                fileset.getFileList().stream().filter(f -> f.getName().equals("protobuf/messageA.proto")).findFirst().get()).get().messageProto().getName());
     }
 
     @Test

File: metadata-io/src/main/java/com/linkedin/metadata/entity/ebean/EbeanAspectDao.java
Patch:
@@ -349,14 +349,17 @@ public ListResult<Long> listVersions(
 
   @Nonnull
   public ListResult<String> listUrns(
+      @Nonnull final String entityName,
       @Nonnull final String aspectName,
       final int start,
       final int pageSize) {
     validateConnection();
 
+    final String urnPrefixMatcher = "urn:li:" + entityName + ":%";
     final PagedList<EbeanAspectV2> pagedList = _server.find(EbeanAspectV2.class)
         .select(EbeanAspectV2.KEY_ID)
         .where()
+        .like(EbeanAspectV2.URN_COLUMN, urnPrefixMatcher)
         .eq(EbeanAspectV2.ASPECT_COLUMN, aspectName)
         .eq(EbeanAspectV2.VERSION_COLUMN, ASPECT_LATEST_VERSION)
         .setFirstRow(start)

File: metadata-io/src/main/java/com/linkedin/metadata/entity/ebean/EbeanEntityService.java
Patch:
@@ -684,7 +684,7 @@ public ListUrnsResult listUrns(@Nonnull final String entityName, final int start
 
     // If a keyAspect exists, the entity exists.
     final String keyAspectName = getEntityRegistry().getEntitySpec(entityName).getKeyAspectSpec().getName();
-    final ListResult<String> keyAspectList = _entityDao.listUrns(keyAspectName, start, count);
+    final ListResult<String> keyAspectList = _entityDao.listUrns(entityName, keyAspectName, start, count);
 
     final ListUrnsResult result = new ListUrnsResult();
     result.setStart(start);

File: metadata-utils/src/main/java/com/linkedin/metadata/Constants.java
Patch:
@@ -80,6 +80,7 @@ public class Constants {
   public static final String SCHEMA_METADATA_ASPECT_NAME = "schemaMetadata";
   public static final String EDITABLE_SCHEMA_METADATA_ASPECT_NAME = "editableSchemaMetadata";
   public static final String VIEW_PROPERTIES_ASPECT_NAME = "viewProperties";
+  public static final String DATASET_PROFILE_ASPECT_NAME = "datasetProfile";
 
   // Chart
   public static final String CHART_KEY_ASPECT_NAME = "chartKey";

File: metadata-io/src/main/java/com/linkedin/metadata/search/utils/ESUtils.java
Patch:
@@ -91,7 +91,7 @@ public static BoolQueryBuilder buildConjunctiveFilterQuery(@Nonnull ConjunctiveC
    * <p>This approach of supporting multiple values using comma as delimiter, prevents us from specifying a value that has comma
    * as one of it's characters. This is particularly true when one of the values is an urn e.g. "urn:li:example:(1,2,3)".
    * Hence we do not split the value (using comma as delimiter) if the value starts with "urn:li:".
-   * TODO(https://github.com/linkedin/datahub-gma/issues/51): support multiple values a field can take without using
+   * TODO(https://github.com/datahub-project/datahub-gma/issues/51): support multiple values a field can take without using
    * delimiters like comma.
    *
    * <p>If the condition between a field and value is not the same as EQUAL, a Range query is constructed. This
@@ -111,7 +111,7 @@ public static QueryBuilder getQueryBuilderFromCriterion(@Nonnull Criterion crite
       if (!criterion.getValues().isEmpty()) {
         return QueryBuilders.termsQuery(criterion.getField(), criterion.getValues());
       }
-      // TODO(https://github.com/linkedin/datahub-gma/issues/51): support multiple values a field can take without using
+      // TODO(https://github.com/datahub-project/datahub-gma/issues/51): support multiple values a field can take without using
       // delimiters like comma. This is a hack to support equals with URN that has a comma in it.
       if (isUrn(criterion.getValue())) {
         return QueryBuilders.matchQuery(criterion.getField(), criterion.getValue().trim());

File: metadata-io/src/main/java/com/linkedin/metadata/search/utils/SearchUtils.java
Patch:
@@ -70,7 +70,7 @@ public static Map<String, String> getRequestMap(@Nullable Filter requestParams)
   }
 
   static boolean isUrn(@Nonnull String value) {
-    // TODO(https://github.com/linkedin/datahub-gma/issues/51): This method is a bit of a hack to support searching for
+    // TODO(https://github.com/datahub-project/datahub-gma/issues/51): This method is a bit of a hack to support searching for
     // URNs that have commas in them, while also using commas a delimiter for search. We should stop supporting commas
     // as delimiter, and then we can stop using this hack.
     return value.startsWith("urn:li:");

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/WeaklyTypedAspectsResolver.java
Patch:
@@ -69,9 +69,7 @@ public CompletableFuture<List<RawAspect>> get(DataFetchingEnvironment environmen
                         return;
                     }
 
-                    DataMap aspectPayload = resolvedAspect.getDataMap(resolvedAspect.keySet().iterator().next());
-
-                    result.setPayload(CODEC.mapToString(aspectPayload));
+                    result.setPayload(CODEC.mapToString(resolvedAspect));
                     result.setAspectName(aspectSpec.getName());
 
                     DataMap renderSpec = aspectSpec.getRenderSpec();

File: metadata-io/src/main/java/com/linkedin/metadata/entity/ebean/EbeanAspectDao.java
Patch:
@@ -19,6 +19,7 @@
 import io.ebean.RawSql;
 import io.ebean.RawSqlBuilder;
 import io.ebean.Transaction;
+import io.ebean.TxScope;
 import io.ebean.annotation.TxIsolation;
 import java.net.URISyntaxException;
 import java.sql.Timestamp;
@@ -443,7 +444,7 @@ public <T> T runInTransactionWithRetry(@Nonnull final Supplier<T> block, final i
 
     T result = null;
     do {
-      try (Transaction transaction = _server.beginTransaction(TxIsolation.REPEATABLE_READ)) {
+      try (Transaction transaction = _server.beginTransaction(TxScope.requiresNew().setIsolation(TxIsolation.REPEATABLE_READ))) {
         transaction.setBatchMode(true);
         result = block.get();
         transaction.commit();

File: metadata-service/restli-servlet-impl/src/main/java/com/linkedin/metadata/resources/entity/EntityResource.java
Patch:
@@ -368,7 +368,7 @@ public Task<RollbackResponse> deleteEntities(@ActionParam("registryId") @Optiona
     return RestliUtil.toTask(() -> {
       RollbackResponse response = new RollbackResponse();
       List<AspectRowSummary> aspectRowsToDelete =
-          _systemMetadataService.findByRegistry(finalRegistryName, finalRegistryVersion.toString());
+          _systemMetadataService.findByRegistry(finalRegistryName, finalRegistryVersion.toString(), false);
       log.info("found {} rows to delete...", stringifyRowCount(aspectRowsToDelete.size()));
       response.setAspectsAffected(aspectRowsToDelete.size());
       response.setEntitiesAffected(
@@ -380,7 +380,7 @@ public Task<RollbackResponse> deleteEntities(@ActionParam("registryId") @Optiona
         Map<String, String> conditions = new HashMap();
         conditions.put("registryName", finalRegistryName1);
         conditions.put("registryVersion", finalRegistryVersion1.toString());
-        _entityService.rollbackWithConditions(aspectRowsToDelete, conditions);
+        _entityService.rollbackWithConditions(aspectRowsToDelete, conditions, false);
       }
       return response;
     }, MetricRegistry.name(this.getClass(), "deleteAll"));

File: metadata-utils/src/main/java/com/linkedin/metadata/Constants.java
Patch:
@@ -13,6 +13,8 @@ public class Constants {
   public static final Long ASPECT_LATEST_VERSION = 0L;
   public static final String UNKNOWN_DATA_PLATFORM = "urn:li:dataPlatform:unknown";
 
+  public static final String DEFAULT_RUN_ID = "no-run-id-provided";
+
   /**
    * Entities
    */

File: entity-registry/src/main/java/com/linkedin/metadata/models/registry/LineageRegistry.java
Patch:
@@ -1,7 +1,7 @@
-package com.linkedin.metadata.graph;
+package com.linkedin.metadata.models.registry;
 
+import com.linkedin.metadata.graph.LineageDirection;
 import com.linkedin.metadata.models.annotation.RelationshipAnnotation;
-import com.linkedin.metadata.models.registry.EntityRegistry;
 import com.linkedin.metadata.query.filter.RelationshipDirection;
 import java.util.ArrayList;
 import java.util.Collection;
@@ -91,7 +91,7 @@ public List<EdgeInfo> getLineageRelationships(String entityName, LineageDirectio
     if (spec == null) {
       return Collections.emptyList();
     }
-    
+
     if (direction == LineageDirection.UPSTREAM) {
       return spec.getUpstreamEdges();
     }

File: entity-registry/src/test/java/com/linkedin/metadata/models/registry/LineageRegistryTest.java
Patch:
@@ -1,10 +1,9 @@
-package com.linkedin.metadata.graph;
+package com.linkedin.metadata.models.registry;
 
 import com.google.common.collect.ImmutableList;
 import com.linkedin.metadata.models.EntitySpec;
 import com.linkedin.metadata.models.RelationshipFieldSpec;
 import com.linkedin.metadata.models.annotation.RelationshipAnnotation;
-import com.linkedin.metadata.models.registry.EntityRegistry;
 import com.linkedin.metadata.query.filter.RelationshipDirection;
 import java.util.Collections;
 import java.util.HashMap;

File: metadata-io/src/main/java/com/linkedin/metadata/graph/GraphService.java
Patch:
@@ -1,6 +1,7 @@
 package com.linkedin.metadata.graph;
 
 import com.linkedin.common.urn.Urn;
+import com.linkedin.metadata.models.registry.LineageRegistry;
 import com.linkedin.metadata.query.filter.Filter;
 import com.linkedin.metadata.query.filter.RelationshipDirection;
 import com.linkedin.metadata.query.filter.RelationshipFilter;

File: metadata-io/src/main/java/com/linkedin/metadata/graph/dgraph/DgraphGraphService.java
Patch:
@@ -6,7 +6,7 @@
 import com.linkedin.common.urn.Urn;
 import com.linkedin.metadata.graph.Edge;
 import com.linkedin.metadata.graph.GraphService;
-import com.linkedin.metadata.graph.LineageRegistry;
+import com.linkedin.metadata.models.registry.LineageRegistry;
 import com.linkedin.metadata.graph.RelatedEntitiesResult;
 import com.linkedin.metadata.graph.RelatedEntity;
 import com.linkedin.metadata.query.filter.Criterion;

File: metadata-io/src/main/java/com/linkedin/metadata/graph/elastic/ESGraphQueryDAO.java
Patch:
@@ -7,8 +7,8 @@
 import com.linkedin.common.urn.Urn;
 import com.linkedin.common.urn.UrnUtils;
 import com.linkedin.metadata.graph.LineageDirection;
-import com.linkedin.metadata.graph.LineageRegistry;
-import com.linkedin.metadata.graph.LineageRegistry.EdgeInfo;
+import com.linkedin.metadata.models.registry.LineageRegistry;
+import com.linkedin.metadata.models.registry.LineageRegistry.EdgeInfo;
 import com.linkedin.metadata.graph.LineageRelationship;
 import com.linkedin.metadata.query.filter.Condition;
 import com.linkedin.metadata.query.filter.ConjunctiveCriterion;

File: metadata-io/src/main/java/com/linkedin/metadata/graph/elastic/ElasticSearchGraphService.java
Patch:
@@ -8,7 +8,7 @@
 import com.linkedin.metadata.graph.EntityLineageResult;
 import com.linkedin.metadata.graph.GraphService;
 import com.linkedin.metadata.graph.LineageDirection;
-import com.linkedin.metadata.graph.LineageRegistry;
+import com.linkedin.metadata.models.registry.LineageRegistry;
 import com.linkedin.metadata.graph.LineageRelationshipArray;
 import com.linkedin.metadata.graph.RelatedEntitiesResult;
 import com.linkedin.metadata.graph.RelatedEntity;

File: metadata-io/src/main/java/com/linkedin/metadata/graph/neo4j/Neo4jGraphService.java
Patch:
@@ -7,7 +7,7 @@
 import com.linkedin.common.urn.Urn;
 import com.linkedin.metadata.graph.Edge;
 import com.linkedin.metadata.graph.GraphService;
-import com.linkedin.metadata.graph.LineageRegistry;
+import com.linkedin.metadata.models.registry.LineageRegistry;
 import com.linkedin.metadata.graph.RelatedEntitiesResult;
 import com.linkedin.metadata.graph.RelatedEntity;
 import com.linkedin.metadata.query.filter.Condition;

File: metadata-io/src/test/java/com/linkedin/metadata/graph/dgraph/DgraphGraphServiceTest.java
Patch:
@@ -2,7 +2,7 @@
 
 import com.linkedin.metadata.graph.GraphService;
 import com.linkedin.metadata.graph.GraphServiceTestBase;
-import com.linkedin.metadata.graph.LineageRegistry;
+import com.linkedin.metadata.models.registry.LineageRegistry;
 import com.linkedin.metadata.graph.RelatedEntity;
 import com.linkedin.metadata.models.registry.SnapshotEntityRegistry;
 import com.linkedin.metadata.query.filter.RelationshipDirection;

File: metadata-io/src/test/java/com/linkedin/metadata/graph/elastic/ElasticSearchGraphServiceTest.java
Patch:
@@ -7,7 +7,7 @@
 import com.linkedin.metadata.graph.GraphService;
 import com.linkedin.metadata.graph.GraphServiceTestBase;
 import com.linkedin.metadata.graph.LineageDirection;
-import com.linkedin.metadata.graph.LineageRegistry;
+import com.linkedin.metadata.models.registry.LineageRegistry;
 import com.linkedin.metadata.graph.LineageRelationship;
 import com.linkedin.metadata.graph.RelatedEntitiesResult;
 import com.linkedin.metadata.graph.RelatedEntity;

File: metadata-io/src/test/java/com/linkedin/metadata/graph/neo4j/Neo4jGraphServiceTest.java
Patch:
@@ -2,7 +2,7 @@
 
 import com.linkedin.metadata.graph.GraphService;
 import com.linkedin.metadata.graph.GraphServiceTestBase;
-import com.linkedin.metadata.graph.LineageRegistry;
+import com.linkedin.metadata.models.registry.LineageRegistry;
 import com.linkedin.metadata.graph.RelatedEntitiesResult;
 import com.linkedin.metadata.graph.RelatedEntity;
 import com.linkedin.metadata.models.registry.SnapshotEntityRegistry;

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/common/ElasticSearchGraphServiceFactory.java
Patch:
@@ -3,7 +3,7 @@
 import com.linkedin.gms.factory.entityregistry.EntityRegistryFactory;
 import com.linkedin.gms.factory.search.BaseElasticSearchComponentsFactory;
 import com.linkedin.gms.factory.spring.YamlPropertySourceFactory;
-import com.linkedin.metadata.graph.LineageRegistry;
+import com.linkedin.metadata.models.registry.LineageRegistry;
 import com.linkedin.metadata.graph.elastic.ESGraphQueryDAO;
 import com.linkedin.metadata.graph.elastic.ESGraphWriteDAO;
 import com.linkedin.metadata.graph.elastic.ElasticSearchGraphService;

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/common/Neo4jGraphServiceFactory.java
Patch:
@@ -1,7 +1,7 @@
 package com.linkedin.gms.factory.common;
 
 import com.linkedin.gms.factory.entityregistry.EntityRegistryFactory;
-import com.linkedin.metadata.graph.LineageRegistry;
+import com.linkedin.metadata.models.registry.LineageRegistry;
 import com.linkedin.metadata.graph.neo4j.Neo4jGraphService;
 import com.linkedin.metadata.models.registry.EntityRegistry;
 import javax.annotation.Nonnull;

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/tag/mappers/TagUpdateInputMapper.java
Patch:
@@ -47,7 +47,7 @@ public Collection<MetadataChangeProposal> apply(
     final Ownership ownership = new Ownership();
     final Owner owner = new Owner();
     owner.setOwner(actor);
-    owner.setType(OwnershipType.DATAOWNER);
+    owner.setType(OwnershipType.NONE);
     owner.setSource(new OwnershipSource().setType(OwnershipSourceType.SERVICE));
     ownership.setOwners(new OwnerArray(owner));
     ownership.setLastModified(auditStamp);

File: datahub-frontend/app/auth/sso/oidc/OidcConfigs.java
Patch:
@@ -47,7 +47,7 @@ public class OidcConfigs extends SsoConfigs {
     private static final String DEFAULT_OIDC_CLIENT_AUTHENTICATION_METHOD = "client_secret_basic";
     private static final String DEFAULT_OIDC_JIT_PROVISIONING_ENABLED = "true";
     private static final String DEFAULT_OIDC_PRE_PROVISIONING_REQUIRED = "false";
-    private static final String DEFAULT_OIDC_EXTRACT_GROUPS_ENABLED = "true";
+    private static final String DEFAULT_OIDC_EXTRACT_GROUPS_ENABLED = "false"; // False since extraction of groups can overwrite existing group membership.
     private static final String DEFAULT_OIDC_GROUPS_CLAIM = "groups";
 
     private String clientId;

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/recommendation/RecommendationServiceFactory.java
Patch:
@@ -2,7 +2,7 @@
 
 import com.google.common.collect.ImmutableList;
 import com.linkedin.gms.factory.recommendation.candidatesource.DomainsCandidateSourceFactory;
-import com.linkedin.gms.factory.recommendation.candidatesource.HighUsageCandidateSourceFactory;
+import com.linkedin.gms.factory.recommendation.candidatesource.MostPopularCandidateSourceFactory;
 import com.linkedin.gms.factory.recommendation.candidatesource.RecentlyViewedCandidateSourceFactory;
 import com.linkedin.gms.factory.recommendation.candidatesource.TopPlatformsCandidateSourceFactory;
 import com.linkedin.gms.factory.recommendation.candidatesource.TopTagsCandidateSourceFactory;
@@ -27,7 +27,7 @@
 
 @Configuration
 @Import({TopPlatformsCandidateSourceFactory.class, RecentlyViewedCandidateSourceFactory.class,
-    HighUsageCandidateSourceFactory.class, TopTagsCandidateSourceFactory.class, TopTermsCandidateSourceFactory.class, DomainsCandidateSourceFactory.class})
+    MostPopularCandidateSourceFactory.class, TopTagsCandidateSourceFactory.class, TopTermsCandidateSourceFactory.class, DomainsCandidateSourceFactory.class})
 public class RecommendationServiceFactory {
 
   @Autowired
@@ -39,7 +39,7 @@ public class RecommendationServiceFactory {
   private RecentlyViewedSource recentlyViewedCandidateSource;
 
   @Autowired
-  @Qualifier("highUsageCandidateSource")
+  @Qualifier("mostPopularCandidateSource")
   private MostPopularSource _mostPopularCandidateSource;
 
   @Autowired

File: metadata-utils/src/main/java/com/linkedin/metadata/Constants.java
Patch:
@@ -11,6 +11,7 @@ public class Constants {
   public static final String SYSTEM_ACTOR = "urn:li:corpuser:__datahub_system"; // DataHub internal service principal.
   public static final String UNKNOWN_ACTOR = "urn:li:corpuser:UNKNOWN"; // Unknown principal.
   public static final Long ASPECT_LATEST_VERSION = 0L;
+  public static final String UNKNOWN_DATA_PLATFORM = "urn:li:dataPlatform:unknown";
 
   /**
    * Entities

File: datahub-frontend/app/auth/sso/oidc/custom/CustomOidcClient.java
Patch:
@@ -20,7 +20,6 @@ public CustomOidcClient(final OidcConfiguration configuration) {
   protected void clientInit() {
     CommonHelper.assertNotNull("configuration", getConfiguration());
     getConfiguration().init();
-
     defaultRedirectActionBuilder(new OidcRedirectActionBuilder(getConfiguration(), this));
     defaultCredentialsExtractor(new OidcExtractor(getConfiguration(), this));
     defaultAuthenticator(new CustomOidcAuthenticator(getConfiguration(), this));

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/EntityTypeMapper.java
Patch:
@@ -13,7 +13,8 @@
 public class EntityTypeMapper {
 
   static final Map<EntityType, String> ENTITY_TYPE_TO_NAME =
-      ImmutableMap.<EntityType, String>builder().put(EntityType.DATASET, "dataset")
+      ImmutableMap.<EntityType, String>builder()
+          .put(EntityType.DATASET, "dataset")
           .put(EntityType.CORP_USER, "corpuser")
           .put(EntityType.CORP_GROUP, "corpGroup")
           .put(EntityType.DATA_PLATFORM, "dataPlatform")

File: entity-registry/src/main/java/com/linkedin/metadata/models/extractor/FieldExtractor.java
Patch:
@@ -1,4 +1,4 @@
-package com.linkedin.metadata.extractor;
+package com.linkedin.metadata.models.extractor;
 
 import com.linkedin.data.schema.PathSpec;
 import com.linkedin.data.template.RecordTemplate;

File: metadata-io/src/main/java/com/linkedin/metadata/search/transformer/SearchDocumentTransformer.java
Patch:
@@ -7,7 +7,7 @@
 import com.linkedin.common.urn.Urn;
 import com.linkedin.data.schema.DataSchema;
 import com.linkedin.data.template.RecordTemplate;
-import com.linkedin.metadata.extractor.FieldExtractor;
+import com.linkedin.metadata.models.extractor.FieldExtractor;
 import com.linkedin.metadata.models.AspectSpec;
 import com.linkedin.metadata.models.EntitySpec;
 import com.linkedin.metadata.models.SearchableFieldSpec;

File: metadata-io/src/main/java/com/linkedin/metadata/timeseries/transformer/TimeseriesAspectTransformer.java
Patch:
@@ -12,7 +12,7 @@
 import com.linkedin.data.schema.ArrayDataSchema;
 import com.linkedin.data.schema.DataSchema;
 import com.linkedin.data.template.RecordTemplate;
-import com.linkedin.metadata.extractor.FieldExtractor;
+import com.linkedin.metadata.models.extractor.FieldExtractor;
 import com.linkedin.metadata.models.AspectSpec;
 import com.linkedin.metadata.models.TimeseriesFieldCollectionSpec;
 import com.linkedin.metadata.models.TimeseriesFieldSpec;

File: metadata-io/src/test/java/com/linkedin/metadata/extractor/FieldExtractorTest.java
Patch:
@@ -7,6 +7,7 @@
 import com.linkedin.metadata.models.AspectSpec;
 import com.linkedin.metadata.models.EntitySpec;
 import com.linkedin.metadata.models.SearchableFieldSpec;
+import com.linkedin.metadata.models.extractor.FieldExtractor;
 import java.util.List;
 import java.util.Map;
 import java.util.function.Function;

File: metadata-jobs/mae-consumer/src/main/java/com/linkedin/metadata/kafka/hook/UpdateIndicesHook.java
Patch:
@@ -11,7 +11,7 @@
 import com.linkedin.gms.factory.search.EntitySearchServiceFactory;
 import com.linkedin.gms.factory.search.SearchDocumentTransformerFactory;
 import com.linkedin.gms.factory.timeseries.TimeseriesAspectServiceFactory;
-import com.linkedin.metadata.extractor.FieldExtractor;
+import com.linkedin.metadata.models.extractor.FieldExtractor;
 import com.linkedin.metadata.graph.Edge;
 import com.linkedin.metadata.graph.GraphService;
 import com.linkedin.metadata.models.AspectSpec;

File: metadata-io/src/main/java/com/linkedin/metadata/search/elasticsearch/indexbuilder/SettingsBuilder.java
Patch:
@@ -65,7 +65,7 @@ private static Map<String, Object> buildTokenizers() {
 
     // Tokenize for urns
     tokenizers.put("urn_char_group",
-        ImmutableMap.<String, Object>builder().put("type", "pattern").put("pattern", "[:\\s(),]").build());
+        ImmutableMap.<String, Object>builder().put("type", "pattern").put("pattern", "[:\\s(), ./]").build());
 
     return tokenizers.build();
   }
@@ -112,12 +112,12 @@ private static Map<String, Object> buildAnalyzers(String mainTokenizer) {
 
     // Analyzer for getting urn components
     analyzers.put("urn_component", ImmutableMap.<String, Object>builder().put("tokenizer", "urn_char_group")
-        .put("filter", ImmutableList.of("lowercase", "urn_stop_filter"))
+        .put("filter", ImmutableList.of("lowercase", "urn_stop_filter", "custom_delimiter"))
         .build());
 
     // Analyzer for partial matching urn components
     analyzers.put("partial_urn_component", ImmutableMap.<String, Object>builder().put("tokenizer", "urn_char_group")
-        .put("filter", ImmutableList.of("lowercase", "urn_stop_filter", "partial_filter"))
+        .put("filter", ImmutableList.of("lowercase", "urn_stop_filter", "custom_delimiter", "partial_filter"))
         .build());
 
     return analyzers.build();

File: metadata-io/src/main/java/com/linkedin/metadata/timeline/differ/GlobalTagsDiffer.java
Patch:
@@ -118,7 +118,7 @@ public ChangeTransaction getSemanticDiff(EbeanAspectV2 previousValue, EbeanAspec
         .equals(GLOBAL_TAGS_ASPECT_NAME)) {
       throw new IllegalArgumentException("Aspect is not " + GLOBAL_TAGS_ASPECT_NAME);
     }
-    assert (currentValue != null);
+
     GlobalTags baseGlobalTags = getGlobalTagsFromAspect(previousValue);
     GlobalTags targetGlobalTags = getGlobalTagsFromAspect(currentValue);
     List<ChangeEvent> changeEvents = new ArrayList<>();

File: metadata-jobs/mae-consumer/src/main/java/com/linkedin/metadata/kafka/hydrator/DatasetHydrator.java
Patch:
@@ -21,7 +21,7 @@ public class DatasetHydrator extends BaseHydrator {
   protected void hydrateFromEntityResponse(ObjectNode document, EntityResponse entityResponse) {
     EnvelopedAspectMap aspectMap = entityResponse.getAspects();
     MappingHelper<ObjectNode> mappingHelper = new MappingHelper<>(aspectMap, document);
-    mappingHelper.mapToResult(CORP_USER_KEY_ASPECT_NAME, this::mapKey);
+    mappingHelper.mapToResult(DATASET_KEY_ASPECT_NAME, this::mapKey);
   }
 
   private void mapKey(ObjectNode jsonNodes, DataMap dataMap) {

File: metadata-io/src/main/java/com/linkedin/metadata/search/aggregator/AllEntitiesSearchAggregator.java
Patch:
@@ -52,10 +52,11 @@ public class AllEntitiesSearchAggregator {
       "entity",
       "typeNames",
       "platform",
-      "domain",
+      "domains",
       "tags",
       "glossaryTerms",
       "container",
+      "owners",
       "origin");
 
   public AllEntitiesSearchAggregator(EntityRegistry entityRegistry, EntitySearchService entitySearchService,

File: metadata-io/src/main/java/com/linkedin/metadata/entity/ebean/EbeanEntityService.java
Patch:
@@ -223,7 +223,7 @@ public Map<Urn, List<EnvelopedAspect>> getLatestEnvelopedAspects(
       List<EnvelopedAspect> aspects = urnToAspects.getOrDefault(urn.toString(), Collections.emptyList());
       EnvelopedAspect keyAspect = getKeyEnvelopedAspect(urn);
       // Add key aspect if it does not exist in the returned aspects
-      if (aspects.isEmpty() || !aspectNames.contains(keyAspect.getName())) {
+      if (aspects.isEmpty() || aspects.stream().noneMatch(aspect -> keyAspect.getName().equals(aspect.getName()))) {
         result.put(urn, ImmutableList.<EnvelopedAspect>builder().addAll(aspects).add(keyAspect).build());
       } else {
         result.put(urn, aspects);

File: metadata-service/restli-servlet-impl/src/main/java/com/linkedin/metadata/filter/RestliLoggingFilter.java
Patch:
@@ -35,7 +35,7 @@ public CompletableFuture<Void> onError(
       final FilterRequestContext requestContext,
       final FilterResponseContext responseContext) {
     logResponse(requestContext, responseContext);
-    log.error(th.getMessage());
+    log.error("Rest.li error: ", th);
     return CompletableFuture.completedFuture(null);
   }
 
@@ -52,8 +52,7 @@ private void logResponse(
     String method = requestContext.getMethod().getName();
     String uri = requestContext.getRequestURI().toString();
 
-    String logStr = String.format("%s %s - %s - %s - %sms", httpMethod, uri, method, status.getCode(), duration);
-    log.info(logStr);
+    log.info("{} {} - {} - {} - {}ms", httpMethod, uri, method, status.getCode(), duration);
   }
 
 }

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/mutate/util/LabelUtils.java
Patch:
@@ -66,7 +66,7 @@ public static void removeTermFromTarget(
       }
 
       removeTermIfExists(editableFieldInfo.getGlossaryTerms(), labelUrn);
-      persistAspect(targetUrn, GLOSSARY_TERM_ASPECT_NAME, editableSchemaMetadata, actor, entityService);
+      persistAspect(targetUrn, EDITABLE_SCHEMA_METADATA, editableSchemaMetadata, actor, entityService);
     }
   }
 

File: ingestion-scheduler/src/test/java/com/datahub/metadata/ingestion/IngestionSchedulerTest.java
Patch:
@@ -106,6 +106,7 @@ public void setupTest() throws Exception {
         Mockito.mock(IngestionConfiguration.class),
         1,
         1200);
+    _ingestionScheduler.init();
     Thread.sleep(2000); // Sleep so the runnable can execute. (not ideal)
   }
 

File: metadata-jobs/mae-consumer/src/main/java/com/linkedin/metadata/kafka/MetadataChangeLogProcessor.java
Patch:
@@ -4,6 +4,7 @@
 import com.codahale.metrics.MetricRegistry;
 import com.codahale.metrics.Timer;
 import com.google.common.collect.ImmutableList;
+import com.linkedin.gms.factory.kafka.KafkaEventConsumerFactory;
 import com.linkedin.metadata.EventUtils;
 import com.linkedin.metadata.kafka.config.MetadataChangeLogProcessorCondition;
 import com.linkedin.metadata.kafka.hook.MetadataChangeLogHook;
@@ -28,7 +29,7 @@
 @Slf4j
 @Component
 @Conditional(MetadataChangeLogProcessorCondition.class)
-@Import({UpdateIndicesHook.class, IngestionSchedulerHook.class})
+@Import({UpdateIndicesHook.class, IngestionSchedulerHook.class, KafkaEventConsumerFactory.class})
 @EnableKafka
 public class MetadataChangeLogProcessor {
 
@@ -39,6 +40,7 @@ public class MetadataChangeLogProcessor {
   public MetadataChangeLogProcessor(@Nonnull final UpdateIndicesHook updateIndicesHook,
       @Nonnull final IngestionSchedulerHook ingestionSchedulerHook) {
     this.hooks = ImmutableList.of(updateIndicesHook, ingestionSchedulerHook);
+    this.hooks.forEach(MetadataChangeLogHook::init);
   }
 
   @KafkaListener(id = "${METADATA_CHANGE_LOG_KAFKA_CONSUMER_GROUP_ID:generic-mae-consumer-job-client}", topics = {

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/group/CreateGroupResolver.java
Patch:
@@ -15,6 +15,7 @@
 import com.linkedin.mxe.MetadataChangeProposal;
 import graphql.schema.DataFetcher;
 import graphql.schema.DataFetchingEnvironment;
+import java.util.UUID;
 import java.util.concurrent.CompletableFuture;
 
 import static com.linkedin.datahub.graphql.resolvers.ResolverUtils.*;
@@ -42,7 +43,8 @@ public CompletableFuture<String> get(final DataFetchingEnvironment environment)
           // First, check if the group already exists.
           // Create the Group key.
           final CorpGroupKey key = new CorpGroupKey();
-          key.setName(input.getName());
+          final String id = input.getId() != null ? input.getId() : UUID.randomUUID().toString();
+          key.setName(id); // 'name' in the key really reflects nothing more than a stable "id".
 
           // Create the Group info.
           final CorpGroupInfo info = new CorpGroupInfo();

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/mlmodel/mappers/MLFeaturePropertiesMapper.java
Patch:
@@ -21,7 +21,9 @@ public MLFeatureProperties apply(@NonNull final com.linkedin.ml.metadata.MLFeatu
         final MLFeatureProperties result = new MLFeatureProperties();
 
         result.setDescription(mlFeatureProperties.getDescription());
-        result.setDataType(MLFeatureDataType.valueOf(mlFeatureProperties.getDataType().toString()));
+        if (mlFeatureProperties.getDataType() != null) {
+            result.setDataType(MLFeatureDataType.valueOf(mlFeatureProperties.getDataType().toString()));
+        }
         if (mlFeatureProperties.getVersion() != null) {
             result.setVersion(VersionTagMapper.map(mlFeatureProperties.getVersion()));
         }

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/mlmodel/mappers/MLFeatureSnapshotMapper.java
Patch:
@@ -49,7 +49,9 @@ public MLFeature apply(@Nonnull final MLFeatureSnapshot mlFeature) {
                 MLFeatureProperties featureProperties = MLFeatureProperties.class.cast(aspect);
                 result.setFeatureProperties(MLFeaturePropertiesMapper.map(featureProperties));
                 result.setDescription(featureProperties.getDescription());
-                result.setDataType(MLFeatureDataType.valueOf(featureProperties.getDataType().toString()));
+                if (featureProperties.getDataType() != null) {
+                    result.setDataType(MLFeatureDataType.valueOf(featureProperties.getDataType().toString()));
+                }
             } else if (aspect instanceof InstitutionalMemory) {
                 InstitutionalMemory institutionalMemory = InstitutionalMemory.class.cast(aspect);
                 result.setInstitutionalMemory(InstitutionalMemoryMapper.map(institutionalMemory));

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/mlmodel/mappers/MLPrimaryKeyPropertiesMapper.java
Patch:
@@ -21,7 +21,9 @@ public MLPrimaryKeyProperties apply(@NonNull final com.linkedin.ml.metadata.MLPr
         final MLPrimaryKeyProperties result = new MLPrimaryKeyProperties();
 
         result.setDescription(mlPrimaryKeyProperties.getDescription());
-        result.setDataType(MLFeatureDataType.valueOf(mlPrimaryKeyProperties.getDataType().toString()));
+        if (mlPrimaryKeyProperties.getDataType() != null) {
+            result.setDataType(MLFeatureDataType.valueOf(mlPrimaryKeyProperties.getDataType().toString()));
+        }
         if (mlPrimaryKeyProperties.getVersion() != null) {
             result.setVersion(VersionTagMapper.map(mlPrimaryKeyProperties.getVersion()));
         }

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/mlmodel/mappers/MLPrimaryKeySnapshotMapper.java
Patch:
@@ -49,7 +49,9 @@ public MLPrimaryKey apply(@Nonnull final MLPrimaryKeySnapshot mlPrimaryKeySnapsh
                 MLPrimaryKeyProperties primaryKeyProperties = MLPrimaryKeyProperties.class.cast(aspect);
                 result.setPrimaryKeyProperties(MLPrimaryKeyPropertiesMapper.map(primaryKeyProperties));
                 result.setDescription(primaryKeyProperties.getDescription());
-                result.setDataType(MLFeatureDataType.valueOf(primaryKeyProperties.getDataType().toString()));
+                if (primaryKeyProperties.getDataType() != null) {
+                    result.setDataType(MLFeatureDataType.valueOf(primaryKeyProperties.getDataType().toString()));
+                }
             } else if (aspect instanceof InstitutionalMemory) {
                 InstitutionalMemory institutionalMemory = InstitutionalMemory.class.cast(aspect);
                 result.setInstitutionalMemory(InstitutionalMemoryMapper.map(institutionalMemory));

File: metadata-io/src/main/java/com/linkedin/metadata/search/elasticsearch/update/BulkListener.java
Patch:
@@ -22,7 +22,7 @@ public void beforeBulk(long executionId, BulkRequest request) {
   @Override
   public void afterBulk(long executionId, BulkRequest request, BulkResponse response) {
     if (response.hasFailures()) {
-      log.info("Failed to feed bulk request. Number of events: " + response.getItems().length + " Took time ms: "
+      log.error("Failed to feed bulk request. Number of events: " + response.getItems().length + " Took time ms: "
               + response.getIngestTookInMillis() + " Message: " + response.buildFailureMessage());
     } else {
       log.info("Successfully fed bulk request. Number of events: " + response.getItems().length + " Took time ms: "
@@ -32,6 +32,6 @@ public void afterBulk(long executionId, BulkRequest request, BulkResponse respon
 
   @Override
   public void afterBulk(long executionId, BulkRequest request, Throwable failure) {
-    log.info("Error feeding bulk request. No retries left", failure);
+    log.error("Error feeding bulk request. No retries left", failure);
   }
 }

File: metadata-jobs/mae-consumer/src/main/java/com/linkedin/metadata/kafka/elasticsearch/ElasticsearchConnector.java
Patch:
@@ -44,7 +44,7 @@ public void afterBulk(long executionId, BulkRequest request, BulkResponse respon
 
       @Override
       public void afterBulk(long executionId, BulkRequest request, Throwable failure) {
-        log.info("Error feeding bulk request. No retries left", failure);
+        log.error("Error feeding bulk request. No retries left", failure);
       }
     };
 

File: metadata-jobs/mce-consumer/src/main/java/com/linkedin/metadata/kafka/MetadataChangeEventsProcessor.java
Patch:
@@ -58,7 +58,7 @@ public class MetadataChangeEventsProcessor {
   public void consume(final ConsumerRecord<String, GenericRecord> consumerRecord) {
     kafkaLagStats.update(System.currentTimeMillis() - consumerRecord.timestamp());
     final GenericRecord record = consumerRecord.value();
-    log.debug("Record ", record);
+    log.debug("Record {}", record);
 
     MetadataChangeEvent event = new MetadataChangeEvent();
 

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/ResolverUtils.java
Patch:
@@ -14,6 +14,7 @@
 import com.linkedin.metadata.query.filter.Filter;
 import com.linkedin.metadata.query.filter.ConjunctiveCriterion;
 import com.linkedin.metadata.query.filter.ConjunctiveCriterionArray;
+import com.linkedin.metadata.search.utils.ESUtils;
 import graphql.schema.DataFetchingEnvironment;
 import java.lang.reflect.InvocationTargetException;
 import java.util.stream.Collectors;
@@ -85,7 +86,7 @@ public static Filter buildFilter(@Nullable List<FacetFilterInput> facetFilterInp
             return null;
         }
         return new Filter().setOr(new ConjunctiveCriterionArray(new ConjunctiveCriterion().setAnd(new CriterionArray(facetFilterInputs.stream()
-            .map(filter -> new Criterion().setField(filter.getField()).setValue(filter.getValue()))
+            .map(filter -> new Criterion().setField(filter.getField() + ESUtils.KEYWORD_SUFFIX).setValue(filter.getValue()))
             .collect(Collectors.toList())))));
     }
 

File: metadata-io/src/main/java/com/linkedin/metadata/search/elasticsearch/query/request/SearchQueryBuilder.java
Patch:
@@ -22,8 +22,7 @@ public class SearchQueryBuilder {
   private static final String TEXT_ANALYZER = "word_delimited";
 
   private static final Set<FieldType> TYPES_WITH_DELIMITED_SUBFIELD =
-      new HashSet<>(Arrays.asList(FieldType.TEXT, FieldType.TEXT_PARTIAL, FieldType.URN,
-          FieldType.URN_PARTIAL));
+      new HashSet<>(Arrays.asList(FieldType.TEXT, FieldType.TEXT_PARTIAL));
   private static final Set<FieldType> TYPES_WITH_NGRAM_SUBFIELD =
       new HashSet<>(Arrays.asList(FieldType.TEXT_PARTIAL, FieldType.URN_PARTIAL));
 

File: metadata-io/src/test/java/com/linkedin/metadata/search/elasticsearch/query/request/SearchQueryBuilderTest.java
Patch:
@@ -34,7 +34,7 @@ public void testQueryBuilder() {
     assertEquals(textQuery.queryString(), "testQuery");
     assertEquals(textQuery.analyzer(), "word_delimited");
     Map<String, Float> textFields = textQuery.fields();
-    assertEquals(textFields.size(), 9);
+    assertEquals(textFields.size(), 7);
     assertEquals(textFields.get("keyPart1.delimited").floatValue(), 4.0f);
     assertFalse(textFields.containsKey("keyPart1.ngram"));
     assertEquals(textFields.get("textFieldOverride.delimited").floatValue(), 0.4f);

File: metadata-integration/java/spark-lineage/src/main/java/com/linkedin/datahub/lineage/spark/model/dataset/JdbcDataset.java
Patch:
@@ -32,6 +32,7 @@ private static String dsName(String url, String tbl) {
     url = url.replaceFirst("jdbc:", "");
     if (url.contains("postgres")) {
       url = url.substring(url.lastIndexOf('/') + 1);
+      url = url.substring(0, url.indexOf('?'));
     }
     // TODO different DBs have different formats. TBD mapping to data source names
     return url + "." + tbl;

File: metadata-io/src/main/java/com/linkedin/metadata/search/elasticsearch/query/ESSearchDAO.java
Patch:
@@ -24,7 +24,6 @@
 import org.elasticsearch.client.RequestOptions;
 import org.elasticsearch.client.RestHighLevelClient;
 import org.elasticsearch.client.core.CountRequest;
-import org.elasticsearch.index.query.QueryBuilders;
 
 
 /**
@@ -41,7 +40,7 @@ public class ESSearchDAO {
   public long docCount(@Nonnull String entityName) {
     EntitySpec entitySpec = entityRegistry.getEntitySpec(entityName);
     CountRequest countRequest =
-        new CountRequest(indexConvention.getIndexName(entitySpec)).query(QueryBuilders.matchAllQuery());
+        new CountRequest(indexConvention.getIndexName(entitySpec)).query(SearchRequestHandler.getFilterQuery(null));
     try (Timer.Context ignored = MetricUtils.timer(this.getClass(), "docCount").time()) {
       return client.count(countRequest, RequestOptions.DEFAULT).getCount();
     } catch (IOException e) {

File: metadata-io/src/main/java/com/linkedin/metadata/search/elasticsearch/query/request/SearchRequestHandler.java
Patch:
@@ -98,7 +98,7 @@ private Set<String> getDefaultQueryFieldNames() {
         .collect(Collectors.toSet());
   }
 
-  private static BoolQueryBuilder getFilterQuery(@Nullable Filter filter) {
+  public static BoolQueryBuilder getFilterQuery(@Nullable Filter filter) {
     BoolQueryBuilder filterQuery = ESUtils.buildFilterQuery(filter);
     // Filter out entities that are marked "removed"
     filterQuery.mustNot(QueryBuilders.matchQuery("removed", true));

File: datahub-upgrade/src/main/java/com/linkedin/datahub/upgrade/UpgradeCliApplication.java
Patch:
@@ -7,9 +7,8 @@
 
 
 @SuppressWarnings("checkstyle:HideUtilityClassConstructor")
-@SpringBootApplication(exclude = {RestClientAutoConfiguration.class}, scanBasePackages = {
-    "com.linkedin.gms.factory.common", "com.linkedin.gms.factory.kafka", "com.linkedin.gms.factory.search",
-    "com.linkedin.datahub.upgrade.config", "com.linkedin.gms.factory.entity"})
+@SpringBootApplication(exclude = {RestClientAutoConfiguration.class}, scanBasePackages = {"com.linkedin.gms.factory",
+    "com.linkedin.datahub.upgrade.config"})
 public class UpgradeCliApplication {
   public static void main(String[] args) {
     new SpringApplicationBuilder(UpgradeCliApplication.class, UpgradeCli.class).web(WebApplicationType.NONE).run(args);

File: datahub-upgrade/src/main/java/com/linkedin/datahub/upgrade/common/steps/GMSDisableWriteModeStep.java
Patch:
@@ -5,7 +5,7 @@
 import com.linkedin.datahub.upgrade.UpgradeStep;
 import com.linkedin.datahub.upgrade.UpgradeStepResult;
 import com.linkedin.datahub.upgrade.impl.DefaultUpgradeStepResult;
-import com.linkedin.entity.client.EntityClient;
+import com.linkedin.entity.client.RestliEntityClient;
 import java.util.function.Function;
 import lombok.RequiredArgsConstructor;
 
@@ -14,7 +14,7 @@
 public class GMSDisableWriteModeStep implements UpgradeStep {
 
   private final Authentication _systemAuthentication;
-  private final EntityClient _entityClient;
+  private final RestliEntityClient _entityClient;
 
   @Override
   public String id() {

File: datahub-upgrade/src/main/java/com/linkedin/datahub/upgrade/common/steps/GMSEnableWriteModeStep.java
Patch:
@@ -5,7 +5,7 @@
 import com.linkedin.datahub.upgrade.UpgradeStep;
 import com.linkedin.datahub.upgrade.UpgradeStepResult;
 import com.linkedin.datahub.upgrade.impl.DefaultUpgradeStepResult;
-import com.linkedin.entity.client.EntityClient;
+import com.linkedin.entity.client.RestliEntityClient;
 import java.util.function.Function;
 import lombok.RequiredArgsConstructor;
 
@@ -14,7 +14,7 @@
 public class GMSEnableWriteModeStep implements UpgradeStep {
 
   private final Authentication _systemAuthentication;
-  private final EntityClient _entityClient;
+  private final RestliEntityClient _entityClient;
 
   @Override
   public String id() {

File: datahub-upgrade/src/main/java/com/linkedin/datahub/upgrade/config/NoCodeUpgradeConfig.java
Patch:
@@ -2,7 +2,6 @@
 
 import com.datahub.authentication.Authentication;
 import com.linkedin.datahub.upgrade.nocode.NoCodeUpgrade;
-
 import com.linkedin.entity.client.RestliEntityClient;
 import com.linkedin.metadata.entity.EntityService;
 import com.linkedin.metadata.models.registry.EntityRegistry;
@@ -22,7 +21,7 @@ public class NoCodeUpgradeConfig {
   ApplicationContext applicationContext;
 
   @Bean(name = "noCodeUpgrade")
-  @DependsOn({"ebeanServer", "entityService",  "systemAuthentication", "javaEntityClient", "entityRegistry"})
+  @DependsOn({"ebeanServer", "entityService", "systemAuthentication", "restliEntityClient", "entityRegistry"})
   @Nonnull
   public NoCodeUpgrade createInstance() {
     final EbeanServer ebeanServer = applicationContext.getBean(EbeanServer.class);

File: datahub-upgrade/src/main/java/com/linkedin/datahub/upgrade/restoreindices/RestoreIndices.java
Patch:
@@ -6,7 +6,6 @@
 import com.linkedin.datahub.upgrade.UpgradeStep;
 import com.linkedin.datahub.upgrade.common.steps.ClearGraphServiceStep;
 import com.linkedin.datahub.upgrade.common.steps.ClearSearchServiceStep;
-import com.linkedin.datahub.upgrade.common.steps.GMSQualificationStep;
 import com.linkedin.metadata.entity.EntityService;
 import com.linkedin.metadata.graph.GraphService;
 import com.linkedin.metadata.models.registry.EntityRegistry;
@@ -42,7 +41,6 @@ private List<UpgradeStep> buildSteps(final EbeanServer server, final EntityServi
       final EntityRegistry entityRegistry, final EntitySearchService entitySearchService,
       final GraphService graphService) {
     final List<UpgradeStep> steps = new ArrayList<>();
-    steps.add(new GMSQualificationStep());
     steps.add(new ClearSearchServiceStep(entitySearchService, false));
     steps.add(new ClearGraphServiceStep(graphService, false));
     steps.add(new SendMAEStep(server, entityService, entityRegistry));

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/tag/TagType.java
Patch:
@@ -154,7 +154,7 @@ private boolean isAuthorized(@Nonnull TagUpdateInput update, @Nonnull QueryConte
         return AuthorizationUtils.isAuthorized(
             context.getAuthorizer(),
             context.getAuthentication().getActor().toUrnStr(),
-            PoliciesConfig.DATASET_PRIVILEGES.getResourceType(),
+            PoliciesConfig.TAG_PRIVILEGES.getResourceType(),
             update.getUrn(),
             orPrivilegeGroups);
     }

File: metadata-jobs/mae-consumer/src/main/java/com/linkedin/metadata/kafka/MetadataAuditEventsProcessor.java
Patch:
@@ -7,6 +7,7 @@
 import com.linkedin.data.template.RecordTemplate;
 import com.linkedin.gms.factory.common.GraphServiceFactory;
 import com.linkedin.gms.factory.common.SystemMetadataServiceFactory;
+import com.linkedin.gms.factory.kafka.KafkaEventConsumerFactory;
 import com.linkedin.gms.factory.search.EntitySearchServiceFactory;
 import com.linkedin.gms.factory.search.SearchDocumentTransformerFactory;
 import com.linkedin.metadata.EventUtils;
@@ -59,7 +60,7 @@
 @Component
 @Conditional(MetadataChangeLogProcessorCondition.class)
 @Import({GraphServiceFactory.class, EntitySearchServiceFactory.class, SystemMetadataServiceFactory.class,
-    SearchDocumentTransformerFactory.class})
+    SearchDocumentTransformerFactory.class, KafkaEventConsumerFactory.class})
 @EnableKafka
 public class MetadataAuditEventsProcessor {
 
@@ -84,7 +85,7 @@ public MetadataAuditEventsProcessor(GraphService graphService, EntitySearchServi
   }
 
   @KafkaListener(id = "${METADATA_AUDIT_EVENT_KAFKA_CONSUMER_GROUP_ID:mae-consumer-job-client}", topics =
-      "${KAFKA_TOPIC_NAME:" + Topics.METADATA_AUDIT_EVENT + "}", containerFactory = "avroSerializedKafkaListener")
+      "${KAFKA_TOPIC_NAME:" + Topics.METADATA_AUDIT_EVENT + "}", containerFactory = "kafkaEventConsumer")
   public void consume(final ConsumerRecord<String, GenericRecord> consumerRecord) {
     kafkaLagStats.update(System.currentTimeMillis() - consumerRecord.timestamp());
 

File: metadata-jobs/mae-consumer/src/main/java/com/linkedin/metadata/kafka/config/EntityHydratorConfig.java
Patch:
@@ -2,6 +2,7 @@
 
 import com.datahub.authentication.Authentication;
 import com.linkedin.entity.client.RestliEntityClient;
+import com.linkedin.gms.factory.auth.SystemAuthenticationFactory;
 import com.linkedin.gms.factory.entity.RestliEntityClientFactory;
 import com.linkedin.metadata.kafka.hydrator.EntityHydrator;
 import org.springframework.beans.factory.annotation.Autowired;
@@ -12,7 +13,7 @@
 
 
 @Configuration
-@Import({RestliEntityClientFactory.class})
+@Import({RestliEntityClientFactory.class, SystemAuthenticationFactory.class})
 public class EntityHydratorConfig {
 
   @Autowired

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/search/BaseElasticSearchComponentsFactory.java
Patch:
@@ -21,7 +21,8 @@
  * Factory for components required for any services using elasticsearch
  */
 @Configuration
-@Import({RestHighLevelClientFactory.class})
+@Import({RestHighLevelClientFactory.class, IndexConventionFactory.class, ElasticSearchBulkProcessorFactory.class,
+    ElasticSearchIndexBuilderFactory.class})
 @PropertySource(value = "classpath:/application.yml", factory = YamlPropertySourceFactory.class)
 public class BaseElasticSearchComponentsFactory {
   @Value

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/group/EntityCountsResolver.java
Patch:
@@ -8,6 +8,7 @@
 import com.linkedin.entity.client.EntityClient;
 import graphql.schema.DataFetcher;
 import graphql.schema.DataFetchingEnvironment;
+import io.opentelemetry.extension.annotations.WithSpan;
 import java.util.List;
 import java.util.Map;
 import java.util.concurrent.CompletableFuture;
@@ -25,6 +26,7 @@ public EntityCountsResolver(final EntityClient entityClient) {
   }
 
   @Override
+  @WithSpan
   public CompletableFuture<EntityCountResults> get(final DataFetchingEnvironment environment) throws Exception {
 
     final QueryContext context = environment.getContext();

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/recommendation/ListRecommendationsResolver.java
Patch:
@@ -21,6 +21,7 @@
 import com.linkedin.metadata.recommendation.SearchRequestContext;
 import graphql.schema.DataFetcher;
 import graphql.schema.DataFetchingEnvironment;
+import io.opentelemetry.extension.annotations.WithSpan;
 import java.net.URISyntaxException;
 import java.util.Collections;
 import java.util.List;
@@ -41,6 +42,7 @@ public class ListRecommendationsResolver implements DataFetcher<CompletableFutur
 
   private final RecommendationsService _recommendationsService;
 
+  @WithSpan
   @Override
   public CompletableFuture<ListRecommendationsResult> get(DataFetchingEnvironment environment) {
     final ListRecommendationsInput input =

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/search/SearchResolver.java
Patch:
@@ -8,6 +8,7 @@
 import com.linkedin.entity.client.EntityClient;
 import graphql.schema.DataFetcher;
 import graphql.schema.DataFetchingEnvironment;
+import io.opentelemetry.extension.annotations.WithSpan;
 import java.util.concurrent.CompletableFuture;
 import lombok.RequiredArgsConstructor;
 import lombok.extern.slf4j.Slf4j;
@@ -28,6 +29,7 @@ public class SearchResolver implements DataFetcher<CompletableFuture<SearchResul
   private final EntityClient _entityClient;
 
   @Override
+  @WithSpan
   public CompletableFuture<SearchResults> get(DataFetchingEnvironment environment) {
     final SearchInput input = bindArgument(environment.getArgument("input"), SearchInput.class);
     final String entityName = EntityTypeMapper.getName(input.getType());

File: metadata-io/src/main/java/com/linkedin/metadata/datahubusage/DataHubUsageEventType.java
Patch:
@@ -14,7 +14,9 @@ public enum DataHubUsageEventType {
   BROWSE_RESULT_CLICK_EVENT("BrowseResultClickEvent"),
   ENTITY_VIEW_EVENT("EntityViewEvent"),
   ENTITY_SECTION_VIEW_EVENT("EntitySectionViewEvent"),
-  ENTITY_ACTION_EVENT("EntityActionEvent");
+  ENTITY_ACTION_EVENT("EntityActionEvent"),
+  RECOMMENDATION_IMPRESSION_EVENT("RecommendationImpressionEvent"),
+  RECOMMENDATION_CLICK_EVENT("RecommendationClickEvent");
 
   private final String type;
 

File: metadata-io/src/main/java/com/linkedin/metadata/recommendation/RecommendationsService.java
Patch:
@@ -4,6 +4,7 @@
 import com.linkedin.metadata.recommendation.candidatesource.RecommendationSource;
 import com.linkedin.metadata.recommendation.ranker.RecommendationModuleRanker;
 import com.linkedin.metadata.utils.ConcurrencyUtils;
+import io.opentelemetry.extension.annotations.WithSpan;
 import java.util.List;
 import java.util.Map;
 import java.util.Optional;
@@ -49,6 +50,7 @@ private void validateRecommendationSources(final List<RecommendationSource> cand
    * @return List of recommendation modules
    */
   @Nonnull
+  @WithSpan
   public List<RecommendationModule> listRecommendations(
       @Nonnull Urn userUrn,
       @Nonnull RecommendationRequestContext requestContext,

File: metadata-io/src/main/java/com/linkedin/metadata/recommendation/candidatesource/MostPopularSource.java
Patch:
@@ -14,6 +14,7 @@
 import com.linkedin.metadata.search.utils.ESUtils;
 import com.linkedin.metadata.utils.elasticsearch.IndexConvention;
 import com.linkedin.metadata.utils.metrics.MetricUtils;
+import io.opentelemetry.extension.annotations.WithSpan;
 import java.io.IOException;
 import java.net.URISyntaxException;
 import java.util.List;
@@ -73,6 +74,7 @@ public boolean isEligible(@Nonnull Urn userUrn, @Nonnull RecommendationRequestCo
   }
 
   @Override
+  @WithSpan
   public List<RecommendationContent> getRecommendations(@Nonnull Urn userUrn,
       @Nonnull RecommendationRequestContext requestContext) {
     SearchRequest searchRequest = buildSearchRequest(userUrn);

File: metadata-io/src/main/java/com/linkedin/metadata/recommendation/candidatesource/RecentlyViewedSource.java
Patch:
@@ -14,6 +14,7 @@
 import com.linkedin.metadata.search.utils.ESUtils;
 import com.linkedin.metadata.utils.elasticsearch.IndexConvention;
 import com.linkedin.metadata.utils.metrics.MetricUtils;
+import io.opentelemetry.extension.annotations.WithSpan;
 import java.io.IOException;
 import java.net.URISyntaxException;
 import java.util.List;
@@ -74,6 +75,7 @@ public boolean isEligible(@Nonnull Urn userUrn, @Nonnull RecommendationRequestCo
   }
 
   @Override
+  @WithSpan
   public List<RecommendationContent> getRecommendations(@Nonnull Urn userUrn,
       @Nonnull RecommendationRequestContext requestContext) {
     SearchRequest searchRequest = buildSearchRequest(userUrn);

File: metadata-io/src/main/java/com/linkedin/metadata/recommendation/candidatesource/RecommendationSource.java
Patch:
@@ -6,6 +6,7 @@
 import com.linkedin.metadata.recommendation.RecommendationModule;
 import com.linkedin.metadata.recommendation.RecommendationRenderType;
 import com.linkedin.metadata.recommendation.RecommendationRequestContext;
+import io.opentelemetry.extension.annotations.WithSpan;
 import java.util.List;
 import java.util.Optional;
 import javax.annotation.Nonnull;
@@ -47,6 +48,7 @@ public interface RecommendationSource {
    * @param requestContext Context of where the recommendations are being requested
    * @return list of recommendation candidates
    */
+  @WithSpan
   List<RecommendationContent> getRecommendations(@Nonnull Urn userUrn, @Nonnull RecommendationRequestContext requestContext);
 
   /**

File: metadata-io/src/main/java/com/linkedin/metadata/search/EntitySearchService.java
Patch:
@@ -93,14 +93,14 @@ AutoCompleteResult autoComplete(@Nonnull String entityName, @Nonnull String quer
   /**
    * Returns number of documents per field value given the field and filters
    *
-   * @param entityName name of the entity
+   * @param entityName name of the entity, if empty aggregate over all entities
    * @param field the field name for aggregate
    * @param requestParams filters to apply before aggregating
    * @param limit the number of aggregations to return
    * @return
    */
   @Nonnull
-  Map<String, Long> aggregateByValue(@Nonnull String entityName, @Nonnull String field, @Nullable Filter requestParams,
+  Map<String, Long> aggregateByValue(@Nullable String entityName, @Nonnull String field, @Nullable Filter requestParams,
       int limit);
 
   /**

File: metadata-io/src/main/java/com/linkedin/metadata/search/elasticsearch/ElasticSearchService.java
Patch:
@@ -7,7 +7,7 @@
 import com.linkedin.metadata.query.filter.SortCriterion;
 import com.linkedin.metadata.search.EntitySearchService;
 import com.linkedin.metadata.search.SearchResult;
-import com.linkedin.metadata.search.elasticsearch.indexbuilder.ESIndexBuilders;
+import com.linkedin.metadata.search.elasticsearch.indexbuilder.EntityIndexBuilders;
 import com.linkedin.metadata.search.elasticsearch.query.ESBrowseDAO;
 import com.linkedin.metadata.search.elasticsearch.query.ESSearchDAO;
 import com.linkedin.metadata.search.elasticsearch.update.ESWriteDAO;
@@ -23,7 +23,7 @@
 @RequiredArgsConstructor
 public class ElasticSearchService implements EntitySearchService {
 
-  private final ESIndexBuilders indexBuilders;
+  private final EntityIndexBuilders indexBuilders;
   private final ESSearchDAO esSearchDAO;
   private final ESBrowseDAO esBrowseDAO;
   private final ESWriteDAO esWriteDAO;
@@ -87,7 +87,7 @@ public AutoCompleteResult autoComplete(@Nonnull String entityName, @Nonnull Stri
 
   @Nonnull
   @Override
-  public Map<String, Long> aggregateByValue(@Nonnull String entityName, @Nonnull String field,
+  public Map<String, Long> aggregateByValue(@Nullable String entityName, @Nonnull String field,
       @Nullable Filter requestParams, int limit) {
     log.debug("Aggregating by value: {}, field: {}, requestParams: {}, limit: {}", entityName, field, requestParams,
         limit);

File: metadata-io/src/main/java/com/linkedin/metadata/search/elasticsearch/indexbuilder/EntityIndexBuilder.java
Patch:
@@ -5,13 +5,12 @@
 import java.util.Map;
 import lombok.RequiredArgsConstructor;
 import lombok.extern.slf4j.Slf4j;
-import org.elasticsearch.client.RestHighLevelClient;
 
 
 @Slf4j
 @RequiredArgsConstructor
 public class EntityIndexBuilder {
-  private final RestHighLevelClient searchClient;
+  private final ESIndexBuilder indexBuilder;
   private final EntitySpec entitySpec;
   private final SettingsBuilder settingsBuilder;
   private final String indexName;
@@ -21,6 +20,6 @@ public void buildIndex() throws IOException {
     Map<String, Object> mappings = MappingsBuilder.getMappings(entitySpec);
     Map<String, Object> settings = settingsBuilder.getSettings();
 
-    new IndexBuilder(searchClient, indexName, mappings, settings).buildIndex();
+    indexBuilder.buildIndex(indexName, mappings, settings);
   }
 }

File: metadata-io/src/main/java/com/linkedin/metadata/search/elasticsearch/indexbuilder/SettingsBuilder.java
Patch:
@@ -28,7 +28,7 @@ private static Map<String, Object> buildSettings(List<String> urnStopWords) {
         .put("normalizer", buildNormalizers())
         .put("analyzer", buildAnalyzers())
         .build());
-    return ImmutableMap.of("index", settings.build());
+    return settings.build();
   }
 
   private static Map<String, Object> buildFilters(List<String> urnStopWords) {
@@ -88,7 +88,7 @@ private static Map<String, Object> buildAnalyzers() {
 
     // Analyzer for text tokenized into words (split by spaces, periods, and slashes)
     analyzers.put("word_delimited", ImmutableMap.<String, Object>builder().put("tokenizer", "main_tokenizer")
-        .put("filter", ImmutableList.of("custom_delimiter", "lowercase"))
+        .put("filter", ImmutableList.of("custom_delimiter", "lowercase", "stop"))
         .build());
 
     // Analyzer for splitting by slashes (used to get depth of browsePath)

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/common/IndexConventionFactory.java
Patch:
@@ -19,7 +19,7 @@
 public class IndexConventionFactory {
   public static final String INDEX_CONVENTION_BEAN = "searchIndexConvention";
 
-  @Value("${elasticsearch.indexPrefix:}")
+  @Value("${elasticsearch.index.prefix:}")
   private String indexPrefix;
 
   @Bean(name = INDEX_CONVENTION_BEAN)

File: metadata-service/restli-client/src/main/java/com/linkedin/entity/client/JavaEntityClient.java
Patch:
@@ -32,6 +32,7 @@
 import com.linkedin.mxe.MetadataChangeProposal;
 import com.linkedin.mxe.SystemMetadata;
 import com.linkedin.r2.RemoteInvocationException;
+import io.opentelemetry.extension.annotations.WithSpan;
 import java.time.Clock;
 import java.util.List;
 import java.util.Map;
@@ -183,6 +184,7 @@ public void batchUpdate(@Nonnull final Set<Entity> entities, @Nonnull final Auth
      * @throws RemoteInvocationException
      */
     @Nonnull
+    @WithSpan
     public SearchResult search(
         @Nonnull String entity,
         @Nonnull String input,

File: metadata-io/src/test/java/com/linkedin/metadata/graph/DgraphGraphServiceTest.java
Patch:
@@ -27,6 +27,7 @@
 import java.util.Set;
 import java.util.concurrent.TimeUnit;
 
+import static com.linkedin.metadata.DockerTestUtils.checkContainerEngine;
 import static com.linkedin.metadata.search.utils.QueryUtils.EMPTY_FILTER;
 import static com.linkedin.metadata.search.utils.QueryUtils.newFilter;
 import static com.linkedin.metadata.search.utils.QueryUtils.newRelationshipFilter;
@@ -52,8 +53,8 @@ public void setup() {
                 .withTmpFs(Collections.singletonMap("/dgraph", "rw,noexec,nosuid,size=1g"))
                 .withStartupTimeout(Duration.ofMinutes(1))
                 .withStartupAttempts(3);
+        checkContainerEngine(_container.getDockerClient());
         _container.start();
-
         Slf4jLogConsumer logConsumer = new Slf4jLogConsumer(log);
         _container.followOutput(logConsumer);
     }

File: metadata-io/src/test/java/com/linkedin/metadata/graph/ElasticSearchGraphServiceTest.java
Patch:
@@ -27,6 +27,7 @@
 import java.util.HashSet;
 import java.util.List;
 
+import static com.linkedin.metadata.DockerTestUtils.checkContainerEngine;
 import static org.testng.Assert.assertEquals;
 
 import static com.linkedin.metadata.graph.elastic.ElasticSearchGraphService.INDEX_NAME;
@@ -45,6 +46,7 @@ public class ElasticSearchGraphServiceTest extends GraphServiceTestBase {
   @BeforeTest
   public void setup() {
     _elasticsearchContainer = new ElasticsearchContainer(IMAGE_NAME);
+    checkContainerEngine(_elasticsearchContainer.getDockerClient());
     _elasticsearchContainer.start();
     _searchClient = buildRestClient();
     _client = buildService();

File: metadata-io/src/test/java/com/linkedin/metadata/search/SearchServiceTest.java
Patch:
@@ -32,6 +32,7 @@
 import org.testng.annotations.BeforeTest;
 import org.testng.annotations.Test;
 
+import static com.linkedin.metadata.DockerTestUtils.checkContainerEngine;
 import static com.linkedin.metadata.ElasticSearchTestUtils.syncAfterWrite;
 import static org.testng.Assert.assertEquals;
 
@@ -57,6 +58,7 @@ public void setup() {
     _indexConvention = new IndexConventionImpl(null);
     _elasticsearchContainer = new ElasticsearchContainer(IMAGE_NAME);
     _settingsBuilder = new SettingsBuilder(Collections.emptyList());
+    checkContainerEngine(_elasticsearchContainer.getDockerClient());
     _elasticsearchContainer.start();
     _searchClient = buildRestClient();
     _elasticSearchService = buildEntitySearchService();

File: metadata-io/src/test/java/com/linkedin/metadata/search/elasticsearch/ElasticSearchServiceTest.java
Patch:
@@ -30,6 +30,7 @@
 import org.testng.annotations.BeforeTest;
 import org.testng.annotations.Test;
 
+import static com.linkedin.metadata.DockerTestUtils.checkContainerEngine;
 import static com.linkedin.metadata.ElasticSearchTestUtils.syncAfterWrite;
 import static org.testng.Assert.assertEquals;
 
@@ -53,6 +54,7 @@ public void setup() {
     _indexConvention = new IndexConventionImpl(null);
     _elasticsearchContainer = new ElasticsearchContainer(IMAGE_NAME);
     _settingsBuilder = new SettingsBuilder(Collections.emptyList());
+    checkContainerEngine(_elasticsearchContainer.getDockerClient());
     _elasticsearchContainer.start();
     _searchClient = buildRestClient();
     _elasticSearchService = buildService();

File: metadata-io/src/test/java/com/linkedin/metadata/systemmetadata/ElasticSearchSystemMetadataServiceTest.java
Patch:
@@ -18,6 +18,7 @@
 import org.testng.annotations.BeforeTest;
 import org.testng.annotations.Test;
 
+import static com.linkedin.metadata.DockerTestUtils.checkContainerEngine;
 import static com.linkedin.metadata.ElasticSearchTestUtils.syncAfterWrite;
 import static com.linkedin.metadata.systemmetadata.ElasticSearchSystemMetadataService.INDEX_NAME;
 import static org.testng.Assert.*;
@@ -36,6 +37,7 @@ public class ElasticSearchSystemMetadataServiceTest {
   @BeforeTest
   public void setup() {
     _elasticsearchContainer = new ElasticsearchContainer(IMAGE_NAME);
+    checkContainerEngine(_elasticsearchContainer.getDockerClient());
     _elasticsearchContainer.start();
     _searchClient = buildRestClient();
     _client = buildService();

File: metadata-io/src/test/java/com/linkedin/metadata/timeseries/elastic/ElasticSearchTimeseriesAspectServiceTest.java
Patch:
@@ -50,6 +50,7 @@
 import org.testng.annotations.BeforeTest;
 import org.testng.annotations.Test;
 
+import static com.linkedin.metadata.DockerTestUtils.checkContainerEngine;
 import static com.linkedin.metadata.ElasticSearchTestUtils.*;
 import static org.testng.Assert.*;
 
@@ -89,6 +90,7 @@ public void setup() {
         TestEntityProfile.class.getClassLoader().getResourceAsStream("test-entity-registry.yml"));
     _indexConvention = new IndexConventionImpl(null);
     _elasticsearchContainer = new ElasticsearchContainer(IMAGE_NAME);
+    checkContainerEngine(_elasticsearchContainer.getDockerClient());
     _elasticsearchContainer.start();
     _searchClient = buildRestClient();
     _elasticSearchTimeseriesAspectService = buildService();

File: metadata-io/src/main/java/com/linkedin/metadata/recommendation/candidatesource/MostPopularSource.java
Patch:
@@ -11,6 +11,7 @@
 import com.linkedin.metadata.recommendation.RecommendationRenderType;
 import com.linkedin.metadata.recommendation.RecommendationRequestContext;
 import com.linkedin.metadata.recommendation.ScenarioType;
+import com.linkedin.metadata.search.utils.ESUtils;
 import com.linkedin.metadata.utils.elasticsearch.IndexConvention;
 import com.linkedin.metadata.utils.metrics.MetricUtils;
 import java.io.IOException;
@@ -95,15 +96,14 @@ private SearchRequest buildSearchRequest(@Nonnull Urn userUrn) {
     SearchRequest request = new SearchRequest();
     SearchSourceBuilder source = new SearchSourceBuilder();
     BoolQueryBuilder query = QueryBuilders.boolQuery();
-    // Filter for the entity view events of the user requesting recommendation
-    query.must(QueryBuilders.termQuery(DataHubUsageEventConstants.ACTOR_URN, userUrn.toString()));
+    // Filter for all entity view events
     query.must(
         QueryBuilders.termQuery(DataHubUsageEventConstants.TYPE, DataHubUsageEventType.ENTITY_VIEW_EVENT.getType()));
     source.query(query);
 
     // Find the entities with the most views
     AggregationBuilder aggregation = AggregationBuilders.terms(ENTITY_AGG_NAME)
-        .field(DataHubUsageEventConstants.ENTITY_URN + ".keyword")
+        .field(DataHubUsageEventConstants.ENTITY_URN + ESUtils.KEYWORD_SUFFIX)
         .size(MAX_CONTENT);
     source.aggregation(aggregation);
     source.size(0);

File: metadata-io/src/main/java/com/linkedin/metadata/search/elasticsearch/query/request/SearchRequestHandler.java
Patch:
@@ -178,7 +178,7 @@ public SearchRequest getAggregationRequest(@Nonnull String field, @Nullable Filt
     final SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder();
     searchSourceBuilder.query(filterQuery);
     searchSourceBuilder.size(0);
-    searchSourceBuilder.aggregation(AggregationBuilders.terms(field).field(field + ".keyword").size(limit));
+    searchSourceBuilder.aggregation(AggregationBuilders.terms(field).field(field + ESUtils.KEYWORD_SUFFIX).size(limit));
     searchRequest.source(searchSourceBuilder);
 
     return searchRequest;
@@ -193,7 +193,7 @@ private List<AggregationBuilder> getAggregations() {
     for (String facet : _facetFields) {
       // All facet fields must have subField keyword
       AggregationBuilder aggBuilder =
-          AggregationBuilders.terms(facet).field(facet + ".keyword").size(_maxTermBucketSize);
+          AggregationBuilders.terms(facet).field(facet + ESUtils.KEYWORD_SUFFIX).size(_maxTermBucketSize);
       aggregationBuilders.add(aggBuilder);
     }
     return aggregationBuilders;

File: metadata-io/src/main/java/com/linkedin/metadata/search/utils/ESUtils.java
Patch:
@@ -24,6 +24,8 @@ public class ESUtils {
 
   private static final String DEFAULT_SEARCH_RESULTS_SORT_BY_FIELD = "urn";
 
+  public static final String KEYWORD_SUFFIX = ".keyword";
+
   /*
    * Refer to https://www.elastic.co/guide/en/elasticsearch/reference/current/regexp-syntax.html for list of reserved
    * characters in an Elasticsearch regular expression.

File: metadata-io/src/main/java/com/linkedin/metadata/search/elasticsearch/query/ESSearchDAO.java
Patch:
@@ -80,11 +80,12 @@ private SearchResult executeAndExtract(@Nonnull EntitySpec entitySpec, @Nonnull
   @Nonnull
   public SearchResult search(@Nonnull String entityName, @Nonnull String input, @Nullable Filter postFilters,
       @Nullable SortCriterion sortCriterion, int from, int size) {
+    final String finalInput = input.isEmpty() ? "*" : input;
     Timer.Context searchRequestTimer = MetricUtils.timer(this.getClass(), "searchRequest").time();
     EntitySpec entitySpec = entityRegistry.getEntitySpec(entityName);
     // Step 1: construct the query
     final SearchRequest searchRequest =
-        SearchRequestHandler.getBuilder(entitySpec).getSearchRequest(input, postFilters, sortCriterion, from, size);
+        SearchRequestHandler.getBuilder(entitySpec).getSearchRequest(finalInput, postFilters, sortCriterion, from, size);
     searchRequest.indices(indexConvention.getIndexName(entitySpec));
     searchRequestTimer.stop();
     // Step 2: execute the query and extract results, validated against document model as well

File: metadata-service/restli-servlet-impl/src/main/java/com/linkedin/metadata/resources/entity/EntityResource.java
Patch:
@@ -243,9 +243,8 @@ public Task<SearchResult> searchAcrossEntities(@ActionParam(PARAM_ENTITIES) @Opt
       @ActionParam(PARAM_COUNT) int count) {
     List<String> entityList = entities == null ? Collections.emptyList() : Arrays.asList(entities);
     log.info("GET SEARCH RESULTS ACROSS ENTITIES for {} with query {}", entityList, input);
-    final String finalInput = input.isEmpty() ? "*" : input;
     return RestliUtil.toTask(
-        () -> _searchService.searchAcrossEntities(entityList, finalInput, filter, sortCriterion, start, count),
+        () -> _searchService.searchAcrossEntities(entityList, input, filter, sortCriterion, start, count),
         "searchAcrossEntities");
   }
 

File: metadata-io/src/main/java/com/linkedin/metadata/timeseries/TimeseriesAspectService.java
Patch:
@@ -19,7 +19,7 @@ public interface TimeseriesAspectService {
   void upsertDocument(@Nonnull String entityName, @Nonnull String aspectName, @Nonnull String docId, @Nonnull JsonNode document);
 
   List<EnvelopedAspect> getAspectValues(@Nonnull final Urn urn, @Nonnull String entityName, @Nonnull String aspectName,
-      @Nullable Long startTimeMillis, Long endTimeMillis, int limit);
+      @Nullable Long startTimeMillis, @Nullable Long endTimeMillis, @Nullable Integer limit);
 
   /**
    * Get the aggregated metrics for the given dataset or column from a time series aspect.

File: metadata-service/restli-client/src/main/java/com/linkedin/entity/client/EntityClient.java
Patch:
@@ -166,7 +166,7 @@ public SearchResult search(
    */
   @Nonnull
   public SearchResult searchAcrossEntities(
-      @Nullable List<String> entities,
+      @Nonnull List<String> entities,
       @Nonnull String input,
       @Nullable Filter filter,
       int start,

File: metadata-service/restli-client/src/main/java/com/linkedin/entity/client/JavaEntityClient.java
Patch:
@@ -250,7 +250,7 @@ public SearchResult search(
      */
     @Nonnull
     public SearchResult searchAcrossEntities(
-        @Nullable List<String> entities,
+        @Nonnull List<String> entities,
         @Nonnull String input,
         @Nullable Filter filter,
         int start,
@@ -273,7 +273,6 @@ public StringArray getBrowsePaths(@Nonnull Urn urn, @Nonnull final Authenticatio
 
     public void setWritable(boolean canWrite, @Nonnull final Authentication authentication) throws RemoteInvocationException {
         _entityService.setWritable(canWrite);
-        return;
     }
 
     @Nonnull
@@ -338,7 +337,6 @@ public List<EnvelopedAspect> getTimeseriesAspectValues(@Nonnull String urn, @Non
         if (endTimeMillis != null) {
             response.setEndTimeMillis(endTimeMillis);
         }
-        response.setLimit(limit);
         response.setValues(new EnvelopedAspectArray(
             _timeseriesAspectService.getAspectValues(Urn.createFromString(urn), entity, aspect, startTimeMillis, endTimeMillis,
                 limit)));

File: metadata-service/restli-client/src/main/java/com/linkedin/entity/client/RestliEntityClient.java
Patch:
@@ -321,7 +321,7 @@ public SearchResult search(
      */
     @Nonnull
     public SearchResult searchAcrossEntities(
-        @Nullable List<String> entities,
+        @Nonnull List<String> entities,
         @Nonnull String input,
         @Nullable Filter filter,
         int start,

File: datahub-frontend/app/controllers/Application.java
Patch:
@@ -106,6 +106,7 @@ public CompletableFuture<Result> proxy(String path) throws ExecutionException, I
             .entrySet()
             .stream()
             .filter(entry -> !Http.HeaderNames.CONTENT_LENGTH.equals(entry.getKey()))
+            .filter(entry -> !Http.HeaderNames.CONTENT_TYPE.equals(entry.getKey()))
             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue))
         )
         .addHeader(Constants.ACTOR_HEADER_NAME, ctx().session().get(ACTOR)) // TODO: Replace with a token to GMS.
@@ -115,6 +116,8 @@ public CompletableFuture<Result> proxy(String path) throws ExecutionException, I
           final ResponseHeader header = new ResponseHeader(apiResponse.getStatus(), apiResponse.getHeaders()
               .entrySet()
               .stream()
+              .filter(entry -> !Http.HeaderNames.CONTENT_LENGTH.equals(entry.getKey()))
+              .filter(entry -> !Http.HeaderNames.CONTENT_TYPE.equals(entry.getKey()))
               .map(entry -> Pair.of(entry.getKey(), String.join(";", entry.getValue())))
               .collect(Collectors.toMap(Pair::getFirst, Pair::getSecond)));
           final HttpEntity body = new HttpEntity.Strict(apiResponse.getBodyAsBytes(), Optional.ofNullable(apiResponse.getContentType()));

File: metadata-io/src/main/java/com/linkedin/metadata/search/aggregator/AllEntitiesSearchAggregator.java
Patch:
@@ -49,7 +49,7 @@ public class AllEntitiesSearchAggregator {
   private final EntitySearchServiceCache _entitySearchServiceCache;
 
   private static final List<String> FILTER_RANKING =
-      ImmutableList.of("entity", "platform", "origin", "tags", "glossaryTerms");
+      ImmutableList.of("entity", "typeNames", "platform", "origin", "tags", "glossaryTerms");
 
   public AllEntitiesSearchAggregator(EntityRegistry entityRegistry, EntitySearchService entitySearchService,
       SearchRanker searchRanker, CacheManager cacheManager, int batchSize) {

File: metadata-service/graphql-api/src/main/java/com/datahub/metadata/graphql/GraphQLEngineFactory.java
Patch:
@@ -56,6 +56,6 @@ protected GraphQLEngine getInstance() {
           new AnalyticsService(elasticClient, indexConvention.getPrefix()), _entityService, _graphClient, _entityClient
       ).builder().build();
     }
-    return new GmsGraphQLEngine().builder().build();
+    return new GmsGraphQLEngine(null, _entityService, _graphClient, _entityClient).builder().build();
   }
 }

File: datahub-upgrade/src/main/java/com/linkedin/datahub/upgrade/restorebackup/backupreader/LocalParquetReader.java
Patch:
@@ -1,5 +1,6 @@
 package com.linkedin.datahub.upgrade.restorebackup.backupreader;
 
+import com.google.common.collect.ImmutableList;
 import com.linkedin.datahub.upgrade.UpgradeContext;
 import java.io.IOException;
 import java.util.Optional;
@@ -41,7 +42,7 @@ public EbeanAspectBackupIterator getBackupIterator(UpgradeContext context) {
 
     try {
       ParquetReader<GenericRecord> reader = AvroParquetReader.<GenericRecord>builder(new Path(path.get())).build();
-      return new ParquetEbeanAspectBackupIterator(reader);
+      return new ParquetEbeanAspectBackupIterator(ImmutableList.of(reader));
     } catch (IOException e) {
       throw new RuntimeException(String.format("Failed to build ParquetReader: %s", e));
     }

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/exception/DataHubDataFetcherExceptionHandler.java
Patch:
@@ -23,9 +23,10 @@ public DataFetcherExceptionHandlerResult onException(DataFetcherExceptionHandler
     DataHubGraphQLErrorCode errorCode = DataHubGraphQLErrorCode.SERVER_ERROR;
     String message = "An unknown error occurred.";
 
-    if (exception instanceof IllegalArgumentException) {
+    // note: make sure to access the true error message via `getCause()`
+    if (exception.getCause() instanceof IllegalArgumentException) {
       errorCode = DataHubGraphQLErrorCode.BAD_REQUEST;
-      message = exception.getMessage();
+      message = exception.getCause().getMessage();
     }
 
     if (exception instanceof DataHubGraphQLException) {

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/mutate/AddTagResolver.java
Patch:
@@ -39,7 +39,8 @@ public CompletableFuture<Boolean> get(DataFetchingEnvironment environment) throw
           input.getSubResource(),
           input.getSubResourceType(),
           "tag",
-          _entityService
+          _entityService,
+          false
       );
       try {
 

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/mutate/AddTermResolver.java
Patch:
@@ -37,7 +37,8 @@ public CompletableFuture<Boolean> get(DataFetchingEnvironment environment) throw
           input.getSubResource(),
           input.getSubResourceType(),
           "glossaryTerm",
-          _entityService
+          _entityService,
+          false
       );
 
       try {

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/mutate/RemoveTagResolver.java
Patch:
@@ -38,7 +38,8 @@ public CompletableFuture<Boolean> get(DataFetchingEnvironment environment) throw
           input.getSubResource(),
           input.getSubResourceType(),
           "tag",
-          _entityService
+          _entityService,
+          true
       );
       try {
 

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/mutate/RemoveTermResolver.java
Patch:
@@ -38,7 +38,8 @@ public CompletableFuture<Boolean> get(DataFetchingEnvironment environment) throw
           input.getSubResource(),
           input.getSubResourceType(),
           "glossaryTerm",
-          _entityService
+          _entityService,
+          true
       );
 
       try {

File: metadata-service/restli-client/src/main/java/com/linkedin/entity/client/RestliEntityClient.java
Patch:
@@ -392,9 +392,11 @@ public SearchResult filter(@Nonnull String entity, @Nonnull Filter filter, @Null
             ENTITIES_REQUEST_BUILDERS.actionFilter()
                 .entityParam(entity)
                 .filterParam(filter)
-                .sortParam(sortCriterion)
                 .startParam(start)
                 .countParam(count);
+        if (sortCriterion != null) {
+            requestBuilder.sortParam(sortCriterion);
+        }
         return sendClientRequest(requestBuilder, actor).getEntity();
     }
 }

File: metadata-service/restli-impl/src/main/java/com/linkedin/metadata/resources/entity/AspectResource.java
Patch:
@@ -1,6 +1,7 @@
 package com.linkedin.metadata.resources.entity;
 
 import com.codahale.metrics.MetricRegistry;
+import com.google.common.collect.ImmutableSet;
 import com.linkedin.aspect.GetTimeseriesAspectValuesResponse;
 import com.linkedin.common.AuditStamp;
 import com.linkedin.common.urn.Urn;
@@ -159,8 +160,7 @@ private List<MetadataChangeProposal> getAdditionalChanges(@Nonnull MetadataChang
     final Urn urn = EntityKeyUtils.getUrnFromProposal(metadataChangeProposal,
         _entityService.getKeyAspectSpec(metadataChangeProposal.getEntityType()));
 
-    return _entityService.getDefaultAspectsFromUrn(urn)
-        .entrySet()
+    return _entityService.generateDefaultAspectsIfMissing(urn, ImmutableSet.of(metadataChangeProposal.getAspectName()))
         .stream()
         .map(entry -> getProposalFromAspect(entry.getKey(), entry.getValue(), metadataChangeProposal))
         .filter(Objects::nonNull)

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/GmsGraphQLEngine.java
Patch:
@@ -40,6 +40,7 @@
 import com.linkedin.datahub.graphql.resolvers.MeResolver;
 import com.linkedin.datahub.graphql.resolvers.group.AddGroupMembersResolver;
 import com.linkedin.datahub.graphql.resolvers.group.CreateGroupResolver;
+import com.linkedin.datahub.graphql.resolvers.group.EntityCountsResolver;
 import com.linkedin.datahub.graphql.resolvers.group.ListGroupsResolver;
 import com.linkedin.datahub.graphql.resolvers.group.RemoveGroupMembersResolver;
 import com.linkedin.datahub.graphql.resolvers.group.RemoveGroupResolver;
@@ -422,6 +423,8 @@ private void configureQueryResolvers(final RuntimeWiring.Builder builder) {
                 new ListUsersResolver(GmsClientFactory.getEntitiesClient()))
             .dataFetcher("listGroups",
                 new ListGroupsResolver(GmsClientFactory.getEntitiesClient()))
+            .dataFetcher("getEntityCounts",
+                new EntityCountsResolver(GmsClientFactory.getEntitiesClient()))
         );
     }
 

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/EntityTypeMapper.java
Patch:
@@ -12,7 +12,7 @@
  */
 public class EntityTypeMapper {
 
-  private static final Map<EntityType, String> ENTITY_TYPE_TO_NAME =
+  static final Map<EntityType, String> ENTITY_TYPE_TO_NAME =
       ImmutableMap.<EntityType, String>builder().put(EntityType.DATASET, "dataset")
           .put(EntityType.CORP_USER, "corpuser")
           .put(EntityType.CORP_GROUP, "corpGroup")

File: datahub-upgrade/src/main/java/com/linkedin/datahub/upgrade/config/NoCodeUpgradeConfig.java
Patch:
@@ -3,7 +3,7 @@
 import com.linkedin.datahub.upgrade.nocode.NoCodeUpgrade;
 import com.linkedin.entity.client.EntityClient;
 import com.linkedin.metadata.entity.EntityService;
-import com.linkedin.metadata.models.registry.SnapshotEntityRegistry;
+import com.linkedin.metadata.models.registry.EntityRegistry;
 import io.ebean.EbeanServer;
 import javax.annotation.Nonnull;
 import org.springframework.beans.factory.annotation.Autowired;
@@ -20,13 +20,13 @@ public class NoCodeUpgradeConfig {
   ApplicationContext applicationContext;
 
   @Bean(name = "noCodeUpgrade")
-  @DependsOn({"ebeanServer", "entityService", "entityClient"})
+  @DependsOn({"ebeanServer", "entityService", "entityClient", "entityRegistry"})
   @Nonnull
   public NoCodeUpgrade createInstance() {
     final EbeanServer ebeanServer = applicationContext.getBean(EbeanServer.class);
     final EntityService entityService = applicationContext.getBean(EntityService.class);
     final EntityClient entityClient = applicationContext.getBean(EntityClient.class);
-    final SnapshotEntityRegistry entityRegistry = new SnapshotEntityRegistry();
+    final EntityRegistry entityRegistry = applicationContext.getBean(EntityRegistry.class);
 
     return new NoCodeUpgrade(ebeanServer, entityService, entityRegistry, entityClient);
   }

File: datahub-upgrade/src/main/java/com/linkedin/datahub/upgrade/impl/DefaultUpgradeManager.java
Patch:
@@ -117,7 +117,7 @@ private UpgradeStepResult executeStepInternal(UpgradeContext context, UpgradeSte
       } catch (Exception e) {
         context.report()
             .addLine(
-                String.format("Caught exception during attempt %s of Step with id %s: %s", i, step.id(), e.toString()));
+                String.format("Caught exception during attempt %s of Step with id %s: %s", i, step.id(), e));
         result = new DefaultUpgradeStepResult(step.id(), UpgradeStepResult.Result.FAILED);
         context.report().addLine(String.format("Retrying %s more times...", maxAttempts - (i + 1)));
       }

File: datahub-upgrade/src/main/java/com/linkedin/datahub/upgrade/nocode/NoCodeUpgrade.java
Patch:
@@ -7,7 +7,7 @@
 import com.linkedin.datahub.upgrade.common.steps.GMSQualificationStep;
 import com.linkedin.entity.client.EntityClient;
 import com.linkedin.metadata.entity.EntityService;
-import com.linkedin.metadata.models.registry.SnapshotEntityRegistry;
+import com.linkedin.metadata.models.registry.EntityRegistry;
 import io.ebean.EbeanServer;
 import java.util.ArrayList;
 import java.util.Collections;
@@ -27,7 +27,7 @@ public class NoCodeUpgrade implements Upgrade {
   public NoCodeUpgrade(
       final EbeanServer server,
       final EntityService entityService,
-      final SnapshotEntityRegistry entityRegistry,
+      final EntityRegistry entityRegistry,
       final EntityClient entityClient) {
     _steps = buildUpgradeSteps(
         server,
@@ -59,7 +59,7 @@ private List<UpgradeCleanupStep> buildCleanupSteps(final EbeanServer server) {
   private List<UpgradeStep> buildUpgradeSteps(
       final EbeanServer server,
       final EntityService entityService,
-      final SnapshotEntityRegistry entityRegistry,
+      final EntityRegistry entityRegistry,
       final EntityClient entityClient) {
     final List<UpgradeStep> steps = new ArrayList<>();
     steps.add(new RemoveAspectV2TableStep(server));

File: metadata-io/src/main/java/com/linkedin/metadata/entity/ebean/EbeanEntityService.java
Patch:
@@ -26,6 +26,7 @@
 import com.linkedin.metadata.utils.EntityKeyUtils;
 import com.linkedin.metadata.utils.GenericAspectUtils;
 import com.linkedin.metadata.utils.metrics.MetricUtils;
+import com.linkedin.metadata.entity.ValidationUtils;
 import com.linkedin.mxe.MetadataAuditOperation;
 import com.linkedin.mxe.MetadataChangeLog;
 import com.linkedin.mxe.MetadataChangeProposal;
@@ -351,8 +352,6 @@ public void setWritable(boolean canWrite) {
   @Override
   public Urn ingestProposal(@Nonnull MetadataChangeProposal metadataChangeProposal, AuditStamp auditStamp) {
 
-    // todo: add restli model validation.
-
     log.debug("entity type = {}", metadataChangeProposal.getEntityType());
     EntitySpec entitySpec = getEntityRegistry().getEntitySpec(metadataChangeProposal.getEntityType());
     log.debug("entity spec = {}", entitySpec);
@@ -381,6 +380,7 @@ public Urn ingestProposal(@Nonnull MetadataChangeProposal metadataChangeProposal
     try {
       aspect = GenericAspectUtils.deserializeAspect(metadataChangeProposal.getAspect().getValue(),
           metadataChangeProposal.getAspect().getContentType(), aspectSpec);
+      ValidationUtils.validateOrThrow(aspect);
     } catch (ModelConversionException e) {
       throw new RuntimeException(
           String.format("Could not deserialize {} for aspect {}", metadataChangeProposal.getAspect().getValue(),

File: metadata-io/src/test/java/com/linkedin/metadata/entity/EbeanEntityServiceTest.java
Patch:
@@ -357,6 +357,7 @@ public void testIngestTimeseriesAspect() throws Exception {
     DatasetProfile datasetProfile = new DatasetProfile();
     datasetProfile.setRowCount(1000);
     datasetProfile.setColumnCount(15);
+    datasetProfile.setTimestampMillis(0L);
     MetadataChangeProposal gmce = new MetadataChangeProposal();
     gmce.setEntityUrn(entityUrn);
     gmce.setChangeType(ChangeType.UPSERT);

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/search/SearchServiceFactory.java
Patch:
@@ -32,7 +32,7 @@ public class SearchServiceFactory {
   @Autowired
   private CacheManager cacheManager;
 
-  @Value("${SEARCH_SERVICE_BATCH_SIZE:1000}")
+  @Value("${SEARCH_SERVICE_BATCH_SIZE:100}")
   private Integer batchSize;
 
   @Bean(name = "searchService")

File: metadata-io/src/main/java/com/linkedin/metadata/search/utils/ESUtils.java
Patch:
@@ -28,7 +28,7 @@ public class ESUtils {
    * Refer to https://www.elastic.co/guide/en/elasticsearch/reference/current/regexp-syntax.html for list of reserved
    * characters in an Elasticsearch regular expression.
    */
-  private static final String ELASTICSEARCH_REGEXP_RESERVED_CHARACTERS = "?+*|{}[]()";
+  private static final String ELASTICSEARCH_REGEXP_RESERVED_CHARACTERS = "?+*|{}[]()#@&<>~";
 
   private ESUtils() {
 

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/dataset/mappers/DatasetSnapshotMapper.java
Patch:
@@ -65,6 +65,7 @@ public Dataset apply(@Nonnull final DatasetSnapshot dataset) {
                     properties.setCustomProperties(StringMapMapper.map(gmsProperties.getCustomProperties()));
                 }
                 result.setProperties(properties);
+                result.setDescription(properties.getDescription());
                 if (gmsProperties.hasUri()) {
                     // Deprecated field.
                     result.setUri(gmsProperties.getUri().toString());

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/search/SearchAcrossEntitiesResolver.java
Patch:
@@ -34,7 +34,7 @@ public class SearchAcrossEntitiesResolver implements DataFetcher<CompletableFutu
   private static final List<EntityType> SEARCHABLE_ENTITY_TYPES =
       ImmutableList.of(EntityType.DATASET, EntityType.DASHBOARD, EntityType.CHART, EntityType.MLMODEL,
           EntityType.MLMODEL_GROUP, EntityType.MLFEATURE_TABLE, EntityType.DATA_FLOW, EntityType.DATA_JOB,
-          EntityType.GLOSSARY_TERM);
+          EntityType.GLOSSARY_TERM, EntityType.TAG, EntityType.CORP_USER, EntityType.CORP_GROUP);
 
   private final EntityClient _entityClient;
 

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/dataset/mappers/EditableSchemaFieldInfoMapper.java
Patch:
@@ -27,6 +27,7 @@ public com.linkedin.datahub.graphql.generated.EditableSchemaFieldInfo apply(@Non
         }
         if (input.hasGlobalTags()) {
             result.setGlobalTags(GlobalTagsMapper.map(input.getGlobalTags()));
+            result.setTags(GlobalTagsMapper.map(input.getGlobalTags()));
         }
         if (input.hasGlossaryTerms()) {
             result.setGlossaryTerms(GlossaryTermsMapper.map(input.getGlossaryTerms()));

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/dataset/mappers/SchemaFieldMapper.java
Patch:
@@ -28,6 +28,7 @@ public SchemaField apply(@Nonnull final com.linkedin.schema.SchemaField input) {
         result.setType(mapSchemaFieldDataType(input.getType()));
         if (input.hasGlobalTags()) {
             result.setGlobalTags(GlobalTagsMapper.map(input.getGlobalTags()));
+            result.setTags(GlobalTagsMapper.map(input.getGlobalTags()));
         }
         if (input.hasGlossaryTerms()) {
             result.setGlossaryTerms(GlossaryTermsMapper.map(input.getGlossaryTerms()));

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/SubTypesResolver.java
Patch:
@@ -20,7 +20,6 @@ public class SubTypesResolver implements DataFetcher<CompletableFuture<SubTypes>
     String _entityType;
     String _aspectName;
 
-
     @Override
     public CompletableFuture<SubTypes> get(DataFetchingEnvironment environment) throws Exception {
         return CompletableFuture.supplyAsync(() -> {

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/GmsGraphQLEngine.java
Patch:
@@ -493,6 +493,9 @@ private void configureDatasetResolvers(final RuntimeWiring.Builder builder) {
                 .dataFetcher("schemaMetadata", new AuthenticatedResolver<>(
                     new AspectResolver())
                 )
+               .dataFetcher("subTypes", new AuthenticatedResolver(new SubTypesResolver(GmsClientFactory.getAspectsClient(),
+                           "dataset",
+                       "subTypes")))
             )
             .type("Owner", typeWiring -> typeWiring
                     .dataFetcher("owner", new AuthenticatedResolver<>(

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/dataset/DatasetType.java
Patch:
@@ -99,7 +99,7 @@ public List<DataFetcherResult<Dataset>> batchLoad(final List<String> urns, final
             return gmsResults.stream()
                 .map(gmsDataset ->
                     gmsDataset == null ? null : DataFetcherResult.<Dataset>newResult()
-                        .data(DatasetSnapshotMapper.map(gmsDataset.getValue().getDatasetSnapshot()))
+                            .data(DatasetSnapshotMapper.map(gmsDataset.getValue().getDatasetSnapshot()))
                         .localContext(AspectExtractor.extractAspects(gmsDataset.getValue().getDatasetSnapshot()))
                         .build()
                 )

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/dataset/mappers/DatasetSnapshotMapper.java
Patch:
@@ -20,7 +20,6 @@
 import com.linkedin.dataset.DatasetDeprecation;
 import com.linkedin.dataset.DatasetProperties;
 import com.linkedin.dataset.EditableDatasetProperties;
-import com.linkedin.metadata.dao.utils.ModelUtils;
 import com.linkedin.metadata.snapshot.DatasetSnapshot;
 import com.linkedin.schema.EditableSchemaMetadata;
 import com.linkedin.schema.SchemaMetadata;
@@ -52,7 +51,7 @@ public Dataset apply(@Nonnull final DatasetSnapshot dataset) {
         partialPlatform.setUrn(dataset.getUrn().getPlatformEntity().toString());
         result.setPlatform(partialPlatform);
 
-        ModelUtils.getAspectsFromSnapshot(dataset).forEach(aspect -> {
+        NewModelUtils.getAspectsFromSnapshot(dataset).forEach(aspect -> {
             if (aspect instanceof DatasetProperties) {
                 final DatasetProperties gmsProperties = (DatasetProperties) aspect;
                 final com.linkedin.datahub.graphql.generated.DatasetProperties properties = new com.linkedin.datahub.graphql.generated.DatasetProperties();

File: metadata-io/src/main/java/com/linkedin/metadata/entity/ebean/EbeanEntityService.java
Patch:
@@ -366,7 +366,7 @@ public Urn ingestProposal(@Nonnull MetadataChangeProposal metadataChangeProposal
 
     if (aspectSpec == null) {
       throw new RuntimeException(
-          String.format("Unknown aspect {} for entity {}", metadataChangeProposal.getAspectName(),
+          String.format("Unknown aspect %s for entity %s", metadataChangeProposal.getAspectName(),
               metadataChangeProposal.getEntityType()));
     }
 

File: metadata-io/src/main/java/com/linkedin/metadata/entity/ebean/EbeanAspectDao.java
Patch:
@@ -24,6 +24,7 @@
 import io.ebean.RawSql;
 import io.ebean.RawSqlBuilder;
 import io.ebean.Transaction;
+import io.ebean.annotation.TxIsolation;
 import io.ebean.config.ServerConfig;
 import java.net.URISyntaxException;
 import java.sql.Timestamp;
@@ -471,7 +472,7 @@ public <T> T runInTransactionWithRetry(@Nonnull final Supplier<T> block, final i
 
     T result = null;
     do {
-      try (Transaction transaction = _server.beginTransaction()) {
+      try (Transaction transaction = _server.beginTransaction(TxIsolation.REPEATABLE_READ)) {
         result = block.get();
         transaction.commit();
         lastException = null;

File: datahub-upgrade/src/main/java/com/linkedin/datahub/upgrade/nocode/DataMigrationStep.java
Patch:
@@ -8,6 +8,7 @@
 import com.linkedin.datahub.upgrade.UpgradeContext;
 import com.linkedin.datahub.upgrade.UpgradeStep;
 import com.linkedin.datahub.upgrade.UpgradeStepResult;
+import com.linkedin.metadata.Constants;
 import com.linkedin.metadata.utils.PegasusUtils;
 import com.linkedin.metadata.dao.utils.RecordUtils;
 import com.linkedin.metadata.entity.EntityService;
@@ -149,7 +150,7 @@ public Function<UpgradeContext, UpgradeStepResult> executable() {
               browsePaths = BrowsePathUtils.buildBrowsePath(urn);
 
               final AuditStamp browsePathsStamp = new AuditStamp();
-              browsePathsStamp.setActor(Urn.createFromString("urn:li:principal:system"));
+              browsePathsStamp.setActor(Urn.createFromString(Constants.SYSTEM_ACTOR));
               browsePathsStamp.setTime(System.currentTimeMillis());
 
               _entityService.ingestAspect(urn, BROWSE_PATHS_ASPECT_NAME, browsePaths, browsePathsStamp);

File: datahub-upgrade/src/main/java/com/linkedin/datahub/upgrade/nocode/NoCodeUpgrade.java
Patch:
@@ -66,7 +66,6 @@ private List<UpgradeStep> buildUpgradeSteps(
     steps.add(new GMSQualificationStep());
     steps.add(new UpgradeQualificationStep(server));
     steps.add(new CreateAspectTableStep(server));
-    steps.add(new IngestDataPlatformsStep(entityService));
     steps.add(new DataMigrationStep(server, entityService, entityRegistry));
     steps.add(new GMSEnableWriteModeStep(entityClient));
     return steps;

File: datahub-upgrade/src/main/java/com/linkedin/datahub/upgrade/nocode/UpgradeQualificationStep.java
Patch:
@@ -61,7 +61,7 @@ private boolean isQualified(EbeanServer server, UpgradeContext context) {
       boolean v2TableExists = AspectStorageValidationUtil.checkV2TableExists(server);
       if (v2TableExists) {
         context.report().addLine("-- V2 table exists");
-        long v2TableRowCount = AspectStorageValidationUtil.getV2RowCount(server);
+        long v2TableRowCount = AspectStorageValidationUtil.getV2NonSystemRowCount(server);
         if (v2TableRowCount == 0) {
           context.report().addLine("-- V2 table is empty");
           return true;

File: metadata-service/factories/src/main/java/com/linkedin/metadata/boot/steps/IngestDataPlatformsStep.java
Patch:
@@ -57,7 +57,7 @@ public void execute() throws IOException, URISyntaxException {
           RecordUtils.toRecordTemplate(DataPlatformInfo.class, dataPlatform.get("aspect").toString());
 
       final AuditStamp aspectAuditStamp =
-          new AuditStamp().setActor(Urn.createFromString(Constants.UNKNOWN_ACTOR)).setTime(System.currentTimeMillis());
+          new AuditStamp().setActor(Urn.createFromString(Constants.SYSTEM_ACTOR)).setTime(System.currentTimeMillis());
 
       _entityService.ingestAspect(urn, PLATFORM_ASPECT_NAME, info, aspectAuditStamp);
     }

File: metadata-service/factories/src/main/java/com/linkedin/metadata/boot/steps/IngestPoliciesStep.java
Patch:
@@ -5,6 +5,7 @@
 import com.linkedin.common.AuditStamp;
 import com.linkedin.common.urn.Urn;
 import com.linkedin.data.template.RecordTemplate;
+import com.linkedin.metadata.Constants;
 import com.linkedin.metadata.boot.BootstrapStep;
 import com.linkedin.events.metadata.ChangeType;
 import com.linkedin.metadata.dao.utils.RecordUtils;
@@ -92,7 +93,7 @@ private void ingestPolicy(final Urn urn, final DataHubPolicyInfo info) throws UR
     keyAspectProposal.setEntityUrn(urn);
 
     _entityService.ingestProposal(keyAspectProposal,
-        new AuditStamp().setActor(Urn.createFromString("urn:li:corpuser:system")).setTime(System.currentTimeMillis()));
+        new AuditStamp().setActor(Urn.createFromString(Constants.SYSTEM_ACTOR)).setTime(System.currentTimeMillis()));
 
     final MetadataChangeProposal proposal = new MetadataChangeProposal();
     proposal.setEntityUrn(urn);
@@ -102,7 +103,7 @@ private void ingestPolicy(final Urn urn, final DataHubPolicyInfo info) throws UR
     proposal.setChangeType(ChangeType.UPSERT);
 
     _entityService.ingestProposal(proposal,
-        new AuditStamp().setActor(Urn.createFromString("urn:li:corpuser:system")).setTime(System.currentTimeMillis()));
+        new AuditStamp().setActor(Urn.createFromString(Constants.SYSTEM_ACTOR)).setTime(System.currentTimeMillis()));
   }
 
   private boolean hasDefaultPolicies() throws URISyntaxException {

File: metadata-utils/src/main/java/com/linkedin/metadata/Constants.java
Patch:
@@ -6,7 +6,7 @@
 public class Constants {
   public static final String ACTOR_HEADER_NAME = "X-DataHub-Actor";
   public static final String DATAHUB_ACTOR = "urn:li:corpuser:datahub"; // Super user.
-  public static final String SYSTEM_ACTOR = "urn:li:principal:datahub"; // DataHub internal service principal.
+  public static final String SYSTEM_ACTOR = "urn:li:corpuser:__datahub_system"; // DataHub internal service principal.
   public static final String UNKNOWN_ACTOR = "urn:li:corpuser:UNKNOWN"; // Unknown principal.
   public static final Long ASPECT_LATEST_VERSION = 0L;
 

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/GmsGraphQLEngine.java
Patch:
@@ -48,6 +48,7 @@
 import com.linkedin.datahub.graphql.resolvers.mutate.MutableTypeResolver;
 import com.linkedin.datahub.graphql.resolvers.mutate.RemoveTagResolver;
 import com.linkedin.datahub.graphql.resolvers.mutate.RemoveTermResolver;
+import com.linkedin.datahub.graphql.resolvers.mutate.UpdateFieldDescriptionResolver;
 import com.linkedin.datahub.graphql.resolvers.policy.DeletePolicyResolver;
 import com.linkedin.datahub.graphql.resolvers.policy.ListPoliciesResolver;
 import com.linkedin.datahub.graphql.resolvers.config.AppConfigResolver;
@@ -390,6 +391,7 @@ private void configureMutationResolvers(final RuntimeWiring.Builder builder) {
             .dataFetcher("createPolicy", new UpsertPolicyResolver(GmsClientFactory.getAspectsClient()))
             .dataFetcher("updatePolicy", new UpsertPolicyResolver(GmsClientFactory.getAspectsClient()))
             .dataFetcher("deletePolicy", new DeletePolicyResolver(GmsClientFactory.getEntitiesClient()))
+            .dataFetcher("updateDescription", new AuthenticatedResolver<>(new UpdateFieldDescriptionResolver(entityService)))
         );
     }
 

File: datahub-frontend/app/auth/sso/oidc/OidcProvider.java
Patch:
@@ -53,7 +53,7 @@ private Client<OidcCredentials, OidcProfile> createPac4jClient() {
     oidcConfiguration.setClientAuthenticationMethodAsString(_oidcConfigs.getClientAuthenticationMethod());
     oidcConfiguration.setScope(_oidcConfigs.getScope());
     _oidcConfigs.getResponseType().ifPresent(oidcConfiguration::setResponseType);
-    _oidcConfigs.getResponseMode().ifPresent(oidcConfiguration::setResponseType);
+    _oidcConfigs.getResponseMode().ifPresent(oidcConfiguration::setResponseMode);
     _oidcConfigs.getUseNonce().ifPresent(oidcConfiguration::setUseNonce);
     _oidcConfigs.getCustomParamResource()
         .ifPresent(value -> oidcConfiguration.setCustomParams(ImmutableMap.of("resource", value)));

File: datahub-frontend/app/controllers/TrackingController.java
Patch:
@@ -127,6 +127,7 @@ private KafkaProducer createKafkaProducer() {
                 setConfig(props, SaslConfigs.SASL_MECHANISM, "analytics.kafka.sasl.mechanism");
                 setConfig(props, SaslConfigs.SASL_JAAS_CONFIG, "analytics.kafka.sasl.jaas.config");
                 setConfig(props, SaslConfigs.SASL_KERBEROS_SERVICE_NAME, "analytics.kafka.sasl.kerberos.service.name");
+                setConfig(props, SaslConfigs.SASL_LOGIN_CALLBACK_HANDLER_CLASS, "analytics.kafka.sasl.login.callback.handler.class");
             }
         }
 

File: metadata-io/src/main/java/com/linkedin/metadata/timeseries/elastic/ElasticSearchTimeseriesAspectService.java
Patch:
@@ -132,7 +132,7 @@ public List<EnvelopedAspect> getAspectValues(@Nonnull final Urn urn, @Nonnull St
     final BoolQueryBuilder filterQueryBuilder = ESUtils.buildFilterQuery(null);
     filterQueryBuilder.must(QueryBuilders.matchQuery("urn", urn.toString()));
     // NOTE: We are interested only in the un-exploded rows as only they carry the `event` payload.
-    filterQueryBuilder.must(QueryBuilders.termQuery(MappingsBuilder.IS_EXPLODED_FIELD, false));
+    filterQueryBuilder.mustNot(QueryBuilders.termQuery(MappingsBuilder.IS_EXPLODED_FIELD, true));
     if (startTimeMillis != null) {
       Criterion startTimeCriterion = new Criterion().setField(TIMESTAMP_FIELD)
           .setCondition(Condition.GREATER_THAN_OR_EQUAL_TO)

File: metadata-service/factories/src/main/java/com/linkedin/metadata/boot/BootstrapManager.java
Patch:
@@ -22,12 +22,12 @@ public void start() {
     // Once the application has been set up, apply boot steps.
     log.info("Starting Bootstrap Process...");
     for (int i = 0; i < _bootSteps.size(); i++) {
-      log.info(String.format("Executing Bootstrap Step %s/%s...", i, _bootSteps.size()));
       final BootstrapStep step = _bootSteps.get(i);
+      log.info(String.format("Executing bootstrap step %s/%s with name %s...", i + 1, _bootSteps.size(), step.name()));
       try {
         step.execute();
       } catch (Exception e) {
-        log.error(String.format("Caught exception while executing bootstrao step %s. Exiting...", step.name()), e);
+        log.error(String.format("Caught exception while executing bootstrap step %s. Exiting...", step.name()), e);
         System.exit(1);
       }
     }

File: metadata-utils/src/main/java/com/linkedin/metadata/authorization/PoliciesConfig.java
Patch:
@@ -82,6 +82,7 @@ public class PoliciesConfig {
 
   public static final List<Privilege> COMMON_ENTITY_PRIVILEGES = ImmutableList.of(
       EDIT_ENTITY_TAGS_PRIVILEGE,
+      EDIT_ENTITY_GLOSSARY_TERMS_PRIVILEGE,
       EDIT_ENTITY_OWNERS_PRIVILEGE,
       EDIT_ENTITY_DOCS_PRIVILEGE,
       EDIT_ENTITY_DOC_LINKS_PRIVILEGE,

File: datahub-frontend/app/auth/sso/oidc/OidcCallbackLogic.java
Patch:
@@ -214,7 +214,7 @@ private List<CorpGroupSnapshot> extractGroups(CommonProfile profile) {
     final OidcConfigs configs = (OidcConfigs) _ssoManager.getSsoProvider().configs();
 
     // First, attempt to extract a list of groups from the profile, using the group name attribute config.
-    final String groupsClaimName = configs.groupsClaimName();
+    final String groupsClaimName = configs.getGroupsClaimName();
     if (profile.containsAttribute(groupsClaimName)) {
       try {
         final List<CorpGroupSnapshot> groupSnapshots = new ArrayList<>();

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/tag/mappers/TagSnapshotMapper.java
Patch:
@@ -37,7 +37,6 @@ public Tag apply(@Nonnull final TagSnapshot tag) {
                 if (TagProperties.class.cast(aspect).hasDescription()) {
                     result.setDescription(TagProperties.class.cast(aspect).getDescription());
                 }
-                result.setName(TagProperties.class.cast(aspect).getName());
             } else if (aspect instanceof Ownership) {
                 result.setOwnership(OwnershipMapper.map((Ownership) aspect));
             }

File: metadata-io/src/main/java/com/linkedin/metadata/search/elasticsearch/query/request/SearchRequestHandler.java
Patch:
@@ -115,7 +115,7 @@ public SearchRequest getSearchRequest(@Nonnull String input, @Nullable Filter fi
     BoolQueryBuilder filterQuery = ESUtils.buildFilterQuery(filter);
     // Filter out entities that are marked "removed"
     filterQuery.mustNot(QueryBuilders.matchQuery("removed", true));
-    searchSourceBuilder.postFilter(filterQuery);
+    searchSourceBuilder.query(QueryBuilders.boolQuery().must(getQuery(input)).must(filterQuery));
     getAggregations(filter).forEach(searchSourceBuilder::aggregation);
     searchSourceBuilder.highlighter(getHighlights());
     ESUtils.buildSortOrder(searchSourceBuilder, sortCriterion);

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/GmsGraphQLEngine.java
Patch:
@@ -91,6 +91,7 @@
 import graphql.execution.DataFetcherResult;
 import graphql.schema.idl.RuntimeWiring;
 import java.util.ArrayList;
+import java.util.Collections;
 import org.apache.commons.io.IOUtils;
 import org.dataloader.BatchLoaderContextProvider;
 import org.dataloader.DataLoader;
@@ -743,7 +744,7 @@ private void configureMLFeatureTableResolvers(final RuntimeWiring.Builder builde
                                     .map(MLModelGroup::getUrn)
                                     .collect(Collectors.toList());
                             }
-                            return null;
+                            return Collections.emptyList();
                         }))
                 )
             )

File: metadata-service/graphql-api/src/main/java/com/datahub/metadata/graphql/GraphQLController.java
Patch:
@@ -64,11 +64,11 @@ CompletableFuture<ResponseEntity<String>> postGraphQL(HttpEntity<String> httpEnt
      * Extract "variables" map
      */
     JsonNode variablesJson = bodyJson.get("variables");
-    final Map<String, Object> variables = variablesJson != null
+    final Map<String, Object> variables = (variablesJson != null && !variablesJson.isNull())
       ? new ObjectMapper().convertValue(variablesJson, new TypeReference<Map<String, Object>>() { })
       : Collections.emptyMap();
 
-    log.debug(String.format("Executing graphQL query: %s, variables: %s", queryJson, variablesJson));
+    log.debug(String.format("Executing graphQL query: %s, variables: %s", queryJson, variables));
 
     /*
      * Init QueryContext

File: metadata-service/factories/src/main/java/com/linkedin/metadata/boot/BootstrapManagerApplicationListener.java
Patch:
@@ -22,7 +22,6 @@ public class BootstrapManagerApplicationListener implements ApplicationListener<
 
   @Override
   public void onApplicationEvent(@Nonnull ContextRefreshedEvent event) {
-    System.out.println("Started the application you idiot");
     _bootstrapManager.start();
   }
 }

File: metadata-ingestion-examples/common/src/main/java/com/linkedin/metadata/examples/configs/TopicConventionFactory.java
Patch:
@@ -29,7 +29,7 @@ public class TopicConventionFactory {
   @Value("${METADATA_CHANGE_PROPOSAL_TOPIC_NAME:" + Topics.METADATA_CHANGE_PROPOSAL + "}")
   private String metadataChangeProposalName;
 
-  @Value("${METADATA_CHANGE_LOG_TOPIC_NAME:" + Topics.METADATA_CHANGE_LOG_VERSIONED + "}")
+  @Value("${METADATA_CHANGE_LOG_VERSIONED_TOPIC_NAME:" + Topics.METADATA_CHANGE_LOG_VERSIONED + "}")
   private String metadataChangeLogName;
 
   @Value("${METADATA_CHANGE_LOG_LIMITED_TOPIC_NAME:" + Topics.METADATA_CHANGE_LOG_TIMESERIES + "}")

File: metadata-service/factories/src/main/java/com/linkedin/gms/factory/common/TopicConventionFactory.java
Patch:
@@ -29,7 +29,7 @@ public class TopicConventionFactory {
   @Value("${METADATA_CHANGE_PROPOSAL_TOPIC_NAME:" + Topics.METADATA_CHANGE_PROPOSAL + "}")
   private String metadataChangeProposalName;
 
-  @Value("${METADATA_CHANGE_LOG_TOPIC_NAME:" + Topics.METADATA_CHANGE_LOG_VERSIONED + "}")
+  @Value("${METADATA_CHANGE_LOG_VERSIONED_TOPIC_NAME:" + Topics.METADATA_CHANGE_LOG_VERSIONED + "}")
   private String metadataChangeLogName;
 
   @Value("${METADATA_CHANGE_LOG_LIMITED_TOPIC_NAME:" + Topics.METADATA_CHANGE_LOG_TIMESERIES + "}")

File: metadata-io/src/main/java/com/linkedin/metadata/search/elasticsearch/query/request/SearchRequestHandler.java
Patch:
@@ -110,11 +110,12 @@ public SearchRequest getSearchRequest(@Nonnull String input, @Nullable Filter fi
     searchSourceBuilder.from(from);
     searchSourceBuilder.size(size);
 
+    searchSourceBuilder.query(getQuery(input));
+
     BoolQueryBuilder filterQuery = ESUtils.buildFilterQuery(filter);
     // Filter out entities that are marked "removed"
     filterQuery.mustNot(QueryBuilders.matchQuery("removed", true));
-    searchSourceBuilder.query(QueryBuilders.boolQuery().should(getQuery(input)).must(filterQuery));
-
+    searchSourceBuilder.postFilter(filterQuery);
     getAggregations(filter).forEach(searchSourceBuilder::aggregation);
     searchSourceBuilder.highlighter(getHighlights());
     ESUtils.buildSortOrder(searchSourceBuilder, sortCriterion);

File: metadata-service/graphql-api/src/main/java/com/datahub/metadata/graphql/GraphQLController.java
Patch:
@@ -29,7 +29,7 @@ public class GraphQLController {
   @Inject
   GraphQLEngine _engine;
 
-  @PostMapping("/graphql")
+  @PostMapping(value = "/graphql", produces = "application/json;charset=utf-8")
   CompletableFuture<ResponseEntity<String>> postGraphQL(HttpEntity<String> httpEntity) {
 
     String jsonStr = httpEntity.getBody();

File: metadata-io/src/main/java/com/linkedin/metadata/search/elasticsearch/query/request/SearchRequestHandler.java
Patch:
@@ -110,12 +110,11 @@ public SearchRequest getSearchRequest(@Nonnull String input, @Nullable Filter fi
     searchSourceBuilder.from(from);
     searchSourceBuilder.size(size);
 
-    searchSourceBuilder.query(getQuery(input));
-
     BoolQueryBuilder filterQuery = ESUtils.buildFilterQuery(filter);
     // Filter out entities that are marked "removed"
     filterQuery.mustNot(QueryBuilders.matchQuery("removed", true));
-    searchSourceBuilder.postFilter(filterQuery);
+    searchSourceBuilder.query(QueryBuilders.boolQuery().should(getQuery(input)).must(filterQuery));
+
     getAggregations(filter).forEach(searchSourceBuilder::aggregation);
     searchSourceBuilder.highlighter(getHighlights());
     ESUtils.buildSortOrder(searchSourceBuilder, sortCriterion);

File: metadata-jobs/mae-consumer/src/main/java/com/linkedin/metadata/kafka/MetadataAuditEventsProcessor.java
Patch:
@@ -81,8 +81,8 @@ public MetadataAuditEventsProcessor(GraphService graphService, SearchService sea
     _systemMetadataService.configure();
   }
 
-  @KafkaListener(id = "${KAFKA_CONSUMER_GROUP_ID:mae-consumer-job-client}", topics = "${KAFKA_TOPIC_NAME:"
-      + Topics.METADATA_AUDIT_EVENT + "}", containerFactory = "avroSerializedKafkaListener")
+  @KafkaListener(id = "${METADATA_AUDIT_EVENT_KAFKA_CONSUMER_GROUP_ID:mae-consumer-job-client}", topics =
+      "${KAFKA_TOPIC_NAME:" + Topics.METADATA_AUDIT_EVENT + "}", containerFactory = "avroSerializedKafkaListener")
   public void consume(final ConsumerRecord<String, GenericRecord> consumerRecord) {
     final GenericRecord record = consumerRecord.value();
     log.debug("Got MAE");

File: metadata-jobs/mce-consumer/src/main/java/com/linkedin/metadata/kafka/MetadataChangeEventsProcessor.java
Patch:
@@ -1,7 +1,7 @@
 package com.linkedin.metadata.kafka;
 
-import com.linkedin.entity.client.EntityClient;
 import com.linkedin.entity.Entity;
+import com.linkedin.entity.client.EntityClient;
 import com.linkedin.metadata.EventUtils;
 import com.linkedin.metadata.kafka.config.MetadataChangeProposalProcessorCondition;
 import com.linkedin.metadata.snapshot.Snapshot;
@@ -42,8 +42,8 @@ public MetadataChangeEventsProcessor(
     this.kafkaTemplate = kafkaTemplate;
   }
 
-  @KafkaListener(id = "${KAFKA_CONSUMER_GROUP_ID:mce-consumer-job-client}", topics = "${KAFKA_MCE_TOPIC_NAME:"
-      + Topics.METADATA_CHANGE_EVENT + "}", containerFactory = "mceKafkaContainerFactory")
+  @KafkaListener(id = "${METADATA_CHANGE_EVENT_KAFKA_CONSUMER_GROUP_ID:mce-consumer-job-client}", topics =
+      "${KAFKA_MCE_TOPIC_NAME:" + Topics.METADATA_CHANGE_EVENT + "}", containerFactory = "mceKafkaContainerFactory")
   public void consume(final ConsumerRecord<String, GenericRecord> consumerRecord) {
     final GenericRecord record = consumerRecord.value();
     log.debug("Record ", record);

File: datahub-upgrade/src/main/java/com/linkedin/datahub/upgrade/UpgradeCliApplication.java
Patch:
@@ -5,9 +5,11 @@
 import org.springframework.boot.autoconfigure.elasticsearch.rest.RestClientAutoConfiguration;
 import org.springframework.boot.builder.SpringApplicationBuilder;
 
+
 @SuppressWarnings("checkstyle:HideUtilityClassConstructor")
 @SpringBootApplication(exclude = {RestClientAutoConfiguration.class}, scanBasePackages = {
-    "com.linkedin.gms.factory.common", "com.linkedin.gms.factory.search", "com.linkedin.datahub.upgrade.config", "com.linkedin.gms.factory.entity"})
+    "com.linkedin.gms.factory.common", "com.linkedin.gms.factory.kafka", "com.linkedin.gms.factory.search",
+    "com.linkedin.datahub.upgrade.config", "com.linkedin.gms.factory.entity"})
 public class UpgradeCliApplication {
   public static void main(String[] args) {
     new SpringApplicationBuilder(UpgradeCliApplication.class, UpgradeCli.class).web(WebApplicationType.NONE).run(args);

File: metadata-jobs/mae-consumer/src/main/java/com/linkedin/metadata/kafka/MetadataAuditEventsProcessor.java
Patch:
@@ -6,6 +6,7 @@
 import com.linkedin.gms.factory.common.GraphServiceFactory;
 import com.linkedin.gms.factory.common.SystemMetadataServiceFactory;
 import com.linkedin.gms.factory.kafka.KafkaEventConsumerFactory;
+import com.linkedin.gms.factory.kafka.SimpleKafkaConsumerFactory;
 import com.linkedin.gms.factory.search.SearchServiceFactory;
 import com.linkedin.gms.factory.usage.UsageServiceFactory;
 import com.linkedin.metadata.EventUtils;
@@ -59,7 +60,7 @@
 @Component
 @Conditional(MetadataChangeLogProcessorCondition.class)
 @Import({GraphServiceFactory.class, SearchServiceFactory.class, UsageServiceFactory.class,
-    SystemMetadataServiceFactory.class, KafkaEventConsumerFactory.class})
+    SystemMetadataServiceFactory.class, KafkaEventConsumerFactory.class, SimpleKafkaConsumerFactory.class})
 @EnableKafka
 public class MetadataAuditEventsProcessor {
 

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/glossary/mappers/GlossaryTermSnapshotMapper.java
Patch:
@@ -31,7 +31,7 @@ public GlossaryTerm apply(@Nonnull final GlossaryTermSnapshot glossaryTerm) {
         result.setUrn(glossaryTerm.getUrn().toString());
         result.setType(EntityType.GLOSSARY_TERM);
         result.setName(GlossaryTermUtils.getGlossaryTermName(glossaryTerm.getUrn().getNameEntity()));
-
+        result.setHierarchicalName(glossaryTerm.getUrn().getNameEntity());
         ModelUtils.getAspectsFromSnapshot(glossaryTerm).forEach(aspect -> {
             if (aspect instanceof GlossaryTermInfo) {
                 result.setGlossaryTermInfo(GlossaryTermInfoMapper.map(GlossaryTermInfo.class.cast(aspect)));

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/chart/mappers/ChartSnapshotMapper.java
Patch:
@@ -84,7 +84,9 @@ private ChartInfo mapChartInfo(final com.linkedin.chart.ChartInfo info) {
         if (info.getLastModified().hasDeleted()) {
             result.setDeleted(AuditStampMapper.map(info.getLastModified().getDeleted()));
         }
-        if (info.hasChartUrl()) {
+        if (info.hasExternalUrl()) {
+            result.setExternalUrl(info.getExternalUrl().toString());
+        } else if (info.hasChartUrl()) {
             // TODO: Migrate to using the External URL field for consistency.
             result.setExternalUrl(info.getChartUrl().toString());
         }

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/dashboard/mappers/DashboardSnapshotMapper.java
Patch:
@@ -70,6 +70,8 @@ private DashboardInfo mapDashboardInfo(final com.linkedin.dashboard.DashboardInf
             return chart;
         }).collect(Collectors.toList()));
         if (info.hasExternalUrl()) {
+            result.setExternalUrl(info.getExternalUrl().toString());
+        } else if (info.hasDashboardUrl()) {
             // TODO: Migrate to using the External URL field for consistency.
             result.setExternalUrl(info.getDashboardUrl().toString());
         }

File: metadata-io/src/main/java/com/linkedin/metadata/extractor/AspectExtractor.java
Patch:
@@ -33,6 +33,9 @@ public static Map<String, DataElement> extractAspects(RecordTemplate snapshot) {
     final Map<String, DataElement> aspectsByName = new HashMap<>();
 
     for (DataElement dataElement = iterator.next(); dataElement != null; dataElement = iterator.next()) {
+      if (dataElement.getSchemaPathSpec() == null) {
+        continue;
+      }
       final PathSpec pathSpec = dataElement.getSchemaPathSpec();
       List<String> pathComponents = pathSpec.getPathComponents();
       // three components representing /aspect/*/<aspectClassName>

File: metadata-io/src/main/java/com/linkedin/metadata/systemmetadata/SystemMetadataService.java
Patch:
@@ -4,14 +4,15 @@
 import com.linkedin.metadata.run.IngestionRunSummary;
 import com.linkedin.mxe.SystemMetadata;
 import java.util.List;
+import javax.annotation.Nullable;
 
 
 public interface SystemMetadataService {
   Boolean delete(String urn, String aspect);
 
   void deleteUrn(String finalOldUrn);
 
-  void insert(SystemMetadata systemMetadata, String urn, String aspect);
+  void insert(@Nullable SystemMetadata systemMetadata, String urn, String aspect);
 
   List<AspectRowSummary> findByRunId(String runId);
 

File: gms/impl/src/main/java/com/linkedin/metadata/resources/entity/AspectResource.java
Patch:
@@ -34,9 +34,9 @@
 import javax.inject.Named;
 import lombok.extern.slf4j.Slf4j;
 
+import static com.linkedin.metadata.resources.ResourceUtils.*;
 import static com.linkedin.metadata.restli.RestliConstants.*;
 
-
 /**
  * Single unified resource for fetching, updating, searching, & browsing DataHub entities
  */
@@ -79,6 +79,8 @@ public Task<VersionedAspect> get(@Nonnull String urnStr, @QueryParam("aspect") @
       final VersionedAspect aspect = _entityService.getVersionedAspect(urn, aspectName, version);
       if (aspect == null) {
         throw RestliUtils.resourceNotFoundException();
+      } else {
+        validateOrWarn(aspect);
       }
       return aspect;
     });

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/dataplatform/DataPlatformType.java
Patch:
@@ -7,7 +7,7 @@
 import com.linkedin.datahub.graphql.types.dataplatform.mappers.DataPlatformSnapshotMapper;
 
 import com.linkedin.entity.client.EntityClient;
-import com.linkedin.metadata.extractor.SnapshotToAspectMap;
+import com.linkedin.metadata.extractor.AspectExtractor;
 import graphql.execution.DataFetcherResult;
 import java.net.URISyntaxException;
 import java.util.ArrayList;
@@ -57,7 +57,8 @@ public List<DataFetcherResult<DataPlatform>> batchLoad(final List<String> urns,
                 .map(gmsPlatform -> gmsPlatform == null ? null
                     : DataFetcherResult.<DataPlatform>newResult()
                         .data(DataPlatformSnapshotMapper.map(gmsPlatform.getValue().getDataPlatformSnapshot()))
-                        .localContext(SnapshotToAspectMap.extractAspectMap(gmsPlatform.getValue().getDataPlatformSnapshot()))
+                        .localContext(AspectExtractor.extractAspects(
+                            gmsPlatform.getValue().getDataPlatformSnapshot()))
                         .build())
                 .collect(Collectors.toList());
         } catch (Exception e) {

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/GmsGraphQLEngine.java
Patch:
@@ -112,7 +112,7 @@ public class GmsGraphQLEngine {
     public static final CorpGroupType CORP_GROUP_TYPE = new CorpGroupType(GmsClientFactory.getEntitiesClient());
     public static final ChartType CHART_TYPE = new ChartType(GmsClientFactory.getEntitiesClient());
     public static final DashboardType DASHBOARD_TYPE = new DashboardType(GmsClientFactory.getEntitiesClient());
-    public static final DataPlatformType DATA_PLATFORM_TYPE = new DataPlatformType(GmsClientFactory.getDataPlatformsClient());
+    public static final DataPlatformType DATA_PLATFORM_TYPE = new DataPlatformType(GmsClientFactory.getEntitiesClient());
     public static final DownstreamLineageType DOWNSTREAM_LINEAGE_TYPE = new DownstreamLineageType(
             GmsClientFactory.getLineagesClient()
     );

File: gms/impl/src/main/java/com/linkedin/metadata/resources/dashboard/Dashboards.java
Patch:
@@ -34,6 +34,7 @@
 import com.linkedin.metadata.search.SearchService;
 import com.linkedin.metadata.snapshot.DashboardSnapshot;
 import com.linkedin.metadata.snapshot.Snapshot;
+import com.linkedin.metadata.utils.BrowseUtil;
 import com.linkedin.parseq.Task;
 import com.linkedin.restli.common.ComplexResourceKey;
 import com.linkedin.restli.common.EmptyRecord;
@@ -319,7 +320,8 @@ public Task<AutoCompleteResult> autocomplete(@ActionParam(PARAM_QUERY) @Nonnull
   public Task<BrowseResult> browse(@ActionParam(PARAM_PATH) @Nonnull String path,
       @ActionParam(PARAM_FILTER) @Optional @Nullable Filter filter, @ActionParam(PARAM_START) int start,
       @ActionParam(PARAM_LIMIT) int limit) {
-    return RestliUtils.toTask(() -> _searchService.browse("dashboard", path, filter, start, limit));
+    return RestliUtils.toTask(
+        () -> BrowseUtil.convertToLegacyResult(_searchService.browse("dashboard", path, filter, start, limit)));
   }
 
   @Action(name = ACTION_GET_BROWSE_PATHS)

File: metadata-builders/src/main/java/com/linkedin/metadata/builders/search/GlossaryTermInfoIndexBuilder.java
Patch:
@@ -32,7 +32,7 @@ private GlossaryTermInfoDocument getDocumentToUpdateFromAspect(@Nonnull Glossary
   }
 
   @Nonnull
-  private static String buildBrowsePath(@Nonnull GlossaryTermUrn urn) {
+  public static String buildBrowsePath(@Nonnull GlossaryTermUrn urn) {
     return "/" + urn.getNameEntity().replace('.', '/').toLowerCase();
   }
 

File: metadata-io/src/main/java/com/linkedin/metadata/search/elasticsearch/indexbuilder/IndexBuilder.java
Patch:
@@ -99,9 +99,10 @@ public void buildIndex() throws IOException {
     }
 
     log.info("Reindex from {} to {} succeeded", indexName, tempIndexName);
+    String indexNamePattern = indexName + "_*";
     // Check if the original index is aliased or not
     GetAliasesResponse aliasesResponse =
-        searchClient.indices().getAlias(new GetAliasesRequest(indexName), RequestOptions.DEFAULT);
+        searchClient.indices().getAlias(new GetAliasesRequest(indexName).indices(indexNamePattern), RequestOptions.DEFAULT);
     // If not aliased, delete the original index
     if (aliasesResponse.getAliases().isEmpty()) {
       searchClient.indices().delete(new DeleteIndexRequest().indices(indexName), RequestOptions.DEFAULT);
@@ -112,7 +113,7 @@ public void buildIndex() throws IOException {
     }
 
     // Add alias for the new index
-    AliasActions removeAction = AliasActions.remove().alias(indexName).index("*");
+    AliasActions removeAction = AliasActions.remove().alias(indexName).index(indexNamePattern);
     AliasActions addAction = AliasActions.add().alias(indexName).index(tempIndexName);
     searchClient.indices()
         .updateAliases(new IndicesAliasesRequest().addAliasAction(removeAction).addAliasAction(addAction),

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/aspect/AspectType.java
Patch:
@@ -35,7 +35,7 @@ public List<DataFetcherResult<Aspect>> batchLoad(@Nonnull List<VersionedAspectKe
           if (e instanceof RestLiResponseException) {
             // if no aspect is found, restli will return a 404 rather than null
             // https://linkedin.github.io/rest.li/user_guide/restli_server#returning-nulls
-            if(((RestLiResponseException) e).getStatus() == 404) {
+            if (((RestLiResponseException) e).getStatus() == 404) {
               return DataFetcherResult.<Aspect>newResult().data(null).build();
             }
           }

File: metadata-jobs/mae-consumer/src/main/java/com/linkedin/metadata/kafka/MetadataAuditEventsProcessor.java
Patch:
@@ -5,6 +5,7 @@
 import com.linkedin.data.template.RecordTemplate;
 import com.linkedin.gms.factory.common.GraphServiceFactory;
 import com.linkedin.gms.factory.search.SearchServiceFactory;
+import com.linkedin.gms.factory.usage.UsageServiceFactory;
 import com.linkedin.metadata.EventUtils;
 import com.linkedin.metadata.PegasusUtils;
 import com.linkedin.metadata.dao.utils.RecordUtils;
@@ -49,7 +50,7 @@
 @Slf4j
 @Component
 @Conditional(MetadataAuditEventsProcessorCondition.class)
-@Import({GraphServiceFactory.class, SearchServiceFactory.class})
+@Import({GraphServiceFactory.class, SearchServiceFactory.class, UsageServiceFactory.class})
 @EnableKafka
 public class MetadataAuditEventsProcessor {
 

File: gms/impl/src/test/java/com/linkedin/metadata/resources/dataplatform/utils/DataPlatformsUtilTest.java
Patch:
@@ -30,7 +30,7 @@ public void testGetPlatformType() {
     assertPlatformType("hdfs", PlatformType.FILE_SYSTEM);
     assertPlatformType("hive", PlatformType.FILE_SYSTEM);
     assertPlatformType("kafka", PlatformType.MESSAGE_BROKER);
-    assertPlatformType("mongo", PlatformType.KEY_VALUE_STORE);
+    assertPlatformType("mongodb", PlatformType.KEY_VALUE_STORE);
     assertPlatformType("mysql", PlatformType.RELATIONAL_DB);
     assertPlatformType("oracle", PlatformType.RELATIONAL_DB);
     assertPlatformType("pinot", PlatformType.OLAP_DATASTORE);
@@ -66,7 +66,7 @@ public void testGetPlatformDelimiter() {
     assertEquals(DataPlatformsUtil.getPlatformDelimiter("hdfs").get(), "/");
     assertEquals(DataPlatformsUtil.getPlatformDelimiter("hive").get(), ".");
     assertEquals(DataPlatformsUtil.getPlatformDelimiter("kafka").get(), ".");
-    assertEquals(DataPlatformsUtil.getPlatformDelimiter("mongo").get(), ".");
+    assertEquals(DataPlatformsUtil.getPlatformDelimiter("mongodb").get(), ".");
     assertEquals(DataPlatformsUtil.getPlatformDelimiter("mysql").get(), ".");
     assertEquals(DataPlatformsUtil.getPlatformDelimiter("oracle").get(), ".");
     assertEquals(DataPlatformsUtil.getPlatformDelimiter("pinot").get(), ".");

File: metadata-utils/src/main/java/com/linkedin/metadata/utils/elasticsearch/ElasticsearchUtil.java
Patch:
@@ -25,7 +25,7 @@ public enum AccessCountType {
       .put("hive", '.')
       .put("kafka", '.')
       .put("kafka-lc", '.')
-      .put("mongo",  '.')
+      .put("mongodb",  '.')
       .put("mysql", '.')
       .put("oracle", '.')
       .put("pinot", '.')

File: datahub-upgrade/src/main/java/com/linkedin/datahub/upgrade/nocode/NoCodeUpgrade.java
Patch:
@@ -1,13 +1,13 @@
 package com.linkedin.datahub.upgrade.nocode;
 
-import com.google.common.collect.ImmutableList;
 import com.linkedin.datahub.upgrade.Upgrade;
 import com.linkedin.datahub.upgrade.UpgradeCleanupStep;
 import com.linkedin.datahub.upgrade.UpgradeStep;
 import com.linkedin.metadata.entity.EntityService;
 import com.linkedin.metadata.models.registry.SnapshotEntityRegistry;
 import io.ebean.EbeanServer;
 import java.util.ArrayList;
+import java.util.Collections;
 import java.util.List;
 
 public class NoCodeUpgrade implements Upgrade {
@@ -48,7 +48,7 @@ public List<UpgradeCleanupStep> cleanupSteps() {
   }
 
   private List<UpgradeCleanupStep> buildCleanupSteps(final EbeanServer server) {
-    return ImmutableList.of(new CleanupStep(server));
+    return Collections.emptyList();
   }
 
   private List<UpgradeStep> buildUpgradeSteps(

File: datahub-frontend/app/react/resolver/GetHighlightsResolver.java
Patch:
@@ -64,7 +64,7 @@ private List<Highlight> getHighlights() {
       String directionChange = percentChange > 0 ? "increase" : "decrease";
 
       bodyText = Double.isInfinite(percentChange) ? ""
-          : String.format("%%%.2f %s from last week", percentChange, directionChange);
+          : String.format("%.2f%% %s from last week", percentChange, directionChange);
     }
 
     highlights.add(Highlight.builder().setTitle(title).setValue(weeklyActiveUsers).setBody(bodyText).build());

File: datahub-frontend/app/react/auth/AuthModule.java
Patch:
@@ -144,7 +144,7 @@ private Result handleOidcCallback(final Result result, final PlayWebContext cont
                 context.getJavaSession().put(ACTOR, actorUrn);
                 return result.withCookies(createActorCookie(actorUrn, _configs.hasPath(SESSION_TTL_CONFIG_PATH)
                         ? _configs.getInt(SESSION_TTL_CONFIG_PATH)
-                        : DEFAULT_SESSION_TTL_DAYS));
+                        : DEFAULT_SESSION_TTL_HOURS));
             } else {
                 throw new RuntimeException(
                         String.format("Failed to extract DataHub username from username claim %s using regex %s",

File: datahub-frontend/app/react/controllers/AuthenticationController.java
Patch:
@@ -78,7 +78,7 @@ public Result authenticate() {
         session().put(ACTOR, DEFAULT_ACTOR_URN.toString());
         return redirect("/").withCookies(createActorCookie(DEFAULT_ACTOR_URN.toString(), _configs.hasPath(SESSION_TTL_CONFIG_PATH)
                 ? _configs.getInt(SESSION_TTL_CONFIG_PATH)
-                : DEFAULT_SESSION_TTL_DAYS));
+                : DEFAULT_SESSION_TTL_HOURS));
     }
 
     /**

File: gms/client/src/main/java/com/linkedin/entity/client/EntityClient.java
Patch:
@@ -8,7 +8,6 @@
 import com.linkedin.data.template.DataTemplateUtil;
 import com.linkedin.data.template.DynamicRecordMetadata;
 import com.linkedin.data.template.FieldDef;
-import com.linkedin.data.template.RecordTemplate;
 import com.linkedin.data.template.StringArray;
 import com.linkedin.entity.EntitiesDoAutocompleteRequestBuilder;
 import com.linkedin.entity.EntitiesDoBrowseRequestBuilder;
@@ -78,7 +77,7 @@ private <T> Response<T> sendClientRequest(Request<T> request) throws RemoteInvoc
     }
 
     @Nonnull
-    public RecordTemplate get(@Nonnull final Urn urn) throws RemoteInvocationException {
+    public Entity get(@Nonnull final Urn urn) throws RemoteInvocationException {
         final GetRequest<Entity> getRequest = ENTITIES_REQUEST_BUILDERS.get()
                 .id(urn.toString())
                 .build();

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/ResolverUtils.java
Patch:
@@ -96,7 +96,7 @@ public static VersionedAspect getAspectFromLocalContext(DataFetchingEnvironment
         Object localContext = environment.getLocalContext();
         // if we have context & the version is 0, we should try to retrieve it from the fetched entity
         // otherwise, we should just fetch the entity from the aspect resource
-        if (localContext == null && version == 0 || version == null) {
+        if (localContext != null && version == 0 || version == null) {
             if (localContext instanceof Map) {
                 // de-register the prefetched aspect from local context. Since aspects will only
                 // ever be first-class properties of an entity type, local context will always

File: gms/impl/src/main/java/com/linkedin/metadata/resources/dataplatform/DataPlatforms.java
Patch:
@@ -95,6 +95,7 @@ public Task<DataPlatform> get(
   @RestMethod.GetAll
   public Task<List<DataPlatform>> getAllDataPlatforms(@Nonnull @PagingContextParam(defaultCount = 100) PagingContext pagingContext) {
     return Task.value(_entityService.listLatestAspects(
+        "dataPlatform",
         "dataPlatformInfo",
         pagingContext.getStart(),
         pagingContext.getCount())

File: metadata-io/src/main/java/com/linkedin/metadata/entity/ebean/EbeanEntityService.java
Patch:
@@ -138,11 +138,12 @@ public VersionedAspect getVersionedAspect(@Nonnull Urn urn, @Nonnull String aspe
   @Override
   @Nonnull
   public ListResult<RecordTemplate> listLatestAspects(
+      @Nonnull final String entityName,
       @Nonnull final String aspectName,
       final int start,
       int count) {
 
-    final ListResult<String> aspectMetadataList = _entityDao.listLatestAspectMetadata(aspectName, start, count);
+    final ListResult<String> aspectMetadataList = _entityDao.listLatestAspectMetadata(entityName, aspectName, start, count);
 
     final List<RecordTemplate> aspects = new ArrayList<>();
     for (int i = 0; i < aspectMetadataList.getValues().size(); i++) {

File: metadata-io/src/test/java/com/linkedin/metadata/entity/EbeanEntityServiceTest.java
Patch:
@@ -201,7 +201,7 @@ public void testIngestListLatestAspects() throws Exception {
     _entityService.ingestAspect(entityUrn3, aspectName, writeAspect3, TEST_AUDIT_STAMP);
 
     // List aspects
-    ListResult<RecordTemplate> batch1 = _entityService.listLatestAspects(aspectName, 0, 2);
+    ListResult<RecordTemplate> batch1 = _entityService.listLatestAspects(entityUrn1.getEntityType(), aspectName, 0, 2);
 
     assertEquals(2, batch1.getNextStart());
     assertEquals(2, batch1.getPageSize());
@@ -211,7 +211,7 @@ public void testIngestListLatestAspects() throws Exception {
     assertTrue(DataTemplateUtil.areEqual(writeAspect1,  batch1.getValues().get(0)));
     assertTrue(DataTemplateUtil.areEqual(writeAspect2,  batch1.getValues().get(1)));
 
-    ListResult<RecordTemplate> batch2 = _entityService.listLatestAspects(aspectName, 2, 2);
+    ListResult<RecordTemplate> batch2 = _entityService.listLatestAspects(entityUrn1.getEntityType(), aspectName, 2, 2);
     assertEquals(1, batch2.getValues().size());
     assertTrue(DataTemplateUtil.areEqual(writeAspect3,  batch2.getValues().get(0)));
   }

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/dataflow/mappers/DataFlowSnapshotMapper.java
Patch:
@@ -12,7 +12,7 @@
 import com.linkedin.datahub.graphql.types.common.mappers.StringMapMapper;
 import com.linkedin.datahub.graphql.types.mappers.ModelMapper;
 import com.linkedin.datahub.graphql.types.tag.mappers.GlobalTagsMapper;
-import com.linkedin.datajob.EditableDataflowProperties;
+import com.linkedin.datajob.EditableDataFlowProperties;
 import com.linkedin.metadata.dao.utils.ModelUtils;
 import com.linkedin.metadata.snapshot.DataFlowSnapshot;
 import javax.annotation.Nonnull;
@@ -47,9 +47,9 @@ public DataFlow apply(@Nonnull final DataFlowSnapshot dataflow) {
                 result.setStatus(StatusMapper.map(status));
             } else if (aspect instanceof GlobalTags) {
                 result.setGlobalTags(GlobalTagsMapper.map(GlobalTags.class.cast(aspect)));
-            } else if (aspect instanceof EditableDataflowProperties) {
+            } else if (aspect instanceof EditableDataFlowProperties) {
                 final DataFlowEditableProperties dataFlowEditableProperties = new DataFlowEditableProperties();
-                dataFlowEditableProperties.setDescription(((EditableDataflowProperties) aspect).getDescription());
+                dataFlowEditableProperties.setDescription(((EditableDataFlowProperties) aspect).getDescription());
                 result.setEditableProperties(dataFlowEditableProperties);
             }
         });

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/datajob/mappers/DataJobSnapshotMapper.java
Patch:
@@ -16,7 +16,7 @@
 import com.linkedin.datahub.graphql.types.common.mappers.StringMapMapper;
 import com.linkedin.datahub.graphql.types.mappers.ModelMapper;
 import com.linkedin.datahub.graphql.types.tag.mappers.GlobalTagsMapper;
-import com.linkedin.datajob.EditableDatajobProperties;
+import com.linkedin.datajob.EditableDataJobProperties;
 import com.linkedin.metadata.dao.utils.ModelUtils;
 import com.linkedin.metadata.snapshot.DataJobSnapshot;
 import java.util.stream.Collectors;
@@ -54,9 +54,9 @@ public DataJob apply(@Nonnull final DataJobSnapshot dataJob) {
                 result.setStatus(StatusMapper.map(status));
             } else if (aspect instanceof GlobalTags) {
                 result.setGlobalTags(GlobalTagsMapper.map(GlobalTags.class.cast(aspect)));
-            } else if (aspect instanceof EditableDatajobProperties) {
+            } else if (aspect instanceof EditableDataJobProperties) {
                 final DataJobEditableProperties dataJobEditableProperties = new DataJobEditableProperties();
-                dataJobEditableProperties.setDescription(((EditableDatajobProperties) aspect).getDescription());
+                dataJobEditableProperties.setDescription(((EditableDataJobProperties) aspect).getDescription());
                 result.setEditableProperties(dataJobEditableProperties);
             }
         });

File: metadata-io/src/main/java/com/linkedin/metadata/search/elasticsearch/indexbuilder/MappingsBuilder.java
Patch:
@@ -71,7 +71,9 @@ private static Map<String, Object> getMappingsForField(@Nonnull final Searchable
         // Add keyword subfield with fielddata set to true for aggregation queries
         subFields.put("keyword", ImmutableMap.of("type", "text", "analyzer", "urn_component", "fielddata", true));
       }
-      mappingForField.put("fields", subFields);
+      if (!subFields.isEmpty()) {
+        mappingForField.put("fields", subFields);
+      }
     } else if (fieldType == FieldType.BOOLEAN) {
       mappingForField.put("type", "boolean");
     } else if (fieldType == FieldType.COUNT) {
@@ -90,5 +92,4 @@ private static Map<String, Object> getMappingsForField(@Nonnull final Searchable
 
     return mappings;
   }
-
 }

File: metadata-io/src/test/java/com/linkedin/metadata/search/elasticsearch/indexbuilder/MappingsBuilderTest.java
Patch:
@@ -6,6 +6,7 @@
 import org.testng.annotations.Test;
 
 import static org.testng.Assert.assertEquals;
+import static org.testng.Assert.assertFalse;
 import static org.testng.Assert.assertTrue;
 
 
@@ -52,8 +53,7 @@ public void testMappingsBuilder() {
     Map<String, Object> foreignKey = (Map<String, Object>) properties.get("foreignKey");
     assertEquals(foreignKey.get("type"), "text");
     assertEquals(foreignKey.get("analyzer"), "urn_component");
-    Map<String, Object> foreignKeySubfields = (Map<String, Object>) foreignKey.get("fields");
-    assertTrue(foreignKeySubfields.isEmpty());
+    assertFalse(foreignKey.containsKey("fields"));
 
     // URN_PARTIAL
     Map<String, Object> nestedForeignKey = (Map<String, Object>) properties.get("nestedForeignKey");

File: metadata-utils/src/main/java/com/linkedin/metadata/utils/elasticsearch/IndexConventionImpl.java
Patch:
@@ -23,7 +23,7 @@ public IndexConventionImpl(@Nullable String prefix) {
   }
 
   private String createIndexName(String baseName) {
-    return (_prefix.map(prefix -> prefix + "_").orElse("") + baseName + SUFFIX + "_" + VERSION).toLowerCase();
+    return (_prefix.map(prefix -> prefix + "_").orElse("") + baseName).toLowerCase();
   }
 
   @Nonnull
@@ -35,7 +35,7 @@ public String getIndexName(Class<? extends RecordTemplate> documentClass) {
   @Nonnull
   @Override
   public String getIndexName(EntitySpec entitySpec) {
-    return this.getIndexName(entitySpec.getName());
+    return this.getIndexName(entitySpec.getName()) + SUFFIX + "_" + VERSION;
   }
 
   @Nonnull

File: metadata-models/src/test/java/com/linkedin/metadata/ModelValidation.java
Patch:
@@ -10,7 +10,6 @@
 import com.linkedin.metadata.validator.RelationshipValidator;
 import com.linkedin.metadata.validator.SnapshotValidator;
 import java.io.IOException;
-import java.util.Collection;
 import java.util.List;
 import java.util.Set;
 import java.util.stream.Collectors;
@@ -67,7 +66,6 @@ public void validateSnapshots() throws Exception {
 
     assertFalse("Failed to find any snapshots", snapshots.isEmpty());
     snapshots.forEach(SnapshotValidator::validateSnapshotSchema);
-    SnapshotValidator.validateUniqueUrn((Collection<Class<? extends RecordTemplate>>) snapshots);
   }
 
   @Test

File: metadata-io/src/main/java/com/linkedin/metadata/entity/ebean/EbeanAspectDao.java
Patch:
@@ -99,7 +99,8 @@ private boolean validateConnection() {
       return true;
     }
     if (!AspectStorageValidationUtil.checkV2TableExists(_server)) {
-      _logger.error("GMS is on a newer version than your storage layer. Please refer to /docs/advanced/no-code-upgrade.md for an easy upgrade guide");
+      _logger.error("GMS is on a newer version than your storage layer. Please refer to "
+                    + "https://datahubproject.io/docs/advanced/no-code-upgrade for an easy upgrade guide");
       _canWrite = false;
       return false;
     } else {

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/GmsGraphQLEngine.java
Patch:
@@ -35,6 +35,7 @@
 import com.linkedin.datahub.graphql.resolvers.browse.BrowsePathsResolver;
 import com.linkedin.datahub.graphql.resolvers.browse.BrowseResolver;
 import com.linkedin.datahub.graphql.resolvers.search.AutoCompleteResolver;
+import com.linkedin.datahub.graphql.resolvers.search.AutoCompleteForAllResolver;
 import com.linkedin.datahub.graphql.resolvers.search.SearchResolver;
 import com.linkedin.datahub.graphql.resolvers.type.EntityInterfaceTypeResolver;
 import com.linkedin.datahub.graphql.resolvers.type.PlatformSchemaUnionTypeResolver;
@@ -213,6 +214,8 @@ private static void configureQueryResolvers(final RuntimeWiring.Builder builder)
                         new SearchResolver(SEARCHABLE_TYPES)))
                 .dataFetcher("autoComplete", new AuthenticatedResolver<>(
                         new AutoCompleteResolver(SEARCHABLE_TYPES)))
+                .dataFetcher("autoCompleteForAll", new AuthenticatedResolver<>(
+                        new AutoCompleteForAllResolver(SEARCHABLE_TYPES)))
                 .dataFetcher("browse", new AuthenticatedResolver<>(
                         new BrowseResolver(BROWSABLE_TYPES)))
                 .dataFetcher("browsePaths", new AuthenticatedResolver<>(

File: metadata-jobs/mae-consumer-job/src/main/java/com/linkedin/metadata/kafka/DataHubUsageEventsProcessor.java
Patch:
@@ -35,7 +35,7 @@ public DataHubUsageEventsProcessor(ElasticsearchConnector elasticSearchConnector
   }
 
   @KafkaListener(id = "${KAFKA_CONSUMER_GROUP_ID:datahub-usage-event-consumer-job-client}", topics =
-      "${KAFKA_TOPIC_NAME:" + Topics.DATAHUB_USAGE_EVENT + "}", containerFactory = "stringSerializedKafkaListener")
+      "${DATAHUB_USAGE_EVENT_NAME:" + Topics.DATAHUB_USAGE_EVENT + "}", containerFactory = "stringSerializedKafkaListener")
   public void consume(final ConsumerRecord<String, String> consumerRecord) {
     final String record = consumerRecord.value();
     log.debug("Got DHUE");

File: gms/client/src/main/java/com/linkedin/BatchGetUtils.java
Patch:
@@ -33,7 +33,7 @@ private BatchGetUtils() {
             RB extends BatchGetEntityRequestBuilderBase<CRK, T, RB>,
             K extends RecordTemplate> Map<U, T> batchGet(
             @Nonnull Set<U> urns,
-            BatchGetEntityRequestBuilderBase<CRK, T, RB> requestBuilders,
+            Function<Void, BatchGetEntityRequestBuilderBase<CRK, T, RB>> requestBuilders,
             Function<U, CRK> getKeyFromUrn,
             Function<CRK, U> getUrnFromKey,
             Client client
@@ -48,7 +48,7 @@ K extends RecordTemplate> Map<U, T> batchGet(
 
         for (List<U> urnsInBatch : entityUrnBatches) {
             BatchGetEntityRequest<CRK, T> batchGetRequest =
-                    requestBuilders
+                    requestBuilders.apply(null)
                             .ids(urnsInBatch.stream().map(getKeyFromUrn).collect(Collectors.toSet()))
                             .build();
             final Map<U, T> batchResponse = client.sendRequest(batchGetRequest).getResponseEntity().getResults()

File: gms/client/src/main/java/com/linkedin/chart/client/Charts.java
Patch:
@@ -77,7 +77,7 @@ public Map<ChartUrn, Chart> batchGet(@Nonnull Set<ChartUrn> urns)
             throws RemoteInvocationException {
         return BatchGetUtils.batchGet(
                 urns,
-                CHARTS_REQUEST_BUILDERS.batchGet(),
+                (Void v) -> CHARTS_REQUEST_BUILDERS.batchGet(),
                 this::getKeyFromUrn,
                 this::getUrnFromKey,
                 _client

File: gms/client/src/main/java/com/linkedin/dashboard/client/Dashboards.java
Patch:
@@ -77,7 +77,7 @@ public Map<DashboardUrn, Dashboard> batchGet(@Nonnull Set<DashboardUrn> urns)
             throws RemoteInvocationException {
         return BatchGetUtils.batchGet(
                 urns,
-                DASHBOARDS_REQUEST_BUILDERS.batchGet(),
+                (Void v) -> DASHBOARDS_REQUEST_BUILDERS.batchGet(),
                 this::getKeyFromUrn,
                 this::getUrnFromKey,
                 _client

File: gms/client/src/main/java/com/linkedin/datajob/client/DataFlows.java
Patch:
@@ -178,7 +178,7 @@ public Map<DataFlowUrn, DataFlow> batchGet(@Nonnull Set<DataFlowUrn> urns)
         throws RemoteInvocationException {
         return BatchGetUtils.batchGet(
                 urns,
-                DATA_FLOWS_REQUEST_BUILDERS.batchGet(),
+                (Void v) -> DATA_FLOWS_REQUEST_BUILDERS.batchGet(),
                 this::getKeyFromUrn,
                 this::getUrnFromKey,
                 _client

File: gms/client/src/main/java/com/linkedin/datajob/client/DataJobs.java
Patch:
@@ -177,7 +177,7 @@ public Map<DataJobUrn, DataJob> batchGet(@Nonnull Set<DataJobUrn> urns)
         throws RemoteInvocationException {
         return BatchGetUtils.batchGet(
                 urns,
-                DATA_JOBS_REQUEST_BUILDERS.batchGet(),
+                (Void v) -> DATA_JOBS_REQUEST_BUILDERS.batchGet(),
                 this::getKeyFromUrn,
                 this::getUrnFromKey,
                 _client

File: gms/client/src/main/java/com/linkedin/dataprocess/client/DataProcesses.java
Patch:
@@ -56,7 +56,7 @@ public Map<DataProcessUrn, DataProcess> batchGet(@Nonnull Set<DataProcessUrn> ur
       throws RemoteInvocationException {
     return BatchGetUtils.batchGet(
             urns,
-            DATA_PROCESSES_REQUEST_BUILDERS.batchGet(),
+            (Void v) -> DATA_PROCESSES_REQUEST_BUILDERS.batchGet(),
             this::getKeyFromUrn,
             this::getUrnFromKey,
             _client

File: gms/client/src/main/java/com/linkedin/dataset/client/Datasets.java
Patch:
@@ -227,7 +227,7 @@ public Map<DatasetUrn, Dataset> batchGet(@Nonnull Set<DatasetUrn> urns)
         throws RemoteInvocationException {
         return BatchGetUtils.batchGet(
                 urns,
-                DATASETS_REQUEST_BUILDERS.batchGet(),
+                (Void v) -> DATASETS_REQUEST_BUILDERS.batchGet(),
                 this::getKeyFromUrn,
                 this::getUrnFromKey,
                 _client

File: gms/client/src/main/java/com/linkedin/identity/client/CorpGroups.java
Patch:
@@ -63,7 +63,7 @@ public Map<CorpGroupUrn, CorpGroup> batchGet(@Nonnull Set<CorpGroupUrn> urns)
       throws RemoteInvocationException {
     return BatchGetUtils.batchGet(
             urns,
-            CORP_GROUPS_REQUEST_BUILDERS.batchGet(),
+            (Void v) -> CORP_GROUPS_REQUEST_BUILDERS.batchGet(),
             this::getKeyFromUrn,
             this::getUrnFromKey,
             _client

File: gms/client/src/main/java/com/linkedin/identity/client/CorpUsers.java
Patch:
@@ -70,7 +70,7 @@ public Map<CorpuserUrn, CorpUser> batchGet(@Nonnull Set<CorpuserUrn> urns)
       throws RemoteInvocationException {
     return BatchGetUtils.batchGet(
             urns,
-            CORP_USERS_REQUEST_BUILDERS.batchGet(),
+            (Void v) -> CORP_USERS_REQUEST_BUILDERS.batchGet(),
             this::getKeyFromUrn,
             this::getUrnFromKey,
             _client

File: gms/client/src/main/java/com/linkedin/ml/client/MLModels.java
Patch:
@@ -125,7 +125,7 @@ public Map<MLModelUrn, MLModel> batchGet(@Nonnull Set<MLModelUrn> urns)
         throws RemoteInvocationException {
         return BatchGetUtils.batchGet(
                 urns,
-                ML_MODELS_REQUEST_BUILDERS.batchGet(),
+                (Void v) -> ML_MODELS_REQUEST_BUILDERS.batchGet(),
                 this::getKeyFromUrn,
                 this::getUrnFromKey,
                 _client

File: gms/client/src/main/java/com/linkedin/tag/client/Tags.java
Patch:
@@ -75,7 +75,7 @@ public Map<TagUrn, Tag> batchGet(@Nonnull Set<TagUrn> urns)
             throws RemoteInvocationException {
         return BatchGetUtils.batchGet(
                 urns,
-                TAGS_REQUEST_BUILDERS.batchGet(),
+                (Void v) -> TAGS_REQUEST_BUILDERS.batchGet(),
                 this::getKeyFromUrn,
                 this::getUrnFromKey,
                 _client

File: gms/impl/src/main/java/com/linkedin/metadata/resources/tag/Tags.java
Patch:
@@ -104,7 +104,9 @@ protected Tag toValue(@Nonnull TagSnapshot snapshot) {
         final Tag value = new Tag().setName(snapshot.getUrn().getName());
         ModelUtils.getAspectsFromSnapshot(snapshot).forEach(aspect -> {
             if (aspect instanceof TagProperties) {
-                value.setDescription(TagProperties.class.cast(aspect).getDescription());
+                if (TagProperties.class.cast(aspect).hasDescription()) {
+                    value.setDescription(TagProperties.class.cast(aspect).getDescription());
+                }
                 value.setName(TagProperties.class.cast(aspect).getName());
             } else if (aspect instanceof Ownership) {
                 value.setOwnership((Ownership) aspect);

File: metadata-dao-impl/restli-dao/src/main/java/com/linkedin/metadata/dao/DataFlowActionRequestBuilder.java
Patch:
@@ -9,7 +9,7 @@
  */
 public class DataFlowActionRequestBuilder extends BaseActionRequestBuilder<DataFlowSnapshot, DataFlowUrn> {
 
-  private static final String BASE_URI_TEMPLATE = "dataflows";
+  private static final String BASE_URI_TEMPLATE = "dataFlows";
 
   public DataFlowActionRequestBuilder() {
     super(DataFlowSnapshot.class, DataFlowUrn.class, BASE_URI_TEMPLATE);

File: metadata-dao-impl/restli-dao/src/main/java/com/linkedin/metadata/dao/DataJobActionRequestBuilder.java
Patch:
@@ -9,7 +9,7 @@
  */
 public class DataJobActionRequestBuilder extends BaseActionRequestBuilder<DataJobSnapshot, DataJobUrn> {
 
-  private static final String BASE_URI_TEMPLATE = "datajobs";
+  private static final String BASE_URI_TEMPLATE = "dataJobs";
 
   public DataJobActionRequestBuilder() {
     super(DataJobSnapshot.class, DataJobUrn.class, BASE_URI_TEMPLATE);

File: gms/client/src/main/java/com/linkedin/tag/client/Tags.java
Patch:
@@ -4,7 +4,7 @@
 import com.linkedin.data.template.StringArray;
 import com.linkedin.metadata.aspect.TagAspect;
 import com.linkedin.metadata.configs.TagSearchConfig;
-import com.linkedin.metadata.dao.TagActionRequestBuilders;
+import com.linkedin.metadata.dao.TagActionRequestBuilder;
 import com.linkedin.metadata.dao.utils.ModelUtils;
 import com.linkedin.metadata.query.AutoCompleteResult;
 import com.linkedin.metadata.query.SortCriterion;
@@ -39,7 +39,7 @@
 public class Tags extends BaseSearchableClient<Tag>  {
 
     private static final TagsRequestBuilders TAGS_REQUEST_BUILDERS = new TagsRequestBuilders();
-    private static final TagActionRequestBuilders TAGS_ACTION_REQUEST_BUILDERS = new TagActionRequestBuilders();
+    private static final TagActionRequestBuilder TAGS_ACTION_REQUEST_BUILDERS = new TagActionRequestBuilder();
     private static final TagSearchConfig TAGS_SEARCH_CONFIG = new TagSearchConfig();
 
     public Tags(@Nonnull Client restliClient) {

File: metadata-dao-impl/restli-dao/src/main/java/com/linkedin/metadata/dao/RequestBuilders.java
Patch:
@@ -28,6 +28,7 @@ public class RequestBuilders {
           add(new DataProcessActionRequestBuilder());
           add(new DatasetActionRequestBuilder());
           add(new MLModelActionRequestBuilder());
+          add(new TagActionRequestBuilder());
         }
       });
 

File: metadata-dao-impl/restli-dao/src/main/java/com/linkedin/metadata/dao/TagActionRequestBuilder.java
Patch:
@@ -6,11 +6,11 @@
 /**
  * An action request builder for tag entities.
  */
-public class TagActionRequestBuilders extends BaseActionRequestBuilder<TagSnapshot, TagUrn> {
+public class TagActionRequestBuilder extends BaseActionRequestBuilder<TagSnapshot, TagUrn> {
 
     private static final String BASE_URI_TEMPLATE = "tags";
 
-    public TagActionRequestBuilders() {
+    public TagActionRequestBuilder() {
         super(TagSnapshot.class, TagUrn.class, BASE_URI_TEMPLATE);
     }
 }

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/dataset/DatasetType.java
Patch:
@@ -151,7 +151,7 @@ public List<BrowsePath> browsePaths(@Nonnull String urn, @Nonnull final QueryCon
     @Override
     public Dataset update(@Nonnull DatasetUpdateInput input, @Nonnull QueryContext context) throws Exception {
         // TODO: Verify that updater is owner.
-        final CorpuserUrn actor = new CorpuserUrn(context.getActor());
+        final CorpuserUrn actor = CorpuserUrn.createFromString(context.getActor());
         final com.linkedin.dataset.Dataset partialDataset = DatasetUpdateInputMapper.map(input);
 
         // Create Audit Stamp

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/tag/TagType.java
Patch:
@@ -115,7 +115,7 @@ public AutoCompleteResults autoComplete(@Nonnull String query,
     @Override
     public Tag update(@Nonnull TagUpdate input, @Nonnull QueryContext context) throws Exception {
         // TODO: Verify that updater is owner.
-        final CorpuserUrn actor = new CorpuserUrn(context.getActor());
+        final CorpuserUrn actor = CorpuserUrn.createFromString(context.getActor());
         final com.linkedin.tag.Tag partialTag = TagUpdateMapper.map(input);
 
         // Create Audit Stamp

File: metadata-jobs/mae-consumer-job/src/main/java/com/linkedin/metadata/kafka/config/KafkaConfig.java
Patch:
@@ -3,6 +3,7 @@
 import io.confluent.kafka.serializers.AbstractKafkaAvroSerDeConfig;
 import io.confluent.kafka.serializers.KafkaAvroDeserializer;
 import java.time.Duration;
+import java.util.Arrays;
 import java.util.Map;
 import lombok.extern.slf4j.Slf4j;
 import org.apache.avro.generic.GenericRecord;
@@ -40,7 +41,7 @@ public KafkaListenerContainerFactory<?> kafkaListenerContainerFactory(KafkaPrope
 
     // KAFKA_BOOTSTRAP_SERVER has precedence over SPRING_KAFKA_BOOTSTRAP_SERVERS
     if (kafkaBootstrapServer != null && kafkaBootstrapServer.length() > 0) {
-      props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaBootstrapServer);
+      props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, Arrays.asList(kafkaBootstrapServer.split(",")));
     } // else we rely on KafkaProperties which defaults to localhost:9092
 
     props.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, kafkaSchemaRegistryUrl);

File: gms/factories/src/main/java/com/linkedin/gms/factory/common/RestHighLevelClientFactory.java
Patch:
@@ -14,10 +14,12 @@
 import org.springframework.beans.factory.annotation.Value;
 import org.springframework.context.annotation.Bean;
 import org.springframework.context.annotation.Configuration;
+import org.springframework.context.annotation.Import;
 
 
 @Slf4j
 @Configuration
+@Import({ElasticsearchSSLContextFactory.class})
 public class RestHighLevelClientFactory {
 
   @Value("${ELASTICSEARCH_HOST:localhost}")

File: metadata-jobs/mae-consumer-job/src/main/java/com/linkedin/metadata/kafka/MetadataAuditEventsProcessor.java
Patch:
@@ -11,8 +11,8 @@
 import com.linkedin.metadata.dao.internal.BaseGraphWriterDAO;
 import com.linkedin.metadata.dao.utils.RecordUtils;
 import com.linkedin.metadata.snapshot.Snapshot;
-import com.linkedin.metadata.utils.elasticsearch.ElasticsearchConnector;
-import com.linkedin.metadata.utils.elasticsearch.MCEElasticEvent;
+import com.linkedin.metadata.kafka.elasticsearch.ElasticsearchConnector;
+import com.linkedin.metadata.kafka.elasticsearch.MCEElasticEvent;
 import com.linkedin.mxe.MetadataAuditEvent;
 import com.linkedin.mxe.Topics;
 import java.io.UnsupportedEncodingException;

File: metadata-jobs/mae-consumer-job/src/main/java/com/linkedin/metadata/kafka/elasticsearch/ElasticEvent.java
Patch:
@@ -1,4 +1,4 @@
-package com.linkedin.metadata.utils.elasticsearch;
+package com.linkedin.metadata.kafka.elasticsearch;
 
 import com.linkedin.events.metadata.ChangeType;
 import lombok.Data;

File: metadata-jobs/mae-consumer-job/src/main/java/com/linkedin/metadata/kafka/elasticsearch/MCEElasticEvent.java
Patch:
@@ -1,4 +1,4 @@
-package com.linkedin.metadata.utils.elasticsearch;
+package com.linkedin.metadata.kafka.elasticsearch;
 
 import com.linkedin.data.template.RecordTemplate;
 import org.elasticsearch.common.xcontent.NamedXContentRegistry;

File: metadata-jobs/mae-consumer-job/src/main/java/com/linkedin/metadata/kafka/config/ElasticSearchConfig.java
Patch:
@@ -1,6 +1,7 @@
 package com.linkedin.metadata.kafka.config;
 
 import com.linkedin.data.template.RecordTemplate;
+import com.linkedin.gms.factory.common.ElasticsearchSSLContextFactory;
 import com.linkedin.gms.factory.common.RestHighLevelClientFactory;
 import com.linkedin.metadata.builders.search.BaseIndexBuilder;
 import com.linkedin.metadata.builders.search.SnapshotProcessor;
@@ -17,7 +18,7 @@
 
 @Slf4j
 @Configuration
-@Import({RestHighLevelClientFactory.class, IndexBuildersConfig.class})
+@Import({RestHighLevelClientFactory.class, IndexBuildersConfig.class, ElasticsearchSSLContextFactory.class})
 public class ElasticSearchConfig {
 
   @Value("${ELASTICSEARCH_HOST:localhost}")

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/browse/BrowsePathsResolver.java
Patch:
@@ -37,7 +37,7 @@ public CompletableFuture<List<BrowsePath>> get(DataFetchingEnvironment environme
                 throw new RuntimeException("Failed to retrieve browse paths: "
                         + String.format("entity type %s, urn %s",
                         input.getType(),
-                        input.getUrn()));
+                        input.getUrn()), e);
             }
         });
     }

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/browse/BrowseResolver.java
Patch:
@@ -52,7 +52,7 @@ public CompletableFuture<BrowseResults> get(DataFetchingEnvironment environment)
                         input.getPath(),
                         input.getFilters(),
                         start,
-                        count));
+                        count), e);
             }
         });
     }

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/search/AutoCompleteResolver.java
Patch:
@@ -61,7 +61,7 @@ public CompletableFuture<AutoCompleteResults> get(DataFetchingEnvironment enviro
                                 input.getField(),
                                 input.getQuery(),
                                 input.getFilters(),
-                                input.getLimit()));
+                                input.getLimit()), e);
             }
         });
     }

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/search/SearchResolver.java
Patch:
@@ -64,7 +64,7 @@ public CompletableFuture<SearchResults> get(DataFetchingEnvironment environment)
                         input.getQuery(),
                         input.getFilters(),
                         start,
-                        count));
+                        count), e);
             }
         });
     }

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/browse/BrowsePathsResolver.java
Patch:
@@ -32,7 +32,7 @@ public CompletableFuture<List<BrowsePath>> get(DataFetchingEnvironment environme
 
         return CompletableFuture.supplyAsync(() -> {
             try {
-                return _typeToEntity.get(input.getType()).browsePaths(input.getUrn());
+                return _typeToEntity.get(input.getType()).browsePaths(input.getUrn(), environment.getContext());
             } catch (Exception e) {
                 throw new RuntimeException("Failed to retrieve browse paths: "
                         + String.format("entity type %s, urn %s",

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/browse/BrowseResolver.java
Patch:
@@ -42,7 +42,8 @@ public CompletableFuture<BrowseResults> get(DataFetchingEnvironment environment)
                         input.getPath(),
                         input.getFilters(),
                         start,
-                        count
+                        count,
+                        environment.getContext()
                 );
             } catch (Exception e) {
                 throw new RuntimeException("Failed to execute browse: "

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/load/LoadableTypeResolver.java
Patch:
@@ -30,7 +30,7 @@ public LoadableTypeResolver(final LoadableType<T> loadableType, final Function<D
     }
 
     @Override
-    public CompletableFuture<T> get(DataFetchingEnvironment environment) throws Exception {
+    public CompletableFuture<T> get(DataFetchingEnvironment environment) {
         final String urn = _urnProvider.apply(environment);
         final DataLoader<String, T> loader = environment.getDataLoaderRegistry().getDataLoader(_loadableType.name());
         return loader.load(urn);

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/search/AutoCompleteResolver.java
Patch:
@@ -51,7 +51,8 @@ public CompletableFuture<AutoCompleteResults> get(DataFetchingEnvironment enviro
                         sanitizedQuery,
                         input.getField(),
                         input.getFilters(),
-                        limit
+                        limit,
+                        environment.getContext()
                 );
             } catch (Exception e) {
                 throw new RuntimeException("Failed to execute autocomplete: "

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/resolvers/search/SearchResolver.java
Patch:
@@ -54,7 +54,8 @@ public CompletableFuture<SearchResults> get(DataFetchingEnvironment environment)
                         sanitizedQuery,
                         input.getFilters(),
                         start,
-                        count
+                        count,
+                        environment.getContext()
                 );
             } catch (Exception e) {
                 throw new RuntimeException("Failed to execute search: "

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/dataplatform/DataPlatformType.java
Patch:
@@ -1,6 +1,7 @@
 package com.linkedin.datahub.graphql.types.dataplatform;
 
 import com.linkedin.common.urn.DataPlatformUrn;
+import com.linkedin.datahub.graphql.QueryContext;
 import com.linkedin.datahub.graphql.types.EntityType;
 import com.linkedin.datahub.graphql.types.mappers.DataPlatformInfoMapper;
 import com.linkedin.datahub.graphql.generated.DataPlatform;
@@ -26,7 +27,7 @@ public Class<DataPlatform> objectClass() {
     }
 
     @Override
-    public List<DataPlatform> batchLoad(List<String> urns) {
+    public List<DataPlatform> batchLoad(final List<String> urns, final QueryContext context) {
         try {
             if (_urnToPlatform == null) {
                 _urnToPlatform = _dataPlatformsClient.getAllPlatforms().stream()

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/dataset/DownstreamLineageType.java
Patch:
@@ -1,6 +1,7 @@
 package com.linkedin.datahub.graphql.types.dataset;
 
 import com.linkedin.common.urn.DatasetUrn;
+import com.linkedin.datahub.graphql.QueryContext;
 import com.linkedin.datahub.graphql.generated.DownstreamLineage;
 import com.linkedin.datahub.graphql.types.LoadableType;
 import com.linkedin.datahub.graphql.types.mappers.DownstreamLineageMapper;
@@ -24,7 +25,7 @@ public Class<DownstreamLineage> objectClass() {
     }
 
     @Override
-    public List<DownstreamLineage> batchLoad(final List<String> keys) {
+    public List<DownstreamLineage> batchLoad(final List<String> keys, final QueryContext context) {
 
         final List<DatasetUrn> datasetUrns = keys.stream()
                 .map(DatasetUtils::getDatasetUrn)

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/mappers/InstitutionalMemoryMetadataMapper.java
Patch:
@@ -17,6 +17,7 @@ public InstitutionalMemoryMetadata apply(@Nonnull final com.linkedin.common.Inst
         final InstitutionalMemoryMetadata result = new InstitutionalMemoryMetadata();
         result.setUrl(input.getUrl().toString());
         result.setDescription(input.getDescription());
+        result.setAuthor(input.getCreateStamp().getActor().toString());
         result.setCreated(AuditStampMapper.map(input.getCreateStamp()));
         return result;
     }

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/dataset/DatasetType.java
Patch:
@@ -107,8 +107,9 @@ public BrowseResults browse(@Nonnull List<String> path,
                                 int start,
                                 int count) throws Exception {
         final Map<String, String> facetFilters = ResolverUtils.buildFacetFilters(filters, FACET_FIELDS);
+        final String pathStr = path.size() > 0 ? BROWSE_PATH_DELIMITER + String.join(BROWSE_PATH_DELIMITER, path) : "";
         final BrowseResult result = _datasetsClient.browse(
-                BROWSE_PATH_DELIMITER + String.join(BROWSE_PATH_DELIMITER, path),
+                pathStr,
                 facetFilters,
                 start,
                 count);

File: datahub-graphql-core/src/main/java/com/linkedin/datahub/graphql/types/mappers/DatasetMapper.java
Patch:
@@ -42,6 +42,7 @@ public Dataset apply(@Nonnull final com.linkedin.dataset.Dataset dataset) {
             if (dataset.getSchemaMetadata().hasDeleted()) {
                 result.setDeleted(AuditStampMapper.map(dataset.getSchemaMetadata().getDeleted()));
             }
+            result.setSchema(SchemaMetadataMapper.map(dataset.getSchemaMetadata()));
         }
         if (dataset.hasPlatformNativeType()) {
             result.setPlatformNativeType(Enum.valueOf(PlatformNativeType.class, dataset.getPlatformNativeType().name()));

File: metadata-jobs/mae-consumer-job/src/main/java/com/linkedin/metadata/kafka/config/IndexBuildersConfig.java
Patch:
@@ -4,6 +4,7 @@
 import com.linkedin.metadata.builders.search.BaseIndexBuilder;
 import com.linkedin.metadata.builders.search.ChartIndexBuilder;
 import com.linkedin.metadata.builders.search.CorpGroupIndexBuilder;
+import com.linkedin.metadata.builders.search.CorpUserInfoIndexBuilder;
 import com.linkedin.metadata.builders.search.DashboardIndexBuilder;
 import com.linkedin.metadata.builders.search.DataProcessIndexBuilder;
 import com.linkedin.metadata.builders.search.DatasetIndexBuilder;
@@ -42,6 +43,7 @@ public Set<BaseIndexBuilder<? extends RecordTemplate>> indexBuilders(@Nonnull Cl
     log.debug("restli client {}", restliClient);
     final Set<BaseIndexBuilder<? extends RecordTemplate>> builders = new HashSet<>();
     builders.add(new CorpGroupIndexBuilder());
+    builders.add(new CorpUserInfoIndexBuilder());
     builders.add(new ChartIndexBuilder());
     builders.add(new DatasetIndexBuilder());
     builders.add(new DataProcessIndexBuilder());

File: metadata-models-generator/src/main/java/com/linkedin/metadata/rythm/StreamTemplateResource.java
Patch:
@@ -15,7 +15,7 @@
  */
 public class StreamTemplateResource extends TemplateResourceBase {
   private final String path;
-  private URLConnection connection = null;
+  private transient URLConnection connection = null;
 
   public StreamTemplateResource(String path, StreamResourceLoader loader) {
     super(loader);
@@ -32,7 +32,7 @@ public StreamTemplateResource(String path, StreamResourceLoader loader) {
       try {
         final File rythm = new File(path);
         if (rythm.exists()) {
-          connection = rythm.toURL().openConnection();
+          connection = rythm.toURI().toURL().openConnection();
         }
       } catch (IOException ex) {
         throw new RuntimeException("Get template resource failed", ex);

File: datahub-dao/src/main/java/com/linkedin/datahub/dao/view/OwnerViewDao.java
Patch:
@@ -43,7 +43,7 @@ public DatasetOwnership getDatasetOwners(@Nonnull String datasetUrn) throws Exce
     datasetOwnership.setDatasetUrn(datasetUrn);
     datasetOwnership.setFromUpstream(false);
     datasetOwnership.setOwners(fillDatasetOwner(ownership, owners));
-    datasetOwnership.setActor(ownership.getLastModified().getActor().getContent());
+    datasetOwnership.setActor(ownership.getLastModified().getActor().getId());
     datasetOwnership.setLastModified(ownership.getLastModified().getTime());
     return datasetOwnership;
   }

File: datahub-dao/src/main/java/com/linkedin/datahub/util/DatasetUtil.java
Patch:
@@ -45,7 +45,7 @@ public static DatasetUrn toDatasetUrn(@Nonnull String datasetUrn) throws URISynt
    */
   public static DatasetView toDatasetView(Dataset dataset) {
     DatasetView view = new DatasetView();
-    view.setPlatform(dataset.getPlatform().getContent());
+    view.setPlatform(dataset.getPlatform().getPlatformNameEntity());
     view.setNativeName(dataset.getName());
     view.setFabric(dataset.getOrigin().name());
     view.setDescription(dataset.getDescription());

File: gms/client/src/main/java/com/linkedin/common/client/DataProcessesClient.java
Patch:
@@ -16,7 +16,7 @@ protected DataProcessKey toDataProcessKey(@Nonnull DataProcessUrn urn) {
         return new DataProcessKey()
                 .setName(urn.getNameEntity())
                 .setOrigin(urn.getOriginEntity())
-                .setOrchestrator(urn.getOrchestrator());
+                .setOrchestrator(urn.getOrchestratorEntity());
     }
 
     @Nonnull

File: gms/impl/src/main/java/com/linkedin/metadata/resources/dataprocess/DataProcesses.java
Patch:
@@ -95,7 +95,7 @@ protected DataProcessUrn toUrn(@Nonnull ComplexResourceKey<DataProcessKey, Empty
     protected ComplexResourceKey<DataProcessKey, EmptyRecord> toKey(@Nonnull DataProcessUrn urn) {
         return new ComplexResourceKey<>(
             new DataProcessKey()
-                .setOrchestrator(urn.getOrchestrator())
+                .setOrchestrator(urn.getOrchestratorEntity())
                 .setName(urn.getNameEntity())
                 .setOrigin(urn.getOriginEntity()),
             new EmptyRecord());
@@ -105,7 +105,7 @@ protected ComplexResourceKey<DataProcessKey, EmptyRecord> toKey(@Nonnull DataPro
     @Override
     protected DataProcess toValue(@Nonnull DataProcessSnapshot processSnapshot) {
         final DataProcess value = new DataProcess()
-                .setOrchestrator(processSnapshot.getUrn().getOrchestrator())
+                .setOrchestrator(processSnapshot.getUrn().getOrchestratorEntity())
                 .setName(processSnapshot.getUrn().getNameEntity())
                 .setOrigin(processSnapshot.getUrn().getOriginEntity());
         ModelUtils.getAspectsFromSnapshot(processSnapshot).forEach(aspect -> {

File: metadata-builders/src/main/java/com/linkedin/metadata/builders/graph/DataProcessGraphBuilder.java
Patch:
@@ -32,7 +32,7 @@ protected List<? extends RecordTemplate> buildEntities(@Nonnull DataProcessSnaps
     final DataProcessUrn urn = snapshot.getUrn();
     final DataProcessEntity entity = new DataProcessEntity().setUrn(urn)
         .setName(urn.getNameEntity())
-        .setOrchestrator(urn.getOrchestrator())
+        .setOrchestrator(urn.getOrchestratorEntity())
         .setOrigin(urn.getOriginEntity());
 
     return Collections.singletonList(entity);

File: metadata-builders/src/main/java/com/linkedin/metadata/builders/search/DataProcessIndexBuilder.java
Patch:
@@ -25,15 +25,15 @@ public DataProcessIndexBuilder() {
 
     @Nonnull
     private static String buildBrowsePath(@Nonnull DataProcessUrn urn) {
-        return ("/" + urn.getOriginEntity() + "/"  + urn.getOrchestrator() + "/" + urn.getNameEntity())
+        return ("/" + urn.getOriginEntity() + "/"  + urn.getOrchestratorEntity() + "/" + urn.getNameEntity())
             .replace('.', '/').toLowerCase();
     }
 
     @Nonnull
     private static DataProcessDocument setUrnDerivedFields(@Nonnull DataProcessUrn urn) {
         return new DataProcessDocument()
             .setName(urn.getNameEntity())
-            .setOrchestrator(urn.getOrchestrator())
+            .setOrchestrator(urn.getOrchestratorEntity())
             .setUrn(urn)
             .setBrowsePaths(new StringArray(Collections.singletonList(buildBrowsePath(urn))));
     }

File: metadata-builders/src/test/java/com/linkedin/metadata/builders/graph/DataProcessGraphBuilderTest.java
Patch:
@@ -20,7 +20,7 @@ public void testBuildEntity() {
     DataProcessSnapshot snapshot = new DataProcessSnapshot().setUrn(urn).setAspects(new DataProcessAspectArray());
     DataProcessEntity expected = new DataProcessEntity().setUrn(urn)
         .setName(urn.getNameEntity())
-        .setOrchestrator(urn.getOrchestrator())
+        .setOrchestrator(urn.getOrchestratorEntity())
         .setOrigin(urn.getOriginEntity());
 
     List<? extends RecordTemplate> dataProcessEntities = new DataProcessGraphBuilder().buildEntities(snapshot);

File: gms/factories/src/main/java/com/linkedin/gms/factory/dataprocess/DataProcessDAOFactory.java
Patch:
@@ -3,6 +3,7 @@
 import com.linkedin.gms.factory.common.TopicConventionFactory;
 import com.linkedin.common.urn.DataProcessUrn;
 import com.linkedin.metadata.aspect.DataProcessAspect;
+import com.linkedin.metadata.dao.BaseLocalDAO;
 import com.linkedin.metadata.dao.EbeanLocalDAO;
 import com.linkedin.metadata.dao.producer.KafkaMetadataEventProducer;
 import com.linkedin.metadata.dao.producer.KafkaProducerCallback;
@@ -26,7 +27,7 @@ public class DataProcessDAOFactory {
 
   @Bean(name = "dataProcessDAO")
   @DependsOn({"gmsEbeanServiceConfig", "kafkaEventProducer", TopicConventionFactory.TOPIC_CONVENTION_BEAN})
-  protected EbeanLocalDAO<DataProcessAspect, DataProcessUrn> createInstance() {
+  protected BaseLocalDAO<DataProcessAspect, DataProcessUrn> createInstance() {
     KafkaMetadataEventProducer<DataProcessSnapshot, DataProcessAspect, DataProcessUrn> producer =
         new KafkaMetadataEventProducer(DataProcessSnapshot.class, DataProcessAspect.class,
             applicationContext.getBean(Producer.class), applicationContext.getBean(TopicConvention.class),

File: gms/factories/src/main/java/com/linkedin/gms/factory/dataset/DatasetDaoFactory.java
Patch:
@@ -3,6 +3,7 @@
 import com.linkedin.gms.factory.common.TopicConventionFactory;
 import com.linkedin.common.urn.DatasetUrn;
 import com.linkedin.metadata.aspect.DatasetAspect;
+import com.linkedin.metadata.dao.BaseLocalDAO;
 import com.linkedin.metadata.dao.EbeanLocalDAO;
 import com.linkedin.metadata.dao.producer.KafkaMetadataEventProducer;
 import com.linkedin.metadata.dao.producer.KafkaProducerCallback;
@@ -24,7 +25,7 @@ public class DatasetDaoFactory {
 
   @Bean(name = "datasetDao")
   @DependsOn({"gmsEbeanServiceConfig", "kafkaEventProducer", TopicConventionFactory.TOPIC_CONVENTION_BEAN})
-  protected EbeanLocalDAO<DatasetAspect, DatasetUrn> createInstance() {
+  protected BaseLocalDAO<DatasetAspect, DatasetUrn> createInstance() {
     KafkaMetadataEventProducer<DatasetSnapshot, DatasetAspect, DatasetUrn> producer =
         new KafkaMetadataEventProducer(DatasetSnapshot.class, DatasetAspect.class,
             applicationContext.getBean(Producer.class), applicationContext.getBean(TopicConvention.class),

File: gms/factories/src/main/java/com/linkedin/gms/factory/identity/CorpGroupDaoFactory.java
Patch:
@@ -3,6 +3,7 @@
 import com.linkedin.gms.factory.common.TopicConventionFactory;
 import com.linkedin.common.urn.CorpGroupUrn;
 import com.linkedin.metadata.aspect.CorpGroupAspect;
+import com.linkedin.metadata.dao.BaseLocalDAO;
 import com.linkedin.metadata.dao.EbeanLocalDAO;
 import com.linkedin.metadata.dao.producer.KafkaMetadataEventProducer;
 import com.linkedin.metadata.snapshot.CorpGroupSnapshot;
@@ -26,7 +27,7 @@ public class CorpGroupDaoFactory {
   @Bean(name = "corpGroupDao")
   @DependsOn({"gmsEbeanServiceConfig", "kafkaEventProducer", TopicConventionFactory.TOPIC_CONVENTION_BEAN})
   @Nonnull
-  protected EbeanLocalDAO createInstance() {
+  protected BaseLocalDAO<CorpGroupAspect, CorpGroupUrn> createInstance() {
     KafkaMetadataEventProducer<CorpGroupSnapshot, CorpGroupAspect, CorpGroupUrn> producer =
         new KafkaMetadataEventProducer(CorpGroupSnapshot.class, CorpGroupAspect.class,
             applicationContext.getBean(Producer.class), applicationContext.getBean(TopicConvention.class));

File: gms/factories/src/main/java/com/linkedin/gms/factory/identity/CorpUserDaoFactory.java
Patch:
@@ -3,6 +3,7 @@
 import com.linkedin.gms.factory.common.TopicConventionFactory;
 import com.linkedin.common.urn.CorpuserUrn;
 import com.linkedin.metadata.aspect.CorpUserAspect;
+import com.linkedin.metadata.dao.BaseLocalDAO;
 import com.linkedin.metadata.dao.EbeanLocalDAO;
 import com.linkedin.metadata.dao.producer.KafkaMetadataEventProducer;
 import com.linkedin.metadata.snapshot.CorpUserSnapshot;
@@ -26,7 +27,7 @@ public class CorpUserDaoFactory {
   @Bean(name = "corpUserDao")
   @DependsOn({"gmsEbeanServiceConfig", "kafkaEventProducer", TopicConventionFactory.TOPIC_CONVENTION_BEAN})
   @Nonnull
-  protected EbeanLocalDAO createInstance() {
+  protected BaseLocalDAO<CorpUserAspect, CorpuserUrn> createInstance() {
     KafkaMetadataEventProducer<CorpUserSnapshot, CorpUserAspect, CorpuserUrn> producer =
         new KafkaMetadataEventProducer(CorpUserSnapshot.class, CorpUserAspect.class,
             applicationContext.getBean(Producer.class), applicationContext.getBean(TopicConvention.class));

File: gms/impl/src/main/java/com/linkedin/metadata/resources/dataprocess/BaseDataProcessesAspectResource.java
Patch:
@@ -5,7 +5,6 @@
 import com.linkedin.dataprocess.DataProcessKey;
 import com.linkedin.metadata.aspect.DataProcessAspect;
 import com.linkedin.metadata.dao.BaseLocalDAO;
-import com.linkedin.metadata.dao.EbeanLocalDAO;
 import com.linkedin.metadata.restli.BaseVersionedAspectResource;
 import com.linkedin.restli.common.ComplexResourceKey;
 import com.linkedin.restli.common.EmptyRecord;
@@ -27,7 +26,7 @@ public BaseDataProcessesAspectResource(Class<ASPECT> aspectClass) {
 
     @Inject
     @Named("dataProcessDAO")
-    private EbeanLocalDAO localDAO;
+    private BaseLocalDAO localDAO;
 
     @Nonnull
     @Override

File: gms/impl/src/main/java/com/linkedin/metadata/resources/identity/BaseCorpUsersAspectResource.java
Patch:
@@ -5,7 +5,6 @@
 import com.linkedin.identity.CorpUserKey;
 import com.linkedin.metadata.aspect.CorpUserAspect;
 import com.linkedin.metadata.dao.BaseLocalDAO;
-import com.linkedin.metadata.dao.EbeanLocalDAO;
 import com.linkedin.metadata.restli.BaseVersionedAspectResource;
 import com.linkedin.restli.common.ComplexResourceKey;
 import com.linkedin.restli.common.EmptyRecord;
@@ -28,7 +27,7 @@ public BaseCorpUsersAspectResource(Class<ASPECT> aspectClass) {
 
   @Inject
   @Named("corpUserDao")
-  private EbeanLocalDAO localDAO;
+  private BaseLocalDAO localDAO;
 
   @Nonnull
   @Override
@@ -41,5 +40,4 @@ protected BaseLocalDAO<CorpUserAspect, CorpuserUrn> getLocalDAO() {
   protected CorpuserUrn getUrn(@PathKeysParam @Nonnull PathKeys keys) {
     return new CorpuserUrn(keys.<ComplexResourceKey<CorpUserKey, EmptyRecord>>get(CORPUSER_KEY).getKey().getName());
   }
-
 }
\ No newline at end of file

File: metadata-builders/src/test/java/com/linkedin/metadata/builders/search/DataProcessIndexBuilderTest.java
Patch:
@@ -39,10 +39,10 @@ public void testGetDocumentsToUpdateFromDataProcessSnapshot() {
         new DataProcessSnapshot().setUrn(dataProcessUrn).setAspects(dataProcessAspectArray);
 
     List<DataProcessDocument> actualDocs = new DataProcessIndexBuilder().getDocumentsToUpdate(dataProcessSnapshot);
-    assertEquals(actualDocs.size(), 1);
-    assertEquals(actualDocs.get(0).getUrn(), dataProcessUrn);
+    assertEquals(actualDocs.size(), 2);
     assertEquals(actualDocs.get(0).getInputs().get(0), inputDatasetUrn);
     assertEquals(actualDocs.get(0).getOutputs().get(0), outputDatasetUrn);
-
+    assertEquals(actualDocs.get(0).getUrn(), dataProcessUrn);
+    assertEquals(actualDocs.get(1).getUrn(), dataProcessUrn);
   }
 }

File: metadata-jobs/mae-consumer-job/src/main/java/com/linkedin/metadata/kafka/MetadataAuditEventsProcessor.java
Patch:
@@ -127,7 +127,7 @@ private void updateElasticsearch(final Snapshot snapshot) {
         String urn = indexBuilderForDoc.getDocumentType().getMethod("getUrn").invoke(doc).toString();
         elasticEvent.setId(URLEncoder.encode(urn.toLowerCase(), "UTF-8"));
       } catch (UnsupportedEncodingException | NoSuchMethodException | IllegalAccessException | InvocationTargetException e) {
-        log.error("Failed to encode the urn with error ", e.toString());
+        log.error("Failed to encode the urn with error: {}", e.toString());
         continue;
       }
       elasticEvent.setActionType(ChangeType.UPDATE);

File: li-utils/src/main/java/com/linkedin/common/urn/AzkabanFlowUrn.java
Patch:
@@ -38,7 +38,6 @@ public String getFlowIdEntity() {
   }
 
   public static AzkabanFlowUrn createFromString(String rawUrn) throws URISyntaxException {
-    validateUrn(rawUrn, ENTITY_TYPE);
     String content = new Urn(rawUrn).getContent();
     String[] parts = content.substring(1, content.length() - 1).split(",");
     return new AzkabanFlowUrn(parts[0], parts[1], parts[2]);

File: li-utils/src/main/java/com/linkedin/common/urn/AzkabanJobUrn.java
Patch:
@@ -31,7 +31,6 @@ public String getJobIdEntity() {
   }
 
   public static AzkabanJobUrn createFromString(String rawUrn) throws URISyntaxException {
-    validateUrn(rawUrn, ENTITY_TYPE);
     String content = new Urn(rawUrn).getContent();
     String flowParts = content.substring(1, content.lastIndexOf(",") + 1);
     String[] parts = content.substring(1, content.length() - 1).split(",");

File: li-utils/src/main/java/com/linkedin/common/urn/ChartUrn.java
Patch:
@@ -31,8 +31,9 @@ public String getChartIdEntity() {
   }
 
   public static ChartUrn createFromString(String rawUrn) throws URISyntaxException {
-    validateUrn(rawUrn, ENTITY_TYPE);
-    String[] urnParts = new Urn(rawUrn).getContent().split(",");
+    Urn urn = new Urn(rawUrn);
+    validateUrn(urn, ENTITY_TYPE);
+    String[] urnParts = urn.getContent().split(",");
     return new ChartUrn(urnParts[0], urnParts[1]);
   }
 

File: li-utils/src/main/java/com/linkedin/common/urn/CorpGroupUrn.java
Patch:
@@ -25,13 +25,14 @@ public String getGroupNameEntity() {
   }
 
   public static CorpGroupUrn createFromString(String rawUrn) throws URISyntaxException {
-    validateUrn(rawUrn, ENTITY_TYPE);
     String groupName = new Urn(rawUrn).getContent();
     return new CorpGroupUrn(groupName);
   }
 
   public static CorpGroupUrn createFromUrn(Urn urn) throws URISyntaxException {
-    validateUrn(urn, ENTITY_TYPE);
+    if (!ENTITY_TYPE.equals(urn.getEntityType())) {
+      throw new URISyntaxException(urn.toString(), "Can't cast URN to CorpGroupUrn, not same ENTITY");
+    }
 
     Matcher matcher = URN_PATTERN.matcher(urn.toString());
     if (matcher.find()) {

File: li-utils/src/main/java/com/linkedin/common/urn/CorpuserUrn.java
Patch:
@@ -26,13 +26,14 @@ public String getUsernameEntity() {
   }
 
   public static CorpuserUrn createFromString(String rawUrn) throws URISyntaxException {
-    validateUrn(rawUrn, ENTITY_TYPE);
     String username = new Urn(rawUrn).getContent();
     return new CorpuserUrn(username);
   }
 
   public static CorpuserUrn createFromUrn(Urn urn) throws URISyntaxException {
-    validateUrn(urn, ENTITY_TYPE);
+    if (!ENTITY_TYPE.equals(urn.getEntityType())) {
+      throw new URISyntaxException(urn.toString(), "Can't cast URN to CorpuserUrn, not same ENTITY");
+    }
 
     Matcher matcher = URN_PATTERN.matcher(urn.toString());
     if (matcher.find()) {

File: li-utils/src/main/java/com/linkedin/common/urn/DashboardUrn.java
Patch:
@@ -31,8 +31,9 @@ public String getDashboardIdEntity() {
   }
 
   public static DashboardUrn createFromString(String rawUrn) throws URISyntaxException {
-    validateUrn(rawUrn, ENTITY_TYPE);
-    String[] urnParts = new Urn(rawUrn).getContent().split(",");
+    Urn urn = new Urn(rawUrn);
+    validateUrn(urn, ENTITY_TYPE);
+    String[] urnParts = urn.getContent().split(",");
     return new DashboardUrn(urnParts[0], urnParts[1]);
   }
 

File: li-utils/src/main/java/com/linkedin/common/urn/DataPlatformUrn.java
Patch:
@@ -23,7 +23,6 @@ public String getPlatformNameEntity() {
   }
 
   public static DataPlatformUrn createFromString(String rawUrn) throws URISyntaxException {
-    validateUrn(rawUrn, ENTITY_TYPE);
     String platformName = new Urn(rawUrn).getContent();
     return new DataPlatformUrn(platformName);
   }

File: li-utils/src/main/java/com/linkedin/common/urn/DataProcessUrn.java
Patch:
@@ -40,7 +40,6 @@ public FabricType getOriginEntity() {
     }
 
     public static DataProcessUrn createFromString(String rawUrn) throws URISyntaxException {
-        validateUrn(rawUrn, ENTITY_TYPE);
         String content = new Urn(rawUrn).getContent();
         String[] parts = content.substring(1, content.length() - 1).split(",");
         return new DataProcessUrn(parts[0], parts[1], toFabricType(parts[2]));

File: li-utils/src/main/java/com/linkedin/common/urn/DatasetUrn.java
Patch:
@@ -41,7 +41,6 @@ public FabricType getOriginEntity() {
   }
 
   public static DatasetUrn createFromString(String rawUrn) throws URISyntaxException {
-    validateUrn(rawUrn, ENTITY_TYPE);
     String content = new Urn(rawUrn).getContent();
     String[] parts = content.substring(1, content.length() - 1).split(",");
     return new DatasetUrn(DataPlatformUrn.createFromString(parts[0]), parts[1], toFabricType(parts[2]));

File: li-utils/src/main/java/com/linkedin/common/urn/MLModelUrn.java
Patch:
@@ -39,7 +39,6 @@ public FabricType getOriginEntity() {
   }
 
   public static MLModelUrn createFromString(String rawUrn) throws URISyntaxException {
-    validateUrn(rawUrn, ENTITY_TYPE);
     String content = new Urn(rawUrn).getContent();
     String[] parts = content.substring(1, content.length() - 1).split(",");
     return new MLModelUrn(DataPlatformUrn.createFromString(parts[0]), parts[1], toFabricType(parts[2]));

File: li-utils/src/main/java/com/linkedin/common/urn/AzkabanFlowUrn.java
Patch:
@@ -38,6 +38,7 @@ public String getFlowIdEntity() {
   }
 
   public static AzkabanFlowUrn createFromString(String rawUrn) throws URISyntaxException {
+    validateUrn(rawUrn, ENTITY_TYPE);
     String content = new Urn(rawUrn).getContent();
     String[] parts = content.substring(1, content.length() - 1).split(",");
     return new AzkabanFlowUrn(parts[0], parts[1], parts[2]);

File: li-utils/src/main/java/com/linkedin/common/urn/AzkabanJobUrn.java
Patch:
@@ -31,6 +31,7 @@ public String getJobIdEntity() {
   }
 
   public static AzkabanJobUrn createFromString(String rawUrn) throws URISyntaxException {
+    validateUrn(rawUrn, ENTITY_TYPE);
     String content = new Urn(rawUrn).getContent();
     String flowParts = content.substring(1, content.lastIndexOf(",") + 1);
     String[] parts = content.substring(1, content.length() - 1).split(",");

File: li-utils/src/main/java/com/linkedin/common/urn/ChartUrn.java
Patch:
@@ -31,9 +31,8 @@ public String getChartIdEntity() {
   }
 
   public static ChartUrn createFromString(String rawUrn) throws URISyntaxException {
-    Urn urn = new Urn(rawUrn);
-    validateUrn(urn, ENTITY_TYPE);
-    String[] urnParts = urn.getContent().split(",");
+    validateUrn(rawUrn, ENTITY_TYPE);
+    String[] urnParts = new Urn(rawUrn).getContent().split(",");
     return new ChartUrn(urnParts[0], urnParts[1]);
   }
 

File: li-utils/src/main/java/com/linkedin/common/urn/CorpGroupUrn.java
Patch:
@@ -25,14 +25,13 @@ public String getGroupNameEntity() {
   }
 
   public static CorpGroupUrn createFromString(String rawUrn) throws URISyntaxException {
+    validateUrn(rawUrn, ENTITY_TYPE);
     String groupName = new Urn(rawUrn).getContent();
     return new CorpGroupUrn(groupName);
   }
 
   public static CorpGroupUrn createFromUrn(Urn urn) throws URISyntaxException {
-    if (!ENTITY_TYPE.equals(urn.getEntityType())) {
-      throw new URISyntaxException(urn.toString(), "Can't cast URN to CorpGroupUrn, not same ENTITY");
-    }
+    validateUrn(urn, ENTITY_TYPE);
 
     Matcher matcher = URN_PATTERN.matcher(urn.toString());
     if (matcher.find()) {

File: li-utils/src/main/java/com/linkedin/common/urn/CorpuserUrn.java
Patch:
@@ -26,14 +26,13 @@ public String getUsernameEntity() {
   }
 
   public static CorpuserUrn createFromString(String rawUrn) throws URISyntaxException {
+    validateUrn(rawUrn, ENTITY_TYPE);
     String username = new Urn(rawUrn).getContent();
     return new CorpuserUrn(username);
   }
 
   public static CorpuserUrn createFromUrn(Urn urn) throws URISyntaxException {
-    if (!ENTITY_TYPE.equals(urn.getEntityType())) {
-      throw new URISyntaxException(urn.toString(), "Can't cast URN to CorpuserUrn, not same ENTITY");
-    }
+    validateUrn(urn, ENTITY_TYPE);
 
     Matcher matcher = URN_PATTERN.matcher(urn.toString());
     if (matcher.find()) {

File: li-utils/src/main/java/com/linkedin/common/urn/DashboardUrn.java
Patch:
@@ -31,9 +31,8 @@ public String getDashboardIdEntity() {
   }
 
   public static DashboardUrn createFromString(String rawUrn) throws URISyntaxException {
-    Urn urn = new Urn(rawUrn);
-    validateUrn(urn, ENTITY_TYPE);
-    String[] urnParts = urn.getContent().split(",");
+    validateUrn(rawUrn, ENTITY_TYPE);
+    String[] urnParts = new Urn(rawUrn).getContent().split(",");
     return new DashboardUrn(urnParts[0], urnParts[1]);
   }
 

File: li-utils/src/main/java/com/linkedin/common/urn/DataPlatformUrn.java
Patch:
@@ -23,6 +23,7 @@ public String getPlatformNameEntity() {
   }
 
   public static DataPlatformUrn createFromString(String rawUrn) throws URISyntaxException {
+    validateUrn(rawUrn, ENTITY_TYPE);
     String platformName = new Urn(rawUrn).getContent();
     return new DataPlatformUrn(platformName);
   }

File: li-utils/src/main/java/com/linkedin/common/urn/DataProcessUrn.java
Patch:
@@ -40,6 +40,7 @@ public FabricType getOriginEntity() {
     }
 
     public static DataProcessUrn createFromString(String rawUrn) throws URISyntaxException {
+        validateUrn(rawUrn, ENTITY_TYPE);
         String content = new Urn(rawUrn).getContent();
         String[] parts = content.substring(1, content.length() - 1).split(",");
         return new DataProcessUrn(parts[0], parts[1], toFabricType(parts[2]));

File: li-utils/src/main/java/com/linkedin/common/urn/DatasetUrn.java
Patch:
@@ -41,6 +41,7 @@ public FabricType getOriginEntity() {
   }
 
   public static DatasetUrn createFromString(String rawUrn) throws URISyntaxException {
+    validateUrn(rawUrn, ENTITY_TYPE);
     String content = new Urn(rawUrn).getContent();
     String[] parts = content.substring(1, content.length() - 1).split(",");
     return new DatasetUrn(DataPlatformUrn.createFromString(parts[0]), parts[1], toFabricType(parts[2]));

File: li-utils/src/main/java/com/linkedin/common/urn/MLModelUrn.java
Patch:
@@ -39,6 +39,7 @@ public FabricType getOriginEntity() {
   }
 
   public static MLModelUrn createFromString(String rawUrn) throws URISyntaxException {
+    validateUrn(rawUrn, ENTITY_TYPE);
     String content = new Urn(rawUrn).getContent();
     String[] parts = content.substring(1, content.length() - 1).split(",");
     return new MLModelUrn(DataPlatformUrn.createFromString(parts[0]), parts[1], toFabricType(parts[2]));

File: gms/factories/src/main/java/com/linkedin/gms/factory/common/KafkaEventProducerFactory.java
Patch:
@@ -1,4 +1,4 @@
-package com.linkedin.common.factory;
+package com.linkedin.gms.factory.common;
 
 import io.confluent.kafka.serializers.KafkaAvroSerializer;
 import io.confluent.kafka.serializers.AbstractKafkaAvroSerDeConfig;

File: gms/factories/src/main/java/com/linkedin/gms/factory/common/LocalEbeanServerConfigFactory.java
Patch:
@@ -1,4 +1,4 @@
-package com.linkedin.common.factory;
+package com.linkedin.gms.factory.common;
 
 import io.ebean.config.ServerConfig;
 import io.ebean.datasource.DataSourceConfig;

File: gms/factories/src/main/java/com/linkedin/gms/factory/common/Neo4jDriverFactory.java
Patch:
@@ -1,4 +1,4 @@
-package com.linkedin.common.factory;
+package com.linkedin.gms.factory.common;
 
 import org.neo4j.driver.AuthTokens;
 import org.neo4j.driver.Driver;

File: gms/factories/src/main/java/com/linkedin/gms/factory/common/RestHighLevelClientFactory.java
Patch:
@@ -1,4 +1,4 @@
-package com.linkedin.common.factory;
+package com.linkedin.gms.factory.common;
 
 import javax.annotation.Nonnull;
 

File: gms/factories/src/main/java/com/linkedin/gms/factory/common/TopicConventionFactory.java
Patch:
@@ -1,4 +1,4 @@
-package com.linkedin.common.factory;
+package com.linkedin.gms.factory.common;
 
 import com.linkedin.mxe.TopicConvention;
 import com.linkedin.mxe.TopicConventionImpl;

File: gms/factories/src/main/java/com/linkedin/gms/factory/dataplatform/DataPlatformLocalDaoFactory.java
Patch:
@@ -1,4 +1,4 @@
-package com.linkedin.dataplatform.factory;
+package com.linkedin.gms.factory.dataplatform;
 
 import com.linkedin.common.urn.DataPlatformUrn;
 import com.linkedin.metadata.aspect.DataPlatformAspect;

File: gms/factories/src/main/java/com/linkedin/gms/factory/dataprocess/DataProcessDAOFactory.java
Patch:
@@ -1,6 +1,6 @@
-package com.linkedin.dataprocess.factory;
+package com.linkedin.gms.factory.dataprocess;
 
-import com.linkedin.common.factory.TopicConventionFactory;
+import com.linkedin.gms.factory.common.TopicConventionFactory;
 import com.linkedin.common.urn.DataProcessUrn;
 import com.linkedin.metadata.aspect.DataProcessAspect;
 import com.linkedin.metadata.dao.EbeanLocalDAO;

File: gms/factories/src/main/java/com/linkedin/gms/factory/dataprocess/DataProcessSearchDAOFactory.java
Patch:
@@ -1,4 +1,4 @@
-package com.linkedin.dataprocess.factory;
+package com.linkedin.gms.factory.dataprocess;
 
 import com.linkedin.metadata.configs.DataProcessSearchConfig;
 import com.linkedin.metadata.dao.search.ESSearchDAO;

File: gms/factories/src/main/java/com/linkedin/gms/factory/dataset/DatasetBrowseDAOFactory.java
Patch:
@@ -1,4 +1,4 @@
-package com.linkedin.dataset.factory;
+package com.linkedin.gms.factory.dataset;
 
 import com.linkedin.metadata.configs.DatasetBrowseConfig;
 import com.linkedin.metadata.dao.browse.ESBrowseDAO;

File: gms/factories/src/main/java/com/linkedin/gms/factory/dataset/DatasetDaoFactory.java
Patch:
@@ -1,6 +1,6 @@
-package com.linkedin.dataset.factory;
+package com.linkedin.gms.factory.dataset;
 
-import com.linkedin.common.factory.TopicConventionFactory;
+import com.linkedin.gms.factory.common.TopicConventionFactory;
 import com.linkedin.common.urn.DatasetUrn;
 import com.linkedin.metadata.aspect.DatasetAspect;
 import com.linkedin.metadata.dao.EbeanLocalDAO;

File: gms/factories/src/main/java/com/linkedin/gms/factory/dataset/DatasetQueryDaoFactory.java
Patch:
@@ -1,4 +1,4 @@
-package com.linkedin.dataset.factory;
+package com.linkedin.gms.factory.dataset;
 
 import com.linkedin.metadata.dao.Neo4jQueryDAO;
 import org.neo4j.driver.Driver;

File: gms/factories/src/main/java/com/linkedin/gms/factory/dataset/DatasetSearchDAOFactory.java
Patch:
@@ -1,4 +1,4 @@
-package com.linkedin.dataset.factory;
+package com.linkedin.gms.factory.dataset;
 
 import com.linkedin.metadata.configs.DatasetSearchConfig;
 import com.linkedin.metadata.dao.search.ESSearchDAO;

File: gms/factories/src/main/java/com/linkedin/gms/factory/identity/CorpGroupDaoFactory.java
Patch:
@@ -1,6 +1,6 @@
-package com.linkedin.identity.factory;
+package com.linkedin.gms.factory.identity;
 
-import com.linkedin.common.factory.TopicConventionFactory;
+import com.linkedin.gms.factory.common.TopicConventionFactory;
 import com.linkedin.common.urn.CorpGroupUrn;
 import com.linkedin.metadata.aspect.CorpGroupAspect;
 import com.linkedin.metadata.dao.EbeanLocalDAO;

File: gms/factories/src/main/java/com/linkedin/gms/factory/identity/CorpGroupSearchDaoFactory.java
Patch:
@@ -1,4 +1,4 @@
-package com.linkedin.identity.factory;
+package com.linkedin.gms.factory.identity;
 
 import com.linkedin.metadata.configs.CorpGroupSearchConfig;
 import com.linkedin.metadata.dao.search.ESSearchDAO;

File: gms/factories/src/main/java/com/linkedin/gms/factory/identity/CorpUserDaoFactory.java
Patch:
@@ -1,6 +1,6 @@
-package com.linkedin.identity.factory;
+package com.linkedin.gms.factory.identity;
 
-import com.linkedin.common.factory.TopicConventionFactory;
+import com.linkedin.gms.factory.common.TopicConventionFactory;
 import com.linkedin.common.urn.CorpuserUrn;
 import com.linkedin.metadata.aspect.CorpUserAspect;
 import com.linkedin.metadata.dao.EbeanLocalDAO;

File: gms/factories/src/main/java/com/linkedin/gms/factory/identity/CorpUserSearchDaoFactory.java
Patch:
@@ -1,4 +1,4 @@
-package com.linkedin.identity.factory;
+package com.linkedin.gms.factory.identity;
 
 import com.linkedin.metadata.configs.CorpUserSearchConfig;
 import com.linkedin.metadata.dao.search.ESSearchDAO;

File: metadata-jobs/mae-consumer-job/src/main/java/com/linkedin/metadata/kafka/config/ElasticSearchConfig.java
Patch:
@@ -5,7 +5,7 @@
 import org.springframework.context.annotation.Configuration;
 import org.springframework.context.annotation.Import;
 
-import com.linkedin.common.factory.RestHighLevelClientFactory;
+import com.linkedin.gms.factory.common.RestHighLevelClientFactory;
 import com.linkedin.metadata.builders.search.RegisteredIndexBuilders;
 import com.linkedin.metadata.builders.search.SnapshotProcessor;
 import com.linkedin.metadata.utils.elasticsearch.ElasticsearchConnector;

File: metadata-jobs/mae-consumer-job/src/main/java/com/linkedin/metadata/kafka/config/Neo4jConfig.java
Patch:
@@ -5,7 +5,7 @@
 import org.springframework.context.annotation.Configuration;
 import org.springframework.context.annotation.Import;
 
-import com.linkedin.common.factory.Neo4jDriverFactory;
+import com.linkedin.gms.factory.common.Neo4jDriverFactory;
 import com.linkedin.metadata.dao.internal.BaseGraphWriterDAO;
 import com.linkedin.metadata.dao.internal.Neo4jGraphWriterDAO;
 

File: metadata-dao-impl/ebean-dao/src/test/java/com/linkedin/metadata/dao/utils/BarUrnPathExtractor.java
Patch:
@@ -1,5 +1,6 @@
 package com.linkedin.metadata.dao.utils;
 
+import com.linkedin.metadata.dao.scsi.UrnPathExtractor;
 import com.linkedin.testing.urn.BarUrn;
 import java.util.Collections;
 import java.util.HashMap;

File: metadata-dao-impl/ebean-dao/src/test/java/com/linkedin/metadata/dao/utils/BazUrnPathExtractor.java
Patch:
@@ -1,5 +1,6 @@
 package com.linkedin.metadata.dao.utils;
 
+import com.linkedin.metadata.dao.scsi.UrnPathExtractor;
 import com.linkedin.testing.urn.BazUrn;
 import java.util.Collections;
 import java.util.HashMap;

File: metadata-dao-impl/ebean-dao/src/test/java/com/linkedin/metadata/dao/utils/FooUrnPathExtractor.java
Patch:
@@ -1,5 +1,6 @@
 package com.linkedin.metadata.dao.utils;
 
+import com.linkedin.metadata.dao.scsi.UrnPathExtractor;
 import com.linkedin.testing.urn.FooUrn;
 import java.util.Collections;
 import java.util.HashMap;

File: metadata-testing/metadata-test-models/src/main/java/com/linkedin/testing/TestUtils.java
Patch:
@@ -48,7 +48,7 @@ public static BazUrn makeBazUrn(int id) {
   }
 
   @Nonnull
-  public static <T> Urn makeUrn(@Nonnull T id) {
+  public static Urn makeUrn(@Nonnull Object id) {
     try {
       return new Urn("urn:li:testing:" + id);
     } catch (URISyntaxException e) {
@@ -57,7 +57,7 @@ public static <T> Urn makeUrn(@Nonnull T id) {
   }
 
   @Nonnull
-  public static <T> Urn makeUrn(@Nonnull T id, @Nonnull String entityType) {
+  public static Urn makeUrn(@Nonnull Object id, @Nonnull String entityType) {
     try {
       return new Urn("urn:li:" + entityType + ":" + id);
     } catch (URISyntaxException e) {

File: gms/factories/src/main/java/com/linkedin/common/factory/KafkaEventProducerFactory.java
Patch:
@@ -18,7 +18,7 @@
 @EnableConfigurationProperties(KafkaProperties.class)
 public class KafkaEventProducerFactory {
 
-  @Value("${KAFKA_BOOTSTRAP_SERVER}")
+  @Value("${KAFKA_BOOTSTRAP_SERVER:http://localhost:9092}")
   private String kafkaBootstrapServers;
 
   @Value("${KAFKA_SCHEMAREGISTRY_URL:http://localhost:8081}")

File: metadata-jobs/mae-consumer-job/src/main/java/com/linkedin/metadata/kafka/config/KafkaConfig.java
Patch:
@@ -20,7 +20,7 @@
 @Slf4j
 @Configuration
 public class KafkaConfig {
-  @Value("${KAFKA_BOOTSTRAP_SERVER}")
+  @Value("${KAFKA_BOOTSTRAP_SERVER:http://localhost:9092}")
   private String kafkaBootstrapServer;
   @Value("${KAFKA_SCHEMAREGISTRY_URL:http://localhost:8081}")
   private String kafkaSchemaRegistryUrl;

File: metadata-jobs/mce-consumer-job/src/main/java/com/linkedin/metadata/kafka/config/KafkaConfig.java
Patch:
@@ -25,7 +25,7 @@
 @Slf4j
 @Configuration
 public class KafkaConfig {
-  @Value("${KAFKA_BOOTSTRAP_SERVER}")
+  @Value("${KAFKA_BOOTSTRAP_SERVER:http://localhost:9092}")
   private String kafkaBootstrapServers;
   @Value("${KAFKA_SCHEMAREGISTRY_URL:http://localhost:8081}")
   private String kafkaSchemaRegistryUrl;

File: metadata-builders/src/test/java/com/linkedin/metadata/builders/graph/DataProcessGraphBuilderTest.java
Patch:
@@ -8,7 +8,7 @@
 import java.util.List;
 import org.testng.annotations.Test;
 
-import static com.linkedin.metadata.utils.TestUtils.*;
+import static com.linkedin.metadata.testing.Urns.*;
 import static org.testng.Assert.*;
 
 

File: metadata-builders/src/test/java/com/linkedin/metadata/builders/graph/DatasetGraphBuilderTest.java
Patch:
@@ -1,5 +1,6 @@
 package com.linkedin.metadata.builders.graph;
 
+import com.linkedin.common.Status;
 import com.linkedin.common.urn.DatasetUrn;
 import com.linkedin.data.template.RecordTemplate;
 import com.linkedin.metadata.aspect.DatasetAspect;
@@ -10,7 +11,7 @@
 import java.util.List;
 import org.testng.annotations.Test;
 
-import static com.linkedin.metadata.utils.TestUtils.*;
+import static com.linkedin.metadata.testing.Urns.*;
 import static org.testng.Assert.*;
 
 
@@ -34,7 +35,7 @@ public void testBuildEntity() {
   public void testBuildRemovedEntity() {
     DatasetUrn urn = makeDatasetUrn("foobar");
     DatasetAspect aspect = new DatasetAspect();
-    aspect.setStatus(makeStatus(true));
+    aspect.setStatus(new Status().setRemoved(true));
     DatasetSnapshot snapshot =
         new DatasetSnapshot().setUrn(urn).setAspects(new DatasetAspectArray(Collections.singleton(aspect)));
     DatasetEntity expected = new DatasetEntity().setUrn(urn)

File: metadata-dao-impl/ebean-dao/src/test/java/com/linkedin/metadata/dao/EbeanLocalDAOTest.java
Patch:
@@ -498,7 +498,7 @@ public void testBackfillUsingSCSI() {
       addMetadata(urn, AspectBar.class.getCanonicalName(), 0, aspectBar);
 
       // only index urn
-      addIndex(urn, FooUrn.class.getCanonicalName(), "/fooId", urn.getId());
+      addIndex(urn, FooUrn.class.getCanonicalName(), "/fooId", urn.getFooIdEntity());
     });
 
     // Backfill in SCSI_ONLY mode

File: metadata-events/mxe-utils-avro-1.7/src/main/java/com/linkedin/metadata/EventUtils.java
Patch:
@@ -53,7 +53,7 @@ private EventUtils() {
 
   @Nonnull
   private static Schema getAvroSchemaFromResource(@Nonnull String resourcePath) {
-    URL url = Resources.getResource(resourcePath);
+    URL url = EventUtils.class.getClassLoader().getResource(resourcePath);
     try {
       return Schema.parse(Resources.toString(url, Charsets.UTF_8));
     } catch (IOException e) {

File: gms/factories/src/main/java/com/linkedin/dataplatform/factory/DataPlatformLocalDaoFactory.java
Patch:
@@ -1,5 +1,6 @@
 package com.linkedin.dataplatform.factory;
 
+import com.linkedin.common.urn.DataPlatformUrn;
 import com.linkedin.metadata.aspect.DataPlatformAspect;
 import com.linkedin.metadata.dao.ImmutableLocalDAO;
 import com.linkedin.metadata.resources.dataplatform.utils.DataPlatformsUtil;
@@ -16,6 +17,7 @@ public class DataPlatformLocalDaoFactory {
 
   @Bean(name = "dataPlatformLocalDAO")
   protected ImmutableLocalDAO createInstance() {
-    return new ImmutableLocalDAO<>(DataPlatformAspect.class, DataPlatformsUtil.getDataPlatformInfoMap());
+    return new ImmutableLocalDAO<>(DataPlatformAspect.class, DataPlatformsUtil.getDataPlatformInfoMap(),
+        DataPlatformUrn.class);
   }
 }

File: gms/factories/src/main/java/com/linkedin/identity/factory/CorpGroupDaoFactory.java
Patch:
@@ -27,6 +27,7 @@ protected EbeanLocalDAO createInstance() {
         KafkaMetadataEventProducer<CorpGroupSnapshot, CorpGroupAspect, CorpGroupUrn> producer =
                 new KafkaMetadataEventProducer(CorpGroupSnapshot.class, CorpGroupAspect.class,
                         applicationContext.getBean(Producer.class));
-        return new EbeanLocalDAO<>(CorpGroupAspect.class, producer, applicationContext.getBean(ServerConfig.class));
+        return new EbeanLocalDAO<>(CorpGroupAspect.class, producer, applicationContext.getBean(ServerConfig.class),
+            CorpGroupUrn.class);
     }
 }
\ No newline at end of file

File: gms/impl/src/main/java/com/linkedin/metadata/resources/dataprocess/DataProcesses.java
Patch:
@@ -12,6 +12,7 @@
 import com.linkedin.metadata.query.Filter;
 import com.linkedin.metadata.query.SearchResultMetadata;
 import com.linkedin.metadata.query.SortCriterion;
+import com.linkedin.metadata.restli.BackfillResult;
 import com.linkedin.metadata.restli.BaseSearchableEntityResource;
 import com.linkedin.metadata.search.DataProcessDocument;
 import com.linkedin.metadata.snapshot.DataProcessSnapshot;
@@ -198,7 +199,7 @@ public Task<DataProcessSnapshot> getSnapshot(@ActionParam(PARAM_URN) @Nonnull St
     @Action(name = ACTION_BACKFILL)
     @Override
     @Nonnull
-    public Task<String[]> backfill(@ActionParam(PARAM_URN) @Nonnull String urnString,
+    public Task<BackfillResult> backfill(@ActionParam(PARAM_URN) @Nonnull String urnString,
                                    @ActionParam(PARAM_ASPECTS) @Optional @Nullable String[] aspectNames) {
         return super.backfill(urnString, aspectNames);
     }

File: gms/impl/src/main/java/com/linkedin/metadata/resources/dataset/Datasets.java
Patch:
@@ -18,6 +18,7 @@
 import com.linkedin.metadata.query.Filter;
 import com.linkedin.metadata.query.SearchResultMetadata;
 import com.linkedin.metadata.query.SortCriterion;
+import com.linkedin.metadata.restli.BackfillResult;
 import com.linkedin.metadata.restli.BaseBrowsableEntityResource;
 import com.linkedin.metadata.search.DatasetDocument;
 import com.linkedin.metadata.snapshot.DatasetSnapshot;
@@ -237,7 +238,7 @@ public Task<DatasetSnapshot> getSnapshot(@ActionParam(PARAM_URN) @Nonnull String
   @Action(name = ACTION_BACKFILL)
   @Override
   @Nonnull
-  public Task<String[]> backfill(@ActionParam(PARAM_URN) @Nonnull String urnString,
+  public Task<BackfillResult> backfill(@ActionParam(PARAM_URN) @Nonnull String urnString,
       @ActionParam(PARAM_ASPECTS) @Optional @Nullable String[] aspectNames) {
     return super.backfill(urnString, aspectNames);
   }

File: gms/impl/src/main/java/com/linkedin/metadata/resources/identity/CorpGroups.java
Patch:
@@ -12,6 +12,7 @@
 import com.linkedin.metadata.query.Filter;
 import com.linkedin.metadata.query.SearchResultMetadata;
 import com.linkedin.metadata.query.SortCriterion;
+import com.linkedin.metadata.restli.BackfillResult;
 import com.linkedin.metadata.restli.BaseSearchableEntityResource;
 import com.linkedin.metadata.search.CorpGroupDocument;
 import com.linkedin.metadata.snapshot.CorpGroupSnapshot;
@@ -170,7 +171,7 @@ public Task<CorpGroupSnapshot> getSnapshot(@ActionParam(PARAM_URN) @Nonnull Stri
   @Action(name = ACTION_BACKFILL)
   @Override
   @Nonnull
-  public Task<String[]> backfill(@ActionParam(PARAM_URN) @Nonnull String urnString,
+  public Task<BackfillResult> backfill(@ActionParam(PARAM_URN) @Nonnull String urnString,
       @ActionParam(PARAM_ASPECTS) @Optional @Nullable String[] aspectNames) {
     return super.backfill(urnString, aspectNames);
   }

File: gms/impl/src/main/java/com/linkedin/metadata/resources/identity/CorpUsers.java
Patch:
@@ -14,6 +14,7 @@
 import com.linkedin.metadata.query.Filter;
 import com.linkedin.metadata.query.SearchResultMetadata;
 import com.linkedin.metadata.query.SortCriterion;
+import com.linkedin.metadata.restli.BackfillResult;
 import com.linkedin.metadata.restli.BaseSearchableEntityResource;
 import com.linkedin.metadata.search.CorpUserInfoDocument;
 import com.linkedin.metadata.snapshot.CorpUserSnapshot;
@@ -185,9 +186,8 @@ public Task<CorpUserSnapshot> getSnapshot(@ActionParam(PARAM_URN) @Nonnull Strin
   @Action(name = ACTION_BACKFILL)
   @Override
   @Nonnull
-  public Task<String[]> backfill(@ActionParam(PARAM_URN) @Nonnull String urnString,
+  public Task<BackfillResult> backfill(@ActionParam(PARAM_URN) @Nonnull String urnString,
       @ActionParam(PARAM_ASPECTS) @Optional @Nullable String[] aspectNames) {
     return super.backfill(urnString, aspectNames);
   }
-
 }

File: metadata-dao-impl/ebean-dao/src/test/java/com/linkedin/metadata/dao/EbeanLocalDAOTest.java
Patch:
@@ -496,7 +496,7 @@ public void testBackfillUsingSCSI() {
       addMetadata(urn, AspectBar.class.getCanonicalName(), 0, aspectBar);
 
       // only index urn
-      addIndex(urn, FooUrn.class.getCanonicalName(), "/fooId", urn.getId());
+      addIndex(urn, FooUrn.class.getCanonicalName(), "/fooId", urn.getFooIdEntity());
     });
 
     // Backfill in SCSI_ONLY mode

File: metadata-builders/src/main/java/com/linkedin/metadata/builders/graph/relationship/BaseRelationshipBuilder.java
Patch:
@@ -16,15 +16,15 @@ public BaseRelationshipBuilder(@Nonnull Class<ASPECT> aspectClass) {
   }
 
   /**
-   * Returns the aspect class this {@link BaseRelationshipBuilder} supports
+   * Returns the aspect class this {@link BaseRelationshipBuilder} supports.
    */
   @Nonnull
   public Class<ASPECT> supportedAspectClass() {
     return _aspectClass;
   }
 
   /**
-   * Returns a list of corresponding relationship updates for the given metadata aspect
+   * Returns a list of corresponding relationship updates for the given metadata aspect.
    */
   @Nonnull
   public abstract <URN extends Urn> List<GraphBuilder.RelationshipUpdates> buildRelationships(@Nonnull URN urn,

File: metadata-dao-impl/ebean-dao/src/main/java/com/linkedin/metadata/dao/EbeanMetadataId.java
Patch:
@@ -12,6 +12,9 @@
 import lombok.Setter;
 
 
+/**
+ * Unique ID for a piece of metadata stored in MySQL.
+ */
 @Getter
 @Setter
 @AllArgsConstructor

File: metadata-dao-impl/ebean-dao/src/main/java/com/linkedin/metadata/dao/EbeanMetadataIndex.java
Patch:
@@ -14,6 +14,9 @@
 import lombok.experimental.Accessors;
 
 
+/**
+ * Index definition for MySQL metadata.
+ */
 @Getter
 @Setter
 // define composite indexes

File: metadata-dao-impl/ebean-dao/src/main/java/com/linkedin/metadata/dao/ImmutableLocalDAO.java
Patch:
@@ -38,7 +38,7 @@ public class ImmutableLocalDAO<ASPECT_UNION extends UnionTemplate, URN extends U
   private static final String GMA_CREATE_ALL_SQL = "gma-create-all.sql";
 
   /**
-   * Constructs an {@link ImmutableLocalDAO} from a hard-coded URN-Aspect map
+   * Constructs an {@link ImmutableLocalDAO} from a hard-coded URN-Aspect map.
    */
   public ImmutableLocalDAO(@Nonnull Class<ASPECT_UNION> aspectUnionClass,
       @Nonnull Map<URN, ? extends RecordTemplate> urnAspectMap, @Nonnull Class<URN> urnClass) {
@@ -60,7 +60,7 @@ public ImmutableLocalDAO(@Nonnull Class<ASPECT_UNION> aspectUnionClass,
   /**
    * Loads a map of URN to aspect values from an {@link InputStream}.
    *
-   * The InputStream is expected to contain a JSON map where the keys are a specific type of URN and values are a
+   * <p>The InputStream is expected to contain a JSON map where the keys are a specific type of URN and values are a
    * specific type of metadata aspect.
    */
   @Nonnull

File: metadata-dao-impl/ebean-dao/src/main/java/com/linkedin/metadata/dao/utils/DatasetUrnPathExtractor.java
Patch:
@@ -7,6 +7,9 @@
 import javax.annotation.Nonnull;
 
 
+/**
+ * Maps schema paths to values for DatasetUrns.
+ */
 public class DatasetUrnPathExtractor implements UrnPathExtractor<DatasetUrn> {
   @Override
   public Map<String, Object> extractPaths(@Nonnull DatasetUrn urn) {

File: metadata-dao-impl/ebean-dao/src/main/java/com/linkedin/metadata/dao/utils/RegisteredUrnPathExtractors.java
Patch:
@@ -10,7 +10,7 @@
 /**
  * A class that holds all the registered {@link UrnPathExtractor}s.
  *
- * Register new type of urn path extractors by adding them to {@link #REGISTERED_URN_PATH_EXTRACTORS}.
+ * <p>Register new type of urn path extractors by adding them to {@link #REGISTERED_URN_PATH_EXTRACTORS}.
  */
 public class RegisteredUrnPathExtractors {
 

File: metadata-dao-impl/ebean-dao/src/main/java/com/linkedin/metadata/dao/utils/UrnPathExtractor.java
Patch:
@@ -5,6 +5,9 @@
 import javax.annotation.Nonnull;
 
 
+/**
+ * Given an urn, extracts a map of schema key to value.
+ */
 public interface UrnPathExtractor<URN extends Urn> {
   @Nonnull
   Map<String, Object> extractPaths(@Nonnull URN urn);

File: metadata-dao-impl/elasticsearch-dao/src/main/java/com/linkedin/metadata/dao/search/ESAutoCompleteQueryForHighCardinalityFields.java
Patch:
@@ -44,7 +44,7 @@ SearchRequest constructAutoCompleteQuery(@Nonnull String input, @Nonnull String
   }
 
   /**
-   * Constructs auto complete query given request
+   * Constructs auto complete query given request.
    *
    * @param input the type ahead query text
    * @param field the field name for the auto complete

File: metadata-dao-impl/elasticsearch-dao/src/main/java/com/linkedin/metadata/dao/search/ESAutoCompleteQueryForLowCardinalityFields.java
Patch:
@@ -44,7 +44,7 @@ SearchRequest constructAutoCompleteQuery(@Nonnull String input, @Nonnull String
   }
 
   /**
-   * Constructs auto complete query given request
+   * Constructs auto complete query given request.
    *
    * @param input the type ahead query text
    * @param field the field name for the auto complete

File: metadata-dao-impl/elasticsearch-dao/src/main/java/com/linkedin/metadata/dao/utils/SearchUtils.java
Patch:
@@ -24,7 +24,7 @@ private SearchUtils() {
   }
 
   /**
-   * Validates the request params and create a request map out of it
+   * Validates the request params and create a request map out of it.
    *
    * @param requestParams the search request with fields and values
    * @return a request map
@@ -45,7 +45,7 @@ public static Map<String, String> getRequestMap(@Nullable Filter requestParams)
   }
 
   /**
-   * Builds search query using criterion
+   * Builds search query using criterion.
    *
    * @param criterion {@link Criterion} single criterion which contains field, value and a comparison operator
    * @return QueryBuilder

File: metadata-dao-impl/kafka-producer/src/main/java/com/linkedin/metadata/dao/producer/KafkaMetadataEventProducer.java
Patch:
@@ -40,7 +40,7 @@ public class KafkaMetadataEventProducer<SNAPSHOT extends RecordTemplate, ASPECT_
   private final Optional<Callback> _callback;
 
   /**
-   * Constructor
+   * Constructor.
    *
    * @param snapshotClass The snapshot class for the produced events
    * @param aspectUnionClass The aspect union in the snapshot
@@ -54,7 +54,7 @@ public KafkaMetadataEventProducer(@Nonnull Class<SNAPSHOT> snapshotClass,
   }
 
   /**
-   * Constructor
+   * Constructor.
    *
    * @param snapshotClass The snapshot class for the produced events
    * @param aspectUnionClass The aspect union in the snapshot

File: metadata-dao-impl/neo4j-dao/src/main/java/com/linkedin/metadata/dao/Neo4jQueryDAO.java
Patch:
@@ -237,11 +237,11 @@ List<List<RecordTemplate>> findPaths(
   }
 
   /**
-   * Runs a query statement with parameters and return StatementResult
+   * Runs a query statement with parameters and return StatementResult.
    *
    * @param statement a statement with parameters to be executed
    * @param mapperFunction lambda to transform query result
-   * @return List<T> list of elements in the query result
+   * @return list of elements in the query result
    */
   @Nonnull
   private <T> List<T> runQuery(@Nonnull Statement statement, @Nonnull Function<Record, T> mapperFunction) {

File: metadata-dao-impl/neo4j-dao/src/main/java/com/linkedin/metadata/dao/internal/Neo4jGraphWriterDAO.java
Patch:
@@ -103,7 +103,7 @@ private void executeStatements(@Nonnull List<Statement> statements) {
   }
 
   /**
-   * Run a query statement with parameters and return StatementResult
+   * Run a query statement with parameters and return StatementResult.
    *
    * @param statement a statement with parameters to be executed
    */
@@ -208,7 +208,7 @@ private <URN extends Urn> Statement removeNode(@Nonnull URN urn) {
   }
 
   /**
-   * Gets Node based on Urn, if not exist, creates placeholder node
+   * Gets Node based on Urn, if not exist, creates placeholder node.
    */
   @Nonnull
   private Statement getOrInsertNode(@Nonnull Urn urn) {

File: metadata-dao-impl/restli-dao/src/main/java/com/linkedin/metadata/dao/BaseActionRequestBuilder.java
Patch:
@@ -30,7 +30,7 @@
 /**
  * A base class for generating rest.li requests against entity-specific action methods.
  *
- * See http://go/gma for more details.
+ * <p>See http://go/gma for more details.
  *
  * @param <SNAPSHOT> must be a valid snapshot type defined in com.linkedin.metadata.snapshot
  * @param <URN> must be the URN type used in {@code SNAPSHOT}

File: metadata-dao-impl/restli-dao/src/main/java/com/linkedin/metadata/dao/internal/RestliRemoteWriterDAO.java
Patch:
@@ -14,7 +14,7 @@
 /**
  * A rest.li implementation of {@link BaseRemoteWriterDAO}.
  *
- * Uses rest.li snapshot endpoints to update metadata on remote services.
+ * <p>Uses rest.li snapshot endpoints to update metadata on remote services.
  */
 public class RestliRemoteWriterDAO extends BaseRemoteWriterDAO {
 

File: metadata-dao/src/main/java/com/linkedin/metadata/dao/BaseReadDAO.java
Patch:
@@ -72,7 +72,7 @@ public <ASPECT extends RecordTemplate> Optional<ASPECT> get(@Nonnull Class<ASPEC
   /**
    * Similar to {@link #get(Class, Urn)} but retrieves multiple aspects latest versions associated with multiple URNs.
    *
-   * The returned {@link Map} contains all the .
+   * <p>The returned {@link Map} contains all the .
    */
   @Nonnull
   public Map<URN, Map<Class<? extends RecordTemplate>, Optional<? extends RecordTemplate>>> get(

File: metadata-dao/src/main/java/com/linkedin/metadata/dao/BaseRemoteDAO.java
Patch:
@@ -8,8 +8,8 @@
 /**
  * A base class for all Remote DAO.
  *
- * Remote DAO is a standardized interface to fetch aspects stored on a remote service.
- * See http://go/gma for more details.
+ * <p>Remote DAO is a standardized interface to fetch aspects stored on a remote service. See http://go/gma for more
+ * details.
  *
  * @param <ASPECT_UNION> must be an aspect union type defined in com.linkedin.metadata.aspect
  */

File: metadata-dao/src/main/java/com/linkedin/metadata/dao/BaseSearchWriterDAO.java
Patch:
@@ -8,7 +8,7 @@
 /**
  * A base class for all Search Writer DAOs.
  *
- * Search Writer DAO is a standardized interface to update a search index.
+ * <p>Search Writer DAO is a standardized interface to update a search index.
  */
 public abstract class BaseSearchWriterDAO<DOCUMENT extends RecordTemplate> {
 

File: metadata-dao/src/main/java/com/linkedin/metadata/dao/internal/BaseGraphWriterDAO.java
Patch:
@@ -10,7 +10,7 @@
 /**
  * A base class for all Graph Writer DAOs.
  *
- * Graph Writer DAO is a standardized interface to update a centralized graph DB.
+ * <p>Graph Writer DAO is a standardized interface to update a centralized graph DB.
  */
 public abstract class BaseGraphWriterDAO {
 
@@ -74,7 +74,7 @@ public <RELATIONSHIP extends RecordTemplate> void addRelationship(@Nonnull RELAT
   }
 
   /**
-   * Adds a relationship in the graph, with removal operations before adding
+   * Adds a relationship in the graph, with removal operations before adding.
    *
    * @param relationship the relationship to be persisted
    * @param removalOption whether to remove existing relationship of the same type
@@ -99,7 +99,7 @@ public <RELATIONSHIP extends RecordTemplate> void addRelationships(@Nonnull List
   }
 
   /**
-   * Adds a batch of relationships in the graph, with removal operations before adding
+   * Adds a batch of relationships in the graph, with removal operations before adding.
    *
    * @param relationships the list of relationships to be persisted
    * @param removalOption whether to remove existing relationship of the same type

File: metadata-dao/src/main/java/com/linkedin/metadata/dao/internal/BaseRemoteWriterDAO.java
Patch:
@@ -8,13 +8,13 @@
 /**
  * A base class for all remote writer DAOs.
  *
- * Remote writer DAO allows updating metadata aspects hosted on a remote service without knowing the exact
+ * <p>Remote writer DAO allows updating metadata aspects hosted on a remote service without knowing the exact
  * URN-to-service mapping.
  */
 public abstract class BaseRemoteWriterDAO {
 
   /**
-   * Creates a new metadata snapshot against a remote service
+   * Creates a new metadata snapshot against a remote service.
    *
    * @param urn the {@link Urn} for the entity
    * @param snapshot the snapshot containing updated metadata aspects

File: metadata-dao/src/main/java/com/linkedin/metadata/dao/producer/BaseMetadataEventProducer.java
Patch:
@@ -11,7 +11,7 @@
 /**
  * A base class for all metadata event producers.
  *
- * See http://go/gma for more details.
+ *<p>See http://go/gma for more details.
  */
 public abstract class BaseMetadataEventProducer<SNAPSHOT extends RecordTemplate, ASPECT_UNION extends UnionTemplate, URN extends Urn> {
 

File: metadata-dao/src/main/java/com/linkedin/metadata/dao/retention/TimeBasedRetention.java
Patch:
@@ -10,7 +10,7 @@
 public class TimeBasedRetention implements Retention {
 
   /**
-   * Constructs a {@link TimeBasedRetention} object
+   * Constructs a {@link TimeBasedRetention} object.
    *
    * @param maxAgeToRetain maximal age (in milliseconds) to retain. Must be positive.
    */

File: metadata-dao/src/main/java/com/linkedin/metadata/dao/retention/VersionBasedRetention.java
Patch:
@@ -9,7 +9,7 @@
 public class VersionBasedRetention implements Retention {
 
   /**
-   * Constructs a {@link VersionBasedRetention} object
+   * Constructs a {@link VersionBasedRetention} object.
    *
    * @param maxVersionsToRetain maximal number of versions to retain. Must be greater than 0.
    */

File: metadata-dao/src/main/java/com/linkedin/metadata/dao/utils/QueryUtils.java
Patch:
@@ -68,7 +68,7 @@ public static Set<AspectVersion> latestAspectVersions(@Nonnull Set<Class<? exten
   }
 
   /**
-   * Calculates the total page count
+   * Calculates the total page count.
    *
    * @param totalCount total count
    * @param size page size
@@ -82,7 +82,7 @@ public static int getTotalPageCount(int totalCount, int size) {
   }
 
   /**
-   * Calculates whether there is more results
+   * Calculates whether there is more results.
    *
    * @param from offset from the first result you want to fetch
    * @param size page size

File: metadata-models-generator/src/main/java/com/linkedin/metadata/generator/EventSchemaComposer.java
Patch:
@@ -13,7 +13,7 @@
 import static com.linkedin.metadata.generator.SchemaGeneratorUtil.*;
 
 
-/***
+/**
  * Render the property annotations to the MXE pdl schema.
  */
 @Slf4j

File: metadata-models-generator/src/main/java/com/linkedin/metadata/generator/EventSpec.java
Patch:
@@ -6,8 +6,8 @@
 import lombok.Data;
 
 
-/***
- *  Getter & setter class for schema event metadata.
+/**
+ * Getter & setter class for schema event metadata.
  */
 @Data
 public class EventSpec {

File: metadata-models-generator/src/main/java/com/linkedin/metadata/generator/SchemaAnnotationRetriever.java
Patch:
@@ -14,7 +14,7 @@
 import static com.linkedin.metadata.generator.SchemaGeneratorConstants.*;
 
 
-/***
+/**
  * Parse the property annotations from the pdl schema.
  */
 @Slf4j

File: metadata-models-generator/src/main/java/com/linkedin/metadata/generator/SchemaGenerator.java
Patch:
@@ -7,6 +7,9 @@
 import org.rythmengine.Rythm;
 
 
+/**
+ * Generates MXE schemas.
+ */
 public class SchemaGenerator {
 
   private final DataSchemaParser _dataSchemaParser;

File: metadata-restli-resource/src/main/java/com/linkedin/metadata/restli/BaseBrowsableClient.java
Patch:
@@ -11,7 +11,8 @@
 import javax.annotation.Nullable;
 
 /**
- * Base client that all entities supporting browse as well as search should implement in their respective restli MPs
+ * Base client that all entities supporting browse as well as search should implement in their respective restli MPs.
+ *
  * @param <VALUE> the client's value type
  * @param <URN> urn type of the entity
  */
@@ -37,7 +38,7 @@ public abstract BrowseResult browse(@Nonnull String inputPath, @Nullable Map<Str
       throws RemoteInvocationException;
 
   /**
-   * Returns a list of paths for a given urn
+   * Returns a list of paths for a given urn.
    *
    * @param urn Urn of the entity
    * @return all paths that are related to the urn

File: metadata-restli-resource/src/main/java/com/linkedin/metadata/restli/BaseBrowsableEntityResource.java
Patch:
@@ -19,9 +19,9 @@
 
 
 /**
- * A base class for the entity rest.li resource that supports CRUD + search + browse methods
+ * A base class for the entity rest.li resource that supports CRUD + search + browse methods.
  *
- * See http://go/gma for more details
+ * <p>See http://go/gma for more details
  *
  * @param <KEY> the resource's key type
  * @param <VALUE> the resource's value type

File: metadata-restli-resource/src/main/java/com/linkedin/metadata/restli/BaseEntityResource.java
Patch:
@@ -42,7 +42,7 @@
 /**
  * A base class for the entity rest.li resource, that supports CRUD methods.
  *
- * See http://go/gma for more details
+ * <p>See http://go/gma for more details
  *
  * @param <KEY> the resource's key type
  * @param <VALUE> the resource's value type
@@ -335,7 +335,7 @@ protected Set<Class<? extends RecordTemplate>> parseAspectsParam(@Nullable Strin
   }
 
   /**
-   * Returns a map of {@link VALUE} models given the collection of {@link URN}s and set of aspect classes
+   * Returns a map of {@link VALUE} models given the collection of {@link URN}s and set of aspect classes.
    *
    * @param urns collection of urns
    * @param aspectClasses set of aspect classes

File: metadata-restli-resource/src/main/java/com/linkedin/metadata/restli/BaseEntitySimpleKeyResource.java
Patch:
@@ -34,7 +34,7 @@
 /**
  * A base class for the entity rest.li resource where the key is of a primitive (simple) type.
  *
- * See http://go/gma for more details
+ * <p>See http://go/gma for more details
  *
  * @param <KEY> the resource's simple key type
  * @param <VALUE> the resource's value type
@@ -229,7 +229,7 @@ protected Set<Class<? extends RecordTemplate>> parseAspectsParam(@Nullable Strin
   }
 
   /**
-   * Returns a map of {@link VALUE} models given the collection of {@link URN}s and set of aspect classes
+   * Returns a map of {@link VALUE} models given the collection of {@link URN}s and set of aspect classes.
    *
    * @param urns collection of urns
    * @param aspectClasses set of aspect classes

File: metadata-restli-resource/src/main/java/com/linkedin/metadata/restli/BaseSearchableEntityResource.java
Patch:
@@ -33,9 +33,9 @@
 
 
 /**
- * A base class for the entity rest.li resource that supports CRUD + search methods
+ * A base class for the entity rest.li resource that supports CRUD + search methods.
  *
- * See http://go/gma for more details
+ * <p>See http://go/gma for more details
  *
  * @param <KEY> the resource's key type
  * @param <VALUE> the resource's value type

File: metadata-restli-resource/src/main/java/com/linkedin/metadata/restli/BaseSingleAspectEntitySimpleKeyVersionedSubResource.java
Patch:
@@ -25,8 +25,8 @@
  * A base resource class for serving a versioned single-aspect entity values as sub resource. This resource class is
  * meant to be used as a child resource for classes extending from {@link BaseSingleAspectEntitySimpleKeyResource}.
  *
- * The key for the sub-resource is typically a version field which is a Long value. The versioned resources
- * are retrieved using {@link #get(Long)} and {@link #getAllWithMetadata(PagingContext)}.
+ * <p>The key for the sub-resource is typically a version field which is a Long value. The versioned resources are
+ * retrieved using {@link #get(Long)} and {@link #getAllWithMetadata(PagingContext)}.
  *
  * @param <VALUE> the resource's value type
  * @param <URN> must be a valid {@link Urn} type
@@ -40,7 +40,7 @@ public abstract class BaseSingleAspectEntitySimpleKeyVersionedSubResource<
     ASPECT extends RecordTemplate,
     ASPECT_UNION extends UnionTemplate>
     extends CollectionResourceTaskTemplate<Long, VALUE> {
-   // @formatter:on
+  // @formatter:on
 
   private final Class<ASPECT> _aspectClass;
   private final Class<VALUE> _valueClass;

File: metadata-restli-resource/src/main/java/com/linkedin/metadata/restli/BaseSingleAspectSearchableEntitySimpleKeyResource.java
Patch:
@@ -18,7 +18,7 @@
 /**
  * A base class for the single aspect entity rest.li resource that supports CRUD + search methods.
  *
- * See http://go/gma for more details
+ * <p>See http://go/gma for more details.
  *
  * @param <KEY> the resource's key type
  * @param <VALUE> the resource's value type

File: metadata-restli-resource/src/main/java/com/linkedin/metadata/restli/BrowsableClient.java
Patch:
@@ -10,7 +10,7 @@
 
 
 /**
- * Interface which all entities supporting browse should implement in their respective restli MPs
+ * Interface which all entities supporting browse should implement in their respective restli MPs.
  *
  * @deprecated Use {@link BaseBrowsableClient} instead
  */

File: metadata-restli-resource/src/main/java/com/linkedin/metadata/restli/SearchableClient.java
Patch:
@@ -11,9 +11,9 @@
 
 
 /**
- * Interface that all entities that support search and autocomplete should implement in their respective restli MPs
- * @param <VALUE> the client's value type
+ * Interface that all entities that support search and autocomplete should implement in their respective restli MPs.
  *
+ * @param <VALUE> the client's value type.
  * @deprecated Use {@link BaseSearchableClient} instead
  */
 public interface SearchableClient<VALUE extends RecordTemplate> {

File: metadata-testing/metadata-test-models/src/main/java/com/linkedin/testing/TestUtils.java
Patch:
@@ -81,7 +81,7 @@ public static EntityDocument makeDocument(@Nonnull Urn urn) {
   }
 
   /**
-   * Returns all test entity classes
+   * Returns all test entity classes.
    */
   @Nonnull
   public static Set<Class<? extends RecordTemplate>> getAllTestEntities() {
@@ -95,7 +95,7 @@ public static Set<Class<? extends RecordTemplate>> getAllTestEntities() {
   }
 
   /**
-   * Returns all test relationship classes
+   * Returns all test relationship classes.
    */
   @Nonnull
   public static Set<Class<? extends RecordTemplate>> getAllTestRelationships() {

File: metadata-validators/src/main/java/com/linkedin/metadata/validator/SnapshotValidator.java
Patch:
@@ -56,9 +56,9 @@ public static void validateSnapshotSchema(@Nonnull Class<? extends RecordTemplat
   }
 
   /**
-   * Validates that the URN class is unique across all snapshots
+   * Validates that the URN class is unique across all snapshots.
    *
-   * @param classes a collection of snapshot classes.
+   * @param snapshotClasses a collection of snapshot classes.
    */
   public static void validateUniqueUrn(@Nonnull Collection<? extends Class> snapshotClasses) {
     Set<Class> urnClasses = new HashSet<>();

File: metadata-dao/src/test/java/com/linkedin/metadata/dao/BaseLocalDAOTest.java
Patch:
@@ -1,7 +1,6 @@
 package com.linkedin.metadata.dao;
 
 import com.linkedin.common.AuditStamp;
-import com.linkedin.common.urn.Urn;
 import com.linkedin.data.template.RecordTemplate;
 import com.linkedin.metadata.dao.producer.BaseMetadataEventProducer;
 import com.linkedin.metadata.dao.retention.TimeBasedRetention;
@@ -94,13 +93,13 @@ public <ASPECT extends RecordTemplate> ListResult<Long> listVersions(Class<ASPEC
     }
 
     @Override
-    public <ASPECT extends RecordTemplate> ListResult<Urn> listUrns(Class<ASPECT> aspectClass, int start,
+    public <ASPECT extends RecordTemplate> ListResult<FooUrn> listUrns(Class<ASPECT> aspectClass, int start,
         int pageSize) {
       return null;
     }
 
     @Override
-    public ListResult<Urn> listUrns(@Nonnull IndexFilter indexFilter, @Nullable FooUrn lastUrn, int pageSize) {
+    public ListResult<FooUrn> listUrns(@Nonnull IndexFilter indexFilter, @Nullable FooUrn lastUrn, int pageSize) {
       return null;
     }
 

File: metadata-builders/src/main/java/com/linkedin/metadata/builders/graph/relationship/BaseRelationshipBuilder.java
Patch:
@@ -16,15 +16,15 @@ public BaseRelationshipBuilder(@Nonnull Class<ASPECT> aspectClass) {
   }
 
   /**
-   * Returns the aspect class this {@link BaseRelationshipBuilder} supports.
+   * Returns the aspect class this {@link BaseRelationshipBuilder} supports
    */
   @Nonnull
   public Class<ASPECT> supportedAspectClass() {
     return _aspectClass;
   }
 
   /**
-   * Returns a list of corresponding relationship updates for the given metadata aspect.
+   * Returns a list of corresponding relationship updates for the given metadata aspect
    */
   @Nonnull
   public abstract <URN extends Urn> List<GraphBuilder.RelationshipUpdates> buildRelationships(@Nonnull URN urn,

File: metadata-dao-impl/ebean-dao/src/main/java/com/linkedin/metadata/dao/EbeanMetadataId.java
Patch:
@@ -12,9 +12,6 @@
 import lombok.Setter;
 
 
-/**
- * Unique ID for a piece of metadata stored in MySQL.
- */
 @Getter
 @Setter
 @AllArgsConstructor

File: metadata-dao-impl/ebean-dao/src/main/java/com/linkedin/metadata/dao/EbeanMetadataIndex.java
Patch:
@@ -14,9 +14,6 @@
 import lombok.experimental.Accessors;
 
 
-/**
- * Index definition for MySQL metadata.
- */
 @Getter
 @Setter
 // define composite indexes

File: metadata-dao-impl/ebean-dao/src/main/java/com/linkedin/metadata/dao/ImmutableLocalDAO.java
Patch:
@@ -38,7 +38,7 @@ public class ImmutableLocalDAO<ASPECT_UNION extends UnionTemplate, URN extends U
   private static final String GMA_CREATE_ALL_SQL = "gma-create-all.sql";
 
   /**
-   * Constructs an {@link ImmutableLocalDAO} from a hard-coded URN-Aspect map.
+   * Constructs an {@link ImmutableLocalDAO} from a hard-coded URN-Aspect map
    */
   public ImmutableLocalDAO(@Nonnull Class<ASPECT_UNION> aspectUnionClass,
       @Nonnull Map<URN, ? extends RecordTemplate> urnAspectMap) {
@@ -60,7 +60,7 @@ public ImmutableLocalDAO(@Nonnull Class<ASPECT_UNION> aspectUnionClass,
   /**
    * Loads a map of URN to aspect values from an {@link InputStream}.
    *
-   * <p>The InputStream is expected to contain a JSON map where the keys are a specific type of URN and values are a
+   * The InputStream is expected to contain a JSON map where the keys are a specific type of URN and values are a
    * specific type of metadata aspect.
    */
   @Nonnull

File: metadata-dao-impl/ebean-dao/src/main/java/com/linkedin/metadata/dao/utils/DatasetUrnPathExtractor.java
Patch:
@@ -7,9 +7,6 @@
 import javax.annotation.Nonnull;
 
 
-/**
- * Maps schema paths to values for DatasetUrns.
- */
 public class DatasetUrnPathExtractor implements UrnPathExtractor<DatasetUrn> {
   @Override
   public Map<String, Object> extractPaths(@Nonnull DatasetUrn urn) {

File: metadata-dao-impl/ebean-dao/src/main/java/com/linkedin/metadata/dao/utils/RegisteredUrnPathExtractors.java
Patch:
@@ -10,7 +10,7 @@
 /**
  * A class that holds all the registered {@link UrnPathExtractor}s.
  *
- * <p>Register new type of urn path extractors by adding them to {@link #REGISTERED_URN_PATH_EXTRACTORS}.
+ * Register new type of urn path extractors by adding them to {@link #REGISTERED_URN_PATH_EXTRACTORS}.
  */
 public class RegisteredUrnPathExtractors {
 

File: metadata-dao-impl/ebean-dao/src/main/java/com/linkedin/metadata/dao/utils/UrnPathExtractor.java
Patch:
@@ -5,9 +5,6 @@
 import javax.annotation.Nonnull;
 
 
-/**
- * Given an urn, extracts a map of schema key to value.
- */
 public interface UrnPathExtractor<URN extends Urn> {
   @Nonnull
   Map<String, Object> extractPaths(@Nonnull URN urn);

File: metadata-dao-impl/elasticsearch-dao/src/main/java/com/linkedin/metadata/dao/search/ESAutoCompleteQueryForHighCardinalityFields.java
Patch:
@@ -44,7 +44,7 @@ SearchRequest constructAutoCompleteQuery(@Nonnull String input, @Nonnull String
   }
 
   /**
-   * Constructs auto complete query given request.
+   * Constructs auto complete query given request
    *
    * @param input the type ahead query text
    * @param field the field name for the auto complete

File: metadata-dao-impl/elasticsearch-dao/src/main/java/com/linkedin/metadata/dao/search/ESAutoCompleteQueryForLowCardinalityFields.java
Patch:
@@ -44,7 +44,7 @@ SearchRequest constructAutoCompleteQuery(@Nonnull String input, @Nonnull String
   }
 
   /**
-   * Constructs auto complete query given request.
+   * Constructs auto complete query given request
    *
    * @param input the type ahead query text
    * @param field the field name for the auto complete

File: metadata-dao-impl/elasticsearch-dao/src/main/java/com/linkedin/metadata/dao/utils/SearchUtils.java
Patch:
@@ -24,7 +24,7 @@ private SearchUtils() {
   }
 
   /**
-   * Validates the request params and create a request map out of it.
+   * Validates the request params and create a request map out of it
    *
    * @param requestParams the search request with fields and values
    * @return a request map
@@ -45,7 +45,7 @@ public static Map<String, String> getRequestMap(@Nullable Filter requestParams)
   }
 
   /**
-   * Builds search query using criterion.
+   * Builds search query using criterion
    *
    * @param criterion {@link Criterion} single criterion which contains field, value and a comparison operator
    * @return QueryBuilder

File: metadata-dao-impl/kafka-producer/src/main/java/com/linkedin/metadata/dao/producer/KafkaMetadataEventProducer.java
Patch:
@@ -40,7 +40,7 @@ public class KafkaMetadataEventProducer<SNAPSHOT extends RecordTemplate, ASPECT_
   private final Optional<Callback> _callback;
 
   /**
-   * Constructor.
+   * Constructor
    *
    * @param snapshotClass The snapshot class for the produced events
    * @param aspectUnionClass The aspect union in the snapshot
@@ -54,7 +54,7 @@ public KafkaMetadataEventProducer(@Nonnull Class<SNAPSHOT> snapshotClass,
   }
 
   /**
-   * Constructor.
+   * Constructor
    *
    * @param snapshotClass The snapshot class for the produced events
    * @param aspectUnionClass The aspect union in the snapshot

File: metadata-dao-impl/neo4j-dao/src/main/java/com/linkedin/metadata/dao/Neo4jQueryDAO.java
Patch:
@@ -237,11 +237,11 @@ List<List<RecordTemplate>> findPaths(
   }
 
   /**
-   * Runs a query statement with parameters and return StatementResult.
+   * Runs a query statement with parameters and return StatementResult
    *
    * @param statement a statement with parameters to be executed
    * @param mapperFunction lambda to transform query result
-   * @return list of elements in the query result
+   * @return List<T> list of elements in the query result
    */
   @Nonnull
   private <T> List<T> runQuery(@Nonnull Statement statement, @Nonnull Function<Record, T> mapperFunction) {

File: metadata-dao-impl/neo4j-dao/src/main/java/com/linkedin/metadata/dao/internal/Neo4jGraphWriterDAO.java
Patch:
@@ -103,7 +103,7 @@ private void executeStatements(@Nonnull List<Statement> statements) {
   }
 
   /**
-   * Run a query statement with parameters and return StatementResult.
+   * Run a query statement with parameters and return StatementResult
    *
    * @param statement a statement with parameters to be executed
    */
@@ -208,7 +208,7 @@ private <URN extends Urn> Statement removeNode(@Nonnull URN urn) {
   }
 
   /**
-   * Gets Node based on Urn, if not exist, creates placeholder node.
+   * Gets Node based on Urn, if not exist, creates placeholder node
    */
   @Nonnull
   private Statement getOrInsertNode(@Nonnull Urn urn) {

File: metadata-dao-impl/restli-dao/src/main/java/com/linkedin/metadata/dao/BaseActionRequestBuilder.java
Patch:
@@ -30,7 +30,7 @@
 /**
  * A base class for generating rest.li requests against entity-specific action methods.
  *
- * <p>See http://go/gma for more details.
+ * See http://go/gma for more details.
  *
  * @param <SNAPSHOT> must be a valid snapshot type defined in com.linkedin.metadata.snapshot
  * @param <URN> must be the URN type used in {@code SNAPSHOT}

File: metadata-dao-impl/restli-dao/src/main/java/com/linkedin/metadata/dao/internal/RestliRemoteWriterDAO.java
Patch:
@@ -14,7 +14,7 @@
 /**
  * A rest.li implementation of {@link BaseRemoteWriterDAO}.
  *
- * <p>Uses rest.li snapshot endpoints to update metadata on remote services.
+ * Uses rest.li snapshot endpoints to update metadata on remote services.
  */
 public class RestliRemoteWriterDAO extends BaseRemoteWriterDAO {
 

File: metadata-dao/src/main/java/com/linkedin/metadata/dao/BaseReadDAO.java
Patch:
@@ -72,7 +72,7 @@ public <ASPECT extends RecordTemplate> Optional<ASPECT> get(@Nonnull Class<ASPEC
   /**
    * Similar to {@link #get(Class, Urn)} but retrieves multiple aspects latest versions associated with multiple URNs.
    *
-   * <p>The returned {@link Map} contains all the .
+   * The returned {@link Map} contains all the .
    */
   @Nonnull
   public Map<URN, Map<Class<? extends RecordTemplate>, Optional<? extends RecordTemplate>>> get(

File: metadata-dao/src/main/java/com/linkedin/metadata/dao/BaseRemoteDAO.java
Patch:
@@ -8,8 +8,8 @@
 /**
  * A base class for all Remote DAO.
  *
- * <p>Remote DAO is a standardized interface to fetch aspects stored on a remote service. See http://go/gma for more
- * details.
+ * Remote DAO is a standardized interface to fetch aspects stored on a remote service.
+ * See http://go/gma for more details.
  *
  * @param <ASPECT_UNION> must be an aspect union type defined in com.linkedin.metadata.aspect
  */

File: metadata-dao/src/main/java/com/linkedin/metadata/dao/BaseSearchWriterDAO.java
Patch:
@@ -8,7 +8,7 @@
 /**
  * A base class for all Search Writer DAOs.
  *
- * <p>Search Writer DAO is a standardized interface to update a search index.
+ * Search Writer DAO is a standardized interface to update a search index.
  */
 public abstract class BaseSearchWriterDAO<DOCUMENT extends RecordTemplate> {
 

File: metadata-dao/src/main/java/com/linkedin/metadata/dao/internal/BaseGraphWriterDAO.java
Patch:
@@ -10,7 +10,7 @@
 /**
  * A base class for all Graph Writer DAOs.
  *
- * <p>Graph Writer DAO is a standardized interface to update a centralized graph DB.
+ * Graph Writer DAO is a standardized interface to update a centralized graph DB.
  */
 public abstract class BaseGraphWriterDAO {
 
@@ -74,7 +74,7 @@ public <RELATIONSHIP extends RecordTemplate> void addRelationship(@Nonnull RELAT
   }
 
   /**
-   * Adds a relationship in the graph, with removal operations before adding.
+   * Adds a relationship in the graph, with removal operations before adding
    *
    * @param relationship the relationship to be persisted
    * @param removalOption whether to remove existing relationship of the same type
@@ -99,7 +99,7 @@ public <RELATIONSHIP extends RecordTemplate> void addRelationships(@Nonnull List
   }
 
   /**
-   * Adds a batch of relationships in the graph, with removal operations before adding.
+   * Adds a batch of relationships in the graph, with removal operations before adding
    *
    * @param relationships the list of relationships to be persisted
    * @param removalOption whether to remove existing relationship of the same type

File: metadata-dao/src/main/java/com/linkedin/metadata/dao/internal/BaseRemoteWriterDAO.java
Patch:
@@ -8,13 +8,13 @@
 /**
  * A base class for all remote writer DAOs.
  *
- * <p>Remote writer DAO allows updating metadata aspects hosted on a remote service without knowing the exact
+ * Remote writer DAO allows updating metadata aspects hosted on a remote service without knowing the exact
  * URN-to-service mapping.
  */
 public abstract class BaseRemoteWriterDAO {
 
   /**
-   * Creates a new metadata snapshot against a remote service.
+   * Creates a new metadata snapshot against a remote service
    *
    * @param urn the {@link Urn} for the entity
    * @param snapshot the snapshot containing updated metadata aspects

File: metadata-dao/src/main/java/com/linkedin/metadata/dao/producer/BaseMetadataEventProducer.java
Patch:
@@ -11,7 +11,7 @@
 /**
  * A base class for all metadata event producers.
  *
- *<p>See http://go/gma for more details.
+ * See http://go/gma for more details.
  */
 public abstract class BaseMetadataEventProducer<SNAPSHOT extends RecordTemplate, ASPECT_UNION extends UnionTemplate, URN extends Urn> {
 

File: metadata-dao/src/main/java/com/linkedin/metadata/dao/retention/TimeBasedRetention.java
Patch:
@@ -10,7 +10,7 @@
 public class TimeBasedRetention implements Retention {
 
   /**
-   * Constructs a {@link TimeBasedRetention} object.
+   * Constructs a {@link TimeBasedRetention} object
    *
    * @param maxAgeToRetain maximal age (in milliseconds) to retain. Must be positive.
    */

File: metadata-dao/src/main/java/com/linkedin/metadata/dao/retention/VersionBasedRetention.java
Patch:
@@ -9,7 +9,7 @@
 public class VersionBasedRetention implements Retention {
 
   /**
-   * Constructs a {@link VersionBasedRetention} object.
+   * Constructs a {@link VersionBasedRetention} object
    *
    * @param maxVersionsToRetain maximal number of versions to retain. Must be greater than 0.
    */

File: metadata-dao/src/main/java/com/linkedin/metadata/dao/utils/QueryUtils.java
Patch:
@@ -68,7 +68,7 @@ public static Set<AspectVersion> latestAspectVersions(@Nonnull Set<Class<? exten
   }
 
   /**
-   * Calculates the total page count.
+   * Calculates the total page count
    *
    * @param totalCount total count
    * @param size page size
@@ -82,7 +82,7 @@ public static int getTotalPageCount(int totalCount, int size) {
   }
 
   /**
-   * Calculates whether there is more results.
+   * Calculates whether there is more results
    *
    * @param from offset from the first result you want to fetch
    * @param size page size

File: metadata-models-generator/src/main/java/com/linkedin/metadata/generator/EventSchemaComposer.java
Patch:
@@ -13,7 +13,7 @@
 import static com.linkedin.metadata.generator.SchemaGeneratorUtil.*;
 
 
-/**
+/***
  * Render the property annotations to the MXE pdl schema.
  */
 @Slf4j

File: metadata-models-generator/src/main/java/com/linkedin/metadata/generator/EventSpec.java
Patch:
@@ -6,8 +6,8 @@
 import lombok.Data;
 
 
-/**
- * Getter & setter class for schema event metadata.
+/***
+ *  Getter & setter class for schema event metadata.
  */
 @Data
 public class EventSpec {

File: metadata-models-generator/src/main/java/com/linkedin/metadata/generator/SchemaAnnotationRetriever.java
Patch:
@@ -14,7 +14,7 @@
 import static com.linkedin.metadata.generator.SchemaGeneratorConstants.*;
 
 
-/**
+/***
  * Parse the property annotations from the pdl schema.
  */
 @Slf4j

File: metadata-models-generator/src/main/java/com/linkedin/metadata/generator/SchemaGenerator.java
Patch:
@@ -7,9 +7,6 @@
 import org.rythmengine.Rythm;
 
 
-/**
- * Generates MXE schemas.
- */
 public class SchemaGenerator {
 
   private final DataSchemaParser _dataSchemaParser;

File: metadata-restli-resource/src/main/java/com/linkedin/metadata/restli/BaseBrowsableClient.java
Patch:
@@ -11,8 +11,7 @@
 import javax.annotation.Nullable;
 
 /**
- * Base client that all entities supporting browse as well as search should implement in their respective restli MPs.
- *
+ * Base client that all entities supporting browse as well as search should implement in their respective restli MPs
  * @param <VALUE> the client's value type
  * @param <URN> urn type of the entity
  */
@@ -38,7 +37,7 @@ public abstract BrowseResult browse(@Nonnull String inputPath, @Nullable Map<Str
       throws RemoteInvocationException;
 
   /**
-   * Returns a list of paths for a given urn.
+   * Returns a list of paths for a given urn
    *
    * @param urn Urn of the entity
    * @return all paths that are related to the urn

File: metadata-restli-resource/src/main/java/com/linkedin/metadata/restli/BaseBrowsableEntityResource.java
Patch:
@@ -19,9 +19,9 @@
 
 
 /**
- * A base class for the entity rest.li resource that supports CRUD + search + browse methods.
+ * A base class for the entity rest.li resource that supports CRUD + search + browse methods
  *
- * <p>See http://go/gma for more details
+ * See http://go/gma for more details
  *
  * @param <KEY> the resource's key type
  * @param <VALUE> the resource's value type

File: metadata-restli-resource/src/main/java/com/linkedin/metadata/restli/BaseEntityResource.java
Patch:
@@ -42,7 +42,7 @@
 /**
  * A base class for the entity rest.li resource, that supports CRUD methods.
  *
- * <p>See http://go/gma for more details
+ * See http://go/gma for more details
  *
  * @param <KEY> the resource's key type
  * @param <VALUE> the resource's value type
@@ -335,7 +335,7 @@ protected Set<Class<? extends RecordTemplate>> parseAspectsParam(@Nullable Strin
   }
 
   /**
-   * Returns a map of {@link VALUE} models given the collection of {@link URN}s and set of aspect classes.
+   * Returns a map of {@link VALUE} models given the collection of {@link URN}s and set of aspect classes
    *
    * @param urns collection of urns
    * @param aspectClasses set of aspect classes

File: metadata-restli-resource/src/main/java/com/linkedin/metadata/restli/BaseEntitySimpleKeyResource.java
Patch:
@@ -34,7 +34,7 @@
 /**
  * A base class for the entity rest.li resource where the key is of a primitive (simple) type.
  *
- * <p>See http://go/gma for more details
+ * See http://go/gma for more details
  *
  * @param <KEY> the resource's simple key type
  * @param <VALUE> the resource's value type
@@ -229,7 +229,7 @@ protected Set<Class<? extends RecordTemplate>> parseAspectsParam(@Nullable Strin
   }
 
   /**
-   * Returns a map of {@link VALUE} models given the collection of {@link URN}s and set of aspect classes.
+   * Returns a map of {@link VALUE} models given the collection of {@link URN}s and set of aspect classes
    *
    * @param urns collection of urns
    * @param aspectClasses set of aspect classes

File: metadata-restli-resource/src/main/java/com/linkedin/metadata/restli/BaseSearchableEntityResource.java
Patch:
@@ -33,9 +33,9 @@
 
 
 /**
- * A base class for the entity rest.li resource that supports CRUD + search methods.
+ * A base class for the entity rest.li resource that supports CRUD + search methods
  *
- * <p>See http://go/gma for more details
+ * See http://go/gma for more details
  *
  * @param <KEY> the resource's key type
  * @param <VALUE> the resource's value type

File: metadata-restli-resource/src/main/java/com/linkedin/metadata/restli/BaseSingleAspectEntitySimpleKeyVersionedSubResource.java
Patch:
@@ -25,8 +25,8 @@
  * A base resource class for serving a versioned single-aspect entity values as sub resource. This resource class is
  * meant to be used as a child resource for classes extending from {@link BaseSingleAspectEntitySimpleKeyResource}.
  *
- * <p>The key for the sub-resource is typically a version field which is a Long value. The versioned resources are
- * retrieved using {@link #get(Long)} and {@link #getAllWithMetadata(PagingContext)}.
+ * The key for the sub-resource is typically a version field which is a Long value. The versioned resources
+ * are retrieved using {@link #get(Long)} and {@link #getAllWithMetadata(PagingContext)}.
  *
  * @param <VALUE> the resource's value type
  * @param <URN> must be a valid {@link Urn} type
@@ -40,7 +40,7 @@ public abstract class BaseSingleAspectEntitySimpleKeyVersionedSubResource<
     ASPECT extends RecordTemplate,
     ASPECT_UNION extends UnionTemplate>
     extends CollectionResourceTaskTemplate<Long, VALUE> {
-  // @formatter:on
+   // @formatter:on
 
   private final Class<ASPECT> _aspectClass;
   private final Class<VALUE> _valueClass;

File: metadata-restli-resource/src/main/java/com/linkedin/metadata/restli/BaseSingleAspectSearchableEntitySimpleKeyResource.java
Patch:
@@ -18,7 +18,7 @@
 /**
  * A base class for the single aspect entity rest.li resource that supports CRUD + search methods.
  *
- * <p>See http://go/gma for more details.
+ * See http://go/gma for more details
  *
  * @param <KEY> the resource's key type
  * @param <VALUE> the resource's value type

File: metadata-restli-resource/src/main/java/com/linkedin/metadata/restli/BrowsableClient.java
Patch:
@@ -10,7 +10,7 @@
 
 
 /**
- * Interface which all entities supporting browse should implement in their respective restli MPs.
+ * Interface which all entities supporting browse should implement in their respective restli MPs
  *
  * @deprecated Use {@link BaseBrowsableClient} instead
  */

File: metadata-restli-resource/src/main/java/com/linkedin/metadata/restli/SearchableClient.java
Patch:
@@ -11,9 +11,9 @@
 
 
 /**
- * Interface that all entities that support search and autocomplete should implement in their respective restli MPs.
+ * Interface that all entities that support search and autocomplete should implement in their respective restli MPs
+ * @param <VALUE> the client's value type
  *
- * @param <VALUE> the client's value type.
  * @deprecated Use {@link BaseSearchableClient} instead
  */
 public interface SearchableClient<VALUE extends RecordTemplate> {

File: metadata-testing/metadata-test-models/src/main/java/com/linkedin/testing/TestUtils.java
Patch:
@@ -81,7 +81,7 @@ public static EntityDocument makeDocument(@Nonnull Urn urn) {
   }
 
   /**
-   * Returns all test entity classes.
+   * Returns all test entity classes
    */
   @Nonnull
   public static Set<Class<? extends RecordTemplate>> getAllTestEntities() {
@@ -95,7 +95,7 @@ public static Set<Class<? extends RecordTemplate>> getAllTestEntities() {
   }
 
   /**
-   * Returns all test relationship classes.
+   * Returns all test relationship classes
    */
   @Nonnull
   public static Set<Class<? extends RecordTemplate>> getAllTestRelationships() {

File: metadata-validators/src/main/java/com/linkedin/metadata/validator/SnapshotValidator.java
Patch:
@@ -56,9 +56,9 @@ public static void validateSnapshotSchema(@Nonnull Class<? extends RecordTemplat
   }
 
   /**
-   * Validates that the URN class is unique across all snapshots.
+   * Validates that the URN class is unique across all snapshots
    *
-   * @param snapshotClasses a collection of snapshot classes.
+   * @param classes a collection of snapshot classes.
    */
   public static void validateUniqueUrn(@Nonnull Collection<? extends Class> snapshotClasses) {
     Set<Class> urnClasses = new HashSet<>();

File: metadata-builders/src/main/java/com/linkedin/metadata/builders/graph/relationship/BaseRelationshipBuilder.java
Patch:
@@ -16,15 +16,15 @@ public BaseRelationshipBuilder(@Nonnull Class<ASPECT> aspectClass) {
   }
 
   /**
-   * Returns the aspect class this {@link BaseRelationshipBuilder} supports
+   * Returns the aspect class this {@link BaseRelationshipBuilder} supports.
    */
   @Nonnull
   public Class<ASPECT> supportedAspectClass() {
     return _aspectClass;
   }
 
   /**
-   * Returns a list of corresponding relationship updates for the given metadata aspect
+   * Returns a list of corresponding relationship updates for the given metadata aspect.
    */
   @Nonnull
   public abstract <URN extends Urn> List<GraphBuilder.RelationshipUpdates> buildRelationships(@Nonnull URN urn,

File: metadata-dao-impl/ebean-dao/src/main/java/com/linkedin/metadata/dao/EbeanMetadataId.java
Patch:
@@ -12,6 +12,9 @@
 import lombok.Setter;
 
 
+/**
+ * Unique ID for a piece of metadata stored in MySQL.
+ */
 @Getter
 @Setter
 @AllArgsConstructor

File: metadata-dao-impl/ebean-dao/src/main/java/com/linkedin/metadata/dao/EbeanMetadataIndex.java
Patch:
@@ -14,6 +14,9 @@
 import lombok.experimental.Accessors;
 
 
+/**
+ * Index definition for MySQL metadata.
+ */
 @Getter
 @Setter
 // define composite indexes

File: metadata-dao-impl/ebean-dao/src/main/java/com/linkedin/metadata/dao/ImmutableLocalDAO.java
Patch:
@@ -38,7 +38,7 @@ public class ImmutableLocalDAO<ASPECT_UNION extends UnionTemplate, URN extends U
   private static final String GMA_CREATE_ALL_SQL = "gma-create-all.sql";
 
   /**
-   * Constructs an {@link ImmutableLocalDAO} from a hard-coded URN-Aspect map
+   * Constructs an {@link ImmutableLocalDAO} from a hard-coded URN-Aspect map.
    */
   public ImmutableLocalDAO(@Nonnull Class<ASPECT_UNION> aspectUnionClass,
       @Nonnull Map<URN, ? extends RecordTemplate> urnAspectMap) {
@@ -60,7 +60,7 @@ public ImmutableLocalDAO(@Nonnull Class<ASPECT_UNION> aspectUnionClass,
   /**
    * Loads a map of URN to aspect values from an {@link InputStream}.
    *
-   * The InputStream is expected to contain a JSON map where the keys are a specific type of URN and values are a
+   * <p>The InputStream is expected to contain a JSON map where the keys are a specific type of URN and values are a
    * specific type of metadata aspect.
    */
   @Nonnull

File: metadata-dao-impl/ebean-dao/src/main/java/com/linkedin/metadata/dao/utils/DatasetUrnPathExtractor.java
Patch:
@@ -7,6 +7,9 @@
 import javax.annotation.Nonnull;
 
 
+/**
+ * Maps schema paths to values for DatasetUrns.
+ */
 public class DatasetUrnPathExtractor implements UrnPathExtractor<DatasetUrn> {
   @Override
   public Map<String, Object> extractPaths(@Nonnull DatasetUrn urn) {

File: metadata-dao-impl/ebean-dao/src/main/java/com/linkedin/metadata/dao/utils/RegisteredUrnPathExtractors.java
Patch:
@@ -10,7 +10,7 @@
 /**
  * A class that holds all the registered {@link UrnPathExtractor}s.
  *
- * Register new type of urn path extractors by adding them to {@link #REGISTERED_URN_PATH_EXTRACTORS}.
+ * <p>Register new type of urn path extractors by adding them to {@link #REGISTERED_URN_PATH_EXTRACTORS}.
  */
 public class RegisteredUrnPathExtractors {
 

File: metadata-dao-impl/ebean-dao/src/main/java/com/linkedin/metadata/dao/utils/UrnPathExtractor.java
Patch:
@@ -5,6 +5,9 @@
 import javax.annotation.Nonnull;
 
 
+/**
+ * Given an urn, extracts a map of schema key to value.
+ */
 public interface UrnPathExtractor<URN extends Urn> {
   @Nonnull
   Map<String, Object> extractPaths(@Nonnull URN urn);

File: metadata-dao-impl/elasticsearch-dao/src/main/java/com/linkedin/metadata/dao/search/ESAutoCompleteQueryForHighCardinalityFields.java
Patch:
@@ -44,7 +44,7 @@ SearchRequest constructAutoCompleteQuery(@Nonnull String input, @Nonnull String
   }
 
   /**
-   * Constructs auto complete query given request
+   * Constructs auto complete query given request.
    *
    * @param input the type ahead query text
    * @param field the field name for the auto complete

File: metadata-dao-impl/elasticsearch-dao/src/main/java/com/linkedin/metadata/dao/search/ESAutoCompleteQueryForLowCardinalityFields.java
Patch:
@@ -44,7 +44,7 @@ SearchRequest constructAutoCompleteQuery(@Nonnull String input, @Nonnull String
   }
 
   /**
-   * Constructs auto complete query given request
+   * Constructs auto complete query given request.
    *
    * @param input the type ahead query text
    * @param field the field name for the auto complete

File: metadata-dao-impl/elasticsearch-dao/src/main/java/com/linkedin/metadata/dao/utils/SearchUtils.java
Patch:
@@ -24,7 +24,7 @@ private SearchUtils() {
   }
 
   /**
-   * Validates the request params and create a request map out of it
+   * Validates the request params and create a request map out of it.
    *
    * @param requestParams the search request with fields and values
    * @return a request map
@@ -45,7 +45,7 @@ public static Map<String, String> getRequestMap(@Nullable Filter requestParams)
   }
 
   /**
-   * Builds search query using criterion
+   * Builds search query using criterion.
    *
    * @param criterion {@link Criterion} single criterion which contains field, value and a comparison operator
    * @return QueryBuilder

File: metadata-dao-impl/kafka-producer/src/main/java/com/linkedin/metadata/dao/producer/KafkaMetadataEventProducer.java
Patch:
@@ -40,7 +40,7 @@ public class KafkaMetadataEventProducer<SNAPSHOT extends RecordTemplate, ASPECT_
   private final Optional<Callback> _callback;
 
   /**
-   * Constructor
+   * Constructor.
    *
    * @param snapshotClass The snapshot class for the produced events
    * @param aspectUnionClass The aspect union in the snapshot
@@ -54,7 +54,7 @@ public KafkaMetadataEventProducer(@Nonnull Class<SNAPSHOT> snapshotClass,
   }
 
   /**
-   * Constructor
+   * Constructor.
    *
    * @param snapshotClass The snapshot class for the produced events
    * @param aspectUnionClass The aspect union in the snapshot

File: metadata-dao-impl/neo4j-dao/src/main/java/com/linkedin/metadata/dao/Neo4jQueryDAO.java
Patch:
@@ -237,11 +237,11 @@ List<List<RecordTemplate>> findPaths(
   }
 
   /**
-   * Runs a query statement with parameters and return StatementResult
+   * Runs a query statement with parameters and return StatementResult.
    *
    * @param statement a statement with parameters to be executed
    * @param mapperFunction lambda to transform query result
-   * @return List<T> list of elements in the query result
+   * @return list of elements in the query result
    */
   @Nonnull
   private <T> List<T> runQuery(@Nonnull Statement statement, @Nonnull Function<Record, T> mapperFunction) {

File: metadata-dao-impl/neo4j-dao/src/main/java/com/linkedin/metadata/dao/internal/Neo4jGraphWriterDAO.java
Patch:
@@ -103,7 +103,7 @@ private void executeStatements(@Nonnull List<Statement> statements) {
   }
 
   /**
-   * Run a query statement with parameters and return StatementResult
+   * Run a query statement with parameters and return StatementResult.
    *
    * @param statement a statement with parameters to be executed
    */
@@ -208,7 +208,7 @@ private <URN extends Urn> Statement removeNode(@Nonnull URN urn) {
   }
 
   /**
-   * Gets Node based on Urn, if not exist, creates placeholder node
+   * Gets Node based on Urn, if not exist, creates placeholder node.
    */
   @Nonnull
   private Statement getOrInsertNode(@Nonnull Urn urn) {

File: metadata-dao-impl/restli-dao/src/main/java/com/linkedin/metadata/dao/BaseActionRequestBuilder.java
Patch:
@@ -30,7 +30,7 @@
 /**
  * A base class for generating rest.li requests against entity-specific action methods.
  *
- * See http://go/gma for more details.
+ * <p>See http://go/gma for more details.
  *
  * @param <SNAPSHOT> must be a valid snapshot type defined in com.linkedin.metadata.snapshot
  * @param <URN> must be the URN type used in {@code SNAPSHOT}

File: metadata-dao-impl/restli-dao/src/main/java/com/linkedin/metadata/dao/internal/RestliRemoteWriterDAO.java
Patch:
@@ -14,7 +14,7 @@
 /**
  * A rest.li implementation of {@link BaseRemoteWriterDAO}.
  *
- * Uses rest.li snapshot endpoints to update metadata on remote services.
+ * <p>Uses rest.li snapshot endpoints to update metadata on remote services.
  */
 public class RestliRemoteWriterDAO extends BaseRemoteWriterDAO {
 

File: metadata-dao/src/main/java/com/linkedin/metadata/dao/BaseReadDAO.java
Patch:
@@ -72,7 +72,7 @@ public <ASPECT extends RecordTemplate> Optional<ASPECT> get(@Nonnull Class<ASPEC
   /**
    * Similar to {@link #get(Class, Urn)} but retrieves multiple aspects latest versions associated with multiple URNs.
    *
-   * The returned {@link Map} contains all the .
+   * <p>The returned {@link Map} contains all the .
    */
   @Nonnull
   public Map<URN, Map<Class<? extends RecordTemplate>, Optional<? extends RecordTemplate>>> get(

File: metadata-dao/src/main/java/com/linkedin/metadata/dao/BaseRemoteDAO.java
Patch:
@@ -8,8 +8,8 @@
 /**
  * A base class for all Remote DAO.
  *
- * Remote DAO is a standardized interface to fetch aspects stored on a remote service.
- * See http://go/gma for more details.
+ * <p>Remote DAO is a standardized interface to fetch aspects stored on a remote service. See http://go/gma for more
+ * details.
  *
  * @param <ASPECT_UNION> must be an aspect union type defined in com.linkedin.metadata.aspect
  */

File: metadata-dao/src/main/java/com/linkedin/metadata/dao/BaseSearchWriterDAO.java
Patch:
@@ -8,7 +8,7 @@
 /**
  * A base class for all Search Writer DAOs.
  *
- * Search Writer DAO is a standardized interface to update a search index.
+ * <p>Search Writer DAO is a standardized interface to update a search index.
  */
 public abstract class BaseSearchWriterDAO<DOCUMENT extends RecordTemplate> {
 

File: metadata-dao/src/main/java/com/linkedin/metadata/dao/internal/BaseGraphWriterDAO.java
Patch:
@@ -10,7 +10,7 @@
 /**
  * A base class for all Graph Writer DAOs.
  *
- * Graph Writer DAO is a standardized interface to update a centralized graph DB.
+ * <p>Graph Writer DAO is a standardized interface to update a centralized graph DB.
  */
 public abstract class BaseGraphWriterDAO {
 
@@ -74,7 +74,7 @@ public <RELATIONSHIP extends RecordTemplate> void addRelationship(@Nonnull RELAT
   }
 
   /**
-   * Adds a relationship in the graph, with removal operations before adding
+   * Adds a relationship in the graph, with removal operations before adding.
    *
    * @param relationship the relationship to be persisted
    * @param removalOption whether to remove existing relationship of the same type
@@ -99,7 +99,7 @@ public <RELATIONSHIP extends RecordTemplate> void addRelationships(@Nonnull List
   }
 
   /**
-   * Adds a batch of relationships in the graph, with removal operations before adding
+   * Adds a batch of relationships in the graph, with removal operations before adding.
    *
    * @param relationships the list of relationships to be persisted
    * @param removalOption whether to remove existing relationship of the same type

File: metadata-dao/src/main/java/com/linkedin/metadata/dao/internal/BaseRemoteWriterDAO.java
Patch:
@@ -8,13 +8,13 @@
 /**
  * A base class for all remote writer DAOs.
  *
- * Remote writer DAO allows updating metadata aspects hosted on a remote service without knowing the exact
+ * <p>Remote writer DAO allows updating metadata aspects hosted on a remote service without knowing the exact
  * URN-to-service mapping.
  */
 public abstract class BaseRemoteWriterDAO {
 
   /**
-   * Creates a new metadata snapshot against a remote service
+   * Creates a new metadata snapshot against a remote service.
    *
    * @param urn the {@link Urn} for the entity
    * @param snapshot the snapshot containing updated metadata aspects

File: metadata-dao/src/main/java/com/linkedin/metadata/dao/producer/BaseMetadataEventProducer.java
Patch:
@@ -11,7 +11,7 @@
 /**
  * A base class for all metadata event producers.
  *
- * See http://go/gma for more details.
+ *<p>See http://go/gma for more details.
  */
 public abstract class BaseMetadataEventProducer<SNAPSHOT extends RecordTemplate, ASPECT_UNION extends UnionTemplate, URN extends Urn> {
 

File: metadata-dao/src/main/java/com/linkedin/metadata/dao/retention/TimeBasedRetention.java
Patch:
@@ -10,7 +10,7 @@
 public class TimeBasedRetention implements Retention {
 
   /**
-   * Constructs a {@link TimeBasedRetention} object
+   * Constructs a {@link TimeBasedRetention} object.
    *
    * @param maxAgeToRetain maximal age (in milliseconds) to retain. Must be positive.
    */

File: metadata-dao/src/main/java/com/linkedin/metadata/dao/retention/VersionBasedRetention.java
Patch:
@@ -9,7 +9,7 @@
 public class VersionBasedRetention implements Retention {
 
   /**
-   * Constructs a {@link VersionBasedRetention} object
+   * Constructs a {@link VersionBasedRetention} object.
    *
    * @param maxVersionsToRetain maximal number of versions to retain. Must be greater than 0.
    */

File: metadata-dao/src/main/java/com/linkedin/metadata/dao/utils/QueryUtils.java
Patch:
@@ -68,7 +68,7 @@ public static Set<AspectVersion> latestAspectVersions(@Nonnull Set<Class<? exten
   }
 
   /**
-   * Calculates the total page count
+   * Calculates the total page count.
    *
    * @param totalCount total count
    * @param size page size
@@ -82,7 +82,7 @@ public static int getTotalPageCount(int totalCount, int size) {
   }
 
   /**
-   * Calculates whether there is more results
+   * Calculates whether there is more results.
    *
    * @param from offset from the first result you want to fetch
    * @param size page size

File: metadata-models-generator/src/main/java/com/linkedin/metadata/generator/EventSchemaComposer.java
Patch:
@@ -13,7 +13,7 @@
 import static com.linkedin.metadata.generator.SchemaGeneratorUtil.*;
 
 
-/***
+/**
  * Render the property annotations to the MXE pdl schema.
  */
 @Slf4j

File: metadata-models-generator/src/main/java/com/linkedin/metadata/generator/EventSpec.java
Patch:
@@ -6,8 +6,8 @@
 import lombok.Data;
 
 
-/***
- *  Getter & setter class for schema event metadata.
+/**
+ * Getter & setter class for schema event metadata.
  */
 @Data
 public class EventSpec {

File: metadata-models-generator/src/main/java/com/linkedin/metadata/generator/SchemaAnnotationRetriever.java
Patch:
@@ -14,7 +14,7 @@
 import static com.linkedin.metadata.generator.SchemaGeneratorConstants.*;
 
 
-/***
+/**
  * Parse the property annotations from the pdl schema.
  */
 @Slf4j

File: metadata-models-generator/src/main/java/com/linkedin/metadata/generator/SchemaGenerator.java
Patch:
@@ -7,6 +7,9 @@
 import org.rythmengine.Rythm;
 
 
+/**
+ * Generates MXE schemas.
+ */
 public class SchemaGenerator {
 
   private final DataSchemaParser _dataSchemaParser;

File: metadata-restli-resource/src/main/java/com/linkedin/metadata/restli/BaseBrowsableClient.java
Patch:
@@ -11,7 +11,8 @@
 import javax.annotation.Nullable;
 
 /**
- * Base client that all entities supporting browse as well as search should implement in their respective restli MPs
+ * Base client that all entities supporting browse as well as search should implement in their respective restli MPs.
+ *
  * @param <VALUE> the client's value type
  * @param <URN> urn type of the entity
  */
@@ -37,7 +38,7 @@ public abstract BrowseResult browse(@Nonnull String inputPath, @Nullable Map<Str
       throws RemoteInvocationException;
 
   /**
-   * Returns a list of paths for a given urn
+   * Returns a list of paths for a given urn.
    *
    * @param urn Urn of the entity
    * @return all paths that are related to the urn

File: metadata-restli-resource/src/main/java/com/linkedin/metadata/restli/BaseBrowsableEntityResource.java
Patch:
@@ -19,9 +19,9 @@
 
 
 /**
- * A base class for the entity rest.li resource that supports CRUD + search + browse methods
+ * A base class for the entity rest.li resource that supports CRUD + search + browse methods.
  *
- * See http://go/gma for more details
+ * <p>See http://go/gma for more details
  *
  * @param <KEY> the resource's key type
  * @param <VALUE> the resource's value type

File: metadata-restli-resource/src/main/java/com/linkedin/metadata/restli/BaseEntityResource.java
Patch:
@@ -42,7 +42,7 @@
 /**
  * A base class for the entity rest.li resource, that supports CRUD methods.
  *
- * See http://go/gma for more details
+ * <p>See http://go/gma for more details
  *
  * @param <KEY> the resource's key type
  * @param <VALUE> the resource's value type
@@ -335,7 +335,7 @@ protected Set<Class<? extends RecordTemplate>> parseAspectsParam(@Nullable Strin
   }
 
   /**
-   * Returns a map of {@link VALUE} models given the collection of {@link URN}s and set of aspect classes
+   * Returns a map of {@link VALUE} models given the collection of {@link URN}s and set of aspect classes.
    *
    * @param urns collection of urns
    * @param aspectClasses set of aspect classes

File: metadata-restli-resource/src/main/java/com/linkedin/metadata/restli/BaseEntitySimpleKeyResource.java
Patch:
@@ -34,7 +34,7 @@
 /**
  * A base class for the entity rest.li resource where the key is of a primitive (simple) type.
  *
- * See http://go/gma for more details
+ * <p>See http://go/gma for more details
  *
  * @param <KEY> the resource's simple key type
  * @param <VALUE> the resource's value type
@@ -229,7 +229,7 @@ protected Set<Class<? extends RecordTemplate>> parseAspectsParam(@Nullable Strin
   }
 
   /**
-   * Returns a map of {@link VALUE} models given the collection of {@link URN}s and set of aspect classes
+   * Returns a map of {@link VALUE} models given the collection of {@link URN}s and set of aspect classes.
    *
    * @param urns collection of urns
    * @param aspectClasses set of aspect classes

File: metadata-restli-resource/src/main/java/com/linkedin/metadata/restli/BaseSearchableEntityResource.java
Patch:
@@ -33,9 +33,9 @@
 
 
 /**
- * A base class for the entity rest.li resource that supports CRUD + search methods
+ * A base class for the entity rest.li resource that supports CRUD + search methods.
  *
- * See http://go/gma for more details
+ * <p>See http://go/gma for more details
  *
  * @param <KEY> the resource's key type
  * @param <VALUE> the resource's value type

File: metadata-restli-resource/src/main/java/com/linkedin/metadata/restli/BaseSingleAspectEntitySimpleKeyVersionedSubResource.java
Patch:
@@ -25,8 +25,8 @@
  * A base resource class for serving a versioned single-aspect entity values as sub resource. This resource class is
  * meant to be used as a child resource for classes extending from {@link BaseSingleAspectEntitySimpleKeyResource}.
  *
- * The key for the sub-resource is typically a version field which is a Long value. The versioned resources
- * are retrieved using {@link #get(Long)} and {@link #getAllWithMetadata(PagingContext)}.
+ * <p>The key for the sub-resource is typically a version field which is a Long value. The versioned resources are
+ * retrieved using {@link #get(Long)} and {@link #getAllWithMetadata(PagingContext)}.
  *
  * @param <VALUE> the resource's value type
  * @param <URN> must be a valid {@link Urn} type
@@ -40,7 +40,7 @@ public abstract class BaseSingleAspectEntitySimpleKeyVersionedSubResource<
     ASPECT extends RecordTemplate,
     ASPECT_UNION extends UnionTemplate>
     extends CollectionResourceTaskTemplate<Long, VALUE> {
-   // @formatter:on
+  // @formatter:on
 
   private final Class<ASPECT> _aspectClass;
   private final Class<VALUE> _valueClass;

File: metadata-restli-resource/src/main/java/com/linkedin/metadata/restli/BaseSingleAspectSearchableEntitySimpleKeyResource.java
Patch:
@@ -18,7 +18,7 @@
 /**
  * A base class for the single aspect entity rest.li resource that supports CRUD + search methods.
  *
- * See http://go/gma for more details
+ * <p>See http://go/gma for more details.
  *
  * @param <KEY> the resource's key type
  * @param <VALUE> the resource's value type

File: metadata-restli-resource/src/main/java/com/linkedin/metadata/restli/BrowsableClient.java
Patch:
@@ -10,7 +10,7 @@
 
 
 /**
- * Interface which all entities supporting browse should implement in their respective restli MPs
+ * Interface which all entities supporting browse should implement in their respective restli MPs.
  *
  * @deprecated Use {@link BaseBrowsableClient} instead
  */

File: metadata-restli-resource/src/main/java/com/linkedin/metadata/restli/SearchableClient.java
Patch:
@@ -11,9 +11,9 @@
 
 
 /**
- * Interface that all entities that support search and autocomplete should implement in their respective restli MPs
- * @param <VALUE> the client's value type
+ * Interface that all entities that support search and autocomplete should implement in their respective restli MPs.
  *
+ * @param <VALUE> the client's value type.
  * @deprecated Use {@link BaseSearchableClient} instead
  */
 public interface SearchableClient<VALUE extends RecordTemplate> {

File: metadata-testing/metadata-test-models/src/main/java/com/linkedin/testing/TestUtils.java
Patch:
@@ -81,7 +81,7 @@ public static EntityDocument makeDocument(@Nonnull Urn urn) {
   }
 
   /**
-   * Returns all test entity classes
+   * Returns all test entity classes.
    */
   @Nonnull
   public static Set<Class<? extends RecordTemplate>> getAllTestEntities() {
@@ -95,7 +95,7 @@ public static Set<Class<? extends RecordTemplate>> getAllTestEntities() {
   }
 
   /**
-   * Returns all test relationship classes
+   * Returns all test relationship classes.
    */
   @Nonnull
   public static Set<Class<? extends RecordTemplate>> getAllTestRelationships() {

File: metadata-validators/src/main/java/com/linkedin/metadata/validator/SnapshotValidator.java
Patch:
@@ -56,9 +56,9 @@ public static void validateSnapshotSchema(@Nonnull Class<? extends RecordTemplat
   }
 
   /**
-   * Validates that the URN class is unique across all snapshots
+   * Validates that the URN class is unique across all snapshots.
    *
-   * @param classes a collection of snapshot classes.
+   * @param snapshotClasses a collection of snapshot classes.
    */
   public static void validateUniqueUrn(@Nonnull Collection<? extends Class> snapshotClasses) {
     Set<Class> urnClasses = new HashSet<>();

File: metadata-dao-impl/restli-dao/src/main/java/com/linkedin/metadata/dao/RequestBuilders.java
Patch:
@@ -18,6 +18,7 @@ public class RequestBuilders {
   private static final Set<BaseRequestBuilder> REQUEST_BUILDERS =
       Collections.unmodifiableSet(new HashSet<BaseRequestBuilder>() {
         {
+          add(new CorpGroupActionRequestBuilder());
           add(new CorpUserActionRequestBuilder());
           add(new DataProcessActionRequestBuilder());
           add(new DatasetActionRequestBuilder());

File: metadata-jobs/mce-consumer-job/src/main/java/com/linkedin/metadata/kafka/config/KafkaConfig.java
Patch:
@@ -74,7 +74,7 @@ public KafkaTemplate<String, GenericRecord> kafkaTemplate(KafkaProperties proper
     props.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, kafkaSchemaRegistryUrl);
 
     KafkaTemplate<String, GenericRecord> template =
-        new KafkaTemplate<>(new DefaultKafkaProducerFactory<>(properties.buildProducerProperties()));
+        new KafkaTemplate<>(new DefaultKafkaProducerFactory<>(props));
 
     log.info("KafkaTemplate built successfully");
 

File: gms/impl/src/main/java/com/linkedin/metadata/resources/dataset/DownstreamLineageResource.java
Patch:
@@ -9,7 +9,6 @@
 import com.linkedin.dataset.UpstreamLineage;
 import com.linkedin.metadata.dao.BaseLocalDAO;
 import com.linkedin.metadata.dao.BaseQueryDAO;
-import com.linkedin.metadata.dao.utils.SearchUtils;
 import com.linkedin.metadata.entity.DatasetEntity;
 import com.linkedin.metadata.query.CriterionArray;
 import com.linkedin.metadata.query.Filter;
@@ -62,7 +61,7 @@ public DownstreamLineageResource() {
   @RestMethod.Get
   public Task<DownstreamLineage> get(@PathKeysParam @Nonnull PathKeys keys) {
     final DatasetUrn datasetUrn = getUrn(keys);
-    final Filter filter = SearchUtils.getFilter(Collections.singletonMap("upstreams", datasetUrn.toString()));
+    final Filter filter = newFilter(Collections.singletonMap("upstreams", datasetUrn.toString()));
 
     return RestliUtils.toTask(() -> {
       final List<DatasetUrn> downstreamDatasets = _queryDAO
@@ -97,4 +96,4 @@ private DatasetUrn getUrn(@PathKeysParam @Nonnull PathKeys keys) {
     DatasetKey key = keys.<ComplexResourceKey<DatasetKey, EmptyRecord>>get(DATASET_KEY).getKey();
     return new DatasetUrn(key.getPlatform(), key.getName(), key.getOrigin());
   }
-}
\ No newline at end of file
+}

File: metadata-builders/src/test/java/com/linkedin/metadata/builders/search/CorpGroupIndexBuilderTest.java
Patch:
@@ -48,4 +48,4 @@ public void testGetDocumentsToUpdateFromCorpGroupSnapshot() {
     assertEquals(actualDocs.get(0).getGroups(), Collections.singletonList(groupName));
     assertEquals(actualDocs.get(0).getEmail(), email);
   }
-}
\ No newline at end of file
+}

File: metadata-builders/src/test/java/com/linkedin/metadata/builders/search/CorpUserInfoIndexBuilderTest.java
Patch:
@@ -49,4 +49,4 @@ public void testGetDocumentsToUpdateFromCorpUserSnapshot() {
     assertEquals(actualDocs.get(2).getSkills(), Collections.emptyList());
     assertEquals(actualDocs.get(2).getTeams(), Arrays.asList("team1", "team2"));
   }
-}
\ No newline at end of file
+}

File: metadata-dao-impl/ebean-dao/src/main/java/com/linkedin/metadata/dao/EbeanMetadataIndex.java
Patch:
@@ -11,6 +11,7 @@
 import lombok.Getter;
 import lombok.NonNull;
 import lombok.Setter;
+import lombok.experimental.Accessors;
 
 
 @Getter
@@ -35,6 +36,7 @@
     EbeanMetadataIndex.URN_COLUMN
 })
 @Entity
+@Accessors(chain = true)
 @Table(name = "metadata_index")
 public class EbeanMetadataIndex extends Model {
 
@@ -74,5 +76,4 @@ public class EbeanMetadataIndex extends Model {
 
   @Column(name = DOUBLE_COLUMN)
   protected Double doubleVal;
-
 }
\ No newline at end of file

File: metadata-dao-impl/elasticsearch-dao/src/test/java/com/linkedin/metadata/dao/search/ESSearchDAOTest.java
Patch:
@@ -186,14 +186,14 @@ public void testFilteredQueryWithRangeFilter() throws IOException {
   }
 
   @Test
-  public void testFilteredQueryUnsupportedCondition() throws IOException {
+  public void testFilteredQueryUnsupportedCondition() {
     int from = 0;
     int size = 10;
     final Filter filter2 = new Filter().setCriteria(new CriterionArray(Arrays.asList(
         new Criterion().setField("field_contain").setValue("value_contain").setCondition(Condition.CONTAIN)
     )));
     SortCriterion sortCriterion = new SortCriterion().setOrder(SortOrder.ASCENDING).setField("urn");
-    assertThrows(IllegalArgumentException.class, () -> _searchDAO.getFilteredSearchQuery(filter2, sortCriterion, from, size));
+    assertThrows(UnsupportedOperationException.class, () -> _searchDAO.getFilteredSearchQuery(filter2, sortCriterion, from, size));
   }
 
   private static SearchHit makeSearchHit(int id) {

File: metadata-models-generator/src/main/java/com/linkedin/metadata/generator/SchemaGeneratorConstants.java
Patch:
@@ -14,6 +14,7 @@ private SchemaGeneratorConstants() {
 
   // used in SchemaAnnotationRetriever
   static final String ASPECT = "Aspect";
+  static final String DELTA = "Delta";
   static final String ENTITY_URNS = "EntityUrns";
 
   // used in EventSchemaComposer

File: metadata-restli-resource/src/main/java/com/linkedin/metadata/restli/BaseSearchableClient.java
Patch:
@@ -34,16 +34,16 @@ public BaseSearchableClient(@Nonnull Client restliClient) {
    * @throws RemoteInvocationException when the rest.li request fails
    */
   @Nonnull
-  public abstract CollectionResponse<VALUE> search(@Nonnull String input, @Nonnull StringArray aspectNames, @Nullable Map<String, String> requestFilters,
+  public abstract CollectionResponse<VALUE> search(@Nonnull String input, @Nullable StringArray aspectNames, @Nullable Map<String, String> requestFilters,
       @Nullable SortCriterion sortCriterion, int start, int count) throws RemoteInvocationException;
 
   /**
-   * Similar to {@link #search(String, StringArray, Map, SortCriterion, int, int)} with empty list for aspect names, meaning all aspects will be returned
+   * Similar to {@link #search(String, StringArray, Map, SortCriterion, int, int)} with null for aspect names, meaning all aspects will be returned
    */
   @Nonnull
   public CollectionResponse<VALUE> search(@Nonnull String input, @Nullable Map<String, String> requestFilters,
       @Nullable SortCriterion sortCriterion, int start, int count) throws RemoteInvocationException {
-    return search(input, new StringArray(), requestFilters, sortCriterion, start, count);
+    return search(input, null, requestFilters, sortCriterion, start, count);
   }
 
   /**

File: metadata-restli-resource/src/main/java/com/linkedin/metadata/restli/BaseSearchableEntitySimpleKeyResource.java
Patch:
@@ -73,7 +73,7 @@ public BaseSearchableEntitySimpleKeyResource(
   @Nonnull
   public Task<List<VALUE>> getAll(
       @Nonnull PagingContext pagingContext,
-      @QueryParam(PARAM_ASPECTS) @Optional("[]") @Nonnull String[] aspectNames,
+      @QueryParam(PARAM_ASPECTS) @Optional @Nullable String[] aspectNames,
       @QueryParam(PARAM_FILTER) @Optional @Nullable Filter filter,
       @QueryParam(PARAM_SORT) @Optional @Nullable SortCriterion sortCriterion) {
 
@@ -91,7 +91,7 @@ public Task<List<VALUE>> getAll(
   @Nonnull
   public Task<CollectionResult<VALUE, SearchResultMetadata>> search(
       @QueryParam(PARAM_INPUT) @Nonnull String input,
-      @QueryParam(PARAM_ASPECTS) @Optional("[]") @Nonnull String[] aspectNames,
+      @QueryParam(PARAM_ASPECTS) @Optional @Nullable String[] aspectNames,
       @QueryParam(PARAM_FILTER) @Optional @Nullable Filter filter,
       @QueryParam(PARAM_SORT) @Optional @Nullable SortCriterion sortCriterion,
       @PagingContextParam @Nonnull PagingContext pagingContext) {

File: metadata-restli-resource/src/main/java/com/linkedin/metadata/restli/RestliConstants.java
Patch:
@@ -7,10 +7,12 @@ private RestliConstants() { }
 
   public static final String ACTION_AUTOCOMPLETE = "autocomplete";
   public static final String ACTION_BACKFILL = "backfill";
+  public static final String ACTION_BATCH_BACKFILL = "batchBackfill";
   public static final String ACTION_BROWSE = "browse";
   public static final String ACTION_GET_BROWSE_PATHS = "getBrowsePaths";
   public static final String ACTION_GET_SNAPSHOT = "getSnapshot";
   public static final String ACTION_INGEST = "ingest";
+  public static final String ACTION_LIST_URNS_FROM_INDEX = "listUrnsFromIndex";
 
   public static final String PARAM_INPUT = "input";
   public static final String PARAM_ASPECTS = "aspects";
@@ -23,4 +25,5 @@ private RestliConstants() { }
   public static final String PARAM_LIMIT = "limit";
   public static final String PARAM_SNAPSHOT = "snapshot";
   public static final String PARAM_URN = "urn";
+  public static final String PARAM_URNS = "urns";
 }

File: metadata-builders/src/main/java/com/linkedin/metadata/builders/search/DatasetIndexBuilder.java
Patch:
@@ -72,6 +72,8 @@ private DatasetDocument getDocumentToUpdateFromAspect(@Nonnull DatasetUrn urn, @
     final DatasetDocument doc = setUrnDerivedFields(urn);
     if (datasetProperties.hasDescription()) {
       doc.setDescription(datasetProperties.getDescription());
+    } else {
+      doc.setDescription("");
     }
     return doc;
   }

File: gms/factories/src/main/java/com/linkedin/restli/server/spring/ParallelRestliHttpRequestHandler.java
Patch:
@@ -32,7 +32,6 @@
 import javax.servlet.http.HttpServletResponse;
 import org.springframework.web.HttpRequestHandler;
 
-
 public class ParallelRestliHttpRequestHandler implements HttpRequestHandler {
 
   private RAPServlet _r2Servlet;

File: metadata-jobs/mae-consumer-job/src/main/java/com/linkedin/metadata/kafka/MaeConsumerApplication.java
Patch:
@@ -3,10 +3,9 @@
 import org.springframework.boot.SpringApplication;
 import org.springframework.boot.autoconfigure.SpringBootApplication;
 import org.springframework.boot.autoconfigure.elasticsearch.rest.RestClientAutoConfiguration;
-import org.springframework.boot.autoconfigure.kafka.KafkaAutoConfiguration;
 
 @SuppressWarnings("checkstyle:HideUtilityClassConstructor")
-@SpringBootApplication(exclude = {RestClientAutoConfiguration.class, KafkaAutoConfiguration.class})
+@SpringBootApplication(exclude = {RestClientAutoConfiguration.class})
 public class MaeConsumerApplication {
 
     public static void main(String[] args) {

File: metadata-jobs/mce-consumer-job/src/main/java/com/linkedin/metadata/kafka/MceConsumerApplication.java
Patch:
@@ -3,10 +3,9 @@
 import org.springframework.boot.SpringApplication;
 import org.springframework.boot.autoconfigure.SpringBootApplication;
 import org.springframework.boot.autoconfigure.elasticsearch.rest.RestClientAutoConfiguration;
-import org.springframework.boot.autoconfigure.kafka.KafkaAutoConfiguration;
 
 @SuppressWarnings("checkstyle:HideUtilityClassConstructor")
-@SpringBootApplication(exclude = {RestClientAutoConfiguration.class, KafkaAutoConfiguration.class})
+@SpringBootApplication(exclude = {RestClientAutoConfiguration.class})
 public class MceConsumerApplication {
 
     public static void main(String[] args) {

File: metadata-builders/src/test/java/com/linkedin/metadata/builders/search/CorpGroupIndexBuilderTest.java
Patch:
@@ -20,7 +20,7 @@
 public class CorpGroupIndexBuilderTest {
 
   @Test
-  public void testGetDocumentsToUpdateFromDatasetSnapshot() {
+  public void testGetDocumentsToUpdateFromCorpGroupSnapshot() {
     CorpGroupUrn corpGroupUrn = new CorpGroupUrn("foo");
     CorpGroupSnapshot corpGroupSnapshot = new CorpGroupSnapshot().setUrn(corpGroupUrn).setAspects(new CorpGroupAspectArray());
     String groupName = "bar";

File: metadata-builders/src/test/java/com/linkedin/metadata/builders/search/CorpUserInfoIndexBuilderTest.java
Patch:
@@ -18,7 +18,7 @@
 public class CorpUserInfoIndexBuilderTest {
 
   @Test
-  public void testGetDocumentsToUpdateFromDatasetSnapshot() {
+  public void testGetDocumentsToUpdateFromCorpUserSnapshot() {
     String testerLdap = "fooBar";
     CorpuserUrn corpuserUrn = new CorpuserUrn(testerLdap);
     CorpUserSnapshot corpUserSnapshot =

File: gms/impl/src/main/java/com/linkedin/metadata/resources/dataset/Datasets.java
Patch:
@@ -150,6 +150,7 @@ protected DatasetSnapshot toSnapshot(@Nonnull Dataset dataset, @Nonnull DatasetU
     if (dataset.hasDeprecation()) {
       aspects.add(ModelUtils.newAspectUnion(DatasetAspect.class, dataset.getDeprecation()));
     }
+
     aspects.add(ModelUtils.newAspectUnion(DatasetAspect.class, new Status().setRemoved(dataset.isRemoved())));
     return ModelUtils.newSnapshot(DatasetSnapshot.class, datasetUrn, aspects);
   }

File: metadata-builders/src/main/java/com/linkedin/metadata/builders/search/RegisteredIndexBuilders.java
Patch:
@@ -16,6 +16,7 @@ public class RegisteredIndexBuilders {
       Collections.unmodifiableSet(new HashSet<BaseIndexBuilder>() {
         {
           add(new CorpUserInfoIndexBuilder());
+          add(new DataProcessIndexBuilder());
           add(new DatasetIndexBuilder());
         }
       });

File: metadata-dao-impl/restli-dao/src/main/java/com/linkedin/metadata/dao/RequestBuilders.java
Patch:
@@ -19,6 +19,7 @@ public class RequestBuilders {
       Collections.unmodifiableSet(new HashSet<BaseRequestBuilder>() {
         {
           add(new CorpUserActionRequestBuilder());
+          add(new DataProcessActionRequestBuilder());
           add(new DatasetActionRequestBuilder());
         }
       });

File: datahub-dao/src/main/java/com/linkedin/datahub/dao/table/LineageDao.java
Patch:
@@ -36,7 +36,7 @@ public List<LineageView> getUpstreamLineage(@Nonnull String datasetUrn) throws E
         .collect(Collectors.toSet()));
 
     return upstreamArray.stream()
-        .map(us -> toLineageView(datasets.get(us.getDataset()), us.getType().name(), us.getAuditStamp().getActor().toString()))
+        .map(us -> toLineageView(datasets.get(us.getDataset()), us.getType().name(), us.getAuditStamp()))
         .collect(Collectors.toList());
   }
 
@@ -51,7 +51,7 @@ public List<LineageView> getDownstreamLineage(@Nonnull String datasetUrn) throws
         .collect(Collectors.toSet()));
 
     return downstreamArray.stream()
-        .map(ds -> toLineageView(datasets.get(ds.getDataset()), ds.getType().name(), ds.getAuditStamp().getActor().toString()))
+        .map(ds -> toLineageView(datasets.get(ds.getDataset()), ds.getType().name(), ds.getAuditStamp()))
         .collect(Collectors.toList());
   }
 }

File: metadata-dao/src/main/java/com/linkedin/metadata/dao/equality/EqualityTester.java
Patch:
@@ -1,12 +1,13 @@
 package com.linkedin.metadata.dao.equality;
 
+import com.linkedin.data.template.DataTemplate;
 import javax.annotation.Nonnull;
 
 
 /**
  * An interface for testing equality between two objects of the same type.
  */
-public interface EqualityTester<T> {
+public interface EqualityTester<T extends DataTemplate> {
 
   boolean equals(@Nonnull T o1, @Nonnull T o2);
 }

File: gms/impl/src/main/java/com/linkedin/metadata/configs/DatasetSearchConfig.java
Patch:
@@ -3,13 +3,11 @@
 import com.linkedin.metadata.dao.search.BaseSearchConfig;
 import com.linkedin.metadata.dao.utils.SearchUtils;
 import com.linkedin.metadata.search.DatasetDocument;
-
-import javax.annotation.Nonnull;
 import java.util.Arrays;
 import java.util.Collections;
 import java.util.HashSet;
 import java.util.Set;
-import javax.annotation.Nullable;
+import javax.annotation.Nonnull;
 
 
 public class DatasetSearchConfig extends BaseSearchConfig<DatasetDocument> {

File: gms/impl/src/main/java/com/linkedin/metadata/resources/identity/CorpGroups.java
Patch:
@@ -12,7 +12,6 @@
 import com.linkedin.metadata.query.Filter;
 import com.linkedin.metadata.query.SearchResultMetadata;
 import com.linkedin.metadata.query.SortCriterion;
-import com.linkedin.metadata.restli.BaseRestliAuditor;
 import com.linkedin.metadata.restli.BaseSearchableEntityResource;
 import com.linkedin.metadata.search.CorpGroupDocument;
 import com.linkedin.metadata.snapshot.CorpGroupSnapshot;

File: gms/impl/src/main/java/com/linkedin/metadata/resources/identity/CorpUsers.java
Patch:
@@ -14,7 +14,6 @@
 import com.linkedin.metadata.query.Filter;
 import com.linkedin.metadata.query.SearchResultMetadata;
 import com.linkedin.metadata.query.SortCriterion;
-import com.linkedin.metadata.restli.BaseRestliAuditor;
 import com.linkedin.metadata.restli.BaseSearchableEntityResource;
 import com.linkedin.metadata.search.CorpUserInfoDocument;
 import com.linkedin.metadata.snapshot.CorpUserSnapshot;

File: gms/impl/src/test/java/com/linkedin/metadata/configs/CorpUserSearchConfigTest.java
Patch:
@@ -1,13 +1,11 @@
 package com.linkedin.metadata.configs;
 
-import com.linkedin.metadata.dao.SearchResult;
-import com.linkedin.metadata.utils.ESTestUtils;
-import com.linkedin.metadata.configs.CorpUserSearchConfig;
 import com.linkedin.metadata.dao.search.ESSearchDAO;
 import com.linkedin.metadata.dao.utils.QueryUtils;
 import com.linkedin.metadata.query.AutoCompleteResult;
 import com.linkedin.metadata.query.Filter;
 import com.linkedin.metadata.search.CorpUserInfoDocument;
+import com.linkedin.metadata.utils.ESTestUtils;
 import java.util.Base64;
 import java.util.HashMap;
 import java.util.Map;

File: li-utils/src/main/java/com/linkedin/common/urn/DatasetUrn.java
Patch:
@@ -42,7 +42,7 @@ public FabricType getOriginEntity() {
 
   public static DatasetUrn createFromString(String rawUrn) throws URISyntaxException {
     String content = new Urn(rawUrn).getContent();
-    String[] parts = content.substring(1, content.length()-1).split(",");
+    String[] parts = content.substring(1, content.length() - 1).split(",");
     return new DatasetUrn(DataPlatformUrn.createFromString(parts[0]), parts[1], toFabricType(parts[2]));
   }
 

File: li-utils/src/main/java/com/linkedin/common/urn/Urn.java
Patch:
@@ -82,7 +82,7 @@ public static boolean isUrn(@Nonnull String urn) {
     try {
       final Urn dummy = Urn.createFromString(urn);
       return true;
-    } catch(URISyntaxException e) {
+    } catch (URISyntaxException e) {
       return false;
     }
   }
@@ -104,7 +104,6 @@ public Urn coerceOutput(Object object) throws TemplateOutputCastException {
           throw new TemplateOutputCastException("Invalid URN syntax: " + e.getMessage(), e);
         }
       }
-
     }, Urn.class);
   }
 }
\ No newline at end of file

File: li-utils/src/main/java/com/linkedin/common/urn/UrnUtils.java
Patch:
@@ -10,6 +10,9 @@ public class UrnUtils {
 
     private static final CorpuserUrn UNKNOWN_ACTOR_URN = new CorpuserUrn("unknown");
 
+    private UrnUtils() {
+    }
+
     /**
      * Convert platform + dataset + origin into DatasetUrn
      * @param platformName String, e.g. hdfs, oracle

File: metadata-dao/src/test/java/com/linkedin/metadata/dao/utils/QueryUtilTest.java
Patch:
@@ -1,8 +1,5 @@
 package com.linkedin.metadata.dao.utils;
 
-import com.google.common.collect.ImmutableSet;
-import com.linkedin.common.Ownership;
-import com.linkedin.data.template.RecordTemplate;
 import com.linkedin.metadata.aspect.AspectVersion;
 import com.linkedin.metadata.query.Condition;
 import com.linkedin.metadata.query.Criterion;

File: metadata-jobs/mae-consumer-job/src/main/java/com/linkedin/metadata/kafka/MaeConsumerApplication.java
Patch:
@@ -5,6 +5,7 @@
 import org.springframework.boot.autoconfigure.elasticsearch.rest.RestClientAutoConfiguration;
 import org.springframework.boot.autoconfigure.kafka.KafkaAutoConfiguration;
 
+@SuppressWarnings("checkstyle:HideUtilityClassConstructor")
 @SpringBootApplication(exclude = {RestClientAutoConfiguration.class, KafkaAutoConfiguration.class})
 public class MaeConsumerApplication {
 

File: metadata-jobs/mce-consumer-job/src/main/java/com/linkedin/metadata/kafka/MceConsumerApplication.java
Patch:
@@ -5,6 +5,7 @@
 import org.springframework.boot.autoconfigure.elasticsearch.rest.RestClientAutoConfiguration;
 import org.springframework.boot.autoconfigure.kafka.KafkaAutoConfiguration;
 
+@SuppressWarnings("checkstyle:HideUtilityClassConstructor")
 @SpringBootApplication(exclude = {RestClientAutoConfiguration.class, KafkaAutoConfiguration.class})
 public class MceConsumerApplication {
 

File: metadata-testing/metadata-test-utils/src/main/java/com/linkedin/metadata/utils/ESTestUtils.java
Patch:
@@ -3,7 +3,6 @@
 import com.fasterxml.jackson.databind.JsonNode;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.fasterxml.jackson.databind.node.ObjectNode;
-import java.io.FileInputStream;
 import java.io.IOException;
 import java.util.Collections;
 import java.util.HashMap;

File: metadata-utils/src/main/java/com/linkedin/metadata/restli/DefaultRestliClientFactory.java
Patch:
@@ -20,6 +20,9 @@ public class DefaultRestliClientFactory {
 
   private static final String DEFAULT_REQUEST_TIMEOUT_IN_MS = "10000";
 
+  private DefaultRestliClientFactory() {
+  }
+
   @Nonnull
   public static RestClient getRestLiD2Client(@Nonnull String restLiClientD2ZkHost,
                                              @Nonnull String restLiClientD2ZkPath) {

File: metadata-utils/src/main/java/com/linkedin/metadata/utils/elasticsearch/ElasticsearchConnectorFactory.java
Patch:
@@ -13,6 +13,9 @@ public class ElasticsearchConnectorFactory {
   private static final int DEFAULT_ES_BULK_REQUESTS_LIMIT = 10000;
   private static final int DEFAULT_ES_BULK_FLUSH_PERIOD = 1;
 
+  private ElasticsearchConnectorFactory() {
+  }
+
   public static ElasticsearchConnector createInstance(@Nonnull String host, @Nonnull int port) {
     return new ElasticsearchConnector(Arrays.asList(host), port,
         DEFAULT_ES_THREAD_COUNT,

File: metadata-dao/src/main/java/com/linkedin/metadata/dao/utils/RecordUtils.java
Patch:
@@ -254,7 +254,7 @@ private static <T> T invokeProtectedMethod(Object object, Method method, Object.
       method.setAccessible(true);
       return (T) method.invoke(object, args);
     } catch (IllegalAccessException | InvocationTargetException e) {
-      throw new RuntimeException(e);
+      throw new RuntimeException(e.getCause());
     } finally {
       method.setAccessible(false);
     }

File: datahub-frontend/app/controllers/api/v1/User.java
Patch:
@@ -17,7 +17,6 @@
 import utils.ControllerUtil;
 
 import javax.annotation.Nonnull;
-import java.security.InvalidParameterException;
 import java.util.Collections;
 import java.util.List;
 import java.util.concurrent.TimeUnit;
@@ -49,7 +48,7 @@ public Result getLoggedInUser() {
     try {
       corpUser = _corpUserViewDao.getByUserName(username);
     } catch (Exception e) {
-      throw new InvalidParameterException("Invalid username: " + username);
+      throw new RuntimeException(e);
     }
 
     if (corpUser == null || !corpUser.getUsername().equals(username)

File: metadata-builders/src/main/java/com/linkedin/metadata/builders/graph/DatasetGraphBuilder.java
Patch:
@@ -39,4 +39,4 @@ protected List<? extends RecordTemplate> buildEntities(@Nonnull DatasetSnapshot
 
     return Collections.singletonList(entity);
   }
-}
+}
\ No newline at end of file

File: metadata-dao-impl/ebean-dao/src/main/java/com/linkedin/metadata/dao/ImmutableLocalDAO.java
Patch:
@@ -83,7 +83,7 @@ public static <URN extends Urn, ASPECT extends RecordTemplate> Map<URN, ASPECT>
 
   @Override
   @Nonnull
-  public <ASPECT extends RecordTemplate> Optional<RecordTemplate> add(@Nonnull URN urn, @Nonnull Class<ASPECT> aspectClass,
+  public <ASPECT extends RecordTemplate> RecordTemplate add(@Nonnull URN urn, @Nonnull Class<ASPECT> aspectClass,
       @Nonnull Function<Optional<RecordTemplate>, RecordTemplate> updateLambda, @Nonnull AuditStamp auditStamp,
       int maxTransactionRetry) {
     throw new UnsupportedOperationException("Not supported by immutable DAO");

File: li-utils/src/main/java/com/linkedin/common/urn/CorpGroupUrn.java
Patch:
@@ -8,7 +8,7 @@ public final class CorpGroupUrn extends Urn {
 
   public static final String ENTITY_TYPE = "corpGroup";
 
-  private static final Pattern URN_PATTERN = Pattern.compile("^" + URN_PREFIX + ENTITY_TYPE + ":(\\w+)$");
+  private static final Pattern URN_PATTERN = Pattern.compile("^" + URN_PREFIX + ENTITY_TYPE + ":([\\-\\w]+)$");
 
   private final String groupNameEntity;
 

File: li-utils/src/main/java/com/linkedin/common/urn/CorpuserUrn.java
Patch:
@@ -4,11 +4,12 @@
 import java.util.regex.Matcher;
 import java.util.regex.Pattern;
 
+
 public final class CorpuserUrn extends Urn {
 
   public static final String ENTITY_TYPE = "corpuser";
 
-  private static final Pattern URN_PATTERN = Pattern.compile("^" + URN_PREFIX + ENTITY_TYPE + ":(\\w+)$");
+  private static final Pattern URN_PATTERN = Pattern.compile("^" + URN_PREFIX + ENTITY_TYPE + ":([\\-\\w]+)$");
 
   private final String usernameEntity;
 

File: datahub-dao/src/main/java/com/linkedin/datahub/dao/view/BrowseDAO.java
Patch:
@@ -53,8 +53,8 @@ public JsonNode getJsonFromBrowseResult(@Nonnull BrowseResult browseResult) thro
     final BrowseResultEntityArray browseResultEntityArray = browseResult.getEntities();
     node.set("elements", collectionToArrayNode(browseResultEntityArray.subList(0, browseResultEntityArray.size())));
     node.put("start", browseResult.getFrom());
-    node.put("count", browseResult.getNumEntities());
-    node.put("total", browseResult.getMetadata().getTotalNumEntities());
+    node.put("count", browseResult.getPageSize());
+    node.put("total", browseResult.getNumEntities());
     node.set("metadata", toJsonNode(browseResult.getMetadata()));
     return node;
   }

File: datahub-frontend/app/controllers/api/v2/CorpUser.java
Patch:
@@ -19,6 +19,8 @@
 import javax.annotation.Nonnull;
 import java.net.URISyntaxException;
 
+import static com.linkedin.datahub.util.RestliUtil.*;
+
 public class CorpUser extends Controller {
 
     private static final JsonNode EMPTY_RESPONSE = Json.newObject();
@@ -38,7 +40,7 @@ public CorpUser() {
     @Nonnull
     public Result getCorpUser(@Nonnull String corpUserUrn) {
         try {
-            return ok(Json.toJson(_corpUserViewDao.get(corpUserUrn)));
+            return ok(toJsonNode(_corpUserViewDao.get(corpUserUrn)));
         } catch (Exception e) {
             if (e.toString().contains("Response status 404")) {
                 return notFound(EMPTY_RESPONSE);

File: metadata-dao-impl/restli-dao/src/main/java/com/linkedin/metadata/dao/internal/RestliRemoteWriterDAO.java
Patch:
@@ -7,7 +7,7 @@
 import com.linkedin.metadata.dao.utils.ModelUtils;
 import com.linkedin.r2.RemoteInvocationException;
 import com.linkedin.restli.client.Client;
-import com.linkedin.restli.client.CreateRequest;
+import com.linkedin.restli.client.Request;
 import javax.annotation.Nonnull;
 
 
@@ -29,7 +29,7 @@ public <URN extends Urn> void create(@Nonnull URN urn, @Nonnull RecordTemplate s
       throws IllegalArgumentException, RestliClientException {
     ModelUtils.validateSnapshotUrn(snapshot.getClass(), urn.getClass());
 
-    final CreateRequest request = RequestBuilders.getBuilder(urn).createRequest(urn, snapshot);
+    final Request request = RequestBuilders.getBuilder(urn).createRequest(urn, snapshot);
 
     try {
       _restliClient.sendRequest(request).getResponse();

File: metadata-dao-impl/restli-dao/src/test/java/com/linkedin/metadata/dao/BaseSnapshotRequestBuilderTest.java
Patch:
@@ -9,7 +9,7 @@
 import java.util.List;
 import java.util.Map;
 
-import static junit.framework.Assert.*;
+import static org.testng.Assert.*;
 
 
 public class BaseSnapshotRequestBuilderTest {

File: metadata-dao-impl/restli-dao/src/test/java/com/linkedin/metadata/dao/DatasetGroupSnapshotRequestBuilderTest.java
Patch:
@@ -30,7 +30,8 @@ public void testGetRequest() {
     String aspectName = ModelUtils.getAspectName(DatasetGroupMembership.class);
     DatasetGroupUrn urn = new DatasetGroupUrn("foo", "bar");
 
-    GetRequest<DatasetGroupSnapshot> request = builder.getRequest(aspectName, urn, 1);
+    GetRequest<DatasetGroupSnapshot> request =
+        (GetRequest<DatasetGroupSnapshot>) builder.getRequest(aspectName, urn, 1);
 
     Map<String, Object> keyPaths = Collections.singletonMap("key", new ComplexResourceKey<>(
         new DatasetGroupKey().setNamespace(urn.getNamespaceEntity()).setName(urn.getNameEntity()), new EmptyRecord()));

File: metadata-restli-resource/src/main/java/com/linkedin/metadata/restli/BaseBrowsableEntityResource.java
Patch:
@@ -6,7 +6,6 @@
 import com.linkedin.data.template.UnionTemplate;
 import com.linkedin.metadata.dao.BaseBrowseDAO;
 import com.linkedin.metadata.query.BrowseResult;
-import com.linkedin.metadata.query.CriterionArray;
 import com.linkedin.metadata.query.Filter;
 import com.linkedin.parseq.Task;
 import com.linkedin.restli.server.annotations.Action;
@@ -41,8 +40,6 @@ public abstract class BaseBrowsableEntityResource<
     // @formatter:on
     extends BaseSearchableEntityResource<KEY, VALUE, URN, SNAPSHOT, ASPECT_UNION, DOCUMENT> {
 
-  private static final Filter EMPTY_FILTER = new Filter().setCriteria(new CriterionArray());
-
   public BaseBrowsableEntityResource(@Nonnull Class<SNAPSHOT> snapshotClass,
       @Nonnull Class<ASPECT_UNION> aspectUnionClass) {
     super(snapshotClass, aspectUnionClass);

File: metadata-restli-resource/src/test/java/com/linkedin/metadata/restli/BaseSearchableEntityResourceTest.java
Patch:
@@ -204,7 +204,7 @@ public void testGetAll() {
     Filter filter1 = new Filter().setCriteria(new CriterionArray());
     SortCriterion sortCriterion1 = new SortCriterion().setField("urn").setOrder(SortOrder.ASCENDING);
 
-    when(_mockSearchDAO.search("*", filter1, sortCriterion1, 1, 2)).thenReturn(
+    when(_mockSearchDAO.filter(filter1, sortCriterion1, 1, 2)).thenReturn(
         makeSearchResult(ImmutableList.of(makeDocument(urn1), makeDocument(urn2)), 2, new SearchResultMetadata()));
 
     String[] aspectNames = new String[]{ModelUtils.getAspectName(AspectFoo.class)};
@@ -224,7 +224,7 @@ public void testGetAll() {
     Filter filter2 = new Filter().setCriteria(new CriterionArray());
     filter2.getCriteria().add(new Criterion().setField("removed").setValue("true"));
     SortCriterion sortCriterion2 = new SortCriterion().setField("urn").setOrder(SortOrder.DESCENDING);
-    when(_mockSearchDAO.search("*", filter2, sortCriterion2, 1, 2)).thenReturn(
+    when(_mockSearchDAO.filter(filter2, sortCriterion2, 1, 2)).thenReturn(
         makeSearchResult(ImmutableList.of(makeDocument(urn1), makeDocument(urn2)), 2, new SearchResultMetadata()));
     values =
         runAndWait(_resource.getAll(new PagingContext(1, 2), aspectNames, filter2, sortCriterion2));
@@ -237,7 +237,7 @@ public void testGetAll() {
     // test the case when there is more results in the search index
     Urn urn3 = makeUrn(3);
     AspectKey<Urn, AspectFoo> aspectKey3 = new AspectKey<>(AspectFoo.class, urn3, BaseLocalDAO.LATEST_VERSION);
-    when(_mockSearchDAO.search("*", filter1, sortCriterion1, 1, 3)).thenReturn(
+    when(_mockSearchDAO.filter(filter1, sortCriterion1, 1, 3)).thenReturn(
         makeSearchResult(ImmutableList.of(makeDocument(urn1), makeDocument(urn2), makeDocument(urn3)), 3, new SearchResultMetadata()));
     when(_mockLocalDAO.get(ImmutableSet.of(aspectKey1, aspectKey2, aspectKey3))).thenReturn(
         ImmutableMap.of(aspectKey1, Optional.of(foo), aspectKey2, Optional.empty()));

File: metadata-builders/src/main/java/com/linkedin/metadata/builders/graph/DatasetGroupGraphBuilder.java
Patch:
@@ -3,7 +3,7 @@
 import com.linkedin.common.urn.DatasetGroupUrn;
 import com.linkedin.data.template.RecordTemplate;
 import com.linkedin.metadata.builders.graph.relationship.BaseRelationshipBuilder;
-import com.linkedin.metadata.builders.graph.relationship.DatasetGroupMembershipRelationshipBuilder;
+import com.linkedin.metadata.builders.graph.relationship.IsPartOfBuilderFromDatasetGroupMembership;
 import com.linkedin.metadata.entity.DatasetGroupEntity;
 import com.linkedin.metadata.snapshot.DatasetGroupSnapshot;
 import java.util.Collections;
@@ -18,7 +18,7 @@ public class DatasetGroupGraphBuilder extends BaseGraphBuilder<DatasetGroupSnaps
   private static final Set<BaseRelationshipBuilder> RELATIONSHIP_BUILDERS =
       Collections.unmodifiableSet(new HashSet<BaseRelationshipBuilder>() {
         {
-          add(new DatasetGroupMembershipRelationshipBuilder());
+          add(new IsPartOfBuilderFromDatasetGroupMembership());
         }
       });
 

File: metadata-builders/src/main/java/com/linkedin/metadata/builders/graph/RegisteredGraphBuilders.java
Patch:
@@ -20,6 +20,8 @@ public class RegisteredGraphBuilders {
   private static final List<BaseGraphBuilder> REGISTERED_GRAPH_BUILDERS =
       Collections.unmodifiableList(new LinkedList<BaseGraphBuilder>() {
         {
+          add(new CorpUserGraphBuilder());
+          add(new DatasetGraphBuilder());
           add(new DatasetGroupGraphBuilder());
         }
       });

File: metadata-builders/src/main/java/com/linkedin/metadata/builders/search/SnapshotProcessor.java
Patch:
@@ -87,7 +87,7 @@ public List<RecordTemplate> getDocumentsToUpdate(@Nonnull Snapshot snapshot) {
               builderClass.getConstructor().newInstance().getDocumentsToUpdate((RecordTemplate) obj);
           docsList.addAll(records);
         } catch (InstantiationException | IllegalAccessException | NoSuchMethodException | InvocationTargetException e) {
-          log.error("Failed to get documents due to error ", e.getMessage());
+          log.error("Failed to get documents due to error ", e);
         }
       }
     }

File: metadata-dao-impl/elasticsearch-dao/src/main/java/com/linkedin/metadata/dao/search/ESAutoCompleteQueryForHighCardinalityFields.java
Patch:
@@ -69,7 +69,7 @@ StringArray getSuggestionList(@Nonnull SearchResponse searchResponse, @Nonnull S
     Integer count = 0;
     for (SearchHit hit : hits) {
       Map<String, Object> source = hit.getSource();
-      if (count > limit) {
+      if (count >= limit) {
         break;
       }
       if (source.containsKey(field)) {

File: metadata-dao/src/main/java/com/linkedin/metadata/dao/utils/QueryUtils.java
Patch:
@@ -36,7 +36,7 @@ public static Criterion newCriterion(@Nonnull String field, @Nonnull String valu
   @Nonnull
   public static Filter newFilter(@Nullable Map<String, String> params) {
     if (params == null) {
-      return new Filter();
+      return new Filter().setCriteria(new CriterionArray());
     }
 
     CriterionArray criteria = params.entrySet()

File: metadata-restli-resource/src/test/java/com/linkedin/metadata/restli/BaseSnapshotResourceTest.java
Patch:
@@ -43,7 +43,7 @@ public class BaseSnapshotResourceTest extends BaseEngineTest {
   class SnapshotResource extends BaseSnapshotResource<Urn, EntitySnapshot, EntityAspectUnion> {
 
     public SnapshotResource() {
-      super(EntitySnapshot.class, EntityAspectUnion.class, new DummyRestliAuditor(_mockClock));
+      super(EntitySnapshot.class, EntityAspectUnion.class);
     }
 
     @Nonnull
@@ -78,13 +78,12 @@ public void testCreate() {
     EntityAspectUnion aspect = ModelUtils.newAspectUnion(EntityAspectUnion.class, foo);
     EntitySnapshot snapshot = ModelUtils.newSnapshot(EntitySnapshot.class, _urn, Arrays.asList(aspect));
     when(_mockClock.millis()).thenReturn(100L);
-    AuditStamp auditStamp = makeAuditStamp(BaseRestliAuditor.DEFAULT_ACTOR, null, 100);
 
     CreateResponse response = runAndWait(resource.create(snapshot));
 
     assertFalse(response.hasError());
 
-    verify(_mockLocalDAO, times(1)).add(_urn, foo, auditStamp);
+    verify(_mockLocalDAO, times(1)).add(eq(_urn), eq(foo), any(AuditStamp.class));
     verifyNoMoreInteractions(_mockLocalDAO);
   }
 

File: metadata-dao-impl/elasticsearch-dao/src/main/java/com/linkedin/metadata/dao/utils/ESUtils.java
Patch:
@@ -30,7 +30,7 @@ public static BoolQueryBuilder buildFilterQuery(@Nonnull Map<String, String> req
       BoolQueryBuilder filters = new BoolQueryBuilder();
       // TODO: Remove checking for urn after solving META-10102
       Arrays.stream(Urn.isUrn(entry.getValue()) ? new String[]{entry.getValue()} : entry.getValue().split(","))
-              .forEach(elem -> filters.should(QueryBuilders.matchQuery(entry.getKey(), elem)));
+          .forEach(elem -> filters.should(QueryBuilders.matchQuery(entry.getKey(), elem)));
       boolFilter.must(filters);
     }
     return boolFilter;

File: metadata-restli-resource/src/test/java/com/linkedin/metadata/restli/BaseSearchableEntityResourceTest.java
Patch:
@@ -204,7 +204,6 @@ public void testGetAll() {
     AspectKey<Urn, AspectFoo> aspectKey2 = new AspectKey<>(AspectFoo.class, urn2, BaseLocalDAO.LATEST_VERSION);
 
     Filter filter1 = new Filter().setCriteria(new CriterionArray());
-    filter1.getCriteria().add(new Criterion().setField("removed").setValue("false"));
     SortCriterion sortCriterion1 = new SortCriterion().setField("urn").setOrder(SortOrder.ASCENDING);
 
     when(_mockSearchDAO.search("*", filter1, sortCriterion1, 1, 2)).thenReturn(

File: metadata-jobs/elasticsearch-index-job/src/main/java/com/linkedin/metadata/kafka/ElasticSearchStreamTask.java
Patch:
@@ -9,6 +9,7 @@
 import com.linkedin.metadata.utils.elasticsearch.ElasticsearchConnectorFactory;
 import com.linkedin.metadata.utils.elasticsearch.MCEElasticEvent;
 import com.linkedin.mxe.MetadataAuditEvent;
+import com.linkedin.mxe.Topics;
 import com.linkedin.util.Configuration;
 import io.confluent.kafka.streams.serdes.avro.GenericAvroSerde;
 import lombok.extern.slf4j.Slf4j;
@@ -29,7 +30,7 @@
 public class ElasticSearchStreamTask {
     private static final String DOC_TYPE = "doc";
 
-    private static final String DEFAULT_KAFKA_TOPIC_NAME = "MetadataAuditEvent";
+    private static final String DEFAULT_KAFKA_TOPIC_NAME = Topics.METADATA_AUDIT_EVENT;
     private static final String DEFAULT_ELASTICSEARCH_HOST = "localhost";
     private static final String DEFAULT_ELASTICSEARCH_PORT = "9200";
     private static final String DEFAULT_KAFKA_BOOTSTRAP_SERVER = "localhost:9092";

File: metadata-jobs/mce-consumer-job/src/main/java/com/linkedin/metadata/kafka/MceStreamTask.java
Patch:
@@ -9,6 +9,7 @@
 import com.linkedin.metadata.dao.utils.RecordUtils;
 import com.linkedin.metadata.restli.DefaultRestliClientFactory;
 import com.linkedin.metadata.snapshot.Snapshot;
+import com.linkedin.mxe.Topics;
 import com.linkedin.restli.client.Client;
 import com.linkedin.util.Configuration;
 import io.confluent.kafka.streams.serdes.avro.GenericAvroSerde;
@@ -28,7 +29,7 @@
 @Slf4j
 public class MceStreamTask {
 
-  private static final String DEFAULT_KAFKA_TOPIC_NAME = "MetadataChangeEvent";
+  private static final String DEFAULT_KAFKA_TOPIC_NAME = Topics.METADATA_CHANGE_EVENT;
   private static final String DEFAULT_GMS_HOST = "localhost";
   private static final String DEFAULT_GMS_PORT = "8080";
   private static final String DEFAULT_KAFKA_BOOTSTRAP_SERVER = "localhost:9092";

File: gms/factories/src/main/java/com/linkedin/identity/factory/CorpUserSearchDaoFactory.java
Patch:
@@ -17,7 +17,7 @@ public class CorpUserSearchDaoFactory {
   @Autowired
   ApplicationContext applicationContext;
 
-  @Bean(name = "corpUserSearchDao")
+  @Bean(name = "corpUserSearchDAO")
   @DependsOn({"elasticSearchRestHighLevelClient"})
   @Nonnull
   protected ESSearchDAO createInstance() {

File: metadata-builders/src/main/java/com/linkedin/metadata/builders/search/CorpUserInfoIndexBuilder.java
Patch:
@@ -18,7 +18,7 @@
 public class CorpUserInfoIndexBuilder extends BaseIndexBuilder<CorpUserInfoDocument> {
 
   public CorpUserInfoIndexBuilder() {
-    super(Collections.singletonList(CorpUserSnapshot.class));
+    super(Collections.singletonList(CorpUserSnapshot.class), CorpUserInfoDocument.class);
   }
 
   @Nonnull
@@ -32,6 +32,7 @@ private CorpUserInfoDocument getDocumentToUpdateFromAspect(@Nonnull CorpuserUrn
         .setLdap(urn.getUsernameEntity())
         .setFullName(fullName)
         .setTitle(title)
+        .setActive(corpUserInfo.isActive())
         .setManagerLdap(managerLdap);
   }
 

File: metadata-builders/src/test/java/com/linkedin/metadata/builders/search/CorpUserInfoIndexBuilderTest.java
Patch:
@@ -29,6 +29,7 @@ public void testGetDocumentsToUpdateFromDatasetSnapshot() {
     assertEquals(actualDocs.size(), 1);
     assertEquals(actualDocs.get(0).getUrn(), corpuserUrn);
     assertEquals(actualDocs.get(0).getTitle(), "fooBarEng");
+    assertTrue(actualDocs.get(0).isActive());
 
     CorpUserEditableInfo corpUserEditableInfo1 = new CorpUserEditableInfo().setAboutMe("An Engineer")
         .setSkills(new StringArray(Arrays.asList("skill1", "skill2", "skill3")));

File: metadata-dao-impl/ebean-dao/src/main/java/com/linkedin/metadata/dao/EbeanLocalDAO.java
Patch:
@@ -137,7 +137,7 @@ protected <T> T runInTransactionWithRetry(@Nonnull Supplier<T> block, int maxTra
   @Nullable
   protected <ASPECT extends RecordTemplate> AspectEntry getLatest(@Nonnull URN urn,
       @Nonnull Class<ASPECT> aspectClass) {
-    final PrimaryKey key = new PrimaryKey(urn.toString(), ModelUtils.getAspectName(aspectClass), BaseLocalDAO.LATEST_VERSION);
+    final PrimaryKey key = new PrimaryKey(urn.toString(), ModelUtils.getAspectName(aspectClass), 0L);
     final EbeanMetadataAspect latest = _server.find(EbeanMetadataAspect.class, key);
     if (latest == null) {
       return null;
@@ -247,9 +247,10 @@ private List<EbeanMetadataAspect> batchGet(@Nonnull Set<AspectKey<URN, ? extends
 
   /**
    * Checks if an {@link AspectKey} and a {@link PrimaryKey} for Ebean are equivalent
+   * @param aspectKey Urn needs to do a ignore case match
    */
   private boolean matchKeys(@Nonnull AspectKey<URN, ? extends RecordTemplate> aspectKey, @Nonnull PrimaryKey pk) {
-    return aspectKey.getUrn().toString().equals(pk.getUrn()) && aspectKey.getVersion() == pk.getVersion()
+    return aspectKey.getUrn().toString().equalsIgnoreCase(pk.getUrn()) && aspectKey.getVersion() == pk.getVersion()
         && ModelUtils.getAspectName(aspectKey.getAspectClass()).equals(pk.getAspect());
   }
 

File: metadata-dao-impl/ebean-dao/src/main/java/com/linkedin/metadata/dao/ImmutableLocalDAO.java
Patch:
@@ -82,7 +82,8 @@ public static <URN extends Urn, ASPECT extends RecordTemplate> Map<URN, ASPECT>
   }
 
   @Override
-  public <ASPECT extends RecordTemplate> void add(@Nonnull URN urn, @Nonnull Class<ASPECT> aspectClass,
+  @Nonnull
+  public <ASPECT extends RecordTemplate> Optional<RecordTemplate> add(@Nonnull URN urn, @Nonnull Class<ASPECT> aspectClass,
       @Nonnull Function<Optional<RecordTemplate>, RecordTemplate> updateLambda, @Nonnull AuditStamp auditStamp,
       int maxTransactionRetry) {
     throw new UnsupportedOperationException("Not supported by immutable DAO");

File: metadata-dao-impl/elasticsearch-dao/src/main/java/com/linkedin/metadata/dao/browse/DatasetBrowseConfig.java
Patch:
@@ -3,7 +3,7 @@
 import com.linkedin.metadata.search.DatasetDocument;
 
 public class DatasetBrowseConfig extends BaseBrowseConfig<DatasetDocument> {
-    public Class getSearchDocument() {
-        return DatasetDocument.class;
-    }
+  public Class getSearchDocument() {
+    return DatasetDocument.class;
+  }
 }

File: metadata-dao-impl/elasticsearch-dao/src/main/java/com/linkedin/metadata/dao/search/BaseESAutoCompleteQuery.java
Patch:
@@ -5,6 +5,7 @@
 import com.linkedin.metadata.query.AutoCompleteResult;
 import com.linkedin.metadata.query.Filter;
 import javax.annotation.Nonnull;
+import javax.annotation.Nullable;
 import lombok.extern.slf4j.Slf4j;
 import org.elasticsearch.action.search.SearchRequest;
 import org.elasticsearch.action.search.SearchResponse;
@@ -27,7 +28,7 @@ public BaseESAutoCompleteQuery() {
    */
   @Nonnull
   abstract SearchRequest constructAutoCompleteQuery(@Nonnull String input, @Nonnull String field,
-      @Nonnull Filter requestParams);
+      @Nullable Filter requestParams);
 
   /**
    * Gets a list of suggestions out of raw search hits

File: metadata-dao/src/main/java/com/linkedin/metadata/dao/BaseBrowseDAO.java
Patch:
@@ -5,6 +5,7 @@
 import com.linkedin.metadata.query.Filter;
 import java.util.List;
 import javax.annotation.Nonnull;
+import javax.annotation.Nullable;
 
 
 /**
@@ -25,7 +26,7 @@ public abstract class BaseBrowseDAO {
    * @return a {@link BrowseResult} that contains a list of groups/entities
    */
   @Nonnull
-  public abstract BrowseResult browse(@Nonnull String path, @Nonnull Filter requestParams, int from, int size);
+  public abstract BrowseResult browse(@Nonnull String path, @Nullable Filter requestParams, int from, int size);
 
   /**
    * Gets a list of paths for a given urn

File: metadata-dao/src/main/java/com/linkedin/metadata/dao/BaseReadDAO.java
Patch:
@@ -23,7 +23,7 @@ public abstract class BaseReadDAO<ASPECT_UNION extends UnionTemplate, URN extend
   private final Set<Class<? extends RecordTemplate>> _validMetadataAspects;
 
   public BaseReadDAO(@Nonnull Class<ASPECT_UNION> aspectUnionClass) {
-    AspectValidator.validateSchema(aspectUnionClass);
+    AspectValidator.validateAspectUnionSchema(aspectUnionClass);
 
     _validMetadataAspects = ModelUtils.getValidAspectTypes(aspectUnionClass);
   }

File: metadata-events/mxe-registration/src/main/java/com/linkedin/mxe/RegisterSchemas.java
Patch:
@@ -27,15 +27,15 @@ public static void registerSchema(String topic, String schemaRegistryUrl) {
     CachedSchemaRegistryClient client = createClient(schemaRegistryUrl);
     Schema schema = Configs.TOPIC_SCHEMA_MAP.get(topic);
     System.out.println(String.format("Registering %s using registry %s (size: %d)", topic, schemaRegistryUrl,
-            schema.toString(false).length()));
+        schema.toString(false).length()));
     registerSchema(topic, schema, client);
   }
 
   public static void main(final String[] args) {
     final String url = args.length == 1 ? args[0] : DEFAULT_SCHEMA_REGISTRY_URL;
     Configs.TOPIC_SCHEMA_MAP.forEach((topic, schema) -> {
       System.out.println(String.format("Registering %s using registry %s (size: %d)", topic,
-              url, schema.toString(false).length()));
+          url, schema.toString(false).length()));
       registerSchema(topic, url);
     });
   }

File: metadata-restli-resource/src/main/java/com/linkedin/metadata/restli/BaseAspectResource.java
Patch:
@@ -28,6 +28,8 @@
  * @param <URN> must be a valid {@link Urn} type
  * @param <ASPECT_UNION> must be a valid union of aspect models defined in com.linkedin.metadata.aspect
  * @param <ASPECT> must be a valid aspect type inside ASPECT_UNION
+ *
+ * @deprecated Use {@link BaseVersionedAspectResource} instead.
  */
 public abstract class BaseAspectResource<
     // @formatter:off

File: metadata-restli-resource/src/main/java/com/linkedin/metadata/restli/BaseAutocompleteResource.java
Patch:
@@ -9,6 +9,7 @@
 import com.linkedin.restli.server.annotations.RestMethod;
 import com.linkedin.restli.server.resources.SimpleResourceTaskTemplate;
 import javax.annotation.Nonnull;
+import javax.annotation.Nullable;
 
 import static com.linkedin.metadata.restli.RestliConstants.*;
 
@@ -45,12 +46,12 @@ public abstract class BaseAutocompleteResource<DOCUMENT extends RecordTemplate>
   @RestMethod.Get
   @Nonnull
   protected abstract Task<AutoCompleteResult> autoComplete(@QueryParam(PARAM_INPUT) @Nonnull String input,
-      @QueryParam(PARAM_FIELD) @Nonnull String field, @QueryParam(PARAM_FILTER) @Nonnull Filter filter,
+      @QueryParam(PARAM_FIELD) @Nullable String field, @QueryParam(PARAM_FILTER) @Nullable Filter filter,
       @QueryParam(PARAM_LIMIT) int limit);
 
   @Nonnull
   protected Task<AutoCompleteResult> getAutoCompleteResult(@Nonnull String input,
-      @Nonnull String field, @Nonnull Filter filter, int limit) {
+      @Nullable String field, @Nullable Filter filter, int limit) {
     return RestliUtils.toTask(() -> getSearchDAO().autoComplete(input, field, filter, limit));
   }
 }

File: metadata-restli-resource/src/main/java/com/linkedin/metadata/restli/BaseBrowseResource.java
Patch:
@@ -18,6 +18,7 @@
 import com.linkedin.restli.server.resources.CollectionResourceTaskTemplate;
 import java.util.LinkedList;
 import javax.annotation.Nonnull;
+import javax.annotation.Nullable;
 
 
 /**
@@ -43,7 +44,7 @@ public abstract class BaseBrowseResource extends CollectionResourceTaskTemplate<
    */
   @Finder("path")
   public Task<CollectionResult<BrowseResultEntity, BrowseResultMetadata>> browse(
-      @QueryParam("inputPath") @Nonnull String inputPath, @QueryParam("filter") @Nonnull Filter filter,
+      @QueryParam("inputPath") @Nonnull String inputPath, @QueryParam("filter") @Nullable Filter filter,
       @PagingContextParam @Nonnull PagingContext pagingContext) {
     return RestliUtils.toTask(() -> {
       final BrowseResult browseResult =

File: metadata-restli-resource/src/main/java/com/linkedin/metadata/restli/BaseSnapshotResource.java
Patch:
@@ -151,6 +151,7 @@ public Task<SNAPSHOT> get(@Nonnull ComplexResourceKey<SnapshotKey, EmptyRecord>
    * @return a snapshot that includes the aspects for which a backfill MAE has been emitted
    */
   @Action(name = BACKFILL_ACTION_NAME)
+  @Nonnull
   public Task<SNAPSHOT> backfill(@ActionParam(ASPECT_NAMES_PARAM_NAME) @Nonnull String[] aspectNames) {
     return RestliUtils.toTask(() -> {
       final URN urn = getUrn(getContext().getPathKeys());

File: metadata-utils/src/main/java/com/linkedin/metadata/utils/elasticsearch/ElasticsearchConnector.java
Patch:
@@ -38,7 +38,7 @@ public class ElasticsearchConnector {
   private static final long DEFAULT_RETRY_INTERVAL = 1L;
 
   public ElasticsearchConnector(List<String> hosts, Integer port, Integer threadCount, Integer bulkRequestsLimit,
-                                Integer bulkFlushPeriod) {
+      Integer bulkFlushPeriod) {
 
     _esPort = port;
     _esHosts = hosts.toArray(new String[0]);

File: metadata-utils/src/main/java/com/linkedin/metadata/utils/elasticsearch/ElasticsearchConnectorFactory.java
Patch:
@@ -15,8 +15,8 @@ public class ElasticsearchConnectorFactory {
 
   public static ElasticsearchConnector createInstance(@Nonnull String host, @Nonnull int port) {
     return new ElasticsearchConnector(Arrays.asList(host), port,
-            DEFAULT_ES_THREAD_COUNT,
-            DEFAULT_ES_BULK_REQUESTS_LIMIT,
-            DEFAULT_ES_BULK_FLUSH_PERIOD);
+        DEFAULT_ES_THREAD_COUNT,
+        DEFAULT_ES_BULK_REQUESTS_LIMIT,
+        DEFAULT_ES_BULK_FLUSH_PERIOD);
   }
 }
\ No newline at end of file

File: wherehows-frontend/app/controllers/api/v2/Dataset.java
Patch:
@@ -100,7 +100,7 @@ public static Promise<Result> listSegments(@Nullable String platform, @Nonnull S
             .collect(Collectors.toList()))));
       }
 
-      List<String> names = DATASET_VIEW_DAO.listSegments(platform, "PROD", getPlatformPrefix(platform, prefix));
+      List<String> names = DATASET_VIEW_DAO.listSegments(platform, "PROD", prefix);
 
       // if prefix is a dataset name, then return empty list
       if (names.size() == 1 && names.get(0).equalsIgnoreCase(prefix)) {

File: wherehows-common/src/main/java/wherehows/common/Constant.java
Patch:
@@ -87,8 +87,8 @@ public class Constant {
   public static final String AW_DB_PASSWORD_KEY = "aw.db.password";
   public static final String AW_DB_NAME_KEY = "aw.db.name";
   public static final String AW_DB_DRIVER_KEY = "aw.db.driver";
-  public static final String AW_DB_PORT_KEY =  "aw.db.port";
-  public static final String AW_ARCHIVE_DIR =  "aw.archive.dir";
+  public static final String AW_DB_PORT_KEY = "aw.db.port";
+  public static final String AW_ARCHIVE_DIR = "aw.archive.dir";
   public static final String AW_REMOTE_HADOOP_SCRIPT_DIR = "aw.remote_hadoop_script_dir";
   public static final String AW_LOCAL_SCRIPT_PATH = "aw.local_script_path";
   public static final String AW_REMOTE_SCRIPT_PATH = "aw.remote_script_path";
@@ -288,5 +288,4 @@ public class Constant {
   public static final String ELASTICSEARCH_URL_REQUEST_TIMEOUT = "elasticsearch.url.request.timeout";
   public static final String WH_DB_MAX_RETRY_TIMES = "wh.db.max.retry.times";
   public static final String WH_ELASTICSEARCH_INDEX_MAPPING_FILE = "wh.elasticsearch.index.mapping.file";
-
 }

File: wherehows-common/src/main/java/wherehows/common/DatasetPath.java
Patch:
@@ -30,8 +30,6 @@ public class DatasetPath {
   public String partitionType;
   public int layoutId;
 
-
-
   /**
    * Use pattern match to separate the comma string.
    * because we need to exclude edge case : /data/path/{2015/10/20,2015/10/21,2015/10/22}

File: wherehows-common/src/main/java/wherehows/common/jobs/Launcher.java
Patch:
@@ -37,8 +37,6 @@ public class Launcher {
   /** command line config file location parameter key */
   private static final String CONFIG_FILE_LOCATION_KEY = "config";
 
-
-
   /**
    * Read config file location from command line. Read all configuration from command line, execute the job.
    * Example command line : java -Dconfig=/path/to/config/file -cp "lib/*" wherehows.common.jobs.Launcher

File: wherehows-common/src/main/java/wherehows/common/schemas/ApplicationRecord.java
Patch:
@@ -23,14 +23,14 @@ public class ApplicationRecord extends AbstractRecord {
   DeploymentRecord deploymentDetail; // not using the dataset info in DeploymentRecord
   String uri;
 
+  public ApplicationRecord() {
+  }
+
   @Override
   public List<Object> fillAllFields() {
     return null;
   }
 
-  public ApplicationRecord() {
-  }
-
   public String getType() {
     return type;
   }

File: wherehows-common/src/main/java/wherehows/common/schemas/AppworxFlowDagRecord.java
Patch:
@@ -27,7 +27,7 @@ public class AppworxFlowDagRecord extends AbstractRecord {
   Long whExecId;
 
   public AppworxFlowDagRecord(Integer appId, Long flowId, String flowPath, Integer flowVersion, String sourceJobPath,
-    String targetJobPath, Long whExecId) {
+      String targetJobPath, Long whExecId) {
     this.appId = appId;
     this.flowId = flowId;
     this.flowPath = flowPath;

File: wherehows-common/src/main/java/wherehows/common/schemas/AppworxFlowOwnerRecord.java
Patch:
@@ -26,7 +26,7 @@ public class AppworxFlowOwnerRecord extends AbstractRecord {
   Long whExecId;
 
   public AppworxFlowOwnerRecord(Integer appId, String flowPath, String ownerId, String permissions, String ownerType,
-    Long whExecId) {
+      Long whExecId) {
     this.appId = appId;
     this.flowPath = flowPath;
     this.ownerId = ownerId;

File: wherehows-common/src/main/java/wherehows/common/schemas/AppworxFlowRecord.java
Patch:
@@ -33,8 +33,7 @@ public class AppworxFlowRecord extends AbstractRecord {
   Long whExecId;
 
   public AppworxFlowRecord(Integer appId, Long flowId, String flowName, String flowGroup, String flowPath,
-                           Integer flowLevel, Long sourceModifiedTime,
-                           Integer sourceVersion, Character isActive, Long whExecId) {
+      Integer flowLevel, Long sourceModifiedTime, Integer sourceVersion, Character isActive, Long whExecId) {
     this.appId = appId;
     this.flowId = flowId;
     this.flowName = flowName;

File: wherehows-common/src/main/java/wherehows/common/schemas/AppworxFlowScheduleRecord.java
Patch:
@@ -28,7 +28,7 @@ public class AppworxFlowScheduleRecord extends AbstractRecord {
   Long whExecId;
 
   public AppworxFlowScheduleRecord(Integer appId, String flowPath, String unit, Integer frequency,
-    Long effectiveStartTime, Long effectiveEndTime, String refId, Long whExecId) {
+      Long effectiveStartTime, Long effectiveEndTime, String refId, Long whExecId) {
     this.appId = appId;
     this.flowPath = flowPath;
     this.unit = unit;

File: wherehows-common/src/main/java/wherehows/common/schemas/AppworxJobRecord.java
Patch:
@@ -31,7 +31,7 @@ public class AppworxJobRecord extends AbstractRecord {
   Long whExecId;
 
   public AppworxJobRecord(Integer appId, Long flowId, String flowPath, Integer sourceVersion, Long jobId,
-                          String jobName, String jobPath, String jobType, Character isCurrent, Long whExecId) {
+      String jobName, String jobPath, String jobType, Character isCurrent, Long whExecId) {
     this.appId = appId;
     this.flowId = flowId;
     this.flowPath = flowPath;

File: wherehows-common/src/main/java/wherehows/common/schemas/AzkabanFlowDagRecord.java
Patch:
@@ -29,7 +29,7 @@ public class AzkabanFlowDagRecord extends AbstractRecord {
   Long whExecId;
 
   public AzkabanFlowDagRecord(Integer appId, String flowPath, Integer flowVersion, String sourceJobPath,
-    String targetJobPath, Long whExecId) {
+      String targetJobPath, Long whExecId) {
     this.appId = appId;
     this.flowPath = flowPath;
     this.flowVersion = flowVersion;

File: wherehows-common/src/main/java/wherehows/common/schemas/AzkabanFlowExecRecord.java
Patch:
@@ -33,8 +33,9 @@ public class AzkabanFlowExecRecord extends AbstractRecord {
   Long endTime;
   Long whExecId;
 
-  public AzkabanFlowExecRecord(Integer appId, String flowName, String flowPath, Integer sourceVersion, Integer flowExecId,
-    String flowExecStatus, Integer attemptId, String executedBy, Long startTime, Long endTime, Long whExecId) {
+  public AzkabanFlowExecRecord(Integer appId, String flowName, String flowPath, Integer sourceVersion,
+      Integer flowExecId, String flowExecStatus, Integer attemptId, String executedBy, Long startTime, Long endTime,
+      Long whExecId) {
     this.appId = appId;
     this.flowName = flowName;
     this.flowPath = flowPath;

File: wherehows-common/src/main/java/wherehows/common/schemas/AzkabanFlowOwnerRecord.java
Patch:
@@ -29,7 +29,7 @@ public class AzkabanFlowOwnerRecord extends AbstractRecord {
   Long whExecId;
 
   public AzkabanFlowOwnerRecord(Integer appId, String flowPath, String ownerId, String permissions, String ownerType,
-    Long whExecId) {
+      Long whExecId) {
     this.appId = appId;
     this.flowPath = flowPath;
     this.ownerId = ownerId;

File: wherehows-common/src/main/java/wherehows/common/schemas/AzkabanFlowRecord.java
Patch:
@@ -31,8 +31,8 @@ public class AzkabanFlowRecord extends AbstractRecord {
   Character isActive;
   Long whExecId;
 
-  public AzkabanFlowRecord(Integer appId, String flowName, String flowGroup, String flowPath, Integer flowLevel, Long sourceModifiedTime,
-    Integer sourceVersion, Character isActive, Long whExecId) {
+  public AzkabanFlowRecord(Integer appId, String flowName, String flowGroup, String flowPath, Integer flowLevel,
+      Long sourceModifiedTime, Integer sourceVersion, Character isActive, Long whExecId) {
     this.appId = appId;
     this.flowName = flowName;
     this.flowGroup = flowGroup;

File: wherehows-common/src/main/java/wherehows/common/schemas/AzkabanJobRecord.java
Patch:
@@ -32,7 +32,7 @@ public class AzkabanJobRecord extends AbstractRecord {
   Long whExecId;
 
   public AzkabanJobRecord(Integer appId, String flowPath, Integer sourceVersion, String jobName, String jobPath,
-    String jobType, Character isCurrent, Long whExecId) {
+      String jobType, Character isCurrent, Long whExecId) {
     this.appId = appId;
     this.flowPath = flowPath;
     this.sourceVersion = sourceVersion;

File: wherehows-common/src/main/java/wherehows/common/schemas/DatasetComplianceRecord.java
Patch:
@@ -24,6 +24,9 @@ public class DatasetComplianceRecord extends AbstractRecord {
   List<DatasetPurgeEntityRecord> compliancePurgeEntities;
   Long modifiedTime;
 
+  public DatasetComplianceRecord() {
+  }
+
   @Override
   public String[] getDbColumnNames() {
     return new String[]{"dataset_id", "dataset_urn", "compliance_purge_type", "compliance_purge_entities",
@@ -35,9 +38,6 @@ public List<Object> fillAllFields() {
     return null;
   }
 
-  public DatasetComplianceRecord() {
-  }
-
   public Integer getDatasetId() {
     return datasetId;
   }

File: wherehows-common/src/main/java/wherehows/common/schemas/DatasetConfidentialEntityRecord.java
Patch:
@@ -21,14 +21,14 @@ public class DatasetConfidentialEntityRecord extends AbstractRecord {
   String identifierField;
   String logicalType;
 
+  public DatasetConfidentialEntityRecord() {
+  }
+
   @Override
   public List<Object> fillAllFields() {
     return null;
   }
 
-  public DatasetConfidentialEntityRecord() {
-  }
-
   public String getIdentifierField() {
     return identifierField;
   }

File: wherehows-common/src/main/java/wherehows/common/schemas/DatasetConstraintRecord.java
Patch:
@@ -30,6 +30,9 @@ public class DatasetConstraintRecord extends AbstractRecord {
   Map<String, String> additionalReferences;
   Long modifiedTime;
 
+  public DatasetConstraintRecord() {
+  }
+
   @Override
   public String[] getDbColumnNames() {
     return new String[]{"dataset_id", "dataset_urn", "constraint_type", "constraint_sub_type", "constraint_name",
@@ -41,9 +44,6 @@ public List<Object> fillAllFields() {
     return null;
   }
 
-  public DatasetConstraintRecord() {
-  }
-
   public Integer getDatasetId() {
     return datasetId;
   }

File: wherehows-common/src/main/java/wherehows/common/schemas/DatasetFieldRecord.java
Patch:
@@ -42,8 +42,9 @@ public class DatasetFieldRecord implements Record {
   List<Object> allFields;
   char SEPR = 0x001A;
 
-  public DatasetFieldRecord(String urn, Integer sortId, Integer parentSortId, String parentPath, String fieldName, String fieldLabel,
-    String dataType, String isNullable, String isIndexed, String isPartitioned, String defaultValue, Integer dataSize, String namespace, String description) {
+  public DatasetFieldRecord(String urn, Integer sortId, Integer parentSortId, String parentPath, String fieldName,
+      String fieldLabel, String dataType, String isNullable, String isIndexed, String isPartitioned,
+      String defaultValue, Integer dataSize, String namespace, String description) {
 
     this.urn = urn;
     this.sortId = sortId;

File: wherehows-common/src/main/java/wherehows/common/schemas/DatasetGeographicAffinityRecord.java
Patch:
@@ -21,14 +21,14 @@ public class DatasetGeographicAffinityRecord extends AbstractRecord {
   String affinity;
   List<DatasetLocaleRecord> locations;
 
+  public DatasetGeographicAffinityRecord() {
+  }
+
   @Override
   public List<Object> fillAllFields() {
     return null;
   }
 
-  public DatasetGeographicAffinityRecord() {
-  }
-
   public String getAffinity() {
     return affinity;
   }

File: wherehows-common/src/main/java/wherehows/common/schemas/DatasetIdentifier.java
Patch:
@@ -22,14 +22,14 @@ public class DatasetIdentifier extends AbstractRecord {
   String nativeName;
   String dataOrigin;
 
+  public DatasetIdentifier() {
+  }
+
   @Override
   public List<Object> fillAllFields() {
     return null;
   }
 
-  public DatasetIdentifier() {
-  }
-
   public String getDataPlatformUrn() {
     return dataPlatformUrn;
   }

File: wherehows-common/src/main/java/wherehows/common/schemas/DatasetIndexRecord.java
Patch:
@@ -26,6 +26,9 @@ public class DatasetIndexRecord extends AbstractRecord {
   List<DatasetFieldIndexRecord> indexedFields;
   Long modifiedTime;
 
+  public DatasetIndexRecord() {
+  }
+
   @Override
   public String[] getDbColumnNames() {
     return new String[]{"dataset_id", "dataset_urn", "index_type", "index_name", "is_unique", "indexed_fields",
@@ -37,9 +40,6 @@ public List<Object> fillAllFields() {
     return null;
   }
 
-  public DatasetIndexRecord() {
-  }
-
   public Integer getDatasetId() {
     return datasetId;
   }

File: wherehows-common/src/main/java/wherehows/common/schemas/DatasetKeySchemaRecord.java
Patch:
@@ -22,14 +22,14 @@ public class DatasetKeySchemaRecord extends AbstractRecord {
   String format;
   String text;
 
+  public DatasetKeySchemaRecord() {
+  }
+
   @Override
   public List<Object> fillAllFields() {
     return null;
   }
 
-  public DatasetKeySchemaRecord() {
-  }
-
   public String getKeyType() {
     return keyType;
   }

File: wherehows-common/src/main/java/wherehows/common/schemas/DatasetOriginalSchemaRecord.java
Patch:
@@ -23,14 +23,14 @@ public class DatasetOriginalSchemaRecord extends AbstractRecord {
   String text;
   Map<String, String> checksum;
 
+  public DatasetOriginalSchemaRecord() {
+  }
+
   @Override
   public List<Object> fillAllFields() {
     return null;
   }
 
-  public DatasetOriginalSchemaRecord() {
-  }
-
   public String getFormat() {
     return format;
   }

File: wherehows-common/src/main/java/wherehows/common/schemas/DatasetPartitionKeyRecord.java
Patch:
@@ -13,8 +13,6 @@
  */
 package wherehows.common.schemas;
 
-import com.fasterxml.jackson.core.JsonProcessingException;
-import com.fasterxml.jackson.databind.ObjectMapper;
 import java.util.List;
 
 

File: wherehows-common/src/main/java/wherehows/common/schemas/DatasetPartitionRangeRecord.java
Patch:
@@ -23,14 +23,14 @@ public class DatasetPartitionRangeRecord extends AbstractRecord {
   String maxPartitionValue;
   String listPartitionValue;
 
+  public DatasetPartitionRangeRecord() {
+  }
+
   @Override
   public List<Object> fillAllFields() {
     return null;
   }
 
-  public DatasetPartitionRangeRecord() {
-  }
-
   public String getPartitionType() {
     return partitionType;
   }

File: wherehows-common/src/main/java/wherehows/common/schemas/DatasetPropertyRecord.java
Patch:
@@ -23,14 +23,14 @@ public class DatasetPropertyRecord extends AbstractRecord {
   String uri;
   DatasetCaseSensitiveRecord caseSensitivity;
 
+  public DatasetPropertyRecord() {
+  }
+
   @Override
   public List<Object> fillAllFields() {
     return null;
   }
 
-  public DatasetPropertyRecord() {
-  }
-
   public ChangeAuditStamp getChangeAuditStamp() {
     return changeAuditStamp;
   }

File: wherehows-common/src/main/java/wherehows/common/schemas/DatasetPurgeEntityRecord.java
Patch:
@@ -23,14 +23,14 @@ public class DatasetPurgeEntityRecord extends AbstractRecord {
   String logicalType;
   Boolean isSubject;
 
+  public DatasetPurgeEntityRecord() {
+  }
+
   @Override
   public List<Object> fillAllFields() {
     return null;
   }
 
-  public DatasetPurgeEntityRecord() {
-  }
-
   public String getIdentifierType() {
     return identifierType;
   }

File: wherehows-common/src/main/java/wherehows/common/schemas/DatasetRecord.java
Patch:
@@ -73,7 +73,7 @@ public List<Object> fillAllFields() {
     allFields.add(sourceCreatedTime);
     allFields.add(sourceModifiedTime);
     // add the created_date, modified_date and wh_etl_exec_id
-    allFields.add(System.currentTimeMillis()/1000);
+    allFields.add(System.currentTimeMillis() / 1000);
     allFields.add(null);
     allFields.add(null);
     return allFields;

File: wherehows-common/src/main/java/wherehows/common/schemas/DatasetReferenceRecord.java
Patch:
@@ -25,6 +25,9 @@ public class DatasetReferenceRecord extends AbstractRecord {
   List<String> referenceList;
   Long modifiedTime;
 
+  public DatasetReferenceRecord() {
+  }
+
   @Override
   public String[] getDbColumnNames() {
     return new String[]{"dataset_id", "dataset_urn", "reference_type", "reference_format", "reference_list",
@@ -36,9 +39,6 @@ public List<Object> fillAllFields() {
     return null;
   }
 
-  public DatasetReferenceRecord() {
-  }
-
   public Integer getDatasetId() {
     return datasetId;
   }

File: wherehows-common/src/main/java/wherehows/common/schemas/DatasetRetentionRecord.java
Patch:
@@ -22,14 +22,14 @@ public class DatasetRetentionRecord extends AbstractRecord {
   Long retentionWindow;
   String retentionWindowUnit;
 
+  public DatasetRetentionRecord() {
+  }
+
   @Override
   public List<Object> fillAllFields() {
     return null;
   }
 
-  public DatasetRetentionRecord() {
-  }
-
   public String getRetentionType() {
     return retentionType;
   }

File: wherehows-common/src/main/java/wherehows/common/schemas/DatasetSchemaInfoRecord.java
Patch:
@@ -35,6 +35,9 @@ public class DatasetSchemaInfoRecord extends AbstractRecord {
   List<DatasetFieldPathRecord> auditFields;
   Long modifiedTime;
 
+  public DatasetSchemaInfoRecord() {
+  }
+
   @Override
   public String[] getDbColumnNames() {
     return new String[]{"dataset_id", "dataset_urn", "is_backward_compatible", "is_field_name_case_sensitive",
@@ -47,9 +50,6 @@ public List<Object> fillAllFields() {
     return null;
   }
 
-  public DatasetSchemaInfoRecord() {
-  }
-
   public Integer getDatasetId() {
     return datasetId;
   }

File: wherehows-common/src/main/java/wherehows/common/schemas/DatasetSecurityRecord.java
Patch:
@@ -27,6 +27,9 @@ public class DatasetSecurityRecord extends AbstractRecord {
   DatasetGeographicAffinityRecord geographicAffinity;
   Long modifiedTime;
 
+  public DatasetSecurityRecord() {
+  }
+
   @Override
   public String[] getDbColumnNames() {
     return new String[]{"dataset_id", "dataset_urn", "classification", "record_owner_type", "retention_policy",
@@ -38,9 +41,6 @@ public List<Object> fillAllFields() {
     return null;
   }
 
-  public DatasetSecurityRecord() {
-  }
-
   public Integer getDatasetId() {
     return datasetId;
   }

File: wherehows-common/src/main/java/wherehows/common/schemas/DatasetTagRecord.java
Patch:
@@ -23,6 +23,9 @@ public class DatasetTagRecord extends AbstractRecord {
   String tag;
   Long modifiedTime;
 
+  public DatasetTagRecord() {
+  }
+
   @Override
   public String[] getDbColumnNames() {
     return new String[]{"dataset_id", "dataset_urn", "tag", "modified_time"};
@@ -33,9 +36,6 @@ public List<Object> fillAllFields() {
     return null;
   }
 
-  public DatasetTagRecord() {
-  }
-
   public Integer getDatasetId() {
     return datasetId;
   }

File: wherehows-common/src/main/java/wherehows/common/schemas/DeploymentRecord.java
Patch:
@@ -31,6 +31,9 @@ public class DeploymentRecord extends AbstractRecord {
   Map<String, String> additionalDeploymentInfo;
   Long modifiedTime;
 
+  public DeploymentRecord() {
+  }
+
   @Override
   public String[] getDbColumnNames() {
     return new String[]{"dataset_id", "dataset_urn", "deployment_tier", "datacenter", "region", "zone", "cluster",
@@ -42,9 +45,6 @@ public List<Object> fillAllFields() {
     return null;
   }
 
-  public DeploymentRecord() {
-  }
-
   public Integer getDatasetId() {
     return datasetId;
   }

File: wherehows-common/src/main/java/wherehows/common/schemas/LineageDatasetMapRecord.java
Patch:
@@ -21,14 +21,14 @@ public class LineageDatasetMapRecord extends AbstractRecord {
   String mapDirectionType;
   List<LineageFieldRecord> fieldLineage;
 
+  public LineageDatasetMapRecord() {
+  }
+
   @Override
   public List<Object> fillAllFields() {
     return null;
   }
 
-  public LineageDatasetMapRecord() {
-  }
-
   public String getMapDirectionType() {
     return mapDirectionType;
   }

File: wherehows-common/src/main/java/wherehows/common/schemas/LineageFieldMapRecord.java
Patch:
@@ -21,14 +21,14 @@ public class LineageFieldMapRecord extends AbstractRecord {
   DatasetIdentifier mappedToDataset;
   List<String> fieldPaths;
 
+  public LineageFieldMapRecord() {
+  }
+
   @Override
   public List<Object> fillAllFields() {
     return null;
   }
 
-  public LineageFieldMapRecord() {
-  }
-
   public DatasetIdentifier getMappedToDataset() {
     return mappedToDataset;
   }

File: wherehows-common/src/main/java/wherehows/common/schemas/LineageFieldRecord.java
Patch:
@@ -21,14 +21,14 @@ public class LineageFieldRecord extends AbstractRecord {
   String fieldPath;
   List<LineageFieldMapRecord> mappedToFields;
 
+  public LineageFieldRecord() {
+  }
+
   @Override
   public List<Object> fillAllFields() {
     return null;
   }
 
-  public LineageFieldRecord() {
-  }
-
   public String getFieldPath() {
     return fieldPath;
   }

File: wherehows-common/src/main/java/wherehows/common/schemas/OozieFlowDagRecord.java
Patch:
@@ -29,7 +29,7 @@ public class OozieFlowDagRecord extends AbstractRecord {
   Long whExecId;
 
   public OozieFlowDagRecord(Integer appId, String flowPath, String sourceVersion, String sourceJobPath,
-    String targetJobPath, Long whExecId) {
+      String targetJobPath, Long whExecId) {
     this.appId = appId;
     this.flowPath = flowPath;
     this.sourceVersion = sourceVersion;

File: wherehows-common/src/main/java/wherehows/common/schemas/OozieFlowExecRecord.java
Patch:
@@ -34,7 +34,7 @@ public class OozieFlowExecRecord extends AbstractRecord {
   Long whExecId;
 
   public OozieFlowExecRecord(Integer appId, String flowName, String flowPath, String flowExecUuid, String sourceVersion,
-    String flowExecStatus, Integer attemptId, String executedBy, Long startTime, Long endTime, Long whExecId) {
+      String flowExecStatus, Integer attemptId, String executedBy, Long startTime, Long endTime, Long whExecId) {
     this.appId = appId;
     this.flowName = flowName;
     this.flowPath = flowPath;

File: wherehows-common/src/main/java/wherehows/common/schemas/OozieFlowRecord.java
Patch:
@@ -31,7 +31,7 @@ public class OozieFlowRecord extends AbstractRecord {
   Long whExecId;
 
   public OozieFlowRecord(Integer appId, String flowName, String flowPath, Integer flowLevel, String sourceVersion,
-    Long sourceCreatedTime, Long sourceModifiedTime, Long whExecId) {
+      Long sourceCreatedTime, Long sourceModifiedTime, Long whExecId) {
     this.appId = appId;
     this.flowName = flowName;
     this.flowPath = flowPath;

File: wherehows-common/src/main/java/wherehows/common/schemas/OozieFlowScheduleRecord.java
Patch:
@@ -18,7 +18,7 @@
  */
 public class OozieFlowScheduleRecord extends AzkabanFlowScheduleRecord {
   public OozieFlowScheduleRecord(Integer appId, String flowPath, String frequency, Integer interval,
-    String cronExpression, Long effectiveStartTime, Long effectiveEndTime, String refId, Long whExecId) {
+      String cronExpression, Long effectiveStartTime, Long effectiveEndTime, String refId, Long whExecId) {
     super(appId, flowPath, frequency, interval, cronExpression, effectiveStartTime, effectiveEndTime, refId, whExecId);
   }
 }

File: wherehows-common/src/main/java/wherehows/common/schemas/OozieJobRecord.java
Patch:
@@ -30,7 +30,7 @@ public class OozieJobRecord extends AbstractRecord {
   Long whExecId;
 
   public OozieJobRecord(Integer appId, String flowPath, String sourceVersion, String jobName, String jobPath,
-    String jobType, Long whExecId) {
+      String jobType, Long whExecId) {
     this.appId = appId;
     this.flowPath = flowPath;
     this.sourceVersion = sourceVersion;

File: wherehows-common/src/main/java/wherehows/common/schemas/SCMOwnerRecord.java
Patch:
@@ -16,6 +16,7 @@
 import java.util.ArrayList;
 import java.util.List;
 
+
 public class SCMOwnerRecord extends AbstractRecord {
 
   String scmUrl;
@@ -35,7 +36,6 @@ public SCMOwnerRecord(String scmUrl, String databaseName, String databaseType, S
     this.filePath = filePath;
     this.committers = committers;
     this.scmType = scmType;
-
   }
 
   @Override

File: wherehows-common/src/main/java/wherehows/common/utils/GitUtil.java
Patch:
@@ -44,7 +44,6 @@
  */
 
 public class GitUtil {
-  private final static String DEFAULT_HOST = "gitli.corp.linkedin.com";
   public static final String HTTPS_PROTOCAL = "https";
   public static final String GIT_PROTOCAL = "git";
   public static final String GIT_SUBFIX = ".git";

File: wherehows-common/src/main/java/wherehows/common/utils/PartitionPatternMatcher.java
Patch:
@@ -48,7 +48,4 @@ public Integer analyze(String partitionFullPath) {
 
     return null;
   }
-
-
-
 }

File: wherehows-common/src/main/java/wherehows/common/utils/ProcessUtil.java
Patch:
@@ -24,5 +24,4 @@ private ProcessUtil() {
   public static long getCurrentProcessId() {
     return Long.parseLong(ManagementFactory.getRuntimeMXBean().getName().split("@")[0]);
   }
-
 }

File: wherehows-common/src/test/java/wherehows/common/utils/JobsUtilTest.java
Patch:
@@ -124,8 +124,9 @@ public void testGetEnabledJobsByType() throws IOException, ConfigurationExceptio
   public void testMultipleLinesSameProperties() throws IOException, ConfigurationException {
     String dir = "/tmp/";
     String propertyStr =
-        "var1=com.mysql.jdbc.driver\n" + "var1=username\n" + "var1=password\n" + "var2=3301\n" + "var2=1000\n" + "var3=http://wherehows.com\n" + "var1=mysql\n"
-            + "var4=wherehows\n" + "var5=1000,10,3\n" + "var5=true\n";;
+        "var1=com.mysql.jdbc.driver\n" + "var1=username\n" + "var1=password\n" + "var2=3301\n" + "var2=1000\n"
+            + "var3=http://wherehows.com\n" + "var1=mysql\n" + "var4=wherehows\n" + "var5=1000,10,3\n" + "var5=true\n";
+    ;
     Properties props = new Properties();
     Path path = createPropertiesFile(propertyStr);
 

File: wherehows-dao/src/test/java/wherehows/dao/FieldDetailDaoTest.java
Patch:
@@ -16,7 +16,6 @@
 import com.linkedin.events.metadata.DatasetSchema;
 import com.linkedin.events.metadata.FieldSchema;
 import java.util.Arrays;
-import java.util.Date;
 import java.util.List;
 import org.testng.annotations.Test;
 import wherehows.dao.table.FieldDetailDao;

File: wherehows-frontend/app/controllers/Application.java
Patch:
@@ -95,6 +95,8 @@ public class Application extends Controller {
       Play.application().configuration().getString("links.wiki.exportPolicy", "");
   private static final String WHZ_WIKI_LINKS__METADATA_HEALTH =
       Play.application().configuration().getString("links.wiki.metadataHealth", "");
+  private static final String WHZ_WIKI_LINKS__PURGE_KEY =
+      Play.application().configuration().getString("links.wiki.purgeKey", "");
 
   private static final String WHZ_LINKS__JIT_ACL_CONTACT =
       Play.application().configuration().getString("links.jitAcl.contact", "");
@@ -269,6 +271,7 @@ private static ObjectNode wikiLinks() {
     wikiLinks.put("metadataCustomRegex", WHZ_WIKI_LINKS__METADATA_CUSTOM_REGEX);
     wikiLinks.put("exportPolicy", WHZ_WIKI_LINKS__EXPORT_POLICY);
     wikiLinks.put("metadataHealth", WHZ_WIKI_LINKS__METADATA_HEALTH);
+    wikiLinks.put("purgeKey", WHZ_WIKI_LINKS__PURGE_KEY);
 
     return wikiLinks;
   }

File: wherehows-frontend/app/controllers/Application.java
Patch:
@@ -95,6 +95,8 @@ public class Application extends Controller {
       Play.application().configuration().getString("links.wiki.exportPolicy", "");
   private static final String WHZ_WIKI_LINKS__METADATA_HEALTH =
       Play.application().configuration().getString("links.wiki.metadataHealth", "");
+  private static final String WHZ_WIKI_LINKS__PURGE_KEY =
+      Play.application().configuration().getString("links.wiki.purgeKey", "");
 
   private static final String WHZ_LINKS__JIT_ACL_CONTACT =
       Play.application().configuration().getString("links.jitAcl.contact", "");
@@ -269,6 +271,7 @@ private static ObjectNode wikiLinks() {
     wikiLinks.put("metadataCustomRegex", WHZ_WIKI_LINKS__METADATA_CUSTOM_REGEX);
     wikiLinks.put("exportPolicy", WHZ_WIKI_LINKS__EXPORT_POLICY);
     wikiLinks.put("metadataHealth", WHZ_WIKI_LINKS__METADATA_HEALTH);
+    wikiLinks.put("purgeKey", WHZ_WIKI_LINKS__PURGE_KEY);
 
     return wikiLinks;
   }

File: wherehows-dao/src/main/java/wherehows/dao/DaoFactory.java
Patch:
@@ -14,15 +14,15 @@
 package wherehows.dao;
 
 import javax.persistence.EntityManagerFactory;
+import wherehows.dao.table.AclDao;
 import wherehows.dao.table.DatasetComplianceDao;
 import wherehows.dao.table.DatasetOwnerDao;
 import wherehows.dao.table.DatasetsDao;
 import wherehows.dao.table.DictDatasetDao;
+import wherehows.dao.table.ExportPolicyDao;
 import wherehows.dao.table.FieldDetailDao;
-import wherehows.dao.table.AclDao;
 import wherehows.dao.table.LineageDao;
 import wherehows.dao.table.SearchDao;
-import wherehows.dao.table.ExportPolicyDao;
 import wherehows.dao.view.DataTypesViewDao;
 import wherehows.dao.view.DatasetViewDao;
 import wherehows.dao.view.OwnerViewDao;

File: wherehows-dao/src/main/java/wherehows/dao/table/DatasetOwnerDao.java
Patch:
@@ -65,7 +65,8 @@ public List<DsOwner> findByIdAndSource(int datasetId, @Nonnull String source) {
    * @param owners List<DatasetOwner>
    * @param user String
    */
-  public void updateDatasetOwners(@Nonnull String datasetUrn, @Nonnull List<DatasetOwner> owners, @Nonnull String user) throws Exception {
+  public void updateDatasetOwners(@Nonnull String datasetUrn, @Nonnull List<DatasetOwner> owners, @Nonnull String user)
+      throws Exception {
     throw new UnsupportedOperationException("Not yet implemented");
   }
 

File: wherehows-dao/src/main/java/wherehows/dao/table/SearchDao.java
Patch:
@@ -214,7 +214,6 @@ public JsonNode elasticSearchDatasetByKeyword(String elasticSearchUrl, String ca
       log.info(" === elasticSearchDatasetByKeyword === The query sent to Elastic Search is: " + queryNode.toString());
 
       responseNode = HttpUtil.httpPostRequest(elasticSearchUrl, queryNode);
-
     } catch (IOException e) {
       log.error("Elastic search dataset query error: {}", e.toString());
     }

File: wherehows-dao/src/main/java/wherehows/dao/view/OwnerViewDao.java
Patch:
@@ -19,8 +19,6 @@
 import javax.annotation.Nonnull;
 import javax.annotation.Nullable;
 import javax.persistence.EntityManagerFactory;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
 import wherehows.models.view.DatasetOwner;
 import wherehows.models.view.DatasetOwnership;
 
@@ -38,7 +36,6 @@ public OwnerViewDao(EntityManagerFactory factory) {
           + "LEFT JOIN dir_external_user_info u on (o.owner_id = u.user_id and u.app_id = 300) "
           + "WHERE o.dataset_urn = :datasetUrn and (o.is_deleted is null OR o.is_deleted != 'Y') ORDER BY o.sort_id";
 
-
   /**
    * Get dataset owner list by WH dataset URN
    * @param datasetUrn String

File: wherehows-dao/src/main/java/wherehows/models/table/DashboardBarData.java
Patch:
@@ -13,9 +13,8 @@
  */
 package wherehows.models.table;
 
-
 public class DashboardBarData {
 
-    public String label;
-    public Long value;
+  public String label;
+  public Long value;
 }

File: wherehows-dao/src/main/java/wherehows/models/table/DatasetClassification.java
Patch:
@@ -14,11 +14,10 @@
 package wherehows.models.table;
 
 import java.util.Date;
-
 import javax.persistence.Column;
 import javax.persistence.Entity;
-import javax.persistence.Table;
 import javax.persistence.Id;
+import javax.persistence.Table;
 import lombok.AllArgsConstructor;
 import lombok.Data;
 import lombok.NoArgsConstructor;

File: wherehows-dao/src/main/java/wherehows/models/table/DatasetInstance.java
Patch:
@@ -15,7 +15,7 @@
 
 public class DatasetInstance {
 
-    public Long datasetId;
-    public String dbCode;
-    public Integer dbId;
+  public Long datasetId;
+  public String dbCode;
+  public Integer dbId;
 }

File: wherehows-dao/src/main/java/wherehows/models/table/DatasetListViewNode.java
Patch:
@@ -15,7 +15,7 @@
 
 public class DatasetListViewNode {
 
-    public Long datasetId;
-    public String nodeName;
-    public String nodeUrl;
+  public Long datasetId;
+  public String nodeName;
+  public String nodeUrl;
 }

File: wherehows-dao/src/main/java/wherehows/models/table/Flow.java
Patch:
@@ -31,7 +31,7 @@ public class Flow {
 
   @Id
   @Column(name = "flow_id")
-  private Long flow_id;
+  private Long flowId;
 
   @Id
   @Column(name = "app_id")
@@ -62,7 +62,7 @@ public class Flow {
   private String appCode;
 
   static class FlowKeys implements Serializable {
-    private String flow_id;
+    private String flowId;
     private String appId;
   }
 }

File: wherehows-dao/src/main/java/wherehows/models/table/LineagePathInfo.java
Patch:
@@ -15,7 +15,7 @@
 
 public class LineagePathInfo {
 
-    public String storageType;
-    public String schemaName;
-    public String filePath;
+  public String storageType;
+  public String schemaName;
+  public String filePath;
 }
\ No newline at end of file

File: wherehows-dao/src/main/java/wherehows/models/table/ScriptRuntime.java
Patch:
@@ -15,7 +15,7 @@
 
 public class ScriptRuntime {
 
-    public String jobStarted;
-    public float elapsedTime;
-    public String jobPath;
+  public String jobStarted;
+  public float elapsedTime;
+  public String jobPath;
 }
\ No newline at end of file

File: wherehows-dao/src/main/java/wherehows/models/table/UserEntity.java
Patch:
@@ -15,7 +15,7 @@
 
 public class UserEntity {
 
-    public String label;
-    public String category;
-    public String displayName;
+  public String label;
+  public String category;
+  public String displayName;
 }

File: wherehows-dao/src/main/java/wherehows/models/table/UserSetting.java
Patch:
@@ -15,9 +15,10 @@
 
 import lombok.Data;
 
+
 @Data
 public class UserSetting {
 
-    private String detailDefaultView;
-    private String defaultWatch;
+  private String detailDefaultView;
+  private String defaultWatch;
 }
\ No newline at end of file

File: wherehows-dao/src/main/java/wherehows/models/view/DatasetValidation.java
Patch:
@@ -12,10 +12,12 @@
  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  */
 package wherehows.models.view;
-import lombok.Getter;
+
 import lombok.AllArgsConstructor;
+import lombok.Getter;
 import lombok.NoArgsConstructor;
 
+
 @Getter
 @NoArgsConstructor
 @AllArgsConstructor

File: wherehows-frontend/app/dao/FlowRowMapper.java
Patch:
@@ -44,7 +44,7 @@ public Flow mapRow(ResultSet rs, int rowNum) throws SQLException {
     Integer level = rs.getInt(FLOW_LEVEL_COLUMN);
 
     Flow flow = new Flow();
-    flow.setFlow_id(id);
+    flow.setFlowId(id);
     flow.setAppId(appId);
     flow.setName(name);
     flow.setAppCode(appCode);

File: wherehows-frontend/app/controllers/Application.java
Patch:
@@ -61,6 +61,8 @@ public class Application extends Controller {
       Play.application().configuration().getString("ui.show.ownership.revamp", "hide");
   private static final Boolean WHZ_STG_BANNER =
       Play.application().configuration().getBoolean("ui.show.staging.banner", false);
+  private static final Boolean WHZ_LIVE_DATA_BANNER =
+      Play.application().configuration().getBoolean("ui.show.live.data.banner", false);
   private static final Boolean WHZ_STALE_SEARCH_ALERT =
       Play.application().configuration().getBoolean("ui.show.stale.search", false);
   private static final Boolean HTTPS_REDIRECT = Play.application().configuration().getBoolean("https.redirect", false);
@@ -223,6 +225,7 @@ public static Result appConfig() {
     // In a staging environment, we can trigger this flag to be true so that the UI can handle based on
     // such config and alert users that their changes will not affect production data
     config.put("isStagingBanner", WHZ_STG_BANNER);
+    config.put("isLiveDataWarning", WHZ_LIVE_DATA_BANNER);
     // Flag set in order to warn users that search is experiencing issues
     config.put("isStaleSearch", WHZ_STALE_SEARCH_ALERT);
 

File: wherehows-frontend/app/controllers/Application.java
Patch:
@@ -61,6 +61,8 @@ public class Application extends Controller {
       Play.application().configuration().getString("ui.show.ownership.revamp", "hide");
   private static final Boolean WHZ_STG_BANNER =
       Play.application().configuration().getBoolean("ui.show.staging.banner", false);
+  private static final Boolean WHZ_LIVE_DATA_BANNER =
+      Play.application().configuration().getBoolean("ui.show.live.data.banner", false);
   private static final Boolean WHZ_STALE_SEARCH_ALERT =
       Play.application().configuration().getBoolean("ui.show.stale.search", false);
   private static final Boolean HTTPS_REDIRECT = Play.application().configuration().getBoolean("https.redirect", false);
@@ -217,6 +219,7 @@ public static Result appConfig() {
     // In a staging environment, we can trigger this flag to be true so that the UI can handle based on
     // such config and alert users that their changes will not affect production data
     config.put("isStagingBanner", WHZ_STG_BANNER);
+    config.put("isLiveDataWarning", WHZ_LIVE_DATA_BANNER);
     // Flag set in order to warn users that search is experiencing issues
     config.put("isStaleSearch", WHZ_STALE_SEARCH_ALERT);
     response.put("status", "ok");

File: wherehows-frontend/app/controllers/Application.java
Patch:
@@ -93,7 +93,7 @@ public class Application extends Controller {
       Play.application().configuration().getString("links.wiki.exportPolicy", "");
 
   private static final String WHZ_LINKS__JIT_ACL_CONTACT =
-      Play.application().configuration().getString("links.jitAcl.contact")
+      Play.application().configuration().getString("links.jitAcl.contact");
 
   private static final String DB_WHEREHOWS_URL =
       Play.application().configuration().getString("database.opensource.url");

File: wherehows-dao/src/main/java/wherehows/dao/table/ExportPolicyDao.java
Patch:
@@ -25,7 +25,7 @@ public DatasetExportPolicy getDatasetExportPolicy(@Nonnull String datasetUrn) th
     throw new UnsupportedOperationException("Not implemented yet");
   }
 
-  public void updateDatasetExportPolicy(@Nonnull datasetUrn, @Nonnull DatasetExportPolicy record, @Nonnull String user) throws Exception {
+  public void updateDatasetExportPolicy(@Nonnull String datasetUrn, @Nonnull DatasetExportPolicy record, @Nonnull String user) throws Exception {
     throw new UnsupportedOperationException("Not implemented yet");
 
   }

File: wherehows-dao/src/main/java/wherehows/models/view/DatasetExportPolicy.java
Patch:
@@ -23,9 +23,9 @@
 @Getter
 @NoArgsConstructor
 public class DatasetExportPolicy {
-  private containsUserGeneratedContent;
+  private boolean containsUserGeneratedContent;
 
-  private containsUserActionGeneratedContent;
+  private boolean containsUserActionGeneratedContent;
 
-  private containsUserDerivedContent;
+  private boolean containsUserDerivedContent;
 }

File: wherehows-dao/src/main/java/wherehows/models/view/DatasetValidation.java
Patch:
@@ -12,8 +12,10 @@
  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  */
 package wherehows.models.view;
+import lombok.Getter;
 import lombok.AllArgsConstructor;
 
+@Getter
 @AllArgsConstructor
 public class DatasetValidation {
 

File: wherehows-dao/src/main/java/wherehows/models/view/DatasetHealth.java
Patch:
@@ -26,4 +26,4 @@ public class DatasetHealth {
   private float score;
 
   private List<DatasetValidation> validations;
-}
\ No newline at end of file
+}

File: wherehows-frontend/app/controllers/api/v2/Dataset.java
Patch:
@@ -44,6 +44,7 @@
 import wherehows.models.view.DatasetSchema;
 import wherehows.models.view.DatasetView;
 import wherehows.models.view.DsComplianceSuggestion;
+import wherehows.models.view.DatasetHealth;
 
 import static controllers.api.v1.Dataset.*;
 import static utils.Dataset.*;
@@ -242,7 +243,7 @@ public static Promise<Result> getDatasetHealth(String datasetUrn) {
       return Promise.promise(() -> notFound(_EMPTY_RESPONSE));
     }
 
-    return Promise.promise(() -> ok(Json.newObject().set("health", Json.toJson(schema))));
+    return Promise.promise(() -> ok(Json.newObject().set("health", Json.toJson(health))));
   }
 
   public static Promise<Result> getDatasetOwners(String datasetUrn) {

File: wherehows-frontend/app/controllers/Application.java
Patch:
@@ -68,7 +68,7 @@ public class Application extends Controller {
       Play.application().configuration().getBoolean("linkedin.show.dataset.lineage", false);
   private static final Boolean WHZ_SHOW_DS_HEALTH =
       Play.application().configuration().getBoolean("linkedin.show.dataset.health", false);
-  private static final Boolean WHZ_SUGGESTION_CONFIDENCE_THRESHOLD =
+  private static final String WHZ_SUGGESTION_CONFIDENCE_THRESHOLD =
       Play.application().configuration().getString("linkedin.suggestion.confidence.threshold", "50");
 
   private static final String WHZ_WIKI_LINKS__GDRP_PII =

File: wherehows-frontend/app/controllers/Application.java
Patch:
@@ -66,6 +66,8 @@ public class Application extends Controller {
   private static final Boolean HTTPS_REDIRECT = Play.application().configuration().getBoolean("https.redirect", false);
   private static final Boolean WHZ_SHOW_BROWSE_REVAMP =
       Play.application().configuration().getBoolean("linkedin.show.browse.revamp", false);
+private static final Boolean WHZ_SHOW_LINEAGE =
+      Play.application().configuration().getBoolean("linkedin.show.dataset.lineage", false);
 
   private static final String WHZ_WIKI_LINKS__GDRP_PII =
       Play.application().configuration().getString("linkedin.links.wiki.gdprPii", "");
@@ -199,6 +201,7 @@ public static Result appConfig() {
     config.put("appVersion", APP_VERSION);
     config.put("isInternal", IS_INTERNAL);
     config.put("shouldShowBrowserRevamp", WHZ_SHOW_BROWSE_REVAMP);
+    config.put("shouldShowDatasetLineage", WHZ_SHOW_LINEAGE);
     config.set("wikiLinks", wikiLinks());
     config.set("JitAclAccessWhitelist", Json.toJson(StringUtils.split(JIT_ACL_WHITELIST, ',')));
     config.set("tracking", trackingInfo());

File: wherehows-frontend/app/controllers/Application.java
Patch:
@@ -197,9 +197,6 @@ public static Result appConfig() {
     config.put("appVersion", APP_VERSION);
     config.put("isInternal", IS_INTERNAL);
     config.set("wikiLinks", wikiLinks());
-    // Ownership tab is currently in a UX revamp, this flag will determine whether to show it or not
-    // under certain environments
-    config.put("showOwnership", WHZ_DS_OWNERSHIP_TAB);
     config.set("JitAclAccessWhitelist", Json.toJson(StringUtils.split(JIT_ACL_WHITELIST, ',')));
     config.set("tracking", trackingInfo());
     // In a staging environment, we can trigger this flag to be true so that the UI can handle based on

File: wherehows-frontend/app/controllers/Application.java
Patch:
@@ -342,7 +342,8 @@ public static Result authenticate() {
     try {
       AuthenticationManager.authenticateUser(username, password);
     } catch (Exception e) {
-      return badRequest("Invalid credentials");
+      Logger.warn("Authentication error!", e);
+      return badRequest(e.getMessage());
     }
 
     // Adds the username to the session cookie

File: wherehows-backend/app/controllers/Application.java
Patch:
@@ -23,7 +23,6 @@
 import play.Play;
 import play.mvc.Controller;
 import play.mvc.Result;
-import wherehows.common.Constant;
 import wherehows.dao.ConnectionPoolProperties;
 import wherehows.dao.DaoFactory;
 
@@ -44,7 +43,6 @@ public class Application extends Controller {
       Play.application().configuration().getString("dao.factory.class", DaoFactory.class.getCanonicalName());
 
   private static final EntityManagerFactory ENTITY_MANAGER_FACTORY = ConnectionPoolProperties.builder()
-      .providerClass(Constant.WH_DB_CONNECTION_PROVIDER_CLASS)
       .dataSourceClassName(WHZ_DB_DSCLASSNAME)
       .dataSourceURL(DB_WHEREHOWS_URL)
       .dataSourceUser(DB_WHEREHOWS_USERNAME)

File: wherehows-common/src/main/java/wherehows/common/Constant.java
Patch:
@@ -48,7 +48,6 @@ public class Constant {
   public static final String WH_DB_USERNAME_KEY = "wherehows.db.username";
   public static final String WH_DB_PASSWORD_KEY = "wherehows.db.password";
   public static final String WH_DB_DRIVER_KEY = "wherehows.db.driver";
-  public static final String WH_DB_CONNECTION_PROVIDER_CLASS = "com.zaxxer.hikari.hibernate.HikariConnectionProvider";
 
   /** Location of the folder that store interim csv file. */
   public static final String WH_APP_FOLDER_KEY = "wherehows.app_folder";

File: wherehows-frontend/app/controllers/Application.java
Patch:
@@ -36,7 +36,6 @@
 import play.mvc.Security;
 import security.AuthenticationManager;
 import utils.Tree;
-import wherehows.common.Constant;
 import wherehows.dao.ConnectionPoolProperties;
 import wherehows.dao.DaoFactory;
 
@@ -74,7 +73,6 @@ public class Application extends Controller {
       Play.application().configuration().getString("dao.factory.class", DaoFactory.class.getCanonicalName());
 
   private static final EntityManagerFactory ENTITY_MANAGER_FACTORY = ConnectionPoolProperties.builder()
-      .providerClass(Constant.WH_DB_CONNECTION_PROVIDER_CLASS)
       .dataSourceClassName(WHZ_DB_DSCLASSNAME)
       .dataSourceURL(DB_WHEREHOWS_URL)
       .dataSourceUser(DB_WHEREHOWS_USERNAME)

File: wherehows-kafka/src/main/java/wherehows/main/ApplicationStart.java
Patch:
@@ -24,7 +24,6 @@
 import javax.persistence.EntityManagerFactory;
 import lombok.extern.slf4j.Slf4j;
 import wherehows.actors.KafkaClientMaster;
-import wherehows.common.Constant;
 import wherehows.common.utils.ProcessUtil;
 import wherehows.dao.ConnectionPoolProperties;
 import wherehows.dao.DaoFactory;
@@ -45,7 +44,6 @@ public class ApplicationStart {
       config.hasPath("dao.factory.class") ? config.getString("dao.factory.class") : DaoFactory.class.getCanonicalName();
 
   private static final EntityManagerFactory ENTITY_MANAGER_FACTORY = ConnectionPoolProperties.builder()
-      .providerClass(Constant.WH_DB_CONNECTION_PROVIDER_CLASS)
       .dataSourceClassName(WHZ_DB_DSCLASSNAME)
       .dataSourceURL(DB_WHEREHOWS_URL)
       .dataSourceUser(DB_WHEREHOWS_USERNAME)

File: wherehows-kafka/src/main/java/wherehows/processors/MetadataChangeProcessor.java
Patch:
@@ -141,7 +141,7 @@ public void processEvent(MetadataChangeEvent event) throws Exception {
     }
   }
 
-  private FailedMetadataChangeEvent newFailedEvent(MetadataChangeEvent event, Throwable throwable) {
+  public FailedMetadataChangeEvent newFailedEvent(MetadataChangeEvent event, Throwable throwable) {
     FailedMetadataChangeEvent failedEvent = new FailedMetadataChangeEvent();
     failedEvent.time = System.currentTimeMillis();
     failedEvent.error = ExceptionUtils.getStackTrace(throwable);

File: wherehows-kafka/src/main/java/wherehows/processors/MetadataInventoryProcessor.java
Patch:
@@ -119,7 +119,7 @@ public void processEvent(MetadataInventoryEvent event) throws Exception {
     }
   }
 
-  private FailedMetadataInventoryEvent newFailedEvent(MetadataInventoryEvent event, Throwable throwable) {
+  public FailedMetadataInventoryEvent newFailedEvent(MetadataInventoryEvent event, Throwable throwable) {
     FailedMetadataInventoryEvent failedEvent = new FailedMetadataInventoryEvent();
     failedEvent.time = System.currentTimeMillis();
     failedEvent.error = ExceptionUtils.getStackTrace(throwable);

File: wherehows-kafka/src/main/java/wherehows/processors/MetadataLineageProcessor.java
Patch:
@@ -118,7 +118,7 @@ private String getActorUrn(MetadataLineageEvent event) {
     return auditStamp.actorUrn.toString();
   }
 
-  private FailedMetadataLineageEvent newFailedEvent(MetadataLineageEvent event, Throwable throwable) {
+  public FailedMetadataLineageEvent newFailedEvent(MetadataLineageEvent event, Throwable throwable) {
     FailedMetadataLineageEvent failedEvent = new FailedMetadataLineageEvent();
     failedEvent.time = System.currentTimeMillis();
     failedEvent.error = ExceptionUtils.getStackTrace(throwable);

File: wherehows-frontend/app/controllers/api/v2/Dataset.java
Patch:
@@ -190,7 +190,7 @@ public static Promise<Result> updateDatasetDeprecation(String datasetUrn) {
       boolean deprecated = record.get("deprecated").asBoolean();
       String deprecationNote = record.hasNonNull("deprecationNote") ? record.get("deprecationNote").asText() : "";
       long decommissionTime = record.hasNonNull("decommissionTime") ? record.get("decommissionTime").asLong() : 0;
-      if (decommissionTime <= 0) {
+      if (deprecated && decommissionTime <= 0) {
         throw new IllegalArgumentException("Invalid decommission time");
       }
 

File: wherehows-dao/src/test/java/wherehows/dao/DatasetComplianceDaoTest.java
Patch:
@@ -55,7 +55,7 @@ public void testFillDsComplianceByCompliancePolicy() {
         + "enterpriseProduct: false, accountStatus: false, addressBookImports: false, microsoftData: false, "
         + "subsidiaryData: false, otherThirdPartyIntegrations: false, device: false, searchHistory: false, "
         + "courseViewingHistory: false, whoViewedMyProfile: false, profileViewsByMe: false, advertising: false, "
-        + "usageOrErrorOrConnectivity: false, otherClickstreamOrBrowsingData: false}";
+        + "usageOrErrorOrConnectivity: false, otherClickstreamOrBrowsingData: false, employeeData: null}";
 
     assertEquals(dsCompliance.getCompliancePurgeType(), purgeType.toString());
     assertEquals(dsCompliance.getCompliancePurgeNote(), purgeNote);

File: wherehows-frontend/app/controllers/api/v2/Dataset.java
Patch:
@@ -265,7 +265,7 @@ public static Promise<Result> updateDatasetOwners(String datasetUrn) {
       }).readValue(content.get("owners"));
 
       long confirmedOwnerUserCount = owners.stream()
-          .filter(s -> "owner".equalsIgnoreCase(s.getType()) && "user".equalsIgnoreCase(s.getIdType())
+          .filter(s -> "DataOwner".equalsIgnoreCase(s.getType()) && "user".equalsIgnoreCase(s.getIdType())
               && "UI".equalsIgnoreCase(s.getSource()))
           .count();
 

File: wherehows-frontend/app/controllers/api/v2/Dataset.java
Patch:
@@ -358,7 +358,7 @@ public static Promise<Result> addUserToDatasetAcl(@Nonnull String datasetUrn) {
     if (!record.hasNonNull("businessJustification")) {
       return Promise.promise(() -> badRequest(errorResponse("Missing business justification")));
     }
-    String businessJustification = request().getQueryString("businessJustification");
+    String businessJustification = record.get("businessJustification").asText();
 
     try {
       if (accessType == null) {

File: wherehows-frontend/app/controllers/api/v2/Dataset.java
Patch:
@@ -35,6 +35,7 @@
 import wherehows.dao.view.DataTypesViewDao;
 import wherehows.dao.view.DatasetViewDao;
 import wherehows.dao.view.OwnerViewDao;
+import wherehows.models.table.AccessControlEntry;
 import wherehows.models.view.DatasetCompliance;
 import wherehows.models.view.DatasetOwner;
 import wherehows.models.view.DatasetOwnership;
@@ -326,7 +327,7 @@ record = COMPLIANCE_DAO.getComplianceSuggestion(datasetUrn);
   }
 
   public static Promise<Result> getDatasetAcls(@Nonnull String datasetUrn) {
-    final List<?> acls;
+    final List<AccessControlEntry> acls;
     try {
       acls = ACL_DAO.getDatasetAcls(datasetUrn);
     } catch (Exception e) {

File: wherehows-dao/src/main/java/wherehows/dao/view/DatasetViewDao.java
Patch:
@@ -140,7 +140,7 @@ public DatasetSchema getDatasetColumnsByID(int datasetId, @Nonnull String datase
   /**
    * Get dataset schema by dataset urn
    */
-  @Nonnull
+  @Nullable
   public DatasetSchema getDatasetSchema(@Nonnull String datasetUrn) throws Exception {
     throw new UnsupportedOperationException("Not implemented yet");
   }

File: wherehows-frontend/app/controllers/api/v2/Dataset.java
Patch:
@@ -176,6 +176,9 @@ public static Promise<Result> getDatasetSchema(String datasetUrn) {
       return Promise.promise(() -> internalServerError(errorResponse(e)));
     }
 
+    if (schema == null) {
+      return Promise.promise(() -> notFound(_EMPTY_RESPONSE));
+    }
     return Promise.promise(() -> ok(Json.newObject().set("schema", Json.toJson(schema))));
   }
 

File: wherehows-dao/src/main/java/wherehows/dao/table/DictDatasetDao.java
Patch:
@@ -68,6 +68,7 @@ public DictDataset findById(int datasetId) {
    * @return dataset id
    * @throws Exception
    */
+  @Nullable
   public DictDataset insertUpdateDataset(@Nonnull DatasetIdentifier identifier, @Nonnull ChangeAuditStamp auditStamp,
       @Nullable DatasetProperty property, @Nullable DatasetSchema schema, @Nullable List<DeploymentDetail> deployments,
       @Nullable List<String> tags, @Nullable List<Capacity> capacities, @Nullable PartitionSpecification partitions)

File: wherehows-dao/src/main/java/wherehows/dao/view/OwnerViewDao.java
Patch:
@@ -26,8 +26,6 @@
 
 public class OwnerViewDao extends BaseViewDao {
 
-  private static final Logger log = LoggerFactory.getLogger(OwnerViewDao.class);
-
   public OwnerViewDao(EntityManagerFactory factory) {
     super(factory);
   }

File: wherehows-kafka/src/main/java/wherehows/processors/MetadataChangeProcessor.java
Patch:
@@ -80,7 +80,7 @@ private void processEvent(MetadataChangeEvent event) throws Exception {
     }
 
     final DatasetIdentifier identifier = event.datasetIdentifier;
-    log.info("MCE: " + identifier + " TS: " + auditHeader.time); // TODO: remove. For debugging only
+    log.debug("MCE: " + identifier + " TS: " + auditHeader.time);
     final ChangeAuditStamp changeAuditStamp = event.changeAuditStamp;
     final ChangeType changeType = changeAuditStamp.type;
 

File: wherehows-kafka/src/main/java/wherehows/processors/MetadataLineageProcessor.java
Patch:
@@ -72,7 +72,7 @@ private void processEvent(MetadataLineageEvent event) throws Exception {
       throw new IllegalArgumentException("No Lineage info in record");
     }
 
-    log.info("MLE: " + event.lineage.toString() + " TS: " + auditHeader.time); // TODO: remove. For debugging only
+    log.debug("MLE: " + event.lineage.toString() + " TS: " + auditHeader.time);
 
     List<DatasetLineage> lineages = event.lineage;
     DeploymentDetail deployments = event.deploymentDetail;

File: wherehows-common/src/main/java/wherehows/common/utils/JobsUtil.java
Patch:
@@ -104,7 +104,6 @@ public static Map<String, Properties> getEnabledJobs(String dir) throws Configur
     Map<String, Properties> jobs = new HashMap<>();
     for (File file : new File(dir).listFiles()) {
       if (file.getAbsolutePath().endsWith(".job")) {
-        Configuration jobParam = new PropertiesConfiguration(file.getAbsolutePath());
         Properties prop = getJobConfigProperties(file);
         if (!prop.containsKey(Constant.JOB_DISABLED_KEY)) {
           // job name = file name without the extension.

File: wherehows-common/src/main/java/wherehows/common/utils/ProcessUtil.java
Patch:
@@ -22,7 +22,7 @@ private ProcessUtil() {
   }
 
   public static long getCurrentProcessId() {
-    return Long.valueOf(ManagementFactory.getRuntimeMXBean().getName().split("@")[0]);
+    return Long.parseLong(ManagementFactory.getRuntimeMXBean().getName().split("@")[0]);
   }
 
 }

File: wherehows-kafka/src/main/java/wherehows/actors/KafkaClientMaster.java
Patch:
@@ -69,7 +69,7 @@ public void preStart() throws Exception {
       final String kafkaJobName = entry.getKey();
       final Properties props = entry.getValue();
 
-      final int numberOfWorkers = Integer.valueOf(props.getProperty(Constant.KAFKA_WORKER_COUNT, "1"));
+      final int numberOfWorkers = Integer.parseInt(props.getProperty(Constant.KAFKA_WORKER_COUNT, "1"));
 
       log.info("Create Kafka client with config: " + props);
       try {

File: wherehows-kafka/src/main/java/wherehows/actors/KafkaWorker.java
Patch:
@@ -104,7 +104,7 @@ private TopicPartition extractPartition(@Nonnull SerializationException exceptio
     if (!matcher.find()) {
       throw new RuntimeException("Unable to parse deserialization error message");
     }
-    int partition = Integer.valueOf(matcher.group(1));
+    int partition = Integer.parseInt(matcher.group(1));
     return new TopicPartition(_topic, partition);
   }
 

File: wherehows-kafka/src/main/java/wherehows/main/ApplicationStart.java
Patch:
@@ -90,6 +90,6 @@ public static void main(String[] args) {
     final ActorSystem actorSystem = ActorSystem.create("KAFKA");
 
     String kafkaJobDir = config.getString("kafka.jobs.dir");
-    final ActorRef master = actorSystem.actorOf(Props.create(KafkaClientMaster.class, kafkaJobDir), "KafkaMaster");
+    actorSystem.actorOf(Props.create(KafkaClientMaster.class, kafkaJobDir), "KafkaMaster");
   }
 }

File: wherehows-kafka/src/main/java/wherehows/processors/MetadataChangeProcessor.java
Patch:
@@ -67,7 +67,7 @@ public void process(IndexedRecord indexedRecord) {
     try {
       processEvent(event);
     } catch (Exception exception) {
-      log.error("MCE Processor Error: {}", exception);
+      log.error("MCE Processor Error:", exception);
       log.error("Message content: {}", event.toString());
       this.PRODUCER.send(new ProducerRecord(_producerTopic, newFailedEvent(event, exception)));
     }

File: wherehows-dao/src/main/java/wherehows/dao/table/DatasetComplianceDao.java
Patch:
@@ -57,7 +57,6 @@ public DatasetCompliance getDatasetComplianceByDatasetId(int datasetId, String d
   public void updateDatasetCompliance(@Nonnull DatasetCompliance record, @Nonnull String user) throws Exception {
     DsCompliance compliance = datasetComplianceToDsCompliance(record);
     compliance.setModifiedBy(user);
-    compliance.setModifiedTime((int) (System.currentTimeMillis() / 1000));
 
     update(compliance);
   }
@@ -155,7 +154,6 @@ public DsCompliance datasetComplianceToDsCompliance(DatasetCompliance compliance
     dsCompliance.setConfidentiality(compliance.getConfidentiality());
     dsCompliance.setDatasetClassification(om.writeValueAsString(compliance.getDatasetClassification()));
     dsCompliance.setModifiedBy(compliance.getModifiedBy());
-    dsCompliance.setModifiedTime((int) (compliance.getModifiedTime() / 1000));
     return dsCompliance;
   }
 

File: wherehows-dao/src/test/java/wherehows/dao/DatasetComplianceDaoTest.java
Patch:
@@ -123,7 +123,7 @@ public void testDatasetComplianceToDsCompliance() throws Exception {
     assertEquals(dsCompliance.getComplianceEntities(), "[]");
     assertEquals(dsCompliance.getConfidentiality(), confidentiality);
     assertEquals(dsCompliance.getModifiedBy(), actor);
-    assertEquals(dsCompliance.getModifiedTime(), new Integer(1));
+    assertEquals(dsCompliance.getModifiedTime(), null);
   }
 
   @Test

File: wherehows-dao/src/test/java/wherehows/dao/DatasetOwnerDaoTest.java
Patch:
@@ -50,7 +50,7 @@ public void testFillDsOwnerByOwnerInfo() {
     assertEquals(dsOwner.getNamespace(), "urn:li:corpuser");
     assertEquals(dsOwner.getIsGroup(), "N");
     assertEquals(dsOwner.getIsActive(), "Y");
-    assertEquals(dsOwner.getOwnerType(), "DATA_OWNER");
+    assertEquals(dsOwner.getOwnerType(), "Owner");
     assertEquals(dsOwner.getOwnerSource(), "SCM");
     assertEquals(dsOwner.getSourceTime().intValue(), createdTime);
 
@@ -69,7 +69,7 @@ public void testFillDsOwnerByOwnerInfo() {
     assertEquals(dsOwner.getNamespace(), "urn:li:corpGroup");
     assertEquals(dsOwner.getIsGroup(), "Y");
     assertEquals(dsOwner.getIsActive(), "Y");
-    assertEquals(dsOwner.getOwnerType(), "PRODUCER");
+    assertEquals(dsOwner.getOwnerType(), "Producer");
     assertEquals(dsOwner.getOwnerSource(), "NUAGE");
     assertEquals(dsOwner.getSourceTime().intValue(), modifiedTime);
     assertEquals(dsOwner.getCreatedTime(), null);

File: wherehows-dao/src/main/java/wherehows/dao/table/DictDatasetDao.java
Patch:
@@ -117,6 +117,7 @@ public void fillDictDataset(@Nonnull DictDataset ds, @Nonnull String urn, @Nonnu
 
     String[] urnParts = parseWhDatasetUrn(urn);
     ds.setDatasetType(urnParts[0]);
+    ds.setSource(urnParts[0]);
     ds.setLocationPrefix(urnParts[1]);
     ds.setParentName(urnParts[2]);
     ds.setName(urnParts[3]);
@@ -161,8 +162,6 @@ public void fillDictDataset(@Nonnull DictDataset ds, @Nonnull String urn, @Nonnu
       }
 
       // if schema is not in the MCE, will not update the source section of the dataset
-      String actor = getUrnEntity(toStringOrNull(auditStamp.actorUrn));
-      ds.setSource(actor);
       int sourceTime = (int) (auditStamp.time / 1000);
       if (ds.getSourceCreatedTime() == null) {
         ds.setSourceCreatedTime(sourceTime);

File: wherehows-dao/src/main/java/wherehows/dao/table/DatasetOwnerDao.java
Patch:
@@ -70,7 +70,7 @@ public void insertUpdateOwnership(DatasetIdentifier identifier, int datasetId, @
     }
 
     // find dataset owners of same source if exist
-    List<DsOwner> dsOwners = findByIdAndSource(datasetId, owners.get(0).ownershipProvider.name());
+    List<DsOwner> dsOwners = findByIdAndSource(datasetId, enumNameOrDefault(owners.get(0).ownershipProvider, ""));
 
     List<List<DsOwner>> updatedList =
         diffOwnerList(dsOwners, owners, datasetId, datasetUrn, (int) (auditStamp.time / 1000));
@@ -94,7 +94,7 @@ public void fillDsOwnerByOwnerInfo(@Nonnull OwnerInfo owner, @Nonnull DsOwner ds
     dsOwner.setOwnerId(owner.owner.toString());
     dsOwner.setOwnerIdType(owner.ownerType.name());
     dsOwner.setOwnerType(owner.ownerCategory.name());
-    dsOwner.setOwnerSource(owner.ownershipProvider == null ? "" : owner.ownershipProvider.name());
+    dsOwner.setOwnerSource(enumNameOrDefault(owner.ownershipProvider, ""));
 
     if (owner.ownerType == OwnerType.USER) {
       dsOwner.setAppId(300);

File: wherehows-dao/src/main/java/wherehows/dao/table/DatasetOwnerDao.java
Patch:
@@ -70,7 +70,7 @@ public void insertUpdateOwnership(DatasetIdentifier identifier, int datasetId, @
     }
 
     // find dataset owners of same source if exist
-    List<DsOwner> dsOwners = findByIdAndSource(datasetId, owners.get(0).ownershipProvider.name());
+    List<DsOwner> dsOwners = findByIdAndSource(datasetId, enumNameOrDefault(owners.get(0).ownershipProvider, ""));
 
     List<List<DsOwner>> updatedList =
         diffOwnerList(dsOwners, owners, datasetId, datasetUrn, (int) (auditStamp.time / 1000));
@@ -94,7 +94,7 @@ public void fillDsOwnerByOwnerInfo(@Nonnull OwnerInfo owner, @Nonnull DsOwner ds
     dsOwner.setOwnerId(owner.owner.toString());
     dsOwner.setOwnerIdType(owner.ownerType.name());
     dsOwner.setOwnerType(owner.ownerCategory.name());
-    dsOwner.setOwnerSource(owner.ownershipProvider == null ? "" : owner.ownershipProvider.name());
+    dsOwner.setOwnerSource(enumNameOrDefault(owner.ownershipProvider, ""));
 
     if (owner.ownerType == OwnerType.USER) {
       dsOwner.setAppId(300);

File: wherehows-dao/src/main/java/wherehows/dao/table/DatasetsDao.java
Patch:
@@ -22,17 +22,15 @@
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
+import lombok.extern.slf4j.Slf4j;
 import org.apache.commons.lang3.StringUtils;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
 import org.springframework.dao.EmptyResultDataAccessException;
 import org.springframework.jdbc.core.BatchPreparedStatementSetter;
 import org.springframework.jdbc.core.JdbcTemplate;
 import org.springframework.jdbc.core.namedparam.NamedParameterJdbcTemplate;
 import wherehows.models.table.DatasetCompliance;
 import wherehows.models.table.DatasetFieldEntity;
 import wherehows.models.view.DatasetOwner;
-import lombok.extern.slf4j.Slf4j;
 
 import static wherehows.util.JsonUtil.*;
 

File: wherehows-dao/src/main/java/wherehows/util/JsonUtil.java
Patch:
@@ -16,6 +16,8 @@
 import com.fasterxml.jackson.core.type.TypeReference;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import java.io.IOException;
+import javax.annotation.Nonnull;
+import javax.annotation.Nullable;
 import org.apache.commons.lang3.StringUtils;
 
 
@@ -30,7 +32,7 @@ public class JsonUtil {
    * @return type object
    * @throws IOException
    */
-  public static <T> T jsonToTypedObject(String jsonString, TypeReference type) throws IOException {
+  public static <T> T jsonToTypedObject(@Nullable String jsonString, @Nonnull TypeReference type) throws IOException {
     return StringUtils.isBlank(jsonString) ? null : new ObjectMapper().readValue(jsonString, type);
   }
 }

File: wherehows-kafka/src/main/java/wherehows/processors/MetadataChangeProcessor.java
Patch:
@@ -68,6 +68,7 @@ public void process(IndexedRecord indexedRecord) throws Exception {
     }
 
     final DatasetIdentifier identifier = record.datasetIdentifier;
+    log.info("MCE: " + identifier); // TODO: remove. For debugging only
     final ChangeAuditStamp changeAuditStamp = record.changeAuditStamp;
     final ChangeType changeType = changeAuditStamp.type;
 

File: wherehows-kafka/src/main/java/wherehows/processors/MetadataChangeProcessor.java
Patch:
@@ -68,6 +68,7 @@ public void process(IndexedRecord indexedRecord) throws Exception {
     }
 
     final DatasetIdentifier identifier = record.datasetIdentifier;
+    log.info("MCE: " + identifier); // TODO: remove. For debugging only
     final ChangeAuditStamp changeAuditStamp = record.changeAuditStamp;
     final ChangeType changeType = changeAuditStamp.type;
 

File: wherehows-dao/src/main/java/wherehows/models/table/DatasetComment.java
Patch:
@@ -21,8 +21,8 @@ public class DatasetComment {
     public String authorName;
     public String authorUserName;
     public String authorEmail;
-    public String created;
-    public String modified;
+    public long created;
+    public long modified;
     public String type;
     public Boolean isAuthor;
 }
\ No newline at end of file

File: wherehows-frontend/app/dao/DatasetsDAO.java
Patch:
@@ -221,6 +221,9 @@ public class DatasetsDAO extends AbstractMySQLOpenSourceDAO
 	private final static String CREATE_DATASET_COMMENT = "INSERT INTO comments " +
 			"(text, user_id, dataset_id, created, modified, comment_type) VALUES(?, ?, ?, NOW(), NOW(), ?)";
 
+	private final static String UPDATE_DATASET_COMMENT = "UPDATE comments " +
+			"SET text = ?, comment_type = ?, modified = NOW() WHERE id = ?";
+
 	private final static String GET_WATCHED_URN_ID = "SELECT id FROM watch " +
 			"WHERE user_id = ? and item_type = 'urn' and urn = ?";
 
@@ -245,9 +248,6 @@ public class DatasetsDAO extends AbstractMySQLOpenSourceDAO
 	private final static String CREATE_COLUMN_COMMENT = "INSERT INTO field_comments " +
 			"(comment, user_id, created, modified, comment_crc32_checksum) VALUES(?, ?, NOW(), NOW(), CRC32(?))";
 
-	private final static String UPDATE_DATASET_COMMENT = "UPDATE comments " +
-			"SET text = ?, comment_type = ?, modified = NOW() WHERE id = ?";
-
 	private final static String UPDATE_COLUMN_COMMENT = "UPDATE field_comments " +
 			"SET comment = ?, modified = NOW() WHERE id = ?";
 

File: wherehows-backend/app/actors/KafkaConsumerMaster.java
Patch:
@@ -31,11 +31,11 @@
 import play.Logger;
 import play.Play;
 import play.db.DB;
-import utils.JobsUtil;
 import wherehows.common.PathAnalyzer;
 import wherehows.common.kafka.schemaregistry.client.CachedSchemaRegistryClient;
 import wherehows.common.kafka.schemaregistry.client.SchemaRegistryClient;
 import wherehows.common.utils.ClusterUtil;
+import wherehows.common.utils.JobsUtil;
 
 
 /**

File: wherehows-backend/app/actors/SchedulerActor.java
Patch:
@@ -24,7 +24,7 @@
 import play.Logger;
 import play.Play;
 import shared.Global;
-import utils.JobsUtil;
+import wherehows.common.utils.JobsUtil;
 
 
 /**

File: wherehows-backend/app/actors/TreeBuilderActor.java
Patch:
@@ -25,8 +25,8 @@
 import org.python.util.PythonInterpreter;
 import play.Logger;
 import play.Play;
-import utils.JobsUtil;
 import wherehows.common.Constant;
+import wherehows.common.utils.JobsUtil;
 
 
 /**

File: wherehows-common/src/main/java/wherehows/common/utils/JobsUtil.java
Patch:
@@ -11,7 +11,7 @@
  * distributed under the License is distributed on an "AS IS" BASIS,
  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  */
-package wherehows.utils;
+package wherehows.common.utils;
 
 import com.google.common.collect.ImmutableList;
 import java.io.BufferedReader;
@@ -96,7 +96,7 @@ public static Map<String, Properties> getScheduledJobs(String dir) {
   }
 
   /**
-   * Returns a map of job name to job properties for kafka jobs.
+   * Returns a map of job name to job properties which are enabled and of certain type.
    */
   public static Map<String, Properties> getEnabledJobsByType(String dir, String type) {
     Map<String, Properties> jobs = new HashMap<>();

File: wherehows-common/src/test/java/LineageCombinerTest.java
Patch:
@@ -35,7 +35,7 @@
 /**
  * Created by zsun on 9/13/15.
  */
-@Test(groups = {"needConfig"})
+@Test(enabled = false, groups = {"needConfig"})
 public class LineageCombinerTest {
   final static String TEST_PROP_FILE_NAME = "wherehows-common-test.properties";
   LineageCombiner lineageCombiner;
@@ -60,7 +60,7 @@ public void setUp()
     PathAnalyzer.initialize(conn);
   }
 
-  @Test(groups = {"needConfig"})
+  @Test(enabled = false, groups = {"needConfig"})
   public void lineageCombinerTest() {
     int appId = Integer.valueOf(testProp.getProperty("lineageRecord.appId"));
     long flowExecId = Long.valueOf(testProp.getProperty("lineageRecord.flowExecId"));
@@ -117,7 +117,7 @@ public void lineageCombinerTest() {
   /**
    * Need the file path pattern info in dataset_partition_layout_pattern table.
    */
-  @Test(groups = {"needConfig"})
+  @Test(enabled = false, groups = {"needConfig"})
   public void analyzeTest() {
     String fullPath = testProp.getProperty("analyzeTest.fullPath");
     DatasetPath datasetPath = PathAnalyzer.analyze(fullPath);

File: wherehows-kafka/src/main/java/wherehows/actors/KafkaConsumerMaster.java
Patch:
@@ -27,12 +27,12 @@
 import kafka.consumer.KafkaStream;
 import kafka.javaapi.consumer.ConsumerConnector;
 import lombok.extern.slf4j.Slf4j;
+import wherehows.common.kafka.schemaregistry.client.CachedSchemaRegistryClient;
+import wherehows.common.kafka.schemaregistry.client.SchemaRegistryClient;
+import wherehows.common.utils.JobsUtil;
 import wherehows.msgs.KafkaCommMsg;
 import wherehows.processors.KafkaConfig;
 import wherehows.processors.KafkaConfig.Topic;
-import wherehows.common.kafka.schemaregistry.client.CachedSchemaRegistryClient;
-import wherehows.common.kafka.schemaregistry.client.SchemaRegistryClient;
-import wherehows.utils.JobsUtil;
 
 import static wherehows.main.ApplicationStart.*;
 

File: wherehows-etl/src/main/java/metadata/etl/lineage/appworx/AppworxLineageEtl.java
Patch:
@@ -26,7 +26,7 @@ public class AppworxLineageEtl extends EtlJob {
 
     Connection conn;
 
-    public AppworxLineageEtl(int appId, long whExecId, Properties properties) {
+    public AppworxLineageEtl(long whExecId, Properties properties) {
         super(whExecId, properties);
         try {
             setUp();

File: wherehows-etl/src/test/java/metadata/etl/lineage/appworx/AppworxLineageEtlTest.java
Patch:
@@ -23,7 +23,7 @@ public class AppworxLineageEtlTest {
 
   @BeforeTest
   public void setUp() throws Exception {
-    awl = new AppworxLineageEtl(3, 0L, new Properties());
+    awl = new AppworxLineageEtl(0L, new Properties());
   }
 
   @Test(groups = {"needConfig"})

File: wherehows-backend/app/controllers/Application.java
Patch:
@@ -1,12 +1,12 @@
 /**
  * Copyright 2015 LinkedIn Corp. All rights reserved.
- * <p>
+ *
  * Licensed under the Apache License, Version 2.0 (the "License");
  * you may not use this file except in compliance with the License.
  * You may obtain a copy of the License at
- * <p>
+ *
  * http://www.apache.org/licenses/LICENSE-2.0
- * <p>
+ *
  * Unless required by applicable law or agreed to in writing, software
  * distributed under the License is distributed on an "AS IS" BASIS,
  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.

File: wherehows-frontend/app/controllers/Application.java
Patch:
@@ -1,12 +1,12 @@
 /**
  * Copyright 2015 LinkedIn Corp. All rights reserved.
- * <p>
+ *
  * Licensed under the Apache License, Version 2.0 (the "License");
  * you may not use this file except in compliance with the License.
  * You may obtain a copy of the License at
- * <p>
+ *
  * http://www.apache.org/licenses/LICENSE-2.0
- * <p>
+ *
  * Unless required by applicable law or agreed to in writing, software
  * distributed under the License is distributed on an "AS IS" BASIS,
  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.

File: wherehows-etl/src/main/java/metadata/etl/JythonEtlJob.java
Patch:
@@ -41,6 +41,7 @@ public void transform() throws Exception {
     String script = prop.getProperty(Constant.JOB_JYTHON_TRANSFORM_KEY);
     if (script == null) {
       logger.info("Skipped transform as no script is defined in {}", Constant.JOB_JYTHON_TRANSFORM_KEY);
+      return;
     }
 
     runScript(script);
@@ -51,6 +52,7 @@ public void load() throws Exception {
     String script = prop.getProperty(Constant.JOB_JYTHON_LOAD_KEY);
     if (script == null) {
       logger.info("Skipped load as no script is defined in {}", Constant.JOB_JYTHON_LOAD_KEY);
+      return;
     }
 
     runScript(script);

File: wherehows-backend/app/models/kafka/GobblinTrackingAuditProcessor.java
Patch:
@@ -47,7 +47,7 @@ public class GobblinTrackingAuditProcessor extends KafkaConsumerProcessor {
   private DatasetClassificationDao datasetClassificationDao;
 
   public GobblinTrackingAuditProcessor() {
-    this.datasetClassificationDao = Application.daoFactory.getDatasetClassificationDao();
+    this.datasetClassificationDao = Application.DAO_FACTORY.getDatasetClassificationDao();
   }
 
   // TODO: Make these regex patterns part of job file

File: wherehows-frontend/app/controllers/api/v1/Dataset.java
Patch:
@@ -46,7 +46,7 @@ public class Dataset extends Controller
     private static final NamedParameterJdbcTemplate namedJdbcTemplate =
         AbstractMySQLOpenSourceDAO.getNamedParameterJdbcTemplate();
 
-    private static final DatasetsDao datasetsDao = Application.daoFactory.getDatasetsDao();
+    private static final DatasetsDao datasetsDao = Application.DAO_FACTORY.getDatasetsDao();
 
     public static Result getDatasetOwnerTypes()
     {

File: wherehows-frontend/app/controllers/Application.java
Patch:
@@ -54,7 +54,8 @@ public class Application extends Controller {
   private static final String PIWIK_SITE_ID = Play.application().configuration().getString("tracking.piwik.siteid");
   private static final String PIWIK_URL = Play.application().configuration().getString("tracking.piwik.url");
   private static final Boolean IS_INTERNAL = Play.application().configuration().getBoolean("linkedin.internal", false);
-  private static final String DB_WHEREHOWS_URL = Play.application().configuration().getString("db.wherehows.url");
+  private static final String DB_WHEREHOWS_URL =
+      Play.application().configuration().getString("database.opensource.url");
   private static final String WHZ_DB_DSCLASSNAME =
       Play.application().configuration().getString("hikaricp.dataSourceClassName");
   private static final String DB_WHEREHOWS_USERNAME =

File: wherehows-etl/src/test/java/metadata/etl/lineage/AzLineageExtractorTest.java
Patch:
@@ -65,7 +65,7 @@ public void extractLineageTest()
     Statement statement = conn.createStatement();
     statement.execute("TRUNCATE TABLE stg_job_execution_data_lineage");
     AzExecMessage message = new AzExecMessage(aje, prop);
-    message.databaseWriter = new DatabaseWriter(connUrl, "stg_job_execution_data_lineage");
+    message.databaseWriter = new DatabaseWriter(connUrl, "", "", "stg_job_execution_data_lineage");
     message.hnne = new HadoopJobHistoryNodeExtractor(prop);
     message.adc = new AzDbCommunicator(prop);
     message.connection = conn;

File: wherehows-common/src/main/java/wherehows/common/writers/DatabaseWriter.java
Patch:
@@ -44,9 +44,9 @@ public DatabaseWriter(DataSource dataSource, String tableName) {
     this.tableName = tableName;
   }
 
-  public DatabaseWriter(String connectionUrl, String tableName)
+  public DatabaseWriter(String connectionUrl, String username, String password, String tableName)
       throws SQLException {
-    DriverManagerDataSource dataSource = new DriverManagerDataSource(connectionUrl);
+    DriverManagerDataSource dataSource = new DriverManagerDataSource(connectionUrl, username, password);
     this.jdbcTemplate = new JdbcTemplate(dataSource);
     this.tableName = tableName;
   }

File: wherehows-etl/src/main/java/metadata/etl/lineage/AzLineageExtractorMaster.java
Patch:
@@ -95,7 +95,7 @@ public void run(int timeFrame, long endTimeStamp)
     String wherehowsUserName = prop.getProperty(Constant.WH_DB_USERNAME_KEY);
     String wherehowsPassWord = prop.getProperty(Constant.WH_DB_PASSWORD_KEY);
     Connection conn = DriverManager.getConnection(wherehowsUrl, wherehowsUserName, wherehowsPassWord);
-    DatabaseWriter databaseWriter = new DatabaseWriter(conn.getMetaData().getURL(), "stg_job_execution_data_lineage");
+    DatabaseWriter databaseWriter = new DatabaseWriter(wherehowsUrl, wherehowsUserName, wherehowsPassWord, "stg_job_execution_data_lineage");
 
     AzLogParser.initialize(conn);
     PathAnalyzer.initialize(conn);

File: wherehows-etl/src/main/java/metadata/etl/lineage/appworx/AppworxLineageEtl.java
Patch:
@@ -46,11 +46,10 @@ public AppworxLineageEtl(int appId, long whExecId, Properties properties) {
 
     public void setUp()
             throws SQLException {
-        String wherehowsHost = this.prop.getProperty(Constant.WH_DB_URL_KEY);
+        String wherehowsUrl = this.prop.getProperty(Constant.WH_DB_URL_KEY);
         String wherehowsUserName = this.prop.getProperty(Constant.WH_DB_USERNAME_KEY);
         String wherehowsPassWord = this.prop.getProperty(Constant.WH_DB_PASSWORD_KEY);
-        conn = DriverManager.getConnection(
-                wherehowsHost + "?" + "user=" + wherehowsUserName + "&password=" + wherehowsPassWord);
+        conn = DriverManager.getConnection(wherehowsUrl, wherehowsUserName, wherehowsPassWord);
     }
 
     @Override

File: wherehows-etl/src/test/java/metadata/etl/lineage/AzLineageExtractorTest.java
Patch:
@@ -43,8 +43,7 @@ public void setUp()
     String wherehowsUrl = prop.getProperty(Constant.WH_DB_URL_KEY);
     String wherehowsUserName = prop.getProperty(Constant.WH_DB_USERNAME_KEY);
     String wherehowsPassWord = prop.getProperty(Constant.WH_DB_PASSWORD_KEY);
-    connUrl = wherehowsUrl + "?" + "user=" + wherehowsUserName + "&password=" + wherehowsPassWord;
-    this.conn = DriverManager.getConnection(connUrl);
+    this.conn = DriverManager.getConnection(wherehowsUrl, wherehowsUserName, wherehowsPassWord);
     AzLogParser.initialize(conn);
     PathAnalyzer.initialize(conn);
   }

File: wherehows-common/src/test/java/LineageCombinerTest.java
Patch:
@@ -51,11 +51,10 @@ public void setUp()
     } else {
       throw new FileNotFoundException("Lack of configuration file for testing: " + TEST_PROP_FILE_NAME);
     }
-    String wherehowsHost = testProp.getProperty("wherehows.db.jdbc.url");
+    String wherehowsUrl = testProp.getProperty("wherehows.db.jdbc.url");
     String wherehowsUserName = testProp.getProperty("wherehows.db.username");
     String wherehowsPassWord = testProp.getProperty("wherehows.db.password");
-    Connection conn = DriverManager
-        .getConnection(wherehowsHost + "?" + "user=" + wherehowsUserName + "&password=" + wherehowsPassWord);
+    Connection conn = DriverManager.getConnection(wherehowsUrl, wherehowsUserName, wherehowsPassWord);
 
     lineageCombiner = new LineageCombiner(conn);
     PathAnalyzer.initialize(conn);

File: wherehows-etl/src/main/java/metadata/etl/lineage/AzDbCommunicator.java
Patch:
@@ -35,11 +35,11 @@ public class AzDbCommunicator {
 
   public AzDbCommunicator(Properties prop)
     throws Exception {
-    String host = prop.getProperty(Constant.AZ_DB_URL_KEY);
+    String url = prop.getProperty(Constant.AZ_DB_URL_KEY);
     String userName = prop.getProperty(Constant.AZ_DB_USERNAME_KEY);
     String passWord = prop.getProperty(Constant.AZ_DB_PASSWORD_KEY);
     // set up connections
-    conn = DriverManager.getConnection(host + "?" + "user=" + userName + "&password=" + passWord);
+    conn = DriverManager.getConnection(url, userName, passWord);
   }
 
   public String getExecLog(long execId, String jobName)

File: wherehows-etl/src/main/java/metadata/etl/lineage/AzJobChecker.java
Patch:
@@ -43,11 +43,11 @@ public class AzJobChecker {
   public AzJobChecker(Properties prop)
     throws SQLException {
     appId = Integer.valueOf(prop.getProperty(Constant.APP_ID_KEY));
-    String host = prop.getProperty(Constant.AZ_DB_URL_KEY);
+    String url = prop.getProperty(Constant.AZ_DB_URL_KEY);
     String userName = prop.getProperty(Constant.AZ_DB_USERNAME_KEY);
     String passWord = prop.getProperty(Constant.AZ_DB_PASSWORD_KEY);
     // set up connections
-    conn = DriverManager.getConnection(host + "?" + "user=" + userName + "&password=" + passWord);
+    conn = DriverManager.getConnection(url, userName, passWord);
   }
 
   /**

File: wherehows-etl/src/main/java/metadata/etl/lineage/AzLineageExtractorMaster.java
Patch:
@@ -94,9 +94,8 @@ public void run(int timeFrame, long endTimeStamp)
     String wherehowsUrl = prop.getProperty(Constant.WH_DB_URL_KEY);
     String wherehowsUserName = prop.getProperty(Constant.WH_DB_USERNAME_KEY);
     String wherehowsPassWord = prop.getProperty(Constant.WH_DB_PASSWORD_KEY);
-    String connUrl = wherehowsUrl + "?" + "user=" + wherehowsUserName + "&password=" + wherehowsPassWord;
-    Connection conn = DriverManager.getConnection(connUrl);
-    DatabaseWriter databaseWriter = new DatabaseWriter(connUrl, "stg_job_execution_data_lineage");
+    Connection conn = DriverManager.getConnection(wherehowsUrl, wherehowsUserName, wherehowsPassWord);
+    DatabaseWriter databaseWriter = new DatabaseWriter(conn.getMetaData().getURL(), "stg_job_execution_data_lineage");
 
     AzLogParser.initialize(conn);
     PathAnalyzer.initialize(conn);

File: wherehows-etl/src/test/java/metadata/etl/lineage/AzLogParserTest.java
Patch:
@@ -43,11 +43,10 @@ public void setUp()
     throws SQLException {
     Properties prop = new LineageTest().properties;
 
-    String wherehowsHost = prop.getProperty(Constant.WH_DB_URL_KEY);
+    String wherehowsUrl = prop.getProperty(Constant.WH_DB_URL_KEY);
     String wherehowsUserName = prop.getProperty(Constant.WH_DB_USERNAME_KEY);
     String wherehowsPassWord = prop.getProperty(Constant.WH_DB_PASSWORD_KEY);
-    Connection conn =
-      DriverManager.getConnection(wherehowsHost + "?" + "user=" + wherehowsUserName + "&password=" + wherehowsPassWord);
+    Connection conn = DriverManager.getConnection(wherehowsUrl, wherehowsUserName, wherehowsPassWord);
     AzLogParser.initialize(conn);
   }
 

File: metadata-etl/src/main/java/metadata/etl/lineage/HadoopJobHistoryNodeExtractor.java
Patch:
@@ -83,8 +83,8 @@ public HadoopJobHistoryNodeExtractor(Properties prop)
     }
     System.setProperty("javax.security.auth.useSubjectCredsOnly", "false");
 
-    System.setProperty("java.security.krb5.realm", prop.getProperty("krb5.realm"));
-    System.setProperty("java.security.krb5.kdc", prop.getProperty("krb5.kdc"));
+    System.setProperty("java.security.krb5.realm", prop.getProperty(Constant.KRB5_REALM));
+    System.setProperty("java.security.krb5.kdc", prop.getProperty(Constant.KRB5_KDC));
 
     PoolingHttpClientConnectionManager cm = new PoolingHttpClientConnectionManager();
     cm.setMaxTotal(200);

File: backend-service/app/actors/ConfigUtil.java
Patch:
@@ -86,6 +86,7 @@ static List<String> generateCommand(EtlJobName etlJobName, long whEtlExecId, Str
         .add("-Dconfig=" + configFile)
         .add("-DCONTEXT=" + etlJobName.name())
         .add("-Dlogback.configurationFile=etl_logback.xml")
+        .add("-DLOG_DIR=" + directoryPath)
         .add("metadata.etl.Launcher")
         .build();
   }

File: backend-service/test/actors/ConfigUtilTest.java
Patch:
@@ -54,7 +54,8 @@ public void shouldGenerateEtlJobCommandWithConfiguredDirectory() {
     // then:
     assertThat(command).contains("java", "-a", "-b", "-cp", System.getProperty("java.class.path"),
         "-Dconfig=" + applicationDirectory + "/exec/1.properties", "-DCONTEXT=LDAP_USER_ETL",
-        "-Dlogback.configurationFile=etl_logback.xml", "metadata.etl.Launcher");
+        "-DLOG_DIR=" + applicationDirectory, "-Dlogback.configurationFile=etl_logback.xml",
+        "metadata.etl.Launcher");
   }
 
   @Test

File: web/app/dao/MetadataStoreDao.java
Patch:
@@ -49,8 +49,8 @@ public static List<DatasetColumn> datasetColumnsMapper(SchemaFieldArray fields)
       DatasetColumn col = new DatasetColumn();
       col.fieldName = field.getFieldPath();
       col.fullFieldPath = field.getFieldPath();
-      col.dataType = field.getNativeDataType();
-      col.comment = field.getDescription();
+      col.dataType = field.hasNativeDataType() ? field.getNativeDataType() : "";
+      col.comment = field.hasDescription() ? field.getDescription() : "";
       columns.add(col);
     }
     return columns;

File: web/app/controllers/Application.java
Patch:
@@ -41,8 +41,6 @@ public class Application extends Controller
     private static final Integer PIWIK_SITE_ID = Play.application().configuration().getInt("tracking.piwik.siteid");
     private static final String PIWIK_URL = Play.application().configuration().getString("tracking.piwik.url");
     private static final Boolean IS_INTERNAL = Play.application().configuration().getBoolean("linkedin.internal", false);
-    private static final InputStream indexHtml = Play.application().classloader().getResourceAsStream("public/index.html");
-
 
     /**
      * Serves the build output index.html for any given path
@@ -51,6 +49,7 @@ public class Application extends Controller
      * @return {Result} build output index.html resource
      */
     private static Result serveAsset(String path) {
+        InputStream indexHtml = Play.application().classloader().getResourceAsStream("public/index.html");
         response().setHeader("Cache-Control", "no-cache");
 
         return ok(indexHtml).as("text/html");

File: web/app/controllers/Application.java
Patch:
@@ -177,8 +177,8 @@ public static Result login()
 
     @BodyParser.Of(BodyParser.Json.class)
     public static Result authenticate() {
-        // Create a new reponse ObjectNode to return when authenticate
-        //   requst is successful
+        // Create a new response ObjectNode to return when authenticate
+        //   request is successful
         ObjectNode response = Json.newObject();
         JsonNode json = request().body().asJson();
         // Extract username and password as String from JsonNode,

File: web/app/controllers/Secured.java
Patch:
@@ -25,8 +25,7 @@ public String getUsername(Context ctx) {
   }
 
   @Override
-  public Result onUnauthorized(Context ctx)
-  {
-    return redirect(controllers.routes.Application.login());
+  public Result onUnauthorized(Context ctx) {
+    return unauthorized();
   }
 }

File: wherehows-common/src/main/java/wherehows/common/Constant.java
Patch:
@@ -248,4 +248,7 @@ public class Constant {
   public static final String ESPRESSO_OUTPUT_KEY = "espresso.metadata";
   public static final String VOLDEMORT_OUTPUT_KEY = "voldemort.metadata";
   public static final String KAFKA_OUTPUT_KEY = "kafka.metadata";
+
+  // metadata-store restli server
+  public static final String WH_RESTLI_SERVER_URL = "wherehows.restli.server.url";
 }

File: web/app/dao/SearchDAO.java
Patch:
@@ -138,7 +138,7 @@ public class SearchDAO extends AbstractMySQLOpenSourceDAO
 			"CASE WHEN `dashboard_name` like '%$keyword' THEN 3 ELSE 0 END rank_18, " +
 			"CASE WHEN `dashboard_name` like '$keyword%' THEN 2 ELSE 0 END rank_19, " +
 			"CASE WHEN `dashboard_name` like '%$keyword%' THEN 1 ELSE 0 END rank_20 " +
-			"FROM dict_business_metric2 WHERE " +
+			"FROM dict_business_metric WHERE " +
 			"`metric_name` like '%$keyword%' or `dashboard_name` like '%$keyword%' or `metric_group` like '%$keyword%' " +
 			"or `metric_category` like '%$keyword%' or `metric_description` like '%$keyword%' ) m ORDER BY " +
 			"rank DESC, `metric_name`, `metric_category`, `metric_group`, `dashboard_name` LIMIT ?, ?;";

File: wherehows-common/src/main/java/wherehows/common/schemas/DatasetSecurityRecord.java
Patch:
@@ -21,7 +21,7 @@ public class DatasetSecurityRecord extends AbstractRecord {
 
   Integer datasetId;
   String datasetUrn;
-  Map<String, List<String>> classification;
+  Map<String, List<DatasetConfidentialEntityRecord>> classification;
   String recordOwnerType;
   DatasetRetentionRecord retentionPolicy;
   DatasetGeographicAffinityRecord geographicAffinity;
@@ -57,11 +57,11 @@ public void setDatasetUrn(String datasetUrn) {
     this.datasetUrn = datasetUrn;
   }
 
-  public Map<String, List<String>> getClassification() {
+  public Map<String, List<DatasetConfidentialEntityRecord>> getClassification() {
     return classification;
   }
 
-  public void setClassification(Map<String, List<String>> classification) {
+  public void setClassification(Map<String, List<DatasetConfidentialEntityRecord>> classification) {
     this.classification = classification;
   }
 

File: backend-service/app/controllers/DatasetController.java
Patch:
@@ -181,7 +181,7 @@ public static Result getDatasetDependentsByUri(String datasetUri)
     String dataset_type = uri_parts[0];
     String dataset_path = uri_parts[1].substring(2);  // start from the 3rd slash
     if (dataset_path.indexOf(".") > 0) {
-      dataset_path.replace(".", "/");
+      dataset_path = dataset_path.replace(".", "/");
     }
 
     if (dataset_path != null) {

File: web/app/dao/LineageDAO.java
Patch:
@@ -13,7 +13,6 @@
  */
 package dao;
 
-import java.math.BigDecimal;
 import java.math.BigInteger;
 import java.util.*;
 
@@ -27,10 +26,10 @@
 import org.springframework.jdbc.core.namedparam.MapSqlParameterSource;
 import org.springframework.jdbc.core.namedparam.NamedParameterJdbcTemplate;
 import play.Logger;
-import play.Play;
 import play.libs.Json;
 import utils.Lineage;
 
+
 public class LineageDAO extends AbstractMySQLOpenSourceDAO
 {
 	private final static String GET_APPLICATION_ID = "SELECT DISTINCT app_id FROM " +
@@ -278,7 +277,7 @@ public static void addTargetNode(
 					{
 						if (target.partition_end != null)
 						{
-							if (target.partition_end == node.partition_end)
+							if (target.partition_end.equals(node.partition_end))
 							{
 								existNode = target;
 								break;

File: wherehows-common/src/main/java/wherehows/common/schemas/DatasetConstraintRecord.java
Patch:
@@ -113,7 +113,7 @@ public Map<String, String> getAdditionalReferences() {
   }
 
   public void setAdditionalReferences(Map<String, String> additionalReferences) {
-    this.additionalReferences = this.additionalReferences;
+    this.additionalReferences = additionalReferences;
   }
 
   public Long getModifiedTime() {

File: backend-service/app/controllers/DatasetController.java
Patch:
@@ -181,7 +181,7 @@ public static Result getDatasetDependentsByUri(String datasetUri)
     String dataset_type = uri_parts[0];
     String dataset_path = uri_parts[1].substring(2);  // start from the 3rd slash
     if (dataset_path.indexOf(".") > 0) {
-      dataset_path.replace(".", "/");
+      dataset_path = dataset_path.replace(".", "/");
     }
 
     if (dataset_path != null) {

File: web/app/dao/LineageDAO.java
Patch:
@@ -13,7 +13,6 @@
  */
 package dao;
 
-import java.math.BigDecimal;
 import java.math.BigInteger;
 import java.util.*;
 
@@ -27,10 +26,10 @@
 import org.springframework.jdbc.core.namedparam.MapSqlParameterSource;
 import org.springframework.jdbc.core.namedparam.NamedParameterJdbcTemplate;
 import play.Logger;
-import play.Play;
 import play.libs.Json;
 import utils.Lineage;
 
+
 public class LineageDAO extends AbstractMySQLOpenSourceDAO
 {
 	private final static String GET_APPLICATION_ID = "SELECT DISTINCT app_id FROM " +
@@ -278,7 +277,7 @@ public static void addTargetNode(
 					{
 						if (target.partition_end != null)
 						{
-							if (target.partition_end == node.partition_end)
+							if (target.partition_end.equals(node.partition_end))
 							{
 								existNode = target;
 								break;

File: wherehows-common/src/main/java/wherehows/common/schemas/DatasetConstraintRecord.java
Patch:
@@ -113,7 +113,7 @@ public Map<String, String> getAdditionalReferences() {
   }
 
   public void setAdditionalReferences(Map<String, String> additionalReferences) {
-    this.additionalReferences = this.additionalReferences;
+    this.additionalReferences = additionalReferences;
   }
 
   public Long getModifiedTime() {

File: backend-service/app/actors/EtlJobActor.java
Patch:
@@ -82,7 +82,7 @@ public void onReceive(Object message)
             if (process.isAlive()) {
               process.destroy();
             }
-            Thread.sleep(60000);
+            Thread.sleep(10000);
           } else {
             break;
           }

File: backend-service/app/actors/EtlJobActor.java
Patch:
@@ -82,7 +82,7 @@ public void onReceive(Object message)
             if (process.isAlive()) {
               process.destroy();
             }
-            Thread.sleep(60000);
+            Thread.sleep(10000);
           } else {
             break;
           }

File: web/app/dao/LineageDAO.java
Patch:
@@ -121,7 +121,7 @@ public class LineageDAO extends AbstractMySQLOpenSourceDAO
 			"FROM_UNIXTIME(job_finished_unixtime) >  CURRENT_DATE - INTERVAL ? DAY";
 
 	private final static String GET_LATEST_FLOW_EXEC_ID = "SELECT max(flow_exec_id) FROM " +
-			"flow_execution where app_id = ? and flow_id = ?";
+			"flow_execution where flow_exec_status in ('SUCCEEDED', 'FINISHED') and app_id = ? and flow_id = ?";
 
 	private final static String GET_FLOW_DATA_LINEAGE = "SELECT ca.app_code, jedl.job_exec_id, jedl.job_name, " +
 			"jedl.storage_type, jedl.abstracted_object_name, jedl.source_target_type, jedl.record_count, " +

File: wherehows-common/src/main/java/wherehows/common/Constant.java
Patch:
@@ -196,6 +196,7 @@ public class Constant {
 
   public static final String WH_ELASTICSEARCH_URL_KEY = "wh.elasticsearch.url";
   public static final String WH_ELASTICSEARCH_PORT_KEY = "wh.elasticsearch.port";
+  public static final String WH_ELASTICSEARCH_INDEX_KEY = "wh.elasticsearch.index";
 
   // Oracle
   public static final String ORA_DB_USERNAME_KEY = "oracle.db.username";

File: metadata-etl/src/main/java/metadata/etl/models/EtlJobFactory.java
Patch:
@@ -28,6 +28,7 @@
 import metadata.etl.scheduler.azkaban.AzkabanExecEtl;
 import metadata.etl.scheduler.oozie.OozieExecEtl;
 import metadata.etl.models.EtlJobName;
+import metadata.etl.git.CodeSearchMetadataEtl;
 
 
 /**
@@ -63,6 +64,8 @@ public static EtlJob getEtlJob(EtlJobName etlJobName, Integer refId, Long whExec
         return new OracleMetadataEtl(refId, whExecId, properties);
       case PRODUCT_REPO_METADATA_ETL:
         return new MultiproductMetadataEtl(refId, whExecId, properties);
+      case DATABASE_SCM_METADATA_ETL:
+        return new CodeSearchMetadataEtl(refId, whExecId, properties);
       default:
         throw new UnsupportedOperationException("Unsupported job type: " + etlJobName);
     }

File: metadata-etl/src/main/java/metadata/etl/models/EtlJobName.java
Patch:
@@ -31,6 +31,7 @@ public enum EtlJobName {
   ORACLE_DATASET_METADATA_ETL(EtlType.DATASET, RefIdType.DB),
   PRODUCT_REPO_METADATA_ETL(EtlType.OPERATION, RefIdType.APP),
   KAFKA_CONSUMER_ETL(EtlType.OPERATION, RefIdType.DB),
+  DATABASE_SCM_METADATA_ETL(EtlType.OPERATION, RefIdType.APP),
   ;
 
   EtlType etlType;

File: backend-service/app/controllers/DatasetController.java
Patch:
@@ -20,7 +20,7 @@
 import java.util.List;
 import models.daos.DatasetDao;
 import models.daos.UserDao;
-import models.utils.Urn;
+import utils.Urn;
 import org.springframework.dao.EmptyResultDataAccessException;
 import play.Logger;
 import play.libs.Json;

File: backend-service/app/controllers/DatasetInfoController.java
Patch:
@@ -19,7 +19,7 @@
 import java.util.List;
 import java.util.Map;
 import models.daos.DatasetInfoDao;
-import models.utils.Urn;
+import utils.Urn;
 import org.springframework.dao.EmptyResultDataAccessException;
 import play.Logger;
 import play.libs.Json;

File: backend-service/app/controllers/LineageController.java
Patch:
@@ -19,7 +19,7 @@
 import java.util.List;
 import java.util.Map;
 import models.daos.LineageDao;
-import models.utils.Urn;
+import utils.Urn;
 import play.libs.Json;
 import play.mvc.BodyParser;
 import play.mvc.Controller;

File: backend-service/app/models/daos/LineageDao.java
Patch:
@@ -20,7 +20,7 @@
 import java.util.List;
 import java.util.Map;
 import java.util.TreeMap;
-import models.utils.Urn;
+import utils.Urn;
 import utils.JdbcUtil;
 import wherehows.common.schemas.LineageRecord;
 import wherehows.common.utils.PartitionPatternMatcher;

File: backend-service/app/models/kafka/KafkaConfig.java
Patch:
@@ -11,7 +11,7 @@
  * distributed under the License is distributed on an "AS IS" BASIS,
  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  */
-package utils;
+package models.kafka;
 
 import java.lang.reflect.Method;
 import java.util.HashMap;
@@ -22,6 +22,7 @@
 import models.daos.EtlJobPropertyDao;
 import org.apache.avro.generic.GenericData;
 import play.Logger;
+import utils.JdbcUtil;
 import wherehows.common.kafka.schemaregistry.client.SchemaRegistryClient;
 import wherehows.common.writers.DatabaseWriter;
 

File: backend-service/app/models/kafka/MetadataInventoryProcessor.java
Patch:
@@ -11,7 +11,7 @@
  * distributed under the License is distributed on an "AS IS" BASIS,
  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  */
-package utils;
+package models.kafka;
 
 import com.fasterxml.jackson.databind.JsonNode;
 import com.fasterxml.jackson.databind.ObjectMapper;
@@ -20,7 +20,7 @@
 import wherehows.common.schemas.Record;
 
 
-public class MetadataInventoryProcessor {
+public class MetadataInventoryProcessor extends KafkaConsumerProcessor {
 
   /**
    * Process a MetadataInventoryEvent record

File: backend-service/app/utils/Urn.java
Patch:
@@ -11,15 +11,15 @@
  * distributed under the License is distributed on an "AS IS" BASIS,
  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  */
-package models.utils;
+package utils;
 import java.util.Arrays;
 import java.util.HashSet;
 import java.util.Set;
 import play.Logger;
 
 
 /**
- * Urn class used for urn convertion
+ * Urn class used for urn conversion
  * Created by zsun on 1/15/15.
  */
 public class Urn {

File: metadata-etl/src/main/java/metadata/etl/models/EtlJobFactory.java
Patch:
@@ -28,6 +28,7 @@
 import metadata.etl.scheduler.azkaban.AzkabanExecEtl;
 import metadata.etl.scheduler.oozie.OozieExecEtl;
 import metadata.etl.models.EtlJobName;
+import metadata.etl.git.CodeSearchMetadataEtl;
 
 
 /**
@@ -63,6 +64,8 @@ public static EtlJob getEtlJob(EtlJobName etlJobName, Integer refId, Long whExec
         return new OracleMetadataEtl(refId, whExecId, properties);
       case PRODUCT_REPO_METADATA_ETL:
         return new MultiproductMetadataEtl(refId, whExecId, properties);
+      case DATABASE_SCM_METADATA_ETL:
+        return new CodeSearchMetadataEtl(refId, whExecId, properties);
       default:
         throw new UnsupportedOperationException("Unsupported job type: " + etlJobName);
     }

File: metadata-etl/src/main/java/metadata/etl/models/EtlJobName.java
Patch:
@@ -31,6 +31,7 @@ public enum EtlJobName {
   ORACLE_DATASET_METADATA_ETL(EtlType.DATASET, RefIdType.DB),
   PRODUCT_REPO_METADATA_ETL(EtlType.OPERATION, RefIdType.APP),
   KAFKA_CONSUMER_ETL(EtlType.OPERATION, RefIdType.DB),
+  DATABASE_SCM_METADATA_ETL(EtlType.OPERATION, RefIdType.APP),
   ;
 
   EtlType etlType;

File: wherehows-common/src/main/java/wherehows/common/schemas/DatasetPartitionRecord.java
Patch:
@@ -20,7 +20,7 @@ public class DatasetPartitionRecord extends AbstractRecord {
 
   Integer datasetId;
   String datasetUrn;
-  String totalPartitionLevel;
+  Integer totalPartitionLevel;
   String partitionSpecText;
   Boolean hasTimePartition;
   Boolean hasHashPartition;
@@ -58,11 +58,11 @@ public void setDatasetUrn(String datasetUrn) {
     this.datasetUrn = datasetUrn;
   }
 
-  public String getTotalPartitionLevel() {
+  public Integer getTotalPartitionLevel() {
     return totalPartitionLevel;
   }
 
-  public void setTotalPartitionLevel(String totalPartitionLevel) {
+  public void setTotalPartitionLevel(Integer totalPartitionLevel) {
     this.totalPartitionLevel = totalPartitionLevel;
   }
 

File: wherehows-common/src/main/java/wherehows/common/schemas/OozieFlowScheduleRecord.java
Patch:
@@ -18,7 +18,7 @@
  */
 public class OozieFlowScheduleRecord extends AzkabanFlowScheduleRecord {
   public OozieFlowScheduleRecord(Integer appId, String flowPath, String frequency, Integer interval,
-    Long effectiveStartTime, Long effectiveEndTime, String refId, Long whExecId) {
-    super(appId, flowPath, frequency, interval, effectiveStartTime, effectiveEndTime, refId, whExecId);
+    String cronExpression, Long effectiveStartTime, Long effectiveEndTime, String refId, Long whExecId) {
+    super(appId, flowPath, frequency, interval, cronExpression, effectiveStartTime, effectiveEndTime, refId, whExecId);
   }
 }

File: wherehows-common/src/main/java/wherehows/common/schemas/DatasetPartitionRecord.java
Patch:
@@ -20,7 +20,7 @@ public class DatasetPartitionRecord extends AbstractRecord {
 
   Integer datasetId;
   String datasetUrn;
-  String totalPartitionLevel;
+  Integer totalPartitionLevel;
   String partitionSpecText;
   Boolean hasTimePartition;
   Boolean hasHashPartition;
@@ -58,11 +58,11 @@ public void setDatasetUrn(String datasetUrn) {
     this.datasetUrn = datasetUrn;
   }
 
-  public String getTotalPartitionLevel() {
+  public Integer getTotalPartitionLevel() {
     return totalPartitionLevel;
   }
 
-  public void setTotalPartitionLevel(String totalPartitionLevel) {
+  public void setTotalPartitionLevel(Integer totalPartitionLevel) {
     this.totalPartitionLevel = totalPartitionLevel;
   }
 

File: wherehows-common/src/main/java/wherehows/common/enums/OwnerType.java
Patch:
@@ -15,7 +15,7 @@
 
 
 public enum OwnerType {
-
+  // the precedence (from high to low) is: OWNER, PRODUCER, DELEGATE, STAKEHOLDER
   OWNER(20),
   PRODUCER(40),
   DELEGATE(60),

File: backend-service/app/actors/KafkaConsumerWorker.java
Patch:
@@ -56,11 +56,11 @@ public void onReceive(Object message) throws Exception {
     if (message.equals("Start")) {
       Logger.info("Starting Thread: " + _threadId + " for topic: " + _topic);
       final ConsumerIterator<byte[], byte[]> it = _kafkaStream.iterator();
+      final Deserializer<Object> avroDeserializer = new KafkaAvroDeserializer(_schemaRegistryRestfulClient);
 
       while (it.hasNext()) { // block for next input
         try {
           MessageAndMetadata<byte[], byte[]> msg = it.next();
-          Deserializer<Object> avroDeserializer = new KafkaAvroDeserializer(_schemaRegistryRestfulClient);
           GenericData.Record kafkaMsgRecord = (GenericData.Record) avroDeserializer.deserialize(_topic, msg.message());
           // Logger.debug("Kafka worker ThreadId " + _threadId + " Topic " + _topic + " record: " + rec);
 

File: wherehows-common/src/main/java/wherehows/common/Constant.java
Patch:
@@ -173,8 +173,9 @@ public class Constant {
   public static final String HIVE_SCHEMA_JSON_FILE_KEY = "hive.schema_json_file";
   public static final String HIVE_DEPENDENCY_CSV_FILE_KEY = "hive.dependency_csv_file";
   public static final String HIVE_INSTANCE_CSV_FILE_KEY = "hive.instance_csv_file";
-  // public static final String HIVE_SAMPLE_CSV_FILE_KEY = "hive.sample_csv";
+  public static final String HIVE_SAMPLE_CSV_FILE_KEY = "hive.sample_csv_file";
   public static final String HIVE_SCHEMA_CSV_FILE_KEY = "hive.schema_csv_file";
+  public static final String HIVE_HDFS_MAP_CSV_FILE_KEY = "hive.hdfs_map_csv_file";
   public static final String HIVE_FIELD_METADATA_KEY = "hive.field_metadata";
 
   /** Property name of app id. For ETL process. ETL process will use this to identify the application */

File: backend-service/app/msgs/EtlJobMessage.java
Patch:
@@ -98,6 +98,8 @@ public void setCmdParam(String cmdParam) {
    * @return
    */
   public String toDebugString() {
-    return "jobtype:" + this.etlJobName.name() + "\trefId:" + this.refId + "\trefIdType:" + this.refIdType + "\twhEtlJobId:" + this.whEtlJobId + "\twhEtlExecId" + this.whEtlExecId;
+    return String.format("(jobType:%1s refIdType:%2s refId:%3d whEtlJobId:%4d whEtlExecId:%5d)",
+                         this.etlJobName.name(), this.refIdType, this.refId,
+                         this.whEtlJobId, this.whEtlExecId);
   }
 }

File: wherehows-common/src/main/java/wherehows/common/Constant.java
Patch:
@@ -28,6 +28,9 @@ public class Constant {
   /** The property_name field in wh_property table. Location of the folder that store interim csv file. */
   public static final String WH_APP_FOLDER_KEY = "wherehows.app_folder";
 
+  /** The property_name for logback CONTEXT_NAME. Used to set/fetch the system property */
+  public static final String LOGGER_CONTEXT_NAME_KEY = "CONTEXT_NAME";
+
   // For property_name in wh_etl_job_property table
   // Lineage
   /** The property_name field in wh_etl_job_property table. Azkaban database connection info */

File: web/app/models/DatasetColumnComment.java
Patch:
@@ -23,4 +23,5 @@ public class DatasetColumnComment {
   public String created;
   public String modified;
   public boolean isDefault;
+  public Boolean isAuthor;
 }

File: web/app/models/DatasetComment.java
Patch:
@@ -24,4 +24,5 @@ public class DatasetComment {
     public String created;
     public String modified;
     public String type;
+    public Boolean isAuthor;
 }
\ No newline at end of file

File: web/app/models/DatasetColumnComment.java
Patch:
@@ -23,4 +23,5 @@ public class DatasetColumnComment {
   public String created;
   public String modified;
   public boolean isDefault;
+  public Boolean isAuthor;
 }

File: web/app/models/DatasetComment.java
Patch:
@@ -24,4 +24,5 @@ public class DatasetComment {
     public String created;
     public String modified;
     public String type;
+    public Boolean isAuthor;
 }
\ No newline at end of file

File: metadata-etl/src/main/java/metadata/etl/kafka/GobblinTrackingCompactionProcessor.java
Patch:
@@ -50,7 +50,7 @@ public Record process(GenericData.Record record, String topic) throws Exception
 
       // for event name "CompactionCompleted" or "CompactionRecordCounts"
       if (name.equals("CompactionCompleted") || name.equals("CompactionRecordCounts")) {
-        logger.info("Processing Gobblin tracking event record: " + name);
+        // logger.info("Processing Gobblin tracking event record: " + name);
         final long timestamp = (long) record.get("timestamp");
         final Map<String, String> metadata = (Map<String, String>) record.get("metadata");
 

File: metadata-etl/src/main/java/metadata/etl/kafka/GobblinTrackingDistcpNgProcessor.java
Patch:
@@ -49,7 +49,7 @@ public Record process(GenericData.Record record, String topic) throws Exception
       final String name = (String) record.get("name");
 
       if (name.equals("DatasetPublished")) { // || name.equals("FilePublished")) {
-        logger.info("Processing Gobblin tracking event record: " + name);
+        // logger.info("Processing Gobblin tracking event record: " + name);
         final long timestamp = (long) record.get("timestamp");
         final Map<String, String> metadata = (Map<String, String>) record.get("metadata");
 

File: metadata-etl/src/main/java/metadata/etl/kafka/GobblinTrackingLumosProcessor.java
Patch:
@@ -57,7 +57,7 @@ public Record process(GenericData.Record record, String topic) throws Exception
       if (name.equals("DeltaPublished") || name.equals("SnapshotPublished")) {
         final long timestamp = (long) record.get("timestamp");
         final Map<String, String> metadata = (Map<String, String>) record.get("metadata");
-        logger.info("Processing Gobblin tracking event record: " + name + ", timestamp: " + timestamp);
+        // logger.info("Processing Gobblin tracking event record: " + name + ", timestamp: " + timestamp);
 
         final String jobContext = "Lumos:" + name;
         final String cluster = ClusterUtil.matchClusterCode(metadata.get("clusterIdentifier"));

File: metadata-etl/src/main/java/metadata/etl/kafka/MetastoreAuditProcessor.java
Patch:
@@ -33,7 +33,7 @@ public Record process(GenericData.Record record, String topic) throws Exception
 
     // handle MetastoreTableAuditEvent and MetastorePartitionAuditEvent
     if (record != null) {
-      logger.info("Processing Metastore Audit event record.");
+      // logger.info("Processing Metastore Audit event record.");
 
       final GenericData.Record auditHeader = (GenericData.Record) record.get("auditHeader");
       final String server = ClusterUtil.matchClusterCode(utf8ToString(auditHeader.get("server")));

File: metadata-etl/src/main/java/metadata/etl/models/EtlJobFactory.java
Patch:
@@ -21,6 +21,7 @@
 import metadata.etl.dataset.teradata.TeradataMetadataEtl;
 import metadata.etl.elasticsearch.ElasticSearchBuildIndexETL;
 import metadata.etl.git.GitMetadataEtl;
+import metadata.etl.git.MultiproductMetadataEtl;
 import metadata.etl.lineage.AzLineageMetadataEtl;
 import metadata.etl.ownership.DatasetOwnerEtl;
 import metadata.etl.ldap.LdapEtl;
@@ -60,6 +61,8 @@ public static EtlJob getEtlJob(EtlJobName etlJobName, Integer refId, Long whExec
         return new ElasticSearchBuildIndexETL(refId, whExecId, properties);
       case ORACLE_DATASET_METADATA_ETL:
         return new OracleMetadataEtl(refId, whExecId, properties);
+      case PRODUCT_REPO_METADATA_ETL:
+        return new MultiproductMetadataEtl(refId, whExecId, properties);
       default:
         throw new UnsupportedOperationException("Unsupported job type: " + etlJobName);
     }

File: metadata-etl/src/main/java/metadata/etl/models/EtlJobName.java
Patch:
@@ -28,7 +28,8 @@ public enum EtlJobName {
   HIVE_DATASET_METADATA_ETL(EtlType.DATASET, RefIdType.DB),
   ELASTICSEARCH_EXECUTION_INDEX_ETL(EtlType.OPERATION, RefIdType.APP),
   TREEBUILDER_EXECUTION_DATASET_ETL(EtlType.OPERATION, RefIdType.APP),
-  ORACLE_DATASET_METADATA_ETL(EtlType.DATASET, RefIdType.APP),
+  ORACLE_DATASET_METADATA_ETL(EtlType.DATASET, RefIdType.DB),
+  PRODUCT_REPO_METADATA_ETL(EtlType.OPERATION, RefIdType.APP),
   KAFKA_CONSUMER_ETL(EtlType.OPERATION, RefIdType.DB),
   ;
 

File: wherehows-common/src/main/java/wherehows/common/schemas/MultiproductRepoOwnerRecord.java
Patch:
@@ -22,13 +22,13 @@ public class MultiproductRepoOwnerRecord extends AbstractRecord {
   Integer appId;
   String scmRepoFullname;
   String scmType;
-  String repoId;
+  Integer repoId;
   String ownerType;
   String ownerName;
   String paths;
   Long whExecId;
 
-  public MultiproductRepoOwnerRecord(Integer appId, String scmRepoFullname, String scmType, String repoId,
+  public MultiproductRepoOwnerRecord(Integer appId, String scmRepoFullname, String scmType, Integer repoId,
       String ownerType, String ownerName, String paths, Long whExecId) {
     this.appId = appId;
     this.scmRepoFullname = scmRepoFullname;

File: wherehows-common/src/main/java/wherehows/common/schemas/MultiproductRepoRecord.java
Patch:
@@ -22,7 +22,7 @@ public class MultiproductRepoRecord extends AbstractRecord {
   Integer appId;
   String scmRepoFullname;
   String scmType;
-  String repoId;
+  Integer repoId;
   String project;
   String ownerType;
   String ownerName;
@@ -32,7 +32,7 @@ public class MultiproductRepoRecord extends AbstractRecord {
   String namespace;
   Long whExecId;
 
-  public MultiproductRepoRecord(Integer appId, String scmRepoFullname, String scmType, String repoId,
+  public MultiproductRepoRecord(Integer appId, String scmRepoFullname, String scmType, Integer repoId,
       String project, String ownerType, String ownerName, Long whExecId) {
     this.appId = appId;
     this.scmRepoFullname = scmRepoFullname;
@@ -78,7 +78,7 @@ public String getScmType() {
     return scmType;
   }
 
-  public String getRepoId() {
+  public Integer getRepoId() {
     return repoId;
   }
 }

File: web/app/dao/DatasetRowMapper.java
Patch:
@@ -53,6 +53,7 @@ public Dataset mapRow(ResultSet rs, int rowNum) throws SQLException
         String source = rs.getString(DATASET_SOURCE_COLUMN);
         String strOwner = rs.getString(DATASET_OWNER_ID_COLUMN);
         String strOwnerName = rs.getString(DATASET_OWNER_NAME_COLUMN);
+        String schema = rs.getString(DATASET_SCHEMA_COLUMN);
         Time created = rs.getTime(DATASET_CREATED_TIME_COLUMN);
         Time modified = rs.getTime(DATASET_MODIFIED_TIME_COLUMN);
         Integer schemaHistoryId = rs.getInt(SCHEMA_HISTORY_ID_COLUMN);
@@ -61,6 +62,7 @@ public Dataset mapRow(ResultSet rs, int rowNum) throws SQLException
         dataset.id = id;
         dataset.name = name;
         dataset.urn = urn;
+        dataset.schema = schema;
         String[] owners = null;
         if (StringUtils.isNotBlank(strOwner))
         {

File: web/app/dao/DatasetRowMapper.java
Patch:
@@ -53,6 +53,7 @@ public Dataset mapRow(ResultSet rs, int rowNum) throws SQLException
         String source = rs.getString(DATASET_SOURCE_COLUMN);
         String strOwner = rs.getString(DATASET_OWNER_ID_COLUMN);
         String strOwnerName = rs.getString(DATASET_OWNER_NAME_COLUMN);
+        String schema = rs.getString(DATASET_SCHEMA_COLUMN);
         Time created = rs.getTime(DATASET_CREATED_TIME_COLUMN);
         Time modified = rs.getTime(DATASET_MODIFIED_TIME_COLUMN);
         Integer schemaHistoryId = rs.getInt(SCHEMA_HISTORY_ID_COLUMN);
@@ -61,6 +62,7 @@ public Dataset mapRow(ResultSet rs, int rowNum) throws SQLException
         dataset.id = id;
         dataset.name = name;
         dataset.urn = urn;
+        dataset.schema = schema;
         String[] owners = null;
         if (StringUtils.isNotBlank(strOwner))
         {

File: web/app/controllers/api/v1/Flow.java
Patch:
@@ -167,7 +167,7 @@ public static Result getPagedFlows(String application, String project)
         return ok(result);
     }
 
-    public static Result getPagedJobs(String application, String project, Long flowId)
+    public static Result getPagedJobs(String application, Long flowId)
     {
         ObjectNode result = Json.newObject();
         int page = 1;
@@ -211,7 +211,7 @@ public static Result getPagedJobs(String application, String project, Long flowI
         }
 
         result.put("status", "ok");
-        result.set("data", FlowsDAO.getPagedJobsByFlow(application, project, flowId, page, size));
+        result.set("data", FlowsDAO.getPagedJobsByFlow(application, flowId, page, size));
         return ok(result);
     }
 

File: web/app/models/Job.java
Patch:
@@ -21,4 +21,5 @@ public class Job {
     public String created;
     public String modified;
     public Long refFlowId;
+    public String refFlowGroup;
 }

File: metadata-etl/src/main/java/metadata/etl/Launcher.java
Patch:
@@ -60,6 +60,7 @@ public static void main(String[] args)
       props.load(propFile);
       etlJobNameString = props.getProperty(JOB_NAME_KEY);
       refId = Integer.valueOf(props.getProperty(REF_ID_KEY));
+      whEtlId = Integer.valueOf(props.getProperty(WH_ETL_EXEC_ID_KEY));
 
     } catch (IOException e) {
        //logger.error("property file '{}' not found" , property_file);

File: wherehows-common/src/main/java/wherehows/common/Constant.java
Patch:
@@ -92,6 +92,8 @@ public class Constant {
   public static final String TD_LOAD_SAMPLE = "teradata.load_sample";
 
   // Hdfs
+  /** The property_name field in wh_etl_job_property table. Whether using remote mode or not */
+  public static final String HDFS_REMOTE = "hdfs.remote.mode";
   /** The property_name field in wh_etl_job_property table. The hfds remote user that run the hadoop job on gateway */
   public static final String HDFS_REMOTE_USER_KEY = "hdfs.remote.user";
   /** The property_name field in wh_etl_job_property table. The gateway machine name*/

File: metadata-etl/src/main/java/metadata/etl/Launcher.java
Patch:
@@ -60,6 +60,7 @@ public static void main(String[] args)
       props.load(propFile);
       etlJobNameString = props.getProperty(JOB_NAME_KEY);
       refId = Integer.valueOf(props.getProperty(REF_ID_KEY));
+      whEtlId = Integer.valueOf(props.getProperty(WH_ETL_EXEC_ID_KEY));
 
     } catch (IOException e) {
        //logger.error("property file '{}' not found" , property_file);

File: metadata-etl/src/main/java/metadata/etl/Launcher.java
Patch:
@@ -60,6 +60,7 @@ public static void main(String[] args)
       props.load(propFile);
       etlJobNameString = props.getProperty(JOB_NAME_KEY);
       refId = Integer.valueOf(props.getProperty(REF_ID_KEY));
+      whEtlId = Integer.valueOf(props.getProperty(WH_ETL_EXEC_ID_KEY));
 
     } catch (IOException e) {
        //logger.error("property file '{}' not found" , property_file);

File: backend-service/app/actors/SchedulerActor.java
Patch:
@@ -45,13 +45,14 @@ public void onReceive(Object message)
     throws Exception {
     if (message.equals("checking")) {
       List<Map<String, Object>> dueJobs = EtlJobDao.getDueJobs();
-      Logger.info("running " + dueJobs.size() + " jobs");
       Set<Integer> whiteList = Global.getWhiteList();
+      Logger.info("total " + dueJobs.size() + " jobs due, white list : " + whiteList);
       for (Map<String, Object> dueJob : dueJobs) {
         Integer whEtlJobId = ((Long) dueJob.get("wh_etl_job_id")).intValue();
         if (whiteList != null && !whiteList.contains(whEtlJobId)) {
           continue; // if we config the white list and it's not in white list, skip this job
         }
+        Logger.info("running job: job id :" + whEtlJobId);
         EtlJobName etlJobName = EtlJobName.valueOf((String) dueJob.get("wh_etl_job_name"));
         EtlType etlType = EtlType.valueOf((String) dueJob.get("wh_etl_type"));
         Integer refId = (Integer) dueJob.get("ref_id");

File: backend-service/app/actors/SchedulerActor.java
Patch:
@@ -45,13 +45,14 @@ public void onReceive(Object message)
     throws Exception {
     if (message.equals("checking")) {
       List<Map<String, Object>> dueJobs = EtlJobDao.getDueJobs();
-      Logger.info("running " + dueJobs.size() + " jobs");
       Set<Integer> whiteList = Global.getWhiteList();
+      Logger.info("total " + dueJobs.size() + " jobs due, white list : " + whiteList);
       for (Map<String, Object> dueJob : dueJobs) {
         Integer whEtlJobId = ((Long) dueJob.get("wh_etl_job_id")).intValue();
         if (whiteList != null && !whiteList.contains(whEtlJobId)) {
           continue; // if we config the white list and it's not in white list, skip this job
         }
+        Logger.info("running job: job id :" + whEtlJobId);
         EtlJobName etlJobName = EtlJobName.valueOf((String) dueJob.get("wh_etl_job_name"));
         EtlType etlType = EtlType.valueOf((String) dueJob.get("wh_etl_type"));
         Integer refId = (Integer) dueJob.get("ref_id");

File: web/app/controllers/api/v1/Search.java
Patch:
@@ -38,12 +38,13 @@ public static Result getSearchAutoComplete()
         return ok(result);
     }
 
-    public static Result searchByKeyword(String keyword)
+    public static Result searchByKeyword()
     {
         ObjectNode result = Json.newObject();
 
         int page = 1;
         int size = 10;
+        String keyword = request().getQueryString("keyword");
         String category = request().getQueryString("category");
         String source = request().getQueryString("source");
         String pageStr = request().getQueryString("page");

File: web/app/controllers/api/v1/Search.java
Patch:
@@ -38,12 +38,13 @@ public static Result getSearchAutoComplete()
         return ok(result);
     }
 
-    public static Result searchByKeyword(String keyword)
+    public static Result searchByKeyword()
     {
         ObjectNode result = Json.newObject();
 
         int page = 1;
         int size = 10;
+        String keyword = request().getQueryString("keyword");
         String category = request().getQueryString("category");
         String source = request().getQueryString("source");
         String pageStr = request().getQueryString("page");

File: web/app/dao/LineageDAO.java
Patch:
@@ -91,7 +91,7 @@ public class LineageDAO extends AbstractMySQLOpenSourceDAO
 
     private final static String GET_ONE_LEVEL_IMPACT_DATABASES = "SELECT DISTINCT j.storage_type, " +
             "j.abstracted_object_name, d.id FROM job_execution_data_lineage j " +
-            "Left join dict_dataset d on substring_index(d.urn, '://', -1) = j.abstracted_object_name " +
+            "LEFT JOIN dict_dataset d ON d.urn = concat(j.storage_type, '://', j.abstracted_object_name) " +
             "WHERE (app_id, job_exec_id) in ( " +
             "SELECT app_id, job_exec_id FROM job_execution_data_lineage " +
             "WHERE abstracted_object_name in (:pathlist) and source_target_type = 'source' and " +

File: web/app/dao/LineageDAO.java
Patch:
@@ -91,7 +91,7 @@ public class LineageDAO extends AbstractMySQLOpenSourceDAO
 
     private final static String GET_ONE_LEVEL_IMPACT_DATABASES = "SELECT DISTINCT j.storage_type, " +
             "j.abstracted_object_name, d.id FROM job_execution_data_lineage j " +
-            "Left join dict_dataset d on substring_index(d.urn, '://', -1) = j.abstracted_object_name " +
+            "LEFT JOIN dict_dataset d ON d.urn = concat(j.storage_type, '://', j.abstracted_object_name) " +
             "WHERE (app_id, job_exec_id) in ( " +
             "SELECT app_id, job_exec_id FROM job_execution_data_lineage " +
             "WHERE abstracted_object_name in (:pathlist) and source_target_type = 'source' and " +

File: backend-service/app/actors/TreeBuilderActor.java
Patch:
@@ -56,14 +56,14 @@ public void onReceive(Object o) throws Exception {
           in = EtlJob.class.getClassLoader().getResourceAsStream("jython/FlowTreeBuilder.py");
           break;
         default:
-          Logger.warn("unknown message : {}", msg);
+          Logger.error("unknown message : {}", msg);
       }
       if (in != null) {
         interpreter.execfile(in);
         in.close();
         Logger.info("Finish build {} tree", msg);
       } else {
-        Logger.warn("can not find jython script");
+        Logger.error("can not find jython script");
       }
     } else {
       throw new Exception("message type is not supported!");

File: backend-service/app/actors/TreeBuilderActor.java
Patch:
@@ -56,14 +56,14 @@ public void onReceive(Object o) throws Exception {
           in = EtlJob.class.getClassLoader().getResourceAsStream("jython/FlowTreeBuilder.py");
           break;
         default:
-          Logger.warn("unknown message : {}", msg);
+          Logger.error("unknown message : {}", msg);
       }
       if (in != null) {
         interpreter.execfile(in);
         in.close();
         Logger.info("Finish build {} tree", msg);
       } else {
-        Logger.warn("can not find jython script");
+        Logger.error("can not find jython script");
       }
     } else {
       throw new Exception("message type is not supported!");

File: web/app/dao/DatasetRowMapper.java
Patch:
@@ -41,7 +41,7 @@ public class DatasetRowMapper implements RowMapper<Dataset>
     public static String DATASET_OWNER_NAME_COLUMN = "owner_name";
     public static String SCHEMA_HISTORY_ID_COLUMN = "schema_history_id";
     public static String HDFS_PREFIX = "hdfs";
-    public static int URN_PREFIX_LEN = 7;
+    public static int HDFS_URN_PREFIX_LEN = 7;  //for hdfs prefix is hdfs:///, but we need the last slash
 
 
     @Override
@@ -103,7 +103,7 @@ public Dataset mapRow(ResultSet rs, int rowNum) throws SQLException
             if (dataset.urn.substring(0, 4).equalsIgnoreCase(HDFS_PREFIX))
             {
                 dataset.hdfsBrowserLink = Play.application().configuration().getString(DatasetsDAO.HDFS_BROWSER_URL_KEY) +
-                        dataset.urn.substring(URN_PREFIX_LEN);
+                        dataset.urn.substring(HDFS_URN_PREFIX_LEN);
             }
         }
         dataset.source = source;

File: web/app/dao/DatasetWithUserRowMapper.java
Patch:
@@ -106,7 +106,7 @@ public Dataset mapRow(ResultSet rs, int rowNum) throws SQLException
             if (dataset.urn.substring(0, 4).equalsIgnoreCase(HDFS_PREFIX))
             {
                 dataset.hdfsBrowserLink = Play.application().configuration().getString(DatasetsDAO.HDFS_BROWSER_URL_KEY) +
-                        dataset.urn.substring(DatasetRowMapper.URN_PREFIX_LEN);
+                        dataset.urn.substring(DatasetRowMapper.HDFS_URN_PREFIX_LEN);
             }
         }
         dataset.source = source;

File: web/app/dao/DatasetsDAO.java
Patch:
@@ -455,7 +455,7 @@ public ObjectNode doInTransaction(TransactionStatus status) {
 						if (ds.urn.substring(0, 4).equalsIgnoreCase(DatasetRowMapper.HDFS_PREFIX))
 						{
 							ds.hdfsBrowserLink = Play.application().configuration().getString(HDFS_BROWSER_URL_KEY) +
-									ds.urn.substring(DatasetRowMapper.URN_PREFIX_LEN);
+									ds.urn.substring(DatasetRowMapper.HDFS_URN_PREFIX_LEN);
 						}
 					}
 					if (favoriteId != null && favoriteId > 0)

File: web/app/dao/DatasetRowMapper.java
Patch:
@@ -41,7 +41,7 @@ public class DatasetRowMapper implements RowMapper<Dataset>
     public static String DATASET_OWNER_NAME_COLUMN = "owner_name";
     public static String SCHEMA_HISTORY_ID_COLUMN = "schema_history_id";
     public static String HDFS_PREFIX = "hdfs";
-    public static int URN_PREFIX_LEN = 7;
+    public static int HDFS_URN_PREFIX_LEN = 7;  //for hdfs prefix is hdfs:///, but we need the last slash
 
 
     @Override
@@ -103,7 +103,7 @@ public Dataset mapRow(ResultSet rs, int rowNum) throws SQLException
             if (dataset.urn.substring(0, 4).equalsIgnoreCase(HDFS_PREFIX))
             {
                 dataset.hdfsBrowserLink = Play.application().configuration().getString(DatasetsDAO.HDFS_BROWSER_URL_KEY) +
-                        dataset.urn.substring(URN_PREFIX_LEN);
+                        dataset.urn.substring(HDFS_URN_PREFIX_LEN);
             }
         }
         dataset.source = source;

File: web/app/dao/DatasetWithUserRowMapper.java
Patch:
@@ -106,7 +106,7 @@ public Dataset mapRow(ResultSet rs, int rowNum) throws SQLException
             if (dataset.urn.substring(0, 4).equalsIgnoreCase(HDFS_PREFIX))
             {
                 dataset.hdfsBrowserLink = Play.application().configuration().getString(DatasetsDAO.HDFS_BROWSER_URL_KEY) +
-                        dataset.urn.substring(DatasetRowMapper.URN_PREFIX_LEN);
+                        dataset.urn.substring(DatasetRowMapper.HDFS_URN_PREFIX_LEN);
             }
         }
         dataset.source = source;

File: web/app/dao/DatasetsDAO.java
Patch:
@@ -455,7 +455,7 @@ public ObjectNode doInTransaction(TransactionStatus status) {
 						if (ds.urn.substring(0, 4).equalsIgnoreCase(DatasetRowMapper.HDFS_PREFIX))
 						{
 							ds.hdfsBrowserLink = Play.application().configuration().getString(HDFS_BROWSER_URL_KEY) +
-									ds.urn.substring(DatasetRowMapper.URN_PREFIX_LEN);
+									ds.urn.substring(DatasetRowMapper.HDFS_URN_PREFIX_LEN);
 						}
 					}
 					if (favoriteId != null && favoriteId > 0)

File: web/app/dao/DatasetRowMapper.java
Patch:
@@ -41,6 +41,7 @@ public class DatasetRowMapper implements RowMapper<Dataset>
     public static String DATASET_OWNER_NAME_COLUMN = "owner_name";
     public static String SCHEMA_HISTORY_ID_COLUMN = "schema_history_id";
     public static String HDFS_PREFIX = "hdfs";
+    public static int URN_PREFIX_LEN = 7;
 
 
     @Override
@@ -102,7 +103,7 @@ public Dataset mapRow(ResultSet rs, int rowNum) throws SQLException
             if (dataset.urn.substring(0, 4).equalsIgnoreCase(HDFS_PREFIX))
             {
                 dataset.hdfsBrowserLink = Play.application().configuration().getString(DatasetsDAO.HDFS_BROWSER_URL_KEY) +
-                        dataset.urn.substring(6);
+                        dataset.urn.substring(URN_PREFIX_LEN);
             }
         }
         dataset.source = source;

File: web/app/dao/DatasetWithUserRowMapper.java
Patch:
@@ -106,7 +106,7 @@ public Dataset mapRow(ResultSet rs, int rowNum) throws SQLException
             if (dataset.urn.substring(0, 4).equalsIgnoreCase(HDFS_PREFIX))
             {
                 dataset.hdfsBrowserLink = Play.application().configuration().getString(DatasetsDAO.HDFS_BROWSER_URL_KEY) +
-                        dataset.urn.substring(6);
+                        dataset.urn.substring(DatasetRowMapper.URN_PREFIX_LEN);
             }
         }
         dataset.source = source;

File: web/app/dao/DatasetsDAO.java
Patch:
@@ -454,7 +454,8 @@ public ObjectNode doInTransaction(TransactionStatus status) {
 					{
 						if (ds.urn.substring(0, 4).equalsIgnoreCase(DatasetRowMapper.HDFS_PREFIX))
 						{
-							ds.hdfsBrowserLink = Play.application().configuration().getString(HDFS_BROWSER_URL_KEY) + ds.urn.substring(7);
+							ds.hdfsBrowserLink = Play.application().configuration().getString(HDFS_BROWSER_URL_KEY) +
+									ds.urn.substring(DatasetRowMapper.URN_PREFIX_LEN);
 						}
 					}
 					if (favoriteId != null && favoriteId > 0)

File: web/app/dao/DatasetRowMapper.java
Patch:
@@ -41,6 +41,7 @@ public class DatasetRowMapper implements RowMapper<Dataset>
     public static String DATASET_OWNER_NAME_COLUMN = "owner_name";
     public static String SCHEMA_HISTORY_ID_COLUMN = "schema_history_id";
     public static String HDFS_PREFIX = "hdfs";
+    public static int URN_PREFIX_LEN = 7;
 
 
     @Override
@@ -102,7 +103,7 @@ public Dataset mapRow(ResultSet rs, int rowNum) throws SQLException
             if (dataset.urn.substring(0, 4).equalsIgnoreCase(HDFS_PREFIX))
             {
                 dataset.hdfsBrowserLink = Play.application().configuration().getString(DatasetsDAO.HDFS_BROWSER_URL_KEY) +
-                        dataset.urn.substring(6);
+                        dataset.urn.substring(URN_PREFIX_LEN);
             }
         }
         dataset.source = source;

File: web/app/dao/DatasetWithUserRowMapper.java
Patch:
@@ -106,7 +106,7 @@ public Dataset mapRow(ResultSet rs, int rowNum) throws SQLException
             if (dataset.urn.substring(0, 4).equalsIgnoreCase(HDFS_PREFIX))
             {
                 dataset.hdfsBrowserLink = Play.application().configuration().getString(DatasetsDAO.HDFS_BROWSER_URL_KEY) +
-                        dataset.urn.substring(6);
+                        dataset.urn.substring(DatasetRowMapper.URN_PREFIX_LEN);
             }
         }
         dataset.source = source;

File: web/app/dao/DatasetsDAO.java
Patch:
@@ -454,7 +454,8 @@ public ObjectNode doInTransaction(TransactionStatus status) {
 					{
 						if (ds.urn.substring(0, 4).equalsIgnoreCase(DatasetRowMapper.HDFS_PREFIX))
 						{
-							ds.hdfsBrowserLink = Play.application().configuration().getString(HDFS_BROWSER_URL_KEY) + ds.urn.substring(7);
+							ds.hdfsBrowserLink = Play.application().configuration().getString(HDFS_BROWSER_URL_KEY) +
+									ds.urn.substring(DatasetRowMapper.URN_PREFIX_LEN);
 						}
 					}
 					if (favoriteId != null && favoriteId > 0)

File: backend-service/app/actors/CmdUtil.java
Patch:
@@ -48,7 +48,7 @@ public static String generateCMD(EtlJobName etlJobName, int refId, long whEtlExe
     }
 
     String classPath = System.getProperty("java.class.path");
-    sb.append("-cp").append(" \"").append(classPath).append("\" ");
+    sb.append("-cp").append(" '").append(classPath).append("' ");
     sb.append("metadata.etl.Launcher");
 
     return sb.toString();

File: metadata-etl/src/main/java/metadata/etl/EtlJob.java
Patch:
@@ -164,6 +164,8 @@ public abstract void load()
 
   public void setup()
     throws Exception {
+    // redirect error to out
+    System.setErr(System.out);
 
   }
 

File: metadata-etl/src/main/java/metadata/etl/dataset/hive/HiveViewDependency.java
Patch:
@@ -27,6 +27,9 @@ public class HiveViewDependency {
   static LineageInfo lineageInfoTool =  new LineageInfo();
 
   public static String[] getViewDependency(String hiveQl) {
+    if (hiveQl == null)
+      return new String[]{};
+
     try {
       lineageInfoTool.getLineageInfo(hiveQl);
       TreeSet<String> inputs = lineageInfoTool.getInputTableList();

File: backend-service/app/actors/EtlJobActor.java
Patch:
@@ -25,6 +25,7 @@
 import models.daos.EtlJobPropertyDao;
 import msgs.EtlJobMessage;
 import play.Logger;
+import shared.Global;
 
 
 /**
@@ -82,6 +83,8 @@ public void onReceive(Object message)
           process.destroy();
         }
         EtlJobDao.endRun(msg.getWhEtlExecId(), EtlJobStatus.ERROR, e.getMessage());
+      } finally {
+        Global.removeRunningJob(((EtlJobMessage) message).getWhEtlJobId());
       }
     }
   }

File: backend-service/app/utils/SchedulerUtil.java
Patch:
@@ -28,7 +28,7 @@ public class SchedulerUtil {
   public static Cancellable schedulerRef;
 
   public static synchronized void start() {
-    start(Play.application().configuration().getLong("scheduler.check.interval"));
+    start(Play.application().configuration().getLong("scheduler.check.interval", 10L));
   }
 
   /**

File: wherehows-common/src/main/java/wherehows/common/Constant.java
Patch:
@@ -157,6 +157,7 @@ public class Constant {
   public static final String HIVE_METASTORE_USERNAME = "hive.metstore.username";
   public static final String HIVE_METASTORE_PASSWORD = "hive.metastore.password";
 
+  public static final String HIVE_DATABASE_WHITELIST_KEY = "hive.database_white_list";
   public static final String HIVE_SCHEMA_JSON_FILE_KEY = "hive.schema_json_file";
   // public static final String HIVE_SAMPLE_CSV_FILE_KEY = "hive.sample_csv";
   public static final String HIVE_SCHEMA_CSV_FILE_KEY = "hive.schema_csv_file";

File: metadata-etl/src/test/java/metadata/etl/dataset/hdfs/HdfsMetadataEtlTest.java
Patch:
@@ -26,7 +26,7 @@ public class HdfsMetadataEtlTest {
   @BeforeTest
   public void setUp()
     throws Exception {
-    ds = new HdfsMetadataEtl(21, 0L);
+    ds = new HdfsMetadataEtl(2, 0L);
   }
 
   @Test(groups = {"needConfig"})

File: metadata-etl/src/test/java/metadata/etl/dataset/hive/HiveTest.java
Patch:
@@ -26,7 +26,7 @@ public class HiveTest {
   @BeforeTest
   public void setUp()
     throws Exception {
-    hm = new HiveMetadataEtl(0, 0L);
+    hm = new HiveMetadataEtl(3, 0L);
   }
 
   @Test

File: metadata-etl/src/test/java/metadata/etl/dataset/hdfs/HdfsMetadataEtlTest.java
Patch:
@@ -26,7 +26,7 @@ public class HdfsMetadataEtlTest {
   @BeforeTest
   public void setUp()
     throws Exception {
-    ds = new HdfsMetadataEtl(21, 0L);
+    ds = new HdfsMetadataEtl(2, 0L);
   }
 
   @Test(groups = {"needConfig"})

File: metadata-etl/src/test/java/metadata/etl/dataset/hive/HiveTest.java
Patch:
@@ -26,7 +26,7 @@ public class HiveTest {
   @BeforeTest
   public void setUp()
     throws Exception {
-    hm = new HiveMetadataEtl(0, 0L);
+    hm = new HiveMetadataEtl(3, 0L);
   }
 
   @Test

File: web/app/controllers/api/v1/Search.java
Patch:
@@ -88,7 +88,7 @@ public static Result searchByKeyword(String keyword)
         result.put("status", "ok");
         if (StringUtils.isBlank(category))
         {
-            category = "dataset";
+            category = "datasets";
         }
         if (StringUtils.isBlank(source))
         {
@@ -98,11 +98,11 @@ public static Result searchByKeyword(String keyword)
         {
             result.set("result", SearchDAO.getPagedMetricByKeyword(keyword, page, size));
         }
-        else if (category.toLowerCase().equalsIgnoreCase("flow"))
+        else if (category.toLowerCase().equalsIgnoreCase("flows"))
         {
             result.set("result", SearchDAO.getPagedFlowByKeyword(keyword, page, size));
         }
-        else if (category.toLowerCase().equalsIgnoreCase("job"))
+        else if (category.toLowerCase().equalsIgnoreCase("jobs"))
         {
             result.set("result", SearchDAO.getPagedJobByKeyword(keyword, page, size));
         }

File: web/app/dao/SearchDAO.java
Patch:
@@ -440,7 +440,6 @@ public ObjectNode doInTransaction(TransactionStatus status)
 			{
 				List<Map<String, Object>> rows = null;
 				String query = SEARCH_DATASET_BY_COMMENTS_WITH_PAGINATION.replace("$keyword", keyword);
-				Logger.error(query);
 				rows = jdbcTemplate.queryForList(query, (page-1)*size, size);
 
 				for (Map row : rows) {

File: web/app/controllers/api/v1/Search.java
Patch:
@@ -88,7 +88,7 @@ public static Result searchByKeyword(String keyword)
         result.put("status", "ok");
         if (StringUtils.isBlank(category))
         {
-            category = "dataset";
+            category = "datasets";
         }
         if (StringUtils.isBlank(source))
         {
@@ -98,11 +98,11 @@ public static Result searchByKeyword(String keyword)
         {
             result.set("result", SearchDAO.getPagedMetricByKeyword(keyword, page, size));
         }
-        else if (category.toLowerCase().equalsIgnoreCase("flow"))
+        else if (category.toLowerCase().equalsIgnoreCase("flows"))
         {
             result.set("result", SearchDAO.getPagedFlowByKeyword(keyword, page, size));
         }
-        else if (category.toLowerCase().equalsIgnoreCase("job"))
+        else if (category.toLowerCase().equalsIgnoreCase("jobs"))
         {
             result.set("result", SearchDAO.getPagedJobByKeyword(keyword, page, size));
         }

File: web/app/dao/SearchDAO.java
Patch:
@@ -440,7 +440,6 @@ public ObjectNode doInTransaction(TransactionStatus status)
 			{
 				List<Map<String, Object>> rows = null;
 				String query = SEARCH_DATASET_BY_COMMENTS_WITH_PAGINATION.replace("$keyword", keyword);
-				Logger.error(query);
 				rows = jdbcTemplate.queryForList(query, (page-1)*size, size);
 
 				for (Map row : rows) {

File: backend-service/app/controllers/Application.java
Patch:
@@ -15,14 +15,13 @@
 
 import play.mvc.Controller;
 import play.mvc.Result;
-import views.html.index;
 
 
 public class Application extends Controller {
 
 
   public static Result index() {
-    return ok(index.render("Your new application is ready."));
+    return ok("TEST");
   }
 
 }

File: backend-service/app/models/daos/EtlJobPropertyDao.java
Patch:
@@ -14,7 +14,7 @@
 package models.daos;
 
 import com.fasterxml.jackson.databind.JsonNode;
-import models.EtlJobName;
+import metadata.etl.models.EtlJobName;
 import org.springframework.jdbc.support.KeyHolder;
 import play.Logger;
 import utils.JdbcUtil;

File: metadata-etl/src/main/java/metadata/etl/dataset/hdfs/HdfsMetadataEtl.java
Patch:
@@ -96,7 +96,7 @@ public void extract()
       String wherehowsExecFolder = remoteJarFile.split("/")[0];
       String cluster = prop.getProperty(Constant.HDFS_CLUSTER_KEY);
       String whiteList = prop.getProperty(Constant.HDFS_WHITE_LIST_KEY);
-      String numOfThread = prop.getProperty(Constant.HDFS_NUM_OF_THREAD_KEY);
+      String numOfThread = prop.getProperty(Constant.HDFS_NUM_OF_THREAD_KEY, String.valueOf(1));
       String execCmd =
         "cd " + wherehowsExecFolder + ";"
           + "export HADOOP_CLIENT_OPTS=\"-Xmx2048m $HADOOP_CLIENT_OPTS\";"

File: metadata-etl/src/main/java/metadata/etl/lineage/AzLineageMetadataEtl.java
Patch:
@@ -53,7 +53,7 @@ public AzLineageMetadataEtl(int azkabanInstanceId) {
    */
   public AzLineageMetadataEtl(int appId, long whExecId, Properties properties) {
     super(appId, null, whExecId, properties);
-    this.timeFrame = Integer.valueOf(this.prop.getProperty(Constant.AZ_LINEAGE_ETL_LOOKBACK_MINS_KEY));
+    this.timeFrame = Integer.valueOf(this.prop.getProperty(Constant.AZ_LINEAGE_ETL_LOOKBACK_MINS_KEY, "90")); //default lookback 90 mins
     if (this.prop.contains(Constant.AZ_LINEAGE_ETL_END_TIMESTAMP_KEY))
       this.endTimeStamp = Long.valueOf(this.prop.getProperty(Constant.AZ_LINEAGE_ETL_END_TIMESTAMP_KEY));
     try {

File: metadata-etl/src/main/java/metadata/etl/models/EtlJobFactory.java
Patch:
@@ -11,7 +11,7 @@
  * distributed under the License is distributed on an "AS IS" BASIS,
  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  */
-package actors;
+package metadata.etl.models;
 
 import java.util.Properties;
 import metadata.etl.EtlJob;
@@ -24,7 +24,7 @@
 import metadata.etl.ldap.LdapEtl;
 import metadata.etl.scheduler.azkaban.AzkabanExecEtl;
 import metadata.etl.scheduler.oozie.OozieExecEtl;
-import models.EtlJobName;
+import metadata.etl.models.EtlJobName;
 
 
 /**

File: metadata-etl/src/main/java/metadata/etl/models/EtlJobName.java
Patch:
@@ -11,7 +11,7 @@
  * distributed under the License is distributed on an "AS IS" BASIS,
  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  */
-package models;
+package metadata.etl.models;
 
 /**
  * Created by zechen on 9/4/15.

File: metadata-etl/src/main/java/metadata/etl/models/EtlJobStatus.java
Patch:
@@ -11,7 +11,7 @@
  * distributed under the License is distributed on an "AS IS" BASIS,
  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  */
-package models;
+package metadata.etl.models;
 
 /**
  * Created by zechen on 9/24/15.

File: metadata-etl/src/main/java/metadata/etl/models/EtlType.java
Patch:
@@ -11,7 +11,7 @@
  * distributed under the License is distributed on an "AS IS" BASIS,
  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  */
-package models;
+package metadata.etl.models;
 
 /**
  * Created by zsun on 9/30/15.

File: metadata-etl/src/main/java/metadata/etl/models/RefIdType.java
Patch:
@@ -11,7 +11,7 @@
  * distributed under the License is distributed on an "AS IS" BASIS,
  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  */
-package models;
+package metadata.etl.models;
 
 /**
  * Created by zechen on 10/21/15.

File: metadata-etl/src/main/java/metadata/etl/EtlJob.java
Patch:
@@ -165,7 +165,8 @@ public void setup()
 
   public void close()
     throws Exception {
-
+    interpreter.cleanup();
+    interpreter.close();
   }
 
   public void run()

File: metadata-etl/src/main/java/metadata/etl/lineage/AzLineageExtractorMaster.java
Patch:
@@ -81,8 +81,10 @@ public void run(int timeFrame, long endTimeStamp)
     logger.info("Total number of azkaban jobs : {}", jobExecList.size());
 
     ActorSystem actorSystem = ActorSystem.create("LineageExtractor");
+    int numOfActor = Integer.valueOf(prop.getProperty(Constant.LINEAGE_ACTOR_NUM, "50"));
     ActorRef lineageExtractorActor = actorSystem
-      .actorOf(Props.create(AzLineageExtractorActor.class).withRouter(new SmallestMailboxRouter(50)), "lineageExtractorActor");
+      .actorOf(Props.create(AzLineageExtractorActor.class)
+          .withRouter(new SmallestMailboxRouter(numOfActor)), "lineageExtractorActor");
 
     // initialize
     //AzkabanServiceCommunicator asc = new AzkabanServiceCommunicator(prop);

File: wherehows-common/src/main/java/wherehows/common/Constant.java
Patch:
@@ -47,6 +47,8 @@ public class Constant {
   public static final String AZ_LINEAGE_ETL_LOOKBACK_MINS_KEY = "az.lineage_etl.lookback_period.in.minutes";
   /** The property_name field in wh_etl_job_property table. In lineage ETl, Akka actor time out time */
   public static final String LINEAGE_ACTOR_TIMEOUT_KEY = "az.lineage.actor.timeout";
+
+  public static final String LINEAGE_ACTOR_NUM = "az.lineage.actor.num";
   /** The property_name field in wh_etl_job_property table. Optional property used for debug. Default end timestamp is now */
   public static final String AZ_LINEAGE_ETL_END_TIMESTAMP_KEY = "az.lineage_etl.end_timestamp";
   /** The property_name field in wh_etl_job_property table. Azkaban Server URL (optional way to get azkaban execution log) */

File: metadata-etl/src/main/java/metadata/etl/EtlJob.java
Patch:
@@ -165,7 +165,8 @@ public void setup()
 
   public void close()
     throws Exception {
-
+    interpreter.cleanup();
+    interpreter.close();
   }
 
   public void run()

File: metadata-etl/src/main/java/metadata/etl/lineage/AzLineageExtractorMaster.java
Patch:
@@ -81,8 +81,10 @@ public void run(int timeFrame, long endTimeStamp)
     logger.info("Total number of azkaban jobs : {}", jobExecList.size());
 
     ActorSystem actorSystem = ActorSystem.create("LineageExtractor");
+    int numOfActor = Integer.valueOf(prop.getProperty(Constant.LINEAGE_ACTOR_NUM, "50"));
     ActorRef lineageExtractorActor = actorSystem
-      .actorOf(Props.create(AzLineageExtractorActor.class).withRouter(new SmallestMailboxRouter(50)), "lineageExtractorActor");
+      .actorOf(Props.create(AzLineageExtractorActor.class)
+          .withRouter(new SmallestMailboxRouter(numOfActor)), "lineageExtractorActor");
 
     // initialize
     //AzkabanServiceCommunicator asc = new AzkabanServiceCommunicator(prop);

File: wherehows-common/src/main/java/wherehows/common/Constant.java
Patch:
@@ -47,6 +47,8 @@ public class Constant {
   public static final String AZ_LINEAGE_ETL_LOOKBACK_MINS_KEY = "az.lineage_etl.lookback_period.in.minutes";
   /** The property_name field in wh_etl_job_property table. In lineage ETl, Akka actor time out time */
   public static final String LINEAGE_ACTOR_TIMEOUT_KEY = "az.lineage.actor.timeout";
+
+  public static final String LINEAGE_ACTOR_NUM = "az.lineage.actor.num";
   /** The property_name field in wh_etl_job_property table. Optional property used for debug. Default end timestamp is now */
   public static final String AZ_LINEAGE_ETL_END_TIMESTAMP_KEY = "az.lineage_etl.end_timestamp";
   /** The property_name field in wh_etl_job_property table. Azkaban Server URL (optional way to get azkaban execution log) */

File: backend-service/app/models/daos/EtlJobPropertyDao.java
Patch:
@@ -40,8 +40,9 @@ public class EtlJobPropertyDao {
       + "VALUES(:propertyName, :propertyValue, :isEncrypted, :groupName)";
 
   public static final String UPDATE_JOB_PROPERTY =
-    " UPDATE wh_etl_job_property " + " SET property_value = :propertyValue, is_encrypted = :isEncrypted "
-      + " WHERE wh_etl_job_name = :etlJobName and ref_id = :refId and property_name = :propertyName ";
+      "INSERT INTO wh_etl_job_property(wh_etl_job_name, ref_id, ref_id_type, property_name, property_value, is_encrypted)"
+          + "VALUES(:etlJobName, :refId, :refIdType, :propertyName, :propertyValue, :isEncrypted)"
+        + "ON DUPLICATE KEY UPDATE property_value = :propertyValue, is_encrypted = :isEncrypted";
 
   public static final String GET_JOB_PROPERTIES =
     "SELECT * FROM wh_etl_job_property WHERE wh_etl_job_name = :etlJobName and ref_id = :refId";

File: hadoop-dataset-extractor-standalone/src/main/java/wherehows/AvroFileAnalyzer.java
Patch:
@@ -54,12 +54,11 @@ public DatasetJsonRecord getSchema(Path targetFilePath)
     String schemaString = reader.getSchema().toString();
     String storage = STORAGE_TYPE;
     String abstractPath = targetFilePath.toUri().getPath();
-    String dataSource = checkDataSource(abstractPath);
 
     FileStatus fstat = fs.getFileStatus(targetFilePath);
     DatasetJsonRecord datasetJsonRecord =
       new DatasetJsonRecord(schemaString, abstractPath, fstat.getModificationTime(), fstat.getOwner(), fstat.getGroup(),
-        fstat.getPermission().toString(), codec, storage, dataSource);
+        fstat.getPermission().toString(), codec, storage, "");
     reader.close();
     sin.close();
     return datasetJsonRecord;

File: hadoop-dataset-extractor-standalone/src/main/java/wherehows/OrcFileAnalyzer.java
Patch:
@@ -45,12 +45,11 @@ public DatasetJsonRecord getSchema(Path targetFilePath)
     String schemaString = orcReader.getObjectInspector().getTypeName();
     String storage = STORAGE_TYPE;
     String abstractPath = targetFilePath.toUri().getPath();
-    String dataSource = checkDataSource(abstractPath);
 
     FileStatus fstat = fs.getFileStatus(targetFilePath);
     DatasetJsonRecord datasetJsonRecord =
       new DatasetJsonRecord(schemaString, abstractPath, fstat.getModificationTime(), fstat.getOwner(), fstat.getGroup(),
-        fstat.getPermission().toString(), codec, storage, dataSource);
+        fstat.getPermission().toString(), codec, storage, "");
 
     return datasetJsonRecord;
   }

File: web/app/utils/Lineage.java
Patch:
@@ -112,7 +112,7 @@ public static String convertToURN(LineagePathInfo pathInfo)
         }
         else
         {
-            return pathInfo.storageType + ":///" + filePath;
+            return pathInfo.storageType.toLowerCase() + ":///" + filePath;
         }
     }
 }

File: web/app/utils/Lineage.java
Patch:
@@ -112,7 +112,7 @@ public static String convertToURN(LineagePathInfo pathInfo)
         }
         else
         {
-            return pathInfo.storageType + ":///" + filePath;
+            return pathInfo.storageType.toLowerCase() + ":///" + filePath;
         }
     }
 }

File: backend-service/app/actors/EtlJobActor.java
Patch:
@@ -54,13 +54,14 @@ public void onReceive(Object message)
         if (msg.getEtlJobName().affectFlow()) {
           ActorRegistry.treeBuilderActor.tell("flow", getSelf());
         }
-      } catch (Exception e) {
+      } catch (Throwable e) { // catch all throwable at the highest level.
+        Logger.error("ETL job {} got a problem", msg.toDebugString());
         StringWriter sw = new StringWriter();
         PrintWriter pw = new PrintWriter(sw);
         e.printStackTrace(pw);
         Logger.error(sw.toString());
         e.printStackTrace();
-        EtlJobDao.endRun(msg.getWhEtlExecId(), EtlJobStatus.ERROR, e.getMessage());
+        EtlJobDao.endRun(msg.getWhEtlExecId(), EtlJobStatus.ERROR, sw.toString().substring(0,500));
       }
     }
   }

File: metadata-etl/src/main/java/metadata/etl/lineage/AzJsonAnalyzer.java
Patch:
@@ -15,7 +15,6 @@
 
 import com.jayway.jsonpath.Configuration;
 import com.jayway.jsonpath.JsonPath;
-import java.util.Arrays;
 import org.codehaus.jettison.json.JSONException;
 import wherehows.common.DatasetPath;
 import wherehows.common.schemas.AzkabanJobExecRecord;

File: metadata-etl/src/main/java/metadata/etl/lineage/AzLineageExtractorMaster.java
Patch:
@@ -69,6 +69,7 @@ public void run(int timeFrame)
    * Entry point.
    * All recent finished azkaban jobs' lineage. Will write to database stagging table
    * @param timeFrame in minutes
+   * @param endTimeStamp in millisecond
    * @throws Exception
    */
   public void run(int timeFrame, long endTimeStamp)

File: metadata-etl/src/main/java/metadata/etl/lineage/HadoopNameNodeExtractor.java
Patch:
@@ -69,7 +69,7 @@ public HadoopNameNodeExtractor(Properties prop)
       String gssFileName = possition + "/gss-jaas.conf";
       File gssFile = new File(gssFileName);
       if (gssFile.exists()) {
-        logger.info("find gss-jaas.conf file in : {}", gssFile.getAbsolutePath());
+        logger.debug("find gss-jaas.conf file in : {}", gssFile.getAbsolutePath());
         System.setProperty("java.security.auth.login.config", gssFile.getAbsolutePath());
         break;
       } else {
@@ -80,7 +80,7 @@ public HadoopNameNodeExtractor(Properties prop)
       String krb5FileName = possition + "/krb5.conf";
       File krb5File = new File(krb5FileName);
       if (krb5File.exists()) {
-        logger.info("find krb5.conf file in : {}", krb5File.getAbsolutePath());
+        logger.debug("find krb5.conf file in : {}", krb5File.getAbsolutePath());
         System.setProperty("java.security.krb5.conf", krb5File.getAbsolutePath());
         break;
       } else {

File: backend-service/app/actors/EtlJobActor.java
Patch:
@@ -54,13 +54,14 @@ public void onReceive(Object message)
         if (msg.getEtlJobName().affectFlow()) {
           ActorRegistry.treeBuilderActor.tell("flow", getSelf());
         }
-      } catch (Exception e) {
+      } catch (Throwable e) { // catch all throwable at the highest level.
+        Logger.error("ETL job {} got a problem", msg.toDebugString());
         StringWriter sw = new StringWriter();
         PrintWriter pw = new PrintWriter(sw);
         e.printStackTrace(pw);
         Logger.error(sw.toString());
         e.printStackTrace();
-        EtlJobDao.endRun(msg.getWhEtlExecId(), EtlJobStatus.ERROR, e.getMessage());
+        EtlJobDao.endRun(msg.getWhEtlExecId(), EtlJobStatus.ERROR, sw.toString().substring(0,500));
       }
     }
   }

File: metadata-etl/src/main/java/metadata/etl/lineage/AzJsonAnalyzer.java
Patch:
@@ -15,7 +15,6 @@
 
 import com.jayway.jsonpath.Configuration;
 import com.jayway.jsonpath.JsonPath;
-import java.util.Arrays;
 import org.codehaus.jettison.json.JSONException;
 import wherehows.common.DatasetPath;
 import wherehows.common.schemas.AzkabanJobExecRecord;

File: metadata-etl/src/main/java/metadata/etl/lineage/AzLineageExtractorMaster.java
Patch:
@@ -69,6 +69,7 @@ public void run(int timeFrame)
    * Entry point.
    * All recent finished azkaban jobs' lineage. Will write to database stagging table
    * @param timeFrame in minutes
+   * @param endTimeStamp in millisecond
    * @throws Exception
    */
   public void run(int timeFrame, long endTimeStamp)

File: metadata-etl/src/main/java/metadata/etl/lineage/HadoopNameNodeExtractor.java
Patch:
@@ -69,7 +69,7 @@ public HadoopNameNodeExtractor(Properties prop)
       String gssFileName = possition + "/gss-jaas.conf";
       File gssFile = new File(gssFileName);
       if (gssFile.exists()) {
-        logger.info("find gss-jaas.conf file in : {}", gssFile.getAbsolutePath());
+        logger.debug("find gss-jaas.conf file in : {}", gssFile.getAbsolutePath());
         System.setProperty("java.security.auth.login.config", gssFile.getAbsolutePath());
         break;
       } else {
@@ -80,7 +80,7 @@ public HadoopNameNodeExtractor(Properties prop)
       String krb5FileName = possition + "/krb5.conf";
       File krb5File = new File(krb5FileName);
       if (krb5File.exists()) {
-        logger.info("find krb5.conf file in : {}", krb5File.getAbsolutePath());
+        logger.debug("find krb5.conf file in : {}", krb5File.getAbsolutePath());
         System.setProperty("java.security.krb5.conf", krb5File.getAbsolutePath());
         break;
       } else {

File: metadata-etl/src/main/java/metadata/etl/dataset/hive/HiveMetadataEtl.java
Patch:
@@ -26,7 +26,7 @@ public void extract()
     logger.info("In Hive metadata ETL, launch extract jython scripts");
     InputStream inputStream = classLoader.getResourceAsStream("jython/HiveExtract.py");
     //logger.info("before call scripts " + interpreter.getSystemState().argv);
-    super.interpreter.execfile(inputStream);
+    interpreter.execfile(inputStream);
     inputStream.close();
   }
 

File: wherehows-common/src/main/java/wherehows/common/schemas/GitCommitRecord.java
Patch:
@@ -41,7 +41,7 @@ public GitCommitRecord(GitUtil.CommitMetadata commitMetadata, String gitRepoUrn)
     this.gitRepoUrn = gitRepoUrn;
     this.commitId = commitMetadata.getCommitId();
     this.filePath = commitMetadata.getFilePath();
-    this.fileName = FilenameUtils.getName(this.filePath);
+    this.fileName = commitMetadata.getFileName();
     this.commitTime = commitMetadata.getCommitTime().getTime() / 1000;
     this.committerName = commitMetadata.getCommitter();
     this.committerEmail = commitMetadata.getCommitterEmail();

File: wherehows-common/src/main/java/wherehows/common/schemas/GitCommitRecord.java
Patch:
@@ -41,7 +41,7 @@ public GitCommitRecord(GitUtil.CommitMetadata commitMetadata, String gitRepoUrn)
     this.gitRepoUrn = gitRepoUrn;
     this.commitId = commitMetadata.getCommitId();
     this.filePath = commitMetadata.getFilePath();
-    this.fileName = FilenameUtils.getName(this.filePath);
+    this.fileName = commitMetadata.getFileName();
     this.commitTime = commitMetadata.getCommitTime().getTime() / 1000;
     this.committerName = commitMetadata.getCommitter();
     this.committerEmail = commitMetadata.getCommitterEmail();

File: backend-service/app/models/daos/LineageDao.java
Patch:
@@ -34,10 +34,8 @@
  */
 public class LineageDao {
   public static final String FIND_JOBS_BY_DATASET =
-    " select distinct ca.short_connection_string, f.flow_group, f.flow_name, jedl.job_name "
+    " select distinct ca.short_connection_string, jedl.job_name, jedl.flow_path "
     + " from job_execution_data_lineage jedl "
-    + " join flow_execution fe on jedl.app_id = fe.app_id and jedl.flow_exec_id = fe.flow_exec_id "
-    + " join flow f on fe.app_id = f.app_id and fe.flow_id = f.flow_id "
     + " join cfg_application ca on ca.app_id = jedl.app_id "
     + " join cfg_database cd on cd.db_id = jedl.db_id "
     + " where source_target_type = :source_target_type "

File: metadata-etl/src/test/java/metadata/etl/lineage/AzLineageExtractorTest.java
Patch:
@@ -44,7 +44,7 @@ public void setUp()
     String wherehowsPassWord = prop.getProperty(Constant.WH_DB_PASSWORD_KEY);
     connUrl = wherehowsUrl + "?" + "user=" + wherehowsUserName + "&password=" + wherehowsPassWord;
     this.conn = DriverManager.getConnection(connUrl);
-    AzLogParser.initialize(conn, Integer.valueOf(prop.getProperty(Constant.AZ_DEFAULT_HADOOP_DATABASE_ID_KEY)));
+    AzLogParser.initialize(conn);
     PathAnalyzer.initialize(conn);
   }
 

File: backend-service/app/models/daos/LineageDao.java
Patch:
@@ -34,10 +34,8 @@
  */
 public class LineageDao {
   public static final String FIND_JOBS_BY_DATASET =
-    " select distinct ca.short_connection_string, f.flow_group, f.flow_name, jedl.job_name "
+    " select distinct ca.short_connection_string, jedl.job_name, jedl.flow_path "
     + " from job_execution_data_lineage jedl "
-    + " join flow_execution fe on jedl.app_id = fe.app_id and jedl.flow_exec_id = fe.flow_exec_id "
-    + " join flow f on fe.app_id = f.app_id and fe.flow_id = f.flow_id "
     + " join cfg_application ca on ca.app_id = jedl.app_id "
     + " join cfg_database cd on cd.db_id = jedl.db_id "
     + " where source_target_type = :source_target_type "

File: metadata-etl/src/test/java/metadata/etl/lineage/AzLineageExtractorTest.java
Patch:
@@ -44,7 +44,7 @@ public void setUp()
     String wherehowsPassWord = prop.getProperty(Constant.WH_DB_PASSWORD_KEY);
     connUrl = wherehowsUrl + "?" + "user=" + wherehowsUserName + "&password=" + wherehowsPassWord;
     this.conn = DriverManager.getConnection(connUrl);
-    AzLogParser.initialize(conn, Integer.valueOf(prop.getProperty(Constant.AZ_DEFAULT_HADOOP_DATABASE_ID_KEY)));
+    AzLogParser.initialize(conn);
     PathAnalyzer.initialize(conn);
   }
 

File: metadata-etl/src/test/java/metadata/etl/lineage/AzLogParserTest.java
Patch:
@@ -45,7 +45,7 @@ public void setUp()
     String wherehowsPassWord = prop.getProperty(Constant.WH_DB_PASSWORD_KEY);
     Connection conn =
       DriverManager.getConnection(wherehowsHost + "?" + "user=" + wherehowsUserName + "&password=" + wherehowsPassWord);
-    AzLogParser.initialize(conn, -1);
+    AzLogParser.initialize(conn);
   }
 
   @Test(groups = {"needConfig"})
@@ -81,7 +81,7 @@ public void getLineageFromLogTest() {
     String logSample = "asfdasdfsadf Moving from staged path[asdf] to final resting place[/tm/b/c] sdaf dsfasdfasdf";
     AzkabanJobExecRecord sampleExecution = new AzkabanJobExecRecord(-1, "someJobName", (long) 0, 0, 0, "S", "path");
     sampleExecution.setJobExecId((long) 11111);
-    List<LineageRecord> result = AzLogParser.getLineageFromLog(logSample, sampleExecution);
+    List<LineageRecord> result = AzLogParser.getLineageFromLog(logSample, sampleExecution, -1);
     System.out.println(result.get(0).toDatabaseValue());
 
     Assert.assertEquals(result.get(0).toDatabaseValue(),
@@ -105,7 +105,7 @@ public void getLineageFromLogTest2() {
       + "17-11-2015 01:32:27 PST endorsements_push-lva-endorsements-member-restrictions INFO - INFO tcp://lva1-voldemort-read-only-2-vip.prod.linkedin.com:10103 : Invoking fetch for Node lva1-app0610.prod.linkedin.com [id 0] for webhdfs://lva1-warnn01.grid.linkedin.com:50070/jobs/endorse/endorsements/master/tmp/endorsements-member-restrictions.store/lva1-voldemort-read-only-2-vip.prod.linkedin.com/node-0\n"
       + "17-11-2015 01:32:27 PST endorsements_push-lva-endorsements-member-restrictions INFO - INFO tcp://lva1-voldemort-rea";
     AzkabanJobExecRecord sampleExecution = new AzkabanJobExecRecord(-1, "someJobName", (long) 0, 0, 0, "S", "path");
-    List<LineageRecord> result = AzLogParser.getLineageFromLog(logSample, sampleExecution);
+    List<LineageRecord> result = AzLogParser.getLineageFromLog(logSample, sampleExecution, -1);
     System.out.println(result.get(0).toDatabaseValue());
     Assert.assertEquals(result.get(0).getFullObjectName(),
       "tcp://lva1-voldemort-read-only-2-vip.prod.linkedin.com:10103/endorsements-member-restrictions");

File: backend-service/app/actors/EtlJobFactory.java
Patch:
@@ -17,6 +17,7 @@
 import metadata.etl.EtlJob;
 import metadata.etl.dataset.hdfs.HdfsMetadataEtl;
 import metadata.etl.dataset.teradata.TeradataMetadataEtl;
+import metadata.etl.git.GitMetadataEtl;
 import metadata.etl.lineage.AzLineageMetadataEtl;
 import metadata.etl.ownership.DatasetOwnerEtl;
 import metadata.etl.ldap.LdapEtl;
@@ -46,6 +47,8 @@ public static EtlJob getEtlJob(EtlJobName etlJobName, Integer refId, Long whExec
         return new DatasetOwnerEtl(refId, whExecId, properties);
       case LDAP_USER_ETL:
         return new LdapEtl(refId, whExecId, properties);
+      case GIT_MEDATA_ETL:
+        return new GitMetadataEtl(refId, whExecId, properties);
       default:
         throw new UnsupportedOperationException("Unsupported job type: " + etlJobName);
     }

File: backend-service/app/models/EtlJobName.java
Patch:
@@ -24,6 +24,7 @@ public enum EtlJobName {
   AZKABAN_LINEAGE_METADATA_ETL(EtlType.LINEAGE, RefIdType.APP),
   HADOOP_DATASET_OWNER_ETL(EtlType.OWNER, RefIdType.DB),
   LDAP_USER_ETL(EtlType.LDAP, RefIdType.APP),
+  GIT_MEDATA_ETL(EtlType.VCS, RefIdType.APP),
   ;
 
   EtlType etlType;

File: backend-service/app/models/EtlType.java
Patch:
@@ -22,5 +22,6 @@ public enum EtlType {
   DATASET,
   OWNER,
   LDAP,
+  VCS,
   ALL
 }

File: metadata-etl/src/main/java/metadata/etl/dataset/hdfs/HdfsMetadataEtl.java
Patch:
@@ -82,6 +82,7 @@ public void extract()
 
       InputStream localJarStream = classLoader.getResourceAsStream("jar/schemaFetch.jar");
       channelSftp.put(localJarStream, remoteJarFile, ChannelSftp.OVERWRITE);
+      localJarStream.close();
 
       String localSchemaFile = prop.getProperty(Constant.HDFS_SCHEMA_LOCAL_PATH_KEY);
       new File(localSchemaFile).getParentFile().mkdirs();

File: metadata-etl/src/main/java/metadata/etl/lineage/AzLineageExtractor.java
Patch:
@@ -61,8 +61,8 @@ public static List<LineageRecord> extractLineage(AzExecMessage message)
     // normalize and combine the path
     LineageCombiner lineageCombiner = new LineageCombiner(message.connection);
     lineageCombiner.addAll(oneAzkabanJobLineage);
-
-    List<LineageRecord> lineageFromLog = AzLogParser.getLineageFromLog(log, message.azkabanJobExecution);
+    Integer defaultDatabaseId = Integer.valueOf(message.prop.getProperty(Constant.AZ_DEFAULT_HADOOP_DATABASE_ID_KEY));
+    List<LineageRecord> lineageFromLog = AzLogParser.getLineageFromLog(log, message.azkabanJobExecution, defaultDatabaseId);
     lineageCombiner.addAll(lineageFromLog);
 
     return lineageCombiner.getCombinedLineage();

File: metadata-etl/src/main/java/metadata/etl/lineage/AzLineageExtractorMaster.java
Patch:
@@ -90,7 +90,7 @@ public void run(int timeFrame)
     Connection conn = DriverManager.getConnection(connUrl);
     DatabaseWriter databaseWriter = new DatabaseWriter(connUrl, "stg_job_execution_data_lineage");
 
-    AzLogParser.initialize(conn, Integer.valueOf(prop.getProperty(Constant.AZ_DEFAULT_HADOOP_DATABASE_ID_KEY)));
+    AzLogParser.initialize(conn);
     PathAnalyzer.initialize(conn);
     int timeout = 30; // default 30 minutes for one job
     if (prop.containsKey(Constant.LINEAGE_ACTOR_TIMEOUT_KEY))

