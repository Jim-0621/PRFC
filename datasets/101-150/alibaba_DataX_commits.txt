File: common/src/main/java/com/alibaba/datax/common/util/Configuration.java
Patch:
@@ -1047,7 +1047,7 @@ private void checkPath(final String path) {
 					"系统编程错误, 该异常代表系统编程错误, 请联系DataX开发团队!.");
 		}
 
-		for (final String each : StringUtils.split(".")) {
+		for (final String each : StringUtils.split(path, ".")) {
 			if (StringUtils.isBlank(each)) {
 				throw new IllegalArgumentException(String.format(
 						"系统编程错误, 路径[%s]不合法, 路径层次之间不能出现空白字符 .", path));

File: common/src/main/java/com/alibaba/datax/common/util/Configuration.java
Patch:
@@ -1047,7 +1047,7 @@ private void checkPath(final String path) {
 					"系统编程错误, 该异常代表系统编程错误, 请联系DataX开发团队!.");
 		}
 
-		for (final String each : StringUtils.split(".")) {
+		for (final String each : StringUtils.split(path, ".")) {
 			if (StringUtils.isBlank(each)) {
 				throw new IllegalArgumentException(String.format(
 						"系统编程错误, 路径[%s]不合法, 路径层次之间不能出现空白字符 .", path));

File: plugin-unstructured-storage-util/src/main/java/com/alibaba/datax/plugin/unstructuredstorage/writer/UnstructuredStorageWriterUtil.java
Patch:
@@ -283,7 +283,8 @@ public static UnstructuredWriter produceUnstructuredWriter(String fileFormat, Co
             String lineSeparator = config.getString(Key.LINE_DELIMITER, IOUtils.LINE_SEPARATOR);
             List<String> headers = config.getList(Key.HEADER, String.class);
             Preconditions.checkArgument(CollectionUtils.isNotEmpty(headers), "column names are empty");
-            unstructuredWriter = new SqlWriter(writer, quoteChar, tableName, lineSeparator, headers);
+            String nullFormat = config.getString(Key.NULL_FORMAT, Constant.DEFAULT_NULL_FORMAT);
+            unstructuredWriter = new SqlWriter(writer, quoteChar, tableName, lineSeparator, headers, nullFormat);
         }
 
         return unstructuredWriter;

File: hologresjdbcwriter/src/main/java/com/alibaba/datax/plugin/writer/hologresjdbcwriter/BaseWriter.java
Patch:
@@ -167,7 +167,7 @@ public void prepare(Configuration originalConfig) {
 				if (null != renderedPreSqls && !renderedPreSqls.isEmpty()) {
 					// 说明有 preSql 配置，则此处删除掉
 					originalConfig.remove(Key.PRE_SQL);
-					String tempJdbcUrl = jdbcUrl.replace("postgresql", "hologres");
+					String tempJdbcUrl = jdbcUrl.replace("jdbc:postgresql://", "jdbc:hologres://");
 					try (Connection conn = DriverManager.getConnection(
 							tempJdbcUrl, username, password)) {
 						LOG.info("Begin to execute preSqls:[{}]. context info:{}.",
@@ -207,7 +207,7 @@ public void post(Configuration originalConfig) {
 				if (null != renderedPostSqls && !renderedPostSqls.isEmpty()) {
 					// 说明有 postSql 配置，则此处删除掉
 					originalConfig.remove(Key.POST_SQL);
-					String tempJdbcUrl = jdbcUrl.replace("postgresql", "hologres");
+					String tempJdbcUrl = jdbcUrl.replace("jdbc:postgresql://", "jdbc:hologres://");
 					try (Connection conn = DriverManager.getConnection(
 							tempJdbcUrl, username, password)) {
 						LOG.info(

File: hologresjdbcwriter/src/main/java/com/alibaba/datax/plugin/writer/hologresjdbcwriter/BaseWriter.java
Patch:
@@ -167,7 +167,7 @@ public void prepare(Configuration originalConfig) {
 				if (null != renderedPreSqls && !renderedPreSqls.isEmpty()) {
 					// 说明有 preSql 配置，则此处删除掉
 					originalConfig.remove(Key.PRE_SQL);
-					String tempJdbcUrl = jdbcUrl.replace("postgresql", "hologres");
+					String tempJdbcUrl = jdbcUrl.replace("jdbc:postgresql://", "jdbc:hologres://");
 					try (Connection conn = DriverManager.getConnection(
 							tempJdbcUrl, username, password)) {
 						LOG.info("Begin to execute preSqls:[{}]. context info:{}.",
@@ -207,7 +207,7 @@ public void post(Configuration originalConfig) {
 				if (null != renderedPostSqls && !renderedPostSqls.isEmpty()) {
 					// 说明有 postSql 配置，则此处删除掉
 					originalConfig.remove(Key.POST_SQL);
-					String tempJdbcUrl = jdbcUrl.replace("postgresql", "hologres");
+					String tempJdbcUrl = jdbcUrl.replace("jdbc:postgresql://", "jdbc:hologres://");
 					try (Connection conn = DriverManager.getConnection(
 							tempJdbcUrl, username, password)) {
 						LOG.info(

File: plugin-unstructured-storage-util/src/main/java/com/alibaba/datax/plugin/unstructuredstorage/writer/SqlWriter.java
Patch:
@@ -58,7 +58,7 @@ private void buildInsertPrefix(List<String> columnNames) {
         int capacity = 18 + tableName.length() + sb.length();
         this.insertPrefix = new StringBuilder(capacity);
         this.insertPrefix
-                .append("INSERT INTO ").append("`").append(tableName).append("`").append(" (").append(sb).append(")").append(" VALUES(");
+                .append("INSERT INTO ").append(quoteChar).append(tableName).append(quoteChar).append(" (").append(sb).append(")").append(" VALUES(");
     }
 
     public void appendCommit() throws IOException {

File: ftpreader/src/main/java/com/alibaba/datax/plugin/reader/ftpreader/SftpHelper.java
Patch:
@@ -64,6 +64,8 @@ public void loginFtpServer(String host, String username, String password, int po
 					String message = String.format("请确认连接ftp服务器端口是否正确，错误的端口: [%s] ", port);
 					LOG.error(message);
 					throw DataXException.asDataXException(FtpReaderErrorCode.FAIL_LOGIN, message, e);
+				}else{
+					throw DataXException.asDataXException(FtpReaderErrorCode.COMMAND_FTP_IO_EXCEPTION, "", e);
 				}
 			}else {
 				if("Auth fail".equals(e.getMessage())){

File: databendwriter/src/main/java/com/alibaba/datax/plugin/writer/databendwriter/util/DatabendWriterUtil.java
Patch:
@@ -32,7 +32,7 @@ public static void dealWriteMode(Configuration originalConfig) {
                 LOG.error("Replace mode must has onConflictColumn conf");
                 return;
             }
-            // for databend if you want to use replace mode, the writeMode should be:  "writeMode": "replace (userid)"
+            // for databend if you want to use replace mode, the writeMode should be:  "writeMode": "replace"
             writeDataSqlTemplate.append("REPLACE INTO %s (")
                     .append(StringUtils.join(columns, ",")).append(") ").append(onConFlictDoString(onConflictColumns))
                     .append(" VALUES");

File: plugin-rdbms-util/src/main/java/com/alibaba/datax/plugin/rdbms/writer/Key.java
Patch:
@@ -11,6 +11,8 @@ public final class Key {
 
     public final static String COLUMN = "column";
 
+    public final static String ONCONFLICT_COLUMN = "onConflictColumn";
+
     //可选值为：insert,replace，默认为 insert （mysql 支持，oracle 没用 replace 机制，只能 insert,oracle 可以不暴露这个参数）
     public final static String WRITE_MODE = "writeMode";
 

File: common/src/main/java/com/alibaba/datax/common/statistics/VMInfo.java
Patch:
@@ -77,8 +77,8 @@ private VMInfo() {
         garbageCollectorMXBeanList = java.lang.management.ManagementFactory.getGarbageCollectorMXBeans();
         memoryPoolMXBeanList = java.lang.management.ManagementFactory.getMemoryPoolMXBeans();
 
-        osInfo = runtimeMXBean.getVmVendor() + " " + runtimeMXBean.getSpecVersion() + " " + runtimeMXBean.getVmVersion();
-        jvmInfo = osMXBean.getName() + " " + osMXBean.getArch() + " " + osMXBean.getVersion();
+        jvmInfo = runtimeMXBean.getVmVendor() + " " + runtimeMXBean.getSpecVersion() + " " + runtimeMXBean.getVmVersion();
+        osInfo = osMXBean.getName() + " " + osMXBean.getArch() + " " + osMXBean.getVersion();
         totalProcessorCount = osMXBean.getAvailableProcessors();
 
         //构建startPhyOSStatus

File: starrockswriter/src/main/java/com/starrocks/connector/datax/plugin/writer/starrockswriter/StarRocksWriter.java
Patch:
@@ -78,7 +78,7 @@ public void post() {
             List<String> renderedPostSqls = StarRocksWriterUtil.renderPreOrPostSqls(options.getPostSqlList(), options.getTable());
             if (null != renderedPostSqls && !renderedPostSqls.isEmpty()) {
                 Connection conn = DBUtil.getConnection(DataBaseType.MySql, jdbcUrl, username, password);
-                LOG.info("Begin to execute preSqls:[{}]. context info:{}.", String.join(";", renderedPostSqls), jdbcUrl);
+                LOG.info("Begin to execute postSqls:[{}]. context info:{}.", String.join(";", renderedPostSqls), jdbcUrl);
                 StarRocksWriterUtil.executeSqls(conn, renderedPostSqls);
                 DBUtil.closeDBResources(null, null, conn);
             }

File: ftpreader/src/main/java/com/alibaba/datax/plugin/reader/ftpreader/SftpHelper.java
Patch:
@@ -64,6 +64,8 @@ public void loginFtpServer(String host, String username, String password, int po
 					String message = String.format("请确认连接ftp服务器端口是否正确，错误的端口: [%s] ", port);
 					LOG.error(message);
 					throw DataXException.asDataXException(FtpReaderErrorCode.FAIL_LOGIN, message, e);
+				}else{
+					throw DataXException.asDataXException(FtpReaderErrorCode.COMMAND_FTP_IO_EXCEPTION, "", e);
 				}
 			}else {
 				if("Auth fail".equals(e.getMessage())){

File: plugin-rdbms-util/src/main/java/com/alibaba/datax/plugin/rdbms/util/DataBaseType.java
Patch:
@@ -134,6 +134,8 @@ public String appendJDBCSuffixForWriter(String jdbc) {
                     result = jdbc + "?" + suffix;
                 }
                 break;
+            case Sybase:
+                break;
             default:
                 throw DataXException.asDataXException(DBUtilErrorCode.UNSUPPORTED_TYPE, "unsupported database type.");
         }

File: common/src/main/java/com/alibaba/datax/common/statistics/VMInfo.java
Patch:
@@ -77,8 +77,8 @@ private VMInfo() {
         garbageCollectorMXBeanList = java.lang.management.ManagementFactory.getGarbageCollectorMXBeans();
         memoryPoolMXBeanList = java.lang.management.ManagementFactory.getMemoryPoolMXBeans();
 
-        osInfo = runtimeMXBean.getVmVendor() + " " + runtimeMXBean.getSpecVersion() + " " + runtimeMXBean.getVmVersion();
-        jvmInfo = osMXBean.getName() + " " + osMXBean.getArch() + " " + osMXBean.getVersion();
+        jvmInfo = runtimeMXBean.getVmVendor() + " " + runtimeMXBean.getSpecVersion() + " " + runtimeMXBean.getVmVersion();
+        osInfo = osMXBean.getName() + " " + osMXBean.getArch() + " " + osMXBean.getVersion();
         totalProcessorCount = osMXBean.getAvailableProcessors();
 
         //构建startPhyOSStatus

File: core/src/main/java/com/alibaba/datax/core/util/container/CoreConstant.java
Patch:
@@ -105,7 +105,7 @@ public class CoreConstant {
 
     public static final String DATAX_JOB_POSTHANDLER_PLUGINNAME = "job.postHandler.pluginName";
     // ----------------------------- 局部使用的变量
-    public static final String JOB_WRITER = "reader";
+    public static final String JOB_WRITER = "writer";
 
 	public static final String JOB_READER = "reader";
 

File: oceanbasev10writer/src/main/java/com/alibaba/datax/plugin/writer/oceanbasev10writer/util/DbUtils.java
Patch:
@@ -62,7 +62,7 @@ public static String fetchSingleValueWithRetry(Configuration config, String quer
                 ++retry;
                 LOG.warn("fetch value with {} error {}", query, e);
             } finally {
-                DBUtil.closeDBResources(result, stmt, null);
+                DBUtil.closeDBResources(result, stmt, conn);
             }
         } while (need_retry);
 

File: oceanbasev10writer/src/main/java/com/alibaba/datax/plugin/writer/oceanbasev10writer/util/DbUtils.java
Patch:
@@ -62,7 +62,7 @@ public static String fetchSingleValueWithRetry(Configuration config, String quer
                 ++retry;
                 LOG.warn("fetch value with {} error {}", query, e);
             } finally {
-                DBUtil.closeDBResources(result, stmt, null);
+                DBUtil.closeDBResources(result, stmt, conn);
             }
         } while (need_retry);
 

File: core/src/main/java/com/alibaba/datax/core/util/container/CoreConstant.java
Patch:
@@ -105,7 +105,7 @@ public class CoreConstant {
 
     public static final String DATAX_JOB_POSTHANDLER_PLUGINNAME = "job.postHandler.pluginName";
     // ----------------------------- 局部使用的变量
-    public static final String JOB_WRITER = "reader";
+    public static final String JOB_WRITER = "writer";
 
 	public static final String JOB_READER = "reader";
 

File: txtfilereader/src/main/java/com/alibaba/datax/plugin/reader/txtfilereader/TxtFileReader.java
Patch:
@@ -182,6 +182,7 @@ private void validateParameter() {
 								delimiterInStr));
 			}
 
+			UnstructuredStorageReaderUtil.validateCsvReaderConfig(this.originConfig);
 		}
 
 		@Override

File: txtfilereader/src/main/java/com/alibaba/datax/plugin/reader/txtfilereader/TxtFileReader.java
Patch:
@@ -182,6 +182,7 @@ private void validateParameter() {
 								delimiterInStr));
 			}
 
+			UnstructuredStorageReaderUtil.validateCsvReaderConfig(this.originConfig);
 		}
 
 		@Override

File: core/src/main/java/com/alibaba/datax/core/container/util/JobAssignUtil.java
Patch:
@@ -114,7 +114,7 @@ private static LinkedHashMap<String, List<Integer>> parseAndGetResourceMarkAndTa
      * 需要实现的效果通过例子来说是：
      * <pre>
      * a 库上有表：0, 1, 2
-     * a 库上有表：3, 4
+     * b 库上有表：3, 4
      * c 库上有表：5, 6, 7
      *
      * 如果有 4个 taskGroup

File: plugin-rdbms-util/src/main/java/com/alibaba/datax/plugin/rdbms/reader/util/OriginalConfPretreatmentUtil.java
Patch:
@@ -261,7 +261,7 @@ private static boolean recognizeTableOrQuerySqlMode(
 
         // 混合配制 table 和 querySql
         if (!ListUtil.checkIfValueSame(tableModeFlags)
-                || !ListUtil.checkIfValueSame(tableModeFlags)) {
+                || !ListUtil.checkIfValueSame(querySqlModeFlags)) {
             throw DataXException.asDataXException(DBUtilErrorCode.TABLE_QUERYSQL_MIXED,
                     "您配置凌乱了. 不能同时既配置table又配置querySql. 请检查您的配置并作出修改.");
         }

File: plugin-rdbms-util/src/main/java/com/alibaba/datax/plugin/rdbms/util/DataBaseType.java
Patch:
@@ -68,6 +68,8 @@ public String appendJDBCSuffixForReader(String jdbc) {
                 break;
             case Oscar:
                 break;
+            case StarRocks:
+                break;
             default:
                 throw DataXException.asDataXException(DBUtilErrorCode.UNSUPPORTED_TYPE, "unsupported database type.");
         }

File: plugin-rdbms-util/src/main/java/com/alibaba/datax/plugin/rdbms/util/DataBaseType.java
Patch:
@@ -68,6 +68,8 @@ public String appendJDBCSuffixForReader(String jdbc) {
                 break;
             case Oscar:
                 break;
+            case StarRocks:
+                break;
             default:
                 throw DataXException.asDataXException(DBUtilErrorCode.UNSUPPORTED_TYPE, "unsupported database type.");
         }

File: doriswriter/src/main/java/com/alibaba/datax/plugin/writer/doriswriter/DorisStreamLoadObserver.java
Patch:
@@ -125,7 +125,7 @@ private void checkStreamLoadState(String host, String label) throws IOException
     private byte[] addRows(List<byte[]> rows, int totalBytes) {
         if (Keys.StreamLoadFormat.CSV.equals(options.getStreamLoadFormat())) {
             Map<String, Object> props = (options.getLoadProps() == null ? new HashMap<> () : options.getLoadProps());
-            byte[] lineDelimiter = DelimiterParser.parse((String)props.get("row_delimiter"), "\n").getBytes(StandardCharsets.UTF_8);
+            byte[] lineDelimiter = DelimiterParser.parse((String)props.get("line_delimiter"), "\n").getBytes(StandardCharsets.UTF_8);
             ByteBuffer bos = ByteBuffer.allocate(totalBytes + rows.size() * lineDelimiter.length);
             for (byte[] row : rows) {
                 bos.put(row);

File: oceanbasev10reader/src/main/java/com/alibaba/datax/plugin/reader/oceanbasev10reader/util/ObVersion.java
Patch:
@@ -16,6 +16,8 @@ public class ObVersion implements Comparable<ObVersion> {
     private int patchNumber;
 
     public static final ObVersion V2276 = valueOf("2.2.76");
+    public static final ObVersion V4000 = valueOf("4.0.0.0");
+
     private static final ObVersion DEFAULT_VERSION =
         valueOf(System.getProperty("defaultObVersion","3.2.3.0"));
 

File: oceanbasev10reader/src/main/java/com/alibaba/datax/plugin/reader/oceanbasev10reader/util/PartitionSplitUtil.java
Patch:
@@ -157,7 +157,8 @@ public static PartInfo getObMySQLPartInfoBySQL(Configuration config, String tabl
 
             conn = DBUtil.getConnection(DataBaseType.OceanBase, jdbcUrl, username, password);
             ObVersion obVersion = ObReaderUtils.getObVersion(conn);
-            if (obVersion.compareTo(ObVersion.V2276) >= 0) {
+            if (obVersion.compareTo(ObVersion.V2276) >= 0 &&
+                obVersion.compareTo(ObVersion.V4000) < 0) {
                 allTable = "__all_table_v2";
             }
 

File: plugin-rdbms-util/src/main/java/com/alibaba/datax/plugin/rdbms/util/DBUtil.java
Patch:
@@ -380,6 +380,9 @@ private static synchronized Connection connect(DataBaseType dataBaseType,
             // unit ms
             prop.put("oracle.jdbc.ReadTimeout", socketTimeout);
         }
+        if (dataBaseType == DataBaseType.OceanBase) {
+            url = url.replace("jdbc:mysql:", "jdbc:oceanbase:");
+        }
 
         return connect(dataBaseType, url, prop);
     }

File: adswriter/src/main/java/com/alibaba/datax/plugin/writer/adswriter/odps/DataType.java
Patch:
@@ -70,7 +70,7 @@ public static byte convertToDataType(String type) throws IllegalArgumentExceptio
         } else if ("datetime".equals(type)) {
             return DATETIME;
         } else {
-            throw new IllegalArgumentException("unkown type: " + type);
+            throw new IllegalArgumentException("unknown type: " + type);
         }
     }
 

File: clickhousewriter/src/main/java/com/alibaba/datax/plugin/writer/clickhousewriter/ClickhouseWriter.java
Patch:
@@ -68,7 +68,7 @@ public void init() {
 
 			this.commonRdbmsWriterSlave = new CommonRdbmsWriter.Task(DATABASE_TYPE) {
 				@Override
-				protected PreparedStatement fillPreparedStatementColumnType(PreparedStatement preparedStatement, int columnIndex, int columnSqltype, Column column) throws SQLException {
+				protected PreparedStatement fillPreparedStatementColumnType(PreparedStatement preparedStatement, int columnIndex, int columnSqltype, String typeName, Column column) throws SQLException {
 					try {
 						if (column.getRawData() == null) {
 							preparedStatement.setNull(columnIndex + 1, columnSqltype);

File: core/src/main/java/com/alibaba/datax/core/transport/transformer/FilterTransformer.java
Patch:
@@ -61,7 +61,7 @@ public Record evaluate(Record record, Object... paras) {
             } else if (code.equalsIgnoreCase("<=")) {
                 return doLess(record, value, column, true);
             } else {
-                throw new RuntimeException("dx_filter can't suport code:" + code);
+                throw new RuntimeException("dx_filter can't support code:" + code);
             }
         } catch (Exception e) {
             throw DataXException.asDataXException(TransformerErrorCode.TRANSFORMER_RUN_EXCEPTION, e.getMessage(), e);

File: core/src/main/java/com/alibaba/datax/core/transport/transformer/TransformerRegistry.java
Patch:
@@ -36,6 +36,7 @@ public class TransformerRegistry {
         registTransformer(new ReplaceTransformer());
         registTransformer(new FilterTransformer());
         registTransformer(new GroovyTransformer());
+        registTransformer(new DigestTransformer());
     }
 
     public static void loadTransformerFromLocalStorage() {

File: hbase20xsqlreader/src/main/java/com/alibaba/datax/plugin/reader/hbase20xsqlreader/HBase20SQLReaderHelper.java
Patch:
@@ -175,7 +175,7 @@ public List<Configuration> doSplit(int adviceNumber) {
         if (querySql == null || querySql.isEmpty()) {
             // 如果splitPoints为空，则根据splitKey自动切分，不过这种切分方式无法保证数据均分，且只支持整形和字符型列
             if (splitPoints == null || splitPoints.isEmpty()) {
-                LOG.info("Split accoring min and max value of splitColumn...");
+                LOG.info("Split according min and max value of splitColumn...");
                 Pair<Object, Object> minMaxPK = getPkRange(configuration);
                 if (null == minMaxPK) {
                     throw DataXException.asDataXException(HBase20xSQLReaderErrorCode.ILLEGAL_SPLIT_PK,
@@ -208,7 +208,7 @@ public List<Configuration> doSplit(int adviceNumber) {
                 }
 
             } else {
-                LOG.info("Split accoring splitPoints...");
+                LOG.info("Split according splitPoints...");
                 // 根据指定splitPoints进行切分
                 rangeList = buildSplitRange();
             }

File: oceanbasev10reader/src/test/java/com/alibaba/datax/plugin/reader/oceanbasev10reader/util/ObReaderUtilsTest.java
Patch:
@@ -18,5 +18,7 @@ public void compareObVersionTest() {
         assert ObReaderUtils.compareObVersion("2.2.70", "2.2.50") == 1;
         assert ObReaderUtils.compareObVersion("2.2.70", "3.1.2") == -1;
         assert ObReaderUtils.compareObVersion("3.1.2", "3.1.2") == 0;
+        assert ObReaderUtils.compareObVersion("3.2.3.0", "3.2.3.0") == 0;
+        assert ObReaderUtils.compareObVersion("3.2.3.0-CE", "3.2.3.0") == 0;
     }
 }

File: oceanbasev10writer/src/main/java/com/alibaba/datax/plugin/writer/oceanbasev10writer/OceanBaseV10Writer.java
Patch:
@@ -61,15 +61,15 @@ public void init() {
 			checkCompatibleMode(originalConfig);
 			//将config中的column和table中的关键字进行转义
 			List<String> columns = originalConfig.getList(Key.COLUMN, String.class);
-			ObWriterUtils.escapeDatabaseKeywords(columns);
+			ObWriterUtils.escapeDatabaseKeyword(columns);
 			originalConfig.set(Key.COLUMN, columns);
 
 			List<JSONObject> conns = originalConfig.getList(Constant.CONN_MARK, JSONObject.class);
 			for (int i = 0; i < conns.size(); i++) {
 				JSONObject conn = conns.get(i);
 				Configuration connConfig = Configuration.from(conn.toString());
 				List<String> tables = connConfig.getList(Key.TABLE, String.class);
-				ObWriterUtils.escapeDatabaseKeywords(tables);
+				ObWriterUtils.escapeDatabaseKeyword(tables);
 				originalConfig.set(String.format("%s[%d].%s", Constant.CONN_MARK, i, Key.TABLE), tables);
 			}
 			this.commonJob = new CommonRdbmsWriter.Job(DATABASE_TYPE);

File: otsreader/src/main/java/com/alibaba/datax/plugin/reader/otsreader/utils/Common.java
Patch:
@@ -119,7 +119,7 @@ public static Record parseRowToLine(Row row, List<OTSColumn> columns, Record lin
                     case BOOLEAN: line.addColumn(new BoolColumn(v.asBoolean()));  break;
                     case BINARY:  line.addColumn(new BytesColumn(v.asBinary()));  break;
                     default:
-                        throw new IllegalArgumentException("Unsuporrt tranform the type: " + col.getValue().getType() + ".");
+                        throw new IllegalArgumentException("Unsupported transform the type: " + col.getValue().getType() + ".");
                     }
                 }
             }

File: plugin-rdbms-util/src/main/java/com/alibaba/datax/plugin/rdbms/util/DataBaseType.java
Patch:
@@ -22,7 +22,8 @@ public enum DataBaseType {
     ClickHouse("clickhouse", "ru.yandex.clickhouse.ClickHouseDriver"),
     KingbaseES("kingbasees", "com.kingbase8.Driver"),
     Oscar("oscar", "com.oscar.Driver"),
-    OceanBase("oceanbase", "com.alipay.oceanbase.jdbc.Driver");
+    OceanBase("oceanbase", "com.alipay.oceanbase.jdbc.Driver"),
+    StarRocks("starrocks", "com.mysql.jdbc.Driver");
 
 
     private String typeName;

File: rdbmswriter/src/main/java/com/alibaba/datax/plugin/reader/rdbmswriter/SubCommonRdbmsWriter.java
Patch:
@@ -29,7 +29,7 @@ public Task(DataBaseType dataBaseType) {
         @Override
         protected PreparedStatement fillPreparedStatementColumnType(
                 PreparedStatement preparedStatement, int columnIndex,
-                int columnSqltype, Column column) throws SQLException {
+                int columnSqltype, String typeName, Column column) throws SQLException {
             java.util.Date utilDate;
             try {
                 switch (columnSqltype) {

File: oceanbasev10reader/src/main/java/com/alibaba/datax/plugin/reader/oceanbasev10reader/util/ObVersion.java
Patch:
@@ -16,6 +16,8 @@ public class ObVersion implements Comparable<ObVersion> {
     private int patchNumber;
 
     public static final ObVersion V2276 = valueOf("2.2.76");
+    public static final ObVersion V4000 = valueOf("4.0.0.0");
+
     private static final ObVersion DEFAULT_VERSION =
         valueOf(System.getProperty("defaultObVersion","3.2.3.0"));
 

File: oceanbasev10reader/src/main/java/com/alibaba/datax/plugin/reader/oceanbasev10reader/util/PartitionSplitUtil.java
Patch:
@@ -157,7 +157,8 @@ public static PartInfo getObMySQLPartInfoBySQL(Configuration config, String tabl
 
             conn = DBUtil.getConnection(DataBaseType.OceanBase, jdbcUrl, username, password);
             ObVersion obVersion = ObReaderUtils.getObVersion(conn);
-            if (obVersion.compareTo(ObVersion.V2276) >= 0) {
+            if (obVersion.compareTo(ObVersion.V2276) >= 0 &&
+                obVersion.compareTo(ObVersion.V4000) < 0) {
                 allTable = "__all_table_v2";
             }
 

File: plugin-rdbms-util/src/main/java/com/alibaba/datax/plugin/rdbms/util/DBUtil.java
Patch:
@@ -380,6 +380,9 @@ private static synchronized Connection connect(DataBaseType dataBaseType,
             // unit ms
             prop.put("oracle.jdbc.ReadTimeout", socketTimeout);
         }
+        if (dataBaseType == DataBaseType.OceanBase) {
+            url = url.replace("jdbc:mysql:", "jdbc:oceanbase:");
+        }
 
         return connect(dataBaseType, url, prop);
     }

File: plugin-rdbms-util/src/main/java/com/alibaba/datax/plugin/rdbms/writer/CommonRdbmsWriter.java
Patch:
@@ -519,7 +519,7 @@ protected PreparedStatement fillPreparedStatementColumnType(PreparedStatement pr
                     break;
 
                 case Types.BOOLEAN:
-                    preparedStatement.setString(columnIndex + 1, column.asString());
+                    preparedStatement.setBoolean(columnIndex + 1, column.asBoolean());
                     break;
 
                 // warn: bit(1) -> Types.BIT 可使用setBoolean

File: plugin-rdbms-util/src/main/java/com/alibaba/datax/plugin/rdbms/writer/CommonRdbmsWriter.java
Patch:
@@ -519,7 +519,7 @@ protected PreparedStatement fillPreparedStatementColumnType(PreparedStatement pr
                     break;
 
                 case Types.BOOLEAN:
-                    preparedStatement.setString(columnIndex + 1, column.asString());
+                    preparedStatement.setBoolean(columnIndex + 1, column.asBoolean());
                     break;
 
                 // warn: bit(1) -> Types.BIT 可使用setBoolean

File: rdbmswriter/src/main/java/com/alibaba/datax/plugin/reader/rdbmswriter/SubCommonRdbmsWriter.java
Patch:
@@ -29,7 +29,7 @@ public Task(DataBaseType dataBaseType) {
         @Override
         protected PreparedStatement fillPreparedStatementColumnType(
                 PreparedStatement preparedStatement, int columnIndex,
-                int columnSqltype, Column column) throws SQLException {
+                int columnSqltype, String typeName, Column column) throws SQLException {
             java.util.Date utilDate;
             try {
                 switch (columnSqltype) {

File: rdbmswriter/src/main/java/com/alibaba/datax/plugin/reader/rdbmswriter/SubCommonRdbmsWriter.java
Patch:
@@ -29,7 +29,7 @@ public Task(DataBaseType dataBaseType) {
         @Override
         protected PreparedStatement fillPreparedStatementColumnType(
                 PreparedStatement preparedStatement, int columnIndex,
-                int columnSqltype, Column column) throws SQLException {
+                int columnSqltype, String typeName, Column column) throws SQLException {
             java.util.Date utilDate;
             try {
                 switch (columnSqltype) {

File: tdenginewriter/src/main/java/com/alibaba/datax/plugin/writer/tdenginewriter/DefaultDataHandler.java
Patch:
@@ -276,7 +276,7 @@ private String buildColumnValue(ColumnMeta colMeta, Record record) throws Except
             case DOUBLE:
             case INT:
             case LONG:
-                column.asString();
+                return column.asString();
             default:
                 throw new Exception("invalid column type: " + type);
         }

File: tdenginewriter/src/main/java/com/alibaba/datax/plugin/writer/tdenginewriter/DefaultDataHandler.java
Patch:
@@ -276,7 +276,7 @@ private String buildColumnValue(ColumnMeta colMeta, Record record) throws Except
             case DOUBLE:
             case INT:
             case LONG:
-                column.asString();
+                return column.asString();
             default:
                 throw new Exception("invalid column type: " + type);
         }

File: clickhousewriter/src/main/java/com/alibaba/datax/plugin/writer/clickhousewriter/ClickhouseWriter.java
Patch:
@@ -68,7 +68,7 @@ public void init() {
 
 			this.commonRdbmsWriterSlave = new CommonRdbmsWriter.Task(DATABASE_TYPE) {
 				@Override
-				protected PreparedStatement fillPreparedStatementColumnType(PreparedStatement preparedStatement, int columnIndex, int columnSqltype, Column column) throws SQLException {
+				protected PreparedStatement fillPreparedStatementColumnType(PreparedStatement preparedStatement, int columnIndex, int columnSqltype, String typeName, Column column) throws SQLException {
 					try {
 						if (column.getRawData() == null) {
 							preparedStatement.setNull(columnIndex + 1, columnSqltype);

File: clickhousewriter/src/main/java/com/alibaba/datax/plugin/writer/clickhousewriter/ClickhouseWriter.java
Patch:
@@ -68,7 +68,7 @@ public void init() {
 
 			this.commonRdbmsWriterSlave = new CommonRdbmsWriter.Task(DATABASE_TYPE) {
 				@Override
-				protected PreparedStatement fillPreparedStatementColumnType(PreparedStatement preparedStatement, int columnIndex, int columnSqltype, Column column) throws SQLException {
+				protected PreparedStatement fillPreparedStatementColumnType(PreparedStatement preparedStatement, int columnIndex, int columnSqltype, String typeName, Column column) throws SQLException {
 					try {
 						if (column.getRawData() == null) {
 							preparedStatement.setNull(columnIndex + 1, columnSqltype);

File: adswriter/src/main/java/com/alibaba/datax/plugin/writer/adswriter/odps/DataType.java
Patch:
@@ -70,7 +70,7 @@ public static byte convertToDataType(String type) throws IllegalArgumentExceptio
         } else if ("datetime".equals(type)) {
             return DATETIME;
         } else {
-            throw new IllegalArgumentException("unkown type: " + type);
+            throw new IllegalArgumentException("unknown type: " + type);
         }
     }
 

File: core/src/main/java/com/alibaba/datax/core/transport/transformer/FilterTransformer.java
Patch:
@@ -61,7 +61,7 @@ public Record evaluate(Record record, Object... paras) {
             } else if (code.equalsIgnoreCase("<=")) {
                 return doLess(record, value, column, true);
             } else {
-                throw new RuntimeException("dx_filter can't suport code:" + code);
+                throw new RuntimeException("dx_filter can't support code:" + code);
             }
         } catch (Exception e) {
             throw DataXException.asDataXException(TransformerErrorCode.TRANSFORMER_RUN_EXCEPTION, e.getMessage(), e);

File: hbase20xsqlreader/src/main/java/com/alibaba/datax/plugin/reader/hbase20xsqlreader/HBase20SQLReaderHelper.java
Patch:
@@ -175,7 +175,7 @@ public List<Configuration> doSplit(int adviceNumber) {
         if (querySql == null || querySql.isEmpty()) {
             // 如果splitPoints为空，则根据splitKey自动切分，不过这种切分方式无法保证数据均分，且只支持整形和字符型列
             if (splitPoints == null || splitPoints.isEmpty()) {
-                LOG.info("Split accoring min and max value of splitColumn...");
+                LOG.info("Split according min and max value of splitColumn...");
                 Pair<Object, Object> minMaxPK = getPkRange(configuration);
                 if (null == minMaxPK) {
                     throw DataXException.asDataXException(HBase20xSQLReaderErrorCode.ILLEGAL_SPLIT_PK,
@@ -208,7 +208,7 @@ public List<Configuration> doSplit(int adviceNumber) {
                 }
 
             } else {
-                LOG.info("Split accoring splitPoints...");
+                LOG.info("Split according splitPoints...");
                 // 根据指定splitPoints进行切分
                 rangeList = buildSplitRange();
             }

File: otsreader/src/main/java/com/alibaba/datax/plugin/reader/otsreader/utils/Common.java
Patch:
@@ -119,7 +119,7 @@ public static Record parseRowToLine(Row row, List<OTSColumn> columns, Record lin
                     case BOOLEAN: line.addColumn(new BoolColumn(v.asBoolean()));  break;
                     case BINARY:  line.addColumn(new BytesColumn(v.asBinary()));  break;
                     default:
-                        throw new IllegalArgumentException("Unsuporrt tranform the type: " + col.getValue().getType() + ".");
+                        throw new IllegalArgumentException("Unsupported transform the type: " + col.getValue().getType() + ".");
                     }
                 }
             }

File: doriswriter/src/main/java/com/alibaba/datax/plugin/writer/doriswriter/Keys.java
Patch:
@@ -31,7 +31,7 @@ public enum StreamLoadFormat {
     private static final String JDBC_URL = "connection[0].jdbcUrl";
     private static final String LABEL_PREFIX = "labelPrefix";
     private static final String MAX_BATCH_ROWS = "maxBatchRows";
-    private static final String MAX_BATCH_SIZE = "maxBatchSize";
+    private static final String MAX_BATCH_SIZE = "batchSize";
     private static final String FLUSH_INTERVAL = "flushInterval";
     private static final String LOAD_URL = "loadUrl";
     private static final String FLUSH_QUEUE_LENGTH = "flushQueueLength";

File: doriswriter/src/main/java/com/alibaba/datax/plugin/writer/doriswriter/DorisBaseSerializer.java
Patch:
@@ -0,0 +1,2 @@
+package com.alibaba.datax.plugin.writer.doriswriter;public class DorisBaseSerializer {
+}

File: doriswriter/src/main/java/com/alibaba/datax/plugin/writer/doriswriter/DorisCsvSerializer.java
Patch:
@@ -0,0 +1,2 @@
+package com.alibaba.datax.plugin.writer.doriswriter;public class DorisCsvSerializer {
+}

File: doriswriter/src/main/java/com/alibaba/datax/plugin/writer/doriswriter/DorisDelimiterParser.java
Patch:
@@ -0,0 +1,2 @@
+package com.alibaba.datax.plugin.writer.doriswriter;public class DorisDelimiterParser {
+}

File: doriswriter/src/main/java/com/alibaba/datax/plugin/writer/doriswriter/DorisJsonSerializer.java
Patch:
@@ -0,0 +1,2 @@
+package com.alibaba.datax.plugin.writer.doriswriter;public class DorisJsonSerializer {
+}

File: doriswriter/src/main/java/com/alibaba/datax/plugin/writer/doriswriter/DorisSerializer.java
Patch:
@@ -0,0 +1,2 @@
+package com.alibaba.datax.plugin.writer.doriswriter;public class DorisSerializer {
+}

File: doriswriter/src/main/java/com/alibaba/datax/plugin/writer/doriswriter/DorisSerializerFactory.java
Patch:
@@ -0,0 +1,2 @@
+package com.alibaba.datax.plugin.writer.doriswriter;public class DorisSerializerFactory {
+}

File: doriswriter/src/main/java/com/alibaba/datax/plugin/writer/doriswriter/DorisStreamLoadExcetion.java
Patch:
@@ -0,0 +1,2 @@
+package com.alibaba.datax.plugin.writer.doriswriter;public class DorisStreamLoadExcetion {
+}

File: doriswriter/src/main/java/com/alibaba/datax/plugin/writer/doriswriter/Key.java
Patch:
@@ -29,7 +29,7 @@
 import java.util.Map;
 
 public class Key implements Serializable {
-    public static final String FE_LOAD_URL = "feLoadUrl";
+    public static final String FE_LOAD_URL = "loadUrl";
     public static final String BE_LOAD_URL = "beLoadUrl";
     public static final String JDBC_URL = "connection[0].jdbcUrl";
 

File: doriswriter/src/main/java/com/alibaba/datax/plugin/writer/doriswriter/DorisWriter.java
Patch:
@@ -95,11 +95,11 @@ public void startWrite(RecordReceiver recordReceiver) {
                 // trigger buffer
                 if (batchCount >= this.keys.getBatchRows() || batchByteSize >= this.keys.getBatchByteSize()) {
                     // generate doris stream load label
-                    flush(flushBatch);
+                    flush (flushBatch);
                     // clear buffer
                     batchCount = 0;
                     batchByteSize = 0L;
-                    flushBatch = new DorisFlushBatch(lineDelimiter, this.keys.getFormat());
+                    flushBatch = new DorisFlushBatch (lineDelimiter, this.keys.getFormat());
                 }
             } // end of while
 

File: doriswriter/src/test/java/com/alibaba/datax/plugin/writer/doriswriter/TestDorisWriterLoad.java
Patch:
@@ -62,7 +62,7 @@ public static void main(String[] args) throws IOException {
         Key key = new Key(configuration);
 
         DorisWriterEmitter emitter = new DorisWriterEmitter(key);
-        DorisFlushBatch flushBatch = new DorisFlushBatch("\n");
+        DorisFlushBatch flushBatch = new DorisFlushBatch("\n","csv");
         flushBatch.setLabel("test4");
         Map<String, String> row1 = Maps.newHashMap();
         row1.put("k1", "2021-02-02");
@@ -83,6 +83,6 @@ public static void main(String[] args) throws IOException {
         for (int i = 0; i < 50000; ++i) {
             flushBatch.putData(rowStr2);
         }
-        emitter.doStreamLoad(flushBatch);
+        emitter.emit (flushBatch);
     }
 }

File: doriswriter/src/main/java/com/alibaba/datax/plugin/writer/doriswriter/DorisCsvCodec.java
Patch:
@@ -26,8 +26,8 @@ public class DorisCsvCodec extends DorisCodec {
 
     private final String columnSeparator;
 
-    public DorisCsvCodec(final List<String> fieldNames, String columnSeparator, String timeZone) {
-        super(fieldNames, timeZone);
+    public DorisCsvCodec(final List<String> fieldNames, String columnSeparator) {
+        super(fieldNames);
         this.columnSeparator = EscapeHandler.escapeString(columnSeparator);
     }
 

File: doriswriter/src/main/java/com/alibaba/datax/plugin/writer/doriswriter/DorisJsonCodec.java
Patch:
@@ -19,6 +19,7 @@
 
 import com.alibaba.datax.common.element.Record;
 import com.alibaba.fastjson.JSON;
+import com.alibaba.fastjson.serializer.SerializerFeature;
 
 import java.util.HashMap;
 import java.util.List;
@@ -28,8 +29,8 @@
 public class DorisJsonCodec extends DorisCodec {
     private Map<String, Object> rowMap;
 
-    public DorisJsonCodec(final List<String> fieldNames, final String timeZone) {
-        super(fieldNames, timeZone);
+    public DorisJsonCodec(final List<String> fieldNames) {
+        super(fieldNames);
         this.rowMap = new HashMap<>(this.fieldNames.size());
     }
 

File: doriswriter/src/main/java/com/alibaba/datax/plugin/writer/doriswriter/DorisWriter.java
Patch:
@@ -57,9 +57,9 @@ public Task() {
         public void init() {
             this.keys = new Key(super.getPluginJobConf());
             if (Key.DEFAULT_FORMAT_CSV.equalsIgnoreCase(this.keys.getFormat())) {
-                this.rowCodec = new DorisCsvCodec(this.keys.getColumns(), this.keys.getColumnSeparator(), this.keys.getTimeZone());
+                this.rowCodec = new DorisCsvCodec(this.keys.getColumns(), this.keys.getColumnSeparator());
             } else {
-                this.rowCodec = new DorisJsonCodec(this.keys.getColumns(), this.keys.getTimeZone());
+                this.rowCodec = new DorisJsonCodec(this.keys.getColumns());
             }
             this.labelPrefix = this.keys.getLabelPrefix() + UUID.randomUUID();
             this.dorisWriterEmitter = new DorisWriterEmitter(keys);

File: doriswriter/src/main/java/com/alibaba/datax/plugin/writer/doriswriter/Key.java
Patch:
@@ -49,15 +49,15 @@ public class Key implements Serializable {
     public static final String LOAD_PROPS_COLUMN_SEPARATOR = "column_separator";
 
     public static final String MAX_BATCH_ROWS = "maxBatchRows";
-    public static final String MAX_BATCH_BYTE_SIZE = "maxBatchByteSize";
+    public static final String BATCH_BYTE_SIZE = "batchByteSize";
     public static final String MAX_RETRIES = "maxRetries";
     public static final String LABEL_PREFIX = "labelPrefix";
     public static final String FORMAT = "format";
     public static final String CONNECT_TIMEOUT = "connectTimeout";
     private final Configuration options;
 
     private static final long DEFAULT_MAX_BATCH_ROWS = 50_0000;
-    private static final long DEFAULT_MAX_BATCH_BYTE_SIZE = 90 * 1024 * 1024; // 90MB
+    private static final long DEFAULT_BATCH_BYTE_SIZE = 90 * 1024 * 1024;
     private static final int DEFAULT_MAX_RETRIES = 0;
 
     private static final String DEFAULT_LABEL_PREFIX = "datax_doris_writer_";
@@ -130,7 +130,7 @@ public long getBatchRows() {
     }
 
     public long getBatchByteSize() {
-        return this.options.getLong(MAX_BATCH_BYTE_SIZE, DEFAULT_MAX_BATCH_BYTE_SIZE);
+        return this.options.getLong(BATCH_BYTE_SIZE, DEFAULT_BATCH_BYTE_SIZE);
     }
 
     public int getMaxRetries() {

File: doriswriter/src/main/java/com/alibaba/datax/plugin/writer/doriswriter/DorisJsonCodec.java
Patch:
@@ -45,6 +45,6 @@ public String serialize(final Record row) {
             rowMap.put(fieldName, this.convertColumn(row.getColumn(idx)));
             ++idx;
         }
-        return JSON.toJSONString(rowMap);
+        return JSON.toJSONString(rowMap, SerializerFeature.WriteMapNullValue);
     }
 }

File: adswriter/src/main/java/com/alibaba/datax/plugin/writer/adswriter/odps/DataType.java
Patch:
@@ -70,7 +70,7 @@ public static byte convertToDataType(String type) throws IllegalArgumentExceptio
         } else if ("datetime".equals(type)) {
             return DATETIME;
         } else {
-            throw new IllegalArgumentException("unkown type: " + type);
+            throw new IllegalArgumentException("unknown type: " + type);
         }
     }
 

File: core/src/main/java/com/alibaba/datax/core/transport/transformer/FilterTransformer.java
Patch:
@@ -61,7 +61,7 @@ public Record evaluate(Record record, Object... paras) {
             } else if (code.equalsIgnoreCase("<=")) {
                 return doLess(record, value, column, true);
             } else {
-                throw new RuntimeException("dx_filter can't suport code:" + code);
+                throw new RuntimeException("dx_filter can't support code:" + code);
             }
         } catch (Exception e) {
             throw DataXException.asDataXException(TransformerErrorCode.TRANSFORMER_RUN_EXCEPTION, e.getMessage(), e);

File: hbase20xsqlreader/src/main/java/com/alibaba/datax/plugin/reader/hbase20xsqlreader/HBase20SQLReaderHelper.java
Patch:
@@ -175,7 +175,7 @@ public List<Configuration> doSplit(int adviceNumber) {
         if (querySql == null || querySql.isEmpty()) {
             // 如果splitPoints为空，则根据splitKey自动切分，不过这种切分方式无法保证数据均分，且只支持整形和字符型列
             if (splitPoints == null || splitPoints.isEmpty()) {
-                LOG.info("Split accoring min and max value of splitColumn...");
+                LOG.info("Split according min and max value of splitColumn...");
                 Pair<Object, Object> minMaxPK = getPkRange(configuration);
                 if (null == minMaxPK) {
                     throw DataXException.asDataXException(HBase20xSQLReaderErrorCode.ILLEGAL_SPLIT_PK,
@@ -208,7 +208,7 @@ public List<Configuration> doSplit(int adviceNumber) {
                 }
 
             } else {
-                LOG.info("Split accoring splitPoints...");
+                LOG.info("Split according splitPoints...");
                 // 根据指定splitPoints进行切分
                 rangeList = buildSplitRange();
             }

File: otsreader/src/main/java/com/alibaba/datax/plugin/reader/otsreader/utils/Common.java
Patch:
@@ -119,7 +119,7 @@ public static Record parseRowToLine(Row row, List<OTSColumn> columns, Record lin
                     case BOOLEAN: line.addColumn(new BoolColumn(v.asBoolean()));  break;
                     case BINARY:  line.addColumn(new BytesColumn(v.asBinary()));  break;
                     default:
-                        throw new IllegalArgumentException("Unsuporrt tranform the type: " + col.getValue().getType() + ".");
+                        throw new IllegalArgumentException("Unsupported transform the type: " + col.getValue().getType() + ".");
                     }
                 }
             }

File: tdenginewriter/src/main/java/com/alibaba/datax/plugin/writer/tdenginewriter/DefaultDataHandler.java
Patch:
@@ -307,6 +307,8 @@ private String buildColumnValue(ColumnMeta colMeta, Record record) {
                 if (colMeta.type.equals("TIMESTAMP"))
                     return "\"" + column.asString() + "\"";
                 String value = column.asString();
+                if (value == null)
+                    return "NULL";
                 return "\'" + Utils.escapeSingleQuota(value) + "\'";
             case NULL:
             case BAD:

File: tdenginewriter/src/main/java/com/alibaba/datax/plugin/writer/tdenginewriter/DefaultDataHandler.java
Patch:
@@ -89,6 +89,7 @@ public int handle(RecordReceiver lineReceiver, TaskPluginCollector collector) {
                     recordBatch.add(record);
                 } else {
                     try {
+                        recordBatch.add(record);
                         affectedRows = writeBatch(conn, recordBatch);
                     } catch (SQLException e) {
                         LOG.warn("use one row insert. because:" + e.getMessage());

File: tdenginewriter/src/main/java/com/alibaba/datax/plugin/writer/tdenginewriter/DefaultDataHandler.java
Patch:
@@ -89,6 +89,7 @@ public int handle(RecordReceiver lineReceiver, TaskPluginCollector collector) {
                     recordBatch.add(record);
                 } else {
                     try {
+                        recordBatch.add(record);
                         affectedRows = writeBatch(conn, recordBatch);
                     } catch (SQLException e) {
                         LOG.warn("use one row insert. because:" + e.getMessage());

File: starrockswriter/src/main/java/com/starrocks/connector/datax/plugin/writer/starrockswriter/manager/StarRocksStreamLoadVisitor.java
Patch:
@@ -25,6 +25,7 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.concurrent.TimeUnit;
@@ -107,7 +108,7 @@ private boolean tryHttpConnection(String host) {
 
     private byte[] joinRows(List<byte[]> rows, int totalBytes) {
         if (StarRocksWriterOptions.StreamLoadFormat.CSV.equals(writerOptions.getStreamLoadFormat())) {
-            Map<String, Object> props = writerOptions.getLoadProps();
+            Map<String, Object> props = (writerOptions.getLoadProps() == null ? new HashMap<>() : writerOptions.getLoadProps());
             byte[] lineDelimiter = StarRocksDelimiterParser.parse((String)props.get("row_delimiter"), "\n").getBytes(StandardCharsets.UTF_8);
             ByteBuffer bos = ByteBuffer.allocate(totalBytes + rows.size() * lineDelimiter.length);
             for (byte[] row : rows) {
@@ -216,7 +217,7 @@ protected boolean isRedirectable(String method) {
             }
         }
     }
-    
+
     private String getBasicAuthHeader(String username, String password) {
         String auth = username + ":" + password;
         byte[] encodedAuth = Base64.encodeBase64(auth.getBytes(StandardCharsets.UTF_8));

File: plugin-rdbms-util/src/main/java/com/alibaba/datax/plugin/rdbms/reader/util/SingleTableSplitUtil.java
Patch:
@@ -257,6 +257,7 @@ private static boolean isLongType(int type) {
 
         switch (SingleTableSplitUtil.DATABASE_TYPE) {
             case Oracle:
+            case OceanBase:
                 isValidLongType |= type == Types.NUMERIC;
                 break;
             default:

File: plugin-rdbms-util/src/main/java/com/alibaba/datax/plugin/rdbms/reader/util/SingleTableSplitUtil.java
Patch:
@@ -93,6 +93,7 @@ public static List<Configuration> splitSingleTable(
 
                 allQuerySql.add(tempQuerySql);
                 tempConfig.set(Key.QUERY_SQL, tempQuerySql);
+                tempConfig.set(Key.WHERE, (hasWhere ? ("(" + where + ") and") : "") + range);
                 pluginParams.add(tempConfig);
             }
         } else {
@@ -103,6 +104,7 @@ public static List<Configuration> splitSingleTable(
                     + String.format(" %s IS NOT NULL", splitPkName);
             allQuerySql.add(tempQuerySql);
             tempConfig.set(Key.QUERY_SQL, tempQuerySql);
+            tempConfig.set(Key.WHERE, (hasWhere ? "(" + where + ") and" : "") + String.format(" %s IS NOT NULL", splitPkName));
             pluginParams.add(tempConfig);
         }
 
@@ -118,6 +120,7 @@ public static List<Configuration> splitSingleTable(
                 StringUtils.join(allQuerySql, "\n"));
 
         tempConfig.set(Key.QUERY_SQL, tempQuerySql);
+        tempConfig.set(Key.WHERE, (hasWhere ? "(" + where + ") and" : "") + String.format(" %s IS NULL", splitPkName));
         pluginParams.add(tempConfig);
         
         return pluginParams;

File: tdenginereader/src/main/java/com/alibaba/datax/plugin/reader/TDengineReader.java
Patch:
@@ -193,6 +193,7 @@ public void startRead(RecordSender recordSender) {
                         sb.append(" and _c0 < '").append(endTime).append("'");
                     }
                     String sql = sb.toString().trim();
+                    sqlList.add(sql);
                 }
             } else {
                 sqlList.addAll(querySql);

File: tdenginereader/src/main/java/com/alibaba/datax/plugin/reader/TDengineReader.java
Patch:
@@ -193,6 +193,7 @@ public void startRead(RecordSender recordSender) {
                         sb.append(" and _c0 < '").append(endTime).append("'");
                     }
                     String sql = sb.toString().trim();
+                    sqlList.add(sql);
                 }
             } else {
                 sqlList.addAll(querySql);

File: tdenginereader/src/main/java/com/alibaba/datax/plugin/reader/Key.java
Patch:
@@ -7,7 +7,7 @@ public class Key {
 //    public static final String PORT = "port";
 //    public static final String DB = "db";
     public static final String TABLE = "table";
-    public static final String USER = "user";
+    public static final String USER = "username";
     public static final String PASSWORD = "password";
     public static final String CONNECTION = "connection";
 //    public static final String SQL = "sql";

File: tdenginereader/src/main/java/com/alibaba/datax/plugin/reader/TDengineReader.java
Patch:
@@ -190,8 +190,9 @@ public void destroy() {
         @Override
         public void startRead(RecordSender recordSender) {
             try (Statement stmt = conn.createStatement()) {
-                for (int i = 0; i < tables.size(); i++) {
-                    String sql = "select " + StringUtils.join(columns, ",") + " from " + tables.get(i) + " where _c0 >= " + startTime + " and _c0 < " + endTime;
+                for (String table : tables) {
+                    String sql = "select " + StringUtils.join(columns, ",") + " from " + table
+                            + " where _c0 >= " + startTime + " and _c0 < " + endTime;
                     ResultSet rs = stmt.executeQuery(sql);
                     ResultSetMetaData metaData = rs.getMetaData();
                     while (rs.next()) {

File: tdenginewriter/src/test/java/com/alibaba/datax/plugin/writer/tdenginewriter/Stream2TDengineTest.java
Patch:
@@ -12,7 +12,6 @@
 public class Stream2TDengineTest {
 
     private String host2 = "192.168.56.105";
-    private String precision;
 
     @Test
     public void s2t_case1() throws Throwable {
@@ -56,7 +55,8 @@ void createSupTable(String precision) throws SQLException {
             stmt.execute("drop database if exists db2");
             stmt.execute("create database if not exists db2 precision '" + precision + "'");
             stmt.execute("create table db2.stb2(ts1 timestamp, ts2 timestamp,ts3 timestamp,ts4 timestamp,ts5 timestamp," +
-                    "ts6 timestamp,ts7 timestamp, f1 tinyint, f2 smallint, f3 int, f4 bigint, f5 float, f6 double," +
+                    "ts6 timestamp,ts7 timestamp, ts8 timestamp, ts9 timestamp, ts10 timestamp, f1 tinyint, f2 smallint," +
+                    "f3 int, f4 bigint, f5 float, f6 double," +
                     "f7 bool, f8 binary(100), f9 nchar(100)) tags(t1 timestamp,t2 timestamp,t3 timestamp,t4 timestamp," +
                     "t5 timestamp,t6 timestamp,t7 timestamp, t8 tinyint, t9 smallint, t10 int, t11 bigint, t12 float," +
                     "t13 double, t14 bool, t15 binary(100), t16 nchar(100))");

File: oceanbasev10reader/src/main/java/com/alibaba/datax/plugin/reader/oceanbasev10reader/util/TaskContext.java
Patch:
@@ -162,6 +162,7 @@ public void setWeakRead(boolean weakRead) {
     public String getUserSavePoint() {
         return userSavePoint;
     }
+
     public void setUserSavePoint(String userSavePoint) {
         this.userSavePoint = userSavePoint;
     }

File: postgresqlwriter/src/main/java/com/alibaba/datax/plugin/writer/postgresqlwriter/PostgresqlWriter.java
Patch:
@@ -67,6 +67,8 @@ public void init() {
 				public String calcValueHolder(String columnType){
 					if("serial".equalsIgnoreCase(columnType)){
 						return "?::int";
+					}else if("bigserial".equalsIgnoreCase(columnType)){
+						return "?::int8";
 					}else if("bit".equalsIgnoreCase(columnType)){
 						return "?::bit varying";
 					}

File: tsdbreader/src/main/java/com/alibaba/datax/plugin/reader/tsdbreader/Constant.java
Patch:
@@ -16,6 +16,8 @@ public final class Constant {
     static final String DEFAULT_DATA_FORMAT = "yyyy-MM-dd HH:mm:ss";
 
     public static final String METRIC_SPECIFY_KEY = "__metric__";
+    public static final String METRIC_SPECIFY_KEY_PREFIX = METRIC_SPECIFY_KEY + ".";
+    public static final int METRIC_SPECIFY_KEY_PREFIX_LENGTH = METRIC_SPECIFY_KEY_PREFIX.length();
     public static final String TS_SPECIFY_KEY = "__ts__";
     public static final String VALUE_SPECIFY_KEY = "__value__";
 

File: tsdbreader/src/test/java/com/alibaba/datax/plugin/reader/tsdbreader/conn/TSDBConnectionTest.java
Patch:
@@ -19,12 +19,12 @@ public class TSDBConnectionTest {
 
     @Test
     public void testVersion() {
-        String version = new TSDBConnection(TSDB_ADDRESS).version();
+        String version = new TSDBConnection(TSDB_ADDRESS,null,null).version();
         Assert.assertNotNull(version);
     }
 
     @Test
     public void testIsSupported() {
-        Assert.assertTrue(new TSDBConnection(TSDB_ADDRESS).isSupported());
+        Assert.assertTrue(new TSDBConnection(TSDB_ADDRESS,null,null).isSupported());
     }
 }

File: tsdbwriter/src/main/java/com/alibaba/datax/plugin/writer/tsdbwriter/TSDBWriterErrorCode.java
Patch:
@@ -13,6 +13,7 @@
 public enum TSDBWriterErrorCode implements ErrorCode {
 
     REQUIRED_VALUE("TSDBWriter-00", "Missing the necessary value"),
+    ILLEGAL_VALUE("TSDBWriter-01", "Illegal value"),
     RUNTIME_EXCEPTION("TSDBWriter-01", "Runtime exception"),
     RETRY_WRITER_EXCEPTION("TSDBWriter-02", "After repeated attempts, the write still fails");
 

File: tsdbwriter/src/test/java/com/alibaba/datax/plugin/writer/conn/TSDBConnectionTest.java
Patch:
@@ -19,12 +19,12 @@ public class TSDBConnectionTest {
 
     @Test
     public void testVersion() {
-        String version = new TSDBConnection(TSDB_ADDRESS).version();
+        String version = new TSDBConnection(TSDB_ADDRESS,null,null,null).version();
         Assert.assertNotNull(version);
     }
 
     @Test
     public void testIsSupported() {
-        Assert.assertTrue(new TSDBConnection(TSDB_ADDRESS).isSupported());
+        Assert.assertTrue(new TSDBConnection(TSDB_ADDRESS,null,null,null).isSupported());
     }
 }

File: tsdbwriter/src/test/java/com/alibaba/datax/plugin/writer/util/HttpUtilsTest.java
Patch:
@@ -24,15 +24,15 @@ public void testSimpleCase() throws Exception {
         Map<String, Object> params = new HashMap<String, Object>();
         params.put("foo", "bar");
 
-        String rsp = HttpUtils.post(url, params);
+        String rsp = HttpUtils.post(url, null,null,params);
         System.out.println(rsp);
         Assert.assertNotNull(rsp);
     }
 
     @Test
     public void testGet() throws Exception {
         String url = String.format("%s/api/version", Const.OPENTSDB_ADDRESS);
-        String rsp = HttpUtils.get(url);
+        String rsp = HttpUtils.get(url,null,null);
         System.out.println(rsp);
         Assert.assertNotNull(rsp);
     }

File: tsdbwriter/src/test/java/com/alibaba/datax/plugin/writer/util/TSDBTest.java
Patch:
@@ -17,11 +17,11 @@ public class TSDBTest {
 
     @Test
     public void testVersion() {
-        String version = TSDBUtils.version(Const.TSDB_ADDRESS);
+        String version = TSDBUtils.version(Const.TSDB_ADDRESS,null,null);
         Assert.assertNotNull(version);
         System.out.println(version);
 
-        version = TSDBUtils.version(Const.OPENTSDB_ADDRESS);
+        version = TSDBUtils.version(Const.OPENTSDB_ADDRESS,null,null);
         Assert.assertNotNull(version);
         System.out.println(version);
     }

File: oceanbasev10reader/src/main/java/com/alibaba/datax/plugin/reader/oceanbasev10reader/util/TaskContext.java
Patch:
@@ -162,6 +162,7 @@ public void setWeakRead(boolean weakRead) {
     public String getUserSavePoint() {
         return userSavePoint;
     }
+
     public void setUserSavePoint(String userSavePoint) {
         this.userSavePoint = userSavePoint;
     }

File: postgresqlwriter/src/main/java/com/alibaba/datax/plugin/writer/postgresqlwriter/PostgresqlWriter.java
Patch:
@@ -67,6 +67,8 @@ public void init() {
 				public String calcValueHolder(String columnType){
 					if("serial".equalsIgnoreCase(columnType)){
 						return "?::int";
+					}else if("bigserial".equalsIgnoreCase(columnType)){
+						return "?::int8";
 					}else if("bit".equalsIgnoreCase(columnType)){
 						return "?::bit varying";
 					}

File: tsdbreader/src/main/java/com/alibaba/datax/plugin/reader/tsdbreader/Constant.java
Patch:
@@ -16,6 +16,8 @@ public final class Constant {
     static final String DEFAULT_DATA_FORMAT = "yyyy-MM-dd HH:mm:ss";
 
     public static final String METRIC_SPECIFY_KEY = "__metric__";
+    public static final String METRIC_SPECIFY_KEY_PREFIX = METRIC_SPECIFY_KEY + ".";
+    public static final int METRIC_SPECIFY_KEY_PREFIX_LENGTH = METRIC_SPECIFY_KEY_PREFIX.length();
     public static final String TS_SPECIFY_KEY = "__ts__";
     public static final String VALUE_SPECIFY_KEY = "__value__";
 

File: tsdbreader/src/test/java/com/alibaba/datax/plugin/reader/tsdbreader/conn/TSDBConnectionTest.java
Patch:
@@ -19,12 +19,12 @@ public class TSDBConnectionTest {
 
     @Test
     public void testVersion() {
-        String version = new TSDBConnection(TSDB_ADDRESS).version();
+        String version = new TSDBConnection(TSDB_ADDRESS,null,null).version();
         Assert.assertNotNull(version);
     }
 
     @Test
     public void testIsSupported() {
-        Assert.assertTrue(new TSDBConnection(TSDB_ADDRESS).isSupported());
+        Assert.assertTrue(new TSDBConnection(TSDB_ADDRESS,null,null).isSupported());
     }
 }

File: tsdbwriter/src/main/java/com/alibaba/datax/plugin/writer/tsdbwriter/TSDBWriterErrorCode.java
Patch:
@@ -13,6 +13,7 @@
 public enum TSDBWriterErrorCode implements ErrorCode {
 
     REQUIRED_VALUE("TSDBWriter-00", "Missing the necessary value"),
+    ILLEGAL_VALUE("TSDBWriter-01", "Illegal value"),
     RUNTIME_EXCEPTION("TSDBWriter-01", "Runtime exception"),
     RETRY_WRITER_EXCEPTION("TSDBWriter-02", "After repeated attempts, the write still fails");
 

File: tsdbwriter/src/test/java/com/alibaba/datax/plugin/writer/conn/TSDBConnectionTest.java
Patch:
@@ -19,12 +19,12 @@ public class TSDBConnectionTest {
 
     @Test
     public void testVersion() {
-        String version = new TSDBConnection(TSDB_ADDRESS).version();
+        String version = new TSDBConnection(TSDB_ADDRESS,null,null,null).version();
         Assert.assertNotNull(version);
     }
 
     @Test
     public void testIsSupported() {
-        Assert.assertTrue(new TSDBConnection(TSDB_ADDRESS).isSupported());
+        Assert.assertTrue(new TSDBConnection(TSDB_ADDRESS,null,null,null).isSupported());
     }
 }

File: tsdbwriter/src/test/java/com/alibaba/datax/plugin/writer/util/HttpUtilsTest.java
Patch:
@@ -24,15 +24,15 @@ public void testSimpleCase() throws Exception {
         Map<String, Object> params = new HashMap<String, Object>();
         params.put("foo", "bar");
 
-        String rsp = HttpUtils.post(url, params);
+        String rsp = HttpUtils.post(url, null,null,params);
         System.out.println(rsp);
         Assert.assertNotNull(rsp);
     }
 
     @Test
     public void testGet() throws Exception {
         String url = String.format("%s/api/version", Const.OPENTSDB_ADDRESS);
-        String rsp = HttpUtils.get(url);
+        String rsp = HttpUtils.get(url,null,null);
         System.out.println(rsp);
         Assert.assertNotNull(rsp);
     }

File: tsdbwriter/src/test/java/com/alibaba/datax/plugin/writer/util/TSDBTest.java
Patch:
@@ -17,11 +17,11 @@ public class TSDBTest {
 
     @Test
     public void testVersion() {
-        String version = TSDBUtils.version(Const.TSDB_ADDRESS);
+        String version = TSDBUtils.version(Const.TSDB_ADDRESS,null,null);
         Assert.assertNotNull(version);
         System.out.println(version);
 
-        version = TSDBUtils.version(Const.OPENTSDB_ADDRESS);
+        version = TSDBUtils.version(Const.OPENTSDB_ADDRESS,null,null);
         Assert.assertNotNull(version);
         System.out.println(version);
     }

File: core/src/main/java/com/alibaba/datax/core/Engine.java
Patch:
@@ -73,7 +73,7 @@ public void start(Configuration allConf) {
         boolean traceEnable = allConf.getBool(CoreConstant.DATAX_CORE_CONTAINER_TRACE_ENABLE, true);
         boolean perfReportEnable = allConf.getBool(CoreConstant.DATAX_CORE_REPORT_DATAX_PERFLOG, true);
 
-        //standlone模式的datax shell任务不进行汇报
+        //standalone模式的 datax shell任务不进行汇报
         if(instanceId == -1){
             perfReportEnable = false;
         }

File: tsdbreader/src/test/java/com/alibaba/datax/plugin/reader/tsdbreader/conn/TSDBConnectionTest.java
Patch:
@@ -19,12 +19,12 @@ public class TSDBConnectionTest {
 
     @Test
     public void testVersion() {
-        String version = new TSDBConnection(TSDB_ADDRESS).version();
+        String version = new TSDBConnection(TSDB_ADDRESS,null,null).version();
         Assert.assertNotNull(version);
     }
 
     @Test
     public void testIsSupported() {
-        Assert.assertTrue(new TSDBConnection(TSDB_ADDRESS).isSupported());
+        Assert.assertTrue(new TSDBConnection(TSDB_ADDRESS,null,null).isSupported());
     }
 }

File: tsdbwriter/src/test/java/com/alibaba/datax/plugin/writer/conn/TSDBConnectionTest.java
Patch:
@@ -19,12 +19,12 @@ public class TSDBConnectionTest {
 
     @Test
     public void testVersion() {
-        String version = new TSDBConnection(TSDB_ADDRESS).version();
+        String version = new TSDBConnection(TSDB_ADDRESS,null,null,null).version();
         Assert.assertNotNull(version);
     }
 
     @Test
     public void testIsSupported() {
-        Assert.assertTrue(new TSDBConnection(TSDB_ADDRESS).isSupported());
+        Assert.assertTrue(new TSDBConnection(TSDB_ADDRESS,null,null,null).isSupported());
     }
 }

File: tsdbwriter/src/test/java/com/alibaba/datax/plugin/writer/util/HttpUtilsTest.java
Patch:
@@ -24,15 +24,15 @@ public void testSimpleCase() throws Exception {
         Map<String, Object> params = new HashMap<String, Object>();
         params.put("foo", "bar");
 
-        String rsp = HttpUtils.post(url, params);
+        String rsp = HttpUtils.post(url, null,null,params);
         System.out.println(rsp);
         Assert.assertNotNull(rsp);
     }
 
     @Test
     public void testGet() throws Exception {
         String url = String.format("%s/api/version", Const.OPENTSDB_ADDRESS);
-        String rsp = HttpUtils.get(url);
+        String rsp = HttpUtils.get(url,null,null);
         System.out.println(rsp);
         Assert.assertNotNull(rsp);
     }

File: tsdbwriter/src/test/java/com/alibaba/datax/plugin/writer/util/TSDBTest.java
Patch:
@@ -17,11 +17,11 @@ public class TSDBTest {
 
     @Test
     public void testVersion() {
-        String version = TSDBUtils.version(Const.TSDB_ADDRESS);
+        String version = TSDBUtils.version(Const.TSDB_ADDRESS,null,null);
         Assert.assertNotNull(version);
         System.out.println(version);
 
-        version = TSDBUtils.version(Const.OPENTSDB_ADDRESS);
+        version = TSDBUtils.version(Const.OPENTSDB_ADDRESS,null,null);
         Assert.assertNotNull(version);
         System.out.println(version);
     }

File: tdenginewriter/src/main/java/com/alibaba/datax/plugin/writer/tdenginewriter/Key.java
Patch:
@@ -4,7 +4,7 @@ public class Key {
     public static final String HOST = "host";
     public static final String PORT = "port";
     public static final String DBNAME = "dbName";
-    public static final String USER = "user";
+    public static final String USER = "username";
     public static final String PASSWORD = "password";
     public static final String BATCH_SIZE = "batchSize";
     public static final String STABLE = "stable";

File: tdenginewriter/src/main/java/com/alibaba/datax/plugin/writer/tdenginewriter/Key.java
Patch:
@@ -3,7 +3,7 @@
 public class Key {
     public static final String HOST = "host";
     public static final String PORT = "port";
-    public static final String DBNAME = "dbname";
+    public static final String DBNAME = "dbName";
     public static final String USER = "user";
     public static final String PASSWORD = "password";
     public static final String BATCH_SIZE = "batchSize";

File: core/src/main/java/com/alibaba/datax/core/Engine.java
Patch:
@@ -73,7 +73,7 @@ public void start(Configuration allConf) {
         boolean traceEnable = allConf.getBool(CoreConstant.DATAX_CORE_CONTAINER_TRACE_ENABLE, true);
         boolean perfReportEnable = allConf.getBool(CoreConstant.DATAX_CORE_REPORT_DATAX_PERFLOG, true);
 
-        //standlone模式的datax shell任务不进行汇报
+        //standalone模式的 datax shell任务不进行汇报
         if(instanceId == -1){
             perfReportEnable = false;
         }

File: oceanbasev10writer/src/main/java/com/alibaba/datax/plugin/writer/oceanbasev10writer/task/InsertTask.java
Patch:
@@ -65,6 +65,7 @@ public InsertTask(
 		this.writeRecordSql = writeRecordSql;
 		this.isStop = false;
 		this.deleteMeta = deleteMeta;
+		connHolder.initConnection();
 	}
 	
 	void setWriterTask(ConcurrentTableWriterTask writerTask) {
@@ -151,7 +152,6 @@ private void doDelete(Connection conn, final List<Record> buffer) throws SQLExce
 
 	public void doMultiInsert(final List<Record> buffer, final boolean printCost, final long restrict) {
 		checkMemstore();
-		connHolder.initConnection();
 		Connection conn = connHolder.getConn();
 		boolean success = false;
 		long cost = 0;
@@ -165,7 +165,6 @@ public void doMultiInsert(final List<Record> buffer, final boolean printCost, fi
 					} catch (InterruptedException e) {
 						LOG.info("thread interrupted ..., ignore");
 					}
-					connHolder.initConnection();
 					conn = connHolder.getConn();
 					LOG.info("retry {}, start do batch insert, size={}", i, buffer.size());
 					checkMemstore();

File: oceanbasev10writer/src/main/java/com/alibaba/datax/plugin/writer/oceanbasev10writer/task/InsertTask.java
Patch:
@@ -165,7 +165,6 @@ public void doMultiInsert(final List<Record> buffer, final boolean printCost, fi
 					} catch (InterruptedException e) {
 						LOG.info("thread interrupted ..., ignore");
 					}
-					connHolder.initConnection();
 					conn = connHolder.getConn();
 					LOG.info("retry {}, start do batch insert, size={}", i, buffer.size());
 					checkMemstore();

File: starrockswriter/src/main/java/com/starrocks/connector/datax/plugin/writer/starrockswriter/manager/StarRocksStreamLoadVisitor.java
Patch:
@@ -168,7 +168,7 @@ protected boolean isRedirectable(String method) {
     
     private String getBasicAuthHeader(String username, String password) {
         String auth = username + ":" + password;
-        byte[] encodedAuth = Base64.encodeBase64(auth.getBytes());
+        byte[] encodedAuth = Base64.encodeBase64(auth.getBytes(StandardCharsets.UTF_8));
         return new StringBuilder("Basic ").append(new String(encodedAuth)).toString();
     }
 

File: starrockswriter/src/main/java/com/starrocks/connector/datax/plugin/writer/starrockswriter/manager/StarRocksWriterManager.java
Patch:
@@ -4,6 +4,7 @@
 import org.slf4j.LoggerFactory;
 
 import java.io.IOException;
+import java.nio.charset.StandardCharsets;
 import java.util.ArrayList;
 import java.util.List;
 import java.util.UUID;
@@ -38,7 +39,7 @@ public final synchronized void writeRecord(String record) throws IOException {
         try {
             buffer.add(record);
             batchCount++;
-            batchSize += record.getBytes().length;
+            batchSize += record.getBytes(StandardCharsets.UTF_8).length;
             if (batchCount >= writerOptions.getBatchRows() || batchSize >= writerOptions.getBatchSize()) {
                 String label = createBatchLabel();
                 LOG.debug(String.format("StarRocks buffer Sinking triggered: rows[%d] label[%s].", batchCount, label));

File: oceanbasev10writer/src/main/java/com/alibaba/datax/plugin/writer/oceanbasev10writer/task/InsertTask.java
Patch:
@@ -65,6 +65,7 @@ public InsertTask(
 		this.writeRecordSql = writeRecordSql;
 		this.isStop = false;
 		this.deleteMeta = deleteMeta;
+		connHolder.initConnection();
 	}
 	
 	void setWriterTask(ConcurrentTableWriterTask writerTask) {
@@ -151,7 +152,6 @@ private void doDelete(Connection conn, final List<Record> buffer) throws SQLExce
 
 	public void doMultiInsert(final List<Record> buffer, final boolean printCost, final long restrict) {
 		checkMemstore();
-		connHolder.initConnection();
 		Connection conn = connHolder.getConn();
 		boolean success = false;
 		long cost = 0;

File: starrockswriter/src/main/java/com/starrocks/connector/datax/plugin/writer/starrockswriter/manager/StarRocksStreamLoadVisitor.java
Patch:
@@ -96,7 +96,7 @@ private boolean tryHttpConnection(String host) {
     private byte[] joinRows(List<String> rows, int totalBytes) {
         if (StarRocksWriterOptions.StreamLoadFormat.CSV.equals(writerOptions.getStreamLoadFormat())) {
             Map<String, Object> props = writerOptions.getLoadProps();
-            byte[] lineDelimiter = (props.containsKey("row_delimiter") ? StarRocksDelimiterParser.parse(String.valueOf(props.get("row_delimiter")), "\n") : "\n").getBytes(StandardCharsets.UTF_8);
+            byte[] lineDelimiter = StarRocksDelimiterParser.parse((String)props.get("row_delimiter"), "\n").getBytes(StandardCharsets.UTF_8);
             ByteBuffer bos = ByteBuffer.allocate(totalBytes + rows.size() * lineDelimiter.length);
             for (String row : rows) {
                 bos.put(row.getBytes(StandardCharsets.UTF_8));

File: doriswriter/src/main/java/com/dorisdb/connector/datax/plugin/writer/doriswriter/manager/DorisStreamLoadVisitor.java
Patch:
@@ -96,8 +96,8 @@ private boolean tryHttpConnection(String host) {
     private byte[] joinRows(List<String> rows, int totalBytes) {
         if (DorisWriterOptions.StreamLoadFormat.CSV.equals(writerOptions.getStreamLoadFormat())) {
             Map<String, Object> props = writerOptions.getLoadProps();
-            ByteBuffer bos = ByteBuffer.allocate(totalBytes + rows.size());
             byte[] lineDelimiter = DorisDelimiterParser.parse(String.valueOf(props.get("row_delimiter")), "\n").getBytes(StandardCharsets.UTF_8);
+            ByteBuffer bos = ByteBuffer.allocate(totalBytes + rows.size() * lineDelimiter.length);
             for (String row : rows) {
                 bos.put(row.getBytes(StandardCharsets.UTF_8));
                 bos.put(lineDelimiter);

File: doriswriter/src/main/java/com/dorisdb/connector/datax/plugin/writer/doriswriter/row/DorisCsvSerializer.java
Patch:
@@ -6,7 +6,7 @@
 
 import com.google.common.base.Strings;
 
-public class DorisCsvSerializer implements DorisISerializer {
+public class DorisCsvSerializer extends DorisBaseSerializer implements DorisISerializer {
     
     private static final long serialVersionUID = 1L;
 
@@ -22,7 +22,7 @@ public DorisCsvSerializer(String sp) {
     public String serialize(Record row) {
         StringBuilder sb = new StringBuilder();
         for (int i = 0; i < row.getColumnNumber(); i++) {
-            Object value = row.getColumn(i).getRawData();
+            String value = fieldConvertion(row.getColumn(i));
             sb.append(null == value ? "\\N" : value);
             if (i < row.getColumnNumber() - 1) {
                 sb.append(columnSeparator);

File: doriswriter/src/main/java/com/dorisdb/connector/datax/plugin/writer/doriswriter/row/DorisJsonSerializer.java
Patch:
@@ -7,7 +7,7 @@
 import com.alibaba.datax.common.element.Record;
 import com.alibaba.fastjson.JSON;
 
-public class DorisJsonSerializer implements DorisISerializer {
+public class DorisJsonSerializer extends DorisBaseSerializer implements DorisISerializer {
 
     private static final long serialVersionUID = 1L;
     
@@ -25,7 +25,8 @@ public String serialize(Record row) {
         Map<String, Object> rowMap = new HashMap<>(fieldNames.size());
         int idx = 0;
         for (String fieldName : fieldNames) {
-            rowMap.put(fieldName, row.getColumn(idx++).getRawData());
+            rowMap.put(fieldName, fieldConvertion(row.getColumn(idx)));
+            idx++;
         }
         return JSON.toJSONString(rowMap);
     }

File: doriswriter/src/main/java/com/dorisdb/connector/datax/plugin/writer/doriswriter/manager/DorisWriterManager.java
Patch:
@@ -33,7 +33,7 @@ public final synchronized void writeRecord(String record) throws IOException {
         try {
             buffer.add(record);
             batchCount++;
-            batchSize += record.length();
+            batchSize += record.getBytes().length;
             if (batchCount >= writerOptions.getBatchRows() || batchSize >= writerOptions.getBatchSize()) {
                 flush(createBatchLabel());
             }

File: doriswriter/src/main/java/com/dorisdb/connector/datax/plugin/writer/doriswriter/DorisWriter.java
Patch:
@@ -112,7 +112,8 @@ public void startWrite(RecordReceiver recordReceiver) {
                     }
                     StringBuilder sb = new StringBuilder();
                     for (int i = 0; i < record.getColumnNumber(); i++) {
-                        sb.append(record.getColumn(i).getRawData().toString());
+                        Object value = record.getColumn(i).getRawData();
+                        sb.append(null == value ? "\\N" : value);
                         if (i < record.getColumnNumber() - 1) {
                             sb.append("\t");
                         }

File: kuduwriter/src/main/java/com/q1/datax/plugin/writer/kudu11xwriter/Kudu11xWriter.java
Patch:
@@ -38,7 +38,7 @@ public void prepare() {
 
         @Override
         public List<Configuration> split(int i) {
-            List<Configuration> splitResultConfigs = new ArrayList<Configuration>();
+            List<Configuration> splitResultConfigs = new ArrayList<>();
             for (int j = 0; j < i; j++) {
                 splitResultConfigs.add(config.clone());
             }
@@ -76,7 +76,7 @@ public void destroy() {
                     kuduTaskProxy.session.close();
                 }
             }catch (Exception e){
-                LOG.warn("kudu session is not gracefully closed !");
+                LOG.warn("The \"kudu session\" was not stopped gracefully !");
             }
             Kudu11xHelper.closeClient(kuduTaskProxy.kuduClient);
 

File: kuduwriter/src/main/java/com/q1/datax/plugin/writer/kudu11xwriter/Kudu11xHelper.java
Patch:
@@ -111,7 +111,7 @@ public static void createTable(Configuration configuration) {
         } catch (Exception e) {
             throw DataXException.asDataXException(Kudu11xWriterErrorcode.GREATE_KUDU_TABLE_ERROR, e);
         } finally {
-            AtomicInteger i = new AtomicInteger(5);
+            AtomicInteger i = new AtomicInteger(10);
             while (i.get() > 0) {
                 try {
                     if (kuduClient.isCreateTableDone(tableName)) {
@@ -124,7 +124,7 @@ public static void createTable(Configuration configuration) {
                 } catch (KuduException e) {
                     LOG.info("Wait for the table to be created..... " + i);
                     try {
-                        Thread.sleep(1000L);
+                        Thread.sleep(100L);
                     } catch (InterruptedException ex) {
                         ex.printStackTrace();
                     }

File: kuduwriter/src/main/java/com/q1/datax/plugin/writer/kudu11xwriter/Kudu11xWriter.java
Patch:
@@ -38,7 +38,7 @@ public void prepare() {
 
         @Override
         public List<Configuration> split(int i) {
-            List<Configuration> splitResultConfigs = new ArrayList<Configuration>();
+            List<Configuration> splitResultConfigs = new ArrayList<>();
             for (int j = 0; j < i; j++) {
                 splitResultConfigs.add(config.clone());
             }
@@ -76,7 +76,7 @@ public void destroy() {
                     kuduTaskProxy.session.close();
                 }
             }catch (Exception e){
-                LOG.warn("kudu session is not gracefully closed !");
+                LOG.warn("The \"kudu session\" was not stopped gracefully !");
             }
             Kudu11xHelper.closeClient(kuduTaskProxy.kuduClient);
 

File: core/src/main/java/com/alibaba/datax/core/job/JobContainer.java
Patch:
@@ -427,7 +427,7 @@ private void adjustChannelNumber() {
             Long channelLimitedByteSpeed = this.configuration
                     .getLong(CoreConstant.DATAX_CORE_TRANSPORT_CHANNEL_SPEED_BYTE);
             if (channelLimitedByteSpeed == null || channelLimitedByteSpeed <= 0) {
-                DataXException.asDataXException(
+                throw DataXException.asDataXException(
                         FrameworkErrorCode.CONFIG_ERROR,
                         "在有总bps限速条件下，单个channel的bps值不能为空，也不能为非正数");
             }
@@ -448,7 +448,7 @@ private void adjustChannelNumber() {
             Long channelLimitedRecordSpeed = this.configuration.getLong(
                     CoreConstant.DATAX_CORE_TRANSPORT_CHANNEL_SPEED_RECORD);
             if (channelLimitedRecordSpeed == null || channelLimitedRecordSpeed <= 0) {
-                DataXException.asDataXException(FrameworkErrorCode.CONFIG_ERROR,
+                throw DataXException.asDataXException(FrameworkErrorCode.CONFIG_ERROR,
                         "在有总tps限速条件下，单个channel的tps值不能为空，也不能为非正数");
             }
 

File: clickhousewriter/src/main/java/com/alibaba/datax/plugin/writer/clickhousewriter/ClickhouseWriter.java
Patch:
@@ -12,7 +12,6 @@
 import com.alibaba.datax.plugin.rdbms.writer.CommonRdbmsWriter;
 import com.alibaba.fastjson.JSON;
 import com.alibaba.fastjson.JSONArray;
-import ru.yandex.clickhouse.ClickHouseTuple;
 
 import java.sql.Array;
 import java.sql.Connection;

File: core/src/main/java/com/alibaba/datax/core/job/JobContainer.java
Patch:
@@ -427,7 +427,7 @@ private void adjustChannelNumber() {
             Long channelLimitedByteSpeed = this.configuration
                     .getLong(CoreConstant.DATAX_CORE_TRANSPORT_CHANNEL_SPEED_BYTE);
             if (channelLimitedByteSpeed == null || channelLimitedByteSpeed <= 0) {
-                DataXException.asDataXException(
+                throw DataXException.asDataXException(
                         FrameworkErrorCode.CONFIG_ERROR,
                         "在有总bps限速条件下，单个channel的bps值不能为空，也不能为非正数");
             }
@@ -448,7 +448,7 @@ private void adjustChannelNumber() {
             Long channelLimitedRecordSpeed = this.configuration.getLong(
                     CoreConstant.DATAX_CORE_TRANSPORT_CHANNEL_SPEED_RECORD);
             if (channelLimitedRecordSpeed == null || channelLimitedRecordSpeed <= 0) {
-                DataXException.asDataXException(FrameworkErrorCode.CONFIG_ERROR,
+                throw DataXException.asDataXException(FrameworkErrorCode.CONFIG_ERROR,
                         "在有总tps限速条件下，单个channel的tps值不能为空，也不能为非正数");
             }
 

File: core/src/main/java/com/alibaba/datax/core/job/JobContainer.java
Patch:
@@ -427,7 +427,7 @@ private void adjustChannelNumber() {
             Long channelLimitedByteSpeed = this.configuration
                     .getLong(CoreConstant.DATAX_CORE_TRANSPORT_CHANNEL_SPEED_BYTE);
             if (channelLimitedByteSpeed == null || channelLimitedByteSpeed <= 0) {
-                DataXException.asDataXException(
+                throw DataXException.asDataXException(
                         FrameworkErrorCode.CONFIG_ERROR,
                         "在有总bps限速条件下，单个channel的bps值不能为空，也不能为非正数");
             }
@@ -448,7 +448,7 @@ private void adjustChannelNumber() {
             Long channelLimitedRecordSpeed = this.configuration.getLong(
                     CoreConstant.DATAX_CORE_TRANSPORT_CHANNEL_SPEED_RECORD);
             if (channelLimitedRecordSpeed == null || channelLimitedRecordSpeed <= 0) {
-                DataXException.asDataXException(FrameworkErrorCode.CONFIG_ERROR,
+                throw DataXException.asDataXException(FrameworkErrorCode.CONFIG_ERROR,
                         "在有总tps限速条件下，单个channel的tps值不能为空，也不能为非正数");
             }
 

File: hbase20xsqlreader/src/main/java/com/alibaba/datax/plugin/reader/hbase20xsqlreader/HBase20SQLReaderHelper.java
Patch:
@@ -49,9 +49,9 @@ public void validateParameter() {
             String schema = configuration.getString(Key.SCHEMA, null);
             String tableName = configuration.getNecessaryValue(Key.TABLE, HBase20xSQLReaderErrorCode.REQUIRED_VALUE);
             if (schema != null && !schema.isEmpty()) {
-                fullTableName = schema + "." + tableName;
+                fullTableName = "\"" + schema + "\".\"" + tableName + "\"";
             } else {
-                fullTableName = tableName;
+                fullTableName = "\"" + tableName + "\"";
             }
             // 如果列名未配置，默认读取全部列*
             columnNames = configuration.getList(Key.COLUMN, String.class);
@@ -248,7 +248,7 @@ public static String buildQuerySql(List<String> columnNames, String table,
         String querySql;
         StringBuilder columnBuilder = new StringBuilder();
         for (String columnName : columnNames) {
-            columnBuilder.append(columnName).append(",");
+            columnBuilder.append("\"").append(columnName).append("\",");
         }
         columnBuilder.setLength(columnBuilder.length() -1);
         if (StringUtils.isBlank(where)) {

File: hbase20xsqlwriter/src/main/java/com/alibaba/datax/plugin/writer/hbase20xsqlwriter/HBase20xSQLWriterTask.java
Patch:
@@ -72,9 +72,9 @@ private void initialize() throws SQLException {
         batchSize = configuration.getInt(Key.BATCHSIZE, Constant.DEFAULT_BATCH_ROW_COUNT);
         String schema = configuration.getString(Key.SCHEMA);
         String tableName = configuration.getNecessaryValue(Key.TABLE, HBase20xSQLWriterErrorCode.REQUIRED_VALUE);
-        fullTableName = tableName;
+        fullTableName = "\"" + tableName + "\"";
         if (schema != null && !schema.isEmpty()) {
-            fullTableName = schema + "." + tableName;
+            fullTableName = "\"" + schema + "\".\"" + tableName + "\"";
         }
         columns = configuration.getList(Key.COLUMN, String.class);
         if (pstmt == null) {
@@ -125,7 +125,7 @@ private int[] getColumnSqlType() throws SQLException {
         int[] types = new int[numberOfColumnsToWrite];
         StringBuilder columnNamesBuilder = new StringBuilder();
         for (String columnName : columns) {
-            columnNamesBuilder.append(columnName).append(",");
+            columnNamesBuilder.append("\"").append(columnName).append("\",");
         }
         columnNamesBuilder.setLength(columnNamesBuilder.length() - 1);
         // 查询一条数据获取表meta信息

File: hbase20xsqlreader/src/main/java/com/alibaba/datax/plugin/reader/hbase20xsqlreader/HBase20SQLReaderHelper.java
Patch:
@@ -49,9 +49,9 @@ public void validateParameter() {
             String schema = configuration.getString(Key.SCHEMA, null);
             String tableName = configuration.getNecessaryValue(Key.TABLE, HBase20xSQLReaderErrorCode.REQUIRED_VALUE);
             if (schema != null && !schema.isEmpty()) {
-                fullTableName = schema + "." + tableName;
+                fullTableName = "\"" + schema + "\".\"" + tableName + "\"";
             } else {
-                fullTableName = tableName;
+                fullTableName = "\"" + tableName + "\"";
             }
             // 如果列名未配置，默认读取全部列*
             columnNames = configuration.getList(Key.COLUMN, String.class);
@@ -248,7 +248,7 @@ public static String buildQuerySql(List<String> columnNames, String table,
         String querySql;
         StringBuilder columnBuilder = new StringBuilder();
         for (String columnName : columnNames) {
-            columnBuilder.append(columnName).append(",");
+            columnBuilder.append("\"").append(columnName).append("\",");
         }
         columnBuilder.setLength(columnBuilder.length() -1);
         if (StringUtils.isBlank(where)) {

File: hbase20xsqlwriter/src/main/java/com/alibaba/datax/plugin/writer/hbase20xsqlwriter/HBase20xSQLWriterTask.java
Patch:
@@ -72,9 +72,9 @@ private void initialize() throws SQLException {
         batchSize = configuration.getInt(Key.BATCHSIZE, Constant.DEFAULT_BATCH_ROW_COUNT);
         String schema = configuration.getString(Key.SCHEMA);
         String tableName = configuration.getNecessaryValue(Key.TABLE, HBase20xSQLWriterErrorCode.REQUIRED_VALUE);
-        fullTableName = tableName;
+        fullTableName = "\"" + tableName + "\"";
         if (schema != null && !schema.isEmpty()) {
-            fullTableName = schema + "." + tableName;
+            fullTableName = "\"" + schema + "\".\"" + tableName + "\"";
         }
         columns = configuration.getList(Key.COLUMN, String.class);
         if (pstmt == null) {
@@ -125,7 +125,7 @@ private int[] getColumnSqlType() throws SQLException {
         int[] types = new int[numberOfColumnsToWrite];
         StringBuilder columnNamesBuilder = new StringBuilder();
         for (String columnName : columns) {
-            columnNamesBuilder.append(columnName).append(",");
+            columnNamesBuilder.append("\"").append(columnName).append("\",");
         }
         columnNamesBuilder.setLength(columnNamesBuilder.length() - 1);
         // 查询一条数据获取表meta信息

