File: langchain4j-google-ai-gemini/src/main/java/dev/langchain4j/model/googleai/GoogleAiGeminiTokenizer.java
Patch:
@@ -1,6 +1,5 @@
 package dev.langchain4j.model.googleai;
 
-import com.google.gson.Gson;
 import dev.langchain4j.agent.tool.ToolExecutionRequest;
 import dev.langchain4j.agent.tool.ToolSpecification;
 import dev.langchain4j.data.message.AiMessage;
@@ -39,7 +38,7 @@ public class GoogleAiGeminiTokenizer implements Tokenizer {
         this.apiKey = ensureNotBlank(apiKey, "apiKey");
         this.maxRetries = getOrDefault(maxRetries, 3);
         this.geminiService = new GeminiService(
-                logRequestsAndResponses ? log : null,
+                getOrDefault(logRequestsAndResponses, false) ? log : null,
                 timeout != null ? timeout : Duration.ofSeconds(60)
         );
     }

File: langchain4j-milvus/src/test/java/dev/langchain4j/store/embedding/milvus/MilvusEmbeddingStoreIT.java
Patch:
@@ -29,7 +29,7 @@ class MilvusEmbeddingStoreIT extends EmbeddingStoreWithFilteringIT {
     private static final String COLLECTION_NAME = "test_collection";
 
     @Container
-    private static final MilvusContainer milvus = new MilvusContainer("milvusdb/milvus:v2.3.16");
+    private static final MilvusContainer milvus = new MilvusContainer("milvusdb/milvus:v2.4.20");
 
     MilvusEmbeddingStore embeddingStore = MilvusEmbeddingStore.builder()
             .uri(milvus.getEndpoint())

File: langchain4j-milvus/src/test/java/dev/langchain4j/store/embedding/milvus/MilvusEmbeddingStoreRemovalIT.java
Patch:
@@ -16,7 +16,7 @@
 class MilvusEmbeddingStoreRemovalIT extends EmbeddingStoreWithRemovalIT {
 
     @Container
-    static MilvusContainer milvus = new MilvusContainer("milvusdb/milvus:v2.3.16");
+    static MilvusContainer milvus = new MilvusContainer("milvusdb/milvus:v2.4.20");
 
     MilvusEmbeddingStore embeddingStore = MilvusEmbeddingStore.builder()
             .uri(milvus.getEndpoint())
@@ -38,4 +38,4 @@ protected EmbeddingStore<TextSegment> embeddingStore() {
     protected EmbeddingModel embeddingModel() {
         return embeddingModel;
     }
-}
\ No newline at end of file
+}

File: langchain4j-core/src/main/java/dev/langchain4j/agent/tool/ToolSpecifications.java
Patch:
@@ -64,7 +64,7 @@ public static void validateSpecifications(List<ToolSpecification> toolSpecificat
         Set<String> names = new HashSet<>();
         for (ToolSpecification toolSpecification : toolSpecifications) {
             if (!names.add(toolSpecification.name())) {
-                throw new IllegalArgumentException("Tool names must be unique. The tool '%s' appears several times".formatted(toolSpecification.name()));
+                throw new IllegalArgumentException(String.format("Tool names must be unique. The tool '%s' appears several times", toolSpecification.name()));
             }
         }
     }

File: langchain4j-core/src/main/java/dev/langchain4j/internal/Exceptions.java
Patch:
@@ -16,7 +16,7 @@ private Exceptions() {}
      * @return the constructed exception.
      */
     public static IllegalArgumentException illegalArgument(String format, Object... args) {
-        return new IllegalArgumentException(format.formatted(args));
+        return new IllegalArgumentException(String.format(format, args));
     }
 
     /**
@@ -29,6 +29,6 @@ public static IllegalArgumentException illegalArgument(String format, Object...
      * @return the constructed exception.
      */
     public static RuntimeException runtime(String format, Object... args) {
-        return new RuntimeException(format.formatted(args));
+        return new RuntimeException(String.format(format, args));
     }
 }

File: langchain4j-core/src/main/java/dev/langchain4j/internal/RetryUtils.java
Patch:
@@ -195,7 +195,7 @@ public <T> T withRetry(Callable<T> action, int maxAttempts) {
                         throw new RuntimeException(e);
                     }
 
-                    log.warn("Exception was thrown on attempt %s of %s".formatted(attempt, maxAttempts), e);
+                    log.warn(String.format("Exception was thrown on attempt %s of %s", attempt, maxAttempts), e);
 
                     sleep(attempt);
                 }

File: langchain4j-core/src/main/java/dev/langchain4j/model/chat/StreamingChatLanguageModel.java
Patch:
@@ -82,7 +82,7 @@ public void onError(Throwable error) {
             if (parameters.toolChoice() == REQUIRED) {
                 if (toolSpecifications.size() != 1) {
                     throw new UnsupportedFeatureException(
-                            "%s.%s is currently supported only when there is a single tool".formatted(
+                            String.format("%s.%s is currently supported only when there is a single tool",
                                     ToolChoice.class.getSimpleName(), REQUIRED.name()));
                 }
                 generate(chatRequest.messages(), toolSpecifications.get(0), legacyHandler);

File: langchain4j-core/src/main/java/dev/langchain4j/rag/content/injector/DefaultContentInjector.java
Patch:
@@ -163,7 +163,7 @@ protected String format(Metadata metadata) {
     protected String format(String segmentContent, String segmentMetadata) {
         return segmentMetadata.isEmpty()
                 ? segmentContent
-                : "content: %s\n%s".formatted(segmentContent, segmentMetadata);
+                : String.format("content: %s\n%s", segmentContent, segmentMetadata);
     }
 
     public static class DefaultContentInjectorBuilder {

File: langchain4j-core/src/main/java/dev/langchain4j/model/chat/request/json/JsonSchemaElementHelper.java
Patch:
@@ -11,6 +11,7 @@
 import java.util.List;
 import java.util.Map;
 import java.util.Optional;
+import java.util.stream.Collectors;
 
 import static dev.langchain4j.internal.TypeUtils.isJsonBoolean;
 import static dev.langchain4j.internal.TypeUtils.isJsonInteger;
@@ -56,7 +57,7 @@ public static JsonSchemaElement jsonSchemaElementFrom(Class<?> clazz,
 
         if (clazz.isEnum()) {
             return JsonEnumSchema.builder()
-                    .enumValues(stream(clazz.getEnumConstants()).map(Object::toString).toList())
+                    .enumValues(stream(clazz.getEnumConstants()).map(Object::toString).collect(Collectors.toList()))
                     .description(Optional.ofNullable(fieldDescription).orElse(descriptionFrom(clazz)))
                     .build();
         }
@@ -265,7 +266,7 @@ public static Map<String, Object> toMap(JsonSchemaElement jsonSchemaElement, boo
             }
             List<Map<String, Object>> anyOf = jsonAnyOfSchema.anyOf().stream()
                     .map(element -> toMap(element, strict))
-                    .toList();
+                    .collect(Collectors.toList());
             properties.put("anyOf", anyOf);
             return properties;
         } else {

File: langchain4j-core/src/main/java/dev/langchain4j/rag/DefaultRetrievalAugmentor.java
Patch:
@@ -27,6 +27,7 @@
 import java.util.concurrent.Executors;
 import java.util.concurrent.SynchronousQueue;
 import java.util.concurrent.ThreadPoolExecutor;
+import java.util.stream.Collectors;
 
 import static dev.langchain4j.internal.Utils.getOrDefault;
 import static dev.langchain4j.internal.ValidationUtils.ensureNotNull;
@@ -209,13 +210,13 @@ private CompletableFuture<Collection<List<Content>>> retrieveFromAll(Collection<
                                                                          Query query) {
         List<CompletableFuture<List<Content>>> futureContents = retrievers.stream()
             .map(retriever -> supplyAsync(() -> retrieve(retriever, query), executor))
-            .toList();
+            .collect(Collectors.toList());
 
         return allOf(futureContents.toArray(new CompletableFuture[0]))
             .thenApply(ignored ->
                 futureContents.stream()
                     .map(CompletableFuture::join)
-                    .toList());
+                    .collect(Collectors.toList()));
     }
 
     private static List<Content> retrieve(ContentRetriever retriever, Query query) {

File: langchain4j-core/src/main/java/dev/langchain4j/rag/content/aggregator/ReRankingContentAggregator.java
Patch:
@@ -13,6 +13,7 @@
 import java.util.List;
 import java.util.Map;
 import java.util.function.Function;
+import java.util.stream.Collectors;
 
 import static dev.langchain4j.internal.Exceptions.illegalArgument;
 import static dev.langchain4j.internal.Utils.getOrDefault;
@@ -125,7 +126,7 @@ protected List<Content> reRankAndFilter(List<Content> contents, Query query) {
 
         List<TextSegment> segments = contents.stream()
                 .map(Content::textSegment)
-                .toList();
+                .collect(Collectors.toList());
 
         List<Double> scores = scoringModel.scoreAll(segments, query.text()).content();
 
@@ -139,7 +140,7 @@ protected List<Content> reRankAndFilter(List<Content> contents, Query query) {
                 .sorted(Map.Entry.<TextSegment, Double>comparingByValue().reversed())
                 .map(entry -> new Content(entry.getKey(), Map.of(ContentMetadata.RERANKED_SCORE, entry.getValue())))
                 .limit(maxResults)
-                .toList();
+                .collect(Collectors.toList());
     }
 
     public static class ReRankingContentAggregatorBuilder {

File: langchain4j-core/src/main/java/dev/langchain4j/rag/content/retriever/EmbeddingStoreContentRetriever.java
Patch:
@@ -16,6 +16,7 @@
 import java.util.List;
 import java.util.Map;
 import java.util.function.Function;
+import java.util.stream.Collectors;
 
 import static dev.langchain4j.internal.Utils.getOrDefault;
 import static dev.langchain4j.internal.ValidationUtils.ensureBetween;
@@ -247,7 +248,7 @@ public List<Content> retrieve(Query query) {
                                 ContentMetadata.EMBEDDING_ID, embeddingMatch.embeddingId()
                         )
                 ))
-                .toList();
+                .collect(Collectors.toList());
     }
 
     @Override

File: langchain4j-mcp/src/main/java/dev/langchain4j/mcp/client/protocol/ClientMethod.java
Patch:
@@ -10,5 +10,7 @@ public enum ClientMethod {
     @JsonProperty("tools/list")
     TOOLS_LIST,
     @JsonProperty("notifications/cancelled")
-    NOTIFICATION_CANCELLED
+    NOTIFICATION_CANCELLED,
+    @JsonProperty("notifications/initialized")
+    NOTIFICATION_INITIALIZED
 }

File: langchain4j-azure-open-ai/src/main/java/dev/langchain4j/model/azure/InternalAzureOpenAiHelper.java
Patch:
@@ -405,6 +405,7 @@ public static FinishReason finishReasonFrom(CompletionsFinishReason openAiFinish
     /**
      * Support for Responsible AI (content filtered by Azure OpenAI for violence, self harm, or hate).
      */
+    // TODO remove this?
     public static FinishReason contentFilterManagement(HttpResponseException httpResponseException, String contentFilterCode) {
         FinishReason exceptionFinishReason = FinishReason.OTHER;
         if (httpResponseException.getValue() instanceof Map) {

File: langchain4j-core/src/main/java/dev/langchain4j/model/chat/request/ResponseFormat.java
Patch:
@@ -10,6 +10,8 @@
 @Experimental
 public class ResponseFormat {
 
+    // TODO name: ChatResponseFormat?
+
     public static final ResponseFormat TEXT = ResponseFormat.builder().type(ResponseFormatType.TEXT).build();
     public static final ResponseFormat JSON = ResponseFormat.builder().type(ResponseFormatType.JSON).build();
 

File: langchain4j-core/src/main/java/dev/langchain4j/model/chat/request/json/JsonSchema.java
Patch:
@@ -12,7 +12,7 @@ public class JsonSchema {
     private final String name;
     private final JsonSchemaElement rootElement;
 
-    public JsonSchema(Builder builder) {
+    private JsonSchema(Builder builder) {
         this.name = builder.name;
         this.rootElement = builder.rootElement;
     }

File: langchain4j-core/src/test/java/dev/langchain4j/model/chat/ChatModelListenerIT.java
Patch:
@@ -25,13 +25,13 @@
  *
  * <dependency>
  *     <groupId>dev.langchain4j</groupId>
- *     <artifactId>langchain4j</artifactId>
+ *     <artifactId>langchain4j-core</artifactId>
  *     <scope>test</scope>
  * </dependency>
  *
  * <dependency>
  *     <groupId>dev.langchain4j</groupId>
- *     <artifactId>langchain4j</artifactId>
+ *     <artifactId>langchain4j-core</artifactId>
  *     <classifier>tests</classifier>
  *     <type>test-jar</type>
  *     <scope>test</scope>

File: langchain4j-core/src/test/java/dev/langchain4j/model/chat/StreamingChatModelListenerIT.java
Patch:
@@ -29,13 +29,13 @@
  *
  * <dependency>
  *     <groupId>dev.langchain4j</groupId>
- *     <artifactId>langchain4j</artifactId>
+ *     <artifactId>langchain4j-core</artifactId>
  *     <scope>test</scope>
  * </dependency>
  *
  * <dependency>
  *     <groupId>dev.langchain4j</groupId>
- *     <artifactId>langchain4j</artifactId>
+ *     <artifactId>langchain4j-core</artifactId>
  *     <classifier>tests</classifier>
  *     <type>test-jar</type>
  *     <scope>test</scope>

File: langchain4j-google-ai-gemini/src/test/java/dev/langchain4j/model/googleai/GoogleAiGeminiChatModelIT.java
Patch:
@@ -46,6 +46,8 @@
 
 import static dev.langchain4j.internal.Utils.readBytes;
 import static dev.langchain4j.model.chat.request.ResponseFormatType.JSON;
+import static dev.langchain4j.model.googleai.FinishReasonMapper.fromGFinishReasonToFinishReason;
+import static dev.langchain4j.model.googleai.GeminiFinishReason.*;
 import static dev.langchain4j.model.googleai.GeminiHarmBlockThreshold.BLOCK_LOW_AND_ABOVE;
 import static dev.langchain4j.model.googleai.GeminiHarmCategory.HARM_CATEGORY_HARASSMENT;
 import static dev.langchain4j.model.googleai.GeminiHarmCategory.HARM_CATEGORY_HATE_SPEECH;
@@ -431,7 +433,7 @@ void should_support_safety_settings() {
             .build());
 
         // then
-        assertThat(response.finishReason()).isEqualTo(FinishReasonMapper.fromGFinishReasonToFinishReason(GeminiFinishReason.SAFETY));
+        assertThat(response.finishReason()).isEqualTo(fromGFinishReasonToFinishReason(SAFETY));
     }
 
     @Test

File: langchain4j-ollama/src/test/java/dev/langchain4j/model/ollama/OllamaChatModelVisionIT.java
Patch:
@@ -10,6 +10,7 @@
 
 import java.time.Duration;
 
+import static dev.langchain4j.model.ollama.AbstractOllamaLanguageModelInfrastructure.ollamaBaseUrl;
 import static dev.langchain4j.model.ollama.OllamaImage.BAKLLAVA_MODEL;
 import static org.assertj.core.api.Assertions.assertThat;
 
@@ -22,7 +23,7 @@ void should_see_cat() {
 
         // given
         ChatLanguageModel model = OllamaChatModel.builder()
-                .baseUrl(ollamaBaseUrl())
+                .baseUrl(ollamaBaseUrl(ollama))
                 .timeout(Duration.ofMinutes(3))
                 .modelName(BAKLLAVA_MODEL)
                 .temperature(0.0)

File: langchain4j-ollama/src/test/java/dev/langchain4j/model/ollama/OllamaEmbeddingModelIT.java
Patch:
@@ -8,14 +8,15 @@
 
 import java.util.List;
 
+import static dev.langchain4j.model.ollama.AbstractOllamaLanguageModelInfrastructure.ollamaBaseUrl;
 import static dev.langchain4j.model.ollama.OllamaImage.ALL_MINILM_MODEL;
 import static java.util.Arrays.asList;
 import static org.assertj.core.api.Assertions.assertThat;
 
 class OllamaEmbeddingModelIT extends AbstractOllamaEmbeddingModelInfrastructure {
 
     EmbeddingModel model = OllamaEmbeddingModel.builder()
-            .baseUrl(ollamaBaseUrl())
+            .baseUrl(ollamaBaseUrl(ollama))
             .modelName(ALL_MINILM_MODEL)
             .build();
 

File: langchain4j-ollama/src/test/java/dev/langchain4j/model/ollama/OllamaLanguageModelIT.java
Patch:
@@ -12,7 +12,7 @@
 class OllamaLanguageModelIT extends AbstractOllamaLanguageModelInfrastructure {
 
     LanguageModel model = OllamaLanguageModel.builder()
-            .baseUrl(ollamaBaseUrl())
+            .baseUrl(ollamaBaseUrl(ollama))
             .modelName(TINY_DOLPHIN_MODEL)
             .temperature(0.0)
             .build();
@@ -37,7 +37,7 @@ void should_respect_numPredict() {
         int numPredict = 1; // max output tokens
 
         LanguageModel model = OllamaLanguageModel.builder()
-                .baseUrl(ollamaBaseUrl())
+                .baseUrl(ollamaBaseUrl(ollama))
                 .modelName(TINY_DOLPHIN_MODEL)
                 .numPredict(numPredict)
                 .temperature(0.0)
@@ -58,7 +58,7 @@ void should_generate_valid_json() {
 
         // given
         LanguageModel model = OllamaLanguageModel.builder()
-                .baseUrl(ollamaBaseUrl())
+                .baseUrl(ollamaBaseUrl(ollama))
                 .modelName(TINY_DOLPHIN_MODEL)
                 .format("json")
                 .temperature(0.0)

File: langchain4j-ollama/src/test/java/dev/langchain4j/model/ollama/OllamaOpenAiChatModelIT.java
Patch:
@@ -26,7 +26,7 @@ class OllamaOpenAiChatModelIT extends AbstractOllamaLanguageModelInfrastructure
 
     ChatLanguageModel model = OpenAiChatModel.builder()
             .apiKey("does not matter") // TODO make apiKey optional when using custom baseUrl?
-            .baseUrl(ollamaBaseUrl() + "/v1") // TODO add "/v1" by default?
+            .baseUrl(ollamaBaseUrl(ollama) + "/v1") // TODO add "/v1" by default?
             .modelName(TINY_DOLPHIN_MODEL)
             .temperature(0.0)
             .logRequests(true)

File: langchain4j-ollama/src/test/java/dev/langchain4j/model/ollama/OllamaOpenAiStreamingChatModelIT.java
Patch:
@@ -20,7 +20,7 @@ class OllamaOpenAiStreamingChatModelIT extends AbstractOllamaLanguageModelInfras
 
     StreamingChatLanguageModel model = OpenAiStreamingChatModel.builder()
             .apiKey("does not matter") // TODO make apiKey optional when using custom baseUrl?
-            .baseUrl(ollamaBaseUrl() + "/v1") // TODO add "/v1" by default?
+            .baseUrl(ollamaBaseUrl(ollama) + "/v1") // TODO add "/v1" by default?
             .modelName(TINY_DOLPHIN_MODEL)
             .temperature(0.0)
             .logRequests(true)

File: langchain4j-open-ai/src/test/java/dev/langchain4j/model/openai/OpenAiChatModelIT.java
Patch:
@@ -162,7 +162,7 @@ void should_execute_a_tool_then_answer() {
     void should_execute_tool_forcefully_then_answer() {
 
         // given
-        UserMessage userMessage = userMessage("2+2=?");
+        UserMessage userMessage = userMessage("I have 2 apples and 2 pears");
 
         // when
         Response<AiMessage> response = model.generate(singletonList(userMessage), calculator);
@@ -183,7 +183,7 @@ void should_execute_tool_forcefully_then_answer() {
         assertThat(tokenUsage.totalTokenCount())
                 .isEqualTo(tokenUsage.inputTokenCount() + tokenUsage.outputTokenCount());
 
-        assertThat(response.finishReason()).isEqualTo(STOP); // not sure if a bug in OpenAI or stop is expected here
+        assertThat(response.finishReason()).isEqualTo(TOOL_EXECUTION);
 
         // given
         ToolExecutionResultMessage toolExecutionResultMessage = from(toolExecutionRequest, "4");
@@ -198,7 +198,7 @@ void should_execute_tool_forcefully_then_answer() {
         assertThat(secondAiMessage.toolExecutionRequests()).isNull();
 
         TokenUsage secondTokenUsage = secondResponse.tokenUsage();
-        assertThat(secondTokenUsage.inputTokenCount()).isEqualTo(37);
+        assertThat(secondTokenUsage.inputTokenCount()).isGreaterThan(0);
         assertThat(secondTokenUsage.outputTokenCount()).isGreaterThan(0);
         assertThat(secondTokenUsage.totalTokenCount())
                 .isEqualTo(secondTokenUsage.inputTokenCount() + secondTokenUsage.outputTokenCount());

File: langchain4j-open-ai/src/test/java/dev/langchain4j/model/openai/OpenAiStreamingChatModelIT.java
Patch:
@@ -224,7 +224,7 @@ public void onError(Throwable error) {
 
         assertTokenUsage(response.tokenUsage());
 
-        assertThat(response.finishReason()).isEqualTo(STOP); // not sure if a bug in OpenAI or stop is expected here
+        assertThat(response.finishReason()).isEqualTo(TOOL_EXECUTION);
 
         // given
         ToolExecutionResultMessage toolExecutionResultMessage = from(toolExecutionRequest, "4");

File: langchain4j/src/test/java/dev/langchain4j/service/AiServicesWithJsonSchemaIT.java
Patch:
@@ -33,6 +33,7 @@
 import static org.mockito.Mockito.verify;
 
 public abstract class AiServicesWithJsonSchemaIT {
+    // TODO move to common, use parameterized tests
 
     protected abstract List<ChatLanguageModel> models();
 

File: langchain4j-azure-open-ai/src/test/java/dev/langchain4j/model/azure/AzureOpenAiChatModelIT.java
Patch:
@@ -307,7 +307,7 @@ void should_use_json_format(String deploymentName, String gptVersion) {
     }
 
     /**
-     * @deprecated Shjould be removed when `AzureOpenAiChatModel.responseFormat(ChatCompletionsResponseFormat chatCompletionsResponseFormat)` is removed.
+     * @deprecated Should be removed when `AzureOpenAiChatModel.responseFormat(ChatCompletionsResponseFormat chatCompletionsResponseFormat)` is removed.
      */
     @ParameterizedTest(name = "Deployment name {0} using {1}")
     @CsvSource({

File: langchain4j-ollama/src/main/java/dev/langchain4j/model/ollama/OllamaChatModel.java
Patch:
@@ -31,7 +31,7 @@
 import static dev.langchain4j.model.ollama.OllamaMessagesUtils.toOllamaMessages;
 import static dev.langchain4j.model.ollama.OllamaMessagesUtils.toOllamaResponseFormat;
 import static dev.langchain4j.model.ollama.OllamaMessagesUtils.toOllamaTools;
-import static dev.langchain4j.model.ollama.OllamaMessagesUtils.toToolExecutionRequest;
+import static dev.langchain4j.model.ollama.OllamaMessagesUtils.toToolExecutionRequests;
 import static dev.langchain4j.spi.ServiceHelper.loadFactories;
 import static java.time.Duration.ofSeconds;
 import static java.util.Arrays.asList;
@@ -160,7 +160,7 @@ private Response<AiMessage> doGenerate(List<ChatMessage> messages, List<ToolSpec
             ChatResponse chatResponse = withRetry(() -> client.chat(request), maxRetries);
             Response<AiMessage> response = Response.from(
                     chatResponse.getMessage().getToolCalls() != null ?
-                            AiMessage.from(toToolExecutionRequest(chatResponse.getMessage().getToolCalls())) :
+                            AiMessage.from(toToolExecutionRequests(chatResponse.getMessage().getToolCalls())) :
                             AiMessage.from(chatResponse.getMessage().getContent()),
                     new TokenUsage(chatResponse.getPromptEvalCount(), chatResponse.getEvalCount())
             );

File: langchain4j-ollama/src/main/java/dev/langchain4j/model/ollama/OllamaMessagesUtils.java
Patch:
@@ -78,13 +78,13 @@ private static Parameters toOllamaParameters(ToolSpecification toolSpecification
         }
     }
 
-    static List<ToolExecutionRequest> toToolExecutionRequest(List<ToolCall> toolCalls) {
+    static List<ToolExecutionRequest> toToolExecutionRequests(List<ToolCall> toolCalls) {
         return toolCalls.stream().map(toolCall ->
                         ToolExecutionRequest.builder()
                                 .name(toolCall.getFunction().getName())
                                 .arguments(toJson(toolCall.getFunction().getArguments()))
                                 .build())
-                .collect(Collectors.toList());
+                .toList();
     }
 
     static String toOllamaResponseFormat(ResponseFormat responseFormat) {

File: langchain4j-anthropic/src/main/java/dev/langchain4j/model/anthropic/internal/sanitizer/MessageSanitizer.java
Patch:
@@ -12,11 +12,12 @@
 
 /**
  * Sanitizes the messages to conform to the format expected by the Anthropic API.
+ * This class is equal to the BedrockAnthropicMessageSanitizer class in the dev.langchain4j.model.bedrock.internal.sanitizer package.
+ * When it is changed in one place, it should be changed in the other place as well.
  */
 @Slf4j
 public class MessageSanitizer {
 
-
     public static List<ChatMessage> sanitizeMessages(List<ChatMessage> messages) {
         ensureNotEmpty(messages, "messages");
         List<ChatMessage> sanitizedMessages = new ArrayList<>(messages);

File: langchain4j-mistral-ai/src/main/java/dev/langchain4j/model/mistralai/internal/api/MistralAiChatMessage.java
Patch:
@@ -25,4 +25,5 @@ public class MistralAiChatMessage {
     private String content;
     private String name;
     private List<MistralAiToolCall> toolCalls;
+    private String toolCallId;
 }

File: langchain4j-mistral-ai/src/main/java/dev/langchain4j/model/mistralai/internal/mapper/MistralAiMapper.java
Patch:
@@ -90,6 +90,7 @@ static MistralAiChatMessage toMistralAiMessage(ChatMessage message) {
         if (message instanceof ToolExecutionResultMessage) {
             return MistralAiChatMessage.builder()
                     .role(MistralAiRole.TOOL)
+                    .toolCallId(((ToolExecutionResultMessage) message).id())
                     .name(((ToolExecutionResultMessage) message).toolName())
                     .content(((ToolExecutionResultMessage) message).text())
                     .build();

File: langchain4j-mcp/src/main/java/dev/langchain4j/mcp/client/protocol/ClientMethod.java
Patch:
@@ -8,5 +8,7 @@ public enum ClientMethod {
     @JsonProperty("tools/call")
     TOOLS_CALL,
     @JsonProperty("tools/list")
-    TOOLS_LIST
+    TOOLS_LIST,
+    @JsonProperty("notifications/cancelled")
+    NOTIFICATION_CANCELLED
 }

File: langchain4j-mcp/src/main/java/dev/langchain4j/mcp/client/protocol/McpClientMessage.java
Patch:
@@ -7,12 +7,10 @@ public class McpClientMessage {
     @JsonInclude
     public final String jsonrpc = "2.0";
 
+    @JsonInclude(JsonInclude.Include.NON_NULL)
     private Long id;
 
     public McpClientMessage(Long id) {
-        if (id == null) {
-            throw new IllegalArgumentException("id must not be null");
-        }
         this.id = id;
     }
 

File: langchain4j-core/src/test/java/dev/langchain4j/store/embedding/EmbeddingStoreTest.java
Patch:
@@ -30,8 +30,8 @@ public List<String> addAll(List<Embedding> embeddings) {
         }
 
         @Override
-        public List<String> addAll(List<Embedding> embeddings, List<String> embedded) {
-            return null;
+        public void addAll(List<String> ids, List<Embedding> embeddings, List<String> embedded) {
+            System.out.println("addAll method executed");
         }
 
         @Override
@@ -103,4 +103,4 @@ public void test_memoryId() {
                                 referenceEmbedding,
                                 "abc, [0.5, 1.5], 12, 0.00"));
     }
-}
\ No newline at end of file
+}

File: langchain4j-core/src/main/java/dev/langchain4j/model/chat/response/ChatResponse.java
Patch:
@@ -4,6 +4,7 @@
 import dev.langchain4j.data.message.AiMessage;
 import dev.langchain4j.model.output.FinishReason;
 import dev.langchain4j.model.output.TokenUsage;
+import org.jspecify.annotations.NonNull;
 
 import java.util.Objects;
 
@@ -16,7 +17,7 @@ public class ChatResponse {
     private final TokenUsage tokenUsage;
     private final FinishReason finishReason;
 
-    private ChatResponse(Builder builder) {
+    private ChatResponse(@NonNull Builder builder) {
         this.aiMessage = ensureNotNull(builder.aiMessage, "aiMessage");
         this.tokenUsage = builder.tokenUsage;
         this.finishReason = builder.finishReason;

File: langchain4j-core/src/test/java/dev/langchain4j/store/embedding/TestUtils.java
Patch:
@@ -5,7 +5,7 @@
 
 import java.time.Duration;
 
-public class TestUtils {
+public final class TestUtils {
     private TestUtils() {
     }
 

File: langchain4j-mongodb-atlas/src/main/java/dev/langchain4j/store/embedding/mongodb/MongoDbMatchedDocument.java
Patch:
@@ -9,7 +9,7 @@ public class MongoDbMatchedDocument {
     private String id;
     private List<Float> embedding;
     private String text;
-    private Map<String, String> metadata;
+    private Map<String, Object> metadata;
     private Double score;
 
     public MongoDbMatchedDocument() {
@@ -47,11 +47,11 @@ public void setText(String text) {
         this.text = text;
     }
 
-    public Map<String, String> getMetadata() {
+    public Map<String, Object> getMetadata() {
         return metadata;
     }
 
-    public void setMetadata(Map<String, String> metadata) {
+    public void setMetadata(Map<String, Object> metadata) {
         this.metadata = metadata;
     }
 

File: langchain4j-core/src/main/java/dev/langchain4j/model/chat/request/json/JsonSchemaElement.java
Patch:
@@ -5,6 +5,7 @@
 /**
  * A base interface for a JSON schema element.
  *
+ * @see JsonAnyOfSchema
  * @see JsonArraySchema
  * @see JsonBooleanSchema
  * @see JsonEnumSchema

File: langchain4j-open-ai/src/main/java/dev/langchain4j/model/openai/OpenAiTokenizer.java
Patch:
@@ -49,6 +49,7 @@ public class OpenAiTokenizer implements Tokenizer {
     /**
      * Creates an instance of the {@code OpenAiTokenizer} for the "gpt-3.5-turbo" model.
      * <s>It should be suitable for all current OpenAI models, as they all use the same cl100k_base encoding.</s>
+     *
      * @deprecated Please use other constructors and specify the model name explicitly.
      */
     @Deprecated(forRemoval = true)

File: langchain4j-open-ai/src/test/java/dev/langchain4j/model/openai/OpenAiChatModelIT.java
Patch:
@@ -14,6 +14,7 @@
 import dev.langchain4j.model.output.Response;
 import dev.langchain4j.model.output.TokenUsage;
 import org.junit.jupiter.api.Test;
+import org.junit.jupiter.api.condition.EnabledIfEnvironmentVariable;
 
 import java.util.Base64;
 import java.util.List;
@@ -30,6 +31,7 @@
 import static java.util.Collections.singletonList;
 import static org.assertj.core.api.Assertions.assertThat;
 
+@EnabledIfEnvironmentVariable(named = "OPENAI_API_KEY", matches = ".+")
 class OpenAiChatModelIT {
 
     static final String CAT_IMAGE_URL = "https://upload.wikimedia.org/wikipedia/commons/e/e9/Felis_silvestris_silvestris_small_gradual_decrease_of_quality.png";

File: langchain4j-open-ai/src/test/java/dev/langchain4j/model/openai/OpenAiChatModelListenerIT.java
Patch:
@@ -4,10 +4,12 @@
 import dev.langchain4j.model.chat.ChatLanguageModel;
 import dev.langchain4j.model.chat.ChatModelListenerIT;
 import dev.langchain4j.model.chat.listener.ChatModelListener;
+import org.junit.jupiter.api.condition.EnabledIfEnvironmentVariable;
 
 import static dev.langchain4j.model.openai.OpenAiChatModelName.GPT_4_O_MINI;
 import static java.util.Collections.singletonList;
 
+@EnabledIfEnvironmentVariable(named = "OPENAI_API_KEY", matches = ".+")
 class OpenAiChatModelListenerIT extends ChatModelListenerIT {
 
     @Override

File: langchain4j-open-ai/src/test/java/dev/langchain4j/model/openai/OpenAiEmbeddingModelIT.java
Patch:
@@ -6,13 +6,15 @@
 import dev.langchain4j.model.output.Response;
 import dev.langchain4j.model.output.TokenUsage;
 import org.junit.jupiter.api.Test;
+import org.junit.jupiter.api.condition.EnabledIfEnvironmentVariable;
 
 import java.util.List;
 
 import static dev.langchain4j.model.openai.OpenAiEmbeddingModelName.TEXT_EMBEDDING_3_SMALL;
 import static java.util.Arrays.asList;
 import static org.assertj.core.api.Assertions.assertThat;
 
+@EnabledIfEnvironmentVariable(named = "OPENAI_API_KEY", matches = ".+")
 class OpenAiEmbeddingModelIT {
 
     EmbeddingModel model = OpenAiEmbeddingModel.builder()
@@ -100,4 +102,4 @@ void should_embed_text_with_embedding_shortening() {
 
         assertThat(response.finishReason()).isNull();
     }
-}
\ No newline at end of file
+}

File: langchain4j-open-ai/src/test/java/dev/langchain4j/model/openai/OpenAiLanguageModelIT.java
Patch:
@@ -4,11 +4,13 @@
 import dev.langchain4j.model.output.Response;
 import dev.langchain4j.model.output.TokenUsage;
 import org.junit.jupiter.api.Test;
+import org.junit.jupiter.api.condition.EnabledIfEnvironmentVariable;
 
 import static dev.langchain4j.model.openai.OpenAiLanguageModelName.GPT_3_5_TURBO_INSTRUCT;
 import static dev.langchain4j.model.output.FinishReason.STOP;
 import static org.assertj.core.api.Assertions.assertThat;
 
+@EnabledIfEnvironmentVariable(named = "OPENAI_API_KEY", matches = ".+")
 class OpenAiLanguageModelIT {
 
     LanguageModel model = OpenAiLanguageModel.builder()
@@ -56,4 +58,4 @@ void should_use_enum_as_model_name() {
         // then
         assertThat(response).containsIgnoringCase("Berlin");
     }
-}
\ No newline at end of file
+}

File: langchain4j-open-ai/src/test/java/dev/langchain4j/model/openai/OpenAiStreamingChatModelIT.java
Patch:
@@ -16,6 +16,7 @@
 import dev.langchain4j.model.output.Response;
 import dev.langchain4j.model.output.TokenUsage;
 import org.junit.jupiter.api.Test;
+import org.junit.jupiter.api.condition.EnabledIfEnvironmentVariable;
 import org.junit.jupiter.params.ParameterizedTest;
 import org.junit.jupiter.params.provider.EnumSource;
 
@@ -38,6 +39,7 @@
 import static org.assertj.core.api.Assertions.assertThat;
 import static org.junit.jupiter.params.provider.EnumSource.Mode.EXCLUDE;
 
+@EnabledIfEnvironmentVariable(named = "OPENAI_API_KEY", matches = ".+")
 class OpenAiStreamingChatModelIT {
 
     OpenAiStreamingChatModel model = OpenAiStreamingChatModel.builder()

File: langchain4j-open-ai/src/test/java/dev/langchain4j/model/openai/OpenAiStreamingChatModelListenerIT.java
Patch:
@@ -4,10 +4,12 @@
 import dev.langchain4j.model.chat.StreamingChatLanguageModel;
 import dev.langchain4j.model.chat.StreamingChatModelListenerIT;
 import dev.langchain4j.model.chat.listener.ChatModelListener;
+import org.junit.jupiter.api.condition.EnabledIfEnvironmentVariable;
 
 import static dev.langchain4j.model.openai.OpenAiChatModelName.GPT_4_O_MINI;
 import static java.util.Collections.singletonList;
 
+@EnabledIfEnvironmentVariable(named = "OPENAI_API_KEY", matches = ".+")
 class OpenAiStreamingChatModelListenerIT extends StreamingChatModelListenerIT {
 
     @Override
@@ -45,4 +47,4 @@ protected StreamingChatLanguageModel createFailingModel(ChatModelListener listen
     protected Class<? extends Exception> expectedExceptionClass() {
         return OpenAiHttpException.class;
     }
-}
\ No newline at end of file
+}

File: langchain4j-open-ai/src/test/java/dev/langchain4j/model/openai/OpenAiStreamingLanguageModelIT.java
Patch:
@@ -6,6 +6,7 @@
 import dev.langchain4j.model.output.Response;
 import dev.langchain4j.model.output.TokenUsage;
 import org.junit.jupiter.api.Test;
+import org.junit.jupiter.api.condition.EnabledIfEnvironmentVariable;
 
 import java.util.concurrent.CompletableFuture;
 
@@ -14,6 +15,7 @@
 import static java.util.concurrent.TimeUnit.SECONDS;
 import static org.assertj.core.api.Assertions.assertThat;
 
+@EnabledIfEnvironmentVariable(named = "OPENAI_API_KEY", matches = ".+")
 class OpenAiStreamingLanguageModelIT {
 
     StreamingLanguageModel model = OpenAiStreamingLanguageModel.builder()
@@ -90,4 +92,4 @@ void should_use_enum_as_model_name() {
         // then
         assertThat(response.content()).containsIgnoringCase("Berlin");
     }
-}
\ No newline at end of file
+}

File: langchain4j/src/main/java/dev/langchain4j/classification/TextClassifier.java
Patch:
@@ -23,8 +23,8 @@ public interface TextClassifier<L> {
      */
     default List<L> classify(String text) {
         return classifyWithScores(text).scoredLabels().stream()
-            .map(ScoredLabel::label)
-            .collect(toList());
+                .map(ScoredLabel::label)
+                .collect(toList());
     }
 
     /**

File: langchain4j/src/main/java/dev/langchain4j/data/document/loader/UrlDocumentLoader.java
Patch:
@@ -35,6 +35,7 @@ public static Document load(String url, DocumentParser documentParser) {
 
     /**
      * Creates a URL from the specified string.
+     *
      * @param url The URL string.
      * @return the URL
      * @throws IllegalArgumentException If specified URL is malformed.

File: langchain4j/src/main/java/dev/langchain4j/service/AiServices.java
Patch:
@@ -398,15 +398,14 @@ public AiServices<T> tools(Map<ToolSpecification, ToolExecutor> tools) {
     }
 
     /**
+     * @param retriever The retriever to be used by the AI Service.
+     * @return builder
      * @deprecated Use {@link #contentRetriever(ContentRetriever)}
      * (e.g. {@link EmbeddingStoreContentRetriever}) instead.
      * <br>
      * Configures a retriever that will be invoked on every method call to fetch relevant information
      * related to the current user message from an underlying source (e.g., embedding store).
      * This relevant information is automatically injected into the message sent to the LLM.
-     *
-     * @param retriever The retriever to be used by the AI Service.
-     * @return builder
      */
     @Deprecated(forRemoval = true)
     public AiServices<T> retriever(Retriever<TextSegment> retriever) {

File: langchain4j/src/main/java/dev/langchain4j/service/Result.java
Patch:
@@ -4,7 +4,6 @@
 import dev.langchain4j.model.output.TokenUsage;
 import dev.langchain4j.rag.content.Content;
 import dev.langchain4j.service.tool.ToolExecution;
-
 import java.util.List;
 
 import static dev.langchain4j.internal.Utils.copyIfNotNull;

File: langchain4j/src/main/java/dev/langchain4j/service/output/OutputParser.java
Patch:
@@ -2,19 +2,22 @@
 
 /**
  * Represents an output parser.
+ *
  * @param <T> the type of the output.
  */
 interface OutputParser<T> {
 
     /**
      * Parse the given text.
+     *
      * @param text the text to parse.
      * @return the parsed output.
      */
     T parse(String text);
 
     /**
      * Description of the text format.
+     *
      * @return the description of the text format.
      */
     String formatInstructions();

File: langchain4j/src/main/java/dev/langchain4j/service/output/ServiceOutputParser.java
Patch:
@@ -170,8 +170,7 @@ private static String descriptionFor(Field field, Set<Class<?>> visited) {
     private static String typeOf(Field field, Set<Class<?>> visited) {
         Type type = field.getGenericType();
 
-        if (type instanceof ParameterizedType) {
-            ParameterizedType parameterizedType = (ParameterizedType) type;
+        if (type instanceof ParameterizedType parameterizedType) {
             Type[] typeArguments = parameterizedType.getActualTypeArguments();
 
             if (parameterizedType.getRawType().equals(List.class)

File: langchain4j/src/test/java/dev/langchain4j/data/document/loader/UrlDocumentLoaderTest.java
Patch:
@@ -11,8 +11,8 @@ void test_bad_url() {
         String url = "bad_url";
 
         assertThatExceptionOfType(IllegalArgumentException.class)
-            .isThrownBy(() -> UrlDocumentLoader.load(url, new TextDocumentParser()))
-            .withMessageContaining("no protocol");
+                .isThrownBy(() -> UrlDocumentLoader.load(url, new TextDocumentParser()))
+                .withMessageContaining("no protocol");
     }
 
     @Test
@@ -24,4 +24,4 @@ void should_load_text_document() {
         assertThat(document.text()).isEqualTo("test\ncontent");
         assertThat(document.metadata("url")).isEqualTo(url);
     }
-}
\ No newline at end of file
+}

File: langchain4j/src/test/java/dev/langchain4j/data/document/splitter/HierarchicalDocumentSplitterTest.java
Patch:
@@ -41,7 +41,7 @@ protected DocumentSplitter defaultSubSplitter() {
     }
 
     @Test
-    public void test_constructor() {
+    void test_constructor() {
         {
             ExampleImpl splitter = new ExampleImpl(1, 1);
             assertThat(splitter.maxSegmentSize).isEqualTo(1);
@@ -83,4 +83,4 @@ public void test_constructor() {
             assertThat(splitter.estimateSize("abc def")).isEqualTo(2);
         }
     }
-}
\ No newline at end of file
+}

File: langchain4j/src/test/java/dev/langchain4j/data/document/splitter/SegmentBuilderTest.java
Patch:
@@ -15,7 +15,7 @@ void shouldAppendText() {
     }
 
     @Test
-    public void test_by_words() {
+    void test_by_words() {
         SegmentBuilder builder = new SegmentBuilder(10,
                 text -> text.split(" ").length,
                 " ; ");

File: langchain4j/src/test/java/dev/langchain4j/exception/IllegalConfigurationExceptionTest.java
Patch:
@@ -5,10 +5,10 @@
 
 class IllegalConfigurationExceptionTest implements WithAssertions {
     @Test
-    public void test_constructors() {
+    void test_constructors() {
         assertThat(new IllegalConfigurationException("message abc 123"))
                 .isInstanceOf(IllegalConfigurationException.class)
-                        .hasMessage("message abc 123");
+                .hasMessage("message abc 123");
 
         assertThat(IllegalConfigurationException.illegalConfiguration("message abc 123"))
                 .isInstanceOf(IllegalConfigurationException.class)
@@ -18,4 +18,4 @@ public void test_constructors() {
                 .isInstanceOf(IllegalConfigurationException.class)
                 .hasMessage("message abc 123");
     }
-}
\ No newline at end of file
+}

File: langchain4j/src/test/java/dev/langchain4j/service/AiServicesIT.java
Patch:
@@ -861,8 +861,8 @@ void should_return_result() {
 
         TokenUsage tokenUsage = result.tokenUsage();
         assertThat(tokenUsage).isNotNull();
-        assertThat(tokenUsage.inputTokenCount()).isGreaterThan(0);
-        assertThat(tokenUsage.outputTokenCount()).isGreaterThan(0);
+        assertThat(tokenUsage.inputTokenCount()).isPositive();
+        assertThat(tokenUsage.outputTokenCount()).isPositive();
         assertThat(tokenUsage.totalTokenCount())
                 .isEqualTo(tokenUsage.inputTokenCount() + tokenUsage.outputTokenCount());
 

File: langchain4j/src/test/java/dev/langchain4j/service/tool/ToolExecutionRequestUtilTest.java
Patch:
@@ -12,9 +12,8 @@
 import java.util.Map;
 import java.util.stream.Stream;
 
-import static org.assertj.core.api.Assertions.assertThat;
-import static org.junit.jupiter.api.Assertions.assertTrue;
 import static org.junit.jupiter.api.Assertions.assertInstanceOf;
+import static org.junit.jupiter.api.Assertions.assertTrue;
 
 class ToolExecutionRequestUtilTest implements WithAssertions {
 
@@ -140,4 +139,4 @@ private static Stream<Arguments> should_remove_trailing_comma() {
                 )
         );
     }
-}
\ No newline at end of file
+}

File: langchain4j/src/test/java/dev/langchain4j/store/embedding/inmemory/InMemoryEmbeddingStoreRemovalTest.java
Patch:
@@ -1,8 +1,8 @@
 package dev.langchain4j.store.embedding.inmemory;
 
 import dev.langchain4j.data.segment.TextSegment;
-import dev.langchain4j.model.embedding.onnx.allminilml6v2q.AllMiniLmL6V2QuantizedEmbeddingModel;
 import dev.langchain4j.model.embedding.EmbeddingModel;
+import dev.langchain4j.model.embedding.onnx.allminilml6v2q.AllMiniLmL6V2QuantizedEmbeddingModel;
 import dev.langchain4j.store.embedding.EmbeddingStore;
 import dev.langchain4j.store.embedding.EmbeddingStoreWithRemovalIT;
 

File: langchain4j/src/test/java/dev/langchain4j/store/embedding/inmemory/InMemoryEmbeddingStoreTest.java
Patch:
@@ -3,8 +3,8 @@
 import dev.langchain4j.data.document.Metadata;
 import dev.langchain4j.data.embedding.Embedding;
 import dev.langchain4j.data.segment.TextSegment;
-import dev.langchain4j.model.embedding.onnx.allminilml6v2q.AllMiniLmL6V2QuantizedEmbeddingModel;
 import dev.langchain4j.model.embedding.EmbeddingModel;
+import dev.langchain4j.model.embedding.onnx.allminilml6v2q.AllMiniLmL6V2QuantizedEmbeddingModel;
 import dev.langchain4j.store.embedding.EmbeddingMatch;
 import dev.langchain4j.store.embedding.EmbeddingStore;
 import dev.langchain4j.store.embedding.EmbeddingStoreWithFilteringIT;

File: langchain4j-azure-ai-search/src/test/java/dev/langchain4j/rag/content/retriever/azure/search/AzureAiSearchContentRetrieverRemovalIT.java
Patch:
@@ -26,7 +26,7 @@ public class AzureAiSearchContentRetrieverRemovalIT extends EmbeddingStoreWithRe
     private final AzureAiSearchContentRetriever contentRetrieverWithVector = AzureAiSearchContentRetriever.builder()
             .endpoint(System.getenv("AZURE_SEARCH_ENDPOINT"))
             .apiKey(System.getenv("AZURE_SEARCH_KEY"))
-            .indexName(randomUUID())
+            .indexName("aaa" + randomUUID())
             .dimensions(embeddingModel.dimension())
             .embeddingModel(embeddingModel)
             .queryType(HYBRID)

File: langchain4j-azure-ai-search/src/test/java/dev/langchain4j/store/embedding/azure/search/AzureAiSearchEmbeddingStoreIT.java
Patch:
@@ -42,7 +42,7 @@ public class AzureAiSearchEmbeddingStoreIT extends EmbeddingStoreWithFilteringIT
     private final AzureAiSearchEmbeddingStore embeddingStore = AzureAiSearchEmbeddingStore.builder()
             .endpoint(AZURE_SEARCH_ENDPOINT)
             .apiKey(AZURE_SEARCH_KEY)
-            .indexName(randomUUID())
+            .indexName("ccc" + randomUUID())
             .dimensions(embeddingModel.dimension())
             .build();
 

File: langchain4j-azure-ai-search/src/test/java/dev/langchain4j/store/embedding/azure/search/AzureAiSearchEmbeddingStoreRemovalIT.java
Patch:
@@ -25,7 +25,7 @@ public class AzureAiSearchEmbeddingStoreRemovalIT extends EmbeddingStoreWithRemo
     private final AzureAiSearchEmbeddingStore embeddingStore = AzureAiSearchEmbeddingStore.builder()
             .endpoint(System.getenv("AZURE_SEARCH_ENDPOINT"))
             .apiKey(System.getenv("AZURE_SEARCH_KEY"))
-            .indexName(randomUUID())
+            .indexName("bbb" + randomUUID())
             .dimensions(embeddingModel.dimension())
             .build();
 

File: langchain4j-github-models/src/test/java/dev/langchain4j/model/github/GitHubModelsAiServicesWithToolsIT.java
Patch:
@@ -2,13 +2,15 @@
 
 import dev.langchain4j.model.chat.ChatLanguageModel;
 import dev.langchain4j.service.AiServicesWithNewToolsIT;
+import org.junit.jupiter.api.condition.EnabledIfEnvironmentVariable;
 
 import java.util.List;
 
 import static dev.langchain4j.model.github.GitHubModelsChatModelName.GPT_4_O_MINI;
 import static java.util.Collections.singletonList;
 
-class GithubModelsAiServicesWithToolsIT extends AiServicesWithNewToolsIT {
+@EnabledIfEnvironmentVariable(named = "GITHUB_TOKEN", matches = ".+")
+class GitHubModelsAiServicesWithToolsIT extends AiServicesWithNewToolsIT {
 
     @Override
     protected List<ChatLanguageModel> models() {

File: langchain4j/src/main/java/dev/langchain4j/data/document/loader/FileSystemDocumentLoader.java
Patch:
@@ -1,7 +1,7 @@
 package dev.langchain4j.data.document.loader;
 
-import dev.langchain4j.data.document.Document;
 import dev.langchain4j.data.document.BlankDocumentException;
+import dev.langchain4j.data.document.Document;
 import dev.langchain4j.data.document.DocumentLoader;
 import dev.langchain4j.data.document.DocumentParser;
 import dev.langchain4j.data.document.parser.TextDocumentParser;

File: langchain4j/src/main/java/dev/langchain4j/data/document/parser/TextDocumentParser.java
Patch:
@@ -1,14 +1,12 @@
 package dev.langchain4j.data.document.parser;
 
-import dev.langchain4j.data.document.Document;
 import dev.langchain4j.data.document.BlankDocumentException;
+import dev.langchain4j.data.document.Document;
 import dev.langchain4j.data.document.DocumentParser;
 
-import java.io.ByteArrayOutputStream;
 import java.io.InputStream;
 import java.nio.charset.Charset;
 
-import static dev.langchain4j.internal.Utils.isNullOrBlank;
 import static dev.langchain4j.internal.ValidationUtils.ensureNotNull;
 import static java.nio.charset.StandardCharsets.UTF_8;
 

File: langchain4j/src/test/java/dev/langchain4j/data/document/splitter/SegmentBuilderTest.java
Patch:
@@ -51,7 +51,7 @@ public void test_reset() {
         builder.reset();
 
         assertThat(builder.isNotEmpty()).isFalse();
-        assertThat(builder.getSize()).isEqualTo(0);
+        assertThat(builder.getSize()).isZero();
         assertThat(builder.toString()).isEqualTo("");
     }
 
@@ -70,4 +70,4 @@ public void test_append_prepend() {
             assertThat(builder.toString()).isEqualTo("Hello world");
         }
     }
-}
\ No newline at end of file
+}

File: langchain4j/src/test/java/dev/langchain4j/rag/query/router/LanguageModelQueryRouterIT.java
Patch:
@@ -4,6 +4,7 @@
 import dev.langchain4j.model.openai.OpenAiChatModel;
 import dev.langchain4j.rag.content.retriever.ContentRetriever;
 import dev.langchain4j.rag.query.Query;
+import org.junit.jupiter.api.condition.EnabledIfEnvironmentVariable;
 import org.junit.jupiter.api.extension.ExtendWith;
 import org.junit.jupiter.params.ParameterizedTest;
 import org.junit.jupiter.params.provider.Arguments;
@@ -19,6 +20,7 @@
 import static org.assertj.core.api.Assertions.assertThat;
 
 @ExtendWith(MockitoExtension.class)
+@EnabledIfEnvironmentVariable(named = "OPENAI_API_KEY", matches = ".+")
 class LanguageModelQueryRouterIT {
 
     @Mock

File: langchain4j/src/test/java/dev/langchain4j/rag/query/transformer/CompressingQueryTransformerIT.java
Patch:
@@ -7,6 +7,7 @@
 import dev.langchain4j.model.openai.OpenAiChatModel;
 import dev.langchain4j.rag.query.Metadata;
 import dev.langchain4j.rag.query.Query;
+import org.junit.jupiter.api.condition.EnabledIfEnvironmentVariable;
 import org.junit.jupiter.params.ParameterizedTest;
 import org.junit.jupiter.params.provider.Arguments;
 import org.junit.jupiter.params.provider.MethodSource;
@@ -18,6 +19,7 @@
 import static java.util.Arrays.asList;
 import static org.assertj.core.api.Assertions.assertThat;
 
+@EnabledIfEnvironmentVariable(named = "OPENAI_API_KEY", matches = ".+")
 class CompressingQueryTransformerIT {
 
     @ParameterizedTest

File: langchain4j/src/test/java/dev/langchain4j/rag/query/transformer/ExpandingQueryTransformerIT.java
Patch:
@@ -3,6 +3,7 @@
 import dev.langchain4j.model.chat.ChatLanguageModel;
 import dev.langchain4j.model.openai.OpenAiChatModel;
 import dev.langchain4j.rag.query.Query;
+import org.junit.jupiter.api.condition.EnabledIfEnvironmentVariable;
 import org.junit.jupiter.params.ParameterizedTest;
 import org.junit.jupiter.params.provider.Arguments;
 import org.junit.jupiter.params.provider.MethodSource;
@@ -12,6 +13,7 @@
 
 import static org.assertj.core.api.Assertions.assertThat;
 
+@EnabledIfEnvironmentVariable(named = "OPENAI_API_KEY", matches = ".+")
 class ExpandingQueryTransformerIT {
 
     @ParameterizedTest
@@ -49,4 +51,4 @@ static Stream<Arguments> should_expand_query() {
                 // TODO add more models
         );
     }
-}
\ No newline at end of file
+}

File: langchain4j/src/test/java/dev/langchain4j/service/AiServicesWithJsonSchemaWithDescriptionsIT.java
Patch:
@@ -16,6 +16,7 @@
 import dev.langchain4j.model.output.structured.Description;
 import org.junit.jupiter.api.AfterEach;
 import org.junit.jupiter.api.Test;
+import org.junit.jupiter.api.condition.EnabledIfEnvironmentVariable;
 import org.junit.jupiter.api.extension.ExtendWith;
 import org.mockito.Spy;
 import org.mockito.junit.jupiter.MockitoExtension;
@@ -38,7 +39,8 @@
 import static org.mockito.Mockito.verifyNoMoreInteractions;
 
 @ExtendWith(MockitoExtension.class)
-public class AiServicesWithJsonSchemaWithDescriptionsIT {
+@EnabledIfEnvironmentVariable(named = "OPENAI_API_KEY", matches = ".+")
+class AiServicesWithJsonSchemaWithDescriptionsIT {
 
     @Spy
     ChatLanguageModel model = OpenAiChatModel.builder()

File: langchain4j/src/test/java/dev/langchain4j/service/OpenAiAiServiceWithToolsIT.java
Patch:
@@ -2,6 +2,7 @@
 
 import dev.langchain4j.model.chat.ChatLanguageModel;
 import dev.langchain4j.model.openai.OpenAiChatModel;
+import org.junit.jupiter.api.condition.EnabledIfEnvironmentVariable;
 
 import java.util.List;
 
@@ -10,6 +11,7 @@
 import static java.util.Collections.singletonList;
 
 // TODO move to langchain4j-open-ai module once cyclic dependency is resolved
+@EnabledIfEnvironmentVariable(named = "OPENAI_API_KEY", matches = ".+")
 class OpenAiAiServiceWithToolsIT extends AiServicesWithNewToolsIT {
 
     @Override

File: langchain4j/src/test/java/dev/langchain4j/service/OpenAiAiServicesWithJsonSchemaIT.java
Patch:
@@ -2,13 +2,15 @@
 
 import dev.langchain4j.model.chat.ChatLanguageModel;
 import dev.langchain4j.model.openai.OpenAiChatModel;
+import org.junit.jupiter.api.condition.EnabledIfEnvironmentVariable;
 
 import java.util.List;
 
 import static dev.langchain4j.model.openai.OpenAiChatModelName.GPT_4_O_MINI;
 import static java.util.Arrays.asList;
 
 // TODO move to langchain4j-open-ai module once dependency cycle is resolved
+@EnabledIfEnvironmentVariable(named = "OPENAI_API_KEY", matches = ".+")
 class OpenAiAiServicesWithJsonSchemaIT extends AiServicesWithJsonSchemaIT {
 
     @Override

File: langchain4j/src/test/java/dev/langchain4j/service/StreamingAiServicesIT.java
Patch:
@@ -17,6 +17,7 @@
 import dev.langchain4j.rag.RetrievalAugmentor;
 import dev.langchain4j.rag.content.Content;
 import org.junit.jupiter.api.Test;
+import org.junit.jupiter.api.condition.EnabledIfEnvironmentVariable;
 import org.junit.jupiter.params.ParameterizedTest;
 import org.junit.jupiter.params.provider.MethodSource;
 
@@ -36,7 +37,8 @@
 import static org.mockito.Mockito.verifyNoMoreInteractions;
 import static org.mockito.Mockito.when;
 
-public class StreamingAiServicesIT {
+@EnabledIfEnvironmentVariable(named = "OPENAI_API_KEY", matches = ".+")
+class StreamingAiServicesIT {
 
     static Stream<StreamingChatLanguageModel> models() {
         return Stream.of(

File: langchain4j/src/test/java/dev/langchain4j/service/StreamingAiServicesWithToolsIT.java
Patch:
@@ -20,6 +20,7 @@
 import dev.langchain4j.service.tool.ToolProvider;
 import dev.langchain4j.service.tool.ToolProviderResult;
 import org.junit.jupiter.api.Test;
+import org.junit.jupiter.api.condition.EnabledIfEnvironmentVariable;
 import org.junit.jupiter.params.ParameterizedTest;
 import org.junit.jupiter.params.provider.MethodSource;
 
@@ -43,6 +44,7 @@
 import static org.mockito.Mockito.verify;
 import static org.mockito.Mockito.verifyNoMoreInteractions;
 
+@EnabledIfEnvironmentVariable(named = "OPENAI_API_KEY", matches = ".+")
 class StreamingAiServicesWithToolsIT {
 
     static Stream<StreamingChatLanguageModel> models() {

File: langchain4j-mistral-ai/src/main/java/dev/langchain4j/model/mistralai/MistralAiChatModelName.java
Patch:
@@ -50,7 +50,9 @@ public enum MistralAiChatModelName {
 
     MISTRAL_MEDIUM_LATEST("mistral-medium-latest"), // aka mistral-medium-2312
 
-    MISTRAL_LARGE_LATEST("mistral-large-latest"); // aka mistral-large-2402
+    MISTRAL_LARGE_LATEST("mistral-large-latest"), // aka mistral-large-2402
+
+    MISTRAL_MODERATION_LATEST("mistral-moderation-latest");
 
     private final String value;
 

File: langchain4j-mistral-ai/src/main/java/dev/langchain4j/model/mistralai/internal/client/MistralAiClient.java
Patch:
@@ -14,6 +14,8 @@ public abstract class MistralAiClient {
 
     public abstract MistralAiEmbeddingResponse embedding(MistralAiEmbeddingRequest request);
 
+    public abstract MistralAiModerationResponse moderation(MistralAiModerationRequest request);
+
     public abstract MistralAiModelResponse listModels();
 
     @SuppressWarnings("rawtypes")

File: langchain4j-anthropic/src/main/java/dev/langchain4j/model/anthropic/internal/api/AnthropicApi.java
Patch:
@@ -20,5 +20,6 @@ Call<AnthropicCreateMessageResponse> createMessage(@Header(X_API_KEY) String api
     @Headers({"content-type: application/json"})
     Call<ResponseBody> streamMessage(@Header(X_API_KEY) String apiKey,
                                      @Header("anthropic-version") String version,
+                                     @Header("anthropic-beta") String beta,
                                      @Body AnthropicCreateMessageRequest request);
 }

File: langchain4j-anthropic/src/main/java/dev/langchain4j/model/anthropic/internal/api/AnthropicCreateMessageRequest.java
Patch:
@@ -24,7 +24,7 @@ public class AnthropicCreateMessageRequest {
 
     public String model;
     public List<AnthropicMessage> messages;
-    public String system;
+    public List<AnthropicTextContent> system;
     public int maxTokens;
     public List<String> stopSequences;
     public boolean stream;

File: langchain4j-anthropic/src/main/java/dev/langchain4j/model/anthropic/internal/api/AnthropicTool.java
Patch:
@@ -21,4 +21,5 @@ public class AnthropicTool {
     public String name;
     public String description;
     public AnthropicToolSchema inputSchema;
+    public AnthropicCacheControl cacheControl;
 }

File: langchain4j-anthropic/src/main/java/dev/langchain4j/model/anthropic/internal/api/AnthropicUsage.java
Patch:
@@ -14,4 +14,6 @@ public class AnthropicUsage {
 
     public Integer inputTokens;
     public Integer outputTokens;
+    public Integer cacheCreationInputTokens;
+    public Integer cacheReadInputTokens;
 }
\ No newline at end of file

File: langchain4j-anthropic/src/main/java/dev/langchain4j/model/anthropic/internal/client/AnthropicClient.java
Patch:
@@ -61,9 +61,6 @@ public B version(String version) {
         }
 
         public B beta(String beta) {
-            if (beta == null) {
-                throw new IllegalArgumentException("beta cannot be null or empty");
-            }
             this.beta = beta;
             return (B) this;
         }

File: langchain4j-anthropic/src/test/java/dev/langchain4j/model/anthropic/AnthropicMapperTest.java
Patch:
@@ -28,7 +28,6 @@ class AnthropicMapperTest {
     @ParameterizedTest
     @MethodSource
     void test_toAnthropicMessages(List<ChatMessage> messages, List<AnthropicMessage> expectedAnthropicMessages) {
-
         // when
         List<AnthropicMessage> anthropicMessages = toAnthropicMessages(messages);
 
@@ -212,7 +211,7 @@ static Stream<Arguments> test_toAnthropicMessages() {
     void test_toAnthropicTool(ToolSpecification toolSpecification, AnthropicTool expectedAnthropicTool) {
 
         // when
-        AnthropicTool anthropicTool = toAnthropicTool(toolSpecification);
+        AnthropicTool anthropicTool = toAnthropicTool(toolSpecification, AnthropicCacheType.NO_CACHE);
 
         // then
         assertThat(anthropicTool).isEqualTo(expectedAnthropicTool);
@@ -258,4 +257,4 @@ private static <K, V> Map<K, V> mapOf(Map.Entry<K, V>... entries) {
     private static <K, V> Map.Entry<K, V> entry(K key, V value) {
         return new AbstractMap.SimpleEntry<>(key, value);
     }
-}
\ No newline at end of file
+}

File: langchain4j-core/src/main/java/dev/langchain4j/data/message/SystemMessage.java
Patch:
@@ -72,4 +72,4 @@ public static SystemMessage from(String text) {
     public static SystemMessage systemMessage(String text) {
         return from(text);
     }
-}
+}
\ No newline at end of file

File: langchain4j-core/src/main/java/dev/langchain4j/data/message/TextContent.java
Patch:
@@ -62,4 +62,4 @@ public String toString() {
     public static TextContent from(String text) {
         return new TextContent(text);
     }
-}
+}
\ No newline at end of file

File: langchain4j-core/src/main/java/dev/langchain4j/model/output/TokenUsage.java
Patch:
@@ -13,6 +13,7 @@ public class TokenUsage {
     private final Integer outputTokenCount;
     private final Integer totalTokenCount;
 
+
     /**
      * Creates a new {@link TokenUsage} instance with all fields set to null.
      */

File: langchain4j-ollama/src/main/java/dev/langchain4j/model/ollama/Function.java
Patch:
@@ -78,4 +78,4 @@ Function build() {
             return new Function(name, description, parameters);
         }
     }
-}
+}
\ No newline at end of file

File: langchain4j-ollama/src/main/java/dev/langchain4j/model/ollama/OllamaClient.java
Patch:
@@ -339,4 +339,4 @@ OllamaClient build() {
             return new OllamaClient(baseUrl, timeout, logRequests, logResponses, logStreamingResponses, customHeaders);
         }
     }
-}
+}
\ No newline at end of file

File: langchain4j-ollama/src/test/java/dev/langchain4j/model/ollama/OllamaApiIT.java
Patch:
@@ -113,4 +113,4 @@ void base_url_without_trailing_slash_with_addition_path() {
 
         assertThat(chatResponse.getMessage().getContent()).endsWith(mockWebServer.getPort() + "/additional/api/chat");
     }
-}
+}
\ No newline at end of file

File: langchain4j-jlama/src/test/java/dev/langchain4j/model/jlama/JlamaChatModelIT.java
Patch:
@@ -26,7 +26,7 @@ static void setup() {
         tmpDir.mkdirs();
 
         model = JlamaChatModel.builder()
-                .modelName("tjake/Meta-Llama-3.1-8B-Instruct-Jlama-Q4")
+                .modelName("tjake/Llama-3.2-1B-Instruct-JQ4")
                 .modelCachePath(tmpDir.toPath())
                 .temperature(0.0f)
                 .maxTokens(64)

File: langchain4j-jlama/src/test/java/dev/langchain4j/model/jlama/JlamaChatModelToolsIT.java
Patch:
@@ -29,7 +29,7 @@ static void setup() {
         tmpDir.mkdirs();
 
         model = JlamaChatModel.builder()
-                .modelName("tjake/Meta-Llama-3.1-8B-Instruct-Jlama-Q4")
+                .modelName("Qwen/Qwen2.5-1.5B-Instruct")
                 .modelCachePath(tmpDir.toPath())
                 .temperature(0.0f)
                 .maxTokens(1024)

File: langchain4j-jlama/src/test/java/dev/langchain4j/model/jlama/JlamaLanguageModelIT.java
Patch:
@@ -23,7 +23,7 @@ static void setup() {
         tmpDir.mkdirs();
 
         model = JlamaLanguageModel.builder()
-                .modelName("tjake/Meta-Llama-3.1-8B-Instruct-Jlama-Q4")
+                .modelName("tjake/Llama-3.2-1B-Instruct-JQ4")
                 .modelCachePath(tmpDir.toPath())
                 .temperature(0.0f)
                 .maxTokens(64)

File: langchain4j-jlama/src/test/java/dev/langchain4j/model/jlama/JlamaStreamingChatModelIT.java
Patch:
@@ -25,7 +25,7 @@ static void setup() {
         tmpDir.mkdirs();
 
         model = JlamaStreamingChatModel.builder()
-                .modelName("tjake/Meta-Llama-3.1-8B-Instruct-Jlama-Q4")
+                .modelName("tjake/Llama-3.2-1B-Instruct-JQ4")
                 .modelCachePath(tmpDir.toPath())
                 .maxTokens(64)
                 .temperature(0.0f)

File: langchain4j-jlama/src/test/java/dev/langchain4j/model/jlama/JlamaStreamingLanguageModelIT.java
Patch:
@@ -24,7 +24,7 @@ static void setup() {
         tmpDir.mkdirs();
 
         model = JlamaStreamingLanguageModel.builder()
-                .modelName("tjake/Meta-Llama-3.1-8B-Instruct-Jlama-Q4")
+                .modelName("tjake/Llama-3.2-1B-Instruct-JQ4")
                 .modelCachePath(tmpDir.toPath())
                 .maxTokens(64)
                 .temperature(0.0f)

File: langchain4j-ollama/src/test/java/dev/langchain4j/model/ollama/LangChain4jOllamaContainer.java
Patch:
@@ -31,11 +31,11 @@ protected void containerIsStarted(InspectContainerResponse containerInfo) {
         if (this.model != null) {
             try {
                 log.info("Start pulling the '{}' model ... would take several minutes ...", this.model);
-                execInContainer("ollama", "pull", this.model);
-                log.info("Model pulling competed!");
+                ExecResult r = execInContainer("ollama", "pull", this.model);
+                log.info("Model pulling competed! {}", r);
             } catch (IOException | InterruptedException e) {
                 throw new RuntimeException("Error pulling model", e);
             }
         }
     }
-}
\ No newline at end of file
+}

File: web-search-engines/langchain4j-web-search-engine-google-custom/src/main/java/dev/langchain4j/web/search/google/customsearch/GoogleCustomWebSearchEngine.java
Patch:
@@ -168,7 +168,7 @@ private static Map<String, String> toResultMetadataMap(Result result, boolean se
             return metadata;
         }
         // Web search type
-        if (!result.getPagemap().isEmpty()) {
+        if (!isNullOrEmpty(result.getPagemap())) {
             result.getPagemap().forEach((key, value) -> {
                 if (key.equals("metatags")) {
                     if (value instanceof List) {

File: langchain4j-core/src/main/java/dev/langchain4j/internal/Utils.java
Patch:
@@ -10,6 +10,7 @@
 import java.security.MessageDigest;
 import java.security.NoSuchAlgorithmException;
 import java.util.Collection;
+import java.util.HexFormat;
 import java.util.List;
 import java.util.Map;
 import java.util.UUID;
@@ -175,9 +176,8 @@ private static MessageDigest getSha256Instance() {
    */
   public static String generateUUIDFrom(String input) {
       byte[] hashBytes = getSha256Instance().digest(input.getBytes(UTF_8));
-      StringBuilder sb = new StringBuilder();
-      for (byte b : hashBytes) sb.append("%02x".formatted(b));
-      return UUID.nameUUIDFromBytes(sb.toString().getBytes(UTF_8)).toString();
+      String hexFormat = HexFormat.of().formatHex(hashBytes);
+      return UUID.nameUUIDFromBytes(hexFormat.getBytes(UTF_8)).toString();
   }
 
   /**

File: langchain4j-core/src/main/java/dev/langchain4j/Experimental.java
Patch:
@@ -2,7 +2,9 @@
 
 import java.lang.annotation.Target;
 
-import static java.lang.annotation.ElementType.*;
+import static java.lang.annotation.ElementType.CONSTRUCTOR;
+import static java.lang.annotation.ElementType.METHOD;
+import static java.lang.annotation.ElementType.TYPE;
 
 /**
  * Indicates that a class/constructor/method is experimental and might change in the future.

File: langchain4j-core/src/main/java/dev/langchain4j/data/message/ChatMessageDeserializer.java
Patch:
@@ -1,8 +1,9 @@
 package dev.langchain4j.data.message;
 
-import static dev.langchain4j.data.message.ChatMessageSerializer.CODEC;
 import java.util.List;
 
+import static dev.langchain4j.data.message.ChatMessageSerializer.CODEC;
+
 /**
  * A deserializer for {@link ChatMessage} objects.
  */

File: langchain4j-core/src/main/java/dev/langchain4j/data/message/GsonChatMessageJsonCodec.java
Patch:
@@ -1,12 +1,14 @@
 package dev.langchain4j.data.message;
 
-import static java.util.Collections.emptyList;
 import com.google.gson.Gson;
 import com.google.gson.GsonBuilder;
 import com.google.gson.reflect.TypeToken;
+
 import java.lang.reflect.Type;
 import java.util.List;
 
+import static java.util.Collections.emptyList;
+
 /**
  * A codec for serializing and deserializing {@link ChatMessage} objects to and from JSON.
  */

File: langchain4j-core/src/main/java/dev/langchain4j/model/embedding/DisabledEmbeddingModel.java
Patch:
@@ -6,7 +6,6 @@
 import dev.langchain4j.model.output.Response;
 
 import java.util.List;
-import java.util.Map;
 
 /**
  * An {@link EmbeddingModel} which throws a {@link ModelDisabledException} for all of its methods

File: langchain4j-core/src/main/java/dev/langchain4j/model/image/ImageModel.java
Patch:
@@ -2,6 +2,7 @@
 
 import dev.langchain4j.data.image.Image;
 import dev.langchain4j.model.output.Response;
+
 import java.util.List;
 
 /**

File: langchain4j-core/src/main/java/dev/langchain4j/model/input/structured/DefaultStructuredPromptFactory.java
Patch:
@@ -7,6 +7,7 @@
 import dev.langchain4j.model.input.Prompt;
 import dev.langchain4j.model.input.PromptTemplate;
 import dev.langchain4j.spi.prompt.structured.StructuredPromptFactory;
+
 import java.util.Map;
 
 /**

File: langchain4j-core/src/main/java/dev/langchain4j/retriever/EmbeddingStoreRetriever.java
Patch:
@@ -1,7 +1,7 @@
 package dev.langchain4j.retriever;
 
-import dev.langchain4j.data.segment.TextSegment;
 import dev.langchain4j.data.embedding.Embedding;
+import dev.langchain4j.data.segment.TextSegment;
 import dev.langchain4j.model.embedding.EmbeddingModel;
 import dev.langchain4j.rag.content.retriever.EmbeddingStoreContentRetriever;
 import dev.langchain4j.store.embedding.EmbeddingMatch;

File: langchain4j-core/src/main/java/dev/langchain4j/web/search/WebSearchResults.java
Patch:
@@ -9,7 +9,6 @@
 import java.util.Objects;
 
 import static dev.langchain4j.internal.Utils.isNullOrEmpty;
-import static dev.langchain4j.internal.ValidationUtils.ensureNotEmpty;
 import static dev.langchain4j.internal.ValidationUtils.ensureNotNull;
 import static java.util.stream.Collectors.toList;
 

File: langchain4j-core/src/test/java/dev/langchain4j/internal/JsonTest.java
Patch:
@@ -1,6 +1,7 @@
 package dev.langchain4j.internal;
 
-import static org.assertj.core.api.Assertions.assertThat;
+import com.google.gson.annotations.SerializedName;
+import org.junit.jupiter.api.Test;
 
 import java.io.BufferedReader;
 import java.io.IOException;
@@ -12,8 +13,7 @@
 import java.util.List;
 import java.util.stream.Collectors;
 
-import com.google.gson.annotations.SerializedName;
-import org.junit.jupiter.api.Test;
+import static org.assertj.core.api.Assertions.assertThat;
 
 class JsonTest {
 

File: langchain4j-core/src/test/java/dev/langchain4j/model/embedding/DimensionAwareEmbeddingModelTest.java
Patch:
@@ -8,9 +8,7 @@
 import org.assertj.core.api.WithAssertions;
 import org.junit.jupiter.api.Test;
 
-import java.util.HashMap;
 import java.util.List;
-import java.util.Map;
 import java.util.stream.Collectors;
 
 class DimensionAwareEmbeddingModelTest implements WithAssertions {

File: langchain4j-core/src/test/java/dev/langchain4j/store/embedding/EmbeddingStoreWithRemovalIT.java
Patch:
@@ -4,14 +4,11 @@
 import dev.langchain4j.data.segment.TextSegment;
 import dev.langchain4j.model.embedding.EmbeddingModel;
 import dev.langchain4j.store.embedding.filter.Filter;
-import org.awaitility.Awaitility;
-import org.awaitility.core.ThrowingRunnable;
 import org.junit.jupiter.api.Test;
 import org.junit.jupiter.params.ParameterizedTest;
 import org.junit.jupiter.params.provider.NullAndEmptySource;
 import org.junit.jupiter.params.provider.ValueSource;
 
-import java.time.Duration;
 import java.util.Collection;
 import java.util.List;
 

File: langchain4j/src/test/java/dev/langchain4j/internal/TestUtils.java
Patch:
@@ -4,7 +4,6 @@
 import dev.langchain4j.data.message.SystemMessage;
 import dev.langchain4j.data.message.UserMessage;
 import dev.langchain4j.model.openai.OpenAiTokenizer;
-import lombok.val;
 import org.junit.jupiter.api.Test;
 import org.junit.jupiter.params.ParameterizedTest;
 import org.junit.jupiter.params.provider.ValueSource;
@@ -89,7 +88,7 @@ void should_repeat_n_times() {
     }
 
     public static List<String> repeat(String s, int n) {
-        val result = new ArrayList<String>();
+        final var result = new ArrayList<String>();
         for (int i = 0; i < n; i++) {
             result.add(s);
         }

File: langchain4j-core/src/main/java/dev/langchain4j/agent/tool/ToolExecutionRequest.java
Patch:
@@ -49,8 +49,8 @@ public String arguments() {
     @Override
     public boolean equals(Object another) {
         if (this == another) return true;
-        return another instanceof ToolExecutionRequest
-                && equalTo((ToolExecutionRequest) another);
+        return another instanceof ToolExecutionRequest ter
+                && equalTo(ter);
     }
 
     private boolean equalTo(ToolExecutionRequest another) {

File: langchain4j-core/src/main/java/dev/langchain4j/agent/tool/ToolParameters.java
Patch:
@@ -75,8 +75,8 @@ public List<String> required() {
     @Override
     public boolean equals(Object another) {
         if (this == another) return true;
-        return another instanceof ToolParameters
-                && equalTo((ToolParameters) another);
+        return another instanceof ToolParameters tp
+                && equalTo(tp);
     }
 
     /**

File: langchain4j-core/src/main/java/dev/langchain4j/agent/tool/ToolSpecification.java
Patch:
@@ -76,8 +76,8 @@ public ToolParameters toolParameters() {
     @Override
     public boolean equals(Object another) {
         if (this == another) return true;
-        return another instanceof ToolSpecification
-                && equalTo((ToolSpecification) another);
+        return another instanceof ToolSpecification ts
+                && equalTo(ts);
     }
 
     private boolean equalTo(ToolSpecification another) {

File: langchain4j-core/src/main/java/dev/langchain4j/agent/tool/ToolSpecifications.java
Patch:
@@ -15,7 +15,6 @@
 import java.util.Set;
 
 import static dev.langchain4j.internal.Utils.isNullOrBlank;
-import static java.lang.String.format;
 import static java.util.Arrays.stream;
 import static java.util.stream.Collectors.toList;
 
@@ -65,7 +64,7 @@ public static void validateSpecifications(List<ToolSpecification> toolSpecificat
         Set<String> names = new HashSet<>();
         for (ToolSpecification toolSpecification : toolSpecifications) {
             if (!names.add(toolSpecification.name())) {
-                throw new IllegalArgumentException(format("Tool names must be unique. The tool '%s' appears several times", toolSpecification.name()));
+                throw new IllegalArgumentException("Tool names must be unique. The tool '%s' appears several times".formatted(toolSpecification.name()));
             }
         }
     }

File: langchain4j-core/src/main/java/dev/langchain4j/data/message/GsonChatMessageAdapter.java
Patch:
@@ -29,7 +29,7 @@ public ChatMessage deserialize(JsonElement messageJsonElement, Type ignored, Jso
         String chatMessageTypeString = messageJsonElement.getAsJsonObject().get(CHAT_MESSAGE_TYPE).getAsString();
         ChatMessageType chatMessageType = ChatMessageType.valueOf(chatMessageTypeString);
         ChatMessage chatMessage = GSON.fromJson(messageJsonElement, chatMessageType.messageClass());
-        if (chatMessage instanceof UserMessage && ((UserMessage) chatMessage).contents() == null) {
+        if (chatMessage instanceof UserMessage message && message.contents() == null) {
             // keeping backward compatibility with old schema TODO remove after a few releases
             chatMessage = UserMessage.from(messageJsonElement.getAsJsonObject().get("text").getAsString());
         }

File: langchain4j-core/src/main/java/dev/langchain4j/internal/CustomMimeTypesFileTypeDetector.java
Patch:
@@ -7,7 +7,6 @@
 import java.net.URLConnection;
 import java.nio.file.Files;
 import java.nio.file.Path;
-import java.nio.file.Paths;
 import java.nio.file.StandardOpenOption;
 import java.nio.file.spi.FileTypeDetector;
 import java.util.Collections;
@@ -136,7 +135,7 @@ public String probeContentType(Path path) {
     }
 
     public String probeContentType(String path) {
-        return probeContentType(Paths.get(path));
+        return probeContentType(Path.of(path));
     }
 
     /**
@@ -149,7 +148,7 @@ public String probeContentType(String path) {
      */
     public String probeContentType(URI uri) {
         // First let's try to guess via the Path
-        Path path = Paths.get(uri.getPath());
+        Path path = Path.of(uri.getPath());
         String mimeTypeFromPath = probeContentType(path);
         if (mimeTypeFromPath != null) {
             return mimeTypeFromPath;

File: langchain4j-core/src/main/java/dev/langchain4j/internal/Exceptions.java
Patch:
@@ -16,7 +16,7 @@ private Exceptions() {}
      * @return the constructed exception.
      */
     public static IllegalArgumentException illegalArgument(String format, Object... args) {
-        return new IllegalArgumentException(String.format(format, args));
+        return new IllegalArgumentException(format.formatted(args));
     }
 
     /**
@@ -29,6 +29,6 @@ public static IllegalArgumentException illegalArgument(String format, Object...
      * @return the constructed exception.
      */
     public static RuntimeException runtime(String format, Object... args) {
-        return new RuntimeException(String.format(format, args));
+        return new RuntimeException(format.formatted(args));
     }
 }

File: langchain4j-core/src/main/java/dev/langchain4j/internal/RetryUtils.java
Patch:
@@ -6,8 +6,6 @@
 import java.util.Random;
 import java.util.concurrent.Callable;
 
-import static java.lang.String.format;
-
 /**
  * Utility class for retrying actions.
  */
@@ -195,7 +193,7 @@ public <T> T withRetry(Callable<T> action, int maxAttempts) {
                         throw new RuntimeException(e);
                     }
 
-                    log.warn(format("Exception was thrown on attempt %s of %s", attempt, maxAttempts), e);
+                    log.warn("Exception was thrown on attempt %s of %s".formatted(attempt, maxAttempts), e);
 
                     sleep(attempt);
                 }

File: langchain4j-core/src/main/java/dev/langchain4j/internal/Utils.java
Patch:
@@ -6,7 +6,7 @@
 import java.net.URI;
 import java.net.URL;
 import java.nio.file.Files;
-import java.nio.file.Paths;
+import java.nio.file.Path;
 import java.security.MessageDigest;
 import java.security.NoSuchAlgorithmException;
 import java.util.Collection;
@@ -176,7 +176,7 @@ private static MessageDigest getSha256Instance() {
   public static String generateUUIDFrom(String input) {
       byte[] hashBytes = getSha256Instance().digest(input.getBytes(UTF_8));
       StringBuilder sb = new StringBuilder();
-      for (byte b : hashBytes) sb.append(String.format("%02x", b));
+      for (byte b : hashBytes) sb.append("%02x".formatted(b));
       return UUID.nameUUIDFromBytes(sb.toString().getBytes(UTF_8)).toString();
   }
 
@@ -253,7 +253,7 @@ public static byte[] readBytes(String url) {
         }
       } else {
         // Handle files
-        return Files.readAllBytes(Paths.get(new URI(url)));
+        return Files.readAllBytes(Path.of(new URI(url)));
       }
     } catch (Exception e) {
       throw new RuntimeException(e);

File: langchain4j-core/src/main/java/dev/langchain4j/store/embedding/filter/comparison/UUIDComparator.java
Patch:
@@ -16,8 +16,8 @@ static boolean containsAsUUID(Object actualUUID, Collection<?> comparisonUUIDs)
     private static UUID toUUID(Object actualUUID) {
         if (actualUUID instanceof String) {
             return UUID.fromString(actualUUID.toString());
-        } else if (actualUUID instanceof UUID) {
-            return (UUID)actualUUID;
+        } else if (actualUUID instanceof UUID iD) {
+            return iD;
         }
 
         throw new IllegalArgumentException("Unsupported type: " + actualUUID.getClass().getName());

File: langchain4j-core/src/main/java/dev/langchain4j/web/search/WebSearchRequest.java
Patch:
@@ -126,8 +126,8 @@ public Map<String, Object> additionalParams() {
     @Override
     public boolean equals(Object another) {
         if (this == another) return true;
-        return another instanceof WebSearchRequest
-                && equalTo((WebSearchRequest) another);
+        return another instanceof WebSearchRequest wsr
+                && equalTo(wsr);
     }
 
     private boolean equalTo(WebSearchRequest another){

File: langchain4j-core/src/test/java/dev/langchain4j/data/document/MetadataTest.java
Patch:
@@ -320,7 +320,7 @@ void should_fail_when_adding_null_key() {
         UUID uuid = UUID.randomUUID();
         assertThatThrownBy(() -> metadata.put(null, uuid))
                 .isExactlyInstanceOf(IllegalArgumentException.class)
-                .hasMessage(String.format("The metadata key with the value '%s' cannot be null or blank", uuid));
+                .hasMessage("The metadata key with the value '%s' cannot be null or blank".formatted(uuid));
 
         assertThatThrownBy(() -> metadata.put(null, 1))
                 .isExactlyInstanceOf(IllegalArgumentException.class)

File: langchain4j-core/src/test/java/dev/langchain4j/internal/CustomMimeTypesFileTypeDetectorTest.java
Patch:
@@ -9,7 +9,6 @@
 import java.net.URISyntaxException;
 import java.net.URL;
 import java.nio.file.Path;
-import java.nio.file.Paths;
 import java.util.HashMap;
 import java.util.Map;
 
@@ -22,7 +21,7 @@ void should_return_a_mime_type_from_default_mapping_from_path() {
         CustomMimeTypesFileTypeDetector detector = new CustomMimeTypesFileTypeDetector();
 
         // when
-        Path path = Paths.get("/foo/bar/index.html");
+        Path path = Path.of("/foo/bar/index.html");
         String mimeType = detector.probeContentType(path);
 
         // then

File: langchain4j-pgvector/src/test/java/dev/langchain4j/store/embedding/pgvector/PgVectorEmbeddingStoreWithColumnsFilteringIT.java
Patch:
@@ -16,7 +16,7 @@ static void beforeAll() {
                         Arrays.asList("key text NULL", "name text NULL", "age float NULL", "city varchar null", "country varchar null",
                                 "string_empty varchar null", "string_space varchar null", "string_abc varchar null", "uuid uuid null",
                                 "integer_min int null", "integer_minus_1 int null", "integer_0 int null", "integer_1 int null", "integer_max int null",
-                                "long_min bigint null", "long_minus_1 bigint null", "long_0 bigint null", "long_1 bigint null", "long_max bigint null",
+                                "long_min bigint null", "long_minus_1 bigint null", "long_0 bigint null", "long_1 bigint null", "long_1746714878034235396 bigint null", "long_max bigint null",
                                 "float_min float null", "float_minus_1 float null", "float_0 float null", "float_1 float null", "float_123 float null", "float_max float null",
                                 "double_minus_1 float8 null", "double_0 float8 null", "double_1 float8 null", "double_123 float8 null"
                         ))

File: langchain4j-weaviate/src/test/java/dev/langchain4j/store/embedding/weaviate/LocalWeaviateEmbeddingStoreIT.java
Patch:
@@ -37,6 +37,7 @@ class LocalWeaviateEmbeddingStoreIT extends EmbeddingStoreIT {
             "long_minus_1",
             "long_0",
             "long_1",
+            "long_1746714878034235396",
             "long_max",
             "float_min",
             "float_minus_1",

File: langchain4j-core/src/main/java/dev/langchain4j/rag/content/injector/DefaultContentInjector.java
Patch:
@@ -38,6 +38,7 @@
  * <br>
  * - {@link #promptTemplate}: The prompt template that defines how the original {@code userMessage}
  * and {@code contents} are combined into the resulting {@link UserMessage}.
+ * The text of the template should contain the {@code {{userMessage}}} and {@code {{contents}}} variables.
  * <br>
  * - {@link #metadataKeysToInclude}: A list of {@link Metadata} keys that should be included
  * with each {@link Content#textSegment()}.

File: langchain4j-dashscope/src/test/java/dev/langchain4j/model/dashscope/WanxImageModelIT.java
Patch:
@@ -42,10 +42,10 @@ void simple_image_edition_works_by_url(String modelName) {
                 .build();
 
         Image image = Image.builder()
-                .url("https://img.alicdn.com/imgextra/i4/O1CN01K1DWat25own2MuQgF_!!6000000007574-0-tps-128-128.jpg")
+                .url("https://help-static-aliyun-doc.aliyuncs.com/assets/img/zh-CN/2476628361/p335710.png")
                 .build();
 
-        Response<Image> response = model.edit(image, "Change the parrot's feathers with yellow");
+        Response<Image> response = model.edit(image, "Draw a parrot");
 
         URI remoteImage = response.content().url();
         log.info("Your remote image is here: {}", remoteImage);

File: langchain4j-azure-open-ai/src/main/java/dev/langchain4j/model/azure/AzureOpenAiTokenizer.java
Patch:
@@ -220,7 +220,7 @@ public int estimateTokenCountInToolSpecifications(Iterable<ToolSpecification> to
                 tokenCount += 2;
                 tokenCount += estimateTokenCountInText(toolSpecification.description());
             }
-            tokenCount += estimateTokenCountInToolParameters(toolSpecification.parameters());
+            tokenCount += estimateTokenCountInToolParameters(toolSpecification.toolParameters());
         }
         return tokenCount;
     }

File: langchain4j-core/src/main/java/dev/langchain4j/model/chat/request/ChatRequest.java
Patch:
@@ -16,7 +16,7 @@
 public class ChatRequest {
 
     private final List<ChatMessage> messages;
-    private final List<ToolSpecification> toolSpecifications;
+    private final List<ToolSpecification> toolSpecifications; // TODO add section with tools?
     private final ResponseFormat responseFormat;
 
     private ChatRequest(Builder builder) {

File: langchain4j-github-models/src/test/java/dev/langchain4j/model/github/GitHubModelsChatModelIT.java
Patch:
@@ -200,7 +200,7 @@ void should_call_function_with_no_argument(String modelName) {
     @CsvSource({
             "gpt-4o"
     })
-    void should_call_three_functions_in_parallel(String modelName) throws Exception {
+    void should_call_three_functions_in_parallel(String modelName) {
 
         ChatLanguageModel model = GitHubModelsChatModel.builder()
                 .gitHubToken(System.getenv("GITHUB_TOKEN"))

File: langchain4j-google-ai-gemini/src/main/java/dev/langchain4j/model/googleai/SchemaMapper.java
Patch:
@@ -18,7 +18,7 @@ static GeminiSchema fromJsonSchemaToGSchema(JsonSchema jsonSchema) {
         return fromJsonSchemaToGSchema(jsonSchema.rootElement());
     }
 
-    private static GeminiSchema fromJsonSchemaToGSchema(JsonSchemaElement jsonSchema) {
+    static GeminiSchema fromJsonSchemaToGSchema(JsonSchemaElement jsonSchema) {
         GeminiSchema.GeminiSchemaBuilder schemaBuilder = GeminiSchema.builder();
 
         if (jsonSchema instanceof JsonStringSchema) {

File: langchain4j-ollama/src/test/java/dev/langchain4j/model/ollama/AbstractOllamaToolsLanguageModelInfrastructure.java
Patch:
@@ -1,6 +1,8 @@
 package dev.langchain4j.model.ollama;
 
-class AbstractOllamaToolsLanguageModelInfrastructure {
+import dev.langchain4j.service.AiServicesWithNewToolsIT;
+
+abstract class AbstractOllamaToolsLanguageModelInfrastructure extends AiServicesWithNewToolsIT {
 
     private static final String LOCAL_OLLAMA_IMAGE = String.format("tc-%s-%s", OllamaImage.OLLAMA_IMAGE, OllamaImage.TOOL_MODEL);
 
@@ -12,6 +14,4 @@ class AbstractOllamaToolsLanguageModelInfrastructure {
         ollama.start();
         ollama.commitToImage(LOCAL_OLLAMA_IMAGE);
     }
-
-
 }

File: langchain4j-ollama/src/test/java/dev/langchain4j/model/ollama/OllamaImage.java
Patch:
@@ -14,7 +14,7 @@ public class OllamaImage {
     static final String BAKLLAVA_MODEL = "bakllava";
 
     static final String TINY_DOLPHIN_MODEL = "tinydolphin";
-    static final String TOOL_MODEL = "mistral";
+    static final String TOOL_MODEL = "llama3.1";
 
     static final String ALL_MINILM_MODEL = "all-minilm";
 

File: langchain4j-open-ai/src/main/java/dev/langchain4j/model/openai/OpenAiTokenizer.java
Patch:
@@ -220,7 +220,7 @@ public int estimateTokenCountInToolSpecifications(Iterable<ToolSpecification> to
                 tokenCount += 2;
                 tokenCount += estimateTokenCountInText(toolSpecification.description());
             }
-            tokenCount += estimateTokenCountInToolParameters(toolSpecification.parameters());
+            tokenCount += estimateTokenCountInToolParameters(toolSpecification.toolParameters());
         }
         return tokenCount;
     }

File: langchain4j-qianfan/src/main/java/dev/langchain4j/model/qianfan/QianfanChatModel.java
Patch:
@@ -25,8 +25,6 @@
 
 public class QianfanChatModel implements ChatLanguageModel {
 
-
-
     private final QianfanClient client;
 
     private final String baseUrl;

File: langchain4j/src/test/java/dev/langchain4j/service/StreamingAiServicesWithToolsWithoutMemoryIT.java
Patch:
@@ -118,7 +118,7 @@ void should_execute_multiple_tools_sequentially_then_answer() throws Exception {
                 .apiKey(System.getenv("OPENAI_API_KEY"))
                 .organizationId(System.getenv("OPENAI_ORGANIZATION_ID"))
                 .modelName(GPT_4_O_MINI)
-                .parallelToolCalls(false) // called sequentially
+                .parallelToolCalls(false) // to force the model to call tools sequentially
                 .temperature(0.0)
                 .logRequests(true)
                 .logResponses(true)

File: langchain4j-core/src/test/java/dev/langchain4j/store/embedding/EmbeddingStoreWithFilteringIT.java
Patch:
@@ -94,7 +94,7 @@ protected static Stream<Arguments> should_filter_by_metadata() {
                         metadataKey("key").isEqualTo(TEST_UUID),
                         asList(
                                 new Metadata().put("key", TEST_UUID),
-                                new Metadata().put("key", TEST_UUID).put("key2", "b")
+                                new Metadata().put("key", TEST_UUID).put("key2", UUID.randomUUID())
                         ),
                         asList(
                                 new Metadata().put("key", UUID.randomUUID()),

File: langchain4j-milvus/src/test/java/dev/langchain4j/store/embedding/milvus/MilvusEmbeddingStoreCloudIT.java
Patch:
@@ -10,13 +10,15 @@
 import dev.langchain4j.store.embedding.EmbeddingStoreWithFilteringIT;
 import org.junit.jupiter.api.AfterEach;
 import org.junit.jupiter.api.Test;
+import org.junit.jupiter.api.condition.EnabledIfEnvironmentVariable;
 
 import java.util.List;
 
 import static io.milvus.common.clientenum.ConsistencyLevelEnum.STRONG;
 import static java.util.Arrays.asList;
 import static org.assertj.core.api.Assertions.assertThat;
 
+@EnabledIfEnvironmentVariable(named = "MILVUS_API_KEY", matches = ".+")
 class MilvusEmbeddingStoreCloudIT extends EmbeddingStoreWithFilteringIT {
 
     private static final String COLLECTION_NAME = "test_collection";

File: langchain4j-oracle/src/test/java/dev/langchain4j/store/embedding/oracle/OracleEmbeddingStoreIT.java
Patch:
@@ -8,7 +8,7 @@
 
 import java.sql.SQLException;
 
-public class OracleEmbeddingStoreWithFilteringIT extends EmbeddingStoreWithFilteringIT {
+public class OracleEmbeddingStoreIT extends EmbeddingStoreWithFilteringIT {
 
     private static final OracleEmbeddingStore EMBEDDING_STORE = CommonTestOperations.newEmbeddingStore();
 

File: langchain4j-qdrant/src/test/java/dev/langchain4j/store/embedding/qdrant/QdrantEmbeddingStoreIT.java
Patch:
@@ -36,8 +36,7 @@
 
 @Testcontainers
 class QdrantEmbeddingStoreIT extends EmbeddingStoreWithFilteringIT {
-    protected static final UUID TEST_UUID = UUID.randomUUID();
-    static final UUID TEST_UUID2 = UUID.randomUUID();
+
     private static String collectionName = "langchain4j-" + randomUUID();
     private static int dimension = 384;
     private static Distance distance = Distance.Cosine;

File: langchain4j-google-ai-gemini/src/test/java/dev/langchain4j/model/googleai/GoogleAiGeminiTokenizerIT.java
Patch:
@@ -11,7 +11,7 @@
 
 import static org.assertj.core.api.Assertions.assertThat;
 
-public class GoogleAiGeminiTokenizerTest {
+public class GoogleAiGeminiTokenizerIT {
     private static final String GOOGLE_AI_GEMINI_API_KEY = System.getenv("GOOGLE_AI_GEMINI_API_KEY");
 
     @Test

File: langchain4j-oracle/src/test/java/dev/langchain4j/store/embedding/oracle/CommonTestOperations.java
Patch:
@@ -21,7 +21,6 @@
 import java.time.Duration;
 import java.util.ArrayList;
 import java.util.List;
-import java.util.Locale;
 import java.util.Random;
 import java.util.logging.Logger;
 
@@ -61,6 +60,8 @@ private CommonTestOperations() {}
 
     private static final PoolDataSource DATA_SOURCE = PoolDataSourceFactory.getPoolDataSource();
 
+    public static final String ORACLE_IMAGE_NAME = "gvenzl/oracle-free:23.5-slim-faststart";
+
     static {
         try {
             DATA_SOURCE.setConnectionFactoryClassName("oracle.jdbc.datasource.impl.OracleDataSource");
@@ -69,7 +70,7 @@ private CommonTestOperations() {}
             if (urlFromEnv == null) {
                 // The Ryuk component is relied upon to stop this container.
                 OracleContainer oracleContainer =
-                    new OracleContainer("gvenzl/oracle-free:23.4-slim-faststart")
+                    new OracleContainer(ORACLE_IMAGE_NAME)
                         .withStartupTimeout(Duration.ofSeconds(600))
                         .withConnectTimeoutSeconds(600)
                         .withDatabaseName("pdb1")

File: langchain4j-anthropic/src/test/java/dev/langchain4j/model/anthropic/internal/client/AnthropicRequestLoggingInterceptorTest.java
Patch:
@@ -43,7 +43,7 @@ void should_mask_secret_headers() {
     void should_mask_secret() {
 
         assertThat(maskSecretKey(null)).isNull();
-        assertThat(maskSecretKey("")).isEqualTo("");
+        assertThat(maskSecretKey("")).isEmpty();
         assertThat(maskSecretKey(" ")).isEqualTo(" ");
         assertThat(maskSecretKey("key")).isEqualTo("...");
         assertThat(maskSecretKey("sk-1234567890")).isEqualTo("sk-12...90");

File: langchain4j-azure-open-ai/src/test/java/dev/langchain4j/model/azure/InternalAzureOpenAiHelperTest.java
Patch:
@@ -24,7 +24,6 @@
 
 import static dev.langchain4j.model.azure.InternalAzureOpenAiHelper.aiMessageFrom;
 import static org.assertj.core.api.Assertions.assertThat;
-import static org.junit.jupiter.api.Assertions.assertEquals;
 import static org.junit.jupiter.api.Assertions.assertInstanceOf;
 
 class InternalAzureOpenAiHelperTest {
@@ -98,9 +97,9 @@ void toToolDefinitionsShouldReturnCorrectToolDefinition() {
 
         List<ChatCompletionsToolDefinition> tools = InternalAzureOpenAiHelper.toToolDefinitions(toolSpecifications);
 
-        assertEquals(toolSpecifications.size(), tools.size());
+        assertThat(tools).hasSize(toolSpecifications.size());
         assertInstanceOf(ChatCompletionsFunctionToolDefinition.class, tools.get(0));
-        assertEquals(toolSpecifications.iterator().next().name(), ((ChatCompletionsFunctionToolDefinition) tools.get(0)).getFunction().getName());
+        assertThat(((ChatCompletionsFunctionToolDefinition) tools.get(0)).getFunction().getName()).isEqualTo(toolSpecifications.iterator().next().name());
     }
 
     @Test

File: langchain4j-core/src/test/java/dev/langchain4j/internal/CustomMimeTypesFileTypeDetectorTest.java
Patch:
@@ -14,7 +14,6 @@
 import java.util.Map;
 
 import static org.assertj.core.api.Assertions.assertThat;
-import static org.assertj.core.api.Assertions.assertThatThrownBy;
 
 public class CustomMimeTypesFileTypeDetectorTest {
     @Test

File: langchain4j-core/src/test/java/dev/langchain4j/internal/UtilsTest.java
Patch:
@@ -96,7 +96,7 @@ public void test_isCollectionEmpty() {
 
     @Test
     public void test_repeat() {
-        assertThat(Utils.repeat("foo", 0)).isEqualTo("");
+        assertThat(Utils.repeat("foo", 0)).isEmpty();
         assertThat(Utils.repeat("foo", 1)).isEqualTo("foo");
         assertThat(Utils.repeat("foo", 2)).isEqualTo("foofoo");
         assertThat(Utils.repeat("foo", 3)).isEqualTo("foofoofoo");

File: langchain4j-core/src/test/java/dev/langchain4j/model/chat/ChatModelListenerIT.java
Patch:
@@ -17,7 +17,7 @@
 import static dev.langchain4j.agent.tool.JsonSchemaProperty.INTEGER;
 import static java.util.Collections.singletonList;
 import static org.assertj.core.api.Assertions.assertThat;
-import static org.assertj.core.api.Fail.fail;
+import static org.assertj.core.api.Assertions.fail;
 
 /**
  * Make sure these dependencies are present in the module where this test class is extended:
@@ -92,7 +92,7 @@ public void onRequest(ChatModelRequestContext requestContext) {
             public void onResponse(ChatModelResponseContext responseContext) {
                 responseReference.set(responseContext.response());
                 assertThat(responseContext.request()).isSameAs(requestReference.get());
-                assertThat(responseContext.attributes().get("id")).isEqualTo("12345");
+                assertThat(responseContext.attributes()).containsEntry("id", "12345");
             }
 
             @Override
@@ -184,7 +184,7 @@ public void onError(ChatModelErrorContext errorContext) {
                 errorReference.set(errorContext.error());
                 assertThat(errorContext.request()).isSameAs(requestReference.get());
                 assertThat(errorContext.partialResponse()).isNull();
-                assertThat(errorContext.attributes().get("id")).isEqualTo("12345");
+                assertThat(errorContext.attributes()).containsEntry("id", "12345");
             }
         };
 

File: langchain4j-core/src/test/java/dev/langchain4j/model/chat/StreamingChatModelListenerIT.java
Patch:
@@ -21,7 +21,7 @@
 import static java.util.Collections.singletonList;
 import static java.util.concurrent.TimeUnit.SECONDS;
 import static org.assertj.core.api.Assertions.assertThat;
-import static org.assertj.core.api.Fail.fail;
+import static org.assertj.core.api.Assertions.fail;
 
 /**
  * Make sure these dependencies are present in the module where this test class is extended:
@@ -96,7 +96,7 @@ public void onRequest(ChatModelRequestContext requestContext) {
             public void onResponse(ChatModelResponseContext responseContext) {
                 responseReference.set(responseContext.response());
                 assertThat(responseContext.request()).isSameAs(requestReference.get());
-                assertThat(responseContext.attributes().get("id")).isEqualTo("12345");
+                assertThat(responseContext.attributes()).containsEntry("id", "12345");
             }
 
             @Override
@@ -201,7 +201,7 @@ public void onError(ChatModelErrorContext errorContext) {
                 errorReference.set(errorContext.error());
                 assertThat(errorContext.request()).isSameAs(requestReference.get());
                 assertThat(errorContext.partialResponse()).isNull(); // can be non-null if it fails in the middle of streaming
-                assertThat(errorContext.attributes().get("id")).isEqualTo("12345");
+                assertThat(errorContext.attributes()).containsEntry("id", "12345");
             }
         };
 

File: langchain4j-core/src/test/java/dev/langchain4j/store/embedding/EmbeddingStoreIT.java
Patch:
@@ -46,7 +46,7 @@ void should_add_embedding_with_segment_with_metadata() {
 
         assertThat(match.embedded().text()).isEqualTo(segment.text());
 
-        assertThat(match.embedded().metadata().getString("string_empty")).isEqualTo("");
+        assertThat(match.embedded().metadata().getString("string_empty")).isEmpty();
         assertThat(match.embedded().metadata().getString("string_space")).isEqualTo(" ");
         assertThat(match.embedded().metadata().getString("string_abc")).isEqualTo("abc");
 

File: langchain4j-core/src/test/java/dev/langchain4j/store/embedding/EmbeddingStoreWithoutMetadataIT.java
Patch:
@@ -15,7 +15,6 @@
 import static dev.langchain4j.internal.Utils.randomUUID;
 import static java.util.Arrays.asList;
 import static org.assertj.core.api.Assertions.assertThat;
-import static org.assertj.core.api.Assertions.assertThatNoException;
 import static org.assertj.core.data.Percentage.withPercentage;
 
 public abstract class EmbeddingStoreWithoutMetadataIT {

File: langchain4j-dashscope/src/test/java/dev/langchain4j/model/dashscope/QwenStreamingChatModelIT.java
Patch:
@@ -20,7 +20,7 @@
 import static java.util.Collections.singletonList;
 import static java.util.concurrent.TimeUnit.SECONDS;
 import static org.assertj.core.api.Assertions.assertThat;
-import static org.assertj.core.api.Fail.fail;
+import static org.assertj.core.api.Assertions.fail;
 
 @EnabledIfEnvironmentVariable(named = "DASHSCOPE_API_KEY", matches = ".+")
 public class QwenStreamingChatModelIT {
@@ -116,7 +116,7 @@ public void onRequest(ChatModelRequestContext requestContext) {
             public void onResponse(ChatModelResponseContext responseContext) {
                 responseReference.set(responseContext.response());
                 assertThat(responseContext.request()).isSameAs(requestReference.get());
-                assertThat(responseContext.attributes().get("id")).isEqualTo("12345");
+                assertThat(responseContext.attributes()).containsEntry("id", "12345");
             }
 
             @Override
@@ -191,7 +191,7 @@ public void onError(ChatModelErrorContext errorContext) {
                 errorReference.set(errorContext.error());
                 assertThat(errorContext.request()).isSameAs(requestReference.get());
                 assertThat(errorContext.partialResponse().aiMessage().text()).isBlank(); // can be non-null if it fails in the middle of streaming
-                assertThat(errorContext.attributes().get("id")).isEqualTo("12345");
+                assertThat(errorContext.attributes()).containsEntry("id", "12345");
             }
         };
 

File: langchain4j-google-ai-gemini/src/test/java/dev/langchain4j/model/googleai/GoogleAiEmbeddingModelIT.java
Patch:
@@ -82,8 +82,9 @@ void should_embed_in_batch() {
 
         // then
         List<Embedding> embeddings = embed.content();
-        assertThat(embeddings).isNotNull();
-        assertThat(embeddings).hasSize(2);
+        assertThat(embeddings)
+                .isNotNull()
+                .hasSize(2);
         assertThat(embeddings.get(0).vector()).isNotNull();
         assertThat(embeddings.get(0).vector()).hasSize(512);
         assertThat(embeddings.get(1).vector()).isNotNull();

File: langchain4j-google-ai-gemini/src/test/java/dev/langchain4j/model/googleai/GoogleAiGeminiChatModelIT.java
Patch:
@@ -392,7 +392,7 @@ void should_support_parallel_tool_execution() {
         assertThat(response.content().hasToolExecutionRequests()).isTrue();
 
         List<ToolExecutionRequest> executionRequests = response.content().toolExecutionRequests();
-        assertThat(executionRequests.size()).isEqualTo(2);
+        assertThat(executionRequests).hasSize(2);
 
         String allArgs = executionRequests.stream()
             .map(ToolExecutionRequest::arguments)

File: langchain4j-google-ai-gemini/src/test/java/dev/langchain4j/model/googleai/GoogleAiGeminiTokenizerTest.java
Patch:
@@ -9,7 +9,7 @@
 
 import java.util.Arrays;
 
-import static org.assertj.core.api.AssertionsForClassTypes.assertThat;
+import static org.assertj.core.api.Assertions.assertThat;
 
 public class GoogleAiGeminiTokenizerTest {
     private static final String GOOGLE_AI_GEMINI_API_KEY = System.getenv("GOOGLE_AI_GEMINI_API_KEY");

File: langchain4j-ollama/src/test/java/dev/langchain4j/model/ollama/OllamaClientIT.java
Patch:
@@ -70,6 +70,6 @@ void should_delete_model() {
 
         // then
         ModelsListResponse afterDeleteModelList = ollamaClient.listModels();
-        assertThat(afterDeleteModelList.getModels().size()).isZero();
+        assertThat(afterDeleteModelList.getModels()).isEmpty();
     }
 }
\ No newline at end of file

File: langchain4j-ollama/src/test/java/dev/langchain4j/model/ollama/OllamaModelsIT.java
Patch:
@@ -62,7 +62,7 @@ void should_return_ollama_model_info_for_given_ollama_model_name() {
         assertThat(response.content().getParameters()).isNotBlank();
         assertThat(response.content().getModifiedAt()).isNotNull();
         assertThat(response.content().getModelInfo().keySet().size()).isPositive();
-        assertThat(response.content().getModelInfo().containsKey("general.architecture")).isTrue();
+        assertThat(response.content().getModelInfo()).containsKey("general.architecture");
         assertThat(response.content().getDetails().getFamily()).isEqualTo("llama");
     }
 

File: langchain4j-oracle/src/test/java/dev/langchain4j/store/embedding/oracle/TestData.java
Patch:
@@ -4,7 +4,6 @@
 import dev.langchain4j.data.segment.TextSegment;
 import dev.langchain4j.store.embedding.EmbeddingMatch;
 import dev.langchain4j.store.embedding.EmbeddingStore;
-import org.junit.jupiter.api.Assertions;
 
 import java.time.OffsetDateTime;
 import java.util.List;
@@ -15,11 +14,12 @@
 import java.util.stream.Stream;
 
 import static dev.langchain4j.store.embedding.oracle.CommonTestOperations.randomFloats;
+import static org.assertj.core.api.Assertions.assertThat;
 
 /**
  * An object representation of the data stored by {@link OracleEmbeddingStore}. This class can be
  * used to generate test data. It implements {@link #equals(Object)}, {@link #toString()}, and {@link #hashCode()} so
- * that results may be verified with {@link Assertions#assertEquals(Object, Object)}
+ * that results may be verified with {@link assertThat(Object)#isEqualTo(Object)}
  */
 final class TestData {
     final String id;

File: langchain4j-vearch/src/test/java/dev/langchain4j/store/embedding/vearch/VearchEmbeddingStoreIT.java
Patch:
@@ -207,7 +207,7 @@ void should_add_embedding_with_segment_with_metadata() {
 
         assertThat(match.embedded().text()).isEqualTo(segment.text());
 
-        assertThat(match.embedded().metadata().getString("string_empty")).isEqualTo("");
+        assertThat(match.embedded().metadata().getString("string_empty")).isEmpty();
         assertThat(match.embedded().metadata().getString("string_space")).isEqualTo(" ");
         assertThat(match.embedded().metadata().getString("string_abc")).isEqualTo("abc");
 

File: langchain4j-vertex-ai-gemini/src/test/java/dev/langchain4j/model/vertexai/ContentsMapperTest.java
Patch:
@@ -36,7 +36,7 @@ void should_split_instructions_and_other_messages() {
         // then
         assertThat(inst.getPartsCount()).isEqualTo(1);
         assertThat(inst.getParts(0).getText()).isEqualTo("You are a smart assistant");
-        assertThat(contents.size()).isEqualTo(2);
+        assertThat(contents).hasSize(2);
         assertThat(contents.get(0).getPartsCount()).isEqualTo(1);
         assertThat(contents.get(0).getParts(0).getText()).isEqualTo("Can you help me, please?");
         assertThat(contents.get(1).getPartsCount()).isEqualTo(1);
@@ -66,7 +66,7 @@ void should_combine_tool_execution_message_in_single_contents() {
         assertThat(inst.getPartsCount()).isEqualTo(1);
         assertThat(inst.getParts(0).getText()).isEqualTo("You are a smart calculator");
 
-        assertThat(contents.size()).isEqualTo(4);
+        assertThat(contents).hasSize(4);
 
         assertThat(contents.get(0).getParts(0).getText()).isEqualTo("Calculate 3+4 and compare the result with 5+6");
 

File: langchain4j-vertex-ai-gemini/src/test/java/dev/langchain4j/model/vertexai/FunctionCallHelperTest.java
Patch:
@@ -29,7 +29,7 @@ void should_unwrap_proto_values() {
         assertThat(unwrapProtoValue(Value.newBuilder().setStringValue("hello").build())).isEqualTo("hello");
         assertThat(unwrapProtoValue(Value.newBuilder().setBoolValue(false).build())).isEqualTo(false);
         assertThat(unwrapProtoValue(Value.newBuilder().setNumberValue(1.23).build())).isEqualTo(1.23);
-        assertThat(unwrapProtoValue(Value.newBuilder().setNullValue(NullValue.NULL_VALUE).build())).isEqualTo(null);
+        assertThat(unwrapProtoValue(Value.newBuilder().setNullValue(NullValue.NULL_VALUE).build())).isNull();
 
         // check list unwrapping
         ListValue listValue = ListValue.newBuilder()

File: langchain4j-vertex-ai-gemini/src/test/java/dev/langchain4j/model/vertexai/PartsMapperTest.java
Patch:
@@ -15,7 +15,6 @@
 import java.util.stream.Stream;
 
 import static org.assertj.core.api.Assertions.assertThat;
-import static org.assertj.core.api.Assertions.assertThatThrownBy;
 
 class PartsMapperTest {
 

File: langchain4j-vertex-ai-gemini/src/test/java/dev/langchain4j/model/vertexai/SchemaHelperTest.java
Patch:
@@ -30,7 +30,7 @@ class Person {
 
         // then
         assertThat(schema.getRequiredList()).contains("name", "age", "isStudent", "friends");
-        assertThat(schema.getPropertiesMap().keySet()).contains("name", "age", "isStudent", "friends");
+        assertThat(schema.getPropertiesMap()).containsKeys("name", "age", "isStudent", "friends");
         assertThat(schema.getPropertiesMap().get("name").getType()).isEqualTo(Type.STRING);
         assertThat(schema.getPropertiesMap().get("age").getType()).isEqualTo(Type.INTEGER);
         assertThat(schema.getPropertiesMap().get("isStudent").getType()).isEqualTo(Type.BOOLEAN);
@@ -76,7 +76,7 @@ void should_convert_json_schema_string_into_schema() {
 
         // then
         assertThat(schema.getRequiredList()).contains("artist-name", "artist-age", "artist-adult", "artist-pets", "artist-address");
-        assertThat(schema.getPropertiesMap().keySet()).contains("artist-name", "artist-age", "artist-adult", "artist-pets", "artist-address");
+        assertThat(schema.getPropertiesMap()).containsKeys("artist-name", "artist-age", "artist-adult", "artist-pets", "artist-address");
         assertThat(schema.getPropertiesMap().get("artist-name").getType()).isEqualTo(Type.STRING);
         assertThat(schema.getPropertiesMap().get("artist-address").getType()).isEqualTo(Type.STRING);
         assertThat(schema.getPropertiesMap().get("artist-age").getType()).isEqualTo(Type.INTEGER);

File: langchain4j-vertex-ai-gemini/src/test/java/dev/langchain4j/model/vertexai/VertexAiGeminiChatModelIT.java
Patch:
@@ -406,7 +406,7 @@ void should_handle_parallel_function_calls() {
         assertThat(messageResponse.content().hasToolExecutionRequests()).isTrue();
 
         List<ToolExecutionRequest> executionRequests = messageResponse.content().toolExecutionRequests();
-        assertThat(executionRequests.size()).isEqualTo(2); // ie. parallel function execution requests
+        assertThat(executionRequests).hasSize(2); // ie. parallel function execution requests
 
         String inventoryStock = executionRequests.stream()
             .map(ToolExecutionRequest::arguments)

File: langchain4j-vertex-ai-gemini/src/test/java/dev/langchain4j/model/vertexai/VertexAiGeminiStreamingChatModelIT.java
Patch:
@@ -462,7 +462,7 @@ void should_work_with_parallel_function_calls() throws InterruptedException, Exe
 
         AiMessage aiMsg = (AiMessage) chatMemory.messages().get(2);
         assertThat(aiMsg.hasToolExecutionRequests()).isTrue();
-        assertThat(aiMsg.toolExecutionRequests().size()).isEqualTo(2);
+        assertThat(aiMsg.toolExecutionRequests()).hasSize(2);
         assertThat(aiMsg.toolExecutionRequests().get(0).name()).isEqualTo("getStockInventory");
         assertThat(aiMsg.toolExecutionRequests().get(0).arguments()).isEqualTo("{\"product\":\"ABC\"}");
         assertThat(aiMsg.toolExecutionRequests().get(1).name()).isEqualTo("getStockInventory");

File: langchain4j-vertex-ai/src/test/java/dev/langchain4j/model/vertexai/VertexAiEmbeddingModelIT.java
Patch:
@@ -13,7 +13,7 @@
 import static dev.langchain4j.model.vertexai.VertexAiEmbeddingModel.TaskType.*;
 import static java.util.Arrays.asList;
 import static org.assertj.core.api.Assertions.assertThat;
-import static org.junit.jupiter.api.Assertions.fail;
+import static org.assertj.core.api.Assertions.fail;
 
 class VertexAiEmbeddingModelIT {
 

File: langchain4j-zhipu-ai/src/test/java/dev/langchain4j/model/zhipu/ZhipuAiStreamingChatModelIT.java
Patch:
@@ -30,7 +30,7 @@
 import static java.util.Collections.singletonList;
 import static java.util.concurrent.TimeUnit.SECONDS;
 import static org.assertj.core.api.Assertions.assertThat;
-import static org.assertj.core.api.Fail.fail;
+import static org.assertj.core.api.Assertions.fail;
 
 @EnabledIfEnvironmentVariable(named = "ZHIPU_API_KEY", matches = ".+")
 public class ZhipuAiStreamingChatModelIT {
@@ -237,7 +237,7 @@ public void onRequest(ChatModelRequestContext requestContext) {
             public void onResponse(ChatModelResponseContext responseContext) {
                 responseReference.set(responseContext.response());
                 assertThat(responseContext.request()).isSameAs(requestReference.get());
-                assertThat(responseContext.attributes().get("id")).isEqualTo("12345");
+                assertThat(responseContext.attributes()).containsEntry("id", "12345");
             }
 
             @Override
@@ -319,7 +319,7 @@ public void onError(ChatModelErrorContext errorContext) {
                 errorReference.set(errorContext.error());
                 assertThat(errorContext.request()).isSameAs(requestReference.get());
                 assertThat(errorContext.partialResponse()).isNull(); // can be non-null if it fails in the middle of streaming
-                assertThat(errorContext.attributes().get("id")).isEqualTo("12345");
+                assertThat(errorContext.attributes()).containsEntry("id", "12345");
             }
         };
 

File: langchain4j/src/test/java/dev/langchain4j/service/output/EnumSetOutputParserTest.java
Patch:
@@ -5,7 +5,7 @@
 import java.util.Iterator;
 import java.util.Set;
 
-import static org.junit.jupiter.api.Assertions.assertEquals;
+import static org.assertj.core.api.Assertions.assertThat;
 
 class EnumSetOutputParserTest {
 
@@ -28,7 +28,7 @@ public void ensureThatOrderIsPreserved() {
 
         // Then
         Iterator<Enum> enumIterator = parsed.iterator();
-        assertEquals(Weather.SUNNY, enumIterator.next());
-        assertEquals(Weather.RAINY, enumIterator.next());
+        assertThat(enumIterator.next()).isEqualTo(Weather.SUNNY);
+        assertThat(enumIterator.next()).isEqualTo(Weather.RAINY);
     }
 }
\ No newline at end of file

File: langchain4j-core/src/test/java/dev/langchain4j/store/embedding/EmbeddingStoreIT.java
Patch:
@@ -17,7 +17,7 @@
 public abstract class EmbeddingStoreIT extends EmbeddingStoreWithoutMetadataIT {
 
     protected static final UUID TEST_UUID = UUID.randomUUID();
-    static final UUID TEST_UUID2 = UUID.randomUUID();
+    protected static final UUID TEST_UUID2 = UUID.randomUUID();
 
     @Test
     void should_add_embedding_with_segment_with_metadata() {

File: langchain4j-core/src/test/java/dev/langchain4j/store/embedding/EmbeddingStoreWithFilteringIT.java
Patch:
@@ -1263,7 +1263,7 @@ protected static Stream<Arguments> should_filter_by_metadata() {
 
     @ParameterizedTest
     @MethodSource
-    void should_filter_by_metadata_not(Filter metadataFilter,
+    protected void should_filter_by_metadata_not(Filter metadataFilter,
                                        List<Metadata> matchingMetadatas,
                                        List<Metadata> notMatchingMetadatas) {
         // given

File: langchain4j-vertex-ai/src/test/java/dev/langchain4j/model/vertexai/VertexAiEmbeddingModelIT.java
Patch:
@@ -76,7 +76,7 @@ void testTokensCountCalculationAndBatching() {
     void testRandomSegments() {
         List<TextSegment> segments = createRandomSegments(10, 100);
 
-        assertThat(segments.size()).isEqualTo(10);
+        assertThat(segments).hasSize(10);
         for (TextSegment segment : segments) {
             assertThat(segment.text()).hasSizeLessThan(100);
         }
@@ -130,7 +130,7 @@ void testBatchingEmbeddings() {
 
         List<Embedding> embeddings = model.embedAll(segments).content();
 
-        assertThat(embeddings.size()).isEqualTo(1234);
+        assertThat(embeddings).hasSize(1234);
     }
 
     @Test
@@ -158,7 +158,7 @@ void testBatchingEmbeddingsWithMaxSet() {
 
         List<Embedding> embeddings = model.embedAll(segments).content();
 
-        assertThat(embeddings.size()).isEqualTo(1234);
+        assertThat(embeddings).hasSize(1234);
     }
 
     @Test

File: langchain4j-ollama/src/test/java/dev/langchain4j/model/ollama/OllamaModelsIT.java
Patch:
@@ -27,6 +27,7 @@ void should_return_ollama_models_list() {
         // then
         assertThat(response.content().size()).isGreaterThan(0);
         assertThat(response.content().get(0).getName()).contains(TINY_DOLPHIN_MODEL);
+        assertThat(response.content().get(0).getModifiedAt()).isNotNull();
     }
 
     @Test

File: langchain4j-open-ai/src/main/java/dev/langchain4j/model/openai/InternalOpenAiHelper.java
Patch:
@@ -133,6 +133,7 @@ public static Message toOpenAiMessage(ChatMessage message) {
                     .collect(toList());
 
             return AssistantMessage.builder()
+                    .content(aiMessage.text())
                     .toolCalls(toolCalls)
                     .build();
         }

File: code-execution-engines/langchain4j-code-execution-engine-graalvm-polyglot/src/test/java/dev/langchain4j/agent/tool/graalvm/GraalVmJavaScriptExecutionToolIT.java
Patch:
@@ -5,6 +5,7 @@
 import dev.langchain4j.service.AiServices;
 import org.junit.jupiter.api.Test;
 
+import static dev.langchain4j.model.openai.OpenAiChatModelName.GPT_4_O_MINI;
 import static org.assertj.core.api.Assertions.assertThat;
 import static org.mockito.Mockito.*;
 
@@ -14,6 +15,7 @@ class GraalVmJavaScriptExecutionToolIT {
             .baseUrl(System.getenv("OPENAI_BASE_URL"))
             .apiKey(System.getenv("OPENAI_API_KEY"))
             .organizationId(System.getenv("OPENAI_ORGANIZATION_ID"))
+            .modelName(GPT_4_O_MINI)
             .logRequests(true)
             .logResponses(true)
             .build();

File: code-execution-engines/langchain4j-code-execution-engine-graalvm-polyglot/src/test/java/dev/langchain4j/agent/tool/graalvm/GraalVmPythonExecutionToolIT.java
Patch:
@@ -5,6 +5,7 @@
 import dev.langchain4j.service.AiServices;
 import org.junit.jupiter.api.Test;
 
+import static dev.langchain4j.model.openai.OpenAiChatModelName.GPT_4_O_MINI;
 import static org.assertj.core.api.Assertions.assertThat;
 import static org.mockito.Mockito.*;
 
@@ -14,6 +15,7 @@ class GraalVmPythonExecutionToolIT {
             .baseUrl(System.getenv("OPENAI_BASE_URL"))
             .apiKey(System.getenv("OPENAI_API_KEY"))
             .organizationId(System.getenv("OPENAI_ORGANIZATION_ID"))
+            .modelName(GPT_4_O_MINI)
             .build();
 
     interface Assistant {

File: langchain4j/src/test/java/dev/langchain4j/service/AiServicesWithToolsIT.java
Patch:
@@ -50,7 +50,6 @@
 import static dev.langchain4j.agent.tool.JsonSchemaProperty.items;
 import static dev.langchain4j.agent.tool.JsonSchemaProperty.type;
 import static dev.langchain4j.model.mistralai.MistralAiChatModelName.MISTRAL_LARGE_LATEST;
-import static dev.langchain4j.model.openai.OpenAiChatModelName.GPT_3_5_TURBO_0613;
 import static dev.langchain4j.model.openai.OpenAiChatModelName.GPT_4_O;
 import static dev.langchain4j.model.openai.OpenAiChatModelName.GPT_4_O_MINI;
 import static dev.langchain4j.model.output.FinishReason.STOP;
@@ -121,7 +120,7 @@ static Stream<ChatLanguageModel> modelsWithoutParallelToolCalling() {
                         .baseUrl(System.getenv("OPENAI_BASE_URL"))
                         .apiKey(System.getenv("OPENAI_API_KEY"))
                         .organizationId(System.getenv("OPENAI_ORGANIZATION_ID"))
-                        .modelName(GPT_3_5_TURBO_0613) // this model can only call tools sequentially
+                        .modelName(GPT_4_O_MINI)
                         .temperature(0.0)
                         .logRequests(true)
                         .logResponses(true)

File: web-search-engines/langchain4j-web-search-engine-google-custom/src/test/java/dev/langchain4j/web/search/google/customsearch/GoogleCustomWebSearchContentRetrieverIT.java
Patch:
@@ -52,7 +52,7 @@ void should_retrieve_web_content_with_google_and_use_AiServices_to_summary_respo
         String answer = assistant.answer(query);
 
         // then
-        assertThat(answer).contains("RAG");
+        assertThat(answer).containsIgnoringCase("memory");
     }
 
     @Override

File: web-search-engines/langchain4j-web-search-engine-searchapi/src/test/java/SearchApiWebSearchToolIT.java
Patch:
@@ -128,9 +128,9 @@ void should_execute_searchApi_tool_with_chatLanguageModel_to_summary_response_in
 
         // then
         assertThat(finalResponse.text())
-                .as("The result string should contain 'madrid' and 'tourist' ignoring case")
+                .as("The result string should contain 'Madrid' and 'Palace' ignoring case")
                 .containsIgnoringCase("Madrid")
-                .containsIgnoringCase("tourist");
+                .containsIgnoringCase("Palace");
     }
 
     @Test

File: langchain4j-jlama/src/main/java/dev/langchain4j/model/jlama/JlamaModelRegistry.java
Patch:
@@ -90,7 +90,7 @@ public JlamaModel downloadModel(String modelName, Optional<String> authToken) th
             name = parts[1];
         }
 
-        File modelDir = SafeTensorSupport.maybeDownloadModel(modelCachePath.toString(), Optional.ofNullable(owner), name, Optional.empty(), authToken, Optional.empty());
+        File modelDir = SafeTensorSupport.maybeDownloadModel(modelCachePath.toString(), Optional.ofNullable(owner), name, true, Optional.empty(), authToken, Optional.empty());
 
         File config = new File(modelDir, "config.json");
         ModelSupport.ModelType type = SafeTensorSupport.detectModel(config);

File: langchain4j-jlama/src/test/java/dev/langchain4j/model/jlama/JlamaChatModelIT.java
Patch:
@@ -29,7 +29,7 @@ static void setup() {
                 .modelName("tjake/Meta-Llama-3.1-8B-Instruct-Jlama-Q4")
                 .modelCachePath(tmpDir.toPath())
                 .temperature(0.0f)
-                .maxTokens(30)
+                .maxTokens(64)
                 .build();
     }
 

File: langchain4j-jlama/src/test/java/dev/langchain4j/model/jlama/JlamaLanguageModelIT.java
Patch:
@@ -26,7 +26,7 @@ static void setup() {
                 .modelName("tjake/Meta-Llama-3.1-8B-Instruct-Jlama-Q4")
                 .modelCachePath(tmpDir.toPath())
                 .temperature(0.0f)
-                .maxTokens(30)
+                .maxTokens(64)
                 .build();
     }
 

File: langchain4j-jlama/src/test/java/dev/langchain4j/model/jlama/JlamaStreamingChatModelIT.java
Patch:
@@ -27,7 +27,7 @@ static void setup() {
         model = JlamaStreamingChatModel.builder()
                 .modelName("tjake/Meta-Llama-3.1-8B-Instruct-Jlama-Q4")
                 .modelCachePath(tmpDir.toPath())
-                .maxTokens(30)
+                .maxTokens(64)
                 .temperature(0.0f)
                 .build();
     }

File: langchain4j-jlama/src/test/java/dev/langchain4j/model/jlama/JlamaStreamingLanguageModelIT.java
Patch:
@@ -26,7 +26,7 @@ static void setup() {
         model = JlamaStreamingLanguageModel.builder()
                 .modelName("tjake/Meta-Llama-3.1-8B-Instruct-Jlama-Q4")
                 .modelCachePath(tmpDir.toPath())
-                .maxTokens(30)
+                .maxTokens(64)
                 .temperature(0.0f)
                 .build();
     }

File: document-loaders/langchain4j-document-loader-google-cloud-storage/src/main/java/dev/langchain4j/data/document/loader/gcs/GoogleCloudStorageDocumentLoader.java
Patch:
@@ -9,7 +9,7 @@
 import dev.langchain4j.data.document.Document;
 import dev.langchain4j.data.document.DocumentLoader;
 import dev.langchain4j.data.document.DocumentParser;
-import dev.langchain4j.data.document.source.GcsSource;
+import dev.langchain4j.data.document.source.gcs.GcsSource;
 
 import java.util.ArrayList;
 import java.util.List;

File: langchain4j-google-ai-gemini/src/main/java/dev/langchain4j/model/googleai/PartsAndContentsMapper.java
Patch:
@@ -219,9 +219,9 @@ static List<GeminiContent> fromMessageToGContent(List<ChatMessage> messages, Gem
                                 .role(GeminiRole.MODEL.toString())
                                 .parts(((AiMessage) msg).toolExecutionRequests().stream()
                                     .map(toolExecutionRequest -> GeminiPart.builder()
-                                        .functionResponse(GeminiFunctionResponse.builder()
+                                        .functionCall(GeminiFunctionCall.builder()
                                             .name(toolExecutionRequest.name())
-                                            .response(GSON.fromJson(toolExecutionRequest.arguments(), Map.class))
+                                            .args(GSON.fromJson(toolExecutionRequest.arguments(), Map.class))
                                             .build())
                                         .build())
                                     .collect(Collectors.toList()))

File: langchain4j-vertex-ai-gemini/src/test/java/dev/langchain4j/model/vertexai/VertexAiGeminiChatModelIT.java
Patch:
@@ -645,7 +645,7 @@ void should_support_json_response_mime_type() {
         assertThat(json).isEqualToIgnoringWhitespace(expectedJson);
     }
 
-    @RetryingTest(2)
+    @RetryingTest(4)
     void should_allow_defining_safety_settings() {
         // given
         HashMap<HarmCategory, SafetyThreshold> safetySettings = new HashMap<>();
@@ -665,7 +665,7 @@ void should_allow_defining_safety_settings() {
 
         // when
         Exception exception = assertThrows(RuntimeException.class, () -> {
-            model.generate("You're a dumb bastard!!!");
+            model.generate("You're a dumb fucking bastard!!! I'm gonna kill you!");
         });
 
         // then

File: langchain4j-anthropic/src/main/java/dev/langchain4j/model/anthropic/internal/api/AnthropicContent.java
Patch:
@@ -14,7 +14,7 @@
 @JsonNaming(SnakeCaseStrategy.class)
 public class AnthropicContent {
 
-    public String type;
+    public AnthropicContentBlockType type;
 
     // when type = "text"
     public String text;

File: langchain4j-anthropic/src/main/java/dev/langchain4j/model/anthropic/internal/api/AnthropicCreateMessageRequest.java
Patch:
@@ -32,4 +32,5 @@ public class AnthropicCreateMessageRequest {
     public Double topP;
     public Integer topK;
     public List<AnthropicTool> tools;
+    public AnthropicToolChoice toolChoice;
 }

File: langchain4j-anthropic/src/main/java/dev/langchain4j/model/anthropic/internal/api/AnthropicDelta.java
Patch:
@@ -15,6 +15,7 @@ public class AnthropicDelta {
     // when AnthropicStreamingData.type = "content_block_delta"
     public String type;
     public String text;
+    public String partialJson;
 
     // when AnthropicStreamingData.type = "message_delta"
     public String stopReason;

File: langchain4j-anthropic/src/main/java/dev/langchain4j/model/anthropic/internal/mapper/AnthropicMapper.java
Patch:
@@ -130,12 +130,12 @@ public static String toAnthropicSystemPrompt(List<ChatMessage> messages) {
     public static AiMessage toAiMessage(List<AnthropicContent> contents) {
 
         String text = contents.stream()
-                .filter(content -> "text".equals(content.type))
+                .filter(content -> AnthropicContentBlockType.TEXT == content.type)
                 .map(content -> content.text)
                 .collect(joining("\n"));
 
         List<ToolExecutionRequest> toolExecutionRequests = contents.stream()
-                .filter(content -> "tool_use".equals(content.type))
+                .filter(content -> AnthropicContentBlockType.TOOL_USE == content.type)
                 .map(content -> {
                     try {
                         return ToolExecutionRequest.builder()

File: langchain4j-ollama/src/main/java/dev/langchain4j/model/ollama/OllamaMessagesUtils.java
Patch:
@@ -42,7 +42,7 @@ static List<Tool> toOllamaTools(List<ToolSpecification> toolSpecifications) {
                                 .function(Function.builder()
                                         .name(toolSpecification.name())
                                         .description(toolSpecification.description())
-                                        .parameters(Parameters.builder()
+                                        .parameters(toolSpecification.parameters() == null ? null : Parameters.builder()
                                                 .properties(toolSpecification.parameters().properties())
                                                 .required(toolSpecification.parameters().required())
                                                 .build())

File: langchain4j-hugging-face/src/test/java/dev/langchain4j/model/huggingface/HuggingFaceEmbeddingModelIT.java
Patch:
@@ -16,6 +16,7 @@ class HuggingFaceEmbeddingModelIT {
     HuggingFaceEmbeddingModel model = HuggingFaceEmbeddingModel.builder()
             .accessToken(System.getenv("HF_API_KEY"))
             .modelId("sentence-transformers/all-MiniLM-L6-v2")
+            .waitForModel(true)
             .build();
 
     @Test

File: langchain4j/src/test/java/dev/langchain4j/service/StreamingAiServicesWithToolsIT.java
Patch:
@@ -248,7 +248,7 @@ void should_use_tool_provider() throws Exception {
                 .toolProvider(toolProvider)
                 .build();
 
-        String userMessage = "What are the amounts of transactions T001 and T002?";
+        String userMessage = "What is the amounts of transactions T001?";
 
         // when
         CompletableFuture<Response<AiMessage>> future = new CompletableFuture<>();
@@ -261,7 +261,7 @@ void should_use_tool_provider() throws Exception {
         Response<AiMessage> response = future.get(60, SECONDS);
 
         // then
-        assertThat(response.content().text()).contains("42", "57");
+        assertThat(response.content().text()).contains("42");
 
         // then
         verify(toolExecutor).execute(any(), any());

File: langchain4j/src/main/java/dev/langchain4j/service/V.java
Patch:
@@ -29,7 +29,7 @@
  * String chat(@V String name, @V int age);
  * </pre>
  * <p>
- * When using Spring Boot, defining the value of this annotation is not required.
+ * When using LangChain4j with Quarkus or Spring Boot, using this annotation is not necessary.
  *
  * @see UserMessage
  * @see SystemMessage

File: langchain4j/src/test/java/dev/langchain4j/service/AiServicesWithToolsIT.java
Patch:
@@ -687,7 +687,7 @@ void should_use_tool_with_pojo(ChatLanguageModel chatLanguageModel) {
                 .tools(queryService)
                 .build();
 
-        Response<AiMessage> response = assistant.chat("List names of 3 users where country is India");
+        Response<AiMessage> response = assistant.chat("Give me the names of 3 users from India");
 
         assertThat(response.content().text()).contains("Amar", "Akbar", "Antony");
     }

File: langchain4j-core/src/main/java/dev/langchain4j/data/document/Metadata.java
Patch:
@@ -53,7 +53,7 @@ public class Metadata {
      * Construct a Metadata object with an empty map of key-value pairs.
      */
     public Metadata() {
-        this(new HashMap<>());
+        this.metadata = new HashMap<>();
     }
 
     /**

File: langchain4j-azure-open-ai/src/test/java/dev/langchain4j/model/azure/AzureOpenAIResponsibleAIIT.java
Patch:
@@ -42,7 +42,7 @@ void chat_message_should_trigger_content_filter_for_violence(String deploymentNa
                 .endpoint(System.getenv("AZURE_OPENAI_ENDPOINT"))
                 .apiKey(System.getenv("AZURE_OPENAI_KEY"))
                 .deploymentName(deploymentName)
-                .tokenizer(new AzureOpenAiTokenizer(gptVersion))
+                .temperature(0.0)
                 .logRequestsAndResponses(true)
                 .build();
 

File: langchain4j/src/test/java/dev/langchain4j/service/AiServicesWithToolsIT.java
Patch:
@@ -435,8 +435,7 @@ void should_use_tool_with_List_of_Strings_parameter(ChatLanguageModel chatLangua
                 .tools(stringListProcessor)
                 .build();
 
-        String userMessage = "Process strings 'cat' and 'dog' together, do not separate them!. " +
-                "Use format ['cat', 'dog'] for the list of strings."; // Specify the format expected to avoid ambiguity
+        String userMessage = "Process strings 'cat' and 'dog' together in a single tool call.";
 
         // when
         assistant.chat(userMessage);

File: langchain4j-mongodb-atlas/src/test/java/dev/langchain4j/store/embedding/mongodb/MongoDbEmbeddingStoreLocalIT.java
Patch:
@@ -10,7 +10,6 @@
 import dev.langchain4j.model.embedding.onnx.allminilml6v2q.AllMiniLmL6V2QuantizedEmbeddingModel;
 import dev.langchain4j.store.embedding.EmbeddingStore;
 import dev.langchain4j.store.embedding.EmbeddingStoreIT;
-import lombok.SneakyThrows;
 import org.bson.codecs.configuration.CodecRegistry;
 import org.bson.codecs.pojo.PojoCodecProvider;
 import org.bson.conversions.Bson;
@@ -49,7 +48,6 @@ class MongoDbEmbeddingStoreLocalIT extends EmbeddingStoreIT {
             .build();
 
     @BeforeAll
-    @SneakyThrows
     static void start() {
         mongodb.start();
 

File: langchain4j/src/test/java/dev/langchain4j/service/AiServicesWithToolsIT.java
Patch:
@@ -107,7 +107,7 @@ static Stream<ChatLanguageModel> models() {
                         .temperature(0.0f)
                         .maxTokens(300)
                         .region(Region.US_EAST_1)
-                        .model(BedrockAnthropicMessageChatModel.Types.AnthropicClaude3_5SonnetV1.getValue())
+                        .model(BedrockAnthropicMessageChatModel.Types.AnthropicClaude3SonnetV1.getValue())
                         .maxRetries(1)
                         .timeout(Duration.ofMinutes(2L))
                         .build()
@@ -131,7 +131,7 @@ static Stream<ChatLanguageModel> modelsWithoutParallelToolCalling() {
                         .temperature(0.0f)
                         .maxTokens(300)
                         .region(Region.US_EAST_1)
-                        .model(BedrockAnthropicMessageChatModel.Types.AnthropicClaude3_5SonnetV1.getValue())
+                        .model(BedrockAnthropicMessageChatModel.Types.AnthropicClaude3SonnetV1.getValue())
                         .maxRetries(1)
                         .timeout(Duration.ofMinutes(2L))
                         .build()

File: langchain4j-google-ai-gemini/src/main/java/dev/langchain4j/model/googleai/GeminiGenerateContentRequest.java
Patch:
@@ -8,6 +8,7 @@
 @Data
 @Builder
 class GeminiGenerateContentRequest {
+    private String model;
     private List<GeminiContent> contents;
     private GeminiTool tools;
     private GeminiToolConfig toolConfig;

File: langchain4j-vertex-ai/src/main/java/dev/langchain4j/model/vertexai/VertexAiEmbeddingModel.java
Patch:
@@ -132,7 +132,7 @@ public Response<List<Embedding>> embedAll(List<TextSegment> segments) {
                         embeddingInstance.setTaskType(taskType);
                         if (this.taskType.equals(TaskType.RETRIEVAL_DOCUMENT)) {
                             // Title metadata is used for calculating embeddings for document retrieval
-                            embeddingInstance.setTitle(segment.metadata(titleMetadataKey));
+                            embeddingInstance.setTitle(segment.metadata().getString(titleMetadataKey));
                         }
                     }
 

File: langchain4j-azure-open-ai/src/test/java/dev/langchain4j/model/azure/AzureOpenAiChatModelListenerIT.java
Patch:
@@ -41,7 +41,7 @@ protected ChatLanguageModel createFailingModel(ChatModelListener listener) {
     }
 
     @Override
-    protected Class<?> expectedExceptionClass() {
+    protected Class<? extends Exception> expectedExceptionClass() {
         return ClientAuthenticationException.class;
     }
 }

File: langchain4j-azure-open-ai/src/test/java/dev/langchain4j/model/azure/AzureOpenAiStreamingChatModelListenerIT.java
Patch:
@@ -42,7 +42,7 @@ protected StreamingChatLanguageModel createFailingModel(ChatModelListener listen
     }
 
     @Override
-    protected Class<?> expectedExceptionClass() {
+    protected Class<? extends Exception> expectedExceptionClass() {
         return ClientAuthenticationException.class;
     }
 

File: langchain4j-google-ai-gemini/src/test/java/dev/langchain4j/model/googleai/GoogleAiGeminiChatModelListenerIT.java
Patch:
@@ -43,7 +43,7 @@ protected ChatLanguageModel createFailingModel(ChatModelListener listener) {
     }
 
     @Override
-    protected Class<?> expectedExceptionClass() {
+    protected Class<? extends Exception> expectedExceptionClass() {
         return RuntimeException.class;
     }
 }

File: langchain4j-ollama/src/main/java/dev/langchain4j/model/ollama/OllamaMessagesUtils.java
Patch:
@@ -34,6 +34,9 @@ static List<Message> toOllamaMessages(List<ChatMessage> messages) {
     }
 
     static List<Tool> toOllamaTools(List<ToolSpecification> toolSpecifications) {
+        if (toolSpecifications == null) {
+            return null;
+        }
         return toolSpecifications.stream().map(toolSpecification ->
                         Tool.builder()
                                 .function(Function.builder()

File: langchain4j-ollama/src/test/java/dev/langchain4j/model/ollama/AbstractOllamaToolsLanguageModelInfrastructure.java
Patch:
@@ -14,5 +14,4 @@ class AbstractOllamaToolsLanguageModelInfrastructure {
     }
 
 
-
 }

File: langchain4j-open-ai/src/test/java/dev/langchain4j/model/openai/OpenAiChatModelListenerIT.java
Patch:
@@ -41,7 +41,7 @@ protected ChatLanguageModel createFailingModel(ChatModelListener listener) {
     }
 
     @Override
-    protected Class<?> expectedExceptionClass() {
+    protected Class<? extends Exception> expectedExceptionClass() {
         return OpenAiHttpException.class;
     }
 }

File: langchain4j-open-ai/src/test/java/dev/langchain4j/model/openai/OpenAiStreamingChatModelListenerIT.java
Patch:
@@ -42,7 +42,7 @@ protected StreamingChatLanguageModel createFailingModel(ChatModelListener listen
     }
 
     @Override
-    protected Class<?> expectedExceptionClass() {
+    protected Class<? extends Exception> expectedExceptionClass() {
         return OpenAiHttpException.class;
     }
 }
\ No newline at end of file

File: langchain4j-vertex-ai-gemini/src/test/java/dev/langchain4j/model/vertexai/VertexAiGeminiChatModelListenerIT.java
Patch:
@@ -45,7 +45,7 @@ protected ChatLanguageModel createFailingModel(ChatModelListener listener) {
     }
 
     @Override
-    protected Class<?> expectedExceptionClass() {
+    protected Class<? extends Exception> expectedExceptionClass() {
         return RuntimeException.class;
     }
 }

File: langchain4j-vertex-ai-gemini/src/test/java/dev/langchain4j/model/vertexai/VertexAiGeminiStreamingChatModelListenerIT.java
Patch:
@@ -46,7 +46,7 @@ protected StreamingChatLanguageModel createFailingModel(ChatModelListener listen
     }
 
     @Override
-    protected Class<?> expectedExceptionClass() {
+    protected Class<? extends Exception> expectedExceptionClass() {
         return NotFoundException.class;
     }
 }

File: langchain4j-azure-open-ai/src/test/java/dev/langchain4j/model/azure/AzureOpenAiChatModelIT.java
Patch:
@@ -367,7 +367,7 @@ public String getLocation() {
     void afterEach() throws InterruptedException {
         String ciDelaySeconds = System.getenv("CI_DELAY_SECONDS_AZURE_OPENAI");
         if (ciDelaySeconds != null) {
-            Thread.sleep(Integer.parseInt(ciDelaySeconds));
+            Thread.sleep(Integer.parseInt(ciDelaySeconds) * 1000L);
         }
     }
 }

File: langchain4j-azure-open-ai/src/test/java/dev/langchain4j/model/azure/AzureOpenAiStreamingChatModelIT.java
Patch:
@@ -409,7 +409,7 @@ void tools_should_work_without_tokenizer() {
     void afterEach() throws InterruptedException {
         String ciDelaySeconds = System.getenv("CI_DELAY_SECONDS_AZURE_OPENAI");
         if (ciDelaySeconds != null) {
-            Thread.sleep(Integer.parseInt(ciDelaySeconds));
+            Thread.sleep(Integer.parseInt(ciDelaySeconds) * 1000L);
         }
     }
 }

File: langchain4j-bedrock/src/test/java/dev/langchain4j/model/bedrock/BedrockChatModelIT.java
Patch:
@@ -337,7 +337,7 @@ void testBedrockMistralAiMixtral8x7bInstructChatModel() {
     void afterEach() throws InterruptedException {
         String ciDelaySeconds = System.getenv("CI_DELAY_SECONDS_BEDROCK");
         if (ciDelaySeconds != null) {
-            Thread.sleep(Integer.parseInt(ciDelaySeconds));
+            Thread.sleep(Integer.parseInt(ciDelaySeconds) * 1000L);
         }
     }
 }

File: langchain4j-vertex-ai-gemini/src/test/java/dev/langchain4j/model/vertexai/VertexAiGeminiChatModelIT.java
Patch:
@@ -918,7 +918,7 @@ void should_support_enum_structured_output() {
     void afterEach() throws InterruptedException {
         String ciDelaySeconds = System.getenv("CI_DELAY_SECONDS_VERTEX_AI_GEMINI");
         if (ciDelaySeconds != null) {
-            Thread.sleep(Integer.parseInt(ciDelaySeconds));
+            Thread.sleep(Integer.parseInt(ciDelaySeconds) * 1000L);
         }
     }
 }
\ No newline at end of file

File: langchain4j-core/src/test/java/dev/langchain4j/model/chat/ChatModelListenerIT.java
Patch:
@@ -9,6 +9,7 @@
 import dev.langchain4j.model.chat.listener.ChatModelRequestContext;
 import dev.langchain4j.model.chat.listener.ChatModelResponse;
 import dev.langchain4j.model.chat.listener.ChatModelResponseContext;
+import org.assertj.core.data.Percentage;
 import org.junit.jupiter.api.Test;
 
 import java.util.concurrent.atomic.AtomicReference;
@@ -116,7 +117,7 @@ public void onError(ChatModelErrorContext errorContext) {
         // then
         ChatModelRequest request = requestReference.get();
         assertThat(request.model()).isEqualTo(modelName());
-        assertThat(request.temperature()).isEqualTo(temperature());
+        assertThat(request.temperature()).isCloseTo(temperature(), Percentage.withPercentage(1));
         assertThat(request.topP()).isEqualTo(topP());
         assertThat(request.maxTokens()).isEqualTo(maxTokens());
         assertThat(request.messages()).containsExactly(userMessage);

File: langchain4j-core/src/test/java/dev/langchain4j/model/chat/StreamingChatModelListenerIT.java
Patch:
@@ -11,6 +11,7 @@
 import dev.langchain4j.model.chat.listener.ChatModelResponse;
 import dev.langchain4j.model.chat.listener.ChatModelResponseContext;
 import dev.langchain4j.model.output.Response;
+import org.assertj.core.data.Percentage;
 import org.junit.jupiter.api.Test;
 
 import java.util.concurrent.CompletableFuture;
@@ -122,7 +123,7 @@ public void onError(ChatModelErrorContext errorContext) {
         // then
         ChatModelRequest request = requestReference.get();
         assertThat(request.model()).isEqualTo(modelName());
-        assertThat(request.temperature()).isEqualTo(temperature());
+        assertThat(request.temperature()).isCloseTo(temperature(), Percentage.withPercentage(1));
         assertThat(request.topP()).isEqualTo(topP());
         assertThat(request.maxTokens()).isEqualTo(maxTokens());
         assertThat(request.messages()).containsExactly(userMessage);

File: langchain4j-vertex-ai-gemini/src/test/java/dev/langchain4j/model/vertexai/VertexAiGeminiChatModelIT.java
Patch:
@@ -366,7 +366,7 @@ void should_accept_tools_for_function_calling() {
         assertThat(weatherResponse.content().text()).containsIgnoringCase("sunny");
     }
 
-    @Test
+    @RetryingTest(5)
     void should_handle_parallel_function_calls() {
         // given
         ChatLanguageModel model = VertexAiGeminiChatModel.builder()

File: langchain4j-nomic/src/main/java/dev/langchain4j/model/nomic/EmbeddingRequest.java
Patch:
@@ -1,10 +1,12 @@
 package dev.langchain4j.model.nomic;
 
 import lombok.Builder;
+import lombok.Getter;
 
 import java.util.List;
 
 @Builder
+@Getter
 class EmbeddingRequest {
 
     private String model;

File: langchain4j-vertex-ai/src/test/java/dev/langchain4j/model/vertexai/VertexAiImageModelIT.java
Patch:
@@ -3,6 +3,7 @@
 import dev.langchain4j.data.image.Image;
 import dev.langchain4j.model.output.Response;
 import org.junit.jupiter.api.BeforeEach;
+import org.junit.jupiter.api.Disabled;
 import org.junit.jupiter.api.Test;
 
 import java.io.File;
@@ -20,6 +21,7 @@
 import static org.assertj.core.api.AssertionsForClassTypes.assertThatExceptionOfType;
 import static org.junit.jupiter.api.Assertions.assertDoesNotThrow;
 
+@Disabled("Run manually before release. Expensive to run very often.")
 public class VertexAiImageModelIT {
 
     private static final String ENDPOINT = System.getenv("GCP_VERTEXAI_ENDPOINT");

File: langchain4j-cohere/src/main/java/dev/langchain4j/model/cohere/EmbedRequest.java
Patch:
@@ -1,9 +1,11 @@
 package dev.langchain4j.model.cohere;
 
 import lombok.Builder;
+import lombok.Getter;
 
 import java.util.List;
 
+@Getter
 @Builder
 class EmbedRequest {
 

File: langchain4j-cohere/src/main/java/dev/langchain4j/model/cohere/RerankRequest.java
Patch:
@@ -1,9 +1,11 @@
 package dev.langchain4j.model.cohere;
 
 import lombok.Builder;
+import lombok.Getter;
 
 import java.util.List;
 
+@Getter
 @Builder
 class RerankRequest {
 

File: langchain4j-azure-open-ai/src/test/java/dev/langchain4j/model/azure/AzureOpenAiStreamingChatModelIT.java
Patch:
@@ -106,9 +106,9 @@ void should_custom_models_work(String deploymentName, String gptVersion, boolean
         OpenAIAsyncClient asyncClient = null;
         OpenAIClient client = null;
         if (useCustomAsyncClient) {
-            asyncClient = InternalAzureOpenAiHelper.setupAsyncClient(System.getenv("AZURE_OPENAI_ENDPOINT"), gptVersion, System.getenv("AZURE_OPENAI_KEY"), Duration.ofSeconds(30), 5, null, true, null);
+            asyncClient = InternalAzureOpenAiHelper.setupAsyncClient(System.getenv("AZURE_OPENAI_ENDPOINT"), gptVersion, System.getenv("AZURE_OPENAI_KEY"), Duration.ofSeconds(30), 5, null, true, null, null);
         } else {
-            client = InternalAzureOpenAiHelper.setupSyncClient(System.getenv("AZURE_OPENAI_ENDPOINT"), gptVersion, System.getenv("AZURE_OPENAI_KEY"), Duration.ofSeconds(30), 5, null, true, null);
+            client = InternalAzureOpenAiHelper.setupSyncClient(System.getenv("AZURE_OPENAI_ENDPOINT"), gptVersion, System.getenv("AZURE_OPENAI_KEY"), Duration.ofSeconds(30), 5, null, true, null, null);
         }
 
         StreamingChatLanguageModel model = AzureOpenAiStreamingChatModel.builder()

File: langchain4j-azure-open-ai/src/test/java/dev/langchain4j/model/azure/InternalAzureOpenAiHelperTest.java
Patch:
@@ -38,7 +38,7 @@ void setupOpenAIClientShouldReturnClientWithCorrectConfiguration() {
         Integer maxRetries = 5;
         boolean logRequestsAndResponses = true;
 
-        OpenAIClient client = InternalAzureOpenAiHelper.setupSyncClient(endpoint, serviceVersion, apiKey, timeout, maxRetries, null, logRequestsAndResponses, null);
+        OpenAIClient client = InternalAzureOpenAiHelper.setupSyncClient(endpoint, serviceVersion, apiKey, timeout, maxRetries, null, logRequestsAndResponses, null, null);
 
         assertThat(client).isNotNull();
     }
@@ -52,7 +52,7 @@ void setupOpenAIAsyncClientShouldReturnClientWithCorrectConfiguration() {
         Integer maxRetries = 5;
         boolean logRequestsAndResponses = true;
 
-        OpenAIAsyncClient client = InternalAzureOpenAiHelper.setupAsyncClient(endpoint, serviceVersion, apiKey, timeout, maxRetries, null, logRequestsAndResponses, null);
+        OpenAIAsyncClient client = InternalAzureOpenAiHelper.setupAsyncClient(endpoint, serviceVersion, apiKey, timeout, maxRetries, null, logRequestsAndResponses, null, null);
 
         assertThat(client).isNotNull();
     }

File: langchain4j-ollama/src/main/java/dev/langchain4j/model/ollama/OllamaApi.java
Patch:
@@ -40,7 +40,7 @@ interface OllamaApi {
     @Headers({"Content-Type: application/json"})
     Call<Void> deleteModel(@Body DeleteModelRequest deleteModelRequest);
 
-    @GET( "api/ps")
+    @GET("api/ps")
     @Headers({"Content-Type: application/json"})
     Call<RunningModelsListResponse> listRunningModels();
 }

File: langchain4j-ollama/src/main/java/dev/langchain4j/model/ollama/OllamaDateDeserializer.java
Patch:
@@ -19,7 +19,7 @@ public OffsetDateTime deserialize(JsonParser p, DeserializationContext ctxt) thr
         String date = p.getText();
         if (date.contains(".")) {
             String[] parts = date.split("\\.");
-            if(parts[1].contains("+")) {
+            if (parts[1].contains("+")) {
                 String nanoseconds = parts[1].substring(0, parts[1].indexOf('+'));
                 date = date.replaceAll(nanoseconds, "");
             } else {

File: langchain4j-ollama/src/main/java/dev/langchain4j/model/ollama/OllamaMessagesUtils.java
Patch:
@@ -3,7 +3,6 @@
 import dev.langchain4j.agent.tool.ToolExecutionRequest;
 import dev.langchain4j.agent.tool.ToolSpecification;
 import dev.langchain4j.data.message.*;
-import dev.langchain4j.internal.Json;
 
 import java.util.List;
 import java.util.Map;
@@ -12,6 +11,7 @@
 
 import static dev.langchain4j.data.message.ContentType.IMAGE;
 import static dev.langchain4j.data.message.ContentType.TEXT;
+import static dev.langchain4j.model.ollama.OllamaJsonUtils.toJson;
 
 class OllamaMessagesUtils {
 
@@ -48,7 +48,7 @@ static List<ToolExecutionRequest> toToolExecutionRequest(List<ToolCall> toolCall
         return toolCalls.stream().map(toolCall ->
                         ToolExecutionRequest.builder()
                                 .name(toolCall.getFunction().getName())
-                                .arguments(Json.toJson(toolCall.getFunction().getArguments()))
+                                .arguments(toJson(toolCall.getFunction().getArguments()))
                                 .build())
                 .collect(Collectors.toList());
     }

File: langchain4j-bedrock/src/main/java/dev/langchain4j/model/bedrock/internal/AbstractBedrockChatModel.java
Patch:
@@ -17,6 +17,8 @@
 import software.amazon.awssdk.services.bedrockruntime.model.InvokeModelRequest;
 import software.amazon.awssdk.services.bedrockruntime.model.InvokeModelResponse;
 
+import java.time.Duration;
+
 /**
  * Bedrock chat model
  */
@@ -95,6 +97,7 @@ private BedrockRuntimeClient initClient() {
         return BedrockRuntimeClient.builder()
                 .region(region)
                 .credentialsProvider(credentialsProvider)
+                .overrideConfiguration(c-> c.apiCallTimeout(timeout))
                 .build();
     }
 }

File: langchain4j-bedrock/src/main/java/dev/langchain4j/model/bedrock/internal/AbstractBedrockStreamingChatModel.java
Patch:
@@ -74,6 +74,7 @@ private BedrockRuntimeAsyncClient initAsyncClient() {
         BedrockRuntimeAsyncClient client = BedrockRuntimeAsyncClient.builder()
                 .region(region)
                 .credentialsProvider(credentialsProvider)
+                .overrideConfiguration(c-> c.apiCallTimeout(timeout))
                 .build();
         return client;
     }

File: langchain4j-hugging-face/src/main/java/dev/langchain4j/model/huggingface/HuggingFaceChatModel.java
Patch:
@@ -83,7 +83,7 @@ public Response<AiMessage> generate(List<ChatMessage> messages) {
 
         TextGenerationResponse textGenerationResponse = client.chat(request);
 
-        return Response.from(AiMessage.from(textGenerationResponse.generatedText()));
+        return Response.from(AiMessage.from(textGenerationResponse.getGeneratedText()));
     }
 
     public static Builder builder() {

File: langchain4j-hugging-face/src/main/java/dev/langchain4j/model/huggingface/HuggingFaceLanguageModel.java
Patch:
@@ -77,7 +77,7 @@ public Response<String> generate(String prompt) {
 
         TextGenerationResponse response = client.generate(request);
 
-        return Response.from(response.generatedText());
+        return Response.from(response.getGeneratedText());
     }
 
     public static Builder builder() {

File: langchain4j-hugging-face/src/main/java/dev/langchain4j/model/huggingface/client/HuggingFaceClient.java
Patch:
@@ -3,6 +3,7 @@
 import java.util.List;
 
 public interface HuggingFaceClient {
+
     TextGenerationResponse chat(TextGenerationRequest request);
 
     TextGenerationResponse generate(TextGenerationRequest request);

File: langchain4j-hugging-face/src/test/java/dev/langchain4j/model/huggingface/HuggingFaceEmbeddingModelIT.java
Patch:
@@ -2,13 +2,15 @@
 
 import dev.langchain4j.data.embedding.Embedding;
 import org.junit.jupiter.api.Test;
+import org.junit.jupiter.api.condition.EnabledIfEnvironmentVariable;
 
 import java.util.List;
 
 import static dev.langchain4j.data.segment.TextSegment.textSegment;
 import static java.util.Arrays.asList;
 import static org.assertj.core.api.Assertions.assertThat;
 
+@EnabledIfEnvironmentVariable(named = "HF_API_KEY", matches = ".+")
 class HuggingFaceEmbeddingModelIT {
 
     HuggingFaceEmbeddingModel model = HuggingFaceEmbeddingModel.builder()

File: langchain4j-bedrock/src/main/java/dev/langchain4j/model/bedrock/BedrockAnthropicMessageChatModel.java
Patch:
@@ -11,7 +11,7 @@
 import dev.langchain4j.data.message.ImageContent;
 import dev.langchain4j.data.message.TextContent;
 import dev.langchain4j.data.message.UserMessage;
-import dev.langchain4j.internal.Json;
+import dev.langchain4j.model.bedrock.internal.Json;
 import dev.langchain4j.model.bedrock.internal.AbstractBedrockChatModel;
 import dev.langchain4j.model.output.Response;
 import java.util.Collections;

File: langchain4j-bedrock/src/main/java/dev/langchain4j/model/bedrock/BedrockMistralAiChatModel.java
Patch:
@@ -4,7 +4,7 @@
 
 import dev.langchain4j.data.message.AiMessage;
 import dev.langchain4j.data.message.ChatMessage;
-import dev.langchain4j.internal.Json;
+import dev.langchain4j.model.bedrock.internal.Json;
 import dev.langchain4j.model.bedrock.internal.AbstractBedrockChatModel;
 import dev.langchain4j.model.output.Response;
 import java.util.HashMap;

File: langchain4j-google-ai-gemini/src/main/java/dev/langchain4j/model/googleai/GeminiType.java
Patch:
@@ -1,7 +1,6 @@
 package dev.langchain4j.model.googleai;
 
 enum GeminiType {
-    TYPE_UNSPECIFIED,
     STRING,
     NUMBER,
     INTEGER,

File: langchain4j/src/main/java/dev/langchain4j/service/output/JsonSchemas.java
Patch:
@@ -31,6 +31,7 @@
 import static dev.langchain4j.internal.TypeUtils.isJsonBoolean;
 import static dev.langchain4j.internal.TypeUtils.isJsonInteger;
 import static dev.langchain4j.internal.TypeUtils.isJsonNumber;
+import static dev.langchain4j.internal.TypeUtils.isJsonString;
 import static dev.langchain4j.service.TypeUtils.getRawClass;
 import static dev.langchain4j.service.TypeUtils.resolveFirstGenericParameterClass;
 import static dev.langchain4j.service.TypeUtils.typeHasRawClass;
@@ -126,7 +127,7 @@ private static String getDescription(Description description) {
 
     private static JsonSchemaElement jsonSchema(Class<?> clazz, Type type, String fieldDescription) {
 
-        if (clazz == String.class) {
+        if (isJsonString(clazz)) {
             return JsonStringSchema.builder()
                     .description(fieldDescription)
                     .build();

File: langchain4j-google-ai-gemini/src/test/java/dev/langchain4j/model/googleai/GoogleAiGeminiChatModelIT.java
Patch:
@@ -43,7 +43,7 @@
 
 public class GoogleAiGeminiChatModelIT {
 
-    private static final String GOOGLE_AI_GEMINI_API_KEY = System.getenv("GEMINI_AI_KEY");
+    private static final String GOOGLE_AI_GEMINI_API_KEY = System.getenv("GOOGLE_AI_GEMINI_API_KEY");
 
     private static final String CAT_IMAGE_URL = "https://upload.wikimedia.org/wikipedia/commons/e/e9/Felis_silvestris_silvestris_small_gradual_decrease_of_quality.png";
     private static final String MD_FILE_URL = "https://raw.githubusercontent.com/langchain4j/langchain4j/main/docs/docs/intro.md";

File: langchain4j-google-ai-gemini/src/main/java/dev/langchain4j/model/googleai/PartsAndContentsMapper.java
Patch:
@@ -144,13 +144,13 @@ static GeminiPart fromContentToGPart(Content content) {
         }
     }
 
-    static AiMessage fromGPartsToAiMessage(List<GeminiPart> parts) {
+    static AiMessage fromGPartsToAiMessage(List<GeminiPart> parts, boolean includeCodeExecutionOutput) {
         StringBuilder fullText = new StringBuilder();
         List<GeminiFunctionCall> functionCalls = new ArrayList<>();
 
         for (GeminiPart part : parts) {
             GeminiExecutableCode executableCode = part.getExecutableCode();
-            if (executableCode != null) { //TODO maybe add a flag to the model like showCodeExecution()
+            if (executableCode != null && includeCodeExecutionOutput) {
                 fullText
                     .append("Code executed:\n")
                     .append("```python")
@@ -162,7 +162,7 @@ static AiMessage fromGPartsToAiMessage(List<GeminiPart> parts) {
             }
 
             GeminiCodeExecutionResult codeExecutionResult = part.getCodeExecutionResult();
-            if (codeExecutionResult != null) {
+            if (codeExecutionResult != null && includeCodeExecutionOutput) {
                 GeminiOutcome outcome = codeExecutionResult.getOutcome();
 
                 if (outcome != GeminiOutcome.OUTCOME_OK) {

File: langchain4j-chatglm/src/main/java/dev/langchain4j/model/chatglm/ChatGlmApi.java
Patch:
@@ -9,7 +9,7 @@ interface ChatGlmApi {
 
     int OK = 200;
 
-    @POST
+    @POST(".")
     @Headers({"Content-Type: application/json"})
     Call<ChatCompletionResponse> chatCompletion(@Body ChatCompletionRequest chatCompletionRequest);
 }

File: langchain4j-vearch/src/main/java/dev/langchain4j/store/embedding/vearch/MetricType.java
Patch:
@@ -1,6 +1,6 @@
 package dev.langchain4j.store.embedding.vearch;
 
-import com.google.gson.annotations.SerializedName;
+import com.fasterxml.jackson.annotation.JsonProperty;
 
 /**
  * if metric type is not set when searching, it will use the parameter specified when building the space
@@ -12,6 +12,6 @@ public enum MetricType {
     /**
      * Inner Product
      */
-    @SerializedName("InnerProduct")
+    @JsonProperty("InnerProduct")
     INNER_PRODUCT
 }

File: langchain4j-vearch/src/main/java/dev/langchain4j/store/embedding/vearch/VearchApi.java
Patch:
@@ -6,7 +6,7 @@
 
 import java.util.List;
 
-public interface VearchApi {
+interface VearchApi {
 
     int OK = 200;
 

File: langchain4j-dashscope/src/main/java/dev/langchain4j/model/dashscope/QwenModelName.java
Patch:
@@ -23,6 +23,8 @@ public class QwenModelName {
     public static final String QWEN2_57B_A14B_INSTRUCT = "qwen2-57b-a14b-instruct";  // Qwen open sourced 57-billion-parameters and 14-billion-activation-parameters MOE model (v2)
     public static final String QWEN_VL_PLUS = "qwen-vl-plus";  // Qwen multi-modal model, supports image and text information.
     public static final String QWEN_VL_MAX = "qwen-vl-max";  // Qwen multi-modal model, offers optimal performance on a wider range of complex tasks.
+    public static final String QWEN_AUDIO_CHAT = "qwen-audio-chat";  // Qwen open sourced speech model, sft for chatting.
+    public static final String QWEN2_AUDIO_INSTRUCT = "qwen2-audio-instruct";  // Qwen open sourced speech model (v2), sft for instruction
 
     // Use with QwenEmbeddingModel
     public static final String TEXT_EMBEDDING_V1 = "text-embedding-v1";  // Support: en, zh, es, fr, pt, id

File: langchain4j-dashscope/src/main/java/dev/langchain4j/model/dashscope/QwenChatModel.java
Patch:
@@ -81,7 +81,7 @@ protected QwenChatModel(String baseUrl,
         this.stops = stops;
         this.maxTokens = maxTokens;
         this.listeners = listeners == null ? emptyList() : new ArrayList<>(listeners);
-        this.isMultimodalModel = QwenHelper.isMultimodalModel(modelName);
+        this.isMultimodalModel = QwenHelper.isMultimodalModel(this.modelName);
 
         if (Utils.isNullOrBlank(baseUrl)) {
             this.conv = isMultimodalModel ? new MultiModalConversation() : null;

File: langchain4j-dashscope/src/main/java/dev/langchain4j/model/dashscope/QwenStreamingChatModel.java
Patch:
@@ -84,7 +84,7 @@ public QwenStreamingChatModel(String baseUrl,
         this.stops = stops;
         this.maxTokens = maxTokens;
         this.listeners = listeners == null ? emptyList() : new ArrayList<>(listeners);
-        this.isMultimodalModel = QwenHelper.isMultimodalModel(modelName);
+        this.isMultimodalModel = QwenHelper.isMultimodalModel(this.modelName);
 
         if (Utils.isNullOrBlank(baseUrl)) {
             this.conv = isMultimodalModel ? new MultiModalConversation() : null;

File: langchain4j-zhipu-ai/src/main/java/dev/langchain4j/model/zhipu/ZhipuAiChatModel.java
Patch:
@@ -26,7 +26,7 @@
 import static dev.langchain4j.internal.Utils.isNullOrEmpty;
 import static dev.langchain4j.internal.ValidationUtils.ensureNotEmpty;
 import static dev.langchain4j.model.zhipu.DefaultZhipuAiHelper.*;
-import static dev.langchain4j.model.zhipu.chat.ChatCompletionModel.GLM_4;
+import static dev.langchain4j.model.zhipu.chat.ChatCompletionModel.GLM_4_FLASH;
 import static dev.langchain4j.model.zhipu.chat.ToolChoiceMode.AUTO;
 import static dev.langchain4j.spi.ServiceHelper.loadFactories;
 import static java.util.Collections.emptyList;
@@ -68,7 +68,7 @@ public ZhipuAiChatModel(
         this.temperature = getOrDefault(temperature, 0.7);
         this.topP = topP;
         this.stops = stops;
-        this.model = getOrDefault(model, GLM_4.toString());
+        this.model = getOrDefault(model, GLM_4_FLASH.toString());
         this.maxRetries = getOrDefault(maxRetries, 3);
         this.maxToken = getOrDefault(maxToken, 512);
         this.listeners = listeners == null ? emptyList() : new ArrayList<>(listeners);

File: langchain4j-zhipu-ai/src/main/java/dev/langchain4j/model/zhipu/ZhipuAiStreamingChatModel.java
Patch:
@@ -27,7 +27,7 @@
 import static dev.langchain4j.internal.Utils.isNullOrEmpty;
 import static dev.langchain4j.internal.ValidationUtils.ensureNotEmpty;
 import static dev.langchain4j.model.zhipu.DefaultZhipuAiHelper.*;
-import static dev.langchain4j.model.zhipu.chat.ChatCompletionModel.GLM_4;
+import static dev.langchain4j.model.zhipu.chat.ChatCompletionModel.GLM_4_FLASH;
 import static dev.langchain4j.spi.ServiceHelper.loadFactories;
 import static java.util.Collections.emptyList;
 import static java.util.Collections.singletonList;
@@ -62,7 +62,7 @@ public ZhipuAiStreamingChatModel(
         this.temperature = getOrDefault(temperature, 0.7);
         this.topP = topP;
         this.stops = stops;
-        this.model = getOrDefault(model, GLM_4.toString());
+        this.model = getOrDefault(model, GLM_4_FLASH.toString());
         this.maxToken = getOrDefault(maxToken, 512);
         this.listeners = listeners == null ? emptyList() : new ArrayList<>(listeners);
         this.client = ZhipuAiClient.builder()

File: langchain4j-zhipu-ai/src/main/java/dev/langchain4j/model/zhipu/chat/ChatCompletionChoice.java
Patch:
@@ -17,6 +17,8 @@ public final class ChatCompletionChoice {
     private Delta delta;
     private String finishReason;
 
+    public ChatCompletionChoice() {}
+
     public ChatCompletionChoice(Integer index, AssistantMessage message, Delta delta, String finishReason) {
         this.index = index;
         this.message = message;

File: langchain4j-zhipu-ai/src/main/java/dev/langchain4j/model/zhipu/chat/ChatCompletionRequest.java
Patch:
@@ -9,7 +9,7 @@
 import java.util.List;
 
 import static com.fasterxml.jackson.annotation.JsonInclude.Include.NON_NULL;
-import static dev.langchain4j.model.zhipu.chat.ChatCompletionModel.GLM_4;
+import static dev.langchain4j.model.zhipu.chat.ChatCompletionModel.GLM_4_FLASH;
 import static java.util.Arrays.asList;
 import static java.util.Collections.unmodifiableList;
 
@@ -94,7 +94,7 @@ public Object getToolChoice() {
 
     public static final class Builder {
 
-        private String model = GLM_4.toString();
+        private String model = GLM_4_FLASH.toString();
         private List<Message> messages;
         private Double temperature;
         private Double topP;

File: langchain4j-zhipu-ai/src/main/java/dev/langchain4j/model/zhipu/chat/ChatCompletionResponse.java
Patch:
@@ -20,6 +20,8 @@ public final class ChatCompletionResponse {
     private List<ChatCompletionChoice> choices;
     private Usage usage;
 
+    public ChatCompletionResponse() {}
+
     public ChatCompletionResponse(String id, Integer created, String model, List<ChatCompletionChoice> choices, Usage usage) {
         this.id = id;
         this.created = created;

File: langchain4j-zhipu-ai/src/main/java/dev/langchain4j/model/zhipu/chat/Content.java
Patch:
@@ -1,4 +1,5 @@
 package dev.langchain4j.model.zhipu.chat;
 
 public interface Content {
+    String getType();
 }

File: langchain4j-zhipu-ai/src/main/java/dev/langchain4j/model/zhipu/chat/Delta.java
Patch:
@@ -18,6 +18,9 @@ public final class Delta {
     private String content;
     private List<ToolCall> toolCalls;
 
+    public Delta() {
+    }
+
     private Delta(Builder builder) {
         this.content = builder.content;
         this.toolCalls = builder.toolCalls;

File: langchain4j-zhipu-ai/src/main/java/dev/langchain4j/model/zhipu/chat/FunctionCall.java
Patch:
@@ -20,6 +20,9 @@ public FunctionCall(String name, String arguments) {
         this.arguments = arguments;
     }
 
+    public FunctionCall() {
+    }
+
     public static FunctionCallBuilder builder() {
         return new FunctionCallBuilder();
     }

File: langchain4j-zhipu-ai/src/main/java/dev/langchain4j/model/zhipu/shared/Usage.java
Patch:
@@ -15,6 +15,9 @@ public final class Usage {
     private Integer completionTokens;
     private Integer totalTokens;
 
+    public Usage() {
+    }
+
     private Usage(Builder builder) {
         this.promptTokens = builder.promptTokens;
         this.completionTokens = builder.completionTokens;

File: langchain4j-zhipu-ai/src/test/java/dev/langchain4j/model/zhipu/ZhipuAiChatModelIT.java
Patch:
@@ -173,7 +173,8 @@ void should_execute_get_current_time_tool_and_then_answer() {
 
         // then
         AiMessage secondAiMessage = secondResponse.content();
-        assertThat(secondAiMessage.text()).contains("2024-04-23 12:00:20");
+        assertThat(secondAiMessage.text()).contains("12:00:20");
+        assertThat(secondAiMessage.text()).contains("2024");
         assertThat(secondAiMessage.toolExecutionRequests()).isNull();
 
         TokenUsage secondTokenUsage = secondResponse.tokenUsage();

File: langchain4j-zhipu-ai/src/test/java/dev/langchain4j/model/zhipu/ZhipuAiStreamingChatModelIT.java
Patch:
@@ -207,7 +207,8 @@ void should_execute_get_current_time_tool_and_then_answer() {
         // then
         Response<AiMessage> secondResponse = secondHandler.get();
         AiMessage secondAiMessage = secondResponse.content();
-        assertThat(secondAiMessage.text()).contains("2024-04-23 12:00:20");
+        assertThat(secondAiMessage.text()).contains("12:00:20");
+        assertThat(secondAiMessage.text()).contains("2024");
         assertThat(secondAiMessage.toolExecutionRequests()).isNull();
 
         TokenUsage secondTokenUsage = secondResponse.tokenUsage();

File: langchain4j-jlama/src/main/java/dev/langchain4j/model/jlama/JlamaStreamingLanguageModel.java
Patch:
@@ -2,6 +2,7 @@
 
 import com.github.tjake.jlama.model.AbstractModel;
 import com.github.tjake.jlama.model.functions.Generator;
+import com.github.tjake.jlama.safetensors.prompt.PromptContext;
 import dev.langchain4j.internal.RetryUtils;
 import dev.langchain4j.model.StreamingResponseHandler;
 import dev.langchain4j.model.jlama.spi.JlamaStreamingLanguageModelBuilderFactory;
@@ -60,11 +61,11 @@ public static JlamaStreamingLanguageModelBuilder builder() {
     @Override
     public void generate(String prompt, StreamingResponseHandler<String> handler) {
         try {
-            Generator.Response r = model.generate(id, prompt, temperature, maxTokens, false, (token, time) -> {
+            Generator.Response r = model.generate(id, PromptContext.of(prompt), temperature, maxTokens, (token, time) -> {
                 handler.onNext(token);
             });
 
-            handler.onComplete(Response.from(r.text, new TokenUsage(r.promptTokens, r.generatedTokens), toFinishReason(r.finishReason)));
+            handler.onComplete(Response.from(r.responseText, new TokenUsage(r.promptTokens, r.generatedTokens), toFinishReason(r.finishReason)));
         } catch (Throwable t) {
             handler.onError(t);
         }

File: langchain4j-jlama/src/test/java/dev/langchain4j/model/jlama/JlamaChatModelIT.java
Patch:
@@ -29,15 +29,16 @@ static void setup() {
         model = JlamaChatModel.builder()
                 .modelName("tjake/TinyLlama-1.1B-Chat-v1.0-Jlama-Q4")
                 .modelCachePath(tmpDir.toPath())
-                .maxTokens(25)
+                .temperature(0.0f)
+                .maxTokens(30)
                 .build();
     }
 
     @Test
     void should_send_messages_and_return_response() {
 
         // given
-        List<ChatMessage> messages = singletonList(UserMessage.from("hello"));
+        List<ChatMessage> messages = singletonList(UserMessage.from("When is the best time of year to visit Japan?"));
 
         // when
         Response<AiMessage> response = model.generate(messages);

File: langchain4j-jlama/src/test/java/dev/langchain4j/model/jlama/JlamaLanguageModelIT.java
Patch:
@@ -26,15 +26,16 @@ static void setup() {
         model = JlamaLanguageModel.builder()
                 .modelName("tjake/TinyLlama-1.1B-Chat-v1.0-Jlama-Q4")
                 .modelCachePath(tmpDir.toPath())
-                .maxTokens(10)
+                .temperature(0.0f)
+                .maxTokens(30)
                 .build();
     }
 
     @Test
     void should_send_prompt_and_return_response() {
 
         // given
-        String prompt = "hello";
+        String prompt = "When is the best time of year to visit Japan?";
 
         // when
         Response<String> response = model.generate(prompt);

File: langchain4j-jlama/src/test/java/dev/langchain4j/model/jlama/JlamaStreamingLanguageModelIT.java
Patch:
@@ -27,15 +27,16 @@ static void setup() {
         model = JlamaStreamingLanguageModel.builder()
                 .modelName("tjake/TinyLlama-1.1B-Chat-v1.0-Jlama-Q4")
                 .modelCachePath(tmpDir.toPath())
-                .maxTokens(10)
+                .maxTokens(30)
+                .temperature(0.0f)
                 .build();
     }
 
     @Test
     void should_stream_answer_and_return_response() throws Exception {
 
         // given
-        String userMessage = "hello";
+        String userMessage = "When is the best time of year to visit Japan?";
 
         // when
         StringBuilder answerBuilder = new StringBuilder();

File: web-search-engines/langchain4j-web-search-engine-tavily/src/test/java/dev/langchain4j/web/search/tavily/TavilyWebSearchEngineIT.java
Patch:
@@ -77,7 +77,7 @@ void should_search_with_answer() {
             assertThat(result.metadata()).containsOnlyKeys("score");
         });
 
-        assertThat(results).anyMatch(result -> result.url().toString().contains("https://github.com/langchain4j"));
+        assertThat(results).anyMatch(result -> result.url().toString().contains("langchain4j.dev"));
     }
 
     @Override

File: langchain4j-redis/src/test/java/dev/langchain4j/store/embedding/redis/RedisEmbeddingStoreIT.java
Patch:
@@ -2,8 +2,8 @@
 
 import com.redis.testcontainers.RedisContainer;
 import dev.langchain4j.data.segment.TextSegment;
-import dev.langchain4j.model.embedding.onnx.allminilml6v2q.AllMiniLmL6V2QuantizedEmbeddingModel;
 import dev.langchain4j.model.embedding.EmbeddingModel;
+import dev.langchain4j.model.embedding.onnx.allminilml6v2q.AllMiniLmL6V2QuantizedEmbeddingModel;
 import dev.langchain4j.store.embedding.EmbeddingStore;
 import dev.langchain4j.store.embedding.EmbeddingStoreIT;
 import org.junit.jupiter.api.AfterAll;

File: langchain4j-mistral-ai/src/test/java/dev/langchain4j/model/mistralai/MistralAiChatModelIT.java
Patch:
@@ -56,7 +56,7 @@ class MistralAiChatModelIT {
 
     @AfterEach
     void afterEach() throws InterruptedException {
-        Thread.sleep(2_000); // to prevent hitting rate limits
+        Thread.sleep(5_000); // to prevent hitting rate limits
     }
 
     @Test

File: langchain4j-zhipu-ai/src/main/java/dev/langchain4j/model/zhipu/DefaultZhipuAiHelper.java
Patch:
@@ -20,7 +20,6 @@
 import dev.langchain4j.model.zhipu.embedding.EmbeddingResponse;
 import dev.langchain4j.model.zhipu.shared.ErrorResponse;
 import dev.langchain4j.model.zhipu.shared.Usage;
-import lombok.Cleanup;
 import okhttp3.ResponseBody;
 
 import java.io.IOException;
@@ -212,7 +211,7 @@ private static ChatCompletionChoice toChatErrorChoice(Object object) {
                     .finishReason(FINISH_REASON_OTHER)
                     .build();
         }
-        @Cleanup ResponseBody errorBody = ((retrofit2.Response<?>) object).errorBody();
+        ResponseBody errorBody = ((retrofit2.Response<?>) object).errorBody();
 
         if (errorBody == null) {
             return ChatCompletionChoice.builder()
@@ -232,6 +231,8 @@ private static ChatCompletionChoice toChatErrorChoice(Object object) {
                     .message(AssistantMessage.builder().content(e.getMessage()).build())
                     .finishReason(FINISH_REASON_OTHER)
                     .build();
+        } finally {
+            errorBody.close();
         }
     }
 

File: langchain4j-zhipu-ai/src/main/java/dev/langchain4j/model/zhipu/ZhipuAiClient.java
Patch:
@@ -17,13 +17,14 @@
 import dev.langchain4j.model.zhipu.image.ImageRequest;
 import dev.langchain4j.model.zhipu.image.ImageResponse;
 import dev.langchain4j.model.zhipu.shared.Usage;
-import lombok.extern.slf4j.Slf4j;
 import okhttp3.OkHttpClient;
 import okhttp3.ResponseBody;
 import okhttp3.sse.EventSource;
 import okhttp3.sse.EventSourceListener;
 import okhttp3.sse.EventSources;
 import org.jetbrains.annotations.NotNull;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 import retrofit2.Retrofit;
 
 import java.io.IOException;
@@ -35,8 +36,8 @@
 import static dev.langchain4j.model.zhipu.Json.OBJECT_MAPPER;
 import static retrofit2.converter.jackson.JacksonConverterFactory.create;
 
-@Slf4j
 public class ZhipuAiClient {
+    private static final Logger log = LoggerFactory.getLogger(ZhipuAiClient.class);
 
     private final ZhipuAiApi zhipuAiApi;
     private final OkHttpClient okHttpClient;

File: langchain4j-vertex-ai/src/test/java/dev/langchain4j/model/vertexai/VertexAiImageModelIT.java
Patch:
@@ -42,7 +42,7 @@ private static Image fromPath(Path path) {
 
     @BeforeEach
     void beforeEach() throws InterruptedException {
-        Thread.sleep(5_000); // to prevent hitting rate limits
+        Thread.sleep(10_000); // to prevent hitting rate limits
     }
 
     @Test

File: langchain4j-mistral-ai/src/test/java/dev/langchain4j/model/mistralai/MistralAiChatModelIT.java
Patch:
@@ -56,7 +56,7 @@ class MistralAiChatModelIT {
 
     @AfterEach
     void afterEach() throws InterruptedException {
-        Thread.sleep(1_000); // to prevent hitting rate limits
+        Thread.sleep(2_000); // to prevent hitting rate limits
     }
 
     @Test

File: langchain4j-jina/src/test/java/dev/langchain4j/model/jina/JinaScoringModelIT.java
Patch:
@@ -62,7 +62,7 @@ void should_score_multiple_segments_with_all_parameters() {
         assertThat(scores).hasSize(2);
         assertThat(scores.get(0)).isLessThan(scores.get(1));
 
-        assertThat(response.tokenUsage().totalTokenCount()).isEqualTo(16);
+        assertThat(response.tokenUsage().totalTokenCount()).isGreaterThan(0);
 
         assertThat(response.finishReason()).isNull();
     }

File: langchain4j-mongodb-atlas/src/test/java/dev/langchain4j/store/embedding/mongodb/MongoDBAtlasContainer.java
Patch:
@@ -13,7 +13,7 @@ public MongoDBAtlasContainer() {
         withCommand("/bin/bash", "-c", "atlas deployments setup local-test --type local --port 27778 --bindIpAll --username root --password root --force && tail -f /dev/null");
         withExposedPorts(27778);
         waitingFor(Wait.forLogMessage(".*Deployment created!.*\\n", 1));
-        withStartupTimeout(Duration.ofMinutes(5));
+        withStartupTimeout(Duration.ofMinutes(10));
     }
 
     public String getConnectionString() {

File: langchain4j-vertex-ai/src/test/java/dev/langchain4j/model/vertexai/VertexAiImageModelIT.java
Patch:
@@ -42,7 +42,7 @@ private static Image fromPath(Path path) {
 
     @BeforeEach
     void beforeEach() throws InterruptedException {
-        Thread.sleep(2_000); // to prevent hitting rate limits
+        Thread.sleep(5_000); // to prevent hitting rate limits
     }
 
     @Test

File: langchain4j-open-ai/src/main/java/dev/langchain4j/model/openai/OpenAiStreamingChatModel.java
Patch:
@@ -5,6 +5,7 @@
 import dev.ai4j.openai4j.chat.ChatCompletionRequest;
 import dev.ai4j.openai4j.chat.ChatCompletionResponse;
 import dev.ai4j.openai4j.chat.Delta;
+import dev.ai4j.openai4j.chat.StreamOptions;
 import dev.langchain4j.agent.tool.ToolSpecification;
 import dev.langchain4j.data.message.AiMessage;
 import dev.langchain4j.data.message.ChatMessage;
@@ -145,6 +146,7 @@ private void generate(List<ChatMessage> messages,
     ) {
         ChatCompletionRequest.Builder requestBuilder = ChatCompletionRequest.builder()
                 .stream(true)
+                .streamOptions(new StreamOptions(true))
                 .model(modelName)
                 .messages(toOpenAiMessages(messages))
                 .temperature(temperature)

File: langchain4j/src/main/java/dev/langchain4j/service/DefaultAiServices.java
Patch:
@@ -240,8 +240,7 @@ public Object invoke(Object proxy, Method method, Object[] args) throws Exceptio
 
                         response = Response.from(response.content(), tokenUsageAccumulator, response.finishReason());
 
-                        Object parsedResponse;
-                        parsedResponse = serviceOutputParser.parse(response, returnType);
+                        Object parsedResponse = serviceOutputParser.parse(response, returnType);
                         if (typeHasRawClass(returnType, Result.class)) {
                             return Result.builder()
                                     .content(parsedResponse)

File: langchain4j/src/main/java/dev/langchain4j/service/DefaultAiServices.java
Patch:
@@ -20,6 +20,7 @@
 import dev.langchain4j.model.output.TokenUsage;
 import dev.langchain4j.rag.AugmentationRequest;
 import dev.langchain4j.rag.AugmentationResult;
+import dev.langchain4j.rag.content.Content;
 import dev.langchain4j.rag.query.Metadata;
 import dev.langchain4j.service.output.ServiceOutputParser;
 import dev.langchain4j.service.tool.ToolExecutor;
@@ -161,7 +162,8 @@ public Object invoke(Object proxy, Method method, Object[] args) throws Exceptio
                         Future<Moderation> moderationFuture = triggerModerationIfNeeded(method, messages);
 
                         if (returnType == TokenStream.class) {
-                            return new AiServiceTokenStream(messages, context, memoryId); // TODO moderation
+                            List<Content> contents = augmentationResult != null ? augmentationResult.contents() : null;
+                            return new AiServiceTokenStream(messages, contents, context, memoryId); // TODO moderation
                         }
 
                         Response<AiMessage> response;

File: langchain4j-core/src/test/java/dev/langchain4j/store/embedding/EmbeddingStoreWithoutMetadataIT.java
Patch:
@@ -32,7 +32,6 @@ void beforeEach() {
     }
 
     protected void ensureStoreIsReady() {
-        awaitUntilAsserted(() -> assertThatNoException().isThrownBy(this::getAllEmbeddings));
     }
 
     protected void clearStore() {

File: langchain4j-ollama/src/test/java/dev/langchain4j/model/ollama/OllamaStreamingChatModelIT.java
Patch:
@@ -49,7 +49,7 @@ void should_stream_answer() {
         assertThat(aiMessage.toolExecutionRequests()).isNull();
 
         TokenUsage tokenUsage = response.tokenUsage();
-        assertThat(tokenUsage.inputTokenCount()).isEqualTo(35);
+        assertThat(tokenUsage.inputTokenCount()).isGreaterThan(0);
         assertThat(tokenUsage.outputTokenCount()).isGreaterThan(0);
         assertThat(tokenUsage.totalTokenCount())
                 .isEqualTo(tokenUsage.inputTokenCount() + tokenUsage.outputTokenCount());

File: langchain4j-ollama/src/test/java/dev/langchain4j/model/ollama/OllamaStreamingLanguageModelIT.java
Patch:
@@ -36,7 +36,7 @@ void should_stream_answer() {
         assertThat(response.content()).isEqualTo(answer);
 
         TokenUsage tokenUsage = response.tokenUsage();
-        assertThat(tokenUsage.inputTokenCount()).isEqualTo(31);
+        assertThat(tokenUsage.inputTokenCount()).isGreaterThan(0);
         assertThat(tokenUsage.outputTokenCount()).isGreaterThan(0);
         assertThat(tokenUsage.totalTokenCount())
                 .isEqualTo(tokenUsage.inputTokenCount() + tokenUsage.outputTokenCount());

File: langchain4j-open-ai/src/test/java/dev/langchain4j/model/openai/OpenAiStreamingChatModelIT.java
Patch:
@@ -25,9 +25,8 @@
 import static dev.langchain4j.internal.Utils.readBytes;
 import static dev.langchain4j.model.openai.OpenAiChatModelIT.CAT_IMAGE_URL;
 import static dev.langchain4j.model.openai.OpenAiChatModelIT.DICE_IMAGE_URL;
-import static dev.langchain4j.model.openai.OpenAiChatModelName.GPT_3_5_TURBO;
+import static dev.langchain4j.model.openai.OpenAiChatModelName.*;
 import static dev.langchain4j.model.openai.OpenAiModelName.GPT_3_5_TURBO_1106;
-import static dev.langchain4j.model.openai.OpenAiModelName.GPT_4_VISION_PREVIEW;
 import static dev.langchain4j.model.output.FinishReason.STOP;
 import static dev.langchain4j.model.output.FinishReason.TOOL_EXECUTION;
 import static java.util.Arrays.asList;
@@ -52,7 +51,7 @@ class OpenAiStreamingChatModelIT {
             .baseUrl(System.getenv("OPENAI_BASE_URL"))
             .apiKey(System.getenv("OPENAI_API_KEY"))
             .organizationId(System.getenv("OPENAI_ORGANIZATION_ID"))
-            .modelName(GPT_4_VISION_PREVIEW)
+            .modelName(GPT_4_O_MINI)
             .temperature(0.0)
             .logRequests(true)
             .logResponses(true)

File: langchain4j/src/test/java/dev/langchain4j/service/StreamingAiServicesIT.java
Patch:
@@ -24,7 +24,7 @@
 import java.util.stream.Stream;
 
 import static dev.langchain4j.model.mistralai.MistralAiChatModelName.MISTRAL_LARGE_LATEST;
-import static dev.langchain4j.model.openai.OpenAiChatModelName.GPT_3_5_TURBO_0613;
+import static dev.langchain4j.model.openai.OpenAiChatModelName.GPT_4_O_MINI;
 import static dev.langchain4j.model.output.FinishReason.STOP;
 import static java.util.concurrent.TimeUnit.SECONDS;
 import static org.assertj.core.api.Assertions.assertThat;
@@ -245,7 +245,8 @@ void should_execute_multiple_tools_sequentially_then_answer() throws Exception {
                 .baseUrl(System.getenv("OPENAI_BASE_URL"))
                 .apiKey(System.getenv("OPENAI_API_KEY"))
                 .organizationId(System.getenv("OPENAI_ORGANIZATION_ID"))
-                .modelName(GPT_3_5_TURBO_0613) // this model can only call tools sequentially
+                .modelName(GPT_4_O_MINI)
+                .parallelToolCalls(false)  // called sequentially
                 .temperature(0.0)
                 .logRequests(true)
                 .logResponses(true)

File: langchain4j/src/test/java/dev/langchain4j/service/StreamingAiServicesWithToolsWithoutMemoryIT.java
Patch:
@@ -20,7 +20,7 @@
 import java.util.concurrent.CompletableFuture;
 import java.util.concurrent.TimeUnit;
 
-import static dev.langchain4j.model.openai.OpenAiChatModelName.GPT_3_5_TURBO_0613;
+import static dev.langchain4j.model.openai.OpenAiChatModelName.GPT_4_O_MINI;
 import static dev.langchain4j.model.output.FinishReason.STOP;
 import static org.assertj.core.api.Assertions.assertThat;
 import static org.mockito.Mockito.*;
@@ -117,7 +117,8 @@ void should_execute_multiple_tools_sequentially_then_answer() throws Exception {
                 .baseUrl(System.getenv("OPENAI_BASE_URL"))
                 .apiKey(System.getenv("OPENAI_API_KEY"))
                 .organizationId(System.getenv("OPENAI_ORGANIZATION_ID"))
-                .modelName(GPT_3_5_TURBO_0613) // this model can only call tools sequentially
+                .modelName(GPT_4_O_MINI)
+                .parallelToolCalls(false) // called sequentially
                 .temperature(0.0)
                 .logRequests(true)
                 .logResponses(true)

File: langchain4j-core/src/test/java/dev/langchain4j/web/search/WebSearchEngineIT.java
Patch:
@@ -19,11 +19,11 @@ public abstract class WebSearchEngineIT {
     void should_search() {
 
         // when
-        WebSearchResults webSearchResults = searchEngine().search("What is Artificial Intelligence?");
+        WebSearchResults webSearchResults = searchEngine().search("What is LangChain4j?");
 
         // then
         List<WebSearchOrganicResult> results = webSearchResults.results();
-        assertThat(results).hasSizeGreaterThan(0);
+        assertThat(results).isNotEmpty();
 
         results.forEach(result -> {
             assertThat(result.title()).isNotBlank();
@@ -32,7 +32,7 @@ void should_search() {
             assertThat(result.content()).isNull();
         });
 
-        assertThat(results).anyMatch(result -> result.url().toString().contains("AI"));
+        assertThat(results).anyMatch(result -> result.url().toString().contains("langchain4j"));
     }
 
     @Test

File: langchain4j-mistral-ai/src/test/java/dev/langchain4j/model/mistralai/MistralAiChatModelIT.java
Patch:
@@ -451,7 +451,7 @@ void should_execute_multiple_tools_using_model_open8x22B_then_answer() {
         AiMessage aiMessage2 = response2.content();
         assertThat(aiMessage2.text()).contains("T123");
         assertThat(aiMessage2.text()).containsIgnoringCase("paid");
-        assertThat(aiMessage2.text()).containsIgnoringWhitespaces("March 11, 2024");
+        assertThat(aiMessage2.text()).contains("11", "2024");
         assertThat(aiMessage2.toolExecutionRequests()).isNull();
 
         TokenUsage tokenUsage2 = response2.tokenUsage();

File: langchain4j-mistral-ai/src/test/java/dev/langchain4j/model/mistralai/MistralAiStreamingChatModelIT.java
Patch:
@@ -452,7 +452,7 @@ void should_execute_multiple_tools_using_model_large_then_answer() {
         AiMessage aiMessage2 = response2.content();
         assertThat(aiMessage2.text()).contains("T123");
         assertThat(aiMessage2.text()).containsIgnoringCase("paid");
-        assertThat(aiMessage2.text()).containsIgnoringWhitespaces("March 11, 2024");
+        assertThat(aiMessage2.text()).contains("11", "2024");
         assertThat(aiMessage2.toolExecutionRequests()).isNull();
 
         TokenUsage tokenUsage2 = response2.tokenUsage();

File: langchain4j-weaviate/src/test/java/dev/langchain4j/store/embedding/weaviate/CloudWeaviateEmbeddingStoreIT.java
Patch:
@@ -1,19 +1,19 @@
 package dev.langchain4j.store.embedding.weaviate;
 
 import dev.langchain4j.data.segment.TextSegment;
-import dev.langchain4j.model.embedding.onnx.allminilml6v2q.AllMiniLmL6V2QuantizedEmbeddingModel;
 import dev.langchain4j.model.embedding.EmbeddingModel;
+import dev.langchain4j.model.embedding.onnx.allminilml6v2q.AllMiniLmL6V2QuantizedEmbeddingModel;
 import dev.langchain4j.store.embedding.EmbeddingStore;
 import dev.langchain4j.store.embedding.EmbeddingStoreIT;
 import io.weaviate.client.Config;
 import io.weaviate.client.WeaviateAuthClient;
 import io.weaviate.client.WeaviateClient;
 import io.weaviate.client.v1.auth.exception.AuthException;
-import org.junit.jupiter.api.condition.EnabledIfEnvironmentVariable;
+import org.junit.jupiter.api.Disabled;
 
 import static dev.langchain4j.internal.Utils.randomUUID;
 
-@EnabledIfEnvironmentVariable(named = "WEAVIATE_API_KEY", matches = ".+")
+@Disabled("Free sandbox expires every 14 days. Run manually before release.")
 class CloudWeaviateEmbeddingStoreIT extends EmbeddingStoreIT {
 
     String objectClass = "Test" + randomUUID().replace("-", "");

File: langchain4j-chroma/src/main/java/dev/langchain4j/store/embedding/chroma/ChromaMetadataFilterMapper.java
Patch:
@@ -23,7 +23,9 @@ class ChromaMetadataFilterMapper {
     }
 
     static Map<String, Object> map(Filter filter) {
-        if (filter instanceof IsEqualTo) {
+        if (filter == null) {
+            return null;
+        } else if (filter instanceof IsEqualTo) {
             return mapEqual((IsEqualTo) filter);
         } else if (filter instanceof IsNotEqualTo) {
             return mapNotEqual((IsNotEqualTo) filter);

File: langchain4j-qianfan/src/main/java/dev/langchain4j/model/qianfan/QianfanStreamingChatModel.java
Patch:
@@ -18,6 +18,7 @@
 
 import java.net.Proxy;
 import java.util.List;
+import java.util.Objects;
 
 import static dev.langchain4j.internal.Utils.getOrDefault;
 import static dev.langchain4j.model.qianfan.InternalQianfanHelper.getSystemMessage;
@@ -143,7 +144,7 @@ private void generate(List<ChatMessage> messages,
     private static void handle(ChatCompletionResponse partialResponse,
                                StreamingResponseHandler<AiMessage> handler) {
         String result = partialResponse.getResult();
-        if (Utils.isNullOrBlank(result)) {
+        if (Objects.isNull(result) || result.isEmpty()) {
             return;
         }
         handler.onNext(result);

File: langchain4j-ollama/src/main/java/dev/langchain4j/model/ollama/ChatRequest.java
Patch:
@@ -27,4 +27,5 @@ class ChatRequest {
     private Options options;
     private String format;
     private Boolean stream;
+    private List<Tool> tools;
 }

File: langchain4j-ollama/src/main/java/dev/langchain4j/model/ollama/Message.java
Patch:
@@ -25,4 +25,5 @@ class Message {
     private Role role;
     private String content;
     private List<String> images;
+    private List<ToolCall> toolCalls;
 }

File: langchain4j-ollama/src/main/java/dev/langchain4j/model/ollama/Role.java
Patch:
@@ -8,7 +8,8 @@ enum Role {
 
     SYSTEM,
     USER,
-    ASSISTANT;
+    ASSISTANT,
+    TOOL;
 
     @JsonValue
     public String serialize() {

File: langchain4j-ollama/src/test/java/dev/langchain4j/model/ollama/OllamaImage.java
Patch:
@@ -14,6 +14,7 @@ public class OllamaImage {
     static final String BAKLLAVA_MODEL = "bakllava";
 
     static final String TINY_DOLPHIN_MODEL = "tinydolphin";
+    static final String TOOL_MODEL = "mistral";
 
     static final String ALL_MINILM_MODEL = "all-minilm";
 

File: langchain4j-cohere/src/main/java/dev/langchain4j/model/cohere/CohereScoringModel.java
Patch:
@@ -6,6 +6,7 @@
 import dev.langchain4j.model.scoring.ScoringModel;
 import lombok.Builder;
 
+import java.net.Proxy;
 import java.time.Duration;
 import java.util.List;
 
@@ -35,13 +36,15 @@ public CohereScoringModel(
             String modelName,
             Duration timeout,
             Integer maxRetries,
+            Proxy proxy,
             Boolean logRequests,
             Boolean logResponses
     ) {
         this.client = CohereClient.builder()
                 .baseUrl(getOrDefault(baseUrl, DEFAULT_BASE_URL))
                 .apiKey(ensureNotBlank(apiKey, "apiKey"))
                 .timeout(getOrDefault(timeout, ofSeconds(60)))
+                .proxy(proxy)
                 .logRequests(getOrDefault(logRequests, false))
                 .logResponses(getOrDefault(logResponses, false))
                 .build();

File: langchain4j-core/src/main/java/dev/langchain4j/agent/tool/ToolSpecification.java
Patch:
@@ -11,6 +11,7 @@
  * Describes a {@link Tool}.
  */
 public class ToolSpecification {
+
     private final String name;
     private final String description;
     private final ToolParameters parameters;

File: langchain4j-core/src/main/java/dev/langchain4j/model/output/structured/Description.java
Patch:
@@ -4,18 +4,20 @@
 import java.lang.annotation.Target;
 
 import static java.lang.annotation.ElementType.FIELD;
+import static java.lang.annotation.ElementType.TYPE;
 import static java.lang.annotation.RetentionPolicy.RUNTIME;
 
 /**
  * Annotation to attach a description to a class field.
  */
-@Target(FIELD)
+@Target({FIELD, TYPE})
 @Retention(RUNTIME)
 public @interface Description {
 
     /**
      * The description can be defined in one line or multiple lines.
      * If the description is defined in multiple lines, the lines will be joined with a space (" ") automatically.
+     *
      * @return The description.
      */
     String[] value();

File: langchain4j/src/main/java/dev/langchain4j/service/output/ServiceOutputParser.java
Patch:
@@ -83,6 +83,7 @@ public String outputFormatInstructions(Type returnType) {
             return "";
         }
 
+        // TODO validate this earlier
         if (returnType == void.class) {
             throw illegalConfiguration("Return type of method '%s' cannot be void");
         }

File: langchain4j-zhipu-ai/src/main/java/dev/langchain4j/model/zhipu/embedding/EmbeddingModel.java
Patch:
@@ -3,6 +3,7 @@
 public enum EmbeddingModel {
 
     EMBEDDING_2("embedding-2"),
+    EMBEDDING_3("embedding-3"),
     TEXT_EMBEDDING("text_embedding"),
     ;
 

File: langchain4j-zhipu-ai/src/main/java/dev/langchain4j/model/zhipu/embedding/EmbeddingRequest.java
Patch:
@@ -23,4 +23,5 @@ public final class EmbeddingRequest {
     private String input;
     @Builder.Default
     private String model = EMBEDDING_2.toString();
+    private Integer dimensions;
 }

File: langchain4j-anthropic/src/main/java/dev/langchain4j/model/anthropic/internal/client/DefaultAnthropicClient.java
Patch:
@@ -2,6 +2,7 @@
 
 import com.fasterxml.jackson.databind.ObjectMapper;
 import dev.langchain4j.data.message.AiMessage;
+import dev.langchain4j.internal.Utils;
 import dev.langchain4j.model.StreamingResponseHandler;
 import dev.langchain4j.model.anthropic.internal.api.*;
 import dev.langchain4j.model.output.Response;
@@ -82,7 +83,7 @@ public DefaultAnthropicClient build() {
 
 
         Retrofit retrofit = new Retrofit.Builder()
-                .baseUrl(ensureNotBlank(builder.baseUrl, "baseUrl"))
+                .baseUrl(Utils.ensureTrailingForwardSlash(ensureNotBlank(builder.baseUrl, "baseUrl")))
                 .client(okHttpClient)
                 .addConverterFactory(JacksonConverterFactory.create(OBJECT_MAPPER))
                 .build();

File: langchain4j-chatglm/src/main/java/dev/langchain4j/model/chatglm/ChatGlmApi.java
Patch:
@@ -9,7 +9,7 @@ interface ChatGlmApi {
 
     int OK = 200;
 
-    @POST("/")
+    @POST
     @Headers({"Content-Type: application/json"})
     Call<ChatCompletionResponse> chatCompletion(@Body ChatCompletionRequest chatCompletionRequest);
 }

File: langchain4j-chatglm/src/main/java/dev/langchain4j/model/chatglm/ChatGlmClient.java
Patch:
@@ -2,6 +2,7 @@
 
 import com.google.gson.Gson;
 import com.google.gson.GsonBuilder;
+import dev.langchain4j.internal.Utils;
 import lombok.Builder;
 import okhttp3.OkHttpClient;
 import retrofit2.Response;
@@ -35,7 +36,7 @@ public ChatGlmClient(String baseUrl, Duration timeout) {
                 .build();
 
         Retrofit retrofit = new Retrofit.Builder()
-                .baseUrl(baseUrl)
+                .baseUrl(Utils.ensureTrailingForwardSlash(baseUrl))
                 .client(okHttpClient)
                 .addConverterFactory(GsonConverterFactory.create(GSON))
                 .build();

File: langchain4j-chroma/src/main/java/dev/langchain4j/store/embedding/chroma/ChromaClient.java
Patch:
@@ -4,6 +4,8 @@
 
 import com.google.gson.Gson;
 import com.google.gson.GsonBuilder;
+import dev.langchain4j.internal.Utils;
+
 import java.io.IOException;
 import java.time.Duration;
 import java.util.List;
@@ -33,7 +35,7 @@ private ChromaClient(Builder builder) {
         Gson gson = new GsonBuilder().setFieldNamingPolicy(LOWER_CASE_WITH_UNDERSCORES).create();
 
         Retrofit retrofit = new Retrofit.Builder()
-            .baseUrl(builder.baseUrl)
+            .baseUrl(Utils.ensureTrailingForwardSlash(builder.baseUrl))
             .client(httpClientBuilder.build())
             .addConverterFactory(GsonConverterFactory.create(gson))
             .build();

File: langchain4j-cohere/src/main/java/dev/langchain4j/model/cohere/CohereClient.java
Patch:
@@ -2,6 +2,7 @@
 
 import com.google.gson.Gson;
 import com.google.gson.GsonBuilder;
+import dev.langchain4j.internal.Utils;
 import lombok.Builder;
 import okhttp3.OkHttpClient;
 import retrofit2.Retrofit;
@@ -40,7 +41,7 @@ class CohereClient {
         }
 
         Retrofit retrofit = new Retrofit.Builder()
-                .baseUrl(baseUrl)
+                .baseUrl(Utils.ensureTrailingForwardSlash(baseUrl))
                 .client(okHttpClientBuilder.build())
                 .addConverterFactory(GsonConverterFactory.create(GSON))
                 .build();

File: langchain4j-hugging-face/src/main/java/dev/langchain4j/model/huggingface/DefaultHuggingFaceClient.java
Patch:
@@ -2,6 +2,7 @@
 
 import com.google.gson.Gson;
 import com.google.gson.GsonBuilder;
+import dev.langchain4j.internal.Utils;
 import dev.langchain4j.model.huggingface.client.EmbeddingRequest;
 import dev.langchain4j.model.huggingface.client.HuggingFaceClient;
 import dev.langchain4j.model.huggingface.client.TextGenerationRequest;
@@ -38,7 +39,7 @@ class DefaultHuggingFaceClient implements HuggingFaceClient {
                 .create();
 
         Retrofit retrofit = new Retrofit.Builder()
-                .baseUrl("https://api-inference.huggingface.co")
+                .baseUrl(Utils.ensureTrailingForwardSlash("https://api-inference.huggingface.co/"))
                 .client(okHttpClient)
                 .addConverterFactory(GsonConverterFactory.create(gson))
                 .build();

File: langchain4j-hugging-face/src/main/java/dev/langchain4j/model/huggingface/HuggingFaceApi.java
Patch:
@@ -13,11 +13,11 @@
 
 interface HuggingFaceApi {
 
-    @POST("/models/{modelId}")
+    @POST("models/{modelId}")
     @Headers({"Content-Type: application/json"})
     Call<List<TextGenerationResponse>> generate(@Body TextGenerationRequest request, @Path("modelId") String modelId);
 
-    @POST("/pipeline/feature-extraction/{modelId}")
+    @POST("pipeline/feature-extraction/{modelId}")
     @Headers({"Content-Type: application/json"})
     Call<List<float[]>> embed(@Body EmbeddingRequest request, @Path("modelId") String modelId);
 }

File: langchain4j-jina/src/main/java/dev/langchain4j/model/jina/internal/client/JinaClient.java
Patch:
@@ -1,6 +1,7 @@
 package dev.langchain4j.model.jina.internal.client;
 
 import com.fasterxml.jackson.databind.ObjectMapper;
+import dev.langchain4j.internal.Utils;
 import dev.langchain4j.model.jina.internal.api.*;
 import lombok.Builder;
 import okhttp3.OkHttpClient;
@@ -37,7 +38,7 @@ public class JinaClient {
         }
 
         Retrofit retrofit = new Retrofit.Builder()
-                .baseUrl(baseUrl)
+                .baseUrl(Utils.ensureTrailingForwardSlash(baseUrl))
                 .client(okHttpClientBuilder.build())
                 .addConverterFactory(JacksonConverterFactory.create(OBJECT_MAPPER))
                 .build();

File: langchain4j-nomic/src/main/java/dev/langchain4j/model/nomic/NomicClient.java
Patch:
@@ -2,6 +2,7 @@
 
 import com.google.gson.Gson;
 import com.google.gson.GsonBuilder;
+import dev.langchain4j.internal.Utils;
 import lombok.Builder;
 import okhttp3.OkHttpClient;
 import retrofit2.Retrofit;
@@ -40,7 +41,7 @@ class NomicClient {
         }
 
         Retrofit retrofit = new Retrofit.Builder()
-                .baseUrl(baseUrl)
+                .baseUrl(Utils.ensureTrailingForwardSlash(baseUrl))
                 .client(okHttpClientBuilder.build())
                 .addConverterFactory(GsonConverterFactory.create(GSON))
                 .build();

File: langchain4j-ollama/src/main/java/dev/langchain4j/model/ollama/OllamaClient.java
Patch:
@@ -2,6 +2,7 @@
 
 import com.fasterxml.jackson.databind.ObjectMapper;
 import dev.langchain4j.data.message.AiMessage;
+import dev.langchain4j.internal.Utils;
 import dev.langchain4j.model.StreamingResponseHandler;
 import dev.langchain4j.model.output.Response;
 import dev.langchain4j.model.output.TokenUsage;
@@ -63,7 +64,7 @@ public OllamaClient(String baseUrl,
         OkHttpClient okHttpClient = okHttpClientBuilder.build();
 
         Retrofit retrofit = new Retrofit.Builder()
-                .baseUrl(baseUrl.endsWith("/") ? baseUrl : baseUrl + "/")
+                .baseUrl(Utils.ensureTrailingForwardSlash(baseUrl))
                 .client(okHttpClient)
                 .addConverterFactory(JacksonConverterFactory.create(OBJECT_MAPPER))
                 .build();

File: langchain4j-ovh-ai/src/main/java/dev/langchain4j/model/ovhai/internal/client/DefaultOvhAiClient.java
Patch:
@@ -6,6 +6,7 @@
 import java.io.IOException;
 import java.util.Arrays;
 import com.fasterxml.jackson.databind.ObjectMapper;
+import dev.langchain4j.internal.Utils;
 import dev.langchain4j.model.ovhai.internal.api.EmbeddingRequest;
 import dev.langchain4j.model.ovhai.internal.api.EmbeddingResponse;
 import dev.langchain4j.model.ovhai.internal.api.OvhAiApi;
@@ -61,7 +62,7 @@ public DefaultOvhAiClient build() {
         this.okHttpClient = okHttpClientBuilder.build();
 
         Retrofit retrofit = new Retrofit.Builder()
-            .baseUrl(ensureNotBlank(builder.baseUrl, "baseUrl"))
+            .baseUrl(Utils.ensureTrailingForwardSlash(ensureNotBlank(builder.baseUrl, "baseUrl")))
             .client(okHttpClient)
             .addConverterFactory(JacksonConverterFactory.create(OBJECT_MAPPER))
             .build();

File: langchain4j-qianfan/src/main/java/dev/langchain4j/model/qianfan/client/QianfanClient.java
Patch:
@@ -1,6 +1,7 @@
 package dev.langchain4j.model.qianfan.client;
 
 
+import dev.langchain4j.internal.Utils;
 import dev.langchain4j.model.qianfan.client.chat.ChatCompletionRequest;
 import dev.langchain4j.model.qianfan.client.chat.ChatCompletionResponse;
 import dev.langchain4j.model.qianfan.client.chat.ChatTokenResponse;
@@ -68,7 +69,7 @@ private QianfanClient(Builder serviceBuilder) {
             this.apiKey = serviceBuilder.apiKey;
             this.secretKey = serviceBuilder.secretKey;
             this.okHttpClient = okHttpClientBuilder.build();
-            Retrofit retrofit = (new Retrofit.Builder()).baseUrl(serviceBuilder.baseUrl).client(this.okHttpClient)
+            Retrofit retrofit = (new Retrofit.Builder()).baseUrl(Utils.ensureTrailingForwardSlash(serviceBuilder.baseUrl)).client(this.okHttpClient)
                     .addConverterFactory(GsonConverterFactory.create(Json.GSON)).build();
             this.qianfanApi = retrofit.create(QianfanApi.class);
         }

File: langchain4j-vearch/src/main/java/dev/langchain4j/store/embedding/vearch/VearchClient.java
Patch:
@@ -2,6 +2,7 @@
 
 import com.google.gson.Gson;
 import com.google.gson.GsonBuilder;
+import dev.langchain4j.internal.Utils;
 import lombok.Builder;
 import okhttp3.MediaType;
 import okhttp3.OkHttpClient;
@@ -37,7 +38,7 @@ public VearchClient(String baseUrl, Duration timeout) {
                 .build();
 
         Retrofit retrofit = new Retrofit.Builder()
-                .baseUrl(baseUrl)
+                .baseUrl(Utils.ensureTrailingForwardSlash(baseUrl))
                 .client(okHttpClient)
                 .addConverterFactory(GsonConverterFactory.create(GSON))
                 .build();

File: langchain4j-vespa/src/main/java/dev/langchain4j/store/embedding/vespa/VespaQueryClient.java
Patch:
@@ -2,6 +2,8 @@
 package dev.langchain4j.store.embedding.vespa;
 
 import com.google.gson.GsonBuilder;
+import dev.langchain4j.internal.Utils;
+
 import java.io.IOException;
 import java.nio.file.Files;
 import java.nio.file.Path;
@@ -72,7 +74,7 @@ public static VespaQueryApi createInstance(String baseUrl, Path certificate, Pat
         .build();
 
       Retrofit retrofit = new Retrofit.Builder()
-        .baseUrl(baseUrl)
+        .baseUrl(Utils.ensureTrailingForwardSlash(baseUrl))
         .client(client)
         .addConverterFactory(GsonConverterFactory.create(new GsonBuilder().create()))
         .build();

File: langchain4j-workers-ai/src/main/java/dev/langchain4j/model/workersai/client/WorkersAiClient.java
Patch:
@@ -1,5 +1,6 @@
 package dev.langchain4j.model.workersai.client;
 
+import dev.langchain4j.internal.Utils;
 import okhttp3.Interceptor;
 import okhttp3.OkHttpClient;
 import okhttp3.Request;
@@ -40,7 +41,7 @@ public static WorkersAiApi createService(String apiToken) {
                 .build();
 
         Retrofit retrofit = new Retrofit.Builder()
-                .baseUrl(BASE_URL)
+                .baseUrl(Utils.ensureTrailingForwardSlash(BASE_URL))
                 .client(okHttpClient)
                 .addConverterFactory(JacksonConverterFactory.create())
                 .build();

File: web-search-engines/langchain4j-web-search-engine-tavily/src/main/java/dev/langchain4j/web/search/tavily/TavilyApi.java
Patch:
@@ -7,7 +7,7 @@
 
 interface TavilyApi {
 
-    @POST("/search")
+    @POST("search")
     @Headers({"Content-Type: application/json"})
     Call<TavilyResponse> search(@Body TavilySearchRequest request);
 }

File: web-search-engines/langchain4j-web-search-engine-tavily/src/main/java/dev/langchain4j/web/search/tavily/TavilyClient.java
Patch:
@@ -2,6 +2,7 @@
 
 import com.google.gson.Gson;
 import com.google.gson.GsonBuilder;
+import dev.langchain4j.internal.Utils;
 import lombok.Builder;
 import okhttp3.OkHttpClient;
 import retrofit2.Response;
@@ -32,7 +33,7 @@ public TavilyClient(String baseUrl, Duration timeout) {
                 .writeTimeout(timeout);
 
         Retrofit retrofit = new Retrofit.Builder()
-                .baseUrl(baseUrl)
+                .baseUrl(Utils.ensureTrailingForwardSlash(baseUrl))
                 .client(okHttpClientBuilder.build())
                 .addConverterFactory(GsonConverterFactory.create(GSON))
                 .build();

File: web-search-engines/langchain4j-web-search-engine-tavily/src/main/java/dev/langchain4j/web/search/tavily/TavilyWebSearchEngine.java
Patch:
@@ -28,7 +28,7 @@
  */
 public class TavilyWebSearchEngine implements WebSearchEngine {
 
-    private static final String DEFAULT_BASE_URL = "https://api.tavily.com";
+    private static final String DEFAULT_BASE_URL = "https://api.tavily.com/";
 
     private final String apiKey;
     private final TavilyClient tavilyClient;

File: langchain4j-ollama/src/main/java/dev/langchain4j/model/ollama/EmbeddingRequest.java
Patch:
@@ -9,6 +9,8 @@
 import lombok.Data;
 import lombok.NoArgsConstructor;
 
+import java.util.List;
+
 import static com.fasterxml.jackson.annotation.JsonInclude.Include.NON_NULL;
 
 @Data
@@ -21,5 +23,5 @@
 class EmbeddingRequest {
 
     private String model;
-    private String prompt;
+    private List<String> input;
 }

File: langchain4j-ollama/src/main/java/dev/langchain4j/model/ollama/OllamaApi.java
Patch:
@@ -15,9 +15,9 @@ interface OllamaApi {
     @Streaming
     Call<ResponseBody> streamingCompletion(@Body CompletionRequest completionRequest);
 
-    @POST("api/embeddings")
+    @POST("api/embed")
     @Headers({"Content-Type: application/json"})
-    Call<EmbeddingResponse> embedd(@Body EmbeddingRequest embeddingRequest);
+    Call<EmbeddingResponse> embed(@Body EmbeddingRequest embeddingRequest);
 
     @POST("api/chat")
     @Headers({"Content-Type: application/json"})

File: langchain4j-ollama/src/main/java/dev/langchain4j/model/ollama/OllamaClient.java
Patch:
@@ -192,7 +192,7 @@ public void onFailure(Call<ResponseBody> call, Throwable throwable) {
 
     public EmbeddingResponse embed(EmbeddingRequest request) {
         try {
-            retrofit2.Response<EmbeddingResponse> retrofitResponse = ollamaApi.embedd(request).execute();
+            retrofit2.Response<EmbeddingResponse> retrofitResponse = ollamaApi.embed(request).execute();
             if (retrofitResponse.isSuccessful()) {
                 return retrofitResponse.body();
             } else {

File: web-search-engines/langchain4j-web-search-engine-tavily/src/main/java/dev/langchain4j/web/search/tavily/TavilyWebSearchEngine.java
Patch:
@@ -98,7 +98,7 @@ public static TavilyWebSearchEngine withApiKey(String apiKey) {
 
     private static WebSearchOrganicResult toWebSearchOrganicResult(TavilySearchResult tavilySearchResult) {
         return WebSearchOrganicResult.from(tavilySearchResult.getTitle(),
-                URI.create(tavilySearchResult.getUrl()),
+                URI.create(tavilySearchResult.getUrl().replaceAll(" ", "%20")),
                 tavilySearchResult.getContent(),
                 tavilySearchResult.getRawContent(),
                 Collections.singletonMap("score", String.valueOf(tavilySearchResult.getScore())));

File: langchain4j-elasticsearch/src/main/java/dev/langchain4j/store/embedding/elasticsearch/Document.java
Patch:
@@ -1,5 +1,6 @@
 package dev.langchain4j.store.embedding.elasticsearch;
 
+import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
 import lombok.AllArgsConstructor;
 import lombok.Builder;
 import lombok.Data;
@@ -11,6 +12,7 @@
 @NoArgsConstructor
 @AllArgsConstructor
 @Builder
+@JsonIgnoreProperties(ignoreUnknown = true)
 class Document {
 
     private float[] vector;

File: langchain4j-core/src/main/java/dev/langchain4j/model/input/DefaultPromptTemplateFactory.java
Patch:
@@ -20,7 +20,8 @@ public DefaultTemplate create(PromptTemplateFactory.Input input) {
 
     static class DefaultTemplate implements Template {
 
-        private static final Pattern VARIABLE_PATTERN = Pattern.compile("\\{\\{(.+?)}}");
+        @SuppressWarnings("RegExpRedundantEscape")
+        private static final Pattern VARIABLE_PATTERN = Pattern.compile("\\{\\{(.+?)\\}\\}");
 
         private final String template;
         private final Set<String> allVariables;

File: langchain4j-azure-ai-search/src/test/java/dev/langchain4j/rag/content/retriever/azure/search/AzureAiSearchContentRetrieverIT.java
Patch:
@@ -6,7 +6,7 @@
 import dev.langchain4j.data.document.Metadata;
 import dev.langchain4j.data.embedding.Embedding;
 import dev.langchain4j.data.segment.TextSegment;
-import dev.langchain4j.model.embedding.AllMiniLmL6V2QuantizedEmbeddingModel;
+import dev.langchain4j.model.embedding.onnx.allminilml6v2q.AllMiniLmL6V2QuantizedEmbeddingModel;
 import dev.langchain4j.model.embedding.EmbeddingModel;
 import dev.langchain4j.rag.content.Content;
 import dev.langchain4j.rag.query.Query;

File: langchain4j-azure-ai-search/src/test/java/dev/langchain4j/rag/content/retriever/azure/search/AzureAiSearchContentRetrieverTest.java
Patch:
@@ -6,7 +6,7 @@
 import com.azure.search.documents.indexes.models.SearchIndex;
 import com.azure.search.documents.models.SearchResult;
 import com.azure.search.documents.models.SemanticSearchResult;
-import dev.langchain4j.model.embedding.AllMiniLmL6V2QuantizedEmbeddingModel;
+import dev.langchain4j.model.embedding.onnx.allminilml6v2q.AllMiniLmL6V2QuantizedEmbeddingModel;
 import dev.langchain4j.model.embedding.EmbeddingModel;
 import org.junit.jupiter.api.Test;
 

File: langchain4j-azure-ai-search/src/test/java/dev/langchain4j/store/embedding/azure/search/AzureAiSearchEmbeddingStoreIT.java
Patch:
@@ -8,11 +8,10 @@
 import com.azure.search.documents.indexes.models.SearchIndex;
 import dev.langchain4j.data.embedding.Embedding;
 import dev.langchain4j.data.segment.TextSegment;
-import dev.langchain4j.model.embedding.AllMiniLmL6V2QuantizedEmbeddingModel;
+import dev.langchain4j.model.embedding.onnx.allminilml6v2q.AllMiniLmL6V2QuantizedEmbeddingModel;
 import dev.langchain4j.model.embedding.EmbeddingModel;
 import dev.langchain4j.store.embedding.EmbeddingMatch;
 import dev.langchain4j.store.embedding.EmbeddingStore;
-import dev.langchain4j.store.embedding.EmbeddingStoreIT;
 import dev.langchain4j.store.embedding.EmbeddingStoreWithFilteringIT;
 import org.junit.jupiter.api.BeforeEach;
 import org.junit.jupiter.api.Test;

File: langchain4j-azure-cosmos-mongo-vcore/src/test/java/dev/langchain4j/store/embedding/azure/cosmos/mongo/vcore/AzureCosmosDBMongoVCoreEmbeddingStoreIT.java
Patch:
@@ -8,7 +8,7 @@
 import com.mongodb.client.model.Filters;
 import dev.langchain4j.data.embedding.Embedding;
 import dev.langchain4j.data.segment.TextSegment;
-import dev.langchain4j.model.embedding.AllMiniLmL6V2QuantizedEmbeddingModel;
+import dev.langchain4j.model.embedding.onnx.allminilml6v2q.AllMiniLmL6V2QuantizedEmbeddingModel;
 import dev.langchain4j.model.embedding.EmbeddingModel;
 import dev.langchain4j.store.embedding.EmbeddingMatch;
 import dev.langchain4j.store.embedding.EmbeddingStore;

File: langchain4j-azure-cosmos-nosql/src/test/java/dev/langchain4j/store/embedding/azure/cosmos/nosql/AzureCosmosDbNoSqlEmbeddingStoreIT.java
Patch:
@@ -7,7 +7,7 @@
 import com.azure.cosmos.models.*;
 import dev.langchain4j.data.embedding.Embedding;
 import dev.langchain4j.data.segment.TextSegment;
-import dev.langchain4j.model.embedding.AllMiniLmL6V2QuantizedEmbeddingModel;
+import dev.langchain4j.model.embedding.onnx.allminilml6v2q.AllMiniLmL6V2QuantizedEmbeddingModel;
 import dev.langchain4j.model.embedding.EmbeddingModel;
 import dev.langchain4j.store.embedding.EmbeddingMatch;
 import dev.langchain4j.store.embedding.EmbeddingStore;

File: langchain4j-cassandra/src/test/java/dev/langchain4j/store/embedding/cassandra/CassandraEmbeddingStoreIT.java
Patch:
@@ -3,7 +3,6 @@
 import dev.langchain4j.data.document.Metadata;
 import dev.langchain4j.data.embedding.Embedding;
 import dev.langchain4j.data.segment.TextSegment;
-import dev.langchain4j.model.embedding.AllMiniLmL6V2QuantizedEmbeddingModel;
 import dev.langchain4j.model.embedding.EmbeddingModel;
 import dev.langchain4j.model.openai.OpenAiEmbeddingModel;
 import dev.langchain4j.model.openai.OpenAiModelName;

File: langchain4j-chroma/src/test/java/dev/langchain4j/store/embedding/chroma/ChromaEmbeddingStoreIT.java
Patch:
@@ -3,7 +3,7 @@
 import static dev.langchain4j.internal.Utils.randomUUID;
 
 import dev.langchain4j.data.segment.TextSegment;
-import dev.langchain4j.model.embedding.AllMiniLmL6V2QuantizedEmbeddingModel;
+import dev.langchain4j.model.embedding.onnx.allminilml6v2q.AllMiniLmL6V2QuantizedEmbeddingModel;
 import dev.langchain4j.model.embedding.EmbeddingModel;
 import dev.langchain4j.store.embedding.EmbeddingStore;
 import dev.langchain4j.store.embedding.EmbeddingStoreIT;

File: langchain4j-chroma/src/test/java/dev/langchain4j/store/embedding/chroma/ChromaEmbeddingStoreRemovalIT.java
Patch:
@@ -8,7 +8,7 @@
 
 import dev.langchain4j.data.document.Metadata;
 import dev.langchain4j.data.segment.TextSegment;
-import dev.langchain4j.model.embedding.AllMiniLmL6V2QuantizedEmbeddingModel;
+import dev.langchain4j.model.embedding.onnx.allminilml6v2q.AllMiniLmL6V2QuantizedEmbeddingModel;
 import dev.langchain4j.model.embedding.EmbeddingModel;
 import dev.langchain4j.store.embedding.EmbeddingMatch;
 import dev.langchain4j.store.embedding.EmbeddingStore;

File: langchain4j-easy-rag/src/main/java/dev/langchain4j/data/document/splitter/recursive/RecursiveDocumentSplitterFactory.java
Patch:
@@ -2,7 +2,7 @@
 
 import dev.langchain4j.data.document.DocumentSplitter;
 import dev.langchain4j.data.document.splitter.DocumentSplitters;
-import dev.langchain4j.model.embedding.HuggingFaceTokenizer;
+import dev.langchain4j.model.embedding.onnx.HuggingFaceTokenizer;
 import dev.langchain4j.spi.data.document.splitter.DocumentSplitterFactory;
 
 public class RecursiveDocumentSplitterFactory implements DocumentSplitterFactory {

File: langchain4j-elasticsearch/src/test/java/dev/langchain4j/store/embedding/elasticsearch/ElasticsearchEmbeddingStoreCloudIT.java
Patch:
@@ -1,7 +1,7 @@
 package dev.langchain4j.store.embedding.elasticsearch;
 
 import dev.langchain4j.data.segment.TextSegment;
-import dev.langchain4j.model.embedding.AllMiniLmL6V2QuantizedEmbeddingModel;
+import dev.langchain4j.model.embedding.onnx.allminilml6v2q.AllMiniLmL6V2QuantizedEmbeddingModel;
 import dev.langchain4j.model.embedding.EmbeddingModel;
 import dev.langchain4j.store.embedding.EmbeddingStore;
 import dev.langchain4j.store.embedding.EmbeddingStoreWithFilteringIT;

File: langchain4j-elasticsearch/src/test/java/dev/langchain4j/store/embedding/elasticsearch/ElasticsearchEmbeddingStoreIT.java
Patch:
@@ -1,7 +1,7 @@
 package dev.langchain4j.store.embedding.elasticsearch;
 
 import dev.langchain4j.data.segment.TextSegment;
-import dev.langchain4j.model.embedding.AllMiniLmL6V2QuantizedEmbeddingModel;
+import dev.langchain4j.model.embedding.onnx.allminilml6v2q.AllMiniLmL6V2QuantizedEmbeddingModel;
 import dev.langchain4j.model.embedding.EmbeddingModel;
 import dev.langchain4j.store.embedding.EmbeddingStore;
 import dev.langchain4j.store.embedding.EmbeddingStoreWithFilteringIT;

File: langchain4j-elasticsearch/src/test/java/dev/langchain4j/store/embedding/elasticsearch/ElasticsearchEmbeddingStoreRemoveIT.java
Patch:
@@ -3,7 +3,7 @@
 import dev.langchain4j.data.document.Metadata;
 import dev.langchain4j.data.embedding.Embedding;
 import dev.langchain4j.data.segment.TextSegment;
-import dev.langchain4j.model.embedding.AllMiniLmL6V2QuantizedEmbeddingModel;
+import dev.langchain4j.model.embedding.onnx.allminilml6v2q.AllMiniLmL6V2QuantizedEmbeddingModel;
 import dev.langchain4j.model.embedding.EmbeddingModel;
 import dev.langchain4j.store.embedding.EmbeddingMatch;
 import dev.langchain4j.store.embedding.EmbeddingSearchRequest;

File: langchain4j-infinispan/src/test/java/dev/langchain4j/store/embedding/infinispan/InfinispanEmbeddingStoreIT.java
Patch:
@@ -1,7 +1,7 @@
 package dev.langchain4j.store.embedding.infinispan;
 
 import dev.langchain4j.data.segment.TextSegment;
-import dev.langchain4j.model.embedding.AllMiniLmL6V2QuantizedEmbeddingModel;
+import dev.langchain4j.model.embedding.onnx.allminilml6v2q.AllMiniLmL6V2QuantizedEmbeddingModel;
 import dev.langchain4j.model.embedding.EmbeddingModel;
 import dev.langchain4j.store.embedding.EmbeddingStore;
 import dev.langchain4j.store.embedding.EmbeddingStoreIT;

File: langchain4j-milvus/src/test/java/dev/langchain4j/store/embedding/milvus/MilvusEmbeddingStoreCloudIT.java
Patch:
@@ -2,7 +2,7 @@
 
 import dev.langchain4j.data.embedding.Embedding;
 import dev.langchain4j.data.segment.TextSegment;
-import dev.langchain4j.model.embedding.AllMiniLmL6V2QuantizedEmbeddingModel;
+import dev.langchain4j.model.embedding.onnx.allminilml6v2q.AllMiniLmL6V2QuantizedEmbeddingModel;
 import dev.langchain4j.model.embedding.EmbeddingModel;
 import dev.langchain4j.store.embedding.EmbeddingMatch;
 import dev.langchain4j.store.embedding.EmbeddingSearchRequest;

File: langchain4j-milvus/src/test/java/dev/langchain4j/store/embedding/milvus/MilvusEmbeddingStoreIT.java
Patch:
@@ -2,7 +2,7 @@
 
 import dev.langchain4j.data.embedding.Embedding;
 import dev.langchain4j.data.segment.TextSegment;
-import dev.langchain4j.model.embedding.AllMiniLmL6V2QuantizedEmbeddingModel;
+import dev.langchain4j.model.embedding.onnx.allminilml6v2q.AllMiniLmL6V2QuantizedEmbeddingModel;
 import dev.langchain4j.model.embedding.EmbeddingModel;
 import dev.langchain4j.store.embedding.EmbeddingMatch;
 import dev.langchain4j.store.embedding.EmbeddingSearchRequest;

File: langchain4j-milvus/src/test/java/dev/langchain4j/store/embedding/milvus/MilvusEmbeddingStoreRemovalIT.java
Patch:
@@ -1,7 +1,7 @@
 package dev.langchain4j.store.embedding.milvus;
 
 import dev.langchain4j.data.segment.TextSegment;
-import dev.langchain4j.model.embedding.AllMiniLmL6V2QuantizedEmbeddingModel;
+import dev.langchain4j.model.embedding.onnx.allminilml6v2q.AllMiniLmL6V2QuantizedEmbeddingModel;
 import dev.langchain4j.model.embedding.EmbeddingModel;
 import dev.langchain4j.store.embedding.EmbeddingStore;
 import dev.langchain4j.store.embedding.EmbeddingStoreWithRemovalIT;

File: langchain4j-mongodb-atlas/src/test/java/dev/langchain4j/store/embedding/mongodb/MongoDbEmbeddingStoreCloudIT.java
Patch:
@@ -6,7 +6,7 @@
 import com.mongodb.client.MongoCollection;
 import com.mongodb.client.model.Filters;
 import dev.langchain4j.data.segment.TextSegment;
-import dev.langchain4j.model.embedding.AllMiniLmL6V2QuantizedEmbeddingModel;
+import dev.langchain4j.model.embedding.onnx.allminilml6v2q.AllMiniLmL6V2QuantizedEmbeddingModel;
 import dev.langchain4j.model.embedding.EmbeddingModel;
 import dev.langchain4j.store.embedding.EmbeddingStore;
 import dev.langchain4j.store.embedding.EmbeddingStoreIT;

File: langchain4j-mongodb-atlas/src/test/java/dev/langchain4j/store/embedding/mongodb/MongoDbEmbeddingStoreFilterIT.java
Patch:
@@ -7,7 +7,7 @@
 import dev.langchain4j.data.document.Metadata;
 import dev.langchain4j.data.embedding.Embedding;
 import dev.langchain4j.data.segment.TextSegment;
-import dev.langchain4j.model.embedding.AllMiniLmL6V2QuantizedEmbeddingModel;
+import dev.langchain4j.model.embedding.onnx.allminilml6v2q.AllMiniLmL6V2QuantizedEmbeddingModel;
 import dev.langchain4j.model.embedding.EmbeddingModel;
 import dev.langchain4j.store.embedding.EmbeddingMatch;
 import dev.langchain4j.store.embedding.EmbeddingStore;

File: langchain4j-mongodb-atlas/src/test/java/dev/langchain4j/store/embedding/mongodb/MongoDbEmbeddingStoreLocalIT.java
Patch:
@@ -6,7 +6,7 @@
 import com.mongodb.client.MongoCollection;
 import com.mongodb.client.model.Filters;
 import dev.langchain4j.data.segment.TextSegment;
-import dev.langchain4j.model.embedding.AllMiniLmL6V2QuantizedEmbeddingModel;
+import dev.langchain4j.model.embedding.onnx.allminilml6v2q.AllMiniLmL6V2QuantizedEmbeddingModel;
 import dev.langchain4j.model.embedding.EmbeddingModel;
 import dev.langchain4j.store.embedding.EmbeddingStore;
 import dev.langchain4j.store.embedding.EmbeddingStoreIT;

File: langchain4j-neo4j/src/test/java/dev/langchain4j/store/embedding/neo4j/Neo4jEmbeddingStoreTest.java
Patch:
@@ -3,7 +3,7 @@
 import dev.langchain4j.data.document.Metadata;
 import dev.langchain4j.data.embedding.Embedding;
 import dev.langchain4j.data.segment.TextSegment;
-import dev.langchain4j.model.embedding.AllMiniLmL6V2QuantizedEmbeddingModel;
+import dev.langchain4j.model.embedding.onnx.allminilml6v2q.AllMiniLmL6V2QuantizedEmbeddingModel;
 import dev.langchain4j.model.embedding.EmbeddingModel;
 import dev.langchain4j.store.embedding.CosineSimilarity;
 import dev.langchain4j.store.embedding.EmbeddingMatch;

File: langchain4j-opensearch/src/test/java/dev/langchain4j/store/embedding/opensearch/OpenSearchEmbeddingStoreAwsIT.java
Patch:
@@ -2,7 +2,7 @@
 
 import com.jayway.jsonpath.JsonPath;
 import dev.langchain4j.data.segment.TextSegment;
-import dev.langchain4j.model.embedding.AllMiniLmL6V2QuantizedEmbeddingModel;
+import dev.langchain4j.model.embedding.onnx.allminilml6v2q.AllMiniLmL6V2QuantizedEmbeddingModel;
 import dev.langchain4j.model.embedding.EmbeddingModel;
 import dev.langchain4j.store.embedding.EmbeddingStore;
 import dev.langchain4j.store.embedding.EmbeddingStoreIT;

File: langchain4j-opensearch/src/test/java/dev/langchain4j/store/embedding/opensearch/OpenSearchEmbeddingStoreLocalIT.java
Patch:
@@ -1,7 +1,7 @@
 package dev.langchain4j.store.embedding.opensearch;
 
 import dev.langchain4j.data.segment.TextSegment;
-import dev.langchain4j.model.embedding.AllMiniLmL6V2QuantizedEmbeddingModel;
+import dev.langchain4j.model.embedding.onnx.allminilml6v2q.AllMiniLmL6V2QuantizedEmbeddingModel;
 import dev.langchain4j.model.embedding.EmbeddingModel;
 import dev.langchain4j.store.embedding.EmbeddingStore;
 import dev.langchain4j.store.embedding.EmbeddingStoreIT;

File: langchain4j-pgvector/src/test/java/dev/langchain4j/store/embedding/pgvector/PgVectorEmbeddingIndexedStoreIT.java
Patch:
@@ -1,7 +1,7 @@
 package dev.langchain4j.store.embedding.pgvector;
 
 import dev.langchain4j.data.segment.TextSegment;
-import dev.langchain4j.model.embedding.AllMiniLmL6V2QuantizedEmbeddingModel;
+import dev.langchain4j.model.embedding.onnx.allminilml6v2q.AllMiniLmL6V2QuantizedEmbeddingModel;
 import dev.langchain4j.model.embedding.EmbeddingModel;
 import dev.langchain4j.store.embedding.EmbeddingStore;
 import dev.langchain4j.store.embedding.EmbeddingStoreWithFilteringIT;

File: langchain4j-pgvector/src/test/java/dev/langchain4j/store/embedding/pgvector/PgVectorEmbeddingStoreConfigIT.java
Patch:
@@ -1,7 +1,7 @@
 package dev.langchain4j.store.embedding.pgvector;
 
 import dev.langchain4j.data.segment.TextSegment;
-import dev.langchain4j.model.embedding.AllMiniLmL6V2QuantizedEmbeddingModel;
+import dev.langchain4j.model.embedding.onnx.allminilml6v2q.AllMiniLmL6V2QuantizedEmbeddingModel;
 import dev.langchain4j.model.embedding.EmbeddingModel;
 import dev.langchain4j.store.embedding.EmbeddingStore;
 import dev.langchain4j.store.embedding.EmbeddingStoreWithFilteringIT;

File: langchain4j-pgvector/src/test/java/dev/langchain4j/store/embedding/pgvector/PgVectorEmbeddingStoreIT.java
Patch:
@@ -1,7 +1,7 @@
 package dev.langchain4j.store.embedding.pgvector;
 
 import dev.langchain4j.data.segment.TextSegment;
-import dev.langchain4j.model.embedding.AllMiniLmL6V2QuantizedEmbeddingModel;
+import dev.langchain4j.model.embedding.onnx.allminilml6v2q.AllMiniLmL6V2QuantizedEmbeddingModel;
 import dev.langchain4j.model.embedding.EmbeddingModel;
 import dev.langchain4j.store.embedding.EmbeddingStore;
 import dev.langchain4j.store.embedding.EmbeddingStoreWithFilteringIT;

File: langchain4j-pgvector/src/test/java/dev/langchain4j/store/embedding/pgvector/PgVectorEmbeddingStoreRemovalIT.java
Patch:
@@ -1,7 +1,7 @@
 package dev.langchain4j.store.embedding.pgvector;
 
 import dev.langchain4j.data.segment.TextSegment;
-import dev.langchain4j.model.embedding.AllMiniLmL6V2QuantizedEmbeddingModel;
+import dev.langchain4j.model.embedding.onnx.allminilml6v2q.AllMiniLmL6V2QuantizedEmbeddingModel;
 import dev.langchain4j.model.embedding.EmbeddingModel;
 import dev.langchain4j.store.embedding.EmbeddingStore;
 import dev.langchain4j.store.embedding.EmbeddingStoreWithRemovalIT;

File: langchain4j-pgvector/src/test/java/dev/langchain4j/store/embedding/pgvector/PgVectorEmbeddingStoreUpgradeIT.java
Patch:
@@ -2,7 +2,7 @@
 
 import dev.langchain4j.data.embedding.Embedding;
 import dev.langchain4j.data.segment.TextSegment;
-import dev.langchain4j.model.embedding.AllMiniLmL6V2QuantizedEmbeddingModel;
+import dev.langchain4j.model.embedding.onnx.allminilml6v2q.AllMiniLmL6V2QuantizedEmbeddingModel;
 import dev.langchain4j.model.embedding.EmbeddingModel;
 import dev.langchain4j.store.embedding.*;
 import org.junit.jupiter.api.BeforeEach;

File: langchain4j-pinecone/src/test/java/dev/langchain4j/store/embedding/pinecone/PineconeEmbeddingStoreIT.java
Patch:
@@ -1,7 +1,7 @@
 package dev.langchain4j.store.embedding.pinecone;
 
 import dev.langchain4j.data.segment.TextSegment;
-import dev.langchain4j.model.embedding.AllMiniLmL6V2QuantizedEmbeddingModel;
+import dev.langchain4j.model.embedding.onnx.allminilml6v2q.AllMiniLmL6V2QuantizedEmbeddingModel;
 import dev.langchain4j.model.embedding.EmbeddingModel;
 import dev.langchain4j.store.embedding.EmbeddingStore;
 import dev.langchain4j.store.embedding.EmbeddingStoreWithoutMetadataIT;

File: langchain4j-qdrant/src/test/java/dev/langchain4j/store/embedding/qdrant/QdrantEmbeddingStoreIT.java
Patch:
@@ -1,7 +1,7 @@
 package dev.langchain4j.store.embedding.qdrant;
 
 import dev.langchain4j.data.segment.TextSegment;
-import dev.langchain4j.model.embedding.AllMiniLmL6V2QuantizedEmbeddingModel;
+import dev.langchain4j.model.embedding.onnx.allminilml6v2q.AllMiniLmL6V2QuantizedEmbeddingModel;
 import dev.langchain4j.model.embedding.EmbeddingModel;
 import dev.langchain4j.store.embedding.EmbeddingStore;
 import dev.langchain4j.store.embedding.EmbeddingStoreIT;

File: langchain4j-redis/src/test/java/dev/langchain4j/store/embedding/redis/RedisEmbeddingStoreIT.java
Patch:
@@ -2,7 +2,7 @@
 
 import com.redis.testcontainers.RedisContainer;
 import dev.langchain4j.data.segment.TextSegment;
-import dev.langchain4j.model.embedding.AllMiniLmL6V2QuantizedEmbeddingModel;
+import dev.langchain4j.model.embedding.onnx.allminilml6v2q.AllMiniLmL6V2QuantizedEmbeddingModel;
 import dev.langchain4j.model.embedding.EmbeddingModel;
 import dev.langchain4j.store.embedding.EmbeddingStore;
 import dev.langchain4j.store.embedding.EmbeddingStoreIT;

File: langchain4j-vearch/src/test/java/dev/langchain4j/store/embedding/vearch/VearchEmbeddingStoreIT.java
Patch:
@@ -3,7 +3,7 @@
 import dev.langchain4j.data.document.Metadata;
 import dev.langchain4j.data.embedding.Embedding;
 import dev.langchain4j.data.segment.TextSegment;
-import dev.langchain4j.model.embedding.AllMiniLmL6V2QuantizedEmbeddingModel;
+import dev.langchain4j.model.embedding.onnx.allminilml6v2q.AllMiniLmL6V2QuantizedEmbeddingModel;
 import dev.langchain4j.model.embedding.EmbeddingModel;
 import dev.langchain4j.store.embedding.*;
 import org.junit.jupiter.api.*;

File: langchain4j-weaviate/src/test/java/dev/langchain4j/store/embedding/weaviate/CloudWeaviateEmbeddingStoreIT.java
Patch:
@@ -1,7 +1,7 @@
 package dev.langchain4j.store.embedding.weaviate;
 
 import dev.langchain4j.data.segment.TextSegment;
-import dev.langchain4j.model.embedding.AllMiniLmL6V2QuantizedEmbeddingModel;
+import dev.langchain4j.model.embedding.onnx.allminilml6v2q.AllMiniLmL6V2QuantizedEmbeddingModel;
 import dev.langchain4j.model.embedding.EmbeddingModel;
 import dev.langchain4j.store.embedding.EmbeddingStore;
 import dev.langchain4j.store.embedding.EmbeddingStoreIT;

File: langchain4j-weaviate/src/test/java/dev/langchain4j/store/embedding/weaviate/LocalGRPCWeaviateEmbeddingStoreIT.java
Patch:
@@ -1,7 +1,7 @@
 package dev.langchain4j.store.embedding.weaviate;
 
 import dev.langchain4j.data.segment.TextSegment;
-import dev.langchain4j.model.embedding.AllMiniLmL6V2QuantizedEmbeddingModel;
+import dev.langchain4j.model.embedding.onnx.allminilml6v2q.AllMiniLmL6V2QuantizedEmbeddingModel;
 import dev.langchain4j.model.embedding.EmbeddingModel;
 import dev.langchain4j.store.embedding.EmbeddingStore;
 import dev.langchain4j.store.embedding.EmbeddingStoreIT;

File: langchain4j-weaviate/src/test/java/dev/langchain4j/store/embedding/weaviate/LocalWeaviateEmbeddingStoreIT.java
Patch:
@@ -1,7 +1,7 @@
 package dev.langchain4j.store.embedding.weaviate;
 
 import dev.langchain4j.data.segment.TextSegment;
-import dev.langchain4j.model.embedding.AllMiniLmL6V2QuantizedEmbeddingModel;
+import dev.langchain4j.model.embedding.onnx.allminilml6v2q.AllMiniLmL6V2QuantizedEmbeddingModel;
 import dev.langchain4j.model.embedding.EmbeddingModel;
 import dev.langchain4j.store.embedding.EmbeddingStore;
 import dev.langchain4j.store.embedding.EmbeddingStoreIT;

File: langchain4j-weaviate/src/test/java/dev/langchain4j/store/embedding/weaviate/LocalWeaviateEmbeddingStoreRemovalIT.java
Patch:
@@ -1,7 +1,7 @@
 package dev.langchain4j.store.embedding.weaviate;
 
 import dev.langchain4j.data.segment.TextSegment;
-import dev.langchain4j.model.embedding.AllMiniLmL6V2QuantizedEmbeddingModel;
+import dev.langchain4j.model.embedding.onnx.allminilml6v2q.AllMiniLmL6V2QuantizedEmbeddingModel;
 import dev.langchain4j.model.embedding.EmbeddingModel;
 import dev.langchain4j.store.embedding.EmbeddingStore;
 import dev.langchain4j.store.embedding.EmbeddingStoreWithRemovalIT;

File: langchain4j-weaviate/src/test/java/dev/langchain4j/store/embedding/weaviate/LocalWeaviateNoMetadataEmbeddingStoreIT.java
Patch:
@@ -2,7 +2,7 @@
 
 import dev.langchain4j.data.segment.TextSegment;
 import static dev.langchain4j.internal.Utils.randomUUID;
-import dev.langchain4j.model.embedding.AllMiniLmL6V2QuantizedEmbeddingModel;
+import dev.langchain4j.model.embedding.onnx.allminilml6v2q.AllMiniLmL6V2QuantizedEmbeddingModel;
 import dev.langchain4j.model.embedding.EmbeddingModel;
 import dev.langchain4j.store.embedding.EmbeddingStore;
 import org.testcontainers.junit.jupiter.Container;

File: langchain4j/src/main/java/dev/langchain4j/chain/ConversationalChain.java
Patch:
@@ -15,7 +15,7 @@
  * A chain for conversing with a specified {@link ChatLanguageModel} while maintaining a memory of the conversation.
  * Includes a default {@link ChatMemory} (a message window with maximum 10 messages), which can be overridden.
  * <br>
- * It is recommended to use {@link AiServices} instead, as it is more powerful.
+ * Chains are not going to be developed further, it is recommended to use {@link AiServices} instead.
  */
 public class ConversationalChain implements Chain<String, String> {
 

File: langchain4j/src/main/java/dev/langchain4j/chain/ConversationalRetrievalChain.java
Patch:
@@ -27,7 +27,7 @@
  * You can fully customize RAG behavior by providing an instance of a {@link RetrievalAugmentor},
  * such as {@link DefaultRetrievalAugmentor}, or your own custom implementation.
  * <br>
- * It is recommended to use {@link AiServices} instead, as it is more powerful.
+ * Chains are not going to be developed further, it is recommended to use {@link AiServices} instead.
  */
 public class ConversationalRetrievalChain implements Chain<String, String> {
 

File: langchain4j/src/main/java/dev/langchain4j/service/AiServiceContext.java
Patch:
@@ -1,6 +1,6 @@
 package dev.langchain4j.service;
 
-import dev.langchain4j.agent.tool.ToolExecutor;
+import dev.langchain4j.service.tool.ToolExecutor;
 import dev.langchain4j.agent.tool.ToolSpecification;
 import dev.langchain4j.memory.ChatMemory;
 import dev.langchain4j.memory.chat.ChatMemoryProvider;

File: langchain4j/src/main/java/dev/langchain4j/service/AiServiceStreamingResponseHandler.java
Patch:
@@ -1,7 +1,7 @@
 package dev.langchain4j.service;
 
 import dev.langchain4j.agent.tool.ToolExecutionRequest;
-import dev.langchain4j.agent.tool.ToolExecutor;
+import dev.langchain4j.service.tool.ToolExecutor;
 import dev.langchain4j.data.message.AiMessage;
 import dev.langchain4j.data.message.ChatMessage;
 import dev.langchain4j.data.message.ToolExecutionResultMessage;

File: langchain4j/src/main/java/dev/langchain4j/service/AiServices.java
Patch:
@@ -1,8 +1,8 @@
 package dev.langchain4j.service;
 
-import dev.langchain4j.agent.tool.DefaultToolExecutor;
+import dev.langchain4j.service.tool.DefaultToolExecutor;
 import dev.langchain4j.agent.tool.Tool;
-import dev.langchain4j.agent.tool.ToolExecutor;
+import dev.langchain4j.service.tool.ToolExecutor;
 import dev.langchain4j.agent.tool.ToolSpecification;
 import dev.langchain4j.data.message.AiMessage;
 import dev.langchain4j.data.message.ChatMessage;

File: langchain4j/src/main/java/dev/langchain4j/service/output/BigDecimalOutputParser.java
Patch:
@@ -1,8 +1,8 @@
-package dev.langchain4j.model.output;
+package dev.langchain4j.service.output;
 
 import java.math.BigDecimal;
 
-public class BigDecimalOutputParser implements OutputParser<BigDecimal> {
+class BigDecimalOutputParser implements OutputParser<BigDecimal> {
 
     @Override
     public BigDecimal parse(String string) {

File: langchain4j/src/main/java/dev/langchain4j/service/output/BigIntegerOutputParser.java
Patch:
@@ -1,8 +1,8 @@
-package dev.langchain4j.model.output;
+package dev.langchain4j.service.output;
 
 import java.math.BigInteger;
 
-public class BigIntegerOutputParser implements OutputParser<BigInteger> {
+class BigIntegerOutputParser implements OutputParser<BigInteger> {
 
     @Override
     public BigInteger parse(String string) {

File: langchain4j/src/main/java/dev/langchain4j/service/output/BooleanOutputParser.java
Patch:
@@ -1,6 +1,6 @@
-package dev.langchain4j.model.output;
+package dev.langchain4j.service.output;
 
-public class BooleanOutputParser implements OutputParser<Boolean> {
+class BooleanOutputParser implements OutputParser<Boolean> {
 
     @Override
     public Boolean parse(String string) {

File: langchain4j/src/main/java/dev/langchain4j/service/output/ByteOutputParser.java
Patch:
@@ -1,6 +1,6 @@
-package dev.langchain4j.model.output;
+package dev.langchain4j.service.output;
 
-public class ByteOutputParser implements OutputParser<Byte> {
+class ByteOutputParser implements OutputParser<Byte> {
 
     @Override
     public Byte parse(String string) {

File: langchain4j/src/main/java/dev/langchain4j/service/output/DateOutputParser.java
Patch:
@@ -1,10 +1,10 @@
-package dev.langchain4j.model.output;
+package dev.langchain4j.service.output;
 
 import java.text.ParseException;
 import java.text.SimpleDateFormat;
 import java.util.Date;
 
-public class DateOutputParser implements OutputParser<Date> {
+class DateOutputParser implements OutputParser<Date> {
 
     private static final String DATE_PATTERN = "yyyy-MM-dd";
     private static final SimpleDateFormat SIMPLE_DATE_FORMAT = new SimpleDateFormat(DATE_PATTERN);

File: langchain4j/src/main/java/dev/langchain4j/service/output/DoubleOutputParser.java
Patch:
@@ -1,6 +1,6 @@
-package dev.langchain4j.model.output;
+package dev.langchain4j.service.output;
 
-public class DoubleOutputParser implements OutputParser<Double> {
+class DoubleOutputParser implements OutputParser<Double> {
 
     @Override
     public Double parse(String string) {

File: langchain4j/src/main/java/dev/langchain4j/service/output/FloatOutputParser.java
Patch:
@@ -1,6 +1,6 @@
-package dev.langchain4j.model.output;
+package dev.langchain4j.service.output;
 
-public class FloatOutputParser implements OutputParser<Float> {
+class FloatOutputParser implements OutputParser<Float> {
 
     @Override
     public Float parse(String string) {

File: langchain4j/src/main/java/dev/langchain4j/service/output/IntOutputParser.java
Patch:
@@ -1,6 +1,6 @@
-package dev.langchain4j.model.output;
+package dev.langchain4j.service.output;
 
-public class IntOutputParser implements OutputParser<Integer> {
+class IntOutputParser implements OutputParser<Integer> {
 
     @Override
     public Integer parse(String string) {

File: langchain4j/src/main/java/dev/langchain4j/service/output/LocalDateOutputParser.java
Patch:
@@ -1,10 +1,10 @@
-package dev.langchain4j.model.output;
+package dev.langchain4j.service.output;
 
 import java.time.LocalDate;
 
 import static java.time.format.DateTimeFormatter.ISO_LOCAL_DATE;
 
-public class LocalDateOutputParser implements OutputParser<LocalDate> {
+class LocalDateOutputParser implements OutputParser<LocalDate> {
 
     @Override
     public LocalDate parse(String string) {

File: langchain4j/src/main/java/dev/langchain4j/service/output/LocalDateTimeOutputParser.java
Patch:
@@ -1,10 +1,10 @@
-package dev.langchain4j.model.output;
+package dev.langchain4j.service.output;
 
 import java.time.LocalDateTime;
 
 import static java.time.format.DateTimeFormatter.ISO_LOCAL_DATE_TIME;
 
-public class LocalDateTimeOutputParser implements OutputParser<LocalDateTime> {
+class LocalDateTimeOutputParser implements OutputParser<LocalDateTime> {
 
     @Override
     public LocalDateTime parse(String string) {

File: langchain4j/src/main/java/dev/langchain4j/service/output/LocalTimeOutputParser.java
Patch:
@@ -1,10 +1,10 @@
-package dev.langchain4j.model.output;
+package dev.langchain4j.service.output;
 
 import java.time.LocalTime;
 
 import static java.time.format.DateTimeFormatter.ISO_LOCAL_TIME;
 
-public class LocalTimeOutputParser implements OutputParser<LocalTime> {
+class LocalTimeOutputParser implements OutputParser<LocalTime> {
 
     @Override
     public LocalTime parse(String string) {

File: langchain4j/src/main/java/dev/langchain4j/service/output/LongOutputParser.java
Patch:
@@ -1,6 +1,6 @@
-package dev.langchain4j.model.output;
+package dev.langchain4j.service.output;
 
-public class LongOutputParser implements OutputParser<Long> {
+class LongOutputParser implements OutputParser<Long> {
 
     @Override
     public Long parse(String string) {

File: langchain4j/src/main/java/dev/langchain4j/service/output/OutputParser.java
Patch:
@@ -1,10 +1,10 @@
-package dev.langchain4j.model.output;
+package dev.langchain4j.service.output;
 
 /**
  * Represents an output parser.
  * @param <T> the type of the output.
  */
-public interface OutputParser<T> {
+interface OutputParser<T> {
 
     /**
      * Parse the given text.

File: langchain4j/src/main/java/dev/langchain4j/service/output/ShortOutputParser.java
Patch:
@@ -1,6 +1,6 @@
-package dev.langchain4j.model.output;
+package dev.langchain4j.service.output;
 
-public class ShortOutputParser implements OutputParser<Short> {
+class ShortOutputParser implements OutputParser<Short> {
 
     @Override
     public Short parse(String string) {

File: langchain4j/src/main/java/dev/langchain4j/service/tool/ToolExecutor.java
Patch:
@@ -1,5 +1,6 @@
-package dev.langchain4j.agent.tool;
+package dev.langchain4j.service.tool;
 
+import dev.langchain4j.agent.tool.ToolExecutionRequest;
 import dev.langchain4j.service.MemoryId;
 
 /**

File: langchain4j/src/test/java/dev/langchain4j/classification/EmbeddingModelTextClassifierTest.java
Patch:
@@ -1,6 +1,6 @@
 package dev.langchain4j.classification;
 
-import dev.langchain4j.model.embedding.AllMiniLmL6V2QuantizedEmbeddingModel;
+import dev.langchain4j.model.embedding.onnx.allminilml6v2q.AllMiniLmL6V2QuantizedEmbeddingModel;
 import org.junit.jupiter.api.Test;
 
 import java.util.HashMap;

File: langchain4j/src/test/java/dev/langchain4j/service/AiServicesWithRagIT.java
Patch:
@@ -9,7 +9,7 @@
 import dev.langchain4j.data.segment.TextSegment;
 import dev.langchain4j.memory.chat.MessageWindowChatMemory;
 import dev.langchain4j.model.chat.ChatLanguageModel;
-import dev.langchain4j.model.embedding.AllMiniLmL6V2QuantizedEmbeddingModel;
+import dev.langchain4j.model.embedding.onnx.allminilml6v2q.AllMiniLmL6V2QuantizedEmbeddingModel;
 import dev.langchain4j.model.embedding.EmbeddingModel;
 import dev.langchain4j.model.openai.OpenAiChatModel;
 import dev.langchain4j.model.openai.OpenAiTokenizer;

File: langchain4j/src/test/java/dev/langchain4j/service/AiServicesWithToolsIT.java
Patch:
@@ -16,6 +16,7 @@
 import dev.langchain4j.model.output.Response;
 import dev.langchain4j.model.output.TokenUsage;
 import dev.langchain4j.model.output.structured.Description;
+import dev.langchain4j.service.tool.ToolExecutor;
 import lombok.AllArgsConstructor;
 import lombok.Data;
 import org.junit.jupiter.api.Disabled;

File: langchain4j/src/test/java/dev/langchain4j/service/output/OutputParserTest.java
Patch:
@@ -1,4 +1,4 @@
-package dev.langchain4j.model.output;
+package dev.langchain4j.service.output;
 
 import dev.langchain4j.model.output.structured.Description;
 import org.assertj.core.api.WithAssertions;
@@ -11,8 +11,8 @@
 import java.util.Calendar;
 import java.util.Date;
 
-
 class OutputParserTest implements WithAssertions {
+
     @Test
     public void test_BigDecimal() {
         BigDecimalOutputParser parser = new BigDecimalOutputParser();

File: langchain4j/src/test/java/dev/langchain4j/service/tool/ToolExecutorTest.java
Patch:
@@ -1,5 +1,7 @@
-package dev.langchain4j.agent.tool;
+package dev.langchain4j.service.tool;
 
+import dev.langchain4j.agent.tool.Tool;
+import dev.langchain4j.agent.tool.ToolExecutionRequest;
 import org.junit.jupiter.params.ParameterizedTest;
 import org.junit.jupiter.params.provider.CsvSource;
 import org.junit.jupiter.params.provider.ValueSource;

File: langchain4j/src/test/java/dev/langchain4j/store/embedding/inmemory/InMemoryEmbeddingStoreRemovalTest.java
Patch:
@@ -1,7 +1,7 @@
 package dev.langchain4j.store.embedding.inmemory;
 
 import dev.langchain4j.data.segment.TextSegment;
-import dev.langchain4j.model.embedding.AllMiniLmL6V2QuantizedEmbeddingModel;
+import dev.langchain4j.model.embedding.onnx.allminilml6v2q.AllMiniLmL6V2QuantizedEmbeddingModel;
 import dev.langchain4j.model.embedding.EmbeddingModel;
 import dev.langchain4j.store.embedding.EmbeddingStore;
 import dev.langchain4j.store.embedding.EmbeddingStoreWithRemovalIT;

File: langchain4j/src/test/java/dev/langchain4j/store/embedding/inmemory/InMemoryEmbeddingStoreSerializedTest.java
Patch:
@@ -1,7 +1,7 @@
 package dev.langchain4j.store.embedding.inmemory;
 
 import dev.langchain4j.data.segment.TextSegment;
-import dev.langchain4j.model.embedding.AllMiniLmL6V2QuantizedEmbeddingModel;
+import dev.langchain4j.model.embedding.onnx.allminilml6v2q.AllMiniLmL6V2QuantizedEmbeddingModel;
 import dev.langchain4j.model.embedding.EmbeddingModel;
 import dev.langchain4j.store.embedding.EmbeddingStore;
 import dev.langchain4j.store.embedding.EmbeddingStoreWithFilteringIT;

File: langchain4j/src/test/java/dev/langchain4j/store/embedding/inmemory/InMemoryEmbeddingStoreTest.java
Patch:
@@ -3,7 +3,7 @@
 import dev.langchain4j.data.document.Metadata;
 import dev.langchain4j.data.embedding.Embedding;
 import dev.langchain4j.data.segment.TextSegment;
-import dev.langchain4j.model.embedding.AllMiniLmL6V2QuantizedEmbeddingModel;
+import dev.langchain4j.model.embedding.onnx.allminilml6v2q.AllMiniLmL6V2QuantizedEmbeddingModel;
 import dev.langchain4j.model.embedding.EmbeddingModel;
 import dev.langchain4j.store.embedding.EmbeddingMatch;
 import dev.langchain4j.store.embedding.EmbeddingStore;

File: langchain4j/src/main/java/dev/langchain4j/model/output/EnumOutputParser.java
Patch:
@@ -39,11 +39,11 @@ public String formatInstructions() {
 
             // 'enums' keyword will hopefully make it clearer that
             // no description should be included (if present)
-            instruction.append("one of these enums:");
+            instruction.append("\nYou must answer strictly with one of these enums:");
 
             for (Enum enumConstant : enumConstants) {
                 instruction.append("\n").append(enumConstant.name().toUpperCase(Locale.ROOT));
-                Optional<String> optionalEnumDescription = getEnumDescription(enumConstant);
+                Optional<String> optionalEnumDescription = getEnumDescription(enumClass, enumConstant);
                 optionalEnumDescription.ifPresent(description -> instruction.append(" - ").append(description));
             }
 
@@ -60,7 +60,7 @@ public String formatInstructions() {
      * @param enumConstant for which description should be returned
      * @return description of the provided enum
      */
-    private Optional<String> getEnumDescription(Enum enumConstant) throws NoSuchFieldException {
+    public static Optional<String> getEnumDescription(Class<? extends Enum> enumClass, Enum enumConstant) throws NoSuchFieldException {
         Field field = enumClass.getDeclaredField(enumConstant.name());
 
         if (field.isAnnotationPresent(Description.class)) {

File: langchain4j/src/main/java/dev/langchain4j/service/OutputParserFactory.java
Patch:
@@ -5,5 +5,5 @@
 import java.util.Optional;
 
 public interface OutputParserFactory {
-    Optional<OutputParser<?>> get(Class<?> returnType);
+    Optional<OutputParser<?>> get(Class<?> rawClass, Class<?> typeArgumentClass);
 }

File: langchain4j/src/test/java/dev/langchain4j/model/output/OutputParserTest.java
Patch:
@@ -118,7 +118,7 @@ public enum EnumWithDescription {
     public void test_Enum_format_instruction() {
         EnumOutputParser parser = new EnumOutputParser(Enum.class);
         assertThat(parser.formatInstructions())
-                .isEqualTo("one of these enums:\n" +
+                .isEqualTo("\nYou must answer strictly with one of these enums:\n" +
                         "A\n" +
                         "B\n" +
                         "C");
@@ -129,7 +129,7 @@ public void test_Enum_format_instruction() {
     public void test_Enum_with_description_format_instruction() {
         EnumOutputParser parser = new EnumOutputParser(EnumWithDescription.class);
         assertThat(parser.formatInstructions())
-                .isEqualTo("one of these enums:\n" +
+                .isEqualTo("\nYou must answer strictly with one of these enums:\n" +
                         "A - Majority of keywords starting with A\n" +
                         "B - Majority of keywords starting with B\n" +
                         "C - Majority of keywords starting with C");

File: langchain4j-ollama/src/main/java/dev/langchain4j/model/ollama/OllamaClient.java
Patch:
@@ -63,7 +63,7 @@ public OllamaClient(String baseUrl,
         OkHttpClient okHttpClient = okHttpClientBuilder.build();
 
         Retrofit retrofit = new Retrofit.Builder()
-                .baseUrl(baseUrl)
+                .baseUrl(baseUrl.endsWith("/") ? baseUrl : baseUrl + "/")
                 .client(okHttpClient)
                 .addConverterFactory(JacksonConverterFactory.create(OBJECT_MAPPER))
                 .build();

File: langchain4j-ollama/src/main/java/dev/langchain4j/model/ollama/OllamaMessagesUtils.java
Patch:
@@ -12,9 +12,9 @@
 
 class OllamaMessagesUtils {
 
-    private final static Predicate<ChatMessage> isUserMessage =
-            chatMessage -> chatMessage instanceof UserMessage;
-    private final static Predicate<UserMessage> hasImages =
+    private static final Predicate<ChatMessage> isUserMessage =
+            UserMessage.class::isInstance;
+    private static final Predicate<UserMessage> hasImages =
             userMessage -> userMessage.contents().stream()
                     .anyMatch(content -> IMAGE.equals(content.type()));
 

File: langchain4j-ollama/src/test/java/dev/langchain4j/model/ollama/OllamaStreamingChatModelIT.java
Patch:
@@ -186,7 +186,6 @@ public void onError(Throwable error) {
 
         // then
         assertThat(future.get())
-                .isExactlyInstanceOf(NullPointerException.class)
-                .hasMessageContaining("is null");
+                .isExactlyInstanceOf(NullPointerException.class);
     }
 }
\ No newline at end of file

File: langchain4j-ollama/src/test/java/dev/langchain4j/model/ollama/OllamaStreamingLanguageModelIT.java
Patch:
@@ -36,7 +36,7 @@ void should_stream_answer() {
         assertThat(response.content()).isEqualTo(answer);
 
         TokenUsage tokenUsage = response.tokenUsage();
-        assertThat(tokenUsage.inputTokenCount()).isEqualTo(13);
+        assertThat(tokenUsage.inputTokenCount()).isEqualTo(31);
         assertThat(tokenUsage.outputTokenCount()).isGreaterThan(0);
         assertThat(tokenUsage.totalTokenCount())
                 .isEqualTo(tokenUsage.inputTokenCount() + tokenUsage.outputTokenCount());
@@ -130,7 +130,6 @@ public void onError(Throwable error) {
 
         // then
         assertThat(future.get())
-                .isExactlyInstanceOf(NullPointerException.class)
-                .hasMessageContaining("is null");
+                .isExactlyInstanceOf(NullPointerException.class);
     }
 }

File: langchain4j-zhipu-ai/src/main/java/dev/langchain4j/model/zhipu/chat/ChatCompletionModel.java
Patch:
@@ -2,6 +2,7 @@
 
 public enum ChatCompletionModel {
     GLM_4("glm-4"),
+    GLM_4V("glm-4v"),
     GLM_4_0520("glm-4-0520"),
     GLM_4_AIR("glm-4-air"),
     GLM_4_AIRX("glm-4-airx"),

File: langchain4j-chroma/src/main/java/dev/langchain4j/store/embedding/chroma/AddEmbeddingsRequest.java
Patch:
@@ -9,7 +9,7 @@ class AddEmbeddingsRequest {
     private final List<String> ids;
     private final List<float[]> embeddings;
     private final List<String> documents;
-    private final List<Map<String, String>> metadatas;
+    private final List<Map<String, Object>> metadatas;
 
     public AddEmbeddingsRequest(Builder builder) {
         this.ids = builder.ids;
@@ -27,7 +27,7 @@ public static class Builder {
         private List<String> ids = new ArrayList<>();
         private List<float[]> embeddings = new ArrayList<>();
         private List<String> documents = new ArrayList<>();
-        private List<Map<String, String>> metadatas = new ArrayList<>();
+        private List<Map<String, Object>> metadatas = new ArrayList<>();
 
         public Builder ids(List<String> ids) {
             if (ids != null) {
@@ -50,7 +50,7 @@ public Builder documents(List<String> documents) {
             return this;
         }
 
-        public Builder metadatas(List<Map<String, String>> metadatas) {
+        public Builder metadatas(List<Map<String, Object>> metadatas) {
             if (metadatas != null) {
                 this.metadatas = metadatas;
             }

File: langchain4j-chroma/src/main/java/dev/langchain4j/store/embedding/chroma/Collection.java
Patch:
@@ -6,7 +6,7 @@ class Collection {
 
     private String id;
     private String name;
-    private Map<String, String> metadata;
+    private Map<String, Object> metadata;
 
     public String id() {
         return id;
@@ -16,7 +16,7 @@ public String name() {
         return name;
     }
 
-    public Map<String, String> metadata() {
+    public Map<String, Object> metadata() {
         return metadata;
     }
 }

File: langchain4j-chroma/src/main/java/dev/langchain4j/store/embedding/chroma/CreateCollectionRequest.java
Patch:
@@ -6,14 +6,14 @@
 class CreateCollectionRequest {
 
     private final String name;
-    private final Map<String, String> metadata;
+    private final Map<String, Object> metadata;
 
     /**
      * Currently, cosine distance is always used as the distance method for chroma implementation
      */
     CreateCollectionRequest(String name) {
         this.name = name;
-        HashMap<String, String> metadata = new HashMap<>();
+        HashMap<String, Object> metadata = new HashMap<>();
         metadata.put("hnsw:space", "cosine");
         this.metadata = metadata;
     }

File: langchain4j-chroma/src/main/java/dev/langchain4j/store/embedding/chroma/QueryResponse.java
Patch:
@@ -8,7 +8,7 @@ class QueryResponse {
     private List<List<String>> ids;
     private List<List<List<Float>>> embeddings;
     private List<List<String>> documents;
-    private List<List<Map<String, String>>> metadatas;
+    private List<List<Map<String, Object>>> metadatas;
     private List<List<Double>> distances;
 
     public List<List<String>> ids() {
@@ -23,7 +23,7 @@ public List<List<String>> documents() {
         return documents;
     }
 
-    public List<List<Map<String, String>>> metadatas() {
+    public List<List<Map<String, Object>>> metadatas() {
         return metadatas;
     }
 

File: langchain4j/src/test/java/dev/langchain4j/service/AiServicesWithToolsIT.java
Patch:
@@ -478,8 +478,6 @@ void processStrings(@P("Array of strings to process") String[] ids) {
 
     @ParameterizedTest
     @MethodSource("models")
-    @Disabled
-        // TODO fix: should automatically convert List<String> into String[]
     void should_use_tool_with_Array_of_Strings_parameter(ChatLanguageModel chatLanguageModel) {
 
         StringArrayProcessor stringArrayProcessor = spy(new StringArrayProcessor());

File: langchain4j/src/main/java/dev/langchain4j/service/ServiceOutputParser.java
Patch:
@@ -69,7 +69,7 @@ public String outputFormatInstructions(Class<?> returnType) {
 
         if (returnType.isEnum()) {
             String formatInstructions = new EnumOutputParser(returnType.asSubclass(Enum.class)).formatInstructions();
-            return "\nYou must answer strictly in the following format: " + formatInstructions;
+            return "\nYou must answer strictly with " + formatInstructions;
         }
 
         Optional<OutputParser<?>> outputParser = outputParserFactory.get(returnType);

File: langchain4j-zhipu-ai/src/main/java/dev/langchain4j/model/zhipu/chat/SystemMessage.java
Patch:
@@ -17,6 +17,7 @@
 @JsonIgnoreProperties(ignoreUnknown = true)
 public final class SystemMessage implements Message {
 
+    @Builder.Default
     private Role role = SYSTEM;
     private String content;
     private String name;

File: langchain4j-zhipu-ai/src/main/java/dev/langchain4j/model/zhipu/chat/ToolMessage.java
Patch:
@@ -17,7 +17,8 @@
 @JsonIgnoreProperties(ignoreUnknown = true)
 public final class ToolMessage implements Message {
 
-    private final Role role = TOOL;
+    @Builder.Default
+    private Role role = TOOL;
     private String toolCallId;
     private String content;
 

File: langchain4j-zhipu-ai/src/main/java/dev/langchain4j/model/zhipu/chat/UserMessage.java
Patch:
@@ -17,7 +17,8 @@
 @JsonIgnoreProperties(ignoreUnknown = true)
 public final class UserMessage implements Message {
 
-    private final Role role = USER;
+    @Builder.Default
+    private Role role = USER;
     private String content;
     private String name;
 

File: langchain4j-qianfan/src/main/java/dev/langchain4j/model/qianfan/QianfanChatModelNameEnum.java
Patch:
@@ -9,7 +9,7 @@ public enum QianfanChatModelNameEnum {
     ERNIE_BOT_4("ERNIE-Bot 4.0", "completions_pro"),
     ERNIE_BOT_8("ERNIE-Bot-8K", "ernie_bot_8k"),
     ERNIE_BOT_TURBO("ERNIE-Bot-turbo", "eb-instant"),
-    ERNIE_SPEED_128K("ERNIE-Speed-128K", "completions"),
+    ERNIE_SPEED_128K("ERNIE-Speed-128K", "ernie-speed-128k"),
     EB_TURBO_APPBUILDER("EB-turbo-AppBuilder", "ai_apaas"),
     YI_34B_CHAT("Yi-34B-Chat", "yi_34b_chat"),
     BLOOMZ_7B("BLOOMZ-7B","bloomz_7b1"),

File: langchain4j-azure-open-ai/src/main/java/dev/langchain4j/model/azure/InternalAzureOpenAiHelper.java
Patch:
@@ -39,6 +39,7 @@
 
 import static dev.langchain4j.data.message.AiMessage.aiMessage;
 import static dev.langchain4j.internal.Utils.getOrDefault;
+import static dev.langchain4j.internal.Utils.isNullOrEmpty;
 import static dev.langchain4j.internal.ValidationUtils.ensureNotBlank;
 import static dev.langchain4j.model.output.FinishReason.*;
 import static java.time.Duration.ofSeconds;
@@ -264,7 +265,7 @@ public void setRequired(List<String> required) {
     }
 
     public static AiMessage aiMessageFrom(ChatResponseMessage chatResponseMessage) {
-        if (chatResponseMessage.getContent() != null) {
+        if (isNullOrEmpty(chatResponseMessage.getToolCalls())) {
             return aiMessage(chatResponseMessage.getContent());
         } else {
             List<ToolExecutionRequest> toolExecutionRequests = chatResponseMessage.getToolCalls()

File: langchain4j-azure-open-ai/src/test/java/dev/langchain4j/model/azure/AzureOpenAIResponsibleAIIT.java
Patch:
@@ -80,7 +80,7 @@ void image_should_trigger_content_filter_for_sexual_content() {
         AzureOpenAiImageModel model = AzureOpenAiImageModel.builder()
                 .endpoint(System.getenv("AZURE_OPENAI_ENDPOINT"))
                 .apiKey(System.getenv("AZURE_OPENAI_KEY"))
-                .deploymentName("dall-e-3")
+                .deploymentName("dall-e-3-30")
                 .logRequestsAndResponses(true)
                 .build();
 
@@ -98,7 +98,7 @@ void language_model_should_trigger_content_filter_for_violence() {
         LanguageModel model = AzureOpenAiLanguageModel.builder()
                 .endpoint(System.getenv("AZURE_OPENAI_ENDPOINT"))
                 .apiKey(System.getenv("AZURE_OPENAI_KEY"))
-                .deploymentName("gpt-35-turbo-instruct")
+                .deploymentName("gpt-35-turbo-instruct-0914")
                 .tokenizer(new AzureOpenAiTokenizer(GPT_3_5_TURBO_INSTRUCT))
                 .temperature(0.0)
                 .maxTokens(20)
@@ -170,7 +170,7 @@ void streaming_language_should_trigger_content_filter_for_violence(String deploy
         StreamingLanguageModel model = AzureOpenAiStreamingLanguageModel.builder()
                 .endpoint(System.getenv("AZURE_OPENAI_ENDPOINT"))
                 .apiKey(System.getenv("AZURE_OPENAI_KEY"))
-                .deploymentName("gpt-35-turbo-instruct")
+                .deploymentName("gpt-35-turbo-instruct-0914")
                 .tokenizer(new AzureOpenAiTokenizer(GPT_3_5_TURBO_INSTRUCT))
                 .temperature(0.0)
                 .maxTokens(20)

File: langchain4j-azure-open-ai/src/test/java/dev/langchain4j/model/azure/AzureOpenAiEmbeddingModelIT.java
Patch:
@@ -25,7 +25,7 @@ class AzureOpenAiEmbeddingModelIT {
     EmbeddingModel model = AzureOpenAiEmbeddingModel.builder()
             .endpoint(System.getenv("AZURE_OPENAI_ENDPOINT"))
             .apiKey(System.getenv("AZURE_OPENAI_KEY"))
-            .deploymentName("text-embedding-ada-002")
+            .deploymentName("text-embedding-ada-002-2")
             .tokenizer(new AzureOpenAiTokenizer(TEXT_EMBEDDING_ADA_002))
             .logRequestsAndResponses(true)
             .build();
@@ -73,7 +73,7 @@ void should_embed_in_batches() {
 
     @ParameterizedTest(name = "Testing model {0}")
     @EnumSource(value = AzureOpenAiEmbeddingModelName.class,
-            mode = EXCLUDE, names = "TEXT_EMBEDDING_ADA_002_2")
+            mode = EXCLUDE, names = {"TEXT_EMBEDDING_ADA_002_2", "TEXT_EMBEDDING_3_SMALL", "TEXT_EMBEDDING_3_SMALL_1", "TEXT_EMBEDDING_3_LARGE", "TEXT_EMBEDDING_ADA_002", "TEXT_EMBEDDING_ADA_002_1"})
     void should_support_all_string_model_names(AzureOpenAiEmbeddingModelName modelName) {
 
         // given

File: langchain4j-azure-open-ai/src/test/java/dev/langchain4j/model/azure/AzureOpenAiLanguageModelIT.java
Patch:
@@ -22,7 +22,7 @@ class AzureOpenAiLanguageModelIT {
     LanguageModel model = AzureOpenAiLanguageModel.builder()
             .endpoint(System.getenv("AZURE_OPENAI_ENDPOINT"))
             .apiKey(System.getenv("AZURE_OPENAI_KEY"))
-            .deploymentName("gpt-35-turbo-instruct")
+            .deploymentName("gpt-35-turbo-instruct-0914")
             .tokenizer(new AzureOpenAiTokenizer(GPT_3_5_TURBO_INSTRUCT))
             .temperature(0.0)
             .maxTokens(20)
@@ -59,7 +59,7 @@ void should_generate_answer_and_finish_reason_length() {
 
     @ParameterizedTest(name = "Testing model {0}")
     @EnumSource(value = AzureOpenAiLanguageModelName.class,
-            mode = EXCLUDE, names = {"TEXT_DAVINCI_002", "TEXT_DAVINCI_002_1"})
+            mode = EXCLUDE, names = {"GPT_3_5_TURBO_INSTRUCT", "TEXT_DAVINCI_002", "TEXT_DAVINCI_002_1"})
     void should_support_all_string_model_names(AzureOpenAiLanguageModelName modelName) {
 
         // given

File: langchain4j-azure-open-ai/src/test/java/dev/langchain4j/model/azure/AzureOpenAiStreamingLanguageModelIT.java
Patch:
@@ -22,7 +22,7 @@ class AzureOpenAiStreamingLanguageModelIT {
     StreamingLanguageModel model = AzureOpenAiStreamingLanguageModel.builder()
             .endpoint(System.getenv("AZURE_OPENAI_ENDPOINT"))
             .apiKey(System.getenv("AZURE_OPENAI_KEY"))
-            .deploymentName("gpt-35-turbo-instruct")
+            .deploymentName("gpt-35-turbo-instruct-0914")
             .tokenizer(new AzureOpenAiTokenizer(GPT_3_5_TURBO_INSTRUCT))
             .temperature(0.0)
             .maxTokens(20)

File: langchain4j-vertex-ai/src/main/java/dev/langchain4j/model/vertexai/VertexAiEmbeddingModel.java
Patch:
@@ -65,7 +65,8 @@ public class VertexAiEmbeddingModel extends DimensionAwareEmbeddingModel {
     private final String titleMetadataKey;
 
     public enum TaskType {
-        RETRIEVAL_QUERY, RETRIEVAL_DOCUMENT, SEMANTIC_SIMILARITY, CLASSIFICATION, CLUSTERING
+        RETRIEVAL_QUERY, RETRIEVAL_DOCUMENT, SEMANTIC_SIMILARITY, CLASSIFICATION,
+        CLUSTERING, QUESTION_ANSWERING, FACT_VERIFICATION
     }
 
     public VertexAiEmbeddingModel(String endpoint,

File: langchain4j-azure-ai-search/src/test/java/dev/langchain4j/rag/content/retriever/azure/search/AzureAiSearchContentRetrieverIT.java
Patch:
@@ -10,7 +10,7 @@
 import dev.langchain4j.rag.content.Content;
 import dev.langchain4j.rag.query.Query;
 import dev.langchain4j.store.embedding.EmbeddingStore;
-import dev.langchain4j.store.embedding.EmbeddingStoreIT;
+import dev.langchain4j.store.embedding.EmbeddingStoreWithFilteringIT;
 import org.junit.jupiter.api.BeforeEach;
 import org.junit.jupiter.api.Test;
 import org.junit.jupiter.api.condition.EnabledIfEnvironmentVariable;
@@ -24,7 +24,7 @@
 import static org.assertj.core.api.Assertions.assertThat;
 
 @EnabledIfEnvironmentVariable(named = "AZURE_SEARCH_ENDPOINT", matches = ".+")
-public class AzureAiSearchContentRetrieverIT extends EmbeddingStoreIT {
+public class AzureAiSearchContentRetrieverIT extends EmbeddingStoreWithFilteringIT {
 
     private static final Logger log = LoggerFactory.getLogger(AzureAiSearchContentRetrieverIT.class);
 

File: langchain4j-azure-ai-search/src/test/java/dev/langchain4j/store/embedding/azure/search/AzureAiSearchEmbeddingStoreTest.java
Patch:
@@ -18,7 +18,7 @@ public class AzureAiSearchEmbeddingStoreTest {
     @Test
     public void empty_endpoint_should_not_be_allowed() {
         try {
-            new AzureAiSearchEmbeddingStore(null, keyCredential, false, dimensions, null);
+            new AzureAiSearchEmbeddingStore(null, keyCredential, false, dimensions, null, null);
             fail("Expected IllegalArgumentException to be thrown");
         } catch (IllegalArgumentException e) {
             assertEquals("endpoint cannot be null", e.getMessage());
@@ -28,7 +28,7 @@ public void empty_endpoint_should_not_be_allowed() {
     @Test
     public void index_and_index_name_should_not_both_be_defined() {
         try {
-            new AzureAiSearchEmbeddingStore(endpoint, keyCredential, false, index, indexName);
+            new AzureAiSearchEmbeddingStore(endpoint, keyCredential, false, index, indexName, null);
             fail("Expected IllegalArgumentException to be thrown");
         } catch (IllegalArgumentException e) {
             assertEquals("index and indexName cannot be both defined", e.getMessage());

File: langchain4j/src/test/java/dev/langchain4j/service/AiServicesWithRagIT.java
Patch:
@@ -537,10 +537,10 @@ void should_use_static_metadata_filter(ChatLanguageModel model) {
                 .build();
 
         // when
-        String answer1 = assistant.answer("Which animal?");
+        String answer = assistant.answer("Which animal is mentioned?");
 
         // then
-        assertThat(answer1).containsIgnoringCase("dog");
+        assertThat(answer).containsIgnoringCase("dog");
     }
 
     @ParameterizedTest

File: langchain4j-core/src/main/java/dev/langchain4j/agent/tool/ToolExecutionRequest.java
Patch:
@@ -5,7 +5,7 @@
 import static dev.langchain4j.internal.Utils.quoted;
 
 /**
- * Represents a request to execute a tool.
+ * Represents an LLM-generated request to execute a tool.
  */
 public class ToolExecutionRequest {
     private final String id;

File: langchain4j-zhipu-ai/src/main/java/dev/langchain4j/model/zhipu/ZhipuAiEmbeddingModel.java
Patch:
@@ -24,7 +24,6 @@
  */
 public class ZhipuAiEmbeddingModel extends DimensionAwareEmbeddingModel {
 
-    private final String baseUrl;
     private final Integer maxRetries;
     private final String model;
     private final ZhipuAiClient client;
@@ -38,11 +37,10 @@ public ZhipuAiEmbeddingModel(
             Boolean logRequests,
             Boolean logResponses
     ) {
-        this.baseUrl = getOrDefault(baseUrl, "https://open.bigmodel.cn/");
         this.model = getOrDefault(model, EMBEDDING_2.toString());
         this.maxRetries = getOrDefault(maxRetries, 3);
         this.client = ZhipuAiClient.builder()
-                .baseUrl(this.baseUrl)
+                .baseUrl(getOrDefault(baseUrl, "https://open.bigmodel.cn/"))
                 .apiKey(apiKey)
                 .logRequests(getOrDefault(logRequests, false))
                 .logResponses(getOrDefault(logResponses, false))

File: langchain4j-azure-open-ai/src/main/java/dev/langchain4j/model/azure/AzureOpenAiChatModel.java
Patch:
@@ -212,7 +212,7 @@ private AzureOpenAiChatModel(String deploymentName,
                                  List<ChatModelListener> listeners) {
 
         this.deploymentName = getOrDefault(deploymentName, "gpt-35-turbo");
-        this.tokenizer = tokenizer;
+        this.tokenizer = getOrDefault(tokenizer, AzureOpenAiTokenizer::new);
         this.maxTokens = maxTokens;
         this.temperature = getOrDefault(temperature, 0.7);
         this.topP = topP;

File: langchain4j-azure-open-ai/src/main/java/dev/langchain4j/model/azure/AzureOpenAiEmbeddingModel.java
Patch:
@@ -116,7 +116,7 @@ private AzureOpenAiEmbeddingModel(String deploymentName,
                                       Tokenizer tokenizer) {
 
         this.deploymentName = getOrDefault(deploymentName, TEXT_EMBEDDING_ADA_002.modelName());
-        this.tokenizer = tokenizer;
+        this.tokenizer = getOrDefault(tokenizer, AzureOpenAiTokenizer::new);
     }
 
     /**

File: langchain4j-azure-open-ai/src/main/java/dev/langchain4j/model/azure/AzureOpenAiLanguageModel.java
Patch:
@@ -190,7 +190,7 @@ private AzureOpenAiLanguageModel(String deploymentName,
                                      Integer bestOf) {
 
         this.deploymentName = getOrDefault(deploymentName, "gpt-35-turbo-instruct");
-        this.tokenizer = tokenizer;
+        this.tokenizer = getOrDefault(tokenizer, AzureOpenAiTokenizer::new);
         this.maxTokens = maxTokens;
         this.temperature = getOrDefault(temperature, 0.7);
         this.topP = topP;

File: langchain4j-azure-open-ai/src/main/java/dev/langchain4j/model/azure/AzureOpenAiStreamingLanguageModel.java
Patch:
@@ -189,7 +189,7 @@ private AzureOpenAiStreamingLanguageModel(String deploymentName,
                                               Double frequencyPenalty) {
 
         this.deploymentName = getOrDefault(deploymentName, "gpt-35-turbo-instruct");
-        this.tokenizer = tokenizer;
+        this.tokenizer = getOrDefault(tokenizer, AzureOpenAiTokenizer::new);
         this.maxTokens = maxTokens;
         this.temperature = getOrDefault(temperature, 0.7);
         this.topP = topP;

File: langchain4j-azure-open-ai/src/test/java/dev/langchain4j/model/azure/AzureOpenAiStreamingChatModelIT.java
Patch:
@@ -594,6 +594,6 @@ void tools_should_work_without_tokenizer() {
         Response<AiMessage> response = handler.get();
 
         assertThat(response.content().hasToolExecutionRequests()).isTrue();
-        assertThat(response.tokenUsage()).isNull();
+        assertThat(response.tokenUsage()).isNotNull();
     }
 }

File: langchain4j-azure-open-ai/src/main/java/dev/langchain4j/model/azure/AzureOpenAiStreamingChatModel.java
Patch:
@@ -31,7 +31,6 @@
 import reactor.core.publisher.Flux;
 
 import java.time.Duration;
-import java.util.Collection;
 import java.util.List;
 import java.util.Map;
 
@@ -236,7 +235,7 @@ private AzureOpenAiStreamingChatModel(String deploymentName,
                                           List<ChatModelListener> listeners) {
 
         this.deploymentName = getOrDefault(deploymentName, "gpt-35-turbo");
-        this.tokenizer = tokenizer;
+        this.tokenizer = getOrDefault(tokenizer, AzureOpenAiTokenizer::new);
         this.maxTokens = maxTokens;
         this.temperature = getOrDefault(temperature, 0.7);
         this.topP = topP;

File: langchain4j/src/test/java/dev/langchain4j/service/StreamingAiServicesIT.java
Patch:
@@ -23,6 +23,7 @@
 import java.util.concurrent.CompletableFuture;
 import java.util.stream.Stream;
 
+import static dev.langchain4j.model.mistralai.MistralAiChatModelName.MISTRAL_LARGE_LATEST;
 import static dev.langchain4j.model.openai.OpenAiChatModelName.GPT_3_5_TURBO_0613;
 import static dev.langchain4j.model.output.FinishReason.STOP;
 import static java.util.concurrent.TimeUnit.SECONDS;
@@ -43,10 +44,12 @@ static Stream<StreamingChatLanguageModel> models() {
                 AzureOpenAiStreamingChatModel.builder()
                         .endpoint(System.getenv("AZURE_OPENAI_ENDPOINT"))
                         .apiKey(System.getenv("AZURE_OPENAI_KEY"))
+                        .deploymentName("gpt-4o")
                         .logRequestsAndResponses(true)
                         .build(),
                 MistralAiStreamingChatModel.builder()
                         .apiKey(System.getenv("MISTRAL_AI_API_KEY"))
+                        .modelName(MISTRAL_LARGE_LATEST)
                         .logRequests(true)
                         .logResponses(true)
                         .build()

File: langchain4j/src/test/java/dev/langchain4j/service/StreamingAiServicesWithToolsIT.java
Patch:
@@ -21,6 +21,7 @@
 
 import static dev.langchain4j.agent.tool.JsonSchemaProperty.description;
 import static dev.langchain4j.agent.tool.JsonSchemaProperty.*;
+import static dev.langchain4j.model.mistralai.MistralAiChatModelName.MISTRAL_LARGE_LATEST;
 import static dev.langchain4j.service.StreamingAiServicesWithToolsIT.TransactionService.EXPECTED_SPECIFICATION;
 import static java.util.Arrays.asList;
 import static java.util.Collections.singletonList;
@@ -42,7 +43,7 @@ static Stream<StreamingChatLanguageModel> models() {
                         .build(),
                 MistralAiStreamingChatModel.builder()
                         .apiKey(System.getenv("MISTRAL_AI_API_KEY"))
-                        .modelName("mistral-large-latest")
+                        .modelName(MISTRAL_LARGE_LATEST)
                         .logRequests(true)
                         .logResponses(true)
                         .build()

File: langchain4j/src/main/java/dev/langchain4j/service/Result.java
Patch:
@@ -28,7 +28,7 @@ public class Result<T> {
     @Builder
     public Result(T content, TokenUsage tokenUsage, List<Content> sources, FinishReason finishReason) {
         this.content = ensureNotNull(content, "content");
-        this.tokenUsage = ensureNotNull(tokenUsage, "tokenUsage");
+        this.tokenUsage = tokenUsage;
         this.sources = copyIfNotNull(sources);
         this.finishReason = finishReason;
     }

File: langchain4j-azure-open-ai/src/main/java/dev/langchain4j/model/azure/AzureOpenAiChatModelName.java
Patch:
@@ -1,5 +1,8 @@
 package dev.langchain4j.model.azure;
 
+import java.util.HashMap;
+import java.util.Map;
+
 /**
  * You can get the latest model names from the Azure OpenAI documentation or by executing the Azure CLI command:
  * az cognitiveservices account list-models --resource-group "$RESOURCE_GROUP" --name "$AI_SERVICE" -o table

File: langchain4j-bedrock/src/main/java/dev/langchain4j/model/bedrock/internal/AbstractBedrockEmbeddingModel.java
Patch:
@@ -58,7 +58,7 @@ public Response<List<Embedding>> embedAll(List<TextSegment> textSegments) {
             embeddings.add(response.toEmbedding());
             totalInputToken += response.getInputTextTokenCount();
         }
-        
+
         return Response.from(
                 embeddings,
                 new TokenUsage(totalInputToken));

File: langchain4j-bedrock/src/test/java/dev/langchain4j/model/bedrock/BedrockEmbeddingIT.java
Patch:
@@ -45,5 +45,7 @@ void testBedrockTitanChatModel() {
         assertThat(tokenUsage.totalTokenCount()).isEqualTo(1);
 
         assertThat(response.finishReason()).isNull();
+
+        assertThat(embeddingModel.dimension()).isEqualTo(1536);
     }
 }

File: langchain4j-cohere/src/main/java/dev/langchain4j/model/cohere/CohereEmbeddingModel.java
Patch:
@@ -2,6 +2,7 @@
 
 import dev.langchain4j.data.embedding.Embedding;
 import dev.langchain4j.data.segment.TextSegment;
+import dev.langchain4j.model.embedding.DimensionAwareEmbeddingModel;
 import dev.langchain4j.model.embedding.EmbeddingModel;
 import dev.langchain4j.model.output.Response;
 import dev.langchain4j.model.output.TokenUsage;
@@ -20,7 +21,7 @@
  * An implementation of an {@link EmbeddingModel} that uses
  * <a href="https://docs.cohere.com/docs/embed">Cohere Embed API</a>.
  */
-public class CohereEmbeddingModel implements EmbeddingModel {
+public class CohereEmbeddingModel extends DimensionAwareEmbeddingModel {
 
     private static final String DEFAULT_BASE_URL = "https://api.cohere.ai/v1/";
 

File: langchain4j-core/src/main/java/dev/langchain4j/model/embedding/DisabledEmbeddingModel.java
Patch:
@@ -6,14 +6,16 @@
 import dev.langchain4j.model.output.Response;
 
 import java.util.List;
+import java.util.Map;
 
 /**
  * An {@link EmbeddingModel} which throws a {@link ModelDisabledException} for all of its methods
  * <p>
- *     This could be used in tests, or in libraries that extend this one to conditionally enable or disable functionality.
+ * This could be used in tests, or in libraries that extend this one to conditionally enable or disable functionality.
  * </p>
  */
 public class DisabledEmbeddingModel implements EmbeddingModel {
+
     @Override
     public Response<Embedding> embed(String text) {
         throw new ModelDisabledException("EmbeddingModel is disabled");

File: langchain4j-core/src/test/java/dev/langchain4j/model/embedding/DisabledEmbeddingModelTest.java
Patch:
@@ -18,5 +18,6 @@ void methodsShouldThrowException() {
         performAssertion(() -> this.model.embed("Hello"));
         performAssertion(() -> this.model.embed((TextSegment) null));
         performAssertion(() -> this.model.embedAll(Collections.emptyList()));
+        performAssertion(() -> this.model.dimension());
     }
 }
\ No newline at end of file

File: langchain4j-hugging-face/src/main/java/dev/langchain4j/model/huggingface/client/EmbeddingRequest.java
Patch:
@@ -1,6 +1,5 @@
 package dev.langchain4j.model.huggingface.client;
 
-import dev.langchain4j.model.huggingface.client.Options;
 import java.util.List;
 
 public class EmbeddingRequest {

File: langchain4j-jina/src/main/java/dev/langchain4j/model/jina/JinaEmbeddingModel.java
Patch:
@@ -2,6 +2,7 @@
 
 import dev.langchain4j.data.embedding.Embedding;
 import dev.langchain4j.data.segment.TextSegment;
+import dev.langchain4j.model.embedding.DimensionAwareEmbeddingModel;
 import dev.langchain4j.model.embedding.EmbeddingModel;
 import dev.langchain4j.model.jina.internal.api.JinaEmbeddingRequest;
 import dev.langchain4j.model.jina.internal.api.JinaEmbeddingResponse;
@@ -22,7 +23,7 @@
  * An implementation of an {@link EmbeddingModel} that uses
  * <a href="https://jina.ai/embeddings">Jina Embeddings API</a>.
  */
-public class JinaEmbeddingModel implements EmbeddingModel {
+public class JinaEmbeddingModel extends DimensionAwareEmbeddingModel {
 
     private static final String DEFAULT_BASE_URL = "https://api.jina.ai/";
     private static final String DEFAULT_MODEL = "jina-embeddings-v2-base-en";

File: langchain4j-local-ai/src/main/java/dev/langchain4j/model/localai/LocalAiEmbeddingModel.java
Patch:
@@ -5,7 +5,7 @@
 import dev.ai4j.openai4j.embedding.EmbeddingResponse;
 import dev.langchain4j.data.embedding.Embedding;
 import dev.langchain4j.data.segment.TextSegment;
-import dev.langchain4j.model.embedding.EmbeddingModel;
+import dev.langchain4j.model.embedding.DimensionAwareEmbeddingModel;
 import dev.langchain4j.model.localai.spi.LocalAiEmbeddingModelBuilderFactory;
 import dev.langchain4j.model.output.Response;
 import lombok.Builder;
@@ -22,7 +22,7 @@
 /**
  * See <a href="https://localai.io/features/embeddings/">LocalAI documentation</a> for more details.
  */
-public class LocalAiEmbeddingModel implements EmbeddingModel {
+public class LocalAiEmbeddingModel extends DimensionAwareEmbeddingModel {
 
     private final OpenAiClient client;
     private final String modelName;

File: langchain4j-nomic/src/main/java/dev/langchain4j/model/nomic/NomicEmbeddingModel.java
Patch:
@@ -2,7 +2,7 @@
 
 import dev.langchain4j.data.embedding.Embedding;
 import dev.langchain4j.data.segment.TextSegment;
-import dev.langchain4j.model.embedding.EmbeddingModel;
+import dev.langchain4j.model.embedding.DimensionAwareEmbeddingModel;
 import dev.langchain4j.model.output.Response;
 import dev.langchain4j.model.output.TokenUsage;
 import lombok.Builder;
@@ -20,7 +20,7 @@
  * An integration with Nomic Atlas's Text Embeddings API.
  * See more details <a href="https://docs.nomic.ai/reference/endpoints/nomic-embed-text">here</a>.
  */
-public class NomicEmbeddingModel implements EmbeddingModel {
+public class NomicEmbeddingModel extends DimensionAwareEmbeddingModel {
 
     private static final String DEFAULT_BASE_URL = "https://api-atlas.nomic.ai/v1/";
 

File: langchain4j-ollama/src/main/java/dev/langchain4j/model/ollama/OllamaEmbeddingModel.java
Patch:
@@ -2,7 +2,7 @@
 
 import dev.langchain4j.data.embedding.Embedding;
 import dev.langchain4j.data.segment.TextSegment;
-import dev.langchain4j.model.embedding.EmbeddingModel;
+import dev.langchain4j.model.embedding.DimensionAwareEmbeddingModel;
 import dev.langchain4j.model.ollama.spi.OllamaEmbeddingModelBuilderFactory;
 import dev.langchain4j.model.output.Response;
 import lombok.Builder;
@@ -21,7 +21,7 @@
 /**
  * <a href="https://github.com/jmorganca/ollama/blob/main/docs/api.md">Ollama API reference</a>
  */
-public class OllamaEmbeddingModel implements EmbeddingModel {
+public class OllamaEmbeddingModel extends DimensionAwareEmbeddingModel {
 
     private final OllamaClient client;
     private final String modelName;

File: langchain4j-open-ai/src/test/java/dev/langchain4j/model/openai/OpenAiEmbeddingModelIT.java
Patch:
@@ -74,14 +74,14 @@ void should_embed_multiple_segments() {
     void should_embed_text_with_embedding_shortening() {
 
         // given
-        int dimensions = 42;
+        int dimension = 42;
 
         EmbeddingModel model = OpenAiEmbeddingModel.builder()
                 .baseUrl(System.getenv("OPENAI_BASE_URL"))
                 .apiKey(System.getenv("OPENAI_API_KEY"))
                 .organizationId(System.getenv("OPENAI_ORGANIZATION_ID"))
                 .modelName("text-embedding-3-small")
-                .dimensions(dimensions)
+                .dimensions(dimension)
                 .logRequests(true)
                 .logResponses(true)
                 .build();
@@ -93,7 +93,7 @@ void should_embed_text_with_embedding_shortening() {
         System.out.println(response);
 
         // then
-        assertThat(response.content().dimension()).isEqualTo(dimensions);
+        assertThat(response.content().dimension()).isEqualTo(dimension);
 
         TokenUsage tokenUsage = response.tokenUsage();
         assertThat(tokenUsage.inputTokenCount()).isEqualTo(2);

File: langchain4j-zhipu-ai/src/main/java/dev/langchain4j/model/zhipu/ZhipuAiEmbeddingModel.java
Patch:
@@ -2,7 +2,7 @@
 
 import dev.langchain4j.data.embedding.Embedding;
 import dev.langchain4j.data.segment.TextSegment;
-import dev.langchain4j.model.embedding.EmbeddingModel;
+import dev.langchain4j.model.embedding.DimensionAwareEmbeddingModel;
 import dev.langchain4j.model.output.Response;
 import dev.langchain4j.model.zhipu.embedding.EmbeddingRequest;
 import dev.langchain4j.model.zhipu.embedding.EmbeddingResponse;
@@ -22,7 +22,7 @@
 /**
  * Represents an ZhipuAI embedding model, such as embedding-2.
  */
-public class ZhipuAiEmbeddingModel implements EmbeddingModel {
+public class ZhipuAiEmbeddingModel extends DimensionAwareEmbeddingModel {
 
     private final String baseUrl;
     private final Integer maxRetries;

File: langchain4j/src/main/java/dev/langchain4j/service/DefaultAiServices.java
Patch:
@@ -204,6 +204,7 @@ public Object invoke(Object proxy, Method method, Object[] args) throws Exceptio
                                     .content(parsedResponse)
                                     .tokenUsage(tokenUsageAccumulator)
                                     .sources(augmentationResult == null ? null : augmentationResult.contents())
+                                    .finishReason(response.finishReason())
                                     .build();
                         } else {
                             return parsedResponse;

File: langchain4j-azure-open-ai/src/test/java/dev/langchain4j/model/azure/AzureOpenAiChatModelIT.java
Patch:
@@ -84,7 +84,8 @@ void should_generate_answer_and_return_token_usage_and_finish_reason_stop(String
     void should_generate_answer_and_return_token_usage_and_finish_reason_length(String deploymentName, String gptVersion) {
 
         ChatLanguageModel model = AzureOpenAiChatModel.builder()
-                .endpoint(System.getenv("AZURE_OPENAI_ENDPOINT"))
+                .endpoint(System.getenv("AZURE_OPENAI" +
+                        "_ENDPOINT"))
                 .apiKey(System.getenv("AZURE_OPENAI_KEY"))
                 .deploymentName(deploymentName)
                 .tokenizer(new AzureOpenAiTokenizer(gptVersion))

File: langchain4j-azure-open-ai/src/test/java/dev/langchain4j/model/azure/AzureOpenAiStreamingChatModelIT.java
Patch:
@@ -122,9 +122,9 @@ void should_custom_models_work(String deploymentName, String gptVersion, boolean
         OpenAIAsyncClient asyncClient = null;
         OpenAIClient client = null;
         if(useCustomAsyncClient) {
-            asyncClient = InternalAzureOpenAiHelper.setupAsyncClient(System.getenv("AZURE_OPENAI_ENDPOINT"), gptVersion, System.getenv("AZURE_OPENAI_KEY"), Duration.ofSeconds(30), 5, null, true);
+            asyncClient = InternalAzureOpenAiHelper.setupAsyncClient(System.getenv("AZURE_OPENAI_ENDPOINT"), gptVersion, System.getenv("AZURE_OPENAI_KEY"), Duration.ofSeconds(30), 5, null, true, null);
         } else {
-            client = InternalAzureOpenAiHelper.setupSyncClient(System.getenv("AZURE_OPENAI_ENDPOINT"), gptVersion, System.getenv("AZURE_OPENAI_KEY"), Duration.ofSeconds(30), 5, null, true);
+            client = InternalAzureOpenAiHelper.setupSyncClient(System.getenv("AZURE_OPENAI_ENDPOINT"), gptVersion, System.getenv("AZURE_OPENAI_KEY"), Duration.ofSeconds(30), 5, null, true, null);
         }
 
         StreamingChatLanguageModel model = AzureOpenAiStreamingChatModel.builder()

File: langchain4j-azure-open-ai/src/test/java/dev/langchain4j/model/azure/InternalAzureOpenAiHelperTest.java
Patch:
@@ -31,7 +31,7 @@ void setupOpenAIClientShouldReturnClientWithCorrectConfiguration() {
         Integer maxRetries = 5;
         boolean logRequestsAndResponses = true;
 
-        OpenAIClient client = InternalAzureOpenAiHelper.setupSyncClient(endpoint, serviceVersion, apiKey, timeout, maxRetries, null, logRequestsAndResponses);
+        OpenAIClient client = InternalAzureOpenAiHelper.setupSyncClient(endpoint, serviceVersion, apiKey, timeout, maxRetries, null, logRequestsAndResponses, null);
 
         assertThat(client).isNotNull();
     }
@@ -45,7 +45,7 @@ void setupOpenAIAsyncClientShouldReturnClientWithCorrectConfiguration() {
         Integer maxRetries = 5;
         boolean logRequestsAndResponses = true;
 
-        OpenAIAsyncClient client = InternalAzureOpenAiHelper.setupAsyncClient(endpoint, serviceVersion, apiKey, timeout, maxRetries, null, logRequestsAndResponses);
+        OpenAIAsyncClient client = InternalAzureOpenAiHelper.setupAsyncClient(endpoint, serviceVersion, apiKey, timeout, maxRetries, null, logRequestsAndResponses, null);
 
         assertThat(client).isNotNull();
     }

File: langchain4j/src/main/java/dev/langchain4j/service/ServiceOutputParser.java
Patch:
@@ -118,7 +118,7 @@ public static String outputFormatInstructions(Class<?> returnType) {
         return "\nYou must answer strictly in the following JSON format: " + jsonStructure(returnType, new HashSet<>());
     }
 
-    private static String jsonStructure(Class<?> structured, Set<Class<?>> visited) {
+    public static String jsonStructure(Class<?> structured, Set<Class<?>> visited) {
         StringBuilder jsonSchema = new StringBuilder();
 
         jsonSchema.append("{\n");

File: langchain4j/src/main/java/dev/langchain4j/model/output/BigDecimalOutputParser.java
Patch:
@@ -6,7 +6,7 @@ public class BigDecimalOutputParser implements OutputParser<BigDecimal> {
 
     @Override
     public BigDecimal parse(String string) {
-        return new BigDecimal(string);
+        return new BigDecimal(string.trim());
     }
 
     @Override

File: langchain4j/src/main/java/dev/langchain4j/model/output/BigIntegerOutputParser.java
Patch:
@@ -6,7 +6,7 @@ public class BigIntegerOutputParser implements OutputParser<BigInteger> {
 
     @Override
     public BigInteger parse(String string) {
-        return new BigInteger(string);
+        return new BigInteger(string.trim());
     }
 
     @Override

File: langchain4j/src/main/java/dev/langchain4j/model/output/BooleanOutputParser.java
Patch:
@@ -4,7 +4,7 @@ public class BooleanOutputParser implements OutputParser<Boolean> {
 
     @Override
     public Boolean parse(String string) {
-        return Boolean.parseBoolean(string);
+        return Boolean.parseBoolean(string.trim());
     }
 
     @Override

File: langchain4j/src/main/java/dev/langchain4j/model/output/ByteOutputParser.java
Patch:
@@ -4,7 +4,7 @@ public class ByteOutputParser implements OutputParser<Byte> {
 
     @Override
     public Byte parse(String string) {
-        return Byte.parseByte(string);
+        return Byte.parseByte(string.trim());
     }
 
     @Override

File: langchain4j/src/main/java/dev/langchain4j/model/output/DoubleOutputParser.java
Patch:
@@ -4,7 +4,7 @@ public class DoubleOutputParser implements OutputParser<Double> {
 
     @Override
     public Double parse(String string) {
-        return Double.parseDouble(string);
+        return Double.parseDouble(string.trim());
     }
 
     @Override

File: langchain4j/src/main/java/dev/langchain4j/model/output/EnumOutputParser.java
Patch:
@@ -13,6 +13,7 @@ public EnumOutputParser(Class<? extends Enum> enumClass) {
 
     @Override
     public Enum parse(String string) {
+        string = string.trim();
         for (Enum enumConstant : enumClass.getEnumConstants()) {
             if (enumConstant.name().equalsIgnoreCase(string)) {
                 return enumConstant;

File: langchain4j/src/main/java/dev/langchain4j/model/output/FloatOutputParser.java
Patch:
@@ -4,7 +4,7 @@ public class FloatOutputParser implements OutputParser<Float> {
 
     @Override
     public Float parse(String string) {
-        return Float.parseFloat(string);
+        return Float.parseFloat(string.trim());
     }
 
     @Override

File: langchain4j/src/main/java/dev/langchain4j/model/output/IntOutputParser.java
Patch:
@@ -4,7 +4,7 @@ public class IntOutputParser implements OutputParser<Integer> {
 
     @Override
     public Integer parse(String string) {
-        return Integer.parseInt(string);
+        return Integer.parseInt(string.trim());
     }
 
     @Override

File: langchain4j/src/main/java/dev/langchain4j/model/output/LocalDateOutputParser.java
Patch:
@@ -8,7 +8,7 @@ public class LocalDateOutputParser implements OutputParser<LocalDate> {
 
     @Override
     public LocalDate parse(String string) {
-        return LocalDate.parse(string, ISO_LOCAL_DATE);
+        return LocalDate.parse(string.trim(), ISO_LOCAL_DATE);
     }
 
     @Override

File: langchain4j/src/main/java/dev/langchain4j/model/output/LocalDateTimeOutputParser.java
Patch:
@@ -8,7 +8,7 @@ public class LocalDateTimeOutputParser implements OutputParser<LocalDateTime> {
 
     @Override
     public LocalDateTime parse(String string) {
-        return LocalDateTime.parse(string, ISO_LOCAL_DATE_TIME);
+        return LocalDateTime.parse(string.trim(), ISO_LOCAL_DATE_TIME);
     }
 
     @Override

File: langchain4j/src/main/java/dev/langchain4j/model/output/LocalTimeOutputParser.java
Patch:
@@ -8,7 +8,7 @@ public class LocalTimeOutputParser implements OutputParser<LocalTime> {
 
     @Override
     public LocalTime parse(String string) {
-        return LocalTime.parse(string, ISO_LOCAL_TIME);
+        return LocalTime.parse(string.trim(), ISO_LOCAL_TIME);
     }
 
     @Override

File: langchain4j/src/main/java/dev/langchain4j/model/output/LongOutputParser.java
Patch:
@@ -4,7 +4,7 @@ public class LongOutputParser implements OutputParser<Long> {
 
     @Override
     public Long parse(String string) {
-        return Long.parseLong(string);
+        return Long.parseLong(string.trim());
     }
 
     @Override

File: langchain4j/src/main/java/dev/langchain4j/model/output/ShortOutputParser.java
Patch:
@@ -4,7 +4,7 @@ public class ShortOutputParser implements OutputParser<Short> {
 
     @Override
     public Short parse(String string) {
-        return Short.parseShort(string);
+        return Short.parseShort(string.trim());
     }
 
     @Override

File: langchain4j-zhipu-ai/src/main/java/dev/langchain4j/model/zhipu/AuthorizationInterceptor.java
Patch:
@@ -1,5 +1,6 @@
 package dev.langchain4j.model.zhipu;
 
+import com.fasterxml.jackson.core.JsonProcessingException;
 import com.google.common.cache.Cache;
 import com.google.common.cache.CacheBuilder;
 import io.jsonwebtoken.Jwts;
@@ -62,7 +63,7 @@ public Response intercept(Chain chain) throws IOException {
         return chain.proceed(request);
     }
 
-    private String generateToken() {
+    private String generateToken() throws JsonProcessingException {
         String[] apiKeyParts = this.apiKey.split("\\.");
         String keyId = apiKeyParts[0];
         String secret = apiKeyParts[1];

File: document-loaders/langchain4j-document-loader-azure-storage-blob/src/main/java/dev/langchain4j/data/document/source/azure/storage/blob/AzureBlobStorageSource.java
Patch:
@@ -36,10 +36,10 @@ public InputStream inputStream() {
     @Override
     public Metadata metadata() {
         Metadata metadata = new Metadata();
-        metadata.add(SOURCE, format("https://%s.blob.core.windows.net/%s/%s", accountName, containerName, blobName));
+        metadata.put(SOURCE, format("https://%s.blob.core.windows.net/%s/%s", accountName, containerName, blobName));
         metadata.add("azure_storage_blob_creation_time", properties.getCreationTime());
         metadata.add("azure_storage_blob_last_modified", properties.getLastModified());
-        metadata.add("azure_storage_blob_content_length", String.valueOf(properties.getBlobSize()));
+        metadata.put("azure_storage_blob_content_length", String.valueOf(properties.getBlobSize()));
         return metadata;
     }
 }

File: document-loaders/langchain4j-document-loader-github/src/test/java/dev/langchain4j/data/document/loader/github/GitHubDocumentLoaderIT.java
Patch:
@@ -36,8 +36,8 @@ public void should_load_file() {
         Document document = loader.loadDocument(TEST_OWNER, TEST_REPO, "main", "pom.xml", parser);
 
         assertThat(document.text()).contains("<groupId>dev.langchain4j</groupId>");
-        assertThat(document.metadata().asMap()).hasSize(9);
-        assertThat(document.metadata("github_git_url")).startsWith("https://api.github.com/repos/langchain4j/langchain4j");
+        assertThat(document.metadata().toMap()).hasSize(9);
+        assertThat(document.metadata().getString("github_git_url")).startsWith("https://api.github.com/repos/langchain4j/langchain4j");
     }
 
     @Test

File: document-parsers/langchain4j-document-parser-apache-pdfbox/src/test/java/dev/langchain4j/data/document/parser/apache/pdfbox/ApachePdfBoxDocumentParserTest.java
Patch:
@@ -21,7 +21,7 @@ void should_parse_pdf_file() {
         Document document = parser.parse(inputStream);
 
         assertThat(document.text()).isEqualToIgnoringWhitespace("test content");
-        assertThat(document.metadata().asMap()).isEmpty();
+        assertThat(document.metadata().toMap()).isEmpty();
     }
 
     @Test

File: document-parsers/langchain4j-document-parser-apache-poi/src/test/java/dev/langchain4j/data/document/parser/apache/poi/ApachePoiDocumentParserTest.java
Patch:
@@ -28,7 +28,7 @@ void should_parse_doc_and_ppt_files(String fileName) {
         Document document = parser.parse(inputStream);
 
         assertThat(document.text()).isEqualToIgnoringWhitespace("test content");
-        assertThat(document.metadata().asMap()).isEmpty();
+        assertThat(document.metadata().toMap()).isEmpty();
     }
 
     @ParameterizedTest
@@ -45,7 +45,7 @@ void should_parse_xls_files(String fileName) {
 
         assertThat(document.text())
                 .isEqualToIgnoringWhitespace("Sheet1\ntest content\nSheet2\ntest content");
-        assertThat(document.metadata().asMap()).isEmpty();
+        assertThat(document.metadata().toMap()).isEmpty();
     }
 
     @ParameterizedTest

File: langchain4j-azure-ai-search/src/main/java/dev/langchain4j/store/embedding/azure/search/AbstractAzureAiSearchEmbeddingStore.java
Patch:
@@ -328,10 +328,10 @@ private void addAllInternal(
                 document.setContent(embedded.get(i).text());
                 Document.Metadata metadata = new Document.Metadata();
                 List<Document.Metadata.Attribute> attributes = new ArrayList<>();
-                for (Map.Entry<String, String> entry : embedded.get(i).metadata().asMap().entrySet()) {
+                for (Map.Entry<String, Object> entry : embedded.get(i).metadata().toMap().entrySet()) {
                     Document.Metadata.Attribute attribute = new Document.Metadata.Attribute();
                     attribute.setKey(entry.getKey());
-                    attribute.setValue(entry.getValue());
+                    attribute.setValue(String.valueOf(entry.getValue()));
                     attributes.add(attribute);
                 }
                 metadata.setAttributes(attributes);

File: langchain4j-cassandra/src/test/java/dev/langchain4j/store/embedding/cassandra/CassandraEmbeddingStoreIT.java
Patch:
@@ -88,8 +88,8 @@ void should_retrieve_inserted_vector_by_ann_and_metadata() {
         String sourceSentence         = "In GOD we trust, everything else we test!";
         Embedding sourceEmbedding     = embeddingModel().embed(sourceSentence).content();
         TextSegment sourceTextSegment = TextSegment.from(sourceSentence, new Metadata()
-                .add("user", "GOD")
-                .add("test", "false"));
+                .put("user", "GOD")
+                .put("test", "false"));
         String id =  embeddingStore().add(sourceEmbedding, sourceTextSegment);
         assertThat(id != null && !id.isEmpty()).isTrue();
 

File: langchain4j-core/src/main/java/dev/langchain4j/data/document/Document.java
Patch:
@@ -91,7 +91,7 @@ public String metadata(String key) {
      * @return a TextSegment.
      */
     public TextSegment toTextSegment() {
-        return TextSegment.from(text, metadata.copy().add("index", "0"));
+        return TextSegment.from(text, metadata.copy().put("index", "0"));
     }
 
     @Override

File: langchain4j-core/src/main/java/dev/langchain4j/web/search/WebSearchOrganicResult.java
Patch:
@@ -184,7 +184,7 @@ private Metadata copyToMetadata() {
         docMetadata.add("url", url);
         if (metadata != null) {
             for (Map.Entry<String, String> entry : metadata.entrySet()) {
-                docMetadata.add(entry.getKey(), entry.getValue());
+                docMetadata.put(entry.getKey(), entry.getValue());
             }
         }
         return docMetadata;

File: langchain4j-core/src/test/java/dev/langchain4j/data/document/DocumentLoaderTest.java
Patch:
@@ -70,9 +70,9 @@ public Document parse(InputStream inputStream) {
 
     @Test
     public void test_load() {
-        StringSource source = new StringSource("Hello, world!", new Metadata().add("foo", "bar"));
+        StringSource source = new StringSource("Hello, world!", new Metadata().put("foo", "bar"));
         Document document = DocumentLoader.load(source, new TrivialParser());
-        assertThat(document).isEqualTo(Document.from("Hello, world!", new Metadata().add("foo", "bar")));
+        assertThat(document).isEqualTo(Document.from("Hello, world!", new Metadata().put("foo", "bar")));
 
         assertThatExceptionOfType(RuntimeException.class)
                 .isThrownBy(() -> DocumentLoader.load(new DocumentSource() {

File: langchain4j-core/src/test/java/dev/langchain4j/data/segment/TextSegmentTransformerTest.java
Patch:
@@ -22,7 +22,7 @@ public TextSegment transform(TextSegment segment) {
     public void test_transformAll() {
         TextSegmentTransformer transformer = new LowercaseFnordTransformer();
         TextSegment ts1 = TextSegment.from("Text");
-        ts1.metadata().add("abc", "123"); // metadata is copied over (not transformed
+        ts1.metadata().put("abc", "123"); // metadata is copied over (not transformed
 
         TextSegment ts2 = TextSegment.from("Segment");
         TextSegment ts3 = TextSegment.from("Fnord will be filtered out");

File: langchain4j-core/src/test/java/dev/langchain4j/rag/content/injector/DefaultContentInjectorTest.java
Patch:
@@ -150,12 +150,12 @@ void should_inject_multiple_contents_with_multiple_metadata_entries(
         TextSegment segment1 = TextSegment.from(
                 "Bananas are awesome!",
                 Metadata.from("source", "trust me bro")
-                        .add("date", "today")
+                        .put("date", "today")
         );
         TextSegment segment2 = TextSegment.from(
                 "Bananas are healthy!",
                 Metadata.from("source", "my doctor")
-                        .add("reliability", "100%")
+                        .put("reliability", "100%")
         );
         List<Content> contents = asList(Content.from(segment1), Content.from(segment2));
 

File: langchain4j-vertex-ai/src/test/java/dev/langchain4j/model/vertexai/VertexAiEmbeddingModelIT.java
Patch:
@@ -211,7 +211,7 @@ void testEmbeddingTask() {
         // Document retrieval embedding
 
         Metadata metadata = new Metadata();
-        metadata.add("title", "Text embeddings");
+        metadata.put("title", "Text embeddings");
 
         TextSegment segmentForRetrieval = new TextSegment("Text embeddings can be used to represent both the " +
             "user's query and the universe of documents in a high-dimensional vector space. Documents " +
@@ -236,7 +236,7 @@ void testEmbeddingTask() {
         // as the embedding model requires "title" to be used only for RETRIEVAL_DOCUMENT
 
         Metadata metadataCustomTitleKey = new Metadata();
-        metadataCustomTitleKey.add("customTitle", "Text embeddings");
+        metadataCustomTitleKey.put("customTitle", "Text embeddings");
 
         TextSegment segmentForRetrievalWithCustomKey = new TextSegment("Text embeddings can be used to represent both the " +
             "user's query and the universe of documents in a high-dimensional vector space. Documents " +

File: langchain4j-weaviate/src/main/java/dev/langchain4j/store/embedding/weaviate/WeaviateEmbeddingStore.java
Patch:
@@ -242,7 +242,7 @@ private WeaviateObject buildObject(String id, Embedding embedding, TextSegment s
         Map<String, Object> metadata = prefillMetadata();
         if (segment != null) {
             props.put(METADATA_TEXT_SEGMENT, segment.text());
-            if (!segment.metadata().asMap().isEmpty()) {
+            if (!segment.metadata().toMap().isEmpty()) {
                 for (String property : metadataKeys) {
                     if (segment.metadata().containsKey(property)) {
                         metadata.put(property, segment.metadata().get(property));

File: langchain4j/src/main/java/dev/langchain4j/data/document/source/FileSystemSource.java
Patch:
@@ -31,8 +31,8 @@ public InputStream inputStream() throws IOException {
     @Override
     public Metadata metadata() {
         return new Metadata()
-                .add(FILE_NAME, path.getFileName().toString())
-                .add(ABSOLUTE_DIRECTORY_PATH, path.toAbsolutePath().getParent().toString());
+                .put(FILE_NAME, path.getFileName().toString())
+                .put(ABSOLUTE_DIRECTORY_PATH, path.toAbsolutePath().getParent().toString());
     }
 
     public static FileSystemSource from(Path filePath) {

File: langchain4j/src/main/java/dev/langchain4j/data/document/splitter/HierarchicalDocumentSplitter.java
Patch:
@@ -227,7 +227,7 @@ int estimateSize(String text) {
      * @param index The index of the segment within the document.
      */
     static TextSegment createSegment(String text, Document document, int index) {
-        Metadata metadata = document.metadata().copy().add(INDEX, String.valueOf(index));
+        Metadata metadata = document.metadata().copy().put(INDEX, String.valueOf(index));
         return TextSegment.from(text, metadata);
     }
 }

File: langchain4j/src/main/java/dev/langchain4j/data/document/transformer/HtmlTextExtractor.java
Patch:
@@ -71,7 +71,7 @@ public Document transform(Document document) {
         Metadata metadata = document.metadata().copy();
         if (metadataCssSelectors != null) {
             metadataCssSelectors.forEach((metadataKey, cssSelector) ->
-                    metadata.add(metadataKey, jsoupDocument.select(cssSelector).text()));
+                    metadata.put(metadataKey, jsoupDocument.select(cssSelector).text()));
         }
 
         return Document.from(text, metadata);

File: langchain4j-qianfan/src/main/java/dev/langchain4j/model/qianfan/QianfanChatModelNameEnum.java
Patch:
@@ -9,6 +9,7 @@ public enum QianfanChatModelNameEnum {
     ERNIE_BOT_4("ERNIE-Bot 4.0", "completions_pro"),
     ERNIE_BOT_8("ERNIE-Bot-8K", "ernie_bot_8k"),
     ERNIE_BOT_TURBO("ERNIE-Bot-turbo", "eb-instant"),
+    ERNIE_SPEED_128K("ERNIE-Speed-128K", "completions"),
     EB_TURBO_APPBUILDER("EB-turbo-AppBuilder", "ai_apaas"),
     YI_34B_CHAT("Yi-34B-Chat", "yi_34b_chat"),
     BLOOMZ_7B("BLOOMZ-7B","bloomz_7b1"),

File: langchain4j-qianfan/src/main/java/dev/langchain4j/model/qianfan/QianfanStreamingChatModel.java
Patch:
@@ -65,7 +65,7 @@ public QianfanStreamingChatModel(String baseUrl,
         this.endpoint=Utils.isNullOrBlank(endpoint)? QianfanChatModelNameEnum.getEndpoint(modelName):endpoint;
 
         if (Utils.isNullOrBlank(this.endpoint)) {
-            throw new IllegalArgumentException("Qianfan is no such model name. You can see model name here: https://cloud.baidu.com/doc/WENXINWORKSHOP/s/Nlks5zkzu");
+            throw new IllegalArgumentException("Qianfan is no such model name(or there is no model definition in the QianfanChatModelNameEnum class). You can see model name here: https://cloud.baidu.com/doc/WENXINWORKSHOP/s/Nlks5zkzu");
         }
 
         this.baseUrl = getOrDefault(baseUrl,  "https://aip.baidubce.com");

File: langchain4j-mistral-ai/src/main/java/dev/langchain4j/model/mistralai/internal/client/DefaultMistralAiClient.java
Patch:
@@ -133,7 +133,7 @@ public void onEvent(EventSource eventSource, String id, String type, String data
                         MistralAiChatCompletionChoice choice = chatCompletionResponse.getChoices().get(0);
 
                         String chunk = choice.getDelta().getContent();
-                        if (isNotNullOrBlank(chunk)) {
+                        if (isNotNullOrEmpty(chunk)) {
                             contentBuilder.append(chunk);
                             handler.onNext(chunk);
                         }

File: langchain4j-azure-open-ai/src/test/java/dev/langchain4j/model/azure/AzureOpenAiEmbeddingModelIT.java
Patch:
@@ -3,7 +3,6 @@
 import dev.langchain4j.data.embedding.Embedding;
 import dev.langchain4j.data.segment.TextSegment;
 import dev.langchain4j.model.embedding.EmbeddingModel;
-import dev.langchain4j.model.openai.OpenAiTokenizer;
 import dev.langchain4j.model.output.Response;
 import dev.langchain4j.model.output.TokenUsage;
 import org.junit.jupiter.api.Test;
@@ -15,7 +14,7 @@
 import java.util.ArrayList;
 import java.util.List;
 
-import static dev.langchain4j.model.openai.OpenAiEmbeddingModelName.TEXT_EMBEDDING_ADA_002;
+import static dev.langchain4j.model.azure.AzureOpenAiEmbeddingModelName.TEXT_EMBEDDING_ADA_002;
 import static org.assertj.core.api.Assertions.assertThat;
 
 public class AzureOpenAiEmbeddingModelIT {
@@ -26,7 +25,7 @@ public class AzureOpenAiEmbeddingModelIT {
             .endpoint(System.getenv("AZURE_OPENAI_ENDPOINT"))
             .apiKey(System.getenv("AZURE_OPENAI_KEY"))
             .deploymentName("text-embedding-ada-002")
-            .tokenizer(new OpenAiTokenizer(TEXT_EMBEDDING_ADA_002))
+            .tokenizer(new AzureOpenAiTokenizer(TEXT_EMBEDDING_ADA_002))
             .logRequestsAndResponses(true)
             .build();
 

File: langchain4j-azure-open-ai/src/test/java/dev/langchain4j/model/azure/AzureOpenAiLanguageModelIT.java
Patch:
@@ -1,7 +1,6 @@
 package dev.langchain4j.model.azure;
 
 import dev.langchain4j.model.language.LanguageModel;
-import dev.langchain4j.model.openai.OpenAiTokenizer;
 import dev.langchain4j.model.output.Response;
 import dev.langchain4j.model.output.TokenUsage;
 import org.junit.jupiter.api.Test;
@@ -10,7 +9,7 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import static dev.langchain4j.model.openai.OpenAiLanguageModelName.GPT_3_5_TURBO_INSTRUCT;
+import static dev.langchain4j.model.azure.AzureOpenAiLanguageModelName.GPT_3_5_TURBO_INSTRUCT;
 import static dev.langchain4j.model.output.FinishReason.LENGTH;
 import static dev.langchain4j.model.output.FinishReason.STOP;
 import static org.assertj.core.api.Assertions.assertThat;
@@ -23,7 +22,7 @@ class AzureOpenAiLanguageModelIT {
             .endpoint(System.getenv("AZURE_OPENAI_ENDPOINT"))
             .apiKey(System.getenv("AZURE_OPENAI_KEY"))
             .deploymentName("gpt-35-turbo-instruct")
-            .tokenizer(new OpenAiTokenizer(GPT_3_5_TURBO_INSTRUCT))
+            .tokenizer(new AzureOpenAiTokenizer(GPT_3_5_TURBO_INSTRUCT))
             .temperature(0.0)
             .maxTokens(20)
             .logRequestsAndResponses(true)

File: langchain4j-azure-open-ai/src/test/java/dev/langchain4j/model/azure/AzureOpenAiStreamingLanguageModelIT.java
Patch:
@@ -2,15 +2,14 @@
 
 import dev.langchain4j.model.StreamingResponseHandler;
 import dev.langchain4j.model.language.StreamingLanguageModel;
-import dev.langchain4j.model.openai.OpenAiTokenizer;
 import dev.langchain4j.model.output.Response;
 import org.junit.jupiter.api.Test;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
 import java.util.concurrent.CompletableFuture;
 
-import static dev.langchain4j.model.openai.OpenAiLanguageModelName.GPT_3_5_TURBO_INSTRUCT;
+import static dev.langchain4j.model.azure.AzureOpenAiLanguageModelName.GPT_3_5_TURBO_INSTRUCT;
 import static dev.langchain4j.model.output.FinishReason.LENGTH;
 import static dev.langchain4j.model.output.FinishReason.STOP;
 import static java.util.concurrent.TimeUnit.SECONDS;
@@ -24,7 +23,7 @@ class AzureOpenAiStreamingLanguageModelIT {
             .endpoint(System.getenv("AZURE_OPENAI_ENDPOINT"))
             .apiKey(System.getenv("AZURE_OPENAI_KEY"))
             .deploymentName("gpt-35-turbo-instruct")
-            .tokenizer(new OpenAiTokenizer(GPT_3_5_TURBO_INSTRUCT))
+            .tokenizer(new AzureOpenAiTokenizer(GPT_3_5_TURBO_INSTRUCT))
             .temperature(0.0)
             .maxTokens(20)
             .logRequestsAndResponses(true)

File: langchain4j-mistral-ai/src/main/java/dev/langchain4j/model/mistralai/internal/api/MistralAiApi.java
Patch:
@@ -1,10 +1,10 @@
-package dev.langchain4j.model.mistralai;
+package dev.langchain4j.model.mistralai.internal.api;
 
 import okhttp3.ResponseBody;
 import retrofit2.Call;
 import retrofit2.http.*;
 
-interface MistralAiApi {
+public interface MistralAiApi {
 
     @POST("chat/completions")
     @Headers({"Content-Type: application/json"})

File: langchain4j-mistral-ai/src/main/java/dev/langchain4j/model/mistralai/internal/client/MistralAiApiKeyInterceptor.java
Patch:
@@ -1,4 +1,4 @@
-package dev.langchain4j.model.mistralai;
+package dev.langchain4j.model.mistralai.internal.client;
 
 import okhttp3.Interceptor;
 import okhttp3.Request;

File: langchain4j-mistral-ai/src/main/java/dev/langchain4j/model/mistralai/internal/client/MistralAiClient.java
Patch:
@@ -1,7 +1,8 @@
-package dev.langchain4j.model.mistralai;
+package dev.langchain4j.model.mistralai.internal.client;
 
 import dev.langchain4j.data.message.AiMessage;
 import dev.langchain4j.model.StreamingResponseHandler;
+import dev.langchain4j.model.mistralai.internal.api.*;
 import dev.langchain4j.spi.ServiceHelper;
 import java.time.Duration;
 

File: langchain4j-mistral-ai/src/main/java/dev/langchain4j/model/mistralai/internal/client/MistralAiClientBuilderFactory.java
Patch:
@@ -1,4 +1,4 @@
-package dev.langchain4j.model.mistralai;
+package dev.langchain4j.model.mistralai.internal.client;
 
 import java.util.function.Supplier;
 

File: langchain4j-mistral-ai/src/main/java/dev/langchain4j/model/mistralai/spi/MistralAiChatModelBuilderFactory.java
Patch:
@@ -7,5 +7,5 @@
 /**
  * A factory for building {@link dev.langchain4j.model.mistralai.MistralAiChatModel.MistralAiChatModelBuilder} instances.
  */
-public interface MistralAiChatModelBuilderFactory extends Supplier<MistralAiChatModel.MistralAiChatModelBuilder>{
+public interface MistralAiChatModelBuilderFactory extends Supplier<MistralAiChatModel.MistralAiChatModelBuilder> {
 }

File: langchain4j-mistral-ai/src/main/java/dev/langchain4j/model/mistralai/spi/MistralAiStreamingChatModelBuilderFactory.java
Patch:
@@ -7,5 +7,5 @@
 /**
  * A factory for building {@link MistralAiStreamingChatModel.MistralAiStreamingChatModelBuilder} instances.
  */
-public interface MistralAiStreamingChatModelBuilderFactory extends Supplier<MistralAiStreamingChatModel.MistralAiStreamingChatModelBuilder>{
+public interface MistralAiStreamingChatModelBuilderFactory extends Supplier<MistralAiStreamingChatModel.MistralAiStreamingChatModelBuilder> {
 }

File: langchain4j-mistral-ai/src/test/java/dev/langchain4j/model/mistralai/MistralAiChatModelIT.java
Patch:
@@ -7,6 +7,7 @@
 import dev.langchain4j.data.message.ToolExecutionResultMessage;
 import dev.langchain4j.data.message.UserMessage;
 import dev.langchain4j.model.chat.ChatLanguageModel;
+import dev.langchain4j.model.mistralai.internal.api.MistralAiResponseFormatType;
 import dev.langchain4j.model.output.Response;
 import dev.langchain4j.model.output.TokenUsage;
 import org.junit.jupiter.api.Test;

File: langchain4j-mistral-ai/src/test/java/dev/langchain4j/model/mistralai/MistralAiModelsIT.java
Patch:
@@ -1,5 +1,6 @@
 package dev.langchain4j.model.mistralai;
 
+import dev.langchain4j.model.mistralai.internal.api.MistralAiModelCard;
 import dev.langchain4j.model.output.Response;
 import org.junit.jupiter.api.Test;
 

File: langchain4j-mistral-ai/src/test/java/dev/langchain4j/model/mistralai/MistralAiStreamingChatModelIT.java
Patch:
@@ -8,6 +8,7 @@
 import dev.langchain4j.data.message.UserMessage;
 import dev.langchain4j.model.chat.StreamingChatLanguageModel;
 import dev.langchain4j.model.chat.TestStreamingResponseHandler;
+import dev.langchain4j.model.mistralai.internal.api.MistralAiResponseFormatType;
 import dev.langchain4j.model.output.Response;
 import dev.langchain4j.model.output.TokenUsage;
 import org.junit.jupiter.api.Test;

File: langchain4j-core/src/main/java/dev/langchain4j/rag/DefaultRetrievalAugmentor.java
Patch:
@@ -27,6 +27,7 @@
 import java.util.concurrent.Executors;
 
 import static dev.langchain4j.internal.Utils.getOrDefault;
+import static dev.langchain4j.internal.Utils.isNotNullOrBlank;
 import static dev.langchain4j.internal.ValidationUtils.ensureNotNull;
 import static java.util.concurrent.CompletableFuture.allOf;
 import static java.util.concurrent.CompletableFuture.supplyAsync;

File: langchain4j-core/src/test/java/dev/langchain4j/model/input/PromptTest.java
Patch:
@@ -24,6 +24,8 @@ public void test_constructor() {
                 .isEqualTo(systemMessage("abc"));
         assertThat(p.toUserMessage())
                 .isEqualTo(userMessage("abc"));
+        assertThat(p.toUserMessage("userName"))
+                .isEqualTo(userMessage("userName", "abc"));
         assertThat(p.toAiMessage())
                 .isEqualTo(aiMessage("abc"));
     }

File: langchain4j-dashscope/src/main/java/dev/langchain4j/model/dashscope/QwenTokenizer.java
Patch:
@@ -1,6 +1,6 @@
 package dev.langchain4j.model.dashscope;
 
-import com.alibaba.dashscope.aigc.generation.models.QwenParam;
+import com.alibaba.dashscope.aigc.generation.GenerationParam;
 import com.alibaba.dashscope.exception.InputRequiredException;
 import com.alibaba.dashscope.exception.NoApiKeyException;
 import com.alibaba.dashscope.tokenizers.Tokenization;
@@ -36,7 +36,7 @@ public QwenTokenizer(String apiKey, String modelName) {
     public int estimateTokenCountInText(String text) {
         String prompt = isBlank(text) ? text + "_" : text;
         try {
-            QwenParam param = QwenParam.builder()
+            GenerationParam param = GenerationParam.builder()
                     .apiKey(apiKey)
                     .model(modelName)
                     .prompt(prompt)
@@ -58,7 +58,7 @@ public int estimateTokenCountInMessage(ChatMessage message) {
     @Override
     public int estimateTokenCountInMessages(Iterable<ChatMessage> messages) {
         try {
-            QwenParam param = QwenParam.builder()
+            GenerationParam param = GenerationParam.builder()
                     .apiKey(apiKey)
                     .model(modelName)
                     .messages(toQwenMessages(messages))

File: experimental/langchain4j-experimental-sql/src/main/java/dev/langchain4j/experimental/rag/content/retriever/sql/SqlDatabaseContentRetriever.java
Patch:
@@ -330,7 +330,9 @@ protected String execute(String sqlQuery, Statement statement) throws SQLExcepti
             while (resultSet.next()) {
                 List<String> columnValues = new ArrayList<>();
                 for (int i = 1; i <= columnCount; i++) {
-                    String columnValue = resultSet.getObject(i).toString();
+
+                    String columnValue = resultSet.getObject(i)==null?"":resultSet.getObject(i).toString();
+
                     if (columnValue.contains(",")) {
                         columnValue = "\"" + columnValue + "\"";
                     }

File: langchain4j-zhipu-ai/src/main/java/dev/langchain4j/model/zhipu/DefaultZhipuAiHelper.java
Patch:
@@ -44,6 +44,9 @@ private static Function toFunction(ToolSpecification toolSpecification) {
     }
 
     private static Parameters toFunctionParameters(ToolParameters toolParameters) {
+        if (toolParameters == null) {
+            return Parameters.builder().build();
+        }
         return Parameters.builder()
                 .properties(toolParameters.properties())
                 .required(toolParameters.required())

File: langchain4j-cohere/src/main/java/dev/langchain4j/model/cohere/BilledUnits.java
Patch:
@@ -5,5 +5,7 @@
 @Getter
 class BilledUnits {
 
+    private Integer inputTokens;
+    private Integer outputTokens;
     private Integer searchUnits;
 }

File: langchain4j-azure-open-ai/src/main/java/dev/langchain4j/model/azure/AzureOpenAiModelName.java
Patch:
@@ -11,13 +11,14 @@ public class AzureOpenAiModelName {
     public static final String GPT_3_5_TURBO_16K = "gpt-3.5-turbo-16k"; // alias for the latest model
     public static final String GPT_3_5_TURBO_16K_0613 = "gpt-3.5-turbo-16k-0613"; // 16k context, functions
 
-    public static final String GPT_4 = "gpt-4"; // alias for the latest model
+    public static final String GPT_4 = "gpt-4"; // alias for the latest gpt-4
     public static final String GPT_4_1106_PREVIEW = "gpt-4-1106-preview"; // 8k context
     public static final String GPT_4_0613 = "gpt-4-0613"; // 8k context, functions
 
     public static final String GPT_4_32K = "gpt-4-32k"; // alias for the latest model
     public static final String GPT_4_32K_0613 = "gpt-4-32k-0613"; // 32k context, functions
 
+    public static final String GPT_4_O = "gpt-4o"; // alias for the latest gpt-4o model
 
     // Use with AzureOpenAiLanguageModel and AzureOpenAiStreamingLanguageModel
     public static final String TEXT_DAVINCI_002 = "text-davinci-002";

File: langchain4j-ollama/src/test/java/dev/langchain4j/model/ollama/OllamaChatModelIT.java
Patch:
@@ -21,6 +21,8 @@ class OllamaChatModelIT extends AbstractOllamaLanguageModelInfrastructure {
             .baseUrl(ollama.getEndpoint())
             .modelName(TINY_DOLPHIN_MODEL)
             .temperature(0.0)
+            .logRequests(true)
+            .logResponses(true)
             .build();
 
     @Test
@@ -39,7 +41,7 @@ void should_generate_response() {
         assertThat(aiMessage.toolExecutionRequests()).isNull();
 
         TokenUsage tokenUsage = response.tokenUsage();
-        assertThat(tokenUsage.inputTokenCount()).isEqualTo(13);
+        assertThat(tokenUsage.inputTokenCount()).isGreaterThan(0);
         assertThat(tokenUsage.outputTokenCount()).isGreaterThan(0);
         assertThat(tokenUsage.totalTokenCount())
                 .isEqualTo(tokenUsage.inputTokenCount() + tokenUsage.outputTokenCount());

File: langchain4j-ollama/src/test/java/dev/langchain4j/model/ollama/OllamaOpenAiChatModelIT.java
Patch:
@@ -43,7 +43,7 @@ void should_generate_response() {
         assertThat(aiMessage.toolExecutionRequests()).isNull();
 
         TokenUsage tokenUsage = response.tokenUsage();
-        assertThat(tokenUsage.inputTokenCount()).isEqualTo(35);
+        assertThat(tokenUsage.inputTokenCount()).isGreaterThan(0);
         assertThat(tokenUsage.outputTokenCount()).isGreaterThan(0);
         assertThat(tokenUsage.totalTokenCount())
                 .isEqualTo(tokenUsage.inputTokenCount() + tokenUsage.outputTokenCount());

File: langchain4j-ollama/src/test/java/dev/langchain4j/model/ollama/OllamaStreamingChatModelIT.java
Patch:
@@ -25,6 +25,8 @@ class OllamaStreamingChatModelIT extends AbstractOllamaLanguageModelInfrastructu
             .baseUrl(ollama.getEndpoint())
             .modelName(TINY_DOLPHIN_MODEL)
             .temperature(0.0)
+            .logRequests(true)
+            .logResponses(true)
             .build();
 
     @Test

File: langchain4j-vertex-ai-gemini/src/main/java/dev/langchain4j/model/vertexai/RoleMapper.java
Patch:
@@ -11,6 +11,8 @@ static String map(ChatMessageType type) {
                 return "user";
             case AI:
                 return "model";
+            case SYSTEM:
+                return "system";
         }
         throw new IllegalArgumentException(type + " is not allowed.");
     }

File: langchain4j-azure-ai-search/src/test/java/dev/langchain4j/rag/content/retriever/azure/search/AzureAiSearchContentRetrieverIT.java
Patch:
@@ -19,7 +19,7 @@
 
 import java.util.List;
 
-import static dev.langchain4j.store.embedding.azure.search.AbstractAzureAiSearchEmbeddingStore.INDEX_NAME;
+import static dev.langchain4j.store.embedding.azure.search.AbstractAzureAiSearchEmbeddingStore.DEFAULT_INDEX_NAME;
 import static java.util.Arrays.asList;
 import static org.assertj.core.api.Assertions.assertThat;
 
@@ -49,7 +49,7 @@ public AzureAiSearchContentRetrieverIT() {
                  .credential(new AzureKeyCredential(System.getenv("AZURE_SEARCH_KEY")))
                  .buildClient();
 
-         searchIndexClient.deleteIndex(INDEX_NAME);
+         searchIndexClient.deleteIndex(DEFAULT_INDEX_NAME);
 
          contentRetrieverWithVector =  createContentRetriever(AzureAiSearchQueryType.VECTOR);
          contentRetrieverWithFullText =  createFullTextSearchContentRetriever();

File: code-execution-engines/langchain4j-code-execution-engine-judge0/src/main/java/dev/langchain4j/code/judge0/JavaScriptCodeFixer.java
Patch:
@@ -1,4 +1,4 @@
-package dev.langchain4j.code;
+package dev.langchain4j.code.judge0;
 
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;

File: code-execution-engines/langchain4j-code-execution-engine-judge0/src/main/java/dev/langchain4j/code/judge0/Judge0JavaScriptEngine.java
Patch:
@@ -1,5 +1,6 @@
-package dev.langchain4j.code;
+package dev.langchain4j.code.judge0;
 
+import dev.langchain4j.code.CodeExecutionEngine;
 import dev.langchain4j.internal.Json;
 import okhttp3.*;
 import org.slf4j.Logger;

File: code-execution-engines/langchain4j-code-execution-engine-judge0/src/main/java/dev/langchain4j/code/judge0/Judge0JavaScriptExecutionTool.java
Patch:
@@ -1,11 +1,11 @@
-package dev.langchain4j.code;
+package dev.langchain4j.code.judge0;
 
 import dev.langchain4j.agent.tool.P;
 import dev.langchain4j.agent.tool.Tool;
 
 import java.time.Duration;
 
-import static dev.langchain4j.code.JavaScriptCodeFixer.fixIfNoLogToConsole;
+import static dev.langchain4j.code.judge0.JavaScriptCodeFixer.fixIfNoLogToConsole;
 import static dev.langchain4j.internal.Utils.isNullOrBlank;
 
 /**

File: code-execution-engines/langchain4j-code-execution-engine-judge0/src/test/java/dev/langchain4j/code/judge0/JavaScriptCodeFixerTest.java
Patch:
@@ -1,4 +1,4 @@
-package dev.langchain4j.code;
+package dev.langchain4j.code.judge0;
 
 import org.junit.jupiter.api.Test;
 

File: code-execution-engines/langchain4j-code-execution-engine-judge0/src/test/java/dev/langchain4j/code/judge0/Judge0JavaScriptEngineIT.java
Patch:
@@ -1,5 +1,6 @@
-package dev.langchain4j.code;
+package dev.langchain4j.code.judge0;
 
+import dev.langchain4j.code.CodeExecutionEngine;
 import org.junit.jupiter.api.Test;
 
 import java.time.Duration;

File: langchain4j-core/src/main/java/dev/langchain4j/rag/content/retriever/ContentRetriever.java
Patch:
@@ -17,7 +17,7 @@
  * - Hybrid of vector and full-text search (see {@code AzureAiSearchContentRetriever} in {@code langchain4j-azure-ai-search} module)
  * - Web Search Engine (see {@link WebSearchContentRetriever})
  * - Knowledge graph (see {@code Neo4jContentRetriever} in {@code langchain4j-neo4j} module)
- * - Relational database
+ * - SQL database (see {@code SqlDatabaseContentRetriever} in {@code langchain4j-experimental-sql} module)
  * - etc.
  * </pre>
  *

File: langchain4j/src/main/java/dev/langchain4j/service/AiServiceStreamingResponseHandler.java
Patch:
@@ -10,7 +10,6 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import java.util.List;
 import java.util.function.Consumer;
 
 import static dev.langchain4j.internal.ValidationUtils.ensureNotNull;
@@ -82,14 +81,14 @@ public void onComplete(Response<AiMessage> response) {
                             tokenHandler,
                             completionHandler,
                             errorHandler,
-                            tokenUsage.add(response.tokenUsage())
+                            TokenUsage.sum(tokenUsage, response.tokenUsage())
                     )
             );
         } else {
             if (completionHandler != null) {
                 completionHandler.accept(Response.from(
                         aiMessage,
-                        tokenUsage.add(response.tokenUsage()),
+                        TokenUsage.sum(tokenUsage, response.tokenUsage()),
                         response.finishReason())
                 );
             }

File: langchain4j/src/main/java/dev/langchain4j/service/DefaultAiServices.java
Patch:
@@ -187,7 +187,7 @@ public Object invoke(Object proxy, Method method, Object[] args) throws Exceptio
                             }
 
                             response = context.chatModel.generate(messages, context.toolSpecifications);
-                            tokenUsageAccumulator = tokenUsageAccumulator.add(response.tokenUsage());
+                            tokenUsageAccumulator = TokenUsage.sum(tokenUsageAccumulator, response.tokenUsage());
                         }
 
                         response = Response.from(response.content(), tokenUsageAccumulator, response.finishReason());

File: langchain4j-azure-open-ai/src/main/java/dev/langchain4j/model/azure/AzureOpenAiStreamingLanguageModel.java
Patch:
@@ -1,7 +1,9 @@
 package dev.langchain4j.model.azure;
 
 import com.azure.ai.openai.OpenAIClient;
-import com.azure.ai.openai.models.*;
+import com.azure.ai.openai.models.Choice;
+import com.azure.ai.openai.models.Completions;
+import com.azure.ai.openai.models.CompletionsOptions;
 import com.azure.core.credential.KeyCredential;
 import com.azure.core.credential.TokenCredential;
 import com.azure.core.exception.HttpResponseException;

File: langchain4j-anthropic/src/main/java/dev/langchain4j/model/anthropic/internal/api/AnthropicApi.java
Patch:
@@ -1,10 +1,10 @@
-package dev.langchain4j.model.anthropic;
+package dev.langchain4j.model.anthropic.internal.api;
 
 import okhttp3.ResponseBody;
 import retrofit2.Call;
 import retrofit2.http.*;
 
-interface AnthropicApi {
+public interface AnthropicApi {
 
     String X_API_KEY = "x-api-key";
 

File: langchain4j-anthropic/src/main/java/dev/langchain4j/model/anthropic/internal/client/AnthropicClient.java
Patch:
@@ -1,7 +1,9 @@
-package dev.langchain4j.model.anthropic;
+package dev.langchain4j.model.anthropic.internal.client;
 
 import dev.langchain4j.data.message.AiMessage;
 import dev.langchain4j.model.StreamingResponseHandler;
+import dev.langchain4j.model.anthropic.internal.api.AnthropicCreateMessageRequest;
+import dev.langchain4j.model.anthropic.internal.api.AnthropicCreateMessageResponse;
 import dev.langchain4j.spi.ServiceHelper;
 
 import java.time.Duration;

File: langchain4j-anthropic/src/main/java/dev/langchain4j/model/anthropic/internal/client/AnthropicClientBuilderFactory.java
Patch:
@@ -1,4 +1,4 @@
-package dev.langchain4j.model.anthropic;
+package dev.langchain4j.model.anthropic.internal.client;
 
 import java.util.function.Supplier;
 

File: langchain4j-anthropic/src/main/java/dev/langchain4j/model/anthropic/internal/client/AnthropicHttpException.java
Patch:
@@ -1,4 +1,4 @@
-package dev.langchain4j.model.anthropic;
+package dev.langchain4j.model.anthropic.internal.client;
 
 public class AnthropicHttpException extends RuntimeException {
 

File: langchain4j-anthropic/src/main/java/dev/langchain4j/model/anthropic/internal/client/AnthropicRequestLoggingInterceptor.java
Patch:
@@ -1,4 +1,4 @@
-package dev.langchain4j.model.anthropic;
+package dev.langchain4j.model.anthropic.internal.client;
 
 import lombok.extern.slf4j.Slf4j;
 import okhttp3.Headers;

File: langchain4j-anthropic/src/main/java/dev/langchain4j/model/anthropic/internal/client/AnthropicResponseLoggingInterceptor.java
Patch:
@@ -1,4 +1,4 @@
-package dev.langchain4j.model.anthropic;
+package dev.langchain4j.model.anthropic.internal.client;
 
 import lombok.extern.slf4j.Slf4j;
 import okhttp3.Interceptor;
@@ -7,7 +7,7 @@
 
 import java.io.IOException;
 
-import static dev.langchain4j.model.anthropic.AnthropicRequestLoggingInterceptor.getHeaders;
+import static dev.langchain4j.model.anthropic.internal.client.AnthropicRequestLoggingInterceptor.getHeaders;
 
 @Slf4j
 class AnthropicResponseLoggingInterceptor implements Interceptor {

File: langchain4j-anthropic/src/test/java/dev/langchain4j/model/anthropic/AnthropicChatModelIT.java
Patch:
@@ -3,6 +3,7 @@
 import dev.langchain4j.agent.tool.ToolExecutionRequest;
 import dev.langchain4j.agent.tool.ToolSpecification;
 import dev.langchain4j.data.message.*;
+import dev.langchain4j.model.anthropic.internal.client.AnthropicHttpException;
 import dev.langchain4j.model.chat.ChatLanguageModel;
 import dev.langchain4j.model.output.Response;
 import dev.langchain4j.model.output.TokenUsage;
@@ -24,7 +25,6 @@
 import static dev.langchain4j.internal.Utils.readBytes;
 import static dev.langchain4j.model.anthropic.AnthropicChatModelName.CLAUDE_3_SONNET_20240229;
 import static dev.langchain4j.model.output.FinishReason.*;
-import static java.lang.System.getenv;
 import static java.util.Arrays.asList;
 import static java.util.Arrays.stream;
 import static java.util.Collections.singletonList;
@@ -73,8 +73,6 @@ void afterEach() throws InterruptedException {
     void should_generate_answer_and_return_token_usage_and_finish_reason_stop() {
 
         // given
-        ChatLanguageModel model = AnthropicChatModel.withApiKey(getenv("ANTHROPIC_API_KEY"));
-
         UserMessage userMessage = userMessage("What is the capital of Germany?");
 
         // when

File: langchain4j-anthropic/src/test/java/dev/langchain4j/model/anthropic/AnthropicStreamingChatModelIT.java
Patch:
@@ -3,6 +3,7 @@
 import dev.langchain4j.data.message.AiMessage;
 import dev.langchain4j.data.message.ImageContent;
 import dev.langchain4j.data.message.UserMessage;
+import dev.langchain4j.model.anthropic.internal.client.AnthropicHttpException;
 import dev.langchain4j.model.chat.StreamingChatLanguageModel;
 import dev.langchain4j.model.chat.TestStreamingResponseHandler;
 import dev.langchain4j.model.output.Response;

File: langchain4j-core/src/main/java/dev/langchain4j/internal/RetryUtils.java
Patch:
@@ -191,7 +191,7 @@ public <T> T withRetry(Callable<T> action, int maxAttempts) {
                 try {
                     return action.call();
                 } catch (Exception e) {
-                    if (attempt == maxAttempts) {
+                    if (attempt >= maxAttempts) {
                         throw new RuntimeException(e);
                     }
 

File: langchain4j-dashscope/src/main/java/dev/langchain4j/model/dashscope/QwenChatModel.java
Patch:
@@ -20,7 +20,7 @@
 
 import java.util.List;
 
-import static com.alibaba.dashscope.aigc.generation.models.QwenParam.ResultFormat.MESSAGE;
+import static com.alibaba.dashscope.aigc.conversation.ConversationParam.ResultFormat.MESSAGE;
 import static dev.langchain4j.model.dashscope.QwenHelper.*;
 import static dev.langchain4j.spi.ServiceHelper.loadFactories;
 

File: langchain4j-dashscope/src/main/java/dev/langchain4j/model/dashscope/QwenLanguageModel.java
Patch:
@@ -14,7 +14,7 @@
 
 import java.util.List;
 
-import static com.alibaba.dashscope.aigc.generation.models.QwenParam.ResultFormat.MESSAGE;
+import static com.alibaba.dashscope.aigc.generation.GenerationParam.ResultFormat.MESSAGE;
 import static dev.langchain4j.internal.Utils.isNullOrBlank;
 import static dev.langchain4j.model.dashscope.QwenHelper.*;
 import static dev.langchain4j.model.dashscope.QwenModelName.QWEN_PLUS;

File: langchain4j-dashscope/src/main/java/dev/langchain4j/model/dashscope/QwenStreamingChatModel.java
Patch:
@@ -21,7 +21,7 @@
 
 import java.util.List;
 
-import static com.alibaba.dashscope.aigc.generation.models.QwenParam.ResultFormat.MESSAGE;
+import static com.alibaba.dashscope.aigc.conversation.ConversationParam.ResultFormat.MESSAGE;
 import static dev.langchain4j.model.dashscope.QwenHelper.toQwenMessages;
 import static dev.langchain4j.model.dashscope.QwenHelper.toQwenMultiModalMessages;
 import static dev.langchain4j.spi.ServiceHelper.loadFactories;

File: langchain4j-dashscope/src/main/java/dev/langchain4j/model/dashscope/QwenStreamingLanguageModel.java
Patch:
@@ -17,7 +17,7 @@
 
 import java.util.List;
 
-import static com.alibaba.dashscope.aigc.generation.models.QwenParam.ResultFormat.MESSAGE;
+import static com.alibaba.dashscope.aigc.generation.GenerationParam.ResultFormat.MESSAGE;
 import static dev.langchain4j.internal.Utils.isNullOrBlank;
 import static dev.langchain4j.model.dashscope.QwenModelName.QWEN_PLUS;
 import static dev.langchain4j.spi.ServiceHelper.loadFactories;

File: langchain4j-zhipu-ai/src/test/java/dev/langchain4j/model/zhipu/ZhipuAiEmbeddingModelIT.java
Patch:
@@ -57,13 +57,13 @@ void should_embed_in_batches() {
         Response<List<Embedding>> response = model.embedAll(segments);
         System.out.println(response);
 
-        assertThat(response.content()).hasSize(1);
+        assertThat(response.content()).hasSize(11);
         assertThat(response.content().get(0).dimension()).isEqualTo(1024);
 
         TokenUsage tokenUsage = response.tokenUsage();
-        assertThat(tokenUsage.inputTokenCount()).isEqualTo(3);
+        assertThat(tokenUsage.inputTokenCount()).isEqualTo(33);
         assertThat(tokenUsage.outputTokenCount()).isEqualTo(0);
-        assertThat(tokenUsage.totalTokenCount()).isEqualTo(3);
+        assertThat(tokenUsage.totalTokenCount()).isEqualTo(33);
 
         assertThat(response.finishReason()).isNull();
     }

File: langchain4j-bedrock/src/main/java/dev/langchain4j/model/bedrock/BedrockCohereChatModelResponse.java
Patch:
@@ -44,7 +44,7 @@ public String getOutputText() {
     @Override
     public FinishReason getFinishReason() {
         final String finishReason = generations.get(0).finish_reason;
-        if (finishReason != null && finishReason.equals("COMPLETE")) {
+        if ("COMPLETE".equals(finishReason)) {
             return FinishReason.STOP;
         }
 

File: langchain4j-open-ai/src/test/java/dev/langchain4j/model/openai/OpenAiStreamingChatModelIT.java
Patch:
@@ -355,7 +355,7 @@ public void onError(Throwable error) {
 
         TokenUsage tokenUsage = response.tokenUsage();
         assertThat(tokenUsage.inputTokenCount()).isCloseTo(57, tokenizerPrecision);
-        assertThat(tokenUsage.outputTokenCount()).isCloseTo(51, tokenizerPrecision);
+        assertThat(tokenUsage.outputTokenCount()).isCloseTo(34, tokenizerPrecision);
         assertThat(tokenUsage.totalTokenCount())
                 .isEqualTo(tokenUsage.inputTokenCount() + tokenUsage.outputTokenCount());
 
@@ -397,7 +397,7 @@ public void onError(Throwable error) {
         assertThat(secondAiMessage.toolExecutionRequests()).isNull();
 
         TokenUsage secondTokenUsage = secondResponse.tokenUsage();
-        assertThat(secondTokenUsage.inputTokenCount()).isCloseTo(83, tokenizerPrecision);
+        assertThat(secondTokenUsage.inputTokenCount()).isCloseTo(88, tokenizerPrecision);
         assertThat(secondTokenUsage.outputTokenCount()).isGreaterThan(0);
         assertThat(secondTokenUsage.totalTokenCount())
                 .isEqualTo(secondTokenUsage.inputTokenCount() + secondTokenUsage.outputTokenCount());

File: langchain4j/src/test/java/dev/langchain4j/service/StreamingAiServicesWithToolsIT.java
Patch:
@@ -17,14 +17,14 @@
 import java.util.List;
 import java.util.concurrent.CompletableFuture;
 import java.util.concurrent.TimeUnit;
-import java.util.stream.Collectors;
 import java.util.stream.Stream;
 
 import static dev.langchain4j.agent.tool.JsonSchemaProperty.description;
 import static dev.langchain4j.agent.tool.JsonSchemaProperty.*;
 import static dev.langchain4j.service.StreamingAiServicesWithToolsIT.TransactionService.EXPECTED_SPECIFICATION;
 import static java.util.Arrays.asList;
 import static java.util.Collections.singletonList;
+import static java.util.stream.Collectors.toList;
 import static org.assertj.core.api.Assertions.assertThat;
 import static org.mockito.Mockito.*;
 
@@ -76,7 +76,7 @@ List<Double> getTransactionAmounts(@P("IDs of transactions") List<String> ids) {
                     default:
                         throw new IllegalArgumentException("Unknown transaction ID: " + id);
                 }
-            }).collect(Collectors.toList());
+            }).collect(toList());
         }
     }
 

File: langchain4j-core/src/main/java/dev/langchain4j/data/message/GsonChatMessageAdapter.java
Patch:
@@ -22,7 +22,7 @@ public JsonElement serialize(ChatMessage chatMessage, Type ignored, JsonSerializ
     }
 
     @Override
-    public ChatMessage deserialize(JsonElement messageJsonElement, Type ignored, JsonDeserializationContext context) throws JsonParseException {
+    public ChatMessage deserialize(JsonElement messageJsonElement, Type ignored, JsonDeserializationContext context) {
         String chatMessageTypeString = messageJsonElement.getAsJsonObject().get(CHAT_MESSAGE_TYPE).getAsString();
         ChatMessageType chatMessageType = ChatMessageType.valueOf(chatMessageTypeString);
         ChatMessage chatMessage = GSON.fromJson(messageJsonElement, chatMessageType.messageClass());

File: langchain4j-core/src/main/java/dev/langchain4j/data/message/GsonContentAdapter.java
Patch:
@@ -18,7 +18,7 @@ public JsonElement serialize(Content content, Type ignored, JsonSerializationCon
     }
 
     @Override
-    public Content deserialize(JsonElement contentJsonElement, Type ignored, JsonDeserializationContext context) throws JsonParseException {
+    public Content deserialize(JsonElement contentJsonElement, Type ignored, JsonDeserializationContext context) {
         String contentTypeString = contentJsonElement.getAsJsonObject().get(CONTENT_TYPE).getAsString();
         ContentType contentType = ContentType.valueOf(contentTypeString);
         return GSON.fromJson(contentJsonElement, contentType.getContentClass());

File: langchain4j-cassandra/src/main/java/dev/langchain4j/store/memory/chat/cassandra/CassandraChatMemoryStore.java
Patch:
@@ -141,8 +141,7 @@ private ChatMessage toChatMessage(@NonNull ClusteredRecord record) {
         try {
             return ChatMessageDeserializer.messageFromJson(record.getBody());
         } catch (Exception e) {
-            log.error("Unable to parse message body", e);
-            throw new IllegalArgumentException("Unable to parse message body");
+            throw new IllegalArgumentException("Unable to parse message body", e);
         }
     }
 
@@ -161,7 +160,6 @@ private ClusteredRecord fromChatMessage(@NonNull String memoryId, @NonNull ChatM
             record.setBody(ChatMessageSerializer.messageToJson(chatMessage));
             return record;
         } catch (Exception e) {
-            log.error("Unable to parse message body", e);
             throw new IllegalArgumentException("Unable to parse message body", e);
         }
     }

File: langchain4j/src/main/java/dev/langchain4j/service/DefaultAiServices.java
Patch:
@@ -228,7 +228,7 @@ private UserMessage prepareUserMessage(Object memoryId, Method method, Object[]
 
         return prepareUserMessageTemplate(memoryId, method)
                 .map(template -> prepareUserMessageFromTemplate(args, template, parameters, getPromptTemplateVariables(args, parameters), userName))
-                .orElse(prepareUserMessage(args, parameters, userName));
+                .orElseGet(() -> prepareUserMessage(args, parameters, userName));
     }
 
     private UserMessage prepareUserMessageFromTemplate(Object[] args, String userMessageTemplate, Parameter[] parameters, Map<String, Object> variables, String userName) {

File: langchain4j/src/main/java/dev/langchain4j/service/ServiceOutputParser.java
Patch:
@@ -130,6 +130,7 @@ private static String jsonStructure(Class<?> structured, Set<Class<?>> visited)
             }
             jsonSchema.append(format("\"%s\": (%s),\n", name, descriptionFor(field, visited)));
         }
+        jsonSchema.delete(jsonSchema.lastIndexOf(","), jsonSchema.lastIndexOf(",")+1);
         jsonSchema.append("}");
         return jsonSchema.toString();
     }

File: langchain4j-zhipu-ai/src/main/java/dev/langchain4j/model/zhipu/ZhipuAiClient.java
Patch:
@@ -230,7 +230,7 @@ public static class Builder {
         private boolean logResponses;
 
         private Builder() {
-            this.baseUrl = "https://aip.baidubce.com/";
+            this.baseUrl = "https://open.bigmodel.cn/";
             this.callTimeout = Duration.ofSeconds(60L);
             this.connectTimeout = Duration.ofSeconds(60L);
             this.readTimeout = Duration.ofSeconds(60L);

File: langchain4j-open-ai/src/main/java/dev/langchain4j/model/openai/OpenAiChatModel.java
Patch:
@@ -101,7 +101,7 @@ public OpenAiChatModel(String baseUrl,
         this.seed = seed;
         this.user = user;
         this.maxRetries = getOrDefault(maxRetries, 3);
-        this.tokenizer = getOrDefault(tokenizer, () -> new OpenAiTokenizer(this.modelName));
+        this.tokenizer = getOrDefault(tokenizer, OpenAiTokenizer::new);
     }
 
     public String modelName() {

File: langchain4j-open-ai/src/main/java/dev/langchain4j/model/openai/OpenAiEmbeddingModel.java
Patch:
@@ -74,7 +74,7 @@ public OpenAiEmbeddingModel(String baseUrl,
         this.dimensions = dimensions;
         this.user = user;
         this.maxRetries = getOrDefault(maxRetries, 3);
-        this.tokenizer = getOrDefault(tokenizer, () -> new OpenAiTokenizer(this.modelName));
+        this.tokenizer = getOrDefault(tokenizer, OpenAiTokenizer::new);
     }
 
     public String modelName() {

File: langchain4j-open-ai/src/main/java/dev/langchain4j/model/openai/OpenAiImageModel.java
Patch:
@@ -56,7 +56,7 @@ public class OpenAiImageModel implements ImageModel {
     @SuppressWarnings("rawtypes")
     public OpenAiImageModel(
             String baseUrl,
-            @NonNull String apiKey,
+            String apiKey,
             String organizationId,
             String modelName,
             String size,

File: langchain4j-open-ai/src/main/java/dev/langchain4j/model/openai/OpenAiLanguageModel.java
Patch:
@@ -65,7 +65,7 @@ public OpenAiLanguageModel(String baseUrl,
         this.modelName = getOrDefault(modelName, GPT_3_5_TURBO_INSTRUCT);
         this.temperature = getOrDefault(temperature, 0.7);
         this.maxRetries = getOrDefault(maxRetries, 3);
-        this.tokenizer = getOrDefault(tokenizer, () -> new OpenAiTokenizer(this.modelName));
+        this.tokenizer = getOrDefault(tokenizer, OpenAiTokenizer::new);
     }
 
     public String modelName() {

File: langchain4j-open-ai/src/main/java/dev/langchain4j/model/openai/OpenAiStreamingLanguageModel.java
Patch:
@@ -63,7 +63,7 @@ public OpenAiStreamingLanguageModel(String baseUrl,
                 .build();
         this.modelName = getOrDefault(modelName, GPT_3_5_TURBO_INSTRUCT);
         this.temperature = getOrDefault(temperature, 0.7);
-        this.tokenizer = getOrDefault(tokenizer, () -> new OpenAiTokenizer(this.modelName));
+        this.tokenizer = getOrDefault(tokenizer, OpenAiTokenizer::new);
     }
 
     public String modelName() {

File: langchain4j/src/main/java/dev/langchain4j/data/document/splitter/DocumentByParagraphSplitter.java
Patch:
@@ -54,7 +54,7 @@ public DocumentByParagraphSplitter(int maxSegmentSizeInTokens,
 
     @Override
     public String[] split(String text) {
-        return text.split("\\s*\\R\\s*\\R\\s*"); // additional whitespaces are ignored
+        return text.split("\\s*(?>\\R)\\s*(?>\\R)\\s*"); // additional whitespaces are ignored
     }
 
     @Override

File: langchain4j/src/test/java/dev/langchain4j/service/AiServicesWithToolsIT.java
Patch:
@@ -52,7 +52,7 @@ static Stream<ChatLanguageModel> chatLanguageModelProvider() {
                         .build(),
                 MistralAiChatModel.builder()
                         .apiKey(System.getenv("MISTRAL_AI_API_KEY"))
-                        .modelName(MistralAiChatModelName.MISTRAL_LARGE_LATEST)
+                        .modelName("mistral-large-latest")
                         .logRequests(true)
                         .logResponses(true)
                         .build()

File: langchain4j-bedrock/src/main/java/dev/langchain4j/model/bedrock/BedrockAI21LabsChatModel.java
Patch:
@@ -13,7 +13,7 @@
 public class BedrockAI21LabsChatModel extends AbstractBedrockChatModel<BedrockAI21LabsChatModelResponse> {
 
     @Builder.Default
-    private final Types model = Types.J2MidV2;
+    private final String model = Types.J2MidV2.getValue();
     @Builder.Default
     private final Map<String, Object> countPenalty = of("scale", 0);
     @Builder.Default
@@ -40,7 +40,7 @@ protected Map<String, Object> getRequestParameters(String prompt) {
 
     @Override
     protected String getModelId() {
-        return model.getValue();
+        return model;
     }
 
     @Override

File: langchain4j-bedrock/src/main/java/dev/langchain4j/model/bedrock/BedrockCohereChatModel.java
Patch:
@@ -23,7 +23,7 @@ public enum ReturnLikelihood {
     @Builder.Default
     private final int topK = 0;
     @Builder.Default
-    private final Types model = Types.CommandTextV14;
+    private final String model = Types.CommandTextV14.getValue();
 
     @Override
     protected Map<String, Object> getRequestParameters(String prompt) {
@@ -42,7 +42,7 @@ protected Map<String, Object> getRequestParameters(String prompt) {
 
     @Override
     protected String getModelId() {
-        return model.getValue();
+        return model;
     }
 
     @Override

File: langchain4j-bedrock/src/main/java/dev/langchain4j/model/bedrock/BedrockTitanChatModel.java
Patch:
@@ -16,7 +16,7 @@
 public class BedrockTitanChatModel extends AbstractBedrockChatModel<BedrockTitanChatModelResponse> {
 
     @Builder.Default
-    private final Types model = Types.TitanTextExpressV1;
+    private final String model = Types.TitanTextExpressV1.getValue();
 
     @Override
     protected Map<String, Object> getRequestParameters(String prompt) {
@@ -36,7 +36,7 @@ protected Map<String, Object> getRequestParameters(String prompt) {
 
     @Override
     protected String getModelId() {
-        return model.getValue();
+        return model;
     }
 
     @Override

File: langchain4j-bedrock/src/main/java/dev/langchain4j/model/bedrock/BedrockTitanEmbeddingModel.java
Patch:
@@ -19,11 +19,11 @@ public class BedrockTitanEmbeddingModel extends AbstractBedrockEmbeddingModel<Be
     private final static String MODEL_ID = "amazon.titan-embed-text-v1";
 
     @Builder.Default
-    private final Types model = Types.TitanEmbedTextV1;
+    private final String model = Types.TitanEmbedTextV1.getValue();
 
     @Override
     protected String getModelId() {
-        return model.getValue();
+        return model;
     }
 
     @Override

File: langchain4j-bedrock/src/test/java/dev/langchain4j/model/bedrock/BedrockEmbeddingIT.java
Patch:
@@ -23,7 +23,7 @@ void testBedrockTitanChatModel() {
                 .builder()
                 .region(Region.US_EAST_1)
                 .maxRetries(1)
-                .model(BedrockTitanEmbeddingModel.Types.TitanEmbedTextV1)
+                .model(BedrockTitanEmbeddingModel.Types.TitanEmbedTextV1.getValue())
                 .build();
 
         assertThat(embeddingModel).isNotNull();

File: langchain4j-ollama/src/test/java/dev/langchain4j/model/ollama/AbstractOllamaVisionModelInfrastructure.java
Patch:
@@ -1,6 +1,6 @@
 package dev.langchain4j.model.ollama;
 
-public class AbstractOllamaInfrastructureVisionModel {
+class AbstractOllamaVisionModelInfrastructure {
 
     private static final String LOCAL_OLLAMA_IMAGE = String.format("tc-%s-%s", OllamaImage.OLLAMA_IMAGE, OllamaImage.BAKLLAVA_MODEL);
 
@@ -12,5 +12,4 @@ public class AbstractOllamaInfrastructureVisionModel {
         ollama.start();
         ollama.commitToImage(LOCAL_OLLAMA_IMAGE);
     }
-
 }

File: langchain4j-ollama/src/test/java/dev/langchain4j/model/ollama/OllamaEmbeddingModelIT.java
Patch:
@@ -5,13 +5,14 @@
 import dev.langchain4j.model.output.Response;
 import org.junit.jupiter.api.Test;
 
+import static dev.langchain4j.model.ollama.OllamaImage.ALL_MINILM_MODEL;
 import static org.assertj.core.api.Assertions.assertThat;
 
-class OllamaEmbeddingModelIT extends AbstractOllamaInfrastructure {
+class OllamaEmbeddingModelIT extends AbstractOllamaEmbeddingModelInfrastructure {
 
     EmbeddingModel model = OllamaEmbeddingModel.builder()
             .baseUrl(ollama.getEndpoint())
-            .modelName(OllamaImage.PHI_MODEL)
+            .modelName(ALL_MINILM_MODEL)
             .build();
 
     @Test

File: langchain4j-ollama/src/test/java/dev/langchain4j/model/ollama/OllamaImage.java
Patch:
@@ -13,7 +13,9 @@ public class OllamaImage {
 
     static final String BAKLLAVA_MODEL = "bakllava";
 
-    static final String PHI_MODEL = "phi";
+    static final String TINY_DOLPHIN_MODEL = "tinydolphin";
+
+    static final String ALL_MINILM_MODEL = "all-minilm";
 
     static DockerImageName resolve(String baseImage, String localImageName) {
         DockerImageName dockerImageName = DockerImageName.parse(baseImage);
@@ -24,5 +26,4 @@ static DockerImageName resolve(String baseImage, String localImageName) {
         }
         return DockerImageName.parse(localImageName).asCompatibleSubstituteFor(baseImage);
     }
-
 }

File: langchain4j-core/src/test/java/dev/langchain4j/rag/DefaultRetrievalAugmentorTest.java
Patch:
@@ -70,13 +70,13 @@ void should_augment_user_message(Executor executor) {
         UserMessage augmented = retrievalAugmentor.augment(userMessage, metadata);
 
         // then
-        assertThat(augmented.text()).isEqualTo(
+        assertThat(augmented.singleText()).isEqualTo(
                 "query\n" +
                         "content 1\n" +
                         "content 2\n" +
                         "content 3\n" +
                         "content 4\n" +
-                        "content 1\n" +
+                        "content 1\n" + // contents are repeating because TestContentAggregator does not perform RRF
                         "content 2\n" +
                         "content 3\n" +
                         "content 4"

File: langchain4j-azure-ai-search/src/test/java/dev/langchain4j/rag/content/retriever/azure/search/AzureAiSearchContentRetrieverIT.java
Patch:
@@ -18,9 +18,9 @@
 import static org.assertj.core.api.Assertions.assertThat;
 
 @EnabledIfEnvironmentVariable(named = "AZURE_SEARCH_ENDPOINT", matches = ".+")
-public class AzureAiSearchContentRetrieverTestIT extends EmbeddingStoreIT {
+public class AzureAiSearchContentRetrieverIT extends EmbeddingStoreIT {
 
-    private static final Logger log = LoggerFactory.getLogger(AzureAiSearchContentRetrieverTestIT.class);
+    private static final Logger log = LoggerFactory.getLogger(AzureAiSearchContentRetrieverIT.class);
 
     private final EmbeddingModel embeddingModel;
 
@@ -34,7 +34,7 @@ public class AzureAiSearchContentRetrieverTestIT extends EmbeddingStoreIT {
 
     private final int dimensions;
 
-    public AzureAiSearchContentRetrieverTestIT() {
+    public AzureAiSearchContentRetrieverIT() {
 
         embeddingModel = new AllMiniLmL6V2QuantizedEmbeddingModel();
         dimensions = embeddingModel.embed("test").content().vector().length;

File: langchain4j-azure-ai-search/src/main/java/dev/langchain4j/store/embedding/azure/search/Document.java
Patch:
@@ -4,7 +4,7 @@
 
 import java.util.Collection;
 
-class Document {
+public class Document {
 
     private String id;
 

File: langchain4j-azure-ai-search/src/test/java/dev/langchain4j/store/embedding/azure/search/AzureAiSearchEmbeddingStoreIT.java
Patch:
@@ -14,7 +14,6 @@
 
 import static java.util.Arrays.asList;
 import static org.assertj.core.api.Assertions.assertThat;
-import static org.assertj.core.data.Percentage.withPercentage;
 
 @EnabledIfEnvironmentVariable(named = "AZURE_SEARCH_ENDPOINT", matches = ".+")
 public class AzureAiSearchEmbeddingStoreIT extends EmbeddingStoreIT {

File: langchain4j-mistral-ai/src/main/java/dev/langchain4j/model/mistralai/DefaultMistralAiHelper.java
Patch:
@@ -15,7 +15,7 @@
 import static dev.langchain4j.model.output.FinishReason.STOP;
 import static java.util.stream.Collectors.toList;
 
-class DefaultMistralAiHelper {
+public class DefaultMistralAiHelper {
 
     static final String MISTRALAI_API_URL = "https://api.mistral.ai/v1";
     static final String MISTRALAI_API_CREATE_EMBEDDINGS_ENCODING_FORMAT = "float";
@@ -64,7 +64,7 @@ private static String toMistralChatMessageContent(ChatMessage message) {
         throw new IllegalArgumentException("Unknown message type: " + message.type());
     }
 
-    static TokenUsage tokenUsageFrom(MistralAiUsage mistralAiUsage) {
+    public static TokenUsage tokenUsageFrom(MistralAiUsage mistralAiUsage) {
         if (mistralAiUsage == null) {
             return null;
         }
@@ -75,7 +75,7 @@ static TokenUsage tokenUsageFrom(MistralAiUsage mistralAiUsage) {
         );
     }
 
-    static FinishReason finishReasonFrom(String mistralAiFinishReason) {
+    public static FinishReason finishReasonFrom(String mistralAiFinishReason) {
         if (mistralAiFinishReason == null) {
             return null;
         }

File: langchain4j-mistral-ai/src/main/java/dev/langchain4j/model/mistralai/MistralAiChatCompletionChoice.java
Patch:
@@ -9,7 +9,7 @@
 @NoArgsConstructor
 @AllArgsConstructor
 @Builder
-class MistralAiChatCompletionChoice {
+public class MistralAiChatCompletionChoice {
 
     private Integer index;
     private MistralAiChatMessage message;

File: langchain4j-mistral-ai/src/main/java/dev/langchain4j/model/mistralai/MistralAiChatCompletionRequest.java
Patch:
@@ -11,7 +11,7 @@
 @NoArgsConstructor
 @AllArgsConstructor
 @Builder
-class MistralAiChatCompletionRequest {
+public class MistralAiChatCompletionRequest {
 
     private String model;
     private List<MistralAiChatMessage> messages;

File: langchain4j-mistral-ai/src/main/java/dev/langchain4j/model/mistralai/MistralAiChatCompletionResponse.java
Patch:
@@ -11,7 +11,7 @@
 @NoArgsConstructor
 @AllArgsConstructor
 @Builder
-class MistralAiChatCompletionResponse {
+public class MistralAiChatCompletionResponse {
 
     private String id;
     private String object;

File: langchain4j-mistral-ai/src/main/java/dev/langchain4j/model/mistralai/MistralAiChatMessage.java
Patch:
@@ -9,7 +9,7 @@
 @NoArgsConstructor
 @AllArgsConstructor
 @Builder
-class MistralAiChatMessage {
+public class MistralAiChatMessage {
 
     private MistralAiRole role;
     private String content;

File: langchain4j-mistral-ai/src/main/java/dev/langchain4j/model/mistralai/MistralAiDeltaMessage.java
Patch:
@@ -9,7 +9,7 @@
 @NoArgsConstructor
 @AllArgsConstructor
 @Builder
-class MistralAiDeltaMessage {
+public class MistralAiDeltaMessage {
 
     private MistralAiRole role;
     private String content;

File: langchain4j-mistral-ai/src/main/java/dev/langchain4j/model/mistralai/MistralAiEmbeddingRequest.java
Patch:
@@ -11,7 +11,7 @@
 @NoArgsConstructor
 @AllArgsConstructor
 @Builder
-class MistralAiEmbeddingRequest {
+public class MistralAiEmbeddingRequest {
 
     private String model;
     private List<String> input;

File: langchain4j-mistral-ai/src/main/java/dev/langchain4j/model/mistralai/MistralAiEmbeddingResponse.java
Patch:
@@ -11,7 +11,7 @@
 @NoArgsConstructor
 @AllArgsConstructor
 @Builder
-class MistralAiEmbeddingResponse {
+public class MistralAiEmbeddingResponse {
 
     private String id;
     private String object;

File: langchain4j-mistral-ai/src/main/java/dev/langchain4j/model/mistralai/MistralAiModelResponse.java
Patch:
@@ -11,7 +11,7 @@
 @NoArgsConstructor
 @AllArgsConstructor
 @Builder
-class MistralAiModelResponse {
+public class MistralAiModelResponse {
 
     private String object;
     private List<MistralAiModelCard> data;

File: langchain4j-mistral-ai/src/main/java/dev/langchain4j/model/mistralai/MistralAiRole.java
Patch:
@@ -4,7 +4,7 @@
 import lombok.Getter;
 
 @Getter
-enum MistralAiRole {
+public enum MistralAiRole {
 
     @SerializedName("system") SYSTEM,
     @SerializedName("user") USER,

File: langchain4j-mistral-ai/src/main/java/dev/langchain4j/model/mistralai/MistralAiUsage.java
Patch:
@@ -9,7 +9,7 @@
 @NoArgsConstructor
 @AllArgsConstructor
 @Builder
-class MistralAiUsage {
+public class MistralAiUsage {
 
     private Integer promptTokens;
     private Integer totalTokens;

File: langchain4j-open-ai/src/main/java/dev/langchain4j/model/openai/InternalOpenAiHelper.java
Patch:
@@ -33,6 +33,9 @@ public class InternalOpenAiHelper {
     static final String OPENAI_DEMO_API_KEY = "demo";
     static final String OPENAI_DEMO_URL = "http://langchain4j.dev/demo/openai/v1";
 
+
+    static final String DEFAULT_USER_AGENT = "langchain4j-openai";
+
     public static List<Message> toOpenAiMessages(List<ChatMessage> messages) {
         return messages.stream()
                 .map(InternalOpenAiHelper::toOpenAiMessage)

File: langchain4j-open-ai/src/main/java/dev/langchain4j/model/openai/OpenAiChatModel.java
Patch:
@@ -87,6 +87,7 @@ public OpenAiChatModel(String baseUrl,
                 .proxy(proxy)
                 .logRequests(logRequests)
                 .logResponses(logResponses)
+                .userAgent(DEFAULT_USER_AGENT)
                 .build();
         this.modelName = getOrDefault(modelName, GPT_3_5_TURBO);
         this.temperature = getOrDefault(temperature, 0.7);

File: langchain4j-open-ai/src/main/java/dev/langchain4j/model/openai/OpenAiEmbeddingModel.java
Patch:
@@ -68,6 +68,7 @@ public OpenAiEmbeddingModel(String baseUrl,
                 .proxy(proxy)
                 .logRequests(logRequests)
                 .logResponses(logResponses)
+                .userAgent(DEFAULT_USER_AGENT)
                 .build();
         this.modelName = getOrDefault(modelName, TEXT_EMBEDDING_ADA_002);
         this.dimensions = dimensions;

File: langchain4j-open-ai/src/main/java/dev/langchain4j/model/openai/OpenAiImageModel.java
Patch:
@@ -18,6 +18,7 @@
 
 import static dev.langchain4j.internal.RetryUtils.withRetry;
 import static dev.langchain4j.internal.Utils.getOrDefault;
+import static dev.langchain4j.model.openai.InternalOpenAiHelper.DEFAULT_USER_AGENT;
 import static dev.langchain4j.model.openai.InternalOpenAiHelper.OPENAI_URL;
 import static dev.langchain4j.model.openai.OpenAiModelName.DALL_E_2;
 import static dev.langchain4j.spi.ServiceHelper.loadFactories;
@@ -85,6 +86,7 @@ public OpenAiImageModel(
                 .proxy(proxy)
                 .logRequests(getOrDefault(logRequests, false))
                 .logResponses(getOrDefault(logResponses, false))
+                .userAgent(DEFAULT_USER_AGENT)
                 .persistTo(persistTo);
 
         if (withPersisting != null && withPersisting) {

File: langchain4j-open-ai/src/main/java/dev/langchain4j/model/openai/OpenAiLanguageModel.java
Patch:
@@ -60,6 +60,7 @@ public OpenAiLanguageModel(String baseUrl,
                 .proxy(proxy)
                 .logRequests(logRequests)
                 .logResponses(logResponses)
+                .userAgent(DEFAULT_USER_AGENT)
                 .build();
         this.modelName = getOrDefault(modelName, GPT_3_5_TURBO_INSTRUCT);
         this.temperature = getOrDefault(temperature, 0.7);

File: langchain4j-open-ai/src/main/java/dev/langchain4j/model/openai/OpenAiModerationModel.java
Patch:
@@ -62,6 +62,7 @@ public OpenAiModerationModel(String baseUrl,
                 .proxy(proxy)
                 .logRequests(logRequests)
                 .logResponses(logResponses)
+                .userAgent(DEFAULT_USER_AGENT)
                 .build();
         this.modelName = getOrDefault(modelName, TEXT_MODERATION_LATEST);
         this.maxRetries = getOrDefault(maxRetries, 3);

File: langchain4j-open-ai/src/main/java/dev/langchain4j/model/openai/OpenAiStreamingChatModel.java
Patch:
@@ -84,6 +84,7 @@ public OpenAiStreamingChatModel(String baseUrl,
                 .proxy(proxy)
                 .logRequests(logRequests)
                 .logStreamingResponses(logResponses)
+                .userAgent(DEFAULT_USER_AGENT)
                 .build();
         this.modelName = getOrDefault(modelName, GPT_3_5_TURBO);
         this.temperature = getOrDefault(temperature, 0.7);

File: langchain4j-open-ai/src/main/java/dev/langchain4j/model/openai/OpenAiStreamingLanguageModel.java
Patch:
@@ -15,6 +15,7 @@
 import java.time.Duration;
 
 import static dev.langchain4j.internal.Utils.getOrDefault;
+import static dev.langchain4j.model.openai.InternalOpenAiHelper.DEFAULT_USER_AGENT;
 import static dev.langchain4j.model.openai.InternalOpenAiHelper.OPENAI_URL;
 import static dev.langchain4j.model.openai.OpenAiModelName.GPT_3_5_TURBO_INSTRUCT;
 import static dev.langchain4j.spi.ServiceHelper.loadFactories;
@@ -58,6 +59,7 @@ public OpenAiStreamingLanguageModel(String baseUrl,
                 .proxy(proxy)
                 .logRequests(logRequests)
                 .logStreamingResponses(logResponses)
+                .userAgent(DEFAULT_USER_AGENT)
                 .build();
         this.modelName = getOrDefault(modelName, GPT_3_5_TURBO_INSTRUCT);
         this.temperature = getOrDefault(temperature, 0.7);

File: langchain4j-core/src/main/java/dev/langchain4j/data/image/Image.java
Patch:
@@ -6,7 +6,7 @@
 import static dev.langchain4j.internal.Utils.quoted;
 
 /**
- * Represents an image.
+ * Represents an image as a URL or as a Base64-encoded string.
  */
 public final class Image {
 

File: langchain4j/src/test/java/dev/langchain4j/service/AiServicesWithRagIT.java
Patch:
@@ -124,7 +124,7 @@ void should_use_content_retriever_and_chat_memory(ChatLanguageModel model) {
                 .chatMemory(chatMemory)
                 .build();
 
-        String query = "Can I cancel my booking?";
+        String query = "In which cases can I cancel my booking?";
 
         // when
         String answer = assistant.answer(query);

File: langchain4j-vertex-ai-gemini/src/main/java/dev/langchain4j/model/vertexai/RoleMapper.java
Patch:
@@ -6,6 +6,7 @@ class RoleMapper {
 
     static String map(ChatMessageType type) {
         switch (type) {
+            case TOOL_EXECUTION_RESULT:
             case USER:
                 return "user";
             case AI:

File: langchain4j/src/main/java/dev/langchain4j/service/AiServices.java
Patch:
@@ -319,8 +319,9 @@ public AiServices<T> retriever(Retriever<TextSegment> retriever) {
             throw illegalConfiguration("Only one out of [retriever, contentRetriever, retrievalAugmentor] can be set");
         }
         if (retriever != null) {
+            AiServices<T> withContentRetriever = contentRetriever(retriever.toContentRetriever());
             retrieverSet = true;
-            return contentRetriever(retriever.toContentRetriever());
+            return withContentRetriever;
         }
         return this;
     }

File: langchain4j-milvus/src/main/java/dev/langchain4j/store/embedding/milvus/MilvusEmbeddingStore.java
Patch:
@@ -90,6 +90,8 @@ public MilvusEmbeddingStore(
       createCollection(milvusClient, this.collectionName, ensureNotNull(dimension, "dimension"));
       createIndex(milvusClient, this.collectionName, getOrDefault(indexType, FLAT), this.metricType);
     }
+
+    loadCollectionInMemory(milvusClient, collectionName);
   }
 
   public void dropCollection(String collectionName) {
@@ -126,7 +128,6 @@ public List<String> addAll(List<Embedding> embeddings, List<TextSegment> embedde
 
   @Override
   public EmbeddingSearchResult<TextSegment> search(EmbeddingSearchRequest embeddingSearchRequest) {
-    loadCollectionInMemory(milvusClient, collectionName); // TODO improve
 
     SearchParam searchParam = buildSearchRequest(
             collectionName,

File: langchain4j-core/src/main/java/dev/langchain4j/data/document/Document.java
Patch:
@@ -77,6 +77,7 @@ public Metadata metadata() {
      * @param key the key to look up.
      * @return the metadata value for the given key, or null if the key is not present.
      */
+    // TODO deprecate once the new experimental API is settled
     public String metadata(String key) {
         return metadata.get(key);
     }

File: langchain4j-core/src/main/java/dev/langchain4j/rag/RetrievalAugmentor.java
Patch:
@@ -1,6 +1,6 @@
 package dev.langchain4j.rag;
 
-import dev.langchain4j.MightChangeInTheFuture;
+import dev.langchain4j.Experimental;
 import dev.langchain4j.data.message.UserMessage;
 import dev.langchain4j.rag.query.Metadata;
 
@@ -13,7 +13,7 @@
  *
  * @see DefaultRetrievalAugmentor
  */
-@MightChangeInTheFuture("This is an experimental feature. Time will tell if this is the right abstraction.")
+@Experimental
 public interface RetrievalAugmentor {
 
     /**

File: langchain4j-core/src/main/java/dev/langchain4j/rag/content/aggregator/ContentAggregator.java
Patch:
@@ -1,6 +1,6 @@
 package dev.langchain4j.rag.content.aggregator;
 
-import dev.langchain4j.MightChangeInTheFuture;
+import dev.langchain4j.Experimental;
 import dev.langchain4j.rag.content.Content;
 import dev.langchain4j.rag.content.retriever.ContentRetriever;
 import dev.langchain4j.rag.query.Query;
@@ -23,7 +23,7 @@
  * @see DefaultContentAggregator
  * @see ReRankingContentAggregator
  */
-@MightChangeInTheFuture("This is an experimental feature. Time will tell if this is the right abstraction.")
+@Experimental
 public interface ContentAggregator {
 
     /**

File: langchain4j-core/src/main/java/dev/langchain4j/rag/content/injector/ContentInjector.java
Patch:
@@ -1,6 +1,6 @@
 package dev.langchain4j.rag.content.injector;
 
-import dev.langchain4j.MightChangeInTheFuture;
+import dev.langchain4j.Experimental;
 import dev.langchain4j.data.message.UserMessage;
 import dev.langchain4j.rag.content.Content;
 
@@ -14,7 +14,7 @@
  *
  * @see DefaultContentInjector
  */
-@MightChangeInTheFuture("This is an experimental feature. Time will tell if this is the right abstraction.")
+@Experimental
 public interface ContentInjector {
 
     /**

File: langchain4j-core/src/main/java/dev/langchain4j/rag/query/router/QueryRouter.java
Patch:
@@ -1,6 +1,6 @@
 package dev.langchain4j.rag.query.router;
 
-import dev.langchain4j.MightChangeInTheFuture;
+import dev.langchain4j.Experimental;
 import dev.langchain4j.classification.TextClassifier;
 import dev.langchain4j.rag.content.Content;
 import dev.langchain4j.rag.content.retriever.ContentRetriever;
@@ -22,7 +22,7 @@
  * @see DefaultQueryRouter
  * @see LanguageModelQueryRouter
  */
-@MightChangeInTheFuture("This is an experimental feature. Time will tell if this is the right abstraction.")
+@Experimental
 public interface QueryRouter {
 
     /**

File: langchain4j-core/src/main/java/dev/langchain4j/rag/query/transformer/CompressingQueryTransformer.java
Patch:
@@ -48,8 +48,8 @@ public class CompressingQueryTransformer implements QueryTransformer {
                     "Do not prepend a query with anything!"
     );
 
-    private final PromptTemplate promptTemplate;
-    private final ChatLanguageModel chatLanguageModel;
+    protected final PromptTemplate promptTemplate;
+    protected final ChatLanguageModel chatLanguageModel;
 
     public CompressingQueryTransformer(ChatLanguageModel chatLanguageModel) {
         this(chatLanguageModel, DEFAULT_PROMPT_TEMPLATE);

File: langchain4j-core/src/main/java/dev/langchain4j/rag/query/transformer/QueryTransformer.java
Patch:
@@ -1,6 +1,6 @@
 package dev.langchain4j.rag.query.transformer;
 
-import dev.langchain4j.MightChangeInTheFuture;
+import dev.langchain4j.Experimental;
 import dev.langchain4j.rag.query.Query;
 
 import java.util.Collection;
@@ -24,7 +24,7 @@
  * @see CompressingQueryTransformer
  * @see ExpandingQueryTransformer
  */
-@MightChangeInTheFuture("This is an experimental feature. Time will tell if this is the right abstraction.")
+@Experimental
 public interface QueryTransformer {
 
     /**

File: langchain4j-elasticsearch/src/main/java/dev/langchain4j/store/embedding/elasticsearch/Document.java
Patch:
@@ -15,5 +15,5 @@ class Document {
 
     private float[] vector;
     private String text;
-    private Map<String, String> metadata;
+    private Map<String, Object> metadata;
 }

File: langchain4j-infinispan/src/test/java/dev/langchain4j/store/embedding/infinispan/InfinispanEmbeddingStoreIT.java
Patch:
@@ -39,7 +39,7 @@ protected void clearStore() {
                 .authentication()
                 .username(DEFAULT_USERNAME)
                 .password(DEFAULT_PASSWORD);
-        // jut to avoid docker 4 mac issues, don't use in production
+        // just to avoid docker 4 mac issues, don't use in production
         builder.clientIntelligence(ClientIntelligence.BASIC);
 
         InfinispanEmbeddingStore embeddingStoreInf = InfinispanEmbeddingStore.builder()

File: langchain4j-redis/src/test/java/dev/langchain4j/store/embedding/redis/RedisEmbeddingStoreIT.java
Patch:
@@ -13,7 +13,6 @@
 import static com.redis.testcontainers.RedisStackContainer.DEFAULT_IMAGE_NAME;
 import static com.redis.testcontainers.RedisStackContainer.DEFAULT_TAG;
 import static dev.langchain4j.internal.Utils.randomUUID;
-import static java.util.Collections.singletonList;
 
 class RedisEmbeddingStoreIT extends EmbeddingStoreIT {
 
@@ -44,7 +43,7 @@ protected void clearStore() {
                 .port(redis.getFirstMappedPort())
                 .indexName(randomUUID())
                 .dimension(384)
-                .metadataFieldsName(singletonList("test-key"))
+                .metadataKeys(createMetadata().toMap().keySet())
                 .build();
     }
 

File: langchain4j-ollama/src/main/java/dev/langchain4j/model/ollama/OllamaChatModel.java
Patch:
@@ -41,6 +41,7 @@ public OllamaChatModel(String baseUrl,
                            Double repeatPenalty,
                            Integer seed,
                            Integer numPredict,
+                           Integer numCtx,
                            List<String> stop,
                            String format,
                            Duration timeout,
@@ -57,6 +58,7 @@ public OllamaChatModel(String baseUrl,
                 .repeatPenalty(repeatPenalty)
                 .seed(seed)
                 .numPredict(numPredict)
+                .numCtx(numCtx)
                 .stop(stop)
                 .build();
         this.format = format;

File: langchain4j-ollama/src/main/java/dev/langchain4j/model/ollama/OllamaLanguageModel.java
Patch:
@@ -37,6 +37,7 @@ public OllamaLanguageModel(String baseUrl,
                                Double repeatPenalty,
                                Integer seed,
                                Integer numPredict,
+                               Integer numCtx,
                                List<String> stop,
                                String format,
                                Duration timeout,
@@ -53,6 +54,7 @@ public OllamaLanguageModel(String baseUrl,
                 .repeatPenalty(repeatPenalty)
                 .seed(seed)
                 .numPredict(numPredict)
+                .numCtx(numCtx)
                 .stop(stop)
                 .build();
         this.format = format;

File: langchain4j-ollama/src/main/java/dev/langchain4j/model/ollama/OllamaStreamingChatModel.java
Patch:
@@ -38,6 +38,7 @@ public OllamaStreamingChatModel(String baseUrl,
                                     Double repeatPenalty,
                                     Integer seed,
                                     Integer numPredict,
+                                    Integer numCtx,
                                     List<String> stop,
                                     String format,
                                     Duration timeout) {
@@ -53,6 +54,7 @@ public OllamaStreamingChatModel(String baseUrl,
                 .repeatPenalty(repeatPenalty)
                 .seed(seed)
                 .numPredict(numPredict)
+                .numCtx(numCtx)
                 .stop(stop)
                 .build();
         this.format = format;

File: langchain4j-ollama/src/main/java/dev/langchain4j/model/ollama/OllamaStreamingLanguageModel.java
Patch:
@@ -34,6 +34,7 @@ public OllamaStreamingLanguageModel(String baseUrl,
                                         Double repeatPenalty,
                                         Integer seed,
                                         Integer numPredict,
+                                        Integer numCtx,
                                         List<String> stop,
                                         String format,
                                         Duration timeout) {
@@ -49,6 +50,7 @@ public OllamaStreamingLanguageModel(String baseUrl,
                 .repeatPenalty(repeatPenalty)
                 .seed(seed)
                 .numPredict(numPredict)
+                .numCtx(numCtx)
                 .stop(stop)
                 .build();
         this.format = format;

File: langchain4j-ollama/src/main/java/dev/langchain4j/model/ollama/Options.java
Patch:
@@ -24,5 +24,6 @@ class Options {
     private Double repeatPenalty;
     private Integer seed;
     private Integer numPredict;
+    private Integer numCtx;
     private List<String> stop;
 }

File: langchain4j-qianfan/src/main/java/dev/langchain4j/model/qianfan/InternalQianfanHelper.java
Patch:
@@ -180,6 +180,7 @@ static String getSystemMessage(List<ChatMessage> messages) {
     }
     public static List<Message> toOpenAiMessages(List<ChatMessage> messages) {
         return messages.stream()
+                .filter(chatMessage -> !(chatMessage instanceof SystemMessage))
                 .map(InternalQianfanHelper::toQianfanMessage)
                 .collect(toList());
     }

File: langchain4j-azure-open-ai/src/main/java/dev/langchain4j/model/azure/AzureOpenAiChatModel.java
Patch:
@@ -28,7 +28,8 @@
 /**
  * Represents an OpenAI language model, hosted on Azure, that has a chat completion interface, such as gpt-3.5-turbo.
  * <p>
- * Mandatory parameters for initialization are: endpoint, serviceVersion, apikey (or an alternate authentication method, see below for more information) and deploymentName.
+ * Mandatory parameters for initialization are: endpoint and apikey (or an alternate authentication method, see below for more information).
+ * Optionally you can set serviceVersion (if not, the latest version is used) and deploymentName (if not, a default name is used).
  * You can also provide your own OpenAIClient instance, if you need more flexibility.
  * <p>
  * There are 3 authentication methods:

File: langchain4j-azure-open-ai/src/main/java/dev/langchain4j/model/azure/AzureOpenAiEmbeddingModel.java
Patch:
@@ -30,7 +30,8 @@
 /**
  * Represents an OpenAI embedding model, hosted on Azure, such as text-embedding-ada-002.
  * <p>
- * Mandatory parameters for initialization are: endpoint, serviceVersion, apikey (or an alternate authentication method, see below for more information) and deploymentName.
+ * Mandatory parameters for initialization are: endpoint and apikey (or an alternate authentication method, see below for more information).
+ * Optionally you can set serviceVersion (if not, the latest version is used) and deploymentName (if not, a default name is used).
  * You can also provide your own OpenAIClient instance, if you need more flexibility.
  * <p>
  * There are 3 authentication methods:

File: langchain4j-azure-open-ai/src/main/java/dev/langchain4j/model/azure/AzureOpenAiImageModel.java
Patch:
@@ -22,7 +22,8 @@
  * <p>
  * You can find a tutorial on using Azure OpenAI to generate images at: https://learn.microsoft.com/en-us/azure/ai-services/openai/dall-e-quickstart?pivots=programming-language-java
  * <p>
- * Mandatory parameters for initialization are: endpoint, serviceVersion, apikey (or an alternate authentication method, see below for more information) and deploymentName.
+ * Mandatory parameters for initialization are: endpoint and apikey (or an alternate authentication method, see below for more information).
+ * Optionally you can set serviceVersion (if not, the latest version is used) and deploymentName (if not, a default name is used).
  * You can also provide your own OpenAIClient instance, if you need more flexibility.
  * <p>
  * There are 3 authentication methods:

File: langchain4j-azure-open-ai/src/main/java/dev/langchain4j/model/azure/AzureOpenAiLanguageModel.java
Patch:
@@ -26,7 +26,8 @@
  * However, it's recommended to use {@link AzureOpenAiChatModel} instead,
  * as it offers more advanced features like function calling, multi-turn conversations, etc.
  * <p>
- * Mandatory parameters for initialization are: endpoint, serviceVersion, apikey (or an alternate authentication method, see below for more information) and deploymentName.
+ * Mandatory parameters for initialization are: endpoint and apikey (or an alternate authentication method, see below for more information).
+ * Optionally you can set serviceVersion (if not, the latest version is used) and deploymentName (if not, a default name is used).
  * You can also provide your own OpenAIClient instance, if you need more flexibility.
  * <p>
  * There are 3 authentication methods:

File: langchain4j-azure-open-ai/src/main/java/dev/langchain4j/model/azure/AzureOpenAiStreamingChatModel.java
Patch:
@@ -32,7 +32,8 @@
  * Represents an OpenAI language model, hosted on Azure, that has a chat completion interface, such as gpt-3.5-turbo.
  * The model's response is streamed token by token and should be handled with {@link StreamingResponseHandler}.
  * <p>
- * Mandatory parameters for initialization are: endpoint, serviceVersion, apikey (or an alternate authentication method, see below for more information) and deploymentName.
+ * Mandatory parameters for initialization are: endpoint and apikey (or an alternate authentication method, see below for more information).
+ * Optionally you can set serviceVersion (if not, the latest version is used) and deploymentName (if not, a default name is used).
  * You can also provide your own OpenAIClient instance, if you need more flexibility.
  * <p>
  * There are 3 authentication methods:

File: langchain4j-azure-open-ai/src/test/java/dev/langchain4j/model/azure/AzureOpenAiEmbeddingModelIT.java
Patch:
@@ -20,7 +20,6 @@ public class AzureOpenAiEmbeddingModelIT {
 
     EmbeddingModel model = AzureOpenAiEmbeddingModel.builder()
             .endpoint(System.getenv("AZURE_OPENAI_ENDPOINT"))
-            .serviceVersion(System.getenv("AZURE_OPENAI_SERVICE_VERSION"))
             .apiKey(System.getenv("AZURE_OPENAI_KEY"))
             .deploymentName("text-embedding-ada-002")
             .logRequestsAndResponses(true)

File: langchain4j-azure-open-ai/src/test/java/dev/langchain4j/model/azure/AzureOpenAiImageModelIT.java
Patch:
@@ -48,8 +48,8 @@ void should_generate_image_with_url() {
     void should_generate_image_in_base64() throws IOException {
         AzureOpenAiImageModel model = AzureOpenAiImageModel.builder()
                 .endpoint(System.getenv("AZURE_OPENAI_ENDPOINT"))
-                .deploymentName("dall-e-3")
                 .apiKey(System.getenv("AZURE_OPENAI_KEY"))
+                .deploymentName("dall-e-3")
                 .logRequestsAndResponses(false) // The image is big, so we don't want to log it by default
                 .responseFormat(ImageGenerationResponseFormat.BASE64.toString())
                 .build();

File: langchain4j-azure-open-ai/src/test/java/dev/langchain4j/model/azure/AzureOpenAiLanguageModelIT.java
Patch:
@@ -17,7 +17,6 @@ class AzureOpenAiLanguageModelIT {
 
     LanguageModel model = AzureOpenAiLanguageModel.builder()
             .endpoint(System.getenv("AZURE_OPENAI_ENDPOINT"))
-            .serviceVersion(System.getenv("AZURE_OPENAI_SERVICE_VERSION"))
             .apiKey(System.getenv("AZURE_OPENAI_KEY"))
             .deploymentName("gpt-35-turbo-instruct")
             .temperature(0.0)

File: langchain4j-azure-open-ai/src/test/java/dev/langchain4j/model/azure/AzureOpenAiStreamingChatModelIT.java
Patch:
@@ -46,7 +46,6 @@ void should_stream_answer(String deploymentName, String gptVersion) throws Excep
 
         StreamingChatLanguageModel model = AzureOpenAiStreamingChatModel.builder()
                 .endpoint(System.getenv("AZURE_OPENAI_ENDPOINT"))
-                .serviceVersion(System.getenv("AZURE_OPENAI_SERVICE_VERSION"))
                 .apiKey(System.getenv("AZURE_OPENAI_KEY"))
                 .deploymentName(deploymentName)
                 .tokenizer(new OpenAiTokenizer(gptVersion))
@@ -103,7 +102,6 @@ void should_use_json_format(String deploymentName, String gptVersion) throws Exc
 
         StreamingChatLanguageModel model = AzureOpenAiStreamingChatModel.builder()
                 .endpoint(System.getenv("AZURE_OPENAI_ENDPOINT"))
-                .serviceVersion(System.getenv("AZURE_OPENAI_SERVICE_VERSION"))
                 .apiKey(System.getenv("AZURE_OPENAI_KEY"))
                 .deploymentName(deploymentName)
                 .tokenizer(new OpenAiTokenizer(gptVersion))
@@ -166,7 +164,6 @@ void should_return_tool_execution_request(String deploymentName, String gptVersi
 
         StreamingChatLanguageModel model = AzureOpenAiStreamingChatModel.builder()
                 .endpoint(System.getenv("AZURE_OPENAI_ENDPOINT"))
-                .serviceVersion(System.getenv("AZURE_OPENAI_SERVICE_VERSION"))
                 .apiKey(System.getenv("AZURE_OPENAI_KEY"))
                 .deploymentName(deploymentName)
                 .tokenizer(new OpenAiTokenizer(gptVersion))

File: langchain4j-azure-open-ai/src/test/java/dev/langchain4j/model/azure/AzureOpenAiStreamingLanguageModelIT.java
Patch:
@@ -20,7 +20,6 @@ class AzureOpenAiStreamingLanguageModelIT {
 
     StreamingLanguageModel model = AzureOpenAiStreamingLanguageModel.builder()
             .endpoint(System.getenv("AZURE_OPENAI_ENDPOINT"))
-            .serviceVersion(System.getenv("AZURE_OPENAI_SERVICE_VERSION"))
             .apiKey(System.getenv("AZURE_OPENAI_KEY"))
             .deploymentName("gpt-35-turbo-instruct")
             .temperature(0.0)

File: langchain4j/src/main/java/dev/langchain4j/service/ServiceOutputParser.java
Patch:
@@ -123,9 +123,7 @@ private static String jsonStructure(Class<?> structured) {
         jsonSchema.append("{\n");
         for (Field field : structured.getDeclaredFields()) {
             String name = field.getName();
-            if (name.equals("__$hits$__")
-                    || java.lang.reflect.Modifier.isStatic(field.getModifiers())
-                    || java.lang.reflect.Modifier.isFinal(field.getModifiers())) {
+            if (name.equals("__$hits$__") || java.lang.reflect.Modifier.isStatic(field.getModifiers())) {
                 // Skip coverage instrumentation field.
                 continue;
             }

File: langchain4j/src/main/java/dev/langchain4j/model/output/LocalDateOutputParser.java
Patch:
@@ -13,6 +13,6 @@ public LocalDate parse(String string) {
 
     @Override
     public String formatInstructions() {
-        return "2023-12-31";
+        return "yyyy-MM-dd";
     }
 }

File: langchain4j/src/main/java/dev/langchain4j/model/output/LocalDateTimeOutputParser.java
Patch:
@@ -13,6 +13,6 @@ public LocalDateTime parse(String string) {
 
     @Override
     public String formatInstructions() {
-        return "2023-12-31T23:59:59";
+        return "yyyy-MM-ddTHH:mm:ss";
     }
 }

File: langchain4j/src/main/java/dev/langchain4j/model/output/LocalTimeOutputParser.java
Patch:
@@ -13,6 +13,6 @@ public LocalTime parse(String string) {
 
     @Override
     public String formatInstructions() {
-        return "23:59:59";
+        return "HH:mm:ss";
     }
 }

File: langchain4j-azure-open-ai/src/test/java/dev/langchain4j/model/azure/AzureOpenAiEmbeddingModelIT.java
Patch:
@@ -22,7 +22,7 @@ public class AzureOpenAiEmbeddingModelIT {
             .endpoint(System.getenv("AZURE_OPENAI_ENDPOINT"))
             .serviceVersion(System.getenv("AZURE_OPENAI_SERVICE_VERSION"))
             .apiKey(System.getenv("AZURE_OPENAI_KEY"))
-            .deploymentName(System.getenv("AZURE_OPENAI_DEPLOYMENT_NAME"))
+            .deploymentName("text-embedding-ada-002")
             .logRequestsAndResponses(true)
             .build();
 

File: langchain4j-core/src/test/java/dev/langchain4j/rag/query/transformer/CompressingQueryTransformerTest.java
Patch:
@@ -45,7 +45,7 @@ void should_compress_query_and_chat_memory_into_single_query() {
 
         String expectedResultingQuery = "How old is Klaus Heisler?";
 
-        ChatModelMock model = ChatModelMock.withStaticResponse(expectedResultingQuery);
+        ChatModelMock model = ChatModelMock.thatAlwaysResponds(expectedResultingQuery);
         CompressingQueryTransformer transformer = new CompressingQueryTransformer(model);
 
         // when
@@ -111,7 +111,7 @@ void should_compress_query_and_chat_memory_into_single_query_using_custom_prompt
         Query query = Query.from(userMessage.text(), metadata);
 
         String expectedResultingQuery = "How old is Klaus Heisler?";
-        ChatModelMock model = ChatModelMock.withStaticResponse(expectedResultingQuery);
+        ChatModelMock model = ChatModelMock.thatAlwaysResponds(expectedResultingQuery);
 
         CompressingQueryTransformer transformer = new CompressingQueryTransformer(model, promptTemplate);
 
@@ -145,7 +145,7 @@ void should_compress_query_and_chat_memory_into_single_query_using_custom_prompt
         Query query = Query.from(userMessage.text(), metadata);
 
         String expectedResultingQuery = "How old is Klaus Heisler?";
-        ChatModelMock model = ChatModelMock.withStaticResponse(expectedResultingQuery);
+        ChatModelMock model = ChatModelMock.thatAlwaysResponds(expectedResultingQuery);
 
         CompressingQueryTransformer transformer = CompressingQueryTransformer.builder()
                 .chatLanguageModel(model)

File: langchain4j/src/test/java/dev/langchain4j/chain/ConversationalChainTest.java
Patch:
@@ -24,7 +24,7 @@ void should_store_user_and_ai_messages_in_chat_memory() {
         ChatMemory chatMemory = spy(MessageWindowChatMemory.withMaxMessages(10));
 
         String aiMessage = "Hi there";
-        ChatModelMock model = ChatModelMock.withStaticResponse(aiMessage);
+        ChatModelMock model = ChatModelMock.thatAlwaysResponds(aiMessage);
 
         ConversationalChain chain = ConversationalChain.builder()
                 .chatLanguageModel(model)

File: langchain4j-dashscope/src/main/java/dev/langchain4j/model/dashscope/QwenHelper.java
Patch:
@@ -117,7 +117,7 @@ static Map<String, Object> toMultiModalContent(Content content) {
                     // or do so upon system reboot.
                     imageContent = saveImageAsTemporaryFile(image.base64Data(), image.mimeType());
 
-                    // In this case, the dashscope sdk requires a writable map.
+                    // In this case, the dashscope sdk requires a mutable map.
                     HashMap<String, Object> contentMap = new HashMap<>(1);
                     contentMap.put("image", imageContent);
                     return contentMap;

File: langchain4j-dashscope/src/main/java/dev/langchain4j/model/dashscope/QwenModelName.java
Patch:
@@ -15,5 +15,6 @@ public class QwenModelName {
     public static final String QWEN_VL_MAX = "qwen-vl-max";  // Qwen multi-modal model, offers optimal performance on a wider range of complex tasks.
 
     // Use with QwenEmbeddingModel
-    public static final String TEXT_EMBEDDING_V1 = "text-embedding-v1";
+    public static final String TEXT_EMBEDDING_V1 = "text-embedding-v1";  // Support: en, zh, es, fr, pt, id
+    public static final String TEXT_EMBEDDING_V2 = "text-embedding-v2";  // Support: en, zh, es, fr, pt, id, ja, ko, de, ru
 }
\ No newline at end of file

File: langchain4j-dashscope/src/test/java/dev/langchain4j/model/dashscope/QwenTestHelper.java
Patch:
@@ -44,7 +44,8 @@ public static Stream<Arguments> multimodalChatModelNameProvider() {
 
     public static Stream<Arguments> embeddingModelNameProvider() {
         return Stream.of(
-                Arguments.of(QwenModelName.TEXT_EMBEDDING_V1)
+                Arguments.of(QwenModelName.TEXT_EMBEDDING_V1),
+                Arguments.of(QwenModelName.TEXT_EMBEDDING_V2)
         );
     }
 

File: document-loaders/langchain4j-document-loader-amazon-s3/src/test/java/dev/langchain4j/data/document/loader/amazon/s3/AmazonS3DocumentLoaderIT.java
Patch:
@@ -42,7 +42,7 @@ public static void beforeAll() {
 
     @BeforeEach
     public void beforeEach() {
-        s3Container = new LocalStackContainer(DockerImageName.parse("localstack/localstack:2.0"))
+        s3Container = new LocalStackContainer(DockerImageName.parse("localstack/localstack:3.1.0"))
                 .withServices(S3)
                 .withEnv("DEFAULT_REGION", TEST_REGION);
         s3Container.start();

File: langchain4j-opensearch/src/test/java/dev/langchain4j/store/embedding/opensearch/OpenSearchEmbeddingStoreAwsIT.java
Patch:
@@ -30,7 +30,7 @@
 class OpenSearchEmbeddingStoreAwsIT extends EmbeddingStoreIT {
 
     @Container
-    private static final LocalStackContainer localstack = new LocalStackContainer(DockerImageName.parse("localstack/localstack:latest"));
+    private static final LocalStackContainer localstack = new LocalStackContainer(DockerImageName.parse("localstack/localstack:3.1.0"));
 
     EmbeddingStore<TextSegment> embeddingStore = OpenSearchEmbeddingStore.builder()
             .serverUrl(String.format("testcontainers-domain.%s.opensearch.localhost.localstack.cloud:%s", localstack.getRegion(), localstack.getMappedPort(4566)))

File: langchain4j-core/src/main/java/dev/langchain4j/model/input/DefaultPromptTemplateFactory.java
Patch:
@@ -62,7 +62,7 @@ private static String replaceAll(String template, String variable, Object value)
             if (value == null || value.toString() == null) {
                 throw illegalArgument("Value for the variable '%s' is null", variable);
             }
-            return template.replaceAll(inDoubleCurlyBrackets(variable), value.toString());
+            return template.replaceAll(inDoubleCurlyBrackets(variable), Matcher.quoteReplacement(value.toString()));
         }
 
         private static String inDoubleCurlyBrackets(String variable) {

File: langchain4j-core/src/main/java/dev/langchain4j/agent/tool/P.java
Patch:
@@ -15,6 +15,7 @@
 
     /**
      * Description of a parameter
+     * @return the description of a parameter
      */
     String value();
 }

File: langchain4j-core/src/main/java/dev/langchain4j/agent/tool/Tool.java
Patch:
@@ -15,12 +15,14 @@
 
     /**
      * Name of the tool. If not provided, method name will be used.
+     * @return name of the tool.
      */
     String name() default "";
 
     /**
      * Description of the tool.
      * It should be clear and descriptive to allow language model to understand the tool's purpose and its intended use.
+     * @return description of the tool.
      */
     String[] value() default "";
 }

File: langchain4j-core/src/main/java/dev/langchain4j/internal/Exceptions.java
Patch:
@@ -1,5 +1,8 @@
 package dev.langchain4j.internal;
 
+/**
+ * Utility methods for creating common exceptions.
+ */
 public class Exceptions {
     private Exceptions() {}
 

File: langchain4j-core/src/main/java/dev/langchain4j/internal/Utils.java
Patch:
@@ -13,6 +13,9 @@
 import java.util.UUID;
 import java.util.function.Supplier;
 
+/**
+ * Utility methods.
+ */
 public class Utils {
   private Utils() {}
 

File: langchain4j-core/src/main/java/dev/langchain4j/internal/ValidationUtils.java
Patch:
@@ -98,6 +98,7 @@ public static void ensureTrue(boolean expression, String msg) {
      * Ensures that the given expression is true.
      * @param i The expression to check.
      * @param name The message to be used in the exception.
+     * @return The value if it is greater than zero.
      * @throws IllegalArgumentException if the expression is false.
      */
     public static int ensureGreaterThanZero(Integer i, String name) {

File: langchain4j-core/src/main/java/dev/langchain4j/memory/ChatMemory.java
Patch:
@@ -13,6 +13,7 @@
 public interface ChatMemory {
 
     /**
+     * The ID of the {@link ChatMemory}.
      * @return The ID of the {@link ChatMemory}.
      */
     Object id();

File: langchain4j-core/src/main/java/dev/langchain4j/data/message/GsonContentAdapter.java
Patch:
@@ -21,6 +21,6 @@ public JsonElement serialize(Content content, Type ignored, JsonSerializationCon
     public Content deserialize(JsonElement contentJsonElement, Type ignored, JsonDeserializationContext context) throws JsonParseException {
         String contentTypeString = contentJsonElement.getAsJsonObject().get(CONTENT_TYPE).getAsString();
         ContentType contentType = ContentType.valueOf(contentTypeString);
-        return GSON.fromJson(contentJsonElement, ContentType.classOf(contentType));
+        return GSON.fromJson(contentJsonElement, contentType.getContentClass());
     }
 }
\ No newline at end of file

File: langchain4j-core/src/main/java/dev/langchain4j/data/message/GsonChatMessageAdapter.java
Patch:
@@ -25,7 +25,7 @@ public JsonElement serialize(ChatMessage chatMessage, Type ignored, JsonSerializ
     public ChatMessage deserialize(JsonElement messageJsonElement, Type ignored, JsonDeserializationContext context) throws JsonParseException {
         String chatMessageTypeString = messageJsonElement.getAsJsonObject().get(CHAT_MESSAGE_TYPE).getAsString();
         ChatMessageType chatMessageType = ChatMessageType.valueOf(chatMessageTypeString);
-        ChatMessage chatMessage = GSON.fromJson(messageJsonElement, ChatMessageType.classOf(chatMessageType));
+        ChatMessage chatMessage = GSON.fromJson(messageJsonElement, chatMessageType.messageClass());
         if (chatMessage instanceof UserMessage && ((UserMessage) chatMessage).contents() == null) {
             // keeping backward compatibility with old schema TODO remove after a few releases
             chatMessage = UserMessage.from(messageJsonElement.getAsJsonObject().get("text").getAsString());

File: langchain4j-ollama/src/test/java/dev/langchain4j/model/ollama/OllamaLanguageModelIT.java
Patch:
@@ -29,7 +29,7 @@ void should_generate_answer() {
         assertThat(response.content()).contains("Berlin");
 
         TokenUsage tokenUsage = response.tokenUsage();
-        assertThat(tokenUsage.inputTokenCount()).isEqualTo(43);
+        assertThat(tokenUsage.inputTokenCount()).isEqualTo(38);
         assertThat(tokenUsage.outputTokenCount()).isGreaterThan(0);
         assertThat(tokenUsage.totalTokenCount())
                 .isEqualTo(tokenUsage.inputTokenCount() + tokenUsage.outputTokenCount());

File: langchain4j-ollama/src/test/java/dev/langchain4j/model/ollama/OllamaStreamingLanguageModelIT.java
Patch:
@@ -61,7 +61,7 @@ public void onError(Throwable error) {
         assertThat(response.content()).isEqualTo(answer);
 
         TokenUsage tokenUsage = response.tokenUsage();
-        assertThat(tokenUsage.inputTokenCount()).isEqualTo(43);
+        assertThat(tokenUsage.inputTokenCount()).isEqualTo(38);
         assertThat(tokenUsage.outputTokenCount()).isGreaterThan(0);
         assertThat(tokenUsage.totalTokenCount())
                 .isEqualTo(tokenUsage.inputTokenCount() + tokenUsage.outputTokenCount());

File: langchain4j-open-ai/src/main/java/dev/langchain4j/model/openai/OpenAiChatModel.java
Patch:
@@ -43,6 +43,7 @@ public class OpenAiChatModel implements ChatLanguageModel, TokenCountEstimator {
     @Builder
     public OpenAiChatModel(String baseUrl,
                            String apiKey,
+                           String organizationId,
                            String modelName,
                            Double temperature,
                            Double topP,
@@ -67,6 +68,7 @@ public OpenAiChatModel(String baseUrl,
         this.client = OpenAiClient.builder()
                 .openAiApiKey(apiKey)
                 .baseUrl(baseUrl)
+                .organizationId(organizationId)
                 .callTimeout(timeout)
                 .connectTimeout(timeout)
                 .readTimeout(timeout)

File: langchain4j-open-ai/src/main/java/dev/langchain4j/model/openai/OpenAiEmbeddingModel.java
Patch:
@@ -35,6 +35,7 @@ public class OpenAiEmbeddingModel implements EmbeddingModel, TokenCountEstimator
     @Builder
     public OpenAiEmbeddingModel(String baseUrl,
                                 String apiKey,
+                                String organizationId,
                                 String modelName,
                                 Duration timeout,
                                 Integer maxRetries,
@@ -53,6 +54,7 @@ public OpenAiEmbeddingModel(String baseUrl,
         this.client = OpenAiClient.builder()
                 .openAiApiKey(apiKey)
                 .baseUrl(baseUrl)
+                .organizationId(organizationId)
                 .callTimeout(timeout)
                 .connectTimeout(timeout)
                 .readTimeout(timeout)

File: langchain4j-open-ai/src/main/java/dev/langchain4j/model/openai/OpenAiLanguageModel.java
Patch:
@@ -35,6 +35,7 @@ public class OpenAiLanguageModel implements LanguageModel, TokenCountEstimator {
     @Builder
     public OpenAiLanguageModel(String baseUrl,
                                String apiKey,
+                               String organizationId,
                                String modelName,
                                Double temperature,
                                Duration timeout,
@@ -49,6 +50,7 @@ public OpenAiLanguageModel(String baseUrl,
         this.client = OpenAiClient.builder()
                 .baseUrl(getOrDefault(baseUrl, OPENAI_URL))
                 .openAiApiKey(apiKey)
+                .organizationId(organizationId)
                 .callTimeout(timeout)
                 .connectTimeout(timeout)
                 .readTimeout(timeout)

File: langchain4j-open-ai/src/main/java/dev/langchain4j/model/openai/OpenAiModerationModel.java
Patch:
@@ -36,6 +36,7 @@ public class OpenAiModerationModel implements ModerationModel {
     @Builder
     public OpenAiModerationModel(String baseUrl,
                                  String apiKey,
+                                 String organizationId,
                                  String modelName,
                                  Duration timeout,
                                  Integer maxRetries,
@@ -53,6 +54,7 @@ public OpenAiModerationModel(String baseUrl,
         this.client = OpenAiClient.builder()
                 .openAiApiKey(apiKey)
                 .baseUrl(baseUrl)
+                .organizationId(organizationId)
                 .callTimeout(timeout)
                 .connectTimeout(timeout)
                 .readTimeout(timeout)

File: langchain4j-open-ai/src/main/java/dev/langchain4j/model/openai/OpenAiStreamingChatModel.java
Patch:
@@ -46,6 +46,7 @@ public class OpenAiStreamingChatModel implements StreamingChatLanguageModel, Tok
     @Builder
     public OpenAiStreamingChatModel(String baseUrl,
                                     String apiKey,
+                                    String organizationId,
                                     String modelName,
                                     Double temperature,
                                     Double topP,
@@ -64,6 +65,7 @@ public OpenAiStreamingChatModel(String baseUrl,
         this.client = OpenAiClient.builder()
                 .baseUrl(getOrDefault(baseUrl, OPENAI_URL))
                 .openAiApiKey(apiKey)
+                .organizationId(organizationId)
                 .callTimeout(timeout)
                 .connectTimeout(timeout)
                 .readTimeout(timeout)

File: langchain4j-open-ai/src/main/java/dev/langchain4j/model/openai/OpenAiStreamingLanguageModel.java
Patch:
@@ -34,6 +34,7 @@ public class OpenAiStreamingLanguageModel implements StreamingLanguageModel, Tok
     @Builder
     public OpenAiStreamingLanguageModel(String baseUrl,
                                         String apiKey,
+                                        String organizationId,
                                         String modelName,
                                         Double temperature,
                                         Duration timeout,
@@ -47,6 +48,7 @@ public OpenAiStreamingLanguageModel(String baseUrl,
         this.client = OpenAiClient.builder()
                 .baseUrl(getOrDefault(baseUrl, OPENAI_URL))
                 .openAiApiKey(apiKey)
+                .organizationId(organizationId)
                 .callTimeout(timeout)
                 .connectTimeout(timeout)
                 .readTimeout(timeout)

File: langchain4j-open-ai/src/test/java/dev/langchain4j/model/openai/OpenAiChatModelIT.java
Patch:
@@ -33,6 +33,7 @@ class OpenAiChatModelIT {
 
     ChatLanguageModel model = OpenAiChatModel.builder()
             .apiKey(System.getenv("OPENAI_API_KEY"))
+            .organizationId(System.getenv("OPENAI_ORGANIZATION_ID"))
             .temperature(0.0)
             .logRequests(true)
             .logResponses(true)
@@ -65,6 +66,7 @@ void should_generate_answer_and_return_token_usage_and_finish_reason_length() {
         // given
         ChatLanguageModel model = OpenAiChatModel.builder()
                 .apiKey(System.getenv("OPENAI_API_KEY"))
+                .organizationId(System.getenv("OPENAI_ORGANIZATION_ID"))
                 .maxTokens(3)
                 .build();
 
@@ -188,6 +190,7 @@ void should_execute_multiple_tools_in_parallel_then_answer() {
         // given
         ChatLanguageModel model = OpenAiChatModel.builder()
                 .apiKey(System.getenv("OPENAI_API_KEY"))
+                .organizationId(System.getenv("OPENAI_ORGANIZATION_ID"))
                 .modelName(GPT_3_5_TURBO_1106.toString()) // supports parallel function calling
                 .temperature(0.0)
                 .build();

File: langchain4j-open-ai/src/test/java/dev/langchain4j/model/openai/OpenAiLanguageModelIT.java
Patch:
@@ -12,6 +12,7 @@ class OpenAiLanguageModelIT {
 
     LanguageModel model = OpenAiLanguageModel.builder()
             .apiKey(System.getenv("OPENAI_API_KEY"))
+            .organizationId(System.getenv("OPENAI_ORGANIZATION_ID"))
             .logRequests(true)
             .logResponses(true)
             .build();

File: langchain4j-open-ai/src/test/java/dev/langchain4j/model/openai/OpenAiModerationModelIT.java
Patch:
@@ -10,6 +10,7 @@ class OpenAiModerationModelIT {
 
     ModerationModel model = OpenAiModerationModel.builder()
             .apiKey(System.getenv("OPENAI_API_KEY"))
+            .organizationId(System.getenv("OPENAI_ORGANIZATION_ID"))
             .build();
 
     @Test

File: langchain4j-open-ai/src/test/java/dev/langchain4j/model/openai/OpenAiStreamingChatModelIT.java
Patch:
@@ -34,6 +34,7 @@ class OpenAiStreamingChatModelIT {
 
     StreamingChatLanguageModel model = OpenAiStreamingChatModel.builder()
             .apiKey(System.getenv("OPENAI_API_KEY"))
+            .organizationId(System.getenv("OPENAI_ORGANIZATION_ID"))
             .temperature(0.0)
             .logRequests(true)
             .logResponses(true)
@@ -288,6 +289,7 @@ void should_execute_multiple_tools_in_parallel_then_stream_answer() throws Excep
         // given
         StreamingChatLanguageModel model = OpenAiStreamingChatModel.builder()
                 .apiKey(System.getenv("OPENAI_API_KEY"))
+                .organizationId(System.getenv("OPENAI_ORGANIZATION_ID"))
                 .modelName(GPT_3_5_TURBO_1106.toString())  // supports parallel function calling
                 .temperature(0.0)
                 .logRequests(true)

File: langchain4j-open-ai/src/test/java/dev/langchain4j/model/openai/OpenAiStreamingLanguageModelIT.java
Patch:
@@ -18,6 +18,7 @@ class OpenAiStreamingLanguageModelIT {
 
     StreamingLanguageModel model = OpenAiStreamingLanguageModel.builder()
             .apiKey(System.getenv("OPENAI_API_KEY"))
+            .organizationId(System.getenv("OPENAI_ORGANIZATION_ID"))
             .logRequests(true)
             .logResponses(true)
             .build();

File: langchain4j/src/test/java/dev/langchain4j/service/AiServicesIT.java
Patch:
@@ -56,6 +56,7 @@ public class AiServicesIT {
     @Spy
     ChatLanguageModel chatLanguageModel = OpenAiChatModel.builder()
             .apiKey(System.getenv("OPENAI_API_KEY"))
+            .organizationId(System.getenv("OPENAI_ORGANIZATION_ID"))
             .temperature(0.0)
             .logRequests(true)
             .logResponses(true)
@@ -67,6 +68,7 @@ public class AiServicesIT {
     @Spy
     ModerationModel moderationModel = OpenAiModerationModel.builder()
             .apiKey(System.getenv("OPENAI_API_KEY"))
+            .organizationId(System.getenv("OPENAI_ORGANIZATION_ID"))
             .build();
 
     ToolSpecification calculatorSpecification = ToolSpecification.builder()
@@ -845,6 +847,7 @@ void should_execute_multiple_tools_in_parallel_then_answer() {
 
         ChatLanguageModel chatLanguageModel = spy(OpenAiChatModel.builder()
                 .apiKey(System.getenv("OPENAI_API_KEY"))
+                .organizationId(System.getenv("OPENAI_ORGANIZATION_ID"))
                 .modelName(GPT_3_5_TURBO_1106)
                 .temperature(0.0)
                 .logRequests(true)

File: langchain4j/src/test/java/dev/langchain4j/service/StreamingAiServicesIT.java
Patch:
@@ -30,6 +30,7 @@ public class StreamingAiServicesIT {
 
     StreamingChatLanguageModel streamingChatModel = OpenAiStreamingChatModel.builder()
             .apiKey(System.getenv("OPENAI_API_KEY"))
+            .organizationId(System.getenv("OPENAI_ORGANIZATION_ID"))
             .temperature(0.0)
             .logRequests(true)
             .logResponses(true)
@@ -298,6 +299,7 @@ void should_execute_multiple_tools_in_parallel_then_answer() throws Exception {
 
         StreamingChatLanguageModel streamingChatModel = OpenAiStreamingChatModel.builder()
                 .apiKey(System.getenv("OPENAI_API_KEY"))
+                .organizationId(System.getenv("OPENAI_ORGANIZATION_ID"))
                 .modelName(GPT_3_5_TURBO_1106)
                 .temperature(0.0)
                 .logRequests(true)

File: langchain4j/src/main/java/dev/langchain4j/store/embedding/inmemory/InMemoryEmbeddingStore.java
Patch:
@@ -67,7 +67,7 @@ public int hashCode() {
         }
     }
 
-    private final List<Entry<Embedded>> entries = new CopyOnWriteArrayList<>();
+    final CopyOnWriteArrayList<Entry<Embedded>> entries = new CopyOnWriteArrayList<>();
 
     @Override
     public String add(Embedding embedding) {

File: langchain4j/src/test/java/dev/langchain4j/store/embedding/inmemory/InMemoryEmbeddingStoreTest.java
Patch:
@@ -14,6 +14,7 @@
 
 import java.nio.file.Path;
 import java.util.List;
+import java.util.concurrent.CopyOnWriteArrayList;
 
 import static dev.langchain4j.internal.Utils.randomUUID;
 import static java.util.Arrays.asList;
@@ -233,6 +234,7 @@ void should_serialize_to_and_deserialize_from_json() {
         InMemoryEmbeddingStore<TextSegment> deserializedEmbeddingStore = InMemoryEmbeddingStore.fromJson(json);
 
         assertThat(deserializedEmbeddingStore).isEqualTo(originalEmbeddingStore);
+        assertThat(deserializedEmbeddingStore.entries).isInstanceOf(CopyOnWriteArrayList.class);
     }
 
     @Test
@@ -245,6 +247,7 @@ void should_serialize_to_and_deserialize_from_file() {
         InMemoryEmbeddingStore<TextSegment> deserializedEmbeddingStore = InMemoryEmbeddingStore.fromFile(filePath);
 
         assertThat(deserializedEmbeddingStore).isEqualTo(originalEmbeddingStore);
+        assertThat(deserializedEmbeddingStore.entries).isInstanceOf(CopyOnWriteArrayList.class);
     }
 
     private InMemoryEmbeddingStore<TextSegment> createEmbeddingStore() {

File: langchain4j/src/main/java/dev/langchain4j/store/embedding/inmemory/InMemoryEmbeddingStore.java
Patch:
@@ -23,6 +23,7 @@
 import java.util.List;
 import java.util.Objects;
 import java.util.PriorityQueue;
+import java.util.concurrent.CopyOnWriteArrayList;
 
 /**
  * An {@link EmbeddingStore} that stores embeddings in memory.
@@ -66,7 +67,7 @@ public int hashCode() {
         }
     }
 
-    private final List<Entry<Embedded>> entries = new ArrayList<>();
+    private final List<Entry<Embedded>> entries = new CopyOnWriteArrayList<>();
 
     @Override
     public String add(Embedding embedding) {

File: langchain4j-open-ai/src/main/java/dev/langchain4j/model/openai/InternalOpenAiHelper.java
Patch:
@@ -130,6 +130,9 @@ public static TokenUsage tokenUsageFrom(Usage openAiUsage) {
     }
 
     public static FinishReason finishReasonFrom(String openAiFinishReason) {
+        if (openAiFinishReason == null) {
+            return null;
+        }
         switch (openAiFinishReason) {
             case "stop":
                 return STOP;

File: langchain4j-pgvector/src/main/java/dev/langchain4j/store/embedding/pgvector/PgVectorEmbeddingStore.java
Patch:
@@ -97,11 +97,12 @@ public PgVectorEmbeddingStore(
             }
 
             if (useIndex) {
+                final String indexName = table + "_ivfflat_index";
                 connection.createStatement().executeUpdate(String.format(
-                        "CREATE INDEX IF NOT EXISTS ON %s " +
+                        "CREATE INDEX IF NOT EXISTS %s ON %s " +
                                 "USING ivfflat (embedding vector_cosine_ops) " +
                                 "WITH (lists = %s)",
-                        table, ensureGreaterThanZero(indexListSize, "indexListSize")));
+                        indexName, table, ensureGreaterThanZero(indexListSize, "indexListSize")));
             }
         } catch (SQLException e) {
             throw new RuntimeException(e);

File: langchain4j/src/main/java/dev/langchain4j/memory/chat/MessageWindowChatMemory.java
Patch:
@@ -8,7 +8,7 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import java.util.ArrayList;
+import java.util.LinkedList;
 import java.util.List;
 import java.util.Optional;
 
@@ -73,7 +73,7 @@ private static Optional<SystemMessage> findSystemMessage(List<ChatMessage> messa
 
     @Override
     public List<ChatMessage> messages() {
-        List<ChatMessage> messages = new ArrayList<>(store.getMessages(id));
+        List<ChatMessage> messages = new LinkedList<>(store.getMessages(id));
         ensureCapacity(messages, maxMessages);
         return messages;
     }

File: langchain4j/src/main/java/dev/langchain4j/memory/chat/TokenWindowChatMemory.java
Patch:
@@ -9,7 +9,7 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import java.util.ArrayList;
+import java.util.LinkedList;
 import java.util.List;
 import java.util.Optional;
 
@@ -77,7 +77,7 @@ private static Optional<SystemMessage> findSystemMessage(List<ChatMessage> messa
 
     @Override
     public List<ChatMessage> messages() {
-        List<ChatMessage> messages = new ArrayList<>(store.getMessages(id));
+        List<ChatMessage> messages = new LinkedList<>(store.getMessages(id));
         ensureCapacity(messages, maxTokens, tokenizer);
         return messages;
     }

File: langchain4j-vertex-ai/src/test/java/dev/langchain4j/model/vertexai/VertexAiLanguageModelIT.java
Patch:
@@ -7,12 +7,12 @@
 
 import static org.assertj.core.api.Assertions.assertThat;
 
-class VextexAiLanguageModelIT {
+class VertexAiLanguageModelIT {
 
     @Test
     @Disabled("To run this test, you must have provide your own endpoint, project and location")
     void testLanguageModel() {
-        VextexAiLanguageModel vextexAiLanguageModel = VextexAiLanguageModel.builder()
+        VertexAiLanguageModel vertexAiLanguageModel = VertexAiLanguageModel.builder()
                 .endpoint("us-central1-aiplatform.googleapis.com:443")
                 .project("langchain4j")
                 .location("us-central1")
@@ -25,7 +25,7 @@ void testLanguageModel() {
                 .maxRetries(3)
                 .build();
 
-        Response<String> response = vextexAiLanguageModel.generate("hi, what is java?");
+        Response<String> response = vertexAiLanguageModel.generate("hi, what is java?");
 
         assertThat(response.content()).containsIgnoringCase("java");
         System.out.println(response);

File: langchain4j/src/main/java/dev/langchain4j/service/AiServiceStreamingResponseHandler.java
Patch:
@@ -106,4 +106,4 @@ public void onError(Throwable error) {
             log.warn("Ignored error", error);
         }
     }
-}
\ No newline at end of file
+}

File: langchain4j/src/main/java/dev/langchain4j/service/AiServiceTokenStream.java
Patch:
@@ -11,13 +11,13 @@
 import static dev.langchain4j.internal.ValidationUtils.ensureNotEmpty;
 import static dev.langchain4j.internal.ValidationUtils.ensureNotNull;
 
-class AiServiceTokenStream implements TokenStream {
+public class AiServiceTokenStream implements TokenStream {
 
     private final List<ChatMessage> messagesToSend;
     private final AiServiceContext context;
     private final Object memoryId;
 
-    AiServiceTokenStream(List<ChatMessage> messagesToSend, AiServiceContext context, Object memoryId) {
+    public AiServiceTokenStream(List<ChatMessage> messagesToSend, AiServiceContext context, Object memoryId) {
         this.messagesToSend = ensureNotEmpty(messagesToSend, "messagesToSend");
         this.context = ensureNotNull(context, "context");
         this.memoryId = ensureNotNull(memoryId, "memoryId");

File: langchain4j/src/main/java/dev/langchain4j/service/ServiceOutputParser.java
Patch:
@@ -19,7 +19,7 @@
 import static java.lang.String.format;
 import static java.util.Arrays.asList;
 
-class ServiceOutputParser {
+public class ServiceOutputParser {
 
     private static final Map<Class<?>, OutputParser<?>> OUTPUT_PARSERS = new HashMap<>();
 
@@ -55,7 +55,7 @@ class ServiceOutputParser {
         OUTPUT_PARSERS.put(LocalDateTime.class, new LocalDateTimeOutputParser());
     }
 
-    static Object parse(Response<AiMessage> response, Class<?> returnType) {
+    public static Object parse(Response<AiMessage> response, Class<?> returnType) {
 
         if (returnType == Response.class) {
             return response;
@@ -87,7 +87,7 @@ static Object parse(Response<AiMessage> response, Class<?> returnType) {
         return Json.fromJson(text, returnType);
     }
 
-    static String outputFormatInstructions(Class<?> returnType) {
+    public static String outputFormatInstructions(Class<?> returnType) {
 
         if (returnType == String.class
                 || returnType == AiMessage.class

File: langchain4j/src/main/java/dev/langchain4j/agent/tool/ToolExecutor.java
Patch:
@@ -28,7 +28,7 @@ public String execute(ToolExecutionRequest toolExecutionRequest) {
 
         // TODO ensure this method never throws exceptions
 
-        Object[] arguments = prepareArguments(toolExecutionRequest.argumentsAsMap());
+        Object[] arguments = prepareArguments(ToolExecutionRequestUtil.argumentsAsMap(toolExecutionRequest.arguments()));
         try {
             String result = execute(arguments);
             log.debug("Tool execution result: {}", result);

File: langchain4j-azure-open-ai/src/main/java/dev/langchain4j/model/azure/AzureOpenAiEmbeddingModel.java
Patch:
@@ -17,6 +17,7 @@
 import java.util.List;
 
 import static dev.langchain4j.internal.RetryUtils.withRetry;
+import static dev.langchain4j.internal.Utils.getOrDefault;
 import static dev.langchain4j.internal.ValidationUtils.ensureNotBlank;
 import static java.time.Duration.ofSeconds;
 import static java.util.stream.Collectors.toList;
@@ -55,8 +56,7 @@ public AzureOpenAiEmbeddingModel(String baseUrl,
                                      Boolean logRequests,
                                      Boolean logResponses) {
 
-        timeout = timeout == null ? ofSeconds(15) : timeout;
-        maxRetries = maxRetries == null ? 3 : maxRetries;
+        timeout = getOrDefault(timeout, ofSeconds(60));
 
         this.client = OpenAiClient.builder()
                 .baseUrl(ensureNotBlank(baseUrl, "baseUrl"))
@@ -70,7 +70,7 @@ public AzureOpenAiEmbeddingModel(String baseUrl,
                 .logRequests(logRequests)
                 .logResponses(logResponses)
                 .build();
-        this.maxRetries = maxRetries;
+        this.maxRetries = getOrDefault(maxRetries, 3);
         this.tokenizer = tokenizer;
     }
 

File: langchain4j-azure-open-ai/src/main/java/dev/langchain4j/model/azure/AzureOpenAiStreamingChatModel.java
Patch:
@@ -19,6 +19,7 @@
 import java.time.Duration;
 import java.util.List;
 
+import static dev.langchain4j.internal.Utils.getOrDefault;
 import static dev.langchain4j.internal.ValidationUtils.ensureNotBlank;
 import static dev.langchain4j.model.openai.InternalOpenAiHelper.toFunctions;
 import static dev.langchain4j.model.openai.InternalOpenAiHelper.toOpenAiMessages;
@@ -68,8 +69,7 @@ public AzureOpenAiStreamingChatModel(String baseUrl,
                                          Boolean logRequests,
                                          Boolean logResponses) {
 
-        temperature = temperature == null ? 0.7 : temperature;
-        timeout = timeout == null ? ofSeconds(15) : timeout;
+        timeout = getOrDefault(timeout, ofSeconds(60));
 
         this.client = OpenAiClient.builder()
                 .baseUrl(ensureNotBlank(baseUrl, "baseUrl"))
@@ -83,7 +83,7 @@ public AzureOpenAiStreamingChatModel(String baseUrl,
                 .logRequests(logRequests)
                 .logStreamingResponses(logResponses)
                 .build();
-        this.temperature = temperature;
+        this.temperature = getOrDefault(temperature, 0.7);
         this.topP = topP;
         this.maxTokens = maxTokens;
         this.presencePenalty = presencePenalty;

File: langchain4j-azure-open-ai/src/main/java/dev/langchain4j/model/azure/AzureOpenAiStreamingLanguageModel.java
Patch:
@@ -14,6 +14,7 @@
 import java.net.Proxy;
 import java.time.Duration;
 
+import static dev.langchain4j.internal.Utils.getOrDefault;
 import static dev.langchain4j.internal.ValidationUtils.ensureNotBlank;
 import static java.time.Duration.ofSeconds;
 
@@ -54,8 +55,7 @@ public AzureOpenAiStreamingLanguageModel(String baseUrl,
                                              Boolean logRequests,
                                              Boolean logResponses) {
 
-        temperature = temperature == null ? 0.7 : temperature;
-        timeout = timeout == null ? ofSeconds(15) : timeout;
+        timeout = getOrDefault(timeout, ofSeconds(60));
 
         this.client = OpenAiClient.builder()
                 .baseUrl(ensureNotBlank(baseUrl, "baseUrl"))
@@ -69,7 +69,7 @@ public AzureOpenAiStreamingLanguageModel(String baseUrl,
                 .logRequests(logRequests)
                 .logStreamingResponses(logResponses)
                 .build();
-        this.temperature = temperature;
+        this.temperature = getOrDefault(temperature, 0.7);
         this.tokenizer = tokenizer;
     }
 

File: langchain4j-open-ai/src/main/java/dev/langchain4j/model/openai/OpenAiTokenizer.java
Patch:
@@ -16,6 +16,7 @@
 import java.util.function.Supplier;
 
 import static dev.langchain4j.internal.Exceptions.illegalArgument;
+import static dev.langchain4j.internal.ValidationUtils.ensureNotBlank;
 import static dev.langchain4j.model.openai.InternalOpenAiHelper.roleFrom;
 import static dev.langchain4j.model.openai.OpenAiModelName.GPT_3_5_TURBO_0301;
 
@@ -25,7 +26,7 @@ public class OpenAiTokenizer implements Tokenizer {
     private final Optional<Encoding> encoding;
 
     public OpenAiTokenizer(String modelName) {
-        this.modelName = modelName;
+        this.modelName = ensureNotBlank(modelName, "modelName");
         // If the model is unknown, we should NOT fail fast during the creation of OpenAiTokenizer.
         // Doing so would cause the failure of every OpenAI***Model that uses this tokenizer.
         // This is done to account for situations when a new OpenAI model is available,

File: langchain4j-cassandra/src/main/java/dev/langchain4j/store/embedding/cassandra/CassandraEmbeddingStoreSupport.java
Patch:
@@ -17,7 +17,6 @@
 
 import java.util.ArrayList;
 import java.util.List;
-import java.util.stream.Collectors;
 
 import static dev.langchain4j.internal.ValidationUtils.ensureBetween;
 import static dev.langchain4j.internal.ValidationUtils.ensureGreaterThanZero;

File: langchain4j-dashscope/src/main/java/dev/langchain4j/model/dashscope/QwenStreamingChatModel.java
Patch:
@@ -10,12 +10,10 @@
 import dev.langchain4j.data.message.ChatMessage;
 import dev.langchain4j.model.StreamingResponseHandler;
 import dev.langchain4j.model.chat.StreamingChatLanguageModel;
-import dev.langchain4j.model.output.Response;
 
 import java.util.List;
 
 import static com.alibaba.dashscope.aigc.generation.models.QwenParam.ResultFormat.MESSAGE;
-import static dev.langchain4j.model.dashscope.QwenHelper.answerFrom;
 import static dev.langchain4j.model.dashscope.QwenHelper.toQwenMessages;
 
 public class QwenStreamingChatModel extends QwenChatModel implements StreamingChatLanguageModel {

File: langchain4j-dashscope/src/main/java/dev/langchain4j/model/dashscope/QwenStreamingLanguageModel.java
Patch:
@@ -11,7 +11,6 @@
 import dev.langchain4j.model.output.Response;
 
 import static com.alibaba.dashscope.aigc.generation.models.QwenParam.ResultFormat.MESSAGE;
-import static dev.langchain4j.model.dashscope.QwenHelper.answerFrom;
 
 public class QwenStreamingLanguageModel extends QwenLanguageModel implements StreamingLanguageModel {
 

File: langchain4j-dashscope/src/test/java/dev/langchain4j/model/dashscope/QwenLanguageModelIT.java
Patch:
@@ -1,6 +1,5 @@
 package dev.langchain4j.model.dashscope;
 
-import dev.langchain4j.data.message.AiMessage;
 import dev.langchain4j.internal.Utils;
 import dev.langchain4j.model.language.LanguageModel;
 import dev.langchain4j.model.output.Response;

File: langchain4j-dashscope/src/test/java/dev/langchain4j/model/dashscope/QwenStreamingLanguageModelIT.java
Patch:
@@ -1,6 +1,5 @@
 package dev.langchain4j.model.dashscope;
 
-import dev.langchain4j.data.message.AiMessage;
 import dev.langchain4j.internal.Utils;
 import dev.langchain4j.model.StreamingResponseHandler;
 import dev.langchain4j.model.language.StreamingLanguageModel;

File: langchain4j-core/src/main/java/dev/langchain4j/data/document/DocumentParser.java
Patch:
@@ -8,8 +8,6 @@
  */
 public interface DocumentParser {
 
-    String DOCUMENT_TYPE = "document_type";
-
     /**
      * Parses an InputStream into a Document.
      * The specific implementation of this method will depend on the type of the document being parsed.

File: langchain4j/src/main/java/dev/langchain4j/data/document/DocumentLoaderUtils.java
Patch:
@@ -22,6 +22,7 @@ static DocumentParser parserFor(DocumentType type) {
         switch (type) {
             case TXT:
             case HTML:
+            case UNKNOWN:
                 return new TextDocumentParser(type);
             case PDF:
                 return new PdfDocumentParser();

File: langchain4j/src/main/java/dev/langchain4j/data/document/parser/PdfDocumentParser.java
Patch:
@@ -2,14 +2,15 @@
 
 import dev.langchain4j.data.document.Document;
 import dev.langchain4j.data.document.DocumentParser;
+import dev.langchain4j.data.document.Metadata;
 import org.apache.pdfbox.pdmodel.PDDocument;
 import org.apache.pdfbox.text.PDFTextStripper;
 
 import java.io.IOException;
 import java.io.InputStream;
 
+import static dev.langchain4j.data.document.Document.DOCUMENT_TYPE;
 import static dev.langchain4j.data.document.DocumentType.PDF;
-import static dev.langchain4j.data.document.Metadata.metadata;
 
 public class PdfDocumentParser implements DocumentParser {
 
@@ -20,7 +21,7 @@ public Document parse(InputStream inputStream) {
             PDFTextStripper stripper = new PDFTextStripper();
             String content = stripper.getText(pdfDocument);
             pdfDocument.close();
-            return Document.from(content, metadata(DOCUMENT_TYPE, PDF));
+            return Document.from(content, Metadata.from(DOCUMENT_TYPE, PDF));
         } catch (IOException e) {
             throw new RuntimeException(e);
         }

File: langchain4j/src/main/java/dev/langchain4j/data/document/parser/TextDocumentParser.java
Patch:
@@ -9,6 +9,7 @@
 import java.io.InputStream;
 import java.nio.charset.Charset;
 
+import static dev.langchain4j.data.document.Document.DOCUMENT_TYPE;
 import static dev.langchain4j.internal.ValidationUtils.ensureNotNull;
 import static java.nio.charset.StandardCharsets.UTF_8;
 
@@ -39,7 +40,7 @@ public Document parse(InputStream inputStream) {
 
             String text = new String(buffer.toByteArray(), charset);
 
-            return Document.from(text, new Metadata().add(DOCUMENT_TYPE, documentType.toString()));
+            return Document.from(text, Metadata.from(DOCUMENT_TYPE, documentType.toString()));
         } catch (Exception e) {
             throw new RuntimeException(e);
         }

File: langchain4j/src/main/java/dev/langchain4j/classification/EmbeddingModelTextClassifier.java
Patch:
@@ -35,7 +35,7 @@
  *     NEGATIVE, List.of("It is pretty bad", "Worst experience ever!")
  * );
  *
- * EmbeddingModel embeddingModel = new InProcessEmbeddingModel(ALL_MINILM_L6_V2_Q);
+ * EmbeddingModel embeddingModel = new AllMiniLmL6V2QuantizedEmbeddingModel();
  *
  * TextClassifier<Sentiment> classifier = new EmbeddingModelTextClassifier<>(embeddingModel, examples);
  *

File: langchain4j/src/main/java/dev/langchain4j/store/embedding/elasticsearch/ElasticsearchEmbeddingStore.java
Patch:
@@ -44,10 +44,10 @@ private static String getMessage() {
                 + "<dependency>\n" +
                 "    <groupId>dev.langchain4j</groupId>\n" +
                 "    <artifactId>langchain4j-elasticsearch</artifactId>\n" +
-                "    <version>0.20.0</version>\n" +
+                "    <version>0.22.0</version>\n" +
                 "</dependency>\n\n"
                 + "Gradle:\n"
-                + "implementation 'dev.langchain4j:langchain4j-elasticsearch:0.20.0'\n";
+                + "implementation 'dev.langchain4j:langchain4j-elasticsearch:0.22.0'\n";
     }
 
     private static EmbeddingStore<TextSegment> loadDynamically(String implementationClassName, String serverUrl, String username, String password, String apiKey, String indexName) throws ClassNotFoundException, NoSuchMethodException, InstantiationException, IllegalAccessException, InvocationTargetException {

File: langchain4j-pinecone/src/main/java/dev/langchain4j/store/embedding/pinecone/PineconeEmbeddingStoreImpl.java
Patch:
@@ -4,6 +4,7 @@
 import com.google.protobuf.Value;
 import dev.langchain4j.data.embedding.Embedding;
 import dev.langchain4j.data.segment.TextSegment;
+import dev.langchain4j.store.embedding.CosineSimilarity;
 import dev.langchain4j.store.embedding.EmbeddingMatch;
 import dev.langchain4j.store.embedding.EmbeddingStore;
 import dev.langchain4j.store.embedding.RelevanceScore;
@@ -176,9 +177,10 @@ private static EmbeddingMatch<TextSegment> toEmbeddingMatch(Vector vector, Embed
                 .get(METADATA_TEXT_SEGMENT);
 
         Embedding embedding = Embedding.from(vector.getValuesList());
+        double cosineSimilarity = CosineSimilarity.between(embedding, referenceEmbedding);
 
         return new EmbeddingMatch<>(
-                RelevanceScore.cosine(embedding.vector(), referenceEmbedding.vector()),
+                RelevanceScore.fromCosineSimilarity(cosineSimilarity),
                 vector.getId(),
                 embedding,
                 textSegmentValue == null ? null : TextSegment.from(textSegmentValue.getStringValue())

File: langchain4j/src/main/java/dev/langchain4j/service/AiServiceStreamingResponseHandler.java
Patch:
@@ -93,10 +93,9 @@ public void onComplete() {
 
             context.chatMemory(memoryId).add(toolExecutionResultMessage);
 
-            // This time, tools are not sent because, at this point, the LLM cannot call another tool; it should respond to the user.
-            // This is the current behavior of OpenAI, though it might change in the future.
             context.streamingChatLanguageModel.sendMessages(
                     context.chatMemory(memoryId).messages(),
+                    context.toolSpecifications,
                     new AiServiceStreamingResponseHandler(context, memoryId, tokenHandler, completionHandler, errorHandler)
             );
         }

File: langchain4j/src/main/java/dev/langchain4j/model/azure/AzureOpenAiStreamingChatModel.java
Patch:
@@ -78,7 +78,7 @@ public AzureOpenAiStreamingChatModel(String baseUrl,
                 .writeTimeout(timeout)
                 .proxy(proxy)
                 .logRequests(logRequests)
-                .logResponses(logResponses)
+                .logStreamingResponses(logResponses)
                 .build();
         this.temperature = temperature;
         this.topP = topP;

File: langchain4j/src/main/java/dev/langchain4j/model/azure/AzureOpenAiStreamingLanguageModel.java
Patch:
@@ -64,7 +64,7 @@ public AzureOpenAiStreamingLanguageModel(String baseUrl,
                 .writeTimeout(timeout)
                 .proxy(proxy)
                 .logRequests(logRequests)
-                .logResponses(logResponses)
+                .logStreamingResponses(logResponses)
                 .build();
         this.temperature = temperature;
         this.tokenizer = tokenizer;

File: langchain4j/src/main/java/dev/langchain4j/model/localai/LocalAiStreamingChatModel.java
Patch:
@@ -49,7 +49,7 @@ public LocalAiStreamingChatModel(String baseUrl,
                 .readTimeout(timeout)
                 .writeTimeout(timeout)
                 .logRequests(logRequests)
-                .logResponses(logResponses)
+                .logStreamingResponses(logResponses)
                 .build();
         this.modelName = ensureNotBlank(modelName, "modelName");
         this.temperature = temperature;

File: langchain4j/src/main/java/dev/langchain4j/model/localai/LocalAiStreamingLanguageModel.java
Patch:
@@ -40,7 +40,7 @@ public LocalAiStreamingLanguageModel(String baseUrl,
                 .readTimeout(timeout)
                 .writeTimeout(timeout)
                 .logRequests(logRequests)
-                .logResponses(logResponses)
+                .logStreamingResponses(logResponses)
                 .build();
         this.modelName = ensureNotBlank(modelName, "modelName");
         this.temperature = temperature;

File: langchain4j/src/main/java/dev/langchain4j/model/openai/OpenAiStreamingChatModel.java
Patch:
@@ -67,7 +67,7 @@ public OpenAiStreamingChatModel(String baseUrl,
                 .writeTimeout(timeout)
                 .proxy(proxy)
                 .logRequests(logRequests)
-                .logResponses(logResponses)
+                .logStreamingResponses(logResponses)
                 .build();
         this.modelName = modelName;
         this.temperature = temperature;

File: langchain4j/src/main/java/dev/langchain4j/model/openai/OpenAiStreamingLanguageModel.java
Patch:
@@ -52,7 +52,7 @@ public OpenAiStreamingLanguageModel(String baseUrl,
                 .writeTimeout(timeout)
                 .proxy(proxy)
                 .logRequests(logRequests)
-                .logResponses(logResponses)
+                .logStreamingResponses(logResponses)
                 .build();
         this.modelName = modelName;
         this.temperature = temperature;

File: langchain4j/src/main/java/dev/langchain4j/data/document/UrlDocumentLoader.java
Patch:
@@ -5,7 +5,6 @@
 import java.net.MalformedURLException;
 import java.net.URL;
 
-import static dev.langchain4j.data.document.DocumentLoaderUtils.detectDocumentType;
 import static dev.langchain4j.data.document.DocumentLoaderUtils.parserFor;
 
 public class UrlDocumentLoader {
@@ -18,7 +17,7 @@ public class UrlDocumentLoader {
      * @throws UnsupportedDocumentTypeException if document type is not supported or cannot be detected automatically
      */
     public static Document load(URL url) {
-        return load(url, detectDocumentType(url.toString()));
+        return load(url, DocumentType.of(url.toString()));
     }
 
     /**

File: langchain4j/src/main/java/dev/langchain4j/model/openai/OpenAiTokenizer.java
Patch:
@@ -30,7 +30,7 @@ public int countTokens(String text) {
     @Override
     public int countTokens(ChatMessage message) {
         return extraTokensPerEachMessage()
-                + countTokens(message.text())
+                + countTokens(message.text()) // TODO count functions
                 + countTokens(roleFrom(message).toString());
     }
 

File: langchain4j/src/main/java/dev/langchain4j/service/ServiceOutputParser.java
Patch:
@@ -84,7 +84,9 @@ static Object parse(AiMessage aiMessage, Class<?> returnType) {
 
     static String outputFormatInstructions(Class<?> returnType) {
 
-        if (returnType == String.class || returnType == AiMessage.class) {
+        if (returnType == String.class
+                || returnType == AiMessage.class
+                || returnType == TokenStream.class) {
             return "";
         }
 

File: langchain4j/src/test/java/dev/langchain4j/agent/tool/ToolExecutorTest.java
Patch:
@@ -219,7 +219,7 @@ private void executeAndAssert(String arguments, String methodName, Class<?> arg0
 
         ToolExecutor toolExecutor = new ToolExecutor(testTool, TestTool.class.getDeclaredMethod(methodName, arg0Type, arg1Type));
 
-        String result = toolExecutor.execute(request.argumentsAsMap());
+        String result = toolExecutor.execute(request);
 
         assertThat(result).isEqualTo(expectedResult);
     }
@@ -231,7 +231,7 @@ private void executeAndExpectFailure(String arguments, String methodName, Class<
 
         ToolExecutor toolExecutor = new ToolExecutor(testTool, TestTool.class.getDeclaredMethod(methodName, arg0Type, arg1Type));
 
-        assertThatThrownBy(() -> toolExecutor.execute(request.argumentsAsMap()))
+        assertThatThrownBy(() -> toolExecutor.execute(request))
                 .isExactlyInstanceOf(IllegalArgumentException.class)
                 .hasMessageContaining(expectedError);
     }

File: langchain4j/src/main/java/dev/langchain4j/service/AiServices.java
Patch:
@@ -334,8 +334,9 @@ private static void validateParameters(Method method) {
         for (Parameter parameter : parameters) {
             V v = parameter.getAnnotation(V.class);
             UserMessage userMessage = parameter.getAnnotation(UserMessage.class);
-            if (v == null && userMessage == null) {
-                throw illegalConfiguration("Parameter '%s' of method '%s' should be annotated either with @V or @UserMessage", parameter.getName(), method.getName());
+            UserName userName = parameter.getAnnotation(UserName.class);
+            if (v == null && userMessage == null && userName == null) {
+                throw illegalConfiguration("Parameter '%s' of method '%s' should be annotated either with @V or @UserMessage or @UserName", parameter.getName(), method.getName());
             }
         }
     }

File: langchain4j-core/src/main/java/dev/langchain4j/agent/tool/JsonSchemaProperty.java
Patch:
@@ -5,6 +5,7 @@
 public class JsonSchemaProperty {
 
     public static final JsonSchemaProperty STRING = type("string");
+    public static final JsonSchemaProperty INTEGER = type("integer");
     public static final JsonSchemaProperty NUMBER = type("number");
     public static final JsonSchemaProperty OBJECT = type("object");
     public static final JsonSchemaProperty ARRAY = type("array");

File: langchain4j-core/src/main/java/dev/langchain4j/model/chat/TokenCountEstimator.java
Patch:
@@ -1,7 +1,7 @@
 package dev.langchain4j.model.chat;
 
 import dev.langchain4j.MightChangeInTheFuture;
-import dev.langchain4j.data.document.DocumentSegment;
+import dev.langchain4j.data.segment.TextSegment;
 import dev.langchain4j.data.message.ChatMessage;
 import dev.langchain4j.data.message.UserMessage;
 import dev.langchain4j.model.input.Prompt;
@@ -22,5 +22,5 @@ public interface TokenCountEstimator {
 
     int estimateTokenCount(List<ChatMessage> messages);
 
-    int estimateTokenCount(DocumentSegment documentSegment);
+    int estimateTokenCount(TextSegment textSegment);
 }

File: langchain4j-core/src/main/java/dev/langchain4j/model/embedding/EmbeddingModel.java
Patch:
@@ -1,6 +1,6 @@
 package dev.langchain4j.model.embedding;
 
-import dev.langchain4j.data.document.DocumentSegment;
+import dev.langchain4j.data.segment.TextSegment;
 import dev.langchain4j.data.embedding.Embedding;
 import dev.langchain4j.model.output.Result;
 
@@ -10,7 +10,7 @@ public interface EmbeddingModel {
 
     Result<Embedding> embed(String text);
 
-    Result<Embedding> embed(DocumentSegment documentSegment);
+    Result<Embedding> embed(TextSegment textSegment);
 
-    Result<List<Embedding>> embedAll(List<DocumentSegment> documentSegments);
+    Result<List<Embedding>> embedAll(List<TextSegment> textSegments);
 }
\ No newline at end of file

File: langchain4j-core/src/main/java/dev/langchain4j/model/embedding/TokenCountEstimator.java
Patch:
@@ -1,14 +1,14 @@
 package dev.langchain4j.model.embedding;
 
-import dev.langchain4j.data.document.DocumentSegment;
+import dev.langchain4j.data.segment.TextSegment;
 
 import java.util.List;
 
 public interface TokenCountEstimator {
 
     int estimateTokenCount(String text);
 
-    int estimateTokenCount(DocumentSegment documentSegment);
+    int estimateTokenCount(TextSegment textSegment);
 
-    int estimateTokenCount(List<DocumentSegment> documentSegments);
+    int estimateTokenCount(List<TextSegment> textSegments);
 }
\ No newline at end of file

File: langchain4j-core/src/main/java/dev/langchain4j/model/language/TokenCountEstimator.java
Patch:
@@ -1,6 +1,6 @@
 package dev.langchain4j.model.language;
 
-import dev.langchain4j.data.document.DocumentSegment;
+import dev.langchain4j.data.segment.TextSegment;
 import dev.langchain4j.model.input.Prompt;
 
 public interface TokenCountEstimator {
@@ -11,5 +11,5 @@ public interface TokenCountEstimator {
 
     int estimateTokenCount(Object structuredPrompt);
 
-    int estimateTokenCount(DocumentSegment documentSegment);
+    int estimateTokenCount(TextSegment textSegment);
 }

File: langchain4j-core/src/main/java/dev/langchain4j/model/moderation/ModerationModel.java
Patch:
@@ -1,6 +1,6 @@
 package dev.langchain4j.model.moderation;
 
-import dev.langchain4j.data.document.DocumentSegment;
+import dev.langchain4j.data.segment.TextSegment;
 import dev.langchain4j.data.message.ChatMessage;
 import dev.langchain4j.model.input.Prompt;
 import dev.langchain4j.model.output.Result;
@@ -19,5 +19,5 @@ public interface ModerationModel {
 
     Result<Moderation> moderate(List<ChatMessage> messages);
 
-    Result<Moderation> moderate(DocumentSegment documentSegment);
+    Result<Moderation> moderate(TextSegment textSegment);
 }

File: langchain4j/src/main/java/dev/langchain4j/data/document/parser/PdfDocumentParser.java
Patch:
@@ -2,7 +2,6 @@
 
 import dev.langchain4j.data.document.Document;
 import dev.langchain4j.data.document.DocumentParser;
-import dev.langchain4j.data.document.Metadata;
 import org.apache.pdfbox.pdmodel.PDDocument;
 import org.apache.pdfbox.text.PDFTextStripper;
 
@@ -18,7 +17,7 @@ public Document parse(InputStream inputStream) {
             PDFTextStripper stripper = new PDFTextStripper();
             String content = stripper.getText(pdfDocument);
             pdfDocument.close();
-            return new Document(content, new Metadata());
+            return Document.from(content);
         } catch (IOException e) {
             throw new RuntimeException(e);
         }

File: langchain4j/src/main/java/dev/langchain4j/data/document/parser/TextDocumentParser.java
Patch:
@@ -14,7 +14,7 @@ public class TextDocumentParser implements DocumentParser {
     private final Charset charset;
 
     public TextDocumentParser() {
-        this.charset = UTF_8;
+        this(UTF_8);
     }
 
     public TextDocumentParser(Charset charset) {

File: langchain4j/src/main/java/dev/langchain4j/data/document/splitter/ParagraphSplitter.java
Patch:
@@ -1,7 +1,7 @@
 package dev.langchain4j.data.document.splitter;
 
 import dev.langchain4j.data.document.Document;
-import dev.langchain4j.data.document.DocumentSegment;
+import dev.langchain4j.data.segment.TextSegment;
 import dev.langchain4j.data.document.DocumentSplitter;
 
 import java.util.List;
@@ -12,7 +12,7 @@
 public class ParagraphSplitter implements DocumentSplitter {
 
     @Override
-    public List<DocumentSegment> split(Document document) {
+    public List<TextSegment> split(Document document) {
         String text = document.text();
         if (text == null || text.isEmpty()) {
             throw new IllegalArgumentException("Document text should not be null or empty");
@@ -21,7 +21,7 @@ public List<DocumentSegment> split(Document document) {
         String[] paragraphs = text.split("\\R\\R");
 
         return stream(paragraphs)
-                .map(paragraph -> DocumentSegment.from(paragraph.trim(), document.metadata()))
+                .map(paragraph -> TextSegment.from(paragraph.trim(), document.metadata()))
                 .collect(toList());
     }
 }

File: langchain4j/src/main/java/dev/langchain4j/data/document/splitter/RegexSplitter.java
Patch:
@@ -1,7 +1,7 @@
 package dev.langchain4j.data.document.splitter;
 
 import dev.langchain4j.data.document.Document;
-import dev.langchain4j.data.document.DocumentSegment;
+import dev.langchain4j.data.segment.TextSegment;
 import dev.langchain4j.data.document.DocumentSplitter;
 
 import java.util.List;
@@ -18,7 +18,7 @@ public RegexSplitter(String regex) {
     }
 
     @Override
-    public List<DocumentSegment> split(Document document) {
+    public List<TextSegment> split(Document document) {
         String text = document.text();
         if (text == null || text.isEmpty()) {
             throw new IllegalArgumentException("Document text should not be null or empty");
@@ -27,7 +27,7 @@ public List<DocumentSegment> split(Document document) {
         String[] segments = text.split(regex);
 
         return stream(segments)
-                .map(segment -> DocumentSegment.from(segment, document.metadata()))
+                .map(segment -> TextSegment.from(segment, document.metadata()))
                 .collect(toList());
     }
 }

File: langchain4j/src/main/java/dev/langchain4j/data/document/splitter/SentenceSplitter.java
Patch:
@@ -1,7 +1,7 @@
 package dev.langchain4j.data.document.splitter;
 
 import dev.langchain4j.data.document.Document;
-import dev.langchain4j.data.document.DocumentSegment;
+import dev.langchain4j.data.segment.TextSegment;
 import dev.langchain4j.data.document.DocumentSplitter;
 
 import java.text.BreakIterator;
@@ -14,7 +14,7 @@
 public class SentenceSplitter implements DocumentSplitter {
 
     @Override
-    public List<DocumentSegment> split(Document document) {
+    public List<TextSegment> split(Document document) {
         String text = document.text();
         if (text == null || text.isEmpty()) {
             throw new IllegalArgumentException("Document text should not be null or empty");
@@ -23,7 +23,7 @@ public List<DocumentSegment> split(Document document) {
         List<String> sentences = splitIntoSentences(text);
 
         return sentences.stream()
-                .map(sentence -> DocumentSegment.from(sentence.trim(), document.metadata()))
+                .map(sentence -> TextSegment.from(sentence.trim(), document.metadata()))
                 .collect(toList());
     }
 

File: langchain4j/src/main/java/dev/langchain4j/model/openai/OpenAiLanguageModel.java
Patch:
@@ -3,7 +3,7 @@
 import dev.ai4j.openai4j.OpenAiClient;
 import dev.ai4j.openai4j.completion.CompletionRequest;
 import dev.ai4j.openai4j.completion.CompletionResponse;
-import dev.langchain4j.data.document.DocumentSegment;
+import dev.langchain4j.data.segment.TextSegment;
 import dev.langchain4j.model.input.Prompt;
 import dev.langchain4j.model.language.LanguageModel;
 import dev.langchain4j.model.language.TokenCountEstimator;
@@ -86,8 +86,8 @@ public int estimateTokenCount(Object structuredPrompt) {
     }
 
     @Override
-    public int estimateTokenCount(DocumentSegment documentSegment) {
-        return estimateTokenCount(documentSegment.text());
+    public int estimateTokenCount(TextSegment textSegment) {
+        return estimateTokenCount(textSegment.text());
     }
 
     public static OpenAiLanguageModel withApiKey(String apiKey) {

File: langchain4j/src/main/java/dev/langchain4j/model/openai/OpenAiModerationModel.java
Patch:
@@ -4,7 +4,7 @@
 import dev.ai4j.openai4j.moderation.ModerationRequest;
 import dev.ai4j.openai4j.moderation.ModerationResponse;
 import dev.ai4j.openai4j.moderation.ModerationResult;
-import dev.langchain4j.data.document.DocumentSegment;
+import dev.langchain4j.data.segment.TextSegment;
 import dev.langchain4j.data.message.ChatMessage;
 import dev.langchain4j.model.input.Prompt;
 import dev.langchain4j.model.moderation.Moderation;
@@ -95,8 +95,8 @@ public Result<Moderation> moderate(List<ChatMessage> messages) {
     }
 
     @Override
-    public Result<Moderation> moderate(DocumentSegment documentSegment) {
-        return moderate(documentSegment.text());
+    public Result<Moderation> moderate(TextSegment textSegment) {
+        return moderate(textSegment.text());
     }
 
     public static OpenAiModerationModel withApiKey(String apiKey) {

File: langchain4j/src/main/java/dev/langchain4j/model/openai/OpenAiStreamingChatModel.java
Patch:
@@ -2,7 +2,7 @@
 
 import dev.ai4j.openai4j.OpenAiClient;
 import dev.ai4j.openai4j.chat.ChatCompletionRequest;
-import dev.langchain4j.data.document.DocumentSegment;
+import dev.langchain4j.data.segment.TextSegment;
 import dev.langchain4j.data.message.ChatMessage;
 import dev.langchain4j.data.message.UserMessage;
 import dev.langchain4j.model.StreamingResultHandler;
@@ -118,8 +118,8 @@ public int estimateTokenCount(List<ChatMessage> messages) {
     }
 
     @Override
-    public int estimateTokenCount(DocumentSegment documentSegment) {
-        return estimateTokenCount(documentSegment.text());
+    public int estimateTokenCount(TextSegment textSegment) {
+        return estimateTokenCount(textSegment.text());
     }
 
     public static OpenAiStreamingChatModel withApiKey(String apiKey) {

File: langchain4j/src/main/java/dev/langchain4j/model/openai/OpenAiStreamingLanguageModel.java
Patch:
@@ -2,7 +2,7 @@
 
 import dev.ai4j.openai4j.OpenAiClient;
 import dev.ai4j.openai4j.completion.CompletionRequest;
-import dev.langchain4j.data.document.DocumentSegment;
+import dev.langchain4j.data.segment.TextSegment;
 import dev.langchain4j.model.StreamingResultHandler;
 import dev.langchain4j.model.input.Prompt;
 import dev.langchain4j.model.language.StreamingLanguageModel;
@@ -91,8 +91,8 @@ public int estimateTokenCount(Object structuredPrompt) {
     }
 
     @Override
-    public int estimateTokenCount(DocumentSegment documentSegment) {
-        return estimateTokenCount(documentSegment.text());
+    public int estimateTokenCount(TextSegment textSegment) {
+        return estimateTokenCount(textSegment.text());
     }
 
     public static OpenAiStreamingLanguageModel withApiKey(String apiKey) {

File: langchain4j/src/main/java/dev/langchain4j/service/ToolSpecifications.java
Patch:
@@ -64,7 +64,7 @@ private static Iterable<JsonSchemaProperty> toJsonSchemaProperties(Parameter par
                 || type == double.class
                 || type == Double.class // TODO bigdecimal, etc
         ) {
-            return removeNulls(NUMBER, description);
+            return removeNulls(NUMBER, description); // TODO test all types!
         }
 
         if (type.isArray()

File: langchain4j/src/test/java/dev/langchain4j/data/document/parser/TextDocumentParserTest.java
Patch:
@@ -13,6 +13,7 @@
 class TextDocumentParserTest {
 
     @Test
+        // TODO This test fails when running it directly in IDE, but works when running in maven
     void should_parse_with_utf8_charset_by_default() throws IOException {
 
         FileSystemSource source = FileSystemSource.from(Paths.get("src/test/resources/test-file-utf8.txt"));
@@ -24,6 +25,7 @@ void should_parse_with_utf8_charset_by_default() throws IOException {
     }
 
     @Test
+        // TODO This test fails when running it directly in IDE, but works when running in maven
     void should_parse_with_specified_charset() throws IOException {
 
         FileSystemSource source = FileSystemSource.from(Paths.get("src/test/resources/test-file-iso-8859-1.txt"));

File: langchain4j/src/main/java/dev/langchain4j/store/embedding/PineconeEmbeddingStore.java
Patch:
@@ -29,10 +29,10 @@ private static String getMessage() {
                 + "<dependency>\n" +
                 "    <groupId>dev.langchain4j</groupId>\n" +
                 "    <artifactId>langchain4j-pinecone</artifactId>\n" +
-                "    <version>0.11.0</version>\n" +
+                "    <version>0.12.0</version>\n" +
                 "</dependency>\n\n"
                 + "Gradle:\n"
-                + "implementation 'dev.langchain4j:langchain4j-pinecone:0.11.0'\n";
+                + "implementation 'dev.langchain4j:langchain4j-pinecone:0.12.0'\n";
     }
 
     private static EmbeddingStore<DocumentSegment> loadDynamically(String implementationClassName, String apiKey, String environment, String projectName, String index, String nameSpace) throws ClassNotFoundException, NoSuchMethodException, InstantiationException, IllegalAccessException, InvocationTargetException {

File: langchain4j/src/main/java/dev/langchain4j/chain/ConversationalRetrievalChain.java
Patch:
@@ -28,7 +28,7 @@ public class ConversationalRetrievalChain implements Chain<String, String> {
 
     private static final DocumentSplitter DEFAULT_DOCUMENT_SPLITTER = new ParagraphSplitter();
     private static final EmbeddingStore<DocumentSegment> DEFAULT_EMBEDDING_STORE = new InMemoryEmbeddingStore();
-    private static final PromptTemplate DEFAULT_PROMPT_TEMPLATE = new PromptTemplate("Answer the following question to the best of your ability: {{question}}\n\nBase your answer on the following information:\n{{information}}");
+    private static final PromptTemplate DEFAULT_PROMPT_TEMPLATE = PromptTemplate.from("Answer the following question to the best of your ability: {{question}}\n\nBase your answer on the following information:\n{{information}}");
 
     private final DocumentLoader documentLoader;
     private final DocumentSplitter documentSplitter;

File: langchain4j/src/test/java/dev/langchain4j/model/huggingface/HuggingFaceChatModelIT.java
Patch:
@@ -12,7 +12,7 @@
 import static org.assertj.core.api.Assertions.assertThat;
 import static org.junit.jupiter.api.Assertions.assertThrows;
 
-class HuggingFaceChatModelTest {
+class HuggingFaceChatModelIT {
 
     @Test
     public void testWhenNullAccessToken() {

File: langchain4j/src/test/java/dev/langchain4j/model/huggingface/HuggingFaceEmbeddingModelIT.java
Patch:
@@ -10,7 +10,7 @@
 import static java.util.Arrays.asList;
 import static org.assertj.core.api.Assertions.assertThat;
 
-class HuggingFaceEmbeddingModelTest {
+class HuggingFaceEmbeddingModelIT {
 
     HuggingFaceEmbeddingModel model = HuggingFaceEmbeddingModel.builder()
             .accessToken(System.getenv("HF_API_KEY"))

File: langchain4j/src/test/java/dev/langchain4j/model/huggingface/HuggingFaceLanguageModelIT.java
Patch:
@@ -6,7 +6,7 @@
 import static org.assertj.core.api.Assertions.assertThat;
 import static org.junit.jupiter.api.Assertions.assertThrows;
 
-class HuggingFaceLanguageModelTest {
+class HuggingFaceLanguageModelIT {
 
     @Test
     public void testWhenNullAccessToken() {

File: langchain4j/src/test/java/dev/langchain4j/model/openai/OpenAiModerationModelIT.java
Patch:
@@ -6,7 +6,7 @@
 
 import static org.assertj.core.api.Assertions.assertThat;
 
-class OpenAiModerationModelTest {
+class OpenAiModerationModelIT {
 
     ModerationModel model = OpenAiModerationModel.builder()
             .apiKey(System.getenv("OPENAI_API_KEY"))

File: langchain4j/src/test/java/dev/langchain4j/service/AiServicesIT.java
Patch:
@@ -27,15 +27,15 @@
 import static dev.langchain4j.data.message.AiMessage.aiMessage;
 import static dev.langchain4j.data.message.SystemMessage.systemMessage;
 import static dev.langchain4j.data.message.UserMessage.userMessage;
-import static dev.langchain4j.service.AiServicesTest.Sentiment.POSITIVE;
+import static dev.langchain4j.service.AiServicesIT.Sentiment.POSITIVE;
 import static java.time.Month.JULY;
 import static java.util.Collections.singletonList;
 import static org.assertj.core.api.Assertions.assertThat;
 import static org.assertj.core.api.Assertions.assertThatThrownBy;
 import static org.mockito.Mockito.*;
 
 @ExtendWith(MockitoExtension.class)
-public class AiServicesTest {
+public class AiServicesIT {
 
     @Spy
     ChatLanguageModel chatLanguageModel = OpenAiChatModel.builder()

File: langchain4j/src/main/java/dev/langchain4j/data/document/DocumentLoader.java
Patch:
@@ -103,6 +103,6 @@ private static DocumentType detectDocumentType(String uri) {
             return DocumentType.PDF;
         }
 
-        throw new RuntimeException("Cannot automatically detect document type for: " + uri);
+        throw new RuntimeException("Cannot automatically detect the document type for '" + uri + "'. Please provide the document type explicitly.");
     }
 }

File: langchain4j/src/main/java/dev/langchain4j/memory/chat/MessageWindowChatMemory.java
Patch:
@@ -16,7 +16,7 @@
 
 public class MessageWindowChatMemory implements ChatMemory {
 
-    private static final Logger log = LoggerFactory.getLogger(TokenWindowChatMemory.class);
+    private static final Logger log = LoggerFactory.getLogger(MessageWindowChatMemory.class);
 
     private final Optional<SystemMessage> maybeSystemMessage;
     private final LinkedList<ChatMessage> previousMessages;

File: langchain4j/src/main/java/dev/langchain4j/model/huggingface/HuggingFaceChatModel.java
Patch:
@@ -111,7 +111,7 @@ public static final class Builder {
 
         private String accessToken;
         private String modelId = TII_UAE_FALCON_7B_INSTRUCT;
-        private Duration timeout = Duration.ofSeconds(60);
+        private Duration timeout = Duration.ofSeconds(15);
         private Double temperature;
         private Integer maxNewTokens;
         private Boolean returnFullText = false;

File: langchain4j/src/main/java/dev/langchain4j/model/huggingface/HuggingFaceLanguageModel.java
Patch:
@@ -82,7 +82,7 @@ public static final class Builder {
 
         private String accessToken;
         private String modelId = TII_UAE_FALCON_7B_INSTRUCT;
-        private Duration timeout = Duration.ofSeconds(60);
+        private Duration timeout = Duration.ofSeconds(15);
         private Double temperature;
         private Integer maxNewTokens;
         private Boolean returnFullText = false;

File: langchain4j/src/main/java/dev/langchain4j/model/openai/OpenAiChatModel.java
Patch:
@@ -28,7 +28,7 @@
 public class OpenAiChatModel implements ChatLanguageModel, TokenCountEstimator {
 
     private static final double DEFAULT_TEMPERATURE = 0.7;
-    private static final Duration DEFAULT_TIMEOUT = Duration.ofSeconds(60);
+    private static final Duration DEFAULT_TIMEOUT = Duration.ofSeconds(15);
 
     private final OpenAiClient client;
     private final String modelName;

File: langchain4j/src/main/java/dev/langchain4j/model/openai/OpenAiEmbeddingModel.java
Patch:
@@ -19,7 +19,7 @@
 
 public class OpenAiEmbeddingModel implements EmbeddingModel, TokenCountEstimator {
 
-    private static final Duration DEFAULT_TIMEOUT = Duration.ofSeconds(60);
+    private static final Duration DEFAULT_TIMEOUT = Duration.ofSeconds(15);
 
     private final OpenAiClient client;
     private final String modelName;

File: langchain4j/src/main/java/dev/langchain4j/model/openai/OpenAiLanguageModel.java
Patch:
@@ -18,7 +18,7 @@
 public class OpenAiLanguageModel implements LanguageModel, TokenCountEstimator {
 
     private static final double DEFAULT_TEMPERATURE = 0.7;
-    private static final Duration DEFAULT_TIMEOUT = Duration.ofSeconds(60);
+    private static final Duration DEFAULT_TIMEOUT = Duration.ofSeconds(15);
 
     private final OpenAiClient client;
     private final String modelName;

File: langchain4j/src/main/java/dev/langchain4j/model/openai/OpenAiModerationModel.java
Patch:
@@ -22,7 +22,7 @@
 
 public class OpenAiModerationModel implements ModerationModel {
 
-    private static final Duration DEFAULT_TIMEOUT = Duration.ofSeconds(60);
+    private static final Duration DEFAULT_TIMEOUT = Duration.ofSeconds(15);
 
     private final OpenAiClient client;
     private final String modelName;

File: langchain4j/src/main/java/dev/langchain4j/model/openai/OpenAiStreamingChatModel.java
Patch:
@@ -22,7 +22,7 @@
 public class OpenAiStreamingChatModel implements StreamingChatLanguageModel, TokenCountEstimator {
 
     private static final double DEFAULT_TEMPERATURE = 0.7;
-    private static final Duration DEFAULT_TIMEOUT = Duration.ofSeconds(60);
+    private static final Duration DEFAULT_TIMEOUT = Duration.ofSeconds(15);
 
     private final OpenAiClient client;
     private final String modelName;

File: langchain4j/src/main/java/dev/langchain4j/model/openai/OpenAiStreamingLanguageModel.java
Patch:
@@ -17,7 +17,7 @@
 public class OpenAiStreamingLanguageModel implements StreamingLanguageModel, TokenCountEstimator {
 
     private static final double DEFAULT_TEMPERATURE = 0.7;
-    private static final Duration DEFAULT_TIMEOUT = Duration.ofSeconds(60);
+    private static final Duration DEFAULT_TIMEOUT = Duration.ofSeconds(15);
 
     private final OpenAiClient client;
     private final String modelName;

File: langchain4j/src/test/java/dev/langchain4j/model/huggingface/HuggingFaceChatModelTest.java
Patch:
@@ -37,7 +37,7 @@ public void testSendUserMessageString() {
         HuggingFaceChatModel model = HuggingFaceChatModel.builder()
                 .accessToken(System.getenv("HF_API_KEY"))
                 .modelId(TII_UAE_FALCON_7B_INSTRUCT)
-                .timeout(Duration.ofSeconds(60))
+                .timeout(Duration.ofSeconds(15))
                 .temperature(0.7)
                 .maxNewTokens(20)
                 .waitForModel(true)

File: langchain4j/src/test/java/dev/langchain4j/data/document/DocumentLoaderTest.java
Patch:
@@ -14,7 +14,7 @@ class DocumentLoaderTest {
 
     @Test
     void should_load_text_document_from_file_system() {
-        DocumentLoader loader = DocumentLoader.from(Paths.get("langchain4j/src/test/resources/test-file-utf8.txt"), TEXT);
+        DocumentLoader loader = DocumentLoader.from(Paths.get("src/test/resources/test-file-utf8.txt"), TEXT);
 
 
         Document document = loader.load();
@@ -44,7 +44,7 @@ void should_load_text_document_from_url() throws MalformedURLException {
 
     @Test
     void should_load_pdf_document_from_file_system() {
-        DocumentLoader loader = DocumentLoader.from(Paths.get("langchain4j/src/test/resources/test-file.pdf"), PDF);
+        DocumentLoader loader = DocumentLoader.from(Paths.get("src/test/resources/test-file.pdf"), PDF);
 
 
         Document document = loader.load();

File: langchain4j/src/test/java/dev/langchain4j/data/document/parser/TextDocumentParserTest.java
Patch:
@@ -15,7 +15,7 @@ class TextDocumentParserTest {
     @Test
     void should_parse_with_utf8_charset_by_default() throws IOException {
 
-        FileSystemSource source = FileSystemSource.from(Paths.get("langchain4j/src/test/resources/test-file-utf8.txt"));
+        FileSystemSource source = FileSystemSource.from(Paths.get("src/test/resources/test-file-utf8.txt"));
         TextDocumentParser parser = new TextDocumentParser();
 
         Document document = parser.parse(source.inputStream());
@@ -26,7 +26,7 @@ void should_parse_with_utf8_charset_by_default() throws IOException {
     @Test
     void should_parse_with_specified_charset() throws IOException {
 
-        FileSystemSource source = FileSystemSource.from(Paths.get("langchain4j/src/test/resources/test-file-iso-8859-1.txt"));
+        FileSystemSource source = FileSystemSource.from(Paths.get("src/test/resources/test-file-iso-8859-1.txt"));
         TextDocumentParser parser = new TextDocumentParser(ISO_8859_1);
 
         Document document = parser.parse(source.inputStream());

