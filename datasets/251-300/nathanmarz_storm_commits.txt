File: storm-core/src/jvm/backtype/storm/metric/SystemBolt.java
Patch:
@@ -74,7 +74,7 @@ public Object getValueAndReset() {
 
     @Override
     public void prepare(final Map stormConf, TopologyContext context, OutputCollector collector) {
-        if(_prepareWasCalled && stormConf.get(Config.STORM_CLUSTER_MODE) != "local") {
+        if(_prepareWasCalled && !"local".equals(stormConf.get(Config.STORM_CLUSTER_MODE))) {
             throw new RuntimeException("A single worker should have 1 SystemBolt instance.");
         }
         _prepareWasCalled = true;

File: storm-core/src/jvm/backtype/storm/utils/Utils.java
Patch:
@@ -23,6 +23,7 @@
 import java.util.ArrayList;
 import java.util.Enumeration;
 import java.util.HashMap;
+import java.util.HashSet;
 import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
@@ -106,7 +107,7 @@ public static List<URL> findResources(String name) {
 
     public static Map findAndReadConfigFile(String name, boolean mustExist) {
         try {
-            List<URL> resources = findResources(name);
+            HashSet<URL> resources = new HashSet<URL>(findResources(name));
             if(resources.isEmpty()) {
                 if(mustExist) throw new RuntimeException("Could not find config file on classpath " + name);
                 else return new HashMap();
@@ -115,7 +116,7 @@ public static Map findAndReadConfigFile(String name, boolean mustExist) {
                 throw new RuntimeException("Found multiple " + name + " resources. You're probably bundling the Storm jars with your topology jar. "
                   + resources);
             }
-            URL resource = resources.get(0);
+            URL resource = resources.iterator().next();
             Yaml yaml = new Yaml();
             Map ret = (Map) yaml.load(new InputStreamReader(resource.openStream()));
             if(ret==null) ret = new HashMap();

File: storm-core/src/jvm/backtype/storm/utils/Utils.java
Patch:
@@ -23,6 +23,7 @@
 import java.util.ArrayList;
 import java.util.Enumeration;
 import java.util.HashMap;
+import java.util.HashSet;
 import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
@@ -106,7 +107,7 @@ public static List<URL> findResources(String name) {
 
     public static Map findAndReadConfigFile(String name, boolean mustExist) {
         try {
-            List<URL> resources = findResources(name);
+            HashSet<URL> resources = new HashSet<URL>(findResources(name));
             if(resources.isEmpty()) {
                 if(mustExist) throw new RuntimeException("Could not find config file on classpath " + name);
                 else return new HashMap();
@@ -115,7 +116,7 @@ public static Map findAndReadConfigFile(String name, boolean mustExist) {
                 throw new RuntimeException("Found multiple " + name + " resources. You're probably bundling the Storm jars with your topology jar. "
                   + resources);
             }
-            URL resource = resources.get(0);
+            URL resource = resources.iterator().next();
             Yaml yaml = new Yaml();
             Map ret = (Map) yaml.load(new InputStreamReader(resource.openStream()));
             if(ret==null) ret = new HashMap();

File: storm-core/src/jvm/backtype/storm/Config.java
Patch:
@@ -777,7 +777,7 @@ public class Config extends HashMap<String, Object> {
      * to backtype.storm.scheduler.IsolationScheduler to make use of the isolation scheduler.
      */
     public static final String ISOLATION_SCHEDULER_MACHINES = "isolation.scheduler.machines";
-    public static final Object ISOLATION_SCHEDULER_MACHINES_SCHEMA = Number.class;
+    public static final Object ISOLATION_SCHEDULER_MACHINES_SCHEMA = Map.class;
 
     public static void setDebug(Map conf, boolean isOn) {
         conf.put(Config.TOPOLOGY_DEBUG, isOn);

File: storm-core/src/jvm/backtype/storm/Config.java
Patch:
@@ -777,7 +777,7 @@ public class Config extends HashMap<String, Object> {
      * to backtype.storm.scheduler.IsolationScheduler to make use of the isolation scheduler.
      */
     public static final String ISOLATION_SCHEDULER_MACHINES = "isolation.scheduler.machines";
-    public static final Object ISOLATION_SCHEDULER_MACHINES_SCHEMA = Number.class;
+    public static final Object ISOLATION_SCHEDULER_MACHINES_SCHEMA = Map.class;
 
     public static void setDebug(Map conf, boolean isOn) {
         conf.put(Config.TOPOLOGY_DEBUG, isOn);

File: storm-core/src/jvm/backtype/storm/nimbus/DefaultTopologyValidator.java
Patch:
@@ -9,6 +9,6 @@ public class DefaultTopologyValidator implements ITopologyValidator {
     public void prepare(Map StormConf){
     }
     @Override
-    public void validate(String topologyName, Map topologyConf, StormTopology topology, Map NimbusConf) throws InvalidTopologyException {        
+    public void validate(String topologyName, Map topologyConf, StormTopology topology) throws InvalidTopologyException {        
     }    
 }

File: storm-core/src/jvm/backtype/storm/nimbus/DefaultTopologyValidator.java
Patch:
@@ -5,6 +5,9 @@
 import java.util.Map;
 
 public class DefaultTopologyValidator implements ITopologyValidator {
+    @Override
+    public void prepare(Map StormConf){
+    }
     @Override
     public void validate(String topologyName, Map topologyConf, StormTopology topology, Map NimbusConf) throws InvalidTopologyException {        
     }    

File: storm-core/src/jvm/backtype/storm/Config.java
Patch:
@@ -554,7 +554,7 @@ public class Config extends HashMap<String, Object> {
      * Each listed class maps 1:1 to a system bolt named __metrics_ClassName#N, and it's parallelism is configurable.
      */
     public static final String TOPOLOGY_METRICS_CONSUMER_REGISTER = "topology.metrics.consumer.register";
-    public static final Object TOPOLOGY_METRICS_CONSUMER_REGISTER_SCHEMA = ConfigValidation.StringsValidator;
+    public static final Object TOPOLOGY_METRICS_CONSUMER_REGISTER_SCHEMA = ConfigValidation.MapsValidator;
 
 
     /**

File: storm-core/src/jvm/backtype/storm/Config.java
Patch:
@@ -554,7 +554,7 @@ public class Config extends HashMap<String, Object> {
      * Each listed class maps 1:1 to a system bolt named __metrics_ClassName#N, and it's parallelism is configurable.
      */
     public static final String TOPOLOGY_METRICS_CONSUMER_REGISTER = "topology.metrics.consumer.register";
-    public static final Object TOPOLOGY_METRICS_CONSUMER_REGISTER_SCHEMA = ConfigValidation.StringsValidator;
+    public static final Object TOPOLOGY_METRICS_CONSUMER_REGISTER_SCHEMA = ConfigValidation.MapsValidator;
 
 
     /**

File: storm-core/src/jvm/backtype/storm/nimbus/DefaultTopologyValidator.java
Patch:
@@ -6,6 +6,6 @@
 
 public class DefaultTopologyValidator implements ITopologyValidator {
     @Override
-    public void validate(String topologyName, Map topologyConf, StormTopology topology) throws InvalidTopologyException {        
+    public void validate(String topologyName, Map topologyConf, StormTopology topology, Map NimbusConf) throws InvalidTopologyException {        
     }    
 }

File: storm-core/src/jvm/storm/trident/testing/LRUMemoryMapState.java
Patch:
@@ -72,7 +72,7 @@ public Factory(int maxSize) {
 
         @Override
         public State makeState(Map conf, IMetricsContext metrics, int partitionIndex, int numPartitions) {
-            return new LRUMemoryMapState(_maxSize, _id);
+            return new LRUMemoryMapState(_maxSize, _id + partitionIndex);
         }
     }
 

File: storm-core/src/jvm/storm/trident/testing/MemoryMapState.java
Patch:
@@ -69,7 +69,7 @@ public Factory() {
 
         @Override
         public State makeState(Map conf, IMetricsContext metrics, int partitionIndex, int numPartitions) {
-            return new MemoryMapState(_id);
+            return new MemoryMapState(_id + partitionIndex);
         }
     }
 

File: storm-netty/src/jvm/backtype/storm/messaging/netty/ControlMessage.java
Patch:
@@ -31,11 +31,10 @@ static ControlMessage mkMessage(short encoded) {
 
     /**
      * encode the current Control Message into a channel buffer
-     * @param bout
      * @throws Exception
      */
     ChannelBuffer buffer() throws Exception {
-        ChannelBufferOutputStream bout = new ChannelBufferOutputStream(ChannelBuffers.dynamicBuffer());      
+        ChannelBufferOutputStream bout = new ChannelBufferOutputStream(ChannelBuffers.buffer(2));      
         write(bout);
         bout.close();
         return bout.buffer();

File: storm-netty/src/jvm/backtype/storm/messaging/netty/Client.java
Patch:
@@ -84,7 +84,7 @@ void reconnect() {
                 close();
             }
         } catch (InterruptedException e) {
-            LOG.info("connection failed", e);
+            LOG.warn("connection failed", e);
         } 
     }
 
@@ -157,7 +157,7 @@ public void close() {
             @Override
             public void run() {
                 if (ready_to_release_resource.get()) {
-                    LOG.info("client resource released");
+                    LOG.debug("client resource released");
                     factory.releaseExternalResources();
                     timer.cancel();
                 }

File: storm-netty/src/jvm/backtype/storm/messaging/netty/MessageDecoder.java
Patch:
@@ -9,8 +9,6 @@
 import backtype.storm.messaging.TaskMessage;
 
 public class MessageDecoder extends FrameDecoder {    
-    private static final Logger LOG = LoggerFactory.getLogger(MessageDecoder.class);
-
     /*
      * Each ControlMessage is encoded as:
      *  code (<0) ... short(2)

File: storm-netty/src/jvm/backtype/storm/messaging/netty/StormClientPipelineFactory.java
Patch:
@@ -5,7 +5,7 @@
 import org.jboss.netty.channel.ChannelPipelineFactory;
 import org.jboss.netty.channel.Channels;
 
-public class StormClientPipelineFactory implements ChannelPipelineFactory {
+class StormClientPipelineFactory implements ChannelPipelineFactory {
     private Client client;
     @SuppressWarnings("rawtypes")
     private Map conf;
@@ -16,7 +16,6 @@ public class StormClientPipelineFactory implements ChannelPipelineFactory {
         this.conf = conf;
     }
 
-    @Override
     public ChannelPipeline getPipeline() throws Exception {
         // Create a default pipeline implementation.
         ChannelPipeline pipeline = Channels.pipeline();

File: storm-netty/src/jvm/backtype/storm/messaging/netty/StormServerPipelineFactory.java
Patch:
@@ -18,7 +18,6 @@ class StormServerPipelineFactory implements  ChannelPipelineFactory {
         this.conf = conf;
     }
     
-    @Override
     public ChannelPipeline getPipeline() throws Exception {
         // Create a default pipeline implementation.
         ChannelPipeline pipeline = Channels.pipeline();

File: storm-netty/src/jvm/backtype/storm/messaging/netty/TaskMessageDecoder.java
Patch:
@@ -7,7 +7,7 @@
 
 import backtype.storm.messaging.TaskMessage;
 
-class TaskMessageDecoder extends FrameDecoder {    
+public class TaskMessageDecoder extends FrameDecoder {    
     /*
      * Each TaskMessage is encoded as:
      *  task ... short(2)

File: storm-netty/src/jvm/backtype/storm/messaging/netty/TaskMessageEncoder.java
Patch:
@@ -13,7 +13,7 @@
 import backtype.storm.messaging.TaskMessage;
 import backtype.storm.utils.Utils;
 
-class TaskMessageEncoder extends OneToOneEncoder {
+public class TaskMessageEncoder extends OneToOneEncoder {
     int estimated_buffer_size;
     
     @SuppressWarnings("rawtypes")

File: storm-netty/src/jvm/backtype/storm/messaging/netty/Util.java
Patch:
@@ -2,7 +2,7 @@
 
 import backtype.storm.messaging.TaskMessage;
 
-public class Util {
+class Util {
     static final int OK = -200; //HTTP status: 200
     static final int FAILURE = -400; //HTTP status: 400 BAD REQUEST
     static final int CLOSE = -410; //HTTP status: 410 GONE

File: src/jvm/backtype/storm/messaging/zmq/TransportPlugin.java
Patch:
@@ -12,8 +12,8 @@
 import org.zeromq.ZMQ.Context;
 import org.zeromq.ZMQ.Socket;
 
-public class TranportPlugin implements IContext {
-    public static final Logger LOG = LoggerFactory.getLogger(TranportPlugin.class);
+public class TransportPlugin implements IContext {
+    public static final Logger LOG = LoggerFactory.getLogger(TransportPlugin.class);
 
     private Context context; 
     private long linger_ms, hwm;

File: src/jvm/backtype/storm/utils/ShellProcess.java
Patch:
@@ -113,6 +113,8 @@ private String readString() throws IOException {
                     else {
                         errorMessage.append(" Currently read output: " + line.toString() + "\n");
                     }
+                    errorMessage.append("Shell Process Exception:\n");
+                    errorMessage.append(getErrorsString() + "\n");
                     throw new RuntimeException(errorMessage.toString());
                 }
                 if(subline.equals("end")) {

File: src/jvm/backtype/storm/task/ShellBolt.java
Patch:
@@ -124,6 +124,8 @@ public void run() {
                         if (write != null) {
                             _process.writeMessage(write);
                         }
+                        // drain the error stream to avoid dead lock because of full error stream buffer
+                        _process.drainErrorStream();
                     } catch (InterruptedException e) {
                     } catch (Throwable t) {
                         die(t);

File: src/jvm/backtype/storm/Constants.java
Patch:
@@ -1,11 +1,14 @@
 package backtype.storm;
 
 import backtype.storm.coordination.CoordinatedBolt;
+import clojure.lang.RT;
 
 
 public class Constants {
     public static final String COORDINATED_STREAM_ID = CoordinatedBolt.class.getName() + "/coord-stream"; 
 
+    public static final long SYSTEM_TASK_ID = -1;
+    public static final Object SYSTEM_EXECUTOR_ID = RT.readString("[-1 -1]");
     public static final String SYSTEM_COMPONENT_ID = "__system";
     public static final String SYSTEM_TICK_STREAM_ID = "__tick";
     public static final String METRICS_COMPONENT_ID_PREFIX = "__metrics";

File: src/jvm/backtype/storm/metric/api/CountMetric.java
Patch:
@@ -2,7 +2,7 @@
 
 import backtype.storm.metric.api.IMetric;
 
-public class CountMetric implements IMetric, java.io.Serializable {
+public class CountMetric implements IMetric {
     long _value = 0;
 
     public CountMetric() {

File: src/jvm/backtype/storm/metric/api/MultiCountMetric.java
Patch:
@@ -4,7 +4,7 @@
 import java.util.HashMap;
 import java.util.Map;
 
-public class MultiCountMetric implements IMetric, java.io.Serializable {
+public class MultiCountMetric implements IMetric {
     Map<String, CountMetric> _value = new HashMap();
 
     public MultiCountMetric() {

File: src/jvm/backtype/storm/task/GeneralTopologyContext.java
Patch:
@@ -63,7 +63,7 @@ public StormTopology getRawTopology() {
      * @return the component id for the input task id
      */
     public String getComponentId(int taskId) {
-        if(taskId==-1) {
+        if(taskId==Constants.SYSTEM_TASK_ID) {
             return Constants.SYSTEM_COMPONENT_ID;
         } else {
             return _taskToComponent.get(taskId);

File: src/jvm/backtype/storm/utils/Utils.java
Patch:
@@ -112,7 +112,7 @@ public static Map findAndReadConfigFile(String name, boolean mustExist) {
                 else return new HashMap();
             }
             if(resources.size() > 1) {
-                throw new RuntimeException("Found multiple " + name + " resources. You're probably bundling the Storm jars with your topology jar. ");
+                throw new RuntimeException("Found multiple " + name + " resources. You're probably bundling the Storm jars with your topology jar.");
             }
             URL resource = resources.get(0);
             Yaml yaml = new Yaml();

File: src/jvm/backtype/storm/utils/Utils.java
Patch:
@@ -112,8 +112,7 @@ public static Map findAndReadConfigFile(String name, boolean mustExist) {
                 else return new HashMap();
             }
             if(resources.size() > 1) {
-                throw new RuntimeException("Found multiple " + name + " resources. You're probably bundling the Storm jars with your topology jar. "
-                  + resources);
+                throw new RuntimeException("Found multiple " + name + " resources. You're probably bundling the Storm jars with your topology jar. ");
             }
             URL resource = resources.get(0);
             Yaml yaml = new Yaml();

File: src/jvm/backtype/storm/security/auth/ITransportPlugin.java
Patch:
@@ -1,7 +1,6 @@
 package backtype.storm.security.auth;
 
 import java.io.IOException;
-
 import org.apache.thrift7.TProcessor;
 import org.apache.thrift7.server.TServer;
 import org.apache.thrift7.transport.TTransport;
@@ -15,13 +14,13 @@
  */
 public interface ITransportPlugin {
     /**
-     * Create a server for server to use
+     * Create a server associated with a given port and service handler
      * @param port listening port
      * @param processor service handler
      * @return server to be binded
      */
     public TServer getServer(int port, TProcessor processor) throws IOException, TTransportException;
-    
+
     /**
      * Connect to the specified server via framed transport 
      * @param transport The underlying Thrift transport.

File: src/jvm/backtype/storm/utils/Utils.java
Patch:
@@ -112,7 +112,8 @@ public static Map findAndReadConfigFile(String name, boolean mustExist) {
                 else return new HashMap();
             }
             if(resources.size() > 1) {
-                throw new RuntimeException("Found multiple " + name + " resources. You're probably bundling the Storm jars with your topology jar.");
+                throw new RuntimeException("Found multiple " + name + " resources. You're probably bundling the Storm jars with your topology jar. "
+                  + resources);
             }
             URL resource = resources.get(0);
             Yaml yaml = new Yaml();

File: src/jvm/backtype/storm/spout/SchemeAsMultiScheme.java
Patch:
@@ -13,7 +13,9 @@ public SchemeAsMultiScheme(Scheme scheme) {
   }
 
   @Override public Iterable<List<Object>> deserialize(final byte[] ser) {
-    return Arrays.asList(scheme.deserialize(ser));
+    List<Object> o = scheme.deserialize(ser);
+    if(o == null) return null;
+    else return Arrays.asList(o);
   }
 
   @Override public Fields getOutputFields() {

File: src/jvm/backtype/storm/spout/SchemeAsMultiScheme.java
Patch:
@@ -13,7 +13,9 @@ public SchemeAsMultiScheme(Scheme scheme) {
   }
 
   @Override public Iterable<List<Object>> deserialize(final byte[] ser) {
-    return Arrays.asList(scheme.deserialize(ser));
+    List<Object> o = scheme.deserialize(ser);
+    if(o == null) return null;
+    else return Arrays.asList(o);
   }
 
   @Override public Fields getOutputFields() {

File: src/jvm/backtype/storm/task/ShellBolt.java
Patch:
@@ -123,6 +123,8 @@ public void run() {
                         if (write != null) {
                             _process.writeMessage(write);
                         }
+                        // drain the error stream to avoid dead lock because of full error stream buffer
+                        _process.drainErrorStream();
                     } catch (InterruptedException e) {
                     } catch (Throwable t) {
                         die(t);

File: src/jvm/storm/trident/operation/impl/GroupedAggregator.java
Patch:
@@ -56,6 +56,7 @@ public void aggregate(Object[] arr, TridentTuple tuple, TridentCollector collect
         } else {
             curr = val.get(group);
         }
+        groupColl.currGroup = group;
         _agg.aggregate(curr, input, groupColl);
         
     }

File: src/jvm/backtype/storm/task/TopologyContext.java
Patch:
@@ -208,7 +208,7 @@ public Collection<ITaskHook> getHooks() {
      * You must call this during IBolt::prepare or ISpout::open.
      * @return The IMetric argument unchanged.
      */
-    public IMetric registerMetric(String name, IMetric metric, int timeBucketSizeInSecs) {
+    public <T extends IMetric> T registerMetric(String name, T metric, int timeBucketSizeInSecs) {
         if((Boolean)_openOrPrepareWasCalled.deref() == true) {
             throw new RuntimeException("TopologyContext.registerMetric can only be called from within overridden " + 
                                        "IBolt::prepare() or ISpout::open() method.");
@@ -237,13 +237,13 @@ public IMetric registerMetric(String name, IMetric metric, int timeBucketSizeInS
     /*
      * Convinience method for registering ReducedMetric.
      */
-    public IMetric registerMetric(String name, IReducer reducer, int timeBucketSizeInSecs) {
+    public ReducedMetric registerMetric(String name, IReducer reducer, int timeBucketSizeInSecs) {
         return registerMetric(name, new ReducedMetric(reducer), timeBucketSizeInSecs);
     }
     /*
      * Convinience method for registering CombinedMetric.
      */
-    public IMetric registerMetric(String name, ICombiner combiner, int timeBucketSizeInSecs) {
+    public CombinedMetric registerMetric(String name, ICombiner combiner, int timeBucketSizeInSecs) {
         return registerMetric(name, new CombinedMetric(combiner), timeBucketSizeInSecs);
     }
 }
\ No newline at end of file

File: src/jvm/storm/trident/operation/TridentOperationContext.java
Patch:
@@ -31,13 +31,13 @@ public int getPartitionIndex() {
         return _topoContext.getThisTaskIndex();
     }
 
-    public IMetric registerMetric(String name, IMetric metric, int timeBucketSizeInSecs) {
+    public <T extends IMetric> T registerMetric(String name, T metric, int timeBucketSizeInSecs) {
         return _topoContext.registerMetric(name, metric, timeBucketSizeInSecs);
     }
-    public IMetric registerMetric(String name, IReducer reducer, int timeBucketSizeInSecs) {
+    public ReducedMetric registerMetric(String name, IReducer reducer, int timeBucketSizeInSecs) {
         return _topoContext.registerMetric(name, new ReducedMetric(reducer), timeBucketSizeInSecs);
     }
-    public IMetric registerMetric(String name, ICombiner combiner, int timeBucketSizeInSecs) {
+    public CombinedMetric registerMetric(String name, ICombiner combiner, int timeBucketSizeInSecs) {
         return _topoContext.registerMetric(name, new CombinedMetric(combiner), timeBucketSizeInSecs);
     }
 }

File: src/jvm/backtype/storm/metric/api/CountMetric.java
Patch:
@@ -8,11 +8,11 @@ public class CountMetric implements IMetric {
     public CountMetric() {
     }
     
-    public void inc() {
+    public void incr() {
         _value++;
     }
 
-    public void inc(long incrementBy) {
+    public void incrBy(long incrementBy) {
         _value += incrementBy;
     }
 

File: src/jvm/backtype/storm/metric/api/IMetricsConsumer.java
Patch:
@@ -7,6 +7,7 @@
 
 public interface IMetricsConsumer {
     public static class TaskInfo {
+        public TaskInfo() {}
         public TaskInfo(String srcWorkerHost, int srcWorkerPort, String srcComponentId, int srcTaskId, long timestamp, int updateIntervalSecs) {
             this.srcWorkerHost = srcWorkerHost;
             this.srcWorkerPort = srcWorkerPort;
@@ -23,6 +24,7 @@ public TaskInfo(String srcWorkerHost, int srcWorkerPort, String srcComponentId,
         public int updateIntervalSecs; 
     }
     public static class DataPoint {
+        public DataPoint() {}
         public DataPoint(String name, Object value) {
             this.name = name;
             this.value = value;

File: src/jvm/backtype/storm/serialization/SerializationFactory.java
Patch:
@@ -39,6 +39,8 @@ public static Kryo getKryo(Map conf) {
         k.register(BigInteger.class, new BigIntegerSerializer());
         k.register(TransactionAttempt.class);
         k.register(Values.class);
+        k.register(backtype.storm.metric.api.IMetricsConsumer.DataPoint.class);
+        k.register(backtype.storm.metric.api.IMetricsConsumer.TaskInfo.class);
         try {
             JavaBridge.registerPrimitives(k);
             JavaBridge.registerCollections(k);

File: src/jvm/backtype/storm/metric/api/IMetricsConsumer.java
Patch:
@@ -7,6 +7,7 @@
 
 public interface IMetricsConsumer {
     public static class TaskInfo {
+        public TaskInfo() {}
         public TaskInfo(String srcWorkerHost, int srcWorkerPort, String srcComponentId, int srcTaskId, long timestamp, int updateIntervalSecs) {
             this.srcWorkerHost = srcWorkerHost;
             this.srcWorkerPort = srcWorkerPort;
@@ -23,6 +24,7 @@ public TaskInfo(String srcWorkerHost, int srcWorkerPort, String srcComponentId,
         public int updateIntervalSecs; 
     }
     public static class DataPoint {
+        public DataPoint() {}
         public DataPoint(String name, Object value) {
             this.name = name;
             this.value = value;

File: src/jvm/backtype/storm/serialization/SerializationFactory.java
Patch:
@@ -38,6 +38,8 @@ public static Kryo getKryo(Map conf) {
         k.register(BigInteger.class, new BigIntegerSerializer());
         k.register(TransactionAttempt.class);
         k.register(Values.class);
+        k.register(backtype.storm.metric.api.IMetricsConsumer.DataPoint.class);
+        k.register(backtype.storm.metric.api.IMetricsConsumer.TaskInfo.class);
         try {
             JavaBridge.registerPrimitives(k);
             JavaBridge.registerCollections(k);

File: src/jvm/backtype/storm/metric/api/IMetricsConsumer.java
Patch:
@@ -7,6 +7,7 @@
 
 public interface IMetricsConsumer {
     public static class TaskInfo {
+        public TaskInfo() {}
         public TaskInfo(String srcWorkerHost, int srcWorkerPort, String srcComponentId, int srcTaskId, long timestamp, int updateIntervalSecs) {
             this.srcWorkerHost = srcWorkerHost;
             this.srcWorkerPort = srcWorkerPort;
@@ -23,6 +24,7 @@ public TaskInfo(String srcWorkerHost, int srcWorkerPort, String srcComponentId,
         public int updateIntervalSecs; 
     }
     public static class DataPoint {
+        public DataPoint() {}
         public DataPoint(String name, Object value) {
             this.name = name;
             this.value = value;

File: src/jvm/backtype/storm/serialization/SerializationFactory.java
Patch:
@@ -38,6 +38,8 @@ public static Kryo getKryo(Map conf) {
         k.register(BigInteger.class, new BigIntegerSerializer());
         k.register(TransactionAttempt.class);
         k.register(Values.class);
+        k.register(backtype.storm.metric.api.IMetricsConsumer.DataPoint.class);
+        k.register(backtype.storm.metric.api.IMetricsConsumer.TaskInfo.class);
         try {
             JavaBridge.registerPrimitives(k);
             JavaBridge.registerCollections(k);

File: src/jvm/backtype/storm/metric/MetricsConsumerBolt.java
Patch:
@@ -1,8 +1,9 @@
 package backtype.storm.metric;
 
-import backtype.storm.metric.api.IMetricsConsumer;
 import backtype.storm.Config;
+import backtype.storm.metric.api.IMetricsConsumer;
 import backtype.storm.task.IBolt;
+import backtype.storm.task.IErrorReporter;
 import backtype.storm.task.OutputCollector;
 import backtype.storm.task.TopologyContext;
 import backtype.storm.tuple.Tuple;
@@ -28,7 +29,7 @@ public void prepare(Map stormConf, TopologyContext context, OutputCollector coll
             throw new RuntimeException("Could not instantiate a class listed in config under section " +
                 Config.TOPOLOGY_METRICS_CONSUMER_REGISTER + " with fully qualified name " + _consumerClassName, e);
         }
-        _metricsConsumer.prepare(stormConf, _registrationArgument, context, collector);
+        _metricsConsumer.prepare(stormConf, _registrationArgument, context, (IErrorReporter)collector);
         _collector = collector;
     }
     

File: src/jvm/backtype/storm/metric/api/IMetricsConsumer.java
Patch:
@@ -1,6 +1,6 @@
 package backtype.storm.metric.api;
 
-import backtype.storm.task.OutputCollector;
+import backtype.storm.task.IErrorReporter;
 import backtype.storm.task.TopologyContext;
 import java.util.Collection;
 import java.util.Map;
@@ -35,7 +35,7 @@ public String toString() {
         public Object value;
     }
 
-    void prepare(Map stormConf, Object registrationArgument, TopologyContext context, OutputCollector outputCollector);
+    void prepare(Map stormConf, Object registrationArgument, TopologyContext context, IErrorReporter errorReporter);
     void handleDataPoints(TaskInfo taskInfo, Collection<DataPoint> dataPoints);
     void cleanup();
 }
\ No newline at end of file

File: src/jvm/backtype/storm/task/IOutputCollector.java
Patch:
@@ -4,13 +4,12 @@
 import java.util.Collection;
 import java.util.List;
 
-public interface IOutputCollector {
+public interface IOutputCollector extends IErrorReporter {
     /**
      *  Returns the task ids that received the tuples.
      */
     List<Integer> emit(String streamId, Collection<Tuple> anchors, List<Object> tuple);
     void emitDirect(int taskId, String streamId, Collection<Tuple> anchors, List<Object> tuple);
     void ack(Tuple input);
     void fail(Tuple input);
-    void reportError(Throwable error);
 }

File: src/jvm/backtype/storm/metric/MetricsConsumerBolt.java
Patch:
@@ -28,7 +28,7 @@ public void prepare(Map stormConf, TopologyContext context, OutputCollector coll
             throw new RuntimeException("Could not instantiate a class listed in config under section " +
                 Config.TOPOLOGY_METRICS_CONSUMER_REGISTER + " with fully qualified name " + _consumerClassName, e);
         }
-        _metricsConsumer.prepare(stormConf, _registrationArgument, context);
+        _metricsConsumer.prepare(stormConf, _registrationArgument, context, collector);
         _collector = collector;
     }
     

File: src/jvm/backtype/storm/metric/api/IMetricsConsumer.java
Patch:
@@ -1,5 +1,6 @@
 package backtype.storm.metric.api;
 
+import backtype.storm.task.OutputCollector;
 import backtype.storm.task.TopologyContext;
 import java.util.Collection;
 import java.util.Map;
@@ -34,7 +35,7 @@ public String toString() {
         public Object value;
     }
 
-    void prepare(Map stormConf, Object registrationArgument, TopologyContext context);
+    void prepare(Map stormConf, Object registrationArgument, TopologyContext context, OutputCollector outputCollector);
     void handleDataPoints(TaskInfo taskInfo, Collection<DataPoint> dataPoints);
     void cleanup();
 }
\ No newline at end of file

File: src/jvm/backtype/storm/task/TopologyContext.java
Patch:
@@ -238,7 +238,7 @@ public IMetric registerMetric(String name, IReducer reducer, int timeBucketSizeI
         return registerMetric(name, new ReducedMetric(reducer), timeBucketSizeInSecs);
     }
     /*
-     * Convinience method for registering ReducedMetric.
+     * Convinience method for registering CombinedMetric.
      */
     public IMetric registerMetric(String name, ICombiner combiner, int timeBucketSizeInSecs) {
         return registerMetric(name, new CombinedMetric(combiner), timeBucketSizeInSecs);

File: src/jvm/backtype/storm/metric/api/ReducedMetric.java
Patch:
@@ -1,7 +1,7 @@
 package backtype.storm.metric.api;
 
 public class ReducedMetric implements IMetric {
-    private IReducer _reducer;
+    private final IReducer _reducer;
     private Object _accumulator;
 
     public ReducedMetric(IReducer reducer) {

File: src/jvm/storm/trident/Stream.java
Patch:
@@ -350,7 +350,7 @@ private void projectionValidation(Fields projFields) {
         Fields allFields = this.getOutputFields();
         for (String field : projFields) {
             if (!allFields.contains(field)) {
-                throw new IllegalArgumentException("Trying to select non-existent field: '" + field + "' from all fields: " + allFields + "!");
+                throw new IllegalArgumentException("Trying to select non-existent field: '" + field + "' from stream containing fields fields: <" + allFields + ">");
             }
         }
     }

File: src/jvm/backtype/storm/hooks/ITaskHook.java
Patch:
@@ -1,6 +1,7 @@
 package backtype.storm.hooks;
 
 import backtype.storm.hooks.info.BoltAckInfo;
+import backtype.storm.hooks.info.BoltExecuteInfo;
 import backtype.storm.hooks.info.SpoutFailInfo;
 import backtype.storm.hooks.info.SpoutAckInfo;
 import backtype.storm.hooks.info.EmitInfo;
@@ -14,6 +15,7 @@ public interface ITaskHook {
     void emit(EmitInfo info);
     void spoutAck(SpoutAckInfo info);
     void spoutFail(SpoutFailInfo info);
+    void boltExecute(BoltExecuteInfo info);
     void boltAck(BoltAckInfo info);
     void boltFail(BoltFailInfo info);
 }

File: src/jvm/backtype/storm/topology/BasicBoltExecutor.java
Patch:
@@ -33,7 +33,9 @@ public void execute(Tuple input) {
             _bolt.execute(input, _collector);
             _collector.getOutputter().ack(input);
         } catch(FailedException e) {
-            LOG.warn("Failed to process tuple", e);
+            if(e instanceof ReportedFailedException) {
+                _collector.reportError(e);
+            }
             _collector.getOutputter().fail(input);
         }
     }

File: src/jvm/backtype/storm/task/ShellBolt.java
Patch:
@@ -181,9 +181,7 @@ private void handleFail(Map action) {
 
     private void handleError(Map action) {
         String msg = (String) action.get("msg");
-        if (msg != null) {
-            _collector.reportError(new Exception("Shell Process Exception: " + msg));
-        }
+        _collector.reportError(new Exception("Shell Process Exception: " + msg));
     }
 
     private void handleEmit(Map action) throws InterruptedException {

File: src/jvm/backtype/storm/scheduler/SchedulerAssignment.java
Patch:
@@ -35,5 +35,7 @@ public interface SchedulerAssignment {
      * Return the executors covered by this assignments
      * @return
      */
-    public Set<ExecutorDetails> getExecutors();
+    public Set<ExecutorDetails> getExecutors();
+    
+    public Set<WorkerSlot> getSlots();
 }
\ No newline at end of file

File: src/jvm/backtype/storm/scheduler/WorkerSlot.java
Patch:
@@ -4,9 +4,9 @@ public class WorkerSlot {
     String nodeId;
     int port;
     
-    public WorkerSlot(String nodeId, int port) {
+    public WorkerSlot(String nodeId, Number port) {
         this.nodeId = nodeId;
-        this.port = port;
+        this.port = port.intValue();
     }
     
     public String getNodeId() {

File: src/jvm/backtype/storm/testing/TestJob.java
Patch:
@@ -20,5 +20,5 @@ public interface TestJob {
 	 * @param cluster the cluster which created by <code>Testing.withSimulatedTimeLocalCluster</code>
 	 *        and <code>Testing.withTrackedCluster</code>.
 	 */
-    public void run(ILocalCluster cluster);
+    public void run(ILocalCluster cluster) throws Exception;
 }

File: src/jvm/backtype/storm/scheduler/Cluster.java
Patch:
@@ -33,7 +33,7 @@ public Cluster(Map<String, SupervisorDetails> supervisors, Map<String, Scheduler
         for (String nodeId : supervisors.keySet()) {
             SupervisorDetails supervisor = supervisors.get(nodeId);
             String host = supervisor.getHost();
-            if (!this.supervisors.containsKey(host)) {
+            if (!this.hostToId.containsKey(host)) {
                 this.hostToId.put(host, new ArrayList<String>());
             }
             this.hostToId.get(host).add(nodeId);

File: src/jvm/backtype/storm/scheduler/SchedulerAssignment.java
Patch:
@@ -35,5 +35,7 @@ public interface SchedulerAssignment {
      * Return the executors covered by this assignments
      * @return
      */
-    public Set<ExecutorDetails> getExecutors();
+    public Set<ExecutorDetails> getExecutors();
+    
+    public Set<WorkerSlot> getSlots();
 }
\ No newline at end of file

File: src/jvm/backtype/storm/scheduler/WorkerSlot.java
Patch:
@@ -4,9 +4,9 @@ public class WorkerSlot {
     String nodeId;
     int port;
     
-    public WorkerSlot(String nodeId, int port) {
+    public WorkerSlot(String nodeId, Number port) {
         this.nodeId = nodeId;
-        this.port = port;
+        this.port = port.intValue();
     }
     
     public String getNodeId() {

File: src/jvm/backtype/storm/testing/TestJob.java
Patch:
@@ -20,5 +20,5 @@ public interface TestJob {
 	 * @param cluster the cluster which created by <code>Testing.withSimulatedTimeLocalCluster</code>
 	 *        and <code>Testing.withTrackedCluster</code>.
 	 */
-    public void run(ILocalCluster cluster);
+    public void run(ILocalCluster cluster) throws Exception;
 }

File: src/jvm/backtype/storm/ILocalCluster.java
Patch:
@@ -4,6 +4,7 @@
 import backtype.storm.generated.ClusterSummary;
 import backtype.storm.generated.InvalidTopologyException;
 import backtype.storm.generated.KillOptions;
+import backtype.storm.generated.SubmitOptions;
 import backtype.storm.generated.NotAliveException;
 import backtype.storm.generated.RebalanceOptions;
 import backtype.storm.generated.StormTopology;
@@ -14,6 +15,7 @@
 
 public interface ILocalCluster {
     void submitTopology(String topologyName, Map conf, StormTopology topology) throws AlreadyAliveException, InvalidTopologyException;
+    void submitTopologyWithOpts(String topologyName, Map conf, StormTopology topology, SubmitOptions submitOpts) throws AlreadyAliveException, InvalidTopologyException;
     void killTopology(String topologyName) throws NotAliveException;
     void killTopologyWithOpts(String name, KillOptions options) throws NotAliveException;
     void activate(String topologyName) throws NotAliveException;

File: src/jvm/storm/trident/TridentState.java
Patch:
@@ -13,7 +13,7 @@ protected TridentState(TridentTopology topology, Node node) {
     }
     
     public Stream newValuesStream() {
-        return new Stream(_topology, _node);
+        return new Stream(_topology, _node.name, _node);
     }
     
     public TridentState parallelismHint(int parallelism) {

File: src/jvm/storm/trident/planner/Node.java
Patch:
@@ -13,16 +13,18 @@ public class Node implements Serializable {
     
     private String nodeId;
     
+    public String name = null;
     public Fields allOutputFields;
     public String streamId;
     public Integer parallelismHint = null;
     public NodeStateInfo stateInfo = null;
     public int creationIndex;
     
-    public Node(String streamId, Fields allOutputFields) {
+    public Node(String streamId, String name, Fields allOutputFields) {
         this.nodeId = UUID.randomUUID().toString();
         this.allOutputFields = allOutputFields;
         this.streamId = streamId;
+        this.name = name;
         this.creationIndex = INDEX.incrementAndGet();
     }
 

File: src/jvm/storm/trident/planner/PartitionNode.java
Patch:
@@ -14,8 +14,8 @@ public class PartitionNode extends Node {
     public transient Grouping thriftGrouping;
     
     //has the streamid/outputFields of the node it's doing the partitioning on
-    public PartitionNode(String streamId, Fields allOutputFields, Grouping grouping) {
-        super(streamId, allOutputFields);
+    public PartitionNode(String streamId, String name, Fields allOutputFields, Grouping grouping) {
+        super(streamId, name, allOutputFields);
         this.thriftGrouping = grouping;
     }
     

File: src/jvm/storm/trident/planner/ProcessorNode.java
Patch:
@@ -8,8 +8,8 @@ public class ProcessorNode extends Node {
     public TridentProcessor processor;
     public Fields selfOutFields;
     
-    public ProcessorNode(String streamId, Fields allOutputFields, Fields selfOutFields, TridentProcessor processor) {
-        super(streamId, allOutputFields);
+    public ProcessorNode(String streamId, String name, Fields allOutputFields, Fields selfOutFields, TridentProcessor processor) {
+        super(streamId, name, allOutputFields);
         this.processor = processor;
         this.selfOutFields = selfOutFields;
     }

File: src/jvm/storm/trident/planner/SpoutNode.java
Patch:
@@ -14,7 +14,7 @@ public static enum SpoutType {
     public SpoutType type;
     
     public SpoutNode(String streamId, Fields allOutputFields, String txid, Object spout, SpoutType type) {
-        super(streamId, allOutputFields);
+        super(streamId, null, allOutputFields);
         this.txId = txid;
         this.spout = spout;
         this.type = type;

File: src/jvm/storm/trident/state/map/SnapshottableMap.java
Patch:
@@ -9,12 +9,12 @@
 public class SnapshottableMap<T> implements MapState<T>, Snapshottable<T> {
     MapState<T> _delegate;
     List<List<Object>> _keys;
-    
+
     public SnapshottableMap(MapState<T> delegate, List<Object> snapshotKey) {
         _delegate = delegate;
         _keys = Arrays.asList(snapshotKey);
     }
-    
+
     @Override
     public List<T> multiGet(List<List<Object>> keys) {
         return _delegate.multiGet(keys);

File: src/jvm/backtype/storm/ILocalCluster.java
Patch:
@@ -4,6 +4,7 @@
 import backtype.storm.generated.ClusterSummary;
 import backtype.storm.generated.InvalidTopologyException;
 import backtype.storm.generated.KillOptions;
+import backtype.storm.generated.SubmitOptions;
 import backtype.storm.generated.NotAliveException;
 import backtype.storm.generated.RebalanceOptions;
 import backtype.storm.generated.StormTopology;
@@ -14,6 +15,7 @@
 
 public interface ILocalCluster {
     void submitTopology(String topologyName, Map conf, StormTopology topology) throws AlreadyAliveException, InvalidTopologyException;
+    void submitTopologyWithOpts(String topologyName, Map conf, StormTopology topology, SubmitOptions submitOpts) throws AlreadyAliveException, InvalidTopologyException;
     void killTopology(String topologyName) throws NotAliveException;
     void killTopologyWithOpts(String name, KillOptions options) throws NotAliveException;
     void activate(String topologyName) throws NotAliveException;

File: src/jvm/storm/trident/TridentState.java
Patch:
@@ -13,7 +13,7 @@ protected TridentState(TridentTopology topology, Node node) {
     }
     
     public Stream newValuesStream() {
-        return new Stream(_topology, _node);
+        return new Stream(_topology, _node.name, _node);
     }
     
     public TridentState parallelismHint(int parallelism) {

File: src/jvm/storm/trident/planner/Node.java
Patch:
@@ -13,16 +13,18 @@ public class Node implements Serializable {
     
     private String nodeId;
     
+    public String name = null;
     public Fields allOutputFields;
     public String streamId;
     public Integer parallelismHint = null;
     public NodeStateInfo stateInfo = null;
     public int creationIndex;
     
-    public Node(String streamId, Fields allOutputFields) {
+    public Node(String streamId, String name, Fields allOutputFields) {
         this.nodeId = UUID.randomUUID().toString();
         this.allOutputFields = allOutputFields;
         this.streamId = streamId;
+        this.name = name;
         this.creationIndex = INDEX.incrementAndGet();
     }
 

File: src/jvm/storm/trident/planner/PartitionNode.java
Patch:
@@ -14,8 +14,8 @@ public class PartitionNode extends Node {
     public transient Grouping thriftGrouping;
     
     //has the streamid/outputFields of the node it's doing the partitioning on
-    public PartitionNode(String streamId, Fields allOutputFields, Grouping grouping) {
-        super(streamId, allOutputFields);
+    public PartitionNode(String streamId, String name, Fields allOutputFields, Grouping grouping) {
+        super(streamId, name, allOutputFields);
         this.thriftGrouping = grouping;
     }
     

File: src/jvm/storm/trident/planner/ProcessorNode.java
Patch:
@@ -8,8 +8,8 @@ public class ProcessorNode extends Node {
     public TridentProcessor processor;
     public Fields selfOutFields;
     
-    public ProcessorNode(String streamId, Fields allOutputFields, Fields selfOutFields, TridentProcessor processor) {
-        super(streamId, allOutputFields);
+    public ProcessorNode(String streamId, String name, Fields allOutputFields, Fields selfOutFields, TridentProcessor processor) {
+        super(streamId, name, allOutputFields);
         this.processor = processor;
         this.selfOutFields = selfOutFields;
     }

File: src/jvm/storm/trident/planner/SpoutNode.java
Patch:
@@ -14,7 +14,7 @@ public static enum SpoutType {
     public SpoutType type;
     
     public SpoutNode(String streamId, Fields allOutputFields, String txid, Object spout, SpoutType type) {
-        super(streamId, allOutputFields);
+        super(streamId, null, allOutputFields);
         this.txId = txid;
         this.spout = spout;
         this.type = type;

File: src/jvm/storm/trident/state/map/SnapshottableMap.java
Patch:
@@ -9,12 +9,12 @@
 public class SnapshottableMap<T> implements MapState<T>, Snapshottable<T> {
     MapState<T> _delegate;
     List<List<Object>> _keys;
-    
+
     public SnapshottableMap(MapState<T> delegate, List<Object> snapshotKey) {
         _delegate = delegate;
         _keys = Arrays.asList(snapshotKey);
     }
-    
+
     @Override
     public List<T> multiGet(List<List<Object>> keys) {
         return _delegate.multiGet(keys);

File: src/jvm/storm/trident/state/ITupleCollection.java
Patch:
@@ -1,4 +1,4 @@
-package backtype.storm.state;
+package storm.trident.state;
 
 import java.util.Iterator;
 import java.util.List;

File: src/jvm/storm/trident/testing/LRUMemoryMapState.java
Patch:
@@ -1,6 +1,6 @@
 package storm.trident.testing;
 
-import backtype.storm.state.ITupleCollection;
+import storm.trident.state.ITupleCollection;
 import backtype.storm.tuple.Values;
 import java.util.*;
 import java.util.Map.Entry;

File: src/jvm/storm/trident/testing/MemoryMapState.java
Patch:
@@ -1,6 +1,6 @@
 package storm.trident.testing;
 
-import backtype.storm.state.ITupleCollection;
+import storm.trident.state.ITupleCollection;
 import backtype.storm.tuple.Values;
 import java.util.*;
 import java.util.Map.Entry;

File: src/jvm/storm/trident/testing/LRUMemoryMapState.java
Patch:
@@ -13,7 +13,7 @@
 import storm.trident.state.snapshot.Snapshottable;
 import storm.trident.util.LRUMap;
 
-public class LRUMemoryMapState<T> implements Snapshottable<T>, ITupleCollection {
+public class LRUMemoryMapState<T> implements Snapshottable<T>, ITupleCollection, MapState<T> {
 
     LRUMemoryMapStateBacking<OpaqueValue> _backing;
     SnapshottableMap<T> _delegate;

File: src/jvm/storm/trident/spout/RichSpoutBatchExecutor.java
Patch:
@@ -73,6 +73,7 @@ public void emitBatch(TransactionAttempt tx, Object coordinatorMeta, TridentColl
             if(now - lastRotate > rotateTime) {
                 Map<Long, List<Object>> failed = idsMap.rotate();
                 for(Long id: failed.keySet()) {
+                    //TODO: this isn't right... it's not in the map anymore
                     fail(id);
                 }
                 lastRotate = now;

File: src/jvm/backtype/storm/StormSubmitter.java
Patch:
@@ -7,7 +7,8 @@
 import java.nio.ByteBuffer;
 import java.util.HashMap;
 import java.util.Map;
-import org.apache.log4j.Logger;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 import org.apache.thrift7.TException;
 import org.json.simple.JSONValue;
 
@@ -17,7 +18,7 @@
  * submit your topologies.
  */
 public class StormSubmitter {
-    public static Logger LOG = Logger.getLogger(StormSubmitter.class);    
+    public static Logger LOG = LoggerFactory.getLogger(StormSubmitter.class);    
 
     private static Nimbus.Iface localNimbus = null;
 

File: src/jvm/backtype/storm/coordination/BatchBoltExecutor.java
Patch:
@@ -11,10 +11,11 @@
 import backtype.storm.utils.Utils;
 import java.util.HashMap;
 import java.util.Map;
-import org.apache.log4j.Logger;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 public class BatchBoltExecutor implements IRichBolt, FinishedCallback, TimeoutCallback {
-    public static Logger LOG = Logger.getLogger(BatchBoltExecutor.class);    
+    public static Logger LOG = LoggerFactory.getLogger(BatchBoltExecutor.class);    
 
     byte[] _boltSer;
     Map<Object, IBatchBolt> _openTransactions;

File: src/jvm/backtype/storm/coordination/CoordinatedBolt.java
Patch:
@@ -23,15 +23,16 @@
 import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
-import org.apache.log4j.Logger;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 import static backtype.storm.utils.Utils.get;
 
 /**
  * Coordination requires the request ids to be globally unique for awhile. This is so it doesn't get confused
  * in the case of retries.
  */
 public class CoordinatedBolt implements IRichBolt {
-    public static Logger LOG = Logger.getLogger(CoordinatedBolt.class);
+    public static Logger LOG = LoggerFactory.getLogger(CoordinatedBolt.class);
 
     public static interface FinishedCallback {
         void finishedId(Object id);

File: src/jvm/backtype/storm/drpc/DRPCSpout.java
Patch:
@@ -16,12 +16,13 @@
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
-import org.apache.log4j.Logger;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 import org.apache.thrift7.TException;
 import org.json.simple.JSONValue;
 
 public class DRPCSpout extends BaseRichSpout {
-    public static Logger LOG = Logger.getLogger(DRPCSpout.class);
+    public static Logger LOG = LoggerFactory.getLogger(DRPCSpout.class);
     
     SpoutOutputCollector _collector;
     List<DRPCInvocationsClient> _clients = new ArrayList<DRPCInvocationsClient>();

File: src/jvm/backtype/storm/drpc/JoinResult.java
Patch:
@@ -11,11 +11,12 @@
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
-import org.apache.log4j.Logger;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 
 public class JoinResult extends BaseRichBolt {
-    public static Logger LOG = Logger.getLogger(JoinResult.class);
+    public static Logger LOG = LoggerFactory.getLogger(JoinResult.class);
 
     String returnComponent;
     Map<Object, Tuple> returns = new HashMap<Object, Tuple>();

File: src/jvm/backtype/storm/drpc/ReturnResults.java
Patch:
@@ -13,13 +13,14 @@
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
-import org.apache.log4j.Logger;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 import org.apache.thrift7.TException;
 import org.json.simple.JSONValue;
 
 
 public class ReturnResults extends BaseRichBolt {
-    public static final Logger LOG = Logger.getLogger(ReturnResults.class);
+    public static final Logger LOG = LoggerFactory.getLogger(ReturnResults.class);
     OutputCollector _collector;
     boolean local;
 

File: src/jvm/backtype/storm/serialization/SerializationFactory.java
Patch:
@@ -22,10 +22,11 @@
 import java.util.List;
 import java.util.Map;
 import java.util.TreeMap;
-import org.apache.log4j.Logger;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 public class SerializationFactory {
-    public static final Logger LOG = Logger.getLogger(SerializationFactory.class);
+    public static final Logger LOG = LoggerFactory.getLogger(SerializationFactory.class);
     
     public static class KryoSerializableDefault extends Kryo {
         boolean _override = false;

File: src/jvm/backtype/storm/spout/ShellSpout.java
Patch:
@@ -7,12 +7,13 @@
 import java.util.Map;
 import java.util.List;
 import java.io.IOException;
-import org.apache.log4j.Logger;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 import org.json.simple.JSONObject;
 
 
 public class ShellSpout implements ISpout {
-    public static Logger LOG = Logger.getLogger(ShellSpout.class);
+    public static Logger LOG = LoggerFactory.getLogger(ShellSpout.class);
 
     private SpoutOutputCollector _collector;
     private String[] _command;

File: src/jvm/backtype/storm/task/ShellBolt.java
Patch:
@@ -14,7 +14,8 @@
 import java.util.List;
 import java.util.Map;
 import java.util.Random;
-import org.apache.log4j.Logger;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 import org.json.simple.JSONObject;
 
 /**
@@ -45,7 +46,7 @@
  * </pre>
  */
 public class ShellBolt implements IBolt {
-    public static Logger LOG = Logger.getLogger(ShellBolt.class);
+    public static Logger LOG = LoggerFactory.getLogger(ShellBolt.class);
     Process _subprocess;
     OutputCollector _collector;
     Map<String, Tuple> _inputs = new ConcurrentHashMap<String, Tuple>();

File: src/jvm/backtype/storm/testing/TestAggregatesCounter.java
Patch:
@@ -8,12 +8,13 @@
 import java.util.Map;
 import backtype.storm.task.TopologyContext;
 import java.util.HashMap;
-import org.apache.log4j.Logger;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 import static backtype.storm.utils.Utils.tuple;
 
 
 public class TestAggregatesCounter extends BaseRichBolt {
-    public static Logger LOG = Logger.getLogger(TestWordCounter.class);
+    public static Logger LOG = LoggerFactory.getLogger(TestWordCounter.class);
 
     Map<String, Integer> _counts;
     OutputCollector _collector;

File: src/jvm/backtype/storm/testing/TestGlobalCount.java
Patch:
@@ -8,11 +8,12 @@
 import java.util.Map;
 import backtype.storm.task.TopologyContext;
 import backtype.storm.tuple.Values;
-import org.apache.log4j.Logger;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 
 public class TestGlobalCount extends BaseRichBolt {
-    public static Logger LOG = Logger.getLogger(TestWordCounter.class);
+    public static Logger LOG = LoggerFactory.getLogger(TestWordCounter.class);
 
     private int _count;
     OutputCollector _collector;

File: src/jvm/backtype/storm/testing/TestWordCounter.java
Patch:
@@ -8,12 +8,13 @@
 import backtype.storm.task.TopologyContext;
 import backtype.storm.topology.BasicOutputCollector;
 import java.util.HashMap;
-import org.apache.log4j.Logger;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 import static backtype.storm.utils.Utils.tuple;
 
 
 public class TestWordCounter extends BaseBasicBolt {
-    public static Logger LOG = Logger.getLogger(TestWordCounter.class);
+    public static Logger LOG = LoggerFactory.getLogger(TestWordCounter.class);
 
     Map<String, Integer> _counts;
     

File: src/jvm/backtype/storm/testing/TestWordSpout.java
Patch:
@@ -11,11 +11,12 @@
 import backtype.storm.utils.Utils;
 import java.util.HashMap;
 import java.util.Random;
-import org.apache.log4j.Logger;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 
 public class TestWordSpout extends BaseRichSpout {
-    public static Logger LOG = Logger.getLogger(TestWordSpout.class);
+    public static Logger LOG = LoggerFactory.getLogger(TestWordSpout.class);
     boolean _isDistributed;
     SpoutOutputCollector _collector;
 

File: src/jvm/backtype/storm/topology/BasicBoltExecutor.java
Patch:
@@ -4,10 +4,11 @@
 import backtype.storm.task.TopologyContext;
 import backtype.storm.tuple.Tuple;
 import java.util.Map;
-import org.apache.log4j.Logger;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 public class BasicBoltExecutor implements IRichBolt {
-    public static Logger LOG = Logger.getLogger(BasicBoltExecutor.class);    
+    public static Logger LOG = LoggerFactory.getLogger(BasicBoltExecutor.class);    
     
     private IBasicBolt _bolt;
     private transient BasicOutputCollector _collector;

File: src/jvm/backtype/storm/transactional/TransactionalSpoutBatchExecutor.java
Patch:
@@ -10,10 +10,11 @@
 import java.math.BigInteger;
 import java.util.Map;
 import java.util.TreeMap;
-import org.apache.log4j.Logger;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 public class TransactionalSpoutBatchExecutor implements IRichBolt {
-    public static Logger LOG = Logger.getLogger(TransactionalSpoutBatchExecutor.class);    
+    public static Logger LOG = LoggerFactory.getLogger(TransactionalSpoutBatchExecutor.class);    
 
     BatchOutputCollectorImpl _collector;
     ITransactionalSpout _spout;

File: src/jvm/backtype/storm/transactional/TransactionalSpoutCoordinator.java
Patch:
@@ -15,10 +15,11 @@
 import java.util.Map;
 import java.util.TreeMap;
 import java.util.Random;
-import org.apache.log4j.Logger;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 public class TransactionalSpoutCoordinator extends BaseRichSpout { 
-    public static final Logger LOG = Logger.getLogger(TransactionalSpoutCoordinator.class);
+    public static final Logger LOG = LoggerFactory.getLogger(TransactionalSpoutCoordinator.class);
     
     public static final BigInteger INIT_TXID = BigInteger.ONE;
     

File: src/jvm/backtype/storm/utils/Time.java
Patch:
@@ -4,11 +4,12 @@
 import java.util.Map;
 import java.util.concurrent.ConcurrentHashMap;
 import java.util.concurrent.atomic.AtomicBoolean;
-import org.apache.log4j.Logger;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 
 public class Time {
-    public static Logger LOG = Logger.getLogger(Time.class);    
+    public static Logger LOG = LoggerFactory.getLogger(Time.class);    
     
     private static AtomicBoolean simulating = new AtomicBoolean(false);
     //TODO: should probably use weak references here or something

File: src/jvm/storm/trident/spout/TridentSpoutCoordinator.java
Patch:
@@ -10,14 +10,15 @@
 import backtype.storm.tuple.Tuple;
 import backtype.storm.tuple.Values;
 import java.util.Map;
-import org.apache.log4j.Logger;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 import storm.trident.topology.MasterBatchCoordinator;
 import storm.trident.topology.state.RotatingTransactionalState;
 import storm.trident.topology.state.TransactionalState;
 
 
 public class TridentSpoutCoordinator implements IBasicBolt {
-    public static final Logger LOG = Logger.getLogger(TridentSpoutCoordinator.class);
+    public static final Logger LOG = LoggerFactory.getLogger(TridentSpoutCoordinator.class);
     private static final String META_DIR = "meta";
 
     ITridentSpout _spout;

File: src/jvm/storm/trident/spout/TridentSpoutExecutor.java
Patch:
@@ -11,7 +11,8 @@
 import java.util.List;
 import java.util.Map;
 import java.util.TreeMap;
-import org.apache.log4j.Logger;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 import storm.trident.operation.TridentCollector;
 import storm.trident.topology.BatchInfo;
 import storm.trident.topology.ITridentBatchBolt;
@@ -21,7 +22,7 @@
 public class TridentSpoutExecutor implements ITridentBatchBolt {
     public static String ID_FIELD = "$tx";
     
-    public static Logger LOG = Logger.getLogger(TridentSpoutExecutor.class);    
+    public static Logger LOG = LoggerFactory.getLogger(TridentSpoutExecutor.class);    
 
     AddIdCollector _collector;
     ITridentSpout _spout;

File: src/jvm/storm/trident/topology/MasterBatchCoordinator.java
Patch:
@@ -13,12 +13,13 @@
 import java.util.Map;
 import java.util.TreeMap;
 import java.util.Random;
-import org.apache.log4j.Logger;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 import storm.trident.spout.ITridentSpout;
 import storm.trident.topology.state.TransactionalState;
 
 public class MasterBatchCoordinator extends BaseRichSpout { 
-    public static final Logger LOG = Logger.getLogger(MasterBatchCoordinator.class);
+    public static final Logger LOG = LoggerFactory.getLogger(MasterBatchCoordinator.class);
     
     public static final long INIT_TXID = 1L;
     

File: src/jvm/storm/trident/operation/builtin/TupleCollectionGet.java
Patch:
@@ -14,16 +14,15 @@ public class TupleCollectionGet extends BaseQueryFunction<State, Object> {
     @Override
     public List<Object> batchRetrieve(State state, List<TridentTuple> args) {
         List<Object> ret = new ArrayList<Object>(args.size());
-        Iterator<List<Object>> tuplesIterator = ((ITupleCollection)state).getTuples();
         for(int i=0; i<args.size(); i++) {
-            ret.add(tuplesIterator);
+            ret.add(((ITupleCollection)state).getTuples());
         }
         return ret;
     }
 
     @Override
     public void execute(TridentTuple tuple, Object result, TridentCollector collector) {
-        Iterator<List<Object>> tuplesIterator = (Iterator<List<Object>>) result;
+        Iterator<List<Object>> tuplesIterator = (Iterator<List<Object>>)result;
         while(tuplesIterator.hasNext()) {
             collector.emit(tuplesIterator.next());
         }

File: src/jvm/backtype/storm/ILocalCluster.java
Patch:
@@ -24,4 +24,5 @@ public interface ILocalCluster {
     StormTopology getTopology(String id);
     ClusterSummary getClusterInfo();
     TopologyInfo getTopologyInfo(String id);
+    Map getState();
 }

File: src/jvm/backtype/storm/serialization/SerializationFactory.java
Patch:
@@ -98,7 +98,6 @@ public static Kryo getKryo(Map conf) {
                 Class klass = Class.forName(klassName);
                 IKryoDecorator decorator = (IKryoDecorator)klass.newInstance();
                 decorator.decorate(k);
-                System.out.println(klassName + ".. executed decorate.");
             } catch(ClassNotFoundException e) {
                 if(skipMissing) {
                     LOG.info("Could not find kryo decorator named " + klassName + ". Skipping registration...");

File: src/jvm/storm/trident/spout/TridentSpoutExecutor.java
Patch:
@@ -48,7 +48,6 @@ public void execute(BatchInfo info, Tuple input) {
         // there won't be a BatchInfo for the success stream
         TransactionAttempt attempt = (TransactionAttempt) input.getValue(0);
         if(input.getSourceStreamId().equals(MasterBatchCoordinator.COMMIT_STREAM_ID)) {
-            _collector.setBatch(info.batchId);
             if(attempt.equals(_activeBatches.get(attempt.getTransactionId()))) {
                 ((ICommitterTridentSpout.Emitter) _emitter).commit(attempt);
                 _activeBatches.remove(attempt.getTransactionId());

File: src/jvm/storm/trident/tuple/TridentTupleView.java
Patch:
@@ -96,7 +96,7 @@ public static class OperationOutputFactory implements Factory {
 
         public OperationOutputFactory(Factory parent, Fields selfFields) {
             _parent = parent;
-            _fieldIndex = parent.getFieldIndex();
+            _fieldIndex = new HashMap(parent.getFieldIndex());
             int myIndex = parent.numDelegates();
             for(int i=0; i<selfFields.size(); i++) {
                 String field = selfFields.get(i);

File: src/jvm/storm/trident/testing/Split.java
Patch:
@@ -10,7 +10,9 @@ public class Split extends BaseFunction {
     @Override
     public void execute(TridentTuple tuple, TridentCollector collector) {
         for(String word: tuple.getString(0).split(" ")) {
-            collector.emit(new Values(word));
+            if(word.length() > 0) {
+                collector.emit(new Values(word));
+            }
         }
     }
     

File: src/jvm/storm/trident/state/Serializer.java
Patch:
@@ -5,5 +5,5 @@
 
 public interface Serializer<T> extends Serializable {
     byte[] serialize(T obj);
-    Object deserialize(byte[] b);
+    T deserialize(byte[] b);
 }

File: src/jvm/backtype/storm/testing/MockedSources.java
Patch:
@@ -33,7 +33,7 @@ public void addMockData(String spoutId, String streamId, Values... valueses) {
         }
     }
     
-    public void addMockedData(String spoutId, Values... valueses) {
+    public void addMockData(String spoutId, Values... valueses) {
         this.addMockData(spoutId, Utils.DEFAULT_STREAM_ID, valueses);
     }
     

File: src/jvm/storm/trident/state/OpaqueValue.java
Patch:
@@ -17,14 +17,14 @@ public OpaqueValue(Long currTxid, T val) {
         this(currTxid, val, null);
     }
     
-    public OpaqueValue update(Long batchTxid, T newVal) {
+    public OpaqueValue<T> update(Long batchTxid, T newVal) {
         T prev;
         if(batchTxid!=null && batchTxid.equals(this.currTxid)) {
             prev = this.prev;
         } else {
             prev = this.curr;
         }
-        return new OpaqueValue(batchTxid, newVal, prev);
+        return new OpaqueValue<T>(batchTxid, newVal, prev);
     }
     
     public T get(Long batchTxid) {

File: src/jvm/storm/trident/testing/LRUMemoryMapState.java
Patch:
@@ -50,7 +50,7 @@ public LRUMemoryMapState(int cacheSize, String id) {
 
     @Override
     public List<T> multiGet(List<List<Object>> keys) {
-        List<T> ret = new ArrayList();
+        List<T> ret = new ArrayList<T>();
         for(List<Object> key: keys) {
             ret.add(db.get(key));
         }

File: src/jvm/storm/trident/state/map/NonTransactionalMap.java
Patch:
@@ -2,13 +2,12 @@
 
 import java.util.ArrayList;
 import java.util.List;
-import storm.trident.state.TransactionalValue;
 import storm.trident.state.ValueUpdater;
 
 
 public class NonTransactionalMap<T> implements MapState<T> {
-    public static MapState build(IBackingMap<TransactionalValue> backing) {
-        return new NonTransactionalMap(backing);
+    public static <T> MapState<T> build(IBackingMap<T> backing) {
+        return new NonTransactionalMap<T>(backing);
     }
     
     IBackingMap<T> _backing;

File: src/jvm/storm/trident/operation/GroupedMultiReducer.java
Patch:
@@ -7,7 +7,7 @@
 
 public interface GroupedMultiReducer<T> extends Serializable {
     void prepare(Map conf, TridentMultiReducerContext context);
-    T init(TridentCollector collector);
+    T init(TridentCollector collector, TridentTuple group);
     void execute(T state, int streamIndex, TridentTuple group, TridentTuple input, TridentCollector collector);
     void complete(T state, TridentTuple group, TridentCollector collector);
     void cleanup();

File: src/jvm/storm/trident/operation/impl/GroupedMultiReducerExecutor.java
Patch:
@@ -26,6 +26,7 @@ public GroupedMultiReducerExecutor(GroupedMultiReducer reducer, List<Fields> gro
         }
         _groupFields = groupFields;
         _inputFields = inputFields;
+        _reducer = reducer;
     }
     
     @Override
@@ -48,11 +49,11 @@ public void execute(Map<TridentTuple, Object> state, int streamIndex, TridentTup
         ProjectionFactory inputFactory = _inputFactories.get(streamIndex);
         
         TridentTuple group = groupFactory.create(full);
-        TridentTuple input = groupFactory.create(full);
+        TridentTuple input = inputFactory.create(full);
         
         Object curr;
         if(!state.containsKey(group)) {
-            curr = _reducer.init(collector);
+            curr = _reducer.init(collector, group);
             state.put(group, curr);
         } else {
             curr = state.get(group);

File: src/jvm/backtype/storm/utils/RotatingMap.java
Patch:
@@ -48,14 +48,15 @@ public RotatingMap(int numBuckets) {
         this(numBuckets, null);
     }   
     
-    public void rotate() {
+    public Map<K, V> rotate() {
         Map<K, V> dead = _buckets.removeLast();
         _buckets.addFirst(new HashMap<K, V>());
         if(_callback!=null) {
             for(Entry<K, V> entry: dead.entrySet()) {
                 _callback.expire(entry.getKey(), entry.getValue());
             }
         }
+        return dead;
     }
 
     public boolean containsKey(K key) {

File: src/jvm/backtype/storm/spout/ShellSpout.java
Patch:
@@ -35,7 +35,7 @@ public void open(Map stormConf, TopologyContext context,
             Number subpid = _process.launch(stormConf, context);
             LOG.info("Launched subprocess with pid " + subpid);
         } catch (IOException e) {
-            throw new RuntimeException("Error when launching multilang subprocess", e);
+            throw new RuntimeException("Error when launching multilang subprocess\n" + _process.getErrorsString(), e);
         }
     }
 

File: src/jvm/backtype/storm/task/ShellBolt.java
Patch:
@@ -79,7 +79,7 @@ public void prepare(Map stormConf, TopologyContext context,
             Number subpid = _process.launch(stormConf, context);
             LOG.info("Launched subprocess with pid " + subpid);
         } catch (IOException e) {
-            throw new RuntimeException("Error when launching multilang subprocess", e);
+            throw new RuntimeException("Error when launching multilang subprocess\n" + _process.getErrorsString(), e);
         }
 
         // reader

File: src/jvm/backtype/storm/serialization/KryoValuesSerializer.java
Patch:
@@ -3,9 +3,7 @@
 import backtype.storm.utils.ListDelegate;
 import com.esotericsoftware.kryo.Kryo;
 import com.esotericsoftware.kryo.io.Output;
-import java.io.ByteArrayOutputStream;
 import java.io.IOException;
-import java.io.OutputStream;
 import java.util.List;
 import java.util.Map;
 

File: src/jvm/backtype/storm/serialization/SerializationFactory.java
Patch:
@@ -14,7 +14,6 @@
 import com.esotericsoftware.kryo.Kryo;
 import com.esotericsoftware.kryo.Serializer;
 import com.esotericsoftware.kryo.serializers.DefaultSerializers.BigIntegerSerializer;
-import com.esotericsoftware.kryo.serializers.JavaSerializer;
 import java.math.BigInteger;
 import java.util.ArrayList;
 import java.util.Collections;
@@ -38,7 +37,7 @@ public void overrideDefault(boolean value) {
         @Override
         public Serializer getDefaultSerializer(Class type) {
             if(_override) {
-                return new JavaSerializer();
+                return new SerializableSerializer();
             } else {
                 return super.getDefaultSerializer(type);
             }

File: src/jvm/backtype/storm/testing/BatchNumberList.java
Patch:
@@ -33,12 +33,10 @@ public BatchNumberList(String wordComponent) {
     @Override
     public void prepare(Map conf, TopologyContext context, BatchOutputCollector collector, Object id) {
         _collector = collector;
-        System.out.println("STARTED: " + id);
     }
 
     @Override
     public void execute(Tuple tuple) {
-        System.out.println("RECEIVED: " + tuple.toString());
         if(tuple.getSourceComponent().equals(_wordComponent)) {
             this.word = tuple.getString(1);
         } else {

File: src/jvm/backtype/storm/scheduler/Cluster.java
Patch:
@@ -69,7 +69,7 @@ public List<TopologyDetails> needsSchedulingTopologies(Topologies topologies) {
      * </ul>
      */
     public boolean needsScheduling(TopologyDetails topology) {
-        int desiredNumWorkers = ((Number) topology.getConf().get(Config.TOPOLOGY_WORKERS)).intValue();
+        int desiredNumWorkers = topology.getNumWorkers();
         int assignedNumWorkers = this.getAssignedNumWorkers(topology);
 
         if (desiredNumWorkers > assignedNumWorkers) {

File: src/jvm/backtype/storm/scheduler/INimbus.java
Patch:
@@ -10,7 +10,7 @@ public interface INimbus {
     //   1. if some slots are used, return as much as it currently has available
     //   2. otherwise return nothing until it has enough slots, or enough time has passed
     // sets the node id as {normalized hostname (invalid chars removed}-{topologyid}
-    Collection<WorkerSlot> availableSlots(Collection<SupervisorDetails> existingSupervisors, Collection<WorkerSlot> usedSlots, TopologyDetails topology);
+    Collection<WorkerSlot> availableSlots(Collection<SupervisorDetails> existingSupervisors, Collection<WorkerSlot> usedSlots, Collection<TopologyDetails> topologies);
     // mesos should call launchTasks on an executor for this topology... 
     // gives it the executor with:
     //   - name: the node id

File: src/jvm/backtype/storm/tuple/TupleImpl.java
Patch:
@@ -1,7 +1,6 @@
 package backtype.storm.tuple;
 
 import backtype.storm.task.GeneralTopologyContext;
-import backtype.storm.task.TopologyContext;
 import java.util.List;
 
 public class TupleImpl extends Tuple {

File: src/jvm/backtype/storm/StormSubmitter.java
Patch:
@@ -81,6 +81,9 @@ private static void submitJar(Map conf) {
     }
     
     public static String submitJar(Map conf, String localJar) {
+        if(localJar==null) {
+            throw new RuntimeException("Must submit topologies using the 'storm' client script so that StormSubmitter knows which jar to upload.");
+        }
         NimbusClient client = NimbusClient.getConfiguredClient(conf);
         try {
             String uploadLocation = client.getClient().beginFileUpload();

File: src/jvm/backtype/storm/coordination/CoordinatedBolt.java
Patch:
@@ -278,7 +278,6 @@ public void execute(Tuple tuple) {
             } else {
                 _collector.ack(tuple);
             }
-            
         } else if(!_sourceArgs.isEmpty()
                 && tuple.getSourceStreamId().equals(Constants.COORDINATED_STREAM_ID)) {
             int count = (Integer) tuple.getValue(1);
@@ -299,8 +298,10 @@ public void execute(Tuple tuple) {
         }
     }
 
+    @Override
     public void cleanup() {
         _delegate.cleanup();
+        _tracked.cleanup();
     }
 
     public void declareOutputFields(OutputFieldsDeclarer declarer) {

File: src/jvm/backtype/storm/grouping/CustomStreamGrouping.java
Patch:
@@ -1,10 +1,9 @@
 package backtype.storm.grouping;
 
-import backtype.storm.task.TopologyContext;
+import backtype.storm.task.WorkerTopologyContext;
 import backtype.storm.tuple.Fields;
 import java.io.Serializable;
 import java.util.List;
-import java.util.Map;
 
 public interface CustomStreamGrouping extends Serializable {
     
@@ -14,7 +13,7 @@ public interface CustomStreamGrouping extends Serializable {
      * 
      * It also tells the grouping the metadata on the stream this grouping will be used on.
      */
-   void prepare(TopologyContext context, Fields outFields, List<Integer> targetTasks);
+   void prepare(WorkerTopologyContext context, Fields outFields, List<Integer> targetTasks);
     
    /**
      * This function implements a custom stream grouping. It takes in as input

File: src/jvm/backtype/storm/hooks/ITaskHook.java
Patch:
@@ -16,5 +16,4 @@ public interface ITaskHook {
     void spoutFail(SpoutFailInfo info);
     void boltAck(BoltAckInfo info);
     void boltFail(BoltFailInfo info);
-    void error(Throwable error);
 }

File: src/jvm/backtype/storm/hooks/info/BoltAckInfo.java
Patch:
@@ -4,9 +4,9 @@
 
 public class BoltAckInfo {
     public Tuple tuple;
-    public long processLatencyMs;
+    public Long processLatencyMs; // null if it wasn't sampled
     
-    public BoltAckInfo(Tuple tuple, long processLatencyMs) {
+    public BoltAckInfo(Tuple tuple, Long processLatencyMs) {
         this.tuple = tuple;
         this.processLatencyMs = processLatencyMs;
     }

File: src/jvm/backtype/storm/hooks/info/BoltFailInfo.java
Patch:
@@ -4,9 +4,9 @@
 
 public class BoltFailInfo {
     public Tuple tuple;
-    public long failLatencyMs;
+    public Long failLatencyMs; // null if it wasn't sampled
     
-    public BoltFailInfo(Tuple tuple, long failLatencyMs) {
+    public BoltFailInfo(Tuple tuple, Long failLatencyMs) {
         this.tuple = tuple;
         this.failLatencyMs = failLatencyMs;
     }

File: src/jvm/backtype/storm/hooks/info/SpoutAckInfo.java
Patch:
@@ -2,9 +2,9 @@
 
 public class SpoutAckInfo {
     public Object messageId;
-    public long completeLatencyMs;
+    public Long completeLatencyMs; // null if it wasn't sampled
     
-    public SpoutAckInfo(Object messageId, long completeLatencyMs) {
+    public SpoutAckInfo(Object messageId, Long completeLatencyMs) {
         this.messageId = messageId;
         this.completeLatencyMs = completeLatencyMs;
     }

File: src/jvm/backtype/storm/hooks/info/SpoutFailInfo.java
Patch:
@@ -2,9 +2,9 @@
 
 public class SpoutFailInfo {
     public Object messageId;
-    public long failLatencyMs;
+    public Long failLatencyMs; // null if it wasn't sampled
     
-    public SpoutFailInfo(Object messageId, long failLatencyMs) {
+    public SpoutFailInfo(Object messageId, Long failLatencyMs) {
         this.messageId = messageId;
         this.failLatencyMs = failLatencyMs;
     }

File: src/jvm/backtype/storm/serialization/SerializationFactory.java
Patch:
@@ -4,6 +4,7 @@
 import backtype.storm.generated.ComponentCommon;
 import backtype.storm.generated.StormTopology;
 import backtype.storm.transactional.TransactionAttempt;
+import backtype.storm.tuple.Values;
 import backtype.storm.utils.ListDelegate;
 import backtype.storm.utils.Utils;
 import carbonite.JavaBridge;
@@ -52,6 +53,7 @@ public static ObjectBuffer getKryo(Map conf) {
         k.register(HashSet.class);
         k.register(BigInteger.class, new BigIntegerSerializer());
         k.register(TransactionAttempt.class);
+        k.register(Values.class);
         JavaBridge clojureSerializersBridge = new JavaBridge();
         clojureSerializersBridge.registerClojureCollections(k);
         clojureSerializersBridge.registerClojurePrimitives(k);

File: src/jvm/backtype/storm/testing/NGrouping.java
Patch:
@@ -1,7 +1,7 @@
 package backtype.storm.testing;
 
 import backtype.storm.grouping.CustomStreamGrouping;
-import backtype.storm.task.TopologyContext;
+import backtype.storm.task.WorkerTopologyContext;
 import backtype.storm.tuple.Fields;
 import java.util.ArrayList;
 import java.util.Collections;
@@ -16,7 +16,7 @@ public NGrouping(int n) {
     }
     
     @Override
-    public void prepare(TopologyContext context, Fields outFields, List<Integer> targetTasks) {
+    public void prepare(WorkerTopologyContext context, Fields outFields, List<Integer> targetTasks) {
         targetTasks = new ArrayList<Integer>(targetTasks);
         Collections.sort(targetTasks);
         _outTasks = new ArrayList<Integer>();

File: src/jvm/backtype/storm/topology/ComponentConfigurationDeclarer.java
Patch:
@@ -7,5 +7,6 @@ public interface ComponentConfigurationDeclarer<T extends ComponentConfiguration
     T addConfiguration(String config, Object value);
     T setDebug(boolean debug);
     T setMaxTaskParallelism(Number val);
-    T setMaxSpoutPending(Number val);    
+    T setMaxSpoutPending(Number val);
+    T setNumTasks(Number val);
 }

File: src/jvm/backtype/storm/coordination/CoordinatedBolt.java
Patch:
@@ -311,6 +311,7 @@ public void execute(Tuple tuple) {
 
     public void cleanup() {
         _delegate.cleanup();
+        _tracked.cleanup();
     }
 
     public void declareOutputFields(OutputFieldsDeclarer declarer) {

File: src/jvm/backtype/storm/testing/TestGlobalCount.java
Patch:
@@ -7,8 +7,8 @@
 import backtype.storm.tuple.Fields;
 import java.util.Map;
 import backtype.storm.task.TopologyContext;
+import backtype.storm.tuple.Values;
 import org.apache.log4j.Logger;
-import static backtype.storm.utils.Utils.tuple;
 
 
 public class TestGlobalCount extends BaseRichBolt {
@@ -24,7 +24,7 @@ public void prepare(Map stormConf, TopologyContext context, OutputCollector coll
 
     public void execute(Tuple input) {
         _count++;
-        _collector.emit(tuple(_count));
+        _collector.emit(input, new Values(_count));
         _collector.ack(input);
     }
 

File: src/jvm/backtype/storm/transactional/TransactionalTopologyBuilder.java
Patch:
@@ -52,7 +52,7 @@ public TransactionalTopologyBuilder(String id, String spoutId, ITransactionalSpo
         _id = id;
         _spoutId = spoutId;
         _spout = spout;
-        _spoutParallelism = spoutParallelism.intValue();
+        _spoutParallelism = (spoutParallelism == null) ? null : spoutParallelism.intValue();
     }
     
     public TransactionalTopologyBuilder(String id, String spoutId, ITransactionalSpout spout) {

File: src/jvm/backtype/storm/transactional/TransactionalTopologyBuilder.java
Patch:
@@ -52,7 +52,7 @@ public TransactionalTopologyBuilder(String id, String spoutId, ITransactionalSpo
         _id = id;
         _spoutId = spoutId;
         _spout = spout;
-        _spoutParallelism = spoutParallelism.intValue();
+        _spoutParallelism = (spoutParallelism == null) ? null : spoutParallelism.intValue();
     }
     
     public TransactionalTopologyBuilder(String id, String spoutId, ITransactionalSpout spout) {

File: src/jvm/backtype/storm/coordination/CoordinatedBolt.java
Patch:
@@ -299,8 +299,10 @@ public void execute(Tuple tuple) {
         }
     }
 
+    @Override
     public void cleanup() {
         _delegate.cleanup();
+        _tracked.cleanup();
     }
 
     public void declareOutputFields(OutputFieldsDeclarer declarer) {

File: src/jvm/backtype/storm/task/TopologyContext.java
Patch:
@@ -34,7 +34,7 @@ public class TopologyContext extends WorkerTopologyContext {
     public TopologyContext(StormTopology topology, Map stormConf,
             Map<Integer, String> taskToComponent, String stormId,
             String codeDir, String pidDir, Integer taskId,
-            Integer workerPort, List<Integer> workerTasks) {
+            Integer workerPort, List<Number> workerTasks) {
         super(topology, stormConf, taskToComponent, stormId, codeDir, pidDir, workerPort, workerTasks);
         _taskId = taskId;
     }

File: src/jvm/backtype/storm/topology/ComponentConfigurationDeclarer.java
Patch:
@@ -7,5 +7,6 @@ public interface ComponentConfigurationDeclarer<T extends ComponentConfiguration
     T addConfiguration(String config, Object value);
     T setDebug(boolean debug);
     T setMaxTaskParallelism(Number val);
-    T setMaxSpoutPending(Number val);    
+    T setMaxSpoutPending(Number val);
+    T setNumTasks(Number val);
 }

File: src/jvm/backtype/storm/serialization/SerializationFactory.java
Patch:
@@ -4,6 +4,7 @@
 import backtype.storm.generated.ComponentCommon;
 import backtype.storm.generated.StormTopology;
 import backtype.storm.transactional.TransactionAttempt;
+import backtype.storm.tuple.Values;
 import backtype.storm.utils.ListDelegate;
 import backtype.storm.utils.Utils;
 import carbonite.JavaBridge;
@@ -52,6 +53,7 @@ public static ObjectBuffer getKryo(Map conf) {
         k.register(HashSet.class);
         k.register(BigInteger.class, new BigIntegerSerializer());
         k.register(TransactionAttempt.class);
+        k.register(Values.class);
         JavaBridge clojureSerializersBridge = new JavaBridge();
         clojureSerializersBridge.registerClojureCollections(k);
         clojureSerializersBridge.registerClojurePrimitives(k);

File: src/jvm/backtype/storm/StormSubmitter.java
Patch:
@@ -81,6 +81,9 @@ private static void submitJar(Map conf) {
     }
     
     public static String submitJar(Map conf, String localJar) {
+        if(localJar==null) {
+            throw new RuntimeException("Must submit topologies using the 'storm' client script so that StormSubmitter knows which jar to upload.");
+        }
         NimbusClient client = NimbusClient.getConfiguredClient(conf);
         try {
             String uploadLocation = client.getClient().beginFileUpload();

File: src/jvm/backtype/storm/utils/Utils.java
Patch:
@@ -103,7 +103,7 @@ public static Map findAndReadConfigFile(String name, boolean mustExist) {
                 else return new HashMap();
             }
             if(resources.size() > 1) {
-                throw new RuntimeException("Found multiple " + name + " resources");
+                throw new RuntimeException("Found multiple " + name + " resources. You're probably bundling the Storm jars with your topology jar.");
             }
             URL resource = resources.get(0);
             Yaml yaml = new Yaml();

File: src/jvm/backtype/storm/topology/ComponentConfigurationDeclarer.java
Patch:
@@ -6,6 +6,6 @@ public interface ComponentConfigurationDeclarer<T extends ComponentConfiguration
     T addConfigurations(Map conf);
     T addConfiguration(String config, Object value);
     T setDebug(boolean debug);
-    T setMaxTaskParallelism(Integer val);
-    T setMaxSpoutPending(Integer val);    
+    T setMaxTaskParallelism(Number val);
+    T setMaxSpoutPending(Number val);    
 }

File: src/jvm/backtype/storm/utils/IndifferentAccessMap.java
Patch:
@@ -80,13 +80,13 @@ public IPersistentMap assoc(Object k, Object v) {
         return new IndifferentAccessMap(getMap().assoc(k, v));
     }
 
-    public IPersistentMap assocEx(Object k, Object v) throws Exception {
+    public IPersistentMap assocEx(Object k, Object v) {
         if(k instanceof Keyword) return assocEx(((Keyword) k).getName(), v);
 
         return new IndifferentAccessMap(getMap().assocEx(k, v));
     }
 
-    public IPersistentMap without(Object k) throws Exception {
+    public IPersistentMap without(Object k) {
         if(k instanceof Keyword) return without(((Keyword) k).getName());
 
         return new IndifferentAccessMap(getMap().without(k));

File: src/jvm/backtype/storm/topology/ComponentConfigurationDeclarer.java
Patch:
@@ -6,6 +6,6 @@ public interface ComponentConfigurationDeclarer<T extends ComponentConfiguration
     T addConfigurations(Map conf);
     T addConfiguration(String config, Object value);
     T setDebug(boolean debug);
-    T setMaxTaskParallelism(Integer val);
-    T setMaxSpoutPending(Integer val);    
+    T setMaxTaskParallelism(Number val);
+    T setMaxSpoutPending(Number val);    
 }

File: src/jvm/backtype/storm/coordination/BatchOutputCollector.java
Patch:
@@ -23,5 +23,7 @@ public void emitDirect(int taskId, List<Object> tuple) {
         emitDirect(taskId, Utils.DEFAULT_STREAM_ID, tuple);
     }
     
-    public abstract void emitDirect(int taskId, String streamId, List<Object> tuple);    
+    public abstract void emitDirect(int taskId, String streamId, List<Object> tuple); 
+    
+    public abstract void reportError(Throwable error);
 }

File: src/jvm/backtype/storm/topology/IBasicOutputCollector.java
Patch:
@@ -5,4 +5,5 @@
 public interface IBasicOutputCollector {
     List<Integer> emit(String streamId, List<Object> tuple);
     void emitDirect(int taskId, String streamId, List<Object> tuple);
+    void reportError(Throwable t);
 }

File: src/jvm/backtype/storm/transactional/partitioned/IOpaquePartitionedTransactionalSpout.java
Patch:
@@ -19,6 +19,7 @@ public interface Coordinator {
          * repeatedly in a loop).
          */
         boolean isReady();
+        void close();
     }
     
     public interface Emitter<X> {

File: src/jvm/backtype/storm/transactional/partitioned/IOpaquePartitionedTransactionalSpout.java
Patch:
@@ -19,6 +19,7 @@ public interface Coordinator {
          * repeatedly in a loop).
          */
         boolean isReady();
+        void close();
     }
     
     public interface Emitter<X> {

File: src/jvm/backtype/storm/coordination/BatchOutputCollector.java
Patch:
@@ -23,5 +23,7 @@ public void emitDirect(int taskId, List<Object> tuple) {
         emitDirect(taskId, Utils.DEFAULT_STREAM_ID, tuple);
     }
     
-    public abstract void emitDirect(int taskId, String streamId, List<Object> tuple);    
+    public abstract void emitDirect(int taskId, String streamId, List<Object> tuple); 
+    
+    public abstract void reportError(Throwable error);
 }

File: src/jvm/backtype/storm/topology/IBasicOutputCollector.java
Patch:
@@ -5,4 +5,5 @@
 public interface IBasicOutputCollector {
     List<Integer> emit(String streamId, List<Object> tuple);
     void emitDirect(int taskId, String streamId, List<Object> tuple);
+    void reportError(Throwable t);
 }

File: src/jvm/backtype/storm/task/IBolt.java
Patch:
@@ -33,7 +33,7 @@ public interface IBolt extends Serializable {
      * 
      * @param stormConf The Storm configuration for this bolt. This is the configuration provided to the topology merged in with cluster configuration on this machine.
      * @param context This object can be used to get information about this task's place within the topology, including the task id and component id of this task, input and output information, etc.
-     * @param collector The collector is used to emit tuples from this bolt. Tuples can be emitted at any time, including the prepare and cleanup methods. The collector is not thread-safe and should be saved as an instance variable of this bolt object.
+     * @param collector The collector is used to emit tuples from this bolt. Tuples can be emitted at any time, including the prepare and cleanup methods. The collector is thread-safe and should be saved as an instance variable of this bolt object.
      */
     void prepare(Map stormConf, TopologyContext context, OutputCollector collector);
 

File: src/jvm/backtype/storm/grouping/CustomStreamGrouping.java
Patch:
@@ -9,8 +9,8 @@
 public interface CustomStreamGrouping extends Serializable {
     
    /**
-     * Tells the stream grouping at runtime the number of tasks in the target bolt.
-     * This information should be used in taskIndicies to determine the target tasks.
+     * Tells the stream grouping at runtime the tasks in the target bolt.
+     * This information should be used in chooseTasks to determine the target tasks.
      * 
      * It also tells the grouping the metadata on the stream this grouping will be used on.
      */

File: src/jvm/backtype/storm/spout/ISpoutOutputCollector.java
Patch:
@@ -8,5 +8,6 @@ public interface ISpoutOutputCollector {
     */
     List<Integer> emit(String streamId, List<Object> tuple, Object messageId);
     void emitDirect(int taskId, String streamId, List<Object> tuple, Object messageId);
+    void reportError(Throwable error);
 }
 

File: src/jvm/backtype/storm/hooks/info/BoltAckInfo.java
Patch:
@@ -4,10 +4,10 @@
 
 public class BoltAckInfo {
     public Tuple tuple;
-    public long completeLatencyMs;
+    public long processLatencyMs;
     
-    public BoltAckInfo(Tuple tuple, long completeLatencyMs) {
+    public BoltAckInfo(Tuple tuple, long processLatencyMs) {
         this.tuple = tuple;
-        this.completeLatencyMs = completeLatencyMs;
+        this.processLatencyMs = processLatencyMs;
     }
 }

File: src/jvm/backtype/storm/StormSubmitter.java
Patch:
@@ -40,6 +40,9 @@ public static void setLocalNimbus(Nimbus.Iface localNimbusHandler) {
      * @throws InvalidTopologyException if an invalid topology was submitted
      */
     public static void submitTopology(String name, Map stormConf, StormTopology topology) throws AlreadyAliveException, InvalidTopologyException {
+        if(!Utils.isValidConf(stormConf)) {
+            throw new IllegalArgumentException("Storm conf is not valid. Must be json-serializable");
+        }
         stormConf = new HashMap(stormConf);
         stormConf.putAll(Utils.readCommandLineOpts());
         Map conf = Utils.readStormConfig();

File: src/jvm/backtype/storm/drpc/DRPCSpout.java
Patch:
@@ -149,6 +149,6 @@ public void declareOutputFields(OutputFieldsDeclarer declarer) {
 
     @Override
     public Map<String, Object> getComponentConfiguration() {
-        return new HashMap<String, Object>();
+        return null;
     }
 }

File: src/jvm/backtype/storm/topology/base/BaseComponent.java
Patch:
@@ -1,12 +1,11 @@
 package backtype.storm.topology.base;
 
 import backtype.storm.topology.IComponent;
-import java.util.HashMap;
 import java.util.Map;
 
 public abstract class BaseComponent implements IComponent {
     @Override
     public Map<String, Object> getComponentConfiguration() {
-        return new HashMap<String, Object>();
+        return null;
     }    
 }

File: src/jvm/backtype/storm/transactional/partitioned/OpaquePartitionedTransactionalSpoutExecutor.java
Patch:
@@ -43,7 +43,7 @@ public Emitter(Map conf, TopologyContext context) {
             _index = context.getThisTaskIndex();
             _numTasks = context.getComponentTasks(context.getThisComponentId()).size();
             _state = TransactionalState.newUserState(conf, (String) conf.get(Config.TOPOLOGY_TRANSACTIONAL_ID), getComponentConfiguration()); 
-            List<String> existingPartitions = _state.list("/");
+            List<String> existingPartitions = _state.list("");
             for(String p: existingPartitions) {
                 int partition = Integer.parseInt(p);
                 if((partition - _index) % _numTasks == 0) {

File: src/jvm/backtype/storm/transactional/TransactionalSpoutCoordinator.java
Patch:
@@ -80,7 +80,7 @@ public void nextTuple() {
     public void ack(Object msgId) {
         TransactionAttempt tx = (TransactionAttempt) msgId;
         TransactionStatus status = _activeTx.get(tx.getTransactionId());
-        if(tx.equals(status.attempt)) {
+        if(status!=null && tx.equals(status.attempt)) {
             if(status.status==AttemptStatus.PROCESSING) {
                 status.status = AttemptStatus.PROCESSED;
             } else if(status.status==AttemptStatus.COMMITTING) {

File: src/jvm/backtype/storm/coordination/CoordinatedBolt.java
Patch:
@@ -64,7 +64,7 @@ public String toString() {
         }
     }
 
-    public class CoordinatedOutputCollector extends OutputCollector {
+    public class CoordinatedOutputCollector implements IOutputCollector {
         IOutputCollector _delegate;
 
         public CoordinatedOutputCollector(IOutputCollector delegate) {
@@ -185,7 +185,7 @@ public void prepare(Map config, TopologyContext context, OutputCollector collect
         }
         _tracked = new TimeCacheMap<Object, TrackingInfo>(Utils.getInt(config.get(Config.TOPOLOGY_MESSAGE_TIMEOUT_SECS)), callback);
         _collector = collector;
-        _delegate.prepare(config, context, new CoordinatedOutputCollector(collector));
+        _delegate.prepare(config, context, new OutputCollector(new CoordinatedOutputCollector(collector)));
         for(String component: Utils.get(context.getThisTargets(),
                                         Constants.COORDINATED_STREAM_ID,
                                         new HashMap<String, Grouping>())

File: src/jvm/backtype/storm/task/OutputCollectorImpl.java
Patch:
@@ -27,6 +27,7 @@ public class OutputCollectorImpl extends OutputCollector {
     private Map<Tuple, List<Long>> _pendingAcks = new ConcurrentHashMap<Tuple, List<Long>>();
     
     public OutputCollectorImpl(TopologyContext context, IInternalOutputCollector collector) {
+        super(null); // TODO: remove
         _context = context;
         _collector = collector;
     }

File: src/jvm/backtype/storm/testing/NGrouping.java
Patch:
@@ -1,7 +1,7 @@
 package backtype.storm.testing;
 
 import backtype.storm.grouping.CustomStreamGrouping;
-import backtype.storm.tuple.Tuple;
+import backtype.storm.tuple.Fields;
 import java.util.ArrayList;
 import java.util.List;
 
@@ -13,11 +13,11 @@ public NGrouping(int n) {
     }
     
     @Override
-    public void prepare(int numTasks) {
+    public void prepare(Fields outFields, int numTasks) {
     }
 
     @Override
-    public List<Integer> taskIndices(Tuple tuple) {
+    public List<Integer> taskIndices(List<Object> values) {
         List<Integer> ret = new ArrayList<Integer>();
         for(int i=0; i<_n; i++) {
             ret.add(i);

File: src/jvm/backtype/storm/tuple/MessageId.java
Patch:
@@ -25,9 +25,9 @@ public static MessageId makeId(Map<Long, Long> anchorsToIds) {
         return new MessageId(anchorsToIds);
     }
         
-    public static MessageId makeRootId(long id) {
+    public static MessageId makeRootId(long id, long val) {
         Map<Long, Long> anchorsToIds = new HashMap<Long, Long>();
-        anchorsToIds.put(id, id);
+        anchorsToIds.put(id, val);
         return new MessageId(anchorsToIds);
     }
     

File: src/jvm/backtype/storm/transactional/TransactionalSpoutBatchExecutor.java
Patch:
@@ -34,13 +34,13 @@ public void execute(Tuple input) {
         try {
             _emitter.emitBatch(attempt, input.getValue(1), _collector);
             _collector.ack(input);
+            // this is valid here because the batch has been successfully emitted, 
+            // so we can safely delete metadata for prior transactions
+            _emitter.cleanupBefore((BigInteger) input.getValue(2));
         } catch(FailedException e) {
             LOG.warn("Failed to emit batch for transaction", e);
             _collector.fail(input);
         }
-        // this is valid here because the batch has been successfully emitted, 
-        // so we can safely delete metadata for prior transactions
-        _emitter.cleanupBefore((BigInteger) input.getValue(2));
     }
 
     @Override

File: src/jvm/backtype/storm/drpc/DRPCSpout.java
Patch:
@@ -57,7 +57,7 @@ public void open(Map conf, TopologyContext context, SpoutOutputCollector collect
 
             int port = Utils.getInt(conf.get(Config.DRPC_INVOCATIONS_PORT));
             List<String> servers = (List<String>) conf.get(Config.DRPC_SERVERS);
-            if(servers.isEmpty()) {
+            if(servers == null || servers.isEmpty()) {
                 throw new RuntimeException("No DRPC servers configured for topology");   
             }
             if(numTasks < servers.size()) {

File: src/jvm/backtype/storm/Constants.java
Patch:
@@ -1,8 +1,8 @@
 package backtype.storm;
 
-import backtype.storm.drpc.CoordinatedBolt;
+import backtype.storm.coordination.CoordinatedBolt;
 
 
 public class Constants {
-    public static final String COORDINATED_STREAM_ID = CoordinatedBolt.class.getName() + "/coord-stream";
+    public static final String COORDINATED_STREAM_ID = CoordinatedBolt.class.getName() + "/coord-stream";    
 }

File: src/jvm/backtype/storm/drpc/LinearDRPCInputDeclarer.java
Patch:
@@ -1,8 +1,9 @@
 package backtype.storm.drpc;
 
+import backtype.storm.topology.ComponentConfigurationDeclarer;
 import backtype.storm.tuple.Fields;
 
-public interface LinearDRPCInputDeclarer {
+public interface LinearDRPCInputDeclarer extends ComponentConfigurationDeclarer<LinearDRPCInputDeclarer> {
     public LinearDRPCInputDeclarer fieldsGrouping(Fields fields);
     public LinearDRPCInputDeclarer fieldsGrouping(String streamId, Fields fields);
 

File: src/jvm/backtype/storm/spout/ISpoutOutputCollector.java
Patch:
@@ -1,7 +1,6 @@
 package backtype.storm.spout;
 
 import java.util.List;
-import backtype.storm.tuple.Tuple;
 
 public interface ISpoutOutputCollector {
     /**

File: src/jvm/backtype/storm/task/IOutputCollector.java
Patch:
@@ -1,13 +1,13 @@
 package backtype.storm.task;
 
-import java.util.List;
 import backtype.storm.tuple.Tuple;
 import java.util.Collection;
+import java.util.List;
 
 public interface IOutputCollector {
     /**
-        Returns the task ids that received the tuples.
-    */
+     *  Returns the task ids that received the tuples.
+     */
     List<Integer> emit(String streamId, Collection<Tuple> anchors, List<Object> tuple);
     void emitDirect(int taskId, String streamId, Collection<Tuple> anchors, List<Object> tuple);
     void ack(Tuple input);

File: src/jvm/backtype/storm/task/OutputCollector.java
Patch:
@@ -12,7 +12,6 @@
  * form of stream processing, see IBasicBolt and BasicOutputCollector.
  */
 public abstract class OutputCollector implements IOutputCollector {
-
     /**
      * Emits a new tuple to a specific stream with a single anchor.
      *
@@ -26,7 +25,7 @@ public List<Integer> emit(String streamId, Tuple anchor, List<Object> tuple) {
     }
 
     /**
-     * Emits a new unanchored tuple to the specified stream. Beacuse it's unanchored,
+     * Emits a new unanchored tuple to the specified stream. Because it's unanchored,
      * if a failure happens downstream, this new tuple won't affect whether any
      * spout tuples are considered failed or not.
      * 

File: src/jvm/backtype/storm/task/ShellBolt.java
Patch:
@@ -14,7 +14,6 @@
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
-import java.util.UUID;
 import org.apache.log4j.Logger;
 import org.json.simple.JSONObject;
 import org.json.simple.JSONValue;

File: src/jvm/backtype/storm/testing/TestAggregatesCounter.java
Patch:
@@ -1,18 +1,18 @@
 package backtype.storm.testing;
 
+import backtype.storm.topology.base.BaseRichBolt;
 import backtype.storm.task.OutputCollector;
 import backtype.storm.topology.OutputFieldsDeclarer;
 import backtype.storm.tuple.Tuple;
 import backtype.storm.tuple.Fields;
 import java.util.Map;
 import backtype.storm.task.TopologyContext;
-import backtype.storm.topology.IRichBolt;
 import java.util.HashMap;
 import org.apache.log4j.Logger;
 import static backtype.storm.utils.Utils.tuple;
 
 
-public class TestAggregatesCounter implements IRichBolt {
+public class TestAggregatesCounter extends BaseRichBolt {
     public static Logger LOG = Logger.getLogger(TestWordCounter.class);
 
     Map<String, Integer> _counts;

File: src/jvm/backtype/storm/testing/TestGlobalCount.java
Patch:
@@ -1,17 +1,17 @@
 package backtype.storm.testing;
 
+import backtype.storm.topology.base.BaseRichBolt;
 import backtype.storm.task.OutputCollector;
 import backtype.storm.topology.OutputFieldsDeclarer;
 import backtype.storm.tuple.Tuple;
 import backtype.storm.tuple.Fields;
 import java.util.Map;
 import backtype.storm.task.TopologyContext;
-import backtype.storm.topology.IRichBolt;
 import org.apache.log4j.Logger;
 import static backtype.storm.utils.Utils.tuple;
 
 
-public class TestGlobalCount implements IRichBolt {
+public class TestGlobalCount extends BaseRichBolt {
     public static Logger LOG = Logger.getLogger(TestWordCounter.class);
 
     private int _count;

File: src/jvm/backtype/storm/testing/TestWordCounter.java
Patch:
@@ -1,18 +1,18 @@
 package backtype.storm.testing;
 
+import backtype.storm.topology.base.BaseBasicBolt;
 import backtype.storm.topology.OutputFieldsDeclarer;
 import backtype.storm.tuple.Tuple;
 import backtype.storm.tuple.Fields;
 import java.util.Map;
 import backtype.storm.task.TopologyContext;
 import backtype.storm.topology.BasicOutputCollector;
-import backtype.storm.topology.IBasicBolt;
 import java.util.HashMap;
 import org.apache.log4j.Logger;
 import static backtype.storm.utils.Utils.tuple;
 
 
-public class TestWordCounter implements IBasicBolt {
+public class TestWordCounter extends BaseBasicBolt {
     public static Logger LOG = Logger.getLogger(TestWordCounter.class);
 
     Map<String, Integer> _counts;

File: src/jvm/backtype/storm/tuple/MessageId.java
Patch:
@@ -1,5 +1,6 @@
 package backtype.storm.tuple;
 
+import backtype.storm.utils.Utils;
 import backtype.storm.utils.WritableUtils;
 import java.io.DataInputStream;
 import java.io.DataOutputStream;
@@ -8,13 +9,12 @@
 import java.util.Map;
 import java.util.Map.Entry;
 import java.util.Set;
-import java.util.UUID;
 
 public class MessageId {
     private Map<Long, Long> _anchorsToIds;
     
     public static long generateId() {
-        return UUID.randomUUID().getLeastSignificantBits();
+        return Utils.randomLong();
     }
 
     public static MessageId makeUnanchored() {

File: src/jvm/backtype/storm/tuple/Tuple.java
Patch:
@@ -1,9 +1,7 @@
 package backtype.storm.tuple;
 
 import backtype.storm.generated.GlobalStreamId;
-import backtype.storm.generated.Grouping;
 import backtype.storm.task.TopologyContext;
-import backtype.storm.utils.Utils;
 import clojure.lang.ILookup;
 import clojure.lang.Seqable;
 import clojure.lang.Indexed;
@@ -73,7 +71,7 @@ public Tuple(TopologyContext context, List<Object> values, int taskId, String st
     public Tuple(TopologyContext context, List<Object> values, int taskId, String streamId) {
         this(context, values, taskId, streamId, MessageId.makeUnanchored());
     }
-
+    
     public Tuple copyWithNewId(long id) {
         Map<Long, Long> newIds = new HashMap<Long, Long>();
         for(Long anchor: this.id.getAnchorsToIds().keySet()) {

File: src/jvm/backtype/storm/drpc/DRPCSpout.java
Patch:
@@ -63,7 +63,7 @@ public void open(Map conf, TopologyContext context, SpoutOutputCollector collect
 
             int port = Utils.getInt(conf.get(Config.DRPC_INVOCATIONS_PORT));
             List<String> servers = (List<String>) conf.get(Config.DRPC_SERVERS);
-            if(servers.isEmpty()) {
+            if(servers == null || servers.isEmpty()) {
                 throw new RuntimeException("No DRPC servers configured for topology");   
             }
             if(numTasks < servers.size()) {

File: src/jvm/backtype/storm/transactional/TransactionalSpoutCoordinator.java
Patch:
@@ -57,7 +57,7 @@ public void open(Map conf, TopologyContext context, SpoutOutputCollector collect
         _coordinator = _spout.getCoordinator(conf, context);
         _currTransaction = getStoredCurrTransaction(_state);
         if(!conf.containsKey(Config.TOPOLOGY_MAX_SPOUT_PENDING)) {
-            _maxTransactionActive = 0;
+            _maxTransactionActive = 1;
         } else {
             _maxTransactionActive = Utils.getInt(conf.get(Config.TOPOLOGY_MAX_SPOUT_PENDING));
         }

File: src/jvm/backtype/storm/transactional/TransactionalSpoutBatchExecutor.java
Patch:
@@ -38,6 +38,8 @@ public void execute(Tuple input) {
             LOG.warn("Failed to emit batch for transaction", e);
             _collector.fail(input);
         }
+        // this is valid here because the batch has been successfully emitted, 
+        // so we can safely delete metadata for prior transactions
         _emitter.cleanupBefore((BigInteger) input.getValue(2));
     }
 

File: src/jvm/backtype/storm/transactional/TransactionalTopologyBuilder.java
Patch:
@@ -38,7 +38,7 @@ public class TransactionalTopologyBuilder {
     String _id;
     String _spoutId;
     ITransactionalSpout _spout;
-    Map<String, Component> _bolts;
+    Map<String, Component> _bolts = new HashMap<String, Component>();
     Integer _spoutParallelism;
     List<Map> _spoutConfs = new ArrayList();
     
@@ -91,8 +91,9 @@ public StormTopology buildTopology() {
         declarer.addConfiguration(Config.TOPOLOGY_TRANSACTIONAL_ID, _id);
 
         builder.setBolt(_spoutId,
+                // TODO: receiving the commit stream should not make it send out coordinated tuples
+                // to consumers... ***********************************
                         new CoordinatedBolt(new TransactionalSpoutBatchExecutor(_spout),
-                                             coordinator,
                                              null,
                                              null),
                         _spoutParallelism)

File: src/jvm/backtype/storm/transactional/TransactionalSpoutCoordinator.java
Patch:
@@ -43,7 +43,7 @@ public TransactionalSpoutCoordinator(ITransactionalSpout spout) {
     
     @Override
     public void open(Map conf, TopologyContext context, SpoutOutputCollector collector) {
-        _state = TransactionalState.newCoordinatorState(conf, (String) conf.get(Config.TOPOLOGY_TRANSACTIONAL_ID), _spout);
+        _state = TransactionalState.newCoordinatorState(conf, (String) conf.get(Config.TOPOLOGY_TRANSACTIONAL_ID), _spout.getComponentConfiguration());
         _coordinatorState = new RotatingTransactionalState(_state, META_DIR, true);
         _collector = collector;
         _coordinator = _spout.getCoordinator(conf, context);

File: src/jvm/backtype/storm/transactional/partitioned/PartitionedTransactionalSpoutExecutor.java
Patch:
@@ -47,7 +47,7 @@ class Emitter implements ITransactionalSpout.Emitter<Integer> {
         
         public Emitter(Map conf, TopologyContext context) {
             _emitter = _spout.getEmitter(conf, context);
-            _state = TransactionalState.newUserState(conf, (String) conf.get(Config.TOPOLOGY_TRANSACTIONAL_ID), PartitionedTransactionalSpoutExecutor.this); 
+            _state = TransactionalState.newUserState(conf, (String) conf.get(Config.TOPOLOGY_TRANSACTIONAL_ID), getComponentConfiguration()); 
             _index = context.getThisTaskIndex();
             _numTasks = context.getComponentTasks(context.getThisComponentId()).size();
         }

File: src/jvm/backtype/storm/transactional/TransactionalTopologyBuilder.java
Patch:
@@ -31,7 +31,7 @@
  * essentially want to implement a file lock on top of zk (use ephemeral nodes?)
  * or just use the topology name?
  * 
- * /
+ */
 
 public class TransactionalTopologyBuilder {
     String _id;

File: src/jvm/backtype/storm/spout/ISpoutOutputCollector.java
Patch:
@@ -1,7 +1,6 @@
 package backtype.storm.spout;
 
 import java.util.List;
-import backtype.storm.tuple.Tuple;
 
 public interface ISpoutOutputCollector {
     /**

File: src/jvm/backtype/storm/transactional/TransactionAttempt.java
Patch:
@@ -32,6 +32,6 @@ public boolean equals(Object o) {
 
     @Override
     public String toString() {
-        return "" + _txid + ": " + _attemptId;
+        return "" + _txid + ":" + _attemptId;
     }    
 }

File: src/jvm/backtype/storm/transactional/TransactionalSpoutCoordinator.java
Patch:
@@ -101,6 +101,8 @@ public void declareOutputFields(OutputFieldsDeclarer declarer) {
     }
     
     private void sync() {
+        // TODO: this code might be redundant. can just find the next transaction that needs a batch or commit tuple
+        // and emit that, instead of iterating through (MAX_SPOUT_PENDING should take care of things)
         TransactionStatus maybeCommit = _activeTx.get(_currTransaction);
         if(maybeCommit!=null && maybeCommit.status == AttemptStatus.PROCESSED) {
             maybeCommit.status = AttemptStatus.COMMITTING;

File: src/jvm/backtype/storm/transactional/TransactionalTopologyBuilder.java
Patch:
@@ -29,6 +29,7 @@
 /**
  * TODO: check to see if there are two topologies active with the same transactional id 
  * essentially want to implement a file lock on top of zk (use ephemeral nodes?)
+ * or just use the topology name?
  * 
  * Testing TODO:
  * 
@@ -45,6 +46,7 @@
  * 8. Test that it picks up where it left off when restarting the topology
  *       - run topology and restart it
  * 9. Test that coordinator and partitioned state are cleaned up properly (and not too early) - test rotatingtransactionalstate
+ * 10. Test that it repeats the meta on a fail instead of recmoputing (for both partition state and coordinator state)
  */
 public class TransactionalTopologyBuilder {
     String _id;

File: src/jvm/backtype/storm/utils/Utils.java
Patch:
@@ -209,7 +209,7 @@ public static long randomLong() {
     public static CuratorFramework newCurator(Map conf, String root) {
         List<String> serverPorts = new ArrayList<String>();
         for(String zkServer: (List<String>) conf.get(Config.STORM_ZOOKEEPER_SERVERS)) {
-            serverPorts.add(zkServer + Utils.getInt(conf.get(Config.STORM_ZOOKEEPER_PORT)));
+            serverPorts.add(zkServer + ":" + Utils.getInt(conf.get(Config.STORM_ZOOKEEPER_PORT)));
         }
         String zkStr = StringUtils.join(serverPorts, ",") + root; 
         try {

File: src/jvm/backtype/storm/transactional/partitioned/PartitionedTransactionalSpoutExecutor.java
Patch:
@@ -54,7 +54,7 @@ public Emitter(Map conf, TopologyContext context) {
         @Override
         public void emitBatch(final TransactionAttempt tx, final Object coordinatorMeta,
                 final TransactionalOutputCollector collector) {
-            int partitions = (int) coordinatorMeta;
+            int partitions = (Integer) coordinatorMeta;
             for(int i=_index; i < partitions; i+=_numTasks) {
                 if(!_partitionStates.containsKey(i)) {
                     _partitionStates.put(i, new RotatingTransactionalState(_state, "" + i));

File: src/jvm/backtype/storm/transactional/ITransactionalSpout.java
Patch:
@@ -16,7 +16,7 @@ public interface Emitter {
         // must always emit same batch for same transaction id
         // must emit attempt as first field in output tuple (any way to enforce this?)
         // for kafka: get up to X tuples, emit, store number of tuples for that partition in zk
-        Object emitBatch(TransactionAttempt tx, TransactionalOutputCollector collector);
+        void emitBatch(TransactionAttempt tx, Object coordinatorMeta, TransactionalOutputCollector collector);
         //can do things like cleanup user state in zk
         void cleanupBefore(BigInteger txid);
         void close();

File: src/jvm/backtype/storm/transactional/TransactionalSpoutBatchExecutor.java
Patch:
@@ -31,7 +31,7 @@ public void execute(Tuple input) {
             _emitter.cleanupBefore(attempt.getTransactionId());
         } else {
             _collector.setAnchor(input);
-            _emitter.emitBatch((TransactionAttempt) input.getValue(0), _collector);
+            _emitter.emitBatch(attempt, input.getValue(1), _collector);
             _collector.ack(input);
         }
     }

File: src/jvm/backtype/storm/transactional/state/TransactionalState.java
Patch:
@@ -10,8 +10,6 @@
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
-import java.util.logging.Level;
-import java.util.logging.Logger;
 import org.apache.zookeeper.CreateMode;
 
 public class TransactionalState {

File: src/jvm/backtype/storm/serialization/SerializationFactory.java
Patch:
@@ -10,7 +10,9 @@
 import com.esotericsoftware.kryo.Kryo;
 import com.esotericsoftware.kryo.ObjectBuffer;
 import com.esotericsoftware.kryo.Serializer;
+import com.esotericsoftware.kryo.serialize.BigIntegerSerializer;
 import com.esotericsoftware.kryo.serialize.SerializableSerializer;
+import java.math.BigInteger;
 import java.util.ArrayList;
 import java.util.Collections;
 import java.util.HashMap;
@@ -49,6 +51,7 @@ public static ObjectBuffer getKryo(Map conf) {
         k.register(HashMap.class);
         k.register(HashSet.class);
         k.register(TransactionAttempt.class);
+        k.register(BigInteger.class, new BigIntegerSerializer());
         JavaBridge clojureSerializersBridge = new JavaBridge();
         clojureSerializersBridge.registerClojureCollections(k);
         clojureSerializersBridge.registerClojurePrimitives(k);

File: src/jvm/backtype/storm/utils/Time.java
Patch:
@@ -76,6 +76,6 @@ public static boolean isThreadWaiting(Thread t) {
         synchronized(sleepTimesLock) {
             time = threadSleepTimes.get(t);
         }
-        return time!=null && currentTimeMillis() < time.longValue();
+        return !t.isAlive() || time!=null && currentTimeMillis() < time.longValue();
     }    
 }

File: src/jvm/backtype/storm/tuple/Tuple.java
Patch:
@@ -382,8 +382,8 @@ private PersistentArrayMap toMap() {
         Object array[] = new Object[values.size()*2];
         List<String> fields = getFields().toList();
         for(int i=0; i < values.size(); i++) {
-            array[i] = fields.get(i);
-            array[i+1] = values.get(i);
+            array[i*2] = fields.get(i);
+            array[(i*2)+1] = values.get(i);
         }
         return new PersistentArrayMap(array);
     }

File: src/jvm/backtype/storm/topology/TopologyBuilder.java
Patch:
@@ -213,9 +213,10 @@ private ComponentCommon getComponentCommon(String id, IComponent component) {
     
     private void initCommon(String id, IComponent component, Integer parallelism) {
         ComponentCommon common = new ComponentCommon();
-        common.set_parallelism_hint(parallelism);
+        common.set_inputs(new HashMap<GlobalStreamId, Grouping>());
+        if(parallelism!=null) common.set_parallelism_hint(parallelism);
         Map conf = component.getComponentConfiguration();
-        common.set_json_conf(JSONValue.toJSONString(conf));
+        if(conf!=null) common.set_json_conf(JSONValue.toJSONString(conf));
         _commons.put(id, common);
     }
 

File: src/jvm/backtype/storm/drpc/DRPCInvocationsClient.java
Patch:
@@ -1,4 +1,4 @@
-package backtype.storm.utils;
+package backtype.storm.drpc;
 
 import backtype.storm.generated.DRPCRequest;
 import backtype.storm.generated.DistributedRPCInvocations;

File: src/jvm/backtype/storm/drpc/DRPCSpout.java
Patch:
@@ -3,15 +3,13 @@
 import backtype.storm.Config;
 import backtype.storm.ILocalDRPC;
 import backtype.storm.generated.DRPCRequest;
-import backtype.storm.generated.DistributedRPC;
 import backtype.storm.generated.DistributedRPCInvocations;
 import backtype.storm.spout.SpoutOutputCollector;
 import backtype.storm.task.TopologyContext;
 import backtype.storm.topology.IRichSpout;
 import backtype.storm.topology.OutputFieldsDeclarer;
 import backtype.storm.tuple.Fields;
 import backtype.storm.tuple.Values;
-import backtype.storm.utils.DRPCInvocationsClient;
 import backtype.storm.utils.ServiceRegistry;
 import backtype.storm.utils.Utils;
 import java.util.ArrayList;

File: src/jvm/backtype/storm/drpc/ReturnResults.java
Patch:
@@ -7,7 +7,6 @@
 import backtype.storm.topology.IRichBolt;
 import backtype.storm.topology.OutputFieldsDeclarer;
 import backtype.storm.tuple.Tuple;
-import backtype.storm.utils.DRPCInvocationsClient;
 import backtype.storm.utils.ServiceRegistry;
 import backtype.storm.utils.Utils;
 import java.util.ArrayList;

File: src/jvm/backtype/storm/drpc/DRPCInvocationsClient.java
Patch:
@@ -1,4 +1,4 @@
-package backtype.storm.utils;
+package backtype.storm.drpc;
 
 import backtype.storm.generated.DRPCRequest;
 import backtype.storm.generated.DistributedRPCInvocations;

File: src/jvm/backtype/storm/drpc/DRPCSpout.java
Patch:
@@ -3,15 +3,13 @@
 import backtype.storm.Config;
 import backtype.storm.ILocalDRPC;
 import backtype.storm.generated.DRPCRequest;
-import backtype.storm.generated.DistributedRPC;
 import backtype.storm.generated.DistributedRPCInvocations;
 import backtype.storm.spout.SpoutOutputCollector;
 import backtype.storm.task.TopologyContext;
 import backtype.storm.topology.IRichSpout;
 import backtype.storm.topology.OutputFieldsDeclarer;
 import backtype.storm.tuple.Fields;
 import backtype.storm.tuple.Values;
-import backtype.storm.utils.DRPCInvocationsClient;
 import backtype.storm.utils.ServiceRegistry;
 import backtype.storm.utils.Utils;
 import java.util.ArrayList;

File: src/jvm/backtype/storm/drpc/ReturnResults.java
Patch:
@@ -7,7 +7,6 @@
 import backtype.storm.topology.IRichBolt;
 import backtype.storm.topology.OutputFieldsDeclarer;
 import backtype.storm.tuple.Tuple;
-import backtype.storm.utils.DRPCInvocationsClient;
 import backtype.storm.utils.ServiceRegistry;
 import backtype.storm.utils.Utils;
 import java.util.ArrayList;

File: src/jvm/backtype/storm/ILocalDRPC.java
Patch:
@@ -2,8 +2,9 @@
 
 import backtype.storm.daemon.Shutdownable;
 import backtype.storm.generated.DistributedRPC;
+import backtype.storm.generated.DistributedRPCInvocations;
 
 
-public interface ILocalDRPC extends DistributedRPC.Iface, Shutdownable {
+public interface ILocalDRPC extends DistributedRPC.Iface, DistributedRPCInvocations.Iface, Shutdownable {
     public String getServiceId();    
 }

File: src/jvm/backtype/storm/drpc/DRPCSpout.java
Patch:
@@ -62,7 +62,7 @@ public void open(Map conf, TopologyContext context, SpoutOutputCollector collect
             int numTasks = context.getComponentTasks(context.getThisComponentId()).size();
             int index = context.getThisTaskIndex();
 
-            int port = ((Long) conf.get(Config.DRPC_PORT)).intValue();
+            int port = Utils.getInt(conf.get(Config.DRPC_PORT));
             List<String> servers = (List<String>) conf.get(Config.DRPC_SERVERS);
             if(servers.isEmpty()) {
                 throw new RuntimeException("No DRPC servers configured for topology");   

File: src/jvm/backtype/storm/drpc/ReturnResults.java
Patch:
@@ -9,6 +9,7 @@
 import backtype.storm.tuple.Tuple;
 import backtype.storm.utils.DRPCClient;
 import backtype.storm.utils.ServiceRegistry;
+import backtype.storm.utils.Utils;
 import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.List;
@@ -36,7 +37,7 @@ public void execute(Tuple input) {
         if(returnInfo!=null) {
             Map retMap = (Map) JSONValue.parse(returnInfo);
             final String host = (String) retMap.get("host");
-            final int port = (int) ((Long) retMap.get("port")).longValue();                                   
+            final int port = Utils.getInt(retMap.get("port"));
             String id = (String) retMap.get("id");
             DistributedRPC.Iface client;
             if(local) {

File: src/jvm/backtype/storm/utils/NimbusClient.java
Patch:
@@ -13,7 +13,7 @@
 public class NimbusClient {
     public static NimbusClient getConfiguredClient(Map conf) {
         String nimbusHost = (String) conf.get(Config.NIMBUS_HOST);
-        int nimbusPort = ((Long) conf.get(Config.NIMBUS_THRIFT_PORT)).intValue();
+        int nimbusPort = Utils.getInt(conf.get(Config.NIMBUS_THRIFT_PORT));
         return new NimbusClient(nimbusHost, nimbusPort);
     }
 

File: src/jvm/backtype/storm/drpc/ReturnResults.java
Patch:
@@ -13,11 +13,13 @@
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
+import org.apache.log4j.Logger;
 import org.apache.thrift7.TException;
 import org.json.simple.JSONValue;
 
 
 public class ReturnResults implements IRichBolt {
+    public static final Logger LOG = Logger.getLogger(ReturnResults.class);
     OutputCollector _collector;
     boolean local;
 
@@ -55,6 +57,7 @@ public void execute(Tuple input) {
                 client.result(id, result);
                 _collector.ack(input);
             } catch(TException e) {
+                LOG.error("Failed to return results to DRPC server", e);
                 _collector.fail(input);
             }
         }

File: src/jvm/backtype/storm/serialization/SerializationFactory.java
Patch:
@@ -41,6 +41,7 @@ protected Serializer newDefaultSerializer(Class type) {
     public static ObjectBuffer getKryo(Map conf) {
         KryoSerializableDefault k = new KryoSerializableDefault();
         k.setRegistrationOptional((Boolean) conf.get(Config.TOPOLOGY_FALL_BACK_ON_JAVA_SERIALIZATION));
+        k.register(byte[].class);
         k.register(ListDelegate.class);
         k.register(ArrayList.class);
         k.register(HashMap.class);

File: src/jvm/backtype/storm/drpc/PrepareRequest.java
Patch:
@@ -9,10 +9,11 @@
 import backtype.storm.tuple.Values;
 import java.util.Map;
 import java.util.Random;
+import backtype.storm.utils.Utils;
 
 
 public class PrepareRequest implements IBasicBolt {
-    public static final String ARGS_STREAM = "args";
+    public static final String ARGS_STREAM = Utils.DEFAULT_STREAM_ID;
     public static final String RETURN_STREAM = "ret";
     public static final String ID_STREAM = "id";
 

File: src/jvm/backtype/storm/drpc/PrepareRequest.java
Patch:
@@ -16,6 +16,7 @@
 public class PrepareRequest implements IBasicBolt {
     public static final int ARGS_STREAM = 1;
     public static final int RETURN_STREAM = 2;
+    public static final int ID_STREAM = 3;
 
     Random rand;
 
@@ -29,6 +30,7 @@ public void execute(Tuple tuple, BasicOutputCollector collector) {
         long requestId = rand.nextLong();
         collector.emit(ARGS_STREAM, new Values(requestId, args));
         collector.emit(RETURN_STREAM, new Values(requestId, returnInfo));
+        collector.emit(ID_STREAM, new Values(requestId));
     }
 
     public void cleanup() {
@@ -37,7 +39,6 @@ public void cleanup() {
     public void declareOutputFields(OutputFieldsDeclarer declarer) {
         declarer.declareStream(ARGS_STREAM, new Fields("request", "args"));
         declarer.declareStream(RETURN_STREAM, new Fields("request", "return"));
+        declarer.declareStream(ID_STREAM, new Fields("request"));
     }
-
-
 }

File: src/jvm/backtype/storm/utils/DRPCClient.java
Patch:
@@ -9,7 +9,7 @@
 import org.apache.thrift7.transport.TSocket;
 import org.apache.thrift7.transport.TTransport;
 
-
+//TODO: needs to auto-reconnect
 public class DRPCClient implements DistributedRPC.Iface {
     private TTransport conn;
     private DistributedRPC.Client client;

File: src/jvm/backtype/storm/drpc/DRPCSpout.java
Patch:
@@ -17,7 +17,6 @@
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
-import java.util.logging.Level;
 import org.apache.log4j.Logger;
 import org.apache.thrift7.TException;
 import org.json.simple.JSONValue;

File: src/jvm/backtype/storm/tuple/Tuple.java
Patch:
@@ -271,6 +271,8 @@ private static final Keyword makeKeyword(String name) {
     
     @Override
     public Object valAt(Object o) {
+        // should change this to get by field name, and push metadata stuff like this
+        // into metadata
         if(o.equals(STREAM_KEYWORD)) {
             return getSourceStreamId();
         } else if(o.equals(COMPONENT_KEYWORD)) {

File: src/jvm/backtype/storm/task/ShellBolt.java
Patch:
@@ -92,7 +92,7 @@ public void prepare(Map stormConf, TopologyContext context, OutputCollector coll
             sendToSubprocess(JSONValue.toJSONString(stormConf));
             sendToSubprocess(context.toJSONString());
         } catch (IOException e) {
-            throw new RuntimeException(e);
+            throw new RuntimeException("Error when launching multilang subprocess", e);
         }
     }
 
@@ -173,7 +173,7 @@ public void execute(Tuple input) {
               }
             }
         } catch(IOException e) {
-            throw new RuntimeException(e);
+            throw new RuntimeException("Error during multilang processing", e);
         }
     }
 

File: src/jvm/backtype/storm/Config.java
Patch:
@@ -343,7 +343,7 @@ public void setMessageTimeoutSecs(int secs) {
         put(Config.TOPOLOGY_MESSAGE_TIMEOUT_SECS, secs);
     }
     
-    public void addSerialization(int token, Class<ISerialization> serialization) {
+    public void addSerialization(int token, Class<? extends ISerialization> serialization) {
         if(!containsKey(Config.TOPOLOGY_SERIALIZATIONS)) {
             put(Config.TOPOLOGY_SERIALIZATIONS, new HashMap());
         }

File: src/jvm/backtype/storm/Config.java
Patch:
@@ -343,7 +343,7 @@ public void setMessageTimeoutSecs(int secs) {
         put(Config.TOPOLOGY_MESSAGE_TIMEOUT_SECS, secs);
     }
     
-    public void addSerialization(int token, Class<ISerialization> serialization) {
+    public void addSerialization(int token, Class<? extends ISerialization> serialization) {
         if(!containsKey(Config.TOPOLOGY_SERIALIZATIONS)) {
             put(Config.TOPOLOGY_SERIALIZATIONS, new HashMap());
         }

File: src/jvm/backtype/storm/serialization/SerializationFactory.java
Patch:
@@ -10,6 +10,7 @@
 
 
 public class SerializationFactory {
+    public static final int SERIALIZATION_TOKEN_BOUNDARY = 32;
     public static Logger LOG = Logger.getLogger(SerializationFactory.class);
     private static byte[] EMPTY_BYTE_ARRAY = new byte[0];
 
@@ -151,7 +152,7 @@ public SerializationFactory(Map conf) {
         for(Object tokenObj: customSerializations.keySet()) {
             String serializationClassName = customSerializations.get(tokenObj);
             int token = toToken(tokenObj);
-            if(token<=32) {
+            if(token<=SERIALIZATION_TOKEN_BOUNDARY) {
                 throw new RuntimeException("Illegal token " + token + " for " + serializationClassName);
             }
             try {

