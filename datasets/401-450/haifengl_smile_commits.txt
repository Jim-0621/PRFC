File: base/src/main/java/smile/datasets/CPU.java
Patch:
@@ -26,7 +26,7 @@
 import smile.util.Paths;
 
 /**
- * CPU dataset. As used by Kilpatrick, D. & Cameron-Jones, M. (1998).
+ * CPU dataset. As used by Kilpatrick, D. &amp; Cameron-Jones, M. (1998).
  * Numeric prediction using instance-based learning with encoding length
  * selection.
  *

File: base/src/main/java/smile/data/DataFrame.java
Patch:
@@ -519,7 +519,7 @@ public DataFrame factorize(String... names) {
                 data[i] = s == null ? (byte) -1 : scale.valueOf(s).intValue();
             }
 
-            StructField field = new StructField(column.name(), DataTypes.IntegerType, scale);
+            StructField field = new StructField(column.name(), DataTypes.IntType, scale);
             return new IntVector(field, data);
         }).toArray(ValueVector[]::new);
 
@@ -1122,7 +1122,7 @@ public static DataFrame of(StructType schema, List<? extends Tuple> data) {
             BitSet nullMask = new BitSet(n);
             StructField field = fields[j];
             columns[j] = switch (field.dtype().id()) {
-                case Integer -> {
+                case Int -> {
                     int[] values = new int[n];
                     for (int i = 0; i < n; i++) {
                         Tuple datum = data.get(i);

File: base/src/main/java/smile/data/formula/Date.java
Patch:
@@ -98,7 +98,7 @@ public List<Feature> bind(StructType schema) {
             features.add(new Feature() {
                 final StructField field = new StructField(
                         String.format("%s_%s", name, feature),
-                        DataTypes.IntegerType,
+                        DataTypes.IntType,
                         feature == DateFeature.MONTH ? month : (feature == DateFeature.DAY_OF_WEEK ? dayOfWeek : null));
 
                 @Override

File: base/src/main/java/smile/data/formula/FactorInteraction.java
Patch:
@@ -111,7 +111,7 @@ private class InteractionFeature implements Feature {
             measure = new NominalScale(levels);
             field = new StructField(
                     String.join(":", factors),
-                    DataTypes.IntegerType,
+                    DataTypes.IntType,
                     measure
             );
         }

File: base/src/main/java/smile/data/formula/Feature.java
Patch:
@@ -136,7 +136,7 @@ default ValueVector apply(DataFrame data) {
 
         int size = data.size();
         return switch (field.dtype().id()) {
-            case Integer -> {
+            case Int -> {
                 int[] values = new int[size];
                 for (int i = 0; i < size; i++) values[i] = applyAsInt(data.get(i));
                 yield new IntVector(field, values);

File: base/src/main/java/smile/data/formula/IntFunction.java
Patch:
@@ -61,7 +61,7 @@ public List<Feature> bind(StructType schema) {
             features.add(new Feature() {
                 final StructField field = new StructField(
                         String.format("%s(%s)", name, xfield.name()),
-                        dtype.isNullable() ? DataTypes.NullableIntegerType : DataTypes.IntegerType,
+                        dtype.isNullable() ? DataTypes.NullableIntType : DataTypes.IntType,
                         xfield.measure());
 
                 @Override

File: base/src/main/java/smile/data/formula/Terms.java
Patch:
@@ -1091,7 +1091,7 @@ public String toString() {
             @Override
             public List<Feature> bind(StructType schema) {
                 Feature feature = new Feature() {
-                    private final StructField field = new StructField(String.valueOf(x), DataTypes.IntegerType, null);
+                    private final StructField field = new StructField(String.valueOf(x), DataTypes.IntType, null);
 
                     @Override
                     public StructField field() {
@@ -1320,7 +1320,7 @@ public List<Feature> bind(StructType schema) {
 
                 for (Feature feature : x.bind(schema)) {
                     features.add(new Feature() {
-                        private final StructField field = new StructField(String.format("%s(%s)", name, feature), DataTypes.IntegerType, null);
+                        private final StructField field = new StructField(String.format("%s(%s)", name, feature), DataTypes.IntType, null);
 
                         @Override
                         public StructField field() {
@@ -1571,7 +1571,7 @@ public List<Feature> bind(StructType schema) {
 
                     features.add(new Feature() {
                         final StructField field = new StructField(String.format("%s(%s, %s)", name, xfield.name(), yfield.name()),
-                                DataTypes.IntegerType,
+                                DataTypes.IntType,
                                 null);
 
                         @Override

File: base/src/main/java/smile/data/measure/CategoricalMeasure.java
Patch:
@@ -207,7 +207,7 @@ public DataType type() {
         } else if (levels.length <= Short.MAX_VALUE + 1) {
             return DataTypes.ShortType;
         } else {
-            return DataTypes.IntegerType;
+            return DataTypes.IntType;
         }
     }
 

File: base/src/main/java/smile/data/type/ArrayType.java
Patch:
@@ -33,7 +33,7 @@ public class ArrayType implements DataType {
     /** Short array type. */
     static final ArrayType ShortArrayType = new ArrayType(DataTypes.ShortType);
     /** Integer array type. */
-    static final ArrayType IntegerArrayType = new ArrayType(DataTypes.IntegerType);
+    static final ArrayType IntArrayType = new ArrayType(DataTypes.IntType);
     /** Long array type. */
     static final ArrayType LongArrayType = new ArrayType(DataTypes.LongType);
     /** Float array type. */
@@ -82,7 +82,7 @@ public String toString(Object o) {
             case Byte -> Arrays.toString((byte[]) o);
             case Char -> Arrays.toString((char[]) o);
             case Short -> Arrays.toString((short[]) o);
-            case Integer -> Arrays.toString((int[]) o);
+            case Int -> Arrays.toString((int[]) o);
             case Long -> Arrays.toString((long[]) o);
             case Float -> Arrays.toString((float[]) o);
             case Double -> Arrays.toString((double[]) o);

File: base/src/main/java/smile/data/type/ObjectType.java
Patch:
@@ -59,9 +59,9 @@ public class ObjectType implements DataType {
         } else if (clazz == Double.class) {
             format = DataTypes.DoubleType::toString;
         } else if (clazz == Integer.class) {
-            format = DataTypes.IntegerType::toString;
+            format = DataTypes.IntType::toString;
         } else if (clazz == Long.class) {
-            format = DataTypes.IntegerType::toString;
+            format = DataTypes.IntType::toString;
         } else {
             format = Object::toString;
         }

File: base/src/main/java/smile/data/type/StructField.java
Patch:
@@ -197,7 +197,7 @@ public static StructField of(Field field) {
      */
     public Field toArrow() {
         return switch (dtype.id()) {
-            case Integer -> new Field(name, new FieldType(dtype().isNullable(), new ArrowType.Int(32, true), null), null);
+            case Int -> new Field(name, new FieldType(dtype().isNullable(), new ArrowType.Int(32, true), null), null);
             case Long -> new Field(name, new FieldType(dtype().isNullable(), new ArrowType.Int(64, true), null), null);
             case Double -> new Field(name, new FieldType(dtype().isNullable(), new ArrowType.FloatingPoint(DOUBLE), null), null);
             case Float -> new Field(name, new FieldType(dtype().isNullable(), new ArrowType.FloatingPoint(SINGLE), null), null);
@@ -235,7 +235,7 @@ public Field toArrow() {
             case Array -> {
                 DataType etype = ((ArrayType) dtype).getComponentType();
                 yield switch (etype.id()) {
-                    case Integer -> new Field(name,
+                    case Int -> new Field(name,
                                 new FieldType(false, new ArrowType.List(), null),
                                 // children type
                                 Collections.singletonList(new Field(null, new FieldType(false, new ArrowType.Int(32, true), null), null))

File: base/src/main/java/smile/data/vector/IntVector.java
Patch:
@@ -39,7 +39,7 @@ public class IntVector extends PrimitiveVector {
      * @param vector the elements of vector.
      */
     public IntVector(String name, int[] vector) {
-        this(new StructField(name, DataTypes.IntegerType), vector);
+        this(new StructField(name, DataTypes.IntType), vector);
     }
 
     /**

File: base/src/main/java/smile/data/vector/NumberVector.java
Patch:
@@ -60,7 +60,7 @@ public void fillna(double value) {
         Number number = switch (dtype().id()) {
             case Byte -> (byte) value;
             case Short -> (short) value;
-            case Integer -> (int) value;
+            case Int -> (int) value;
             case Long -> (long) value;
             case Float -> (float) value;
             case Double -> value;

File: base/src/main/java/smile/data/vector/StringVector.java
Patch:
@@ -94,14 +94,14 @@ public ValueVector factorize(CategoricalMeasure scale) {
                 StructField field = new StructField(name(), DataTypes.ShortType, scale);
                 return new ShortVector(field, data);
             }
-            case Integer: {
+            case Int: {
                 int[] data = new int[size()];
                 for (int i = 0; i < data.length; i++) {
                     String s = get(i);
                     data[i] = s == null ? -1 : scale.valueOf(s).intValue();
                 }
 
-                StructField field = new StructField(name(), DataTypes.IntegerType, scale);
+                StructField field = new StructField(name(), DataTypes.IntType, scale);
                 return new IntVector(field, data);
             }
             default:

File: base/src/main/java/smile/io/Arff.java
Patch:
@@ -315,7 +315,7 @@ private StructField nextAttribute() throws IOException, ParseException {
                 readTillEOL();
 
             } else if (tokenizer.sval.equalsIgnoreCase(ARFF_ATTRIBUTE_INTEGER)) {
-                attribute = new StructField(name, DataTypes.IntegerType);
+                attribute = new StructField(name, DataTypes.IntType);
                 readTillEOL();
 
             } else if (tokenizer.sval.equalsIgnoreCase(ARFF_ATTRIBUTE_STRING)) {

File: base/src/main/java/smile/io/Arrow.java
Patch:
@@ -299,7 +299,7 @@ public void write(DataFrame data, Path path) throws IOException {
                     FieldVector vector = root.getVector(field.getName());
                     DataType type = data.schema().field(field.getName()).dtype();
                     switch (type.id()) {
-                        case Integer:
+                        case Int:
                             writeIntField(data, vector, from, count);
                             break;
                         case Long:

File: base/src/test/java/smile/data/DataFrameTest.java
Patch:
@@ -112,7 +112,7 @@ public void testSchema() {
         System.out.println(df.structure());
         System.out.println(df);
         smile.data.type.StructType schema = new StructType(
-                new StructField("age", DataTypes.IntegerType),
+                new StructField("age", DataTypes.IntType),
                 new StructField("birthday", DataTypes.DateType),
                 new StructField("gender", DataTypes.ByteType, new NominalScale("Male", "Female")),
                 new StructField("name", DataTypes.StringType),
@@ -137,7 +137,7 @@ public void testNames() {
     @Test
     public void testTypes() {
         System.out.println("dtypes");
-        DataType[] dtypes = {DataTypes.IntegerType, DataTypes.DateType, DataTypes.ByteType, DataTypes.StringType, DataTypes.NullableDoubleType};
+        DataType[] dtypes = {DataTypes.IntType, DataTypes.DateType, DataTypes.ByteType, DataTypes.StringType, DataTypes.NullableDoubleType};
         assertArrayEquals(dtypes, df.dtypes());
     }
 

File: base/src/test/java/smile/data/IndexDataFrameTest.java
Patch:
@@ -113,7 +113,7 @@ public void testSchema() {
         System.out.println(df.structure());
         System.out.println(df);
         smile.data.type.StructType schema = new StructType(
-                new StructField("age", DataTypes.IntegerType),
+                new StructField("age", DataTypes.IntType),
                 new StructField("birthday", DataTypes.DateType),
                 new StructField("gender", DataTypes.ByteType, new NominalScale("Male", "Female")),
                 new StructField("name", DataTypes.StringType),
@@ -138,7 +138,7 @@ public void testNames() {
     @Test
     public void testTypes() {
         System.out.println("dtypes");
-        DataType[] dtypes = {DataTypes.IntegerType, DataTypes.DateType, DataTypes.ByteType, DataTypes.StringType, DataTypes.NullableDoubleType};
+        DataType[] dtypes = {DataTypes.IntType, DataTypes.DateType, DataTypes.ByteType, DataTypes.StringType, DataTypes.NullableDoubleType};
         assertArrayEquals(dtypes, df.dtypes());
     }
 

File: base/src/test/java/smile/data/formula/DateFeatureTest.java
Patch:
@@ -75,7 +75,7 @@ public void testDateFeatures() {
         System.out.println(output);
 
         for (int i = 0; i < output.columns().length; i++) {
-            assertEquals(DataTypes.IntegerType, schema.field(i).dtype());
+            assertEquals(DataTypes.IntType, schema.field(i).dtype());
             if (i == 1 || i == 3) {
                 assertTrue(schema.field(i).measure() instanceof NominalScale);
             } else {

File: base/src/test/java/smile/data/type/DataTypeTest.java
Patch:
@@ -48,7 +48,7 @@ public void tearDown() {
     @Test
     public void testInt() throws ClassNotFoundException {
         System.out.println("int");
-        assertEquals(DataTypes.IntegerType, DataType.of("int"));
+        assertEquals(DataTypes.IntType, DataType.of("int"));
     }
 
     @Test
@@ -66,7 +66,7 @@ public void testDouble() throws ClassNotFoundException {
     @Test
     public void testArray() throws ClassNotFoundException {
         System.out.println("array");
-        assertEquals(DataTypes.array(DataTypes.IntegerType), DataType.of("Array[int]"));
+        assertEquals(DataTypes.array(DataTypes.IntType), DataType.of("Array[int]"));
     }
 
     @Test
@@ -79,7 +79,7 @@ public void testObject() throws ClassNotFoundException {
     public void testStruct() throws ClassNotFoundException {
         System.out.println("struct");
         StructType type = new StructType(
-                new StructField("age", DataTypes.IntegerType),
+                new StructField("age", DataTypes.IntType),
                 new StructField("birthday", DataTypes.DateType),
                 new StructField("gender", DataTypes.CharType),
                 new StructField("name", DataTypes.StringType),

File: base/src/test/java/smile/io/ArffTest.java
Patch:
@@ -163,7 +163,7 @@ public void testParseSparse() throws Exception {
             DataFrame sparse = arff.read();
 
             StructType schema = new StructType(
-                    new StructField("V1", DataTypes.IntegerType),
+                    new StructField("V1", DataTypes.IntType),
                     new StructField("V2", DataTypes.ByteType, new NominalScale("U", "W", "X", "Y")),
                     new StructField("V3", DataTypes.ByteType, new NominalScale("U", "W", "X", "Y")),
                     new StructField("V4", DataTypes.ByteType, new NominalScale("U", "W", "X", "Y")),

File: base/src/test/java/smile/io/AvroTest.java
Patch:
@@ -93,7 +93,7 @@ public void testSchema() {
                 new StructField("cc", DataTypes.LongType),
                 new StructField("country", DataTypes.StringType),
                 new StructField("birthdate", DataTypes.StringType),
-                new StructField("salary", DataTypes.DoubleType),
+                new StructField("salary", DataTypes.NullableDoubleType),
                 new StructField("title", DataTypes.StringType),
                 new StructField("comments", DataTypes.StringType)
         );

File: base/src/test/java/smile/io/ParquetTest.java
Patch:
@@ -80,11 +80,11 @@ public void testWidth() {
     public void testSchema() {
         System.out.println("schema");
         System.out.println(df.schema());
-        System.out.println(df.structure());
+        System.out.println(df.structure().toString(15));
         System.out.println(df);
         smile.data.type.StructType schema = new StructType(
                 new StructField("registration_dttm", DataTypes.DateTimeType),
-                new StructField("id", DataTypes.IntegerType),
+                new StructField("id", DataTypes.IntType),
                 new StructField("first_name", DataTypes.StringType),
                 new StructField("last_name", DataTypes.StringType),
                 new StructField("email", DataTypes.StringType),
@@ -93,7 +93,7 @@ public void testSchema() {
                 new StructField("cc", DataTypes.StringType),
                 new StructField("country", DataTypes.StringType),
                 new StructField("birthdate", DataTypes.StringType),
-                new StructField("salary", DataTypes.DoubleType),
+                new StructField("salary", DataTypes.NullableDoubleType),
                 new StructField("title", DataTypes.StringType),
                 new StructField("comments", DataTypes.StringType)
         );

File: base/src/test/java/smile/test/data/Sequence.java
Patch:
@@ -109,7 +109,7 @@ static Dataset read(String resource) throws IOException {
             Arrays.sort(values);
             base[i] = values[0];
             NominalScale scale = new NominalScale(IntStream.range(0, values.length).mapToObj(String::valueOf).toArray(String[]::new));
-            return new StructField("V"+(i+1), DataTypes.IntegerType, scale);
+            return new StructField("V"+(i+1), DataTypes.IntType, scale);
         }).toArray(StructField[]::new));
 
         int n = x.stream().mapToInt(xi -> xi.length).sum();

File: core/src/main/java/smile/feature/extraction/BagOfWords.java
Patch:
@@ -101,7 +101,7 @@ public BagOfWords(String[] columns, Function<String, String[]> tokenizer, String
         }
 
         StructField[] fields = Arrays.stream(words)
-                .map(word -> new StructField("BoW_" + word, DataTypes.IntegerType))
+                .map(word -> new StructField("BoW_" + word, DataTypes.IntType))
                 .toArray(StructField[]::new);
         this.schema = new StructType(fields);
     }

File: core/src/main/java/smile/sequence/CRF.java
Patch:
@@ -92,7 +92,7 @@ public CRF(StructType schema, RegressionTree[][] potentials, double shrinkage) {
 
         int k = potentials.length;
         NominalScale scale = new NominalScale(IntStream.range(0, k+1).mapToObj(String::valueOf).toArray(String[]::new));
-        StructField field = new StructField("s(t-1)", DataTypes.IntegerType, scale);
+        StructField field = new StructField("s(t-1)", DataTypes.IntType, scale);
 
         int length = schema.length();
         StructField[] fields = new StructField[length + 1];
@@ -293,7 +293,7 @@ public static CRF fit(Tuple[][] sequences, int[][] labels, int ntrees, int maxDe
         }
 
         NominalScale scale = new NominalScale(IntStream.range(0, k+1).mapToObj(String::valueOf).toArray(String[]::new));
-        IntVector t1 = new IntVector(new StructField("s(t-1)", DataTypes.IntegerType, scale), state);
+        IntVector t1 = new IntVector(new StructField("s(t-1)", DataTypes.IntType, scale), state);
         DataFrame data = DataFrame.of(x.getFirst().schema(), x).merge(t1);
 
         StructField field = new StructField("residual", DataTypes.DoubleType);

File: base/src/main/java/smile/data/type/ArrayType.java
Patch:
@@ -1,5 +1,5 @@
 /*
- * Copyright (c) 2010-2021 Haifeng Li. All rights reserved.
+ * Copyright (c) 2010-2025 Haifeng Li. All rights reserved.
  *
  * Smile is free software: you can redistribute it and/or modify
  * it under the terms of the GNU General Public License as published by
@@ -14,7 +14,6 @@
  * You should have received a copy of the GNU General Public License
  * along with Smile.  If not, see <https://www.gnu.org/licenses/>.
  */
-
 package smile.data.type;
 
 import java.util.Arrays;

File: base/src/main/java/smile/data/type/DateTimeType.java
Patch:
@@ -1,5 +1,5 @@
 /*
- * Copyright (c) 2010-2021 Haifeng Li. All rights reserved.
+ * Copyright (c) 2010-2025 Haifeng Li. All rights reserved.
  *
  * Smile is free software: you can redistribute it and/or modify
  * it under the terms of the GNU General Public License as published by

File: base/src/main/java/smile/data/type/DateType.java
Patch:
@@ -1,5 +1,5 @@
 /*
- * Copyright (c) 2010-2021 Haifeng Li. All rights reserved.
+ * Copyright (c) 2010-2025 Haifeng Li. All rights reserved.
  *
  * Smile is free software: you can redistribute it and/or modify
  * it under the terms of the GNU General Public License as published by

File: base/src/main/java/smile/data/type/DecimalType.java
Patch:
@@ -1,5 +1,5 @@
 /*
- * Copyright (c) 2010-2021 Haifeng Li. All rights reserved.
+ * Copyright (c) 2010-2025 Haifeng Li. All rights reserved.
  *
  * Smile is free software: you can redistribute it and/or modify
  * it under the terms of the GNU General Public License as published by
@@ -14,7 +14,6 @@
  * You should have received a copy of the GNU General Public License
  * along with Smile.  If not, see <https://www.gnu.org/licenses/>.
  */
-
 package smile.data.type;
 
 import java.math.BigDecimal;

File: base/src/main/java/smile/data/type/ObjectType.java
Patch:
@@ -1,5 +1,5 @@
 /*
- * Copyright (c) 2010-2021 Haifeng Li. All rights reserved.
+ * Copyright (c) 2010-2025 Haifeng Li. All rights reserved.
  *
  * Smile is free software: you can redistribute it and/or modify
  * it under the terms of the GNU General Public License as published by
@@ -14,7 +14,6 @@
  * You should have received a copy of the GNU General Public License
  * along with Smile.  If not, see <https://www.gnu.org/licenses/>.
  */
-
 package smile.data.type;
 
 import java.util.function.Function;

File: base/src/main/java/smile/data/type/StringType.java
Patch:
@@ -1,5 +1,5 @@
 /*
- * Copyright (c) 2010-2021 Haifeng Li. All rights reserved.
+ * Copyright (c) 2010-2025 Haifeng Li. All rights reserved.
  *
  * Smile is free software: you can redistribute it and/or modify
  * it under the terms of the GNU General Public License as published by
@@ -14,7 +14,6 @@
  * You should have received a copy of the GNU General Public License
  * along with Smile.  If not, see <https://www.gnu.org/licenses/>.
  */
-
 package smile.data.type;
 
 /**

File: base/src/main/java/smile/data/type/TimeType.java
Patch:
@@ -1,5 +1,5 @@
 /*
- * Copyright (c) 2010-2021 Haifeng Li. All rights reserved.
+ * Copyright (c) 2010-2025 Haifeng Li. All rights reserved.
  *
  * Smile is free software: you can redistribute it and/or modify
  * it under the terms of the GNU General Public License as published by
@@ -14,7 +14,6 @@
  * You should have received a copy of the GNU General Public License
  * along with Smile.  If not, see <https://www.gnu.org/licenses/>.
  */
-
 package smile.data.type;
 
 import java.time.LocalTime;

File: base/src/main/java/smile/data/type/package-info.java
Patch:
@@ -1,5 +1,5 @@
 /*
- * Copyright (c) 2010-2021 Haifeng Li. All rights reserved.
+ * Copyright (c) 2010-2025 Haifeng Li. All rights reserved.
  *
  * Smile is free software: you can redistribute it and/or modify
  * it under the terms of the GNU General Public License as published by

File: base/src/main/java/smile/data/vector/PrimitiveVector.java
Patch:
@@ -17,6 +17,9 @@
 package smile.data.vector;
 
 import java.util.BitSet;
+import java.util.stream.Collectors;
+import java.util.stream.IntStream;
+
 import smile.data.type.StructField;
 import smile.util.Index;
 

File: base/src/test/java/smile/data/DataFrameTest.java
Patch:
@@ -116,7 +116,7 @@ public void testSchema() {
                 new StructField("birthday", DataTypes.DateType),
                 new StructField("gender", DataTypes.ByteType, new NominalScale("Male", "Female")),
                 new StructField("name", DataTypes.StringType),
-                new StructField("salary", DataTypes.object(Double.class))
+                new StructField("salary", DataTypes.NullableDoubleType)
         );
         assertEquals(schema, df.schema());
     }
@@ -136,8 +136,8 @@ public void testNames() {
      */
     @Test
     public void testTypes() {
-        System.out.println("names");
-        DataType[] dtypes = {DataTypes.IntegerType, DataTypes.DateType, DataTypes.ByteType, DataTypes.StringType, DataTypes.object(Double.class)};
+        System.out.println("dtypes");
+        DataType[] dtypes = {DataTypes.IntegerType, DataTypes.DateType, DataTypes.ByteType, DataTypes.StringType, DataTypes.NullableDoubleType};
         assertArrayEquals(dtypes, df.dtypes());
     }
 

File: base/src/test/java/smile/data/IndexDataFrameTest.java
Patch:
@@ -117,7 +117,7 @@ public void testSchema() {
                 new StructField("birthday", DataTypes.DateType),
                 new StructField("gender", DataTypes.ByteType, new NominalScale("Male", "Female")),
                 new StructField("name", DataTypes.StringType),
-                new StructField("salary", DataTypes.object(Double.class))
+                new StructField("salary", DataTypes.NullableDoubleType)
         );
         assertEquals(schema, df.schema());
     }
@@ -137,8 +137,8 @@ public void testNames() {
      */
     @Test
     public void testTypes() {
-        System.out.println("names");
-        DataType[] dtypes = {DataTypes.IntegerType, DataTypes.DateType, DataTypes.ByteType, DataTypes.StringType, DataTypes.object(Double.class)};
+        System.out.println("dtypes");
+        DataType[] dtypes = {DataTypes.IntegerType, DataTypes.DateType, DataTypes.ByteType, DataTypes.StringType, DataTypes.NullableDoubleType};
         assertArrayEquals(dtypes, df.dtypes());
     }
 

File: base/src/test/java/smile/data/formula/FormulaTest.java
Patch:
@@ -75,6 +75,7 @@ public FormulaTest() {
         persons.add(new Person("Amy", Gender.Female, LocalDate.of(2005, 12, 10), 13, null));
 
         df = DataFrame.of(Person.class, persons);
+        System.out.println(df);
         try {
             weather = Read.arff(Paths.getTestData("weka/weather.nominal.arff"));
         } catch (Exception ex) {
@@ -128,7 +129,7 @@ public void testDot() {
         System.out.println(output);
 
         StructType schema = new StructType(
-                new StructField("salary", DataTypes.DoubleObjectType),
+                new StructField("salary", DataTypes.DoubleType),
                 new StructField("age", DataTypes.IntegerType),
                 new StructField("birthday", DataTypes.DateType),
                 new StructField("name", DataTypes.StringType),

File: base/src/test/java/smile/data/DataFrameJDBCTest.java
Patch:
@@ -23,6 +23,7 @@
 import java.sql.Statement;
 import smile.data.type.DataTypes;
 import smile.data.type.StructField;
+import smile.data.type.StructType;
 import smile.math.matrix.Matrix;
 import smile.util.Paths;
 import org.junit.jupiter.api.*;
@@ -97,7 +98,7 @@ public void testSchema() {
         System.out.println(df.schema());
         System.out.println(df.structure());
         System.out.println(df);
-        smile.data.type.StructType schema = DataTypes.struct(
+        smile.data.type.StructType schema = new StructType(
                 new StructField("Employee First", DataTypes.StringType),
                 new StructField("Employee Last", DataTypes.StringType),
                 new StructField("Customer First", DataTypes.StringType),

File: base/src/test/java/smile/data/DataFrameTest.java
Patch:
@@ -23,6 +23,7 @@
 import smile.data.type.DataType;
 import smile.data.type.DataTypes;
 import smile.data.type.StructField;
+import smile.data.type.StructType;
 import smile.data.vector.StringVector;
 import smile.math.matrix.Matrix;
 import org.junit.jupiter.api.*;
@@ -110,7 +111,7 @@ public void testSchema() {
         System.out.println(df.schema());
         System.out.println(df.structure());
         System.out.println(df);
-        smile.data.type.StructType schema = DataTypes.struct(
+        smile.data.type.StructType schema = new StructType(
                 new StructField("age", DataTypes.IntegerType),
                 new StructField("birthday", DataTypes.DateType),
                 new StructField("gender", DataTypes.ByteType, new NominalScale("Male", "Female")),

File: base/src/test/java/smile/data/IndexDataFrameTest.java
Patch:
@@ -23,6 +23,7 @@
 import smile.data.type.DataType;
 import smile.data.type.DataTypes;
 import smile.data.type.StructField;
+import smile.data.type.StructType;
 import smile.math.matrix.Matrix;
 import org.junit.jupiter.api.*;
 import smile.util.Index;
@@ -111,7 +112,7 @@ public void testSchema() {
         System.out.println(df.schema());
         System.out.println(df.structure());
         System.out.println(df);
-        smile.data.type.StructType schema = DataTypes.struct(
+        smile.data.type.StructType schema = new StructType(
                 new StructField("age", DataTypes.IntegerType),
                 new StructField("birthday", DataTypes.DateType),
                 new StructField("gender", DataTypes.ByteType, new NominalScale("Male", "Female")),

File: base/src/test/java/smile/data/type/DataTypeTest.java
Patch:
@@ -78,7 +78,7 @@ public void testObject() throws ClassNotFoundException {
     @Test
     public void testStruct() throws ClassNotFoundException {
         System.out.println("struct");
-        StructType type = DataTypes.struct(
+        StructType type = new StructType(
                 new StructField("age", DataTypes.IntegerType),
                 new StructField("birthday", DataTypes.DateType),
                 new StructField("gender", DataTypes.CharType),

File: base/src/test/java/smile/io/AvroTest.java
Patch:
@@ -20,6 +20,7 @@
 import smile.data.DataFrame;
 import smile.data.type.DataTypes;
 import smile.data.type.StructField;
+import smile.data.type.StructType;
 import smile.math.matrix.Matrix;
 import smile.util.Paths;
 import org.junit.jupiter.api.*;
@@ -81,7 +82,7 @@ public void testSchema() {
         System.out.println(df.schema());
         System.out.println(df.structure());
         System.out.println(df);
-        smile.data.type.StructType schema = DataTypes.struct(
+        smile.data.type.StructType schema = new StructType(
                 new StructField("registration_dttm", DataTypes.StringType),
                 new StructField("id", DataTypes.LongType),
                 new StructField("first_name", DataTypes.StringType),

File: base/src/test/java/smile/io/CSVTest.java
Patch:
@@ -101,7 +101,7 @@ public void testGdp() throws Exception {
         assertEquals(68, gdp.size());
         assertEquals(4, gdp.ncol());
 
-        StructType schema = DataTypes.struct(
+        StructType schema = new StructType(
                 new StructField("Country", DataTypes.StringType),
                 new StructField("GDP Growth", DataTypes.DoubleType),
                 new StructField("Debt", DataTypes.DoubleType),
@@ -172,7 +172,7 @@ public void testProstate() throws Exception {
         assertEquals(67, prostate.size());
         assertEquals(9, prostate.ncol());
 
-        StructType schema = DataTypes.struct(
+        StructType schema = new StructType(
                 new StructField("lcavol", DataTypes.DoubleType),
                 new StructField("lweight", DataTypes.DoubleType),
                 new StructField("age", DataTypes.IntegerType),
@@ -244,7 +244,7 @@ public void testUserdata() throws Exception {
         assertEquals(1000, df.size());
         assertEquals(13, df.ncol());
 
-        smile.data.type.StructType schema = DataTypes.struct(
+        smile.data.type.StructType schema = new StructType(
                 new StructField("registration_dttm", DataTypes.StringType),
                 new StructField("id", DataTypes.IntegerType),
                 new StructField("first_name", DataTypes.StringType),

File: base/src/test/java/smile/io/JSONTest.java
Patch:
@@ -62,7 +62,7 @@ public void testBooks() throws Exception {
         assertEquals(7, df.size());
         assertEquals(10, df.ncol());
 
-        StructType schema = DataTypes.struct(
+        StructType schema = new StructType(
                 new StructField("series_t", DataTypes.StringType),
                 new StructField("pages_i", DataTypes.IntegerType),
                 new StructField("author", DataTypes.StringType),
@@ -99,7 +99,7 @@ public void testBooksMultiLine() throws Exception {
         assertEquals(7, df.size());
         assertEquals(10, df.ncol());
 
-        StructType schema = DataTypes.struct(
+        StructType schema = new StructType(
                 new StructField("series_t", DataTypes.StringType),
                 new StructField("pages_i", DataTypes.IntegerType),
                 new StructField("author", DataTypes.StringType),

File: base/src/test/java/smile/io/ParquetTest.java
Patch:
@@ -21,6 +21,7 @@
 import smile.data.DataFrame;
 import smile.data.type.DataTypes;
 import smile.data.type.StructField;
+import smile.data.type.StructType;
 import smile.math.matrix.Matrix;
 import smile.util.Paths;
 import org.junit.jupiter.api.*;
@@ -81,7 +82,7 @@ public void testSchema() {
         System.out.println(df.schema());
         System.out.println(df.structure());
         System.out.println(df);
-        smile.data.type.StructType schema = DataTypes.struct(
+        smile.data.type.StructType schema = new StructType(
                 new StructField("registration_dttm", DataTypes.DateTimeType),
                 new StructField("id", DataTypes.IntegerType),
                 new StructField("first_name", DataTypes.StringType),

File: base/src/test/java/smile/test/data/Abalone.java
Patch:
@@ -43,7 +43,7 @@ public class Abalone {
     public static double[] testy;
 
     static {
-        StructType schema = DataTypes.struct(
+        StructType schema = new StructType(
                 new StructField("sex", DataTypes.ByteType, new NominalScale("F", "M", "I")),
                 new StructField("length", DataTypes.DoubleType),
                 new StructField("diameter", DataTypes.DoubleType),

File: base/src/test/java/smile/test/data/PenDigits.java
Patch:
@@ -43,7 +43,7 @@ public class PenDigits {
         ArrayList<StructField> fields = new ArrayList<>();
         IntStream.range(1, 17).forEach(i -> fields.add(new StructField("V"+i, DataTypes.DoubleType)));
         fields.add(new StructField("class", DataTypes.ByteType));
-        StructType schema = DataTypes.struct(fields);
+        StructType schema = new StructType(fields);
 
         try {
             CSVFormat format = CSVFormat.Builder.create().setDelimiter('\t').build();

File: base/src/test/java/smile/test/data/USPS.java
Patch:
@@ -47,7 +47,7 @@ public class USPS {
         ArrayList<StructField> fields = new ArrayList<>();
         fields.add(new StructField("class", DataTypes.ByteType));
         IntStream.range(1, 257).forEach(i -> fields.add(new StructField("V"+i, DataTypes.DoubleType)));
-        StructType schema = DataTypes.struct(fields);
+        StructType schema = new StructType(fields);
 
         CSVFormat format = CSVFormat.Builder.create().setDelimiter(' ').build();
         CSV csv = new CSV(format);

File: base/src/main/java/smile/data/DataFrame.java
Patch:
@@ -1260,7 +1260,7 @@ public static DataFrame of(StructType schema, List<? extends Tuple> data) {
      * @return the data frame.
      */
     public static DataFrame of(ResultSet rs) throws SQLException {
-        StructType schema = DataTypes.struct(rs);
+        StructType schema = StructType.of(rs);
         List<Tuple> rows = new ArrayList<>();
         while (rs.next()) {
             rows.add(Tuple.of(schema, rs));

File: base/src/main/java/smile/data/type/StructField.java
Patch:
@@ -304,7 +304,7 @@ yield switch (precision) {
 
             case Struct -> {
                 List<StructField> children = field.getChildren().stream().map(StructField::of).toList();
-                yield new StructField(name, DataTypes.struct(children));
+                yield new StructField(name, new StructType(children));
             }
 
             default ->

File: base/src/main/java/smile/io/Arff.java
Patch:
@@ -285,7 +285,7 @@ private void readHeader() throws IOException, ParseException {
             throw new ParseException("no attributes declared", tokenizer.lineno());
         }
         
-        schema = DataTypes.struct(fields);
+        schema = new StructType(fields);
         parser = schema.parser();
     }
 

File: base/src/main/java/smile/io/CSV.java
Patch:
@@ -225,7 +225,7 @@ public StructType inferSchema(Reader reader, int limit) throws IOException {
             for (int i = 0; i < fields.length; i++) {
                 fields[i] = new StructField(names[i], types[i] == null ? DataTypes.StringType : types[i]);
             }
-            return DataTypes.struct(fields);
+            return new StructType(fields);
         }
     }
     

File: base/src/main/java/smile/io/JSON.java
Patch:
@@ -262,6 +262,6 @@ public StructType inferSchema(BufferedReader reader, int limit) throws IOExcepti
         for (Map.Entry<String, DataType> type : types.entrySet()) {
             fields[i++] = new StructField(type.getKey(), type.getValue());
         }
-        return DataTypes.struct(fields);
+        return new StructType(fields);
     }
 }

File: base/src/main/java/smile/io/Arff.java
Patch:
@@ -434,7 +434,6 @@ public DataFrame read(int limit) throws IOException, ParseException {
             rows.add(Tuple.of(schema, row));
         }
 
-        schema = schema.boxed(rows);
         return DataFrame.of(schema, rows);
     }
 

File: base/src/main/java/smile/io/CSV.java
Patch:
@@ -168,7 +168,6 @@ private DataFrame read(Reader reader, int limit) throws IOException {
                 if (rows.size() >= limit) break;
             }
 
-            schema = schema.boxed(rows);
             return DataFrame.of(schema, rows);
         }
     }

File: base/src/main/java/smile/io/JSON.java
Patch:
@@ -190,7 +190,6 @@ public DataFrame read(BufferedReader reader, int limit) throws IOException {
             }
         }
 
-        schema = schema.boxed(rows);
         return DataFrame.of(schema, rows);
     }
 

File: base/src/test/java/smile/data/DataFrameTest.java
Patch:
@@ -136,8 +136,8 @@ public void testNames() {
     @Test
     public void testTypes() {
         System.out.println("names");
-        DataType[] types = {DataTypes.IntegerType, DataTypes.DateType, DataTypes.ByteType, DataTypes.StringType, DataTypes.object(Double.class)};
-        assertArrayEquals(types, df.types());
+        DataType[] dtypes = {DataTypes.IntegerType, DataTypes.DateType, DataTypes.ByteType, DataTypes.StringType, DataTypes.object(Double.class)};
+        assertArrayEquals(dtypes, df.dtypes());
     }
 
     /**

File: base/src/test/java/smile/data/IndexDataFrameTest.java
Patch:
@@ -137,8 +137,8 @@ public void testNames() {
     @Test
     public void testTypes() {
         System.out.println("names");
-        DataType[] types = {DataTypes.IntegerType, DataTypes.DateType, DataTypes.ByteType, DataTypes.StringType, DataTypes.object(Double.class)};
-        assertArrayEquals(types, df.types());
+        DataType[] dtypes = {DataTypes.IntegerType, DataTypes.DateType, DataTypes.ByteType, DataTypes.StringType, DataTypes.object(Double.class)};
+        assertArrayEquals(dtypes, df.dtypes());
     }
 
     /**

File: deep/src/main/java/smile/deep/tensor/Index.java
Patch:
@@ -129,7 +129,7 @@ public static Index slice(Integer start, Integer end) {
     }
 
     /**
-     * Returns the slice index for [start, end) with step 1.
+     * Returns the slice index for [start, end) with the given step.
      *
      * @param start the start index.
      * @param end the end index.

File: deep/src/main/java/smile/deep/tensor/Tensor.java
Patch:
@@ -1244,7 +1244,7 @@ public Tensor outer(Tensor other) {
 
     /**
      * Computes element-wise equality.
-     * @param other the sclar to compare.
+     * @param other the scalar to compare.
      * @return the output tensor.
      */
     public Tensor eq(int other) {
@@ -1271,7 +1271,7 @@ public Tensor eq(Tensor other) {
 
     /**
      * Computes element-wise inequality.
-     * @param other the sclar to compare.
+     * @param other the scalar to compare.
      * @return the output tensor.
      */
     public Tensor ne(int other) {

File: core/src/test/java/smile/manifold/UMAPTest.java
Patch:
@@ -18,8 +18,6 @@
 
 import java.util.Arrays;
 
-import smile.graph.NearestNeighborGraph;
-import smile.io.Read;
 import smile.math.MathEx;
 import smile.test.data.MNIST;
 import smile.test.data.SwissRoll;
@@ -72,7 +70,7 @@ public void testSwissRoll() throws Exception {
         long start = System.currentTimeMillis();
         double[][] coordinates = UMAP.of(data, 15);
         long end = System.currentTimeMillis();
-        System.out.format("UMAP takes %.2f seconds\n", (end - start) / 1000.0); 
+        System.out.format("UMAP takes %.2f seconds\n", (end - start) / 1000.0);
         assertEquals(data.length, coordinates.length);
     }
 }

File: base/src/main/java/smile/neighbor/RandomProjectionForest.java
Patch:
@@ -93,7 +93,7 @@ public int[] search(double[] point) {
      * @param data the data set.
      * @param numTrees the number of trees.
      * @param leafSize The maximum size of leaf node.
-     * @param angular true for cosine metric, otherwise Euclidean.
+     * @param angular true for angular metric, otherwise Euclidean.
      * @return random projection forest
      */
     public static RandomProjectionForest of(double[][] data, int numTrees, int leafSize, boolean angular) {

File: base/src/test/java/smile/neighbor/RandomProjectionForestTest.java
Patch:
@@ -81,7 +81,7 @@ public void testAngular() {
         System.out.println("angular");
 
         RandomProjectionForest forest = RandomProjectionForest.of(x, 5, 10, true);
-        LinearSearch<double[], double[]> naive = LinearSearch.of(x, MathEx::cosine);
+        LinearSearch<double[], double[]> naive = LinearSearch.of(x, MathEx::angular);
         int[] recall = new int[testx.length];
         for (int i = 0; i < testx.length; i++) {
             int k = 7;

File: base/src/test/java/smile/neighbor/RandomProjectionTreeTest.java
Patch:
@@ -81,7 +81,7 @@ public void testAngular() {
         System.out.println("angular");
 
         RandomProjectionTree tree = RandomProjectionTree.of(x, 10, true);
-        LinearSearch<double[], double[]> naive = LinearSearch.of(x, MathEx::cosine);
+        LinearSearch<double[], double[]> naive = LinearSearch.of(x, MathEx::angular);
         int[] recall = new int[testx.length];
         for (int i = 0; i < testx.length; i++) {
             int k = 7;

File: base/src/main/java/smile/data/Tuple.java
Patch:
@@ -616,7 +616,7 @@ default Tuple getStruct(String field) {
     /**
      * Returns the value at position i.
      * For primitive types if value is null it returns 'zero value' specific for primitive
-     * ie. 0 for Int - use isNullAt to ensure that value is not null
+     * i.e. 0 for Int - use isNullAt to ensure that value is not null
      *
      * @param i the index of field.
      * @param <T> the data type of field.

File: base/src/main/java/smile/data/measure/IntervalScale.java
Patch:
@@ -25,7 +25,7 @@
  * the Celsius scale, which has two defined points (the freezing and
  * boiling point of water at specific conditions) and then separated
  * into 100 intervals. Ratios are not meaningful since 20 °C
- * cannot be said to be "twice as hot" as 10 °C. Other examples include
+ * cannot be said to be "twice hot" as 10 °C. Other examples include
  * date when measured from an arbitrary epoch (such as AD) since
  * multiplication/division cannot be carried out between any two dates
  * directly. However, ratios of differences can be expressed; for example,

File: base/src/main/java/smile/math/matrix/Lanczos.java
Patch:
@@ -21,7 +21,7 @@
 /**
  * The Lanczos algorithm is a direct algorithm devised by Cornelius Lanczos
  * that is an adaptation of power methods to find the most useful eigenvalues
- * and eigenvectors of an n<sup>th</sup> order linear system with a limited
+ * and eigenvectors of a n<sup>th</sup> order linear system with a limited
  * number of operations, m, where m is much smaller than n.
  * <p>
  * Although computationally efficient in principle, the method as initially

File: core/src/main/java/smile/base/cart/OrdinalNode.java
Patch:
@@ -24,7 +24,7 @@
 import java.io.Serial;
 
 /**
- * A node with a ordinal split variable (real-valued or ordinal categorical value).
+ * A node with an ordinal split variable (real-valued or ordinal categorical value).
  *
  * @author Haifeng Li
  */

File: core/src/main/java/smile/clustering/BBDTree.java
Patch:
@@ -28,8 +28,8 @@
  * The structure works as follows:
  * <ul>
  * <li> All data are placed into a tree where we choose child nodes by
- *      partitioning all data data along a plane parallel to the axis.
- * <li> We maintain for each node, the bounding box of all data data stored
+ *      partitioning all data along a plane parallel to the axis.
+ * <li> We maintain for each node, the bounding box of all data stored
  *      at that node.
  * <li> To do a k-means iteration, we need to assign data to clusters and
  *      calculate the sum and the number of data assigned to each cluster.

File: core/src/main/java/smile/clustering/FastPair.java
Patch:
@@ -36,7 +36,7 @@
  * Total space: 20n bytes. (Could be reduced to 4n at some cost in update time.)
  * Time per insertion or single distance update: O(n)
  * Time per deletion or point update: O(n) expected, O(n<sup>2</sup>) worst case
- * Time per closest pair: O(n)
+ * Time per the closest pair: O(n)
  *
  * <h2>References</h2>
  * <ol>

File: core/src/main/java/smile/regression/LinearModel.java
Patch:
@@ -131,7 +131,7 @@ public class LinearModel implements DataFrameRegression {
     final double RSquared;
     /**
      * Adjusted R<sup>2</sup>. The adjusted R<sup>2</sup> has almost same
-     * explanation as R<sup>2</sup> but it penalizes the statistic as
+     * explanation as R<sup>2</sup>, but it penalizes the statistic as
      * extra variables are included in the model.
      */
     final double adjustedRSquared;
@@ -301,7 +301,7 @@ public double RSquared() {
 
     /**
      * Returns adjusted R<sup>2</sup> statistic. The adjusted R<sup>2</sup>
-     * has almost same explanation as R<sup>2</sup> but it penalizes the
+     * has almost same explanation as R<sup>2</sup>, but it penalizes the
      * statistic as extra variables are included in the model.
      *
      * @return adjusted R<sup>2</sup> statistic.

File: core/src/main/java/smile/timeseries/AR.java
Patch:
@@ -145,7 +145,7 @@ public String toString() {
     private final double R2;
     /**
      * Adjusted R<sup>2</sup>. The adjusted R<sup>2</sup> has almost same
-     * explanation as R<sup>2</sup> but it penalizes the statistic as
+     * explanation as R<sup>2</sup>, but it penalizes the statistic as
      * extra variables are included in the model.
      */
     private final double adjustedR2;
@@ -328,7 +328,7 @@ public double R2() {
 
     /**
      * Returns adjusted R<sup>2</sup> statistic. The adjusted R<sup>2</sup>
-     * has almost same explanation as R<sup>2</sup> but it penalizes the
+     * has almost same explanation as R<sup>2</sup>, but it penalizes the
      * statistic as extra variables are included in the model.
      *
      * @return Adjusted R<sup>2</sup> statistic.

File: core/src/main/java/smile/timeseries/ARMA.java
Patch:
@@ -115,7 +115,7 @@ public class ARMA implements Serializable {
     private final double R2;
     /**
      * Adjusted R<sup>2</sup>. The adjusted R<sup>2</sup> has almost same
-     * explanation as R<sup>2</sup> but it penalizes the statistic as
+     * explanation as R<sup>2</sup>, but it penalizes the statistic as
      * extra variables are included in the model.
      */
     private final double adjustedR2;
@@ -293,7 +293,7 @@ public double R2() {
 
     /**
      * Returns adjusted R<sup>2</sup> statistic. The adjusted R<sup>2</sup>
-     * has almost same explanation as R<sup>2</sup> but it penalizes the
+     * has almost same explanation as R<sup>2</sup>, but it penalizes the
      * statistic as extra variables are included in the model.
      *
      * @return Adjusted R<sup>2</sup> statistic.

File: core/src/main/java/smile/validation/metric/MatthewsCorrelation.java
Patch:
@@ -25,7 +25,7 @@
  * It is considered as a balanced measure for binary classification,
  * even in unbalanced data sets. It  varies between -1 (perfect
  * disagreement) and +1 (perfect agreement). When it is 0,
- * the model is not better then random.
+ * the model is not better than random.
  *
  * @author digital-thinking
  */

File: core/src/main/java/smile/vq/GrowingNeuralGas.java
Patch:
@@ -34,7 +34,7 @@
  * <li> Local Error measurements are noted at each step helping it to locally
  * insert/delete nodes.</li>
  * <li> Edges are connected between nodes, so a sufficiently old edges is
- * deleted. Such edges are intended place holders for localized data distribution.</li>
+ * deleted. Such edges are intended placeholders for localized data distribution.</li>
  * <li> Such edges also help to locate distinct clusters (those clusters are
  * not connected by edges).</li>
  * </ul>

File: plot/src/main/java/smile/plot/swing/BoxPlot.java
Patch:
@@ -42,7 +42,7 @@
  * quartile from the third quartile. (q<sub>3</sub> ? q<sub>1</sub>)
  * <li> Construct a box above the number line bounded on the bottom by the first
  * quartile (q<sub>1</sub>) and on the top by the third quartile (q<sub>3</sub>).
- * <li> Indicate where the median lies inside of the box with the presence of
+ * <li> Indicate where the median lies inside the box with the presence of
  * a line dividing the box at the median value.
  * <li> Any data observation which lies more than 1.5*IQR lower than the first
  * quartile or 1.5IQR higher than the third quartile is considered an outlier.

File: plot/src/main/java/smile/swing/FileChooser.java
Patch:
@@ -141,7 +141,7 @@ public void loadPreview() {
                             if (buf[i] != 0x09 && // tab
                                 buf[i] != 0x0A && // line feed
                                 buf[i] != 0x0C && // form feed
-                                buf[i] != 0x0D) { // carriage return)
+                                buf[i] != 0x0D) { // carriage return
                                 binary = true;
                                 break;
                             }

File: base/src/main/java/smile/neighbor/LSH.java
Patch:
@@ -241,7 +241,7 @@ public Neighbor<double[], E>[] search(double[] q, int k) {
         Set<Integer> candidates = getCandidates(q);
         k = Math.min(k, candidates.size());
 
-        HeapSelect<Neighbor<double[], E>> heap = new HeapSelect<>(new Neighbor[k]);
+        HeapSelect<Neighbor<double[], E>> heap = new HeapSelect<Neighbor<double[], E>>(new Neighbor[k]);
 
         for (int index : candidates) {
             double[] key = keys.get(index);

File: base/src/main/java/smile/neighbor/MPLSH.java
Patch:
@@ -214,7 +214,7 @@ public Neighbor<double[], E>[] search(double[] q, int k, double recall, int T) {
         Set<Integer> candidates = getCandidates(q, recall, T);
         k = Math.min(k, candidates.size());
 
-        HeapSelect<Neighbor<double[], E>> heap = new HeapSelect<>(new Neighbor[k]);
+        HeapSelect<Neighbor<double[], E>> heap = new HeapSelect<Neighbor<double[], E>>(new Neighbor[k]);
 
         for (int index : candidates) {
             double[] key = keys.get(index);

File: base/src/main/java/smile/gap/Fitness.java
Patch:
@@ -24,7 +24,7 @@
  *
  * @author Haifeng Li
  */
-public interface Fitness<T extends Chromosome> {
+public interface Fitness<T extends Chromosome<T>> {
     /**
      * Returns the non-negative fitness value of a chromosome. Large values
      * indicate better fitness.

File: base/src/main/java/smile/gap/GeneticAlgorithm.java
Patch:
@@ -284,7 +284,7 @@ public T evolve(int generation, double threshold) {
             best = population[size - 1];
             
             double avg = 0.0;
-            for (Chromosome ch : population) {
+            for (Chromosome<T> ch : population) {
                 avg += ch.fitness();
             }
             avg /= size;

File: base/src/main/java/smile/gap/LamarckianChromosome.java
Patch:
@@ -27,7 +27,7 @@
  *
  * @author Haifeng Li
  */
-public interface LamarckianChromosome extends Chromosome {
+public interface LamarckianChromosome<T extends Chromosome<T>> extends Chromosome<T> {
     /**
      * Performs a step of (hill-climbing) local search to evolve this chromosome.
      */

File: base/src/main/java/smile/neighbor/MutableLSH.java
Patch:
@@ -85,8 +85,9 @@ public void remove(double[] key, E value) {
                     Bucket bucket = h.get(key);
                     if (bucket == null) {
                         logger.error("null bucket when removing an entry");
+                    } else {
+                        bucket.remove(i);
                     }
-                    bucket.remove(i);
                 }
                 return;
             }

File: deep/src/main/java/smile/llm/tokenizer/SentencePiece.java
Patch:
@@ -27,7 +27,7 @@
  * @author Haifeng Li
  */
 public class SentencePiece implements Tokenizer {
-    private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(Tokenizer.class);
+    private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(SentencePiece.class);
     /** SentencePiece tokenizer. */
     private final SpProcessor tokenizer;
     /** Unknown token (<unk>), default id 0. */

File: deep/src/main/java/smile/llm/tokenizer/Tiktoken.java
Patch:
@@ -36,7 +36,7 @@
  * @author Haifeng Li
  */
 public class Tiktoken implements Tokenizer {
-    private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(Tokenizer.class);
+    private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(Tiktoken.class);
     private static final int MAX = Integer.MAX_VALUE;
 
     /** The regex pattern to split the input text into tokens. */

File: plot/src/main/java/smile/swing/table/DateCellEditor.java
Patch:
@@ -44,7 +44,7 @@
  */
 @SuppressWarnings("serial")
 public class DateCellEditor extends DefaultCellEditor {
-    private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(DoubleCellEditor.class);
+    private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(DateCellEditor.class);
     
     public static final DateCellEditor YYYYMMDD        = new DateCellEditor("yyyy-MM-dd");
     public static final DateCellEditor MMDDYY          = new DateCellEditor("MM/dd/yy");

File: base/src/main/java/smile/util/AutoScope.java
Patch:
@@ -52,7 +52,6 @@ public <T extends AutoCloseable> T add(T resource) {
     /**
      * Detaches resources from this scope.
      * @param resource the resources to be detached from this scope.
-     * @return this object.
      */
     public void remove(AutoCloseable resource) {
         this.resources.remove(resource);

File: core/src/test/java/smile/feature/selection/GAFETest.java
Patch:
@@ -98,6 +98,6 @@ public void testRegressionTree() {
             System.out.format("%.4f %s%n", -bits.fitness(), bits);
         }
 
-        assertEquals(2.3840, -result[result.length-1].fitness(), 1E-4);
+        assertEquals(2.2278, -result[result.length-1].fitness(), 1E-4);
     }
 }

File: core/src/test/java/smile/validation/ValidationTest.java
Patch:
@@ -66,7 +66,7 @@ public void testAbalone() {
                 (formula, data) -> RegressionTree.fit(formula, data));
 
         System.out.println(result);
-        assertEquals(2.5567, result.metrics.rmse, 1E-4);
-        assertEquals(1.8666, result.metrics.mad, 1E-4);
+        assertEquals(2.3194, result.metrics.rmse, 1E-4);
+        assertEquals(1.6840, result.metrics.mad, 1E-4);
     }
 }

File: core/src/main/java/smile/base/cart/CART.java
Patch:
@@ -164,7 +164,7 @@ public CART(DataFrame x, StructField y, int maxDepth, int maxNodes, int nodeSize
         int p = x.ncol();
 
         if (mtry < 1 || mtry > p) {
-            logger.debug("Invalid mtry. Use all features.");
+            logger.warn("Invalid mtry. Use all features.");
             this.mtry = p;
         }
 

File: core/src/main/java/smile/classification/RandomForest.java
Patch:
@@ -190,7 +190,7 @@ public static RandomForest fit(Formula formula, DataFrame data, Properties param
         int mtry = Integer.parseInt(params.getProperty("smile.random_forest.mtry", "0"));
         SplitRule rule = SplitRule.valueOf(params.getProperty("smile.random_forest.split_rule", "GINI"));
         int maxDepth = Integer.parseInt(params.getProperty("smile.random_forest.max_depth", "20"));
-        int maxNodes = Integer.parseInt(params.getProperty("smile.random_forest.max_nodes", String.valueOf(data.size() / 5)));
+        int maxNodes = Integer.parseInt(params.getProperty("smile.random_forest.max_nodes", String.valueOf(Math.max(2, data.size() / 5))));
         int nodeSize = Integer.parseInt(params.getProperty("smile.random_forest.node_size", "5"));
         double subsample = Double.parseDouble(params.getProperty("smile.random_forest.sampling_rate", "1.0"));
         int[] classWeight = Strings.parseIntArray(params.getProperty("smile.random_forest.class_weight"));

File: core/src/main/java/smile/regression/RandomForest.java
Patch:
@@ -159,7 +159,7 @@ public static RandomForest fit(Formula formula, DataFrame data, Properties param
         int ntrees = Integer.parseInt(params.getProperty("smile.random_forest.trees", "500"));
         int mtry = Integer.parseInt(params.getProperty("smile.random_forest.mtry", "0"));
         int maxDepth = Integer.parseInt(params.getProperty("smile.random_forest.max_depth", "20"));
-        int maxNodes = Integer.parseInt(params.getProperty("smile.random_forest.max_nodes", String.valueOf(data.size() / 5)));
+        int maxNodes = Integer.parseInt(params.getProperty("smile.random_forest.max_nodes", String.valueOf(Math.max(2, data.size() / 5))));
         int nodeSize = Integer.parseInt(params.getProperty("smile.random_forest.node_size", "5"));
         double subsample = Double.parseDouble(params.getProperty("smile.random_forest.sampling_rate", "1.0"));
         return fit(formula, data, ntrees, mtry, maxDepth, maxNodes, nodeSize, subsample);

File: base/src/main/java/smile/data/type/ObjectType.java
Patch:
@@ -47,7 +47,7 @@ public class ObjectType implements DataType {
     /** Object Class. */
     private final Class<?> clazz;
     /** toString lambda. */
-    private final Function<Object, String> format;
+    private final transient Function<Object, String> format;
 
     /**
      * Constructor.

File: deep/src/main/java/smile/llm/llama/Llama.java
Patch:
@@ -285,7 +285,7 @@ public CompletionPrediction[] generate(int[][] prompts, int maxGenLen, double te
                             publisher.submit(chunk);
                             chunkPos = curPos + 1;
                         } catch (Exception ex) {
-                            logger.info("Cannot decode a chunk", ex);
+                            logger.debug("Cannot decode a chunk", ex);
                         }
                     }
                 }

File: deep/src/main/java/smile/llm/llama/Llama.java
Patch:
@@ -100,7 +100,7 @@ public static Llama build(String checkpointDir, String tokenizerPath, int maxBat
      * @param tokenizerPath the path of tokenizer file.
      * @param maxSeqLen the maximum sequence length for input text.
      * @param maxBatchSize the maximum batch size for inference.
-     * @param seed the optional CUDA device ID.
+     * @param deviceId the optional CUDA device ID.
      * @return an instance of Llama model.
      */
     public static Llama build(String checkpointDir, String tokenizerPath, int maxBatchSize, int maxSeqLen, Integer deviceId) throws IOException {

File: deep/src/main/java/smile/llm/llama/Attention.java
Patch:
@@ -19,6 +19,7 @@
 import org.bytedeco.pytorch.Module;
 import smile.deep.layer.LinearLayer;
 import smile.deep.tensor.Index;
+import smile.deep.tensor.ScalarType;
 import smile.deep.tensor.Tensor;
 import smile.llm.RotaryPositionalEncoding;
 import smile.util.AutoScope;
@@ -116,7 +117,7 @@ public Tensor forward(Tensor x, int startPos, Tensor cis, Tensor mask) {
             if (mask != null) {
                 scores = scores.add_(mask);  // (bs, n_local_heads, seqlen, cache_len + seqlen)
             }
-            scores = scope.add(scores.softmax(-1));
+            scores = scope.add(scores.to(ScalarType.Float32).softmax(-1).to(xq.dtype()));
             var output = scope.add(scores.matmul(values));  // (bs, n_local_heads, seqlen, head_dim)
             output = output.transpose(1, 2).contiguous().view(batchSize, seqlen, -1);
             return wo.forward(output);

File: deep/src/main/java/smile/deep/tensor/Tensor.java
Patch:
@@ -1816,8 +1816,7 @@ public Tensor or_(Tensor other) {
      * @return this tensor.
      */
     public Tensor softmax(int dim) {
-        torch.softmax(value, dim);
-        return this;
+        return new Tensor(torch.softmax(value, dim));
     }
 
     /**

File: deep/src/main/java/smile/llm/tokenizer/Tiktoken.java
Patch:
@@ -182,7 +182,7 @@ public int[] encode(String text, boolean bos, boolean eos) {
      */
     private void bytePairEncode(Bytes piece, ArrayList<IntPair> parts, IntArrayList output) {
         bytePairMerge(piece, parts);
-        for (int i = 0; i < parts.size() - 1; i+=2) {
+        for (int i = 0; i < parts.size() - 1; i++) {
             int token = getRank(piece, parts.get(i)._1(), parts.get(i+1)._1());
             assert token != MAX : "Token should not be MAX";
             output.add(token);

File: deep/src/test/java/smile/llm/RotaryPositionalEncodingTest.java
Patch:
@@ -61,5 +61,8 @@ public void test() {
         assertEquals(0.8415f, real.getFloat(1, 0, 1), 1E-4);
         assertEquals(0.6861f, real.getFloat(1, 1, 0), 1E-4);
         assertEquals(0.7275f, real.getFloat(1, 1, 1), 1E-4);
+
+        assertEquals(0.9999f, real.getFloat(4095, 63, 0), 1E-4);
+        assertEquals(0.0101f, real.getFloat(4095, 63, 1), 1E-4);
     }
 }

File: deep/src/main/java/smile/llm/llama/FeedForward.java
Patch:
@@ -57,7 +57,7 @@ public FeedForward(int dim, int hiddenDim, int multipleOf, Double ffnDimMultipli
         this.module = new Module();
         this.module.register_module("w1", w1.asTorch());
         this.module.register_module("w2", w2.asTorch());
-        this.module.register_module("w2", w3.asTorch());
+        this.module.register_module("w3", w3.asTorch());
     }
 
     /**

File: deep/src/main/java/smile/llm/llama/Tokenizer.java
Patch:
@@ -77,9 +77,9 @@ private static String[] specialTokens() {
         };
 
         int base = specialTokens.length;
-        specialTokens = Arrays.copyOf(specialTokens, specialTokens.length + numReservedSpecialTokens);
-        for (int i = 0; i < numReservedSpecialTokens; i++) {
-            specialTokens[base + i] = String.format("<|reserved_special_token_{%d}|>", i + 5);
+        specialTokens = Arrays.copyOf(specialTokens, numReservedSpecialTokens);
+        for (int i = base; i < numReservedSpecialTokens; i++) {
+            specialTokens[i] = String.format("<|reserved_special_token_{%d}|>", i - base + 5);
         }
 
         return specialTokens;

File: deep/src/main/java/smile/llm/tokenizer/Tiktoken.java
Patch:
@@ -87,11 +87,11 @@ public Tiktoken(Pattern pattern, Map<Bytes, Integer> ranks, String bos, String e
     }
 
     /**
-     * Returns the size of vocabulary excluding special tokens.
-     * @return the size of vocabulary excluding special tokens.
+     * Returns the vocabulary size.
+     * @return the vocabulary size.
      */
     public int size() {
-        return ranks.size();
+        return decoder.length;
     }
 
     /**

File: deep/src/main/java/smile/llm/llama/ModelArgs.java
Patch:
@@ -73,7 +73,7 @@ public static ModelArgs from(String path, int maxBatchSize, int maxSeqLength) th
                 node.get("n_kv_heads").asInt(),
                 node.get("vocab_size").asInt(),
                 node.get("multiple_of").asInt(),
-                node.get("ffn_dim_multipler").asDouble(),
+                node.get("ffn_dim_multiplier").asDouble(),
                 node.get("norm_eps").asDouble(),
                 node.get("rope_theta").asDouble(),
                 maxBatchSize,

File: base/src/main/java/smile/data/formula/Terms.java
Patch:
@@ -89,9 +89,7 @@ public interface Terms {
             if (tokens.length > 1) {
                 for (int i = 0; i < tokens.length; i++) {
                     tokens[i] = tokens[i].trim();
-                    System.out.print(tokens[i]+" ");
                 }
-                System.out.println();
                 String rank = matcher.group(2);
                 return cross(rank == null ? tokens.length : Integer.parseInt(rank), tokens);
             }

File: base/src/main/java/smile/data/formula/Formula.java
Patch:
@@ -367,7 +367,7 @@ public StructType bind(StructType inputSchema) {
                              .map(Feature::field)
                              .toArray(StructField[]::new)
                 );
-            } catch (NullPointerException ex) {
+            } catch (IllegalArgumentException ex) {
                 logger.debug("The response variable {} doesn't exist in the schema {}", response, inputSchema);
             }
         }

File: deep/src/main/java/smile/vision/layer/Conv2dNormActivation.java
Patch:
@@ -66,7 +66,7 @@ public record Options(int in, int out, int kernel, int stride, int padding, int
          * @param kernel the window/kernel size.
          */
         public Options(int in, int out, int kernel) {
-            this(in, out, kernel, BatchNorm2dLayer::new, new ReLU(true));
+            this(in, out, kernel, channels -> new BatchNorm2dLayer(channels), new ReLU(true));
         }
 
         /**

File: base/src/main/java/smile/data/DataFrame.java
Patch:
@@ -1016,15 +1016,15 @@ default DataFrame drop(String... columns) {
      * Merges data frames horizontally by columns.
      * @param dataframes the data frames to merge.
      * @return a new data frame that combines this DataFrame
-     * with one more more other DataFrames by columns.
+     * with one more other DataFrames by columns.
      */
     DataFrame merge(DataFrame... dataframes);
 
     /**
      * Merges vectors with this data frame.
      * @param vectors the vectors to merge.
      * @return a new data frame that combines this DataFrame
-     * with one more more additional vectors.
+     * with one more additional vectors.
      */
     DataFrame merge(BaseVector... vectors);
 

File: base/src/main/java/smile/data/Tuple.java
Patch:
@@ -633,7 +633,7 @@ default <T> T getAs(int i) {
     /**
      * Returns the value of a given fieldName.
      * For primitive types if value is null it returns 'zero value' specific for primitive
-     * ie. 0 for Int - use isNullAt to ensure that value is not null
+     * i.e. 0 for Int - use isNullAt to ensure that value is not null
      *
      * @param field the name of field.
      * @param <T> the data type of field.

File: base/src/main/java/smile/data/formula/Formula.java
Patch:
@@ -49,7 +49,7 @@
  * {@code b} and {@code c} together with their second-order interactions.
  * The {@code -} operator removes the specified terms, so that
  * {@code (a+b+c)^2 - a::b} is identical to {@code a + b + c + b::c + a::c}.
- * It can also used to remove the intercept term: when fitting a linear model
+ * It can also be used to remove the intercept term: when fitting a linear model
  * {@code y ~ x - 1} specifies a line through the origin. A model with
  * no intercept can be also specified as {@code y ~ x + 0}.
  * <p>
@@ -426,7 +426,7 @@ public String toString() {
     }
 
     /**
-     * Apply the formula on a tuple to generate the predictors data.
+     * Apply the formula on a tuple to generate the predictor data.
      * @param tuple the input tuple.
      * @return the output tuple.
      */
@@ -525,7 +525,7 @@ private boolean hasBias() {
     /**
      * Returns the design matrix of predictors.
      * All categorical variables will be dummy encoded.
-     * If the formula doesn't has an Intercept term, the bias
+     * If the formula doesn't have an Intercept term, the bias
      * column will be included. Otherwise, it is based on the
      * setting of Intercept term.
      *

File: base/src/main/java/smile/data/formula/Term.java
Patch:
@@ -28,7 +28,7 @@
  * An abstract term in the formula. A term is recursively constructed
  * from constant symbols, variables and function symbols. A formula
  * consists of a series of terms. To be concise, we also allow
- * HyperTerms that can be can be expanded to multiple simple terms.
+ * HyperTerms that can be expanded to multiple simple terms.
  *
  * @author Haifeng Li
  */

File: base/src/main/java/smile/data/type/DataType.java
Patch:
@@ -71,7 +71,7 @@ enum ID {
 
     /**
      * Returns the type name used in external catalogs.
-     * DataType.of(name()) should returns the same type.
+     * DataType.of(name()) should return the same type.
      * @return the type name used in external catalogs.
      */
     String name();
@@ -261,7 +261,7 @@ default DataType unboxed() {
     }
 
     /**
-     * Infers the type of a string.
+     * Infers the type of string.
      * @param s the string value.
      * @return the inferred data type of string value.
      */

File: base/src/main/java/smile/data/type/DataTypes.java
Patch:
@@ -123,7 +123,7 @@ public static smile.data.type.DateTimeType datetime(String pattern) {
     }
 
     /**
-     * Creates an object data type of a given class.
+     * Creates an object data type of given class.
      * @param clazz the object class.
      * @return the object data type.
      */

File: base/src/main/java/smile/gap/BitString.java
Patch:
@@ -31,9 +31,9 @@
  * However, it this does not mean that the new generation is the same because
  * of mutation. Crossover is made in hope that new chromosomes will have good
  * parts of old chromosomes and maybe the new chromosomes will be better.
- * However it is good to leave some part of population survive to next
+ * However, it is good to leave some part of population survive to next
  * generation. Crossover rate generally should be high, about 80% - 95%.
- * However some results show that for some problems crossover rate about 60% is
+ * However, some results show that for some problems crossover rate about 60% is
  * the best.
  * <p>
  * Mutation rate determines how often will be parts of chromosome mutated.

File: base/src/main/java/smile/gap/Chromosome.java
Patch:
@@ -52,7 +52,7 @@ public interface Chromosome extends Comparable<Chromosome> {
      * For genetic algorithms, this method mutates the chromosome randomly.
      * The offspring may have no changes since the mutation rate is usually
      * very low. For Lamarckian algorithms, this method actually does the local
-     * search such as such as hill-climbing.
+     * search such as hill-climbing.
      */
     void mutate();
 }

File: base/src/main/java/smile/gap/GeneticAlgorithm.java
Patch:
@@ -101,7 +101,7 @@
  * sometimes rank selection can be better. In general, elitism should be used
  * unless other method is used to save the best found solution.</dd>
  * </dl>
- * This implementation also supports Lamarckian algorithm that is a hybrid of
+ * This implementation also supports Lamarckian algorithm that is a hybrid
  * of evolutionary computation and a local improver such as hill-climbing.
  * Lamarckian algorithm augments an EA with some
  * hill-climbing during the fitness assessment phase to revise each individual
@@ -132,7 +132,7 @@ public class GeneticAlgorithm <T extends Chromosome> {
     /**
      * The number of best chromosomes to copy to new population. When creating
      * new population by crossover and mutation, we have a big chance, that we
-     * will loose the best chromosome. Elitism first copies the best chromosome
+     * will lose the best chromosome. Elitism first copies the best chromosome
      * (or a few best chromosomes) to new population. The rest is done in
      * classical way. Elitism can very rapidly increase performance of GA,
      * because it prevents losing the best found solution.
@@ -160,7 +160,7 @@ public GeneticAlgorithm(T[] seeds) {
      * @param seeds the initial population, which is usually randomly generated.
      * @param elitism the number of best chromosomes to copy to new population.
      *                When creating new population by crossover and mutation,
-     *                we have a big chance, that we will loose the best chromosome.
+     *                we have a big chance, that we will lose the best chromosome.
      *                Elitism first copies the best chromosome (or a few best
      *                chromosomes) to new population. The rest is done in classical
      *                way. Elitism can very rapidly increase performance of GA,

File: base/src/main/java/smile/gap/LamarckianChromosome.java
Patch:
@@ -19,7 +19,7 @@
 
 /**
  * Artificial chromosomes used in Lamarckian algorithm that is a hybrid of
- * of evolutionary computation and a local improver such as hill-climbing.
+ * evolutionary computation and a local improver such as hill-climbing.
  * Lamarckian algorithm augments an EA with some
  * hill-climbing during the fitness assessment phase to revise each individual
  * as it is being assessed. The revised individual replaces the original one

File: base/src/main/java/smile/gap/Selection.java
Patch:
@@ -102,7 +102,7 @@ public <T extends Chromosome> T apply(T[] population) {
 
     /**
      * Rank Selection. The Roulette Wheel Selection will have problems when
-     * the fitnesses differs very much. For example, if the best chromosome
+     * the fitnesses differ very much. For example, if the best chromosome
      * fitness is 90% of all the roulette wheel then the other chromosomes
      * will have very few chances to be selected. Rank selection first ranks
      * the population and then every chromosome receives fitness from this
@@ -136,8 +136,8 @@ public <T extends Chromosome> T apply(T[] population) {
      * individual of some t individuals picked at random, with replacement,
      * from the population. First choose t (the tournament size) individuals
      * from the population at random. Then choose the best individual from
-     * tournament with probability p, choose the second best individual with
-     * probability p*(1-p), choose the third best individual with
+     * tournament with probability p, choose the second-best individual with
+     * probability p*(1-p), choose the third-best individual with
      * probability p*((1-p)<sup>2</sup>), and so on... Tournament Selection has become
      * the primary selection technique used for the Genetic Algorithm.
      * First, it's not sensitive to the particulars of the fitness function.

File: base/src/main/java/smile/interpolation/LaplaceInterpolation.java
Patch:
@@ -20,7 +20,7 @@
 import java.util.Arrays;
 
 /**
- * Laplace interpolation to restore missing or unmeasured values on a 2-dimensional
+ * Laplace's interpolation to restore missing or unmeasured values on a 2-dimensional
  * evenly spaced regular grid. In some sense, Laplace interpolation
  * produces the smoothest possible interpolant, which are obtained by solving
  * a very sparse linear equations with biconjugate gradient method.

File: base/src/main/java/smile/io/Arff.java
Patch:
@@ -52,7 +52,7 @@
  * with a '%', whereby the remainder of the line is ignored.
  * <p>
  * A significant advantage of the ARFF data file over the CSV data file is
- * the meta data information.
+ * the metadata information.
  * <p>
  * Also, the ability to include comments ensure we can record extra information
  * about the data set, including how it was derived, where it came from, and
@@ -152,7 +152,7 @@ public Arff(Path path, Charset charset) throws IOException, ParseException {
      * @param reader the file reader.
      * @throws IOException when fails to read the file.
      * @throws ParseException when fails to parse the file.
-]     */
+     */
     public Arff(Reader reader) throws IOException, ParseException {
         this.reader = reader;
 
@@ -268,7 +268,7 @@ private void readHeader() throws IOException, ParseException {
 
         while (ARFF_ATTRIBUTE.equalsIgnoreCase(tokenizer.sval)) {
             StructField attribute = nextAttribute();
-            // We may meet an relational attribute, which parseAttribute returns null
+            // We may meet a relational attribute, which parseAttribute returns null
             // as it flats the relational attribute out.
             if (attribute != null) {
                 fields.add(attribute);

File: base/src/main/java/smile/io/Read.java
Patch:
@@ -311,7 +311,7 @@ static DataFrame json(Path path, JSON.Mode mode, StructType schema) throws IOExc
      * with a '%', whereby the remainder of the line is ignored.
      * <p>
      * A significant advantage of the ARFF data file over the CSV data file is
-     * the meta data information.
+     * the metadata information.
      * <p>
      * Also, the ability to include comments ensure we can record extra information
      * about the data set, including how it was derived, where it came from, and
@@ -347,7 +347,7 @@ static DataFrame arff(String path) throws IOException, ParseException, URISyntax
      * with a '%', whereby the remainder of the line is ignored.
      * <p>
      * A significant advantage of the ARFF data file over the CSV data file is
-     * the meta data information.
+     * the metadata information.
      * <p>
      * Also, the ability to include comments ensure we can record extra information
      * about the data set, including how it was derived, where it came from, and

File: base/src/main/java/smile/math/BFGS.java
Patch:
@@ -48,7 +48,7 @@
  * updates specified by gradient evaluations (or approximate gradient
  * evaluations). Quasi-Newton methods are generalizations of the secant
  * method to find the root of the first derivative for multidimensional
- * problems. In multi-dimensional problems, the secant equation does not
+ * problems. In multidimensional problems, the secant equation does not
  * specify a unique solution, and quasi-Newton methods differ in how they
  * constrain the solution. The BFGS method is one of the most popular
  * members of this class.

File: base/src/main/java/smile/math/Histogram.java
Patch:
@@ -31,7 +31,7 @@
  * so experimentation is usually needed to determine an appropriate width.
  * <p>
  * Note that this class provides only tools to choose the bin width or the
- * number of bins and frequency counting. It does NOT providing plotting
+ * number of bins and frequency counting. It does NOT provide plotting
  * services.
  * 
  * @author Haifeng Li

File: base/src/main/java/smile/math/Root.java
Patch:
@@ -33,7 +33,7 @@ private Root() {
     /**
      * Brent's method for root-finding. It combines the bisection method,
      * the secant method and inverse quadratic interpolation. It has the
-     * reliability of bisection but it can be as quick as some of the
+     * reliability of bisection, but it can be as quick as some of the
      * less-reliable methods. The algorithm tries to use the potentially
      * fast-converging secant method or inverse quadratic interpolation
      * if possible, but it falls back to the more robust bisection method

File: base/src/main/java/smile/math/TimeFunction.java
Patch:
@@ -112,7 +112,7 @@ public String toString() {
     /**
      * Returns the linear learning rate decay function that starts with
      * an initial learning rate and reach an end learning rate in the given
-     * decay steps..
+     * decay steps.
      *
      * @param initLearningRate the initial learning rate.
      * @param decaySteps the decay steps.

File: base/src/main/java/smile/math/distance/EditDistance.java
Patch:
@@ -32,7 +32,7 @@
  * Given two strings x and y of length m and n (suppose {@code n >= m}), this
  * implementation takes O(ne) time and O(mn) space by an extended Ukkonen's
  * algorithm in case of unit cost, where e is the edit distance between x and y.
- * Thus this algorithm is output sensitive. The smaller the distance, the faster
+ * Thus, this algorithm is output sensitive. The smaller the distance, the faster
  * it runs.
  * <p>
  * For weighted cost, this implements the regular dynamic programming algorithm,

File: base/src/main/java/smile/math/kernel/DotProductKernel.java
Patch:
@@ -41,7 +41,7 @@ default double f(double dot) {
     double k(double dot);
 
     /**
-     * Computes the dot product kernel function and its gradient over hyperparameters..
+     * Computes the dot product kernel function and its gradient over hyperparameters.
      * @param dot The dot product.
      * @return the kernel value and gradient.
      */

File: base/src/main/java/smile/math/kernel/IsotropicKernel.java
Patch:
@@ -43,7 +43,7 @@ default double f(double dist) {
     double k(double dist);
 
     /**
-     * Computes the isotropic kernel function and its gradient over hyperparameters..
+     * Computes the isotropic kernel function and its gradient over hyperparameters.
      * @param dist The distance.
      * @return the kernel value and gradient.
      */

File: base/src/main/java/smile/math/kernel/SparseGaussianKernel.java
Patch:
@@ -28,7 +28,7 @@
  * where &sigma; {@code > 0} is the scale parameter of the kernel.
  * <p>
  * The Gaussian kernel is a good choice for a great deal of applications,
- * although sometimes it is remarked as being over used.
+ * although sometimes it is remarked as being overused.
 
  * @author Haifeng Li
  */

File: base/src/main/java/smile/math/kernel/SparseHyperbolicTangentKernel.java
Patch:
@@ -36,7 +36,7 @@
  * due to its origin from neural networks. However, it should be used carefully
  * since the kernel matrix may not be positive semi-definite. Besides, it was
  * reported the hyperbolic tangent kernel is not better than the Gaussian kernel
- * in general..
+ * in general.
  *
  * @author Haifeng Li
  */

File: base/src/main/java/smile/math/matrix/ARPACK.java
Patch:
@@ -309,7 +309,7 @@ public static Matrix.EVD eigen(IMatrix A, AsymmOption which, int nev, int ncv, d
     }
 
     /**
-     * Computes k largest approximate singular triples of a matrix.
+     * Computes k-largest approximate singular triples of a matrix.
      *
      * @param A the matrix to decompose.
      * @param k the number of singular triples to compute.
@@ -320,7 +320,7 @@ public static Matrix.SVD svd(IMatrix A, int k) {
     }
 
     /**
-     * Computes k largest approximate singular triples of a matrix.
+     * Computes k-largest approximate singular triples of a matrix.
      *
      * @param A the matrix to decompose.
      * @param k the number of singular triples to compute.

File: base/src/main/java/smile/math/matrix/BandMatrix.java
Patch:
@@ -52,11 +52,11 @@
  * From a computational point of view, working with band matrices is always
  * preferential to working with similarly dimensioned dense square matrices.
  * A band matrix can be likened in complexity to a rectangular matrix whose
- * row dimension is equal to the bandwidth of the band matrix. Thus the work
+ * row dimension is equal to the bandwidth of the band matrix. Thus, the work
  * involved in performing operations such as multiplication falls significantly,
  * often leading to huge savings in terms of calculation time and complexity.
  * <p>
- * Given a n-by-n band matrix with m<sub>1</sub> rows below the diagonal
+ * Given an n-by-n band matrix with m<sub>1</sub> rows below the diagonal
  * and m<sub>2</sub> rows above. The matrix is compactly stored in an array
  * A[0,n-1][0,m<sub>1</sub>+m<sub>2</sub>]. The diagonal elements are in
  * A[0,n-1][m<sub>1</sub>]. The subdiagonal elements are in A[j,n-1][0,m<sub>1</sub>-1]

File: base/src/main/java/smile/math/matrix/Lanczos.java
Patch:
@@ -35,7 +35,7 @@ public class Lanczos {
     private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(Lanczos.class);
 
     /**
-     * Find k largest approximate eigen pairs of a symmetric matrix by the
+     * Find k-largest approximate eigen pairs of a symmetric matrix by the
      * Lanczos algorithm.
      *
      * @param A the matrix supporting matrix vector multiplication operation.
@@ -48,7 +48,7 @@ public static Matrix.EVD eigen(IMatrix A, int k) {
     }
 
     /**
-     * Find k largest approximate eigen pairs of a symmetric matrix by the
+     * Find k-largest approximate eigen pairs of a symmetric matrix by the
      * Lanczos algorithm.
      *
      * @param A the matrix supporting matrix vector multiplication operation.

File: base/src/main/java/smile/math/matrix/PageRank.java
Patch:
@@ -21,7 +21,7 @@
 import smile.math.MathEx;
 
 /**
- * PageRank is a link analysis algorithm and it assigns a numerical weighting
+ * PageRank is a link analysis algorithm, and it assigns a numerical weighting
  * to each element of a hyperlinked set of documents, such as the World Wide
  * Web, with the purpose of "measuring" its relative importance within the
  * set. The algorithm may be applied to any collection of entities with

File: base/src/main/java/smile/math/matrix/SparseMatrix.java
Patch:
@@ -104,7 +104,7 @@ public class Entry {
         public final int index;
 
         /**
-         * Private constructor. Only the enclosure matrix can creates
+         * Private constructor. Only the enclosure matrix can create
          * the instances of entry.
          * @param i the row index.
          * @param j the column index.
@@ -655,7 +655,7 @@ public double[] diag() {
      * For details, see
      * <a href="http://people.sc.fsu.edu/~jburkardt/data/hb/hb.html">http://people.sc.fsu.edu/~jburkardt/data/hb/hb.html</a>.
      * <p>
-     * Note that our implementation supports only real-valued matrix and we
+     * Note that our implementation supports only real-valued matrix, and we
      * ignore the optional supplementary data (e.g. right hand side vectors).
      *
      * @param path the input file path.
@@ -720,7 +720,7 @@ public static SparseMatrix harwell(Path path) throws IOException {
      * Especially, the supplementary data in the form of right-hand sides,
      * estimates or solutions are treated as separate files.
      * <p>
-     * Note that our implementation supports only real-valued matrix and we ignore
+     * Note that our implementation supports only real-valued matrix, and we ignore
      * the optional supplementary data (e.g. right hand side vectors).
      *
      * @param path the input file path.

File: base/src/main/java/smile/math/matrix/fp32/ARPACK.java
Patch:
@@ -309,7 +309,7 @@ public static Matrix.EVD eigen(IMatrix A, AsymmOption which, int nev, int ncv, f
     }
 
     /**
-     * Computes k largest approximate singular triples of a matrix.
+     * Computes k-largest approximate singular triples of a matrix.
      *
      * @param A the matrix to decompose.
      * @param k the number of singular triples to compute.
@@ -320,7 +320,7 @@ public static Matrix.SVD svd(IMatrix A, int k) {
     }
 
     /**
-     * Computes k largest approximate singular triples of a matrix.
+     * Computes k-largest approximate singular triples of a matrix.
      *
      * @param A the matrix to decompose.
      * @param k the number of singular triples to compute.

File: base/src/main/java/smile/math/matrix/fp32/BandMatrix.java
Patch:
@@ -52,11 +52,11 @@
  * From a computational point of view, working with band matrices is always
  * preferential to working with similarly dimensioned dense square matrices.
  * A band matrix can be likened in complexity to a rectangular matrix whose
- * row dimension is equal to the bandwidth of the band matrix. Thus the work
+ * row dimension is equal to the bandwidth of the band matrix. Thus, the work
  * involved in performing operations such as multiplication falls significantly,
  * often leading to huge savings in terms of calculation time and complexity.
  * <p>
- * Given a n-by-n band matrix with m<sub>1</sub> rows below the diagonal
+ * Given an n-by-n band matrix with m<sub>1</sub> rows below the diagonal
  * and m<sub>2</sub> rows above. The matrix is compactly stored in an array
  * A[0,n-1][0,m<sub>1</sub>+m<sub>2</sub>]. The diagonal elements are in
  * A[0,n-1][m<sub>1</sub>]. The subdiagonal elements are in A[j,n-1][0,m<sub>1</sub>-1]

File: base/src/main/java/smile/math/matrix/fp32/SparseMatrix.java
Patch:
@@ -104,7 +104,7 @@ public class Entry {
         public final int index;
 
         /**
-         * Private constructor. Only the enclosure matrix can creates
+         * Private constructor. Only the enclosure matrix can create
          * the instances of entry.
          * @param i the row index.
          * @param j the column index.
@@ -641,7 +641,7 @@ public float[] diag() {
      * For details, see
      * <a href="http://people.sc.fsu.edu/~jburkardt/data/hb/hb.html">http://people.sc.fsu.edu/~jburkardt/data/hb/hb.html</a>.
      * <p>
-     * Note that our implementation supports only real-valued matrix and we
+     * Note that our implementation supports only real-valued matrix, and we
      * ignore the optional supplementary data (e.g. right hand side vectors).
      *
      * @param path the input file path.
@@ -706,7 +706,7 @@ public static SparseMatrix harwell(Path path) throws IOException {
      * Especially, the supplementary data in the form of right-hand sides,
      * estimates or solutions are treated as separate files.
      * <p>
-     * Note that our implementation supports only real-valued matrix and we ignore
+     * Note that our implementation supports only real-valued matrix, and we ignore
      * the optional supplementary data (e.g. right hand side vectors).
      *
      * @param path the input file path.

File: base/src/main/java/smile/math/random/MersenneTwister64.java
Patch:
@@ -56,7 +56,7 @@ public class MersenneTwister64 implements RandomNumberGenerator {
 
     /**
      * Internal state to hold 64 bits, that might
-     * used to generate two 32 bit values.
+     * be used to generate two 32 bit values.
      */
     private long bits64;
     private boolean bitState = true;

File: base/src/main/java/smile/math/special/Beta.java
Patch:
@@ -74,7 +74,7 @@ public static double beta(double x, double y) {
      */
     public static double regularizedIncompleteBetaFunction(double alpha, double beta, double x) {
         // This function is often used to calculate p-value of model fitting.
-        // Due to floating error, the model may provide a x that could be slightly
+        // Due to floating error, the model may provide an x that could be slightly
         // greater than 1 or less than 0. We allow tiny slack here to avoid brute exception.
         if (x < 0.0 && abs(x) < EPS) {
             return 0.0;

File: base/src/main/java/smile/neighbor/CoverTree.java
Patch:
@@ -481,7 +481,7 @@ private Node newLeaf(int idx) {
 
     /**
      * Returns the max distance of the reference point p in current node to
-     * it's children nodes.
+     * its children nodes.
      * @param v the stack of DistanceNode objects.
      * @return the distance of the furthest child.
      */

File: base/src/main/java/smile/neighbor/MPLSH.java
Patch:
@@ -64,7 +64,7 @@ public class MPLSH <E> extends LSH<E> {
      * @param k the number of random projection hash functions, which is usually
      * set to log(N) where N is the dataset size.
      * @param w the width of random projections. It should be sufficiently
-     * away from 0. But we should not choose an w value that is too large, which
+     * away from 0. But we should not choose a w value that is too large, which
      * will increase the query time.
      */
     public MPLSH(int d, int L, int k, double w) {

File: base/src/main/java/smile/neighbor/lsh/Hash.java
Patch:
@@ -80,7 +80,7 @@ public class Hash implements Serializable {
      * @param k the number of random projection hash functions, which is usually
      *          set to log(N) where N is the dataset size.
      * @param w the width of random projections. It should be sufficiently away
-     *          from 0. But we should not choose an w value that is too large,
+     *          from 0. But we should not choose a w value that is too large,
      *          which will increase the query time.
      * @param H the size of universal hash tables.
      */

File: base/src/main/java/smile/neighbor/lsh/HashValueParzenModel.java
Patch:
@@ -130,7 +130,7 @@ public double mean() {
 
     /**
      * Returns the standard deviation.
-     * @return the conditional standard deviation..
+     * @return the conditional standard deviation.
      */
     public double sd() {
         return sd;

File: base/src/main/java/smile/neighbor/lsh/MultiProbeHash.java
Patch:
@@ -43,7 +43,7 @@ public class MultiProbeHash extends Hash {
      * @param k the number of random projection hash functions, which is usually
      *          set to log(N) where N is the dataset size.
      * @param w the width of random projections. It should be sufficiently away
-     *          from 0. But we should not choose an w value that is too large,
+     *          from 0. But we should not choose a w value that is too large,
      *          which will increase the query time.
      * @param H the size of universal hash tables.
      */

File: base/src/main/java/smile/sort/QuickSort.java
Patch:
@@ -47,7 +47,7 @@
  * always appear before S in the sorted list.
  * <p>
  * For speed of execution, we implement it without recursion. Instead,
- * we requires an auxiliary array (stack) of storage, of length
+ * we require an auxiliary array (stack) of storage, of length
  * 2 log<sub><small>2</small></sub> n. When a subarray has gotten down to size 7,
  * we sort it by straight insertion.
  * 

File: base/src/main/java/smile/stat/GoodTuring.java
Patch:
@@ -25,7 +25,7 @@
  * given a set of past observations of objects from different species.
  * In drawing balls from an urn, the 'objects' would be balls and the
  * 'species' would be the distinct colors of the balls (finite but
- * unknown in number). After drawing R_red red balls, R_black black
+ * unknown number). After drawing R_red red balls, R_black black
  * balls and , R_green green balls, we would ask what is the probability
  * of drawing a red ball, a black ball, a green ball or one of a
  * previously unseen color.

File: base/src/main/java/smile/stat/Hypothesis.java
Patch:
@@ -88,7 +88,7 @@ static TTest test(double[] x, double[] y) {
          *               from populations with the same true variance.
          *               "unequal.var if the data arrays are allowed to be drawn
          *               from populations with unequal variances.
-         *               "paired" if x and y are two values (i.e., pair of values)
+         *               "paired" if x and y are two values (i.e., a pair of values)
          *               for the same samples.
          * @return the test results.
          */

File: base/src/main/java/smile/stat/Sampling.java
Patch:
@@ -77,7 +77,7 @@ static int[][] strata(int[] category) {
             }
         }
 
-        // # of samples in each strata
+        // # of samples in each stratum
         int[] ni = new int[m];
         for (int i : y) ni[i]++;
 

File: base/src/main/java/smile/stat/distribution/BernoulliDistribution.java
Patch:
@@ -20,7 +20,7 @@
 import smile.math.MathEx;
 
 /**
- * Bernoulli distribution is a discrete probability distribution, which takes
+ * Bernoulli's distribution is a discrete probability distribution, which takes
  * value 1 with success probability p and value 0 with failure probability
  * q = 1 - p.
  * <p>
@@ -63,7 +63,7 @@ public BernoulliDistribution(double p) {
 
     /**
      * Estimates the distribution parameters by MLE.
-     * @param data data[i] == 1 if the i-<i>th</i> trail is success. Otherwise 0.
+     * @param data data[i] == 1 if the i-<i>th</i> trail is success. Otherwise, 0.
      * @return the distribution.
      */
     public static BernoulliDistribution fit(int[] data) {
@@ -81,7 +81,7 @@ public static BernoulliDistribution fit(int[] data) {
     }
 
     /**
-     * Construct an Bernoulli from the given samples. Parameter
+     * Construct a Bernoulli from the given samples. Parameter
      * will be estimated from the data by MLE.
      * @param data the boolean array to indicate if the i-<i>th</i> trail success.
      */

File: base/src/main/java/smile/stat/distribution/BinomialDistribution.java
Patch:
@@ -196,7 +196,7 @@ public double rand() {
         boolean inv = p > 0.5;
         double np = n * Math.min(p, 1.0 - p);
 
-        // Poisson approximation for extremely low np
+        // Poisson's approximation for extremely low np
         int x;
         if (np < 1E-6) {
             x = PoissonDistribution.tinyLambdaRand(np);

File: base/src/main/java/smile/stat/distribution/DiscreteExponentialFamily.java
Patch:
@@ -32,7 +32,7 @@
 public interface DiscreteExponentialFamily {
 
     /**
-     * The M step in the EM algorithm, which depends the specific distribution.
+     * The M step in the EM algorithm, which depends on the specific distribution.
      *
      * @param x the input data for estimation
      * @param posteriori the posteriori probability.

File: base/src/main/java/smile/stat/distribution/DiscreteMixture.java
Patch:
@@ -138,7 +138,7 @@ public double variance() {
     }
 
     /**
-     * Shannon entropy. Not supported.
+     * Shannon's entropy. Not supported.
      */
     @Override
     public double entropy() {

File: base/src/main/java/smile/stat/distribution/EmpiricalDistribution.java
Patch:
@@ -26,7 +26,7 @@
  * An empirical distribution function or empirical cdf, is a cumulative
  * probability distribution function that concentrates probability 1/n at
  * each of the n numbers in a sample. As n grows the empirical distribution
- * will getting closer to the true distribution.
+ * will get closer to the true distribution.
  * Empirical distribution is a very important estimator in Statistics. In
  * particular, the Bootstrap method rely heavily on the empirical distribution.
  *

File: base/src/main/java/smile/stat/distribution/FDistribution.java
Patch:
@@ -79,7 +79,7 @@ public double variance() {
     }
 
     /**
-     * Shannon entropy. Not supported.
+     * Shannon's entropy. Not supported.
      */
     @Override
     public double entropy() {

File: base/src/main/java/smile/stat/distribution/GaussianMixture.java
Patch:
@@ -20,7 +20,7 @@
 import smile.math.MathEx;
 
 /**
- * Finite univariate Gaussian mixture. The EM algorithm is provide to learned
+ * Finite univariate Gaussian mixture. The EM algorithm is provided to learn
  * the mixture model from data. BIC score is employed to estimate the number
  * of components.
  *

File: base/src/main/java/smile/stat/distribution/GeometricDistribution.java
Patch:
@@ -23,7 +23,7 @@
  * <code>{1, 2, 3, &hellip;}</code>. Sometimes, people define that the probability
  * distribution of the number <code>Y = X - 1</code> of failures before the first
  * success, supported on the set <code>{0, 1, 2, 3, &hellip;}</code>. To reduce
- * the confusion, we denote the later as shifted geometric distribution.
+ * the confusion, we denote the latter as shifted geometric distribution.
  * If the probability of success on each trial is p, then the probability that
  * the k-<i>th</i> trial (out of k trials) is the first success is
  * Pr(X = k) = (1 - p)<sup>k-1</sup> p.
@@ -110,7 +110,7 @@ public double sd() {
     }
 
     /**
-     * Shannon entropy. Not supported.
+     * Shannon's entropy. Not supported.
      */
     @Override
     public double entropy() {

File: base/src/main/java/smile/stat/distribution/KernelDensity.java
Patch:
@@ -130,7 +130,7 @@ public double sd() {
     }
 
     /**
-     * Shannon entropy. Not supported.
+     * Shannon's entropy. Not supported.
      */
     @Override
     public double entropy() {

File: base/src/main/java/smile/stat/distribution/Mixture.java
Patch:
@@ -151,7 +151,7 @@ public double variance() {
     }
 
     /**
-     * Shannon entropy. Not supported.
+     * Shannon's entropy. Not supported.
      */
     @Override
     public double entropy() {

File: base/src/main/java/smile/stat/distribution/MultivariateDistribution.java
Patch:
@@ -37,7 +37,7 @@ public interface MultivariateDistribution extends Serializable {
     int length();
 
     /**
-     * Shannon entropy of the distribution.
+     * Shannon's entropy of the distribution.
      * @return Shannon entropy
      */
     double entropy();

File: base/src/main/java/smile/stat/distribution/MultivariateExponentialFamily.java
Patch:
@@ -31,7 +31,7 @@
 public interface MultivariateExponentialFamily {
 
     /**
-     * The M step in the EM algorithm, which depends the specific distribution.
+     * The M step in the EM algorithm, which depends on the specific distribution.
      *
      * @param x the input data for estimation
      * @param posteriori the posteriori probability.

File: base/src/main/java/smile/stat/distribution/MultivariateGaussianMixture.java
Patch:
@@ -21,7 +21,7 @@
 import smile.math.matrix.Matrix;
 
 /**
- * Finite multivariate Gaussian mixture. The EM algorithm is provide to learned
+ * Finite multivariate Gaussian mixture. The EM algorithm is provided to learn
  * the mixture model from data. The BIC score is employed to estimate the number
  * of components.
  *

File: base/src/main/java/smile/stat/distribution/NegativeBinomialDistribution.java
Patch:
@@ -103,7 +103,7 @@ public double sd() {
     }
 
     /**
-     * Shannon entropy. Not supported.
+     * Shannon's entropy. Not supported.
      */
     @Override
     public double entropy() {

File: core/src/main/java/smile/anomaly/SVM.java
Patch:
@@ -51,7 +51,7 @@ public SVM(MercerKernel<T> kernel, T[] vectors, double[] weight, double b) {
     }
 
     /**
-     * Fits an one-class SVM.
+     * Fits a one-class SVM.
      * @param x training samples.
      * @param kernel the kernel function.
      * @param <T> the data type.
@@ -62,7 +62,7 @@ public static <T> SVM<T> fit(T[] x, MercerKernel<T> kernel) {
     }
 
     /**
-     * Fits an one-class SVM.
+     * Fits a one-class SVM.
      * @param x training samples.
      * @param kernel the kernel function.
      * @param nu the parameter sets an upper bound on the fraction of outliers

File: core/src/main/java/smile/anomaly/package-info.java
Patch:
@@ -28,7 +28,7 @@
  * least to the remainder of the data set. Supervised anomaly detection
  * techniques require a data set that has been labeled as "normal" and
  * "abnormal" and involves training a classifier (the key difference to
- * many other statistical classification problems is the inherent unbalanced
+ * many other statistical classification problems is the inherently unbalanced
  * nature of outlier detection). Semi-supervised anomaly detection techniques
  * construct a model representing normal behavior from a given normal training
  * data set, and then test the likelihood of a test instance to be generated

File: core/src/main/java/smile/association/ARM.java
Patch:
@@ -28,7 +28,7 @@
  * be a set of n binary attributes called items. Let
  * <code>D = {t<sub>1</sub>, t<sub>2</sub>,..., t<sub>m</sub>}</code>
  * be a set of transactions called the database. Each transaction in
- * <code>D</code> has an unique transaction ID and contains a subset of
+ * <code>D</code> has a unique transaction ID and contains a subset of
  * the items in <code>I</code>. An association rule is defined as an
  * implication of the form <code>X &rArr; Y</code>
  * where <code>X, Y &sube; I</code> and <code>X &cap; Y = &Oslash;</code>.

File: core/src/main/java/smile/association/AssociationRule.java
Patch:
@@ -25,7 +25,7 @@
  * be a set of <code>n</code> binary attributes called items. Let
  * <code>D = {t<sub>1</sub>, t<sub>2</sub>,..., t<sub>m</sub>}</code>
  * be a set of transactions called the database. Each transaction in
- * <code>D</code> has an unique transaction ID and contains a subset
+ * <code>D</code> has a unique transaction ID and contains a subset
  * of the items in <code>I</code>. An association rule is defined
  * as an implication of the form <code>X &rArr; Y</code>
  * where <code>X, Y &sube; I</code> and <code>X &cap; Y = &Oslash;</code>.

File: core/src/main/java/smile/association/FPGrowth.java
Patch:
@@ -111,7 +111,7 @@ public boolean hasNext() {
                      *   new local header table and (iii) populate with ancestors.
                      * - If new local FP tree is not empty repeat mining operation.
                      *
-                     * Otherwise end.
+                     * Otherwise, end.
                      */
                     if (i-- > 0) {
                         grow(T0.headerTable[i], null, localItemSupport, prefixItemset);
@@ -149,7 +149,7 @@ public static Stream<ItemSet> apply(FPTree tree) {
      * new local header table and (iii) populate with ancestors.
      * <LI> If new local FP tree is not empty repeat mining operation.
      * </OL>
-     * Otherwise end.
+     * Otherwise, end.
      * @param itemset the current item sets as generated so far (null at start).
      */
     private void grow(FPTree fptree, int[] itemset, int[] localItemSupport, int[] prefixItemset) {
@@ -283,7 +283,7 @@ private FPTree getLocalFPTree(FPTree.Node node, int[] localItemSupport, int[] pr
     }
 
     /**
-     * Insert a item to the front of an item set.
+     * Insert an item to the front of an item set.
      * @param itemset the original item set.
      * @param item the new item to be inserted.
      * @return the combined item set

File: core/src/main/java/smile/association/TotalSupportTree.java
Patch:
@@ -204,7 +204,7 @@ private int getSupport(int[] itemset, int index, Node node) {
         int item = order[itemset[index]];
         Node child = node.children[item];
         if (child != null) {
-            // If the index is 0, then this is the last element (i.e the
+            // If the index is 0, then this is the last element (i.e. the
             // input is a 1 itemset)  and therefore item set found
             if (index == 0) {
                 return child.support;

File: core/src/main/java/smile/base/svm/LASVM.java
Patch:
@@ -386,7 +386,7 @@ private boolean process(int i, T x, int y) {
             // Parallel stream may cause unreproducible results due to
             // different numeric round-off because of different data
             // partitions (i.e. different number of cores/threads).
-            // The speed up of parallel stream is also limited as
+            // The speedup of parallel stream is also limited as
             // the number of support vectors is often small.
             double k = kernel.k(v.x, x);
             cache[v.i] = k;

File: core/src/main/java/smile/base/svm/OCSVM.java
Patch:
@@ -109,7 +109,7 @@ public OCSVM(MercerKernel<T> kernel, double nu, double tol) {
     }
 
     /**
-     * Fits an one-class support vector machine.
+     * Fits a one-class support vector machine.
      * @param x training instances.
      * @return the model.
      */

File: core/src/main/java/smile/classification/AdaBoost.java
Patch:
@@ -38,7 +38,7 @@
  * decision trees is probably the most popular combination. AdaBoost is adaptive
  * in the sense that subsequent classifiers built are tweaked in favor of those
  * instances misclassified by previous classifiers. AdaBoost is sensitive to
- * noisy data and outliers. However in some problems it can be less susceptible
+ * noisy data and outliers. However, in some problems it can be less susceptible
  * to the over-fitting problem than most learning algorithms.
  * <p>
  * AdaBoost calls a weak classifier repeatedly in a series of rounds from
@@ -147,7 +147,7 @@ public static AdaBoost fit(Formula formula, DataFrame data) {
      *
      * @param formula a symbolic description of the model to be fitted.
      * @param data the data frame of the explanatory and response variables.
-     * @param params the hyper-parameters.
+     * @param params the hyperparameters.
      * @return the model.
      */
     public static AdaBoost fit(Formula formula, DataFrame data, Properties params) {

File: core/src/main/java/smile/classification/FLD.java
Patch:
@@ -41,7 +41,7 @@
  * FLD is also closely related to principal component analysis (PCA), which also
  * looks for linear combinations of variables which best explain the data.
  * As a supervised method, FLD explicitly attempts to model the
- * difference between the classes of data. On the other hand, PCA is a
+ * difference between the classes of data. On the other hand, PCA is an
  * unsupervised method and does not take into account any difference in class.
  * <p>
  * One complication in applying FLD (and LDA) to real data

File: core/src/main/java/smile/classification/KNN.java
Patch:
@@ -78,7 +78,7 @@ public class KNN<T> extends AbstractClassifier<T> {
      */
     private final KNNSearch<T, T> knn;
     /**
-     * The labels of training samples.
+     * The labels of training sample.
      */
     private final int[] y;
     /**

File: core/src/main/java/smile/classification/LogisticRegression.java
Patch:
@@ -539,7 +539,7 @@ static class BinomialObjective implements DifferentiableMultivariateFunction {
 
         @Override
         public double f(double[] w) {
-            // Since BFGS try to minimize the objective function
+            // Since BFGS try to minimize the objective function,
             // and we try to maximize the log-likelihood, we really
             // return the negative log-likelihood here.
             double f = IntStream.range(0, x.length).parallel().mapToDouble(i -> {

File: core/src/main/java/smile/classification/MLP.java
Patch:
@@ -38,7 +38,7 @@
  * transformation, called activation function, is a bounded non-decreasing
  * (non-linear) function.
  * <p>
- * The representational capabilities of a MLP are determined by the range of
+ * The representational capabilities of an MLP are determined by the range of
  * mappings it may implement through weight variation. Single layer perceptrons
  * are capable of solving only linearly separable problems. With the sigmoid
  * function as activation function, the single-layer network is identical
@@ -70,7 +70,7 @@
  * at large &eta;, is to make the change in weight dependent on the past weight
  * change by adding a momentum term.
  * <p>
- * Although the back-propagation algorithm may performs gradient
+ * Although the back-propagation algorithm may perform gradient
  * descent on the total error of all instances in a batch way, 
  * the learning rule is often applied to each instance separately in an online
  * way or stochastic way. There exists empirical indication that the stochastic

File: core/src/main/java/smile/classification/Maxent.java
Patch:
@@ -507,7 +507,7 @@ static class BinomialObjective implements DifferentiableMultivariateFunction {
         
         @Override
         public double f(double[] w) {
-            // Since BFGS try to minimize the objective function
+            // Since BFGS try to minimize the objective function,
             // and we try to maximize the log-likelihood, we really
             // return the negative log-likelihood here.
             double f = IntStream.range(0, x.length).parallel().mapToDouble(i -> {

File: core/src/main/java/smile/classification/RandomForest.java
Patch:
@@ -470,7 +470,7 @@ public StructType schema() {
 
     /**
      * Returns the overall out-of-bag metric estimations. The OOB estimate is
-     * quite accurate given that enough trees have been grown. Otherwise the
+     * quite accurate given that enough trees have been grown. Otherwise, the
      * OOB error estimate can bias upward.
      * 
      * @return the out-of-bag metrics estimations.

File: core/src/main/java/smile/classification/SVM.java
Patch:
@@ -54,13 +54,13 @@
  * transformed feature space. The transformation may be nonlinear and
  * the transformed space be high dimensional. For example, the feature space
  * corresponding Gaussian kernel is a Hilbert space of infinite dimension.
- * Thus though the classifier is a hyperplane in the high-dimensional feature
+ * Thus, though the classifier is a hyperplane in the high-dimensional feature
  * space, it may be nonlinear in the original input space. Maximum margin
  * classifiers are well regularized, so the infinite dimension does not spoil
  * the results.
  * <p>
  * The effectiveness of SVM depends on the selection of kernel, the kernel's
- * parameters, and the soft margin parameter C. Given a kernel, best combination
+ * parameters, and the soft margin parameter C. Given a kernel, the best combination
  * of C and kernel's parameters is often selected by a grid-search with
  * cross validation.
  * <p>

File: core/src/main/java/smile/classification/SparseLogisticRegression.java
Patch:
@@ -478,7 +478,7 @@ static class BinomialObjective implements DifferentiableMultivariateFunction {
 
         @Override
         public double f(double[] w) {
-            // Since BFGS try to minimize the objective function
+            // Since BFGS try to minimize the objective function,
             // and we try to maximize the log-likelihood, we really
             // return the negative log-likelihood here.
             double f = IntStream.range(0, x.length).parallel().mapToDouble(i -> {

File: core/src/main/java/smile/clustering/FastPair.java
Patch:
@@ -155,7 +155,7 @@ public void remove(int p) {
     }
 
     /**
-     * Find closest pair by scanning list of nearest neighbors
+     * Find the closest pair by scanning list of nearest neighbors
      */
     public double getNearestPair(int[] pair) {
         if (npoints < 2) {

File: core/src/main/java/smile/clustering/HierarchicalClustering.java
Patch:
@@ -106,7 +106,7 @@ public static HierarchicalClustering fit(Linkage linkage) {
             height[i] = fp.getNearestPair(merge[i]);
             linkage.merge(merge[i][0], merge[i][1]);     // merge clusters into one
             fp.remove(merge[i][1]);           // drop b
-            fp.updatePoint(merge[i][0]);      // and tell closest pairs about merger
+            fp.updatePoint(merge[i][0]);      // and tell the closest pairs about merger
 
             int p = merge[i][0];
             int q = merge[i][1];

File: core/src/main/java/smile/clustering/KModes.java
Patch:
@@ -25,7 +25,7 @@
 
 /**
  * K-Modes clustering. K-Modes is the binary equivalent for K-Means.
- * The mean update for centroids is replace by the mode one which is
+ * The mean update for centroids is replaced by the mode one which is
  * a majority vote among element of each cluster.
  *
  * <h2>References</h2>

File: core/src/main/java/smile/clustering/PartitionClustering.java
Patch:
@@ -89,7 +89,7 @@ public String toString() {
 
     /**
      * Initialize cluster membership of input objects with K-Means++ algorithm.
-     * Many clustering methods, e.g. k-means, need a initial clustering
+     * Many clustering methods, e.g. k-means, need an initial clustering
      * configuration as a seed.
      * <p>
      * K-Means++ is based on the intuition of spreading the k initial cluster

File: core/src/main/java/smile/clustering/linkage/WPGMALinkage.java
Patch:
@@ -25,7 +25,7 @@
  * that are about to fuse.
  * <p>
  * Note that the terms weighted and unweighted refer to the final result,
- * not the math by which it is achieved. Thus the simple averaging in WPGMA
+ * not the math by which it is achieved. Thus, the simple averaging in WPGMA
  * produces a weighted result, and the proportional averaging in UPGMA produces
  * an unweighted result.
  * 

File: core/src/main/java/smile/clustering/linkage/WardLinkage.java
Patch:
@@ -24,7 +24,7 @@
  * The dissimilarity between two clusters is computed as the
  * increase in the "error sum of squares" (ESS) after fusing two clusters
  * into a single cluster. Ward's Method seeks to choose the successive
- * clustering steps so as to minimize the increase in ESS at each step.
+ * clustering steps to minimize the increase in ESS at each step.
  * Note that it is only valid for Euclidean distance based proximity matrix.
  * 
  * @author Haifeng Li

File: core/src/main/java/smile/feature/extraction/package-info.java
Patch:
@@ -75,9 +75,9 @@
  * <p>
  * A different approach to nonlinear dimensionality reduction is through the
  * use of autoencoders, a special kind of feed-forward neural networks with
- * a bottle-neck hidden layer. The training of deep encoders is typically
+ * a bottleneck hidden layer. The training of deep encoders is typically
  * performed using a greedy layer-wise pre-training (e.g., using a stack of
- * Restricted Boltzmann machines) that is followed by a fine tuning stage based
+ * Restricted Boltzmann machines) that is followed by a fine-tuning stage based
  * on backpropagation.
  *
  * @author Haifeng Li

File: core/src/main/java/smile/feature/selection/GAFE.java
Patch:
@@ -55,7 +55,7 @@ public class GAFE {
     /**
      * The number of best chromosomes to copy to new population. When creating
      * new population by crossover and mutation, we have a big chance, that we
-     * will loose the best chromosome. Elitism first copies the best chromosome
+     * will lose the best chromosome. Elitism first copies the best chromosome
      * (or a few best chromosomes) to new population. The rest is done in
      * classical way. Elitism can very rapidly increase performance of GA,
      * because it prevents losing the best found solution.

File: core/src/main/java/smile/manifold/IsoMap.java
Patch:
@@ -162,7 +162,7 @@ public static <T> IsoMap of(T[] data, Distance<T> distance, int k, int d, boolea
             }
         }
 
-        // Use largest connected component of nearest neighbor graph.
+        // Use the largest connected component of nearest neighbor graph.
         NearestNeighborGraph nng = NearestNeighborGraph.largest(graph);
 
         int[] index = nng.index;

File: core/src/main/java/smile/manifold/LLE.java
Patch:
@@ -17,6 +17,7 @@
 
 package smile.manifold;
 
+import java.io.Serial;
 import java.io.Serializable;
 import java.util.Arrays;
 import smile.graph.AdjacencyList;
@@ -52,6 +53,7 @@
  * @author Haifeng Li
  */
 public class LLE implements Serializable {
+    @Serial
     private static final long serialVersionUID = 2L;
     private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(LLE.class);
 
@@ -106,7 +108,7 @@ public static LLE of(double[][] data, int k, int d) {
             tol = 1E-3;
         }
 
-        // Use largest connected component of nearest neighbor graph.
+        // Use the largest connected component of nearest neighbor graph.
         int[][] N = new int[data.length][k];
         AdjacencyList graph = NearestNeighborGraph.of(data, k, false, (v1, v2, weight, j) -> N[v1][j] = v2);
         NearestNeighborGraph nng = NearestNeighborGraph.largest(graph);

File: core/src/main/java/smile/manifold/LaplacianEigenmap.java
Patch:
@@ -40,7 +40,7 @@
  * <p>
  * The locality preserving character of the Laplacian Eigenmap algorithm makes
  * it relatively insensitive to outliers and noise. It is also not prone to
- * "short circuiting" as only the local distances are used.
+ * "short-circuiting" as only the local distances are used.
  *
  * @see IsoMap
  * @see LLE
@@ -144,7 +144,7 @@ public static <T> LaplacianEigenmap of(T[] data, Distance<T> distance, int k) {
      * @return the model.
      */
     public static <T> LaplacianEigenmap of(T[] data, Distance<T> distance, int k, int d, double t) {
-        // Use largest connected component of nearest neighbor graph.
+        // Use the largest connected component of nearest neighbor graph.
         AdjacencyList graph = NearestNeighborGraph.of(data, distance, k, false, null);
         NearestNeighborGraph nng = NearestNeighborGraph.largest(graph);
 

File: core/src/main/java/smile/manifold/MDS.java
Patch:
@@ -27,7 +27,7 @@
  * Classical multidimensional scaling, also known as principal coordinates
  * analysis. Given a matrix of dissimilarities (e.g. pairwise distances), MDS
  * finds a set of points in low dimensional space that well-approximates the
- * dissimilarities. We are not restricted to using an Euclidean
+ * dissimilarities. We are not restricted to using Euclidean
  * distance metric. However, when Euclidean distances are used MDS is
  * equivalent to PCA.
  *

File: core/src/main/java/smile/manifold/SammonMapping.java
Patch:
@@ -38,7 +38,7 @@
  * trying to achieve equality between corresponding inter-point distances we
  * can minimize the difference between corresponding inter-point distances.
  * This is one goal of the Sammon's mapping algorithm. A second goal of the Sammon's
- * mapping algorithm is to preserve the topology as best as possible by giving
+ * mapping algorithm is to preserve the topology as good as possible by giving
  * greater emphasize to smaller interpoint distances. The Sammon's mapping
  * algorithm has the advantage that whenever it is possible to isometrically
  * project an object into a lower dimensional space it will be isometrically

File: core/src/main/java/smile/manifold/UMAP.java
Patch:
@@ -404,7 +404,7 @@ private static AdjacencyList computeFuzzySimplicialSet(AdjacencyList nng, int k,
 
         // Computes a continuous version of the distance to the kth nearest neighbor.
         // That is, this is similar to knn-distance but allows continuous k values
-        // rather than requiring an integral k. In essence we are simply computing
+        // rather than requiring an integral k. In essence, we are simply computing
         // the distance such that the cardinality of fuzzy set we generate is k.
         for (int i = 0; i < n; i++) {
             for (Edge edge : nng.getEdges(i)) {

File: core/src/main/java/smile/regression/GaussianProcessRegression.java
Patch:
@@ -63,7 +63,7 @@
  * <p>
  * Experimental evidence suggests that for large m the SR and Nystrom methods
  * have similar performance, but for small m the Nystrom method can be quite
- * poor. Also embarrassments can occur like the approximated predictive
+ * poor. Also, embarrassments can occur like the approximated predictive
  * variance being negative. For these reasons we do not recommend the
  * Nystrom method over the SR method.
  *

File: core/src/main/java/smile/regression/LASSO.java
Patch:
@@ -43,7 +43,7 @@
  * <p>
  * For over-determined systems (more instances than variables, commonly in
  * machine learning), we normalize variables with mean 0 and standard deviation
- * 1. For under-determined systems (less instances than variables, e.g.
+ * 1. For under-determined systems (fewer instances than variables, e.g.
  * compressed sensing), we assume white noise (i.e. no intercept in the linear
  * model) and do not perform normalization. Note that the solution
  * is not unique in this case.

File: core/src/main/java/smile/regression/LinearModel.java
Patch:
@@ -410,7 +410,7 @@ public void update(double[] x, double y) {
      * In some adaptive configurations it can be useful not to give equal
      * importance to all the historical data but to assign higher weights
      * to the most recent data (and then to forget the oldest one). This
-     * may happen when the phenomenon underlying the data is non stationary
+     * may happen when the phenomenon underlying the data is non-stationary
      * or when we want to approximate a nonlinear dependence by using a
      * linear model which is local in time. Both these situations are common
      * in adaptive control problems.

File: core/src/main/java/smile/regression/RandomForest.java
Patch:
@@ -348,7 +348,7 @@ public StructType schema() {
 
     /**
      * Returns the overall out-of-bag metric estimations. The OOB estimate is
-     * quite accurate given that enough trees have been grown. Otherwise the
+     * quite accurate given that enough trees have been grown. Otherwise, the
      * OOB error estimate can bias upward.
      * 
      * @return the overall out-of-bag metric estimations.

File: core/src/main/java/smile/regression/RegressionTree.java
Patch:
@@ -100,7 +100,7 @@ protected LeafNode newNode(int[] nodeSamples) {
         // in gradient tree boosting.
         double out = loss.output(nodeSamples, samples);
 
-        // RSS computation should always based on the sample mean in the node.
+        // RSS computation should always be based on the sample mean in the node.
         double mean = out;
         if (!loss.toString().equals("LeastSquares")) {
             int n = 0;

File: core/src/main/java/smile/regression/RidgeRegression.java
Patch:
@@ -35,7 +35,7 @@
  * errors in the observed response <code>Y</code>, producing a large variance.
  * <p>
  * Ridge regression is one method to address these issues. In ridge regression,
- * the matrix <code>X'X</code> is perturbed so as to make its determinant
+ * the matrix <code>X'X</code> is perturbed to make its determinant
  * appreciably different from 0.
  * <p>
  * Ridge regression is a kind of Tikhonov regularization, which is the most
@@ -56,14 +56,14 @@
  * variance 1), and then we perform ridge regression.
  * <p>
  * When including an intercept term in the regression, we usually leave
- * this coefficient unpenalized. Otherwise we could add some constant
+ * this coefficient unpenalized. Otherwise, we could add some constant
  * amount to the vector <code>y</code>, and this would not result in
  * the same solution. If we center the columns of <code>X</code>, then
  * the intercept estimate ends up just being the mean of <code>y</code>.
  * <p>
  * Ridge regression does not set coefficients exactly to zero unless
  * <code>&lambda; = &infin;</code>, in which case they're all zero.
- * Hence ridge regression cannot perform variable selection, and
+ * Hence, ridge regression cannot perform variable selection, and
  * even though it performs well in terms of prediction accuracy,
  * it does poorly in terms of offering a clear interpretation.
  *

File: core/src/main/java/smile/sequence/CRF.java
Patch:
@@ -41,7 +41,7 @@
  * often used for labeling or parsing of sequential data.
  * <p>
  * A CRF is a Markov random field that was trained discriminatively.
- * Therefore it is not necessary to model the distribution over always
+ * Therefore, it is not necessary to model the distribution over always
  * observed variables, which makes it possible to include arbitrarily
  * complicated features of the observed variables into the model.
  * <p>

File: core/src/main/java/smile/sequence/HMM.java
Patch:
@@ -33,7 +33,7 @@
  * and therefore the state transition probabilities are the only parameters.
  * In a hidden Markov model, the state is not directly visible, but output,
  * dependent on the state, is visible. Each state has a probability
- * distribution over the possible output tokens. Therefore the sequence of
+ * distribution over the possible output tokens. Therefore, the sequence of
  * tokens generated by an HMM gives some information about the sequence of
  * states.
  *

File: core/src/main/java/smile/timeseries/package-info.java
Patch:
@@ -18,7 +18,7 @@
 /**
  * Time series analysis. A time series is a series of data points indexed
  * in time order. Most commonly, a time series is a sequence taken at
- * successive equally spaced points in time. Thus it is a sequence of
+ * successive equally spaced points in time. Thus, it is a sequence of
  * discrete-time data.
  * <p>
  * Methods for time series analysis may be divided into two classes:
@@ -56,7 +56,7 @@
  * stationary. Differencing can help stabilize the mean of the time
  * series by removing changes in the level of a time series, and so
  * eliminating (or reducing) trend and seasonality. In addition,
- * transformations such as logarithms can help to stabilise the
+ * transformations such as logarithms can help to stabilize the
  * variance of a time series.
  *
  * @author Haifeng Li

File: core/src/main/java/smile/validation/metric/AdjustedMutualInformation.java
Patch:
@@ -33,7 +33,7 @@
  * two partitions are identical and 0 when the MI between two partitions
  * equals the value expected due to chance alone.
  * <p>
- * WARNING: The computation of adjustment is is really really slow.
+ * WARNING: The computation of adjustment is really slow.
  *
  * <h2>References</h2>
  * <ol>

File: core/src/main/java/smile/validation/metric/LogLoss.java
Patch:
@@ -18,7 +18,7 @@
 package smile.validation.metric;
 
 /**
- * Log loss is a evaluation metric for binary classifiers and it is sometimes
+ * Log loss is an evaluation metric for binary classifiers, and it is sometimes
  * the optimization objective as well in case of logistic regression and neural
  * networks. Log Loss takes into account the uncertainty of the prediction
  * based on how much it varies from the actual label. This provides a more

File: core/src/main/java/smile/vq/BIRCH.java
Patch:
@@ -28,7 +28,7 @@
  * Balanced Iterative Reducing and Clustering using Hierarchies. BIRCH performs
  * hierarchical clustering over particularly large data. An advantage of
  * BIRCH is its ability to incrementally and dynamically cluster incoming,
- * multi-dimensional metric data points in an attempt to produce the high
+ * multidimensional metric data points in an attempt to produce the high
  * quality clustering for a given set of resources (memory and time constraints).
  * <p>
  * BIRCH has several advantages. For example, each clustering decision is made

File: core/src/main/java/smile/vq/Neighborhood.java
Patch:
@@ -28,8 +28,8 @@
 public interface Neighborhood extends Serializable {
     /**
      * Returns the changing rate of neighborhood at a given iteration.
-     * @param i the row distance of topology from the the winner neuron.
-     * @param j the column distance of topology from the the winner neuron.
+     * @param i the row distance of topology from the winner neuron.
+     * @param j the column distance of topology from the winner neuron.
      * @param t the order number of current iteration.
      * @return the changing rate of neighborhood.
      */

File: core/src/main/java/smile/vq/NeuralMap.java
Patch:
@@ -26,7 +26,7 @@
  * NeuralMap is an efficient competitive learning algorithm inspired by growing
  * neural gas and BIRCH. Like growing neural gas, NeuralMap has the ability to
  * add and delete neurons with competitive Hebbian learning. Edges exist between
- * neurons close to each other. Such edges are intended place holders for
+ * neurons close to each other. Such edges are intended placeholders for
  * localized data distribution. Such edges also help to locate distinct clusters
  * (those clusters are not connected by edges).
  *

File: core/src/main/java/smile/vq/SOM.java
Patch:
@@ -27,7 +27,7 @@
 import smile.sort.QuickSort;
 
 /**
- * Self-Organizing Map. An SOM is a unsupervised learning method to produce
+ * Self-Organizing Map. An SOM is an unsupervised learning method to produce
  * a low-dimensional (typically two-dimensional) discretized representation
  * (called a map) of the input space of the training samples. The model was
  * first described as an artificial neural network by Teuvo Kohonen, and is

File: nlp/src/main/java/smile/nlp/NGram.java
Patch:
@@ -22,7 +22,7 @@
 
 /**
  * An n-gram is a contiguous sequence of n words from a given sequence of text.
- * An n-gram of size 1 is referred to as a unigram; size 2 is a bigram;
+ * An n-gram of size 1 is referred to as an unigram; size 2 is a bigram;
  * size 3 is a trigram.
  *
  * @author Haifeng Li

File: nlp/src/main/java/smile/nlp/Trie.java
Patch:
@@ -41,7 +41,7 @@ public class Trie<K, V> {
     /**
      * The root node is specialized as a hash map for 
      * quick search. In case of Named Entities, the root
-     * node will contains a lot of children. A plain list
+     * node will contain a lot of children. A plain list
      * will be slow for search.
      */
     private final HashMap<K, Node> root;

File: nlp/src/main/java/smile/nlp/collocation/NGram.java
Patch:
@@ -24,7 +24,7 @@
 
 /**
  * An n-gram is a contiguous sequence of n words from a given sequence of text.
- * An n-gram of size 1 is referred to as a unigram; size 2 is a bigram;
+ * An n-gram of size 1 is referred to as an unigram; size 2 is a bigram;
  * size 3 is a trigram.
  *
  * @author Haifeng Li

File: nlp/src/main/java/smile/nlp/dictionary/StopWords.java
Patch:
@@ -21,7 +21,7 @@
  * A set of stop words in some language. Stop words is the name given to word
  * s which are filtered out prior to, or after, processing of natural language
  * text. There is no definite list of stop words which all NLP
- * tools incorporate. Not all NLP tools use a stoplist. Some
+ * tools incorporate. Not all NLP tools use a stop list. Some
  * tools specifically avoid using them to support phrase search.
  * <p>
  * Stop words can cause problems when using a search engine to search for

File: nlp/src/main/java/smile/nlp/relevance/BM25.java
Patch:
@@ -30,7 +30,7 @@
  * <p>
  * At the extreme values of the coefficient b, BM25 turns into ranking functions
  * known as BM11 (for b = 1) and BM15 (for b = 0). BM25F is a modification of
- * BM25 in which the document is considered to be composed from several fields
+ * BM25 in which the document is considered to be composed of several fields
  * (such as headlines, main text, anchor text) with possibly different degrees
  * of importance.
  * <p>

File: nlp/src/main/java/smile/nlp/tokenizer/PennTreebankTokenizer.java
Patch:
@@ -40,7 +40,7 @@
  * This tokenizer assumes that the text has already been segmented into
  * sentences. Any periods -- apart from those at the end of a string or before
  * newline -- are assumed to be part of the word they are attached to (e.g. for
- * abbreviations, etc), and are not separately tokenized.
+ * abbreviations, etc.), and are not separately tokenized.
  *
  * @author Haifeng Li
  */

File: nlp/src/main/java/smile/nlp/tokenizer/SimpleParagraphSplitter.java
Patch:
@@ -43,7 +43,7 @@
  */
 public class SimpleParagraphSplitter implements ParagraphSplitter {
     /**
-     * Remove whitespaces in an blank line. Note to turn multiline mode.
+     * Remove whitespaces in a blank line. Note to turn multiline mode.
      */
     private static final Pattern REGEX_BLANK_LINE = Pattern.compile("(?m)^\\s+$");
     /**

File: nlp/src/main/java/smile/nlp/tokenizer/SimpleSentenceSplitter.java
Patch:
@@ -32,7 +32,7 @@
  * Recognizing the end of a sentence is not an easy task for a computer.
  * In English, punctuation marks that usually appear at the end of a sentence
  * may not indicate the end of a sentence. The period is the worst offender.
- * A period can end a sentence but it can also be part of an abbreviation
+ * A period can end a sentence, but it can also be part of an abbreviation
  * or acronym, an ellipsis, a decimal number, or part of a bracket of periods
  * surrounding a Roman numeral. A period can even act both as the end of an
  * abbreviation and the end of a sentence at the same time. Other the other
@@ -124,7 +124,7 @@ public String[] split(String text) {
         // $2 = the punctuation mark - either [.!?:]
         // $3 = the brackets or quotes after the [!?.:]. This is non-grouping i.e. does not consume.
         // $4 = the next word after the [.?!:].This is non-grouping i.e. does not consume.
-        // $5 = rather than a next word, it may have been the last sentence in the file. Therefore capture
+        // $5 = rather than a next word, it may have been the last sentence in the file. Therefore, capture
         //      punctuation and brackets before end of file. This is non-grouping i.e. does not consume.
         Matcher matcher = REGEX_SENTENCE.matcher(text);
         StringBuilder currentSentence = new StringBuilder();
@@ -177,7 +177,7 @@ public String[] split(String text) {
                         || EnglishAbbreviations.contains(lastWord.toLowerCase())) {
 
                     // We have an abbreviation, but this could come at the middle or end of a
-                    // sentence. Therefore we assume that the abbreviation is not at the end of
+                    // sentence. Therefore, we assume that the abbreviation is not at the end of
                     // a sentence if the next word is a common word and the abbreviation occurs
                     // less than 5 words from the start of the sentence.
                     if (EnglishDictionary.CONCISE.contains(nextWord) && len > 6) {

File: deep/src/main/java/smile/deep/Optimizer.java
Patch:
@@ -47,11 +47,9 @@ public void step() {
      * @param rate the learning rate.
      */
     public void setLearningRate(double rate) {
-        var options = new OptimizerOptions();
-        options.set_lr(rate);
         var groups = optimizer.param_groups();
         for (int i = 0; i < groups.size(); i++) {
-            groups.get(i).set_options(options);
+            groups.get(i).options().set_lr(rate);
         }
     }
 

File: deep/src/main/java/smile/vision/transform/ImageClassification.java
Patch:
@@ -65,7 +65,7 @@ public Tensor forward(BufferedImage... images) {
         BufferedImage[] output = new BufferedImage[images.length];
         for (int i = 0; i < images.length; i++) {
             BufferedImage image = resize(images[i], resizeSize, hints);
-            output[i] = crop(image, cropSize, cropSize, true);
+            output[i] = crop(image, cropSize, true);
         }
         return toTensor(mean, std, output);
     }

File: deep/src/main/java/smile/vision/transform/ImageClassification.java
Patch:
@@ -29,8 +29,6 @@
  * @author Haifeng Li
  */
 class ImageClassification implements Transform {
-    private static float[] DEFAULT_MEAN = {0.485f, 0.456f, 0.406f};
-    private static float[] DEFAULT_STD = {0.229f, 0.224f, 0.225f};
     private final int cropSize;
     private final int resizeSize;
     private final float[] mean;

File: deep/src/main/java/smile/deep/layer/Layer.java
Patch:
@@ -232,7 +232,7 @@ static FullyConnectedLayer hardShrink(int in, int out) {
      * @return a convolutional layer.
      */
     static Conv2dLayer conv2d(int in, int out, int kernel) {
-        return new Conv2dLayer(in, out, kernel, 1, "same", 1, 1, true, "zeros");
+        return new Conv2dLayer(in, out, kernel, 1, 0, 1, 1, true, "zeros");
     }
 
     /**

File: base/src/main/java/smile/io/Arff.java
Patch:
@@ -411,7 +411,7 @@ public DataFrame read() throws IOException, ParseException {
     /**
      * Reads a limited number of records.
      *
-     * @param limit the number number of records to read.
+     * @param limit the number of records to read.
      * @throws IOException when fails to read the file.
      * @throws ParseException when fails to parse the file.
      * @return the data frame.

File: base/src/main/java/smile/io/Arrow.java
Patch:
@@ -129,7 +129,7 @@ public DataFrame read(Path path) throws IOException {
      * Reads an arrow file.
      *
      * @param path the input file path.
-     * @param limit the number number of records to read.
+     * @param limit the number of records to read.
      * @throws IOException when fails to read the file.
      * @return the data frame.
      */
@@ -153,7 +153,7 @@ public DataFrame read(String path) throws IOException, URISyntaxException {
      * Reads a limited number of records from an arrow file.
      *
      * @param path the input file path.
-     * @param limit the number number of records to read.
+     * @param limit the number of records to read.
      * @throws IOException when fails to read the file.
      * @throws URISyntaxException when the file path syntax is wrong.
      * @return the data frame.
@@ -166,7 +166,7 @@ public DataFrame read(String path, int limit) throws IOException, URISyntaxExcep
      * Reads a limited number of records from an arrow file.
      *
      * @param input the input stream.
-     * @param limit the number number of records to read.
+     * @param limit the number of records to read.
      * @throws IOException when fails to read the file.
      * @return the data frame.
      */

File: base/src/main/java/smile/io/Avro.java
Patch:
@@ -116,7 +116,7 @@ public DataFrame read(String path) throws IOException, URISyntaxException {
      * Reads a limited number of records from an avro file.
      *
      * @param input the input stream of data file.
-     * @param limit the number number of records to read.
+     * @param limit the number of records to read.
      * @throws IOException when fails to read the file.
      * @return the data frame.
      */

File: base/src/main/java/smile/io/CSV.java
Patch:
@@ -105,7 +105,7 @@ public DataFrame read(String path) throws IOException, URISyntaxException {
     /**
      * Reads a limited number of records from a CSV file.
      * @param path the input file path.
-     * @param limit the number number of records to read.
+     * @param limit the number of records to read.
      * @throws IOException when fails to read the file.
      * @throws URISyntaxException when the file path syntax is wrong.
      * @return the data frame.
@@ -132,7 +132,7 @@ public DataFrame read(Path path) throws IOException {
     /**
      * Reads a limited number of records from a CSV file.
      * @param path the input file path.
-     * @param limit the number number of records to read.
+     * @param limit the number of records to read.
      * @throws IOException when fails to read the file.
      * @return the data frame.
      */

File: base/src/main/java/smile/io/SAS.java
Patch:
@@ -70,7 +70,7 @@ static DataFrame read(String path) throws IOException, URISyntaxException {
      * Reads a limited number of records from a SAS7BDAT file.
      *
      * @param input a SAS7BDAT file input stream.
-     * @param limit the number number of records to read.
+     * @param limit the number of records to read.
      * @throws IOException when fails to write the file.
      * @return the data frame.
      */

File: deep/src/main/java/smile/vision/FusedMBConv.java
Patch:
@@ -42,7 +42,7 @@ public class FusedMBConv implements Layer {
      */
     public FusedMBConv(MBConvConfig config, double stochasticDepthProb) {
         int stride = config.stride();
-        if (stride >= 1 && stride <= 2) {
+        if (stride < 1 || stride > 2) {
             throw new IllegalArgumentException("Illegal stride value: " + stride);
         }
 

File: deep/src/main/java/smile/vision/MBConv.java
Patch:
@@ -46,7 +46,7 @@ public class MBConv implements Layer {
      */
     public MBConv(MBConvConfig config, double stochasticDepthProb) {
         int stride = config.stride();
-        if (stride >= 1 && stride <= 2) {
+        if (stride < 1 || stride > 2) {
             throw new IllegalArgumentException("Illegal stride value: " + stride);
         }
 

File: deep/src/main/java/smile/deep/Model.java
Patch:
@@ -127,6 +127,7 @@ public void train(int epochs, Optimizer optimizer, Loss loss, Dataset train) {
      * @param train the training data.
      * @param eval optional evaluation data.
      * @param checkpoint optional checkpoint file path.
+     * @param metrics the evaluation metrics.
      */
     public void train(int epochs, Optimizer optimizer, Loss loss, Dataset train, Dataset eval, String checkpoint, Metric... metrics) {
         train(); // training mode
@@ -184,6 +185,7 @@ public void train(int epochs, Optimizer optimizer, Loss loss, Dataset train, Dat
     /**
      * Evaluates the model accuracy on a test dataset.
      * @param dataset the test dataset.
+     * @param metrics the evaluation metrics.
      * @return the accuracy.
      */
     public Map<String, Double> eval(Dataset dataset, Metric... metrics) {

File: deep/src/main/java/smile/deep/tensor/DeviceType.java
Patch:
@@ -24,6 +24,7 @@
  * @author Haifeng Li
  */
 public enum DeviceType {
+    /** CPU */
     CPU(torch.DeviceType.CPU),
     /** NVIDIA GPU */
     CUDA(torch.DeviceType.CUDA),
@@ -41,6 +42,7 @@ public enum DeviceType {
     /**
      * Returns the byte value of device type,
      * which is compatible with PyTorch.
+     * @return the byte value of device type.
      */
     public byte value() {
         return value.value;

File: base/src/main/java/smile/util/Strings.java
Patch:
@@ -131,7 +131,7 @@ static String htmlEscape(String input, String encoding) {
         StringBuilder escaped = new StringBuilder(input.length() * 2);
         for (int i = 0; i < input.length(); i++) {
             char character = input.charAt(i);
-            String reference = html.escape(character);
+            String reference = html.escape(character, encoding);
             if (reference != null) {
                 escaped.append(reference);
             } else {

File: plot/src/main/java/smile/plot/vega/Mark.java
Patch:
@@ -89,7 +89,7 @@ public Mark description(String description) {
      * @return this object.
      */
     public Mark style(String... style) {
-        if (style.length == 0) {
+        if (style.length == 1) {
             spec.put("style", style[0]);
         } else {
             ArrayNode node = spec.putArray("style");

File: plot/src/main/java/smile/plot/vega/Config.java
Patch:
@@ -19,7 +19,7 @@
 import com.fasterxml.jackson.databind.node.ObjectNode;
 
 /**
- * Vega-Lite’s config object lists configuration properties of
+ * Vega-Lite's config object lists configuration properties of
  * a visualization for creating a consistent theme.
  *
  * @author Haifeng Li

File: plot/src/main/java/smile/plot/vega/Projection.java
Patch:
@@ -47,7 +47,7 @@ public class Projection {
     }
 
     /**
-     * Sets the projection’s center, a two-element array of longitude and latitude in degrees.
+     * Sets the projection's center, a two-element array of longitude and latitude in degrees.
      * @param longitude longitude in degrees.
      * @param latitude latitude in degrees.
      * @return this object.
@@ -60,7 +60,7 @@ public Projection center(double longitude, double latitude) {
     }
 
     /**
-     * Sets the projection’s clipping circle radius to the specified angle
+     * Sets the projection's clipping circle radius to the specified angle
      * in degrees.
      * @param angle The clip angle in degrees.
      * @return this object.

File: core/src/main/java/smile/classification/DiscreteNaiveBayes.java
Patch:
@@ -119,7 +119,7 @@ public enum Model {
         /**
          * Complement Naive Bayes.
          * To deal with skewed training data, CNB estimates parameters
-         * of a class c using data from all classes except c. CNB’s estimates
+         * of a class c using data from all classes except c. CNB's estimates
          * may be more effective because each uses a more even amount of
          * training data per class, which will lessen the bias in the weight
          * estimates.
@@ -129,7 +129,7 @@ public enum Model {
         /**
          * Weight-normalized Complement Naive Bayes.
          * In practice, it is often the case that weights tend to lean toward
-         * one class or the other. When the magnitude of Naive Bayes’ weight
+         * one class or the other. When the magnitude of Naive Bayes' weight
          * vector is larger in one class than the others, the larger magnitude
          * class may be preferred. To correct for the fact that some classes
          * have greater dependencies, WCNB normalizes the weight vectors.

File: core/src/main/java/smile/feature/selection/InformationValue.java
Patch:
@@ -73,7 +73,7 @@
  * <p>
  * On arranging a numerical feature in ascending order, if the WoE
  * values are all linear, we know that the feature has the right
- * linear relation with the target. However, if the feature’s WoE
+ * linear relation with the target. However, if the feature's WoE
  * is non-linear, we should either discard it or consider some other
  * variable transformation to ensure the linearity. Hence, WoE helps
  * check the linear relationship of a feature with its dependent variable

File: core/src/main/java/smile/regression/RidgeRegression.java
Patch:
@@ -62,7 +62,7 @@
  * the intercept estimate ends up just being the mean of <code>y</code>.
  * <p>
  * Ridge regression does not set coefficients exactly to zero unless
- * <code>&lambda; = &infin;</code>, in which case they’re all zero.
+ * <code>&lambda; = &infin;</code>, in which case they're all zero.
  * Hence ridge regression cannot perform variable selection, and
  * even though it performs well in terms of prediction accuracy,
  * it does poorly in terms of offering a clear interpretation.

File: core/src/main/java/smile/timeseries/AR.java
Patch:
@@ -49,7 +49,7 @@
  * and the second approach is to use some information criteria.
  * <p>
  * Autoregression is a good start point for more complicated models.
- * They often fit quite well (don’t need the MA terms).
+ * They often fit quite well (don't need the MA terms).
  * And the fitting process is fast (MLEs require some iterations).
  * In applications, easily fitting autoregressions is important
  * for obtaining initial values of parameters and in getting estimates of

File: deep/src/test/java/smile/deep/ModelTest.java
Patch:
@@ -65,7 +65,8 @@ org.bytedeco.pytorch.Tensor forward(org.bytedeco.pytorch.Tensor x) {
     public static void setUpClass() throws Exception {
         System.out.format("CUDA available: %s\n", cuda_is_available());
         System.out.format("CUDA device count: %d\n", cuda_device_count());
-        //device(new Device(cuda_is_available() ? "cuda" : "cpu"));
+        Device device = cuda_is_available() ? DeviceType.CUDA.device() : DeviceType.CPU.device();
+        device.setDefaultDevice();
 
         // try to use MKL when available
         System.setProperty("org.bytedeco.openblas.load", "mkl");

File: core/src/main/java/smile/base/mlp/HiddenLayer.java
Patch:
@@ -32,10 +32,11 @@ public class HiddenLayer extends Layer {
      * Constructor.
      * @param n the number of neurons.
      * @param p the number of input variables (not including bias value).
+     * @param dropout the dropout rate.
      * @param activation the activation function.
      */
-    public HiddenLayer(int n, int p, ActivationFunction activation) {
-        super(n, p);
+    public HiddenLayer(int n, int p, double dropout, ActivationFunction activation) {
+        super(n, p, dropout);
         this.activation = activation;
     }
 

File: core/src/main/java/smile/base/mlp/HiddenLayerBuilder.java
Patch:
@@ -45,6 +45,6 @@ public String toString() {
 
     @Override
     public HiddenLayer build(int p) {
-        return new HiddenLayer(neurons, p, activation);
+        return new HiddenLayer(neurons, p, dropout, activation);
     }
 }

File: core/src/test/java/smile/classification/MLPTest.java
Patch:
@@ -225,7 +225,7 @@ public void testUSPS() throws Exception {
         int k = MathEx.max(USPS.y) + 1;
 
         MLP model = new MLP(Layer.input(p),
-                Layer.leaky(768, 0.5, 0.02),
+                Layer.leaky(768, 0.2, 0.02),
                 Layer.rectifier(192),
                 Layer.rectifier(30),
                 Layer.mle(k, OutputFunction.SOFTMAX)
@@ -246,7 +246,7 @@ public void testUSPS() throws Exception {
             System.out.println("Test Error = " + error);
         }
 
-        assertEquals(109, error);
+        assertEquals(115, error, 5);
 
         java.nio.file.Path temp = Write.object(model);
         Read.object(temp);
@@ -281,7 +281,7 @@ public void testUSPSMiniBatch() {
         double[][] batchx = new double[batch][];
         int[] batchy = new int[batch];
         int error = 0;
-        for (int epoch = 1; epoch <= 10; epoch++) {
+        for (int epoch = 1; epoch <= 8; epoch++) {
             System.out.format("----- epoch %d -----%n", epoch);
             int[] permutation = MathEx.permutate(x.length);
             int i = 0;

File: plot/src/main/java/smile/plot/swing/BarPlot.java
Patch:
@@ -66,10 +66,10 @@ public double[] getLowerBound() {
 
         for (int k = 1; k < bars.length; k++) {
             for (double[] x : bars[k].data) {
-                if (bound[0] > x[0] - bars[k].width / 2) {
+                if (bound[0] < x[0] - bars[k].width / 2) {
                     bound[0] = x[0] - bars[k].width / 2;
                 }
-                if (bound[1] > x[1]) {
+                if (bound[1] < x[1]) {
                     bound[1] = x[1];
                 }
             }

File: base/src/main/java/smile/io/Arff.java
Patch:
@@ -555,6 +555,8 @@ else if (field.type.isIntegral()) {
             } else {
                 writer.println(" REAL");
             }
+        } else {
+            writer.println(" STRING");
         }
     }
 }

File: core/src/main/java/smile/validation/metric/MatthewsCorrelation.java
Patch:
@@ -55,8 +55,8 @@ public static double of(int[] truth, int[] prediction) {
             throw new IllegalArgumentException("MCC can only be applied to binary classification: " + confusion);
         }
 
-        int tp = matrix[0][0];
-        int tn = matrix[1][1];
+        int tp = matrix[1][1];
+        int tn = matrix[0][0];
         int fp = matrix[0][1];
         int fn = matrix[1][0];
 

File: core/src/main/java/smile/classification/GradientTreeBoost.java
Patch:
@@ -629,8 +629,9 @@ public double[] shap(Tuple x) {
             ntrees = trees.length;
             for (RegressionTree tree : trees) {
                 double[] phii = tree.shap(xt);
-                for (int i = 0; i < phi.length; i++) {
-                    phi[i] += phii[i];
+                for (int i = 0; i < p; i++) {
+                    phi[2*i] += phii[i];
+                    phi[2*i+1] += phii[i];
                 }
             }
         } else {

File: core/src/main/java/smile/feature/extraction/BagOfWords.java
Patch:
@@ -108,6 +108,7 @@ public BagOfWords(String[] columns, Function<String, String[]> tokenizer, String
 
     /**
      * Returns the feature words.
+     * @return the feature words.
      */
     public String[] features() {
         return words;
@@ -116,6 +117,8 @@ public String[] features() {
     /**
      * Learns a vocabulary dictionary of top-k frequent tokens in the raw documents.
      * @param data training data.
+     * @param tokenizer the tokenizer of text, which may include additional processing
+     *                  such as filtering stop word, converting to lowercase, stemming, etc.
      * @param k the limit of vocabulary size.
      * @param columns the text columns.
      * @return the model.

File: core/src/main/java/smile/feature/selection/InformationValue.java
Patch:
@@ -151,6 +151,7 @@ public static String toString(InformationValue[] iv) {
 
     /**
      * Returns the data transformation that covert feature value to its weight of evidence.
+     * @param values the information value objects of features.
      * @return the transform.
      */
     public static ColumnTransform toTransform(InformationValue[] values) {

File: base/src/main/java/smile/io/Read.java
Patch:
@@ -47,6 +47,8 @@ public interface Read {
      * Reads a serialized object from a file.
      * @param path the file path.
      * @return the serialized object.
+     * @throws IOException when fails to read the stream.
+     * @throws ClassNotFoundException when fails to load the class.
      */
     static Object object(Path path) throws IOException, ClassNotFoundException {
         InputStream file = Files.newInputStream(path);

File: base/src/main/java/smile/io/Write.java
Patch:
@@ -37,6 +37,7 @@ public interface Write {
      * The temporary file will be deleted when the VM exits.
      * @param o the object to serialize.
      * @return the path of temporary file.
+     * @throws IOException when fails to write the stream.
      */
     static Path object(Serializable o) throws IOException {
         Path temp = Files.createTempFile("smile-test-", ".tmp");
@@ -49,6 +50,7 @@ static Path object(Serializable o) throws IOException {
      * Writes a serializable object to a file.
      * @param o the object to serialize.
      * @param path the file path.
+     * @throws IOException when fails to write the stream.
      */
     static void object(Serializable o, Path path) throws IOException {
         OutputStream file = Files.newOutputStream(path);

File: base/src/main/java/smile/math/matrix/BigMatrix.java
Patch:
@@ -1659,7 +1659,7 @@ public BigMatrix aat() {
      * Returns {@code A * D * B}, where D is a diagonal matrix.
      * @param transA normal, transpose, or conjugate transpose
      *               operation on the matrix A.
-     * @param B the operand.
+     * @param A the operand.
      * @param D the diagonal matrix.
      * @param transB normal, transpose, or conjugate transpose
      *               operation on the matrix B.

File: base/src/main/java/smile/math/matrix/Matrix.java
Patch:
@@ -1585,7 +1585,7 @@ public Matrix aat() {
      * Returns {@code A * D * B}, where D is a diagonal matrix.
      * @param transA normal, transpose, or conjugate transpose
      *               operation on the matrix A.
-     * @param B the operand.
+     * @param A the operand.
      * @param D the diagonal matrix.
      * @param transB normal, transpose, or conjugate transpose
      *               operation on the matrix B.

File: base/src/main/java/smile/math/matrix/fp32/Matrix.java
Patch:
@@ -1656,7 +1656,7 @@ public Matrix aat() {
      * Returns {@code A * D * B}, where D is a diagonal matrix.
      * @param transA normal, transpose, or conjugate transpose
      *               operation on the matrix A.
-     * @param B the operand.
+     * @param A the operand.
      * @param D the diagonal matrix.
      * @param transB normal, transpose, or conjugate transpose
      *               operation on the matrix B.

File: base/src/main/java/smile/neighbor/KDTree.java
Patch:
@@ -139,6 +139,7 @@ public KDTree(double[][] key, E[] data) {
     /**
      * Return a KD-tree of the data.
      * @param data the data objects, which are also used as key.
+     * @return KD-tree.
      */
     public static KDTree<double[]> of(double[][] data) {
         return new KDTree<>(data, data);

File: core/src/main/java/smile/deep/activation/LeakyReLU.java
Patch:
@@ -28,10 +28,12 @@
 public class LeakyReLU implements ActivationFunction {
     /** Default instance. */
     static LeakyReLU instance = new LeakyReLU(0.01);
-
+    /** The leaky parameter {@code 0 <= a < 1}. */
     private double a;
+
     /**
      * Constructor.
+     * @param a leaky parameter {@code 0 <= a < 1}.
      */
     public LeakyReLU(double a) {
         if (a < 0 || a >= 1.0) {

File: core/src/main/java/smile/feature/extraction/GHA.java
Patch:
@@ -180,7 +180,6 @@ public double update(Tuple x) {
     /**
      * Update the model with a set of samples.
      * @param data the centered learning samples whose E(x) = 0.
-     * @return the approximation error for input sample.
      */
     public void update(double[][] data) {
         for (double[] x : data) {

File: core/src/main/java/smile/feature/extraction/KernelPCA.java
Patch:
@@ -37,6 +37,9 @@
 public class KernelPCA extends Projection {
     private static final long serialVersionUID = 2L;
 
+    /**
+     * Kernel PCA.
+     */
     public final KPCA<double[]> kpca;
 
     /**

File: core/src/main/java/smile/feature/extraction/ProbabilisticPCA.java
Patch:
@@ -134,6 +134,7 @@ public static ProbabilisticPCA fit(DataFrame data, int k, String... columns) {
      * Fits probabilistic principal component analysis.
      * @param data training data of which each row is a sample.
      * @param k the number of principal component to learn.
+     * @param columns the columns to transform when applied on Tuple/DataFrame.
      * @return the model.
      */
     public static ProbabilisticPCA fit(double[][] data, int k, String... columns) {

File: core/src/main/java/smile/manifold/SammonMapping.java
Patch:
@@ -52,7 +52,7 @@
  * also provides a heuristic that is simple and works reasonably well.
  *
  * @see MDS
- * @see smile.projection.KPCA
+ * @see smile.manifold.KPCA
  *
  * @author Haifeng Li
  */

File: core/src/main/java/smile/feature/imputation/KNNImputer.java
Patch:
@@ -66,7 +66,7 @@ public Tuple apply(Tuple x) {
             @Override
             public Object get(int i) {
                 Object xi = x.get(i);
-                if (xi != null) {
+                if (!SimpleImputer.isMissing(xi)) {
                     return xi;
                 } else {
                     StructField field = schema.field(i);

File: base/src/main/java/smile/math/kernel/Matern.java
Patch:
@@ -106,7 +106,7 @@ public double k(double dist) {
 
         if (nu == 2.5) {
             d *= SQRT5;
-            return (1.0 + d) * Math.exp(-d);
+            return (1.0 + d + d*d/3) * Math.exp(-d);
         }
 
         if (nu == 0.5) {
@@ -131,8 +131,8 @@ public double[] kg(double dist) {
             g = (2.0 + d) * Math.exp(-d) * d / sigma;
         } else if (nu == 2.5) {
             d *= SQRT5;
-            k = (1.0 + d) * Math.exp(-d);
-            g = (2.0 + d) * Math.exp(-d) * d / sigma;
+            k = (1.0 + d + d*d/3) * Math.exp(-d);
+            g = (2.0 + 5*d/3 + d*d/3) * Math.exp(-d) * d / sigma;
         } else if (nu == 0.5) {
             k = Math.exp(-d);
             g = k * d / sigma;

File: base/src/main/java/smile/util/Paths.java
Patch:
@@ -18,6 +18,7 @@
 package smile.util;
 
 import java.io.BufferedReader;
+import java.io.File;
 import java.io.IOException;
 import java.nio.file.Path;
 import java.util.stream.Stream;
@@ -37,7 +38,7 @@ public interface Paths {
      * @return the file path to the test data.
      */
     static Path getTestData(String... path) {
-        return java.nio.file.Paths.get(home + "/data", path);
+        return java.nio.file.Paths.get(home + File.separator + "data", path);
     }
 
     /**

File: base/src/main/java/smile/util/Paths.java
Patch:
@@ -29,15 +29,15 @@
  */
 public interface Paths {
     /** Smile home directory. */
-    String home = System.getProperty("smile.home", "shell/src/universal/");
+    String home = System.getProperty("smile.home", "shell/src/universal/data");
 
     /**
      * Get the file path of a test sample dataset.
      * @param path the path strings to be joined to form the path.
      * @return the file path to the test data.
      */
     static Path getTestData(String... path) {
-        return java.nio.file.Paths.get(home + "/data", path);
+        return java.nio.file.Paths.get(home, path);
     }
 
     /**

File: core/src/main/java/smile/imputation/KNNImputation.java
Patch:
@@ -24,7 +24,7 @@
  * selects instances similar to the instance of interest to impute
  * missing values. If we consider instance A that has one missing value on
  * attribute i, this method would find K other instances, which have a value
- * present on attribute 1, with values most similar (in term of some distance,
+ * present on attribute 1, with values most similar (in terms of some distance,
  * e.g. Euclidean distance) to A on other attributes without missing values.
  * The average of values on attribute i from the K nearest
  * neighbors is then used as an estimate for the missing value in instance A.

File: io/src/main/java/smile/io/CSV.java
Patch:
@@ -182,7 +182,7 @@ private DataFrame read(Reader reader, int limit) throws IOException {
      * </ol>
      *
      * @param reader the file reader.
-     * @param limit the number number of records to read.
+     * @param limit the number of records to read.
      * @throws IOException when fails to read the file.
      * @return the data frame.
      */

File: core/src/main/java/smile/manifold/TSNE.java
Patch:
@@ -274,7 +274,7 @@ private void sne(int i, double[] dY, double[] dC) {
         }
     }
 
-    /** Compute the Gaussian kernel (search the width for given perplexity. */
+    /** Compute the Gaussian kernel (search the width for given perplexity). */
     private double[][] expd(double[][] D, double perplexity, double tol) {
         int n          = D.length;
         double[][] P   = new double[n][n];

File: core/src/main/java/smile/regression/LinearModel.java
Patch:
@@ -226,7 +226,7 @@ public double[][] ttest() {
      * @return the linear coefficients without intercept.
      */
     public double[] coefficients() {
-        return bias ? Arrays.copyOfRange(w, 1, w.length - 1) : w;
+        return bias ? Arrays.copyOfRange(w, 1, w.length) : w;
     }
 
     /**

File: math/src/main/java/smile/math/BFGS.java
Patch:
@@ -694,7 +694,7 @@ public static double minimize(DifferentiableMultivariateFunction func, int m, do
                 sHistory.add(s);
 
                 int h = yHistory.size();
-                if (iter <= m) {
+                if (Y == null || Y.ncol() < h) {
                     Y = new Matrix(n, h);
                     S = new Matrix(n, h);
                     W = new Matrix(n, 2*h);

File: core/src/main/java/smile/neighbor/CoverTree.java
Patch:
@@ -556,12 +556,12 @@ public Neighbor<E, E>[] knn(E q, int k) {
 
         Arrays.sort(neighbors);
 
-        MathEx.reverse(neighbors);
-
         if (neighbors.length > k) {
             neighbors = Arrays.copyOf(neighbors, k);
         }
 
+        MathEx.reverse(neighbors);
+
         return neighbors;
     }
 

File: math/src/main/java/smile/math/DifferentiableMultivariateFunction.java
Patch:
@@ -40,8 +40,9 @@ default double g(double[] x, double[] gradient) {
         double fx = f(x);
 
         int n = x.length;
-        double[] xh = x.clone();
+        double[] xh = new double[n];
         for (int i = 0; i < n; i++) {
+            System.arraycopy(x, 0, xh, 0, n);
             double xi = x[i];
             double h = EPSILON * Math.abs(xi);
             if (h == 0.0) {

File: math/src/main/java/smile/math/DifferentiableMultivariateFunction.java
Patch:
@@ -40,7 +40,7 @@ default double g(double[] x, double[] gradient) {
         double fx = f(x);
 
         int n = x.length;
-        double[] xh = new double[n];
+        double[] xh = x.clone();
         for (int i = 0; i < n; i++) {
             double xi = x[i];
             double h = EPSILON * Math.abs(xi);

File: math/src/main/java/smile/math/DifferentiableMultivariateFunction.java
Patch:
@@ -40,7 +40,7 @@ default double g(double[] x, double[] gradient) {
         double fx = f(x);
 
         int n = x.length;
-        double[] xh = new double[n];
+        double[] xh = x.clone();
         for (int i = 0; i < n; i++) {
             double xi = x[i];
             double h = EPSILON * Math.abs(xi);

File: core/src/main/java/smile/regression/RBFNetwork.java
Patch:
@@ -69,7 +69,7 @@
  * </ol>
  * 
  * @see RadialBasisFunction
- * @see SVR
+ * @see SVM
  *
  * @param <T> the data type of samples.
  *

File: core/src/main/java/smile/classification/MLP.java
Patch:
@@ -262,7 +262,7 @@ public static MLP fit(double[][] x, int[] y, Properties params) {
         model.setParameters(params);
 
         int epochs = Integer.parseInt(params.getProperty("smile.mlp.epochs", "100"));
-        int batch = Integer.parseInt(params.getProperty("smile.mlp.mini_batch", "256"));
+        int batch = Integer.parseInt(params.getProperty("smile.mlp.mini_batch", "32"));
         double[][] batchx = new double[batch][];
         int[] batchy = new int[batch];
         for (int epoch = 1; epoch <= epochs; epoch++) {

File: core/src/main/java/smile/regression/MLP.java
Patch:
@@ -142,7 +142,7 @@ public static MLP fit(double[][] x, double[] y, Properties params) {
         model.setParameters(params);
 
         int epochs = Integer.parseInt(params.getProperty("smile.mlp.epochs", "100"));
-        int batch = Integer.parseInt(params.getProperty("smile.mlp.mini_batch", "256"));
+        int batch = Integer.parseInt(params.getProperty("smile.mlp.mini_batch", "32"));
         double[][] batchx = new double[batch][];
         double[] batchy = new double[batch];
         for (int epoch = 1; epoch <= epochs; epoch++) {

File: mkl/src/test/java/smile/math/matrix/FloatMatrixTest.java
Patch:
@@ -792,7 +792,7 @@ public void testSVD5() {
         };
 
         FloatMatrix.SVD svd = new FloatMatrix(A).svd();
-        assertArrayEquals(s, svd.s, 1E-6f);
+        assertArrayEquals(s, svd.s, 1E-5f);
 
         assertEquals(U.length, svd.U.nrow());
         assertEquals(U[0].length, svd.U.ncol());

File: core/src/main/java/smile/classification/SVM.java
Patch:
@@ -211,7 +211,7 @@ public static Classifier<double[]> fit(double[][] x, int[] y, Properties params)
         double tol = Double.parseDouble(params.getProperty("smile.svm.tolerance", "1E-3"));
 
         int[] classes = MathEx.unique(y);
-        String trainer = params.getProperty("smile.svm.strategy", classes.length == 2 ? "binary" : "ovr").toLowerCase();
+        String trainer = params.getProperty("smile.svm.type", classes.length == 2 ? "binary" : "ovr").toLowerCase();
         switch (trainer) {
             case "ovr":
                 if (kernel instanceof LinearKernel) {

File: core/src/test/java/smile/classification/SVMTest.java
Patch:
@@ -149,7 +149,7 @@ public void testAdult() throws IOException {
     public void testSegment() {
         System.out.println("Segment");
 
-        MathEx.setSeed(19650217); // to get repeatable results.
+        MathEx.setSeed(19650218); // to get repeatable results.
 
         Standardizer scaler = Standardizer.fit(Segment.x);
         double[][] x = scaler.transform(Segment.x);

File: core/src/main/java/smile/vq/SOM.java
Patch:
@@ -141,7 +141,7 @@ public Neuron(int i, int j, double[] w) {
      */
     private final Neighborhood theta;
     /**
-     * The threshold to update neuron if alpha * theta > eps.
+     * The threshold to update neuron if {@code alpha * theta > eps}.
      */
     private final double tol = 1E-5;
     /**

File: math/src/main/java/smile/math/Scaler.java
Patch:
@@ -51,6 +51,7 @@ public class Scaler implements Function {
      * Constructor.
      * @param scale the scaling factor.
      * @param offset the offset.
+     * @param clip if true, clip the value in [0, 1].
      */
     public Scaler(double scale, double offset, boolean clip) {
         this.scale = MathEx.isZero(scale) ? 1.0 : scale;
@@ -180,6 +181,6 @@ public static Scaler of(String scaler, double[] data) {
             return Scaler.standardizer(data, robust);
         }
 
-        throw new IllegalArgumentException("Invalid scaler: " + scaler);
+        throw new IllegalArgumentException("Unsupported scaler: " + scaler);
     }
 }

File: core/src/main/java/smile/classification/Classifier.java
Patch:
@@ -40,8 +40,8 @@
  * and a desired output value. The inferred function is called a classifier
  * if the output is discrete or a regression function if the output is
  * continuous.
- * 
- * @param <T> the type of input object
+ *
+ * @param <T> the type of model input object.
  * 
  * @author Haifeng Li
  */
@@ -254,6 +254,7 @@ default void update(Dataset<Instance<T>> batch) {
      * predictive performance.
      *
      * @param models the base models.
+     * @param <T> the type of model input object.
      * @return the ensemble model.
      */
     @SafeVarargs

File: core/src/test/java/smile/regression/MLPTest.java
Patch:
@@ -86,7 +86,6 @@ public void test(String dataset, double[][] x, double[] y, Scaler scaler, double
 
         RegressionValidations<MLP> result = CrossValidation.regression(10, x, y, (xi, yi) -> {
             MLP model = new MLP(scaler, builders);
-            System.out.println(model);
             // small learning rate and weight decay to counter exploding gradient
             model.setLearningRate(TimeFunction.linear(0.01, 10000, 0.001));
             model.setWeightDecay(0.1);

File: core/src/main/java/smile/regression/Ensemble.java
Patch:
@@ -33,7 +33,6 @@ public class Ensemble<T> implements Regression<T> {
     /**
      * Returns an ensemble model.
      * @param models the base models.
-     * @return the ensemble model.
      */
     @SafeVarargs
     public Ensemble(Regression<T>... models) {

File: core/src/main/java/smile/regression/LinearModel.java
Patch:
@@ -395,6 +395,8 @@ public boolean online() {
      * Growing window recursive least squares with lambda = 1.
      * RLS updates an ordinary least squares with samples that
      * arrive sequentially.
+     * @param x training instance.
+     * @param y response variable.
      */
     public void update(double[] x, double y) {
         update(x, y, 1.0);

File: core/src/main/java/smile/validation/RegressionMetrics.java
Patch:
@@ -87,6 +87,7 @@ public String toString() {
      * @param scoreTime the time in milliseconds of scoring the validation data.
      * @param truth the ground truth.
      * @param prediction the predictions.
+     * @return the validation metrics.
      */
     public static RegressionMetrics of(double fitTime, double scoreTime, double[] truth, double[] prediction) {
         return new RegressionMetrics(

File: math/src/main/java/smile/math/MathEx.java
Patch:
@@ -1215,6 +1215,7 @@ public static <T> void reverse(T[] a) {
      * If there are multiple modes, one of them will be returned.
      *
      * @param a the array. The order of elements will be changed on output.
+     * @return the mode.
      */
     public static int mode(int[] a) {
         Arrays.sort(a);

File: math/src/main/java/smile/stat/distribution/MultivariateGaussianDistribution.java
Patch:
@@ -356,7 +356,7 @@ public MultivariateMixture.Component M(double[][] data, double[] posteriori) {
                 variance[i] /= alpha;
             }
 
-            gaussian = new MultivariateGaussianDistribution(mean, new Matrix(variance));
+            gaussian = new MultivariateGaussianDistribution(mean, variance);
         } else {
             Matrix cov = new Matrix(d, d);
             for (int k = 0; k < n; k++) {

File: nlp/src/main/java/smile/nlp/pos/HMMPOSTagger.java
Patch:
@@ -406,7 +406,7 @@ public static void walkin(File dir, List<File> files) {
     
     /**
      * Train the default model on WSJ and BROWN datasets.
-     * @param argv the command line arguments.
+     * @param args the command line arguments.
      */
     public static void main(String[] args) {
         List<String[]> sentences = new ArrayList<>();

File: core/src/test/java/smile/regression/GradientTreeBoostTest.java
Patch:
@@ -69,7 +69,7 @@ public void testLongley() throws Exception {
         double[] importance = model.importance();
         System.out.println("----- importance -----");
         for (int i = 0; i < importance.length; i++) {
-            System.out.format("%-15s %12.4f%n", model.schema().fieldName(i), importance[i]);
+            System.out.format("%-15s %12.4f%n", model.schema().name(i), importance[i]);
         }
 
         System.out.println("----- Progressive RMSE -----");
@@ -96,7 +96,7 @@ public void test(Loss loss, String name, Formula formula, DataFrame data, double
         double[] importance = model.importance();
         System.out.println("----- importance -----");
         for (int i = 0; i < importance.length; i++) {
-            System.out.format("%-15s %12.4f%n", model.schema().fieldName(i), importance[i]);
+            System.out.format("%-15s %12.4f%n", model.schema().name(i), importance[i]);
         }
 
         RegressionValidations<GradientTreeBoost> result = CrossValidation.regression(10, formula, data,

File: core/src/test/java/smile/regression/RandomForestTest.java
Patch:
@@ -99,7 +99,7 @@ public void testLongley() throws Exception {
         double[] importance = model.importance();
         System.out.println("----- importance -----");
         for (int i = 0; i < importance.length; i++) {
-            System.out.format("%-15s %12.4f%n", model.schema().fieldName(i), importance[i]);
+            System.out.format("%-15s %12.4f%n", model.schema().name(i), importance[i]);
         }
 
         assertEquals(39293.8193, importance[0], 1E-4);
@@ -139,7 +139,7 @@ public void test(String name, Formula formula, DataFrame data, double expected)
         double[] importance = model.importance();
         System.out.println("----- importance -----");
         for (int i = 0; i < importance.length; i++) {
-            System.out.format("%-15s %12.4f%n", model.schema().fieldName(i), importance[i]);
+            System.out.format("%-15s %12.4f%n", model.schema().name(i), importance[i]);
         }
     }
 

File: core/src/test/java/smile/regression/RegressionTreeTest.java
Patch:
@@ -69,7 +69,7 @@ public void testLongley() throws Exception {
         double[] importance = model.importance();
         System.out.println("----- importance -----");
         for (int i = 0; i < importance.length; i++) {
-            System.out.format("%-15s %.4f%n", model.schema().fieldName(i), importance[i]);
+            System.out.format("%-15s %.4f%n", model.schema().name(i), importance[i]);
         }
 
         RegressionMetrics metrics = LOOCV.regression(Longley.formula, Longley.data, (formula, x) -> RegressionTree.fit(formula, x, 100, 20, 2));
@@ -93,7 +93,7 @@ public void test(String name, Formula formula, DataFrame data, double expected)
         double[] importance = model.importance();
         System.out.println("----- importance -----");
         for (int i = 0; i < importance.length; i++) {
-            System.out.format("%-15s %.4f%n", model.schema().fieldName(i), importance[i]);
+            System.out.format("%-15s %.4f%n", model.schema().name(i), importance[i]);
         }
 
         RegressionValidations<RegressionTree> result = CrossValidation.regression(10, formula, data, RegressionTree::fit);

File: data/src/main/java/smile/data/DataFrame.java
Patch:
@@ -1164,7 +1164,7 @@ default Matrix toMatrix(boolean bias, CategoricalEncoder encoder, String rowName
         Matrix matrix = new Matrix(nrow, colNames.size());
         matrix.colNames(colNames.toArray(new String[0]));
         if (rowNames != null) {
-            int j = schema.fieldIndex(rowNames);
+            int j = schema.indexOf(rowNames);
             String[] rows = new String[nrow];
             for (int i = 0; i < nrow; i++) {
                 rows[i] = getString(i, j);
@@ -1573,7 +1573,7 @@ static <T> DataFrame of(Collection<Map<String, T>> data, StructType schema) {
         List<Tuple> rows = data.stream().map(map -> {
             Object[] row = new Object[schema.length()];
             for (int i = 0; i < row.length; i++) {
-                row[i] = map.get(schema.fieldName(i));
+                row[i] = map.get(schema.name(i));
             }
             return Tuple.of(row, schema);
         }).collect(java.util.stream.Collectors.toList());

File: data/src/main/java/smile/data/DataFrameImpl.java
Patch:
@@ -348,7 +348,7 @@ public Iterator<BaseVector> iterator() {
 
     @Override
     public int indexOf(String name) {
-        return schema.fieldIndex(name);
+        return schema.indexOf(name);
     }
 
     @Override

File: data/src/main/java/smile/data/Tuple.java
Patch:
@@ -622,7 +622,7 @@ default <T> T getAs(String field) {
      * @return the field index.
      */
     default int indexOf(String field) {
-        return schema().fieldIndex(field);
+        return schema().indexOf(field);
     }
 
     /**

File: data/src/main/java/smile/data/formula/Date.java
Patch:
@@ -62,7 +62,7 @@ public Set<String> variables() {
 
     @Override
     public List<Feature> bind(StructType schema) {
-        int index = schema.fieldIndex(name);
+        int index = schema.indexOf(name);
         DataType type = schema.field(name).type;
         switch (type.id()) {
             case Date:

File: data/src/main/java/smile/data/formula/Variable.java
Patch:
@@ -77,7 +77,7 @@ public Set<String> variables() {
     public List<Feature> bind(StructType schema) {
         Feature feature = new Feature() {
             /** The column index in the schema. */
-            private final int index = schema.fieldIndex(name);
+            private final int index = schema.indexOf(name);
             /** The struct field. */
             private final StructField field = schema.field(index);
 

File: data/src/main/java/smile/data/type/StructType.java
Patch:
@@ -80,7 +80,7 @@ public StructField[] fields() {
      * @return the field.
      */
     public StructField field(String name) {
-        return fields[fieldIndex(name)];
+        return fields[indexOf(name)];
     }
 
     /**
@@ -97,7 +97,7 @@ public StructField field(int i) {
      * @param field the field name.
      * @return the index of field.
      */
-    public int fieldIndex(String field) {
+    public int indexOf(String field) {
         return index.get(field);
     }
 
@@ -106,7 +106,7 @@ public int fieldIndex(String field) {
      * @param i the index of field.
      * @return the field name.
      */
-    public String fieldName(int i) {
+    public String name(int i) {
         return fields[i].name;
     }
 

File: math/src/main/java/smile/math/kernel/BinarySparseMaternKernel.java
Patch:
@@ -23,7 +23,7 @@
  * The class of Matérn kernels is a generalization of the Gaussian/RBF.
  * It has an additional parameter nu which controls the smoothness of
  * the kernel function. The smaller nu, the less smooth the approximated
- * function is. As nu -> inf, the kernel becomes equivalent to the
+ * function is. As {@code nu -> inf}, the kernel becomes equivalent to the
  * Gaussian/RBF kernel. When nu = 1/2, the kernel becomes identical to the
  * Laplacian kernel. The Matern kernel become especially simple
  * when nu is half-integer. Important intermediate values are 3/2

File: math/src/main/java/smile/math/kernel/Matern.java
Patch:
@@ -21,7 +21,7 @@
  * The class of Matérn kernels is a generalization of the Gaussian/RBF.
  * It has an additional parameter nu which controls the smoothness of
  * the kernel function. The smaller nu, the less smooth the approximated
- * function is. As nu -> inf, the kernel becomes equivalent to the
+ * function is. As {@code nu -> inf}, the kernel becomes equivalent to the
  * Gaussian/RBF kernel. When nu = 1/2, the kernel becomes identical to the
  * Laplacian kernel. The Matern kernel become especially simple
  * when nu is half-integer. Important intermediate values are 3/2

File: math/src/main/java/smile/math/kernel/MaternKernel.java
Patch:
@@ -23,7 +23,7 @@
  * The class of Matérn kernels is a generalization of the Gaussian/RBF.
  * It has an additional parameter nu which controls the smoothness of
  * the kernel function. The smaller nu, the less smooth the approximated
- * function is. As nu -> inf, the kernel becomes equivalent to the
+ * function is. As {@code nu -> inf}, the kernel becomes equivalent to the
  * Gaussian/RBF kernel. When nu = 1/2, the kernel becomes identical to the
  * Laplacian kernel. The Matern kernel become especially simple
  * when nu is half-integer. Important intermediate values are 3/2

File: math/src/main/java/smile/math/kernel/SparseMaternKernel.java
Patch:
@@ -24,7 +24,7 @@
  * The class of Matérn kernels is a generalization of the Gaussian/RBF.
  * It has an additional parameter nu which controls the smoothness of
  * the kernel function. The smaller nu, the less smooth the approximated
- * function is. As nu -> inf, the kernel becomes equivalent to the
+ * function is. As {@code nu -> inf}, the kernel becomes equivalent to the
  * Gaussian/RBF kernel. When nu = 1/2, the kernel becomes identical to the
  * Laplacian kernel. The Matern kernel become especially simple
  * when nu is half-integer. Important intermediate values are 3/2

File: math/src/main/java/smile/stat/distribution/KernelDensity.java
Patch:
@@ -42,7 +42,7 @@ public class KernelDensity implements Distribution {
      */
     private final GaussianDistribution gaussian;
     /**
-     * h > 0 is a smoothing parameter called the bandwidth.
+     * {@code h > 0} is a smoothing parameter called the bandwidth.
      */
     private final double h;
     /**

File: core/src/main/java/smile/classification/OneVersusOne.java
Patch:
@@ -32,11 +32,11 @@
 /**
  * One-vs-one strategy for reducing the problem of
  * multiclass classification to multiple binary classification problems.
- * This approach trains <code>K (K − 1) / 2</code> binary classifiers for a
+ * This approach trains {@code K (K − 1) / 2} binary classifiers for a
  * K-way multiclass problem; each receives the samples of a pair of
  * classes from the original training set, and must learn to distinguish
  * these two classes. At prediction time, a voting scheme is applied:
- * all <code>K (K − 1) / 2</code> classifiers are applied to an unseen
+ * all {@code K (K − 1) / 2} classifiers are applied to an unseen
  * sample and the class that got the highest number of positive predictions
  * gets predicted by the combined classifier. Like One-vs-rest, one-vs-one
  * suffers from ambiguities in that some regions of its input space may

File: core/src/main/java/smile/regression/RidgeRegression.java
Patch:
@@ -117,9 +117,9 @@ public static LinearModel fit(Formula formula, DataFrame data, double lambda) {
      * Fits a generalized ridge regression model that minimizes a
      * weighted least squares criterion augmented with a
      * generalized ridge penalty:
-     * <pre><code>
+     * <pre>{@code
      *     (Y - X'*beta)' * W * (Y - X'*beta) + (beta - beta0)' * lambda * (beta - beta0)
-     * </code></pre>
+     * }</pre>
      *
      * @param formula a symbolic description of the model to be fitted.
      * @param data the data frame of the explanatory and response variables.

File: core/src/main/java/smile/sequence/HMM.java
Patch:
@@ -248,7 +248,7 @@ private void backward(int[] o, double[][] beta, double[] scaling) {
     /**
      * Returns the most likely state sequence given the observation sequence by
      * the Viterbi algorithm, which maximizes the probability of
-     * <code>P(I | O, HMM)</code>. In the calculation, we may get ties. In this
+     * {@code P(I | O, HMM)}. In the calculation, we may get ties. In this
      * case, one of them is chosen randomly.
      *
      * @param o an observation sequence.

File: data/src/main/java/smile/data/formula/Add.java
Patch:
@@ -25,7 +25,7 @@
 import smile.data.type.StructType;
 
 /**
- * The term of <code>a + b</code> expression.
+ * The term of {@code a + b} expression.
  *
  * @author Haifeng Li
  */

File: data/src/main/java/smile/data/formula/Div.java
Patch:
@@ -25,7 +25,7 @@
 import smile.data.type.StructType;
 
 /**
- * The term of <code>a / b</code> expression.
+ * The term of {@code a / b} expression.
  *
  * @author Haifeng Li
  */

File: data/src/main/java/smile/data/formula/Mul.java
Patch:
@@ -25,7 +25,7 @@
 import smile.data.type.StructType;
 
 /**
- * The term of <code>a * b</code> expression.
+ * The term of {@code a * b} expression.
  *
  * @author Haifeng Li
  */

File: data/src/main/java/smile/data/formula/Sub.java
Patch:
@@ -25,7 +25,7 @@
 import smile.data.type.StructType;
 
 /**
- * The term of <code>a - b</code> expression.
+ * The term of {@code a - b} expression.
  *
  * @author Haifeng Li
  */

File: math/src/main/java/smile/hash/PerfectHash.java
Patch:
@@ -38,7 +38,7 @@
  * Komlós and Szemerédi (1984) chooses a large prime <code>p</code>
  * (larger than the size of the universe from which <code>S</code> is drawn),
  * and a parameter <code>k</code>, and maps each element <code>x</code> of
- * <code>S</code> to the index <code>g(x) = (kx mod p) mod n</code>.
+ * <code>S</code> to the index {@code g(x) = (kx mod p) mod n}.
  *
  * @author Haifeng Li
  */

File: math/src/main/java/smile/interpolation/RBFInterpolation.java
Patch:
@@ -28,9 +28,8 @@
  * are irregularly distributed in space. In its basic form, radial basis
  * function interpolation is in the form
  * <p>
- * <pre>
  *     y(x) = &Sigma; w<sub>i</sub> &phi;(||x-c<sub>i</sub>||)
- * </pre>
+ * <p>
  * where the approximating function y(x) is represented as a sum of N radial
  * basis functions &phi;, each associated with a different center c<sub>i</sub>,
  * and weighted by an appropriate coefficient w<sub>i</sub>. For distance,

File: math/src/main/java/smile/interpolation/RBFInterpolation1D.java
Patch:
@@ -27,9 +27,8 @@
  * are irregularly distributed in space. In its basic form, radial basis
  * function interpolation is in the form
  * <p>
- * <pre>
  *     y(x) = &Sigma; w<sub>i</sub> &phi;(||x-c<sub>i</sub>||)
- * </pre>
+ * <p>
  * where the approximating function y(x) is represented as a sum of N radial
  * basis functions &phi;, each associated with a different center c<sub>i</sub>,
  * and weighted by an appropriate coefficient w<sub>i</sub>. For distance,

File: math/src/main/java/smile/interpolation/RBFInterpolation2D.java
Patch:
@@ -27,9 +27,8 @@
  * are irregularly distributed in space. In its basic form, radial basis
  * function interpolation is in the form
  * <p>
- * <pre>
  *     y(x) = &Sigma; w<sub>i</sub> &phi;(||x-c<sub>i</sub>||)
- * </pre>
+ * <p>
  * where the approximating function y(x) is represented as a sum of N radial
  * basis functions &phi;, each associated with a different center c<sub>i</sub>,
  * and weighted by an appropriate coefficient w<sub>i</sub>. For distance,

File: math/src/main/java/smile/interpolation/variogram/ExponentialVariogram.java
Patch:
@@ -20,9 +20,8 @@
 /**
  * Exponential variogram.
  * <p>
- * <pre>
  *     v(r) = c + b * (1 - e<sup>-3r/a</sup>)
- * </pre>
+ * <p>
  * where a is the range parameter and b is sill paramter. The distance of two
  * pairs increase, the variogram of those two pairs also increase. Eventually,
  * the increase of the distance can not cause the variogram increase. The

File: math/src/main/java/smile/interpolation/variogram/GaussianVariogram.java
Patch:
@@ -22,9 +22,8 @@
 /**
  * Gaussian variogram.
  * <p>
- * <pre>
- *     v(r) = c + b * (1 - e<sup>-3r<sup>2</sup>/a<sup>2</sup></sup>)
- * </pre>
+ *     v(r) = c + b * (1 - e<sup>-3r^2/a^2</sup>)
+ * <p>
  * where a is the range parameter and b is sill parameter. The distance of two
  * pairs increase, the variogram of those two pairs also increase. Eventually,
  * the increase of the distance can not cause the variogram increase. The

File: math/src/main/java/smile/interpolation/variogram/PowerVariogram.java
Patch:
@@ -22,9 +22,8 @@
 /**
  * Power variogram.
  * <p>
- * <pre>
  *     v(r) = c + &alpha; r<sup>&beta;</sup>
- * </pre>
+ * <p>
  * where &beta; is fixed and &alpha; is fitted by unweighted least squares
  * over all pairs of data points. The value of &beta; should be in the range
  * <code>1 &le; &beta; &lt; 2</code>. A good general choice is 1.5, but for

File: math/src/main/java/smile/math/MathEx.java
Patch:
@@ -3992,9 +3992,8 @@ public static int[][] sort(double[][] x) {
 
     /**
      * Solve the tridiagonal linear set which is of diagonal dominance
+     *     |b<sub>i</sub>| {@code >} |a<sub>i</sub>| + |c<sub>i</sub>|.
      * <pre>
-     *     |b<sub>i</sub>| &gt; |a<sub>i</sub>| + |c<sub>i</sub>|.
-     *
      *     | b0 c0  0  0  0 ...                        |
      *     | a1 b1 c1  0  0 ...                        |
      *     |  0 a2 b2 c2  0 ...                        |

File: math/src/main/java/smile/math/distance/JensenShannonDistance.java
Patch:
@@ -26,7 +26,6 @@
  * <p>
  * The Jensen-Shannon divergence is a symmetrized and smoothed version of the
  * Kullback-Leibler divergence . It is defined by
- * <p>
  * <pre>
  *     J(P||Q) = (D(P||M) + D(Q||M)) / 2
  * </pre>

File: math/src/main/java/smile/math/distance/LeeDistance.java
Patch:
@@ -24,9 +24,8 @@
  * of equal length n over the q-ary alphabet <code>{0, 1, ..., q-1}</code>
  * of size <code>q &ge; 2</code>, defined as
  * <p>
- * <pre>
  *     sum min(|x<sub>i</sub>-y<sub>i</sub>|, q-|x<sub>i</sub>-y<sub>i</sub>|)
- * </pre>
+ * <p>
  * If <code>q = 2</code> or <code>q = 3</code> the Lee distance coincides with the Hamming distance.
  *
  * @author Haifeng Li

File: math/src/main/java/smile/math/kernel/BinarySparseHyperbolicTangentKernel.java
Patch:
@@ -22,9 +22,8 @@
 /**
  * The hyperbolic tangent kernel on binary sparse data.
  * <p>
- * <pre>
  *     k(u, v) = tanh(&gamma; u<sup>T</sup>v - &lambda;)
- * </pre>
+ * <p>
  * where &gamma; is the scale of the used inner product and &lambda; is
  * the offset of the used inner product. If the offset is negative the
  * likelihood of obtaining a kernel matrix that is not positive definite

File: math/src/main/java/smile/math/kernel/BinarySparseLaplacianKernel.java
Patch:
@@ -22,10 +22,9 @@
 /**
  * Laplacian kernel, also referred as exponential kernel.
  * <p>
- * <pre>
  *     k(u, v) = e<sup>-||u-v|| / &sigma;</sup>
- * </pre>
- * where <code>&sigma; &gt; 0</code> is the scale parameter of the kernel.
+ * <p>
+ * where {@code &sigma; > 0} is the scale parameter of the kernel.
  * The kernel works sparse binary array as int[], which are the indices
  * of nonzero elements.
  *

File: math/src/main/java/smile/math/kernel/BinarySparsePolynomialKernel.java
Patch:
@@ -22,9 +22,8 @@
 /**
  * The polynomial kernel on binary sparse data.
  * <p>
- * <pre>
  *     k(u, v) = (&gamma; u<sup>T</sup>v - &lambda;)<sup>d</sup>
- * </pre>
+ * <p>
  * where &gamma; is the scale of the used inner product, &lambda; the offset of
  * the used inner product, and <i>d</i> the order of the polynomial kernel.
  * The kernel works on sparse binary array as int[], which are the indices of

File: math/src/main/java/smile/math/kernel/HyperbolicTangent.java
Patch:
@@ -20,9 +20,8 @@
 /**
  * The hyperbolic tangent kernel.
  * <p>
- * <pre>
  *     k(u, v) = tanh(&gamma; u<sup>T</sup>v - &lambda;)
- * </pre>
+ * <p>
  * where &gamma; is the scale of the used inner product and &lambda; is
  * the offset of the used inner product. If the offset is negative the
  * likelihood of obtaining a kernel matrix that is not positive definite

File: math/src/main/java/smile/math/kernel/HyperbolicTangentKernel.java
Patch:
@@ -22,9 +22,8 @@
 /**
  * The hyperbolic tangent kernel.
  * <p>
- * <pre>
  *     k(u, v) = tanh(&gamma; u<sup>T</sup>v - &lambda;)
- * </pre>
+ * <p>
  * where &gamma; is the scale of the used inner product and &lambda; is
  * the offset of the used inner product. If the offset is negative the
  * likelihood of obtaining a kernel matrix that is not positive definite

File: math/src/main/java/smile/math/kernel/Laplacian.java
Patch:
@@ -20,10 +20,9 @@
 /**
  * Laplacian kernel, also referred as exponential kernel.
  * <p>
- * <pre>
  *     k(u, v) = e<sup>-||u-v|| / &sigma;</sup>
- * </pre>
- * where <code>&sigma; &gt; 0</code> is the scale parameter of the kernel.
+ * <p>
+ * where {@code &sigma; > 0} is the scale parameter of the kernel.
  *
  * @author Haifeng Li
  */

File: math/src/main/java/smile/math/kernel/LaplacianKernel.java
Patch:
@@ -22,10 +22,9 @@
 /**
  * Laplacian kernel, also referred as exponential kernel.
  * <p>
- * <pre>
  *     k(u, v) = e<sup>-||u-v|| / &sigma;</sup>
- * </pre>
- * where <code>&sigma; &gt; 0</code> is the scale parameter of the kernel.
+ * <p>
+ * where {@code &sigma; > 0} is the scale parameter of the kernel.
  *
  * @author Haifeng Li
  */

File: math/src/main/java/smile/math/kernel/MercerKernel.java
Patch:
@@ -26,13 +26,13 @@
 /**
  * Mercer kernel, also called covariance function in Gaussian process.
  * A kernel is a continuous function that takes two variables x and y and
- * map them to a real value such that <code>k(x,y) = k(y,x)</code>.
+ * map them to a real value such that {@code k(x,y) = k(y,x)}.
  * A Mercer kernel is a kernel that is positive Semi-definite. When a kernel
  * is positive semi-definite, one may exploit the kernel trick, the idea of
  * implicitly mapping data to a high-dimensional feature space where some
  * linear algorithm is applied that works exclusively with inner products.
  * Assume we have some mapping &#934; from an input space X to a feature space H,
- * then a kernel <code>k(u, v) = &lt;&#934;(u), &#934;(v)&gt;</code> may be used
+ * then a kernel {@code k(u, v) = <&#934;(u), &#934;(v)>} may be used
  * to define the inner product in feature space H.
  * <p>
  * Positive definiteness in the context of kernel functions also implies that

File: math/src/main/java/smile/math/kernel/Polynomial.java
Patch:
@@ -20,9 +20,8 @@
 /**
  * The polynomial kernel.
  * <p>
- * <pre>
  *     k(u, v) = (&gamma; u<sup>T</sup>v - &lambda;)<sup>d</sup>
- * </pre>
+ * <p>
  * where &gamma; is the scale of the used inner product, &lambda; the offset of
  * the used inner product, and <i>d</i> the order of the polynomial kernel.
  *

File: math/src/main/java/smile/math/kernel/PolynomialKernel.java
Patch:
@@ -22,9 +22,8 @@
 /**
  * The polynomial kernel.
  * <p>
- * <pre>
  *     k(u, v) = (&gamma; u<sup>T</sup>v - &lambda;)<sup>d</sup>
- * </pre>
+ * <p>
  * where &gamma; is the scale of the used inner product, &lambda; the offset of
  * the used inner product, and <i>d</i> the order of the polynomial kernel.
  * 

File: math/src/main/java/smile/math/kernel/SparseHyperbolicTangentKernel.java
Patch:
@@ -23,9 +23,8 @@
 /**
  * The hyperbolic tangent kernel on sparse data.
  * <p>
- * <pre>
  *     k(u, v) = tanh(&gamma; u<sup>T</sup>v - &lambda;)
- * </pre>
+ * <p>
  * where &gamma; is the scale of the used inner product and &lambda; is
  * the offset of the used inner product. If the offset is negative the
  * likelihood of obtaining a kernel matrix that is not positive definite

File: math/src/main/java/smile/math/kernel/SparseLaplacianKernel.java
Patch:
@@ -23,10 +23,9 @@
 /**
  * Laplacian kernel, also referred as exponential kernel.
  * <p>
- * <pre>
  *     k(u, v) = e<sup>-||u-v|| / &sigma;</sup>
- * </pre>
- * where <code>&sigma; &gt; 0</code> is the scale parameter of the kernel.
+ * <p>
+ * where {@code &sigma; > 0} is the scale parameter of the kernel.
 
  * @author Haifeng Li
  */

File: math/src/main/java/smile/math/kernel/SparsePolynomialKernel.java
Patch:
@@ -23,9 +23,8 @@
 /**
  * The polynomial kernel on sparse data.
  * <p>
- * <pre>
  *     k(u, v) = (&gamma; u<sup>T</sup>v - &lambda;)<sup>d</sup>
- * </pre>
+ * <p>
  * where &gamma; is the scale of the used inner product, &lambda; the offset of
  * the used inner product, and <i>d</i> the order of the polynomial kernel.
  * 

File: math/src/main/java/smile/math/kernel/SparseThinPlateSplineKernel.java
Patch:
@@ -23,10 +23,9 @@
 /**
  * The Thin Plate Spline kernel on sparse data.
  * <p>
- * <pre>
  *     k(u, v) = (||u-v|| / &sigma;)<sup>2</sup> log (||u-v|| / &sigma;)
- * </pre>
- * where <code>&sigma; &gt; 0</code> is the scale parameter of the kernel.
+ * <p>
+ * where {@code &sigma; > 0} is the scale parameter of the kernel.
  * 
  * @author Haifeng Li
  */

File: math/src/main/java/smile/math/kernel/ThinPlateSpline.java
Patch:
@@ -20,10 +20,9 @@
 /**
  * The Thin Plate Spline kernel.
  * <p>
- * <pre>
  *     k(u, v) = (||u-v|| / &sigma;)<sup>2</sup> log (||u-v|| / &sigma;)
- * </pre>
- * where <code>&sigma; &gt; 0</code> is the scale parameter of the kernel.
+ * <p>
+ * where {@code &sigma; > 0} is the scale parameter of the kernel.
  *
  * @author Haifeng Li
  */

File: math/src/main/java/smile/math/kernel/ThinPlateSplineKernel.java
Patch:
@@ -22,10 +22,9 @@
 /**
  * The Thin Plate Spline kernel.
  * <p>
- * <pre>
  *     k(u, v) = (||u-v|| / &sigma;)<sup>2</sup> log (||u-v|| / &sigma;)
- * </pre>
- * where <code>&sigma; &gt; 0</code> is the scale parameter of the kernel.
+ * <p>
+ * where {@code &sigma; > 0} is the scale parameter of the kernel.
  * 
  * @author Haifeng Li
  */

File: math/src/main/java/smile/math/kernel/package-info.java
Patch:
@@ -21,7 +21,7 @@
  * the kernel trick, the idea of implicitly mapping data to a high-dimensional
  * feature space where some linear algorithm is applied that works exclusively
  * with inner products. Assume we have some mapping &#934; from an input
- * space X to a feature space H, then a kernel <code>k(u, v) = &lt;&#934;(u), &#934;(v)&gt;</code>
+ * space X to a feature space H, then a kernel {@code k(u, v) = <&#934;(u), &#934;(v)>}
  * may be used to define the inner product in feature space H.
  * <p>
  * Positive definiteness in the context of kernel functions also implies that

File: math/src/main/java/smile/math/random/MersenneTwister.java
Patch:
@@ -32,7 +32,7 @@
  * far larger seed value.
  *
  * <h2>References</h2>
- * <uol>
+ * <ol>
  * <li> Makato Matsumoto and Takuji Nishimura,
  * <a href="http://www.math.sci.hiroshima-u.ac.jp/~m-mat/MT/ARTICLES/mt.pdf">"Mersenne Twister: A 623-Dimensionally Equidistributed Uniform Pseudo-Random Number Generator"</a>,
  * <i>ACM Transactions on Modeling and Computer Simulation, </i> Vol. 8, No. 1,

File: math/src/main/java/smile/math/rbf/GaussianRadialBasis.java
Patch:
@@ -18,7 +18,7 @@
 package smile.math.rbf;
 
 /**
- * Gaussian RBF. &phi;(r) = e<sup>-0.5 * r<sup>2</sup> / r<sup>2</sup><sub>0</sub></sup>
+ * Gaussian RBF. &phi;(r) = e<sup>-0.5 * r^2 / r<sub>0</sub>^2</sup>
  * where r<sub>0</sub> is a scale factor. The interpolation accuracy using
  * Gaussian basis functions can be very sensitive to r<sub>0</sub>, and they
  * are often avoided for this reason. However, for smooth functions and with

File: math/src/main/java/smile/math/special/Beta.java
Patch:
@@ -23,9 +23,8 @@
 /**
  * The beta function, also called the Euler integral of the first kind.
  * <p>
- * <pre>
- *     B(x, y) = <i><big>&#8747;</big><sub><small>0</small></sub><sup><small>1</small></sup> t<sup>x-1</sup> (1-t)<sup>y-1</sup>dt</i>
- * </pre>
+ *     B(x, y) = <i>&#8747;<sub><small>0</small></sub><sup><small>1</small></sup> t<sup>x-1</sup> (1-t)<sup>y-1</sup>dt</i>
+ * <p>
  * for <code>x, y &gt; 0</code> and the integration is over [0, 1].
  * The beta function is symmetric, i.e. <code>B(x, y) = B(y, x)</code>.
  *

File: math/src/main/java/smile/math/special/Erf.java
Patch:
@@ -25,9 +25,8 @@
  * statistics, materials science, and partial differential equations.
  * It is defined as:
  * <p>
- * <pre>
- *     erf(x) = <i><big>&#8747;</big><sub><small>0</small></sub><sup><small>x</small></sup> e<sup>-t<sup>2</sup></sup>dt</i>
- * </pre>
+ *     erf(x) = <i>&#8747;<sub><small>0</small></sub><sup><small>x</small></sup> e<sup>-t^2</sup>dt</i>
+ * <p>
  * The complementary error function, denoted erfc, is defined as
  * <code>erfc(x) = 1 - erf(x)</code>. The error function and complementary
  * error function are special cases of the incomplete gamma function.

File: math/src/main/java/smile/stat/distribution/BinomialDistribution.java
Patch:
@@ -32,9 +32,8 @@
  * distribution. The probability of getting exactly k successes in n trials
  * is given by the probability mass function:
  * <p>
- * <pre>
  *     Pr(K = k) = <sub>n</sub>C<sub>k</sub> p<sup>k</sup> (1-p)<sup>n-k</sup>
- * </pre>
+ * <p>
  * where <code><sub>n</sub>C<sub>k</sub></code> is n choose k.
  * <p>
  * It is frequently used to model number of successes in a sample of size

File: math/src/main/java/smile/stat/distribution/ShiftedGeometricDistribution.java
Patch:
@@ -25,10 +25,7 @@
  * <code>{0, 1, 2, 3, &hellip;}</code>.
  * If the probability of success on each trial is p, then the probability that
  * the k-<i>th</i> trial (out of k trials) is the first success is
- * <p>
- * <pre>
  *     Pr(X = k) = (1 - p)<sup>k</sup> p
- * </pre>.
  *
  * @see GeometricDistribution
  *

File: math/src/main/java/smile/util/IntHashSet.java
Patch:
@@ -20,8 +20,8 @@
 import java.util.Arrays;
 
 /**
- * HashSet<int> for primitive types.
- * Integer.MIN_VALUE (0x80000000) is not allowed as key.
+ * {@code HashSet<int>} for primitive types.
+ * {@code Integer.MIN_VALUE (0x80000000)} is not allowed as key.
  */
 public class IntHashSet {
     private static final int FREE_KEY = Integer.MIN_VALUE;

File: core/src/main/java/smile/association/ARM.java
Patch:
@@ -24,7 +24,7 @@
 
 /**
  * Association Rule Mining.
- * Let <code></code>I = {i<sub>1</sub>, i<sub>2</sub>,..., i<sub>n</sub>}</code>
+ * Let <code>I = {i<sub>1</sub>, i<sub>2</sub>,..., i<sub>n</sub>}</code>
  * be a set of n binary attributes called items. Let
  * <code>D = {t<sub>1</sub>, t<sub>2</sub>,..., t<sub>m</sub>}</code>
  * be a set of transactions called the database. Each transaction in
@@ -114,6 +114,8 @@ public AssociationRule next() {
     /**
      * Mines the association rules.
      * @param confidence the confidence threshold for association rules.
+     * @param tree the FP-tree.
+     * @return the stream of association rules.
      */
     public static Stream<AssociationRule> apply(double confidence, FPTree tree) {
         TotalSupportTree ttree = new TotalSupportTree(tree);

File: core/src/main/java/smile/clustering/PartitionClustering.java
Patch:
@@ -111,8 +111,7 @@ public String toString() {
      * of k-means. Although the initial selection in the algorithm takes extra time,
      * the k-means part itself converges very fast after this seeding and thus
      * the algorithm actually lowers the computation time too.
-     * 
-     * <h2>References</h2>
+     *
      * <ol>
      * <li> D. Arthur and S. Vassilvitskii. "K-means++: the advantages of careful seeding". ACM-SIAM symposium on Discrete algorithms, 1027-1035, 2007.</li>
      * <li> Anna D. Peterson, Arka P. Ghosh and Ranjan Maitra. A systematic evaluation of different methods for initializing the K-means clustering algorithm. 2010.</li>

File: core/src/main/java/smile/feature/SHAP.java
Patch:
@@ -40,7 +40,7 @@
  * In game theory, the Shapley value is the average expected marginal
  * contribution of one player after all possible combinations have
  * been considered.
- * <p>
+ *
  * <h2>References</h2>
  * <ol>
  * <li>Lundberg, Scott M., and Su-In Lee. A unified approach to interpreting model predictions. NIPS, 2017.</li>

File: core/src/main/java/smile/glm/GLM.java
Patch:
@@ -45,7 +45,6 @@
  * to be generated from a particular distribution in an exponential family.
  * The mean, <code>&mu;</code>, of the distribution depends on the
  * independent variables, <code>X</code>, through:
- * <p>
  * <pre>
  *     E(Y) = &mu; = g<sup>-1</sup>(X&beta;)
  * </pre>
@@ -57,11 +56,10 @@
  * <p>
  * In this framework, the variance is typically a function, <code>V</code>,
  * of the mean:
- * <p>
  * <pre>
  *     Var(Y) = V(&mu;) = V(g<sup>-1</sup>(X&beta;))
  * </pre>
- * </p>
+ * <p>
  * It is convenient if <code>V</code> follows from an exponential family
  * of distributions, but it may simply be that the variance is a function
  * of the predicted value, such as <code>V(&mu;<sub>i</sub>) = &mu;<sub>i</sub></code>

File: core/src/main/java/smile/manifold/UMAP.java
Patch:
@@ -37,19 +37,16 @@
  * UMAP is a dimension reduction technique that can be used for visualization
  * similarly to t-SNE, but also for general non-linear dimension reduction.
  * The algorithm is founded on three assumptions about the data:
- * <p>
  * <ul>
  * <li>The data is uniformly distributed on a Riemannian manifold;</li>
  * <li>The Riemannian metric is locally constant (or can be approximated as
  * such);</li>
  * <li>The manifold is locally connected.</li>
  * </ul>
- * <p>
  * From these assumptions it is possible to model the manifold with a fuzzy
  * topological structure. The embedding is found by searching for a low
  * dimensional projection of the data that has the closest possible equivalent
  * fuzzy topological structure.
- * <p>
  * <h2>References</h2>
  * <ol>
  * <li>McInnes, L, Healy, J, UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction, ArXiv e-prints 1802.03426, 2018</li>

File: core/src/main/java/smile/projection/KPCA.java
Patch:
@@ -54,7 +54,7 @@
  * @see smile.manifold.IsoMap
  * @see smile.manifold.LLE
  * @see smile.manifold.LaplacianEigenmap
- * @see smile.mds.SammonMapping
+ * @see smile.manifold.SammonMapping
  *
  * @author Haifeng Li
  */

File: core/src/main/java/smile/projection/ProbabilisticPCA.java
Patch:
@@ -26,7 +26,6 @@
  * Probabilistic principal component analysis. Probabilistic PCA is
  * a simplified factor analysis that employs a latent variable model
  * with linear relationship:
- * <p>
  * <pre>
  *     y &sim; W * x + &mu; + &epsilon;
  * </pre>

File: core/src/main/java/smile/validation/Hyperparameters.java
Patch:
@@ -38,7 +38,6 @@
  * while learning rate and mini-batch size are algorithm hyperparameters.
  * <p>
  * The below example shows how to tune the hyperparameters of random forest.
- * <p>
  * <pre>
  * {@code
  *    import smile.io.*;

File: core/src/main/java/smile/validation/metric/FDR.java
Patch:
@@ -20,7 +20,6 @@
 /**
  * The false discovery rate (FDR) is ratio of false positives
  * to combined true and false positives, which is actually 1 - precision.
- * <p>
  * <pre>
  *     FDR = FP / (TP + FP)
  * </pre>

File: core/src/main/java/smile/validation/metric/Fallout.java
Patch:
@@ -19,7 +19,6 @@
 
 /**
  * Fall-out, false alarm rate, or false positive rate (FPR)
- * <p>
  * <pre>
  *     FPR = FP / N = FP / (FP + TN)
  * </pre>

File: core/src/main/java/smile/validation/metric/Precision.java
Patch:
@@ -20,7 +20,6 @@
 /**
  * The precision or positive predictive value (PPV) is ratio of true positives
  * to combined true and false positives, which is different from sensitivity.
- * <p>
  * <pre>
  *     PPV = TP / (TP + FP)
  * </pre>

File: core/src/main/java/smile/validation/metric/Sensitivity.java
Patch:
@@ -22,7 +22,6 @@
  * statistical measures of the performance of a binary classification test.
  * Sensitivity is the proportion of actual positives which are correctly
  * identified as such.
- * <p>
  * <pre>
  *     TPR = TP / P = TP / (TP + FN)
  * </pre>

File: core/src/main/java/smile/validation/metric/Specificity.java
Patch:
@@ -21,7 +21,6 @@
  * Specificity (SPC) or True Negative Rate is a statistical measures of the
  * performance of a binary classification test. Specificity measures the
  * proportion of negatives which are correctly identified.
- * <p>
  * <pre>
  *     SPC = TN / N = TN / (FP + TN) = 1 - FPR
  * </pre>

File: math/src/main/java/smile/interpolation/variogram/package-info.java
Patch:
@@ -19,9 +19,8 @@
  * Variogram functions. In spatial statistics the theoretical variogram
  * <code>2&gamma;(x,y)</code> is a function describing the degree of
  * spatial dependence of a spatial random field or stochastic process
- * <code>Z(x) * <p>. It is defined as the expected squared increment
+ * <code>Z(x)</code>. It is defined as the expected squared increment
  * of the values between locations x and y:
- * <p>
  * <pre>
  *     2&gamma;(x,y)=E(|Z(x)-Z(y)|<sup>2</sup>)
  * </pre>

File: math/src/main/java/smile/math/special/Gamma.java
Patch:
@@ -119,7 +119,7 @@ public static double lgamma(double x) {
 
     /**
      * Regularized Incomplete Gamma Function
-     * P(s,x) = <i><big>&#8747;</big><sub><small>0</small></sub><sup><small>x</small></sup> e<sup>-t</sup> t<sup>(s-1)</sup> dt</i>
+     * P(s,x) = <i>&#8747;<sub><small>0</small></sub><sup><small>x</small></sup> e<sup>-t</sup> t<sup>(s-1)</sup> dt</i>
      */
     public static double regularizedIncompleteGamma(double s, double x) {
         if (s < 0.0) {
@@ -145,7 +145,7 @@ public static double regularizedIncompleteGamma(double s, double x) {
 
     /**
      * Regularized Upper/Complementary Incomplete Gamma Function
-     * Q(s,x) = 1 - P(s,x) = 1 - <i><big>&#8747;</big><sub><small>0</small></sub><sup><small>x</small></sup> e<sup>-t</sup> t<sup>(s-1)</sup> dt</i>
+     * Q(s,x) = 1 - P(s,x) = 1 - <i>&#8747;<sub><small>0</small></sub><sup><small>x</small></sup> e<sup>-t</sup> t<sup>(s-1)</sup> dt</i>
      */
     public static double regularizedUpperIncompleteGamma(double s, double x) {
         if (s < 0.0) {

File: plot/src/main/java/smile/swing/FontChooser.java
Patch:
@@ -65,7 +65,6 @@
  * The <code>FontChooser</code> class is a swing component 
  * for font selection with <code>JFileChooser</code>-like APIs.
  * The following code pops up a font chooser dialog.
- * <p>
  * <pre>
  * {@code
  *   FontChooser fontChooser = FontChooser.getInstance();

File: math/src/main/java/smile/math/matrix/BandMatrix.java
Patch:
@@ -137,8 +137,8 @@ public BandMatrix(int m, int n, int kl, int ku) {
      * @param n the number of columns.
      * @param kl the number of subdiagonals.
      * @param ku the number of superdiagonals.
-     * @param AB the band matrix. A[i, j] is stored in
-     *           AB[ku+i-j, j] for max(0, j-ku) <= i <= min(m-1, j+kl).
+     * @param AB the band matrix. A[i, j] is stored in {@code AB[ku+i-j, j]}
+     *           for {@code max(0, j-ku) <= i <= min(m-1, j+kl)}.
      */
     public BandMatrix(int m, int n, int kl, int ku, double[][] AB) {
         this(m, n, kl, ku);

File: math/src/main/java/smile/math/matrix/DMatrix.java
Patch:
@@ -156,7 +156,6 @@ public void tv(double alpha, double[] x, double beta, double[] y) {
      *
      * @param path the input file path.
      * @return a dense or sparse matrix.
-     * @author Haifeng Li
      */
     public static DMatrix market(Path path) throws IOException, ParseException {
         try (LineNumberReader reader = new LineNumberReader(Files.newBufferedReader(path));

File: math/src/main/java/smile/math/matrix/FloatBandMatrix.java
Patch:
@@ -139,8 +139,8 @@ public FloatBandMatrix(int m, int n, int kl, int ku) {
      * @param n the number of columns.
      * @param kl the number of subdiagonals.
      * @param ku the number of superdiagonals.
-     * @param AB the band matrix. A[i, j] is stored in
-     *           AB[ku+i-j, j] for max(0, j-ku) <= i <= min(m-1, j+kl).
+     * @param AB the band matrix. A[i, j] is stored in {@code AB[ku+i-j, j]}
+     *           for {@code max(0, j-ku) <= i <= min(m-1, j+kl)}.
      */
     public FloatBandMatrix(int m, int n, int kl, int ku, float[][] AB) {
         this(m, n, kl, ku);

File: math/src/main/java/smile/math/matrix/SMatrix.java
Patch:
@@ -156,7 +156,6 @@ public void tv(float alpha, float[] x, float beta, float[] y) {
      *
      * @param path the input file path.
      * @return a dense or sparse matrix.
-     * @author Haifeng Li
      */
     public static SMatrix market(Path path) throws IOException, ParseException {
         try (LineNumberReader reader = new LineNumberReader(Files.newBufferedReader(path));

File: core/src/main/java/smile/classification/MLP.java
Patch:
@@ -193,7 +193,7 @@ public void update(double[] x, int y) {
         t++;
     }
 
-    /** Updates the model with a mini-batch. RMSProp is applied if rho > 0. */
+    /** Updates the model with a mini-batch. RMSProp is applied if {@code rho > 0}. */
     @Override
     public void update(double[][] x, int[] y) {
         for (int i = 0; i < x.length; i++) {

File: core/src/main/java/smile/regression/GaussianProcessRegression.java
Patch:
@@ -326,7 +326,7 @@ public static <T> GaussianProcessRegression<T> fit(T[] x, double[] y, MercerKern
      * @param noise the noise variance, which also works as a regularization parameter.
      * @param normalize the flag if normalize the response variable.
      * @param tol the stopping tolerance for HPO.
-     * @param maxIter the maximum number of iterations for HPO. No HPO if maxIter <= 0.
+     * @param maxIter the maximum number of iterations for HPO. No HPO if {@code maxIter <= 0}.
      */
     public static <T> GaussianProcessRegression<T> fit(T[] x, double[] y, MercerKernel<T> kernel, double noise, boolean normalize, double tol, int maxIter) {
         if (x.length != y.length) {

File: core/src/main/java/smile/regression/MLP.java
Patch:
@@ -75,7 +75,7 @@ public void update(double[] x, double y) {
         t++;
     }
 
-    /** Updates the model with a mini-batch. RMSProp is applied if rho > 0. */
+    /** Updates the model with a mini-batch. RMSProp is applied if {@code rho > 0}. */
     @Override
     public void update(double[][] x, double[] y) {
         double[] target = this.target.get();

File: nlp/src/main/java/smile/nlp/embedding/GloVe.java
Patch:
@@ -49,8 +49,7 @@
  */
 public class GloVe {
     /**
-     * Loads a <a href="https://nlp.stanford.edu/projects/glove/>pre-trained</a>
-     * GloVe model.
+     * Loads a GloVe model.
      */
     public static Word2Vec of(Path file) throws IOException {
         try (Stream<String> stream = Files.lines(file)) {

File: data/src/main/java/smile/data/DataFrame.java
Patch:
@@ -115,8 +115,7 @@ default DataFrame of(int... index) {
 
     /** Returns a new data frame with boolean indexing. */
     default DataFrame of(boolean... index) {
-        int[] idx = IntStream.range(0, index.length).filter(i -> index[i]).toArray();
-        return new IndexDataFrame(this, idx);
+        return of(IntStream.range(0, index.length).filter(i -> index[i]).toArray());
     }
 
     /**
@@ -1264,7 +1263,7 @@ static DataFrame of(ResultSet rs) throws SQLException {
             rows.add(Tuple.of(rs, schema));
         }
 
-        return of(rows);
+        return DataFrame.of(rows);
     }
 
     /**

File: data/src/main/java/smile/data/vector/BooleanVector.java
Patch:
@@ -37,6 +37,9 @@ default DataType type() {
     @Override
     boolean[] array();
 
+    @Override
+    BooleanVector get(int... index);
+
     /**
      * Returns the value at position i.
      */

File: data/src/main/java/smile/data/vector/ByteVector.java
Patch:
@@ -37,6 +37,9 @@ default DataType type() {
     @Override
     byte[] array();
 
+    @Override
+    ByteVector get(int... index);
+
     @Override
     default short getShort(int i) {
         return getByte(i);

File: data/src/main/java/smile/data/vector/CharVector.java
Patch:
@@ -37,6 +37,9 @@ default DataType type() {
     @Override
     char[] array();
 
+    @Override
+    CharVector get(int... index);
+
     /**
      * Returns the value at position i.
      */

File: data/src/main/java/smile/data/vector/DoubleVector.java
Patch:
@@ -37,6 +37,9 @@ default DataType type() {
     @Override
     double[] array();
 
+    @Override
+    DoubleVector get(int... index);
+
     @Override
     default byte getByte(int i) {
         throw new UnsupportedOperationException("cast double to byte");

File: data/src/main/java/smile/data/vector/FloatVector.java
Patch:
@@ -37,6 +37,9 @@ default DataType type() {
     @Override
     float[] array();
 
+    @Override
+    FloatVector get(int... index);
+
     @Override
     default byte getByte(int i) {
         throw new UnsupportedOperationException("cast float to byte");

File: data/src/main/java/smile/data/vector/IntVector.java
Patch:
@@ -37,6 +37,9 @@ default DataType type() {
     @Override
     int[] array();
 
+    @Override
+    IntVector get(int... index);
+
     @Override
     default byte getByte(int i) {
         throw new UnsupportedOperationException("cast int to byte");

File: data/src/main/java/smile/data/vector/LongVector.java
Patch:
@@ -37,6 +37,9 @@ default DataType type() {
     @Override
     long[] array();
 
+    @Override
+    LongVector get(int... index);
+
     @Override
     default byte getByte(int i) {
         throw new UnsupportedOperationException("cast long to byte");

File: data/src/main/java/smile/data/vector/ShortVector.java
Patch:
@@ -37,6 +37,9 @@ default DataType type() {
     @Override
     short[] array();
 
+    @Override
+    ShortVector get(int... index);
+
     @Override
     default byte getByte(int i) {
         throw new UnsupportedOperationException("cast short to byte");

File: data/src/main/java/smile/data/vector/StringVector.java
Patch:
@@ -32,6 +32,8 @@
  * @author Haifeng Li
  */
 public interface StringVector extends Vector<String> {
+    @Override
+    StringVector get(int... index);
 
     /**
      * Returns a vector of LocalDate. This method assumes that this is a string vector and

File: data/src/main/java/smile/data/vector/Vector.java
Patch:
@@ -34,6 +34,9 @@
  * @author Haifeng Li
  */
 public interface Vector<T> extends BaseVector<T, T, Stream<T>> {
+    @Override
+    Vector<T> get(int... index);
+
     /** Returns the array of elements. */
     T[] toArray();
 

File: core/src/main/java/smile/regression/SVR.java
Patch:
@@ -117,7 +117,7 @@ public static Regression<SparseArray> fit(SparseArray[] x, double[] y, int p, do
 
             @Override
             public double predict(SparseArray x) {
-                return model.f(x) > 0 ? +1 : -1;
+                return model.f(x);
             }
         };
     }

File: math/src/main/java/smile/math/kernel/MercerKernel.java
Patch:
@@ -138,7 +138,7 @@ default Matrix K(T[] x, T[] y) {
         Matrix K = new Matrix(m, n);
         IntStream.range(0, n).parallel().forEach(j -> {
             T yj = y[j];
-            for (int i = 0; i < n; i++) {
+            for (int i = 0; i < m; i++) {
                 K.set(i, j, k(x[i], yj));
             }
         });

File: math/src/main/java/smile/math/kernel/BinarySparseGaussianKernel.java
Patch:
@@ -45,7 +45,7 @@ public class BinarySparseGaussianKernel implements MercerKernel<int[]>, Isotropi
      */
     public BinarySparseGaussianKernel(double sigma) {
         if (sigma <= 0) {
-            throw new IllegalArgumentException("sigma is not positive.");
+            throw new IllegalArgumentException("sigma is not positive: " + sigma);
         }
 
         this.gamma = 0.5 / (sigma * sigma);

File: math/src/main/java/smile/math/kernel/BinarySparseLaplacianKernel.java
Patch:
@@ -43,7 +43,7 @@ public class BinarySparseLaplacianKernel implements MercerKernel<int[]>, Isotrop
      */
     public BinarySparseLaplacianKernel(double sigma) {
         if (sigma <= 0) {
-            throw new IllegalArgumentException("sigma is not positive.");
+            throw new IllegalArgumentException("sigma is not positive: " + sigma);
         }
 
         this.gamma = 1.0 / sigma;

File: math/src/main/java/smile/math/kernel/BinarySparsePolynomialKernel.java
Patch:
@@ -49,11 +49,11 @@ public BinarySparsePolynomialKernel(int degree) {
      */
     public BinarySparsePolynomialKernel(int degree, double scale, double offset) {
         if (degree <= 0) {
-            throw new IllegalArgumentException("Non-positive polynomial degree.");
+            throw new IllegalArgumentException("Non-positive polynomial degree: " + degree);
         }
 
         if (offset < 0.0) {
-            throw new IllegalArgumentException("Negative offset: the kernel does not satisfy Mercer's condition.");
+            throw new IllegalArgumentException("Negative offset: the kernel does not satisfy Mercer's condition: " + offset);
         }
         
         this.degree = degree;

File: math/src/main/java/smile/math/kernel/BinarySparseThinPlateSplineKernel.java
Patch:
@@ -43,7 +43,7 @@ public class BinarySparseThinPlateSplineKernel implements MercerKernel<int[]>, I
      */
     public BinarySparseThinPlateSplineKernel(double sigma) {
         if (sigma <= 0) {
-            throw new IllegalArgumentException("sigma is not positive.");
+            throw new IllegalArgumentException("sigma is not positive: " + sigma);
         }
 
         this.sigma = sigma;

File: math/src/main/java/smile/math/kernel/SparseGaussianKernel.java
Patch:
@@ -47,7 +47,7 @@ public class SparseGaussianKernel implements MercerKernel<SparseArray>, Isotropi
      */
     public SparseGaussianKernel(double sigma) {
         if (sigma <= 0) {
-            throw new IllegalArgumentException("sigma is not positive.");
+            throw new IllegalArgumentException("sigma is not positive: " + sigma);
         }
 
         this.gamma = 0.5 / (sigma * sigma);

File: math/src/main/java/smile/math/kernel/SparseLaplacianKernel.java
Patch:
@@ -44,7 +44,7 @@ public class SparseLaplacianKernel implements MercerKernel<SparseArray>, Isotrop
      */
     public SparseLaplacianKernel(double sigma) {
         if (sigma <= 0) {
-            throw new IllegalArgumentException("sigma is not positive.");
+            throw new IllegalArgumentException("sigma is not positive: " + sigma);
         }
 
         this.gamma = 1.0 / sigma;

File: math/src/main/java/smile/math/kernel/SparsePolynomialKernel.java
Patch:
@@ -50,11 +50,11 @@ public SparsePolynomialKernel(int degree) {
      */
     public SparsePolynomialKernel(int degree, double scale, double offset) {
         if (degree <= 0) {
-            throw new IllegalArgumentException("Non-positive polynomial degree.");
+            throw new IllegalArgumentException("Non-positive polynomial degree: " + degree);
         }
 
         if (offset < 0.0) {
-            throw new IllegalArgumentException("Negative offset: the kernel does not satisfy Mercer's condition.");
+            throw new IllegalArgumentException("Negative offset: the kernel does not satisfy Mercer's condition: " + offset);
         }
         
         this.degree = degree;

File: math/src/main/java/smile/math/kernel/SparseThinPlateSplineKernel.java
Patch:
@@ -45,7 +45,7 @@ public class SparseThinPlateSplineKernel implements MercerKernel<SparseArray>, I
      */
     public SparseThinPlateSplineKernel(double sigma) {
         if (sigma <= 0) {
-            throw new IllegalArgumentException("sigma is not positive.");
+            throw new IllegalArgumentException("sigma is not positive: " + sigma);
         }
 
         this.sigma = sigma;

File: data/src/main/java/smile/data/type/DataTypes.java
Patch:
@@ -165,7 +165,7 @@ public static StructType struct(ResultSetMetaData meta, String dbms) throws SQLE
         for (int i = 1; i <= ncols; i++) {
             String name = meta.getColumnName(i);
             DataType type = DataType.of(
-                    JDBCType.valueOf(meta.getColumnTypeName(i)),
+                    JDBCType.valueOf(meta.getColumnType(i)),
                     meta.isNullable(i) != ResultSetMetaData.columnNoNulls,
                     dbms);
             fields[i-1] = new StructField(name, type);

File: core/src/main/java/smile/base/mlp/MultilayerPerceptron.java
Patch:
@@ -67,7 +67,7 @@ public abstract class MultilayerPerceptron implements Serializable {
     /**
      * The discounting factor for the history/coming gradient in RMSProp.
      */
-    protected double rho = 0.9;
+    protected double rho = 0.0;
     /**
      * A small constant for numerical stability in RMSProp.
      */

File: math/src/main/java/smile/math/matrix/IMatrix.java
Patch:
@@ -119,7 +119,7 @@ public String[] colNames() {
 
     /** Returns the name of i-th column. */
     public String colName(int i) {
-        return colName(i);
+        return colNames[i];
     }
 
     /** Sets the column names. */

File: data/src/main/java/smile/data/vector/ByteVector.java
Patch:
@@ -17,6 +17,7 @@
 
 package smile.data.vector;
 
+import java.util.function.IntFunction;
 import java.util.stream.Collectors;
 import java.util.stream.IntStream;
 import smile.data.type.DataType;
@@ -67,8 +68,9 @@ default double getDouble(int i) {
      * @param n Number of elements to show
      */
     default String toString(int n) {
+        IntFunction<String> toString = field()::toString;
         String suffix = n >= size() ? "]" : String.format(", ... %,d more]", size() - n);
-        return stream().limit(n).mapToObj(i -> measure().map(m -> m.toString(i)).orElseGet(() -> String.valueOf(i))).collect(Collectors.joining(", ", "[", suffix));
+        return stream().limit(n).mapToObj(toString).collect(Collectors.joining(", ", "[", suffix));
     }
 
     /** Creates a named byte vector.

File: data/src/main/java/smile/data/vector/ByteVectorImpl.java
Patch:
@@ -61,8 +61,8 @@ public String name() {
     }
 
     @Override
-    public Optional<Measure> measure() {
-        return Optional.ofNullable(measure);
+    public Measure measure() {
+        return measure;
     }
 
     @Override

File: data/src/main/java/smile/data/vector/CharVectorImpl.java
Patch:
@@ -17,9 +17,8 @@
 
 package smile.data.vector;
 
-import smile.data.type.StructField;
-
 import java.util.stream.IntStream;
+import smile.data.type.StructField;
 
 /**
  * An immutable char vector.

File: data/src/main/java/smile/data/vector/DoubleVector.java
Patch:
@@ -17,6 +17,7 @@
 
 package smile.data.vector;
 
+import java.util.function.DoubleFunction;
 import java.util.stream.Collectors;
 import java.util.stream.DoubleStream;
 import smile.data.type.DataType;
@@ -67,8 +68,9 @@ default float getFloat(int i) {
      * @param n Number of elements to show
      */
     default String toString(int n) {
+        DoubleFunction<String> toString = field()::toString;
         String suffix = n >= size() ? "]" : String.format(", ... %,d more]", size() - n);
-        return stream().limit(n).mapToObj(i -> measure().map(m -> m.toString(i)).orElseGet(() -> String.valueOf(i))).collect(Collectors.joining(", ", "[", suffix));
+        return stream().limit(n).mapToObj(toString).collect(Collectors.joining(", ", "[", suffix));
     }
 
     /** Creates a named double vector.

File: data/src/main/java/smile/data/vector/FloatVector.java
Patch:
@@ -17,6 +17,7 @@
 
 package smile.data.vector;
 
+import java.util.function.DoubleFunction;
 import java.util.stream.Collectors;
 import java.util.stream.DoubleStream;
 import smile.data.type.DataType;
@@ -62,14 +63,14 @@ default double getDouble(int i) {
         return getFloat(i);
     }
 
-
     /**
      * Returns the string representation of vector.
      * @param n Number of elements to show
      */
     default String toString(int n) {
+        DoubleFunction<String> toString = field()::toString;
         String suffix = n >= size() ? "]" : String.format(", ... %,d more]", size() - n);
-        return stream().limit(n).mapToObj(i -> measure().map(m -> m.toString(i)).orElseGet(() -> String.valueOf(i))).collect(Collectors.joining(", ", "[", suffix));
+        return stream().limit(n).mapToObj(toString).collect(Collectors.joining(", ", "[", suffix));
     }
 
     /** Creates a named float vector.

File: data/src/main/java/smile/data/vector/IntVector.java
Patch:
@@ -17,6 +17,7 @@
 
 package smile.data.vector;
 
+import java.util.function.IntFunction;
 import java.util.stream.Collectors;
 import java.util.stream.IntStream;
 import smile.data.type.DataType;
@@ -67,8 +68,9 @@ default double getDouble(int i) {
      * @param n Number of elements to show
      */
     default String toString(int n) {
+        IntFunction<String> toString = field()::toString;
         String suffix = n >= size() ? "]" : String.format(", ... %,d more]", size() - n);
-        return stream().limit(n).mapToObj(i -> measure().map(m -> m.toString(i)).orElseGet(() -> String.valueOf(i))).collect(Collectors.joining(", ", "[", suffix));
+        return stream().limit(n).mapToObj(toString).collect(Collectors.joining(", ", "[", suffix));
     }
 
     /** Creates a named integer vector.

File: data/src/main/java/smile/data/vector/IntVectorImpl.java
Patch:
@@ -62,8 +62,8 @@ public String name() {
     }
 
     @Override
-    public Optional<Measure> measure() {
-        return Optional.ofNullable(measure);
+    public Measure measure() {
+        return measure;
     }
 
     @Override

File: data/src/main/java/smile/data/vector/LongVector.java
Patch:
@@ -17,6 +17,7 @@
 
 package smile.data.vector;
 
+import java.util.function.LongFunction;
 import java.util.stream.Collectors;
 import java.util.stream.LongStream;
 import smile.data.type.DataType;
@@ -67,8 +68,9 @@ default double getDouble(int i) {
      * @param n Number of elements to show
      */
     default String toString(int n) {
+        LongFunction<String> toString = field()::toString;
         String suffix = n >= size() ? "]" : String.format(", ... %,d more]", size() - n);
-        return stream().limit(n).mapToObj(i -> measure().map(m -> m.toString(i)).orElseGet(() -> String.valueOf(i))).collect(Collectors.joining(", ", "[", suffix));
+        return stream().limit(n).mapToObj(toString).collect(Collectors.joining(", ", "[", suffix));
     }
 
     /** Creates a named long vector.

File: data/src/main/java/smile/data/vector/ShortVector.java
Patch:
@@ -17,6 +17,7 @@
 
 package smile.data.vector;
 
+import java.util.function.IntFunction;
 import java.util.stream.Collectors;
 import java.util.stream.IntStream;
 import smile.data.type.DataType;
@@ -67,8 +68,9 @@ default double getDouble(int i) {
      * @param n Number of elements to show
      */
     default String toString(int n) {
+        IntFunction<String> toString = field()::toString;
         String suffix = n >= size() ? "]" : String.format(", ... %,d more]", size() - n);
-        return stream().limit(n).mapToObj(i -> measure().map(m -> m.toString(i)).orElseGet(() -> String.valueOf(i))).collect(Collectors.joining(", ", "[", suffix));
+        return stream().limit(n).mapToObj(toString).collect(Collectors.joining(", ", "[", suffix));
     }
 
     /** Creates a named short integer vector.

File: data/src/main/java/smile/data/vector/StringVectorImpl.java
Patch:
@@ -17,7 +17,6 @@
 
 package smile.data.vector;
 
-
 import java.time.LocalDate;
 import java.time.LocalDateTime;
 import java.time.LocalTime;

File: data/src/main/java/smile/data/vector/VectorImpl.java
Patch:
@@ -24,7 +24,6 @@
 import java.time.ZoneId;
 import java.util.Arrays;
 import java.util.Date;
-import java.util.Optional;
 import java.util.stream.Stream;
 
 import smile.data.measure.CategoricalMeasure;
@@ -93,8 +92,8 @@ public DataType type() {
     }
 
     @Override
-    public Optional<Measure> measure() {
-        return Optional.ofNullable(measure);
+    public Measure measure() {
+        return measure;
     }
 
     @Override

File: data/src/main/java/smile/data/vector/BooleanVectorImpl.java
Patch:
@@ -83,7 +83,7 @@ public Boolean get(int i) {
     public BooleanVector get(int... index) {
         boolean[] v = new boolean[index.length];
         for (int i = 0; i < index.length; i++) v[i] = vector[index[i]];
-        return new BooleanVectorImpl(name, v);
+        return new BooleanVectorImpl(field(), v);
     }
 
     @Override

File: data/src/main/java/smile/data/vector/ByteVectorImpl.java
Patch:
@@ -96,7 +96,7 @@ public Byte get(int i) {
     public ByteVector get(int... index) {
         byte[] v = new byte[index.length];
         for (int i = 0; i < index.length; i++) v[i] = vector[index[i]];
-        return new ByteVectorImpl(name, v);
+        return new ByteVectorImpl(field(), v);
     }
 
     @Override

File: data/src/main/java/smile/data/vector/CharVectorImpl.java
Patch:
@@ -84,7 +84,7 @@ public Character get(int i) {
     public CharVector get(int... index) {
         char[] v = new char[index.length];
         for (int i = 0; i < index.length; i++) v[i] = vector[index[i]];
-        return new CharVectorImpl(name, v);
+        return new CharVectorImpl(field(), v);
     }
 
     @Override

File: data/src/main/java/smile/data/vector/DoubleVectorImpl.java
Patch:
@@ -96,7 +96,7 @@ public Double get(int i) {
     public DoubleVector get(int... index) {
         double[] v = new double[index.length];
         for (int i = 0; i < index.length; i++) v[i] = vector[index[i]];
-        return new DoubleVectorImpl(name, v);
+        return new DoubleVectorImpl(field(), v);
     }
 
     @Override

File: data/src/main/java/smile/data/vector/FloatVectorImpl.java
Patch:
@@ -91,7 +91,7 @@ public Float get(int i) {
     public FloatVector get(int... index) {
         float[] v = new float[index.length];
         for (int i = 0; i < index.length; i++) v[i] = vector[index[i]];
-        return new FloatVectorImpl(name, v);
+        return new FloatVectorImpl(field(), v);
     }
 
     @Override

File: data/src/main/java/smile/data/vector/IntVectorImpl.java
Patch:
@@ -102,7 +102,7 @@ public Integer get(int i) {
     public IntVector get(int... index) {
         int[] v = new int[index.length];
         for (int i = 0; i < index.length; i++) v[i] = vector[index[i]];
-        return new IntVectorImpl(name, v);
+        return new IntVectorImpl(field(), v);
     }
 
     @Override

File: data/src/main/java/smile/data/vector/LongVectorImpl.java
Patch:
@@ -91,7 +91,7 @@ public Long get(int i) {
     public LongVector get(int... index) {
         long[] v = new long[index.length];
         for (int i = 0; i < index.length; i++) v[i] = vector[index[i]];
-        return new LongVectorImpl(name, v);
+        return new LongVectorImpl(field(), v);
     }
 
     @Override

File: data/src/main/java/smile/data/vector/ShortVectorImpl.java
Patch:
@@ -96,7 +96,7 @@ public Short get(int i) {
     public ShortVector get(int... index) {
         short[] v = new short[index.length];
         for (int i = 0; i < index.length; i++) v[i] = vector[index[i]];
-        return new ShortVectorImpl(name, v);
+        return new ShortVectorImpl(field(), v);
     }
 
     @Override

File: data/src/main/java/smile/data/vector/StringVectorImpl.java
Patch:
@@ -55,7 +55,7 @@ public StringVectorImpl(StructField field, String[] vector) {
     public StringVector get(int... index) {
         String[] v = new String[index.length];
         for (int i = 0; i < index.length; i++) v[i] = get(index[i]);
-        return new StringVectorImpl(name(), v);
+        return new StringVectorImpl(field(), v);
     }
 
     @Override

File: data/src/main/java/smile/data/vector/VectorImpl.java
Patch:
@@ -112,7 +112,7 @@ public Vector<T> get(int... index) {
         @SuppressWarnings("unchecked")
         T[] v = (T[]) java.lang.reflect.Array.newInstance(vector.getClass().getComponentType(), index.length);
         for (int i = 0; i < index.length; i++) v[i] = vector[index[i]];
-        return new VectorImpl<>(name, type, v);
+        return new VectorImpl<>(field(), v);
     }
 
     @Override

File: math/src/main/java/smile/math/matrix/FloatMatrix.java
Patch:
@@ -1202,10 +1202,10 @@ private Transpose flip(Transpose trans) {
      * </code></pre>
      */
     public void mm(Transpose transA, Transpose transB, float alpha, FloatMatrix B, float beta, FloatMatrix C) {
-        if (isSymmetric()) {
+        if (isSymmetric() && transB == NO_TRANSPOSE) {
             BLAS.engine.symm(C.layout(), LEFT, uplo, C.m, C.n, alpha, A, ld, B.A, B.ld, beta, C.A, C.ld);
-        } else if (B.isSymmetric()) {
-            BLAS.engine.symm(C.layout(), RIGHT, uplo, C.m, C.n, alpha, B.A, B.ld, A, ld, beta, C.A, C.ld);
+        } else if (B.isSymmetric() && transA == NO_TRANSPOSE) {
+            BLAS.engine.symm(C.layout(), RIGHT, B.uplo, C.m, C.n, alpha, B.A, B.ld, A, ld, beta, C.A, C.ld);
         } else {
             if (C.layout() != layout()) transA = flip(transA);
             if (C.layout() != B.layout()) transB = flip(transB);

File: math/src/main/java/smile/math/matrix/Matrix.java
Patch:
@@ -1201,10 +1201,10 @@ private Transpose flip(Transpose trans) {
      * </code></pre>
      */
     public void mm(Transpose transA, Transpose transB, double alpha, Matrix B, double beta, Matrix C) {
-        if (isSymmetric()) {
+        if (isSymmetric() && transB == NO_TRANSPOSE) {
             BLAS.engine.symm(C.layout(), LEFT, uplo, C.m, C.n, alpha, A, ld, B.A, B.ld, beta, C.A, C.ld);
-        } else if (B.isSymmetric()) {
-            BLAS.engine.symm(C.layout(), RIGHT, uplo, C.m, C.n, alpha, B.A, B.ld, A, ld, beta, C.A, C.ld);
+        } else if (B.isSymmetric() && transA == NO_TRANSPOSE) {
+            BLAS.engine.symm(C.layout(), RIGHT, B.uplo, C.m, C.n, alpha, B.A, B.ld, A, ld, beta, C.A, C.ld);
         } else {
             if (C.layout() != layout()) transA = flip(transA);
             if (C.layout() != B.layout()) transB = flip(transB);

File: core/src/main/java/smile/neighbor/KDTree.java
Patch:
@@ -266,7 +266,7 @@ private void search(double[] q, Node node, NeighborBuilder<double[], E> neighbor
             search(q, nearer, neighbor);
 
             // now look in further half
-            if (neighbor.distance >= diff) {
+            if (neighbor.distance >= Math.abs(diff)) {
                 search(q, further, neighbor);
             }
         }
@@ -310,7 +310,7 @@ private void search(double[] q, Node node, HeapSelect<NeighborBuilder<double[],
             search(q, nearer, heap);
 
             // now look in further half
-            if (heap.peek().distance >= diff) {
+            if (heap.peek().distance >= Math.abs(diff)) {
                 search(q, further, heap);
             }
         }

File: core/src/main/java/smile/neighbor/KDTree.java
Patch:
@@ -266,7 +266,7 @@ private void search(double[] q, Node node, NeighborBuilder<double[], E> neighbor
             search(q, nearer, neighbor);
 
             // now look in further half
-            if (neighbor.distance >= diff) {
+            if (neighbor.distance >= Math.abs(diff)) {
                 search(q, further, neighbor);
             }
         }
@@ -310,7 +310,7 @@ private void search(double[] q, Node node, HeapSelect<NeighborBuilder<double[],
             search(q, nearer, heap);
 
             // now look in further half
-            if (heap.peek().distance >= diff) {
+            if (heap.peek().distance >= Math.abs(diff)) {
                 search(q, further, heap);
             }
         }

File: math/src/main/java/smile/math/matrix/DMatrix.java
Patch:
@@ -109,7 +109,7 @@ public void mv(double[] x, double[] y) {
 
     @Override
     public double[] tv(double[] x) {
-        double[] y = new double[nrows()];
+        double[] y = new double[ncols()];
         mv(TRANSPOSE, 1.0, x, 0.0, y);
         return y;
     }

File: math/src/main/java/smile/math/matrix/SMatrix.java
Patch:
@@ -109,7 +109,7 @@ public void mv(float[] x, float[] y) {
 
     @Override
     public float[] tv(float[] x) {
-        float[] y = new float[nrows()];
+        float[] y = new float[ncols()];
         mv(Transpose.TRANSPOSE, 1.0f, x, 0.0f, y);
         return y;
     }

File: math/src/main/java/smile/stat/distribution/MultivariateGaussianDistribution.java
Patch:
@@ -18,6 +18,7 @@
 package smile.stat.distribution;
 
 import smile.math.MathEx;
+import smile.math.blas.UPLO;
 import smile.math.matrix.Matrix;
 
 /**
@@ -165,6 +166,7 @@ public static MultivariateGaussianDistribution fit(double[][] data, boolean diag
      */
     private void init() {
         dim = mu.length;
+        sigma.uplo(UPLO.LOWER);
         Matrix.Cholesky cholesky = sigma.cholesky();
         sigmaInv = cholesky.inverse();
         sigmaDet = cholesky.det();

File: netlib/src/main/java/smile/netlib/NLMatrix.java
Patch:
@@ -205,7 +205,7 @@ public NLMatrix atbtmm(DenseMatrix B) {
             NLMatrix C = new NLMatrix(m, n);
             BLAS.getInstance().dgemm(Transpose, Transpose,
                     m, n, k, 1.0, data(), k, B.data(),
-                    k, 0.0, C.data(), m);
+                    n, 0.0, C.data(), m);
             return C;
         }
 

File: core/src/main/java/smile/classification/FLD.java
Patch:
@@ -57,7 +57,7 @@
  *
  * <h2>References</h2>
  * <ol>
- * <li> Robust and Accurate Cancer Classification with Gene Expression Profiling http://alumni.cs.ucr.edu/~hli/paper/hli05tumor.pdf.</li>
+ * <li> H. Li, K. Zhang, and T. Jiang. Robust and Accurate Cancer Classification with Gene Expression Profiling. CSB'05, pp 310-321.</li>
  * </ol>
  *
  * @see LDA

File: graph/src/main/java/smile/graph/AdjacencyMatrix.java
Patch:
@@ -23,6 +23,7 @@
 import java.util.LinkedList;
 import java.util.Queue;
 import smile.math.MathEx;
+import smile.math.matrix.DenseMatrix;
 import smile.math.matrix.Matrix;
 import smile.util.PriorityQueue;
 
@@ -636,7 +637,7 @@ public double pushRelabel(double[][] flow, int source, int sink) {
     }
 
     @Override
-    public Matrix toMatrix() {
+    public DenseMatrix toMatrix() {
         return Matrix.of(graph);
     }
 }

File: core/src/main/java/smile/manifold/NearestNeighborGraph.java
Patch:
@@ -66,7 +66,7 @@ public interface EdgeConsumer {
      * @param digraph flag to create a directed graph.
      * @param consumer an optional lambda to perform some side effect operations.
      */
-    public static Graph of(double[][] data, int k, boolean digraph, EdgeConsumer consumer) {
+    public static AdjacencyList of(double[][] data, int k, boolean digraph, EdgeConsumer consumer) {
         return of(data, new EuclideanDistance(), k, digraph ,consumer);
     }
 
@@ -79,12 +79,12 @@ public static Graph of(double[][] data, int k, boolean digraph, EdgeConsumer con
      * @param digraph flag to create a directed graph.
      * @param consumer an optional lambda to perform some side effect operations.
      */
-    public static <T> Graph of(T[] data, Distance<T> distance, int k, boolean digraph, EdgeConsumer consumer) {
+    public static <T> AdjacencyList of(T[] data, Distance<T> distance, int k, boolean digraph, EdgeConsumer consumer) {
         // This is actually faster on many core systems.
         LinearSearch<T> knn = new LinearSearch<>(data, distance);
 
         int n = data.length;
-        Graph graph = new AdjacencyList(n, digraph);
+        AdjacencyList graph = new AdjacencyList(n, digraph);
 
         if (consumer != null) {
             for (int i = 0; i < n; i++) {

File: core/src/main/java/smile/vq/SOM.java
Patch:
@@ -144,7 +144,7 @@ public Neuron(int i, int j, double[] w) {
      * The current iteration.
      */
     private int t = 0;
-    /**
+    /*
      * The threshold to update neuron if alpha * theta > eps.
      */
     private double eps = 1E-5;

File: math/src/main/java/smile/stat/distribution/KernelDensity.java
Patch:
@@ -41,7 +41,7 @@ public class KernelDensity implements Distribution {
      * integrates to one. Here we just Gaussian density function.
      */
     private GaussianDistribution gaussian;
-    /**
+    /*
      * h > 0 is a smoothing parameter called the bandwidth.
      */
     private double h;

File: netlib/src/main/java/smile/netlib/ARPACK.java
Patch:
@@ -84,7 +84,7 @@ public static EVD eigen(Matrix A, int k, String which) {
      * Find k approximate eigen pairs of a symmetric matrix by the
      * Lanczos algorithm.
      *
-     * @param k Number of eigenvalues of OP to be computed. 0 < k < N.
+     * @param k Number of eigenvalues of OP to be computed. 0 &lt; k &lt; N.
      * @param ritz Specify which of the Ritz values to compute.
      * @param kappa Relative accuracy of ritz values acceptable as eigenvalues.
      * @param maxIter Maximum number of iterations.
@@ -97,7 +97,7 @@ public static EVD eigen(Matrix A, int k, Ritz ritz, double kappa, int maxIter) {
      * Find k approximate eigen pairs of a symmetric matrix by the
      * Lanczos algorithm.
      *
-     * @param k Number of eigenvalues of OP to be computed. 0 < NEV < N.
+     * @param k Number of eigenvalues of OP to be computed. 0 &lt; NEV &lt; N.
      * @param which Specify which of the Ritz values to compute.
      * @param kappa Relative accuracy of ritz values acceptable as eigenvalues.
      * @param maxIter Maximum number of iterations.

File: core/src/main/java/smile/base/svm/LASVM.java
Patch:
@@ -71,7 +71,7 @@ public class LASVM<T> implements Serializable {
      * True if minmax() is already called after update.
      */
     private boolean minmaxflag = false;
-    /**
+    /*
      * Most violating pair.
      * argmin gi of m_i < alpha_i
      * argmax gi of alpha_i < M_i

File: core/src/main/java/smile/regression/RandomForest.java
Patch:
@@ -154,7 +154,7 @@ public static RandomForest fit(Formula formula, DataFrame data, Properties prop)
      * @param nodeSize the number of instances in a node below which the tree will
      *                 not split, nodeSize = 5 generally gives good results.
      * @param subsample the sampling rate for training tree. 1.0 means sampling with
-     *                  replacement. < 1.0 means sampling without replacement.
+     *                  replacement. &lt; 1.0 means sampling without replacement.
      */
     public static RandomForest fit(Formula formula, DataFrame data, int ntrees, int mtry, int maxDepth, int maxNodes, int nodeSize, double subsample) {
         return fit(formula, data, ntrees, mtry, maxDepth, maxNodes, nodeSize, subsample, null);
@@ -174,7 +174,7 @@ public static RandomForest fit(Formula formula, DataFrame data, int ntrees, int
      * @param nodeSize the number of instances in a node below which the tree will
      *                 not split, nodeSize = 5 generally gives good results.
      * @param subsample the sampling rate for training tree. 1.0 means sampling with
-     *                  replacement. < 1.0 means sampling without replacement.
+     *                  replacement. &lt; 1.0 means sampling without replacement.
      * @param seeds optional RNG seeds for each regression tree.
      */
     public static RandomForest fit(Formula formula, DataFrame data, int ntrees, int mtry, int maxDepth, int maxNodes, int nodeSize, double subsample, LongStream seeds) {

File: core/src/main/java/smile/vq/NeuralGas.java
Patch:
@@ -115,7 +115,7 @@ public Neuron(int i, double[] w) {
      * The current iteration.
      */
     private int t = 0;
-    /**
+    /*
      * The threshold to update neuron if alpha * theta > eps.
      */
     private double eps = 1E-7;

File: io/src/main/java/smile/io/HadoopInput.java
Patch:
@@ -30,7 +30,7 @@
 
 /**
  * Static methods that return the InputStream/Reader of a HDFS/S3.
- * Local files, HTTP & FTP URLs are supported too.
+ * Local files, HTTP and FTP URLs are supported too.
  *
  * @author Haifeng Li
  */

File: math/src/main/java/smile/math/distance/EditDistance.java
Patch:
@@ -61,7 +61,7 @@ public class EditDistance implements Metric<String> {
     /**
      * Cost matrix. Because Java automatically initialize arrays, it
      * takes O(mn) to declare this cost matrix every time before
-     * calculate edit distance. But the whole point of Berghel & Roach
+     * calculate edit distance. But the whole point of Berghel and Roach
      * algorithm is to calculate fewer cells than O(mn). Therefore,
      * we create this cost matrix here. Therefore, the methods using
      * this cost matrix is not multi-thread safe.

File: math/src/main/java/smile/util/IntDoubleHashMap.java
Patch:
@@ -20,7 +20,7 @@
 import java.util.Arrays;
 
 /**
- * HashMap<int, double> for primitive types.
+ * HashMap&lt;int, double&gt; for primitive types.
  * Integer.MIN_VALUE (0x80000000) is not allowed as key.
  */
 public class IntDoubleHashMap {

File: netlib/src/main/java/smile/netlib/ARPACK.java
Patch:
@@ -62,7 +62,7 @@ public enum Ritz {
      * Find k approximate eigen pairs of a symmetric matrix by the
      * Lanczos algorithm.
      *
-     * @param k Number of eigenvalues of OP to be computed. 0 < k < N.
+     * @param k Number of eigenvalues of OP to be computed. 0 &lt; k &lt; N.
      * @param ritz Specify which of the Ritz values to compute.
      */
     public static EVD eigen(Matrix A, int k, Ritz ritz) {
@@ -73,7 +73,7 @@ public static EVD eigen(Matrix A, int k, Ritz ritz) {
      * Find k approximate eigen pairs of a symmetric matrix by the
      * Lanczos algorithm.
      *
-     * @param k Number of eigenvalues of OP to be computed. 0 < k < N.
+     * @param k Number of eigenvalues of OP to be computed. 0 &lt; k &lt; N.
      * @param which Specify which of the Ritz values to compute.
      */
     public static EVD eigen(Matrix A, int k, String which) {
@@ -200,4 +200,4 @@ private static void av(Matrix A, double[] work, int inputOffset, int outputOffse
         A.ax(x, y);
         System.arraycopy(y, 0, work, outputOffset, n);
     }
-}
\ No newline at end of file
+}

File: core/src/main/java/smile/classification/GradientTreeBoost.java
Patch:
@@ -141,7 +141,7 @@ public class GradientTreeBoost implements SoftClassifier<Tuple>, DataFrameClassi
     /**
      * The shrinkage parameter in (0, 1] controls the learning rate of procedure.
      */
-    private double shrinkage = 0.005;
+    private double shrinkage = 0.05;
     /**
      * The class label encoder.
      */

File: core/src/main/java/smile/regression/GradientTreeBoost.java
Patch:
@@ -134,7 +134,7 @@ public class GradientTreeBoost implements Regression<Tuple>, DataFrameRegression
     /**
      * The shrinkage parameter in (0, 1] controls the learning rate of procedure.
      */
-    private double shrinkage = 0.005;
+    private double shrinkage = 0.05;
 
     /**
      * Constructor. Learns a gradient tree boosting for regression.

File: core/src/main/java/smile/association/ARM.java
Patch:
@@ -41,7 +41,7 @@
  * Association rules are usually required to satisfy a user-specified minimum
  * support and a user-specified minimum confidence at the same time.
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class ARM implements Iterable<AssociationRule> {
 

File: core/src/main/java/smile/association/AssociationRule.java
Patch:
@@ -36,7 +36,7 @@
  * the probability of finding the RHS of the rule in transactions under the
  * condition that these transactions also contain the LHS.
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class AssociationRule {
 

File: core/src/main/java/smile/association/FPGrowth.java
Patch:
@@ -56,7 +56,7 @@
  * <li> Christian Borgelt. An Implementation of the FP-growth Algorithm. OSDM, 1-5, 2005.</li>
  * </ol>
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class FPGrowth implements Iterable<ItemSet> {
     /**

File: core/src/main/java/smile/association/FPTree.java
Patch:
@@ -35,7 +35,7 @@
  * states the total number of occurrences of the item in the
  * database.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class FPTree {
 

File: core/src/main/java/smile/association/ItemSet.java
Patch:
@@ -25,7 +25,7 @@
  * In this class, the support is actually the raw frequency rather than the
  * ratio.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class ItemSet {
 

File: core/src/main/java/smile/association/TotalSupportTree.java
Patch:
@@ -32,7 +32,7 @@
  * <li> Frans Coenen, Paul Leng, and Shakil Ahmed. Data Structure for Association Rule Mining: T-Trees and P-Trees. IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, 16(6):774-778, 2004.</li>
  * </ol>
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 class TotalSupportTree implements Iterable<ItemSet> {
 

File: core/src/main/java/smile/association/package-info.java
Patch:
@@ -92,6 +92,6 @@
  * <li> Jiawei Han, Jian Pei, Yiwen Yin, and Runying Mao. Mining frequent patterns without candidate generation. Data Mining and Knowledge Discovery 8:53-87, 2004.</li>
  * </ol>
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 package smile.association;

File: core/src/main/java/smile/base/mlp/ActivationFunction.java
Patch:
@@ -24,7 +24,7 @@
 /**
  * The activation function in hidden layers.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public interface ActivationFunction extends Serializable {
 

File: core/src/main/java/smile/base/mlp/MultilayerPerceptron.java
Patch:
@@ -33,7 +33,7 @@
  * transformation, called activation function, is a bounded non-decreasing
  * (non-linear) function.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public abstract class MultilayerPerceptron implements Serializable {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/base/mlp/OutputFunction.java
Patch:
@@ -22,7 +22,7 @@
 /**
  * The output function of neural networks.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public enum OutputFunction {
     /**

File: core/src/main/java/smile/base/package-info.java
Patch:
@@ -18,6 +18,6 @@
 /**
  * Base classes used by both classification and regression.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 package smile.base;
\ No newline at end of file

File: core/src/main/java/smile/base/rbf/RBF.java
Patch:
@@ -48,7 +48,7 @@
  * with weight parameters undergo a supervised learning processing
  * (e.g. error-correction learning).
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class RBF<T> implements Serializable {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/base/svm/KernelMachine.java
Patch:
@@ -34,7 +34,7 @@
  * a new representation) and learn for it a corresponding weight. Prediction
  * for unlabeled inputs is treated by the application of a similiarity function.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class KernelMachine<T> implements Serializable {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/base/svm/SVR.java
Patch:
@@ -41,7 +41,7 @@
  * <li> Chih-Chung Chang and Chih-Jen Lin. LIBSVM: a Library for Support Vector Machines.</li>
  * </ol>
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class SVR<T> {
     private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(SVR.class);

File: core/src/main/java/smile/classification/AdaBoost.java
Patch:
@@ -59,7 +59,7 @@
  * <li> Ji Zhu, Hui Zhou, Saharon Rosset and Trevor Hastie. Multi-class Adaboost, 2009.</li>
  * </ol>
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class AdaBoost implements SoftClassifier<Tuple>, DataFrameClassifier {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/classification/ClassLabels.java
Patch:
@@ -31,7 +31,7 @@
 /**
  * To support arbitrary class labels.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class ClassLabels implements Serializable {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/classification/Classifier.java
Patch:
@@ -37,7 +37,7 @@
  * 
  * @param <T> the type of input object
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public interface Classifier<T> extends ToIntFunction<T>, ToDoubleFunction<T>, Serializable {
     /**

File: core/src/main/java/smile/classification/DataFrameClassifier.java
Patch:
@@ -25,7 +25,7 @@
 /**
  * Classification trait on DataFrame.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public interface DataFrameClassifier {
 

File: core/src/main/java/smile/classification/DecisionTree.java
Patch:
@@ -99,7 +99,7 @@
  * @see GradientTreeBoost
  * @see RandomForest
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class DecisionTree extends CART implements SoftClassifier<Tuple>, DataFrameClassifier {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/classification/DiscreteNaiveBayes.java
Patch:
@@ -80,7 +80,7 @@
  * <li> Kevin P. Murphy. Machina Learning A Probability Perspective, Chapter 3, 2012.</li>
  * </ol>
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class DiscreteNaiveBayes implements OnlineClassifier<int[]>, SoftClassifier<int[]> {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/classification/FLD.java
Patch:
@@ -63,7 +63,7 @@
  * @see LDA
  * @see smile.projection.PCA
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class FLD implements Classifier<double[]>, Projection<double[]> {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/classification/GradientTreeBoost.java
Patch:
@@ -105,7 +105,7 @@
  * <li> J. H. Friedman. Stochastic Gradient Boosting, 1999.</li>
  * </ol>
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class GradientTreeBoost implements SoftClassifier<Tuple>, DataFrameClassifier {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/classification/IsotonicRegressionScaling.java
Patch:
@@ -35,7 +35,7 @@
  * <li>Alexandru Niculescu-Mizil and Rich Caruana. Predicting Good Probabilities With Supervised Learning. ICML, 2005.</li>
  * </ol>
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class IsotonicRegressionScaling implements Serializable {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/classification/KNN.java
Patch:
@@ -66,7 +66,7 @@
  * guaranteed to approach the Bayes error rate, for some value of k (where k
  * increases as a function of the number of data points).
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class KNN<T> implements SoftClassifier<T> {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/classification/LDA.java
Patch:
@@ -57,7 +57,7 @@
  * @see RDA
  * @see NaiveBayes
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class LDA implements SoftClassifier<double[]> {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/classification/LogisticRegression.java
Patch:
@@ -74,7 +74,7 @@
  * @see Maxent
  * @see LDA
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class LogisticRegression implements SoftClassifier<double[]>, OnlineClassifier<double[]> {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/classification/MLP.java
Patch:
@@ -102,7 +102,7 @@
  * the neural network for categorical target variables, the outputs
  * can be interpreted as posterior probabilities, which are very useful.
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class MLP extends MultilayerPerceptron implements OnlineClassifier<double[]>, SoftClassifier<double[]>, Serializable {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/classification/Maxent.java
Patch:
@@ -47,7 +47,7 @@
  * <li> A. L. Berger, S. D. Pietra, and V. J. D. Pietra. A maximum entropy approach to natural language processing. Computational Linguistics 22(1):39-71, 1996.</li>
  * </ol>
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class Maxent implements SoftClassifier<int[]>, OnlineClassifier<int[]> {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/classification/NaiveBayes.java
Patch:
@@ -48,7 +48,7 @@
  * <li> Kevin P. Murphy. Machina Learning A Probability Perspective, Chapter 3, 2012.</li>
  * </ol>
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class NaiveBayes implements SoftClassifier<double[]> {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/classification/OnlineClassifier.java
Patch:
@@ -24,7 +24,7 @@
  * 
  * @param <T> the type of input object
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public interface OnlineClassifier <T> extends Classifier <T> {
     /**

File: core/src/main/java/smile/classification/PlattScaling.java
Patch:
@@ -39,7 +39,7 @@
  * <li> John Platt. Probabilistic Outputs for Support Vector Machines and Comparisons to Regularized Likelihood Methods. Advances in large margin classifiers. 10 (3): 61–74.</li>
  * </ol>
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class PlattScaling implements Serializable {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/classification/QDA.java
Patch:
@@ -46,7 +46,7 @@
  * @see RDA
  * @see NaiveBayes
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class QDA implements SoftClassifier<double[]> {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/classification/RBFNetwork.java
Patch:
@@ -88,7 +88,7 @@
  * @see SVM
  * @see MLP
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class RBFNetwork<T> implements Classifier<T> {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/classification/RDA.java
Patch:
@@ -40,7 +40,7 @@
  * @see LDA
  * @see QDA
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class RDA extends QDA {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/classification/RandomForest.java
Patch:
@@ -71,7 +71,7 @@
  * not reliable for this type of data.
  * </ul>
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class RandomForest implements SoftClassifier<Tuple>, DataFrameClassifier {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/classification/SVM.java
Patch:
@@ -81,7 +81,7 @@
  * @see OneVersusOne
  * @see OneVersusRest
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class SVM<T> extends KernelMachine<T> implements Classifier<T> {
     /**

File: core/src/main/java/smile/classification/SoftClassifier.java
Patch:
@@ -23,7 +23,7 @@
  *
  * @param <T> the type of input object
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public interface SoftClassifier<T> extends Classifier<T> {
     /**

File: core/src/main/java/smile/classification/package-info.java
Patch:
@@ -126,6 +126,6 @@
  * between bias and variance.</dd>
  * </dl>
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 package smile.classification;

File: core/src/main/java/smile/clustering/BBDTree.java
Patch:
@@ -47,7 +47,7 @@
  * @see KMeans
  * @see smile.vq.SOM
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class BBDTree {
 

File: core/src/main/java/smile/clustering/CLARANS.java
Patch:
@@ -49,7 +49,7 @@
  * 
  * @param <T> the type of input object.
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class CLARANS<T> extends CentroidClustering<T, T> {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/clustering/CentroidClustering.java
Patch:
@@ -46,7 +46,7 @@
  * @param <U> the tpe of observations. Usually, T and U are the same.
  *            But in case of SIB, they are different.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public abstract class CentroidClustering<T, U> extends PartitionClustering implements Comparable<CentroidClustering<T, U>> {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/clustering/DBSCAN.java
Patch:
@@ -86,7 +86,7 @@
  * 
  * @param <T> the type of input object.
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class DBSCAN<T> extends PartitionClustering {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/clustering/DENCLUE.java
Patch:
@@ -40,7 +40,7 @@
  * <li> Alexander Hinneburg and Hans-Henning Gabriel. DENCLUE 2.0: Fast Clustering based on Kernel Density Estimation. IDA, 2007.</li>
  * </ol>
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class DENCLUE extends PartitionClustering {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/clustering/DeterministicAnnealing.java
Patch:
@@ -42,7 +42,7 @@
  * <li> Kenneth Rose. Deterministic Annealing for Clustering, Compression, Classification, Regression, and Speech Recognition. </li>
  * </ol>
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class DeterministicAnnealing extends CentroidClustering<double[], double[]> {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/clustering/FastPair.java
Patch:
@@ -45,7 +45,7 @@
  * 
  * @see HierarchicalClustering
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 class FastPair {
 

File: core/src/main/java/smile/clustering/GMeans.java
Patch:
@@ -39,7 +39,7 @@
  * @see KMeans
  * @see XMeans
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class GMeans extends CentroidClustering<double[], double[]> {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/clustering/HierarchicalClustering.java
Patch:
@@ -51,7 +51,7 @@
  * 
  * @see Linkage
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class HierarchicalClustering implements Serializable {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/clustering/KMeans.java
Patch:
@@ -75,7 +75,7 @@
  * @see smile.vq.NeuralGas
  * @see BBDTree
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class KMeans extends CentroidClustering<double[], double[]> {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/clustering/KModes.java
Patch:
@@ -34,7 +34,7 @@
  *
  * @see KMeans
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class KModes extends CentroidClustering<int[], int[]> {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/clustering/MEC.java
Patch:
@@ -53,7 +53,7 @@
  * <li> Haifeng Li. All rights reserved., Keshu Zhang, and Tao Jiang. Minimum Entropy Clustering and Applications to Gene Expression Analysis. CSB, 2004. </li>
  * </ol>
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class MEC<T> extends PartitionClustering implements Comparable<MEC<T>> {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/clustering/PartitionClustering.java
Patch:
@@ -29,7 +29,7 @@
  * Partition clustering. Partition methods classify the observations
  * into distinct non-overlapping groups.
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public abstract class PartitionClustering implements Serializable {
     /**

File: core/src/main/java/smile/clustering/SIB.java
Patch:
@@ -54,7 +54,7 @@
  * <li>Jaakko Peltonen, Janne Sinkkonen, and Samuel Kaski. Sequential information bottleneck for finite data. ICML, 2004.</li>
  * </ol>
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class SIB extends CentroidClustering<double[], SparseArray> {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/clustering/SpectralClustering.java
Patch:
@@ -43,7 +43,7 @@
  * <li> Deepak Verma and Marina Meila. A Comparison of Spectral Clustering Algorithms. 2003. </li>
  * </ol>
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class SpectralClustering extends PartitionClustering implements Serializable {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/clustering/XMeans.java
Patch:
@@ -41,7 +41,7 @@
  * @see KMeans
  * @see GMeans
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class XMeans extends CentroidClustering<double[], double[]> {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/clustering/linkage/CompleteLinkage.java
Patch:
@@ -26,7 +26,7 @@
  * groups is now defined as the distance between the most distant pair of
  * objects, one from each group.
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class CompleteLinkage extends Linkage {
     /**

File: core/src/main/java/smile/clustering/linkage/Linkage.java
Patch:
@@ -30,7 +30,7 @@
  * 
  * @see smile.clustering.HierarchicalClustering
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public abstract class Linkage {
     /** The data size. */

File: core/src/main/java/smile/clustering/linkage/SingleLinkage.java
Patch:
@@ -33,7 +33,7 @@
  * trees what matters is the set of pairs of points that form distances chosen
  * by the algorithm.
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class SingleLinkage extends Linkage {
     /**

File: core/src/main/java/smile/clustering/linkage/UPGMALinkage.java
Patch:
@@ -30,7 +30,7 @@
  * relationships unless this assumption has been tested and justified
  * for the data set being used.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class UPGMALinkage extends Linkage {
     /**

File: core/src/main/java/smile/clustering/linkage/UPGMCLinkage.java
Patch:
@@ -25,7 +25,7 @@
  * centroids, as calculated by arithmetic mean. Only valid for Euclidean
  * distance based proximity matrix.
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class UPGMCLinkage extends Linkage {
     /**

File: core/src/main/java/smile/clustering/linkage/WPGMALinkage.java
Patch:
@@ -29,7 +29,7 @@
  * produces a weighted result, and the proportional averaging in UPGMA produces
  * an unweighted result.
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class WPGMALinkage extends Linkage {
     /**

File: core/src/main/java/smile/clustering/linkage/WPGMCLinkage.java
Patch:
@@ -24,7 +24,7 @@
  * The distance between two clusters is the Euclidean distance between their
  * weighted centroids. Only valid for Euclidean distance based proximity matrix.
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class WPGMCLinkage extends Linkage {
     /**

File: core/src/main/java/smile/clustering/linkage/WardLinkage.java
Patch:
@@ -27,7 +27,7 @@
  * clustering steps so as to minimize the increase in ESS at each step.
  * Note that it is only valid for Euclidean distance based proximity matrix.
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class WardLinkage extends Linkage {
     /**

File: core/src/main/java/smile/clustering/linkage/package-info.java
Patch:
@@ -31,6 +31,6 @@
  * <li> Ward's linkage</li>
  * </ul>
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 package smile.clustering.linkage;

File: core/src/main/java/smile/clustering/package-info.java
Patch:
@@ -51,6 +51,6 @@
  * usually do not however work with arbitrary feature combinations as in general
  * subspace methods.
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 package smile.clustering;

File: core/src/main/java/smile/feature/Bag.java
Patch:
@@ -26,7 +26,7 @@
  * sentence or a document) is represented as an unordered collection of words,
  * disregarding grammar and even word order.
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class Bag {
     /**

File: core/src/main/java/smile/feature/FeatureRanking.java
Patch:
@@ -20,7 +20,7 @@
 /**
  * Univariate feature ranking metric.
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public interface FeatureRanking {
     

File: core/src/main/java/smile/feature/FeatureTransform.java
Patch:
@@ -26,7 +26,7 @@
  * standardization of the data set. If some outliers are present in the
  * set, robust transformers are more appropriate.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public interface FeatureTransform extends Serializable {
 

File: core/src/main/java/smile/feature/GAFE.java
Patch:
@@ -45,7 +45,7 @@
  * <li> Leping Li and Clarice R. Weinberg. Gene Selection and Sample Classification Using a Genetic Algorithm/k-Nearest Neighbor Method.</li>
  * </ol>
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class GAFE {
     /**

File: core/src/main/java/smile/feature/MaxAbsScaler.java
Patch:
@@ -35,7 +35,7 @@
  * of each feature in the training set will be 1.0. It does not shift/center
  * the data, and thus does not destroy any sparsity.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class MaxAbsScaler implements FeatureTransform {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/feature/Normalizer.java
Patch:
@@ -29,7 +29,7 @@
  * Scaling inputs to unit norms is a common operation for text
  * classification or clustering for instance.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class Normalizer implements FeatureTransform {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/feature/RobustStandardizer.java
Patch:
@@ -26,7 +26,7 @@
  * Robustly standardizes numeric feature by subtracting
  * the median and dividing by the IQR.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class RobustStandardizer extends Standardizer {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/feature/Scaler.java
Patch:
@@ -38,7 +38,7 @@
  * specified range is indicate in terms of percentiles of the original
  * distribution (like the 5th and 95th percentile).
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class Scaler implements FeatureTransform {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/feature/SignalNoiseRatio.java
Patch:
@@ -34,7 +34,7 @@
  * <li> M. Shipp, et al. Diffuse large B-cell lymphoma outcome prediction by gene-expression profiling and supervised machine learning. Nature Medicine, 2002.</li>
  * </ol>
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class SignalNoiseRatio implements FeatureRanking {
     public static final SignalNoiseRatio instance = new SignalNoiseRatio();

File: core/src/main/java/smile/feature/SparseOneHotEncoder.java
Patch:
@@ -30,7 +30,7 @@
  * elements are stored in an integer array. In Maximum Entropy Classifier,
  * the data are expected to store in this format.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class SparseOneHotEncoder {
     /**

File: core/src/main/java/smile/feature/Standardizer.java
Patch:
@@ -35,7 +35,7 @@
  * A robust alternative is to subtract the median and divide by the IQR
  * by <code>RobustStandardizer</code>.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class Standardizer implements FeatureTransform {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/feature/SumSquaresRatio.java
Patch:
@@ -35,7 +35,7 @@
  * <li> S. Dudoit, J. Fridlyand and T. Speed. Comparison of discrimination methods for the classification of tumors using gene expression data. J Am Stat Assoc, 97:77-87, 2002.</li>
  * </ol>
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class SumSquaresRatio implements FeatureRanking {
     public static final SumSquaresRatio instance = new SumSquaresRatio();

File: core/src/main/java/smile/feature/WinsorScaler.java
Patch:
@@ -32,7 +32,7 @@
  * specified range is indicate in terms of percentiles of the original
  * distribution (like the 5th and 95th percentile).
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class WinsorScaler extends Scaler {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/feature/package-info.java
Patch:
@@ -47,6 +47,6 @@
  * numbers of features are available. Commonly, heuristic methods such as
  * genetic algorithms are employed for subset selection.
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 package smile.feature;
\ No newline at end of file

File: core/src/main/java/smile/gap/BitString.java
Patch:
@@ -43,7 +43,7 @@
  * occur very often, because then GA will in fact change to random search.
  * Best rates reported are about 0.5% - 1%.
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class BitString implements Chromosome {
 

File: core/src/main/java/smile/gap/Chromosome.java
Patch:
@@ -22,7 +22,7 @@
  * solutions to an optimization problem. Note that chromosomes have to
  * implement Comparable interface to support comparison of their fitness.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public interface Chromosome extends Comparable<Chromosome> {
 

File: core/src/main/java/smile/gap/FitnessMeasure.java
Patch:
@@ -20,7 +20,7 @@
 /**
  * A measure to evaluate the fitness of chromosomes.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public interface FitnessMeasure <T extends Chromosome> {
 

File: core/src/main/java/smile/gap/GeneticAlgorithm.java
Patch:
@@ -114,7 +114,7 @@
  * t is very small, then we're spending more time in the outer algorithm and
  * thus doing more exploring.
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class GeneticAlgorithm <T extends Chromosome> {
     private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(GeneticAlgorithm.class);

File: core/src/main/java/smile/gap/LamarckianChromosome.java
Patch:
@@ -25,7 +25,7 @@
  * as it is being assessed. The revised individual replaces the original one
  * in the population.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public interface LamarckianChromosome extends Chromosome {
     /**

File: core/src/main/java/smile/gap/package-info.java
Patch:
@@ -104,6 +104,6 @@
  * a plateau such that successive iterations no longer produce better results</li>
  * </ul>
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 package smile.gap;

File: core/src/main/java/smile/imputation/AverageImputation.java
Patch:
@@ -24,7 +24,7 @@
  * non-missing attributes in the same instance. Note that this is not the
  * average of same attribute across different instances.
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class AverageImputation implements MissingValueImputation {
     /**

File: core/src/main/java/smile/imputation/KMeansImputation.java
Patch:
@@ -25,7 +25,7 @@
  * with missing values and then impute missing values with the average value of each attribute
  * in the clusters.
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class KMeansImputation implements MissingValueImputation {
 

File: core/src/main/java/smile/imputation/KNNImputation.java
Patch:
@@ -31,7 +31,7 @@
  * In the weighted average, the contribution of each instance is weighted by
  * similarity between it and instance A.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class KNNImputation implements MissingValueImputation {
 

File: core/src/main/java/smile/imputation/LLSImputation.java
Patch:
@@ -28,7 +28,7 @@
  * a linear combination of similar instances, which are selected by k-nearest
  * neighbors method. 
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class LLSImputation implements MissingValueImputation {
     /**

File: core/src/main/java/smile/imputation/MissingValueImputationException.java
Patch:
@@ -20,7 +20,7 @@
 /**
  * Exception of missing value imputation.
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class MissingValueImputationException extends Exception {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/imputation/SVDImputation.java
Patch:
@@ -42,7 +42,7 @@
  * obtained matrix, until the total change in the matrix falls below the
  * empirically determined threshold (say 0.01).
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class SVDImputation implements MissingValueImputation {
 

File: core/src/main/java/smile/imputation/package-info.java
Patch:
@@ -96,6 +96,6 @@
  * better performance in cases where the missing data is structurally absent,
  * rather than missing due to measurement noise.
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 package smile.imputation;

File: core/src/main/java/smile/manifold/IsoMap.java
Patch:
@@ -67,7 +67,7 @@
  * <li> J. B. Tenenbaum, V. de Silva and J. C. Langford  A Global Geometric Framework for Nonlinear Dimensionality Reduction. Science 290(5500):2319-2323, 2000. </li> 
  * </ol>
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class IsoMap implements Serializable {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/manifold/LLE.java
Patch:
@@ -50,7 +50,7 @@
  * <li> Sam T. Roweis and Lawrence K. Saul. Nonlinear Dimensionality Reduction by Locally Linear Embedding. Science 290(5500):2323-2326, 2000. </li>
  * </ol>
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class LLE implements Serializable {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/manifold/LaplacianEigenmap.java
Patch:
@@ -50,7 +50,7 @@
  * <li> Mikhail Belkin and Partha Niyogi. Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering. NIPS, 2001. </li>
  * </ol>
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class LaplacianEigenmap implements Serializable {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/manifold/TSNE.java
Patch:
@@ -50,7 +50,7 @@
  * <li>L.J.P. van der Maaten and G.E. Hinton. Visualizing High-Dimensional Data Using t-SNE. Journal of Machine Learning Research 9(Nov):2579-2605, 2008. </li>
  * </ol>
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class TSNE {
     private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(TSNE.class);

File: core/src/main/java/smile/manifold/package-info.java
Patch:
@@ -48,6 +48,6 @@
  * minimizes the divergence between distributions over pairs of points),
  * and curvilinear component analysis.
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 package smile.manifold;
\ No newline at end of file

File: core/src/main/java/smile/mds/IsotonicMDS.java
Patch:
@@ -34,7 +34,7 @@
  * distances is usually not possible. The relationship is typically found
  * using isotonic regression.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class IsotonicMDS {
     private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(IsotonicMDS.class);

File: core/src/main/java/smile/mds/MDS.java
Patch:
@@ -34,7 +34,7 @@
  * @see smile.projection.PCA
  * @see SammonMapping
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class MDS {
     private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(MDS.class);

File: core/src/main/java/smile/mds/SammonMapping.java
Patch:
@@ -54,7 +54,7 @@
  * @see MDS
  * @see smile.projection.KPCA
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class SammonMapping {
     private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(SammonMapping.class);

File: core/src/main/java/smile/mds/package-info.java
Patch:
@@ -48,6 +48,6 @@
  * into another.</dd>
  * </dl>
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 package smile.mds;

File: core/src/main/java/smile/neighbor/BKTree.java
Patch:
@@ -52,7 +52,7 @@
  *
  * @param <E> the type of data objects in the tree.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class BKTree<E> implements RNNSearch<E, E>, Serializable {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/neighbor/CoverTree.java
Patch:
@@ -53,7 +53,7 @@
  *
  * @param <E> the type of data objects in the tree.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class CoverTree<E> implements NearestNeighborSearch<E, E>, KNNSearch<E, E>, RNNSearch<E, E>, Serializable {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/neighbor/KDTree.java
Patch:
@@ -51,7 +51,7 @@
  *
  * @param <E> the type of data objects in the tree.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class KDTree <E> implements NearestNeighborSearch<double[], E>, KNNSearch<double[], E>, RNNSearch<double[], E>, Serializable {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/neighbor/KNNSearch.java
Patch:
@@ -27,7 +27,7 @@
  * @param <K> the type of keys.
  * @param <V> the type of associated objects.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public interface KNNSearch<K, V> {
     /**

File: core/src/main/java/smile/neighbor/LSH.java
Patch:
@@ -47,7 +47,7 @@
  *
  * @param <E> the type of data objects in the hash table.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class LSH <E> implements NearestNeighborSearch<double[], E>, KNNSearch<double[], E>, RNNSearch<double[], E>, Serializable {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/neighbor/LinearSearch.java
Patch:
@@ -47,7 +47,7 @@
  *
  * @param <T> the type of data objects.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class LinearSearch<T> implements NearestNeighborSearch<T,T>, KNNSearch<T,T>, RNNSearch<T,T>, Serializable {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/neighbor/MPLSH.java
Patch:
@@ -48,7 +48,7 @@
  *
  * @param <E> the type of data objects in the hash table.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class MPLSH <E> extends LSH<E> {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/neighbor/NearestNeighborSearch.java
Patch:
@@ -32,7 +32,7 @@
  * @param <K> the type of keys.
  * @param <V> the type of associated objects.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public interface NearestNeighborSearch<K, V> {
     /**

File: core/src/main/java/smile/neighbor/Neighbor.java
Patch:
@@ -30,7 +30,7 @@
  * @param <K> the type of keys.
  * @param <V> the type of associated objects.
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class Neighbor<K, V> implements Comparable<Neighbor<K,V>> {
     /**

File: core/src/main/java/smile/neighbor/NeighborBuilder.java
Patch:
@@ -23,7 +23,7 @@
  * @param <K> the type of keys.
  * @param <V> the type of associated objects.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 class NeighborBuilder<K, V> implements Comparable<NeighborBuilder<K,V>> {
     /**

File: core/src/main/java/smile/neighbor/RNNSearch.java
Patch:
@@ -27,7 +27,7 @@
  * @param <K> the type of keys.
  * @param <V> the type of associated objects.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public interface RNNSearch<K, V> {
     /**

File: core/src/main/java/smile/neighbor/lsh/Bucket.java
Patch:
@@ -25,7 +25,7 @@
  * function g (function g is a vector of k LSH functions). A bucket is specified
  * by a vector in integers of length k.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class Bucket implements Serializable {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/neighbor/lsh/Hash.java
Patch:
@@ -25,7 +25,7 @@
 /**
  * The hash function for Euclidean spaces.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class Hash implements Serializable {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/neighbor/package-info.java
Patch:
@@ -54,6 +54,6 @@
  * doubling constant. The bound on search time is O(c12 log n) where c is
  * the expansion constant of the dataset.
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 package smile.neighbor;
\ No newline at end of file

File: core/src/main/java/smile/projection/GHA.java
Patch:
@@ -54,7 +54,7 @@
  * 
  * @see PCA
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class GHA implements LinearProjection, Serializable {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/projection/ICA.java
Patch:
@@ -56,7 +56,7 @@
  * <li>Aapo Hyvärinen, Erkki Oja: Independent component analysis: Algorithms and applications, 2000</li>
  * </ol>
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class ICA implements Serializable {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/projection/KPCA.java
Patch:
@@ -57,7 +57,7 @@
  * @see smile.manifold.LaplacianEigenmap
  * @see smile.mds.SammonMapping
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class KPCA<T> implements Projection<T>, Serializable {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/projection/PCA.java
Patch:
@@ -60,7 +60,7 @@
  * @see PPCA
  * @see GHA
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class PCA implements LinearProjection, Serializable {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/projection/PPCA.java
Patch:
@@ -44,7 +44,7 @@
  *
  * @see PCA
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class PPCA implements LinearProjection, Serializable {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/projection/Projection.java
Patch:
@@ -24,7 +24,7 @@
  * projection. However, kernel-based methods, e.g. Kernel PCA, can actually map
  * the data into a much higher dimensional space.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public interface Projection<T> {
     /**

File: core/src/main/java/smile/projection/RandomProjection.java
Patch:
@@ -52,7 +52,7 @@
  * <li> Chinmay Hegde, Michael Wakin, and Richard Baraniuk. Random projections for manifold learning. NIPS, 2007.</li>
  * </ol>
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class RandomProjection implements LinearProjection, Serializable {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/projection/package-info.java
Patch:
@@ -80,6 +80,6 @@
  * Restricted Boltzmann machines) that is followed by a finetuning stage based
  * on backpropagation.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 package smile.projection;
\ No newline at end of file

File: core/src/main/java/smile/regression/DataFrameRegression.java
Patch:
@@ -25,7 +25,7 @@
 /**
  * Regression trait on DataFrame.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public interface DataFrameRegression {
     /**

File: core/src/main/java/smile/regression/GaussianProcessRegression.java
Patch:
@@ -66,7 +66,7 @@
  * <li> Kai Zhang and James T. Kwok. Clustered Nystrom Method for Large Scale Manifold Learning and Dimension Reduction. IEEE Transactions on Neural Networks, 2010. </li>
  * <li> </li>
  * </ol>
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class GaussianProcessRegression {
     /**

File: core/src/main/java/smile/regression/GradientTreeBoost.java
Patch:
@@ -101,7 +101,7 @@
  * <li> J. H. Friedman. Stochastic Gradient Boosting, 1999.</li>
  * </ol>
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class GradientTreeBoost implements Regression<Tuple>, DataFrameRegression {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/regression/KernelMachine.java
Patch:
@@ -32,7 +32,7 @@
  * a new representation) and learn for it a corresponding weight. Prediction
  * for unlabeled inputs is treated by the application of a similiarity function.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class KernelMachine<T> extends smile.base.svm.KernelMachine<T> implements Regression<T> {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/regression/LASSO.java
Patch:
@@ -64,7 +64,7 @@
  * <li> Seung-Jean Kim, K. Koh, M. Lustig, Stephen Boyd, and Dimitry Gorinevsky. An Interior-Point Method for Large-Scale L1-Regularized Least Squares. IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING, VOL. 1, NO. 4, 2007.</li>
  * </ol>
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class LASSO {
     private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(LASSO.class);

File: core/src/main/java/smile/regression/LinearModel.java
Patch:
@@ -51,7 +51,7 @@
  * however, a central limit theorem can be invoked such that hypothesis
  * testing may proceed using asymptotic approximations.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class LinearModel implements OnlineRegression<double[]>, DataFrameRegression {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/regression/MLP.java
Patch:
@@ -31,7 +31,7 @@
  * transformation, called activation function, is a bounded non-decreasing
  * (non-linear) function.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
  public class MLP extends MultilayerPerceptron implements OnlineRegression<double[]> {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/regression/OLS.java
Patch:
@@ -73,7 +73,7 @@
  * however, a central limit theorem can be invoked such that hypothesis
  * testing may proceed using asymptotic approximations.
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class OLS {
     private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(OLS.class);

File: core/src/main/java/smile/regression/OnlineRegression.java
Patch:
@@ -24,7 +24,7 @@
  * 
  * @param <T> the type of input object
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public interface OnlineRegression <T> extends Regression<T> {
     /**

File: core/src/main/java/smile/regression/RBFNetwork.java
Patch:
@@ -72,7 +72,7 @@
  * @see RadialBasisFunction
  * @see SVR
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class RBFNetwork<T> implements Regression<T> {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/regression/RandomForest.java
Patch:
@@ -68,7 +68,7 @@
  * not reliable for this type of data.
  * </ul>
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class RandomForest implements Regression<Tuple>, DataFrameRegression {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/regression/Regression.java
Patch:
@@ -28,7 +28,7 @@
  * Regression analysis is widely used for prediction and forecasting, where
  * its use has substantial overlap with the field of machine learning. 
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public interface Regression<T> extends ToDoubleFunction<T>, Serializable {
     /**

File: core/src/main/java/smile/regression/RegressionTree.java
Patch:
@@ -72,7 +72,7 @@
  * Some techniques such as bagging, boosting, and random forest use more than
  * one decision tree for their analysis.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  * @see GradientTreeBoost
  * @see RandomForest
  */

File: core/src/main/java/smile/regression/RidgeRegression.java
Patch:
@@ -65,7 +65,7 @@
  * even though it performs well in terms of prediction accuracy,
  * it does poorly in terms of offering a clear interpretation.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class RidgeRegression {
     /**

File: core/src/main/java/smile/regression/SVR.java
Patch:
@@ -42,7 +42,7 @@
  * <li> Chih-Chung Chang and Chih-Jen Lin. LIBSVM: a Library for Support Vector Machines.</li>
  * </ol>
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class SVR {
     /**

File: core/src/main/java/smile/regression/package-info.java
Patch:
@@ -26,6 +26,6 @@
  * used for prediction and forecasting, where its use has substantial overlap
  * with the field of machine learning. 
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 package smile.regression;

File: core/src/main/java/smile/sampling/Bagging.java
Patch:
@@ -26,7 +26,7 @@
  * Bagging (Bootstrap aggregating) is a way to improve the classification by
  * combining classifications of randomly generated training sets.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class Bagging {
 

File: core/src/main/java/smile/sampling/package-info.java
Patch:
@@ -20,6 +20,6 @@
  * from within a statistical population to estimate characteristics of
  * the whole population.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 package smile.sampling;
\ No newline at end of file

File: core/src/main/java/smile/sequence/CRF.java
Patch:
@@ -60,7 +60,7 @@
  * <li> Thomas G. Dietterich, Guohua Hao, and Adam Ashenfelter. Gradient Tree Boosting for Training Conditional Random Fields. JMLR, 2008.</li>
  * </ol>
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class CRF implements Serializable {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/sequence/CRFLabeler.java
Patch:
@@ -25,7 +25,7 @@
 /**
  * First-order CRF sequence labeler.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class CRFLabeler<T> implements SequenceLabeler<T> {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/sequence/HMM.java
Patch:
@@ -39,7 +39,7 @@
  * tokens generated by an HMM gives some information about the sequence of
  * states.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class HMM implements Serializable {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/sequence/HMMLabeler.java
Patch:
@@ -23,7 +23,7 @@
 /**
  * First-order Hidden Markov Model sequence labeler.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class HMMLabeler<T> implements SequenceLabeler<T> {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/sequence/SequenceLabeler.java
Patch:
@@ -22,7 +22,7 @@
 /**
  * A sequence labeler assigns a class label to each position of the sequence.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public interface SequenceLabeler<T> extends Serializable {
     /**

File: core/src/main/java/smile/sequence/package-info.java
Patch:
@@ -18,6 +18,6 @@
 /**
  * Learning algorithms for sequence data.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 package smile.sequence;
\ No newline at end of file

File: core/src/main/java/smile/taxonomy/Concept.java
Patch:
@@ -28,7 +28,7 @@
  * Concept is a set of synonyms, i.e. group of words that are roughly
  * synonymous in a given context.
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class Concept {
 

File: core/src/main/java/smile/taxonomy/TaxonomicDistance.java
Patch:
@@ -26,7 +26,7 @@
  * The distance between two concepts a and b is defined by the length of the
  * path from a to their lowest common ancestor and then to b.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class TaxonomicDistance implements Distance<Concept> {
     /**

File: core/src/main/java/smile/taxonomy/Taxonomy.java
Patch:
@@ -26,7 +26,7 @@
  * A taxonomy is a tree of terms (aka concept) where leaves
  * must be named but intermediary nodes can be anonymous.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class Taxonomy {
 

File: core/src/main/java/smile/taxonomy/package-info.java
Patch:
@@ -23,6 +23,6 @@
  * The distance between two concepts a and b is defined by the length of the
  * path from a to their lowest common ancestor and then to b.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 package smile.taxonomy;

File: core/src/main/java/smile/validation/AUC.java
Patch:
@@ -37,7 +37,7 @@
  * We calculate AUC based on Mann-Whitney U test
  * (https://en.wikipedia.org/wiki/Mann-Whitney_U_test).
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class AUC {
     /**

File: core/src/main/java/smile/validation/Accuracy.java
Patch:
@@ -21,7 +21,7 @@
  * The accuracy is the proportion of true results (both true positives and
  * true negatives) in the population.
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class Accuracy implements ClassificationMeasure {
     public final static Accuracy instance = new Accuracy();

File: core/src/main/java/smile/validation/AdjustedMutualInformation.java
Patch:
@@ -40,7 +40,7 @@
  * <li>X. Vinh, J. Epps, J. Bailey. Information Theoretic Measures for Clusterings Comparison: Variants, Properties, Normalization and Correction for Chance. JMLR, 2010.</li>
  * </ol>
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class AdjustedMutualInformation implements ClusterMeasure {
     /** The normalization method. */

File: core/src/main/java/smile/validation/AdjustedRandIndex.java
Patch:
@@ -35,7 +35,7 @@
  *
  * @see RandIndex
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class AdjustedRandIndex implements ClusterMeasure {
     public final static AdjustedRandIndex instance = new AdjustedRandIndex();

File: core/src/main/java/smile/validation/Bootstrap.java
Patch:
@@ -35,7 +35,7 @@
  * to each of the bootstrap datasets and examine the behavior of the fits over
  * the k replications.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class Bootstrap {
     /**

File: core/src/main/java/smile/validation/ClassificationMeasure.java
Patch:
@@ -20,7 +20,7 @@
 /**
  * An abstract interface to measure the classification performance.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public interface ClassificationMeasure {
 

File: core/src/main/java/smile/validation/ClusterMeasure.java
Patch:
@@ -20,7 +20,7 @@
 /**
  * An abstract interface to measure the clustering performance.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public interface ClusterMeasure {
 

File: core/src/main/java/smile/validation/CrossValidation.java
Patch:
@@ -39,7 +39,7 @@
  * rounds of cross-validation are performed using different partitions, and the
  * validation results are averaged over the rounds.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class CrossValidation {
     /**

File: core/src/main/java/smile/validation/Error.java
Patch:
@@ -20,7 +20,7 @@
 /**
  * The number of errors in the population.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class Error implements ClassificationMeasure {
     public final static Error instance = new Error();

File: core/src/main/java/smile/validation/FDR.java
Patch:
@@ -23,7 +23,7 @@
  * <p>
  * FDR = FP / (TP + FP)
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class FDR implements ClassificationMeasure {
     public final static FDR instance = new FDR();

File: core/src/main/java/smile/validation/FMeasure.java
Patch:
@@ -31,7 +31,7 @@
  * the effectiveness of retrieval with respect to a user who attaches &beta; times
  * as much importance to recall as precision.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class FMeasure implements ClassificationMeasure {
     public final static FMeasure instance = new FMeasure();

File: core/src/main/java/smile/validation/Fallout.java
Patch:
@@ -25,7 +25,7 @@
  * Fall-out is actually Type I error and closely related to specificity
  * (1 - specificity).
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class Fallout implements ClassificationMeasure {
     public final static Fallout instance = new Fallout();

File: core/src/main/java/smile/validation/LOOCV.java
Patch:
@@ -37,7 +37,7 @@
  * usually very expensive from a computational point of view because of the
  * large number of times the training process is repeated.
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class LOOCV {
     /**

File: core/src/main/java/smile/validation/MSE.java
Patch:
@@ -22,7 +22,7 @@
 /**
  * Mean squared error.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class MSE implements RegressionMeasure {
     public final static MSE instance = new MSE();

File: core/src/main/java/smile/validation/MeanAbsoluteDeviation.java
Patch:
@@ -20,7 +20,7 @@
 /**
  * Mean absolute deviation error.
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class MeanAbsoluteDeviation implements RegressionMeasure {
     public final static MeanAbsoluteDeviation instance = new MeanAbsoluteDeviation();

File: core/src/main/java/smile/validation/MutualInformation.java
Patch:
@@ -28,7 +28,7 @@
  * <li>X. Vinh, J. Epps, J. Bailey. Information Theoretic Measures for Clusterings Comparison: Variants, Properties, Normalization and Correction for Chance. JMLR, 2010.</li>
  * </ol>
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class MutualInformation implements ClusterMeasure {
     public final static MutualInformation instance = new MutualInformation();

File: core/src/main/java/smile/validation/NormalizedMutualInformation.java
Patch:
@@ -33,7 +33,7 @@
  * <li>X. Vinh, J. Epps, J. Bailey. Information Theoretic Measures for Clusterings Comparison: Variants, Properties, Normalization and Correction for Chance. JMLR, 2010.</li>
  * </ol>
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class NormalizedMutualInformation implements ClusterMeasure {
     /** The normalization method. */

File: core/src/main/java/smile/validation/Precision.java
Patch:
@@ -23,7 +23,7 @@
  * <p>
  * PPV = TP / (TP + FP)
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class Precision implements ClassificationMeasure {
     public final static Precision instance = new Precision();

File: core/src/main/java/smile/validation/RMSE.java
Patch:
@@ -22,7 +22,7 @@
 /**
  * Root mean squared error.
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class RMSE implements RegressionMeasure {
     public final static RMSE instance = new RMSE();

File: core/src/main/java/smile/validation/RSS.java
Patch:
@@ -22,7 +22,7 @@
 /**
  * Residual sum of squares.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class RSS implements RegressionMeasure {
     public final static RSS instance = new RSS();

File: core/src/main/java/smile/validation/RandIndex.java
Patch:
@@ -33,7 +33,7 @@
  * between two partitions. The adjusted Rand index is recommended for measuring
  * agreement even when the partitions compared have different numbers of clusters.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class RandIndex implements ClusterMeasure {
     public final static RandIndex instance = new RandIndex();

File: core/src/main/java/smile/validation/Recall.java
Patch:
@@ -22,7 +22,7 @@
  *
  * @see Sensitivity
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class Recall implements ClassificationMeasure {
     public final static Recall instance = new Recall();

File: core/src/main/java/smile/validation/RegressionMeasure.java
Patch:
@@ -20,7 +20,7 @@
 /**
  * An abstract interface to measure the regression performance.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public interface RegressionMeasure {
 

File: core/src/main/java/smile/validation/Sensitivity.java
Patch:
@@ -32,7 +32,7 @@
  * In this implementation, the class label 1 is regarded as positive and 0
  * is regarded as negative.
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class Sensitivity implements ClassificationMeasure {
     public final static Sensitivity instance = new Sensitivity();

File: core/src/main/java/smile/validation/Specificity.java
Patch:
@@ -31,7 +31,7 @@
  * In this implementation, the class label 1 is regarded as positive and 0
  * is regarded as negative.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class Specificity implements ClassificationMeasure {
     public final static Specificity instance = new Specificity();

File: core/src/main/java/smile/validation/package-info.java
Patch:
@@ -18,6 +18,6 @@
 /**
  * Model validation.
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 package smile.validation;

File: core/src/main/java/smile/vq/BIRCH.java
Patch:
@@ -52,7 +52,7 @@
  * 
  * @see smile.clustering.HierarchicalClustering
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class BIRCH implements VectorQuantizer {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/vq/GrowingNeuralGas.java
Patch:
@@ -47,7 +47,7 @@
  * @see NeuralGas
  * @see NeuralMap
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class GrowingNeuralGas implements VectorQuantizer {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/vq/Neighborhood.java
Patch:
@@ -23,7 +23,7 @@
  * The neighborhood function for 2-dimensional lattice topology (e.g. SOM).
  * It determines the rate of change around the winner neuron.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public interface Neighborhood extends Serializable {
     /**

File: core/src/main/java/smile/vq/NeuralGas.java
Patch:
@@ -62,7 +62,7 @@
  * @see GrowingNeuralGas
  * @see SOM
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class NeuralGas implements VectorQuantizer {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/vq/NeuralMap.java
Patch:
@@ -34,7 +34,7 @@
  * @see GrowingNeuralGas
  * @see BIRCH
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class NeuralMap implements VectorQuantizer {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/vq/SOM.java
Patch:
@@ -83,7 +83,7 @@
  * <li> Teuvo KohonenDan. Self-organizing maps. Springer, 3rd edition, 2000. </li>
  * </ol>
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class SOM implements VectorQuantizer {
     private static final long serialVersionUID = 2L;

File: core/src/main/java/smile/vq/VectorQuantizer.java
Patch:
@@ -22,7 +22,7 @@
 /**
  * Vector quantizer with competitive learning.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public interface VectorQuantizer extends Serializable {
     /**

File: core/src/main/java/smile/vq/hebb/package-info.java
Patch:
@@ -37,6 +37,6 @@
  * function, it is often regarded as the neuronal basis of unsupervised
  * learning.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 package smile.vq.hebb;
\ No newline at end of file

File: core/src/main/java/smile/vq/package-info.java
Patch:
@@ -24,6 +24,6 @@
  * into a set of non-overlapping regions. The vector is encoded by
  * the nearest reference vector (known as codevector) in the codebook.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 package smile.vq;
\ No newline at end of file

File: core/src/main/java/smile/wavelet/BestLocalizedWavelet.java
Patch:
@@ -20,7 +20,7 @@
 /**
  * Best localized wavelets.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class BestLocalizedWavelet extends Wavelet {
     /**

File: core/src/main/java/smile/wavelet/CoifletWavelet.java
Patch:
@@ -23,7 +23,7 @@
  * vanishing moments and scaling functions N / 3 ? 1, and has been used in
  * many applications using Calderon-Zygmund Operators.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class CoifletWavelet extends Wavelet {
     /**

File: core/src/main/java/smile/wavelet/D4Wavelet.java
Patch:
@@ -22,7 +22,7 @@
  * Note that this class uses the different centering method from the one used in
  * the Daubechies class.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class D4Wavelet extends Wavelet {
 

File: core/src/main/java/smile/wavelet/DaubechiesWavelet.java
Patch:
@@ -54,7 +54,7 @@
  * of shift-invariance, has led to the development of several different versions
  * of a shift-invariant (discrete) wavelet transform.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class DaubechiesWavelet extends Wavelet {
     /**

File: core/src/main/java/smile/wavelet/HaarWavelet.java
Patch:
@@ -27,7 +27,7 @@
  * for the analysis of signals with sudden transitions, such as monitoring
  * of tool failure in machines.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class HaarWavelet extends Wavelet {
 

File: core/src/main/java/smile/wavelet/SymletWavelet.java
Patch:
@@ -21,7 +21,7 @@
  * Symlet wavelets. The symlets have compact support and were constructed to
  * be as nearly symmetric (least asymmetric) as possible.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class SymletWavelet extends Wavelet {
     /**

File: core/src/main/java/smile/wavelet/Wavelet.java
Patch:
@@ -29,7 +29,7 @@
  * The wavelet transform is invertible and in fact orthogonal. Both FFT and DWT
  * can be viewed as a rotation in function space.
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class Wavelet {
 

File: core/src/main/java/smile/wavelet/WaveletShrinkage.java
Patch:
@@ -44,7 +44,7 @@
  * so-called scaled median absolute deviation (MAD) computed from the high-pass
  * wavelet coefficients of the first level of the transform. 
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public interface WaveletShrinkage {
     /**

File: core/src/main/java/smile/wavelet/package-info.java
Patch:
@@ -18,6 +18,6 @@
 /**
  * Discrete wavelet transform (DWT).
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 package smile.wavelet;

File: core/src/test/java/smile/association/ARMTest.java
Patch:
@@ -30,7 +30,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 @SuppressWarnings("unused")
 public class ARMTest {

File: core/src/test/java/smile/association/ItemSetTestData.java
Patch:
@@ -21,7 +21,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public interface ItemSetTestData {
 

File: core/src/test/java/smile/association/TotalSupportTreeTest.java
Patch:
@@ -30,7 +30,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 @SuppressWarnings("unused")
 public class TotalSupportTreeTest {

File: core/src/test/java/smile/classification/DiscreteNaiveBayesTest.java
Patch:
@@ -33,7 +33,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class DiscreteNaiveBayesTest {
 

File: core/src/test/java/smile/classification/FLDTest.java
Patch:
@@ -41,7 +41,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class FLDTest {
 

File: core/src/test/java/smile/classification/GradientTreeBoostTest.java
Patch:
@@ -31,7 +31,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class GradientTreeBoostTest {
     

File: core/src/test/java/smile/classification/KNNTest.java
Patch:
@@ -33,7 +33,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class KNNTest {
 

File: core/src/test/java/smile/classification/LDATest.java
Patch:
@@ -31,7 +31,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class LDATest {
 

File: core/src/test/java/smile/classification/MLPTest.java
Patch:
@@ -38,7 +38,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class MLPTest {
 

File: core/src/test/java/smile/classification/MaxentTest.java
Patch:
@@ -31,7 +31,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class MaxentTest {
 

File: core/src/test/java/smile/classification/NaiveBayesTest.java
Patch:
@@ -37,7 +37,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class NaiveBayesTest {
 

File: core/src/test/java/smile/classification/QDATest.java
Patch:
@@ -33,7 +33,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class QDATest {
 

File: core/src/test/java/smile/classification/RBFNetworkTest.java
Patch:
@@ -36,7 +36,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 @SuppressWarnings("unused")
 public class RBFNetworkTest {

File: core/src/test/java/smile/classification/RDATest.java
Patch:
@@ -36,7 +36,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class RDATest {
 

File: core/src/test/java/smile/classification/SVMTest.java
Patch:
@@ -39,7 +39,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class SVMTest {
 

File: core/src/test/java/smile/clustering/CLARANSTest.java
Patch:
@@ -30,7 +30,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class CLARANSTest {
     

File: core/src/test/java/smile/clustering/DBSCANTest.java
Patch:
@@ -33,7 +33,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class DBSCANTest {
     

File: core/src/test/java/smile/clustering/DENCLUETest.java
Patch:
@@ -30,7 +30,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class DENCLUETest {
     

File: core/src/test/java/smile/clustering/DeterministicAnnealingTest.java
Patch:
@@ -29,7 +29,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class DeterministicAnnealingTest {
     

File: core/src/test/java/smile/clustering/KMeansTest.java
Patch:
@@ -31,7 +31,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class KMeansTest {
     double[][] x = GaussianMixture.x;

File: core/src/test/java/smile/clustering/SpectralClusteringTest.java
Patch:
@@ -30,7 +30,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class SpectralClusteringTest {
     

File: core/src/test/java/smile/data/Sequence.java
Patch:
@@ -30,7 +30,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public interface Sequence {
 

File: core/src/test/java/smile/data/Serialize.java
Patch:
@@ -24,7 +24,7 @@
 /**
  * Serialization help functions.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public interface Serialize {
     /**

File: core/src/test/java/smile/feature/BagTest.java
Patch:
@@ -31,7 +31,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class BagTest {
     

File: core/src/test/java/smile/feature/DateFeatureTest.java
Patch:
@@ -35,7 +35,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class DateFeatureTest {
     

File: core/src/test/java/smile/feature/GAFETest.java
Patch:
@@ -37,7 +37,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class GAFETest {
     

File: core/src/test/java/smile/feature/SignalNoiseRatioTest.java
Patch:
@@ -27,7 +27,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class SignalNoiseRatioTest {
     

File: core/src/test/java/smile/feature/SparseOneHotEncoderTest.java
Patch:
@@ -29,7 +29,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class SparseOneHotEncoderTest {
     

File: core/src/test/java/smile/feature/SumSquaresRatioTest.java
Patch:
@@ -31,7 +31,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class SumSquaresRatioTest {
     

File: core/src/test/java/smile/gap/BitStringTest.java
Patch:
@@ -28,7 +28,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class BitStringTest {
 

File: core/src/test/java/smile/gap/CrossoverTest.java
Patch:
@@ -28,7 +28,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class CrossoverTest {
 

File: core/src/test/java/smile/gap/GeneticAlgorithmTest.java
Patch:
@@ -26,7 +26,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class GeneticAlgorithmTest {
     

File: core/src/test/java/smile/imputation/MissingValueImputationTest.java
Patch:
@@ -30,7 +30,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class MissingValueImputationTest {
 

File: core/src/test/java/smile/manifold/IsoMapTest.java
Patch:
@@ -28,7 +28,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class IsoMapTest {
 

File: core/src/test/java/smile/manifold/LLETest.java
Patch:
@@ -39,7 +39,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class LLETest {
 

File: core/src/test/java/smile/manifold/LaplacianEigenmapTest.java
Patch:
@@ -28,7 +28,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class LaplacianEigenmapTest {
 

File: core/src/test/java/smile/mds/IsotonicMDSTest.java
Patch:
@@ -27,7 +27,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class IsotonicMDSTest {
 

File: core/src/test/java/smile/mds/MDSTest.java
Patch:
@@ -27,7 +27,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class MDSTest {
 

File: core/src/test/java/smile/mds/SammonMappingTest.java
Patch:
@@ -28,7 +28,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class SammonMappingTest {
 

File: core/src/test/java/smile/neighbor/BKTreeTest.java
Patch:
@@ -32,7 +32,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class BKTreeTest {
 

File: core/src/test/java/smile/neighbor/CoverTreeTest.java
Patch:
@@ -38,7 +38,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 @SuppressWarnings("rawtypes")
 public class CoverTreeTest {

File: core/src/test/java/smile/neighbor/KDTreeTest.java
Patch:
@@ -35,7 +35,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class KDTreeTest {
 

File: core/src/test/java/smile/neighbor/LSHTest.java
Patch:
@@ -33,7 +33,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 @SuppressWarnings("rawtypes")
 public class LSHTest {

File: core/src/test/java/smile/neighbor/LinearSearchTest.java
Patch:
@@ -39,7 +39,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 @SuppressWarnings("rawtypes")
 public class LinearSearchTest {

File: core/src/test/java/smile/neighbor/MPLSHTest.java
Patch:
@@ -34,7 +34,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 @SuppressWarnings("rawtypes")
 public class MPLSHTest {

File: core/src/test/java/smile/projection/GHATest.java
Patch:
@@ -30,7 +30,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 @SuppressWarnings("unused")
 public class GHATest {

File: core/src/test/java/smile/projection/KPCATest.java
Patch:
@@ -35,7 +35,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 @SuppressWarnings({"rawtypes", "unchecked"})
 public class KPCATest {

File: core/src/test/java/smile/projection/PCATest.java
Patch:
@@ -27,7 +27,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class PCATest {
 

File: core/src/test/java/smile/projection/RandomProjectionTest.java
Patch:
@@ -29,7 +29,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class RandomProjectionTest {
 

File: core/src/test/java/smile/regression/GaussianProcessRegressionTest.java
Patch:
@@ -40,7 +40,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class GaussianProcessRegressionTest {
     public GaussianProcessRegressionTest() {

File: core/src/test/java/smile/regression/GradientTreeBoostTest.java
Patch:
@@ -34,7 +34,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class GradientTreeBoostTest {
     

File: core/src/test/java/smile/regression/LASSOTest.java
Patch:
@@ -37,7 +37,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class LASSOTest {
     public LASSOTest() {

File: core/src/test/java/smile/regression/MLPTest.java
Patch:
@@ -33,7 +33,7 @@
 
 /**
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class MLPTest {
     public MLPTest(){

File: core/src/test/java/smile/regression/OLSTest.java
Patch:
@@ -34,7 +34,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class OLSTest {
 

File: core/src/test/java/smile/regression/RBFNetworkTest.java
Patch:
@@ -33,7 +33,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class RBFNetworkTest {
 

File: core/src/test/java/smile/regression/RandomForestTest.java
Patch:
@@ -31,7 +31,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class RandomForestTest {
     long[] seeds = {

File: core/src/test/java/smile/regression/RegressionTreeTest.java
Patch:
@@ -35,7 +35,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class RegressionTreeTest {
     

File: core/src/test/java/smile/regression/RidgeRegressionTest.java
Patch:
@@ -34,7 +34,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class RidgeRegressionTest {
     public RidgeRegressionTest() {

File: core/src/test/java/smile/regression/SVRTest.java
Patch:
@@ -35,7 +35,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class SVRTest {
 

File: core/src/test/java/smile/sequence/CRFTest.java
Patch:
@@ -30,7 +30,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 @SuppressWarnings("unused")
 public class CRFTest {

File: core/src/test/java/smile/sequence/HMMTest.java
Patch:
@@ -31,7 +31,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 @SuppressWarnings({"rawtypes", "unchecked"})
 public class HMMTest {

File: core/src/test/java/smile/taxonomy/TaxonomyTest.java
Patch:
@@ -28,7 +28,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class TaxonomyTest {
 

File: core/src/test/java/smile/validation/AccuracyTest.java
Patch:
@@ -26,7 +26,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class AccuracyTest {
 

File: core/src/test/java/smile/validation/AdjustedRandIndexTest.java
Patch:
@@ -26,7 +26,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class AdjustedRandIndexTest {
 

File: core/src/test/java/smile/validation/BootstrapTest.java
Patch:
@@ -32,7 +32,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class BootstrapTest {
 

File: core/src/test/java/smile/validation/CrossValidationTest.java
Patch:
@@ -26,7 +26,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class CrossValidationTest {
 

File: core/src/test/java/smile/validation/FDRTest.java
Patch:
@@ -26,7 +26,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class FDRTest {
 

File: core/src/test/java/smile/validation/FMeasureTest.java
Patch:
@@ -26,7 +26,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class FMeasureTest {
 

File: core/src/test/java/smile/validation/FalloutTest.java
Patch:
@@ -26,7 +26,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class FalloutTest {
 

File: core/src/test/java/smile/validation/LOOCVTest.java
Patch:
@@ -26,7 +26,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class LOOCVTest {
 

File: core/src/test/java/smile/validation/MSETest.java
Patch:
@@ -26,7 +26,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class MSETest {
 

File: core/src/test/java/smile/validation/PrecisionTest.java
Patch:
@@ -26,7 +26,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class PrecisionTest {
 

File: core/src/test/java/smile/validation/RMSETest.java
Patch:
@@ -26,7 +26,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class RMSETest {
 

File: core/src/test/java/smile/validation/RSSTest.java
Patch:
@@ -26,7 +26,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class RSSTest {
 

File: core/src/test/java/smile/validation/RandIndexTest.java
Patch:
@@ -26,7 +26,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class RandIndexTest {
 

File: core/src/test/java/smile/validation/RecallTest.java
Patch:
@@ -26,7 +26,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class RecallTest {
 

File: core/src/test/java/smile/validation/SensitivityTest.java
Patch:
@@ -26,7 +26,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class SensitivityTest {
 

File: core/src/test/java/smile/validation/SpecificityTest.java
Patch:
@@ -26,7 +26,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class SpecificityTest {
 

File: core/src/test/java/smile/vq/NeuralGasTest.java
Patch:
@@ -29,7 +29,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class NeuralGasTest {
     

File: core/src/test/java/smile/wavelet/BestLocalizedWaveletTest.java
Patch:
@@ -26,7 +26,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class BestLocalizedWaveletTest {
 

File: core/src/test/java/smile/wavelet/CoifletWaveletTest.java
Patch:
@@ -26,7 +26,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class CoifletWaveletTest {
 

File: core/src/test/java/smile/wavelet/D4WaveletTest.java
Patch:
@@ -26,7 +26,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class D4WaveletTest {
 

File: core/src/test/java/smile/wavelet/HaarWaveletTest.java
Patch:
@@ -26,7 +26,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class HaarWaveletTest {
 

File: core/src/test/java/smile/wavelet/SymletWaveletTest.java
Patch:
@@ -26,7 +26,7 @@
 
 /**
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class SymletWaveletTest {
 

File: data/src/main/java/smile/data/AbstractTuple.java
Patch:
@@ -20,7 +20,7 @@
 /**
  * Abstract tuple base class.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public abstract class AbstractTuple implements Tuple {
     @Override

File: data/src/main/java/smile/data/BinarySparseDataset.java
Patch:
@@ -28,7 +28,7 @@
  * Binary sparse dataset. Each item is stored as an integer array, which
  * are the indices of nonzero elements in ascending order.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public interface BinarySparseDataset extends Dataset<int[]> {
     /** Returns the number of nonzero entries. */

File: data/src/main/java/smile/data/BinarySparseDatasetImpl.java
Patch:
@@ -27,7 +27,7 @@
  * Binary sparse dataset. Each item is stored as an integer array, which
  * are the indices of nonzero elements in ascending order.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 class BinarySparseDatasetImpl implements BinarySparseDataset {
     private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(BinarySparseDatasetImpl.class);

File: data/src/main/java/smile/data/DataFrame.java
Patch:
@@ -37,7 +37,7 @@
 /**
  * An immutable collection of data organized into named columns.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public interface DataFrame extends Dataset<Tuple>, Iterable<BaseVector> {
     /** Returns the schema of DataFrame. */

File: data/src/main/java/smile/data/DataFrameImpl.java
Patch:
@@ -34,7 +34,7 @@
 /**
  * A simple implementation of DataFrame that store columnar data in single machine's memory.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 class DataFrameImpl implements DataFrame {
     private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(DataFrameImpl.class);

File: data/src/main/java/smile/data/Dataset.java
Patch:
@@ -32,7 +32,7 @@
  *
  * @param <T> the type of data objects.
  * 
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public interface Dataset<T> {
     /**

File: data/src/main/java/smile/data/DatasetImpl.java
Patch:
@@ -26,7 +26,7 @@
  *
  * @param <T> the type of data objects.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 class DatasetImpl<T> implements Dataset<T> {
     /**

File: data/src/main/java/smile/data/DatasetSpliterator.java
Patch:
@@ -23,7 +23,7 @@
 /**
  * A spliterator traverse and partition a local dataset.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 class DatasetSpliterator<T> implements Spliterator<T> {
     /** The underlying Dataset. */

File: data/src/main/java/smile/data/IndexDataFrame.java
Patch:
@@ -30,7 +30,7 @@
 /**
  * A data frame with a new index instead of the default [0, n) row index.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public class IndexDataFrame implements DataFrame {
     private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(IndexDataFrame.class);

File: data/src/main/java/smile/data/Instance.java
Patch:
@@ -22,7 +22,7 @@
  *
  * @param <T> the type of instance.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public interface Instance <T> {
     /**

File: data/src/main/java/smile/data/SparseDataset.java
Patch:
@@ -43,7 +43,7 @@
  * column-compressed sparse matrix format, which is more efficient for matrix
  * operations.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public interface SparseDataset extends Dataset<SparseArray> {
     /** Returns the number of nonzero entries. */

File: data/src/main/java/smile/data/SparseDatasetImpl.java
Patch:
@@ -32,7 +32,7 @@
  * column-compressed sparse matrix format, which is more efficient for matrix
  * operations.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 class SparseDatasetImpl implements SparseDataset {
     private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(SparseDatasetImpl.class);

File: data/src/main/java/smile/data/Tuple.java
Patch:
@@ -37,7 +37,7 @@
  * that is null, instead a user must check `isNullAt` before attempting
  * to retrieve a value that might be null.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public interface Tuple extends Serializable {
     /** Returns the schema of tuple. */

File: data/src/main/java/smile/data/formula/Abs.java
Patch:
@@ -24,7 +24,7 @@
 /**
  * The term of abs function.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 class Abs extends AbstractFunction {
     /**

File: data/src/main/java/smile/data/formula/AbstractBiFunction.java
Patch:
@@ -24,7 +24,7 @@
 /**
  * This class provides a skeletal implementation of the bi-function term.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public abstract class AbstractBiFunction extends AbstractTerm {
     /** The name of function. */

File: data/src/main/java/smile/data/formula/AbstractFunction.java
Patch:
@@ -23,7 +23,7 @@
 /**
  * This class provides a skeletal implementation of the function term.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public abstract class AbstractFunction extends AbstractTerm {
     /** The name of function. */

File: data/src/main/java/smile/data/formula/AbstractTerm.java
Patch:
@@ -21,7 +21,7 @@
  * This class provides a skeletal implementation of the Term interface,
  * to minimize the effort required to implement this interface.
  *
- * @author Haifeng Li. All rights reserved.
+ * @author Haifeng Li
  */
 public abstract class AbstractTerm implements Term {
     /**

File: core/src/main/java/smile/base/mlp/ActivationFunction.java
Patch:
@@ -92,7 +92,7 @@ public void f(double[] x) {
             @Override
             public void g(double[] g, double[] y) {
                 for (int i = 0; i < g.length; i++) {
-                    g[i] = y[i] > 0 ? 1 : 0;
+                    g[i] *= y[i] > 0 ? 1 : 0;
                 }
 
             }

File: core/src/test/java/smile/classification/MLPTest.java
Patch:
@@ -221,7 +221,7 @@ public void testUSPS() throws Exception {
         model.setMomentum(0.0);
 
         int error = 0;
-        for (int epoch = 1; epoch <= 4; epoch++) {
+        for (int epoch = 1; epoch <= 5; epoch++) {
             System.out.format("----- epoch %d -----%n", epoch);
             int[] permutation = MathEx.permutate(x.length);
             for (int i : permutation) {

File: io/src/test/java/smile/io/ArrowTest.java
Patch:
@@ -119,7 +119,7 @@ public void testSchema() {
                 new StructField("Customer First", DataTypes.StringType),
                 new StructField("Customer Last", DataTypes.StringType),
                 new StructField("Country", DataTypes.StringType),
-                new StructField("Total", DataTypes.DoubleType)
+                new StructField("Total", DataTypes.DoubleObjectType)
         );
         assertEquals(schema, df.schema());
     }

File: core/src/test/java/smile/data/Puma8NH.java
Patch:
@@ -35,7 +35,7 @@ public class Puma8NH {
 
     static {
         try {
-            data = DatasetReader.arff(Paths.getTestData("weka/regression/puma8nh.arff"));
+            data = DatasetReader.arff(Paths.getTestData("weka/regression/puma8NH.arff"));
 
             x = formula.x(data).toArray();
             y = formula.y(data).toDoubleArray();

File: core/src/test/java/smile/data/Puma8NH.java
Patch:
@@ -35,7 +35,7 @@ public class Puma8NH {
 
     static {
         try {
-            data = DatasetReader.arff(Paths.getTestData("weka/regression/puma8nh.arff"));
+            data = DatasetReader.arff(Paths.getTestData("weka/regression/puma8NH.arff"));
 
             x = formula.x(data).toArray();
             y = formula.y(data).toDoubleArray();

File: core/src/main/java/smile/vq/SOM.java
Patch:
@@ -147,7 +147,7 @@ public Neuron(int i, int j, double[] w) {
     /**
      * The threshold to update neuron if alpha * theta > eps.
      */
-    private double eps = 1E-7;
+    private double eps = 1E-5;
 
     /**
      * Constructor.

File: core/src/main/java/smile/classification/RandomForest.java
Patch:
@@ -299,7 +299,7 @@ public static RandomForest fit(Formula formula, DataFrame data, int ntrees, int
         DataFrame x = formula.x(data);
         BaseVector y = formula.y(data);
 
-        if (mtry < 1 || mtry > x.ncols()) {
+        if (mtry > x.ncols()) {
             throw new IllegalArgumentException("Invalid number of variables to split on at a node of the tree: " + mtry);
         }
 

File: demo/src/main/java/smile/demo/stat/distribution/EmpiricalDistributionDemo.java
Patch:
@@ -62,7 +62,7 @@ public EmpiricalDistributionDemo() {
         p = new double[11][2];
         for (int i = 0; i < p.length; i++) {
             p[i][0] = i;
-            p[i][1] = emp.cdf(p[i][0]);
+            p[i][1] = emp.cdf(i);
         }
 
         canvas = StaircasePlot.plot(p);

File: core/src/main/java/smile/vq/BIRCH.java
Patch:
@@ -472,9 +472,9 @@ public void update(double[] x) {
     }
 
     @Override
-    public Optional<double[]> quantize(double[] x) {
-        BIRCH.ClusteringFeature cf = root.nearest(x);
-        return Optional.of(cf.centroid());
+    public double[] quantize(double[] x) {
+        ClusteringFeature cluster = root.nearest(x);
+        return cluster.centroid();
     }
 
     /** Returns the cluster centroids of leaf nodes. */

File: core/src/main/java/smile/vq/GrowingNeuralGas.java
Patch:
@@ -235,9 +235,9 @@ public Neuron[] neurons() {
     }
 
     @Override
-    public Optional<double[]> quantize(double[] x) {
+    public double[] quantize(double[] x) {
         neurons.stream().parallel().forEach(node -> node.distance(x));
         Collections.sort(neurons);
-        return Optional.ofNullable(neurons.get(0).w);
+        return neurons.get(0).w;
     }
 }

File: core/src/main/java/smile/vq/NeuralGas.java
Patch:
@@ -19,7 +19,6 @@
 
 import java.io.Serializable;
 import java.util.Arrays;
-import java.util.Optional;
 import java.util.stream.IntStream;
 import smile.clustering.CentroidClustering;
 import smile.graph.AdjacencyMatrix;
@@ -199,9 +198,9 @@ public void update(double[] x) {
     }
 
     @Override
-    public Optional<double[]> quantize(double[] x) {
+    public double[] quantize(double[] x) {
         sortNeurons(x);
-        return Optional.of(neurons[MathEx.whichMin(dist)].w);
+        return neurons[MathEx.whichMin(dist)].w;
     }
 
     /** Sorts the neurons by their distances to the input observation. */

File: core/src/main/java/smile/vq/NeuralMap.java
Patch:
@@ -205,9 +205,9 @@ public void clear(double eps) {
     }
 
     @Override
-    public Optional<double[]> quantize(double[] x) {
+    public double[] quantize(double[] x) {
         neurons.stream().parallel().forEach(node -> node.distance(x));
         Collections.sort(neurons);
-        return Optional.ofNullable(neurons.get(0).w);
+        return neurons.get(0).w;
     }
 }

File: core/src/main/java/smile/vq/SOM.java
Patch:
@@ -19,7 +19,6 @@
 
 import java.io.Serializable;
 import java.util.Arrays;
-import java.util.Optional;
 import java.util.stream.IntStream;
 import smile.clustering.CentroidClustering;
 import smile.math.MathEx;
@@ -289,8 +288,8 @@ public double[][] umatrix() {
     }
 
     @Override
-    public Optional<double[]> quantize(double[] x) {
-        return Optional.of(bmu(x).w);
+    public double[] quantize(double[] x) {
+        return bmu(x).w;
     }
 
     /** Returns the best matching unit. */

File: core/src/main/java/smile/vq/VectorQuantizer.java
Patch:
@@ -18,7 +18,6 @@
 package smile.vq;
 
 import java.io.Serializable;
-import java.util.Optional;
 
 /**
  * Vector quantizer with competitive learning.
@@ -41,5 +40,5 @@ public interface VectorQuantizer extends Serializable {
      * if the observation is noise.
      * @param x a new observation.
      */
-    Optional<double[]> quantize(double[] x);
+    double[] quantize(double[] x);
 }

File: core/src/main/java/smile/base/cart/DecisionNode.java
Patch:
@@ -31,7 +31,7 @@
  * A leaf node in decision tree.
  */
 public class DecisionNode extends LeafNode {
-    private static final long serialVersionUID = 1L;
+    private static final long serialVersionUID = 2L;
 
     /** The predicted output. */
     private int output;

File: core/src/main/java/smile/base/cart/NominalNode.java
Patch:
@@ -35,7 +35,7 @@
  * A node with a nominal split variable.
  */
 public class NominalNode extends InternalNode {
-    private static final long serialVersionUID = 1L;
+    private static final long serialVersionUID = 2L;
 
     /**
      * The split value.

File: core/src/main/java/smile/base/cart/OrdinalNode.java
Patch:
@@ -34,7 +34,7 @@
  * A node with a ordinal split variable (real-valued or ordinal categorical value).
  */
 public class OrdinalNode extends InternalNode {
-    private static final long serialVersionUID = 1L;
+    private static final long serialVersionUID = 2L;
 
     /**
      * The split value.

File: core/src/main/java/smile/base/rbf/RBF.java
Patch:
@@ -51,7 +51,7 @@
  * @author Haifeng Li
  */
 public class RBF<T> implements Serializable {
-    private static final long serialVersionUID = 1L;
+    private static final long serialVersionUID = 2L;
 
     /** The center of neuron. */
     private T center;

File: core/src/main/java/smile/classification/Maxent.java
Patch:
@@ -168,7 +168,7 @@ public static Maxent fit(int p, int[][] x, int[] y, Properties prop) {
      * @param lambda &lambda; &gt; 0 gives a "regularized" estimate of linear
      * weights which often has superior generalization performance, especially
      * when the dimensionality is high.
-     * @param tol tolerance for stopping iterations.
+     * @param tol the tolerance for stopping iterations.
      * @param maxIter maximum number of iterations.
      */
     public static Maxent fit(int p, int[][] x, int[] y, double lambda, double tol, int maxIter) {

File: core/src/main/java/smile/clustering/BIRCH.java
Patch:
@@ -58,7 +58,7 @@
  * @author Haifeng Li
  */
 public class BIRCH implements Clustering<double[]> {
-    private static final long serialVersionUID = 1L;
+    private static final long serialVersionUID = 2L;
 
     /**
      * Branching factor. Maximum number of children nodes.
@@ -85,7 +85,7 @@ public class BIRCH implements Clustering<double[]> {
      * Internal node of CF tree.
      */
     class Node implements Serializable {
-        private static final long serialVersionUID = 1L;
+        private static final long serialVersionUID = 2L;
 
         /**
          * The number of observations

File: core/src/main/java/smile/clustering/GMeans.java
Patch:
@@ -71,7 +71,7 @@ public static GMeans fit(double[][] data, int kmax) {
      * @param data the input data of which each row is an observation.
      * @param kmax the maximum number of clusters.
      * @param maxIter the maximum number of iterations for each running.
-     * @param tol tol the tolerance of convergence test.
+     * @param tol the tolerance of convergence test.
      */
     public static GMeans fit(double[][] data, int kmax, int maxIter, double tol) {
         if (kmax < 2) {

File: core/src/main/java/smile/clustering/KMeans.java
Patch:
@@ -117,7 +117,7 @@ public static KMeans fit(double[][] data, int k) {
      * @param data the input data of which each row is an observation.
      * @param k the number of clusters.
      * @param maxIter the maximum number of iterations for each running.
-     * @param tol tol the tolerance of convergence test.
+     * @param tol the tolerance of convergence test.
      */
     public static KMeans fit(double[][] data, int k, int maxIter, double tol) {
         return fit(new BBDTree(data), data, k, maxIter, tol);
@@ -129,7 +129,7 @@ public static KMeans fit(double[][] data, int k, int maxIter, double tol) {
      * @param data the input data of which each row is an observation.
      * @param k the number of clusters.
      * @param maxIter the maximum number of iterations for each running.
-     * @param tol tol the tolerance of convergence test.
+     * @param tol the tolerance of convergence test.
      */
     public static KMeans fit(BBDTree bbd, double[][] data, int k, int maxIter, double tol) {
         if (k < 2) {
@@ -185,7 +185,7 @@ public static KMeans lloyd(double[][] data, int k) {
      * @param data the input data of which each row is an observation.
      * @param k the number of clusters.
      * @param maxIter the maximum number of iterations for each running.
-     * @param tol tol the tolerance of convergence test.
+     * @param tol the tolerance of convergence test.
      */
     private static KMeans lloyd(double[][] data, int k, int maxIter, double tol) {
         if (k < 2) {

File: core/src/main/java/smile/clustering/PartitionClustering.java
Patch:
@@ -76,7 +76,7 @@ public String toString() {
         sb.append(String.format("Cluster size of %d data points:%n", y.length));
         for (int i = 0; i < k; i++) {
             double r = 100.0 * size[i] / y.length;
-            sb.append(String.format("Cluster %4d %6d (%4.1f%%)%n", i, size[i], r));
+            sb.append(String.format("Cluster %4d %6d (%4.1f%%)%n", i+1, size[i], r));
         }
 
         if (size[k] != 0) {

File: core/src/main/java/smile/clustering/SpectralClustering.java
Patch:
@@ -79,7 +79,7 @@ public static SpectralClustering fit(DenseMatrix W, int k) {
      * @param W the adjacency matrix of graph, which will be modified.
      * @param k the number of clusters.
      * @param maxIter the maximum number of iterations for each running.
-     * @param tol tol the tolerance of convergence test.
+     * @param tol the tolerance of convergence test.
      */
     public static SpectralClustering fit(DenseMatrix W, int k, int maxIter, double tol) {
         if (k < 2) {
@@ -137,7 +137,7 @@ public static SpectralClustering fit(double[][] data, int k, double sigma) {
      *              setting, one may pick the value that gives the tightest
      *              clusters (smallest distortion) in feature space.
      * @param maxIter the maximum number of iterations for each running.
-     * @param tol tol the tolerance of convergence test.
+     * @param tol the tolerance of convergence test.
      */
     public static SpectralClustering fit(double[][] data, int k, double sigma, int maxIter, double tol) {
         if (k < 2) {
@@ -187,7 +187,7 @@ public static SpectralClustering fit(double[][] data, int k, int l, double sigma
      *              setting, one may pick the value that gives the tightest
      *              clusters (smallest distortion) in feature space.
      * @param maxIter the maximum number of iterations for each running.
-     * @param tol tol the tolerance of convergence test.
+     * @param tol the tolerance of convergence test.
      */
     public static SpectralClustering fit(double[][] data, int k, int l, double sigma, int maxIter, double tol) {
         if (l < k || l >= data.length) {

File: core/src/main/java/smile/clustering/XMeans.java
Patch:
@@ -74,7 +74,7 @@ public static XMeans fit(double[][] data, int kmax) {
      * @param data the input data of which each row is an observation.
      * @param kmax the maximum number of clusters.
      * @param maxIter the maximum number of iterations for each running.
-     * @param tol tol the tolerance of convergence test.
+     * @param tol the tolerance of convergence test.
      */
     public static XMeans fit(double[][] data, int kmax, int maxIter, double tol) {
         if (kmax < 2) {

File: core/src/main/java/smile/mds/IsotonicMDS.java
Patch:
@@ -112,7 +112,7 @@ public static IsotonicMDS of(double[][] proximity, Properties prop) {
      * @param proximity the nonnegative proximity matrix of dissimilarities. The
      * diagonal should be zero and all other elements should be positive and symmetric.
      * @param k the dimension of the projection.
-     * @param tol tolerance for stopping iterations.
+     * @param tol the tolerance for stopping iterations.
      * @param maxIter maximum number of iterations.
      */
     public static IsotonicMDS of(double[][] proximity, int k, double tol, int maxIter) {
@@ -127,7 +127,7 @@ public static IsotonicMDS of(double[][] proximity, int k, double tol, int maxIte
      * diagonal should be zero and all other elements should be positive and symmetric.
      * @param init the initial projected coordinates, of which the column
      * size is the projection dimension.
-     * @param tol tolerance for stopping iterations.
+     * @param tol the tolerance for stopping iterations.
      * @param maxIter maximum number of iterations.
      */
     public static IsotonicMDS of(double[][] proximity, double[][] init, double tol, int maxIter) {

File: core/src/main/java/smile/neighbor/BKTree.java
Patch:
@@ -55,7 +55,7 @@
  * @author Haifeng Li
  */
 public class BKTree<E> implements RNNSearch<E, E>, Serializable {
-    private static final long serialVersionUID = 1L;
+    private static final long serialVersionUID = 2L;
     
     /**
      * The root in the BK-tree.

File: core/src/main/java/smile/neighbor/CoverTree.java
Patch:
@@ -56,7 +56,7 @@
  * @author Haifeng Li
  */
 public class CoverTree<E> implements NearestNeighborSearch<E, E>, KNNSearch<E, E>, RNNSearch<E, E>, Serializable {
-    private static final long serialVersionUID = 1L;
+    private static final long serialVersionUID = 2L;
     private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(CoverTree.class);
 
     /**

File: core/src/main/java/smile/neighbor/KDTree.java
Patch:
@@ -54,7 +54,7 @@
  * @author Haifeng Li
  */
 public class KDTree <E> implements NearestNeighborSearch<double[], E>, KNNSearch<double[], E>, RNNSearch<double[], E>, Serializable {
-    private static final long serialVersionUID = 1L;
+    private static final long serialVersionUID = 2L;
 
     /**
      * The root in the KD-tree.

File: core/src/main/java/smile/neighbor/SNLSH.java
Patch:
@@ -59,7 +59,7 @@
  * @author Qiyang Zuo
  */
 public class SNLSH<K, V> implements RNNSearch<K, V>, Serializable {
-    private static final long serialVersionUID = 1L;
+    private static final long serialVersionUID = 2L;
 
     /** Hash function mask. */
     private final long mask;

File: core/src/main/java/smile/projection/GHA.java
Patch:
@@ -57,7 +57,7 @@
  * @author Haifeng Li
  */
 public class GHA implements LinearProjection, Serializable {
-    private static final long serialVersionUID = 1L;
+    private static final long serialVersionUID = 2L;
 
     /**
      * The dimension of feature space.

File: core/src/main/java/smile/projection/KPCA.java
Patch:
@@ -60,7 +60,7 @@
  * @author Haifeng Li
  */
 public class KPCA<T> implements Projection<T>, Serializable {
-    private static final long serialVersionUID = 1L;
+    private static final long serialVersionUID = 2L;
 
     /**
      * The dimension of feature space.

File: core/src/main/java/smile/projection/PCA.java
Patch:
@@ -63,7 +63,7 @@
  * @author Haifeng Li
  */
 public class PCA implements LinearProjection, Serializable {
-    private static final long serialVersionUID = 1L;
+    private static final long serialVersionUID = 2L;
 
     /**
      * The dimension of feature space.

File: core/src/main/java/smile/projection/PPCA.java
Patch:
@@ -47,7 +47,7 @@
  * @author Haifeng Li
  */
 public class PPCA implements LinearProjection, Serializable {
-    private static final long serialVersionUID = 1L;
+    private static final long serialVersionUID = 2L;
 
     /**
      * The sample mean.

File: core/src/main/java/smile/projection/RandomProjection.java
Patch:
@@ -55,7 +55,7 @@
  * @author Haifeng Li
  */
 public class RandomProjection implements LinearProjection, Serializable {
-    private static final long serialVersionUID = 1L;
+    private static final long serialVersionUID = 2L;
 
     /**
      * Probability distribution to generate random projection.

File: core/src/main/java/smile/clustering/GMeans.java
Patch:
@@ -92,7 +92,6 @@ public static GMeans fit(double[][] data, int kmax, int maxIter, double tol) {
         double[][] centroids = {mean};
 
         double distortion = Arrays.stream(data).parallel().mapToDouble(x -> MathEx.squaredDistance(x, mean)).sum();
-        logger.info(String.format("distortion after initialization: %.4f", distortion));
 
         BBDTree bbd = new BBDTree(data);
         KMeans[] kmeans = new KMeans[kmax];
@@ -162,7 +161,7 @@ public static GMeans fit(double[][] data, int kmax, int maxIter, double tol) {
             centroids = centers.toArray(new double[k][]);
 
             double diff = Double.MAX_VALUE;
-            for (int iter = 1; iter <= maxIter && diff < tol; iter++) {
+            for (int iter = 1; iter <= maxIter && diff > tol; iter++) {
                 double wcss = bbd.clustering(centroids, sum, size, y);
 
                 diff = distortion - wcss;

File: core/src/main/java/smile/clustering/XMeans.java
Patch:
@@ -97,7 +97,6 @@ public static XMeans fit(double[][] data, int kmax, int maxIter, double tol) {
         double distortion = Arrays.stream(data).parallel().mapToDouble(x -> MathEx.squaredDistance(x, mean)).sum();
         double[] distortions = new double[kmax];
         distortions[0] = distortion;
-        logger.info(String.format("distortion after initialization: %.4f", distortion));
 
         BBDTree bbd = new BBDTree(data);
         KMeans[] kmeans = new KMeans[kmax];
@@ -156,7 +155,7 @@ public static XMeans fit(double[][] data, int kmax, int maxIter, double tol) {
             centroids = centers.toArray(new double[k][]);
 
             double diff = Double.MAX_VALUE;
-            for (int iter = 1; iter <= maxIter && diff < tol; iter++) {
+            for (int iter = 1; iter <= maxIter && diff > tol; iter++) {
                 double wcss = bbd.clustering(centroids, sum, size, y);
 
                 diff = distortion - wcss;

File: core/src/main/java/smile/projection/ICA.java
Patch:
@@ -60,6 +60,7 @@
  */
 public class ICA implements Serializable {
     private static final long serialVersionUID = 2L;
+    private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(ICA.class);
 
     /**
      * The independent components (row-wise).
@@ -161,7 +162,7 @@ public static ICA fit(double[][] data, int p, DifferentiableFunction contrast, d
             double[] w = W[i];
 
             double diff = Double.MAX_VALUE;
-            for (int iter = 0; diff > tol && iter < maxIter; iter++) {
+            for (int iter = 0; iter < maxIter && diff > tol; iter++) {
                 System.arraycopy(w, 0, wold, 0, n);
 
                 // Calculate derivative of projection
@@ -213,7 +214,7 @@ public static ICA fit(double[][] data, int p, DifferentiableFunction contrast, d
             }
 
             if (diff > tol) {
-                throw new IllegalStateException(String.format("Component %d did not converge in %d iterations.", i, maxIter));
+                logger.warn(String.format("Component %d did not converge in %d iterations.", i, maxIter));
             }
         }
 

File: data/src/main/java/smile/data/formula/Delete.java
Patch:
@@ -41,7 +41,7 @@ public Delete(HyperTerm x) {
 
     @Override
     public String toString() {
-        return String.format("-%s", x);
+        return String.format("- %s", x);
     }
 
     @Override

File: data/src/test/java/smile/data/formula/FormulaTest.java
Patch:
@@ -203,7 +203,7 @@ public void testCrossing() {
                 new StructField("sowing_density", DataTypes.ByteType, new NominalScale("low", "high")),
                 new StructField("wind", DataTypes.ByteType, new NominalScale("weak", "strong"))
         );
-        assertEquals(" ~ (water + sowing_density + wind)^2", formula.toString());
+        assertEquals(" ~ (water x sowing_density x wind)^2", formula.toString());
 
         StructType outputSchema = formula.bind(inputSchema);
         StructType schema = DataTypes.struct(

File: core/src/main/java/smile/feature/GAFE.java
Patch:
@@ -185,7 +185,7 @@ public static FitnessMeasure<BitString> fitness(double[][] x, double[] y, double
         return chromosome -> {
             byte[] bits = chromosome.bits();
             int[] features = indexOfOnes(bits);
-            if (features == null) return 0.0;
+            if (features == null) return Double.NEGATIVE_INFINITY;
 
             double[][] xx = select(x, features);
             double[][] testxx = select(testx, features);
@@ -254,7 +254,7 @@ public static FitnessMeasure<BitString> fitness(String y, DataFrame train, DataF
         return chromosome -> {
             byte[] bits = chromosome.bits();
             String[] features = selectedFeatures(bits, names, y);
-            if (features == null) return 0.0;
+            if (features == null) return Double.NEGATIVE_INFINITY;
 
             Formula formula = Formula.of(y, features);
             DataFrameRegression model = trainer.apply(formula, train);

File: nd4j/src/main/java/smile/nd4j/NDMatrix.java
Patch:
@@ -40,7 +40,9 @@ public class NDMatrix implements DenseMatrix {
         // (float). Here we set the order globally to double precision.
         // Alternatively, we can set the property when launching the JVM:
         // -Ddtype=double
-        Nd4j.setDataType(org.nd4j.linalg.api.buffer.DataBuffer.Type.DOUBLE);
+
+        // since beta4
+        Nd4j.setDefaultDataTypes(org.nd4j.linalg.api.buffer.DataType.DOUBLE, org.nd4j.linalg.api.buffer.DataType.DOUBLE);
     }
 
     /**

File: core/src/test/java/smile/neighbor/BKTreeTest.java
Patch:
@@ -42,7 +42,7 @@ public class BKTreeTest {
 
     public BKTreeTest() {
         long start = System.currentTimeMillis();
-        bktree = new BKTree<>(new EditDistance(true));
+        bktree = new BKTree<>(new EditDistance(50, true));
         bktree.add(words);
         double time = (System.currentTimeMillis() - start) / 1000.0;
         System.out.format("Building BK-tree: %.2fs%n", time);

File: math/src/main/java/smile/math/distance/EditDistance.java
Patch:
@@ -438,7 +438,7 @@ public void f(char[] x, char[] y, IntArray2D FKP, int ZERO_K, int k, int p) {
             int t = FKP.get(k + ZERO_K, p) + 1;
             int mnk = Math.min(x.length, y.length - k);
 
-            if (t > 1 && k + t > 1 && t < mnk) {
+            if (t >= 1 && k + t >= 1 && t < mnk) {
                 if (x[t - 1] == y[k + t] && x[t] == y[k + t - 1]) {
                     t++;
                 }
@@ -458,7 +458,7 @@ public void f(String x, String y, IntArray2D FKP, int ZERO_K, int k, int p) {
             int t = FKP.get(k + ZERO_K, p) + 1;
             int mnk = Math.min(x.length(), y.length() - k);
 
-            if (t > 1 && k + t > 1 && t < mnk) {
+            if (t >= 1 && k + t >= 1 && t < mnk) {
                 if (x.charAt(t - 1) == y.charAt(k + t) && x.charAt(t) == y.charAt(k + t - 1)) {
                     t++;
                 }

File: core/src/main/java/smile/neighbor/BKTree.java
Patch:
@@ -203,7 +203,7 @@ private void search(Node node, E q, int k, List<Neighbor<E, E>> neighbors) {
 
     @Override
     public void range(E q, double radius, List<Neighbor<E, E>> neighbors) {
-        if (radius != (int) radius) {
+        if (radius <= 0 || radius != (int) radius) {
             throw new IllegalArgumentException("The parameter radius has to be an integer: " + radius);
         }
         

File: data/src/main/java/smile/data/vector/Vector.java
Patch:
@@ -39,6 +39,9 @@
  * @author Haifeng Li
  */
 public interface Vector<T> extends BaseVector<T, T, Stream<T>> {
+    /** Returns the array of elements. */
+    T[] toArray();
+
     /**
      * Returns a vector of LocalDate. If the vector is of strings, it uses the default
      * ISO date formatter that parses a date without an offset, such as '2011-12-03'.

File: demo/src/main/java/smile/demo/manifold/LaplacianEigenmapDemo.java
Patch:
@@ -62,9 +62,9 @@ public JComponent learn() {
         sigmaField.setEnabled(false);
 
         double[][] data = dataset[datasetIndex].toArray();
-        if (data.length > 1000) {
-            double[][] x = new double[1000][];
-            for (int i = 0; i < 1000; i++) {
+        if (data.length > 2000) {
+            double[][] x = new double[2000][];
+            for (int i = 0; i < 2000; i++) {
                 x[i] = data[i];
             }
             data = x;

File: demo/src/main/java/smile/demo/manifold/ManifoldDemo.java
Patch:
@@ -110,6 +110,7 @@ public void run() {
             validate();
         } catch (Exception ex) {
             System.err.println(ex);
+            ex.printStackTrace();
         }
         
         startButton.setEnabled(true);

File: demo/src/main/java/smile/demo/neighbor/ApproximateStringSearchDemo.java
Patch:
@@ -97,7 +97,7 @@ public void run() {
             List<String> words = new ArrayList<>();
 
             try {
-                FileInputStream stream = new FileInputStream(smile.util.Paths.getTestData("index.noun").toFile());
+                FileInputStream stream = new FileInputStream(smile.util.Paths.getTestData("neighbor/index.noun").toFile());
                 BufferedReader input = new BufferedReader(new InputStreamReader(stream));
                 String line = input.readLine();
                 while (line != null) {

File: demo/src/main/java/smile/demo/plot/SparseMatrixPlotDemo.java
Patch:
@@ -35,17 +35,18 @@ public SparseMatrixPlotDemo() {
         super(new GridLayout(1,2));
 
         try {
-            SparseMatrix m1 = SparseMatrix.harwell(smile.util.Paths.getTestData("matrix/08blocks.txt"));
+            SparseMatrix m1 = SparseMatrix.text(smile.util.Paths.getTestData("matrix/08blocks.txt"));
             PlotCanvas canvas = SparseMatrixPlot.plot(m1);
             canvas.setTitle("08blocks");
             add(canvas);
 
-            SparseMatrix m2 = SparseMatrix.harwell(smile.util.Paths.getTestData("matrix/mesh2em5.txt"));
+            SparseMatrix m2 = SparseMatrix.text(smile.util.Paths.getTestData("matrix/mesh2em5.txt"));
             canvas = SparseMatrixPlot.plot(m2, Palette.jet(256));
             canvas.setTitle("mesh2em5");
             add(canvas);
         } catch (Exception ex) {
             System.err.println(ex);
+            ex.printStackTrace();
         }
     }
 

File: core/src/main/java/smile/base/cart/CART.java
Patch:
@@ -283,7 +283,7 @@ protected Optional<Split> findBestSplit(LeafNode node, int lo, int hi, boolean[]
 
         // skip the pure columns
         int p = schema.length();
-        int[] columns = IntStream.range(0, p).filter(i -> pure == null || !pure[i]).toArray();
+        int[] columns = IntStream.range(0, p).filter(i -> !pure[i]).toArray();
 
         // random forest
         if (mtry < p) {

File: core/src/main/java/smile/base/cart/LeafNode.java
Patch:
@@ -50,5 +50,5 @@ public int depth() {
     }
 
     @Override
-    public Node toLeaf() { return this; }
+    public Node merge() { return this; }
 }

File: core/src/main/java/smile/base/cart/Node.java
Patch:
@@ -53,6 +53,6 @@ public interface Node extends Serializable {
      * Try to merge the children nodes and return a leaf node.
      * If not able to merge, return this node itself.
      */
-    Node toLeaf();
+    Node merge();
 }
 

File: core/src/main/java/smile/base/cart/RegressionNodeOutput.java
Patch:
@@ -27,8 +27,6 @@ public interface RegressionNodeOutput {
     /**
      * Calculate the node output.
      *
-     * @param lo the inclusive lower bound of the data partition in the reordered sample index array.
-     * @param hi the exclusive upper bound of the data partition in the reordered sample index array.
      * @param nodeSamples the index of node samples to their original locations in training dataset.
      * @param sampleCount samples[i] is the number of sampling of dataset[i]. 0 means that the
      *               datum is not included and values of greater than 1 are

File: core/src/main/java/smile/classification/DecisionTree.java
Patch:
@@ -273,7 +273,7 @@ public DecisionTree(DataFrame x, BaseVector y, int k, SplitRule rule, int maxNod
         }
 
         // merge the sister leaves that produce the same output.
-        this.root = this.root.toLeaf();
+        this.root = this.root.merge();
 
         clear();
     }

File: core/src/main/java/smile/classification/RandomForest.java
Patch:
@@ -291,9 +291,9 @@ public static RandomForest fit(Formula formula, DataFrame data, int ntrees, int
                     // We used to do up sampling.
                     // But we switch to down sampling, which seems has better performance.
                     int size = (int) Math.round(subsample * cl.length / weight[l]);
-                    int[] perm = MathEx.permutate(cl.length);
+                    int[] permutation = MathEx.permutate(cl.length);
                     for (int i = 0; i < size; i++) {
-                        samples[cl[perm[i]]] += 1; //classWeight[l];
+                        samples[cl[permutation[i]]] += 1; //classWeight[l];
                     }
                 });
             }

File: core/src/test/java/smile/regression/RLSTest.java
Patch:
@@ -68,13 +68,13 @@ public void testLongley(){
         DataFrame online = Longley.data.of(IntStream.range(n/2, n).toArray());
         LinearModel model = OLS.fit(Longley.formula, batch);
         double[] prediction = Validation.test(model, online);
-        double rmse = RMSE.apply(Longley.y, prediction);
+        double rmse = RMSE.apply(Longley.formula.y(online).toDoubleArray(), prediction);
         System.out.println("Batch RMSE = " + rmse);
         assertEquals(6.229663, rmse, 1E-4);
 
         model.update(online);
         prediction = Validation.test(model, online);
-        rmse = RMSE.apply(Longley.y, prediction);
+        rmse = RMSE.apply(Longley.formula.y(online).toDoubleArray(), prediction);
         System.out.println("Online RMSE = " + rmse);
         assertEquals(0.973663, rmse, 1E-4);
     }

File: core/src/main/java/smile/feature/MaxAbsScaler.java
Patch:
@@ -98,7 +98,7 @@ public Tuple transform(Tuple x) {
             throw new IllegalArgumentException(String.format("Invalid schema %s, expected %s", x.schema(), schema));
         }
 
-        return new Tuple() {
+        return new smile.data.AbstractTuple() {
             @Override
             public Object get(int i) {
                 if (DataType.isDouble(schema.field(i).type)) {

File: core/src/main/java/smile/feature/Scaler.java
Patch:
@@ -114,7 +114,7 @@ public Tuple transform(Tuple x) {
             throw new IllegalArgumentException(String.format("Invalid schema %s, expected %s", x.schema(), schema));
         }
 
-        return new Tuple() {
+        return new smile.data.AbstractTuple() {
             @Override
             public Object get(int i) {
                 if (DataType.isDouble(schema.field(i).type)) {

File: core/src/main/java/smile/feature/Standardizer.java
Patch:
@@ -110,7 +110,7 @@ public Tuple transform(Tuple x) {
             throw new IllegalArgumentException(String.format("Invalid schema %s, expected %s", x.schema(), schema));
         }
 
-        return new Tuple() {
+        return new smile.data.AbstractTuple() {
             @Override
             public Object get(int i) {
                 if (DataType.isDouble(schema.field(i).type)) {

File: data/src/main/java/smile/data/formula/Formula.java
Patch:
@@ -148,7 +148,7 @@ public Term[] terms() {
      * Apply the formula on a tuple to generate the model data.
      */
     public Tuple apply(Tuple t) {
-        return new Tuple() {
+        return new smile.data.AbstractTuple() {
             @Override
             public StructType schema() {
                 return schema;

File: data/src/main/java/smile/data/vector/BaseVector.java
Patch:
@@ -67,7 +67,7 @@ default double[] toDoubleArray() {
      * @return the input array <code>a</code>.
      */
     default double[] toDoubleArray(double[] a) {
-        throw new UnsupportedOperationException();
+        throw new UnsupportedOperationException(name() + ":" + type());
     }
 
     /**

File: io/src/main/java/smile/io/Arff.java
Patch:
@@ -374,8 +374,8 @@ public DataFrame read(int limit) throws IOException, ParseException {
             rows.add(Tuple.of(row, schema));
         }
 
-        schema.boxed(rows);
-        return DataFrame.of(rows);
+        schema = schema.boxed(rows);
+        return DataFrame.of(rows, schema);
     }
 
     /**

File: io/src/main/java/smile/io/CSV.java
Patch:
@@ -124,8 +124,8 @@ public DataFrame read(Path path, int limit) throws IOException {
                 if (rows.size() >= limit) break;
             }
 
-            schema.boxed(rows);
-            return DataFrame.of(rows);
+            schema = schema.boxed(rows);
+            return DataFrame.of(rows, schema);
         }
     }
 

File: io/src/main/java/smile/io/JSON.java
Patch:
@@ -131,8 +131,8 @@ public DataFrame read(Path path, int limit) throws IOException {
             });
         }
 
-        schema.boxed(rows);
-        return DataFrame.of(rows);
+        schema = schema.boxed(rows);
+        return DataFrame.of(rows, schema);
     }
 
     /** Converts a map to tuple. */

File: core/src/main/java/smile/base/cart/DecisionNode.java
Patch:
@@ -54,7 +54,7 @@ public int[] count() {
     }
 
     @Override
-    public String toDot(StructType schema, int id) {
+    public String dot(StructType schema, int id) {
         return String.format(" %d [label=<class = %d>, fillcolor=\"#00000000\", shape=ellipse];\n", id, output);
     }
 

File: core/src/main/java/smile/base/cart/Node.java
Patch:
@@ -32,8 +32,10 @@ public interface Node extends Serializable {
 
     /**
      * Returns a dot representation for visualization.
+     * @param schema data schema
+     * @param id node id
      */
-    String toDot(StructType schema, int id);
+    String dot(StructType schema, int id);
 
     /** Returns the number of samples in the node. */
     int size();

File: core/src/main/java/smile/base/cart/NominalNode.java
Patch:
@@ -46,7 +46,7 @@ public LeafNode predict(Tuple x) {
     }
 
     @Override
-    public String toDot(StructType schema, int id) {
+    public String dot(StructType schema, int id) {
         StructField field = schema.field(feature);
         Measure measure = field.measure;
         String valueStr = (measure != null && measure instanceof DiscreteMeasure) ?

File: core/src/main/java/smile/base/cart/OrdinalNode.java
Patch:
@@ -46,7 +46,7 @@ public LeafNode predict(Tuple x) {
     }
 
     @Override
-    public String toDot(StructType schema, int id) {
+    public String dot(StructType schema, int id) {
         StructField field = schema.field(feature);
         Measure measure = field.measure;
         String valueStr = (measure != null && measure instanceof DiscreteMeasure) ?

File: core/src/main/java/smile/base/cart/RegressionNode.java
Patch:
@@ -75,7 +75,7 @@ public double impurity() {
     }
 
     @Override
-    public String toDot(StructType schema, int id) {
+    public String dot(StructType schema, int id) {
         return String.format(" %d [label=<%.4f>, fillcolor=\"#00000000\", shape=ellipse];\n", id, output);
     }
 

File: data/src/main/java/smile/data/formula/Formula.java
Patch:
@@ -92,7 +92,7 @@ public Formula(Term response, HyperTerm[] predictors) {
 
     @Override
     public String toString() {
-        return String.format("%s ~ %",
+        return String.format("%s ~ %s",
                 response.map(Objects::toString).orElse(""),
                 Arrays.stream(predictors).map(Objects::toString).collect(Collectors.joining(" + ")));
     }

File: core/src/main/java/smile/base/cart/CART.java
Patch:
@@ -120,7 +120,7 @@ public CART(DataFrame x, BaseVector y, int nodeSize, int maxNodes, int mtry, int
         this.mtry = mtry;
 
         if (mtry < 1 || mtry > schema.length()) {
-            logger.warn("Invalid number of variables to split on at a node of the tree: . Use all features." + mtry);
+            logger.debug("Invalid mtry. Use all features.");
             this.mtry = schema.length();
         }
 

File: core/src/main/java/smile/regression/LinearModel.java
Patch:
@@ -412,10 +412,10 @@ public String toString() {
 
         builder.append("\nCoefficients:\n");
         if (ttest != null) {
-            builder.append("            Estimate        Std. Error        t value        Pr(>|t|)\n");
-            builder.append(String.format("Intercept%11.4f%18.4f%15.4f%16.4f %s%n", ttest[p][0], ttest[p][1], ttest[p][2], ttest[p][3], significance(ttest[p][3])));
+            builder.append("                  Estimate Std. Error    t value   Pr(>|t|)\n");
+            builder.append(String.format("Intercept       %10.4f %10.4f %10.4f %10.4f %s%n", ttest[p][0], ttest[p][1], ttest[p][2], ttest[p][3], significance(ttest[p][3])));
             for (int i = 0; i < p; i++) {
-                builder.append(String.format("%s\t %11.4f%18.4f%15.4f%16.4f %s%n", names[i], ttest[i][0], ttest[i][1], ttest[i][2], ttest[i][3], significance(ttest[i][3])));
+                builder.append(String.format("%-15s %10.4f %10.4f %10.4f %10.4f %s%n", names[i], ttest[i][0], ttest[i][1], ttest[i][2], ttest[i][3], significance(ttest[i][3])));
             }
 
             builder.append("---------------------------------------------------------------------\n");

File: core/src/test/java/smile/data/Abalone.java
Patch:
@@ -58,8 +58,8 @@ public class Abalone {
         csv.schema(schema);
 
         try {
-            train = csv.read(Paths.getTestData("regression/abalone-train.csv"));
-            test = csv.read(Paths.getTestData("regression/abalone-test.csv"));
+            train = csv.read(Paths.getTestData("regression/abalone-train.data"));
+            test = csv.read(Paths.getTestData("regression/abalone-test.data"));
 
             x = formula.frame(train).toArray();
             y = formula.response(train).toDoubleArray();

File: core/src/test/java/smile/data/AutoMPG.java
Patch:
@@ -35,7 +35,7 @@ public class AutoMPG {
 
     static {
         try {
-            Arff arff = new Arff(Paths.getTestData("weka/autoMpg.arff"));
+            Arff arff = new Arff(Paths.getTestData("weka/regression/autoMpg.arff"));
             data = arff.read();
 
             x = formula.frame(data).toArray();

File: data/src/main/java/smile/data/IndexDataFrame.java
Patch:
@@ -92,7 +92,7 @@ public Stream<Tuple> stream() {
 
     @Override
     public BaseVector column(int i) {
-        return df.column(i);
+        return df.column(i).get(index);
     }
 
     @Override

File: data/src/main/java/smile/data/Tuple.java
Patch:
@@ -282,8 +282,7 @@ default String toString(int i) {
         if (o instanceof String) {
             return (String) o;
         } else {
-            Measure scale = schema().measure(schema().fieldName(i));
-            return scale != null ? scale.toString(o) : schema().field(i).type.toString(o);
+            return schema().field(i).toString(o);
         }
     }
 
@@ -372,7 +371,7 @@ default java.time.LocalDateTime getDateTime(String field) {
      * @throws ClassCastException when the data is not nominal or ordinal.
      */
     default String getScale(int i) {
-        return ((DiscreteMeasure) schema().measure(schema().fieldName(i))).toString(getInt(i));
+        return ((DiscreteMeasure) schema().field(i).measure).toString(getInt(i));
     }
 
     /**

File: data/src/main/java/smile/data/formula/OneHot.java
Patch:
@@ -69,8 +69,7 @@ public Set<String> variables() {
     public void bind(StructType schema) {
         index = schema.fieldIndex(name);
 
-        Measure measure = schema.measure(name);
-
+        Measure measure = schema.field(index).measure;
         if (measure == null || !(measure instanceof NominalScale)) {
             throw new UnsupportedOperationException(String.format("The variable %s is not of nominal", name));
         }

File: data/src/main/java/smile/data/vector/StringVector.java
Patch:
@@ -86,7 +86,7 @@ default Vector<LocalDateTime> toDateTime(String pattern) {
      * in the nominal scale, the type of returned vector may be byte, short
      * or integer. The missing values/nulls will be converted to -1.
      */
-    BaseVector toFactor(DiscreteMeasure scale);
+    BaseVector factorize(DiscreteMeasure scale);
 
     /**
      * Returns the string representation of vector.

File: data/src/main/java/smile/data/vector/StringVectorImpl.java
Patch:
@@ -81,7 +81,7 @@ public NominalScale nominal() {
     }
 
     @Override
-    public BaseVector toFactor(DiscreteMeasure scale) {
+    public BaseVector factorize(DiscreteMeasure scale) {
         int[] data = stream().mapToInt(s -> s == null ? -1 : (int) scale.valueOf(s)).toArray();
 
         switch (scale.type().id()) {

File: io/src/test/java/smile/io/ArffTest.java
Patch:
@@ -138,6 +138,8 @@ public void testParseString() throws Exception {
                 new StructField("LCSH", DataTypes.StringType));
         assertEquals(schema, string.schema());
 
+        System.out.println(string);
+        System.out.println(string.schema());
         assertEquals(5, string.nrows());
         assertEquals(2, string.ncols());
         assertEquals("AG5", string.get(0).get(0));

File: io/src/test/java/smile/io/ArrowTest.java
Patch:
@@ -120,7 +120,7 @@ public void testSchema() {
                 new StructField("Customer First", DataTypes.StringType),
                 new StructField("Customer Last", DataTypes.StringType),
                 new StructField("Country", DataTypes.StringType),
-                new StructField("Total", DataTypes.DoubleObjectType)
+                new StructField("Total", DataTypes.DoubleType)
         );
         assertEquals(schema, df.schema());
     }

File: core/src/test/java/smile/classification/MaxentTest.java
Patch:
@@ -146,6 +146,6 @@ public void testLearnHyphen() {
 
         System.out.format("Protein error is %d of %d%n", error, test.x.length);
         System.out.format("Hyphen error rate = %.2f%%%n", 100.0 * error / test.x.length);
-        assertEquals(765, error);
+        assertEquals(768, error);
     }
-}
\ No newline at end of file
+}

File: core/src/test/java/smile/classification/MaxentTest.java
Patch:
@@ -146,6 +146,6 @@ public void testLearnHyphen() {
 
         System.out.format("Protein error is %d of %d%n", error, test.x.length);
         System.out.format("Hyphen error rate = %.2f%%%n", 100.0 * error / test.x.length);
-        assertEquals(765, error);
+        assertEquals(768, error);
     }
-}
\ No newline at end of file
+}

File: core/src/main/java/smile/classification/DecisionTree.java
Patch:
@@ -1020,7 +1020,9 @@ public DecisionTree(Attribute[] attributes, double[][] x, int[] y, int maxNodes,
                 if (node == null) {
                     break;
                 }
-                node.split(nextSplits); // Split the parent node into two children nodes
+                if(!node.split(nextSplits)) { // Split the parent node into two children nodes
+                    leaves--;
+                }
             }
         }
 

File: core/src/main/java/smile/classification/DecisionTree.java
Patch:
@@ -328,6 +328,8 @@ private void markAsLeaf() {
             this.splitFeature = -1;
             this.splitValue = Double.NaN;
             this.splitScore = 0.0;
+            this.trueChild = null;
+            this.falseChild = null;
         }
 
         /**

File: core/src/main/java/smile/classification/LDA.java
Patch:
@@ -269,7 +269,7 @@ public LDA(double[][] x, int[] y, double[] priori, double tol) {
         this.priori = priori;
         ct = new double[k];
         for (int i = 0; i < k; i++) {
-            ct[i] = MathEx.log(priori[i]);
+            ct[i] = Math.log(priori[i]);
         }
         
         for (int i = 0; i < n; i++) {
@@ -359,7 +359,7 @@ public int predict(double[] x, double[] posteriori) {
         if (posteriori != null) {
             double sum = 0.0;
             for (int i = 0; i < k; i++) {
-                posteriori[i] = MathEx.exp(posteriori[i] - max);
+                posteriori[i] = Math.exp(posteriori[i] - max);
                 sum += posteriori[i];
             }
             

File: core/src/main/java/smile/classification/SVM.java
Patch:
@@ -1529,7 +1529,7 @@ private double posterior(LASVM svm, double y) {
         final double minProb = 1e-7;
         final double maxProb = 1 - minProb;
 
-        return MathEx.min(MathEx.max(svm.platt.predict(y), minProb), maxProb);
+        return Math.min(Math.max(svm.platt.predict(y), minProb), maxProb);
     }
 
     @Override

File: core/src/main/java/smile/clustering/BIRCH.java
Patch:
@@ -126,7 +126,7 @@ class Node implements Serializable {
                 double d = sum[i] / n - x[i];
                 dist += d * d;
             }
-            return MathEx.sqrt(dist);
+            return Math.sqrt(dist);
         }
 
         /**
@@ -138,7 +138,7 @@ class Node implements Serializable {
                 double d = sum[i] / n - node.sum[i] / node.n;
                 dist += d * d;
             }
-            return MathEx.sqrt(dist);
+            return Math.sqrt(dist);
         }
 
         /**

File: core/src/main/java/smile/clustering/CLARANS.java
Patch:
@@ -98,7 +98,7 @@ public CLARANS(T[] data, Distance<T> distance, int k) {
      * @param maxNeighbor the maximum number of neighbors examined during a random search of local minima.
      */
     public CLARANS(T[] data, Distance<T> distance, int k, int maxNeighbor) {
-        this(data, distance, k, maxNeighbor, MathEx.max(2, MulticoreExecutor.getThreadPoolSize()));
+        this(data, distance, k, maxNeighbor, Math.max(2, MulticoreExecutor.getThreadPoolSize()));
     }
     
     /**

File: core/src/main/java/smile/clustering/GMeans.java
Patch:
@@ -191,7 +191,7 @@ private static double AndersonDarling(double[] x) {
 
         double A = 0.0;
         for (int i = 0; i < n; i++) {
-            A -= (2*i+1) * (MathEx.log(x[i]) + MathEx.log(1-x[n-i-1]));
+            A -= (2*i+1) * (Math.log(x[i]) + Math.log(1-x[n-i-1]));
         }
 
         A = A / n - n;

File: core/src/main/java/smile/gap/BitString.java
Patch:
@@ -190,7 +190,7 @@ public int[] bits() {
 
     @Override
     public int compareTo(Chromosome o) {
-        return (int) MathEx.signum(fitness - o.fitness());
+        return (int) Math.signum(fitness - o.fitness());
     }
     
     @Override

File: core/src/main/java/smile/mds/MDS.java
Patch:
@@ -194,7 +194,7 @@ public MDS(double[][] proximity, int k, boolean add) {
                 throw new IllegalArgumentException(String.format("Some of the first %d eigenvalues are < 0.", k));
             }
 
-            double scale = MathEx.sqrt(eigen.getEigenValues()[j]);
+            double scale = Math.sqrt(eigen.getEigenValues()[j]);
             for (int i = 0; i < n; i++) {
                 coordinates[i][j] = eigen.getEigenVectors().get(i, j) * scale;
             }

File: core/src/main/java/smile/mds/SammonMapping.java
Patch:
@@ -209,7 +209,7 @@ public SammonMapping(double[][] proximity, double[][] init, double lambda, doubl
                         rij += xd * xd;
                         xv[l] = xd;
                     }
-                    rij = MathEx.sqrt(rij);
+                    rij = Math.sqrt(rij);
                     if (rij == 0.0) rij = 1.0E-10;
 
                     double dq = dij - rij;
@@ -222,7 +222,7 @@ public SammonMapping(double[][] proximity, double[][] init, double lambda, doubl
 
                 // Correction
                 for (int l = 0; l < k; l++) {
-                    xu[i][l] = ri[l] + lambda * e1[l] / MathEx.abs(e2[l]);
+                    xu[i][l] = ri[l] + lambda * e1[l] / Math.abs(e2[l]);
                 }
             }
 

File: core/src/main/java/smile/projection/KPCA.java
Patch:
@@ -170,7 +170,7 @@ public KPCA(T[] data, MercerKernel<T> kernel, int k, double threshold) {
         projection = Matrix.zeros(p, n);
         for (int j = 0; j < p; j++) {
             latent[j] = eigen.getEigenValues()[j];
-            double s = MathEx.sqrt(latent[j]);
+            double s = Math.sqrt(latent[j]);
             for (int i = 0; i < n; i++) {
                 projection.set(j, i, eigen.getEigenVectors().get(i, j) / s);
             }

File: core/src/main/java/smile/projection/PCA.java
Patch:
@@ -158,7 +158,7 @@ public PCA(double[][] data, boolean cor) {
             if (cor) {
                 sd = new double[n];
                 for (int i = 0; i < n; i++) {
-                    sd[i] = MathEx.sqrt(cov.get(i, i));
+                    sd[i] = Math.sqrt(cov.get(i, i));
                 }
 
                 for (int i = 0; i < n; i++) {

File: core/src/main/java/smile/projection/PPCA.java
Patch:
@@ -163,7 +163,7 @@ public PPCA(double[][] data, int k) {
         loading = Matrix.zeros(n, k);
         for (int i = 0; i < n; i++) {
             for (int j = 0; j < k; j++) {
-                loading.set(i, j, evectors.get(i, j) * MathEx.sqrt(evalues[j] - noise));
+                loading.set(i, j, evectors.get(i, j) * Math.sqrt(evalues[j] - noise));
             }
         }
 

File: core/src/main/java/smile/regression/NeuralNetwork.java
Patch:
@@ -301,7 +301,7 @@ public NeuralNetwork(ActivationFunction activation, double alpha, double lambda,
         for (int l = 1; l < numLayers; l++) {
             net[l].weight = new double[numUnits[l]][numUnits[l - 1] + 1];
             net[l].delta = new double[numUnits[l]][numUnits[l - 1] + 1];
-            double r = 1.0 / MathEx.sqrt(net[l - 1].units);
+            double r = 1.0 / Math.sqrt(net[l - 1].units);
             for (int i = 0; i < net[l].units; i++) {
                 for (int j = 0; j <= net[l - 1].units; j++) {
                     net[l].weight[i][j] = MathEx.random(-r, r);

File: core/src/main/java/smile/validation/RMSE.java
Patch:
@@ -37,7 +37,7 @@ public double measure(double[] truth, double[] prediction) {
             rss += MathEx.sqr(truth[i] - prediction[i]);
         }
 
-        return MathEx.sqrt(rss/n);
+        return Math.sqrt(rss/n);
     }
 
     @Override

File: core/src/main/java/smile/vq/NeuralGas.java
Patch:
@@ -89,7 +89,7 @@ class Neuron implements Comparable<Neuron> {
 
         @Override
         public int compareTo(Neuron o) {
-            return (int) MathEx.signum(dist - o.dist);
+            return (int) Math.signum(dist - o.dist);
         }
     }
 
@@ -99,7 +99,7 @@ public int compareTo(Neuron o) {
      * of clusters.
      */
     public NeuralGas(double[][] data, int k) {
-        this(data, k, MathEx.min(10, MathEx.max(1, k/2)), 0.01, 0.5, 0.005, 25);
+        this(data, k, Math.min(10, Math.max(1, k/2)), 0.01, 0.5, 0.005, 25);
     }
 
     /**
@@ -188,7 +188,7 @@ public NeuralGas(double[][] data, int k, double lambda_i, double lambda_f, doubl
                 Arrays.sort(nodes);
 
                 for (int i = 0; i < k; i++) {
-                    double delta = eps * MathEx.exp(-i / lambda);
+                    double delta = eps * Math.exp(-i / lambda);
                     if (delta > 0) {
                         for (int j = 0; j < d; j++) {
                             nodes[i].w[j] += delta * (signal[j] - nodes[i].w[j]);

File: core/src/main/java/smile/vq/NeuralMap.java
Patch:
@@ -100,7 +100,7 @@ class Neighbor implements Comparable<Neighbor> {
 
         @Override
         public int compareTo(Neighbor o) {
-            return (int) MathEx.signum(distance - o.distance);
+            return (int) Math.signum(distance - o.distance);
         }
     }
 
@@ -199,7 +199,7 @@ int hash(double[] x) {
                 long r = 0;
                 for (int i = 0; i < k; i++) {
                     double ri = hash(x, i);
-                    r += c[i] * (int) MathEx.floor(ri);
+                    r += c[i] * (int) Math.floor(ri);
                 }
 
                 int h = (int) (r % P);

File: core/src/test/java/smile/imputation/MissingValueImputationTest.java
Patch:
@@ -103,7 +103,7 @@ private double impute(AttributeDataset dataset, MissingValueImputation imputatio
             }
         }
 
-        rmse = MathEx.sqrt(rmse / n);
+        rmse = Math.sqrt(rmse / n);
         return rmse;
     }
 

File: core/src/test/java/smile/manifold/LLETest.java
Patch:
@@ -1080,7 +1080,7 @@ public void testLearn() {
         double[][] coords = lle.getCoordinates();
         for (int i = 0; i < points.length; i++) {
             for (int j = 0; j < points[0].length; j++) {
-                assertEquals(MathEx.abs(points[i][j]), MathEx.abs(coords[i][j]), 1E-4);
+                assertEquals(Math.abs(points[i][j]), Math.abs(coords[i][j]), 1E-4);
             }
         }
     }

File: core/src/test/java/smile/mds/IsotonicMDSTest.java
Patch:
@@ -156,12 +156,12 @@ public void testLearn_doubleArrArr() {
 
         IsotonicMDS mds = new IsotonicMDS(swiss, 2);
 
-        double sign = MathEx.signum(points[0][0] / mds.getCoordinates()[0][0]);
+        double sign = Math.signum(points[0][0] / mds.getCoordinates()[0][0]);
         for (int i = 0; i < points.length; i++) {
             points[i][0] *= sign;
         }
 
-        sign = MathEx.signum(points[0][1] / mds.getCoordinates()[0][1]);
+        sign = Math.signum(points[0][1] / mds.getCoordinates()[0][1]);
         for (int i = 0; i < points.length; i++) {
             points[i][1] *= sign;
         }

File: core/src/test/java/smile/mds/MDSTest.java
Patch:
@@ -119,7 +119,7 @@ public void testLearn_doubleArrArr() {
         double[][] coords = mds.getCoordinates();
         for (int i = 0; i < points.length; i++) {
             for (int j = 0; j < points[0].length; j++) {
-                assertEquals(MathEx.abs(points[i][j]), MathEx.abs(coords[i][j]), 1E-2);
+                assertEquals(Math.abs(points[i][j]), Math.abs(coords[i][j]), 1E-2);
             }
         }
     }
@@ -162,7 +162,7 @@ public void testLearn_doubleArrArr_double() {
         double[][] coords = mds.getCoordinates();
         for (int i = 0; i < points.length; i++) {
             for (int j = 0; j < points[0].length; j++) {
-                assertEquals(MathEx.abs(points[i][j]), MathEx.abs(coords[i][j]), 1E-2);
+                assertEquals(Math.abs(points[i][j]), Math.abs(coords[i][j]), 1E-2);
             }
         }
     }

File: core/src/test/java/smile/mds/SammonMappingTest.java
Patch:
@@ -162,7 +162,7 @@ public void testLearn_doubleArrArr() {
         double[][] coords = sammon.getCoordinates();
         for (int i = 0; i < points.length; i++) {
             for (int j = 0; j < points[0].length; j++) {
-                assertEquals(MathEx.abs(points[i][j]), MathEx.abs(coords[i][j]), 1E-2);
+                assertEquals(Math.abs(points[i][j]), Math.abs(coords[i][j]), 1E-2);
             }
         }
     }

File: core/src/test/java/smile/projection/KPCATest.java
Patch:
@@ -226,7 +226,7 @@ public void testKPCAThreshold() {
             AttributeDataset iris = arffParser.parse(smile.data.parser.IOUtils.getTestDataFile("weka/iris.arff"));
 
             double[][] x = iris.toArray(new double[iris.size()][]);
-            KPCA<double[]> kpca = new KPCA(x, new GaussianKernel(MathEx.sqrt(2.5)), 1E-4);
+            KPCA<double[]> kpca = new KPCA(x, new GaussianKernel(Math.sqrt(2.5)), 1E-4);
             assertTrue(MathEx.equals(latent, kpca.getVariances(), 1E-3));
             double[][] points = kpca.project(x);
             points[0] = kpca.project(x[0]);
@@ -258,7 +258,7 @@ public void testKPCAK() {
             AttributeDataset iris = arffParser.parse(smile.data.parser.IOUtils.getTestDataFile("weka/iris.arff"));
 
             double[][] x = iris.toArray(new double[iris.size()][]);
-            KPCA<double[]> kpca = new KPCA(x, new GaussianKernel(MathEx.sqrt(2.5)), 29);
+            KPCA<double[]> kpca = new KPCA(x, new GaussianKernel(Math.sqrt(2.5)), 29);
             assertTrue(MathEx.equals(latent, kpca.getVariances(), 1E-3));
             double[][] points = kpca.project(x);
             points[0] = kpca.project(x[0]);

File: core/src/test/java/smile/regression/ElasticNetTest.java
Patch:
@@ -85,7 +85,7 @@ public void testCPU() {
                 }
             }
 
-            System.out.println("CPU 10-CV RMSE = " + MathEx.sqrt(rss / n));
+            System.out.println("CPU 10-CV RMSE = " + Math.sqrt(rss / n));
         } catch (Exception ex) {
             System.err.println(ex);
         }
@@ -198,7 +198,7 @@ public void tesDiabetes() {
                 }
             }
 
-            System.out.println("Diabetes 40-CV RMSE = " + MathEx.sqrt(rss / n));
+            System.out.println("Diabetes 40-CV RMSE = " + Math.sqrt(rss / n));
         } catch (Exception ex) {
             System.err.println(ex);
         }

File: core/src/test/java/smile/regression/RandomForestTest.java
Patch:
@@ -128,11 +128,11 @@ public void test(String dataset, String url, int response) {
                 for (int j = 0; j < testx.length; j++) {
                     double r = testy[j] - forest.predict(testx[j]);
                     rss += r * r;
-                    ad += MathEx.abs(r);
+                    ad += Math.abs(r);
                 }
             }
 
-            System.out.format("10-CV RMSE = %.4f \t AbsoluteDeviation = %.4f%n", MathEx.sqrt(rss/n), ad/n);
+            System.out.format("10-CV RMSE = %.4f \t AbsoluteDeviation = %.4f%n", Math.sqrt(rss/n), ad/n);
          } catch (Exception ex) {
              System.err.println(ex);
          }

File: core/src/test/java/smile/regression/RegressionTreeTest.java
Patch:
@@ -128,11 +128,11 @@ public void test(String dataset, String url, int response) {
                 for (int j = 0; j < testx.length; j++) {
                     double r = testy[j] - tree.predict(testx[j]);
                     rss += r * r;
-                    ad += MathEx.abs(r);
+                    ad += Math.abs(r);
                 }
             }
 
-            System.out.format("10-CV RMSE = %.4f \t AbsoluteDeviation = %.4f%n", MathEx.sqrt(rss/n), ad/n);
+            System.out.format("10-CV RMSE = %.4f \t AbsoluteDeviation = %.4f%n", Math.sqrt(rss/n), ad/n);
          } catch (Exception ex) {
              System.err.println(ex);
          }

File: core/src/test/java/smile/regression/SVRTest.java
Patch:
@@ -86,7 +86,7 @@ public void testCPU() {
                 }
             }
 
-            System.out.println("10-CV RMSE = " + MathEx.sqrt(rss / n));
+            System.out.println("10-CV RMSE = " + Math.sqrt(rss / n));
          } catch (Exception ex) {
              System.err.println(ex);
          }

File: core/src/main/java/smile/classification/AdaBoost.java
Patch:
@@ -331,8 +331,8 @@ public AdaBoost(Attribute[] attributes, double[][] x, int[] y, int ntrees, int m
             } else failures = 0;
             
             error[t] = e;
-            alpha[t] = MathEx.log((1-e)/ MathEx.max(1E-10,e)) + b;
-            double a = MathEx.exp(alpha[t]);
+            alpha[t] = Math.log((1-e)/ Math.max(1E-10,e)) + b;
+            double a = Math.exp(alpha[t]);
             for (int i = 0; i < n; i++) {
                 if (err[i]) {
                     w[i] *= a;

File: core/src/main/java/smile/classification/RandomForest.java
Patch:
@@ -379,7 +379,7 @@ public Tree call() {
                 }
 
                 for (int l = 0; l < k; l++) {
-                    int subj = (int) MathEx.round(nc[l] * subsample / classWeight[l]);
+                    int subj = (int) Math.round(nc[l] * subsample / classWeight[l]);
                     int count = 0;
                     for (int i = 0; i < n && count < subj; i++) {
                         int xi = perm[i];
@@ -455,7 +455,7 @@ public RandomForest(double[][] x, int[] y, int ntrees, int mtry) {
      * @param ntrees the number of trees.
      */
     public RandomForest(Attribute[] attributes, double[][] x, int[] y, int ntrees) {
-        this(attributes, x, y, ntrees, (int) MathEx.floor(MathEx.sqrt(x[0].length)));
+        this(attributes, x, y, ntrees, (int) Math.floor(Math.sqrt(x[0].length)));
     }
 
     /**

File: core/src/main/java/smile/regression/GaussianProcessRegression.java
Patch:
@@ -268,7 +268,7 @@ public GaussianProcessRegression(T[] x, double[] y, T[] t, MercerKernel<T> kerne
         DenseMatrix U = eigen.getEigenVectors();
         DenseMatrix D = eigen.getD();
         for (int i = 0; i < m; i++) {
-            D.set(i, i, 1.0 / MathEx.sqrt(D.get(i, i)));
+            D.set(i, i, 1.0 / Math.sqrt(D.get(i, i)));
         }
 
         DenseMatrix UD = U.abmm(D);

File: core/src/main/java/smile/regression/RandomForest.java
Patch:
@@ -313,7 +313,7 @@ public RegressionTree call() {
                 }
 
                 MathEx.permutate(perm);
-                int m = (int) MathEx.round(n * subsample);
+                int m = (int) Math.round(n * subsample);
                 for (int i = 0; i < m; i++) {
                     samples[perm[i]]++;
                 }
@@ -573,7 +573,7 @@ public RandomForest(Attribute[] attributes, double[][] x, double[] y, int ntrees
         }
 
         if (m > 0) {
-            error = MathEx.sqrt(error / m);
+            error = Math.sqrt(error / m);
         }
 
         importance = calculateImportance(trees, attributes.length);

File: core/src/main/java/smile/regression/SVR.java
Patch:
@@ -295,7 +295,7 @@ public SVR(T[] x, double[] y, double[] weight, MercerKernel<T> kernel, double ep
         }
 
         minmax();
-        int phase = MathEx.min(n, 1000);
+        int phase = Math.min(n, 1000);
         for (int count = 1; smo(tol); count++) {
             if (count % phase == 0) {
                 logger.info("SVR finishes {} SMO iterations", count);

File: core/src/main/java/smile/util/SmileUtils.java
Patch:
@@ -20,7 +20,6 @@
 
 import smile.clustering.CLARANS;
 import smile.clustering.KMeans;
-import smile.data.Attribute;
 import smile.math.MathEx;
 import smile.math.distance.Metric;
 import smile.math.rbf.GaussianRadialBasis;

File: core/src/main/java/smile/clustering/FastPair.java
Patch:
@@ -217,14 +217,14 @@ public void updateDistance(int p, int q) {
         float d = linkage.d(p, q);
 
         if (d < distance[p]) {
-            distance[p] = q;
+            distance[p] = d;
             neighbor[p] = q;
         } else if (neighbor[p] == q && d > distance[p]) {
             findNeighbor(p);
         }
 
         if (d < distance[q]) {
-            distance[q] = p;
+            distance[q] = d;
             neighbor[q] = p;
         } else if (neighbor[q] == p && d > distance[q]) {
             findNeighbor(q);

File: core/src/main/java/smile/classification/RandomForest.java
Patch:
@@ -179,7 +179,7 @@ public Trainer(int mtry, int ntrees) {
          */
         public Trainer(Attribute[] attributes, int ntrees) {
             super(attributes);
-            this.mtry = (int) Math.floor(Math.sqrt(attributes.length))
+            this.mtry = (int) Math.floor(Math.sqrt(attributes.length));
             if (ntrees < 1) {
                 throw new IllegalArgumentException("Invalid number of trees: " + ntrees);
             }

File: plot/src/main/java/smile/plot/Heatmap.java
Patch:
@@ -128,7 +128,7 @@ public Heatmap(String[] rowLabels, String[] columnLabels, double[][] z, Color[]
      * @param z a data matrix to be shown in pseudo heat map.
      */
     public Heatmap(double[] x, double[] y, double[][] z) {
-        this(z, 16);
+        this(x, y, z, 16);
     }
 
     /**

File: plot/src/main/java/smile/plot/Heatmap.java
Patch:
@@ -128,7 +128,7 @@ public Heatmap(String[] rowLabels, String[] columnLabels, double[][] z, Color[]
      * @param z a data matrix to be shown in pseudo heat map.
      */
     public Heatmap(double[] x, double[] y, double[][] z) {
-        this(z, 16);
+        this(x, y, z, 16);
     }
 
     /**

File: data/src/main/java/smile/data/measure/ContinuousMeasure.java
Patch:
@@ -13,7 +13,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  *******************************************************************************/
-package smile.data.type;
+package smile.data.measure;
 
 /**
  * Continuous data are numerical data that can theoretically be measured

File: data/src/main/java/smile/data/measure/DiscreteMeasure.java
Patch:
@@ -13,7 +13,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  *******************************************************************************/
-package smile.data.type;
+package smile.data.measure;
 
 import java.util.Arrays;
 import java.util.HashMap;

File: data/src/main/java/smile/data/measure/Measure.java
Patch:
@@ -13,7 +13,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  *******************************************************************************/
-package smile.data.type;
+package smile.data.measure;
 
 import java.io.Serializable;
 

File: data/src/main/java/smile/data/measure/NominalScale.java
Patch:
@@ -13,7 +13,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  *******************************************************************************/
-package smile.data.type;
+package smile.data.measure;
 
 import java.util.Arrays;
 import java.util.HashMap;

File: data/src/main/java/smile/data/measure/OrdinalScale.java
Patch:
@@ -13,7 +13,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  *******************************************************************************/
-package smile.data.type;
+package smile.data.measure;
 
 import java.util.Arrays;
 import java.util.HashMap;

File: data/src/main/java/smile/data/type/StructType.java
Patch:
@@ -18,6 +18,8 @@
 import java.util.*;
 import java.util.stream.Collectors;
 import smile.data.Tuple;
+import smile.data.measure.DiscreteMeasure;
+import smile.data.measure.Measure;
 
 /**
  * Struct data type is determined by the fixed order of the fields

File: io/src/main/java/smile/io/Arff.java
Patch:
@@ -23,10 +23,11 @@
 import java.text.ParseException;
 import java.util.ArrayList;
 import java.util.List;
-import java.util.Map;
 import java.util.HashMap;
+import java.util.Map;
 import smile.data.DataFrame;
 import smile.data.Tuple;
+import smile.data.measure.NominalScale;
 import smile.data.type.*;
 
 /**

File: data/src/main/java/smile/data/DataFrame.java
Patch:
@@ -15,9 +15,7 @@
  *******************************************************************************/
 package smile.data;
 
-import java.sql.JDBCType;
 import java.sql.ResultSet;
-import java.sql.ResultSetMetaData;
 import java.sql.SQLException;
 import java.util.*;
 import java.util.stream.Collector;

File: data/src/main/java/smile/data/type/ContinuousMeasure.java
Patch:
@@ -22,7 +22,7 @@
  *
  * @author Haifeng Li
  */
-public enum ContinuousMeasure {
+public enum ContinuousMeasure implements Measure {
     /**
      * The interval scale allows for the degree of difference between items,
      * but not the ratio between them. Examples include temperature with

File: io/src/main/java/smile/io/IOUtils.java
Patch:
@@ -13,7 +13,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  *******************************************************************************/
-package smile.data.parser;
+package smile.io;
 
 import java.io.*;
 import java.nio.charset.Charset;

File: io/src/main/java/smile/io/package-info.java
Patch:
@@ -19,4 +19,4 @@
  * 
  * @author Haifeng Li
  */
-package smile.data.parser;
+package smile.io;

File: arrow/src/test/java/smile/data/arrow/ArrowTest.java
Patch:
@@ -118,7 +118,7 @@ public void testSchema() {
                 new StructField("Customer First", DataTypes.StringType),
                 new StructField("Customer Last", DataTypes.StringType),
                 new StructField("Country", DataTypes.StringType),
-                new StructField("Total", DataTypes.DoubleType)
+                new StructField("Total", DataTypes.DoubleObjectType)
         );
         assertEquals(schema, df.schema());
     }

File: data/src/main/java/smile/data/formula/Model.java
Patch:
@@ -74,7 +74,7 @@ public StructType bind(StructType inputSchema) {
      */
     public Instance<Tuple> map(Tuple o) {
         DataType type = y.type();
-        if (type == DataTypes.DoubleObjectType || type == DataTypes.FloatType) {
+        if (type == DataTypes.DoubleType || type == DataTypes.FloatType) {
             return new Instance<Tuple>() {
                 @Override
                 public Tuple x() {

File: data/src/main/java/smile/data/DataFrame.java
Patch:
@@ -501,7 +501,7 @@ static DataFrame of(List<Tuple> data) {
      * @param rs The JDBC result set.
      */
     static DataFrame of(ResultSet rs) throws SQLException {
-        StructType schema = DataTypes.struct(rs.getMetaData());
+        StructType schema = DataTypes.struct(rs);
         List<Tuple> rows = new ArrayList<>();
         while (rs.next()) {
             rows.add(Tuple.of(rs, schema));

File: math/src/main/java/smile/math/BFGS.java
Patch:
@@ -84,7 +84,7 @@ public BFGS() {
     }
 
     /** Returns the instance with default settings. */
-    public static BFGS getInstance() {
+    public static BFGS getDefaultInstance() {
         return instance;
     }
 

File: math/src/main/java/smile/math/Root.java
Patch:
@@ -43,7 +43,7 @@ public Root() {
     }
 
     /** Returns the instance with default settings. */
-    public static Root getInstance() {
+    public static Root getDefaultInstance() {
         return instance;
     }
 

File: math/src/test/java/smile/math/BFGSTest.java
Patch:
@@ -87,7 +87,7 @@ public double g(double[] x, double[] g) {
             x[j + 1 - 1] = 1.e0;
         }
 
-        double result = BFGS.getInstance().minimize(func, 5, x);
+        double result = BFGS.getDefaultInstance().minimize(func, 5, x);
         assertEquals(3.2760183604E-14, result, 1E-15);
     }
 
@@ -130,7 +130,7 @@ public double g(double[] x, double[] g) {
             x[j + 1 - 1] = 1.e0;
         }
 
-        double result = BFGS.getInstance().minimize(func, x);
+        double result = BFGS.getDefaultInstance().minimize(func, x);
         assertEquals(2.2388137801857536E-12, result, 1E-15);
     }
 }

File: math/src/test/java/smile/math/RootTest.java
Patch:
@@ -54,7 +54,7 @@ public void tearDown() {
     @Test
     public void testBrent() {
         System.out.println("Brent");
-        double result = Root.getInstance().find(x -> x * x * x + x * x - 5 * x + 3, -4, -2);
+        double result = Root.getDefaultInstance().find(x -> x * x * x + x * x - 5 * x + 3, -4, -2);
         assertEquals(-3, result, 1E-7);
     }
 
@@ -76,7 +76,7 @@ public double g(double x) {
                 return 3 * x * x + 2 * x - 5;
             }
         };
-        double result = Root.getInstance().find(func, -4, -2);
+        double result = Root.getDefaultInstance().find(func, -4, -2);
         assertEquals(-3, result, 1E-7);
     }
 }

File: nd4j/src/main/java/smile/nd4j/LU.java
Patch:
@@ -17,7 +17,6 @@
 package smile.nd4j;
 
 import smile.math.matrix.DenseMatrix;
-import smile.math.matrix.Matrix;
 import org.nd4j.linalg.api.ndarray.INDArray;
 import org.nd4j.linalg.factory.Nd4j;
 

File: nd4j/src/main/java/smile/nd4j/QR.java
Patch:
@@ -63,11 +63,11 @@ public Cholesky CholeskyOfAtA() {
 
     @Override
     public DenseMatrix getR() {
-        int m = qr.nrows();
         int n = qr.ncols();
-        DenseMatrix R = Matrix.zeros(m, n);
+        DenseMatrix R = Matrix.zeros(n, n);
         for (int i = 0; i < n; i++) {
-            for (int j = i; j < n; j++) {
+            R.set(i, i, tau[i]);
+            for (int j = i+1; j < n; j++) {
                 R.set(i, j, qr.get(i, j));
             }
         }

File: netlib/src/main/java/smile/netlib/QR.java
Patch:
@@ -67,11 +67,11 @@ public Cholesky CholeskyOfAtA() {
 
     @Override
     public DenseMatrix getR() {
-        int m = qr.nrows();
         int n = qr.ncols();
-        DenseMatrix R = Matrix.zeros(m, n);
+        DenseMatrix R = Matrix.zeros(n, n);
         for (int i = 0; i < n; i++) {
-            for (int j = i; j < n; j++) {
+            R.set(i, i, tau[i]);
+            for (int j = i+1; j < n; j++) {
                 R.set(i, j, qr.get(i, j));
             }
         }

File: math/src/main/java/smile/math/matrix/DenseMatrix.java
Patch:
@@ -41,7 +41,9 @@ public interface DenseMatrix extends Matrix, MatrixMultiplication<DenseMatrix, D
      *
      * @return the leading dimension
      */
-    int ld();
+    default int ld() {
+        return nrows();
+    }
 
     /**
      * Set the entry value at row i and column j.

File: nd4j/src/test/java/smile/nd4j/EVDTest.java
Patch:
@@ -102,7 +102,7 @@ public void testDecompose2() {
         NDMatrix a = new NDMatrix(A);
         a.setSymmetric(true);
         double[] result = a.eig();
-        assertEquals(2*a.nrows(), result.length);
+        assertEquals(a.nrows(), result.length);
         for (int i = 0; i < eigenValues.length; i++)
             assertEquals(eigenValues[i], result[i], 1E-7);
         for (int i = eigenValues.length; i < result.length; i++)

File: nd4j/src/test/java/smile/nd4j/NDMatrixTest.java
Patch:
@@ -172,7 +172,7 @@ public void testMm() {
 
         assertTrue(MathEx.equals(a.abmm(b).array(), C, 1E-7));
         assertTrue(MathEx.equals(a.abtmm(b).array(), D, 1E-7));
-        assertTrue(MathEx.equals(a.atbmm(b).array(), E, 1E-5));
-        assertTrue(MathEx.equals(a.atbtmm(b).array(), F, 1E-5));
+        assertTrue(MathEx.equals(a.atbmm(b).array(), E, 1E-7));
+        assertTrue(MathEx.equals(a.atbtmm(b).array(), F, 1E-7));
     }
 }

File: data/src/main/java/smile/data/formula/Formula.java
Patch:
@@ -18,10 +18,10 @@
 import java.io.Serializable;
 import java.time.LocalDate;
 import java.time.LocalDateTime;
+import java.time.LocalTime;
 import java.util.*;
 import java.util.function.*;
 import java.util.stream.Collectors;
-import smile.data.DataFrame;
 import smile.data.Tuple;
 import smile.data.type.DataType;
 import smile.data.type.DataTypes;
@@ -790,6 +790,7 @@ public static Factor val(final Object x) {
         final DataType type = x instanceof String ? DataTypes.StringType :
                 x instanceof LocalDate ? DataTypes.DateType :
                 x instanceof LocalDateTime ? DataTypes.DateTimeType :
+                x instanceof LocalTime ? DataTypes.TimeType :
                 DataType.of(x.getClass());
 
         return new Factor() {

File: data/src/main/java/smile/data/type/DateTimeType.java
Patch:
@@ -37,7 +37,9 @@ public class DateTimeType implements DataType {
      * or parses a date without an offset, such as '2011-12-03T10:15:30'.
      */
     DateTimeType() {
-        pattern = "uuuu-MM-dd'T'HH:mm:ss";
+        // This is only an approximation.
+        // ISO_LOCAL_DATE_TIME cannot be fully encoded by a pattern string.
+        pattern = "uuuu-MM-dd'T'HH:mm[:ss]";
         formatter = DateTimeFormatter.ISO_LOCAL_DATE_TIME;
     }
 

File: data/src/main/java/smile/data/type/StructType.java
Patch:
@@ -75,7 +75,7 @@ public int fieldIndex(String field) {
     public String name() {
         return Arrays.stream(fields)
                 .map(field -> String.format("%s: %s", field.name, field.type.name()))
-                .collect(Collectors.joining(",", "struct[", "]"));
+                .collect(Collectors.joining(", ", "struct[", "]"));
     }
 
     @Override
@@ -87,7 +87,7 @@ public String toString() {
     public String toString(Object o) {
         Tuple t = (Tuple) o;
         return Arrays.stream(fields)
-                .map(field -> String.format("%s : %s", field.name, field.type.toString(t.get(field.name))))
+                .map(field -> String.format("%s: %s", field.name, field.type.toString(t.get(field.name))))
                 .collect(Collectors.joining(",\n", "{", "}"));
     }
 

File: data/src/test/java/smile/data/DataFrameTest.java
Patch:
@@ -71,6 +71,7 @@ public DataFrameTest() {
         System.out.println(Arrays.toString(df.types()));
         System.out.println(df.structure());
         System.out.println(df.schema());
+        System.out.println(df.schema().name());
     }
 
     @BeforeClass

File: data/src/main/java/smile/data/DataFrame.java
Patch:
@@ -151,7 +151,7 @@ default DoubleVector doubleColumn(Enum<?> e) {
 
     /** Selects a new DataFrame with given column names. */
     default DataFrame select(String... cols) {
-        int[] indices = Arrays.asList(cols).stream().mapToInt(col -> columnIndex(col)).toArray();
+        int[] indices = Arrays.asList(cols).stream().mapToInt(this::columnIndex).toArray();
         return select(indices);
     }
 
@@ -172,7 +172,7 @@ default DataFrame select(String... cols) {
 
     /** Returns a new DataFrame without given column names. */
     default DataFrame drop(String... cols) {
-        int[] indices = Arrays.asList(cols).stream().mapToInt(col -> columnIndex(col)).toArray();
+        int[] indices = Arrays.asList(cols).stream().mapToInt(this::columnIndex).toArray();
         return drop(indices);
     }
 

File: data/src/main/java/smile/data/type/DataTypes.java
Patch:
@@ -44,6 +44,8 @@ public class DataTypes {
     public static smile.data.type.DateType DateType = new smile.data.type.DateType();
     /** DateTime data type with ISO format. */
     public static smile.data.type.DateTimeType DateTimeType = new smile.data.type.DateTimeType();
+    /** Plain Object data type. */
+    public static smile.data.type.ObjectType ObjectType = smile.data.type.ObjectType.instance;
 
     /** Date data type with customized format. */
     public static smile.data.type.DateType date(String pattern) {
@@ -65,7 +67,7 @@ public static OrdinalType ordinal(String... values) {
         return new OrdinalType(values);
     }
 
-    /** Creates an object data type. */
+    /** Creates an object data type of a given class. */
     public static ObjectType object(Class clazz) {
         return new ObjectType(clazz);
     }

File: data/src/main/java/smile/data/type/NominalType.java
Patch:
@@ -56,7 +56,7 @@ public NominalType(String... values) {
      */
     public NominalType(int... values) {
         this.levels = new String[values.length];
-        this.levels = Arrays.stream(values).mapToObj(v -> String.valueOf(v)).collect(Collectors.toList()).toArray(this.levels);
+        this.levels = Arrays.stream(values).mapToObj(String::valueOf).toArray(String[]::new);
 
         for (int i = 0; i < values.length; i++) {
             map.put(this.levels[i], i);

File: data/src/main/java/smile/data/type/ObjectType.java
Patch:
@@ -21,6 +21,8 @@
  * @author Haifeng Li
  */
 public class ObjectType implements DataType {
+    /** Singleton instance. */
+    static ObjectType instance = new ObjectType(Object.class);
 
     /** Object Class. */
     private Class clazz;

File: data/src/main/java/smile/data/type/OrdinalType.java
Patch:
@@ -59,7 +59,7 @@ public OrdinalType(String... values) {
      */
     public OrdinalType(int... values) {
         this.levels = new String[values.length];
-        this.levels = Arrays.stream(values).mapToObj(v -> String.valueOf(v)).collect(Collectors.toList()).toArray(this.levels);
+        this.levels = Arrays.stream(values).mapToObj(String::valueOf).toArray(String[]::new);
 
         for (int i = 0; i < values.length; i++) {
             map.put(this.levels[i], i);

File: data/src/main/java/smile/data/vector/BaseVector.java
Patch:
@@ -18,6 +18,7 @@
 
 import java.io.Serializable;
 import java.util.stream.BaseStream;
+import smile.data.type.DataType;
 
 /**
  * Base interface for immutable named vectors, which are sequences of elements supporting
@@ -34,7 +35,7 @@ public interface BaseVector<T, TS, S extends BaseStream<TS, S>> extends Serializ
     String name();
 
     /** Returns the element type. */
-    Class<?> type();
+    DataType type();
 
     /** Number of elements in the vector. */
     int size();

File: data/src/main/java/smile/data/type/DataType.java
Patch:
@@ -13,7 +13,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  *******************************************************************************/
-package smile.data;
+package smile.data.type;
 
 import java.io.Serializable;
 import java.text.ParseException;

File: data/src/main/java/smile/data/type/StructField.java
Patch:
@@ -13,7 +13,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  *******************************************************************************/
-package smile.data;
+package smile.data.type;
 
 /**
  * A field in a Struct data type.

File: core/src/test/java/smile/regression/ElasticNetTest.java
Patch:
@@ -249,7 +249,7 @@ public void testWeather() {
             }
 
             System.out.format("Weather 10-CV avg error rate =  %.2f%%%n", (100.0 * error / testSize) / k);
-            assertTrue(error == 9);
+            assertTrue(error <= 10);
         } catch (Exception ex) {
             System.err.println(ex);
         }

File: core/src/test/java/smile/classification/LogisticRegressionTest.java
Patch:
@@ -214,7 +214,7 @@ public void testSegmentWithSgd() {
         ArffParser arffParser = new ArffParser();
         arffParser.setResponseIndex(19);
         try {
-        	AttributeDataset train = arffParser.parse(smile.data.parser.IOUtils.getTestDataFile("weka/segment-challenge.arff"));
+            AttributeDataset train = arffParser.parse(smile.data.parser.IOUtils.getTestDataFile("weka/segment-challenge.arff"));
             AttributeDataset test = arffParser.parse(smile.data.parser.IOUtils.getTestDataFile("weka/segment-test.arff"));
 
             double[][] x = train.toArray(new double[train.size()][]);

File: core/src/main/java/smile/regression/OLS.java
Patch:
@@ -172,7 +172,7 @@ public OLS(double[][] x, double[] y) {
      * @param x a matrix containing the explanatory variables. NO NEED to include a constant column of 1s for bias.
      * @param y the response values.
      * @param SVD If true, use SVD to fit the model. Otherwise, use QR decomposition. SVD is slower than QR but
-     *            can handle rand-deficient matrix.
+     *            can handle rank-deficient matrix.
      */
     public OLS(double[][] x, double[] y, boolean SVD) {
         if (x.length != y.length) {
@@ -208,7 +208,7 @@ public OLS(double[][] x, double[] y, boolean SVD) {
                 logger.warn("Matrix is not of full rank, try SVD instead");
                 SVD = true;
                 svd = X.svd();
-                Arrays.fill(w1, 0.0);
+                Arrays.fill(w1, 0.0);//re-init w1 with zero after exception caught
                 svd.solve(y, w1);
             }
         }
@@ -248,7 +248,7 @@ public OLS(double[][] x, double[] y, boolean SVD) {
                 coefficients[i][0] = w1[i];
                 double s = svd.getSingularValues()[i];
                 if (!Math.isZero(s, 1E-10)) {
-                    double se = error / svd.getSingularValues()[i];
+                    double se = error / s;
                     coefficients[i][1] = se;
                     double t = w1[i] / se;
                     coefficients[i][2] = t;

File: math/src/main/java/smile/math/matrix/QR.java
Patch:
@@ -125,7 +125,7 @@ public DenseMatrix getQ() {
    /**
      * Solve the least squares A*x = b.
      * @param b   right hand side of linear system.
-     * @param x   the output solution vector that minimizes the L2 norm of Q*R*x - b.
+     * @param x   the output solution vector that minimizes the L2 norm of A*x - b.
      * @exception  RuntimeException if matrix is rank deficient.
      */
     public void solve(double[] b, double[] x) {

File: math/src/main/java/smile/math/matrix/SVD.java
Patch:
@@ -253,7 +253,7 @@ public Cholesky CholeskyOfAtA() {
     /**
      * Solve the least squares A*x = b.
      * @param b   right hand side of linear system.
-     * @param x   the output solution vector that minimizes the L2 norm of Q*R*x - b.
+     * @param x   the output solution vector that minimizes the L2 norm of A*x - b.
      * @exception  RuntimeException if matrix is rank deficient.
      */
     public void solve(double[] b, double[] x) {

File: math/src/main/java/smile/math/matrix/QR.java
Patch:
@@ -125,7 +125,7 @@ public DenseMatrix getQ() {
    /**
      * Solve the least squares A*x = b.
      * @param b   right hand side of linear system.
-     * @param x   the output solution vector that minimizes the L2 norm of Q*R*x - b.
+     * @param x   the output solution vector that minimizes the L2 norm of A*x - b.
      * @exception  RuntimeException if matrix is rank deficient.
      */
     public void solve(double[] b, double[] x) {

File: math/src/main/java/smile/math/matrix/SVD.java
Patch:
@@ -253,7 +253,7 @@ public Cholesky CholeskyOfAtA() {
     /**
      * Solve the least squares A*x = b.
      * @param b   right hand side of linear system.
-     * @param x   the output solution vector that minimizes the L2 norm of U*x - b.
+     * @param x   the output solution vector that minimizes the L2 norm of A*x - b.
      * @exception  RuntimeException if matrix is rank deficient.
      */
     public void solve(double[] b, double[] x) {

File: core/src/main/java/smile/regression/OLS.java
Patch:
@@ -172,7 +172,7 @@ public OLS(double[][] x, double[] y) {
      * @param x a matrix containing the explanatory variables. NO NEED to include a constant column of 1s for bias.
      * @param y the response values.
      * @param SVD If true, use SVD to fit the model. Otherwise, use QR decomposition. SVD is slower than QR but
-     *            can handle rand-deficient matrix.
+     *            can handle rank-deficient matrix.
      */
     public OLS(double[][] x, double[] y, boolean SVD) {
         if (x.length != y.length) {
@@ -208,7 +208,7 @@ public OLS(double[][] x, double[] y, boolean SVD) {
                 logger.warn("Matrix is not of full rank, try SVD instead");
                 SVD = true;
                 svd = X.svd();
-                Arrays.fill(w1, 0.0);
+                Arrays.fill(w1, 0.0);//re-init w1 with zero after exception caught
                 svd.solve(y, w1);
             }
         }
@@ -248,7 +248,7 @@ public OLS(double[][] x, double[] y, boolean SVD) {
                 coefficients[i][0] = w1[i];
                 double s = svd.getSingularValues()[i];
                 if (!Math.isZero(s, 1E-10)) {
-                    double se = error / svd.getSingularValues()[i];
+                    double se = error / s;
                     coefficients[i][1] = se;
                     double t = w1[i] / se;
                     coefficients[i][2] = t;

File: math/src/main/java/smile/math/matrix/SVD.java
Patch:
@@ -253,7 +253,7 @@ public Cholesky CholeskyOfAtA() {
     /**
      * Solve the least squares A*x = b.
      * @param b   right hand side of linear system.
-     * @param x   the output solution vector that minimizes the L2 norm of Q*R*x - b.
+     * @param x   the output solution vector that minimizes the L2 norm of U*x - b.
      * @exception  RuntimeException if matrix is rank deficient.
      */
     public void solve(double[] b, double[] x) {

File: core/src/main/java/smile/clustering/GMeans.java
Patch:
@@ -123,7 +123,7 @@ public GMeans(double[][] data, int kmax) {
 
             int[] index = QuickSort.sort(score);
             for (int i = 0; i < k; i++) {
-                if (score[index[i]] <= 1.8692) {
+                if (score[i] <= 1.8692) {
                     centers.add(centroids[index[i]]);
                 }
             }

File: core/src/main/java/smile/clustering/XMeans.java
Patch:
@@ -115,7 +115,7 @@ public XMeans(double[][] data, int kmax) {
 
             int[] index = QuickSort.sort(score);
             for (int i = 0; i < k; i++) {
-                if (score[index[i]] <= 0.0) {
+                if (score[i] <= 0.0) {
                     centers.add(centroids[index[i]]);
                 }
             }

File: core/src/test/java/smile/clustering/DBScanTest.java
Patch:
@@ -52,7 +52,7 @@ public void tearDown() {
     }
 
     /**
-     * Test of learn method, of class DBScan.
+     * Test of learn method, of class DBSCAN.
      */
     @Test
     public void testToy() {

File: demo/src/main/java/smile/demo/clustering/DBScanDemo.java
Patch:
@@ -104,12 +104,12 @@ public JComponent learn() {
 
     @Override
     public String toString() {
-        return "DBScan";
+        return "DBSCAN";
     }
 
     public static void main(String argv[]) {
         ClusteringDemo demo = new DBScanDemo();
-        JFrame f = new JFrame("DBScan");
+        JFrame f = new JFrame("DBSCAN");
         f.setSize(new Dimension(1000, 1000));
         f.setLocationRelativeTo(null);
         f.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE);

File: core/src/test/java/smile/clustering/DBScanTest.java
Patch:
@@ -52,7 +52,7 @@ public void tearDown() {
     }
 
     /**
-     * Test of learn method, of class DBScan.
+     * Test of learn method, of class DBSCAN.
      */
     @Test
     public void testToy() {

File: demo/src/main/java/smile/demo/clustering/DBScanDemo.java
Patch:
@@ -104,12 +104,12 @@ public JComponent learn() {
 
     @Override
     public String toString() {
-        return "DBScan";
+        return "DBSCAN";
     }
 
     public static void main(String argv[]) {
         ClusteringDemo demo = new DBScanDemo();
-        JFrame f = new JFrame("DBScan");
+        JFrame f = new JFrame("DBSCAN");
         f.setSize(new Dimension(1000, 1000));
         f.setLocationRelativeTo(null);
         f.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE);

File: core/src/main/java/smile/classification/NaiveBayes.java
Patch:
@@ -524,7 +524,7 @@ public void learn(SparseArray x, int y) {
                 ntc[y][e.i] += e.x;
                 nt[y] += e.x;
             }
-        } else if (model == Model.MULTINOMIAL) {
+        } else if (model == Model.POLYAURN) {
             for (SparseArray.Entry e : x) {
                 ntc[y][e.i] += e.x * 2;
                 nt[y] += e.x * 2;

File: demo/src/main/java/smile/demo/classification/FLDDemo.java
Patch:
@@ -41,10 +41,11 @@ public double[][] learn(double[] x, double[] y) {
         int[] label = dataset[datasetIndex].toArray(new int[dataset[datasetIndex].size()]);
         
         FLD fisher = new FLD(data, label);
+        int[] pred = new int[label.length];
         for (int i = 0; i < label.length; i++) {
-            label[i] = fisher.predict(data[i]);
+            pred[i] = fisher.predict(data[i]);
         }
-        double trainError = error(label, label);
+        double trainError = error(label, pred);
 
         System.out.format("training error = %.2f%%\n", 100*trainError);
 

File: demo/src/main/java/smile/demo/classification/KNNDemo.java
Patch:
@@ -64,7 +64,7 @@ public double[][] learn(double[] x, double[] y) {
         for (int i = 0; i < label.length; i++) {
             pred[i] = knn.predict(data[i]);
         }
-        double trainError = error(pred, label);
+        double trainError = error(label, pred);
 
         System.out.format("training error = %.2f%%\n", 100*trainError);
 

File: demo/src/main/java/smile/demo/classification/LDADemo.java
Patch:
@@ -41,10 +41,11 @@ public double[][] learn(double[] x, double[] y) {
         int[] label = dataset[datasetIndex].toArray(new int[dataset[datasetIndex].size()]);
         
         LDA lda = new LDA(data, label);
+        int[] pred = new int[label.length];
         for (int i = 0; i < label.length; i++) {
-            label[i] = lda.predict(data[i]);
+            pred[i] = lda.predict(data[i]);
         }
-        double trainError = error(label, label);
+        double trainError = error(label, pred);
 
         System.out.format("training error = %.2f%%\n", 100*trainError);
 

File: demo/src/main/java/smile/demo/classification/LogisticRegressionDemo.java
Patch:
@@ -60,10 +60,11 @@ public double[][] learn(double[] x, double[] y) {
         int[] label = dataset[datasetIndex].toArray(new int[dataset[datasetIndex].size()]);
         
         LogisticRegression logit = new LogisticRegression(data, label, lambda);
+        int[] pred = new int[label.length];
         for (int i = 0; i < label.length; i++) {
-            label[i] = logit.predict(data[i]);
+            pred[i] = logit.predict(data[i]);
         }
-        double trainError = error(label, label);
+        double trainError = error(label, pred);
 
         System.out.format("training error = %.2f%%\n", 100*trainError);
 

File: demo/src/main/java/smile/demo/classification/QDADemo.java
Patch:
@@ -41,10 +41,11 @@ public double[][] learn(double[] x, double[] y) {
         int[] label = dataset[datasetIndex].toArray(new int[dataset[datasetIndex].size()]);
         
         QDA qda = new QDA(data, label);
+        int[] pred = new int[label.length];
         for (int i = 0; i < label.length; i++) {
-            label[i] = qda.predict(data[i]);
+            pred[i] = qda.predict(data[i]);
         }
-        double trainError = error(label, label);
+        double trainError = error(label, pred);
 
         System.out.format("training error = %.2f%%\n", 100*trainError);
 

File: demo/src/main/java/smile/demo/classification/RBFNetworkDemo.java
Patch:
@@ -65,11 +65,11 @@ public double[][] learn(double[] x, double[] y) {
         double[][] centers = new double[k][];
         RadialBasisFunction basis = SmileUtils.learnGaussianRadialBasis(data, centers);
         RBFNetwork<double[]> rbf = new RBFNetwork<>(data, label, new EuclideanDistance(), basis, centers);
-        
+        int[] pred = new int[label.length];
         for (int i = 0; i < label.length; i++) {
-            label[i] = rbf.predict(data[i]);
+            pred[i] = rbf.predict(data[i]);
         }
-        double trainError = error(label, label);
+        double trainError = error(label, pred);
 
         System.out.format("training error = %.2f%%\n", 100*trainError);
 

File: demo/src/main/java/smile/demo/classification/RDADemo.java
Patch:
@@ -60,10 +60,11 @@ public double[][] learn(double[] x, double[] y) {
         int[] label = dataset[datasetIndex].toArray(new int[dataset[datasetIndex].size()]);
         
         RDA qda = new RDA(data, label, alpha);
+        int[] pred = new int[label.length];
         for (int i = 0; i < label.length; i++) {
-            label[i] = qda.predict(data[i]);
+            pred[i] = qda.predict(data[i]);
         }
-        double trainError = error(label, label);
+        double trainError = error(label, pred);
 
         System.out.format("training error = %.2f%%\n", 100*trainError);
 

File: math/src/main/java/smile/math/random/MersenneTwister.java
Patch:
@@ -75,7 +75,7 @@ public MersenneTwister(int seed) {
 
     @Override
     public void setSeed(long seed) {
-        setSeed(seed % UniversalGenerator.BIG_PRIME);
+        setSeed((int)(seed % UniversalGenerator.BIG_PRIME));
     }
 
     public void setSeed(int seed) {

File: math/src/main/java/smile/math/random/MersenneTwister.java
Patch:
@@ -75,7 +75,7 @@ public MersenneTwister(int seed) {
 
     @Override
     public void setSeed(long seed) {
-        setSeed(seed % UniversalGenerator.BIG_PRIME);
+        setSeed((int)seed % UniversalGenerator.BIG_PRIME);
     }
 
     public void setSeed(int seed) {

File: core/src/main/java/smile/classification/RandomForest.java
Patch:
@@ -384,7 +384,9 @@ public Tree call() {
                 }
             }
 
-            DecisionTree tree = new DecisionTree(attributes, x, y, maxNodes, nodeSize, mtry, rule, samples, order);
+            // samples will be changed during tree construction.
+            // make a copy so that we can estimate oob error correctly.
+            DecisionTree tree = new DecisionTree(attributes, x, y, maxNodes, nodeSize, mtry, rule, samples.clone(), order);
 
             // estimate OOB error
             int oob = 0;

File: demo/src/main/java/smile/demo/classification/NeuralNetworkDemo.java
Patch:
@@ -64,7 +64,7 @@ public double[][] learn(double[] x, double[] y) {
 
         try {
             epochs = Integer.parseInt(epochsField.getText().trim());
-            if (units <= 0) {
+            if (epochs <= 0) {
                 JOptionPane.showMessageDialog(this, "Invalid number of epochs: " + epochs, "Error", JOptionPane.ERROR_MESSAGE);
                 return null;
             }

File: core/src/main/java/smile/feature/DateFeature.java
Patch:
@@ -131,11 +131,11 @@ public Attribute[] attributes() {
     @SuppressWarnings("deprecation")
     public double f(double[] object, int id) {
         if (object.length != attributes.length) {
-            throw new IllegalArgumentException(String.format("Invalide object size %d, expected %d", object.length, attributes.length));            
+            throw new IllegalArgumentException(String.format("Invalid object size %d, expected %d", object.length, attributes.length));
         }
         
         if (id < 0 || id >= features.length) {
-            throw new IllegalArgumentException("Invalide feature id: " + id);
+            throw new IllegalArgumentException("Invalid feature id: " + id);
         }
         
         Date date = new Date(Double.doubleToLongBits(object[map[id]]));

File: data/src/main/java/smile/data/parser/ArffParser.java
Patch:
@@ -477,8 +477,8 @@ private void readInstance(StreamTokenizer tokenizer, AttributeDataset data, Attr
                 }
             }
         }
-        
-        data.add(x, y);
+
+        if (Double.isNaN(y)) data.add(x); else data.add(x, y);
     }
 
     /**
@@ -528,6 +528,6 @@ private void readSparseInstance(StreamTokenizer tokenizer, AttributeDataset data
             
         } while (tokenizer.ttype == StreamTokenizer.TT_WORD);
         
-        data.add(x, y);
+        if (Double.isNaN(y)) data.add(x); else data.add(x, y);
     }
 }

File: data/src/main/java/smile/data/parser/DelimitedTextParser.java
Patch:
@@ -343,7 +343,7 @@ private AttributeDataset parse(String name, Attribute[] attributes, BufferedRead
                 }
             }
 
-            AttributeDataset.Row datum = data.add(x, y);
+            AttributeDataset.Row datum = Double.isNaN(y) ? data.add(x) : data.add(x, y);
             datum.name = rowName;
         }
 
@@ -372,7 +372,7 @@ private AttributeDataset parse(String name, Attribute[] attributes, BufferedRead
                 }
             }
 
-            AttributeDataset.Row datum = data.add(x, y);
+            AttributeDataset.Row datum = Double.isNaN(y) ? data.add(x) : data.add(x, y);
             datum.name = rowName;
         }
 

File: demo/src/main/java/smile/demo/classification/KNNDemo.java
Patch:
@@ -60,10 +60,11 @@ public double[][] learn(double[] x, double[] y) {
         int[] label = dataset[datasetIndex].toArray(new int[dataset[datasetIndex].size()]);
         
         KNN<double[]> knn = KNN.learn(data, label, k);
+        int[] pred = new int[label.length];
         for (int i = 0; i < label.length; i++) {
-            label[i] = knn.predict(data[i]);
+            pred[i] = knn.predict(data[i]);
         }
-        double trainError = error(label, label);
+        double trainError = error(pred, label);
 
         System.out.format("training error = %.2f%%\n", 100*trainError);
 

File: netlib/src/main/java/smile/netlib/package-info.java
Patch:
@@ -18,7 +18,7 @@
  * netlib-java is a wrapper for low-level BLAS, LAPACK and ARPACK that
  * performs as fast as the C / Fortran interfaces with a pure JVM fallback.
  *
- * To enable machine optimised natives in netlib-java, end-users make
+ * To enable machine optimized natives in netlib-java, end-users make
  * their machine-optimised libblas3 (CBLAS) and liblapack3 (Fortran)
  * available as shared libraries at runtime.
  *

File: demo/src/main/java/smile/demo/manifold/TSNEDemo.java
Patch:
@@ -84,7 +84,7 @@ public JComponent learn() {
         pca.setProjection(50);
         double[][] X = pca.project(data);
         long clock = System.currentTimeMillis();
-        TSNE tsne = new TSNE(X, 2, perplexity, 500, 1000);
+        TSNE tsne = new TSNE(X, 2, perplexity, 200, 1000);
         System.out.format("Learn t-SNE from %d samples in %dms\n", data.length, System.currentTimeMillis() - clock);
 
         double[][] y = tsne.getCoordinates();

File: core/src/main/java/smile/projection/PCA.java
Patch:
@@ -149,7 +149,7 @@ public PCA(double[][] data, boolean cor) {
 
             for (int i = 0; i < n; i++) {
                 for (int j = 0; j <= i; j++) {
-                    cov.div(i, j, m); // divide m instead of m-1 for S-PLUS compatibilit
+                    cov.div(i, j, m); // divide m instead of m-1 for S-PLUS compatibility
                     cov.set(j, i, cov.get(i, j));
                 }
             }

File: math/src/main/java/smile/math/matrix/Matrix.java
Patch:
@@ -308,9 +308,8 @@ public String toString() {
      */
     public String toString(boolean full) {
         StringBuilder sb = new StringBuilder();
-        final int fields = 7;
-        int m = Math.min(fields, nrows());
-        int n = Math.min(fields, ncols());
+        int m = full ? nrows() : Math.min(7, nrows());
+        int n = full ? ncols() : Math.min(7, ncols());
 
         String newline = n < ncols() ? "...\n" : "\n";
 

File: core/src/main/java/smile/manifold/LLE.java
Patch:
@@ -154,9 +154,6 @@ public int compare(Neighbor<double[], double[]> o1, Neighbor<double[], double[]>
 
         DenseMatrix C = Matrix.zeros(k, k);
         double[] b = new double[k];
-        for (int i = 0; i < k; i++) {
-            b[i] = 1.0;
-        }
 
         int m = 0;
         for (int i : index) {
@@ -178,6 +175,7 @@ public int compare(Neighbor<double[], double[]> o1, Neighbor<double[], double[]>
                 }
             }
 
+            Arrays.fill(b, 1.0);
             LU lu = C.lu(true);
             lu.solve(b);
 

File: nlp/src/test/java/smile/nlp/SimpleCorpusTest.java
Patch:
@@ -173,9 +173,9 @@ public void testSearch2() {
      */
     @Test
     public void testSearch2WithNoHits() {
-        System.out.println("search 'romantic comedy'");
-        String[] terms = {"find", "words"};
+        System.out.println("search 'no hits'");
+        String[] terms = {"thisisnotaword"};
         Iterator<Relevance> hits = corpus.search(new BM25(), terms);
-        assertEquals(Collections.emptyIterator(),hits);
+        assertEquals(false, hits.hasNext());
     }
 }
\ No newline at end of file

File: core/src/main/java/smile/regression/OLS.java
Patch:
@@ -218,14 +218,14 @@ public OLS(double[][] x, double[] y, boolean SVD) {
         System.arraycopy(w1, 0, w, 0, p);
 
         double[] yhat = new double[n];
-        X.ax(w1, yhat);
+        Matrix.newInstance(x).ax(w, yhat);
 
         double TSS = 0.0;
         RSS = 0.0;
         double ybar = Math.mean(y);
         residuals = new double[n];
         for (int i = 0; i < n; i++) {
-            double r = y[i] - yhat[i];
+            double r = y[i] - yhat[i] - b;
             residuals[i] = r;
             RSS += Math.sqr(r);
             TSS += Math.sqr(y[i] - ybar);

File: core/src/main/java/smile/regression/RidgeRegression.java
Patch:
@@ -243,14 +243,14 @@ public RidgeRegression(double[][] x, double[] y, double lambda) {
         b = ym - Math.dot(w, center);
 
         double[] yhat = new double[n];
-        X.ax(w, yhat);
+        Matrix.newInstance(x).ax(w, yhat);
 
         double TSS = 0.0;
         RSS = 0.0;
         double ybar = Math.mean(y);
         residuals = new double[n];
         for (int i = 0; i < n; i++) {
-            double r = y[i] - yhat[i];
+            double r = y[i] - yhat[i] - b;
             residuals[i] = r;
             RSS += Math.sqr(r);
             TSS += Math.sqr(y[i] - ybar);

File: core/src/test/java/smile/gap/GeneticAlgorithmTest.java
Patch:
@@ -95,7 +95,7 @@ public void testEvolve() {
         assertEquals(18, result.fitness(), 1E-7);
 
         int[] best = {1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};
-        for (int i : result.bits()) {
+        for (int i = 0; i < best.length; i++) {
             assertEquals(best[i], result.bits()[i]);
         }
         

File: core/src/test/java/smile/gap/GeneticAlgorithmTest.java
Patch:
@@ -95,7 +95,7 @@ public void testEvolve() {
         assertEquals(18, result.fitness(), 1E-7);
 
         int[] best = {1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};
-        for (int i : result.bits()) {
+        for (int i = 0; i < best.length; i++) {
             assertEquals(best[i], result.bits()[i]);
         }
         

File: core/src/main/java/smile/classification/RBFNetwork.java
Patch:
@@ -156,7 +156,7 @@ public Trainer(Metric<T> distance) {
          * @param rbf the radial basis function.
          * @param m the number of basis functions.
          */
-        public Trainer setRBF(RadialBasisFunction rbf, int m) {
+        public Trainer<T> setRBF(RadialBasisFunction rbf, int m) {
             this.m = m;
             this.rbf = rep(rbf, m);
             return this;
@@ -166,7 +166,7 @@ public Trainer setRBF(RadialBasisFunction rbf, int m) {
          * Sets the radial basis functions.
          * @param rbf the radial basis functions.
          */
-        public Trainer setRBF(RadialBasisFunction[] rbf) {
+        public Trainer<T> setRBF(RadialBasisFunction[] rbf) {
             this.m = rbf.length;
             this.rbf = rbf;
             return this;
@@ -176,7 +176,7 @@ public Trainer setRBF(RadialBasisFunction[] rbf) {
          * Sets true to learn normalized RBF network.
          * @param normalized true to learn normalized RBF network.
          */
-        public Trainer setNormalized(boolean normalized) {
+        public Trainer<T> setNormalized(boolean normalized) {
             this.normalized = normalized;
             return this;
         }

File: core/src/main/java/smile/neighbor/BKTree.java
Patch:
@@ -16,9 +16,10 @@
 
 package smile.neighbor;
 
-import java.util.List;
 import java.util.ArrayList;
 import java.util.Collection;
+import java.util.List;
+
 import smile.math.distance.Metric;
 
 /**
@@ -179,7 +180,7 @@ public void add(E datum) {
     /**
      * Set if exclude query object self from the neighborhood.
      */
-    public BKTree setIdenticalExcluded(boolean excluded) {
+    public BKTree<E> setIdenticalExcluded(boolean excluded) {
         identicalExcluded = excluded;
         return this;
     }

File: core/src/main/java/smile/neighbor/CoverTree.java
Patch:
@@ -21,6 +21,7 @@
 
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
+
 import smile.math.Math;
 import smile.math.distance.Metric;
 import smile.sort.DoubleHeapSelect;
@@ -244,7 +245,7 @@ public String toString() {
     /**
      * Set if exclude query object self from the neighborhood.
      */
-    public CoverTree setIdenticalExcluded(boolean excluded) {
+    public CoverTree<E> setIdenticalExcluded(boolean excluded) {
         identicalExcluded = excluded;
         return this;
     }

File: core/src/main/java/smile/neighbor/KDTree.java
Patch:
@@ -16,6 +16,7 @@
 package smile.neighbor;
 
 import java.util.List;
+
 import smile.math.Math;
 import smile.sort.HeapSelect;
 
@@ -234,7 +235,7 @@ private Node buildNode(int begin, int end) {
     /**
      * Set if exclude query object self from the neighborhood.
      */
-    public KDTree setIdenticalExcluded(boolean excluded) {
+    public KDTree<E> setIdenticalExcluded(boolean excluded) {
         identicalExcluded = excluded;
         return this;
     }

File: core/src/main/java/smile/neighbor/LSH.java
Patch:
@@ -16,10 +16,11 @@
 package smile.neighbor;
 
 import java.util.ArrayList;
+import java.util.LinkedHashSet;
 import java.util.LinkedList;
 import java.util.List;
 import java.util.Set;
-import java.util.LinkedHashSet;
+
 import smile.math.IntArrayList;
 import smile.math.Math;
 import smile.sort.HeapSelect;
@@ -474,7 +475,7 @@ public boolean isIdenticalExcluded() {
     /**
      * Set if exclude query object self from the neighborhood.
      */
-    public LSH setIdenticalExcluded(boolean excluded) {
+    public LSH<E> setIdenticalExcluded(boolean excluded) {
         identicalExcluded = excluded;
         return this;
     }

File: core/src/main/java/smile/neighbor/LinearSearch.java
Patch:
@@ -17,6 +17,7 @@
 package smile.neighbor;
 
 import java.util.List;
+
 import smile.math.distance.Distance;
 import smile.sort.HeapSelect;
 
@@ -77,7 +78,7 @@ public String toString() {
     /**
      * Set if exclude query object self from the neighborhood.
      */
-    public LinearSearch setIdenticalExcluded(boolean excluded) {
+    public LinearSearch<T> setIdenticalExcluded(boolean excluded) {
         identicalExcluded = excluded;
         return this;
     }

File: core/src/main/java/smile/neighbor/MPLSH.java
Patch:
@@ -15,10 +15,11 @@
  *******************************************************************************/
 package smile.neighbor;
 
-import java.util.Arrays;
 import java.util.ArrayList;
+import java.util.Arrays;
 import java.util.List;
 import java.util.PriorityQueue;
+
 import smile.math.IntArrayList;
 import smile.math.Math;
 import smile.sort.HeapSelect;
@@ -819,7 +820,7 @@ public boolean isIdenticalExcluded() {
     /**
      * Set if exclude query object self from the neighborhood.
      */
-    public MPLSH setIdenticalExcluded(boolean excluded) {
+    public MPLSH<E> setIdenticalExcluded(boolean excluded) {
         identicalExcluded = excluded;
         return this;
     }

File: interpolation/src/main/java/smile/interpolation/BilinearInterpolation.java
Patch:
@@ -21,7 +21,7 @@
  * of linear interpolation for interpolating functions of two variables on a
  * regular grid. The key idea is to perform linear interpolation first in one
  * direction, and then again in the other direction.
-
+ *
  * @author Haifeng Li
  */
 public class BilinearInterpolation implements Interpolation2D {

File: interpolation/src/main/java/smile/interpolation/ShepardInterpolation.java
Patch:
@@ -18,7 +18,7 @@
 import smile.math.Math;
 
 /**
- * Shepard interplation is a special case of normalized radial basis function
+ * Shepard interpolation is a special case of normalized radial basis function
  * interpolation if the function &phi;(r) goes to infinity as r &rarr; 0, and is
  * finite for r &gt; 0. In this case, the weights w<sub>i</sub> are just equal to
  * the respective function values y<sub>i</sub>. So we need not solve linear
@@ -28,7 +28,7 @@
  * 1 &lt; p &le; 3.
  * <p>
  * Shepard interpolation is rarely as accurate as the well-tuned application of
- * other radial basis functions. However, it is simple, fast, and often jut the
+ * other radial basis functions. However, it is simple, fast, and often just the
  * thing for quick and dirty applications.
  *
  * @author Haifeng Li

File: math/src/main/java/smile/stat/hypothesis/CorTest.java
Patch:
@@ -38,7 +38,7 @@
  *   <tr>
  *     <td align="center" colspan="2" width="100%" bgcolor="#FFFF99">
  *     <p align="left" style="margin-top: 0; margin-bottom: 0">
- *     Parametric? variables follow normal distribution and linear
+ *     Parametric variables follow normal distribution and linear
  *     relationship between x and y)</td>
  *   </tr>
  *   <tr>
@@ -75,7 +75,7 @@
  *   </tr>
  * </table>
  * <p>
- * To deal with measures of assocation between nominal variables, we can use Chi-square
+ * To deal with measures of association between nominal variables, we can use Chi-square
  * test for independence. For any pair of nominal variables, the data can be
  * displayed as a contingency table, whose rows are labels by the values of one
  * nominal variable, whose columns are labels by the values of the other nominal

File: core/src/main/java/smile/classification/RandomForest.java
Patch:
@@ -351,8 +351,8 @@ public Tree call() {
 
                     // We used to do up sampling.
                     // But we switch to down sampling, which seems has better performance.
-                    nj /= classWeight[l];
-                    for (int i = 0; i < nj; i++) {
+                    int size = nj / classWeight[l];
+                    for (int i = 0; i < size; i++) {
                         int xi = Math.randomInt(nj);
                         samples[cj.get(xi)] += 1; //classWeight[l];
                     }

File: math/src/main/java/smile/math/matrix/Matrix.java
Patch:
@@ -88,7 +88,9 @@ public interface Matrix {
     /**
      * Returns the entry value at row i and column j. For Scala users.
      */
-    public double apply(int i, int j);
+    default public double apply(int i, int j) {
+        return get(i, j);
+    }
 
     /**
      * Returns the diagonal elements.

File: core/src/main/java/smile/classification/RandomForest.java
Patch:
@@ -481,7 +481,7 @@ public RandomForest(Attribute[] attributes, double[][] x, int[] y, int ntrees, i
      *                  sampling without replacement.
      */
     public RandomForest(Attribute[] attributes, double[][] x, int[] y, int ntrees, int maxNodes, int nodeSize, int mtry, double subsample) {
-        this(attributes, x, y, ntrees, 100, 5, mtry, subsample, DecisionTree.SplitRule.GINI);
+        this(attributes, x, y, ntrees, maxNodes, nodeSize, mtry, subsample, DecisionTree.SplitRule.GINI);
     }
 
     /**

File: core/src/main/java/smile/classification/NeuralNetwork.java
Patch:
@@ -707,6 +707,7 @@ private double computeOutputError(double[] output, double[] gradient) {
                 if (activationFunction == ActivationFunction.SOFTMAX) {
                     error -= output[i] * log(out);
                 } else if (activationFunction == ActivationFunction.LOGISTIC_SIGMOID) {
+                    // We have only one output neuron in this case.
                     error = -output[i] * log(out) - (1.0 - output[i]) * log(1.0 - out);
                 }
             }

File: core/src/main/java/smile/wavelet/SymletWavelet.java
Patch:
@@ -104,7 +104,7 @@ public class SymletWavelet extends Wavelet {
      * Constructor. Create a Symmlet wavelet with n coefficients.
      * n = 8, 10, 12, 14, 16, 18, or 20 are supported.
      */
-    public SymmletWavelet(int n) {
+    public SymletWavelet(int n) {
         super(n == 8 ? c8 :
               n == 10 ? c10 :
               n == 12 ? c12 :

File: core/src/main/java/smile/classification/NeuralNetwork.java
Patch:
@@ -343,7 +343,7 @@ public Trainer setMomentum(double alpha) {
          */
         public Trainer setWeightDecay(double lambda) {
             if (lambda < 0.0 || lambda > 0.1) {
-                throw new IllegalArgumentException("Invalid momentum factor: " + alpha);
+                throw new IllegalArgumentException("Invalid weight decay factor: " + lambda);
             }
 
             this.lambda = lambda;
@@ -569,7 +569,7 @@ public double getMomentum() {
      */
     public void setWeightDecay(double lambda) {
         if (lambda < 0.0 || lambda > 0.1) {
-            throw new IllegalArgumentException("Invalid momentum factor: " + alpha);
+            throw new IllegalArgumentException("Invalid weight decay factor: " + lambda);
         }
 
         this.lambda = lambda;

File: core/src/main/java/smile/regression/RandomForest.java
Patch:
@@ -355,7 +355,7 @@ public RandomForest(double[][] x, double[] y, int ntrees, int maxNodes, int node
      * @param ntrees the number of trees.
      */
     public RandomForest(Attribute[] attributes, double[][] x, double[] y, int ntrees) {
-        this(attributes, x, y, ntrees, x.length);
+        this(attributes, x, y, ntrees, 100);
     }
 
     /**

File: math/src/main/java/smile/sort/DoubleHeapSelect.java
Patch:
@@ -71,7 +71,7 @@ public void add(double datum) {
         if (n < k) {
             heap[n++] = datum;
             if (n == k) {
-                heapify(heap);
+                sort(heap, k);
             }
         } else {
             n++;
@@ -86,6 +86,7 @@ public void add(double datum) {
      * Returns the k-<i>th</i> smallest value seen so far.
      */
     public double peek() {
+        if (n < k) sort(heap, n);
         return heap[0];
     }
 

File: core/src/main/java/smile/classification/Maxent.java
Patch:
@@ -897,8 +897,8 @@ public int predict(int[] x, double[] posteriori) {
             double f = 1.0 / (1.0 + Math.exp(-dot(x, w)));
 
             if (posteriori != null) {
-                posteriori[0] = f;
-                posteriori[1] = 1.0 - f;
+                posteriori[0] = 1.0 - f;
+                posteriori[1] = f;
             }
 
             if (f < 0.5) {

File: core/src/main/java/smile/classification/LogisticRegression.java
Patch:
@@ -900,8 +900,8 @@ public int predict(double[] x, double[] posteriori) {
             double f = 1.0 / (1.0 + Math.exp(-dot(x, w)));
 
             if (posteriori != null) {
-                posteriori[0] = f;
-                posteriori[1] = 1.0 - f;
+                posteriori[0] = 1.0 - f;
+                posteriori[1] = f;
             }
 
             if (f < 0.5) {

File: core/src/main/java/smile/neighbor/CoverTree.java
Patch:
@@ -516,6 +516,7 @@ public Neighbor<E, E>[] knn(E q, int k) {
 
         //if root is the only node
         if (root.children == null) {
+            a1[0] = n1;
             return a1;
         }
 

File: core/src/test/java/smile/classification/FLDTest.java
Patch:
@@ -113,7 +113,7 @@ public void testUSPS() {
             }
 
             System.out.format("USPS error rate = %.2f%%%n", 100.0 * error / testx.length);
-            assertEquals(521, error);
+            assertEquals(561, error);
         } catch (Exception ex) {
             System.err.println(ex);
         }

File: core/src/test/java/smile/regression/LASSOTest.java
Patch:
@@ -51,7 +51,7 @@ public void setUp() {
     @After
     public void tearDown() {
     }
-    
+    /*
     @Test
     public void testToy() {
         double[][] A = {
@@ -80,6 +80,7 @@ public void testToy() {
             assertEquals(w[i], lasso.coefficients()[i], 1E-4);
         }
     }
+    */
 
     @Test
     public void testToy2() {
@@ -159,7 +160,7 @@ public void testLongley() {
         }
 
         System.out.println("LOOCV MSE = " + rss / n);
-        assertEquals(2.004387308497487, rss/n, 1E-4);
+        assertEquals(2.0012529348358212, rss/n, 1E-4);
     }
 
     /**

File: math/src/main/java/smile/math/matrix/EigenValueDecomposition.java
Patch:
@@ -384,8 +384,8 @@ public static double[] pagerank(IMatrix A, double[] v, double damping, double to
     }
 
     /**
-     * Calculate and normalize y = (A - pI) x.* Returns the largest element of y
-     * in magnitude.
+     * Calculate and normalize y = (A - pI) x.
+     * Returns the largest element of y in magnitude.
      */
     private static double ax(IMatrix A, double[] x, double[] y, double p) {
         A.ax(x, y);

File: math/src/main/java/smile/math/matrix/IMatrix.java
Patch:
@@ -21,7 +21,7 @@
  * multiplication, which is the only operation needed in many iterative matrix
  * algorithms, e.g. biconjugate gradient method for solving linear equations and
  * power iteration and Lanczos algorithm for eigen decomposition, which are
- * usually very effecient for very large and sparse matrices.
+ * usually very efficient for very large and sparse matrices.
  *
  * @author Haifeng Li
  */

File: math/src/test/java/smile/stat/distribution/ExponentialFamilyMixtureTest.java
Patch:
@@ -16,6 +16,8 @@
 
 package smile.stat.distribution;
 
+import java.util.ArrayList;
+import java.util.List;
 import java.util.Vector;
 import org.junit.After;
 import org.junit.AfterClass;
@@ -71,7 +73,7 @@ public void testEM() {
         for (int i = 1000; i < 2000; i++)
             data[i] = gamma.rand();
 
-        Vector<Mixture.Component> m = new Vector<>();
+        List<Mixture.Component> m = new ArrayList<>();
         Mixture.Component c = new Mixture.Component();
         c.priori = 0.25;
         c.distribution = new GaussianDistribution(0.0, 1.0);

File: nlp/src/main/java/smile/nlp/NGram.java
Patch:
@@ -57,7 +57,7 @@ public NGram(String[] words, int freq) {
 
     @Override
     public String toString() {
-    	StringBuffer sb = new StringBuffer();
+    	StringBuilder sb = new StringBuilder();
     	sb.append('(')
           .append(Arrays.toString(words))
           .append(", ")

File: data/src/main/java/smile/data/parser/DelimitedTextParser.java
Patch:
@@ -223,17 +223,17 @@ public AttributeDataset parse(File file) throws IOException, ParseException {
     /**
      * Parse a dataset from given file.
      * @param file the file of data source.
-     * @throws java.io.FileNotFoundException
+     * @throws java.io.IOException
      */
-    public AttributeDataset parse(String name, File file) throws FileNotFoundException, IOException, ParseException {
+    public AttributeDataset parse(String name, File file) throws IOException, ParseException {
         return parse(name, new FileInputStream(file));
     }
 
     /**
      * Parse a dataset from given file.
      * @param file the file of data source.
      * @param attributes the list attributes of data in proper order.
-     * @throws java.io.FileNotFoundException
+     * @throws java.io.IOException
      */
     public AttributeDataset parse(Attribute[] attributes, File file) throws IOException, ParseException {
         String name = file.getPath();

File: core/src/main/java/smile/association/ARM.java
Patch:
@@ -125,7 +125,7 @@ public long learn(double confidence, PrintStream out) {
      * @param confidence the confidence threshold for association rules.
      */
     public List<AssociationRule> learn(double confidence) {
-        List<AssociationRule> list = new ArrayList<AssociationRule>();
+        List<AssociationRule> list = new ArrayList<>();
         ttree = fim.buildTotalSupportTree();
         for (int i = 0; i < ttree.root.children.length; i++) {
             if (ttree.root.children[i] != null) {

File: core/src/main/java/smile/association/FPTree.java
Patch:
@@ -89,7 +89,7 @@ class Node {
          */
         void add(int index, int end, int[] itemset, int support) {
             if (children == null) {
-                children = new HashMap<Integer, Node>();
+                children = new HashMap<>();
             }
             
             Node child = children.get(itemset[index]);
@@ -114,7 +114,7 @@ void add(int index, int end, int[] itemset, int support) {
          */
         void append(int index, int end, int[] itemset, int support) {
             if (children == null) {
-                children = new HashMap<Integer, Node>();
+                children = new HashMap<>();
             }
             
             if (index >= maxItemSetSize) {

File: core/src/main/java/smile/association/TotalSupportTree.java
Patch:
@@ -168,7 +168,7 @@ private int getSupport(int[] itemset, int index, Node node) {
      * @return the list of frequent item sets
      */
     public List<ItemSet> getFrequentItemsets() {
-        List<ItemSet> list = new ArrayList<ItemSet>();
+        List<ItemSet> list = new ArrayList<>();
         getFrequentItemsets(null, list);
         return list;
     }

File: core/src/main/java/smile/classification/DecisionTree.java
Patch:
@@ -511,7 +511,7 @@ public boolean findBestSplit() {
                 }
             } else {
 
-                List<SplitTask> tasks = new ArrayList<SplitTask>(mtry);
+                List<SplitTask> tasks = new ArrayList<>(mtry);
                 for (int j = 0; j < mtry; j++) {
                     tasks.add(new SplitTask(n, count, impurity, variables[j]));
                 }
@@ -944,7 +944,7 @@ public DecisionTree(Attribute[] attributes, double[][] x, int[] y, int maxNodes,
         }
 
         // Priority queue for best-first tree growing.
-        PriorityQueue<TrainNode> nextSplits = new PriorityQueue<TrainNode>();
+        PriorityQueue<TrainNode> nextSplits = new PriorityQueue<>();
 
         int n = y.length;
         int[] count = new int[k];

File: core/src/main/java/smile/classification/RBFNetwork.java
Patch:
@@ -186,9 +186,9 @@ public RBFNetwork<T> train(T[] x, int[] y) {
             GaussianRadialBasis gaussian = SmileUtils.learnGaussianRadialBasis(x, centers, distance);
             
             if (rbf == null) {
-                return new RBFNetwork<T>(x, y, distance, gaussian, centers, normalized);
+                return new RBFNetwork<>(x, y, distance, gaussian, centers, normalized);
             } else {
-                return new RBFNetwork<T>(x, y, distance, rbf, centers, normalized);
+                return new RBFNetwork<>(x, y, distance, rbf, centers, normalized);
             }
         }
         
@@ -201,7 +201,7 @@ public RBFNetwork<T> train(T[] x, int[] y) {
          * @return a trained RBF network
          */
         public RBFNetwork<T> train(T[] x, int[] y, T[] centers) {
-            return new RBFNetwork<T>(x, y, distance, rbf, centers, normalized);
+            return new RBFNetwork<>(x, y, distance, rbf, centers, normalized);
         }
     }
     

File: core/src/main/java/smile/clustering/BIRCH.java
Patch:
@@ -387,9 +387,9 @@ public int partition(int k) {
      * @return the number of non-outlier leaves.
      */
     public int partition(int k, int minPts) {
-        ArrayList<Leaf> leaves = new ArrayList<Leaf>();
-        ArrayList<double[]> centers = new ArrayList<double[]>();
-        Queue<Node> queue = new LinkedList<Node>();
+        ArrayList<Leaf> leaves = new ArrayList<>();
+        ArrayList<double[]> centers = new ArrayList<>();
+        Queue<Node> queue = new LinkedList<>();
         queue.offer(root);
 
         for (Node node = queue.poll(); node != null; node = queue.poll()) {

File: core/src/main/java/smile/clustering/CLARANS.java
Patch:
@@ -143,7 +143,7 @@ public CLARANS(T[] data, Distance<T> distance, int k, int maxNeighbor, int numLo
         this.numLocal = numLocal;
         this.maxNeighbor = maxNeighbor;
         
-        List<CLARANSTask> tasks = new ArrayList<CLARANSTask>();
+        List<CLARANSTask> tasks = new ArrayList<>();
         for (int i = 0; i < numLocal; i++) {
             tasks.add(new CLARANSTask(data));
         }

File: core/src/main/java/smile/clustering/DeterministicAnnealing.java
Patch:
@@ -92,7 +92,7 @@ public DeterministicAnnealing(double[][] data, int Kmax, double alpha) {
 
         int np = MulticoreExecutor.getThreadPoolSize();
         if (n >= 1000 && np >= 2) {
-            tasks = new ArrayList<UpdateThread>(np + 1);
+            tasks = new ArrayList<>(np + 1);
             int step = n / np;
             if (step < 100) {
                 step = 100;
@@ -107,7 +107,7 @@ public DeterministicAnnealing(double[][] data, int Kmax, double alpha) {
             }
             tasks.add(new UpdateThread(data, centroids, posteriori, priori, start, n));
             
-            ctasks = new ArrayList<CentroidThread>(2 * Kmax);
+            ctasks = new ArrayList<>(2 * Kmax);
             for (int i = 0; i < 2*Kmax; i++) {
                 ctasks.add(new CentroidThread(data, centroids, posteriori, priori, i));
             }

File: core/src/main/java/smile/clustering/GMeans.java
Patch:
@@ -81,7 +81,7 @@ public GMeans(double[][] data, int kmax) {
 
         BBDTree bbd = new BBDTree(data);
         while (k < kmax) {
-            ArrayList<double[]> centers = new ArrayList<double[]>();
+            ArrayList<double[]> centers = new ArrayList<>();
             double[] score = new double[k];
             KMeans[] kmeans = new KMeans[k];
             

File: core/src/main/java/smile/clustering/HierarchicalClustering.java
Patch:
@@ -185,7 +185,7 @@ public int[] partition(double h) {
      */
     private void bfs(int[] membership, int cluster, int id) {
         int n = merge.length + 1;
-        Queue<Integer> queue = new LinkedList<Integer>();
+        Queue<Integer> queue = new LinkedList<>();
         queue.offer(cluster);
 
         for (Integer i = queue.poll(); i != null; i = queue.poll()) {

File: core/src/main/java/smile/clustering/KMeans.java
Patch:
@@ -226,7 +226,7 @@ public KMeans(double[][] data, int k, int maxIter, int runs) {
 
         BBDTree bbd = new BBDTree(data);
 
-        List<KMeansThread> tasks = new ArrayList<KMeansThread>();
+        List<KMeansThread> tasks = new ArrayList<>();
         for (int i = 0; i < runs; i++) {
             tasks.add(new KMeansThread(bbd, data, k, maxIter));
         }
@@ -321,7 +321,7 @@ public static KMeans lloyd(double[][] data, int k, int maxIter) {
         int np = MulticoreExecutor.getThreadPoolSize();
         List<LloydThread> tasks = null;
         if (n >= 1000 && np >= 2) {
-            tasks = new ArrayList<LloydThread>(np + 1);
+            tasks = new ArrayList<>(np + 1);
             int step = n / np;
             if (step < 100) {
                 step = 100;

File: core/src/main/java/smile/clustering/SIB.java
Patch:
@@ -186,7 +186,7 @@ public SIB(double[][] data, int k, int maxIter, int runs) {
             throw new IllegalArgumentException("Invalid number of runs: " + runs);
         }
 
-        List<SIBThread> tasks = new ArrayList<SIBThread>();
+        List<SIBThread> tasks = new ArrayList<>();
         for (int i = 0; i < runs; i++) {
             tasks.add(new SIBThread(data, k, maxIter));
         }
@@ -395,7 +395,7 @@ public SIB(SparseDataset data, int k, int maxIter, int runs) {
             throw new IllegalArgumentException("Invalid number of runs: " + runs);
         }
 
-        List<SIBThread> tasks = new ArrayList<SIBThread>();
+        List<SIBThread> tasks = new ArrayList<>();
         for (int i = 0; i < runs; i++) {
             tasks.add(new SIBThread(data, k, maxIter));
         }

File: core/src/main/java/smile/clustering/XMeans.java
Patch:
@@ -85,7 +85,7 @@ public XMeans(double[][] data, int kmax) {
 
         BBDTree bbd = new BBDTree(data);
         while (k < kmax) {
-            ArrayList<double[]> centers = new ArrayList<double[]>();
+            ArrayList<double[]> centers = new ArrayList<>();
             double[] score = new double[k];
             KMeans[] kmeans = new KMeans[k];
             

File: core/src/main/java/smile/feature/Bag.java
Patch:
@@ -58,7 +58,7 @@ public Bag(T[] features) {
      */
     public Bag(T[] features, boolean binary) {
         this.binary = binary;
-        this.features = new HashMap<T, Integer>();
+        this.features = new HashMap<>();
         for (int i = 0, k = 0; i < features.length; i++) {
             if (!this.features.containsKey(features[i])) {
                 this.features.put(features[i], k++);

File: core/src/main/java/smile/feature/FeatureSet.java
Patch:
@@ -35,11 +35,11 @@ public class FeatureSet <T> {
     /**
      * Feature generators.
      */
-    List<Feature<T>> features = new ArrayList<Feature<T>>();
+    List<Feature<T>> features = new ArrayList<>();
     /**
      * The variable attributes of generated features.
      */
-    List<Attribute> attributes = new ArrayList<Attribute>();
+    List<Attribute> attributes = new ArrayList<>();
     
     /**
      * Constructor.
@@ -123,7 +123,7 @@ public AttributeDataset f(Dataset<T> data) {
 
         for (int i = 0; i < data.size(); i++) {
             Datum<T> datum = data.get(i);
-            Datum<double[]> x = new Datum<double[]>(f(datum.x), datum.y, datum.weight);
+            Datum<double[]> x = new Datum<>(f(datum.x), datum.y, datum.weight);
             x.name = datum.name;
             x.description = datum.description;
             x.timestamp = datum.timestamp;

File: core/src/main/java/smile/gap/GeneticAlgorithm.java
Patch:
@@ -219,7 +219,7 @@ public enum Selection {
     /**
      * Parallel tasks to evaluate the fitness of population.
      */
-    private List<Task> tasks = new ArrayList<Task>();
+    private List<Task> tasks = new ArrayList<>();
     
     /**
      * Constructor. The default selection strategy is tournament selection

File: core/src/main/java/smile/manifold/IsoMap.java
Patch:
@@ -109,9 +109,9 @@ public IsoMap(double[][] data, int d, int k, boolean CIsomap) {
 
         KNNSearch<double[], double[]> knn = null;
         if (data[0].length < 10) {
-            knn = new KDTree<double[]>(data, data);
+            knn = new KDTree<>(data, data);
         } else {
-            knn = new CoverTree<double[]>(data, new EuclideanDistance());
+            knn = new CoverTree<>(data, new EuclideanDistance());
         }
 
         graph = new AdjacencyList(n);

File: core/src/main/java/smile/manifold/LLE.java
Patch:
@@ -88,9 +88,9 @@ public LLE(double[][] data, int d, int k) {
 
         KNNSearch<double[], double[]> knn = null;
         if (D < 10) {
-            knn = new KDTree<double[]>(data, data);
+            knn = new KDTree<>(data, data);
         } else {
-            knn = new CoverTree<double[]>(data, new EuclideanDistance());
+            knn = new CoverTree<>(data, new EuclideanDistance());
         }
 
         Comparator<Neighbor<double[], double[]>> comparator = new Comparator<Neighbor<double[], double[]>>() {

File: core/src/main/java/smile/manifold/LaplacianEigenmap.java
Patch:
@@ -98,9 +98,9 @@ public LaplacianEigenmap(double[][] data, int d, int k, double t) {
         int n = data.length;
         KNNSearch<double[], double[]> knn = null;
         if (data[0].length < 10) {
-            knn = new KDTree<double[]>(data, data);
+            knn = new KDTree<>(data, data);
         } else {
-            knn = new CoverTree<double[]>(data, new EuclideanDistance());
+            knn = new CoverTree<>(data, new EuclideanDistance());
         }
 
         graph = new AdjacencyList(n);

File: core/src/main/java/smile/neighbor/BKTree.java
Patch:
@@ -93,7 +93,7 @@ private void add(E datum) {
             }
 
             if (children == null) {
-                children = new ArrayList<Node>();
+                children = new ArrayList<>();
             }
 
             while (children.size() <= d) {
@@ -203,7 +203,7 @@ private void search(Node node, E q, int k, List<Neighbor<E, E>> neighbors) {
 
         if (d <= k) {
             if (node.object != q || !identicalExcluded) {
-                neighbors.add(new Neighbor<E, E>(node.object, node.object, node.index, d));
+                neighbors.add(new Neighbor<>(node.object, node.object, node.index, d));
             }
         }
 

File: core/src/main/java/smile/regression/GaussianProcessRegression.java
Patch:
@@ -111,7 +111,7 @@ public Trainer(MercerKernel<T> kernel, double lambda) {
         
         @Override
         public GaussianProcessRegression<T> train(T[] x, double[] y) {
-            return new GaussianProcessRegression<T>(x, y, kernel, lambda);
+            return new GaussianProcessRegression<>(x, y, kernel, lambda);
         }
         
         /**
@@ -125,7 +125,7 @@ public GaussianProcessRegression<T> train(T[] x, double[] y) {
          * @return a trained Gaussian Process.
          */
         public GaussianProcessRegression<T> train(T[] x, double[] y, T[] t) {
-            return new GaussianProcessRegression<T>(x, y, t, kernel, lambda);
+            return new GaussianProcessRegression<>(x, y, t, kernel, lambda);
         }
     }
     

File: core/src/main/java/smile/regression/RBFNetwork.java
Patch:
@@ -173,9 +173,9 @@ public RBFNetwork<T> train(T[] x, double[] y) {
             T[] centers = (T[]) java.lang.reflect.Array.newInstance(x.getClass().getComponentType(), m);
             GaussianRadialBasis gaussian = SmileUtils.learnGaussianRadialBasis(x, centers, distance);
             if (rbf == null) {
-                return new RBFNetwork<T>(x, y, distance, gaussian, centers, normalized);
+                return new RBFNetwork<>(x, y, distance, gaussian, centers, normalized);
             } else {
-                return new RBFNetwork<T>(x, y, distance, rbf, centers, normalized);
+                return new RBFNetwork<>(x, y, distance, rbf, centers, normalized);
             }
         }
         
@@ -188,7 +188,7 @@ public RBFNetwork<T> train(T[] x, double[] y) {
          * @return a trained RBF network
          */
         public RBFNetwork<T> train(T[] x, double[] y, T[] centers) {
-            return new RBFNetwork<T>(x, y, distance, rbf, centers, normalized);
+            return new RBFNetwork<>(x, y, distance, rbf, centers, normalized);
         }
     }
     

File: core/src/main/java/smile/regression/RandomForest.java
Patch:
@@ -458,7 +458,7 @@ public RandomForest(Attribute[] attributes, double[][] x, double[] y, int ntrees
         int[] oob = new int[n];
         
         int[][] order = SmileUtils.sort(attributes, x);
-        List<TrainingTask> tasks = new ArrayList<TrainingTask>();
+        List<TrainingTask> tasks = new ArrayList<>();
         for (int i = 0; i < ntrees; i++) {
             tasks.add(new TrainingTask(attributes, x, y, maxNodes, nodeSize, mtry, subsample, order, prediction, oob));
         }
@@ -468,7 +468,7 @@ public RandomForest(Attribute[] attributes, double[][] x, double[] y, int ntrees
         } catch (Exception ex) {
             ex.printStackTrace();
 
-            trees = new ArrayList<RegressionTree>(ntrees);
+            trees = new ArrayList<>(ntrees);
             for (int i = 0; i < ntrees; i++) {
                 trees.add(tasks.get(i).call());
             }
@@ -547,7 +547,7 @@ public void trim(int ntrees) {
             throw new IllegalArgumentException("Invalid new model size: " + ntrees);
         }
         
-        List<RegressionTree> model = new ArrayList<RegressionTree>(ntrees);
+        List<RegressionTree> model = new ArrayList<>(ntrees);
         for (int i = 0; i < ntrees; i++) {
             model.add(trees.get(i));
         }

File: core/src/main/java/smile/regression/RegressionTree.java
Patch:
@@ -422,7 +422,7 @@ public boolean findBestSplit() {
                 }
             } else {
 
-                List<SplitTask> tasks = new ArrayList<SplitTask>(mtry);
+                List<SplitTask> tasks = new ArrayList<>(mtry);
                 for (int j = 0; j < mtry; j++) {
                     tasks.add(new SplitTask(n, sum, variables[j]));
                 }
@@ -965,7 +965,7 @@ public RegressionTree(Attribute[] attributes, double[][] x, double[] y, int maxN
         }
 
         // Priority queue for best-first tree growing.
-        PriorityQueue<TrainNode> nextSplits = new PriorityQueue<TrainNode>();
+        PriorityQueue<TrainNode> nextSplits = new PriorityQueue<>();
 
         int n = 0;
         double sum = 0.0;
@@ -1063,7 +1063,7 @@ public RegressionTree(int numFeatures, int[][] x, double[] y, int maxNodes, int
         importance = new double[numFeatures];
         
         // Priority queue for best-first tree growing.
-        PriorityQueue<SparseBinaryTrainNode> nextSplits = new PriorityQueue<SparseBinaryTrainNode>();
+        PriorityQueue<SparseBinaryTrainNode> nextSplits = new PriorityQueue<>();
 
         int n = 0;
         double sum = 0.0;

File: core/src/main/java/smile/regression/SVR.java
Patch:
@@ -74,7 +74,7 @@ public class SVR<T> implements Regression<T> {
     /**
      * Support vectors.
      */
-    private List<SupportVector> sv = new ArrayList<SupportVector>();
+    private List<SupportVector> sv = new ArrayList<>();
     /**
      * Threshold of decision function.
      */
@@ -189,7 +189,7 @@ public Trainer setTolerance(double tol) {
 
         @Override
         public SVR<T> train(T[] x, double[] y) {
-            SVR<T> svr = new SVR<T>(x, y, kernel, eps, C, tol);
+            SVR<T> svr = new SVR<>(x, y, kernel, eps, C, tol);
             return svr;
         }
     }
@@ -415,7 +415,7 @@ private void gram(SupportVector i) {
                 i.kcache.add(kernel.k(i.x, v.x));
             }
         } else {
-            List<KernelTask> tasks = new ArrayList<KernelTask>(m + 1);
+            List<KernelTask> tasks = new ArrayList<>(m + 1);
             int step = n / m;
             if (step < 100) {
                 step = 100;

File: core/src/main/java/smile/sampling/Bagging.java
Patch:
@@ -52,7 +52,7 @@ public Bagging(int k, int[] y, double[] classWeight, double subsample) {
             // Training samples draw with replacement.
             for (int l = 0; l < k; l++) {
                 int nj = 0;
-                ArrayList<Integer> cj = new ArrayList<Integer>();
+                ArrayList<Integer> cj = new ArrayList<>();
                 for (int i = 0; i < n; i++) {
                     if (y[i] == l) {
                         cj.add(i);

File: core/src/main/java/smile/sequence/HMM.java
Patch:
@@ -123,7 +123,7 @@ public HMM(double[] pi, double[][] a, double[][] b, O[] symbols) {
                 throw new IllegalArgumentException("Invalid size of emission symbol list.");
             }
 
-            this.symbols = new HashMap<O, Integer>();
+            this.symbols = new HashMap<>();
             for (int i = 0; i < symbols.length; i++) {
                 this.symbols.put(symbols[i], i);
             }
@@ -481,7 +481,7 @@ public HMM(O[][] observations, int[][] labels) {
         }
 
         int index = 0;
-        symbols = new HashMap<O, Integer>();
+        symbols = new HashMap<>();
         for (int i = 0; i < observations.length; i++) {
             if (observations[i].length != labels[i].length) {
                 throw new IllegalArgumentException(String.format("The length of observation sequence %d and that of corresponding label sequence are different.", i));
@@ -573,7 +573,7 @@ public HMM<O> learn(int[][] observations, int iterations) {
      * @return an updated HMM.
      */
     private HMM<O> iterate(int[][] sequences) {
-        HMM<O> hmm = new HMM<O>(numStates, numSymbols);
+        HMM<O> hmm = new HMM<>(numStates, numSymbols);
         hmm.symbols = symbols;
 
         // gamma[n] = gamma array associated to observation sequence n

File: core/src/main/java/smile/taxonomy/Taxonomy.java
Patch:
@@ -32,7 +32,7 @@ public class Taxonomy {
     /**
      * All the concepts in this taxonomy.
      */
-    HashMap<String, Concept> concepts = new HashMap<String, Concept>();
+    HashMap<String, Concept> concepts = new HashMap<>();
     /**
      * The root node in the taxonomy.
      */
@@ -84,7 +84,7 @@ public List<String> getConcepts() {
      * Returns all named concepts from this taxonomy
      */
     private List<String> getConcepts(Concept c) {
-        List<String> keywords = new ArrayList<String>();
+        List<String> keywords = new ArrayList<>();
 
         while (c != null) {
             if (c.synset != null) {

File: core/src/main/java/smile/util/MulticoreExecutor.java
Patch:
@@ -90,7 +90,7 @@ public static int getThreadPoolSize() {
     public static <T> List<T> run(Collection<? extends Callable<T>> tasks) throws Exception {
         createThreadPool();
 
-        List<T> results = new ArrayList<T>();
+        List<T> results = new ArrayList<>();
         if (threads == null) {
             for (Callable<T> task : tasks) {
                 results.add(task.call());

File: core/src/main/java/smile/util/SmileUtils.java
Patch:
@@ -198,7 +198,7 @@ public static GaussianRadialBasis[] learnGaussianRadialBasis(double[][] x, doubl
      */
     public static <T> GaussianRadialBasis learnGaussianRadialBasis(T[] x, T[] centers, Metric<T> distance) {
         int k = centers.length;
-        CLARANS<T> clarans = new CLARANS<T>(x, distance, k, Math.min(100, (int) Math.round(0.01 * k * (x.length-k))));
+        CLARANS<T> clarans = new CLARANS<>(x, distance, k, Math.min(100, (int) Math.round(0.01 * k * (x.length - k))));
         System.arraycopy(clarans.medoids(), 0, centers, 0, k);
 
         double r0 = 0.0;
@@ -234,7 +234,7 @@ public static <T> GaussianRadialBasis[] learnGaussianRadialBasis(T[] x, T[] cent
         }
         
         int k = centers.length;
-        CLARANS<T> clarans = new CLARANS<T>(x, distance, k, Math.min(100, (int) Math.round(0.01 * k * (x.length-k))));
+        CLARANS<T> clarans = new CLARANS<>(x, distance, k, Math.min(100, (int) Math.round(0.01 * k * (x.length - k))));
         System.arraycopy(clarans.medoids(), 0, centers, 0, k);
 
         p = Math.min(p, k-1);
@@ -274,7 +274,7 @@ public static <T> GaussianRadialBasis[] learnGaussianRadialBasis(T[] x, T[] cent
         }
         
         int k = centers.length;
-        CLARANS<T> clarans = new CLARANS<T>(x, distance, k, Math.min(100, (int) Math.round(0.01 * k * (x.length-k))));
+        CLARANS<T> clarans = new CLARANS<>(x, distance, k, Math.min(100, (int) Math.round(0.01 * k * (x.length - k))));
         System.arraycopy(clarans.medoids(), 0, centers, 0, k);
 
         int n = x.length;

File: core/src/main/java/smile/validation/ConfusionMatrix.java
Patch:
@@ -31,7 +31,7 @@ public ConfusionMatrix(int[] truth, int[] prediction) {
 			 throw new IllegalArgumentException(String.format("The vector sizes don't match: %d != %d.", truth.length, prediction.length));
 		}
 		
-		Set<Integer> ySet = new HashSet<Integer>();
+		Set<Integer> ySet = new HashSet<>();
 		
 		for(int i = 0; i < truth.length; i++){
 			ySet.add(truth[i]);

File: core/src/test/java/smile/association/ARMTest.java
Patch:
@@ -114,7 +114,7 @@ public void testLearn() {
     public void testLearnPima() {
         System.out.println("pima");
 
-        List<int[]> dataList = new ArrayList<int[]>(1000);
+        List<int[]> dataList = new ArrayList<>(1000);
 
         try {
             BufferedReader input = smile.data.parser.IOUtils.getTestDataReader("transaction/pima.D38.N768.C2");
@@ -157,7 +157,7 @@ public void testLearnPima() {
     public void testLearnKosarak() {
         System.out.println("kosarak");
 
-        List<int[]> dataList = new ArrayList<int[]>(1000);
+        List<int[]> dataList = new ArrayList<>(1000);
 
         try {
             BufferedReader input = smile.data.parser.IOUtils.getTestDataReader("transaction/kosarak.dat");
@@ -170,7 +170,7 @@ public void testLearnKosarak() {
 
                 String[] s = line.split(" ");
 
-                Set<Integer> items = new HashSet<Integer>();
+                Set<Integer> items = new HashSet<>();
                 for (int i = 0; i < s.length; i++) {
                     items.add(Integer.parseInt(s[i]));
                 }

File: core/src/test/java/smile/association/FPGrowthTest.java
Patch:
@@ -120,7 +120,7 @@ public void testLearn_PrintStream() {
     public void testPima() {
         System.out.println("pima");
 
-        List<int[]> dataList = new ArrayList<int[]>(1000);
+        List<int[]> dataList = new ArrayList<>(1000);
 
         try {
             BufferedReader input = smile.data.parser.IOUtils.getTestDataReader("transaction/pima.D38.N768.C2");
@@ -168,7 +168,7 @@ public void testPima() {
     public void testKosarak() {
         System.out.println("kosarak");
 
-        List<int[]> dataList = new ArrayList<int[]>(1000);
+        List<int[]> dataList = new ArrayList<>(1000);
 
         try {
             BufferedReader input = smile.data.parser.IOUtils.getTestDataReader("transaction/kosarak.dat");
@@ -181,7 +181,7 @@ public void testKosarak() {
 
                 String[] s = line.split(" ");
 
-                Set<Integer> items = new HashSet<Integer>();
+                Set<Integer> items = new HashSet<>();
                 for (int i = 0; i < s.length; i++) {
                     items.add(Integer.parseInt(s[i]));
                 }

File: core/src/test/java/smile/association/TotalSupportTreeTest.java
Patch:
@@ -146,7 +146,7 @@ public void testGetFrequentItemsets_PrintStream() {
     public void testPima() {
         System.out.println("pima");
 
-        List<int[]> dataList = new ArrayList<int[]>(1000);
+        List<int[]> dataList = new ArrayList<>(1000);
 
         try {
             InputStream stream = getClass().getResourceAsStream("/smile/data/transaction/pima.D38.N768.C2");
@@ -199,7 +199,7 @@ public void testPima() {
     public void testKosarak() {
         System.out.println("kosarak");
 
-        List<int[]> dataList = new ArrayList<int[]>(1000);
+        List<int[]> dataList = new ArrayList<>(1000);
 
         try {
             InputStream stream = getClass().getResourceAsStream("/smile/data/transaction/kosarak.dat");
@@ -213,7 +213,7 @@ public void testKosarak() {
 
                 String[] s = line.split(" ");
 
-                Set<Integer> items = new HashSet<Integer>();
+                Set<Integer> items = new HashSet<>();
                 for (int i = 0; i < s.length; i++) {
                     items.add(Integer.parseInt(s[i]));
                 }

File: core/src/test/java/smile/classification/NaiveBayesTest.java
Patch:
@@ -73,7 +73,7 @@ public NaiveBayesTest() {
 
         moviex = new double[x.length][];
         moviey = new int[y.length];
-        Bag<String> bag = new Bag<String>(feature);
+        Bag<String> bag = new Bag<>(feature);
         for (int i = 0; i < x.length; i++) {
             moviex[i] = bag.feature(x[i]);
             moviey[i] = y[i];
@@ -124,7 +124,7 @@ public void testPredict() {
                 for (int i = 0; i < k; i++) {
                     priori[i] = 1.0 / k;
                     for (int j = 0; j < p; j++) {
-                        ArrayList<Double> axi = new ArrayList<Double>();
+                        ArrayList<Double> axi = new ArrayList<>();
                         for (int m = 0; m < trainx.length; m++) {
                             if (trainy[m] == i) {
                                 axi.add(trainx[m][j]);

File: core/src/test/java/smile/classification/RBFNetworkTest.java
Patch:
@@ -83,7 +83,7 @@ public void testLearn() {
 
                 double[][] centers = new double[10][];
                 RadialBasisFunction[] basis = SmileUtils.learnGaussianRadialBasis(trainx, centers, 5.0);
-                RBFNetwork<double[]> rbf = new RBFNetwork<double[]>(trainx, trainy, new EuclideanDistance(), basis, centers);
+                RBFNetwork<double[]> rbf = new RBFNetwork<>(trainx, trainy, new EuclideanDistance(), basis, centers);
 
                 if (y[loocv.test[i]] != rbf.predict(x[loocv.test[i]]))
                     error++;
@@ -115,7 +115,7 @@ public void testSegment() {
             
             double[][] centers = new double[100][];
             RadialBasisFunction[] basis = SmileUtils.learnGaussianRadialBasis(x, centers, 5.0);
-            RBFNetwork<double[]> rbf = new RBFNetwork<double[]>(x, y, new EuclideanDistance(), basis, centers);
+            RBFNetwork<double[]> rbf = new RBFNetwork<>(x, y, new EuclideanDistance(), basis, centers);
             
             int error = 0;
             for (int i = 0; i < testx.length; i++) {
@@ -150,7 +150,7 @@ public void testUSPS() {
             
             double[][] centers = new double[200][];
             RadialBasisFunction basis = SmileUtils.learnGaussianRadialBasis(x, centers);
-            RBFNetwork<double[]> rbf = new RBFNetwork<double[]>(x, y, new EuclideanDistance(), new GaussianRadialBasis(8.0), centers);
+            RBFNetwork<double[]> rbf = new RBFNetwork<>(x, y, new EuclideanDistance(), new GaussianRadialBasis(8.0), centers);
                 
             int error = 0;
             for (int i = 0; i < testx.length; i++) {

File: core/src/test/java/smile/clustering/CLARANSTest.java
Patch:
@@ -72,7 +72,7 @@ public void testUSPS() {
             
             AdjustedRandIndex ari = new AdjustedRandIndex();
             RandIndex rand = new RandIndex();
-            CLARANS<double[]> clarans = new CLARANS<double[]>(x, new EuclideanDistance(), 10, 50, 8);
+            CLARANS<double[]> clarans = new CLARANS<>(x, new EuclideanDistance(), 10, 50, 8);
 
             double r = rand.measure(y, clarans.getClusterLabel());
             double r2 = ari.measure(y, clarans.getClusterLabel());

File: core/src/test/java/smile/clustering/DBScanTest.java
Patch:
@@ -93,7 +93,7 @@ public void testToy() {
             label[i] = 3;
         }
         
-        DBScan<double[]> dbscan = new DBScan<double[]>(data, new KDTree<double[]>(data, data), 200, 0.8);
+        DBScan<double[]> dbscan = new DBScan<>(data, new KDTree<>(data, data), 200, 0.8);
         System.out.println(dbscan);
         
         int[] size = dbscan.getClusterSize();

File: core/src/test/java/smile/clustering/MECTest.java
Patch:
@@ -72,7 +72,7 @@ public void testUSPS() {
             
             AdjustedRandIndex ari = new AdjustedRandIndex();
             RandIndex rand = new RandIndex();
-            MEC<double[]> mec = new MEC<double[]>(x, new EuclideanDistance(), 10, 8.0);
+            MEC<double[]> mec = new MEC<>(x, new EuclideanDistance(), 10, 8.0);
             
             double r = rand.measure(y, mec.getClusterLabel());
             double r2 = ari.measure(y, mec.getClusterLabel());

File: core/src/test/java/smile/feature/BagTest.java
Patch:
@@ -63,11 +63,11 @@ public void testUniquenessOfFeatures() {
         String[] featuresForBuildingStories = {"truck", "concrete", "foundation", "steel", "crane"};
         String testMessage = "This story is about a crane and a sparrow";
 
-        ArrayList<String> mergedFeatureLists = new ArrayList<String>();
+        ArrayList<String> mergedFeatureLists = new ArrayList<>();
         mergedFeatureLists.addAll(Arrays.asList(featuresForBirdStories));
         mergedFeatureLists.addAll(Arrays.asList(featuresForBuildingStories));
 
-        Bag<String> bag = new Bag<String>(mergedFeatureLists.toArray(new String[featuresForBirdStories.length + featuresForBuildingStories.length]));
+        Bag<String> bag = new Bag<>(mergedFeatureLists.toArray(new String[featuresForBirdStories.length + featuresForBuildingStories.length]));
 
         double[] result = bag.feature(testMessage.split(" "));
         assertEquals(9, result.length);
@@ -98,7 +98,7 @@ public void testFeature() {
             "perfectly", "masterpiece", "realistic", "flaws"
         };
         
-        Bag<String> bag = new Bag<String>(feature);
+        Bag<String> bag = new Bag<>(feature);
         
         double[][] x = new double[text.length][];
         for (int i = 0; i < text.length; i++) {

File: core/src/test/java/smile/feature/FeatureSetTest.java
Patch:
@@ -62,7 +62,7 @@ public void testAttributes() {
             AttributeDataset data = parser.parse(smile.data.parser.IOUtils.getTestDataFile("weka/regression/abalone.arff"));
             double[][] x = data.toArray(new double[data.size()][]);
             
-            FeatureSet<double[]> features = new FeatureSet<double[]>();
+            FeatureSet<double[]> features = new FeatureSet<>();
             features.add(new Nominal2Binary(data.attributes()));
             features.add(new NumericAttributeFeature(data.attributes(), 0.05, 0.95, x));
             Attribute[] attributes = features.attributes();
@@ -87,7 +87,7 @@ public void testF() {
             AttributeDataset data = parser.parse(smile.data.parser.IOUtils.getTestDataFile("weka/regression/abalone.arff"));
             double[][] x = data.toArray(new double[data.size()][]);
             
-            FeatureSet<double[]> features = new FeatureSet<double[]>();
+            FeatureSet<double[]> features = new FeatureSet<>();
             features.add(new Nominal2Binary(data.attributes()));
             features.add(new NumericAttributeFeature(data.attributes(), 0.05, 0.95, x));
             

File: core/src/test/java/smile/gap/GeneticAlgorithmTest.java
Patch:
@@ -87,7 +87,7 @@ public void testEvolve() {
             seeds[i] = new BitString(15, new Knapnack(), BitString.Crossover.UNIFORM, 1.0, 0.2);
         }
         
-        GeneticAlgorithm<BitString> instance = new GeneticAlgorithm<BitString>(seeds, GeneticAlgorithm.Selection.TOURNAMENT);
+        GeneticAlgorithm<BitString> instance = new GeneticAlgorithm<>(seeds, GeneticAlgorithm.Selection.TOURNAMENT);
         instance.setElitism(2);
         instance.setTournament(3, 0.95);
         

File: core/src/test/java/smile/neighbor/BKTreeSpeedTest.java
Patch:
@@ -32,7 +32,7 @@
  */
 public class BKTreeSpeedTest {
 
-    List<String> words = new ArrayList<String>();
+    List<String> words = new ArrayList<>();
     BKTree<String> bktree;
 
     public BKTreeSpeedTest() {
@@ -57,7 +57,7 @@ public BKTreeSpeedTest() {
         String[] data = words.toArray(new String[words.size()]);
 
         start = System.currentTimeMillis();
-        bktree = new BKTree<String>(new EditDistance(50, true));
+        bktree = new BKTree<>(new EditDistance(50, true));
         bktree.add(data);
         time = (System.currentTimeMillis() - start) / 1000.0;
         System.out.format("Building BK-tree: %.2fs%n", time);
@@ -86,7 +86,7 @@ public void tearDown() {
     public void testBKTreeSpeed() {
         System.out.println("BK-Tree range 1 speed");
         long start = System.currentTimeMillis();
-        List<Neighbor<String, String>> neighbors = new ArrayList<Neighbor<String, String>>();
+        List<Neighbor<String, String>> neighbors = new ArrayList<>();
         for (int i = 1000; i < 1100; i++) {
             bktree.range(words.get(i), 1, neighbors);
             neighbors.clear();

File: core/src/test/java/smile/neighbor/CoverTreeSpeedTest.java
Patch:
@@ -55,7 +55,7 @@ public CoverTreeSpeedTest() {
         System.out.format("Loading data: %.2fs%n", time);
 
         start = System.currentTimeMillis();
-        coverTree = new CoverTree<double[]>(x, new EuclideanDistance());
+        coverTree = new CoverTree<>(x, new EuclideanDistance());
         time = (System.currentTimeMillis() - start) / 1000.0;
         System.out.format("Building cover tree: %.2fs%n", time);
     }
@@ -96,7 +96,7 @@ public void testCoverTree() {
         System.out.format("10-NN: %.2fs%n", time);
 
         start = System.currentTimeMillis();
-        List<Neighbor<double[], double[]>> n = new ArrayList<Neighbor<double[], double[]>>();
+        List<Neighbor<double[], double[]>> n = new ArrayList<>();
         for (int i = 0; i < testx.length; i++) {
             coverTree.range(testx[i], 8.0, n);
             n.clear();

File: core/src/test/java/smile/neighbor/CoverTreeStringSpeedTest.java
Patch:
@@ -32,7 +32,7 @@
  */
 public class CoverTreeStringSpeedTest {
 
-    List<String> words = new ArrayList<String>();
+    List<String> words = new ArrayList<>();
     CoverTree<String> cover;
 
     public CoverTreeStringSpeedTest() {
@@ -57,7 +57,7 @@ public CoverTreeStringSpeedTest() {
         String[] data = words.toArray(new String[words.size()]);
 
         start = System.currentTimeMillis();
-        cover = new CoverTree<String>(data, new EditDistance(50, true));
+        cover = new CoverTree<>(data, new EditDistance(50, true));
         time = (System.currentTimeMillis() - start) / 1000.0;
         System.out.format("Building cover tree: %.2fs%n", time);
     }
@@ -85,7 +85,7 @@ public void tearDown() {
     public void testNaiveSpeed() {
         System.out.println("cover tree");
         long start = System.currentTimeMillis();
-        List<Neighbor<String, String>> neighbors = new ArrayList<Neighbor<String, String>>();
+        List<Neighbor<String, String>> neighbors = new ArrayList<>();
         for (int i = 1000; i < 1100; i++) {
             cover.range(words.get(i), 1, neighbors);
             neighbors.clear();

File: core/src/test/java/smile/neighbor/MPLSHSpeedTest.java
Patch:
@@ -79,7 +79,7 @@ public void testUSPS() {
         System.out.format("Loading USPS: %.2fs%n", time);
 
         start = System.currentTimeMillis();
-        MPLSH<double[]> lsh = new MPLSH<double[]>(256, 100, 3, 4.0);
+        MPLSH<double[]> lsh = new MPLSH<>(256, 100, 3, 4.0);
         for (double[] xi : x) {
             lsh.put(xi, xi);
         }
@@ -90,7 +90,7 @@ public void testUSPS() {
             train[i] = x[index[i]];
         }
 
-        LinearSearch<double[]> naive = new LinearSearch<double[]>(x, new EuclideanDistance());
+        LinearSearch<double[]> naive = new LinearSearch<>(x, new EuclideanDistance());
         lsh.learn(naive, train, 8.0);
         time = (System.currentTimeMillis() - start) / 1000.0;
         System.out.format("Building LSH: %.2fs%n", time);
@@ -110,7 +110,7 @@ public void testUSPS() {
         System.out.format("10-NN: %.2fs%n", time);
 
         start = System.currentTimeMillis();
-        List<Neighbor<double[], double[]>> n = new ArrayList<Neighbor<double[], double[]>>();
+        List<Neighbor<double[], double[]>> n = new ArrayList<>();
         for (int i = 0; i < testx.length; i++) {
             lsh.range(testx[i], 8.0, n);
             n.clear();

File: core/src/test/java/smile/regression/SVRTest.java
Patch:
@@ -77,7 +77,7 @@ public void testCPU() {
                 double[][] testx = Math.slice(datax, cv.test[i]);
                 double[] testy = Math.slice(datay, cv.test[i]);
 
-                SVR<double[]> svr = new SVR<double[]>(trainx, trainy, new PolynomialKernel(3, 1.0, 1.0), 0.1, 1.0);
+                SVR<double[]> svr = new SVR<>(trainx, trainy, new PolynomialKernel(3, 1.0, 1.0), 0.1, 1.0);
 
                 for (int j = 0; j < testx.length; j++) {
                     double r = testy[j] - svr.predict(testx[j]);

File: core/src/test/java/smile/taxonomy/TaxonomyTest.java
Patch:
@@ -113,7 +113,7 @@ public void testLowestCommonAncestor() {
     @Test
     public void testGetPathToRoot() {
         System.out.println("getPathToRoot");
-        LinkedList<Concept> expResult = new LinkedList<Concept>();
+        LinkedList<Concept> expResult = new LinkedList<>();
         expResult.addFirst(instance.getRoot());
         expResult.addFirst(ad);
         expResult.addFirst(a);
@@ -129,7 +129,7 @@ public void testGetPathToRoot() {
     @Test
     public void testGetPathFromRoot() {
         System.out.println("getPathToRoot");
-        LinkedList<Concept> expResult = new LinkedList<Concept>();
+        LinkedList<Concept> expResult = new LinkedList<>();
         expResult.add(instance.getRoot());
         expResult.add(ad);
         expResult.add(a);

File: data/src/main/java/smile/data/StringAttribute.java
Patch:
@@ -38,11 +38,11 @@ public class StringAttribute extends Attribute {
     /**
      * The list of unique string values of this attribute.
      */
-    private List<String> values = new ArrayList<String>();
+    private List<String> values = new ArrayList<>();
     /**
      * Map a string to an integer level.
      */
-    private Map<String, Integer> map = new HashMap<String, Integer>();
+    private Map<String, Integer> map = new HashMap<>();
 
     /**
      * Constructor.

File: data/src/main/java/smile/data/parser/BinarySparseDatasetParser.java
Patch:
@@ -122,7 +122,7 @@ public BinarySparseDataset parse(String name, InputStream stream) throws IOExcep
                 throw new IOException("Empty data source.");
            }
         
-           Set<Integer> items = new HashSet<Integer>();
+           Set<Integer> items = new HashSet<>();
            do {
                 line = line.trim();
                 if (line.isEmpty()) {

File: data/src/main/java/smile/data/parser/DelimitedTextParser.java
Patch:
@@ -348,7 +348,7 @@ private AttributeDataset parse(String name, Attribute[] attributes, BufferedRead
                 }
             }
 
-            Datum<double[]> datum = new Datum<double[]>(x, y);
+            Datum<double[]> datum = new Datum<>(x, y);
             datum.name = rowName;
             data.add(datum);
         }
@@ -378,7 +378,7 @@ private AttributeDataset parse(String name, Attribute[] attributes, BufferedRead
                 }
             }
 
-            Datum<double[]> datum = new Datum<double[]>(x, y);
+            Datum<double[]> datum = new Datum<>(x, y);
             datum.name = rowName;
             data.add(datum);
         }

File: data/src/main/java/smile/data/parser/IOUtils.java
Patch:
@@ -74,7 +74,7 @@ public static List<String> readLines(InputStream input, Charset charset) throws
      */
     public static List<String> readLines(Reader input) throws IOException {
         BufferedReader reader = input instanceof BufferedReader ? (BufferedReader) input : new BufferedReader(input);
-        List<String> list = new ArrayList<String>();
+        List<String> list = new ArrayList<>();
         String line = reader.readLine();
         while (line != null) {
             list.add(line);

File: data/src/main/java/smile/data/parser/SparseDatasetParser.java
Patch:
@@ -173,9 +173,9 @@ public SparseDataset parse(String name, InputStream stream) throws IOException,
                     throw new ParseException("Invalid number of tokens.", nrow);
                 }
 
-                int d = Integer.valueOf(tokens[0]) - arrayIndexOrigin;
-                int w = Integer.valueOf(tokens[1]) - arrayIndexOrigin;
-                double c = Double.valueOf(tokens[2]);
+                int d = Integer.parseInt(tokens[0]) - arrayIndexOrigin;
+                int w = Integer.parseInt(tokens[1]) - arrayIndexOrigin;
+                double c = Double.parseDouble(tokens[2]);
                 sparse.set(d, w, c);
 
                 line = reader.readLine();

File: data/src/main/java/smile/data/parser/microarray/GCTParser.java
Patch:
@@ -168,8 +168,8 @@ public AttributeDataset parse(String name, InputStream stream) throws IOExceptio
             throw new IOException("Invalid data size inforamation.");            
         }
         
-        int n = Integer.valueOf(tokens[0]);
-        int p = Integer.valueOf(tokens[1]);
+        int n = Integer.parseInt(tokens[0]);
+        int p = Integer.parseInt(tokens[1]);
         if (n <= 0 || p <= 0) {
             throw new IOException(String.format("Invalid data size %d x %d.", n, p));                        
         }
@@ -211,7 +211,7 @@ public AttributeDataset parse(String name, InputStream stream) throws IOExceptio
                 }
             }
 
-            Datum<double[]> datum = new Datum<double[]>(x);
+            Datum<double[]> datum = new Datum<>(x);
             datum.name = tokens[0];
             datum.description = tokens[1];
             data.add(datum);

File: data/src/main/java/smile/data/parser/microarray/PCLParser.java
Patch:
@@ -180,7 +180,7 @@ public AttributeDataset parse(String name, InputStream stream) throws IOExceptio
                 }
             }
 
-            Datum<double[]> datum = new Datum<double[]>(x);
+            Datum<double[]> datum = new Datum<>(x);
             datum.name = tokens[0];
             datum.description = tokens[1];
             datum.weight = Double.valueOf(tokens[2]);

File: data/src/main/java/smile/data/parser/microarray/RESParser.java
Patch:
@@ -187,7 +187,7 @@ public AttributeDataset parse(String name, InputStream stream) throws IOExceptio
             throw new IOException("Premature end of file.");
         }
 
-        int n = Integer.valueOf(line);
+        int n = Integer.parseInt(line);
         if (n <= 0) {
             throw new IOException("Invalid number of rows: " + n);            
         }
@@ -210,7 +210,7 @@ public AttributeDataset parse(String name, InputStream stream) throws IOExceptio
                 x[j] = Double.valueOf(tokens[2*j+2]);
             }
 
-            Datum<double[]> datum = new Datum<double[]>(x);
+            Datum<double[]> datum = new Datum<>(x);
             datum.name = tokens[1];
             datum.description = tokens[0];
             data.add(datum);

File: data/src/main/java/smile/data/parser/microarray/TXTParser.java
Patch:
@@ -159,7 +159,7 @@ public AttributeDataset parse(String name, InputStream stream) throws IOExceptio
                 }
             }
 
-            Datum<double[]> datum = new Datum<double[]>(x);
+            Datum<double[]> datum = new Datum<>(x);
             datum.name = tokens[0];
             if (start == 2) {
                 datum.description = tokens[1];

File: demo/src/main/java/smile/demo/classification/ClassificationDemo.java
Patch:
@@ -82,7 +82,7 @@ public ClassificationDemo() {
         startButton.setActionCommand("startButton");
         startButton.addActionListener(this);
 
-        datasetBox = new JComboBox<String>();
+        datasetBox = new JComboBox<>();
         for (int i = 0; i < datasetName.length; i++) {
             datasetBox.addItem(datasetName[i]);
         }

File: demo/src/main/java/smile/demo/classification/RBFNetworkDemo.java
Patch:
@@ -64,7 +64,7 @@ public double[][] learn(double[] x, double[] y) {
 
         double[][] centers = new double[k][];
         RadialBasisFunction basis = SmileUtils.learnGaussianRadialBasis(data, centers);
-        RBFNetwork<double[]> rbf = new RBFNetwork<double[]>(data, label, new EuclideanDistance(), basis, centers);
+        RBFNetwork<double[]> rbf = new RBFNetwork<>(data, label, new EuclideanDistance(), basis, centers);
         
         for (int i = 0; i < label.length; i++) {
             label[i] = rbf.predict(data[i]);

File: demo/src/main/java/smile/demo/classification/SVMDemo.java
Patch:
@@ -76,7 +76,7 @@ public double[][] learn(double[] x, double[] y) {
         double[][] data = dataset[datasetIndex].toArray(new double[dataset[datasetIndex].size()][]);
         int[] label = dataset[datasetIndex].toArray(new int[dataset[datasetIndex].size()]);
         
-        SVM<double[]> svm = new SVM<double[]>(new GaussianKernel(gamma), C);
+        SVM<double[]> svm = new SVM<>(new GaussianKernel(gamma), C);
         svm.learn(data, label);
         svm.finish();
         

File: demo/src/main/java/smile/demo/clustering/DBScanDemo.java
Patch:
@@ -82,7 +82,7 @@ public JComponent learn() {
         }
 
         long clock = System.currentTimeMillis();
-        DBScan<double[]> dbscan = new DBScan<double[]>(dataset[datasetIndex], new EuclideanDistance(), minPts, range);
+        DBScan<double[]> dbscan = new DBScan<>(dataset[datasetIndex], new EuclideanDistance(), minPts, range);
         System.out.format("DBSCAN clusterings %d samples in %dms\n", dataset[datasetIndex].length, System.currentTimeMillis()-clock);
 
         JPanel pane = new JPanel(new GridLayout(1, 2));

File: demo/src/main/java/smile/demo/clustering/HierarchicalClusteringDemo.java
Patch:
@@ -48,7 +48,7 @@ public class HierarchicalClusteringDemo extends ClusteringDemo {
     JComboBox<String> linkageBox;
 
     public HierarchicalClusteringDemo() {
-        linkageBox = new JComboBox<String>();
+        linkageBox = new JComboBox<>();
         linkageBox.addItem("Single");
         linkageBox.addItem("Complete");
         linkageBox.addItem("UPGMA");

File: demo/src/main/java/smile/demo/clustering/MECDemo.java
Patch:
@@ -59,7 +59,7 @@ public JComponent learn() {
         }
 
         long clock = System.currentTimeMillis();
-        MEC<double[]> mec = new MEC<double[]>(dataset[datasetIndex], new EuclideanDistance(), clusterNumber, range);
+        MEC<double[]> mec = new MEC<>(dataset[datasetIndex], new EuclideanDistance(), clusterNumber, range);
         System.out.format("MEC clusterings %d samples in %dms\n", dataset[datasetIndex].length, System.currentTimeMillis()-clock);
 
         PlotCanvas plot = ScatterPlot.plot(dataset[datasetIndex], pointLegend);

File: demo/src/main/java/smile/demo/clustering/SIBDemo.java
Patch:
@@ -64,7 +64,7 @@ public SIBDemo() {
         startButton.setActionCommand("startButton");
         startButton.addActionListener(this);
 
-        datasetBox = new JComboBox<String>();
+        datasetBox = new JComboBox<>();
         for (int i = 0; i < datasetName.length; i++) {
             datasetBox.addItem(datasetName[i]);
         }

File: demo/src/main/java/smile/demo/interpolation/ScatterDataInterpolationDemo.java
Patch:
@@ -64,8 +64,8 @@ public ScatterDataInterpolationDemo() {
         add(canvas);
 
         double[][] ww = new double[26][26];
-        ArrayList<double[]> xx = new ArrayList<double[]>();
-        ArrayList<Double> zz = new ArrayList<Double>();
+        ArrayList<double[]> xx = new ArrayList<>();
+        ArrayList<Double> zz = new ArrayList<>();
         for (int i = 0; i <= 25; i++) {
             for (int j = 0; j <= 25; j++) {
                 if (Math.random() < 0.2)

File: demo/src/main/java/smile/demo/manifold/ManifoldDemo.java
Patch:
@@ -63,7 +63,7 @@ public ManifoldDemo() {
         startButton.setActionCommand("startButton");
         startButton.addActionListener(this);
 
-        datasetBox = new JComboBox<String>();
+        datasetBox = new JComboBox<>();
         for (int i = 0; i < datasetName.length; i++) {
             datasetBox.addItem(datasetName[i]);
         }

File: demo/src/main/java/smile/demo/mds/IsotonicMDSDemo.java
Patch:
@@ -70,7 +70,7 @@ public IsotonicMDSDemo() {
         startButton.setActionCommand("startButton");
         startButton.addActionListener(this);
 
-        datasetBox = new JComboBox<String>();
+        datasetBox = new JComboBox<>();
         for (int i = 0; i < datasetName.length; i++) {
             datasetBox.addItem(datasetName[i]);
         }

File: demo/src/main/java/smile/demo/mds/MDSDemo.java
Patch:
@@ -70,7 +70,7 @@ public MDSDemo() {
         startButton.setActionCommand("startButton");
         startButton.addActionListener(this);
 
-        datasetBox = new JComboBox<String>();
+        datasetBox = new JComboBox<>();
         for (int i = 0; i < datasetName.length; i++) {
             datasetBox.addItem(datasetName[i]);
         }

File: demo/src/main/java/smile/demo/mds/SammonMappingDemo.java
Patch:
@@ -70,7 +70,7 @@ public SammonMappingDemo() {
         startButton.setActionCommand("startButton");
         startButton.addActionListener(this);
 
-        datasetBox = new JComboBox<String>();
+        datasetBox = new JComboBox<>();
         for (int i = 0; i < datasetName.length; i++) {
             datasetBox.addItem(datasetName[i]);
         }

File: demo/src/main/java/smile/demo/projection/KPCADemo.java
Patch:
@@ -120,7 +120,7 @@ public JComponent learn() {
         plot.setTitle("PCA");
         pane.add(plot);
 
-        KPCA<double[]> kpca = new KPCA<double[]>(data, new GaussianKernel(gamma[datasetIndex]), 2);
+        KPCA<double[]> kpca = new KPCA<>(data, new GaussianKernel(gamma[datasetIndex]), 2);
 
         y = kpca.getCoordinates();
         plot = new PlotCanvas(Math.colMin(y), Math.colMax(y));
@@ -139,7 +139,7 @@ public JComponent learn() {
         pane.add(plot);
 
         clock = System.currentTimeMillis();
-        kpca = new KPCA<double[]>(data, new GaussianKernel(gamma[datasetIndex]), 3);
+        kpca = new KPCA<>(data, new GaussianKernel(gamma[datasetIndex]), 3);
         System.out.format("Learn KPCA from %d samples in %dms\n", data.length, System.currentTimeMillis() - clock);
 
         y = kpca.getCoordinates();

File: demo/src/main/java/smile/demo/projection/LDADemo.java
Patch:
@@ -65,7 +65,7 @@ public LDADemo() {
         startButton.setActionCommand("startButton");
         startButton.addActionListener(this);
 
-        datasetBox = new JComboBox<String>();
+        datasetBox = new JComboBox<>();
         for (int i = 0; i < datasetName.length; i++) {
             datasetBox.addItem(datasetName[i]);
         }

File: demo/src/main/java/smile/demo/projection/PCADemo.java
Patch:
@@ -39,7 +39,7 @@ public class PCADemo extends ProjectionDemo {
     JComboBox<String> corBox;
 
     public PCADemo() {
-        corBox = new JComboBox<String>();
+        corBox = new JComboBox<>();
         corBox.addItem("Covariance");
         corBox.addItem("Correlation");
         corBox.setSelectedIndex(0);

File: demo/src/main/java/smile/demo/projection/ProjectionDemo.java
Patch:
@@ -65,7 +65,7 @@ public ProjectionDemo() {
         startButton.setActionCommand("startButton");
         startButton.addActionListener(this);
 
-        datasetBox = new JComboBox<String>();
+        datasetBox = new JComboBox<>();
         for (int i = 0; i < datasetName.length; i++) {
             datasetBox.addItem(datasetName[i]);
         }

File: demo/src/main/java/smile/demo/stat/distribution/BernoulliDistributionDemo.java
Patch:
@@ -52,7 +52,7 @@ public class BernoulliDistributionDemo extends JPanel implements ChangeListener
     public BernoulliDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         for (int i = 1; i < 10; i+=2) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i/10.0)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/BetaDistributionDemo.java
Patch:
@@ -58,7 +58,7 @@ public class BetaDistributionDemo extends JPanel implements ChangeListener {
     public BetaDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         for (int i = 0; i <= 100; i+=20) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i / 10)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/BinomialDistributionDemo.java
Patch:
@@ -54,7 +54,7 @@ public class BinomialDistributionDemo extends JPanel implements ChangeListener {
     public BinomialDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> nLabelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> nLabelTable = new Hashtable<>();
         nLabelTable.put(1, new JLabel(String.valueOf(1)));
         for (int i = 10; i <= 50; i+=10) {
             nLabelTable.put(new Integer(i), new JLabel(String.valueOf(i)));
@@ -68,7 +68,7 @@ public BinomialDistributionDemo() {
         nSlider.setPaintTicks(true);
         nSlider.setPaintLabels(true);
 
-        Hashtable<Integer, JLabel> probLabelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> probLabelTable = new Hashtable<>();
         for (int i = 1; i < 10; i+=2) {
             probLabelTable.put(new Integer(i), new JLabel(String.valueOf(i/10.0)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/ChiSquareDistributionDemo.java
Patch:
@@ -55,7 +55,7 @@ public class ChiSquareDistributionDemo extends JPanel implements ChangeListener
     public ChiSquareDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         labelTable.put(1, new JLabel(String.valueOf(1)));
         for (int i = 5; i <= 20; i += 5) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i)));

File: demo/src/main/java/smile/demo/stat/distribution/ExponentialDistributionDemo.java
Patch:
@@ -56,7 +56,7 @@ public class ExponentialDistributionDemo extends JPanel implements ChangeListene
     public ExponentialDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         for (int i = 0; i <= 50; i+=10) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i/10)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/FDistributionDemo.java
Patch:
@@ -57,7 +57,7 @@ public class FDistributionDemo extends JPanel implements ChangeListener {
     public FDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         labelTable.put(1, new JLabel(String.valueOf(1)));
         for (int i = 20; i <= 100; i += 20) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i)));

File: demo/src/main/java/smile/demo/stat/distribution/GammaDistributionDemo.java
Patch:
@@ -58,7 +58,7 @@ public class GammaDistributionDemo extends JPanel implements ChangeListener {
     public GammaDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> shapeLabelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> shapeLabelTable = new Hashtable<>();
         for (int i = 0; i <= 10; i+=2) {
             shapeLabelTable.put(new Integer(i), new JLabel(String.valueOf(i)));
         }
@@ -71,7 +71,7 @@ public GammaDistributionDemo() {
         shapeSlider.setPaintTicks(true);
         shapeSlider.setPaintLabels(true);
 
-        Hashtable<Integer, JLabel> scaleLabelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> scaleLabelTable = new Hashtable<>();
         for (int i = 0; i <= 50; i+=10) {
             scaleLabelTable.put(new Integer(i), new JLabel(String.valueOf(i/10)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/GaussianDistributionDemo.java
Patch:
@@ -56,7 +56,7 @@ public class GaussianDistributionDemo extends JPanel implements ChangeListener {
     public GaussianDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         for (int i = 0; i <= 50; i+=10) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i/10)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/GeometricDistributionDemo.java
Patch:
@@ -52,7 +52,7 @@ public class GeometricDistributionDemo extends JPanel implements ChangeListener
     public GeometricDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         for (int i = 1; i < 10; i+=2) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i/10.0)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/HyperGeometricDistributionDemo.java
Patch:
@@ -55,7 +55,7 @@ public class HyperGeometricDistributionDemo extends JPanel implements ChangeList
     public HyperGeometricDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         labelTable.put(1, new JLabel(String.valueOf(1)));
         for (int i = 20; i <= 100; i+=20) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i)));

File: demo/src/main/java/smile/demo/stat/distribution/LogNormalDistributionDemo.java
Patch:
@@ -56,7 +56,7 @@ public class LogNormalDistributionDemo extends JPanel implements ChangeListener
     public LogNormalDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         for (int i = 0; i <= 20; i+=10) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i/10)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/LogisticDistributionDemo.java
Patch:
@@ -56,7 +56,7 @@ public class LogisticDistributionDemo extends JPanel implements ChangeListener {
     public LogisticDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         for (int i = 0; i <= 50; i+=10) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i/10)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/MultivariateGaussianDistributionDemo.java
Patch:
@@ -51,7 +51,7 @@ public class MultivariateGaussianDistributionDemo extends JPanel implements Chan
     public MultivariateGaussianDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         for (int i = 0; i <= 30; i+=10) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i/10.0)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/NegativeBinomialDistributionDemo.java
Patch:
@@ -54,7 +54,7 @@ public class NegativeBinomialDistributionDemo extends JPanel implements ChangeLi
     public NegativeBinomialDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> nLabelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> nLabelTable = new Hashtable<>();
         nLabelTable.put(1, new JLabel(String.valueOf(1)));
         for (int i = 10; i <= 50; i+=10) {
             nLabelTable.put(new Integer(i), new JLabel(String.valueOf(i)));
@@ -68,7 +68,7 @@ public NegativeBinomialDistributionDemo() {
         nSlider.setPaintTicks(true);
         nSlider.setPaintLabels(true);
 
-        Hashtable<Integer, JLabel> probLabelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> probLabelTable = new Hashtable<>();
         for (int i = 1; i < 10; i+=2) {
             probLabelTable.put(new Integer(i), new JLabel(String.valueOf(i/10.0)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/PoissonDistributionDemo.java
Patch:
@@ -52,7 +52,7 @@ public class PoissonDistributionDemo extends JPanel implements ChangeListener {
     public PoissonDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         for (int i = 0; i <= 10; i+=2) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/ShiftedGeometricDistributionDemo.java
Patch:
@@ -52,7 +52,7 @@ public class ShiftedGeometricDistributionDemo extends JPanel implements ChangeLi
     public ShiftedGeometricDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         for (int i = 1; i < 10; i+=2) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i/10.0)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/TDistributionDemo.java
Patch:
@@ -55,7 +55,7 @@ public class TDistributionDemo extends JPanel implements ChangeListener {
     public TDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         labelTable.put(1, new JLabel(String.valueOf(1)));
         for (int i = 5; i <= 20; i += 5) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i)));

File: demo/src/main/java/smile/demo/stat/distribution/WeibullDistributionDemo.java
Patch:
@@ -58,7 +58,7 @@ public class WeibullDistributionDemo extends JPanel implements ChangeListener {
     public WeibullDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> shapeLabelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> shapeLabelTable = new Hashtable<>();
         for (int i = 0; i <= 10; i+=2) {
             shapeLabelTable.put(new Integer(i), new JLabel(String.valueOf(i)));
         }
@@ -71,7 +71,7 @@ public WeibullDistributionDemo() {
         shapeSlider.setPaintTicks(true);
         shapeSlider.setPaintLabels(true);
 
-        Hashtable<Integer, JLabel> scaleLabelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> scaleLabelTable = new Hashtable<>();
         for (int i = 0; i <= 50; i+=10) {
             scaleLabelTable.put(new Integer(i), new JLabel(String.valueOf(i/10)));
         }

File: demo/src/main/java/smile/demo/util/ParameterParser.java
Patch:
@@ -82,7 +82,7 @@ private class Parameter {
      */
     public ParameterParser(String usage) {
         this.usage = usage;
-        parameters = new HashMap<String, Parameter>();
+        parameters = new HashMap<>();
     }
 
     /**
@@ -135,7 +135,7 @@ public String get(String name) {
      * @return a list of expanded name-value pair if monads are detected
      */
     private List<String> filterMonadics(String[] args) {// name-value for monads
-        List<String> filteredArgs = new ArrayList<String>();       // Y <- return List
+        List<String> filteredArgs = new ArrayList<>();       // Y <- return List
         for (String arg : args) {                        // iterate over args
             filteredArgs.add(arg);
             Parameter param = parameters.get(arg);
@@ -153,7 +153,7 @@ private List<String> filterMonadics(String[] args) {// name-value for monads
      * name.
      */
     public List<String> parse(String[] args) {    // merge args & defaults
-        List<String> extras = new ArrayList<String>();
+        List<String> extras = new ArrayList<>();
         List<String> filteredArgs = filterMonadics(args);          // detect and fill mons
         for (int i = 0; i < filteredArgs.size(); i++) {
             String key = filteredArgs.get(i);

File: demo/src/main/java/smile/demo/vq/VQDemo.java
Patch:
@@ -114,7 +114,7 @@ public VQDemo() {
         startButton.setActionCommand("startButton");
         startButton.addActionListener(this);
 
-        datasetBox = new JComboBox<String>();
+        datasetBox = new JComboBox<>();
         for (int i = 0; i < datasetName.length; i++) {
             datasetBox.addItem(datasetName[i]);
         }

File: math/src/main/java/smile/math/Math.java
Patch:
@@ -4017,7 +4017,7 @@ public static double[][] pow(double[][] x, double n) {
      * @return the same values as in x but with no repetitions.
      */
     public static int[] unique(int[] x) {
-        HashSet<Integer> hash = new HashSet<Integer>();
+        HashSet<Integer> hash = new HashSet<>();
         for (int i = 0; i < x.length; i++) {
             hash.add(x[i]);
         }
@@ -4038,7 +4038,7 @@ public static int[] unique(int[] x) {
      * @return the same values as in x but with no repetitions.
      */
     public static String[] unique(String[] x) {
-        HashSet<String> hash = new HashSet<String>(Arrays.asList(x));
+        HashSet<String> hash = new HashSet<>(Arrays.asList(x));
 
         String[] y = new String[hash.size()];
 

File: math/src/main/java/smile/math/SparseArray.java
Patch:
@@ -67,7 +67,7 @@ public SparseArray() {
      * @param size the number of nonzero entries in the matrix.
      */
     private SparseArray(int initialCapacity) {
-        array = new ArrayList<Entry>(initialCapacity);
+        array = new ArrayList<>(initialCapacity);
     }
 
     /**

File: math/src/main/java/smile/math/matrix/EigenValueDecomposition.java
Patch:
@@ -906,7 +906,7 @@ private static int error_bound(boolean[] enough, double[] ritz, double[] bnd, in
 
         logger.info("Lancozs method found {} converged eigenvalues of the {}-by-{} matrix", neig, step + 1, step + 1);
         if (neig != 0) {
-            for (int i = 0, n = 0; i <= step; i++) {
+            for (int i = 0; i <= step; i++) {
                 if (bnd[i] <= 16.0 * Math.EPSILON * Math.abs(ritz[i])) {
                     logger.info("ritz[{}] = {}", i, ritz[i]);
                 }

File: math/src/main/java/smile/stat/distribution/DiscreteExponentialFamilyMixture.java
Patch:
@@ -158,7 +158,7 @@ public DiscreteExponentialFamilyMixture(List<Component> mixture, int[] data) {
             }
 
             // Maximization step
-            List<Component> newConfig = new ArrayList<Component>();
+            List<Component> newConfig = new ArrayList<>();
             for (int i = 0; i < m; i++)
                 newConfig.add(((DiscreteExponentialFamily) components.get(i).distribution).M(x, posteriori[i]));
 

File: math/src/main/java/smile/stat/distribution/DiscreteMixture.java
Patch:
@@ -47,15 +47,15 @@ public static class Component {
      * Constructor.
      */
     DiscreteMixture() {
-        components = new ArrayList<Component>();
+        components = new ArrayList<>();
     }
 
     /**
      * Constructor.
      * @param mixture a list of discrete distributions.
      */
     public DiscreteMixture(List<Component> mixture) {
-        components = new ArrayList<Component>();
+        components = new ArrayList<>();
         components.addAll(mixture);
 
         double sum = 0.0;

File: math/src/main/java/smile/stat/distribution/ExponentialFamilyMixture.java
Patch:
@@ -159,7 +159,7 @@ public ExponentialFamilyMixture(List<Component> mixture, double[] data) {
             }
 
             // Maximization step
-            ArrayList<Component> newConfig = new ArrayList<Component>();
+            ArrayList<Component> newConfig = new ArrayList<>();
             for (int i = 0; i < m; i++)
                 newConfig.add(((ExponentialFamily) components.get(i).distribution).M(x, posteriori[i]));
 

File: math/src/main/java/smile/stat/distribution/GaussianMixture.java
Patch:
@@ -70,7 +70,7 @@ public GaussianMixture(double[] data) {
         if (data.length < 20)
             throw new IllegalArgumentException("Too few samples.");
         
-        ArrayList<Component> mixture = new ArrayList<Component>();
+        ArrayList<Component> mixture = new ArrayList<>();
         Component c = new Component();
         c.priori = 1.0;
         c.distribution = new GaussianDistribution(data);

File: math/src/main/java/smile/stat/distribution/Mixture.java
Patch:
@@ -61,15 +61,15 @@ public static class Component {
      * Constructor.
      */
     Mixture() {
-        components = new ArrayList<Component>();
+        components = new ArrayList<>();
     }
 
     /**
      * Constructor.
      * @param mixture a list of distributions.
      */
     public Mixture(List<Component> mixture) {
-        components = new ArrayList<Component>();
+        components = new ArrayList<>();
         components.addAll(mixture);
 
         double sum = 0.0;

File: math/src/main/java/smile/stat/distribution/MultivariateExponentialFamilyMixture.java
Patch:
@@ -155,7 +155,7 @@ public MultivariateExponentialFamilyMixture(List<Component> mixture, double[][]
             }
 
             // Maximization step
-            ArrayList<Component> newConfig = new ArrayList<Component>();
+            ArrayList<Component> newConfig = new ArrayList<>();
             for (int i = 0; i < m; i++)
                 newConfig.add(((MultivariateExponentialFamily) components.get(i).distribution).M(x, posteriori[i]));
 

File: math/src/main/java/smile/stat/distribution/MultivariateGaussianMixture.java
Patch:
@@ -168,7 +168,7 @@ public MultivariateGaussianMixture(double[][] data, boolean diagonal) {
         if (data.length < 20)
             throw new IllegalArgumentException("Too few samples.");
 
-        ArrayList<Component> mixture = new ArrayList<Component>();
+        ArrayList<Component> mixture = new ArrayList<>();
         Component c = new Component();
         c.priori = 1.0;
         c.distribution = new MultivariateGaussianDistribution(data, diagonal);

File: math/src/main/java/smile/stat/distribution/MultivariateMixture.java
Patch:
@@ -46,15 +46,15 @@ public static class Component {
      * Construct an empty Mixture.
      */
     MultivariateMixture() {
-        components = new ArrayList<Component>();
+        components = new ArrayList<>();
     }
 
     /**
      * Constructor.
      * @param mixture a list of multivariate distributions.
      */
     public MultivariateMixture(List<Component> mixture) {
-        components = new ArrayList<Component>();
+        components = new ArrayList<>();
         components.addAll(mixture);
     }
 

File: math/src/test/java/smile/math/distance/JaccardDistanceTest.java
Patch:
@@ -56,13 +56,13 @@ public void tearDown() {
     @Test
     public void testDistance() {
         System.out.println("distance");
-        Set<Integer> a = new HashSet<Integer>();
+        Set<Integer> a = new HashSet<>();
         a.add(1);
         a.add(2);
         a.add(3);
         a.add(4);
 
-        Set<Integer> b = new HashSet<Integer>();
+        Set<Integer> b = new HashSet<>();
         b.add(3);
         b.add(4);
         b.add(5);

File: math/src/test/java/smile/sort/HeapSelectTest.java
Patch:
@@ -55,7 +55,7 @@ public void tearDown() {
     @Test
     public void testSelect() {
         System.out.println("HeapSelect");
-        HeapSelect<Integer> instance = new HeapSelect<Integer>(new Integer[10]);
+        HeapSelect<Integer> instance = new HeapSelect<>(new Integer[10]);
         for (int i = 0; i < 1000; i++) {
             instance.add(i);
             if (i > 10) {
@@ -65,7 +65,7 @@ public void testSelect() {
             }
         }
 
-        instance = new HeapSelect<Integer>(new Integer[10]);
+        instance = new HeapSelect<>(new Integer[10]);
         for (int i = 0; i < 1000; i++) {
             instance.add(1000-i);
             if (i >= 9) {
@@ -82,7 +82,7 @@ public void testSelect() {
     @Test
     public void testSelectBig() {
         System.out.println("HeapSelect Big");
-        HeapSelect<Double> instance = new HeapSelect<Double>(new Double[10]);
+        HeapSelect<Double> instance = new HeapSelect<>(new Double[10]);
         for (int i = 0; i < 100000000; i++) {
             instance.add(Math.random());
         }

File: nlp/src/main/java/smile/nlp/SimpleText.java
Patch:
@@ -37,7 +37,7 @@ public class SimpleText extends Text implements TextTerms, AnchorText {
     /**
      * The term frequency.
      */
-    private HashMap<String, Integer> freq = new HashMap<String, Integer>();
+    private HashMap<String, Integer> freq = new HashMap<>();
     /**
      * The maximum term frequency over all terms in the documents;
      */

File: nlp/src/main/java/smile/nlp/Trie.java
Patch:
@@ -55,7 +55,7 @@ public class Node {
         public Node(K key) {
             this.key = key;
             this.value = null;
-            this.children = new LinkedList<Node>();
+            this.children = new LinkedList<>();
         }
         
         public K getKey() {
@@ -117,15 +117,15 @@ public void addChild(K[] key, V value, int index) {
      * Constructor.
      */
     public Trie() {
-        root = new HashMap<K, Node>();
+        root = new HashMap<>();
     }
 
     /**
      * Constructor.
      * @param initialCapacity the initial capacity of root node.
      */
     public Trie(int initialCapacity) {
-        root = new HashMap<K, Node>(initialCapacity);
+        root = new HashMap<>(initialCapacity);
     }
 
     /**

File: nlp/src/main/java/smile/nlp/collocation/BigramCollocationFinder.java
Patch:
@@ -65,7 +65,7 @@ public BigramCollocationFinder(int minFreq) {
      */
     public BigramCollocation[] find(Corpus corpus, int k) {
         BigramCollocation[] bigrams = new BigramCollocation[k];
-        HeapSelect<BigramCollocation> heap = new HeapSelect<BigramCollocation>(bigrams);
+        HeapSelect<BigramCollocation> heap = new HeapSelect<>(bigrams);
         
         Iterator<Bigram> iterator = corpus.getBigrams();
         while (iterator.hasNext()) {
@@ -106,7 +106,7 @@ public BigramCollocation[] find(Corpus corpus, double p) {
 
         double cutoff = chisq.quantile(p);
         
-        ArrayList<BigramCollocation> bigrams = new ArrayList<BigramCollocation>();
+        ArrayList<BigramCollocation> bigrams = new ArrayList<>();
 
         Iterator<Bigram> iterator = corpus.getBigrams();
         while (iterator.hasNext()) {

File: nlp/src/main/java/smile/nlp/dictionary/EnglishDictionary.java
Patch:
@@ -43,7 +43,7 @@ public enum EnglishDictionary implements Dictionary {
      * text, in which each line is a word.
      */
     private EnglishDictionary(String resource) {
-        dict = new HashSet<String>();
+        dict = new HashSet<>();
 
         try (BufferedReader input = new BufferedReader(new InputStreamReader(this.getClass().getResourceAsStream(resource)))) {
         

File: nlp/src/main/java/smile/nlp/dictionary/EnglishPunctuations.java
Patch:
@@ -32,7 +32,7 @@ public class EnglishPunctuations implements Punctuations {
     /**
      * A set of punctuation marks.
      */
-    private HashSet<String> dict = new HashSet<String>(50);
+    private HashSet<String> dict = new HashSet<>(50);
 
     /**
      * Constructor.

File: nlp/src/main/java/smile/nlp/dictionary/EnglishStopWords.java
Patch:
@@ -54,7 +54,7 @@ public enum EnglishStopWords implements StopWords {
      * Constructor.
      */
     private EnglishStopWords(String resource) {
-        dict = new HashSet<String>();
+        dict = new HashSet<>();
 
         try (BufferedReader input = new BufferedReader(new InputStreamReader(this.getClass().getResourceAsStream(resource)))) {
             String line = null;

File: nlp/src/main/java/smile/nlp/dictionary/SimpleDictionary.java
Patch:
@@ -41,7 +41,7 @@ public class SimpleDictionary implements Dictionary {
      * text, in which each line is a word.
      */
     public SimpleDictionary(String resource) {
-        dict = new HashSet<String>();
+        dict = new HashSet<>();
 
         File file = new File(resource);
         try (BufferedReader input = file.exists() ?

File: nlp/src/main/java/smile/nlp/pos/EnglishPOSLexicon.java
Patch:
@@ -34,7 +34,7 @@ public class EnglishPOSLexicon {
     /**
      * A list of English words with POS tags.
      */
-    private static final HashMap<String, PennTreebankPOS[]> dict = new HashMap<String, PennTreebankPOS[]>();
+    private static final HashMap<String, PennTreebankPOS[]> dict = new HashMap<>();
 
     /**
      * The part-of-speech.txt file contains is a combination of

File: nlp/src/main/java/smile/nlp/pos/PennTreebankPOS.java
Patch:
@@ -413,7 +413,7 @@ public String toString() {
      */
     private static final Map<String, String> map;
     static {
-        map = new HashMap<String, String>();
+        map = new HashMap<>();
 
         map.put(".", "SENT");
         map.put("?", "SENT");

File: nlp/src/main/java/smile/nlp/stemmer/LancasterStemmer.java
Patch:
@@ -56,7 +56,7 @@ public class LancasterStemmer implements Stemmer {
         /**
          * Load rules from Lancaster_rules.txt
          */
-        RULES = new ArrayList<String>();
+        RULES = new ArrayList<>();
 
         try (BufferedReader input = new BufferedReader(new InputStreamReader(LancasterStemmer.class.getResourceAsStream("/smile/nlp/stemmer/Lancaster_rules.txt")))) {
             String line = null;

File: nlp/src/main/java/smile/nlp/tokenizer/BreakIteratorSentenceSplitter.java
Patch:
@@ -49,7 +49,7 @@ public BreakIteratorSentenceSplitter(Locale locale) {
     @Override
     public String[] split(String text) {
         boundary.setText(text);
-        ArrayList<String> sentences = new ArrayList<String>();
+        ArrayList<String> sentences = new ArrayList<>();
         int start = boundary.first();
         for (int end = boundary.next();
                 end != BreakIterator.DONE;

File: nlp/src/main/java/smile/nlp/tokenizer/BreakIteratorTokenizer.java
Patch:
@@ -49,7 +49,7 @@ public BreakIteratorTokenizer(Locale locale) {
     @Override
     public String[] split(String text) {
         boundary.setText(text);
-        ArrayList<String> words = new ArrayList<String>();
+        ArrayList<String> words = new ArrayList<>();
         int start = boundary.first();
         int end = boundary.next();
 

File: nlp/src/main/java/smile/nlp/tokenizer/EnglishAbbreviations.java
Patch:
@@ -36,7 +36,7 @@ class EnglishAbbreviations {
     private static final HashSet<String> DICTIONARY;
 
     static {
-        DICTIONARY = new HashSet<String>();
+        DICTIONARY = new HashSet<>();
 
         try (BufferedReader input = new BufferedReader(new InputStreamReader(EnglishAbbreviations.class.getResourceAsStream("/smile/nlp/tokenizer/abbreviations_en.txt")))) {
             String line = null;

File: nlp/src/main/java/smile/nlp/tokenizer/SimpleSentenceSplitter.java
Patch:
@@ -93,7 +93,7 @@ public static SimpleSentenceSplitter getInstance() {
 
     @Override
     public String[] split(String text) {
-        ArrayList<String> sentences = new ArrayList<String>();
+        ArrayList<String> sentences = new ArrayList<>();
 
         // The number of words in the sentence.
         int len = 0;

File: nlp/src/main/java/smile/nlp/tokenizer/SimpleTokenizer.java
Patch:
@@ -144,7 +144,7 @@ public String[] split(String text) {
             }
         }
         
-        ArrayList<String> result = new ArrayList<String>();
+        ArrayList<String> result = new ArrayList<>();
         for (String token : words) {
             if (!token.isEmpty()) {
                 result.add(token);

File: nlp/src/test/java/smile/nlp/collocation/AprioriPhraseExtractorTest.java
Patch:
@@ -71,7 +71,7 @@ public void testExtract() throws FileNotFoundException {
         
         PorterStemmer stemmer = new PorterStemmer();
         SimpleTokenizer tokenizer = new SimpleTokenizer();
-        ArrayList<String[]> sentences = new ArrayList<String[]>();
+        ArrayList<String[]> sentences = new ArrayList<>();
         for (String paragraph : SimpleParagraphSplitter.getInstance().split(text)) {
             for (String s : SimpleSentenceSplitter.getInstance().split(paragraph)) {
                 String[] sentence = tokenizer.split(s);

File: plot/src/main/java/smile/plot/Axis.java
Patch:
@@ -321,7 +321,7 @@ public Axis setRotation(double rotation) {
      */
     public Axis addLabel(String label, double location) {
         if (labels == null) {
-            labels = new HashMap<String, Double>();
+            labels = new HashMap<>();
         }
 
         labels.put(label, location);
@@ -340,7 +340,7 @@ public Axis addLabel(String[] label, double[] location) {
         }
 
         if (labels == null) {
-            labels = new HashMap<String, Double>();
+            labels = new HashMap<>();
         }
 
         for (int i = 0; i < label.length; i++) {

File: plot/src/main/java/smile/plot/PlotCanvas.java
Patch:
@@ -135,7 +135,7 @@ public class PlotCanvas extends JPanel {
     /**
      * The shapes in the canvas, e.g. label, plots, etc.
      */
-    private List<Shape> shapes = new ArrayList<Shape>();
+    private List<Shape> shapes = new ArrayList<>();
     /**
      * The real canvas for plots.
      */

File: plot/src/main/java/smile/plot/ScatterPlot.java
Patch:
@@ -161,7 +161,7 @@ public ScatterPlot(double[][] data, int[] y, char[] legends, Color[] palette) {
         int[] id = Math.unique(y);
         Arrays.sort(id);
 
-        classLookupTable = new HashMap<Integer, Integer>(id.length);
+        classLookupTable = new HashMap<>(id.length);
 
         for (int i = 0; i < id.length; i++) {
             classLookupTable.put(id[i], i);

File: plot/src/main/java/smile/swing/FileChooser.java
Patch:
@@ -283,7 +283,7 @@ public static class SimpleFileFilter extends FileFilter {
         /**
          * The file extensions in lower case.
          */
-        private TreeSet<String> filters = new TreeSet<String>();
+        private TreeSet<String> filters = new TreeSet<>();
         /**
          * The human readable description of this filter.
          */

File: plot/src/main/java/smile/swing/FontChooser.java
Patch:
@@ -227,7 +227,7 @@ private JTextField getFontSizeTextField() {
 
     private JList<String> getFontFamilyList() {
         if (fontNameList == null) {
-            fontNameList = new JList<String>(getFontFamilies());
+            fontNameList = new JList<>(getFontFamilies());
             fontNameList.setSelectionMode(ListSelectionModel.SINGLE_SELECTION);
             fontNameList.addListSelectionListener(
                     new ListSelectionHandler(getFontFamilyTextField()));
@@ -240,7 +240,7 @@ private JList<String> getFontFamilyList() {
 
     private JList<String> getFontStyleList() {
         if (fontStyleList == null) {
-            fontStyleList = new JList<String>(getFontStyleNames());
+            fontStyleList = new JList<>(getFontStyleNames());
             fontStyleList.setSelectionMode(ListSelectionModel.SINGLE_SELECTION);
             fontStyleList.addListSelectionListener(
                     new ListSelectionHandler(getFontStyleTextField()));
@@ -253,7 +253,7 @@ private JList<String> getFontStyleList() {
 
     private JList<String> getFontSizeList() {
         if (fontSizeList == null) {
-            fontSizeList = new JList<String>(this.fontSizeStrings);
+            fontSizeList = new JList<>(this.fontSizeStrings);
             fontSizeList.setSelectionMode(ListSelectionModel.SINGLE_SELECTION);
             fontSizeList.addListSelectionListener(
                     new ListSelectionHandler(getFontSizeTextField()));

File: plot/src/main/java/smile/swing/table/IntegerArrayCellEditor.java
Patch:
@@ -75,7 +75,7 @@ public Object stringToValue(String string) throws ParseException {
 
                 int[] data = new int[items.length];
                 for (int i = 0; i < data.length; i++) {
-                    data[i] = Integer.valueOf(items[i].trim());
+                    data[i] = Integer.parseInt(items[i].trim());
                 }
 
                 return data;

File: core/src/main/java/smile/association/ARM.java
Patch:
@@ -125,7 +125,7 @@ public long learn(double confidence, PrintStream out) {
      * @param confidence the confidence threshold for association rules.
      */
     public List<AssociationRule> learn(double confidence) {
-        List<AssociationRule> list = new ArrayList<AssociationRule>();
+        List<AssociationRule> list = new ArrayList<>();
         ttree = fim.buildTotalSupportTree();
         for (int i = 0; i < ttree.root.children.length; i++) {
             if (ttree.root.children[i] != null) {

File: core/src/main/java/smile/association/FPTree.java
Patch:
@@ -89,7 +89,7 @@ class Node {
          */
         void add(int index, int end, int[] itemset, int support) {
             if (children == null) {
-                children = new HashMap<Integer, Node>();
+                children = new HashMap<>();
             }
             
             Node child = children.get(itemset[index]);
@@ -114,7 +114,7 @@ void add(int index, int end, int[] itemset, int support) {
          */
         void append(int index, int end, int[] itemset, int support) {
             if (children == null) {
-                children = new HashMap<Integer, Node>();
+                children = new HashMap<>();
             }
             
             if (index >= maxItemSetSize) {

File: core/src/main/java/smile/association/TotalSupportTree.java
Patch:
@@ -168,7 +168,7 @@ private int getSupport(int[] itemset, int index, Node node) {
      * @return the list of frequent item sets
      */
     public List<ItemSet> getFrequentItemsets() {
-        List<ItemSet> list = new ArrayList<ItemSet>();
+        List<ItemSet> list = new ArrayList<>();
         getFrequentItemsets(null, list);
         return list;
     }

File: core/src/main/java/smile/classification/DecisionTree.java
Patch:
@@ -511,7 +511,7 @@ public boolean findBestSplit() {
                 }
             } else {
 
-                List<SplitTask> tasks = new ArrayList<SplitTask>(mtry);
+                List<SplitTask> tasks = new ArrayList<>(mtry);
                 for (int j = 0; j < mtry; j++) {
                     tasks.add(new SplitTask(n, count, impurity, variables[j]));
                 }
@@ -944,7 +944,7 @@ public DecisionTree(Attribute[] attributes, double[][] x, int[] y, int maxNodes,
         }
 
         // Priority queue for best-first tree growing.
-        PriorityQueue<TrainNode> nextSplits = new PriorityQueue<TrainNode>();
+        PriorityQueue<TrainNode> nextSplits = new PriorityQueue<>();
 
         int n = y.length;
         int[] count = new int[k];

File: core/src/main/java/smile/classification/RBFNetwork.java
Patch:
@@ -186,9 +186,9 @@ public RBFNetwork<T> train(T[] x, int[] y) {
             GaussianRadialBasis gaussian = SmileUtils.learnGaussianRadialBasis(x, centers, distance);
             
             if (rbf == null) {
-                return new RBFNetwork<T>(x, y, distance, gaussian, centers, normalized);
+                return new RBFNetwork<>(x, y, distance, gaussian, centers, normalized);
             } else {
-                return new RBFNetwork<T>(x, y, distance, rbf, centers, normalized);
+                return new RBFNetwork<>(x, y, distance, rbf, centers, normalized);
             }
         }
         
@@ -201,7 +201,7 @@ public RBFNetwork<T> train(T[] x, int[] y) {
          * @return a trained RBF network
          */
         public RBFNetwork<T> train(T[] x, int[] y, T[] centers) {
-            return new RBFNetwork<T>(x, y, distance, rbf, centers, normalized);
+            return new RBFNetwork<>(x, y, distance, rbf, centers, normalized);
         }
     }
     

File: core/src/main/java/smile/clustering/BIRCH.java
Patch:
@@ -387,9 +387,9 @@ public int partition(int k) {
      * @return the number of non-outlier leaves.
      */
     public int partition(int k, int minPts) {
-        ArrayList<Leaf> leaves = new ArrayList<Leaf>();
-        ArrayList<double[]> centers = new ArrayList<double[]>();
-        Queue<Node> queue = new LinkedList<Node>();
+        ArrayList<Leaf> leaves = new ArrayList<>();
+        ArrayList<double[]> centers = new ArrayList<>();
+        Queue<Node> queue = new LinkedList<>();
         queue.offer(root);
 
         for (Node node = queue.poll(); node != null; node = queue.poll()) {

File: core/src/main/java/smile/clustering/CLARANS.java
Patch:
@@ -143,7 +143,7 @@ public CLARANS(T[] data, Distance<T> distance, int k, int maxNeighbor, int numLo
         this.numLocal = numLocal;
         this.maxNeighbor = maxNeighbor;
         
-        List<CLARANSTask> tasks = new ArrayList<CLARANSTask>();
+        List<CLARANSTask> tasks = new ArrayList<>();
         for (int i = 0; i < numLocal; i++) {
             tasks.add(new CLARANSTask(data));
         }

File: core/src/main/java/smile/clustering/DeterministicAnnealing.java
Patch:
@@ -92,7 +92,7 @@ public DeterministicAnnealing(double[][] data, int Kmax, double alpha) {
 
         int np = MulticoreExecutor.getThreadPoolSize();
         if (n >= 1000 && np >= 2) {
-            tasks = new ArrayList<UpdateThread>(np + 1);
+            tasks = new ArrayList<>(np + 1);
             int step = n / np;
             if (step < 100) {
                 step = 100;
@@ -107,7 +107,7 @@ public DeterministicAnnealing(double[][] data, int Kmax, double alpha) {
             }
             tasks.add(new UpdateThread(data, centroids, posteriori, priori, start, n));
             
-            ctasks = new ArrayList<CentroidThread>(2 * Kmax);
+            ctasks = new ArrayList<>(2 * Kmax);
             for (int i = 0; i < 2*Kmax; i++) {
                 ctasks.add(new CentroidThread(data, centroids, posteriori, priori, i));
             }

File: core/src/main/java/smile/clustering/GMeans.java
Patch:
@@ -81,7 +81,7 @@ public GMeans(double[][] data, int kmax) {
 
         BBDTree bbd = new BBDTree(data);
         while (k < kmax) {
-            ArrayList<double[]> centers = new ArrayList<double[]>();
+            ArrayList<double[]> centers = new ArrayList<>();
             double[] score = new double[k];
             KMeans[] kmeans = new KMeans[k];
             

File: core/src/main/java/smile/clustering/HierarchicalClustering.java
Patch:
@@ -185,7 +185,7 @@ public int[] partition(double h) {
      */
     private void bfs(int[] membership, int cluster, int id) {
         int n = merge.length + 1;
-        Queue<Integer> queue = new LinkedList<Integer>();
+        Queue<Integer> queue = new LinkedList<>();
         queue.offer(cluster);
 
         for (Integer i = queue.poll(); i != null; i = queue.poll()) {

File: core/src/main/java/smile/clustering/KMeans.java
Patch:
@@ -226,7 +226,7 @@ public KMeans(double[][] data, int k, int maxIter, int runs) {
 
         BBDTree bbd = new BBDTree(data);
 
-        List<KMeansThread> tasks = new ArrayList<KMeansThread>();
+        List<KMeansThread> tasks = new ArrayList<>();
         for (int i = 0; i < runs; i++) {
             tasks.add(new KMeansThread(bbd, data, k, maxIter));
         }
@@ -321,7 +321,7 @@ public static KMeans lloyd(double[][] data, int k, int maxIter) {
         int np = MulticoreExecutor.getThreadPoolSize();
         List<LloydThread> tasks = null;
         if (n >= 1000 && np >= 2) {
-            tasks = new ArrayList<LloydThread>(np + 1);
+            tasks = new ArrayList<>(np + 1);
             int step = n / np;
             if (step < 100) {
                 step = 100;

File: core/src/main/java/smile/clustering/SIB.java
Patch:
@@ -186,7 +186,7 @@ public SIB(double[][] data, int k, int maxIter, int runs) {
             throw new IllegalArgumentException("Invalid number of runs: " + runs);
         }
 
-        List<SIBThread> tasks = new ArrayList<SIBThread>();
+        List<SIBThread> tasks = new ArrayList<>();
         for (int i = 0; i < runs; i++) {
             tasks.add(new SIBThread(data, k, maxIter));
         }
@@ -395,7 +395,7 @@ public SIB(SparseDataset data, int k, int maxIter, int runs) {
             throw new IllegalArgumentException("Invalid number of runs: " + runs);
         }
 
-        List<SIBThread> tasks = new ArrayList<SIBThread>();
+        List<SIBThread> tasks = new ArrayList<>();
         for (int i = 0; i < runs; i++) {
             tasks.add(new SIBThread(data, k, maxIter));
         }

File: core/src/main/java/smile/clustering/XMeans.java
Patch:
@@ -85,7 +85,7 @@ public XMeans(double[][] data, int kmax) {
 
         BBDTree bbd = new BBDTree(data);
         while (k < kmax) {
-            ArrayList<double[]> centers = new ArrayList<double[]>();
+            ArrayList<double[]> centers = new ArrayList<>();
             double[] score = new double[k];
             KMeans[] kmeans = new KMeans[k];
             

File: core/src/main/java/smile/feature/Bag.java
Patch:
@@ -58,7 +58,7 @@ public Bag(T[] features) {
      */
     public Bag(T[] features, boolean binary) {
         this.binary = binary;
-        this.features = new HashMap<T, Integer>();
+        this.features = new HashMap<>();
         for (int i = 0, k = 0; i < features.length; i++) {
             if (!this.features.containsKey(features[i])) {
                 this.features.put(features[i], k++);

File: core/src/main/java/smile/feature/FeatureSet.java
Patch:
@@ -35,11 +35,11 @@ public class FeatureSet <T> {
     /**
      * Feature generators.
      */
-    List<Feature<T>> features = new ArrayList<Feature<T>>();
+    List<Feature<T>> features = new ArrayList<>();
     /**
      * The variable attributes of generated features.
      */
-    List<Attribute> attributes = new ArrayList<Attribute>();
+    List<Attribute> attributes = new ArrayList<>();
     
     /**
      * Constructor.
@@ -123,7 +123,7 @@ public AttributeDataset f(Dataset<T> data) {
 
         for (int i = 0; i < data.size(); i++) {
             Datum<T> datum = data.get(i);
-            Datum<double[]> x = new Datum<double[]>(f(datum.x), datum.y, datum.weight);
+            Datum<double[]> x = new Datum<>(f(datum.x), datum.y, datum.weight);
             x.name = datum.name;
             x.description = datum.description;
             x.timestamp = datum.timestamp;

File: core/src/main/java/smile/gap/GeneticAlgorithm.java
Patch:
@@ -219,7 +219,7 @@ public enum Selection {
     /**
      * Parallel tasks to evaluate the fitness of population.
      */
-    private List<Task> tasks = new ArrayList<Task>();
+    private List<Task> tasks = new ArrayList<>();
     
     /**
      * Constructor. The default selection strategy is tournament selection

File: core/src/main/java/smile/manifold/IsoMap.java
Patch:
@@ -109,9 +109,9 @@ public IsoMap(double[][] data, int d, int k, boolean CIsomap) {
 
         KNNSearch<double[], double[]> knn = null;
         if (data[0].length < 10) {
-            knn = new KDTree<double[]>(data, data);
+            knn = new KDTree<>(data, data);
         } else {
-            knn = new CoverTree<double[]>(data, new EuclideanDistance());
+            knn = new CoverTree<>(data, new EuclideanDistance());
         }
 
         graph = new AdjacencyList(n);

File: core/src/main/java/smile/manifold/LLE.java
Patch:
@@ -88,9 +88,9 @@ public LLE(double[][] data, int d, int k) {
 
         KNNSearch<double[], double[]> knn = null;
         if (D < 10) {
-            knn = new KDTree<double[]>(data, data);
+            knn = new KDTree<>(data, data);
         } else {
-            knn = new CoverTree<double[]>(data, new EuclideanDistance());
+            knn = new CoverTree<>(data, new EuclideanDistance());
         }
 
         Comparator<Neighbor<double[], double[]>> comparator = new Comparator<Neighbor<double[], double[]>>() {

File: core/src/main/java/smile/manifold/LaplacianEigenmap.java
Patch:
@@ -98,9 +98,9 @@ public LaplacianEigenmap(double[][] data, int d, int k, double t) {
         int n = data.length;
         KNNSearch<double[], double[]> knn = null;
         if (data[0].length < 10) {
-            knn = new KDTree<double[]>(data, data);
+            knn = new KDTree<>(data, data);
         } else {
-            knn = new CoverTree<double[]>(data, new EuclideanDistance());
+            knn = new CoverTree<>(data, new EuclideanDistance());
         }
 
         graph = new AdjacencyList(n);

File: core/src/main/java/smile/neighbor/BKTree.java
Patch:
@@ -93,7 +93,7 @@ private void add(E datum) {
             }
 
             if (children == null) {
-                children = new ArrayList<Node>();
+                children = new ArrayList<>();
             }
 
             while (children.size() <= d) {
@@ -203,7 +203,7 @@ private void search(Node node, E q, int k, List<Neighbor<E, E>> neighbors) {
 
         if (d <= k) {
             if (node.object != q || !identicalExcluded) {
-                neighbors.add(new Neighbor<E, E>(node.object, node.object, node.index, d));
+                neighbors.add(new Neighbor<>(node.object, node.object, node.index, d));
             }
         }
 

File: core/src/main/java/smile/regression/GaussianProcessRegression.java
Patch:
@@ -111,7 +111,7 @@ public Trainer(MercerKernel<T> kernel, double lambda) {
         
         @Override
         public GaussianProcessRegression<T> train(T[] x, double[] y) {
-            return new GaussianProcessRegression<T>(x, y, kernel, lambda);
+            return new GaussianProcessRegression<>(x, y, kernel, lambda);
         }
         
         /**
@@ -125,7 +125,7 @@ public GaussianProcessRegression<T> train(T[] x, double[] y) {
          * @return a trained Gaussian Process.
          */
         public GaussianProcessRegression<T> train(T[] x, double[] y, T[] t) {
-            return new GaussianProcessRegression<T>(x, y, t, kernel, lambda);
+            return new GaussianProcessRegression<>(x, y, t, kernel, lambda);
         }
     }
     

File: core/src/main/java/smile/regression/RBFNetwork.java
Patch:
@@ -173,9 +173,9 @@ public RBFNetwork<T> train(T[] x, double[] y) {
             T[] centers = (T[]) java.lang.reflect.Array.newInstance(x.getClass().getComponentType(), m);
             GaussianRadialBasis gaussian = SmileUtils.learnGaussianRadialBasis(x, centers, distance);
             if (rbf == null) {
-                return new RBFNetwork<T>(x, y, distance, gaussian, centers, normalized);
+                return new RBFNetwork<>(x, y, distance, gaussian, centers, normalized);
             } else {
-                return new RBFNetwork<T>(x, y, distance, rbf, centers, normalized);
+                return new RBFNetwork<>(x, y, distance, rbf, centers, normalized);
             }
         }
         
@@ -188,7 +188,7 @@ public RBFNetwork<T> train(T[] x, double[] y) {
          * @return a trained RBF network
          */
         public RBFNetwork<T> train(T[] x, double[] y, T[] centers) {
-            return new RBFNetwork<T>(x, y, distance, rbf, centers, normalized);
+            return new RBFNetwork<>(x, y, distance, rbf, centers, normalized);
         }
     }
     

File: core/src/main/java/smile/regression/RandomForest.java
Patch:
@@ -458,7 +458,7 @@ public RandomForest(Attribute[] attributes, double[][] x, double[] y, int ntrees
         int[] oob = new int[n];
         
         int[][] order = SmileUtils.sort(attributes, x);
-        List<TrainingTask> tasks = new ArrayList<TrainingTask>();
+        List<TrainingTask> tasks = new ArrayList<>();
         for (int i = 0; i < ntrees; i++) {
             tasks.add(new TrainingTask(attributes, x, y, maxNodes, nodeSize, mtry, subsample, order, prediction, oob));
         }
@@ -468,7 +468,7 @@ public RandomForest(Attribute[] attributes, double[][] x, double[] y, int ntrees
         } catch (Exception ex) {
             ex.printStackTrace();
 
-            trees = new ArrayList<RegressionTree>(ntrees);
+            trees = new ArrayList<>(ntrees);
             for (int i = 0; i < ntrees; i++) {
                 trees.add(tasks.get(i).call());
             }
@@ -547,7 +547,7 @@ public void trim(int ntrees) {
             throw new IllegalArgumentException("Invalid new model size: " + ntrees);
         }
         
-        List<RegressionTree> model = new ArrayList<RegressionTree>(ntrees);
+        List<RegressionTree> model = new ArrayList<>(ntrees);
         for (int i = 0; i < ntrees; i++) {
             model.add(trees.get(i));
         }

File: core/src/main/java/smile/regression/RegressionTree.java
Patch:
@@ -422,7 +422,7 @@ public boolean findBestSplit() {
                 }
             } else {
 
-                List<SplitTask> tasks = new ArrayList<SplitTask>(mtry);
+                List<SplitTask> tasks = new ArrayList<>(mtry);
                 for (int j = 0; j < mtry; j++) {
                     tasks.add(new SplitTask(n, sum, variables[j]));
                 }
@@ -965,7 +965,7 @@ public RegressionTree(Attribute[] attributes, double[][] x, double[] y, int maxN
         }
 
         // Priority queue for best-first tree growing.
-        PriorityQueue<TrainNode> nextSplits = new PriorityQueue<TrainNode>();
+        PriorityQueue<TrainNode> nextSplits = new PriorityQueue<>();
 
         int n = 0;
         double sum = 0.0;
@@ -1063,7 +1063,7 @@ public RegressionTree(int numFeatures, int[][] x, double[] y, int maxNodes, int
         importance = new double[numFeatures];
         
         // Priority queue for best-first tree growing.
-        PriorityQueue<SparseBinaryTrainNode> nextSplits = new PriorityQueue<SparseBinaryTrainNode>();
+        PriorityQueue<SparseBinaryTrainNode> nextSplits = new PriorityQueue<>();
 
         int n = 0;
         double sum = 0.0;

File: core/src/main/java/smile/regression/SVR.java
Patch:
@@ -74,7 +74,7 @@ public class SVR<T> implements Regression<T> {
     /**
      * Support vectors.
      */
-    private List<SupportVector> sv = new ArrayList<SupportVector>();
+    private List<SupportVector> sv = new ArrayList<>();
     /**
      * Threshold of decision function.
      */
@@ -189,7 +189,7 @@ public Trainer setTolerance(double tol) {
 
         @Override
         public SVR<T> train(T[] x, double[] y) {
-            SVR<T> svr = new SVR<T>(x, y, kernel, eps, C, tol);
+            SVR<T> svr = new SVR<>(x, y, kernel, eps, C, tol);
             return svr;
         }
     }
@@ -415,7 +415,7 @@ private void gram(SupportVector i) {
                 i.kcache.add(kernel.k(i.x, v.x));
             }
         } else {
-            List<KernelTask> tasks = new ArrayList<KernelTask>(m + 1);
+            List<KernelTask> tasks = new ArrayList<>(m + 1);
             int step = n / m;
             if (step < 100) {
                 step = 100;

File: core/src/main/java/smile/sampling/Bagging.java
Patch:
@@ -52,7 +52,7 @@ public Bagging(int k, int[] y, double[] classWeight, double subsample) {
             // Training samples draw with replacement.
             for (int l = 0; l < k; l++) {
                 int nj = 0;
-                ArrayList<Integer> cj = new ArrayList<Integer>();
+                ArrayList<Integer> cj = new ArrayList<>();
                 for (int i = 0; i < n; i++) {
                     if (y[i] == l) {
                         cj.add(i);

File: core/src/main/java/smile/sequence/HMM.java
Patch:
@@ -123,7 +123,7 @@ public HMM(double[] pi, double[][] a, double[][] b, O[] symbols) {
                 throw new IllegalArgumentException("Invalid size of emission symbol list.");
             }
 
-            this.symbols = new HashMap<O, Integer>();
+            this.symbols = new HashMap<>();
             for (int i = 0; i < symbols.length; i++) {
                 this.symbols.put(symbols[i], i);
             }
@@ -481,7 +481,7 @@ public HMM(O[][] observations, int[][] labels) {
         }
 
         int index = 0;
-        symbols = new HashMap<O, Integer>();
+        symbols = new HashMap<>();
         for (int i = 0; i < observations.length; i++) {
             if (observations[i].length != labels[i].length) {
                 throw new IllegalArgumentException(String.format("The length of observation sequence %d and that of corresponding label sequence are different.", i));
@@ -573,7 +573,7 @@ public HMM<O> learn(int[][] observations, int iterations) {
      * @return an updated HMM.
      */
     private HMM<O> iterate(int[][] sequences) {
-        HMM<O> hmm = new HMM<O>(numStates, numSymbols);
+        HMM<O> hmm = new HMM<>(numStates, numSymbols);
         hmm.symbols = symbols;
 
         // gamma[n] = gamma array associated to observation sequence n

File: core/src/main/java/smile/taxonomy/Taxonomy.java
Patch:
@@ -32,7 +32,7 @@ public class Taxonomy {
     /**
      * All the concepts in this taxonomy.
      */
-    HashMap<String, Concept> concepts = new HashMap<String, Concept>();
+    HashMap<String, Concept> concepts = new HashMap<>();
     /**
      * The root node in the taxonomy.
      */
@@ -84,7 +84,7 @@ public List<String> getConcepts() {
      * Returns all named concepts from this taxonomy
      */
     private List<String> getConcepts(Concept c) {
-        List<String> keywords = new ArrayList<String>();
+        List<String> keywords = new ArrayList<>();
 
         while (c != null) {
             if (c.synset != null) {

File: core/src/main/java/smile/util/MulticoreExecutor.java
Patch:
@@ -90,7 +90,7 @@ public static int getThreadPoolSize() {
     public static <T> List<T> run(Collection<? extends Callable<T>> tasks) throws Exception {
         createThreadPool();
 
-        List<T> results = new ArrayList<T>();
+        List<T> results = new ArrayList<>();
         if (threads == null) {
             for (Callable<T> task : tasks) {
                 results.add(task.call());

File: core/src/main/java/smile/util/SmileUtils.java
Patch:
@@ -198,7 +198,7 @@ public static GaussianRadialBasis[] learnGaussianRadialBasis(double[][] x, doubl
      */
     public static <T> GaussianRadialBasis learnGaussianRadialBasis(T[] x, T[] centers, Metric<T> distance) {
         int k = centers.length;
-        CLARANS<T> clarans = new CLARANS<T>(x, distance, k, Math.min(100, (int) Math.round(0.01 * k * (x.length-k))));
+        CLARANS<T> clarans = new CLARANS<>(x, distance, k, Math.min(100, (int) Math.round(0.01 * k * (x.length - k))));
         System.arraycopy(clarans.medoids(), 0, centers, 0, k);
 
         double r0 = 0.0;
@@ -234,7 +234,7 @@ public static <T> GaussianRadialBasis[] learnGaussianRadialBasis(T[] x, T[] cent
         }
         
         int k = centers.length;
-        CLARANS<T> clarans = new CLARANS<T>(x, distance, k, Math.min(100, (int) Math.round(0.01 * k * (x.length-k))));
+        CLARANS<T> clarans = new CLARANS<>(x, distance, k, Math.min(100, (int) Math.round(0.01 * k * (x.length - k))));
         System.arraycopy(clarans.medoids(), 0, centers, 0, k);
 
         p = Math.min(p, k-1);
@@ -274,7 +274,7 @@ public static <T> GaussianRadialBasis[] learnGaussianRadialBasis(T[] x, T[] cent
         }
         
         int k = centers.length;
-        CLARANS<T> clarans = new CLARANS<T>(x, distance, k, Math.min(100, (int) Math.round(0.01 * k * (x.length-k))));
+        CLARANS<T> clarans = new CLARANS<>(x, distance, k, Math.min(100, (int) Math.round(0.01 * k * (x.length - k))));
         System.arraycopy(clarans.medoids(), 0, centers, 0, k);
 
         int n = x.length;

File: core/src/main/java/smile/validation/ConfusionMatrix.java
Patch:
@@ -31,7 +31,7 @@ public ConfusionMatrix(int[] truth, int[] prediction) {
 			 throw new IllegalArgumentException(String.format("The vector sizes don't match: %d != %d.", truth.length, prediction.length));
 		}
 		
-		Set<Integer> ySet = new HashSet<Integer>();
+		Set<Integer> ySet = new HashSet<>();
 		
 		for(int i = 0; i < truth.length; i++){
 			ySet.add(truth[i]);

File: core/src/test/java/smile/association/ARMTest.java
Patch:
@@ -114,7 +114,7 @@ public void testLearn() {
     public void testLearnPima() {
         System.out.println("pima");
 
-        List<int[]> dataList = new ArrayList<int[]>(1000);
+        List<int[]> dataList = new ArrayList<>(1000);
 
         try {
             BufferedReader input = smile.data.parser.IOUtils.getTestDataReader("transaction/pima.D38.N768.C2");
@@ -157,7 +157,7 @@ public void testLearnPima() {
     public void testLearnKosarak() {
         System.out.println("kosarak");
 
-        List<int[]> dataList = new ArrayList<int[]>(1000);
+        List<int[]> dataList = new ArrayList<>(1000);
 
         try {
             BufferedReader input = smile.data.parser.IOUtils.getTestDataReader("transaction/kosarak.dat");
@@ -170,7 +170,7 @@ public void testLearnKosarak() {
 
                 String[] s = line.split(" ");
 
-                Set<Integer> items = new HashSet<Integer>();
+                Set<Integer> items = new HashSet<>();
                 for (int i = 0; i < s.length; i++) {
                     items.add(Integer.parseInt(s[i]));
                 }

File: core/src/test/java/smile/association/FPGrowthTest.java
Patch:
@@ -120,7 +120,7 @@ public void testLearn_PrintStream() {
     public void testPima() {
         System.out.println("pima");
 
-        List<int[]> dataList = new ArrayList<int[]>(1000);
+        List<int[]> dataList = new ArrayList<>(1000);
 
         try {
             BufferedReader input = smile.data.parser.IOUtils.getTestDataReader("transaction/pima.D38.N768.C2");
@@ -168,7 +168,7 @@ public void testPima() {
     public void testKosarak() {
         System.out.println("kosarak");
 
-        List<int[]> dataList = new ArrayList<int[]>(1000);
+        List<int[]> dataList = new ArrayList<>(1000);
 
         try {
             BufferedReader input = smile.data.parser.IOUtils.getTestDataReader("transaction/kosarak.dat");
@@ -181,7 +181,7 @@ public void testKosarak() {
 
                 String[] s = line.split(" ");
 
-                Set<Integer> items = new HashSet<Integer>();
+                Set<Integer> items = new HashSet<>();
                 for (int i = 0; i < s.length; i++) {
                     items.add(Integer.parseInt(s[i]));
                 }

File: core/src/test/java/smile/association/TotalSupportTreeTest.java
Patch:
@@ -146,7 +146,7 @@ public void testGetFrequentItemsets_PrintStream() {
     public void testPima() {
         System.out.println("pima");
 
-        List<int[]> dataList = new ArrayList<int[]>(1000);
+        List<int[]> dataList = new ArrayList<>(1000);
 
         try {
             InputStream stream = getClass().getResourceAsStream("/smile/data/transaction/pima.D38.N768.C2");
@@ -199,7 +199,7 @@ public void testPima() {
     public void testKosarak() {
         System.out.println("kosarak");
 
-        List<int[]> dataList = new ArrayList<int[]>(1000);
+        List<int[]> dataList = new ArrayList<>(1000);
 
         try {
             InputStream stream = getClass().getResourceAsStream("/smile/data/transaction/kosarak.dat");
@@ -213,7 +213,7 @@ public void testKosarak() {
 
                 String[] s = line.split(" ");
 
-                Set<Integer> items = new HashSet<Integer>();
+                Set<Integer> items = new HashSet<>();
                 for (int i = 0; i < s.length; i++) {
                     items.add(Integer.parseInt(s[i]));
                 }

File: core/src/test/java/smile/classification/NaiveBayesTest.java
Patch:
@@ -73,7 +73,7 @@ public NaiveBayesTest() {
 
         moviex = new double[x.length][];
         moviey = new int[y.length];
-        Bag<String> bag = new Bag<String>(feature);
+        Bag<String> bag = new Bag<>(feature);
         for (int i = 0; i < x.length; i++) {
             moviex[i] = bag.feature(x[i]);
             moviey[i] = y[i];
@@ -124,7 +124,7 @@ public void testPredict() {
                 for (int i = 0; i < k; i++) {
                     priori[i] = 1.0 / k;
                     for (int j = 0; j < p; j++) {
-                        ArrayList<Double> axi = new ArrayList<Double>();
+                        ArrayList<Double> axi = new ArrayList<>();
                         for (int m = 0; m < trainx.length; m++) {
                             if (trainy[m] == i) {
                                 axi.add(trainx[m][j]);

File: core/src/test/java/smile/classification/RBFNetworkTest.java
Patch:
@@ -83,7 +83,7 @@ public void testLearn() {
 
                 double[][] centers = new double[10][];
                 RadialBasisFunction[] basis = SmileUtils.learnGaussianRadialBasis(trainx, centers, 5.0);
-                RBFNetwork<double[]> rbf = new RBFNetwork<double[]>(trainx, trainy, new EuclideanDistance(), basis, centers);
+                RBFNetwork<double[]> rbf = new RBFNetwork<>(trainx, trainy, new EuclideanDistance(), basis, centers);
 
                 if (y[loocv.test[i]] != rbf.predict(x[loocv.test[i]]))
                     error++;
@@ -115,7 +115,7 @@ public void testSegment() {
             
             double[][] centers = new double[100][];
             RadialBasisFunction[] basis = SmileUtils.learnGaussianRadialBasis(x, centers, 5.0);
-            RBFNetwork<double[]> rbf = new RBFNetwork<double[]>(x, y, new EuclideanDistance(), basis, centers);
+            RBFNetwork<double[]> rbf = new RBFNetwork<>(x, y, new EuclideanDistance(), basis, centers);
             
             int error = 0;
             for (int i = 0; i < testx.length; i++) {
@@ -150,7 +150,7 @@ public void testUSPS() {
             
             double[][] centers = new double[200][];
             RadialBasisFunction basis = SmileUtils.learnGaussianRadialBasis(x, centers);
-            RBFNetwork<double[]> rbf = new RBFNetwork<double[]>(x, y, new EuclideanDistance(), new GaussianRadialBasis(8.0), centers);
+            RBFNetwork<double[]> rbf = new RBFNetwork<>(x, y, new EuclideanDistance(), new GaussianRadialBasis(8.0), centers);
                 
             int error = 0;
             for (int i = 0; i < testx.length; i++) {

File: core/src/test/java/smile/clustering/CLARANSTest.java
Patch:
@@ -72,7 +72,7 @@ public void testUSPS() {
             
             AdjustedRandIndex ari = new AdjustedRandIndex();
             RandIndex rand = new RandIndex();
-            CLARANS<double[]> clarans = new CLARANS<double[]>(x, new EuclideanDistance(), 10, 50, 8);
+            CLARANS<double[]> clarans = new CLARANS<>(x, new EuclideanDistance(), 10, 50, 8);
 
             double r = rand.measure(y, clarans.getClusterLabel());
             double r2 = ari.measure(y, clarans.getClusterLabel());

File: core/src/test/java/smile/clustering/DBScanTest.java
Patch:
@@ -93,7 +93,7 @@ public void testToy() {
             label[i] = 3;
         }
         
-        DBScan<double[]> dbscan = new DBScan<double[]>(data, new KDTree<double[]>(data, data), 200, 0.8);
+        DBScan<double[]> dbscan = new DBScan<>(data, new KDTree<>(data, data), 200, 0.8);
         System.out.println(dbscan);
         
         int[] size = dbscan.getClusterSize();

File: core/src/test/java/smile/clustering/MECTest.java
Patch:
@@ -72,7 +72,7 @@ public void testUSPS() {
             
             AdjustedRandIndex ari = new AdjustedRandIndex();
             RandIndex rand = new RandIndex();
-            MEC<double[]> mec = new MEC<double[]>(x, new EuclideanDistance(), 10, 8.0);
+            MEC<double[]> mec = new MEC<>(x, new EuclideanDistance(), 10, 8.0);
             
             double r = rand.measure(y, mec.getClusterLabel());
             double r2 = ari.measure(y, mec.getClusterLabel());

File: core/src/test/java/smile/feature/BagTest.java
Patch:
@@ -63,11 +63,11 @@ public void testUniquenessOfFeatures() {
         String[] featuresForBuildingStories = {"truck", "concrete", "foundation", "steel", "crane"};
         String testMessage = "This story is about a crane and a sparrow";
 
-        ArrayList<String> mergedFeatureLists = new ArrayList<String>();
+        ArrayList<String> mergedFeatureLists = new ArrayList<>();
         mergedFeatureLists.addAll(Arrays.asList(featuresForBirdStories));
         mergedFeatureLists.addAll(Arrays.asList(featuresForBuildingStories));
 
-        Bag<String> bag = new Bag<String>(mergedFeatureLists.toArray(new String[featuresForBirdStories.length + featuresForBuildingStories.length]));
+        Bag<String> bag = new Bag<>(mergedFeatureLists.toArray(new String[featuresForBirdStories.length + featuresForBuildingStories.length]));
 
         double[] result = bag.feature(testMessage.split(" "));
         assertEquals(9, result.length);
@@ -98,7 +98,7 @@ public void testFeature() {
             "perfectly", "masterpiece", "realistic", "flaws"
         };
         
-        Bag<String> bag = new Bag<String>(feature);
+        Bag<String> bag = new Bag<>(feature);
         
         double[][] x = new double[text.length][];
         for (int i = 0; i < text.length; i++) {

File: core/src/test/java/smile/feature/FeatureSetTest.java
Patch:
@@ -62,7 +62,7 @@ public void testAttributes() {
             AttributeDataset data = parser.parse(smile.data.parser.IOUtils.getTestDataFile("weka/regression/abalone.arff"));
             double[][] x = data.toArray(new double[data.size()][]);
             
-            FeatureSet<double[]> features = new FeatureSet<double[]>();
+            FeatureSet<double[]> features = new FeatureSet<>();
             features.add(new Nominal2Binary(data.attributes()));
             features.add(new NumericAttributeFeature(data.attributes(), 0.05, 0.95, x));
             Attribute[] attributes = features.attributes();
@@ -87,7 +87,7 @@ public void testF() {
             AttributeDataset data = parser.parse(smile.data.parser.IOUtils.getTestDataFile("weka/regression/abalone.arff"));
             double[][] x = data.toArray(new double[data.size()][]);
             
-            FeatureSet<double[]> features = new FeatureSet<double[]>();
+            FeatureSet<double[]> features = new FeatureSet<>();
             features.add(new Nominal2Binary(data.attributes()));
             features.add(new NumericAttributeFeature(data.attributes(), 0.05, 0.95, x));
             

File: core/src/test/java/smile/gap/GeneticAlgorithmTest.java
Patch:
@@ -87,7 +87,7 @@ public void testEvolve() {
             seeds[i] = new BitString(15, new Knapnack(), BitString.Crossover.UNIFORM, 1.0, 0.2);
         }
         
-        GeneticAlgorithm<BitString> instance = new GeneticAlgorithm<BitString>(seeds, GeneticAlgorithm.Selection.TOURNAMENT);
+        GeneticAlgorithm<BitString> instance = new GeneticAlgorithm<>(seeds, GeneticAlgorithm.Selection.TOURNAMENT);
         instance.setElitism(2);
         instance.setTournament(3, 0.95);
         

File: core/src/test/java/smile/neighbor/BKTreeSpeedTest.java
Patch:
@@ -32,7 +32,7 @@
  */
 public class BKTreeSpeedTest {
 
-    List<String> words = new ArrayList<String>();
+    List<String> words = new ArrayList<>();
     BKTree<String> bktree;
 
     public BKTreeSpeedTest() {
@@ -57,7 +57,7 @@ public BKTreeSpeedTest() {
         String[] data = words.toArray(new String[words.size()]);
 
         start = System.currentTimeMillis();
-        bktree = new BKTree<String>(new EditDistance(50, true));
+        bktree = new BKTree<>(new EditDistance(50, true));
         bktree.add(data);
         time = (System.currentTimeMillis() - start) / 1000.0;
         System.out.format("Building BK-tree: %.2fs%n", time);
@@ -86,7 +86,7 @@ public void tearDown() {
     public void testBKTreeSpeed() {
         System.out.println("BK-Tree range 1 speed");
         long start = System.currentTimeMillis();
-        List<Neighbor<String, String>> neighbors = new ArrayList<Neighbor<String, String>>();
+        List<Neighbor<String, String>> neighbors = new ArrayList<>();
         for (int i = 1000; i < 1100; i++) {
             bktree.range(words.get(i), 1, neighbors);
             neighbors.clear();

File: core/src/test/java/smile/neighbor/CoverTreeSpeedTest.java
Patch:
@@ -55,7 +55,7 @@ public CoverTreeSpeedTest() {
         System.out.format("Loading data: %.2fs%n", time);
 
         start = System.currentTimeMillis();
-        coverTree = new CoverTree<double[]>(x, new EuclideanDistance());
+        coverTree = new CoverTree<>(x, new EuclideanDistance());
         time = (System.currentTimeMillis() - start) / 1000.0;
         System.out.format("Building cover tree: %.2fs%n", time);
     }
@@ -96,7 +96,7 @@ public void testCoverTree() {
         System.out.format("10-NN: %.2fs%n", time);
 
         start = System.currentTimeMillis();
-        List<Neighbor<double[], double[]>> n = new ArrayList<Neighbor<double[], double[]>>();
+        List<Neighbor<double[], double[]>> n = new ArrayList<>();
         for (int i = 0; i < testx.length; i++) {
             coverTree.range(testx[i], 8.0, n);
             n.clear();

File: core/src/test/java/smile/neighbor/CoverTreeStringSpeedTest.java
Patch:
@@ -32,7 +32,7 @@
  */
 public class CoverTreeStringSpeedTest {
 
-    List<String> words = new ArrayList<String>();
+    List<String> words = new ArrayList<>();
     CoverTree<String> cover;
 
     public CoverTreeStringSpeedTest() {
@@ -57,7 +57,7 @@ public CoverTreeStringSpeedTest() {
         String[] data = words.toArray(new String[words.size()]);
 
         start = System.currentTimeMillis();
-        cover = new CoverTree<String>(data, new EditDistance(50, true));
+        cover = new CoverTree<>(data, new EditDistance(50, true));
         time = (System.currentTimeMillis() - start) / 1000.0;
         System.out.format("Building cover tree: %.2fs%n", time);
     }
@@ -85,7 +85,7 @@ public void tearDown() {
     public void testNaiveSpeed() {
         System.out.println("cover tree");
         long start = System.currentTimeMillis();
-        List<Neighbor<String, String>> neighbors = new ArrayList<Neighbor<String, String>>();
+        List<Neighbor<String, String>> neighbors = new ArrayList<>();
         for (int i = 1000; i < 1100; i++) {
             cover.range(words.get(i), 1, neighbors);
             neighbors.clear();

File: core/src/test/java/smile/neighbor/MPLSHSpeedTest.java
Patch:
@@ -79,7 +79,7 @@ public void testUSPS() {
         System.out.format("Loading USPS: %.2fs%n", time);
 
         start = System.currentTimeMillis();
-        MPLSH<double[]> lsh = new MPLSH<double[]>(256, 100, 3, 4.0);
+        MPLSH<double[]> lsh = new MPLSH<>(256, 100, 3, 4.0);
         for (double[] xi : x) {
             lsh.put(xi, xi);
         }
@@ -90,7 +90,7 @@ public void testUSPS() {
             train[i] = x[index[i]];
         }
 
-        LinearSearch<double[]> naive = new LinearSearch<double[]>(x, new EuclideanDistance());
+        LinearSearch<double[]> naive = new LinearSearch<>(x, new EuclideanDistance());
         lsh.learn(naive, train, 8.0);
         time = (System.currentTimeMillis() - start) / 1000.0;
         System.out.format("Building LSH: %.2fs%n", time);
@@ -110,7 +110,7 @@ public void testUSPS() {
         System.out.format("10-NN: %.2fs%n", time);
 
         start = System.currentTimeMillis();
-        List<Neighbor<double[], double[]>> n = new ArrayList<Neighbor<double[], double[]>>();
+        List<Neighbor<double[], double[]>> n = new ArrayList<>();
         for (int i = 0; i < testx.length; i++) {
             lsh.range(testx[i], 8.0, n);
             n.clear();

File: core/src/test/java/smile/regression/SVRTest.java
Patch:
@@ -77,7 +77,7 @@ public void testCPU() {
                 double[][] testx = Math.slice(datax, cv.test[i]);
                 double[] testy = Math.slice(datay, cv.test[i]);
 
-                SVR<double[]> svr = new SVR<double[]>(trainx, trainy, new PolynomialKernel(3, 1.0, 1.0), 0.1, 1.0);
+                SVR<double[]> svr = new SVR<>(trainx, trainy, new PolynomialKernel(3, 1.0, 1.0), 0.1, 1.0);
 
                 for (int j = 0; j < testx.length; j++) {
                     double r = testy[j] - svr.predict(testx[j]);

File: core/src/test/java/smile/taxonomy/TaxonomyTest.java
Patch:
@@ -113,7 +113,7 @@ public void testLowestCommonAncestor() {
     @Test
     public void testGetPathToRoot() {
         System.out.println("getPathToRoot");
-        LinkedList<Concept> expResult = new LinkedList<Concept>();
+        LinkedList<Concept> expResult = new LinkedList<>();
         expResult.addFirst(instance.getRoot());
         expResult.addFirst(ad);
         expResult.addFirst(a);
@@ -129,7 +129,7 @@ public void testGetPathToRoot() {
     @Test
     public void testGetPathFromRoot() {
         System.out.println("getPathToRoot");
-        LinkedList<Concept> expResult = new LinkedList<Concept>();
+        LinkedList<Concept> expResult = new LinkedList<>();
         expResult.add(instance.getRoot());
         expResult.add(ad);
         expResult.add(a);

File: data/src/main/java/smile/data/StringAttribute.java
Patch:
@@ -38,11 +38,11 @@ public class StringAttribute extends Attribute {
     /**
      * The list of unique string values of this attribute.
      */
-    private List<String> values = new ArrayList<String>();
+    private List<String> values = new ArrayList<>();
     /**
      * Map a string to an integer level.
      */
-    private Map<String, Integer> map = new HashMap<String, Integer>();
+    private Map<String, Integer> map = new HashMap<>();
 
     /**
      * Constructor.

File: data/src/main/java/smile/data/parser/IOUtils.java
Patch:
@@ -74,7 +74,7 @@ public static List<String> readLines(InputStream input, Charset charset) throws
      */
     public static List<String> readLines(Reader input) throws IOException {
         BufferedReader reader = input instanceof BufferedReader ? (BufferedReader) input : new BufferedReader(input);
-        List<String> list = new ArrayList<String>();
+        List<String> list = new ArrayList<>();
         String line = reader.readLine();
         while (line != null) {
             list.add(line);

File: demo/src/main/java/smile/demo/classification/ClassificationDemo.java
Patch:
@@ -82,7 +82,7 @@ public ClassificationDemo() {
         startButton.setActionCommand("startButton");
         startButton.addActionListener(this);
 
-        datasetBox = new JComboBox<String>();
+        datasetBox = new JComboBox<>();
         for (int i = 0; i < datasetName.length; i++) {
             datasetBox.addItem(datasetName[i]);
         }

File: demo/src/main/java/smile/demo/classification/RBFNetworkDemo.java
Patch:
@@ -64,7 +64,7 @@ public double[][] learn(double[] x, double[] y) {
 
         double[][] centers = new double[k][];
         RadialBasisFunction basis = SmileUtils.learnGaussianRadialBasis(data, centers);
-        RBFNetwork<double[]> rbf = new RBFNetwork<double[]>(data, label, new EuclideanDistance(), basis, centers);
+        RBFNetwork<double[]> rbf = new RBFNetwork<>(data, label, new EuclideanDistance(), basis, centers);
         
         for (int i = 0; i < label.length; i++) {
             label[i] = rbf.predict(data[i]);

File: demo/src/main/java/smile/demo/classification/SVMDemo.java
Patch:
@@ -76,7 +76,7 @@ public double[][] learn(double[] x, double[] y) {
         double[][] data = dataset[datasetIndex].toArray(new double[dataset[datasetIndex].size()][]);
         int[] label = dataset[datasetIndex].toArray(new int[dataset[datasetIndex].size()]);
         
-        SVM<double[]> svm = new SVM<double[]>(new GaussianKernel(gamma), C);
+        SVM<double[]> svm = new SVM<>(new GaussianKernel(gamma), C);
         svm.learn(data, label);
         svm.finish();
         

File: demo/src/main/java/smile/demo/clustering/DBScanDemo.java
Patch:
@@ -82,7 +82,7 @@ public JComponent learn() {
         }
 
         long clock = System.currentTimeMillis();
-        DBScan<double[]> dbscan = new DBScan<double[]>(dataset[datasetIndex], new EuclideanDistance(), minPts, range);
+        DBScan<double[]> dbscan = new DBScan<>(dataset[datasetIndex], new EuclideanDistance(), minPts, range);
         System.out.format("DBSCAN clusterings %d samples in %dms\n", dataset[datasetIndex].length, System.currentTimeMillis()-clock);
 
         JPanel pane = new JPanel(new GridLayout(1, 2));

File: demo/src/main/java/smile/demo/clustering/HierarchicalClusteringDemo.java
Patch:
@@ -48,7 +48,7 @@ public class HierarchicalClusteringDemo extends ClusteringDemo {
     JComboBox<String> linkageBox;
 
     public HierarchicalClusteringDemo() {
-        linkageBox = new JComboBox<String>();
+        linkageBox = new JComboBox<>();
         linkageBox.addItem("Single");
         linkageBox.addItem("Complete");
         linkageBox.addItem("UPGMA");

File: demo/src/main/java/smile/demo/clustering/MECDemo.java
Patch:
@@ -59,7 +59,7 @@ public JComponent learn() {
         }
 
         long clock = System.currentTimeMillis();
-        MEC<double[]> mec = new MEC<double[]>(dataset[datasetIndex], new EuclideanDistance(), clusterNumber, range);
+        MEC<double[]> mec = new MEC<>(dataset[datasetIndex], new EuclideanDistance(), clusterNumber, range);
         System.out.format("MEC clusterings %d samples in %dms\n", dataset[datasetIndex].length, System.currentTimeMillis()-clock);
 
         PlotCanvas plot = ScatterPlot.plot(dataset[datasetIndex], pointLegend);

File: demo/src/main/java/smile/demo/clustering/SIBDemo.java
Patch:
@@ -64,7 +64,7 @@ public SIBDemo() {
         startButton.setActionCommand("startButton");
         startButton.addActionListener(this);
 
-        datasetBox = new JComboBox<String>();
+        datasetBox = new JComboBox<>();
         for (int i = 0; i < datasetName.length; i++) {
             datasetBox.addItem(datasetName[i]);
         }

File: demo/src/main/java/smile/demo/interpolation/ScatterDataInterpolationDemo.java
Patch:
@@ -64,8 +64,8 @@ public ScatterDataInterpolationDemo() {
         add(canvas);
 
         double[][] ww = new double[26][26];
-        ArrayList<double[]> xx = new ArrayList<double[]>();
-        ArrayList<Double> zz = new ArrayList<Double>();
+        ArrayList<double[]> xx = new ArrayList<>();
+        ArrayList<Double> zz = new ArrayList<>();
         for (int i = 0; i <= 25; i++) {
             for (int j = 0; j <= 25; j++) {
                 if (Math.random() < 0.2)

File: demo/src/main/java/smile/demo/manifold/ManifoldDemo.java
Patch:
@@ -63,7 +63,7 @@ public ManifoldDemo() {
         startButton.setActionCommand("startButton");
         startButton.addActionListener(this);
 
-        datasetBox = new JComboBox<String>();
+        datasetBox = new JComboBox<>();
         for (int i = 0; i < datasetName.length; i++) {
             datasetBox.addItem(datasetName[i]);
         }

File: demo/src/main/java/smile/demo/mds/IsotonicMDSDemo.java
Patch:
@@ -70,7 +70,7 @@ public IsotonicMDSDemo() {
         startButton.setActionCommand("startButton");
         startButton.addActionListener(this);
 
-        datasetBox = new JComboBox<String>();
+        datasetBox = new JComboBox<>();
         for (int i = 0; i < datasetName.length; i++) {
             datasetBox.addItem(datasetName[i]);
         }

File: demo/src/main/java/smile/demo/mds/MDSDemo.java
Patch:
@@ -70,7 +70,7 @@ public MDSDemo() {
         startButton.setActionCommand("startButton");
         startButton.addActionListener(this);
 
-        datasetBox = new JComboBox<String>();
+        datasetBox = new JComboBox<>();
         for (int i = 0; i < datasetName.length; i++) {
             datasetBox.addItem(datasetName[i]);
         }

File: demo/src/main/java/smile/demo/mds/SammonMappingDemo.java
Patch:
@@ -70,7 +70,7 @@ public SammonMappingDemo() {
         startButton.setActionCommand("startButton");
         startButton.addActionListener(this);
 
-        datasetBox = new JComboBox<String>();
+        datasetBox = new JComboBox<>();
         for (int i = 0; i < datasetName.length; i++) {
             datasetBox.addItem(datasetName[i]);
         }

File: demo/src/main/java/smile/demo/projection/KPCADemo.java
Patch:
@@ -120,7 +120,7 @@ public JComponent learn() {
         plot.setTitle("PCA");
         pane.add(plot);
 
-        KPCA<double[]> kpca = new KPCA<double[]>(data, new GaussianKernel(gamma[datasetIndex]), 2);
+        KPCA<double[]> kpca = new KPCA<>(data, new GaussianKernel(gamma[datasetIndex]), 2);
 
         y = kpca.getCoordinates();
         plot = new PlotCanvas(Math.colMin(y), Math.colMax(y));
@@ -139,7 +139,7 @@ public JComponent learn() {
         pane.add(plot);
 
         clock = System.currentTimeMillis();
-        kpca = new KPCA<double[]>(data, new GaussianKernel(gamma[datasetIndex]), 3);
+        kpca = new KPCA<>(data, new GaussianKernel(gamma[datasetIndex]), 3);
         System.out.format("Learn KPCA from %d samples in %dms\n", data.length, System.currentTimeMillis() - clock);
 
         y = kpca.getCoordinates();

File: demo/src/main/java/smile/demo/projection/LDADemo.java
Patch:
@@ -65,7 +65,7 @@ public LDADemo() {
         startButton.setActionCommand("startButton");
         startButton.addActionListener(this);
 
-        datasetBox = new JComboBox<String>();
+        datasetBox = new JComboBox<>();
         for (int i = 0; i < datasetName.length; i++) {
             datasetBox.addItem(datasetName[i]);
         }

File: demo/src/main/java/smile/demo/projection/PCADemo.java
Patch:
@@ -39,7 +39,7 @@ public class PCADemo extends ProjectionDemo {
     JComboBox<String> corBox;
 
     public PCADemo() {
-        corBox = new JComboBox<String>();
+        corBox = new JComboBox<>();
         corBox.addItem("Covariance");
         corBox.addItem("Correlation");
         corBox.setSelectedIndex(0);

File: demo/src/main/java/smile/demo/projection/ProjectionDemo.java
Patch:
@@ -65,7 +65,7 @@ public ProjectionDemo() {
         startButton.setActionCommand("startButton");
         startButton.addActionListener(this);
 
-        datasetBox = new JComboBox<String>();
+        datasetBox = new JComboBox<>();
         for (int i = 0; i < datasetName.length; i++) {
             datasetBox.addItem(datasetName[i]);
         }

File: demo/src/main/java/smile/demo/stat/distribution/BernoulliDistributionDemo.java
Patch:
@@ -52,7 +52,7 @@ public class BernoulliDistributionDemo extends JPanel implements ChangeListener
     public BernoulliDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         for (int i = 1; i < 10; i+=2) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i/10.0)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/BetaDistributionDemo.java
Patch:
@@ -58,7 +58,7 @@ public class BetaDistributionDemo extends JPanel implements ChangeListener {
     public BetaDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         for (int i = 0; i <= 100; i+=20) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i / 10)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/BinomialDistributionDemo.java
Patch:
@@ -54,7 +54,7 @@ public class BinomialDistributionDemo extends JPanel implements ChangeListener {
     public BinomialDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> nLabelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> nLabelTable = new Hashtable<>();
         nLabelTable.put(1, new JLabel(String.valueOf(1)));
         for (int i = 10; i <= 50; i+=10) {
             nLabelTable.put(new Integer(i), new JLabel(String.valueOf(i)));
@@ -68,7 +68,7 @@ public BinomialDistributionDemo() {
         nSlider.setPaintTicks(true);
         nSlider.setPaintLabels(true);
 
-        Hashtable<Integer, JLabel> probLabelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> probLabelTable = new Hashtable<>();
         for (int i = 1; i < 10; i+=2) {
             probLabelTable.put(new Integer(i), new JLabel(String.valueOf(i/10.0)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/ChiSquareDistributionDemo.java
Patch:
@@ -55,7 +55,7 @@ public class ChiSquareDistributionDemo extends JPanel implements ChangeListener
     public ChiSquareDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         labelTable.put(1, new JLabel(String.valueOf(1)));
         for (int i = 5; i <= 20; i += 5) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i)));

File: demo/src/main/java/smile/demo/stat/distribution/ExponentialDistributionDemo.java
Patch:
@@ -56,7 +56,7 @@ public class ExponentialDistributionDemo extends JPanel implements ChangeListene
     public ExponentialDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         for (int i = 0; i <= 50; i+=10) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i/10)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/ExponentialFamilyMixtureDemo.java
Patch:
@@ -57,7 +57,7 @@ public ExponentialFamilyMixtureDemo() {
         for (int i = 1000; i < 2000; i++)
             data[i] = gamma.rand();
 
-        Vector<Mixture.Component> m = new Vector<Mixture.Component>();
+        Vector<Mixture.Component> m = new Vector<>();
         Mixture.Component c = new Mixture.Component();
         c.priori = 0.25;
         c.distribution = new GaussianDistribution(0.0, 1.0);
@@ -114,7 +114,7 @@ public static void main(String[] args) {
         for (int i = 1000; i < 2000; i++)
             data[i] = gamma.rand();
 
-        Vector<Mixture.Component> m = new Vector<Mixture.Component>();
+        Vector<Mixture.Component> m = new Vector<>();
         Mixture.Component c = new Mixture.Component();
         c.priori = 0.25;
         c.distribution = new GaussianDistribution(0.0, 1.0);

File: demo/src/main/java/smile/demo/stat/distribution/FDistributionDemo.java
Patch:
@@ -57,7 +57,7 @@ public class FDistributionDemo extends JPanel implements ChangeListener {
     public FDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         labelTable.put(1, new JLabel(String.valueOf(1)));
         for (int i = 20; i <= 100; i += 20) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i)));

File: demo/src/main/java/smile/demo/stat/distribution/GammaDistributionDemo.java
Patch:
@@ -58,7 +58,7 @@ public class GammaDistributionDemo extends JPanel implements ChangeListener {
     public GammaDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> shapeLabelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> shapeLabelTable = new Hashtable<>();
         for (int i = 0; i <= 10; i+=2) {
             shapeLabelTable.put(new Integer(i), new JLabel(String.valueOf(i)));
         }
@@ -71,7 +71,7 @@ public GammaDistributionDemo() {
         shapeSlider.setPaintTicks(true);
         shapeSlider.setPaintLabels(true);
 
-        Hashtable<Integer, JLabel> scaleLabelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> scaleLabelTable = new Hashtable<>();
         for (int i = 0; i <= 50; i+=10) {
             scaleLabelTable.put(new Integer(i), new JLabel(String.valueOf(i/10)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/GaussianDistributionDemo.java
Patch:
@@ -56,7 +56,7 @@ public class GaussianDistributionDemo extends JPanel implements ChangeListener {
     public GaussianDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         for (int i = 0; i <= 50; i+=10) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i/10)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/GeometricDistributionDemo.java
Patch:
@@ -52,7 +52,7 @@ public class GeometricDistributionDemo extends JPanel implements ChangeListener
     public GeometricDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         for (int i = 1; i < 10; i+=2) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i/10.0)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/HyperGeometricDistributionDemo.java
Patch:
@@ -55,7 +55,7 @@ public class HyperGeometricDistributionDemo extends JPanel implements ChangeList
     public HyperGeometricDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         labelTable.put(1, new JLabel(String.valueOf(1)));
         for (int i = 20; i <= 100; i+=20) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i)));

File: demo/src/main/java/smile/demo/stat/distribution/LogNormalDistributionDemo.java
Patch:
@@ -56,7 +56,7 @@ public class LogNormalDistributionDemo extends JPanel implements ChangeListener
     public LogNormalDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         for (int i = 0; i <= 20; i+=10) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i/10)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/LogisticDistributionDemo.java
Patch:
@@ -56,7 +56,7 @@ public class LogisticDistributionDemo extends JPanel implements ChangeListener {
     public LogisticDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         for (int i = 0; i <= 50; i+=10) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i/10)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/MultivariateGaussianDistributionDemo.java
Patch:
@@ -51,7 +51,7 @@ public class MultivariateGaussianDistributionDemo extends JPanel implements Chan
     public MultivariateGaussianDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         for (int i = 0; i <= 30; i+=10) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i/10.0)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/NegativeBinomialDistributionDemo.java
Patch:
@@ -54,7 +54,7 @@ public class NegativeBinomialDistributionDemo extends JPanel implements ChangeLi
     public NegativeBinomialDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> nLabelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> nLabelTable = new Hashtable<>();
         nLabelTable.put(1, new JLabel(String.valueOf(1)));
         for (int i = 10; i <= 50; i+=10) {
             nLabelTable.put(new Integer(i), new JLabel(String.valueOf(i)));
@@ -68,7 +68,7 @@ public NegativeBinomialDistributionDemo() {
         nSlider.setPaintTicks(true);
         nSlider.setPaintLabels(true);
 
-        Hashtable<Integer, JLabel> probLabelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> probLabelTable = new Hashtable<>();
         for (int i = 1; i < 10; i+=2) {
             probLabelTable.put(new Integer(i), new JLabel(String.valueOf(i/10.0)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/PoissonDistributionDemo.java
Patch:
@@ -52,7 +52,7 @@ public class PoissonDistributionDemo extends JPanel implements ChangeListener {
     public PoissonDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         for (int i = 0; i <= 10; i+=2) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/ShiftedGeometricDistributionDemo.java
Patch:
@@ -52,7 +52,7 @@ public class ShiftedGeometricDistributionDemo extends JPanel implements ChangeLi
     public ShiftedGeometricDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         for (int i = 1; i < 10; i+=2) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i/10.0)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/TDistributionDemo.java
Patch:
@@ -55,7 +55,7 @@ public class TDistributionDemo extends JPanel implements ChangeListener {
     public TDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         labelTable.put(1, new JLabel(String.valueOf(1)));
         for (int i = 5; i <= 20; i += 5) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i)));

File: demo/src/main/java/smile/demo/stat/distribution/WeibullDistributionDemo.java
Patch:
@@ -58,7 +58,7 @@ public class WeibullDistributionDemo extends JPanel implements ChangeListener {
     public WeibullDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> shapeLabelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> shapeLabelTable = new Hashtable<>();
         for (int i = 0; i <= 10; i+=2) {
             shapeLabelTable.put(new Integer(i), new JLabel(String.valueOf(i)));
         }
@@ -71,7 +71,7 @@ public WeibullDistributionDemo() {
         shapeSlider.setPaintTicks(true);
         shapeSlider.setPaintLabels(true);
 
-        Hashtable<Integer, JLabel> scaleLabelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> scaleLabelTable = new Hashtable<>();
         for (int i = 0; i <= 50; i+=10) {
             scaleLabelTable.put(new Integer(i), new JLabel(String.valueOf(i/10)));
         }

File: demo/src/main/java/smile/demo/util/ParameterParser.java
Patch:
@@ -82,7 +82,7 @@ private class Parameter {
      */
     public ParameterParser(String usage) {
         this.usage = usage;
-        parameters = new HashMap<String, Parameter>();
+        parameters = new HashMap<>();
     }
 
     /**
@@ -135,7 +135,7 @@ public String get(String name) {
      * @return a list of expanded name-value pair if monads are detected
      */
     private List<String> filterMonadics(String[] args) {// name-value for monads
-        List<String> filteredArgs = new ArrayList<String>();       // Y <- return List
+        List<String> filteredArgs = new ArrayList<>();       // Y <- return List
         for (String arg : args) {                        // iterate over args
             filteredArgs.add(arg);
             Parameter param = parameters.get(arg);
@@ -153,7 +153,7 @@ private List<String> filterMonadics(String[] args) {// name-value for monads
      * name.
      */
     public List<String> parse(String[] args) {    // merge args & defaults
-        List<String> extras = new ArrayList<String>();
+        List<String> extras = new ArrayList<>();
         List<String> filteredArgs = filterMonadics(args);          // detect and fill mons
         for (int i = 0; i < filteredArgs.size(); i++) {
             String key = filteredArgs.get(i);

File: demo/src/main/java/smile/demo/vq/VQDemo.java
Patch:
@@ -114,7 +114,7 @@ public VQDemo() {
         startButton.setActionCommand("startButton");
         startButton.addActionListener(this);
 
-        datasetBox = new JComboBox<String>();
+        datasetBox = new JComboBox<>();
         for (int i = 0; i < datasetName.length; i++) {
             datasetBox.addItem(datasetName[i]);
         }

File: math/src/main/java/smile/math/Math.java
Patch:
@@ -4017,7 +4017,7 @@ public static double[][] pow(double[][] x, double n) {
      * @return the same values as in x but with no repetitions.
      */
     public static int[] unique(int[] x) {
-        HashSet<Integer> hash = new HashSet<Integer>();
+        HashSet<Integer> hash = new HashSet<>();
         for (int i = 0; i < x.length; i++) {
             hash.add(x[i]);
         }
@@ -4038,7 +4038,7 @@ public static int[] unique(int[] x) {
      * @return the same values as in x but with no repetitions.
      */
     public static String[] unique(String[] x) {
-        HashSet<String> hash = new HashSet<String>(Arrays.asList(x));
+        HashSet<String> hash = new HashSet<>(Arrays.asList(x));
 
         String[] y = new String[hash.size()];
 

File: math/src/main/java/smile/math/SparseArray.java
Patch:
@@ -67,7 +67,7 @@ public SparseArray() {
      * @param size the number of nonzero entries in the matrix.
      */
     private SparseArray(int initialCapacity) {
-        array = new ArrayList<Entry>(initialCapacity);
+        array = new ArrayList<>(initialCapacity);
     }
 
     /**

File: math/src/main/java/smile/math/matrix/EigenValueDecomposition.java
Patch:
@@ -906,7 +906,7 @@ private static int error_bound(boolean[] enough, double[] ritz, double[] bnd, in
 
         logger.info("Lancozs method found {} converged eigenvalues of the {}-by-{} matrix", neig, step + 1, step + 1);
         if (neig != 0) {
-            for (int i = 0, n = 0; i <= step; i++) {
+            for (int i = 0; i <= step; i++) {
                 if (bnd[i] <= 16.0 * Math.EPSILON * Math.abs(ritz[i])) {
                     logger.info("ritz[{}] = {}", i, ritz[i]);
                 }

File: math/src/main/java/smile/stat/distribution/DiscreteExponentialFamilyMixture.java
Patch:
@@ -158,7 +158,7 @@ public DiscreteExponentialFamilyMixture(List<Component> mixture, int[] data) {
             }
 
             // Maximization step
-            List<Component> newConfig = new ArrayList<Component>();
+            List<Component> newConfig = new ArrayList<>();
             for (int i = 0; i < m; i++)
                 newConfig.add(((DiscreteExponentialFamily) components.get(i).distribution).M(x, posteriori[i]));
 

File: math/src/main/java/smile/stat/distribution/DiscreteMixture.java
Patch:
@@ -47,15 +47,15 @@ public static class Component {
      * Constructor.
      */
     DiscreteMixture() {
-        components = new ArrayList<Component>();
+        components = new ArrayList<>();
     }
 
     /**
      * Constructor.
      * @param mixture a list of discrete distributions.
      */
     public DiscreteMixture(List<Component> mixture) {
-        components = new ArrayList<Component>();
+        components = new ArrayList<>();
         components.addAll(mixture);
 
         double sum = 0.0;

File: math/src/main/java/smile/stat/distribution/ExponentialFamilyMixture.java
Patch:
@@ -159,7 +159,7 @@ public ExponentialFamilyMixture(List<Component> mixture, double[] data) {
             }
 
             // Maximization step
-            ArrayList<Component> newConfig = new ArrayList<Component>();
+            ArrayList<Component> newConfig = new ArrayList<>();
             for (int i = 0; i < m; i++)
                 newConfig.add(((ExponentialFamily) components.get(i).distribution).M(x, posteriori[i]));
 

File: math/src/main/java/smile/stat/distribution/GaussianMixture.java
Patch:
@@ -70,7 +70,7 @@ public GaussianMixture(double[] data) {
         if (data.length < 20)
             throw new IllegalArgumentException("Too few samples.");
         
-        ArrayList<Component> mixture = new ArrayList<Component>();
+        ArrayList<Component> mixture = new ArrayList<>();
         Component c = new Component();
         c.priori = 1.0;
         c.distribution = new GaussianDistribution(data);

File: math/src/main/java/smile/stat/distribution/Mixture.java
Patch:
@@ -61,15 +61,15 @@ public static class Component {
      * Constructor.
      */
     Mixture() {
-        components = new ArrayList<Component>();
+        components = new ArrayList<>();
     }
 
     /**
      * Constructor.
      * @param mixture a list of distributions.
      */
     public Mixture(List<Component> mixture) {
-        components = new ArrayList<Component>();
+        components = new ArrayList<>();
         components.addAll(mixture);
 
         double sum = 0.0;

File: math/src/main/java/smile/stat/distribution/MultivariateExponentialFamilyMixture.java
Patch:
@@ -155,7 +155,7 @@ public MultivariateExponentialFamilyMixture(List<Component> mixture, double[][]
             }
 
             // Maximization step
-            ArrayList<Component> newConfig = new ArrayList<Component>();
+            ArrayList<Component> newConfig = new ArrayList<>();
             for (int i = 0; i < m; i++)
                 newConfig.add(((MultivariateExponentialFamily) components.get(i).distribution).M(x, posteriori[i]));
 

File: math/src/main/java/smile/stat/distribution/MultivariateGaussianMixture.java
Patch:
@@ -168,7 +168,7 @@ public MultivariateGaussianMixture(double[][] data, boolean diagonal) {
         if (data.length < 20)
             throw new IllegalArgumentException("Too few samples.");
 
-        ArrayList<Component> mixture = new ArrayList<Component>();
+        ArrayList<Component> mixture = new ArrayList<>();
         Component c = new Component();
         c.priori = 1.0;
         c.distribution = new MultivariateGaussianDistribution(data, diagonal);

File: math/src/main/java/smile/stat/distribution/MultivariateMixture.java
Patch:
@@ -46,15 +46,15 @@ public static class Component {
      * Construct an empty Mixture.
      */
     MultivariateMixture() {
-        components = new ArrayList<Component>();
+        components = new ArrayList<>();
     }
 
     /**
      * Constructor.
      * @param mixture a list of multivariate distributions.
      */
     public MultivariateMixture(List<Component> mixture) {
-        components = new ArrayList<Component>();
+        components = new ArrayList<>();
         components.addAll(mixture);
     }
 

File: math/src/test/java/smile/math/distance/JaccardDistanceTest.java
Patch:
@@ -56,13 +56,13 @@ public void tearDown() {
     @Test
     public void testDistance() {
         System.out.println("distance");
-        Set<Integer> a = new HashSet<Integer>();
+        Set<Integer> a = new HashSet<>();
         a.add(1);
         a.add(2);
         a.add(3);
         a.add(4);
 
-        Set<Integer> b = new HashSet<Integer>();
+        Set<Integer> b = new HashSet<>();
         b.add(3);
         b.add(4);
         b.add(5);

File: math/src/test/java/smile/sort/HeapSelectTest.java
Patch:
@@ -55,7 +55,7 @@ public void tearDown() {
     @Test
     public void testSelect() {
         System.out.println("HeapSelect");
-        HeapSelect<Integer> instance = new HeapSelect<Integer>(new Integer[10]);
+        HeapSelect<Integer> instance = new HeapSelect<>(new Integer[10]);
         for (int i = 0; i < 1000; i++) {
             instance.add(i);
             if (i > 10) {
@@ -65,7 +65,7 @@ public void testSelect() {
             }
         }
 
-        instance = new HeapSelect<Integer>(new Integer[10]);
+        instance = new HeapSelect<>(new Integer[10]);
         for (int i = 0; i < 1000; i++) {
             instance.add(1000-i);
             if (i >= 9) {
@@ -82,7 +82,7 @@ public void testSelect() {
     @Test
     public void testSelectBig() {
         System.out.println("HeapSelect Big");
-        HeapSelect<Double> instance = new HeapSelect<Double>(new Double[10]);
+        HeapSelect<Double> instance = new HeapSelect<>(new Double[10]);
         for (int i = 0; i < 100000000; i++) {
             instance.add(Math.random());
         }

File: math/src/test/java/smile/stat/distribution/ExponentialFamilyMixtureTest.java
Patch:
@@ -71,7 +71,7 @@ public void testEM() {
         for (int i = 1000; i < 2000; i++)
             data[i] = gamma.rand();
 
-        Vector<Mixture.Component> m = new Vector<Mixture.Component>();
+        Vector<Mixture.Component> m = new Vector<>();
         Mixture.Component c = new Mixture.Component();
         c.priori = 0.25;
         c.distribution = new GaussianDistribution(0.0, 1.0);

File: nlp/src/main/java/smile/nlp/SimpleText.java
Patch:
@@ -37,7 +37,7 @@ public class SimpleText extends Text implements TextTerms, AnchorText {
     /**
      * The term frequency.
      */
-    private HashMap<String, Integer> freq = new HashMap<String, Integer>();
+    private HashMap<String, Integer> freq = new HashMap<>();
     /**
      * The maximum term frequency over all terms in the documents;
      */

File: nlp/src/main/java/smile/nlp/Trie.java
Patch:
@@ -55,7 +55,7 @@ public class Node {
         public Node(K key) {
             this.key = key;
             this.value = null;
-            this.children = new LinkedList<Node>();
+            this.children = new LinkedList<>();
         }
         
         public K getKey() {
@@ -117,15 +117,15 @@ public void addChild(K[] key, V value, int index) {
      * Constructor.
      */
     public Trie() {
-        root = new HashMap<K, Node>();
+        root = new HashMap<>();
     }
 
     /**
      * Constructor.
      * @param initialCapacity the initial capacity of root node.
      */
     public Trie(int initialCapacity) {
-        root = new HashMap<K, Node>(initialCapacity);
+        root = new HashMap<>(initialCapacity);
     }
 
     /**

File: nlp/src/main/java/smile/nlp/collocation/BigramCollocationFinder.java
Patch:
@@ -65,7 +65,7 @@ public BigramCollocationFinder(int minFreq) {
      */
     public BigramCollocation[] find(Corpus corpus, int k) {
         BigramCollocation[] bigrams = new BigramCollocation[k];
-        HeapSelect<BigramCollocation> heap = new HeapSelect<BigramCollocation>(bigrams);
+        HeapSelect<BigramCollocation> heap = new HeapSelect<>(bigrams);
         
         Iterator<Bigram> iterator = corpus.getBigrams();
         while (iterator.hasNext()) {
@@ -106,7 +106,7 @@ public BigramCollocation[] find(Corpus corpus, double p) {
 
         double cutoff = chisq.quantile(p);
         
-        ArrayList<BigramCollocation> bigrams = new ArrayList<BigramCollocation>();
+        ArrayList<BigramCollocation> bigrams = new ArrayList<>();
 
         Iterator<Bigram> iterator = corpus.getBigrams();
         while (iterator.hasNext()) {

File: nlp/src/main/java/smile/nlp/dictionary/EnglishDictionary.java
Patch:
@@ -43,7 +43,7 @@ public enum EnglishDictionary implements Dictionary {
      * text, in which each line is a word.
      */
     private EnglishDictionary(String resource) {
-        dict = new HashSet<String>();
+        dict = new HashSet<>();
 
         try (BufferedReader input = new BufferedReader(new InputStreamReader(this.getClass().getResourceAsStream(resource)))) {
         

File: nlp/src/main/java/smile/nlp/dictionary/EnglishPunctuations.java
Patch:
@@ -32,7 +32,7 @@ public class EnglishPunctuations implements Punctuations {
     /**
      * A set of punctuation marks.
      */
-    private HashSet<String> dict = new HashSet<String>(50);
+    private HashSet<String> dict = new HashSet<>(50);
 
     /**
      * Constructor.

File: nlp/src/main/java/smile/nlp/dictionary/EnglishStopWords.java
Patch:
@@ -54,7 +54,7 @@ public enum EnglishStopWords implements StopWords {
      * Constructor.
      */
     private EnglishStopWords(String resource) {
-        dict = new HashSet<String>();
+        dict = new HashSet<>();
 
         try (BufferedReader input = new BufferedReader(new InputStreamReader(this.getClass().getResourceAsStream(resource)))) {
             String line = null;

File: nlp/src/main/java/smile/nlp/dictionary/SimpleDictionary.java
Patch:
@@ -41,7 +41,7 @@ public class SimpleDictionary implements Dictionary {
      * text, in which each line is a word.
      */
     public SimpleDictionary(String resource) {
-        dict = new HashSet<String>();
+        dict = new HashSet<>();
 
         File file = new File(resource);
         try (BufferedReader input = file.exists() ?

File: nlp/src/main/java/smile/nlp/pos/EnglishPOSLexicon.java
Patch:
@@ -34,7 +34,7 @@ public class EnglishPOSLexicon {
     /**
      * A list of English words with POS tags.
      */
-    private static final HashMap<String, PennTreebankPOS[]> dict = new HashMap<String, PennTreebankPOS[]>();
+    private static final HashMap<String, PennTreebankPOS[]> dict = new HashMap<>();
 
     /**
      * The part-of-speech.txt file contains is a combination of

File: nlp/src/main/java/smile/nlp/pos/PennTreebankPOS.java
Patch:
@@ -413,7 +413,7 @@ public String toString() {
      */
     private static final Map<String, String> map;
     static {
-        map = new HashMap<String, String>();
+        map = new HashMap<>();
 
         map.put(".", "SENT");
         map.put("?", "SENT");

File: nlp/src/main/java/smile/nlp/stemmer/LancasterStemmer.java
Patch:
@@ -56,7 +56,7 @@ public class LancasterStemmer implements Stemmer {
         /**
          * Load rules from Lancaster_rules.txt
          */
-        RULES = new ArrayList<String>();
+        RULES = new ArrayList<>();
 
         try (BufferedReader input = new BufferedReader(new InputStreamReader(LancasterStemmer.class.getResourceAsStream("/smile/nlp/stemmer/Lancaster_rules.txt")))) {
             String line = null;

File: nlp/src/main/java/smile/nlp/tokenizer/BreakIteratorSentenceSplitter.java
Patch:
@@ -49,7 +49,7 @@ public BreakIteratorSentenceSplitter(Locale locale) {
     @Override
     public String[] split(String text) {
         boundary.setText(text);
-        ArrayList<String> sentences = new ArrayList<String>();
+        ArrayList<String> sentences = new ArrayList<>();
         int start = boundary.first();
         for (int end = boundary.next();
                 end != BreakIterator.DONE;

File: nlp/src/main/java/smile/nlp/tokenizer/BreakIteratorTokenizer.java
Patch:
@@ -49,7 +49,7 @@ public BreakIteratorTokenizer(Locale locale) {
     @Override
     public String[] split(String text) {
         boundary.setText(text);
-        ArrayList<String> words = new ArrayList<String>();
+        ArrayList<String> words = new ArrayList<>();
         int start = boundary.first();
         int end = boundary.next();
 

File: nlp/src/main/java/smile/nlp/tokenizer/EnglishAbbreviations.java
Patch:
@@ -36,7 +36,7 @@ class EnglishAbbreviations {
     private static final HashSet<String> DICTIONARY;
 
     static {
-        DICTIONARY = new HashSet<String>();
+        DICTIONARY = new HashSet<>();
 
         try (BufferedReader input = new BufferedReader(new InputStreamReader(EnglishAbbreviations.class.getResourceAsStream("/smile/nlp/tokenizer/abbreviations_en.txt")))) {
             String line = null;

File: nlp/src/main/java/smile/nlp/tokenizer/SimpleSentenceSplitter.java
Patch:
@@ -93,7 +93,7 @@ public static SimpleSentenceSplitter getInstance() {
 
     @Override
     public String[] split(String text) {
-        ArrayList<String> sentences = new ArrayList<String>();
+        ArrayList<String> sentences = new ArrayList<>();
 
         // The number of words in the sentence.
         int len = 0;

File: nlp/src/main/java/smile/nlp/tokenizer/SimpleTokenizer.java
Patch:
@@ -144,7 +144,7 @@ public String[] split(String text) {
             }
         }
         
-        ArrayList<String> result = new ArrayList<String>();
+        ArrayList<String> result = new ArrayList<>();
         for (String token : words) {
             if (!token.isEmpty()) {
                 result.add(token);

File: nlp/src/test/java/smile/nlp/collocation/AprioriPhraseExtractorTest.java
Patch:
@@ -71,7 +71,7 @@ public void testExtract() throws FileNotFoundException {
         
         PorterStemmer stemmer = new PorterStemmer();
         SimpleTokenizer tokenizer = new SimpleTokenizer();
-        ArrayList<String[]> sentences = new ArrayList<String[]>();
+        ArrayList<String[]> sentences = new ArrayList<>();
         for (String paragraph : SimpleParagraphSplitter.getInstance().split(text)) {
             for (String s : SimpleSentenceSplitter.getInstance().split(paragraph)) {
                 String[] sentence = tokenizer.split(s);

File: plot/src/main/java/smile/plot/Axis.java
Patch:
@@ -321,7 +321,7 @@ public Axis setRotation(double rotation) {
      */
     public Axis addLabel(String label, double location) {
         if (labels == null) {
-            labels = new HashMap<String, Double>();
+            labels = new HashMap<>();
         }
 
         labels.put(label, location);
@@ -340,7 +340,7 @@ public Axis addLabel(String[] label, double[] location) {
         }
 
         if (labels == null) {
-            labels = new HashMap<String, Double>();
+            labels = new HashMap<>();
         }
 
         for (int i = 0; i < label.length; i++) {

File: plot/src/main/java/smile/plot/PlotCanvas.java
Patch:
@@ -135,7 +135,7 @@ public class PlotCanvas extends JPanel {
     /**
      * The shapes in the canvas, e.g. label, plots, etc.
      */
-    private List<Shape> shapes = new ArrayList<Shape>();
+    private List<Shape> shapes = new ArrayList<>();
     /**
      * The real canvas for plots.
      */

File: plot/src/main/java/smile/plot/ScatterPlot.java
Patch:
@@ -161,7 +161,7 @@ public ScatterPlot(double[][] data, int[] y, char[] legends, Color[] palette) {
         int[] id = Math.unique(y);
         Arrays.sort(id);
 
-        classLookupTable = new HashMap<Integer, Integer>(id.length);
+        classLookupTable = new HashMap<>(id.length);
 
         for (int i = 0; i < id.length; i++) {
             classLookupTable.put(id[i], i);

File: plot/src/main/java/smile/swing/FileChooser.java
Patch:
@@ -283,7 +283,7 @@ public static class SimpleFileFilter extends FileFilter {
         /**
          * The file extensions in lower case.
          */
-        private TreeSet<String> filters = new TreeSet<String>();
+        private TreeSet<String> filters = new TreeSet<>();
         /**
          * The human readable description of this filter.
          */

File: plot/src/main/java/smile/swing/FontChooser.java
Patch:
@@ -227,7 +227,7 @@ private JTextField getFontSizeTextField() {
 
     private JList<String> getFontFamilyList() {
         if (fontNameList == null) {
-            fontNameList = new JList<String>(getFontFamilies());
+            fontNameList = new JList<>(getFontFamilies());
             fontNameList.setSelectionMode(ListSelectionModel.SINGLE_SELECTION);
             fontNameList.addListSelectionListener(
                     new ListSelectionHandler(getFontFamilyTextField()));
@@ -240,7 +240,7 @@ private JList<String> getFontFamilyList() {
 
     private JList<String> getFontStyleList() {
         if (fontStyleList == null) {
-            fontStyleList = new JList<String>(getFontStyleNames());
+            fontStyleList = new JList<>(getFontStyleNames());
             fontStyleList.setSelectionMode(ListSelectionModel.SINGLE_SELECTION);
             fontStyleList.addListSelectionListener(
                     new ListSelectionHandler(getFontStyleTextField()));
@@ -253,7 +253,7 @@ private JList<String> getFontStyleList() {
 
     private JList<String> getFontSizeList() {
         if (fontSizeList == null) {
-            fontSizeList = new JList<String>(this.fontSizeStrings);
+            fontSizeList = new JList<>(this.fontSizeStrings);
             fontSizeList.setSelectionMode(ListSelectionModel.SINGLE_SELECTION);
             fontSizeList.addListSelectionListener(
                     new ListSelectionHandler(getFontSizeTextField()));

File: plot/src/main/java/smile/swing/table/IntegerArrayCellEditor.java
Patch:
@@ -75,7 +75,7 @@ public Object stringToValue(String string) throws ParseException {
 
                 int[] data = new int[items.length];
                 for (int i = 0; i < data.length; i++) {
-                    data[i] = Integer.valueOf(items[i].trim());
+                    data[i] = Integer.parseInt(items[i].trim());
                 }
 
                 return data;

File: data/src/main/java/smile/data/parser/ArffParser.java
Patch:
@@ -502,7 +502,7 @@ private Datum<double[]> getSparseInstance(StreamTokenizer tokenizer, Attribute[]
             
             String s = tokenizer.sval.trim();
             if (index < 0) {
-                index = Integer.valueOf(s);
+                index = Integer.parseInt(s);
                 if (index < 0 || index >= attributes.length) {
                     throw new ParseException("Invalid attribute index: " + index, tokenizer.lineno());
                 }

File: data/src/main/java/smile/data/parser/SparseDatasetParser.java
Patch:
@@ -173,9 +173,9 @@ public SparseDataset parse(String name, InputStream stream) throws IOException,
                     throw new ParseException("Invalid number of tokens.", nrow);
                 }
 
-                int d = Integer.valueOf(tokens[0]) - arrayIndexOrigin;
-                int w = Integer.valueOf(tokens[1]) - arrayIndexOrigin;
-                double c = Double.valueOf(tokens[2]);
+                int d = Integer.parseInt(tokens[0]) - arrayIndexOrigin;
+                int w = Integer.parseInt(tokens[1]) - arrayIndexOrigin;
+                double c = Double.parseDouble(tokens[2]);
                 sparse.set(d, w, c);
 
                 line = reader.readLine();

File: data/src/main/java/smile/data/parser/microarray/GCTParser.java
Patch:
@@ -168,8 +168,8 @@ public AttributeDataset parse(String name, InputStream stream) throws IOExceptio
             throw new IOException("Invalid data size inforamation.");            
         }
         
-        int n = Integer.valueOf(tokens[0]);
-        int p = Integer.valueOf(tokens[1]);
+        int n = Integer.parseInt(tokens[0]);
+        int p = Integer.parseInt(tokens[1]);
         if (n <= 0 || p <= 0) {
             throw new IOException(String.format("Invalid data size %d x %d.", n, p));                        
         }

File: data/src/main/java/smile/data/parser/microarray/RESParser.java
Patch:
@@ -187,7 +187,7 @@ public AttributeDataset parse(String name, InputStream stream) throws IOExceptio
             throw new IOException("Premature end of file.");
         }
 
-        int n = Integer.valueOf(line);
+        int n = Integer.parseInt(line);
         if (n <= 0) {
             throw new IOException("Invalid number of rows: " + n);            
         }

File: plot/src/main/java/smile/swing/table/IntegerArrayCellEditor.java
Patch:
@@ -75,7 +75,7 @@ public Object stringToValue(String string) throws ParseException {
 
                 int[] data = new int[items.length];
                 for (int i = 0; i < data.length; i++) {
-                    data[i] = Integer.valueOf(items[i].trim());
+                    data[i] = Integer.parseInt(items[i].trim());
                 }
 
                 return data;

File: core/src/main/java/smile/association/ARM.java
Patch:
@@ -125,7 +125,7 @@ public long learn(double confidence, PrintStream out) {
      * @param confidence the confidence threshold for association rules.
      */
     public List<AssociationRule> learn(double confidence) {
-        List<AssociationRule> list = new ArrayList<AssociationRule>();
+        List<AssociationRule> list = new ArrayList<>();
         ttree = fim.buildTotalSupportTree();
         for (int i = 0; i < ttree.root.children.length; i++) {
             if (ttree.root.children[i] != null) {

File: core/src/main/java/smile/association/FPTree.java
Patch:
@@ -89,7 +89,7 @@ class Node {
          */
         void add(int index, int end, int[] itemset, int support) {
             if (children == null) {
-                children = new HashMap<Integer, Node>();
+                children = new HashMap<>();
             }
             
             Node child = children.get(itemset[index]);
@@ -114,7 +114,7 @@ void add(int index, int end, int[] itemset, int support) {
          */
         void append(int index, int end, int[] itemset, int support) {
             if (children == null) {
-                children = new HashMap<Integer, Node>();
+                children = new HashMap<>();
             }
             
             if (index >= maxItemSetSize) {

File: core/src/main/java/smile/association/TotalSupportTree.java
Patch:
@@ -168,7 +168,7 @@ private int getSupport(int[] itemset, int index, Node node) {
      * @return the list of frequent item sets
      */
     public List<ItemSet> getFrequentItemsets() {
-        List<ItemSet> list = new ArrayList<ItemSet>();
+        List<ItemSet> list = new ArrayList<>();
         getFrequentItemsets(null, list);
         return list;
     }

File: core/src/main/java/smile/classification/DecisionTree.java
Patch:
@@ -511,7 +511,7 @@ public boolean findBestSplit() {
                 }
             } else {
 
-                List<SplitTask> tasks = new ArrayList<SplitTask>(mtry);
+                List<SplitTask> tasks = new ArrayList<>(mtry);
                 for (int j = 0; j < mtry; j++) {
                     tasks.add(new SplitTask(n, count, impurity, variables[j]));
                 }
@@ -944,7 +944,7 @@ public DecisionTree(Attribute[] attributes, double[][] x, int[] y, int maxNodes,
         }
 
         // Priority queue for best-first tree growing.
-        PriorityQueue<TrainNode> nextSplits = new PriorityQueue<TrainNode>();
+        PriorityQueue<TrainNode> nextSplits = new PriorityQueue<>();
 
         int n = y.length;
         int[] count = new int[k];

File: core/src/main/java/smile/classification/RBFNetwork.java
Patch:
@@ -186,9 +186,9 @@ public RBFNetwork<T> train(T[] x, int[] y) {
             GaussianRadialBasis gaussian = SmileUtils.learnGaussianRadialBasis(x, centers, distance);
             
             if (rbf == null) {
-                return new RBFNetwork<T>(x, y, distance, gaussian, centers, normalized);
+                return new RBFNetwork<>(x, y, distance, gaussian, centers, normalized);
             } else {
-                return new RBFNetwork<T>(x, y, distance, rbf, centers, normalized);
+                return new RBFNetwork<>(x, y, distance, rbf, centers, normalized);
             }
         }
         
@@ -201,7 +201,7 @@ public RBFNetwork<T> train(T[] x, int[] y) {
          * @return a trained RBF network
          */
         public RBFNetwork<T> train(T[] x, int[] y, T[] centers) {
-            return new RBFNetwork<T>(x, y, distance, rbf, centers, normalized);
+            return new RBFNetwork<>(x, y, distance, rbf, centers, normalized);
         }
     }
     

File: core/src/main/java/smile/clustering/BIRCH.java
Patch:
@@ -387,9 +387,9 @@ public int partition(int k) {
      * @return the number of non-outlier leaves.
      */
     public int partition(int k, int minPts) {
-        ArrayList<Leaf> leaves = new ArrayList<Leaf>();
-        ArrayList<double[]> centers = new ArrayList<double[]>();
-        Queue<Node> queue = new LinkedList<Node>();
+        ArrayList<Leaf> leaves = new ArrayList<>();
+        ArrayList<double[]> centers = new ArrayList<>();
+        Queue<Node> queue = new LinkedList<>();
         queue.offer(root);
 
         for (Node node = queue.poll(); node != null; node = queue.poll()) {

File: core/src/main/java/smile/clustering/CLARANS.java
Patch:
@@ -143,7 +143,7 @@ public CLARANS(T[] data, Distance<T> distance, int k, int maxNeighbor, int numLo
         this.numLocal = numLocal;
         this.maxNeighbor = maxNeighbor;
         
-        List<CLARANSTask> tasks = new ArrayList<CLARANSTask>();
+        List<CLARANSTask> tasks = new ArrayList<>();
         for (int i = 0; i < numLocal; i++) {
             tasks.add(new CLARANSTask(data));
         }

File: core/src/main/java/smile/clustering/DeterministicAnnealing.java
Patch:
@@ -92,7 +92,7 @@ public DeterministicAnnealing(double[][] data, int Kmax, double alpha) {
 
         int np = MulticoreExecutor.getThreadPoolSize();
         if (n >= 1000 && np >= 2) {
-            tasks = new ArrayList<UpdateThread>(np + 1);
+            tasks = new ArrayList<>(np + 1);
             int step = n / np;
             if (step < 100) {
                 step = 100;
@@ -107,7 +107,7 @@ public DeterministicAnnealing(double[][] data, int Kmax, double alpha) {
             }
             tasks.add(new UpdateThread(data, centroids, posteriori, priori, start, n));
             
-            ctasks = new ArrayList<CentroidThread>(2 * Kmax);
+            ctasks = new ArrayList<>(2 * Kmax);
             for (int i = 0; i < 2*Kmax; i++) {
                 ctasks.add(new CentroidThread(data, centroids, posteriori, priori, i));
             }

File: core/src/main/java/smile/clustering/GMeans.java
Patch:
@@ -81,7 +81,7 @@ public GMeans(double[][] data, int kmax) {
 
         BBDTree bbd = new BBDTree(data);
         while (k < kmax) {
-            ArrayList<double[]> centers = new ArrayList<double[]>();
+            ArrayList<double[]> centers = new ArrayList<>();
             double[] score = new double[k];
             KMeans[] kmeans = new KMeans[k];
             

File: core/src/main/java/smile/clustering/HierarchicalClustering.java
Patch:
@@ -185,7 +185,7 @@ public int[] partition(double h) {
      */
     private void bfs(int[] membership, int cluster, int id) {
         int n = merge.length + 1;
-        Queue<Integer> queue = new LinkedList<Integer>();
+        Queue<Integer> queue = new LinkedList<>();
         queue.offer(cluster);
 
         for (Integer i = queue.poll(); i != null; i = queue.poll()) {

File: core/src/main/java/smile/clustering/KMeans.java
Patch:
@@ -226,7 +226,7 @@ public KMeans(double[][] data, int k, int maxIter, int runs) {
 
         BBDTree bbd = new BBDTree(data);
 
-        List<KMeansThread> tasks = new ArrayList<KMeansThread>();
+        List<KMeansThread> tasks = new ArrayList<>();
         for (int i = 0; i < runs; i++) {
             tasks.add(new KMeansThread(bbd, data, k, maxIter));
         }
@@ -321,7 +321,7 @@ public static KMeans lloyd(double[][] data, int k, int maxIter) {
         int np = MulticoreExecutor.getThreadPoolSize();
         List<LloydThread> tasks = null;
         if (n >= 1000 && np >= 2) {
-            tasks = new ArrayList<LloydThread>(np + 1);
+            tasks = new ArrayList<>(np + 1);
             int step = n / np;
             if (step < 100) {
                 step = 100;

File: core/src/main/java/smile/clustering/SIB.java
Patch:
@@ -186,7 +186,7 @@ public SIB(double[][] data, int k, int maxIter, int runs) {
             throw new IllegalArgumentException("Invalid number of runs: " + runs);
         }
 
-        List<SIBThread> tasks = new ArrayList<SIBThread>();
+        List<SIBThread> tasks = new ArrayList<>();
         for (int i = 0; i < runs; i++) {
             tasks.add(new SIBThread(data, k, maxIter));
         }
@@ -395,7 +395,7 @@ public SIB(SparseDataset data, int k, int maxIter, int runs) {
             throw new IllegalArgumentException("Invalid number of runs: " + runs);
         }
 
-        List<SIBThread> tasks = new ArrayList<SIBThread>();
+        List<SIBThread> tasks = new ArrayList<>();
         for (int i = 0; i < runs; i++) {
             tasks.add(new SIBThread(data, k, maxIter));
         }

File: core/src/main/java/smile/clustering/XMeans.java
Patch:
@@ -85,7 +85,7 @@ public XMeans(double[][] data, int kmax) {
 
         BBDTree bbd = new BBDTree(data);
         while (k < kmax) {
-            ArrayList<double[]> centers = new ArrayList<double[]>();
+            ArrayList<double[]> centers = new ArrayList<>();
             double[] score = new double[k];
             KMeans[] kmeans = new KMeans[k];
             

File: core/src/main/java/smile/feature/Bag.java
Patch:
@@ -58,7 +58,7 @@ public Bag(T[] features) {
      */
     public Bag(T[] features, boolean binary) {
         this.binary = binary;
-        this.features = new HashMap<T, Integer>();
+        this.features = new HashMap<>();
         for (int i = 0, k = 0; i < features.length; i++) {
             if (!this.features.containsKey(features[i])) {
                 this.features.put(features[i], k++);

File: core/src/main/java/smile/feature/FeatureSet.java
Patch:
@@ -35,11 +35,11 @@ public class FeatureSet <T> {
     /**
      * Feature generators.
      */
-    List<Feature<T>> features = new ArrayList<Feature<T>>();
+    List<Feature<T>> features = new ArrayList<>();
     /**
      * The variable attributes of generated features.
      */
-    List<Attribute> attributes = new ArrayList<Attribute>();
+    List<Attribute> attributes = new ArrayList<>();
     
     /**
      * Constructor.
@@ -123,7 +123,7 @@ public AttributeDataset f(Dataset<T> data) {
 
         for (int i = 0; i < data.size(); i++) {
             Datum<T> datum = data.get(i);
-            Datum<double[]> x = new Datum<double[]>(f(datum.x), datum.y, datum.weight);
+            Datum<double[]> x = new Datum<>(f(datum.x), datum.y, datum.weight);
             x.name = datum.name;
             x.description = datum.description;
             x.timestamp = datum.timestamp;

File: core/src/main/java/smile/gap/GeneticAlgorithm.java
Patch:
@@ -219,7 +219,7 @@ public enum Selection {
     /**
      * Parallel tasks to evaluate the fitness of population.
      */
-    private List<Task> tasks = new ArrayList<Task>();
+    private List<Task> tasks = new ArrayList<>();
     
     /**
      * Constructor. The default selection strategy is tournament selection

File: core/src/main/java/smile/manifold/IsoMap.java
Patch:
@@ -109,9 +109,9 @@ public IsoMap(double[][] data, int d, int k, boolean CIsomap) {
 
         KNNSearch<double[], double[]> knn = null;
         if (data[0].length < 10) {
-            knn = new KDTree<double[]>(data, data);
+            knn = new KDTree<>(data, data);
         } else {
-            knn = new CoverTree<double[]>(data, new EuclideanDistance());
+            knn = new CoverTree<>(data, new EuclideanDistance());
         }
 
         graph = new AdjacencyList(n);

File: core/src/main/java/smile/manifold/LLE.java
Patch:
@@ -88,9 +88,9 @@ public LLE(double[][] data, int d, int k) {
 
         KNNSearch<double[], double[]> knn = null;
         if (D < 10) {
-            knn = new KDTree<double[]>(data, data);
+            knn = new KDTree<>(data, data);
         } else {
-            knn = new CoverTree<double[]>(data, new EuclideanDistance());
+            knn = new CoverTree<>(data, new EuclideanDistance());
         }
 
         Comparator<Neighbor<double[], double[]>> comparator = new Comparator<Neighbor<double[], double[]>>() {

File: core/src/main/java/smile/manifold/LaplacianEigenmap.java
Patch:
@@ -98,9 +98,9 @@ public LaplacianEigenmap(double[][] data, int d, int k, double t) {
         int n = data.length;
         KNNSearch<double[], double[]> knn = null;
         if (data[0].length < 10) {
-            knn = new KDTree<double[]>(data, data);
+            knn = new KDTree<>(data, data);
         } else {
-            knn = new CoverTree<double[]>(data, new EuclideanDistance());
+            knn = new CoverTree<>(data, new EuclideanDistance());
         }
 
         graph = new AdjacencyList(n);

File: core/src/main/java/smile/neighbor/BKTree.java
Patch:
@@ -93,7 +93,7 @@ private void add(E datum) {
             }
 
             if (children == null) {
-                children = new ArrayList<Node>();
+                children = new ArrayList<>();
             }
 
             while (children.size() <= d) {
@@ -203,7 +203,7 @@ private void search(Node node, E q, int k, List<Neighbor<E, E>> neighbors) {
 
         if (d <= k) {
             if (node.object != q || !identicalExcluded) {
-                neighbors.add(new Neighbor<E, E>(node.object, node.object, node.index, d));
+                neighbors.add(new Neighbor<>(node.object, node.object, node.index, d));
             }
         }
 

File: core/src/main/java/smile/regression/GaussianProcessRegression.java
Patch:
@@ -111,7 +111,7 @@ public Trainer(MercerKernel<T> kernel, double lambda) {
         
         @Override
         public GaussianProcessRegression<T> train(T[] x, double[] y) {
-            return new GaussianProcessRegression<T>(x, y, kernel, lambda);
+            return new GaussianProcessRegression<>(x, y, kernel, lambda);
         }
         
         /**
@@ -125,7 +125,7 @@ public GaussianProcessRegression<T> train(T[] x, double[] y) {
          * @return a trained Gaussian Process.
          */
         public GaussianProcessRegression<T> train(T[] x, double[] y, T[] t) {
-            return new GaussianProcessRegression<T>(x, y, t, kernel, lambda);
+            return new GaussianProcessRegression<>(x, y, t, kernel, lambda);
         }
     }
     

File: core/src/main/java/smile/regression/RBFNetwork.java
Patch:
@@ -173,9 +173,9 @@ public RBFNetwork<T> train(T[] x, double[] y) {
             T[] centers = (T[]) java.lang.reflect.Array.newInstance(x.getClass().getComponentType(), m);
             GaussianRadialBasis gaussian = SmileUtils.learnGaussianRadialBasis(x, centers, distance);
             if (rbf == null) {
-                return new RBFNetwork<T>(x, y, distance, gaussian, centers, normalized);
+                return new RBFNetwork<>(x, y, distance, gaussian, centers, normalized);
             } else {
-                return new RBFNetwork<T>(x, y, distance, rbf, centers, normalized);
+                return new RBFNetwork<>(x, y, distance, rbf, centers, normalized);
             }
         }
         
@@ -188,7 +188,7 @@ public RBFNetwork<T> train(T[] x, double[] y) {
          * @return a trained RBF network
          */
         public RBFNetwork<T> train(T[] x, double[] y, T[] centers) {
-            return new RBFNetwork<T>(x, y, distance, rbf, centers, normalized);
+            return new RBFNetwork<>(x, y, distance, rbf, centers, normalized);
         }
     }
     

File: core/src/main/java/smile/regression/RandomForest.java
Patch:
@@ -458,7 +458,7 @@ public RandomForest(Attribute[] attributes, double[][] x, double[] y, int ntrees
         int[] oob = new int[n];
         
         int[][] order = SmileUtils.sort(attributes, x);
-        List<TrainingTask> tasks = new ArrayList<TrainingTask>();
+        List<TrainingTask> tasks = new ArrayList<>();
         for (int i = 0; i < ntrees; i++) {
             tasks.add(new TrainingTask(attributes, x, y, maxNodes, nodeSize, mtry, subsample, order, prediction, oob));
         }
@@ -468,7 +468,7 @@ public RandomForest(Attribute[] attributes, double[][] x, double[] y, int ntrees
         } catch (Exception ex) {
             ex.printStackTrace();
 
-            trees = new ArrayList<RegressionTree>(ntrees);
+            trees = new ArrayList<>(ntrees);
             for (int i = 0; i < ntrees; i++) {
                 trees.add(tasks.get(i).call());
             }
@@ -547,7 +547,7 @@ public void trim(int ntrees) {
             throw new IllegalArgumentException("Invalid new model size: " + ntrees);
         }
         
-        List<RegressionTree> model = new ArrayList<RegressionTree>(ntrees);
+        List<RegressionTree> model = new ArrayList<>(ntrees);
         for (int i = 0; i < ntrees; i++) {
             model.add(trees.get(i));
         }

File: core/src/main/java/smile/regression/RegressionTree.java
Patch:
@@ -422,7 +422,7 @@ public boolean findBestSplit() {
                 }
             } else {
 
-                List<SplitTask> tasks = new ArrayList<SplitTask>(mtry);
+                List<SplitTask> tasks = new ArrayList<>(mtry);
                 for (int j = 0; j < mtry; j++) {
                     tasks.add(new SplitTask(n, sum, variables[j]));
                 }
@@ -965,7 +965,7 @@ public RegressionTree(Attribute[] attributes, double[][] x, double[] y, int maxN
         }
 
         // Priority queue for best-first tree growing.
-        PriorityQueue<TrainNode> nextSplits = new PriorityQueue<TrainNode>();
+        PriorityQueue<TrainNode> nextSplits = new PriorityQueue<>();
 
         int n = 0;
         double sum = 0.0;
@@ -1063,7 +1063,7 @@ public RegressionTree(int numFeatures, int[][] x, double[] y, int maxNodes, int
         importance = new double[numFeatures];
         
         // Priority queue for best-first tree growing.
-        PriorityQueue<SparseBinaryTrainNode> nextSplits = new PriorityQueue<SparseBinaryTrainNode>();
+        PriorityQueue<SparseBinaryTrainNode> nextSplits = new PriorityQueue<>();
 
         int n = 0;
         double sum = 0.0;

File: core/src/main/java/smile/regression/SVR.java
Patch:
@@ -74,7 +74,7 @@ public class SVR<T> implements Regression<T> {
     /**
      * Support vectors.
      */
-    private List<SupportVector> sv = new ArrayList<SupportVector>();
+    private List<SupportVector> sv = new ArrayList<>();
     /**
      * Threshold of decision function.
      */
@@ -189,7 +189,7 @@ public Trainer setTolerance(double tol) {
 
         @Override
         public SVR<T> train(T[] x, double[] y) {
-            SVR<T> svr = new SVR<T>(x, y, kernel, eps, C, tol);
+            SVR<T> svr = new SVR<>(x, y, kernel, eps, C, tol);
             return svr;
         }
     }
@@ -415,7 +415,7 @@ private void gram(SupportVector i) {
                 i.kcache.add(kernel.k(i.x, v.x));
             }
         } else {
-            List<KernelTask> tasks = new ArrayList<KernelTask>(m + 1);
+            List<KernelTask> tasks = new ArrayList<>(m + 1);
             int step = n / m;
             if (step < 100) {
                 step = 100;

File: core/src/main/java/smile/sampling/Bagging.java
Patch:
@@ -52,7 +52,7 @@ public Bagging(int k, int[] y, double[] classWeight, double subsample) {
             // Training samples draw with replacement.
             for (int l = 0; l < k; l++) {
                 int nj = 0;
-                ArrayList<Integer> cj = new ArrayList<Integer>();
+                ArrayList<Integer> cj = new ArrayList<>();
                 for (int i = 0; i < n; i++) {
                     if (y[i] == l) {
                         cj.add(i);

File: core/src/main/java/smile/sequence/HMM.java
Patch:
@@ -123,7 +123,7 @@ public HMM(double[] pi, double[][] a, double[][] b, O[] symbols) {
                 throw new IllegalArgumentException("Invalid size of emission symbol list.");
             }
 
-            this.symbols = new HashMap<O, Integer>();
+            this.symbols = new HashMap<>();
             for (int i = 0; i < symbols.length; i++) {
                 this.symbols.put(symbols[i], i);
             }
@@ -481,7 +481,7 @@ public HMM(O[][] observations, int[][] labels) {
         }
 
         int index = 0;
-        symbols = new HashMap<O, Integer>();
+        symbols = new HashMap<>();
         for (int i = 0; i < observations.length; i++) {
             if (observations[i].length != labels[i].length) {
                 throw new IllegalArgumentException(String.format("The length of observation sequence %d and that of corresponding label sequence are different.", i));
@@ -573,7 +573,7 @@ public HMM<O> learn(int[][] observations, int iterations) {
      * @return an updated HMM.
      */
     private HMM<O> iterate(int[][] sequences) {
-        HMM<O> hmm = new HMM<O>(numStates, numSymbols);
+        HMM<O> hmm = new HMM<>(numStates, numSymbols);
         hmm.symbols = symbols;
 
         // gamma[n] = gamma array associated to observation sequence n

File: core/src/main/java/smile/taxonomy/Taxonomy.java
Patch:
@@ -32,7 +32,7 @@ public class Taxonomy {
     /**
      * All the concepts in this taxonomy.
      */
-    HashMap<String, Concept> concepts = new HashMap<String, Concept>();
+    HashMap<String, Concept> concepts = new HashMap<>();
     /**
      * The root node in the taxonomy.
      */
@@ -84,7 +84,7 @@ public List<String> getConcepts() {
      * Returns all named concepts from this taxonomy
      */
     private List<String> getConcepts(Concept c) {
-        List<String> keywords = new ArrayList<String>();
+        List<String> keywords = new ArrayList<>();
 
         while (c != null) {
             if (c.synset != null) {

File: core/src/main/java/smile/util/MulticoreExecutor.java
Patch:
@@ -90,7 +90,7 @@ public static int getThreadPoolSize() {
     public static <T> List<T> run(Collection<? extends Callable<T>> tasks) throws Exception {
         createThreadPool();
 
-        List<T> results = new ArrayList<T>();
+        List<T> results = new ArrayList<>();
         if (threads == null) {
             for (Callable<T> task : tasks) {
                 results.add(task.call());

File: core/src/main/java/smile/util/SmileUtils.java
Patch:
@@ -198,7 +198,7 @@ public static GaussianRadialBasis[] learnGaussianRadialBasis(double[][] x, doubl
      */
     public static <T> GaussianRadialBasis learnGaussianRadialBasis(T[] x, T[] centers, Metric<T> distance) {
         int k = centers.length;
-        CLARANS<T> clarans = new CLARANS<T>(x, distance, k, Math.min(100, (int) Math.round(0.01 * k * (x.length-k))));
+        CLARANS<T> clarans = new CLARANS<>(x, distance, k, Math.min(100, (int) Math.round(0.01 * k * (x.length - k))));
         System.arraycopy(clarans.medoids(), 0, centers, 0, k);
 
         double r0 = 0.0;
@@ -234,7 +234,7 @@ public static <T> GaussianRadialBasis[] learnGaussianRadialBasis(T[] x, T[] cent
         }
         
         int k = centers.length;
-        CLARANS<T> clarans = new CLARANS<T>(x, distance, k, Math.min(100, (int) Math.round(0.01 * k * (x.length-k))));
+        CLARANS<T> clarans = new CLARANS<>(x, distance, k, Math.min(100, (int) Math.round(0.01 * k * (x.length - k))));
         System.arraycopy(clarans.medoids(), 0, centers, 0, k);
 
         p = Math.min(p, k-1);
@@ -274,7 +274,7 @@ public static <T> GaussianRadialBasis[] learnGaussianRadialBasis(T[] x, T[] cent
         }
         
         int k = centers.length;
-        CLARANS<T> clarans = new CLARANS<T>(x, distance, k, Math.min(100, (int) Math.round(0.01 * k * (x.length-k))));
+        CLARANS<T> clarans = new CLARANS<>(x, distance, k, Math.min(100, (int) Math.round(0.01 * k * (x.length - k))));
         System.arraycopy(clarans.medoids(), 0, centers, 0, k);
 
         int n = x.length;

File: core/src/main/java/smile/validation/ConfusionMatrix.java
Patch:
@@ -31,7 +31,7 @@ public ConfusionMatrix(int[] truth, int[] prediction) {
 			 throw new IllegalArgumentException(String.format("The vector sizes don't match: %d != %d.", truth.length, prediction.length));
 		}
 		
-		Set<Integer> ySet = new HashSet<Integer>();
+		Set<Integer> ySet = new HashSet<>();
 		
 		for(int i = 0; i < truth.length; i++){
 			ySet.add(truth[i]);

File: core/src/test/java/smile/association/ARMTest.java
Patch:
@@ -114,7 +114,7 @@ public void testLearn() {
     public void testLearnPima() {
         System.out.println("pima");
 
-        List<int[]> dataList = new ArrayList<int[]>(1000);
+        List<int[]> dataList = new ArrayList<>(1000);
 
         try {
             BufferedReader input = smile.data.parser.IOUtils.getTestDataReader("transaction/pima.D38.N768.C2");
@@ -157,7 +157,7 @@ public void testLearnPima() {
     public void testLearnKosarak() {
         System.out.println("kosarak");
 
-        List<int[]> dataList = new ArrayList<int[]>(1000);
+        List<int[]> dataList = new ArrayList<>(1000);
 
         try {
             BufferedReader input = smile.data.parser.IOUtils.getTestDataReader("transaction/kosarak.dat");
@@ -170,7 +170,7 @@ public void testLearnKosarak() {
 
                 String[] s = line.split(" ");
 
-                Set<Integer> items = new HashSet<Integer>();
+                Set<Integer> items = new HashSet<>();
                 for (int i = 0; i < s.length; i++) {
                     items.add(Integer.parseInt(s[i]));
                 }

File: core/src/test/java/smile/association/FPGrowthTest.java
Patch:
@@ -120,7 +120,7 @@ public void testLearn_PrintStream() {
     public void testPima() {
         System.out.println("pima");
 
-        List<int[]> dataList = new ArrayList<int[]>(1000);
+        List<int[]> dataList = new ArrayList<>(1000);
 
         try {
             BufferedReader input = smile.data.parser.IOUtils.getTestDataReader("transaction/pima.D38.N768.C2");
@@ -168,7 +168,7 @@ public void testPima() {
     public void testKosarak() {
         System.out.println("kosarak");
 
-        List<int[]> dataList = new ArrayList<int[]>(1000);
+        List<int[]> dataList = new ArrayList<>(1000);
 
         try {
             BufferedReader input = smile.data.parser.IOUtils.getTestDataReader("transaction/kosarak.dat");
@@ -181,7 +181,7 @@ public void testKosarak() {
 
                 String[] s = line.split(" ");
 
-                Set<Integer> items = new HashSet<Integer>();
+                Set<Integer> items = new HashSet<>();
                 for (int i = 0; i < s.length; i++) {
                     items.add(Integer.parseInt(s[i]));
                 }

File: core/src/test/java/smile/association/TotalSupportTreeTest.java
Patch:
@@ -146,7 +146,7 @@ public void testGetFrequentItemsets_PrintStream() {
     public void testPima() {
         System.out.println("pima");
 
-        List<int[]> dataList = new ArrayList<int[]>(1000);
+        List<int[]> dataList = new ArrayList<>(1000);
 
         try {
             InputStream stream = getClass().getResourceAsStream("/smile/data/transaction/pima.D38.N768.C2");
@@ -199,7 +199,7 @@ public void testPima() {
     public void testKosarak() {
         System.out.println("kosarak");
 
-        List<int[]> dataList = new ArrayList<int[]>(1000);
+        List<int[]> dataList = new ArrayList<>(1000);
 
         try {
             InputStream stream = getClass().getResourceAsStream("/smile/data/transaction/kosarak.dat");
@@ -213,7 +213,7 @@ public void testKosarak() {
 
                 String[] s = line.split(" ");
 
-                Set<Integer> items = new HashSet<Integer>();
+                Set<Integer> items = new HashSet<>();
                 for (int i = 0; i < s.length; i++) {
                     items.add(Integer.parseInt(s[i]));
                 }

File: core/src/test/java/smile/classification/MaxentTest.java
Patch:
@@ -44,8 +44,8 @@ class Dataset {
     @SuppressWarnings("unused")
     Dataset load(String resource) {
         int p = 0;
-        ArrayList<int[]> x = new ArrayList<int[]>();
-        ArrayList<Integer> y = new ArrayList<Integer>();
+        ArrayList<int[]> x = new ArrayList<>();
+        ArrayList<Integer> y = new ArrayList<>();
 
         try (BufferedReader input = new BufferedReader(new InputStreamReader(this.getClass().getResourceAsStream(resource)))) {
             String[] words = input.readLine().split(" ");

File: core/src/test/java/smile/classification/NaiveBayesTest.java
Patch:
@@ -73,7 +73,7 @@ public NaiveBayesTest() {
 
         moviex = new double[x.length][];
         moviey = new int[y.length];
-        Bag<String> bag = new Bag<String>(feature);
+        Bag<String> bag = new Bag<>(feature);
         for (int i = 0; i < x.length; i++) {
             moviex[i] = bag.feature(x[i]);
             moviey[i] = y[i];
@@ -124,7 +124,7 @@ public void testPredict() {
                 for (int i = 0; i < k; i++) {
                     priori[i] = 1.0 / k;
                     for (int j = 0; j < p; j++) {
-                        ArrayList<Double> axi = new ArrayList<Double>();
+                        ArrayList<Double> axi = new ArrayList<>();
                         for (int m = 0; m < trainx.length; m++) {
                             if (trainy[m] == i) {
                                 axi.add(trainx[m][j]);

File: core/src/test/java/smile/classification/RBFNetworkTest.java
Patch:
@@ -83,7 +83,7 @@ public void testLearn() {
 
                 double[][] centers = new double[10][];
                 RadialBasisFunction[] basis = SmileUtils.learnGaussianRadialBasis(trainx, centers, 5.0);
-                RBFNetwork<double[]> rbf = new RBFNetwork<double[]>(trainx, trainy, new EuclideanDistance(), basis, centers);
+                RBFNetwork<double[]> rbf = new RBFNetwork<>(trainx, trainy, new EuclideanDistance(), basis, centers);
 
                 if (y[loocv.test[i]] != rbf.predict(x[loocv.test[i]]))
                     error++;
@@ -115,7 +115,7 @@ public void testSegment() {
             
             double[][] centers = new double[100][];
             RadialBasisFunction[] basis = SmileUtils.learnGaussianRadialBasis(x, centers, 5.0);
-            RBFNetwork<double[]> rbf = new RBFNetwork<double[]>(x, y, new EuclideanDistance(), basis, centers);
+            RBFNetwork<double[]> rbf = new RBFNetwork<>(x, y, new EuclideanDistance(), basis, centers);
             
             int error = 0;
             for (int i = 0; i < testx.length; i++) {
@@ -150,7 +150,7 @@ public void testUSPS() {
             
             double[][] centers = new double[200][];
             RadialBasisFunction basis = SmileUtils.learnGaussianRadialBasis(x, centers);
-            RBFNetwork<double[]> rbf = new RBFNetwork<double[]>(x, y, new EuclideanDistance(), new GaussianRadialBasis(8.0), centers);
+            RBFNetwork<double[]> rbf = new RBFNetwork<>(x, y, new EuclideanDistance(), new GaussianRadialBasis(8.0), centers);
                 
             int error = 0;
             for (int i = 0; i < testx.length; i++) {

File: core/src/test/java/smile/clustering/CLARANSTest.java
Patch:
@@ -72,7 +72,7 @@ public void testUSPS() {
             
             AdjustedRandIndex ari = new AdjustedRandIndex();
             RandIndex rand = new RandIndex();
-            CLARANS<double[]> clarans = new CLARANS<double[]>(x, new EuclideanDistance(), 10, 50, 8);
+            CLARANS<double[]> clarans = new CLARANS<>(x, new EuclideanDistance(), 10, 50, 8);
 
             double r = rand.measure(y, clarans.getClusterLabel());
             double r2 = ari.measure(y, clarans.getClusterLabel());

File: core/src/test/java/smile/clustering/DBScanTest.java
Patch:
@@ -93,7 +93,7 @@ public void testToy() {
             label[i] = 3;
         }
         
-        DBScan<double[]> dbscan = new DBScan<double[]>(data, new KDTree<double[]>(data, data), 200, 0.8);
+        DBScan<double[]> dbscan = new DBScan<>(data, new KDTree<>(data, data), 200, 0.8);
         System.out.println(dbscan);
         
         int[] size = dbscan.getClusterSize();

File: core/src/test/java/smile/clustering/MECTest.java
Patch:
@@ -72,7 +72,7 @@ public void testUSPS() {
             
             AdjustedRandIndex ari = new AdjustedRandIndex();
             RandIndex rand = new RandIndex();
-            MEC<double[]> mec = new MEC<double[]>(x, new EuclideanDistance(), 10, 8.0);
+            MEC<double[]> mec = new MEC<>(x, new EuclideanDistance(), 10, 8.0);
             
             double r = rand.measure(y, mec.getClusterLabel());
             double r2 = ari.measure(y, mec.getClusterLabel());

File: core/src/test/java/smile/feature/BagTest.java
Patch:
@@ -63,11 +63,11 @@ public void testUniquenessOfFeatures() {
         String[] featuresForBuildingStories = {"truck", "concrete", "foundation", "steel", "crane"};
         String testMessage = "This story is about a crane and a sparrow";
 
-        ArrayList<String> mergedFeatureLists = new ArrayList<String>();
+        ArrayList<String> mergedFeatureLists = new ArrayList<>();
         mergedFeatureLists.addAll(Arrays.asList(featuresForBirdStories));
         mergedFeatureLists.addAll(Arrays.asList(featuresForBuildingStories));
 
-        Bag<String> bag = new Bag<String>(mergedFeatureLists.toArray(new String[featuresForBirdStories.length + featuresForBuildingStories.length]));
+        Bag<String> bag = new Bag<>(mergedFeatureLists.toArray(new String[featuresForBirdStories.length + featuresForBuildingStories.length]));
 
         double[] result = bag.feature(testMessage.split(" "));
         assertEquals(9, result.length);
@@ -98,7 +98,7 @@ public void testFeature() {
             "perfectly", "masterpiece", "realistic", "flaws"
         };
         
-        Bag<String> bag = new Bag<String>(feature);
+        Bag<String> bag = new Bag<>(feature);
         
         double[][] x = new double[text.length][];
         for (int i = 0; i < text.length; i++) {

File: core/src/test/java/smile/feature/FeatureSetTest.java
Patch:
@@ -62,7 +62,7 @@ public void testAttributes() {
             AttributeDataset data = parser.parse(smile.data.parser.IOUtils.getTestDataFile("weka/regression/abalone.arff"));
             double[][] x = data.toArray(new double[data.size()][]);
             
-            FeatureSet<double[]> features = new FeatureSet<double[]>();
+            FeatureSet<double[]> features = new FeatureSet<>();
             features.add(new Nominal2Binary(data.attributes()));
             features.add(new NumericAttributeFeature(data.attributes(), 0.05, 0.95, x));
             Attribute[] attributes = features.attributes();
@@ -87,7 +87,7 @@ public void testF() {
             AttributeDataset data = parser.parse(smile.data.parser.IOUtils.getTestDataFile("weka/regression/abalone.arff"));
             double[][] x = data.toArray(new double[data.size()][]);
             
-            FeatureSet<double[]> features = new FeatureSet<double[]>();
+            FeatureSet<double[]> features = new FeatureSet<>();
             features.add(new Nominal2Binary(data.attributes()));
             features.add(new NumericAttributeFeature(data.attributes(), 0.05, 0.95, x));
             

File: core/src/test/java/smile/gap/GeneticAlgorithmTest.java
Patch:
@@ -87,7 +87,7 @@ public void testEvolve() {
             seeds[i] = new BitString(15, new Knapnack(), BitString.Crossover.UNIFORM, 1.0, 0.2);
         }
         
-        GeneticAlgorithm<BitString> instance = new GeneticAlgorithm<BitString>(seeds, GeneticAlgorithm.Selection.TOURNAMENT);
+        GeneticAlgorithm<BitString> instance = new GeneticAlgorithm<>(seeds, GeneticAlgorithm.Selection.TOURNAMENT);
         instance.setElitism(2);
         instance.setTournament(3, 0.95);
         

File: core/src/test/java/smile/neighbor/BKTreeSpeedTest.java
Patch:
@@ -32,7 +32,7 @@
  */
 public class BKTreeSpeedTest {
 
-    List<String> words = new ArrayList<String>();
+    List<String> words = new ArrayList<>();
     BKTree<String> bktree;
 
     public BKTreeSpeedTest() {
@@ -57,7 +57,7 @@ public BKTreeSpeedTest() {
         String[] data = words.toArray(new String[words.size()]);
 
         start = System.currentTimeMillis();
-        bktree = new BKTree<String>(new EditDistance(50, true));
+        bktree = new BKTree<>(new EditDistance(50, true));
         bktree.add(data);
         time = (System.currentTimeMillis() - start) / 1000.0;
         System.out.format("Building BK-tree: %.2fs%n", time);
@@ -86,7 +86,7 @@ public void tearDown() {
     public void testBKTreeSpeed() {
         System.out.println("BK-Tree range 1 speed");
         long start = System.currentTimeMillis();
-        List<Neighbor<String, String>> neighbors = new ArrayList<Neighbor<String, String>>();
+        List<Neighbor<String, String>> neighbors = new ArrayList<>();
         for (int i = 1000; i < 1100; i++) {
             bktree.range(words.get(i), 1, neighbors);
             neighbors.clear();

File: core/src/test/java/smile/neighbor/CoverTreeSpeedTest.java
Patch:
@@ -55,7 +55,7 @@ public CoverTreeSpeedTest() {
         System.out.format("Loading data: %.2fs%n", time);
 
         start = System.currentTimeMillis();
-        coverTree = new CoverTree<double[]>(x, new EuclideanDistance());
+        coverTree = new CoverTree<>(x, new EuclideanDistance());
         time = (System.currentTimeMillis() - start) / 1000.0;
         System.out.format("Building cover tree: %.2fs%n", time);
     }
@@ -96,7 +96,7 @@ public void testCoverTree() {
         System.out.format("10-NN: %.2fs%n", time);
 
         start = System.currentTimeMillis();
-        List<Neighbor<double[], double[]>> n = new ArrayList<Neighbor<double[], double[]>>();
+        List<Neighbor<double[], double[]>> n = new ArrayList<>();
         for (int i = 0; i < testx.length; i++) {
             coverTree.range(testx[i], 8.0, n);
             n.clear();

File: core/src/test/java/smile/neighbor/CoverTreeStringSpeedTest.java
Patch:
@@ -32,7 +32,7 @@
  */
 public class CoverTreeStringSpeedTest {
 
-    List<String> words = new ArrayList<String>();
+    List<String> words = new ArrayList<>();
     CoverTree<String> cover;
 
     public CoverTreeStringSpeedTest() {
@@ -57,7 +57,7 @@ public CoverTreeStringSpeedTest() {
         String[] data = words.toArray(new String[words.size()]);
 
         start = System.currentTimeMillis();
-        cover = new CoverTree<String>(data, new EditDistance(50, true));
+        cover = new CoverTree<>(data, new EditDistance(50, true));
         time = (System.currentTimeMillis() - start) / 1000.0;
         System.out.format("Building cover tree: %.2fs%n", time);
     }
@@ -85,7 +85,7 @@ public void tearDown() {
     public void testNaiveSpeed() {
         System.out.println("cover tree");
         long start = System.currentTimeMillis();
-        List<Neighbor<String, String>> neighbors = new ArrayList<Neighbor<String, String>>();
+        List<Neighbor<String, String>> neighbors = new ArrayList<>();
         for (int i = 1000; i < 1100; i++) {
             cover.range(words.get(i), 1, neighbors);
             neighbors.clear();

File: core/src/test/java/smile/neighbor/MPLSHSpeedTest.java
Patch:
@@ -79,7 +79,7 @@ public void testUSPS() {
         System.out.format("Loading USPS: %.2fs%n", time);
 
         start = System.currentTimeMillis();
-        MPLSH<double[]> lsh = new MPLSH<double[]>(256, 100, 3, 4.0);
+        MPLSH<double[]> lsh = new MPLSH<>(256, 100, 3, 4.0);
         for (double[] xi : x) {
             lsh.put(xi, xi);
         }
@@ -90,7 +90,7 @@ public void testUSPS() {
             train[i] = x[index[i]];
         }
 
-        LinearSearch<double[]> naive = new LinearSearch<double[]>(x, new EuclideanDistance());
+        LinearSearch<double[]> naive = new LinearSearch<>(x, new EuclideanDistance());
         lsh.learn(naive, train, 8.0);
         time = (System.currentTimeMillis() - start) / 1000.0;
         System.out.format("Building LSH: %.2fs%n", time);
@@ -110,7 +110,7 @@ public void testUSPS() {
         System.out.format("10-NN: %.2fs%n", time);
 
         start = System.currentTimeMillis();
-        List<Neighbor<double[], double[]>> n = new ArrayList<Neighbor<double[], double[]>>();
+        List<Neighbor<double[], double[]>> n = new ArrayList<>();
         for (int i = 0; i < testx.length; i++) {
             lsh.range(testx[i], 8.0, n);
             n.clear();

File: core/src/test/java/smile/regression/SVRTest.java
Patch:
@@ -77,7 +77,7 @@ public void testCPU() {
                 double[][] testx = Math.slice(datax, cv.test[i]);
                 double[] testy = Math.slice(datay, cv.test[i]);
 
-                SVR<double[]> svr = new SVR<double[]>(trainx, trainy, new PolynomialKernel(3, 1.0, 1.0), 0.1, 1.0);
+                SVR<double[]> svr = new SVR<>(trainx, trainy, new PolynomialKernel(3, 1.0, 1.0), 0.1, 1.0);
 
                 for (int j = 0; j < testx.length; j++) {
                     double r = testy[j] - svr.predict(testx[j]);

File: core/src/test/java/smile/taxonomy/TaxonomyTest.java
Patch:
@@ -113,7 +113,7 @@ public void testLowestCommonAncestor() {
     @Test
     public void testGetPathToRoot() {
         System.out.println("getPathToRoot");
-        LinkedList<Concept> expResult = new LinkedList<Concept>();
+        LinkedList<Concept> expResult = new LinkedList<>();
         expResult.addFirst(instance.getRoot());
         expResult.addFirst(ad);
         expResult.addFirst(a);
@@ -129,7 +129,7 @@ public void testGetPathToRoot() {
     @Test
     public void testGetPathFromRoot() {
         System.out.println("getPathToRoot");
-        LinkedList<Concept> expResult = new LinkedList<Concept>();
+        LinkedList<Concept> expResult = new LinkedList<>();
         expResult.add(instance.getRoot());
         expResult.add(ad);
         expResult.add(a);

File: data/src/main/java/smile/data/StringAttribute.java
Patch:
@@ -38,11 +38,11 @@ public class StringAttribute extends Attribute {
     /**
      * The list of unique string values of this attribute.
      */
-    private List<String> values = new ArrayList<String>();
+    private List<String> values = new ArrayList<>();
     /**
      * Map a string to an integer level.
      */
-    private Map<String, Integer> map = new HashMap<String, Integer>();
+    private Map<String, Integer> map = new HashMap<>();
 
     /**
      * Constructor.

File: data/src/main/java/smile/data/parser/BinarySparseDatasetParser.java
Patch:
@@ -122,7 +122,7 @@ public BinarySparseDataset parse(String name, InputStream stream) throws IOExcep
                 throw new IOException("Empty data source.");
            }
         
-           Set<Integer> items = new HashSet<Integer>();
+           Set<Integer> items = new HashSet<>();
            do {
                 line = line.trim();
                 if (line.isEmpty()) {

File: data/src/main/java/smile/data/parser/DelimitedTextParser.java
Patch:
@@ -348,7 +348,7 @@ private AttributeDataset parse(String name, Attribute[] attributes, BufferedRead
                 }
             }
 
-            Datum<double[]> datum = new Datum<double[]>(x, y);
+            Datum<double[]> datum = new Datum<>(x, y);
             datum.name = rowName;
             data.add(datum);
         }
@@ -378,7 +378,7 @@ private AttributeDataset parse(String name, Attribute[] attributes, BufferedRead
                 }
             }
 
-            Datum<double[]> datum = new Datum<double[]>(x, y);
+            Datum<double[]> datum = new Datum<>(x, y);
             datum.name = rowName;
             data.add(datum);
         }

File: data/src/main/java/smile/data/parser/IOUtils.java
Patch:
@@ -74,7 +74,7 @@ public static List<String> readLines(InputStream input, Charset charset) throws
      */
     public static List<String> readLines(Reader input) throws IOException {
         BufferedReader reader = input instanceof BufferedReader ? (BufferedReader) input : new BufferedReader(input);
-        List<String> list = new ArrayList<String>();
+        List<String> list = new ArrayList<>();
         String line = reader.readLine();
         while (line != null) {
             list.add(line);

File: data/src/main/java/smile/data/parser/microarray/GCTParser.java
Patch:
@@ -211,7 +211,7 @@ public AttributeDataset parse(String name, InputStream stream) throws IOExceptio
                 }
             }
 
-            Datum<double[]> datum = new Datum<double[]>(x);
+            Datum<double[]> datum = new Datum<>(x);
             datum.name = tokens[0];
             datum.description = tokens[1];
             data.add(datum);

File: data/src/main/java/smile/data/parser/microarray/PCLParser.java
Patch:
@@ -180,7 +180,7 @@ public AttributeDataset parse(String name, InputStream stream) throws IOExceptio
                 }
             }
 
-            Datum<double[]> datum = new Datum<double[]>(x);
+            Datum<double[]> datum = new Datum<>(x);
             datum.name = tokens[0];
             datum.description = tokens[1];
             datum.weight = Double.valueOf(tokens[2]);

File: data/src/main/java/smile/data/parser/microarray/RESParser.java
Patch:
@@ -210,7 +210,7 @@ public AttributeDataset parse(String name, InputStream stream) throws IOExceptio
                 x[j] = Double.valueOf(tokens[2*j+2]);
             }
 
-            Datum<double[]> datum = new Datum<double[]>(x);
+            Datum<double[]> datum = new Datum<>(x);
             datum.name = tokens[1];
             datum.description = tokens[0];
             data.add(datum);

File: data/src/main/java/smile/data/parser/microarray/TXTParser.java
Patch:
@@ -159,7 +159,7 @@ public AttributeDataset parse(String name, InputStream stream) throws IOExceptio
                 }
             }
 
-            Datum<double[]> datum = new Datum<double[]>(x);
+            Datum<double[]> datum = new Datum<>(x);
             datum.name = tokens[0];
             if (start == 2) {
                 datum.description = tokens[1];

File: demo/src/main/java/smile/demo/classification/ClassificationDemo.java
Patch:
@@ -82,7 +82,7 @@ public ClassificationDemo() {
         startButton.setActionCommand("startButton");
         startButton.addActionListener(this);
 
-        datasetBox = new JComboBox<String>();
+        datasetBox = new JComboBox<>();
         for (int i = 0; i < datasetName.length; i++) {
             datasetBox.addItem(datasetName[i]);
         }

File: demo/src/main/java/smile/demo/classification/RBFNetworkDemo.java
Patch:
@@ -64,7 +64,7 @@ public double[][] learn(double[] x, double[] y) {
 
         double[][] centers = new double[k][];
         RadialBasisFunction basis = SmileUtils.learnGaussianRadialBasis(data, centers);
-        RBFNetwork<double[]> rbf = new RBFNetwork<double[]>(data, label, new EuclideanDistance(), basis, centers);
+        RBFNetwork<double[]> rbf = new RBFNetwork<>(data, label, new EuclideanDistance(), basis, centers);
         
         for (int i = 0; i < label.length; i++) {
             label[i] = rbf.predict(data[i]);

File: demo/src/main/java/smile/demo/classification/SVMDemo.java
Patch:
@@ -76,7 +76,7 @@ public double[][] learn(double[] x, double[] y) {
         double[][] data = dataset[datasetIndex].toArray(new double[dataset[datasetIndex].size()][]);
         int[] label = dataset[datasetIndex].toArray(new int[dataset[datasetIndex].size()]);
         
-        SVM<double[]> svm = new SVM<double[]>(new GaussianKernel(gamma), C);
+        SVM<double[]> svm = new SVM<>(new GaussianKernel(gamma), C);
         svm.learn(data, label);
         svm.finish();
         

File: demo/src/main/java/smile/demo/clustering/CLARANSDemo.java
Patch:
@@ -76,7 +76,7 @@ public JComponent learn() {
         }
 
         long clock = System.currentTimeMillis();
-        CLARANS<double[]> clarans = new CLARANS<double[]>(dataset[datasetIndex], new EuclideanDistance(), clusterNumber, maxNeighbor, numLocal);
+        CLARANS<double[]> clarans = new CLARANS<>(dataset[datasetIndex], new EuclideanDistance(), clusterNumber, maxNeighbor, numLocal);
         System.out.format("CLARANS clusterings %d samples in %dms\n", dataset[datasetIndex].length, System.currentTimeMillis()-clock);
 
         PlotCanvas plot = ScatterPlot.plot(clarans.medoids(), '@');

File: demo/src/main/java/smile/demo/clustering/ClusteringDemo.java
Patch:
@@ -101,7 +101,7 @@ public ClusteringDemo() {
         startButton.setActionCommand("startButton");
         startButton.addActionListener(this);
 
-        datasetBox = new JComboBox<String>();
+        datasetBox = new JComboBox<>();
         for (int i = 0; i < datasetName.length; i++) {
             datasetBox.addItem(datasetName[i]);
         }

File: demo/src/main/java/smile/demo/clustering/DBScanDemo.java
Patch:
@@ -82,7 +82,7 @@ public JComponent learn() {
         }
 
         long clock = System.currentTimeMillis();
-        DBScan<double[]> dbscan = new DBScan<double[]>(dataset[datasetIndex], new EuclideanDistance(), minPts, range);
+        DBScan<double[]> dbscan = new DBScan<>(dataset[datasetIndex], new EuclideanDistance(), minPts, range);
         System.out.format("DBSCAN clusterings %d samples in %dms\n", dataset[datasetIndex].length, System.currentTimeMillis()-clock);
 
         JPanel pane = new JPanel(new GridLayout(1, 2));

File: demo/src/main/java/smile/demo/clustering/HierarchicalClusteringDemo.java
Patch:
@@ -48,7 +48,7 @@ public class HierarchicalClusteringDemo extends ClusteringDemo {
     JComboBox<String> linkageBox;
 
     public HierarchicalClusteringDemo() {
-        linkageBox = new JComboBox<String>();
+        linkageBox = new JComboBox<>();
         linkageBox.addItem("Single");
         linkageBox.addItem("Complete");
         linkageBox.addItem("UPGMA");

File: demo/src/main/java/smile/demo/clustering/MECDemo.java
Patch:
@@ -59,7 +59,7 @@ public JComponent learn() {
         }
 
         long clock = System.currentTimeMillis();
-        MEC<double[]> mec = new MEC<double[]>(dataset[datasetIndex], new EuclideanDistance(), clusterNumber, range);
+        MEC<double[]> mec = new MEC<>(dataset[datasetIndex], new EuclideanDistance(), clusterNumber, range);
         System.out.format("MEC clusterings %d samples in %dms\n", dataset[datasetIndex].length, System.currentTimeMillis()-clock);
 
         PlotCanvas plot = ScatterPlot.plot(dataset[datasetIndex], pointLegend);

File: demo/src/main/java/smile/demo/clustering/SIBDemo.java
Patch:
@@ -64,7 +64,7 @@ public SIBDemo() {
         startButton.setActionCommand("startButton");
         startButton.addActionListener(this);
 
-        datasetBox = new JComboBox<String>();
+        datasetBox = new JComboBox<>();
         for (int i = 0; i < datasetName.length; i++) {
             datasetBox.addItem(datasetName[i]);
         }

File: demo/src/main/java/smile/demo/interpolation/ScatterDataInterpolationDemo.java
Patch:
@@ -64,8 +64,8 @@ public ScatterDataInterpolationDemo() {
         add(canvas);
 
         double[][] ww = new double[26][26];
-        ArrayList<double[]> xx = new ArrayList<double[]>();
-        ArrayList<Double> zz = new ArrayList<Double>();
+        ArrayList<double[]> xx = new ArrayList<>();
+        ArrayList<Double> zz = new ArrayList<>();
         for (int i = 0; i <= 25; i++) {
             for (int j = 0; j <= 25; j++) {
                 if (Math.random() < 0.2)

File: demo/src/main/java/smile/demo/manifold/ManifoldDemo.java
Patch:
@@ -63,7 +63,7 @@ public ManifoldDemo() {
         startButton.setActionCommand("startButton");
         startButton.addActionListener(this);
 
-        datasetBox = new JComboBox<String>();
+        datasetBox = new JComboBox<>();
         for (int i = 0; i < datasetName.length; i++) {
             datasetBox.addItem(datasetName[i]);
         }

File: demo/src/main/java/smile/demo/mds/IsotonicMDSDemo.java
Patch:
@@ -70,7 +70,7 @@ public IsotonicMDSDemo() {
         startButton.setActionCommand("startButton");
         startButton.addActionListener(this);
 
-        datasetBox = new JComboBox<String>();
+        datasetBox = new JComboBox<>();
         for (int i = 0; i < datasetName.length; i++) {
             datasetBox.addItem(datasetName[i]);
         }

File: demo/src/main/java/smile/demo/mds/MDSDemo.java
Patch:
@@ -70,7 +70,7 @@ public MDSDemo() {
         startButton.setActionCommand("startButton");
         startButton.addActionListener(this);
 
-        datasetBox = new JComboBox<String>();
+        datasetBox = new JComboBox<>();
         for (int i = 0; i < datasetName.length; i++) {
             datasetBox.addItem(datasetName[i]);
         }

File: demo/src/main/java/smile/demo/mds/SammonMappingDemo.java
Patch:
@@ -70,7 +70,7 @@ public SammonMappingDemo() {
         startButton.setActionCommand("startButton");
         startButton.addActionListener(this);
 
-        datasetBox = new JComboBox<String>();
+        datasetBox = new JComboBox<>();
         for (int i = 0; i < datasetName.length; i++) {
             datasetBox.addItem(datasetName[i]);
         }

File: demo/src/main/java/smile/demo/projection/KPCADemo.java
Patch:
@@ -120,7 +120,7 @@ public JComponent learn() {
         plot.setTitle("PCA");
         pane.add(plot);
 
-        KPCA<double[]> kpca = new KPCA<double[]>(data, new GaussianKernel(gamma[datasetIndex]), 2);
+        KPCA<double[]> kpca = new KPCA<>(data, new GaussianKernel(gamma[datasetIndex]), 2);
 
         y = kpca.getCoordinates();
         plot = new PlotCanvas(Math.colMin(y), Math.colMax(y));
@@ -139,7 +139,7 @@ public JComponent learn() {
         pane.add(plot);
 
         clock = System.currentTimeMillis();
-        kpca = new KPCA<double[]>(data, new GaussianKernel(gamma[datasetIndex]), 3);
+        kpca = new KPCA<>(data, new GaussianKernel(gamma[datasetIndex]), 3);
         System.out.format("Learn KPCA from %d samples in %dms\n", data.length, System.currentTimeMillis() - clock);
 
         y = kpca.getCoordinates();

File: demo/src/main/java/smile/demo/projection/LDADemo.java
Patch:
@@ -65,7 +65,7 @@ public LDADemo() {
         startButton.setActionCommand("startButton");
         startButton.addActionListener(this);
 
-        datasetBox = new JComboBox<String>();
+        datasetBox = new JComboBox<>();
         for (int i = 0; i < datasetName.length; i++) {
             datasetBox.addItem(datasetName[i]);
         }

File: demo/src/main/java/smile/demo/projection/PCADemo.java
Patch:
@@ -39,7 +39,7 @@ public class PCADemo extends ProjectionDemo {
     JComboBox<String> corBox;
 
     public PCADemo() {
-        corBox = new JComboBox<String>();
+        corBox = new JComboBox<>();
         corBox.addItem("Covariance");
         corBox.addItem("Correlation");
         corBox.setSelectedIndex(0);

File: demo/src/main/java/smile/demo/projection/ProjectionDemo.java
Patch:
@@ -65,7 +65,7 @@ public ProjectionDemo() {
         startButton.setActionCommand("startButton");
         startButton.addActionListener(this);
 
-        datasetBox = new JComboBox<String>();
+        datasetBox = new JComboBox<>();
         for (int i = 0; i < datasetName.length; i++) {
             datasetBox.addItem(datasetName[i]);
         }

File: demo/src/main/java/smile/demo/stat/distribution/BernoulliDistributionDemo.java
Patch:
@@ -52,7 +52,7 @@ public class BernoulliDistributionDemo extends JPanel implements ChangeListener
     public BernoulliDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         for (int i = 1; i < 10; i+=2) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i/10.0)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/BetaDistributionDemo.java
Patch:
@@ -58,7 +58,7 @@ public class BetaDistributionDemo extends JPanel implements ChangeListener {
     public BetaDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         for (int i = 0; i <= 100; i+=20) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i / 10)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/BinomialDistributionDemo.java
Patch:
@@ -54,7 +54,7 @@ public class BinomialDistributionDemo extends JPanel implements ChangeListener {
     public BinomialDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> nLabelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> nLabelTable = new Hashtable<>();
         nLabelTable.put(1, new JLabel(String.valueOf(1)));
         for (int i = 10; i <= 50; i+=10) {
             nLabelTable.put(new Integer(i), new JLabel(String.valueOf(i)));
@@ -68,7 +68,7 @@ public BinomialDistributionDemo() {
         nSlider.setPaintTicks(true);
         nSlider.setPaintLabels(true);
 
-        Hashtable<Integer, JLabel> probLabelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> probLabelTable = new Hashtable<>();
         for (int i = 1; i < 10; i+=2) {
             probLabelTable.put(new Integer(i), new JLabel(String.valueOf(i/10.0)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/ChiSquareDistributionDemo.java
Patch:
@@ -55,7 +55,7 @@ public class ChiSquareDistributionDemo extends JPanel implements ChangeListener
     public ChiSquareDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         labelTable.put(1, new JLabel(String.valueOf(1)));
         for (int i = 5; i <= 20; i += 5) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i)));

File: demo/src/main/java/smile/demo/stat/distribution/ExponentialDistributionDemo.java
Patch:
@@ -56,7 +56,7 @@ public class ExponentialDistributionDemo extends JPanel implements ChangeListene
     public ExponentialDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         for (int i = 0; i <= 50; i+=10) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i/10)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/ExponentialFamilyMixtureDemo.java
Patch:
@@ -57,7 +57,7 @@ public ExponentialFamilyMixtureDemo() {
         for (int i = 1000; i < 2000; i++)
             data[i] = gamma.rand();
 
-        Vector<Mixture.Component> m = new Vector<Mixture.Component>();
+        Vector<Mixture.Component> m = new Vector<>();
         Mixture.Component c = new Mixture.Component();
         c.priori = 0.25;
         c.distribution = new GaussianDistribution(0.0, 1.0);
@@ -114,7 +114,7 @@ public static void main(String[] args) {
         for (int i = 1000; i < 2000; i++)
             data[i] = gamma.rand();
 
-        Vector<Mixture.Component> m = new Vector<Mixture.Component>();
+        Vector<Mixture.Component> m = new Vector<>();
         Mixture.Component c = new Mixture.Component();
         c.priori = 0.25;
         c.distribution = new GaussianDistribution(0.0, 1.0);

File: demo/src/main/java/smile/demo/stat/distribution/FDistributionDemo.java
Patch:
@@ -57,7 +57,7 @@ public class FDistributionDemo extends JPanel implements ChangeListener {
     public FDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         labelTable.put(1, new JLabel(String.valueOf(1)));
         for (int i = 20; i <= 100; i += 20) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i)));

File: demo/src/main/java/smile/demo/stat/distribution/GammaDistributionDemo.java
Patch:
@@ -58,7 +58,7 @@ public class GammaDistributionDemo extends JPanel implements ChangeListener {
     public GammaDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> shapeLabelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> shapeLabelTable = new Hashtable<>();
         for (int i = 0; i <= 10; i+=2) {
             shapeLabelTable.put(new Integer(i), new JLabel(String.valueOf(i)));
         }
@@ -71,7 +71,7 @@ public GammaDistributionDemo() {
         shapeSlider.setPaintTicks(true);
         shapeSlider.setPaintLabels(true);
 
-        Hashtable<Integer, JLabel> scaleLabelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> scaleLabelTable = new Hashtable<>();
         for (int i = 0; i <= 50; i+=10) {
             scaleLabelTable.put(new Integer(i), new JLabel(String.valueOf(i/10)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/GaussianDistributionDemo.java
Patch:
@@ -56,7 +56,7 @@ public class GaussianDistributionDemo extends JPanel implements ChangeListener {
     public GaussianDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         for (int i = 0; i <= 50; i+=10) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i/10)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/GeometricDistributionDemo.java
Patch:
@@ -52,7 +52,7 @@ public class GeometricDistributionDemo extends JPanel implements ChangeListener
     public GeometricDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         for (int i = 1; i < 10; i+=2) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i/10.0)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/HyperGeometricDistributionDemo.java
Patch:
@@ -55,7 +55,7 @@ public class HyperGeometricDistributionDemo extends JPanel implements ChangeList
     public HyperGeometricDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         labelTable.put(1, new JLabel(String.valueOf(1)));
         for (int i = 20; i <= 100; i+=20) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i)));

File: demo/src/main/java/smile/demo/stat/distribution/LogNormalDistributionDemo.java
Patch:
@@ -56,7 +56,7 @@ public class LogNormalDistributionDemo extends JPanel implements ChangeListener
     public LogNormalDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         for (int i = 0; i <= 20; i+=10) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i/10)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/LogisticDistributionDemo.java
Patch:
@@ -56,7 +56,7 @@ public class LogisticDistributionDemo extends JPanel implements ChangeListener {
     public LogisticDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         for (int i = 0; i <= 50; i+=10) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i/10)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/MultivariateGaussianDistributionDemo.java
Patch:
@@ -51,7 +51,7 @@ public class MultivariateGaussianDistributionDemo extends JPanel implements Chan
     public MultivariateGaussianDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         for (int i = 0; i <= 30; i+=10) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i/10.0)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/NegativeBinomialDistributionDemo.java
Patch:
@@ -54,7 +54,7 @@ public class NegativeBinomialDistributionDemo extends JPanel implements ChangeLi
     public NegativeBinomialDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> nLabelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> nLabelTable = new Hashtable<>();
         nLabelTable.put(1, new JLabel(String.valueOf(1)));
         for (int i = 10; i <= 50; i+=10) {
             nLabelTable.put(new Integer(i), new JLabel(String.valueOf(i)));
@@ -68,7 +68,7 @@ public NegativeBinomialDistributionDemo() {
         nSlider.setPaintTicks(true);
         nSlider.setPaintLabels(true);
 
-        Hashtable<Integer, JLabel> probLabelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> probLabelTable = new Hashtable<>();
         for (int i = 1; i < 10; i+=2) {
             probLabelTable.put(new Integer(i), new JLabel(String.valueOf(i/10.0)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/PoissonDistributionDemo.java
Patch:
@@ -52,7 +52,7 @@ public class PoissonDistributionDemo extends JPanel implements ChangeListener {
     public PoissonDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         for (int i = 0; i <= 10; i+=2) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/ShiftedGeometricDistributionDemo.java
Patch:
@@ -52,7 +52,7 @@ public class ShiftedGeometricDistributionDemo extends JPanel implements ChangeLi
     public ShiftedGeometricDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         for (int i = 1; i < 10; i+=2) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i/10.0)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/TDistributionDemo.java
Patch:
@@ -55,7 +55,7 @@ public class TDistributionDemo extends JPanel implements ChangeListener {
     public TDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         labelTable.put(1, new JLabel(String.valueOf(1)));
         for (int i = 5; i <= 20; i += 5) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i)));

File: demo/src/main/java/smile/demo/stat/distribution/WeibullDistributionDemo.java
Patch:
@@ -58,7 +58,7 @@ public class WeibullDistributionDemo extends JPanel implements ChangeListener {
     public WeibullDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> shapeLabelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> shapeLabelTable = new Hashtable<>();
         for (int i = 0; i <= 10; i+=2) {
             shapeLabelTable.put(new Integer(i), new JLabel(String.valueOf(i)));
         }
@@ -71,7 +71,7 @@ public WeibullDistributionDemo() {
         shapeSlider.setPaintTicks(true);
         shapeSlider.setPaintLabels(true);
 
-        Hashtable<Integer, JLabel> scaleLabelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> scaleLabelTable = new Hashtable<>();
         for (int i = 0; i <= 50; i+=10) {
             scaleLabelTable.put(new Integer(i), new JLabel(String.valueOf(i/10)));
         }

File: demo/src/main/java/smile/demo/util/ParameterParser.java
Patch:
@@ -82,7 +82,7 @@ private class Parameter {
      */
     public ParameterParser(String usage) {
         this.usage = usage;
-        parameters = new HashMap<String, Parameter>();
+        parameters = new HashMap<>();
     }
 
     /**
@@ -135,7 +135,7 @@ public String get(String name) {
      * @return a list of expanded name-value pair if monads are detected
      */
     private List<String> filterMonadics(String[] args) {// name-value for monads
-        List<String> filteredArgs = new ArrayList<String>();       // Y <- return List
+        List<String> filteredArgs = new ArrayList<>();       // Y <- return List
         for (String arg : args) {                        // iterate over args
             filteredArgs.add(arg);
             Parameter param = parameters.get(arg);
@@ -153,7 +153,7 @@ private List<String> filterMonadics(String[] args) {// name-value for monads
      * name.
      */
     public List<String> parse(String[] args) {    // merge args & defaults
-        List<String> extras = new ArrayList<String>();
+        List<String> extras = new ArrayList<>();
         List<String> filteredArgs = filterMonadics(args);          // detect and fill mons
         for (int i = 0; i < filteredArgs.size(); i++) {
             String key = filteredArgs.get(i);

File: demo/src/main/java/smile/demo/vq/VQDemo.java
Patch:
@@ -114,7 +114,7 @@ public VQDemo() {
         startButton.setActionCommand("startButton");
         startButton.addActionListener(this);
 
-        datasetBox = new JComboBox<String>();
+        datasetBox = new JComboBox<>();
         for (int i = 0; i < datasetName.length; i++) {
             datasetBox.addItem(datasetName[i]);
         }

File: math/src/main/java/smile/math/Math.java
Patch:
@@ -4017,7 +4017,7 @@ public static double[][] pow(double[][] x, double n) {
      * @return the same values as in x but with no repetitions.
      */
     public static int[] unique(int[] x) {
-        HashSet<Integer> hash = new HashSet<Integer>();
+        HashSet<Integer> hash = new HashSet<>();
         for (int i = 0; i < x.length; i++) {
             hash.add(x[i]);
         }
@@ -4038,7 +4038,7 @@ public static int[] unique(int[] x) {
      * @return the same values as in x but with no repetitions.
      */
     public static String[] unique(String[] x) {
-        HashSet<String> hash = new HashSet<String>(Arrays.asList(x));
+        HashSet<String> hash = new HashSet<>(Arrays.asList(x));
 
         String[] y = new String[hash.size()];
 

File: math/src/main/java/smile/math/SparseArray.java
Patch:
@@ -67,7 +67,7 @@ public SparseArray() {
      * @param size the number of nonzero entries in the matrix.
      */
     private SparseArray(int initialCapacity) {
-        array = new ArrayList<Entry>(initialCapacity);
+        array = new ArrayList<>(initialCapacity);
     }
 
     /**

File: math/src/main/java/smile/stat/distribution/DiscreteExponentialFamilyMixture.java
Patch:
@@ -158,7 +158,7 @@ public DiscreteExponentialFamilyMixture(List<Component> mixture, int[] data) {
             }
 
             // Maximization step
-            List<Component> newConfig = new ArrayList<Component>();
+            List<Component> newConfig = new ArrayList<>();
             for (int i = 0; i < m; i++)
                 newConfig.add(((DiscreteExponentialFamily) components.get(i).distribution).M(x, posteriori[i]));
 

File: math/src/main/java/smile/stat/distribution/DiscreteMixture.java
Patch:
@@ -47,15 +47,15 @@ public static class Component {
      * Constructor.
      */
     DiscreteMixture() {
-        components = new ArrayList<Component>();
+        components = new ArrayList<>();
     }
 
     /**
      * Constructor.
      * @param mixture a list of discrete distributions.
      */
     public DiscreteMixture(List<Component> mixture) {
-        components = new ArrayList<Component>();
+        components = new ArrayList<>();
         components.addAll(mixture);
 
         double sum = 0.0;

File: math/src/main/java/smile/stat/distribution/ExponentialFamilyMixture.java
Patch:
@@ -159,7 +159,7 @@ public ExponentialFamilyMixture(List<Component> mixture, double[] data) {
             }
 
             // Maximization step
-            ArrayList<Component> newConfig = new ArrayList<Component>();
+            ArrayList<Component> newConfig = new ArrayList<>();
             for (int i = 0; i < m; i++)
                 newConfig.add(((ExponentialFamily) components.get(i).distribution).M(x, posteriori[i]));
 

File: math/src/main/java/smile/stat/distribution/GaussianMixture.java
Patch:
@@ -70,7 +70,7 @@ public GaussianMixture(double[] data) {
         if (data.length < 20)
             throw new IllegalArgumentException("Too few samples.");
         
-        ArrayList<Component> mixture = new ArrayList<Component>();
+        ArrayList<Component> mixture = new ArrayList<>();
         Component c = new Component();
         c.priori = 1.0;
         c.distribution = new GaussianDistribution(data);

File: math/src/main/java/smile/stat/distribution/Mixture.java
Patch:
@@ -61,15 +61,15 @@ public static class Component {
      * Constructor.
      */
     Mixture() {
-        components = new ArrayList<Component>();
+        components = new ArrayList<>();
     }
 
     /**
      * Constructor.
      * @param mixture a list of distributions.
      */
     public Mixture(List<Component> mixture) {
-        components = new ArrayList<Component>();
+        components = new ArrayList<>();
         components.addAll(mixture);
 
         double sum = 0.0;

File: math/src/main/java/smile/stat/distribution/MultivariateExponentialFamilyMixture.java
Patch:
@@ -155,7 +155,7 @@ public MultivariateExponentialFamilyMixture(List<Component> mixture, double[][]
             }
 
             // Maximization step
-            ArrayList<Component> newConfig = new ArrayList<Component>();
+            ArrayList<Component> newConfig = new ArrayList<>();
             for (int i = 0; i < m; i++)
                 newConfig.add(((MultivariateExponentialFamily) components.get(i).distribution).M(x, posteriori[i]));
 

File: math/src/main/java/smile/stat/distribution/MultivariateGaussianMixture.java
Patch:
@@ -168,7 +168,7 @@ public MultivariateGaussianMixture(double[][] data, boolean diagonal) {
         if (data.length < 20)
             throw new IllegalArgumentException("Too few samples.");
 
-        ArrayList<Component> mixture = new ArrayList<Component>();
+        ArrayList<Component> mixture = new ArrayList<>();
         Component c = new Component();
         c.priori = 1.0;
         c.distribution = new MultivariateGaussianDistribution(data, diagonal);

File: math/src/main/java/smile/stat/distribution/MultivariateMixture.java
Patch:
@@ -46,15 +46,15 @@ public static class Component {
      * Construct an empty Mixture.
      */
     MultivariateMixture() {
-        components = new ArrayList<Component>();
+        components = new ArrayList<>();
     }
 
     /**
      * Constructor.
      * @param mixture a list of multivariate distributions.
      */
     public MultivariateMixture(List<Component> mixture) {
-        components = new ArrayList<Component>();
+        components = new ArrayList<>();
         components.addAll(mixture);
     }
 

File: math/src/test/java/smile/math/distance/JaccardDistanceTest.java
Patch:
@@ -56,13 +56,13 @@ public void tearDown() {
     @Test
     public void testDistance() {
         System.out.println("distance");
-        Set<Integer> a = new HashSet<Integer>();
+        Set<Integer> a = new HashSet<>();
         a.add(1);
         a.add(2);
         a.add(3);
         a.add(4);
 
-        Set<Integer> b = new HashSet<Integer>();
+        Set<Integer> b = new HashSet<>();
         b.add(3);
         b.add(4);
         b.add(5);

File: math/src/test/java/smile/sort/HeapSelectTest.java
Patch:
@@ -55,7 +55,7 @@ public void tearDown() {
     @Test
     public void testSelect() {
         System.out.println("HeapSelect");
-        HeapSelect<Integer> instance = new HeapSelect<Integer>(new Integer[10]);
+        HeapSelect<Integer> instance = new HeapSelect<>(new Integer[10]);
         for (int i = 0; i < 1000; i++) {
             instance.add(i);
             if (i > 10) {
@@ -65,7 +65,7 @@ public void testSelect() {
             }
         }
 
-        instance = new HeapSelect<Integer>(new Integer[10]);
+        instance = new HeapSelect<>(new Integer[10]);
         for (int i = 0; i < 1000; i++) {
             instance.add(1000-i);
             if (i >= 9) {
@@ -82,7 +82,7 @@ public void testSelect() {
     @Test
     public void testSelectBig() {
         System.out.println("HeapSelect Big");
-        HeapSelect<Double> instance = new HeapSelect<Double>(new Double[10]);
+        HeapSelect<Double> instance = new HeapSelect<>(new Double[10]);
         for (int i = 0; i < 100000000; i++) {
             instance.add(Math.random());
         }

File: math/src/test/java/smile/stat/distribution/ExponentialFamilyMixtureTest.java
Patch:
@@ -71,7 +71,7 @@ public void testEM() {
         for (int i = 1000; i < 2000; i++)
             data[i] = gamma.rand();
 
-        Vector<Mixture.Component> m = new Vector<Mixture.Component>();
+        Vector<Mixture.Component> m = new Vector<>();
         Mixture.Component c = new Mixture.Component();
         c.priori = 0.25;
         c.distribution = new GaussianDistribution(0.0, 1.0);

File: nlp/src/main/java/smile/nlp/SimpleText.java
Patch:
@@ -37,7 +37,7 @@ public class SimpleText extends Text implements TextTerms, AnchorText {
     /**
      * The term frequency.
      */
-    private HashMap<String, Integer> freq = new HashMap<String, Integer>();
+    private HashMap<String, Integer> freq = new HashMap<>();
     /**
      * The maximum term frequency over all terms in the documents;
      */

File: nlp/src/main/java/smile/nlp/Trie.java
Patch:
@@ -55,7 +55,7 @@ public class Node {
         public Node(K key) {
             this.key = key;
             this.value = null;
-            this.children = new LinkedList<Node>();
+            this.children = new LinkedList<>();
         }
         
         public K getKey() {
@@ -117,15 +117,15 @@ public void addChild(K[] key, V value, int index) {
      * Constructor.
      */
     public Trie() {
-        root = new HashMap<K, Node>();
+        root = new HashMap<>();
     }
 
     /**
      * Constructor.
      * @param initialCapacity the initial capacity of root node.
      */
     public Trie(int initialCapacity) {
-        root = new HashMap<K, Node>(initialCapacity);
+        root = new HashMap<>(initialCapacity);
     }
 
     /**

File: nlp/src/main/java/smile/nlp/collocation/BigramCollocationFinder.java
Patch:
@@ -65,7 +65,7 @@ public BigramCollocationFinder(int minFreq) {
      */
     public BigramCollocation[] find(Corpus corpus, int k) {
         BigramCollocation[] bigrams = new BigramCollocation[k];
-        HeapSelect<BigramCollocation> heap = new HeapSelect<BigramCollocation>(bigrams);
+        HeapSelect<BigramCollocation> heap = new HeapSelect<>(bigrams);
         
         Iterator<Bigram> iterator = corpus.getBigrams();
         while (iterator.hasNext()) {
@@ -106,7 +106,7 @@ public BigramCollocation[] find(Corpus corpus, double p) {
 
         double cutoff = chisq.quantile(p);
         
-        ArrayList<BigramCollocation> bigrams = new ArrayList<BigramCollocation>();
+        ArrayList<BigramCollocation> bigrams = new ArrayList<>();
 
         Iterator<Bigram> iterator = corpus.getBigrams();
         while (iterator.hasNext()) {

File: nlp/src/main/java/smile/nlp/dictionary/EnglishDictionary.java
Patch:
@@ -43,7 +43,7 @@ public enum EnglishDictionary implements Dictionary {
      * text, in which each line is a word.
      */
     private EnglishDictionary(String resource) {
-        dict = new HashSet<String>();
+        dict = new HashSet<>();
 
         try (BufferedReader input = new BufferedReader(new InputStreamReader(this.getClass().getResourceAsStream(resource)))) {
         

File: nlp/src/main/java/smile/nlp/dictionary/EnglishPunctuations.java
Patch:
@@ -32,7 +32,7 @@ public class EnglishPunctuations implements Punctuations {
     /**
      * A set of punctuation marks.
      */
-    private HashSet<String> dict = new HashSet<String>(50);
+    private HashSet<String> dict = new HashSet<>(50);
 
     /**
      * Constructor.

File: nlp/src/main/java/smile/nlp/dictionary/EnglishStopWords.java
Patch:
@@ -54,7 +54,7 @@ public enum EnglishStopWords implements StopWords {
      * Constructor.
      */
     private EnglishStopWords(String resource) {
-        dict = new HashSet<String>();
+        dict = new HashSet<>();
 
         try (BufferedReader input = new BufferedReader(new InputStreamReader(this.getClass().getResourceAsStream(resource)))) {
             String line = null;

File: nlp/src/main/java/smile/nlp/dictionary/SimpleDictionary.java
Patch:
@@ -41,7 +41,7 @@ public class SimpleDictionary implements Dictionary {
      * text, in which each line is a word.
      */
     public SimpleDictionary(String resource) {
-        dict = new HashSet<String>();
+        dict = new HashSet<>();
 
         File file = new File(resource);
         try (BufferedReader input = file.exists() ?

File: nlp/src/main/java/smile/nlp/pos/EnglishPOSLexicon.java
Patch:
@@ -34,7 +34,7 @@ public class EnglishPOSLexicon {
     /**
      * A list of English words with POS tags.
      */
-    private static final HashMap<String, PennTreebankPOS[]> dict = new HashMap<String, PennTreebankPOS[]>();
+    private static final HashMap<String, PennTreebankPOS[]> dict = new HashMap<>();
 
     /**
      * The part-of-speech.txt file contains is a combination of

File: nlp/src/main/java/smile/nlp/pos/PennTreebankPOS.java
Patch:
@@ -413,7 +413,7 @@ public String toString() {
      */
     private static final Map<String, String> map;
     static {
-        map = new HashMap<String, String>();
+        map = new HashMap<>();
 
         map.put(".", "SENT");
         map.put("?", "SENT");

File: nlp/src/main/java/smile/nlp/stemmer/LancasterStemmer.java
Patch:
@@ -56,7 +56,7 @@ public class LancasterStemmer implements Stemmer {
         /**
          * Load rules from Lancaster_rules.txt
          */
-        RULES = new ArrayList<String>();
+        RULES = new ArrayList<>();
 
         try (BufferedReader input = new BufferedReader(new InputStreamReader(LancasterStemmer.class.getResourceAsStream("/smile/nlp/stemmer/Lancaster_rules.txt")))) {
             String line = null;

File: nlp/src/main/java/smile/nlp/tokenizer/BreakIteratorSentenceSplitter.java
Patch:
@@ -49,7 +49,7 @@ public BreakIteratorSentenceSplitter(Locale locale) {
     @Override
     public String[] split(String text) {
         boundary.setText(text);
-        ArrayList<String> sentences = new ArrayList<String>();
+        ArrayList<String> sentences = new ArrayList<>();
         int start = boundary.first();
         for (int end = boundary.next();
                 end != BreakIterator.DONE;

File: nlp/src/main/java/smile/nlp/tokenizer/BreakIteratorTokenizer.java
Patch:
@@ -49,7 +49,7 @@ public BreakIteratorTokenizer(Locale locale) {
     @Override
     public String[] split(String text) {
         boundary.setText(text);
-        ArrayList<String> words = new ArrayList<String>();
+        ArrayList<String> words = new ArrayList<>();
         int start = boundary.first();
         int end = boundary.next();
 

File: nlp/src/main/java/smile/nlp/tokenizer/EnglishAbbreviations.java
Patch:
@@ -36,7 +36,7 @@ class EnglishAbbreviations {
     private static final HashSet<String> DICTIONARY;
 
     static {
-        DICTIONARY = new HashSet<String>();
+        DICTIONARY = new HashSet<>();
 
         try (BufferedReader input = new BufferedReader(new InputStreamReader(EnglishAbbreviations.class.getResourceAsStream("/smile/nlp/tokenizer/abbreviations_en.txt")))) {
             String line = null;

File: nlp/src/main/java/smile/nlp/tokenizer/SimpleSentenceSplitter.java
Patch:
@@ -93,7 +93,7 @@ public static SimpleSentenceSplitter getInstance() {
 
     @Override
     public String[] split(String text) {
-        ArrayList<String> sentences = new ArrayList<String>();
+        ArrayList<String> sentences = new ArrayList<>();
 
         // The number of words in the sentence.
         int len = 0;

File: nlp/src/main/java/smile/nlp/tokenizer/SimpleTokenizer.java
Patch:
@@ -144,7 +144,7 @@ public String[] split(String text) {
             }
         }
         
-        ArrayList<String> result = new ArrayList<String>();
+        ArrayList<String> result = new ArrayList<>();
         for (String token : words) {
             if (!token.isEmpty()) {
                 result.add(token);

File: nlp/src/test/java/smile/nlp/collocation/AprioriPhraseExtractorTest.java
Patch:
@@ -71,7 +71,7 @@ public void testExtract() throws FileNotFoundException {
         
         PorterStemmer stemmer = new PorterStemmer();
         SimpleTokenizer tokenizer = new SimpleTokenizer();
-        ArrayList<String[]> sentences = new ArrayList<String[]>();
+        ArrayList<String[]> sentences = new ArrayList<>();
         for (String paragraph : SimpleParagraphSplitter.getInstance().split(text)) {
             for (String s : SimpleSentenceSplitter.getInstance().split(paragraph)) {
                 String[] sentence = tokenizer.split(s);

File: plot/src/main/java/smile/plot/Axis.java
Patch:
@@ -321,7 +321,7 @@ public Axis setRotation(double rotation) {
      */
     public Axis addLabel(String label, double location) {
         if (labels == null) {
-            labels = new HashMap<String, Double>();
+            labels = new HashMap<>();
         }
 
         labels.put(label, location);
@@ -340,7 +340,7 @@ public Axis addLabel(String[] label, double[] location) {
         }
 
         if (labels == null) {
-            labels = new HashMap<String, Double>();
+            labels = new HashMap<>();
         }
 
         for (int i = 0; i < label.length; i++) {

File: plot/src/main/java/smile/plot/Contour.java
Patch:
@@ -87,7 +87,7 @@ class Isoline {
         /**
          * The coordinates of points along the contour line.
          */
-        List<double[]> points = new ArrayList<double[]>();
+        List<double[]> points = new ArrayList<>();
         /**
          * The level value of contour line.
          */
@@ -453,7 +453,7 @@ private void init() {
         zMin = Math.min(z);
         zMax = Math.max(z);
 
-        contours = new ArrayList<Isoline>(numLevels);
+        contours = new ArrayList<>(numLevels);
 
         if (logScale && zMin <= 0.0) {
             throw new IllegalArgumentException("Log scale is not support for non-positive data");

File: plot/src/main/java/smile/plot/PlotCanvas.java
Patch:
@@ -135,7 +135,7 @@ public class PlotCanvas extends JPanel {
     /**
      * The shapes in the canvas, e.g. label, plots, etc.
      */
-    private List<Shape> shapes = new ArrayList<Shape>();
+    private List<Shape> shapes = new ArrayList<>();
     /**
      * The real canvas for plots.
      */

File: plot/src/main/java/smile/plot/ScatterPlot.java
Patch:
@@ -161,7 +161,7 @@ public ScatterPlot(double[][] data, int[] y, char[] legends, Color[] palette) {
         int[] id = Math.unique(y);
         Arrays.sort(id);
 
-        classLookupTable = new HashMap<Integer, Integer>(id.length);
+        classLookupTable = new HashMap<>(id.length);
 
         for (int i = 0; i < id.length; i++) {
             classLookupTable.put(id[i], i);

File: plot/src/main/java/smile/swing/FileChooser.java
Patch:
@@ -283,7 +283,7 @@ public static class SimpleFileFilter extends FileFilter {
         /**
          * The file extensions in lower case.
          */
-        private TreeSet<String> filters = new TreeSet<String>();
+        private TreeSet<String> filters = new TreeSet<>();
         /**
          * The human readable description of this filter.
          */

File: plot/src/main/java/smile/swing/FontChooser.java
Patch:
@@ -227,7 +227,7 @@ private JTextField getFontSizeTextField() {
 
     private JList<String> getFontFamilyList() {
         if (fontNameList == null) {
-            fontNameList = new JList<String>(getFontFamilies());
+            fontNameList = new JList<>(getFontFamilies());
             fontNameList.setSelectionMode(ListSelectionModel.SINGLE_SELECTION);
             fontNameList.addListSelectionListener(
                     new ListSelectionHandler(getFontFamilyTextField()));
@@ -240,7 +240,7 @@ private JList<String> getFontFamilyList() {
 
     private JList<String> getFontStyleList() {
         if (fontStyleList == null) {
-            fontStyleList = new JList<String>(getFontStyleNames());
+            fontStyleList = new JList<>(getFontStyleNames());
             fontStyleList.setSelectionMode(ListSelectionModel.SINGLE_SELECTION);
             fontStyleList.addListSelectionListener(
                     new ListSelectionHandler(getFontStyleTextField()));
@@ -253,7 +253,7 @@ private JList<String> getFontStyleList() {
 
     private JList<String> getFontSizeList() {
         if (fontSizeList == null) {
-            fontSizeList = new JList<String>(this.fontSizeStrings);
+            fontSizeList = new JList<>(this.fontSizeStrings);
             fontSizeList.setSelectionMode(ListSelectionModel.SINGLE_SELECTION);
             fontSizeList.addListSelectionListener(
                     new ListSelectionHandler(getFontSizeTextField()));

File: math/src/test/java/smile/stat/distribution/ExponentialFamilyMixtureTest.java
Patch:
@@ -16,6 +16,8 @@
 
 package smile.stat.distribution;
 
+import java.util.ArrayList;
+import java.util.List;
 import java.util.Vector;
 import org.junit.After;
 import org.junit.AfterClass;
@@ -71,7 +73,7 @@ public void testEM() {
         for (int i = 1000; i < 2000; i++)
             data[i] = gamma.rand();
 
-        Vector<Mixture.Component> m = new Vector<Mixture.Component>();
+        List<Mixture.Component> m = new ArrayList<>();
         Mixture.Component c = new Mixture.Component();
         c.priori = 0.25;
         c.distribution = new GaussianDistribution(0.0, 1.0);

File: nlp/src/main/java/smile/nlp/NGram.java
Patch:
@@ -57,7 +57,7 @@ public NGram(String[] words, int freq) {
 
     @Override
     public String toString() {
-    	StringBuffer sb = new StringBuffer();
+    	StringBuilder sb = new StringBuilder();
     	sb.append('(')
           .append(Arrays.toString(words))
           .append(", ")

File: data/src/main/java/smile/data/parser/ArffParser.java
Patch:
@@ -501,7 +501,7 @@ private Datum<double[]> getSparseInstance(StreamTokenizer tokenizer, Attribute[]
             
             String s = tokenizer.sval.trim();
             if (index < 0) {
-                index = Integer.valueOf(s);
+                index = Integer.parseInt(s);
                 if (index < 0 || index >= attributes.length) {
                     throw new ParseException("Invalid attribute index: " + index, tokenizer.lineno());
                 }

File: data/src/main/java/smile/data/parser/SparseDatasetParser.java
Patch:
@@ -173,9 +173,9 @@ public SparseDataset parse(String name, InputStream stream) throws IOException,
                     throw new ParseException("Invalid number of tokens.", nrow);
                 }
 
-                int d = Integer.valueOf(tokens[0]) - arrayIndexOrigin;
-                int w = Integer.valueOf(tokens[1]) - arrayIndexOrigin;
-                double c = Double.valueOf(tokens[2]);
+                int d = Integer.parseInt(tokens[0]) - arrayIndexOrigin;
+                int w = Integer.parseInt(tokens[1]) - arrayIndexOrigin;
+                double c = Double.parseDouble(tokens[2]);
                 sparse.set(d, w, c);
 
                 line = reader.readLine();

File: data/src/main/java/smile/data/parser/microarray/GCTParser.java
Patch:
@@ -168,8 +168,8 @@ public AttributeDataset parse(String name, InputStream stream) throws IOExceptio
             throw new IOException("Invalid data size inforamation.");            
         }
         
-        int n = Integer.valueOf(tokens[0]);
-        int p = Integer.valueOf(tokens[1]);
+        int n = Integer.parseInt(tokens[0]);
+        int p = Integer.parseInt(tokens[1]);
         if (n <= 0 || p <= 0) {
             throw new IOException(String.format("Invalid data size %d x %d.", n, p));                        
         }

File: data/src/main/java/smile/data/parser/microarray/RESParser.java
Patch:
@@ -187,7 +187,7 @@ public AttributeDataset parse(String name, InputStream stream) throws IOExceptio
             throw new IOException("Premature end of file.");
         }
 
-        int n = Integer.valueOf(line);
+        int n = Integer.parseInt(line);
         if (n <= 0) {
             throw new IOException("Invalid number of rows: " + n);            
         }

File: plot/src/main/java/smile/swing/table/IntegerArrayCellEditor.java
Patch:
@@ -75,7 +75,7 @@ public Object stringToValue(String string) throws ParseException {
 
                 int[] data = new int[items.length];
                 for (int i = 0; i < data.length; i++) {
-                    data[i] = Integer.valueOf(items[i].trim());
+                    data[i] = Integer.parseInt(items[i].trim());
                 }
 
                 return data;

File: core/src/main/java/smile/association/ARM.java
Patch:
@@ -125,7 +125,7 @@ public long learn(double confidence, PrintStream out) {
      * @param confidence the confidence threshold for association rules.
      */
     public List<AssociationRule> learn(double confidence) {
-        List<AssociationRule> list = new ArrayList<AssociationRule>();
+        List<AssociationRule> list = new ArrayList<>();
         ttree = fim.buildTotalSupportTree();
         for (int i = 0; i < ttree.root.children.length; i++) {
             if (ttree.root.children[i] != null) {

File: core/src/main/java/smile/association/FPTree.java
Patch:
@@ -89,7 +89,7 @@ class Node {
          */
         void add(int index, int end, int[] itemset, int support) {
             if (children == null) {
-                children = new HashMap<Integer, Node>();
+                children = new HashMap<>();
             }
             
             Node child = children.get(itemset[index]);
@@ -114,7 +114,7 @@ void add(int index, int end, int[] itemset, int support) {
          */
         void append(int index, int end, int[] itemset, int support) {
             if (children == null) {
-                children = new HashMap<Integer, Node>();
+                children = new HashMap<>();
             }
             
             if (index >= maxItemSetSize) {

File: core/src/main/java/smile/association/TotalSupportTree.java
Patch:
@@ -168,7 +168,7 @@ private int getSupport(int[] itemset, int index, Node node) {
      * @return the list of frequent item sets
      */
     public List<ItemSet> getFrequentItemsets() {
-        List<ItemSet> list = new ArrayList<ItemSet>();
+        List<ItemSet> list = new ArrayList<>();
         getFrequentItemsets(null, list);
         return list;
     }

File: core/src/main/java/smile/classification/DecisionTree.java
Patch:
@@ -511,7 +511,7 @@ public boolean findBestSplit() {
                 }
             } else {
 
-                List<SplitTask> tasks = new ArrayList<SplitTask>(mtry);
+                List<SplitTask> tasks = new ArrayList<>(mtry);
                 for (int j = 0; j < mtry; j++) {
                     tasks.add(new SplitTask(n, count, impurity, variables[j]));
                 }
@@ -944,7 +944,7 @@ public DecisionTree(Attribute[] attributes, double[][] x, int[] y, int maxNodes,
         }
 
         // Priority queue for best-first tree growing.
-        PriorityQueue<TrainNode> nextSplits = new PriorityQueue<TrainNode>();
+        PriorityQueue<TrainNode> nextSplits = new PriorityQueue<>();
 
         int n = y.length;
         int[] count = new int[k];

File: core/src/main/java/smile/classification/RBFNetwork.java
Patch:
@@ -186,9 +186,9 @@ public RBFNetwork<T> train(T[] x, int[] y) {
             GaussianRadialBasis gaussian = SmileUtils.learnGaussianRadialBasis(x, centers, distance);
             
             if (rbf == null) {
-                return new RBFNetwork<T>(x, y, distance, gaussian, centers, normalized);
+                return new RBFNetwork<>(x, y, distance, gaussian, centers, normalized);
             } else {
-                return new RBFNetwork<T>(x, y, distance, rbf, centers, normalized);
+                return new RBFNetwork<>(x, y, distance, rbf, centers, normalized);
             }
         }
         
@@ -201,7 +201,7 @@ public RBFNetwork<T> train(T[] x, int[] y) {
          * @return a trained RBF network
          */
         public RBFNetwork<T> train(T[] x, int[] y, T[] centers) {
-            return new RBFNetwork<T>(x, y, distance, rbf, centers, normalized);
+            return new RBFNetwork<>(x, y, distance, rbf, centers, normalized);
         }
     }
     

File: core/src/main/java/smile/clustering/BIRCH.java
Patch:
@@ -387,9 +387,9 @@ public int partition(int k) {
      * @return the number of non-outlier leaves.
      */
     public int partition(int k, int minPts) {
-        ArrayList<Leaf> leaves = new ArrayList<Leaf>();
-        ArrayList<double[]> centers = new ArrayList<double[]>();
-        Queue<Node> queue = new LinkedList<Node>();
+        ArrayList<Leaf> leaves = new ArrayList<>();
+        ArrayList<double[]> centers = new ArrayList<>();
+        Queue<Node> queue = new LinkedList<>();
         queue.offer(root);
 
         for (Node node = queue.poll(); node != null; node = queue.poll()) {

File: core/src/main/java/smile/clustering/CLARANS.java
Patch:
@@ -143,7 +143,7 @@ public CLARANS(T[] data, Distance<T> distance, int k, int maxNeighbor, int numLo
         this.numLocal = numLocal;
         this.maxNeighbor = maxNeighbor;
         
-        List<CLARANSTask> tasks = new ArrayList<CLARANSTask>();
+        List<CLARANSTask> tasks = new ArrayList<>();
         for (int i = 0; i < numLocal; i++) {
             tasks.add(new CLARANSTask(data));
         }

File: core/src/main/java/smile/clustering/DeterministicAnnealing.java
Patch:
@@ -92,7 +92,7 @@ public DeterministicAnnealing(double[][] data, int Kmax, double alpha) {
 
         int np = MulticoreExecutor.getThreadPoolSize();
         if (n >= 1000 && np >= 2) {
-            tasks = new ArrayList<UpdateThread>(np + 1);
+            tasks = new ArrayList<>(np + 1);
             int step = n / np;
             if (step < 100) {
                 step = 100;
@@ -107,7 +107,7 @@ public DeterministicAnnealing(double[][] data, int Kmax, double alpha) {
             }
             tasks.add(new UpdateThread(data, centroids, posteriori, priori, start, n));
             
-            ctasks = new ArrayList<CentroidThread>(2 * Kmax);
+            ctasks = new ArrayList<>(2 * Kmax);
             for (int i = 0; i < 2*Kmax; i++) {
                 ctasks.add(new CentroidThread(data, centroids, posteriori, priori, i));
             }

File: core/src/main/java/smile/clustering/GMeans.java
Patch:
@@ -81,7 +81,7 @@ public GMeans(double[][] data, int kmax) {
 
         BBDTree bbd = new BBDTree(data);
         while (k < kmax) {
-            ArrayList<double[]> centers = new ArrayList<double[]>();
+            ArrayList<double[]> centers = new ArrayList<>();
             double[] score = new double[k];
             KMeans[] kmeans = new KMeans[k];
             

File: core/src/main/java/smile/clustering/HierarchicalClustering.java
Patch:
@@ -185,7 +185,7 @@ public int[] partition(double h) {
      */
     private void bfs(int[] membership, int cluster, int id) {
         int n = merge.length + 1;
-        Queue<Integer> queue = new LinkedList<Integer>();
+        Queue<Integer> queue = new LinkedList<>();
         queue.offer(cluster);
 
         for (Integer i = queue.poll(); i != null; i = queue.poll()) {

File: core/src/main/java/smile/clustering/KMeans.java
Patch:
@@ -226,7 +226,7 @@ public KMeans(double[][] data, int k, int maxIter, int runs) {
 
         BBDTree bbd = new BBDTree(data);
 
-        List<KMeansThread> tasks = new ArrayList<KMeansThread>();
+        List<KMeansThread> tasks = new ArrayList<>();
         for (int i = 0; i < runs; i++) {
             tasks.add(new KMeansThread(bbd, data, k, maxIter));
         }
@@ -321,7 +321,7 @@ public static KMeans lloyd(double[][] data, int k, int maxIter) {
         int np = MulticoreExecutor.getThreadPoolSize();
         List<LloydThread> tasks = null;
         if (n >= 1000 && np >= 2) {
-            tasks = new ArrayList<LloydThread>(np + 1);
+            tasks = new ArrayList<>(np + 1);
             int step = n / np;
             if (step < 100) {
                 step = 100;

File: core/src/main/java/smile/clustering/SIB.java
Patch:
@@ -186,7 +186,7 @@ public SIB(double[][] data, int k, int maxIter, int runs) {
             throw new IllegalArgumentException("Invalid number of runs: " + runs);
         }
 
-        List<SIBThread> tasks = new ArrayList<SIBThread>();
+        List<SIBThread> tasks = new ArrayList<>();
         for (int i = 0; i < runs; i++) {
             tasks.add(new SIBThread(data, k, maxIter));
         }
@@ -395,7 +395,7 @@ public SIB(SparseDataset data, int k, int maxIter, int runs) {
             throw new IllegalArgumentException("Invalid number of runs: " + runs);
         }
 
-        List<SIBThread> tasks = new ArrayList<SIBThread>();
+        List<SIBThread> tasks = new ArrayList<>();
         for (int i = 0; i < runs; i++) {
             tasks.add(new SIBThread(data, k, maxIter));
         }

File: core/src/main/java/smile/clustering/XMeans.java
Patch:
@@ -85,7 +85,7 @@ public XMeans(double[][] data, int kmax) {
 
         BBDTree bbd = new BBDTree(data);
         while (k < kmax) {
-            ArrayList<double[]> centers = new ArrayList<double[]>();
+            ArrayList<double[]> centers = new ArrayList<>();
             double[] score = new double[k];
             KMeans[] kmeans = new KMeans[k];
             

File: core/src/main/java/smile/feature/Bag.java
Patch:
@@ -58,7 +58,7 @@ public Bag(T[] features) {
      */
     public Bag(T[] features, boolean binary) {
         this.binary = binary;
-        this.features = new HashMap<T, Integer>();
+        this.features = new HashMap<>();
         for (int i = 0, k = 0; i < features.length; i++) {
             if (!this.features.containsKey(features[i])) {
                 this.features.put(features[i], k++);

File: core/src/main/java/smile/feature/FeatureSet.java
Patch:
@@ -35,11 +35,11 @@ public class FeatureSet <T> {
     /**
      * Feature generators.
      */
-    List<Feature<T>> features = new ArrayList<Feature<T>>();
+    List<Feature<T>> features = new ArrayList<>();
     /**
      * The variable attributes of generated features.
      */
-    List<Attribute> attributes = new ArrayList<Attribute>();
+    List<Attribute> attributes = new ArrayList<>();
     
     /**
      * Constructor.
@@ -123,7 +123,7 @@ public AttributeDataset f(Dataset<T> data) {
 
         for (int i = 0; i < data.size(); i++) {
             Datum<T> datum = data.get(i);
-            Datum<double[]> x = new Datum<double[]>(f(datum.x), datum.y, datum.weight);
+            Datum<double[]> x = new Datum<>(f(datum.x), datum.y, datum.weight);
             x.name = datum.name;
             x.description = datum.description;
             x.timestamp = datum.timestamp;

File: core/src/main/java/smile/gap/GeneticAlgorithm.java
Patch:
@@ -219,7 +219,7 @@ public enum Selection {
     /**
      * Parallel tasks to evaluate the fitness of population.
      */
-    private List<Task> tasks = new ArrayList<Task>();
+    private List<Task> tasks = new ArrayList<>();
     
     /**
      * Constructor. The default selection strategy is tournament selection

File: core/src/main/java/smile/manifold/IsoMap.java
Patch:
@@ -109,9 +109,9 @@ public IsoMap(double[][] data, int d, int k, boolean CIsomap) {
 
         KNNSearch<double[], double[]> knn = null;
         if (data[0].length < 10) {
-            knn = new KDTree<double[]>(data, data);
+            knn = new KDTree<>(data, data);
         } else {
-            knn = new CoverTree<double[]>(data, new EuclideanDistance());
+            knn = new CoverTree<>(data, new EuclideanDistance());
         }
 
         graph = new AdjacencyList(n);

File: core/src/main/java/smile/manifold/LLE.java
Patch:
@@ -88,9 +88,9 @@ public LLE(double[][] data, int d, int k) {
 
         KNNSearch<double[], double[]> knn = null;
         if (D < 10) {
-            knn = new KDTree<double[]>(data, data);
+            knn = new KDTree<>(data, data);
         } else {
-            knn = new CoverTree<double[]>(data, new EuclideanDistance());
+            knn = new CoverTree<>(data, new EuclideanDistance());
         }
 
         Comparator<Neighbor<double[], double[]>> comparator = new Comparator<Neighbor<double[], double[]>>() {

File: core/src/main/java/smile/manifold/LaplacianEigenmap.java
Patch:
@@ -98,9 +98,9 @@ public LaplacianEigenmap(double[][] data, int d, int k, double t) {
         int n = data.length;
         KNNSearch<double[], double[]> knn = null;
         if (data[0].length < 10) {
-            knn = new KDTree<double[]>(data, data);
+            knn = new KDTree<>(data, data);
         } else {
-            knn = new CoverTree<double[]>(data, new EuclideanDistance());
+            knn = new CoverTree<>(data, new EuclideanDistance());
         }
 
         graph = new AdjacencyList(n);

File: core/src/main/java/smile/neighbor/BKTree.java
Patch:
@@ -93,7 +93,7 @@ private void add(E datum) {
             }
 
             if (children == null) {
-                children = new ArrayList<Node>();
+                children = new ArrayList<>();
             }
 
             while (children.size() <= d) {
@@ -203,7 +203,7 @@ private void search(Node node, E q, int k, List<Neighbor<E, E>> neighbors) {
 
         if (d <= k) {
             if (node.object != q || !identicalExcluded) {
-                neighbors.add(new Neighbor<E, E>(node.object, node.object, node.index, d));
+                neighbors.add(new Neighbor<>(node.object, node.object, node.index, d));
             }
         }
 

File: core/src/main/java/smile/regression/GaussianProcessRegression.java
Patch:
@@ -111,7 +111,7 @@ public Trainer(MercerKernel<T> kernel, double lambda) {
         
         @Override
         public GaussianProcessRegression<T> train(T[] x, double[] y) {
-            return new GaussianProcessRegression<T>(x, y, kernel, lambda);
+            return new GaussianProcessRegression<>(x, y, kernel, lambda);
         }
         
         /**
@@ -125,7 +125,7 @@ public GaussianProcessRegression<T> train(T[] x, double[] y) {
          * @return a trained Gaussian Process.
          */
         public GaussianProcessRegression<T> train(T[] x, double[] y, T[] t) {
-            return new GaussianProcessRegression<T>(x, y, t, kernel, lambda);
+            return new GaussianProcessRegression<>(x, y, t, kernel, lambda);
         }
     }
     

File: core/src/main/java/smile/regression/RBFNetwork.java
Patch:
@@ -173,9 +173,9 @@ public RBFNetwork<T> train(T[] x, double[] y) {
             T[] centers = (T[]) java.lang.reflect.Array.newInstance(x.getClass().getComponentType(), m);
             GaussianRadialBasis gaussian = SmileUtils.learnGaussianRadialBasis(x, centers, distance);
             if (rbf == null) {
-                return new RBFNetwork<T>(x, y, distance, gaussian, centers, normalized);
+                return new RBFNetwork<>(x, y, distance, gaussian, centers, normalized);
             } else {
-                return new RBFNetwork<T>(x, y, distance, rbf, centers, normalized);
+                return new RBFNetwork<>(x, y, distance, rbf, centers, normalized);
             }
         }
         
@@ -188,7 +188,7 @@ public RBFNetwork<T> train(T[] x, double[] y) {
          * @return a trained RBF network
          */
         public RBFNetwork<T> train(T[] x, double[] y, T[] centers) {
-            return new RBFNetwork<T>(x, y, distance, rbf, centers, normalized);
+            return new RBFNetwork<>(x, y, distance, rbf, centers, normalized);
         }
     }
     

File: core/src/main/java/smile/regression/RandomForest.java
Patch:
@@ -458,7 +458,7 @@ public RandomForest(Attribute[] attributes, double[][] x, double[] y, int ntrees
         int[] oob = new int[n];
         
         int[][] order = SmileUtils.sort(attributes, x);
-        List<TrainingTask> tasks = new ArrayList<TrainingTask>();
+        List<TrainingTask> tasks = new ArrayList<>();
         for (int i = 0; i < ntrees; i++) {
             tasks.add(new TrainingTask(attributes, x, y, maxNodes, nodeSize, mtry, subsample, order, prediction, oob));
         }
@@ -468,7 +468,7 @@ public RandomForest(Attribute[] attributes, double[][] x, double[] y, int ntrees
         } catch (Exception ex) {
             ex.printStackTrace();
 
-            trees = new ArrayList<RegressionTree>(ntrees);
+            trees = new ArrayList<>(ntrees);
             for (int i = 0; i < ntrees; i++) {
                 trees.add(tasks.get(i).call());
             }
@@ -547,7 +547,7 @@ public void trim(int ntrees) {
             throw new IllegalArgumentException("Invalid new model size: " + ntrees);
         }
         
-        List<RegressionTree> model = new ArrayList<RegressionTree>(ntrees);
+        List<RegressionTree> model = new ArrayList<>(ntrees);
         for (int i = 0; i < ntrees; i++) {
             model.add(trees.get(i));
         }

File: core/src/main/java/smile/regression/RegressionTree.java
Patch:
@@ -422,7 +422,7 @@ public boolean findBestSplit() {
                 }
             } else {
 
-                List<SplitTask> tasks = new ArrayList<SplitTask>(mtry);
+                List<SplitTask> tasks = new ArrayList<>(mtry);
                 for (int j = 0; j < mtry; j++) {
                     tasks.add(new SplitTask(n, sum, variables[j]));
                 }
@@ -965,7 +965,7 @@ public RegressionTree(Attribute[] attributes, double[][] x, double[] y, int maxN
         }
 
         // Priority queue for best-first tree growing.
-        PriorityQueue<TrainNode> nextSplits = new PriorityQueue<TrainNode>();
+        PriorityQueue<TrainNode> nextSplits = new PriorityQueue<>();
 
         int n = 0;
         double sum = 0.0;
@@ -1063,7 +1063,7 @@ public RegressionTree(int numFeatures, int[][] x, double[] y, int maxNodes, int
         importance = new double[numFeatures];
         
         // Priority queue for best-first tree growing.
-        PriorityQueue<SparseBinaryTrainNode> nextSplits = new PriorityQueue<SparseBinaryTrainNode>();
+        PriorityQueue<SparseBinaryTrainNode> nextSplits = new PriorityQueue<>();
 
         int n = 0;
         double sum = 0.0;

File: core/src/main/java/smile/regression/SVR.java
Patch:
@@ -74,7 +74,7 @@ public class SVR<T> implements Regression<T> {
     /**
      * Support vectors.
      */
-    private List<SupportVector> sv = new ArrayList<SupportVector>();
+    private List<SupportVector> sv = new ArrayList<>();
     /**
      * Threshold of decision function.
      */
@@ -189,7 +189,7 @@ public Trainer setTolerance(double tol) {
 
         @Override
         public SVR<T> train(T[] x, double[] y) {
-            SVR<T> svr = new SVR<T>(x, y, kernel, eps, C, tol);
+            SVR<T> svr = new SVR<>(x, y, kernel, eps, C, tol);
             return svr;
         }
     }
@@ -415,7 +415,7 @@ private void gram(SupportVector i) {
                 i.kcache.add(kernel.k(i.x, v.x));
             }
         } else {
-            List<KernelTask> tasks = new ArrayList<KernelTask>(m + 1);
+            List<KernelTask> tasks = new ArrayList<>(m + 1);
             int step = n / m;
             if (step < 100) {
                 step = 100;

File: core/src/main/java/smile/sampling/Bagging.java
Patch:
@@ -52,7 +52,7 @@ public Bagging(int k, int[] y, double[] classWeight, double subsample) {
             // Training samples draw with replacement.
             for (int l = 0; l < k; l++) {
                 int nj = 0;
-                ArrayList<Integer> cj = new ArrayList<Integer>();
+                ArrayList<Integer> cj = new ArrayList<>();
                 for (int i = 0; i < n; i++) {
                     if (y[i] == l) {
                         cj.add(i);

File: core/src/main/java/smile/sequence/HMM.java
Patch:
@@ -123,7 +123,7 @@ public HMM(double[] pi, double[][] a, double[][] b, O[] symbols) {
                 throw new IllegalArgumentException("Invalid size of emission symbol list.");
             }
 
-            this.symbols = new HashMap<O, Integer>();
+            this.symbols = new HashMap<>();
             for (int i = 0; i < symbols.length; i++) {
                 this.symbols.put(symbols[i], i);
             }
@@ -481,7 +481,7 @@ public HMM(O[][] observations, int[][] labels) {
         }
 
         int index = 0;
-        symbols = new HashMap<O, Integer>();
+        symbols = new HashMap<>();
         for (int i = 0; i < observations.length; i++) {
             if (observations[i].length != labels[i].length) {
                 throw new IllegalArgumentException(String.format("The length of observation sequence %d and that of corresponding label sequence are different.", i));
@@ -573,7 +573,7 @@ public HMM<O> learn(int[][] observations, int iterations) {
      * @return an updated HMM.
      */
     private HMM<O> iterate(int[][] sequences) {
-        HMM<O> hmm = new HMM<O>(numStates, numSymbols);
+        HMM<O> hmm = new HMM<>(numStates, numSymbols);
         hmm.symbols = symbols;
 
         // gamma[n] = gamma array associated to observation sequence n

File: core/src/main/java/smile/taxonomy/Taxonomy.java
Patch:
@@ -32,7 +32,7 @@ public class Taxonomy {
     /**
      * All the concepts in this taxonomy.
      */
-    HashMap<String, Concept> concepts = new HashMap<String, Concept>();
+    HashMap<String, Concept> concepts = new HashMap<>();
     /**
      * The root node in the taxonomy.
      */
@@ -84,7 +84,7 @@ public List<String> getConcepts() {
      * Returns all named concepts from this taxonomy
      */
     private List<String> getConcepts(Concept c) {
-        List<String> keywords = new ArrayList<String>();
+        List<String> keywords = new ArrayList<>();
 
         while (c != null) {
             if (c.synset != null) {

File: core/src/main/java/smile/util/MulticoreExecutor.java
Patch:
@@ -90,7 +90,7 @@ public static int getThreadPoolSize() {
     public static <T> List<T> run(Collection<? extends Callable<T>> tasks) throws Exception {
         createThreadPool();
 
-        List<T> results = new ArrayList<T>();
+        List<T> results = new ArrayList<>();
         if (threads == null) {
             for (Callable<T> task : tasks) {
                 results.add(task.call());

File: core/src/main/java/smile/util/SmileUtils.java
Patch:
@@ -198,7 +198,7 @@ public static GaussianRadialBasis[] learnGaussianRadialBasis(double[][] x, doubl
      */
     public static <T> GaussianRadialBasis learnGaussianRadialBasis(T[] x, T[] centers, Metric<T> distance) {
         int k = centers.length;
-        CLARANS<T> clarans = new CLARANS<T>(x, distance, k, Math.min(100, (int) Math.round(0.01 * k * (x.length-k))));
+        CLARANS<T> clarans = new CLARANS<>(x, distance, k, Math.min(100, (int) Math.round(0.01 * k * (x.length - k))));
         System.arraycopy(clarans.medoids(), 0, centers, 0, k);
 
         double r0 = 0.0;
@@ -234,7 +234,7 @@ public static <T> GaussianRadialBasis[] learnGaussianRadialBasis(T[] x, T[] cent
         }
         
         int k = centers.length;
-        CLARANS<T> clarans = new CLARANS<T>(x, distance, k, Math.min(100, (int) Math.round(0.01 * k * (x.length-k))));
+        CLARANS<T> clarans = new CLARANS<>(x, distance, k, Math.min(100, (int) Math.round(0.01 * k * (x.length - k))));
         System.arraycopy(clarans.medoids(), 0, centers, 0, k);
 
         p = Math.min(p, k-1);
@@ -274,7 +274,7 @@ public static <T> GaussianRadialBasis[] learnGaussianRadialBasis(T[] x, T[] cent
         }
         
         int k = centers.length;
-        CLARANS<T> clarans = new CLARANS<T>(x, distance, k, Math.min(100, (int) Math.round(0.01 * k * (x.length-k))));
+        CLARANS<T> clarans = new CLARANS<>(x, distance, k, Math.min(100, (int) Math.round(0.01 * k * (x.length - k))));
         System.arraycopy(clarans.medoids(), 0, centers, 0, k);
 
         int n = x.length;

File: core/src/main/java/smile/validation/ConfusionMatrix.java
Patch:
@@ -31,7 +31,7 @@ public ConfusionMatrix(int[] truth, int[] prediction) {
 			 throw new IllegalArgumentException(String.format("The vector sizes don't match: %d != %d.", truth.length, prediction.length));
 		}
 		
-		Set<Integer> ySet = new HashSet<Integer>();
+		Set<Integer> ySet = new HashSet<>();
 		
 		for(int i = 0; i < truth.length; i++){
 			ySet.add(truth[i]);

File: core/src/test/java/smile/association/ARMTest.java
Patch:
@@ -114,7 +114,7 @@ public void testLearn() {
     public void testLearnPima() {
         System.out.println("pima");
 
-        List<int[]> dataList = new ArrayList<int[]>(1000);
+        List<int[]> dataList = new ArrayList<>(1000);
 
         try {
             BufferedReader input = smile.data.parser.IOUtils.getTestDataReader("transaction/pima.D38.N768.C2");
@@ -157,7 +157,7 @@ public void testLearnPima() {
     public void testLearnKosarak() {
         System.out.println("kosarak");
 
-        List<int[]> dataList = new ArrayList<int[]>(1000);
+        List<int[]> dataList = new ArrayList<>(1000);
 
         try {
             BufferedReader input = smile.data.parser.IOUtils.getTestDataReader("transaction/kosarak.dat");
@@ -170,7 +170,7 @@ public void testLearnKosarak() {
 
                 String[] s = line.split(" ");
 
-                Set<Integer> items = new HashSet<Integer>();
+                Set<Integer> items = new HashSet<>();
                 for (int i = 0; i < s.length; i++) {
                     items.add(Integer.parseInt(s[i]));
                 }

File: core/src/test/java/smile/association/FPGrowthTest.java
Patch:
@@ -120,7 +120,7 @@ public void testLearn_PrintStream() {
     public void testPima() {
         System.out.println("pima");
 
-        List<int[]> dataList = new ArrayList<int[]>(1000);
+        List<int[]> dataList = new ArrayList<>(1000);
 
         try {
             BufferedReader input = smile.data.parser.IOUtils.getTestDataReader("transaction/pima.D38.N768.C2");
@@ -168,7 +168,7 @@ public void testPima() {
     public void testKosarak() {
         System.out.println("kosarak");
 
-        List<int[]> dataList = new ArrayList<int[]>(1000);
+        List<int[]> dataList = new ArrayList<>(1000);
 
         try {
             BufferedReader input = smile.data.parser.IOUtils.getTestDataReader("transaction/kosarak.dat");
@@ -181,7 +181,7 @@ public void testKosarak() {
 
                 String[] s = line.split(" ");
 
-                Set<Integer> items = new HashSet<Integer>();
+                Set<Integer> items = new HashSet<>();
                 for (int i = 0; i < s.length; i++) {
                     items.add(Integer.parseInt(s[i]));
                 }

File: core/src/test/java/smile/association/TotalSupportTreeTest.java
Patch:
@@ -146,7 +146,7 @@ public void testGetFrequentItemsets_PrintStream() {
     public void testPima() {
         System.out.println("pima");
 
-        List<int[]> dataList = new ArrayList<int[]>(1000);
+        List<int[]> dataList = new ArrayList<>(1000);
 
         try {
             InputStream stream = getClass().getResourceAsStream("/smile/data/transaction/pima.D38.N768.C2");
@@ -199,7 +199,7 @@ public void testPima() {
     public void testKosarak() {
         System.out.println("kosarak");
 
-        List<int[]> dataList = new ArrayList<int[]>(1000);
+        List<int[]> dataList = new ArrayList<>(1000);
 
         try {
             InputStream stream = getClass().getResourceAsStream("/smile/data/transaction/kosarak.dat");
@@ -213,7 +213,7 @@ public void testKosarak() {
 
                 String[] s = line.split(" ");
 
-                Set<Integer> items = new HashSet<Integer>();
+                Set<Integer> items = new HashSet<>();
                 for (int i = 0; i < s.length; i++) {
                     items.add(Integer.parseInt(s[i]));
                 }

File: core/src/test/java/smile/classification/MaxentTest.java
Patch:
@@ -44,8 +44,8 @@ class Dataset {
     @SuppressWarnings("unused")
     Dataset load(String resource) {
         int p = 0;
-        ArrayList<int[]> x = new ArrayList<int[]>();
-        ArrayList<Integer> y = new ArrayList<Integer>();
+        ArrayList<int[]> x = new ArrayList<>();
+        ArrayList<Integer> y = new ArrayList<>();
 
         try (BufferedReader input = new BufferedReader(new InputStreamReader(this.getClass().getResourceAsStream(resource)))) {
             String[] words = input.readLine().split(" ");

File: core/src/test/java/smile/classification/NaiveBayesTest.java
Patch:
@@ -73,7 +73,7 @@ public NaiveBayesTest() {
 
         moviex = new double[x.length][];
         moviey = new int[y.length];
-        Bag<String> bag = new Bag<String>(feature);
+        Bag<String> bag = new Bag<>(feature);
         for (int i = 0; i < x.length; i++) {
             moviex[i] = bag.feature(x[i]);
             moviey[i] = y[i];
@@ -124,7 +124,7 @@ public void testPredict() {
                 for (int i = 0; i < k; i++) {
                     priori[i] = 1.0 / k;
                     for (int j = 0; j < p; j++) {
-                        ArrayList<Double> axi = new ArrayList<Double>();
+                        ArrayList<Double> axi = new ArrayList<>();
                         for (int m = 0; m < trainx.length; m++) {
                             if (trainy[m] == i) {
                                 axi.add(trainx[m][j]);

File: core/src/test/java/smile/classification/RBFNetworkTest.java
Patch:
@@ -83,7 +83,7 @@ public void testLearn() {
 
                 double[][] centers = new double[10][];
                 RadialBasisFunction[] basis = SmileUtils.learnGaussianRadialBasis(trainx, centers, 5.0);
-                RBFNetwork<double[]> rbf = new RBFNetwork<double[]>(trainx, trainy, new EuclideanDistance(), basis, centers);
+                RBFNetwork<double[]> rbf = new RBFNetwork<>(trainx, trainy, new EuclideanDistance(), basis, centers);
 
                 if (y[loocv.test[i]] != rbf.predict(x[loocv.test[i]]))
                     error++;
@@ -115,7 +115,7 @@ public void testSegment() {
             
             double[][] centers = new double[100][];
             RadialBasisFunction[] basis = SmileUtils.learnGaussianRadialBasis(x, centers, 5.0);
-            RBFNetwork<double[]> rbf = new RBFNetwork<double[]>(x, y, new EuclideanDistance(), basis, centers);
+            RBFNetwork<double[]> rbf = new RBFNetwork<>(x, y, new EuclideanDistance(), basis, centers);
             
             int error = 0;
             for (int i = 0; i < testx.length; i++) {
@@ -150,7 +150,7 @@ public void testUSPS() {
             
             double[][] centers = new double[200][];
             RadialBasisFunction basis = SmileUtils.learnGaussianRadialBasis(x, centers);
-            RBFNetwork<double[]> rbf = new RBFNetwork<double[]>(x, y, new EuclideanDistance(), new GaussianRadialBasis(8.0), centers);
+            RBFNetwork<double[]> rbf = new RBFNetwork<>(x, y, new EuclideanDistance(), new GaussianRadialBasis(8.0), centers);
                 
             int error = 0;
             for (int i = 0; i < testx.length; i++) {

File: core/src/test/java/smile/clustering/CLARANSTest.java
Patch:
@@ -72,7 +72,7 @@ public void testUSPS() {
             
             AdjustedRandIndex ari = new AdjustedRandIndex();
             RandIndex rand = new RandIndex();
-            CLARANS<double[]> clarans = new CLARANS<double[]>(x, new EuclideanDistance(), 10, 50, 8);
+            CLARANS<double[]> clarans = new CLARANS<>(x, new EuclideanDistance(), 10, 50, 8);
 
             double r = rand.measure(y, clarans.getClusterLabel());
             double r2 = ari.measure(y, clarans.getClusterLabel());

File: core/src/test/java/smile/clustering/DBScanTest.java
Patch:
@@ -93,7 +93,7 @@ public void testToy() {
             label[i] = 3;
         }
         
-        DBScan<double[]> dbscan = new DBScan<double[]>(data, new KDTree<double[]>(data, data), 200, 0.8);
+        DBScan<double[]> dbscan = new DBScan<>(data, new KDTree<>(data, data), 200, 0.8);
         System.out.println(dbscan);
         
         int[] size = dbscan.getClusterSize();

File: core/src/test/java/smile/clustering/MECTest.java
Patch:
@@ -72,7 +72,7 @@ public void testUSPS() {
             
             AdjustedRandIndex ari = new AdjustedRandIndex();
             RandIndex rand = new RandIndex();
-            MEC<double[]> mec = new MEC<double[]>(x, new EuclideanDistance(), 10, 8.0);
+            MEC<double[]> mec = new MEC<>(x, new EuclideanDistance(), 10, 8.0);
             
             double r = rand.measure(y, mec.getClusterLabel());
             double r2 = ari.measure(y, mec.getClusterLabel());

File: core/src/test/java/smile/feature/BagTest.java
Patch:
@@ -63,11 +63,11 @@ public void testUniquenessOfFeatures() {
         String[] featuresForBuildingStories = {"truck", "concrete", "foundation", "steel", "crane"};
         String testMessage = "This story is about a crane and a sparrow";
 
-        ArrayList<String> mergedFeatureLists = new ArrayList<String>();
+        ArrayList<String> mergedFeatureLists = new ArrayList<>();
         mergedFeatureLists.addAll(Arrays.asList(featuresForBirdStories));
         mergedFeatureLists.addAll(Arrays.asList(featuresForBuildingStories));
 
-        Bag<String> bag = new Bag<String>(mergedFeatureLists.toArray(new String[featuresForBirdStories.length + featuresForBuildingStories.length]));
+        Bag<String> bag = new Bag<>(mergedFeatureLists.toArray(new String[featuresForBirdStories.length + featuresForBuildingStories.length]));
 
         double[] result = bag.feature(testMessage.split(" "));
         assertEquals(9, result.length);
@@ -98,7 +98,7 @@ public void testFeature() {
             "perfectly", "masterpiece", "realistic", "flaws"
         };
         
-        Bag<String> bag = new Bag<String>(feature);
+        Bag<String> bag = new Bag<>(feature);
         
         double[][] x = new double[text.length][];
         for (int i = 0; i < text.length; i++) {

File: core/src/test/java/smile/feature/FeatureSetTest.java
Patch:
@@ -62,7 +62,7 @@ public void testAttributes() {
             AttributeDataset data = parser.parse(smile.data.parser.IOUtils.getTestDataFile("weka/regression/abalone.arff"));
             double[][] x = data.toArray(new double[data.size()][]);
             
-            FeatureSet<double[]> features = new FeatureSet<double[]>();
+            FeatureSet<double[]> features = new FeatureSet<>();
             features.add(new Nominal2Binary(data.attributes()));
             features.add(new NumericAttributeFeature(data.attributes(), 0.05, 0.95, x));
             Attribute[] attributes = features.attributes();
@@ -87,7 +87,7 @@ public void testF() {
             AttributeDataset data = parser.parse(smile.data.parser.IOUtils.getTestDataFile("weka/regression/abalone.arff"));
             double[][] x = data.toArray(new double[data.size()][]);
             
-            FeatureSet<double[]> features = new FeatureSet<double[]>();
+            FeatureSet<double[]> features = new FeatureSet<>();
             features.add(new Nominal2Binary(data.attributes()));
             features.add(new NumericAttributeFeature(data.attributes(), 0.05, 0.95, x));
             

File: core/src/test/java/smile/gap/GeneticAlgorithmTest.java
Patch:
@@ -87,7 +87,7 @@ public void testEvolve() {
             seeds[i] = new BitString(15, new Knapnack(), BitString.Crossover.UNIFORM, 1.0, 0.2);
         }
         
-        GeneticAlgorithm<BitString> instance = new GeneticAlgorithm<BitString>(seeds, GeneticAlgorithm.Selection.TOURNAMENT);
+        GeneticAlgorithm<BitString> instance = new GeneticAlgorithm<>(seeds, GeneticAlgorithm.Selection.TOURNAMENT);
         instance.setElitism(2);
         instance.setTournament(3, 0.95);
         

File: core/src/test/java/smile/neighbor/BKTreeSpeedTest.java
Patch:
@@ -32,7 +32,7 @@
  */
 public class BKTreeSpeedTest {
 
-    List<String> words = new ArrayList<String>();
+    List<String> words = new ArrayList<>();
     BKTree<String> bktree;
 
     public BKTreeSpeedTest() {
@@ -57,7 +57,7 @@ public BKTreeSpeedTest() {
         String[] data = words.toArray(new String[words.size()]);
 
         start = System.currentTimeMillis();
-        bktree = new BKTree<String>(new EditDistance(50, true));
+        bktree = new BKTree<>(new EditDistance(50, true));
         bktree.add(data);
         time = (System.currentTimeMillis() - start) / 1000.0;
         System.out.format("Building BK-tree: %.2fs%n", time);
@@ -86,7 +86,7 @@ public void tearDown() {
     public void testBKTreeSpeed() {
         System.out.println("BK-Tree range 1 speed");
         long start = System.currentTimeMillis();
-        List<Neighbor<String, String>> neighbors = new ArrayList<Neighbor<String, String>>();
+        List<Neighbor<String, String>> neighbors = new ArrayList<>();
         for (int i = 1000; i < 1100; i++) {
             bktree.range(words.get(i), 1, neighbors);
             neighbors.clear();

File: core/src/test/java/smile/neighbor/CoverTreeSpeedTest.java
Patch:
@@ -55,7 +55,7 @@ public CoverTreeSpeedTest() {
         System.out.format("Loading data: %.2fs%n", time);
 
         start = System.currentTimeMillis();
-        coverTree = new CoverTree<double[]>(x, new EuclideanDistance());
+        coverTree = new CoverTree<>(x, new EuclideanDistance());
         time = (System.currentTimeMillis() - start) / 1000.0;
         System.out.format("Building cover tree: %.2fs%n", time);
     }
@@ -96,7 +96,7 @@ public void testCoverTree() {
         System.out.format("10-NN: %.2fs%n", time);
 
         start = System.currentTimeMillis();
-        List<Neighbor<double[], double[]>> n = new ArrayList<Neighbor<double[], double[]>>();
+        List<Neighbor<double[], double[]>> n = new ArrayList<>();
         for (int i = 0; i < testx.length; i++) {
             coverTree.range(testx[i], 8.0, n);
             n.clear();

File: core/src/test/java/smile/neighbor/CoverTreeStringSpeedTest.java
Patch:
@@ -32,7 +32,7 @@
  */
 public class CoverTreeStringSpeedTest {
 
-    List<String> words = new ArrayList<String>();
+    List<String> words = new ArrayList<>();
     CoverTree<String> cover;
 
     public CoverTreeStringSpeedTest() {
@@ -57,7 +57,7 @@ public CoverTreeStringSpeedTest() {
         String[] data = words.toArray(new String[words.size()]);
 
         start = System.currentTimeMillis();
-        cover = new CoverTree<String>(data, new EditDistance(50, true));
+        cover = new CoverTree<>(data, new EditDistance(50, true));
         time = (System.currentTimeMillis() - start) / 1000.0;
         System.out.format("Building cover tree: %.2fs%n", time);
     }
@@ -85,7 +85,7 @@ public void tearDown() {
     public void testNaiveSpeed() {
         System.out.println("cover tree");
         long start = System.currentTimeMillis();
-        List<Neighbor<String, String>> neighbors = new ArrayList<Neighbor<String, String>>();
+        List<Neighbor<String, String>> neighbors = new ArrayList<>();
         for (int i = 1000; i < 1100; i++) {
             cover.range(words.get(i), 1, neighbors);
             neighbors.clear();

File: core/src/test/java/smile/neighbor/MPLSHSpeedTest.java
Patch:
@@ -79,7 +79,7 @@ public void testUSPS() {
         System.out.format("Loading USPS: %.2fs%n", time);
 
         start = System.currentTimeMillis();
-        MPLSH<double[]> lsh = new MPLSH<double[]>(256, 100, 3, 4.0);
+        MPLSH<double[]> lsh = new MPLSH<>(256, 100, 3, 4.0);
         for (double[] xi : x) {
             lsh.put(xi, xi);
         }
@@ -90,7 +90,7 @@ public void testUSPS() {
             train[i] = x[index[i]];
         }
 
-        LinearSearch<double[]> naive = new LinearSearch<double[]>(x, new EuclideanDistance());
+        LinearSearch<double[]> naive = new LinearSearch<>(x, new EuclideanDistance());
         lsh.learn(naive, train, 8.0);
         time = (System.currentTimeMillis() - start) / 1000.0;
         System.out.format("Building LSH: %.2fs%n", time);
@@ -110,7 +110,7 @@ public void testUSPS() {
         System.out.format("10-NN: %.2fs%n", time);
 
         start = System.currentTimeMillis();
-        List<Neighbor<double[], double[]>> n = new ArrayList<Neighbor<double[], double[]>>();
+        List<Neighbor<double[], double[]>> n = new ArrayList<>();
         for (int i = 0; i < testx.length; i++) {
             lsh.range(testx[i], 8.0, n);
             n.clear();

File: core/src/test/java/smile/regression/SVRTest.java
Patch:
@@ -77,7 +77,7 @@ public void testCPU() {
                 double[][] testx = Math.slice(datax, cv.test[i]);
                 double[] testy = Math.slice(datay, cv.test[i]);
 
-                SVR<double[]> svr = new SVR<double[]>(trainx, trainy, new PolynomialKernel(3, 1.0, 1.0), 0.1, 1.0);
+                SVR<double[]> svr = new SVR<>(trainx, trainy, new PolynomialKernel(3, 1.0, 1.0), 0.1, 1.0);
 
                 for (int j = 0; j < testx.length; j++) {
                     double r = testy[j] - svr.predict(testx[j]);

File: core/src/test/java/smile/taxonomy/TaxonomyTest.java
Patch:
@@ -113,7 +113,7 @@ public void testLowestCommonAncestor() {
     @Test
     public void testGetPathToRoot() {
         System.out.println("getPathToRoot");
-        LinkedList<Concept> expResult = new LinkedList<Concept>();
+        LinkedList<Concept> expResult = new LinkedList<>();
         expResult.addFirst(instance.getRoot());
         expResult.addFirst(ad);
         expResult.addFirst(a);
@@ -129,7 +129,7 @@ public void testGetPathToRoot() {
     @Test
     public void testGetPathFromRoot() {
         System.out.println("getPathToRoot");
-        LinkedList<Concept> expResult = new LinkedList<Concept>();
+        LinkedList<Concept> expResult = new LinkedList<>();
         expResult.add(instance.getRoot());
         expResult.add(ad);
         expResult.add(a);

File: data/src/main/java/smile/data/StringAttribute.java
Patch:
@@ -38,11 +38,11 @@ public class StringAttribute extends Attribute {
     /**
      * The list of unique string values of this attribute.
      */
-    private List<String> values = new ArrayList<String>();
+    private List<String> values = new ArrayList<>();
     /**
      * Map a string to an integer level.
      */
-    private Map<String, Integer> map = new HashMap<String, Integer>();
+    private Map<String, Integer> map = new HashMap<>();
 
     /**
      * Constructor.

File: data/src/main/java/smile/data/parser/BinarySparseDatasetParser.java
Patch:
@@ -122,7 +122,7 @@ public BinarySparseDataset parse(String name, InputStream stream) throws IOExcep
                 throw new IOException("Empty data source.");
            }
         
-           Set<Integer> items = new HashSet<Integer>();
+           Set<Integer> items = new HashSet<>();
            do {
                 line = line.trim();
                 if (line.isEmpty()) {

File: data/src/main/java/smile/data/parser/DelimitedTextParser.java
Patch:
@@ -348,7 +348,7 @@ private AttributeDataset parse(String name, Attribute[] attributes, BufferedRead
                 }
             }
 
-            Datum<double[]> datum = new Datum<double[]>(x, y);
+            Datum<double[]> datum = new Datum<>(x, y);
             datum.name = rowName;
             data.add(datum);
         }
@@ -378,7 +378,7 @@ private AttributeDataset parse(String name, Attribute[] attributes, BufferedRead
                 }
             }
 
-            Datum<double[]> datum = new Datum<double[]>(x, y);
+            Datum<double[]> datum = new Datum<>(x, y);
             datum.name = rowName;
             data.add(datum);
         }

File: data/src/main/java/smile/data/parser/IOUtils.java
Patch:
@@ -74,7 +74,7 @@ public static List<String> readLines(InputStream input, Charset charset) throws
      */
     public static List<String> readLines(Reader input) throws IOException {
         BufferedReader reader = input instanceof BufferedReader ? (BufferedReader) input : new BufferedReader(input);
-        List<String> list = new ArrayList<String>();
+        List<String> list = new ArrayList<>();
         String line = reader.readLine();
         while (line != null) {
             list.add(line);

File: data/src/main/java/smile/data/parser/microarray/GCTParser.java
Patch:
@@ -211,7 +211,7 @@ public AttributeDataset parse(String name, InputStream stream) throws IOExceptio
                 }
             }
 
-            Datum<double[]> datum = new Datum<double[]>(x);
+            Datum<double[]> datum = new Datum<>(x);
             datum.name = tokens[0];
             datum.description = tokens[1];
             data.add(datum);

File: data/src/main/java/smile/data/parser/microarray/PCLParser.java
Patch:
@@ -180,7 +180,7 @@ public AttributeDataset parse(String name, InputStream stream) throws IOExceptio
                 }
             }
 
-            Datum<double[]> datum = new Datum<double[]>(x);
+            Datum<double[]> datum = new Datum<>(x);
             datum.name = tokens[0];
             datum.description = tokens[1];
             datum.weight = Double.valueOf(tokens[2]);

File: data/src/main/java/smile/data/parser/microarray/RESParser.java
Patch:
@@ -210,7 +210,7 @@ public AttributeDataset parse(String name, InputStream stream) throws IOExceptio
                 x[j] = Double.valueOf(tokens[2*j+2]);
             }
 
-            Datum<double[]> datum = new Datum<double[]>(x);
+            Datum<double[]> datum = new Datum<>(x);
             datum.name = tokens[1];
             datum.description = tokens[0];
             data.add(datum);

File: data/src/main/java/smile/data/parser/microarray/TXTParser.java
Patch:
@@ -159,7 +159,7 @@ public AttributeDataset parse(String name, InputStream stream) throws IOExceptio
                 }
             }
 
-            Datum<double[]> datum = new Datum<double[]>(x);
+            Datum<double[]> datum = new Datum<>(x);
             datum.name = tokens[0];
             if (start == 2) {
                 datum.description = tokens[1];

File: demo/src/main/java/smile/demo/classification/ClassificationDemo.java
Patch:
@@ -82,7 +82,7 @@ public ClassificationDemo() {
         startButton.setActionCommand("startButton");
         startButton.addActionListener(this);
 
-        datasetBox = new JComboBox<String>();
+        datasetBox = new JComboBox<>();
         for (int i = 0; i < datasetName.length; i++) {
             datasetBox.addItem(datasetName[i]);
         }

File: demo/src/main/java/smile/demo/classification/RBFNetworkDemo.java
Patch:
@@ -64,7 +64,7 @@ public double[][] learn(double[] x, double[] y) {
 
         double[][] centers = new double[k][];
         RadialBasisFunction basis = SmileUtils.learnGaussianRadialBasis(data, centers);
-        RBFNetwork<double[]> rbf = new RBFNetwork<double[]>(data, label, new EuclideanDistance(), basis, centers);
+        RBFNetwork<double[]> rbf = new RBFNetwork<>(data, label, new EuclideanDistance(), basis, centers);
         
         for (int i = 0; i < label.length; i++) {
             label[i] = rbf.predict(data[i]);

File: demo/src/main/java/smile/demo/classification/SVMDemo.java
Patch:
@@ -76,7 +76,7 @@ public double[][] learn(double[] x, double[] y) {
         double[][] data = dataset[datasetIndex].toArray(new double[dataset[datasetIndex].size()][]);
         int[] label = dataset[datasetIndex].toArray(new int[dataset[datasetIndex].size()]);
         
-        SVM<double[]> svm = new SVM<double[]>(new GaussianKernel(gamma), C);
+        SVM<double[]> svm = new SVM<>(new GaussianKernel(gamma), C);
         svm.learn(data, label);
         svm.finish();
         

File: demo/src/main/java/smile/demo/clustering/CLARANSDemo.java
Patch:
@@ -75,7 +75,7 @@ public JComponent learn() {
         }
 
         long clock = System.currentTimeMillis();
-        CLARANS<double[]> clarans = new CLARANS<double[]>(dataset[datasetIndex], new EuclideanDistance(), clusterNumber, maxNeighbor, numLocal);
+        CLARANS<double[]> clarans = new CLARANS<>(dataset[datasetIndex], new EuclideanDistance(), clusterNumber, maxNeighbor, numLocal);
         System.out.format("CLARANS clusterings %d samples in %dms\n", dataset[datasetIndex].length, System.currentTimeMillis()-clock);
 
         PlotCanvas plot = ScatterPlot.plot(clarans.medoids(), '@');

File: demo/src/main/java/smile/demo/clustering/ClusteringDemo.java
Patch:
@@ -100,7 +100,7 @@ public ClusteringDemo() {
         startButton.setActionCommand("startButton");
         startButton.addActionListener(this);
 
-        datasetBox = new JComboBox<String>();
+        datasetBox = new JComboBox<>();
         for (int i = 0; i < datasetName.length; i++) {
             datasetBox.addItem(datasetName[i]);
         }

File: demo/src/main/java/smile/demo/clustering/DBScanDemo.java
Patch:
@@ -82,7 +82,7 @@ public JComponent learn() {
         }
 
         long clock = System.currentTimeMillis();
-        DBScan<double[]> dbscan = new DBScan<double[]>(dataset[datasetIndex], new EuclideanDistance(), minPts, range);
+        DBScan<double[]> dbscan = new DBScan<>(dataset[datasetIndex], new EuclideanDistance(), minPts, range);
         System.out.format("DBSCAN clusterings %d samples in %dms\n", dataset[datasetIndex].length, System.currentTimeMillis()-clock);
 
         JPanel pane = new JPanel(new GridLayout(1, 2));

File: demo/src/main/java/smile/demo/clustering/HierarchicalClusteringDemo.java
Patch:
@@ -48,7 +48,7 @@ public class HierarchicalClusteringDemo extends ClusteringDemo {
     JComboBox<String> linkageBox;
 
     public HierarchicalClusteringDemo() {
-        linkageBox = new JComboBox<String>();
+        linkageBox = new JComboBox<>();
         linkageBox.addItem("Single");
         linkageBox.addItem("Complete");
         linkageBox.addItem("UPGMA");

File: demo/src/main/java/smile/demo/clustering/MECDemo.java
Patch:
@@ -59,7 +59,7 @@ public JComponent learn() {
         }
 
         long clock = System.currentTimeMillis();
-        MEC<double[]> mec = new MEC<double[]>(dataset[datasetIndex], new EuclideanDistance(), clusterNumber, range);
+        MEC<double[]> mec = new MEC<>(dataset[datasetIndex], new EuclideanDistance(), clusterNumber, range);
         System.out.format("MEC clusterings %d samples in %dms\n", dataset[datasetIndex].length, System.currentTimeMillis()-clock);
 
         PlotCanvas plot = ScatterPlot.plot(dataset[datasetIndex], pointLegend);

File: demo/src/main/java/smile/demo/clustering/SIBDemo.java
Patch:
@@ -64,7 +64,7 @@ public SIBDemo() {
         startButton.setActionCommand("startButton");
         startButton.addActionListener(this);
 
-        datasetBox = new JComboBox<String>();
+        datasetBox = new JComboBox<>();
         for (int i = 0; i < datasetName.length; i++) {
             datasetBox.addItem(datasetName[i]);
         }

File: demo/src/main/java/smile/demo/interpolation/ScatterDataInterpolationDemo.java
Patch:
@@ -64,8 +64,8 @@ public ScatterDataInterpolationDemo() {
         add(canvas);
 
         double[][] ww = new double[26][26];
-        ArrayList<double[]> xx = new ArrayList<double[]>();
-        ArrayList<Double> zz = new ArrayList<Double>();
+        ArrayList<double[]> xx = new ArrayList<>();
+        ArrayList<Double> zz = new ArrayList<>();
         for (int i = 0; i <= 25; i++) {
             for (int j = 0; j <= 25; j++) {
                 if (Math.random() < 0.2)

File: demo/src/main/java/smile/demo/manifold/ManifoldDemo.java
Patch:
@@ -63,7 +63,7 @@ public ManifoldDemo() {
         startButton.setActionCommand("startButton");
         startButton.addActionListener(this);
 
-        datasetBox = new JComboBox<String>();
+        datasetBox = new JComboBox<>();
         for (int i = 0; i < datasetName.length; i++) {
             datasetBox.addItem(datasetName[i]);
         }

File: demo/src/main/java/smile/demo/mds/IsotonicMDSDemo.java
Patch:
@@ -70,7 +70,7 @@ public IsotonicMDSDemo() {
         startButton.setActionCommand("startButton");
         startButton.addActionListener(this);
 
-        datasetBox = new JComboBox<String>();
+        datasetBox = new JComboBox<>();
         for (int i = 0; i < datasetName.length; i++) {
             datasetBox.addItem(datasetName[i]);
         }

File: demo/src/main/java/smile/demo/mds/MDSDemo.java
Patch:
@@ -70,7 +70,7 @@ public MDSDemo() {
         startButton.setActionCommand("startButton");
         startButton.addActionListener(this);
 
-        datasetBox = new JComboBox<String>();
+        datasetBox = new JComboBox<>();
         for (int i = 0; i < datasetName.length; i++) {
             datasetBox.addItem(datasetName[i]);
         }

File: demo/src/main/java/smile/demo/mds/SammonMappingDemo.java
Patch:
@@ -70,7 +70,7 @@ public SammonMappingDemo() {
         startButton.setActionCommand("startButton");
         startButton.addActionListener(this);
 
-        datasetBox = new JComboBox<String>();
+        datasetBox = new JComboBox<>();
         for (int i = 0; i < datasetName.length; i++) {
             datasetBox.addItem(datasetName[i]);
         }

File: demo/src/main/java/smile/demo/projection/KPCADemo.java
Patch:
@@ -120,7 +120,7 @@ public JComponent learn() {
         plot.setTitle("PCA");
         pane.add(plot);
 
-        KPCA<double[]> kpca = new KPCA<double[]>(data, new GaussianKernel(gamma[datasetIndex]), 2);
+        KPCA<double[]> kpca = new KPCA<>(data, new GaussianKernel(gamma[datasetIndex]), 2);
 
         y = kpca.getCoordinates();
         plot = new PlotCanvas(Math.colMin(y), Math.colMax(y));
@@ -139,7 +139,7 @@ public JComponent learn() {
         pane.add(plot);
 
         clock = System.currentTimeMillis();
-        kpca = new KPCA<double[]>(data, new GaussianKernel(gamma[datasetIndex]), 3);
+        kpca = new KPCA<>(data, new GaussianKernel(gamma[datasetIndex]), 3);
         System.out.format("Learn KPCA from %d samples in %dms\n", data.length, System.currentTimeMillis() - clock);
 
         y = kpca.getCoordinates();

File: demo/src/main/java/smile/demo/projection/LDADemo.java
Patch:
@@ -65,7 +65,7 @@ public LDADemo() {
         startButton.setActionCommand("startButton");
         startButton.addActionListener(this);
 
-        datasetBox = new JComboBox<String>();
+        datasetBox = new JComboBox<>();
         for (int i = 0; i < datasetName.length; i++) {
             datasetBox.addItem(datasetName[i]);
         }

File: demo/src/main/java/smile/demo/projection/PCADemo.java
Patch:
@@ -39,7 +39,7 @@ public class PCADemo extends ProjectionDemo {
     JComboBox<String> corBox;
 
     public PCADemo() {
-        corBox = new JComboBox<String>();
+        corBox = new JComboBox<>();
         corBox.addItem("Covariance");
         corBox.addItem("Correlation");
         corBox.setSelectedIndex(0);

File: demo/src/main/java/smile/demo/projection/ProjectionDemo.java
Patch:
@@ -65,7 +65,7 @@ public ProjectionDemo() {
         startButton.setActionCommand("startButton");
         startButton.addActionListener(this);
 
-        datasetBox = new JComboBox<String>();
+        datasetBox = new JComboBox<>();
         for (int i = 0; i < datasetName.length; i++) {
             datasetBox.addItem(datasetName[i]);
         }

File: demo/src/main/java/smile/demo/stat/distribution/BernoulliDistributionDemo.java
Patch:
@@ -52,7 +52,7 @@ public class BernoulliDistributionDemo extends JPanel implements ChangeListener
     public BernoulliDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         for (int i = 1; i < 10; i+=2) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i/10.0)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/BetaDistributionDemo.java
Patch:
@@ -58,7 +58,7 @@ public class BetaDistributionDemo extends JPanel implements ChangeListener {
     public BetaDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         for (int i = 0; i <= 100; i+=20) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i / 10)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/BinomialDistributionDemo.java
Patch:
@@ -54,7 +54,7 @@ public class BinomialDistributionDemo extends JPanel implements ChangeListener {
     public BinomialDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> nLabelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> nLabelTable = new Hashtable<>();
         nLabelTable.put(1, new JLabel(String.valueOf(1)));
         for (int i = 10; i <= 50; i+=10) {
             nLabelTable.put(new Integer(i), new JLabel(String.valueOf(i)));
@@ -68,7 +68,7 @@ public BinomialDistributionDemo() {
         nSlider.setPaintTicks(true);
         nSlider.setPaintLabels(true);
 
-        Hashtable<Integer, JLabel> probLabelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> probLabelTable = new Hashtable<>();
         for (int i = 1; i < 10; i+=2) {
             probLabelTable.put(new Integer(i), new JLabel(String.valueOf(i/10.0)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/ChiSquareDistributionDemo.java
Patch:
@@ -55,7 +55,7 @@ public class ChiSquareDistributionDemo extends JPanel implements ChangeListener
     public ChiSquareDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         labelTable.put(1, new JLabel(String.valueOf(1)));
         for (int i = 5; i <= 20; i += 5) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i)));

File: demo/src/main/java/smile/demo/stat/distribution/ExponentialDistributionDemo.java
Patch:
@@ -56,7 +56,7 @@ public class ExponentialDistributionDemo extends JPanel implements ChangeListene
     public ExponentialDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         for (int i = 0; i <= 50; i+=10) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i/10)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/ExponentialFamilyMixtureDemo.java
Patch:
@@ -57,7 +57,7 @@ public ExponentialFamilyMixtureDemo() {
         for (int i = 1000; i < 2000; i++)
             data[i] = gamma.rand();
 
-        Vector<Mixture.Component> m = new Vector<Mixture.Component>();
+        Vector<Mixture.Component> m = new Vector<>();
         Mixture.Component c = new Mixture.Component();
         c.priori = 0.25;
         c.distribution = new GaussianDistribution(0.0, 1.0);
@@ -114,7 +114,7 @@ public static void main(String[] args) {
         for (int i = 1000; i < 2000; i++)
             data[i] = gamma.rand();
 
-        Vector<Mixture.Component> m = new Vector<Mixture.Component>();
+        Vector<Mixture.Component> m = new Vector<>();
         Mixture.Component c = new Mixture.Component();
         c.priori = 0.25;
         c.distribution = new GaussianDistribution(0.0, 1.0);

File: demo/src/main/java/smile/demo/stat/distribution/FDistributionDemo.java
Patch:
@@ -57,7 +57,7 @@ public class FDistributionDemo extends JPanel implements ChangeListener {
     public FDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         labelTable.put(1, new JLabel(String.valueOf(1)));
         for (int i = 20; i <= 100; i += 20) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i)));

File: demo/src/main/java/smile/demo/stat/distribution/GammaDistributionDemo.java
Patch:
@@ -58,7 +58,7 @@ public class GammaDistributionDemo extends JPanel implements ChangeListener {
     public GammaDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> shapeLabelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> shapeLabelTable = new Hashtable<>();
         for (int i = 0; i <= 10; i+=2) {
             shapeLabelTable.put(new Integer(i), new JLabel(String.valueOf(i)));
         }
@@ -71,7 +71,7 @@ public GammaDistributionDemo() {
         shapeSlider.setPaintTicks(true);
         shapeSlider.setPaintLabels(true);
 
-        Hashtable<Integer, JLabel> scaleLabelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> scaleLabelTable = new Hashtable<>();
         for (int i = 0; i <= 50; i+=10) {
             scaleLabelTable.put(new Integer(i), new JLabel(String.valueOf(i/10)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/GaussianDistributionDemo.java
Patch:
@@ -56,7 +56,7 @@ public class GaussianDistributionDemo extends JPanel implements ChangeListener {
     public GaussianDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         for (int i = 0; i <= 50; i+=10) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i/10)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/GeometricDistributionDemo.java
Patch:
@@ -52,7 +52,7 @@ public class GeometricDistributionDemo extends JPanel implements ChangeListener
     public GeometricDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         for (int i = 1; i < 10; i+=2) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i/10.0)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/HyperGeometricDistributionDemo.java
Patch:
@@ -55,7 +55,7 @@ public class HyperGeometricDistributionDemo extends JPanel implements ChangeList
     public HyperGeometricDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         labelTable.put(1, new JLabel(String.valueOf(1)));
         for (int i = 20; i <= 100; i+=20) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i)));

File: demo/src/main/java/smile/demo/stat/distribution/LogNormalDistributionDemo.java
Patch:
@@ -56,7 +56,7 @@ public class LogNormalDistributionDemo extends JPanel implements ChangeListener
     public LogNormalDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         for (int i = 0; i <= 20; i+=10) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i/10)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/LogisticDistributionDemo.java
Patch:
@@ -56,7 +56,7 @@ public class LogisticDistributionDemo extends JPanel implements ChangeListener {
     public LogisticDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         for (int i = 0; i <= 50; i+=10) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i/10)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/MultivariateGaussianDistributionDemo.java
Patch:
@@ -51,7 +51,7 @@ public class MultivariateGaussianDistributionDemo extends JPanel implements Chan
     public MultivariateGaussianDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         for (int i = 0; i <= 30; i+=10) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i/10.0)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/NegativeBinomialDistributionDemo.java
Patch:
@@ -54,7 +54,7 @@ public class NegativeBinomialDistributionDemo extends JPanel implements ChangeLi
     public NegativeBinomialDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> nLabelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> nLabelTable = new Hashtable<>();
         nLabelTable.put(1, new JLabel(String.valueOf(1)));
         for (int i = 10; i <= 50; i+=10) {
             nLabelTable.put(new Integer(i), new JLabel(String.valueOf(i)));
@@ -68,7 +68,7 @@ public NegativeBinomialDistributionDemo() {
         nSlider.setPaintTicks(true);
         nSlider.setPaintLabels(true);
 
-        Hashtable<Integer, JLabel> probLabelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> probLabelTable = new Hashtable<>();
         for (int i = 1; i < 10; i+=2) {
             probLabelTable.put(new Integer(i), new JLabel(String.valueOf(i/10.0)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/PoissonDistributionDemo.java
Patch:
@@ -52,7 +52,7 @@ public class PoissonDistributionDemo extends JPanel implements ChangeListener {
     public PoissonDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         for (int i = 0; i <= 10; i+=2) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/ShiftedGeometricDistributionDemo.java
Patch:
@@ -52,7 +52,7 @@ public class ShiftedGeometricDistributionDemo extends JPanel implements ChangeLi
     public ShiftedGeometricDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         for (int i = 1; i < 10; i+=2) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i/10.0)));
         }

File: demo/src/main/java/smile/demo/stat/distribution/TDistributionDemo.java
Patch:
@@ -55,7 +55,7 @@ public class TDistributionDemo extends JPanel implements ChangeListener {
     public TDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> labelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> labelTable = new Hashtable<>();
         labelTable.put(1, new JLabel(String.valueOf(1)));
         for (int i = 5; i <= 20; i += 5) {
             labelTable.put(new Integer(i), new JLabel(String.valueOf(i)));

File: demo/src/main/java/smile/demo/stat/distribution/WeibullDistributionDemo.java
Patch:
@@ -58,7 +58,7 @@ public class WeibullDistributionDemo extends JPanel implements ChangeListener {
     public WeibullDistributionDemo() {
         super(new BorderLayout());
 
-        Hashtable<Integer, JLabel> shapeLabelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> shapeLabelTable = new Hashtable<>();
         for (int i = 0; i <= 10; i+=2) {
             shapeLabelTable.put(new Integer(i), new JLabel(String.valueOf(i)));
         }
@@ -71,7 +71,7 @@ public WeibullDistributionDemo() {
         shapeSlider.setPaintTicks(true);
         shapeSlider.setPaintLabels(true);
 
-        Hashtable<Integer, JLabel> scaleLabelTable = new Hashtable<Integer, JLabel>();
+        Hashtable<Integer, JLabel> scaleLabelTable = new Hashtable<>();
         for (int i = 0; i <= 50; i+=10) {
             scaleLabelTable.put(new Integer(i), new JLabel(String.valueOf(i/10)));
         }

File: demo/src/main/java/smile/demo/util/ParameterParser.java
Patch:
@@ -82,7 +82,7 @@ private class Parameter {
      */
     public ParameterParser(String usage) {
         this.usage = usage;
-        parameters = new HashMap<String, Parameter>();
+        parameters = new HashMap<>();
     }
 
     /**
@@ -135,7 +135,7 @@ public String get(String name) {
      * @return a list of expanded name-value pair if monads are detected
      */
     private List<String> filterMonadics(String[] args) {// name-value for monads
-        List<String> filteredArgs = new ArrayList<String>();       // Y <- return List
+        List<String> filteredArgs = new ArrayList<>();       // Y <- return List
         for (String arg : args) {                        // iterate over args
             filteredArgs.add(arg);
             Parameter param = parameters.get(arg);
@@ -153,7 +153,7 @@ private List<String> filterMonadics(String[] args) {// name-value for monads
      * name.
      */
     public List<String> parse(String[] args) {    // merge args & defaults
-        List<String> extras = new ArrayList<String>();
+        List<String> extras = new ArrayList<>();
         List<String> filteredArgs = filterMonadics(args);          // detect and fill mons
         for (int i = 0; i < filteredArgs.size(); i++) {
             String key = filteredArgs.get(i);

File: demo/src/main/java/smile/demo/vq/VQDemo.java
Patch:
@@ -114,7 +114,7 @@ public VQDemo() {
         startButton.setActionCommand("startButton");
         startButton.addActionListener(this);
 
-        datasetBox = new JComboBox<String>();
+        datasetBox = new JComboBox<>();
         for (int i = 0; i < datasetName.length; i++) {
             datasetBox.addItem(datasetName[i]);
         }

File: math/src/main/java/smile/math/Math.java
Patch:
@@ -4017,7 +4017,7 @@ public static double[][] pow(double[][] x, double n) {
      * @return the same values as in x but with no repetitions.
      */
     public static int[] unique(int[] x) {
-        HashSet<Integer> hash = new HashSet<Integer>();
+        HashSet<Integer> hash = new HashSet<>();
         for (int i = 0; i < x.length; i++) {
             hash.add(x[i]);
         }
@@ -4038,7 +4038,7 @@ public static int[] unique(int[] x) {
      * @return the same values as in x but with no repetitions.
      */
     public static String[] unique(String[] x) {
-        HashSet<String> hash = new HashSet<String>(Arrays.asList(x));
+        HashSet<String> hash = new HashSet<>(Arrays.asList(x));
 
         String[] y = new String[hash.size()];
 

File: math/src/main/java/smile/math/SparseArray.java
Patch:
@@ -67,7 +67,7 @@ public SparseArray() {
      * @param size the number of nonzero entries in the matrix.
      */
     private SparseArray(int initialCapacity) {
-        array = new ArrayList<Entry>(initialCapacity);
+        array = new ArrayList<>(initialCapacity);
     }
 
     /**

File: math/src/main/java/smile/stat/distribution/DiscreteExponentialFamilyMixture.java
Patch:
@@ -158,7 +158,7 @@ public DiscreteExponentialFamilyMixture(List<Component> mixture, int[] data) {
             }
 
             // Maximization step
-            List<Component> newConfig = new ArrayList<Component>();
+            List<Component> newConfig = new ArrayList<>();
             for (int i = 0; i < m; i++)
                 newConfig.add(((DiscreteExponentialFamily) components.get(i).distribution).M(x, posteriori[i]));
 

File: math/src/main/java/smile/stat/distribution/DiscreteMixture.java
Patch:
@@ -47,15 +47,15 @@ public static class Component {
      * Constructor.
      */
     DiscreteMixture() {
-        components = new ArrayList<Component>();
+        components = new ArrayList<>();
     }
 
     /**
      * Constructor.
      * @param mixture a list of discrete distributions.
      */
     public DiscreteMixture(List<Component> mixture) {
-        components = new ArrayList<Component>();
+        components = new ArrayList<>();
         components.addAll(mixture);
 
         double sum = 0.0;

File: math/src/main/java/smile/stat/distribution/ExponentialFamilyMixture.java
Patch:
@@ -159,7 +159,7 @@ public ExponentialFamilyMixture(List<Component> mixture, double[] data) {
             }
 
             // Maximization step
-            ArrayList<Component> newConfig = new ArrayList<Component>();
+            ArrayList<Component> newConfig = new ArrayList<>();
             for (int i = 0; i < m; i++)
                 newConfig.add(((ExponentialFamily) components.get(i).distribution).M(x, posteriori[i]));
 

File: math/src/main/java/smile/stat/distribution/GaussianMixture.java
Patch:
@@ -70,7 +70,7 @@ public GaussianMixture(double[] data) {
         if (data.length < 20)
             throw new IllegalArgumentException("Too few samples.");
         
-        ArrayList<Component> mixture = new ArrayList<Component>();
+        ArrayList<Component> mixture = new ArrayList<>();
         Component c = new Component();
         c.priori = 1.0;
         c.distribution = new GaussianDistribution(data);

File: math/src/main/java/smile/stat/distribution/Mixture.java
Patch:
@@ -61,15 +61,15 @@ public static class Component {
      * Constructor.
      */
     Mixture() {
-        components = new ArrayList<Component>();
+        components = new ArrayList<>();
     }
 
     /**
      * Constructor.
      * @param mixture a list of distributions.
      */
     public Mixture(List<Component> mixture) {
-        components = new ArrayList<Component>();
+        components = new ArrayList<>();
         components.addAll(mixture);
 
         double sum = 0.0;

File: math/src/main/java/smile/stat/distribution/MultivariateExponentialFamilyMixture.java
Patch:
@@ -155,7 +155,7 @@ public MultivariateExponentialFamilyMixture(List<Component> mixture, double[][]
             }
 
             // Maximization step
-            ArrayList<Component> newConfig = new ArrayList<Component>();
+            ArrayList<Component> newConfig = new ArrayList<>();
             for (int i = 0; i < m; i++)
                 newConfig.add(((MultivariateExponentialFamily) components.get(i).distribution).M(x, posteriori[i]));
 

File: math/src/main/java/smile/stat/distribution/MultivariateGaussianMixture.java
Patch:
@@ -168,7 +168,7 @@ public MultivariateGaussianMixture(double[][] data, boolean diagonal) {
         if (data.length < 20)
             throw new IllegalArgumentException("Too few samples.");
 
-        ArrayList<Component> mixture = new ArrayList<Component>();
+        ArrayList<Component> mixture = new ArrayList<>();
         Component c = new Component();
         c.priori = 1.0;
         c.distribution = new MultivariateGaussianDistribution(data, diagonal);

File: math/src/main/java/smile/stat/distribution/MultivariateMixture.java
Patch:
@@ -46,15 +46,15 @@ public static class Component {
      * Construct an empty Mixture.
      */
     MultivariateMixture() {
-        components = new ArrayList<Component>();
+        components = new ArrayList<>();
     }
 
     /**
      * Constructor.
      * @param mixture a list of multivariate distributions.
      */
     public MultivariateMixture(List<Component> mixture) {
-        components = new ArrayList<Component>();
+        components = new ArrayList<>();
         components.addAll(mixture);
     }
 

File: math/src/test/java/smile/math/distance/JaccardDistanceTest.java
Patch:
@@ -56,13 +56,13 @@ public void tearDown() {
     @Test
     public void testDistance() {
         System.out.println("distance");
-        Set<Integer> a = new HashSet<Integer>();
+        Set<Integer> a = new HashSet<>();
         a.add(1);
         a.add(2);
         a.add(3);
         a.add(4);
 
-        Set<Integer> b = new HashSet<Integer>();
+        Set<Integer> b = new HashSet<>();
         b.add(3);
         b.add(4);
         b.add(5);

File: math/src/test/java/smile/sort/HeapSelectTest.java
Patch:
@@ -55,7 +55,7 @@ public void tearDown() {
     @Test
     public void testSelect() {
         System.out.println("HeapSelect");
-        HeapSelect<Integer> instance = new HeapSelect<Integer>(new Integer[10]);
+        HeapSelect<Integer> instance = new HeapSelect<>(new Integer[10]);
         for (int i = 0; i < 1000; i++) {
             instance.add(i);
             if (i > 10) {
@@ -65,7 +65,7 @@ public void testSelect() {
             }
         }
 
-        instance = new HeapSelect<Integer>(new Integer[10]);
+        instance = new HeapSelect<>(new Integer[10]);
         for (int i = 0; i < 1000; i++) {
             instance.add(1000-i);
             if (i >= 9) {
@@ -82,7 +82,7 @@ public void testSelect() {
     @Test
     public void testSelectBig() {
         System.out.println("HeapSelect Big");
-        HeapSelect<Double> instance = new HeapSelect<Double>(new Double[10]);
+        HeapSelect<Double> instance = new HeapSelect<>(new Double[10]);
         for (int i = 0; i < 100000000; i++) {
             instance.add(Math.random());
         }

File: math/src/test/java/smile/stat/distribution/ExponentialFamilyMixtureTest.java
Patch:
@@ -71,7 +71,7 @@ public void testEM() {
         for (int i = 1000; i < 2000; i++)
             data[i] = gamma.rand();
 
-        Vector<Mixture.Component> m = new Vector<Mixture.Component>();
+        Vector<Mixture.Component> m = new Vector<>();
         Mixture.Component c = new Mixture.Component();
         c.priori = 0.25;
         c.distribution = new GaussianDistribution(0.0, 1.0);

File: nlp/src/main/java/smile/nlp/SimpleText.java
Patch:
@@ -37,7 +37,7 @@ public class SimpleText extends Text implements TextTerms, AnchorText {
     /**
      * The term frequency.
      */
-    private HashMap<String, Integer> freq = new HashMap<String, Integer>();
+    private HashMap<String, Integer> freq = new HashMap<>();
     /**
      * The maximum term frequency over all terms in the documents;
      */

File: nlp/src/main/java/smile/nlp/Trie.java
Patch:
@@ -55,7 +55,7 @@ public class Node {
         public Node(K key) {
             this.key = key;
             this.value = null;
-            this.children = new LinkedList<Node>();
+            this.children = new LinkedList<>();
         }
         
         public K getKey() {
@@ -117,15 +117,15 @@ public void addChild(K[] key, V value, int index) {
      * Constructor.
      */
     public Trie() {
-        root = new HashMap<K, Node>();
+        root = new HashMap<>();
     }
 
     /**
      * Constructor.
      * @param initialCapacity the initial capacity of root node.
      */
     public Trie(int initialCapacity) {
-        root = new HashMap<K, Node>(initialCapacity);
+        root = new HashMap<>(initialCapacity);
     }
 
     /**

File: nlp/src/main/java/smile/nlp/collocation/BigramCollocationFinder.java
Patch:
@@ -65,7 +65,7 @@ public BigramCollocationFinder(int minFreq) {
      */
     public BigramCollocation[] find(Corpus corpus, int k) {
         BigramCollocation[] bigrams = new BigramCollocation[k];
-        HeapSelect<BigramCollocation> heap = new HeapSelect<BigramCollocation>(bigrams);
+        HeapSelect<BigramCollocation> heap = new HeapSelect<>(bigrams);
         
         Iterator<Bigram> iterator = corpus.getBigrams();
         while (iterator.hasNext()) {
@@ -106,7 +106,7 @@ public BigramCollocation[] find(Corpus corpus, double p) {
 
         double cutoff = chisq.quantile(p);
         
-        ArrayList<BigramCollocation> bigrams = new ArrayList<BigramCollocation>();
+        ArrayList<BigramCollocation> bigrams = new ArrayList<>();
 
         Iterator<Bigram> iterator = corpus.getBigrams();
         while (iterator.hasNext()) {

File: nlp/src/main/java/smile/nlp/dictionary/EnglishDictionary.java
Patch:
@@ -43,7 +43,7 @@ public enum EnglishDictionary implements Dictionary {
      * text, in which each line is a word.
      */
     private EnglishDictionary(String resource) {
-        dict = new HashSet<String>();
+        dict = new HashSet<>();
 
         try (BufferedReader input = new BufferedReader(new InputStreamReader(this.getClass().getResourceAsStream(resource)))) {
         

File: nlp/src/main/java/smile/nlp/dictionary/EnglishPunctuations.java
Patch:
@@ -32,7 +32,7 @@ public class EnglishPunctuations implements Punctuations {
     /**
      * A set of punctuation marks.
      */
-    private HashSet<String> dict = new HashSet<String>(50);
+    private HashSet<String> dict = new HashSet<>(50);
 
     /**
      * Constructor.

File: nlp/src/main/java/smile/nlp/dictionary/EnglishStopWords.java
Patch:
@@ -54,7 +54,7 @@ public enum EnglishStopWords implements StopWords {
      * Constructor.
      */
     private EnglishStopWords(String resource) {
-        dict = new HashSet<String>();
+        dict = new HashSet<>();
 
         try (BufferedReader input = new BufferedReader(new InputStreamReader(this.getClass().getResourceAsStream(resource)))) {
             String line = null;

File: nlp/src/main/java/smile/nlp/dictionary/SimpleDictionary.java
Patch:
@@ -41,7 +41,7 @@ public class SimpleDictionary implements Dictionary {
      * text, in which each line is a word.
      */
     public SimpleDictionary(String resource) {
-        dict = new HashSet<String>();
+        dict = new HashSet<>();
 
         File file = new File(resource);
         try (BufferedReader input = file.exists() ?

File: nlp/src/main/java/smile/nlp/pos/EnglishPOSLexicon.java
Patch:
@@ -34,7 +34,7 @@ public class EnglishPOSLexicon {
     /**
      * A list of English words with POS tags.
      */
-    private static final HashMap<String, PennTreebankPOS[]> dict = new HashMap<String, PennTreebankPOS[]>();
+    private static final HashMap<String, PennTreebankPOS[]> dict = new HashMap<>();
 
     /**
      * The part-of-speech.txt file contains is a combination of

File: nlp/src/main/java/smile/nlp/pos/PennTreebankPOS.java
Patch:
@@ -413,7 +413,7 @@ public String toString() {
      */
     private static final Map<String, String> map;
     static {
-        map = new HashMap<String, String>();
+        map = new HashMap<>();
 
         map.put(".", "SENT");
         map.put("?", "SENT");

File: nlp/src/main/java/smile/nlp/stemmer/LancasterStemmer.java
Patch:
@@ -56,7 +56,7 @@ public class LancasterStemmer implements Stemmer {
         /**
          * Load rules from Lancaster_rules.txt
          */
-        RULES = new ArrayList<String>();
+        RULES = new ArrayList<>();
 
         try (BufferedReader input = new BufferedReader(new InputStreamReader(LancasterStemmer.class.getResourceAsStream("/smile/nlp/stemmer/Lancaster_rules.txt")))) {
             String line = null;

File: nlp/src/main/java/smile/nlp/tokenizer/BreakIteratorSentenceSplitter.java
Patch:
@@ -49,7 +49,7 @@ public BreakIteratorSentenceSplitter(Locale locale) {
     @Override
     public String[] split(String text) {
         boundary.setText(text);
-        ArrayList<String> sentences = new ArrayList<String>();
+        ArrayList<String> sentences = new ArrayList<>();
         int start = boundary.first();
         for (int end = boundary.next();
                 end != BreakIterator.DONE;

File: nlp/src/main/java/smile/nlp/tokenizer/BreakIteratorTokenizer.java
Patch:
@@ -49,7 +49,7 @@ public BreakIteratorTokenizer(Locale locale) {
     @Override
     public String[] split(String text) {
         boundary.setText(text);
-        ArrayList<String> words = new ArrayList<String>();
+        ArrayList<String> words = new ArrayList<>();
         int start = boundary.first();
         int end = boundary.next();
 

File: nlp/src/main/java/smile/nlp/tokenizer/EnglishAbbreviations.java
Patch:
@@ -36,7 +36,7 @@ class EnglishAbbreviations {
     private static final HashSet<String> DICTIONARY;
 
     static {
-        DICTIONARY = new HashSet<String>();
+        DICTIONARY = new HashSet<>();
 
         try (BufferedReader input = new BufferedReader(new InputStreamReader(EnglishAbbreviations.class.getResourceAsStream("/smile/nlp/tokenizer/abbreviations_en.txt")))) {
             String line = null;

File: nlp/src/main/java/smile/nlp/tokenizer/SimpleSentenceSplitter.java
Patch:
@@ -93,7 +93,7 @@ public static SimpleSentenceSplitter getInstance() {
 
     @Override
     public String[] split(String text) {
-        ArrayList<String> sentences = new ArrayList<String>();
+        ArrayList<String> sentences = new ArrayList<>();
 
         // The number of words in the sentence.
         int len = 0;

File: nlp/src/main/java/smile/nlp/tokenizer/SimpleTokenizer.java
Patch:
@@ -144,7 +144,7 @@ public String[] split(String text) {
             }
         }
         
-        ArrayList<String> result = new ArrayList<String>();
+        ArrayList<String> result = new ArrayList<>();
         for (String token : words) {
             if (!token.isEmpty()) {
                 result.add(token);

File: nlp/src/test/java/smile/nlp/collocation/AprioriPhraseExtractorTest.java
Patch:
@@ -71,7 +71,7 @@ public void testExtract() throws FileNotFoundException {
         
         PorterStemmer stemmer = new PorterStemmer();
         SimpleTokenizer tokenizer = new SimpleTokenizer();
-        ArrayList<String[]> sentences = new ArrayList<String[]>();
+        ArrayList<String[]> sentences = new ArrayList<>();
         for (String paragraph : SimpleParagraphSplitter.getInstance().split(text)) {
             for (String s : SimpleSentenceSplitter.getInstance().split(paragraph)) {
                 String[] sentence = tokenizer.split(s);

File: plot/src/main/java/smile/plot/Axis.java
Patch:
@@ -321,7 +321,7 @@ public Axis setRotation(double rotation) {
      */
     public Axis addLabel(String label, double location) {
         if (labels == null) {
-            labels = new HashMap<String, Double>();
+            labels = new HashMap<>();
         }
 
         labels.put(label, location);
@@ -340,7 +340,7 @@ public Axis addLabel(String[] label, double[] location) {
         }
 
         if (labels == null) {
-            labels = new HashMap<String, Double>();
+            labels = new HashMap<>();
         }
 
         for (int i = 0; i < label.length; i++) {

File: plot/src/main/java/smile/plot/Contour.java
Patch:
@@ -84,7 +84,7 @@ class Isoline {
         /**
          * The coordinates of points along the contour line.
          */
-        List<double[]> points = new ArrayList<double[]>();
+        List<double[]> points = new ArrayList<>();
         /**
          * The level value of contour line.
          */
@@ -450,7 +450,7 @@ private void init() {
         zMin = Math.min(z);
         zMax = Math.max(z);
 
-        contours = new ArrayList<Isoline>(numLevels);
+        contours = new ArrayList<>(numLevels);
 
         if (logScale && zMin <= 0.0) {
             throw new IllegalArgumentException("Log scale is not support for non-positive data");

File: plot/src/main/java/smile/plot/PlotCanvas.java
Patch:
@@ -135,7 +135,7 @@ public class PlotCanvas extends JPanel {
     /**
      * The shapes in the canvas, e.g. label, plots, etc.
      */
-    private List<Shape> shapes = new ArrayList<Shape>();
+    private List<Shape> shapes = new ArrayList<>();
     /**
      * The real canvas for plots.
      */

File: plot/src/main/java/smile/plot/ScatterPlot.java
Patch:
@@ -161,7 +161,7 @@ public ScatterPlot(double[][] data, int[] y, char[] legends, Color[] palette) {
         int[] id = Math.unique(y);
         Arrays.sort(id);
 
-        classLookupTable = new HashMap<Integer, Integer>(id.length);
+        classLookupTable = new HashMap<>(id.length);
 
         for (int i = 0; i < id.length; i++) {
             classLookupTable.put(id[i], i);

File: plot/src/main/java/smile/swing/FileChooser.java
Patch:
@@ -283,7 +283,7 @@ public static class SimpleFileFilter extends FileFilter {
         /**
          * The file extensions in lower case.
          */
-        private TreeSet<String> filters = new TreeSet<String>();
+        private TreeSet<String> filters = new TreeSet<>();
         /**
          * The human readable description of this filter.
          */

File: plot/src/main/java/smile/swing/FontChooser.java
Patch:
@@ -227,7 +227,7 @@ private JTextField getFontSizeTextField() {
 
     private JList<String> getFontFamilyList() {
         if (fontNameList == null) {
-            fontNameList = new JList<String>(getFontFamilies());
+            fontNameList = new JList<>(getFontFamilies());
             fontNameList.setSelectionMode(ListSelectionModel.SINGLE_SELECTION);
             fontNameList.addListSelectionListener(
                     new ListSelectionHandler(getFontFamilyTextField()));
@@ -240,7 +240,7 @@ private JList<String> getFontFamilyList() {
 
     private JList<String> getFontStyleList() {
         if (fontStyleList == null) {
-            fontStyleList = new JList<String>(getFontStyleNames());
+            fontStyleList = new JList<>(getFontStyleNames());
             fontStyleList.setSelectionMode(ListSelectionModel.SINGLE_SELECTION);
             fontStyleList.addListSelectionListener(
                     new ListSelectionHandler(getFontStyleTextField()));
@@ -253,7 +253,7 @@ private JList<String> getFontStyleList() {
 
     private JList<String> getFontSizeList() {
         if (fontSizeList == null) {
-            fontSizeList = new JList<String>(this.fontSizeStrings);
+            fontSizeList = new JList<>(this.fontSizeStrings);
             fontSizeList.setSelectionMode(ListSelectionModel.SINGLE_SELECTION);
             fontSizeList.addListSelectionListener(
                     new ListSelectionHandler(getFontSizeTextField()));

File: core/src/main/java/smile/association/FPGrowth.java
Patch:
@@ -337,7 +337,7 @@ private long grow(PrintStream out, List<ItemSet> list, TotalSupportTree ttree, H
                 for (int i = 0; i < itemset.length; i++) {
                     out.format("%d ", itemset[i]);
                 }
-                out.format("(%d)\n", support);
+                out.format("(%d)%n", support);
             }
         }
         if (ttree != null) {
@@ -364,7 +364,7 @@ private long grow(PrintStream out, List<ItemSet> list, TotalSupportTree ttree, H
                             for (int i = 0; i < newItemset.length; i++) {
                                 out.format("%d ", newItemset[i]);
                             }
-                            out.format("(%d)\n", support);
+                            out.format("(%d)%n", support);
                         }
                     }
                     if (ttree != null) {

File: core/src/main/java/smile/clustering/CLARANS.java
Patch:
@@ -321,11 +321,11 @@ public int predict(T x) {
     public String toString() {
         StringBuilder sb = new StringBuilder();
         
-        sb.append(String.format("CLARANS distortion: %.5f\n", distortion));
-        sb.append(String.format("Clusters of %d data points:\n", y.length));
+        sb.append(String.format("CLARANS distortion: %.5f%n", distortion));
+        sb.append(String.format("Clusters of %d data points:%n", y.length));
         for (int i = 0; i < k; i++) {
             int r = (int) Math.round(1000.0 * size[i] / y.length);
-            sb.append(String.format("%3d\t%5d (%2d.%1d%%)\n", i, size[i], r / 10, r % 10));
+            sb.append(String.format("%3d\t%5d (%2d.%1d%%)%n", i, size[i], r / 10, r % 10));
         }
         
         return sb.toString();

File: core/src/main/java/smile/clustering/DBScan.java
Patch:
@@ -239,14 +239,14 @@ public int predict(T x) {
     public String toString() {
         StringBuilder sb = new StringBuilder();
         
-        sb.append(String.format("DBScan clusters of %d data points:\n", y.length));
+        sb.append(String.format("DBScan clusters of %d data points:%n", y.length));
         for (int i = 0; i < k; i++) {
             int r = (int) Math.round(1000.0 * size[i] / y.length);
-            sb.append(String.format("%3d\t%5d (%2d.%1d%%)\n", i, size[i], r / 10, r % 10));
+            sb.append(String.format("%3d\t%5d (%2d.%1d%%)%n", i, size[i], r / 10, r % 10));
         }
 
         int r = (int) Math.round(1000.0 * size[k] / y.length);
-        sb.append(String.format("Noise\t%5d (%2d.%1d%%)\n", size[k], r / 10, r % 10));
+        sb.append(String.format("Noise\t%5d (%2d.%1d%%)%n", size[k], r / 10, r % 10));
         
         return sb.toString();
     }

File: core/src/main/java/smile/clustering/DENCLUE.java
Patch:
@@ -349,10 +349,10 @@ public int predict(double[] x) {
     public String toString() {
         StringBuilder sb = new StringBuilder();
 
-        sb.append(String.format("DENCLUE clusters of %d data points:\n", y.length));
+        sb.append(String.format("DENCLUE clusters of %d data points:%n", y.length));
         for (int i = 0; i < k; i++) {
             int r = (int) Math.round(1000.0 * size[i] / y.length);
-            sb.append(String.format("%3d\t%5d (%2d.%1d%%)\n", i, size[i], r / 10, r % 10));
+            sb.append(String.format("%3d\t%5d (%2d.%1d%%)%n", i, size[i], r / 10, r % 10));
         }
 
         return sb.toString();

File: core/src/main/java/smile/clustering/KMeans.java
Patch:
@@ -515,11 +515,11 @@ public Double call() {
     public String toString() {
         StringBuilder sb = new StringBuilder();
         
-        sb.append(String.format("K-Means distortion: %.5f\n", distortion));
-        sb.append(String.format("Clusters of %d data points of dimension %d:\n", y.length, centroids[0].length));
+        sb.append(String.format("K-Means distortion: %.5f%n", distortion));
+        sb.append(String.format("Clusters of %d data points of dimension %d:%n", y.length, centroids[0].length));
         for (int i = 0; i < k; i++) {
             int r = (int) Math.round(1000.0 * size[i] / y.length);
-            sb.append(String.format("%3d\t%5d (%2d.%1d%%)\n", i, size[i], r / 10, r % 10));
+            sb.append(String.format("%3d\t%5d (%2d.%1d%%)%n", i, size[i], r / 10, r % 10));
         }
         
         return sb.toString();

File: core/src/main/java/smile/clustering/MEC.java
Patch:
@@ -378,11 +378,11 @@ public int predict(T x) {
     public String toString() {
         StringBuilder sb = new StringBuilder();
 
-        sb.append(String.format("MEC cluster conditional entropy: %.5f\n", entropy));
-        sb.append(String.format("Clusters of %d data points:\n", y.length));
+        sb.append(String.format("MEC cluster conditional entropy: %.5f%n", entropy));
+        sb.append(String.format("Clusters of %d data points:%n", y.length));
         for (int i = 0; i < k; i++) {
             int r = (int) Math.round(1000.0 * size[i] / y.length);
-            sb.append(String.format("%3d\t%5d (%2d.%1d%%)\n", i, size[i], r / 10, r % 10));
+            sb.append(String.format("%3d\t%5d (%2d.%1d%%)%n", i, size[i], r / 10, r % 10));
         }
 
         return sb.toString();

File: core/src/main/java/smile/clustering/SIB.java
Patch:
@@ -520,11 +520,11 @@ public double[][] centroids() {
     public String toString() {
         StringBuilder sb = new StringBuilder();
         
-        sb.append(String.format("Sequential Information Bottleneck distortion: %.5f\n", distortion));
-        sb.append(String.format("Clusters of %d data points of dimension %d:\n", y.length, centroids[0].length));
+        sb.append(String.format("Sequential Information Bottleneck distortion: %.5f%n", distortion));
+        sb.append(String.format("Clusters of %d data points of dimension %d:%n", y.length, centroids[0].length));
         for (int i = 0; i < k; i++) {
             int r = (int) Math.round(1000.0 * size[i] / y.length);
-            sb.append(String.format("%3d\t%5d (%2d.%1d%%)\n", i, size[i], r / 10, r % 10));
+            sb.append(String.format("%3d\t%5d (%2d.%1d%%)%n", i, size[i], r / 10, r % 10));
         }
         
         return sb.toString();

File: core/src/main/java/smile/clustering/SpectralClustering.java
Patch:
@@ -336,11 +336,11 @@ public double distortion() {
     public String toString() {
         StringBuilder sb = new StringBuilder();
 
-        sb.append(String.format("Spectral Clustering distortion in feature space: %.5f\n", distortion));
-        sb.append(String.format("Clusters of %d data points:\n", y.length));
+        sb.append(String.format("Spectral Clustering distortion in feature space: %.5f%n", distortion));
+        sb.append(String.format("Clusters of %d data points:%n", y.length));
         for (int i = 0; i < k; i++) {
             int r = (int) Math.round(1000.0 * size[i] / y.length);
-            sb.append(String.format("%3d\t%5d (%2d.%1d%%)\n", i, size[i], r / 10, r % 10));
+            sb.append(String.format("%3d\t%5d (%2d.%1d%%)%n", i, size[i], r / 10, r % 10));
         }
 
         return sb.toString();

File: core/src/main/java/smile/clustering/XMeans.java
Patch:
@@ -235,11 +235,11 @@ private static double logLikelihood(int k, int n, int ni, int d, double variance
     public String toString() {
         StringBuilder sb = new StringBuilder();
 
-        sb.append(String.format("X-Means distortion: %.5f\n", distortion));
-        sb.append(String.format("Clusters of %d data points of dimension %d:\n", y.length, centroids[0].length));
+        sb.append(String.format("X-Means distortion: %.5f%n", distortion));
+        sb.append(String.format("Clusters of %d data points of dimension %d:%n", y.length, centroids[0].length));
         for (int i = 0; i < k; i++) {
             int r = (int) Math.round(1000.0 * size[i] / y.length);
-            sb.append(String.format("%3d\t%5d (%2d.%1d%%)\n", i, size[i], r / 10, r % 10));
+            sb.append(String.format("%3d\t%5d (%2d.%1d%%)%n", i, size[i], r / 10, r % 10));
         }
 
         return sb.toString();

File: core/src/main/java/smile/sequence/HMM.java
Patch:
@@ -714,7 +714,7 @@ private double[][] estimateGamma(double[][][] xi) {
     @Override
     public String toString() {
         StringBuilder sb = new StringBuilder();
-        sb.append(String.format("HMM (%d states, %d emission symbols)\n", numStates, numSymbols));
+        sb.append(String.format("HMM (%d states, %d emission symbols)%n", numStates, numSymbols));
 
         sb.append("\tInitial state probability:\n\t\t");
         for (int i = 0; i < numStates; i++) {

File: core/src/main/java/smile/vq/NeuralGas.java
Patch:
@@ -264,11 +264,11 @@ public int predict(double[] x) {
     public String toString() {
         StringBuilder sb = new StringBuilder();
         
-        sb.append(String.format("Neural Gas distortion: %.5f\n", distortion));
-        sb.append(String.format("Clusters of %d data points of dimension %d:\n", y.length, centroids[0].length));
+        sb.append(String.format("Neural Gas distortion: %.5f%n", distortion));
+        sb.append(String.format("Clusters of %d data points of dimension %d:%n", y.length, centroids[0].length));
         for (int i = 0; i < k; i++) {
             int r = (int) Math.round(1000.0 * size[i] / y.length);
-            sb.append(String.format("%3d\t%5d (%2d.%1d%%)\n", i, size[i], r / 10, r % 10));
+            sb.append(String.format("%3d\t%5d (%2d.%1d%%)%n", i, size[i], r / 10, r % 10));
         }
         
         return sb.toString();

File: core/src/test/java/smile/classification/DecisionTreeTest.java
Patch:
@@ -145,7 +145,7 @@ public void testUSPS() {
                 }
             }
 
-            System.out.format("USPS error rate = %.2f%%\n", 100.0 * error / testx.length);
+            System.out.format("USPS error rate = %.2f%%%n", 100.0 * error / testx.length);
             assertEquals(328, error);
         } catch (Exception ex) {
             System.err.println(ex);
@@ -200,12 +200,12 @@ public void testUSPSNominal() {
                 }
             }
 
-            System.out.format("USPS error rate = %.2f%%\n", 100.0 * error / testx.length);
+            System.out.format("USPS error rate = %.2f%%%n", 100.0 * error / testx.length);
             
             double[] importance = tree.importance();
             int[] index = QuickSort.sort(importance);
             for (int i = importance.length; i-- > 0; ) {
-                System.out.format("%s importance is %.4f\n", train.attributes()[index[i]], importance[i]);
+                System.out.format("%s importance is %.4f%n", train.attributes()[index[i]], importance[i]);
             }
             
             assertEquals(324, error);

File: core/src/test/java/smile/classification/FLDTest.java
Patch:
@@ -112,7 +112,7 @@ public void testUSPS() {
                 }
             }
 
-            System.out.format("USPS error rate = %.2f%%\n", 100.0 * error / testx.length);
+            System.out.format("USPS error rate = %.2f%%%n", 100.0 * error / testx.length);
             assertEquals(521, error);
         } catch (Exception ex) {
             System.err.println(ex);

File: core/src/test/java/smile/classification/KNNTest.java
Patch:
@@ -177,7 +177,7 @@ public void testSegment() throws ParseException {
                 }
             }
 
-            System.out.format("Segment error rate = %.2f%%\n", 100.0 * error / testx.length);
+            System.out.format("Segment error rate = %.2f%%%n", 100.0 * error / testx.length);
             assertEquals(39, error);
         } catch (IOException ex) {
             System.err.println(ex);
@@ -210,7 +210,7 @@ public void testUSPS() {
                 }
             }
 
-            System.out.format("USPS error rate = %.2f%%\n", 100.0 * error / testx.length);
+            System.out.format("USPS error rate = %.2f%%%n", 100.0 * error / testx.length);
             assertEquals(113, error);
         } catch (Exception ex) {
             System.err.println(ex);

File: core/src/test/java/smile/classification/LDATest.java
Patch:
@@ -115,7 +115,7 @@ public void testUSPS() {
                 }
             }
 
-            System.out.format("USPS error rate = %.2f%%\n", 100.0 * error / testx.length);
+            System.out.format("USPS error rate = %.2f%%%n", 100.0 * error / testx.length);
             assertEquals(256, error);
         } catch (Exception ex) {
             System.err.println(ex);

File: core/src/test/java/smile/classification/LogisticRegressionTest.java
Patch:
@@ -152,7 +152,7 @@ public void testSegment() {
                 }
             }
 
-            System.out.format("Segment error rate = %.2f%%\n", 100.0 * error / testx.length);
+            System.out.format("Segment error rate = %.2f%%%n", 100.0 * error / testx.length);
             assertEquals(48, error);
         } catch (Exception ex) {
             System.err.println(ex);
@@ -185,7 +185,7 @@ public void testUSPS() {
                 }
             }
 
-            System.out.format("USPS error rate = %.2f%%\n", 100.0 * error / testx.length);
+            System.out.format("USPS error rate = %.2f%%%n", 100.0 * error / testx.length);
             assertEquals(188, error);
         } catch (Exception ex) {
             System.err.println(ex);

File: core/src/test/java/smile/classification/RBFNetworkTest.java
Patch:
@@ -124,7 +124,7 @@ public void testSegment() {
                 }
             }
 
-            System.out.format("Segment error rate = %.2f%%\n", 100.0 * error / testx.length);
+            System.out.format("Segment error rate = %.2f%%%n", 100.0 * error / testx.length);
             assertTrue(error <= 210);
         } catch (Exception ex) {
             System.err.println(ex);
@@ -159,7 +159,7 @@ public void testUSPS() {
                 }
             }
 
-            System.out.format("USPS error rate = %.2f%%\n", 100.0 * error / testx.length);
+            System.out.format("USPS error rate = %.2f%%%n", 100.0 * error / testx.length);
             assertTrue(error <= 150);
         } catch (Exception ex) {
             System.err.println(ex);

File: core/src/test/java/smile/classification/RDATest.java
Patch:
@@ -234,7 +234,7 @@ public void testUSPS() {
                 }
             }
 
-            System.out.format("USPS error rate = %.2f%%\n", 100.0 * error / testx.length);
+            System.out.format("USPS error rate = %.2f%%%n", 100.0 * error / testx.length);
             assertEquals(235, error);
         } catch (Exception ex) {
             System.err.println(ex);

File: core/src/test/java/smile/classification/SVMTest.java
Patch:
@@ -158,7 +158,7 @@ public void testSegment() {
                 }
             }
 
-            System.out.format("Segment error rate = %.2f%%\n", 100.0 * error / testx.length);
+            System.out.format("Segment error rate = %.2f%%%n", 100.0 * error / testx.length);
             assertTrue(error < 70);
         } catch (Exception ex) {
             ex.printStackTrace();
@@ -193,7 +193,7 @@ public void testUSPS() {
                 }
             }
 
-            System.out.format("USPS error rate = %.2f%%\n", 100.0 * error / testx.length);
+            System.out.format("USPS error rate = %.2f%%%n", 100.0 * error / testx.length);
             assertTrue(error < 95);
             
             System.out.println("USPS one more epoch...");
@@ -210,7 +210,7 @@ public void testUSPS() {
                     error++;
                 }
             }
-            System.out.format("USPS error rate = %.2f%%\n", 100.0 * error / testx.length);
+            System.out.format("USPS error rate = %.2f%%%n", 100.0 * error / testx.length);
             assertTrue(error < 95);
         } catch (Exception ex) {
             ex.printStackTrace();

File: core/src/test/java/smile/clustering/BIRCHTest.java
Patch:
@@ -90,7 +90,7 @@ public void testUSPS() {
             
             double r = rand.measure(y, p);
             double r2 = ari.measure(y, p);
-            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
             assertTrue(r > 0.65);
             assertTrue(r2 > 0.20);
             
@@ -101,7 +101,7 @@ public void testUSPS() {
             
             r = rand.measure(testy, p);
             r2 = ari.measure(testy, p);
-            System.out.format("Testing rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+            System.out.format("Testing rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
             assertTrue(r > 0.65);
             assertTrue(r2 > 0.20);
         } catch (Exception ex) {

File: core/src/test/java/smile/clustering/CLARANSTest.java
Patch:
@@ -76,7 +76,7 @@ public void testUSPS() {
 
             double r = rand.measure(y, clarans.getClusterLabel());
             double r2 = ari.measure(y, clarans.getClusterLabel());
-            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
             assertTrue(r > 0.8);
             assertTrue(r2 > 0.28);
             
@@ -87,7 +87,7 @@ public void testUSPS() {
             
             r = rand.measure(testy, p);
             r2 = ari.measure(testy, p);
-            System.out.format("Testing rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+            System.out.format("Testing rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
             assertTrue(r > 0.8);
             assertTrue(r2 > 0.25);
         } catch (Exception ex) {

File: core/src/test/java/smile/clustering/DBScanTest.java
Patch:
@@ -116,7 +116,7 @@ public void testToy() {
         double r = rand.measure(y1, y2);
         double r2 = ari.measure(y1, y2);
         System.out.println("The number of clusters: " + dbscan.getNumClusters());
-        System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+        System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
         assertTrue(r > 0.40);
         assertTrue(r2 > 0.15);
     }

File: core/src/test/java/smile/clustering/DENCLUETest.java
Patch:
@@ -98,7 +98,7 @@ public void testToy() {
         double r = rand.measure(label, denclue.getClusterLabel());
         double r2 = ari.measure(label, denclue.getClusterLabel());
         System.out.println("The number of clusters: " + denclue.getNumClusters());
-        System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+        System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
         assertTrue(r > 0.54);
         assertTrue(r2 > 0.2);
     }

File: core/src/test/java/smile/clustering/DeterministicAnnealingTest.java
Patch:
@@ -75,7 +75,7 @@ public void testUSPS() {
             RandIndex rand = new RandIndex();
             double r = rand.measure(y, annealing.getClusterLabel());
             double r2 = ari.measure(y, annealing.getClusterLabel());
-            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
             assertTrue(r > 0.75);
             assertTrue(r2 > 0.25);
             
@@ -86,7 +86,7 @@ public void testUSPS() {
             
             r = rand.measure(testy, p);
             r2 = ari.measure(testy, p);
-            System.out.format("Testing rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+            System.out.format("Testing rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
             assertTrue(r > 0.75);
             assertTrue(r2 > 0.3);
         } catch (Exception ex) {

File: core/src/test/java/smile/clustering/GMeansTest.java
Patch:
@@ -75,7 +75,7 @@ public void testUSPS() {
             
             double r = rand.measure(y, gmeans.getClusterLabel());
             double r2 = ari.measure(y, gmeans.getClusterLabel());
-            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
             assertTrue(r > 0.85);
             assertTrue(r2 > 0.4);
             
@@ -86,7 +86,7 @@ public void testUSPS() {
             
             r = rand.measure(testy, p);
             r2 = ari.measure(testy, p);
-            System.out.format("Testing rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+            System.out.format("Testing rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
             assertTrue(r > 0.85);
             assertTrue(r2 > 0.4);
         } catch (Exception ex) {

File: core/src/test/java/smile/clustering/MECTest.java
Patch:
@@ -76,7 +76,7 @@ public void testUSPS() {
             
             double r = rand.measure(y, mec.getClusterLabel());
             double r2 = ari.measure(y, mec.getClusterLabel());
-            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
             assertTrue(r > 0.85);
             assertTrue(r2 > 0.35);
             
@@ -87,7 +87,7 @@ public void testUSPS() {
             
             r = rand.measure(testy, p);
             r2 = ari.measure(testy, p);
-            System.out.format("Testing rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+            System.out.format("Testing rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
             assertTrue(r > 0.85);
             assertTrue(r2 > 0.35);
         } catch (Exception ex) {

File: core/src/test/java/smile/clustering/SIBTest.java
Patch:
@@ -71,7 +71,7 @@ public void testParseNG20() throws Exception {
             RandIndex rand = new RandIndex();
             double r = rand.measure(y, sib.getClusterLabel());
             double r2 = ari.measure(y, sib.getClusterLabel());
-            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
             assertTrue(r > 0.85);
             assertTrue(r2 > 0.2);
             
@@ -82,7 +82,7 @@ public void testParseNG20() throws Exception {
             
             r = rand.measure(testy, p);
             r2 = ari.measure(testy, p);
-            System.out.format("Testing rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+            System.out.format("Testing rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
             assertTrue(r > 0.85);
             assertTrue(r2 > 0.2);
         } catch (Exception ex) {

File: core/src/test/java/smile/clustering/SpectralClusteringTest.java
Patch:
@@ -72,7 +72,7 @@ public void testUSPS() {
             RandIndex rand = new RandIndex();
             double r = rand.measure(y, spectral.getClusterLabel());
             double r2 = ari.measure(y, spectral.getClusterLabel());
-            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
             assertTrue(r > 0.85);
             assertTrue(r2 > 0.45);            
         } catch (Exception ex) {
@@ -100,7 +100,7 @@ public void testUSPSNystrom() {
             RandIndex rand = new RandIndex();
             double r = rand.measure(y, spectral.getClusterLabel());
             double r2 = ari.measure(y, spectral.getClusterLabel());
-            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
             assertTrue(r > 0.8);
             assertTrue(r2 > 0.35);            
         } catch (Exception ex) {

File: core/src/test/java/smile/clustering/XMeansTest.java
Patch:
@@ -76,7 +76,7 @@ public void testUSPS() {
             
             double r = rand.measure(y, xmeans.getClusterLabel());
             double r2 = ari.measure(y, xmeans.getClusterLabel());
-            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
             assertTrue(r > 0.85);
             assertTrue(r2 > 0.4);
             
@@ -87,7 +87,7 @@ public void testUSPS() {
             
             r = rand.measure(testy, p);
             r2 = ari.measure(testy, p);
-            System.out.format("Testing rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+            System.out.format("Testing rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
             assertTrue(r > 0.85);
             assertTrue(r2 > 0.4);
         } catch (Exception ex) {

File: core/src/test/java/smile/feature/SumSquaresRatioTest.java
Patch:
@@ -125,7 +125,7 @@ public void testLearn() {
             }
 
             double accuracy = new Accuracy().measure(testy, prediction);
-            System.out.format("SSR %.2f%%\n", 100 * accuracy);
+            System.out.format("SSR %.2f%%%n", 100 * accuracy);
         } catch (Exception ex) {
             System.err.println(ex);
         }

File: core/src/test/java/smile/neighbor/CoverTreeStringSpeedTest.java
Patch:
@@ -52,14 +52,14 @@ public CoverTreeStringSpeedTest() {
         }
 
         double time = (System.currentTimeMillis() - start) / 1000.0;
-        System.out.format("Loading data: %.2fs\n", time);
+        System.out.format("Loading data: %.2fs%n", time);
 
         String[] data = words.toArray(new String[words.size()]);
 
         start = System.currentTimeMillis();
         cover = new CoverTree<String>(data, new EditDistance(50, true));
         time = (System.currentTimeMillis() - start) / 1000.0;
-        System.out.format("Building cover tree: %.2fs\n", time);
+        System.out.format("Building cover tree: %.2fs%n", time);
     }
 
     @BeforeClass
@@ -91,6 +91,6 @@ public void testNaiveSpeed() {
             neighbors.clear();
         }
         double time = (System.currentTimeMillis() - start) / 1000.0;
-        System.out.format("Cover tree string search: %.2fs\n", time);
+        System.out.format("Cover tree string search: %.2fs%n", time);
     }
 }
\ No newline at end of file

File: core/src/test/java/smile/projection/GHATest.java
Patch:
@@ -135,7 +135,7 @@ public void testLearn() {
             error /= USArrests.length;
 
             if (iter % 100 == 0) {
-                System.out.format("Iter %3d, Error = %.5g\n", iter, error);
+                System.out.format("Iter %3d, Error = %.5g%n", iter, error);
             }
         }
 

File: core/src/test/java/smile/regression/RegressionTreeTest.java
Patch:
@@ -132,7 +132,7 @@ public void test(String dataset, String url, int response) {
                 }
             }
 
-            System.out.format("10-CV RMSE = %.4f \t AbsoluteDeviation = %.4f\n", Math.sqrt(rss/n), ad/n);
+            System.out.format("10-CV RMSE = %.4f \t AbsoluteDeviation = %.4f%n", Math.sqrt(rss/n), ad/n);
          } catch (Exception ex) {
              System.err.println(ex);
          }
@@ -186,12 +186,12 @@ public void testCPU() {
             }
 
             RegressionTree tree = new RegressionTree(data.attributes(), trainx, trainy, 20);
-            System.out.format("RMSE = %.4f\n", Validation.test(tree, testx, testy));
+            System.out.format("RMSE = %.4f%n", Validation.test(tree, testx, testy));
             
             double[] importance = tree.importance();
             index = QuickSort.sort(importance);
             for (int i = importance.length; i-- > 0; ) {
-                System.out.format("%s importance is %.4f\n", data.attributes()[index[i]], importance[i]);
+                System.out.format("%s importance is %.4f%n", data.attributes()[index[i]], importance[i]);
             }
         } catch (Exception ex) {
             System.err.println(ex);

File: core/src/test/java/smile/regression/RidgeRegressionTest.java
Patch:
@@ -144,7 +144,7 @@ public void testPredict() {
                 rss += r * r;
             }
 
-            System.out.format("LOOCV MSE with lambda %.2f = %.3f\n", 0.01*lambda, rss/n);
+            System.out.format("LOOCV MSE with lambda %.2f = %.3f%n", 0.01*lambda, rss/n);
         }
     }
 

File: core/src/test/java/smile/validation/BootstrapTest.java
Patch:
@@ -105,8 +105,8 @@ public void testOrthogonal() {
             }
         }
 
-        System.out.format("Train coverage: %d\t%d\t%d\n", Math.min(trainhit), Math.median(trainhit), Math.max(trainhit));
-        System.out.format("Test coverage: %d\t%d\t%d\n", Math.min(testhit), Math.median(testhit), Math.max(testhit));
+        System.out.format("Train coverage: %d\t%d\t%d%n", Math.min(trainhit), Math.median(trainhit), Math.max(trainhit));
+        System.out.format("Test coverage: %d\t%d\t%d%n", Math.min(testhit), Math.median(testhit), Math.max(testhit));
 
         for (int j = 0; j < n; j++) {
             assertTrue(trainhit[j] > 60);

File: core/src/test/java/smile/vq/GrowingNeuralGasTest.java
Patch:
@@ -91,7 +91,7 @@ public void testUSPS() {
             
             double r = rand.measure(y, p);
             double r2 = ari.measure(y, p);
-            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
             assertTrue(r > 0.85);
             assertTrue(r2 > 0.40);
             
@@ -102,7 +102,7 @@ public void testUSPS() {
             
             r = rand.measure(testy, p);
             r2 = ari.measure(testy, p);
-            System.out.format("Testing rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+            System.out.format("Testing rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
             assertTrue(r > 0.85);
             assertTrue(r2 > 0.40);
         } catch (Exception ex) {

File: core/src/test/java/smile/vq/NeuralGasTest.java
Patch:
@@ -75,7 +75,7 @@ public void testUSPS() {
             RandIndex rand = new RandIndex();
             double r = rand.measure(y, gas.getClusterLabel());
             double r2 = ari.measure(y, gas.getClusterLabel());
-            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
             assertTrue(r > 0.88);
             assertTrue(r2 > 0.45);
             
@@ -86,7 +86,7 @@ public void testUSPS() {
             
             r = rand.measure(testy, p);
             r2 = ari.measure(testy, p);
-            System.out.format("Testing rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+            System.out.format("Testing rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
             assertTrue(r > 0.88);
             assertTrue(r2 > 0.45);
         } catch (Exception ex) {

File: core/src/test/java/smile/vq/NeuralMapTest.java
Patch:
@@ -89,7 +89,7 @@ public void testUSPS() {
             
             double r = rand.measure(y, p);
             double r2 = ari.measure(y, p);
-            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
             //assertTrue(r > 0.65);
             //assertTrue(r2 > 0.18);
             
@@ -100,7 +100,7 @@ public void testUSPS() {
             
             r = rand.measure(testy, p);
             r2 = ari.measure(testy, p);
-            System.out.format("Testing rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+            System.out.format("Testing rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
             //assertTrue(r > 0.65);
             //assertTrue(r2 > 0.18);
         } catch (Exception ex) {

File: core/src/test/java/smile/vq/SOMTest.java
Patch:
@@ -77,7 +77,7 @@ public void testUSPS() {
             RandIndex rand = new RandIndex();
             double r = rand.measure(y, label);
             double r2 = ari.measure(y, label);
-            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
             assertTrue(r > 0.88);
             assertTrue(r2 > 0.45);
             
@@ -88,7 +88,7 @@ public void testUSPS() {
             
             r = rand.measure(testy, p);
             r2 = ari.measure(testy, p);
-            System.out.format("Testing rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+            System.out.format("Testing rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
             assertTrue(r > 0.88);
             assertTrue(r2 > 0.45);
         } catch (Exception ex) {

File: core/src/test/java/smile/wavelet/BestLocalizedWaveletTest.java
Patch:
@@ -53,7 +53,7 @@ public void testFilter() {
         System.out.println("filter");
         int[] order = {14, 18, 20};
         for (int p : order) {
-            System.out.format("p = %d\n", p);
+            System.out.format("p = %d%n", p);
             double[] a = {.2, -.4, -.6, -.5, -.8, -.4, -.9, 0, -.2, .1, -.1, .1, .7, .9, 0, .3};
             double[] b = a.clone();
             Wavelet instance = new BestLocalizedWavelet(p);

File: core/src/test/java/smile/wavelet/CoifletWaveletTest.java
Patch:
@@ -52,7 +52,7 @@ public void tearDown() {
     public void testFilter() {
         System.out.println("filter");
         for (int p = 6; p <= 30; p += 6) {
-            System.out.format("p = %d\n", p);
+            System.out.format("p = %d%n", p);
             double[] a = {.2, -.4, -.6, -.5, -.8, -.4, -.9, 0, -.2, .1, -.1, .1, .7, .9, 0, .3};
             double[] b = a.clone();
             Wavelet instance = new CoifletWavelet(p);

File: core/src/test/java/smile/wavelet/DaubechiesWaveletTest.java
Patch:
@@ -55,7 +55,7 @@ public void tearDown() {
     public void testFilter() {
         System.out.println("filter");
         for (int p = 2; p <= 20; p += 2) {
-            System.out.format("p = %d\n", p);
+            System.out.format("p = %d%n", p);
             double[] a = {.2, -.4, -.6, -.5, -.8, -.4, -.9, 0, -.2, .1, -.1, .1, .7, .9, 0, .3};
             double[] b = a.clone();
             Wavelet instance = new DaubechiesWavelet(p);

File: core/src/test/java/smile/wavelet/SymmletWaveletTest.java
Patch:
@@ -52,7 +52,7 @@ public void tearDown() {
     public void testFilter() {
         System.out.println("filter");
         for (int p = 8; p <= 20; p += 2) {
-            System.out.format("p = %d\n", p);
+            System.out.format("p = %d%n", p);
             double[] a = {.2, -.4, -.6, -.5, -.8, -.4, -.9, 0, -.2, .1, -.1, .1, .7, .9, 0, .3};
             double[] b = a.clone();
             Wavelet instance = new SymmletWavelet(p);

File: math/src/test/java/smile/stat/distribution/ShiftedGeometricDistributionTest.java
Patch:
@@ -59,7 +59,7 @@ public void testShiftedGeometricDistribution() {
         for (int i = 0; i < data.length; i++)
             data[i] = (int) instance.rand();
         ShiftedGeometricDistribution est = new ShiftedGeometricDistribution(data);
-        assertEquals(0.0, (0.4 - est.getProb()) / 0.4, 0.05);
+        assertEquals(0.0, (0.4 - est.getProb()) / 0.4, 0.1);
     }
 
     /**

File: core/src/main/java/smile/classification/SVM.java
Patch:
@@ -750,7 +750,7 @@ boolean process(T x, int y, double weight) {
             // Compute gradient
             double g = y;
             DoubleArrayList kcache = new DoubleArrayList(sv.size() + 1);
-            if (sv.size() > 0) {
+            if (!sv.isEmpty()) {
                 for (SupportVector v : sv) {
                     if (v != null) {
                         // Bail out if already in expansion?

File: core/src/main/java/smile/clustering/MEC.java
Patch:
@@ -230,7 +230,7 @@ private void learn(T[] data, RNNSearch<T,T> nns, int k, double radius, int[] y)
         // The number of samples with nonzero conditional entropy.
         entropy = 0.0;
         for (int i = 0; i < n; i++) {
-            if (neighbors.get(i).size() > 0) {
+            if (!neighbors.get(i).isEmpty()) {
                 int ni = neighbors.get(i).size();
                 double m = 0.0;
                 for (int j = 0; j < k; j++) {
@@ -295,7 +295,7 @@ private void learn(T[] data, RNNSearch<T,T> nns, int k, double radius, int[] y)
             double prevObj = entropy;
             entropy = 0.0;
             for (int i = 0; i < n; i++) {
-                if (neighbors.get(i).size() > 0) {
+                if (!neighbors.get(i).isEmpty()) {
                     int ni = neighbors.get(i).size();
                     double m = 0.0;
                     for (int j = 0; j < k; j++) {

File: core/src/main/java/smile/neighbor/CoverTree.java
Patch:
@@ -304,7 +304,7 @@ private Node batchInsert(int p, int maxScale, int topScale, ArrayList<DistanceSe
                 ArrayList<Node> children = new ArrayList<Node>();
                 Node leaf = newLeaf(p);
                 children.add(leaf);
-                while (pointSet.size() > 0) {
+                while (!pointSet.isEmpty()) {
                     DistanceSet set = pointSet.get(pointSet.size() - 1);
                     pointSet.remove(pointSet.size() - 1);
                     leaf = newLeaf(set.idx);
@@ -533,7 +533,7 @@ public Neighbor<E, E>[] knn(E q, int k) {
             emptyHeap = false;
         }
 
-        while (currentCoverSet.size() > 0) {
+        while (!currentCoverSet.isEmpty()) {
             ArrayList<DistanceNode> nextCoverSet = new ArrayList<DistanceNode>();
             for (int i = 0; i < currentCoverSet.size(); i++) {
                 DistanceNode par = currentCoverSet.get(i);
@@ -597,7 +597,7 @@ public void range(E q, double radius, List<Neighbor<E, E>> neighbors) {
         double d = distance.d(root.getObject(), q);
         currentCoverSet.add(new DistanceNode(d, root));
 
-        while (currentCoverSet.size() > 0) {
+        while (!currentCoverSet.isEmpty()) {
             ArrayList<DistanceNode> nextCoverSet = new ArrayList<DistanceNode>();
             for (int i = 0; i < currentCoverSet.size(); i++) {
                 DistanceNode par = currentCoverSet.get(i);

File: core/src/main/java/smile/taxonomy/Concept.java
Patch:
@@ -366,7 +366,7 @@ public List<Concept> getPathToRoot() {
     @Override
     public String toString() {
         String displayName = "anonymous";
-        if (synset != null && synset.size() > 0) {
+        if (synset != null && !synset.isEmpty()) {
             StringBuilder builder = new StringBuilder();
             builder.append('(');
             Iterator<String> iter = synset.iterator();

File: core/src/main/java/smile/vq/GrowingNeuralGas.java
Patch:
@@ -316,11 +316,11 @@ public void update(double[] x) {
                     edge.a.edges.remove(edge);
                     // If it results in neuron having no emanating edges,
                     // remove the neuron as well.
-                    if (edge.a.edges.size() == 0)
+                    if (edge.a.edges.isEmpty())
                         nodes.remove(edge.a);
                 } else {
                     edge.b.edges.remove(edge);
-                    if (edge.b.edges.size() == 0)
+                    if (edge.b.edges.isEmpty())
                         nodes.remove(edge.b);
                 }
             }

File: core/src/main/java/smile/vq/NeuralMap.java
Patch:
@@ -502,14 +502,14 @@ public void update(double[] x) {
                 iter.remove();
             }
 
-            if (neighbor.neighbors.size() > 0) {
+            if (!neighbor.neighbors.isEmpty()) {
                 lsh.add(neighbor);
             } else {
                 neurons.remove(neighbor);
             }
         }
 
-        if (neuron.neighbors.size() == 0) {
+        if (neuron.neighbors.isEmpty()) {
             lsh.remove(neuron);
             neurons.remove(neuron);
         }

File: core/src/test/java/smile/neighbor/LSHTest.java
Patch:
@@ -161,7 +161,7 @@ public void testRange() {
                     }
                 }
             }
-            if (n2.size() > 0) {
+            if (!n2.isEmpty()) {
                 recall += 1.0 * hit / n2.size();
             }
         }

File: core/src/test/java/smile/neighbor/MPLSHTest.java
Patch:
@@ -161,7 +161,7 @@ public void testRangePosteriori() {
                     }
                 }
             }
-            if (n2.size() > 0) {
+            if (!n2.isEmpty()) {
                 recall += 1.0 * hit / n2.size();
             }
         }

File: graph/src/main/java/smile/graph/AdjacencyList.java
Patch:
@@ -451,7 +451,7 @@ public int[] sortbfs() {
             }
         }
 
-        for (int i = 0; queue.size() > 0; i++) {
+        for (int i = 0; !queue.isEmpty(); i++) {
             int t = queue.poll();
             ts[i] = t;
             for (Edge edge : graph[t]) {
@@ -475,7 +475,7 @@ private void bfs(int v, int[] cc, int id) {
         cc[v] = id;
         Queue<Integer> queue = new LinkedList<Integer>();
         queue.offer(v);
-        while (queue.size() > 0) {
+        while (!queue.isEmpty()) {
             int t = queue.poll();
             for (Edge edge : graph[t]) {
                 int i = edge.v2;
@@ -533,7 +533,7 @@ private void bfs(Visitor visitor, int v, int[] cc, int id) {
         cc[v] = id;
         Queue<Integer> queue = new LinkedList<Integer>();
         queue.offer(v);
-        while (queue.size() > 0) {
+        while (!queue.isEmpty()) {
             int t = queue.poll();
             for (Edge edge : graph[t]) {
                 int i = edge.v2;

File: graph/src/main/java/smile/graph/AdjacencyMatrix.java
Patch:
@@ -390,7 +390,7 @@ public int[] sortbfs() {
             }
         }
 
-        for (int i = 0; queue.size() > 0; i++) {
+        for (int i = 0; !queue.isEmpty(); i++) {
             int t = queue.poll();
             ts[i] = t;
             for (int v = 0; v < n; v++) {
@@ -415,7 +415,7 @@ private void bfs(int v, int[] cc, int id) {
         cc[v] = id;
         Queue<Integer> queue = new LinkedList<Integer>();
         queue.offer(v);
-        while (queue.size() > 0) {
+        while (!queue.isEmpty()) {
             int t = queue.poll();
             for (int i = 0; i < n; i++) {
                 if (graph[t][i] != 0.0 && cc[i] == -1) {
@@ -469,7 +469,7 @@ private void bfs(Visitor visitor, int v, int[] cc, int id) {
         cc[v] = id;
         Queue<Integer> queue = new LinkedList<Integer>();
         queue.offer(v);
-        while (queue.size() > 0) {
+        while (!queue.isEmpty()) {
             int t = queue.poll();
             for (int i = 0; i < n; i++) {
                 if (graph[t][i] != 0.0 && cc[i] == -1) {

File: nlp/src/main/java/smile/nlp/pos/HMMPOSTagger.java
Patch:
@@ -355,7 +355,7 @@ public static void load(String dir, List<String[]> sentences, List<PennTreebankP
                 while ((line = reader.readLine()) != null) {
                     line = line.trim();
                     if (line.isEmpty()) {
-                        if (sent.size() > 0) {
+                        if (!sent.isEmpty()) {
                             sentences.add(sent.toArray(new String[sent.size()]));
                             labels.add(label.toArray(new PennTreebankPOS[label.size()]));
                             sent.clear();
@@ -378,7 +378,7 @@ public static void load(String dir, List<String[]> sentences, List<PennTreebankP
                     }
                 }
                 
-                if (sent.size() > 0) {
+                if (!sent.isEmpty()) {
                     sentences.add(sent.toArray(new String[sent.size()]));
                     labels.add(label.toArray(new PennTreebankPOS[label.size()]));
                     sent.clear();

File: nlp/src/main/java/smile/nlp/tokenizer/BreakIteratorTokenizer.java
Patch:
@@ -55,7 +55,7 @@ public String[] split(String text) {
 
         while (end != BreakIterator.DONE) {
             String word = text.substring(start, end).trim();
-            if (word.length() > 0) {
+            if (!word.isEmpty()) {
                 words.add(word);
             }
             start = end;

File: nlp/src/test/java/smile/nlp/pos/HMMPOSTaggerTest.java
Patch:
@@ -59,7 +59,7 @@ public void load(String dir) {
                 while ((line = reader.readLine()) != null) {
                     line = line.trim();
                     if (line.isEmpty()) {
-                        if (sent.size() > 0) {
+                        if (!sent.isEmpty()) {
                             sentences.add(sent.toArray(new String[sent.size()]));
                             labels.add(label.toArray(new PennTreebankPOS[label.size()]));
                             sent.clear();
@@ -82,7 +82,7 @@ public void load(String dir) {
                     }
                 }
                 
-                if (sent.size() > 0) {
+                if (!sent.isEmpty()) {
                     sentences.add(sent.toArray(new String[sent.size()]));
                     labels.add(label.toArray(new PennTreebankPOS[label.size()]));
                     sent.clear();

File: plot/src/main/java/smile/plot/Axis.java
Patch:
@@ -504,7 +504,7 @@ public void paint(Graphics g) {
                 int prevx = xy[0];
                 int prevy = xy[1];
                 for (int i = 0; i < gridLabels.length; i++) {
-                    if (gridLabels[i].text.length() > 0) {
+                    if (!gridLabels[i].text.isEmpty()) {
                         double[] coord = gridLabels[i].getCoordinate();
                         xy = g.projection.screenProjection(coord);
                         int x = xy[0];

File: plot/src/main/java/smile/plot/Contour.java
Patch:
@@ -789,7 +789,7 @@ private void init() {
                         }
                         contour.add(s.x1, s.y1);
                         
-                        if (contour.points.size() > 0) {
+                        if (!contour.points.isEmpty()) {
                             contours.add(contour);
                         }
                     }

File: plot/src/main/java/smile/swing/FontChooser.java
Patch:
@@ -539,7 +539,7 @@ private void update(DocumentEvent event) {
                 logger.error("update(DocumentEvent) exception", ex);
             }
 
-            if (newValue.length() > 0) {
+            if (!newValue.isEmpty()) {
                 int index = targetList.getNextMatch(newValue, 0, Position.Bias.Forward);
                 if (index < 0) {
                     index = 0;

File: plot/src/main/java/smile/swing/table/DefaultTableHeaderCellRenderer.java
Patch:
@@ -127,7 +127,7 @@ protected SortKey getSortKey(JTable table, int column) {
         }
 
         List sortedColumns = rowSorter.getSortKeys();
-        if (sortedColumns.size() > 0) {
+        if (!sortedColumns.isEmpty()) {
             return (SortKey) sortedColumns.get(0);
         }
         return null;

File: math/src/test/java/smile/stat/distribution/GammaDistributionTest.java
Patch:
@@ -59,8 +59,8 @@ public void testGammaDistribution() {
         for (int i = 0; i < data.length; i++)
             data[i] = instance.rand();
         GammaDistribution est = new GammaDistribution(data);
-        assertEquals(2.1, est.getScale(), 3E-1);
-        assertEquals(3, est.getShape(), 3E-1);
+        assertEquals(0.0, (est.getScale() - 2.1) / 2.1, 0.1);
+        assertEquals(0.0, (est.getShape() - 3.0) / 3.0, 0.1);
     }
 
     /**

File: math/src/test/java/smile/stat/distribution/GaussianDistributionTest.java
Patch:
@@ -59,8 +59,8 @@ public void testGaussianDistribution() {
         for (int i = 0; i < data.length; i++)
             data[i] = instance.rand();
         GaussianDistribution est = new GaussianDistribution(data);
-        assertEquals(3, est.mean(), 1E-1);
-        assertEquals(2.1, est.sd(), 1E-1);
+        assertEquals(0.0, (est.mean() - 3.0) / 3.0, 0.1);
+        assertEquals(0.0, (est.sd() - 2.1) / 2.1, 0.1);
     }
 
     /**

File: math/src/test/java/smile/stat/distribution/LogNormalDistributionTest.java
Patch:
@@ -59,8 +59,8 @@ public void testLogNormalDistribution() {
         for (int i = 0; i < data.length; i++)
             data[i] = instance.rand();
         LogNormalDistribution est = new LogNormalDistribution(data);
-        assertEquals(3, est.getMu(), 1.5E-1);
-        assertEquals(2.1, est.getSigma(), 1.5E-1);
+        assertEquals(0.0, (est.getMu() - 3.0) / 3.0, 0.1);
+        assertEquals(0.0, (est.getSigma() - 2.1) / 2.1, 0.1);
     }
 
     /**

File: math/src/test/java/smile/stat/distribution/ShiftedGeometricDistributionTest.java
Patch:
@@ -59,7 +59,7 @@ public void testShiftedGeometricDistribution() {
         for (int i = 0; i < data.length; i++)
             data[i] = (int) instance.rand();
         ShiftedGeometricDistribution est = new ShiftedGeometricDistribution(data);
-        assertEquals(0.4, est.getProb(), 2E-2);
+        assertEquals(0.0, (0.4 - est.getProb()) / 0.4, 0.05);
     }
 
     /**

File: core/src/main/java/smile/classification/SVM.java
Patch:
@@ -750,7 +750,7 @@ boolean process(T x, int y, double weight) {
             // Compute gradient
             double g = y;
             DoubleArrayList kcache = new DoubleArrayList(sv.size() + 1);
-            if (sv.size() > 0) {
+            if (!sv.isEmpty()) {
                 for (SupportVector v : sv) {
                     if (v != null) {
                         // Bail out if already in expansion?

File: core/src/main/java/smile/clustering/MEC.java
Patch:
@@ -230,7 +230,7 @@ private void learn(T[] data, RNNSearch<T,T> nns, int k, double radius, int[] y)
         // The number of samples with nonzero conditional entropy.
         entropy = 0.0;
         for (int i = 0; i < n; i++) {
-            if (neighbors.get(i).size() > 0) {
+            if (!neighbors.get(i).isEmpty()) {
                 int ni = neighbors.get(i).size();
                 double m = 0.0;
                 for (int j = 0; j < k; j++) {
@@ -295,7 +295,7 @@ private void learn(T[] data, RNNSearch<T,T> nns, int k, double radius, int[] y)
             double prevObj = entropy;
             entropy = 0.0;
             for (int i = 0; i < n; i++) {
-                if (neighbors.get(i).size() > 0) {
+                if (!neighbors.get(i).isEmpty()) {
                     int ni = neighbors.get(i).size();
                     double m = 0.0;
                     for (int j = 0; j < k; j++) {

File: core/src/main/java/smile/neighbor/CoverTree.java
Patch:
@@ -304,7 +304,7 @@ private Node batchInsert(int p, int maxScale, int topScale, ArrayList<DistanceSe
                 ArrayList<Node> children = new ArrayList<Node>();
                 Node leaf = newLeaf(p);
                 children.add(leaf);
-                while (pointSet.size() > 0) {
+                while (!pointSet.isEmpty()) {
                     DistanceSet set = pointSet.get(pointSet.size() - 1);
                     pointSet.remove(pointSet.size() - 1);
                     leaf = newLeaf(set.idx);
@@ -533,7 +533,7 @@ public Neighbor<E, E>[] knn(E q, int k) {
             emptyHeap = false;
         }
 
-        while (currentCoverSet.size() > 0) {
+        while (!currentCoverSet.isEmpty()) {
             ArrayList<DistanceNode> nextCoverSet = new ArrayList<DistanceNode>();
             for (int i = 0; i < currentCoverSet.size(); i++) {
                 DistanceNode par = currentCoverSet.get(i);
@@ -597,7 +597,7 @@ public void range(E q, double radius, List<Neighbor<E, E>> neighbors) {
         double d = distance.d(root.getObject(), q);
         currentCoverSet.add(new DistanceNode(d, root));
 
-        while (currentCoverSet.size() > 0) {
+        while (!currentCoverSet.isEmpty()) {
             ArrayList<DistanceNode> nextCoverSet = new ArrayList<DistanceNode>();
             for (int i = 0; i < currentCoverSet.size(); i++) {
                 DistanceNode par = currentCoverSet.get(i);

File: core/src/main/java/smile/taxonomy/Concept.java
Patch:
@@ -366,7 +366,7 @@ public List<Concept> getPathToRoot() {
     @Override
     public String toString() {
         String displayName = "anonymous";
-        if (synset != null && synset.size() > 0) {
+        if (synset != null && !synset.isEmpty()) {
             StringBuilder builder = new StringBuilder();
             builder.append('(');
             Iterator<String> iter = synset.iterator();

File: core/src/main/java/smile/vq/GrowingNeuralGas.java
Patch:
@@ -316,11 +316,11 @@ public void update(double[] x) {
                     edge.a.edges.remove(edge);
                     // If it results in neuron having no emanating edges,
                     // remove the neuron as well.
-                    if (edge.a.edges.size() == 0)
+                    if (edge.a.edges.isEmpty())
                         nodes.remove(edge.a);
                 } else {
                     edge.b.edges.remove(edge);
-                    if (edge.b.edges.size() == 0)
+                    if (edge.b.edges.isEmpty())
                         nodes.remove(edge.b);
                 }
             }

File: core/src/main/java/smile/vq/NeuralMap.java
Patch:
@@ -502,14 +502,14 @@ public void update(double[] x) {
                 iter.remove();
             }
 
-            if (neighbor.neighbors.size() > 0) {
+            if (!neighbor.neighbors.isEmpty()) {
                 lsh.add(neighbor);
             } else {
                 neurons.remove(neighbor);
             }
         }
 
-        if (neuron.neighbors.size() == 0) {
+        if (neuron.neighbors.isEmpty()) {
             lsh.remove(neuron);
             neurons.remove(neuron);
         }

File: core/src/test/java/smile/neighbor/LSHTest.java
Patch:
@@ -161,7 +161,7 @@ public void testRange() {
                     }
                 }
             }
-            if (n2.size() > 0) {
+            if (!n2.isEmpty()) {
                 recall += 1.0 * hit / n2.size();
             }
         }

File: core/src/test/java/smile/neighbor/MPLSHTest.java
Patch:
@@ -161,7 +161,7 @@ public void testRangePosteriori() {
                     }
                 }
             }
-            if (n2.size() > 0) {
+            if (!n2.isEmpty()) {
                 recall += 1.0 * hit / n2.size();
             }
         }

File: graph/src/main/java/smile/graph/AdjacencyList.java
Patch:
@@ -451,7 +451,7 @@ public int[] sortbfs() {
             }
         }
 
-        for (int i = 0; queue.size() > 0; i++) {
+        for (int i = 0; !queue.isEmpty(); i++) {
             int t = queue.poll();
             ts[i] = t;
             for (Edge edge : graph[t]) {
@@ -475,7 +475,7 @@ private void bfs(int v, int[] cc, int id) {
         cc[v] = id;
         Queue<Integer> queue = new LinkedList<Integer>();
         queue.offer(v);
-        while (queue.size() > 0) {
+        while (!queue.isEmpty()) {
             int t = queue.poll();
             for (Edge edge : graph[t]) {
                 int i = edge.v2;
@@ -533,7 +533,7 @@ private void bfs(Visitor visitor, int v, int[] cc, int id) {
         cc[v] = id;
         Queue<Integer> queue = new LinkedList<Integer>();
         queue.offer(v);
-        while (queue.size() > 0) {
+        while (!queue.isEmpty()) {
             int t = queue.poll();
             for (Edge edge : graph[t]) {
                 int i = edge.v2;

File: graph/src/main/java/smile/graph/AdjacencyMatrix.java
Patch:
@@ -390,7 +390,7 @@ public int[] sortbfs() {
             }
         }
 
-        for (int i = 0; queue.size() > 0; i++) {
+        for (int i = 0; !queue.isEmpty(); i++) {
             int t = queue.poll();
             ts[i] = t;
             for (int v = 0; v < n; v++) {
@@ -415,7 +415,7 @@ private void bfs(int v, int[] cc, int id) {
         cc[v] = id;
         Queue<Integer> queue = new LinkedList<Integer>();
         queue.offer(v);
-        while (queue.size() > 0) {
+        while (!queue.isEmpty()) {
             int t = queue.poll();
             for (int i = 0; i < n; i++) {
                 if (graph[t][i] != 0.0 && cc[i] == -1) {
@@ -469,7 +469,7 @@ private void bfs(Visitor visitor, int v, int[] cc, int id) {
         cc[v] = id;
         Queue<Integer> queue = new LinkedList<Integer>();
         queue.offer(v);
-        while (queue.size() > 0) {
+        while (!queue.isEmpty()) {
             int t = queue.poll();
             for (int i = 0; i < n; i++) {
                 if (graph[t][i] != 0.0 && cc[i] == -1) {

File: nlp/src/main/java/smile/nlp/pos/HMMPOSTagger.java
Patch:
@@ -355,7 +355,7 @@ public static void load(String dir, List<String[]> sentences, List<PennTreebankP
                 while ((line = reader.readLine()) != null) {
                     line = line.trim();
                     if (line.isEmpty()) {
-                        if (sent.size() > 0) {
+                        if (!sent.isEmpty()) {
                             sentences.add(sent.toArray(new String[sent.size()]));
                             labels.add(label.toArray(new PennTreebankPOS[label.size()]));
                             sent.clear();
@@ -378,7 +378,7 @@ public static void load(String dir, List<String[]> sentences, List<PennTreebankP
                     }
                 }
                 
-                if (sent.size() > 0) {
+                if (!sent.isEmpty()) {
                     sentences.add(sent.toArray(new String[sent.size()]));
                     labels.add(label.toArray(new PennTreebankPOS[label.size()]));
                     sent.clear();

File: nlp/src/main/java/smile/nlp/tokenizer/BreakIteratorTokenizer.java
Patch:
@@ -55,7 +55,7 @@ public String[] split(String text) {
 
         while (end != BreakIterator.DONE) {
             String word = text.substring(start, end).trim();
-            if (word.length() > 0) {
+            if (!word.isEmpty()) {
                 words.add(word);
             }
             start = end;

File: nlp/src/test/java/smile/nlp/pos/HMMPOSTaggerTest.java
Patch:
@@ -59,7 +59,7 @@ public void load(String dir) {
                 while ((line = reader.readLine()) != null) {
                     line = line.trim();
                     if (line.isEmpty()) {
-                        if (sent.size() > 0) {
+                        if (!sent.isEmpty()) {
                             sentences.add(sent.toArray(new String[sent.size()]));
                             labels.add(label.toArray(new PennTreebankPOS[label.size()]));
                             sent.clear();
@@ -82,7 +82,7 @@ public void load(String dir) {
                     }
                 }
                 
-                if (sent.size() > 0) {
+                if (!sent.isEmpty()) {
                     sentences.add(sent.toArray(new String[sent.size()]));
                     labels.add(label.toArray(new PennTreebankPOS[label.size()]));
                     sent.clear();

File: plot/src/main/java/smile/plot/Axis.java
Patch:
@@ -504,7 +504,7 @@ public void paint(Graphics g) {
                 int prevx = xy[0];
                 int prevy = xy[1];
                 for (int i = 0; i < gridLabels.length; i++) {
-                    if (gridLabels[i].text.length() > 0) {
+                    if (!gridLabels[i].text.isEmpty()) {
                         double[] coord = gridLabels[i].getCoordinate();
                         xy = g.projection.screenProjection(coord);
                         int x = xy[0];

File: plot/src/main/java/smile/plot/Contour.java
Patch:
@@ -789,7 +789,7 @@ private void init() {
                         }
                         contour.add(s.x1, s.y1);
                         
-                        if (contour.points.size() > 0) {
+                        if (!contour.points.isEmpty()) {
                             contours.add(contour);
                         }
                     }

File: plot/src/main/java/smile/swing/FontChooser.java
Patch:
@@ -539,7 +539,7 @@ private void update(DocumentEvent event) {
                 logger.error("update(DocumentEvent) exception", ex);
             }
 
-            if (newValue.length() > 0) {
+            if (!newValue.isEmpty()) {
                 int index = targetList.getNextMatch(newValue, 0, Position.Bias.Forward);
                 if (index < 0) {
                     index = 0;

File: plot/src/main/java/smile/swing/table/DefaultTableHeaderCellRenderer.java
Patch:
@@ -127,7 +127,7 @@ protected SortKey getSortKey(JTable table, int column) {
         }
 
         List sortedColumns = rowSorter.getSortKeys();
-        if (sortedColumns.size() > 0) {
+        if (!sortedColumns.isEmpty()) {
             return (SortKey) sortedColumns.get(0);
         }
         return null;

File: core/src/main/java/smile/association/FPGrowth.java
Patch:
@@ -337,7 +337,7 @@ private long grow(PrintStream out, List<ItemSet> list, TotalSupportTree ttree, H
                 for (int i = 0; i < itemset.length; i++) {
                     out.format("%d ", itemset[i]);
                 }
-                out.format("(%d)\n", support);
+                out.format("(%d)%n", support);
             }
         }
         if (ttree != null) {
@@ -364,7 +364,7 @@ private long grow(PrintStream out, List<ItemSet> list, TotalSupportTree ttree, H
                             for (int i = 0; i < newItemset.length; i++) {
                                 out.format("%d ", newItemset[i]);
                             }
-                            out.format("(%d)\n", support);
+                            out.format("(%d)%n", support);
                         }
                     }
                     if (ttree != null) {

File: core/src/main/java/smile/clustering/CLARANS.java
Patch:
@@ -321,11 +321,11 @@ public int predict(T x) {
     public String toString() {
         StringBuilder sb = new StringBuilder();
         
-        sb.append(String.format("CLARANS distortion: %.5f\n", distortion));
-        sb.append(String.format("Clusters of %d data points:\n", y.length));
+        sb.append(String.format("CLARANS distortion: %.5f%n", distortion));
+        sb.append(String.format("Clusters of %d data points:%n", y.length));
         for (int i = 0; i < k; i++) {
             int r = (int) Math.round(1000.0 * size[i] / y.length);
-            sb.append(String.format("%3d\t%5d (%2d.%1d%%)\n", i, size[i], r / 10, r % 10));
+            sb.append(String.format("%3d\t%5d (%2d.%1d%%)%n", i, size[i], r / 10, r % 10));
         }
         
         return sb.toString();

File: core/src/main/java/smile/clustering/DBScan.java
Patch:
@@ -239,14 +239,14 @@ public int predict(T x) {
     public String toString() {
         StringBuilder sb = new StringBuilder();
         
-        sb.append(String.format("DBScan clusters of %d data points:\n", y.length));
+        sb.append(String.format("DBScan clusters of %d data points:%n", y.length));
         for (int i = 0; i < k; i++) {
             int r = (int) Math.round(1000.0 * size[i] / y.length);
-            sb.append(String.format("%3d\t%5d (%2d.%1d%%)\n", i, size[i], r / 10, r % 10));
+            sb.append(String.format("%3d\t%5d (%2d.%1d%%)%n", i, size[i], r / 10, r % 10));
         }
 
         int r = (int) Math.round(1000.0 * size[k] / y.length);
-        sb.append(String.format("Noise\t%5d (%2d.%1d%%)\n", size[k], r / 10, r % 10));
+        sb.append(String.format("Noise\t%5d (%2d.%1d%%)%n", size[k], r / 10, r % 10));
         
         return sb.toString();
     }

File: core/src/main/java/smile/clustering/DENCLUE.java
Patch:
@@ -349,10 +349,10 @@ public int predict(double[] x) {
     public String toString() {
         StringBuilder sb = new StringBuilder();
 
-        sb.append(String.format("DENCLUE clusters of %d data points:\n", y.length));
+        sb.append(String.format("DENCLUE clusters of %d data points:%n", y.length));
         for (int i = 0; i < k; i++) {
             int r = (int) Math.round(1000.0 * size[i] / y.length);
-            sb.append(String.format("%3d\t%5d (%2d.%1d%%)\n", i, size[i], r / 10, r % 10));
+            sb.append(String.format("%3d\t%5d (%2d.%1d%%)%n", i, size[i], r / 10, r % 10));
         }
 
         return sb.toString();

File: core/src/main/java/smile/clustering/KMeans.java
Patch:
@@ -515,11 +515,11 @@ public Double call() {
     public String toString() {
         StringBuilder sb = new StringBuilder();
         
-        sb.append(String.format("K-Means distortion: %.5f\n", distortion));
-        sb.append(String.format("Clusters of %d data points of dimension %d:\n", y.length, centroids[0].length));
+        sb.append(String.format("K-Means distortion: %.5f%n", distortion));
+        sb.append(String.format("Clusters of %d data points of dimension %d:%n", y.length, centroids[0].length));
         for (int i = 0; i < k; i++) {
             int r = (int) Math.round(1000.0 * size[i] / y.length);
-            sb.append(String.format("%3d\t%5d (%2d.%1d%%)\n", i, size[i], r / 10, r % 10));
+            sb.append(String.format("%3d\t%5d (%2d.%1d%%)%n", i, size[i], r / 10, r % 10));
         }
         
         return sb.toString();

File: core/src/main/java/smile/clustering/MEC.java
Patch:
@@ -378,11 +378,11 @@ public int predict(T x) {
     public String toString() {
         StringBuilder sb = new StringBuilder();
 
-        sb.append(String.format("MEC cluster conditional entropy: %.5f\n", entropy));
-        sb.append(String.format("Clusters of %d data points:\n", y.length));
+        sb.append(String.format("MEC cluster conditional entropy: %.5f%n", entropy));
+        sb.append(String.format("Clusters of %d data points:%n", y.length));
         for (int i = 0; i < k; i++) {
             int r = (int) Math.round(1000.0 * size[i] / y.length);
-            sb.append(String.format("%3d\t%5d (%2d.%1d%%)\n", i, size[i], r / 10, r % 10));
+            sb.append(String.format("%3d\t%5d (%2d.%1d%%)%n", i, size[i], r / 10, r % 10));
         }
 
         return sb.toString();

File: core/src/main/java/smile/clustering/SIB.java
Patch:
@@ -520,11 +520,11 @@ public double[][] centroids() {
     public String toString() {
         StringBuilder sb = new StringBuilder();
         
-        sb.append(String.format("Sequential Information Bottleneck distortion: %.5f\n", distortion));
-        sb.append(String.format("Clusters of %d data points of dimension %d:\n", y.length, centroids[0].length));
+        sb.append(String.format("Sequential Information Bottleneck distortion: %.5f%n", distortion));
+        sb.append(String.format("Clusters of %d data points of dimension %d:%n", y.length, centroids[0].length));
         for (int i = 0; i < k; i++) {
             int r = (int) Math.round(1000.0 * size[i] / y.length);
-            sb.append(String.format("%3d\t%5d (%2d.%1d%%)\n", i, size[i], r / 10, r % 10));
+            sb.append(String.format("%3d\t%5d (%2d.%1d%%)%n", i, size[i], r / 10, r % 10));
         }
         
         return sb.toString();

File: core/src/main/java/smile/clustering/SpectralClustering.java
Patch:
@@ -336,11 +336,11 @@ public double distortion() {
     public String toString() {
         StringBuilder sb = new StringBuilder();
 
-        sb.append(String.format("Spectral Clustering distortion in feature space: %.5f\n", distortion));
-        sb.append(String.format("Clusters of %d data points:\n", y.length));
+        sb.append(String.format("Spectral Clustering distortion in feature space: %.5f%n", distortion));
+        sb.append(String.format("Clusters of %d data points:%n", y.length));
         for (int i = 0; i < k; i++) {
             int r = (int) Math.round(1000.0 * size[i] / y.length);
-            sb.append(String.format("%3d\t%5d (%2d.%1d%%)\n", i, size[i], r / 10, r % 10));
+            sb.append(String.format("%3d\t%5d (%2d.%1d%%)%n", i, size[i], r / 10, r % 10));
         }
 
         return sb.toString();

File: core/src/main/java/smile/clustering/XMeans.java
Patch:
@@ -235,11 +235,11 @@ private static double logLikelihood(int k, int n, int ni, int d, double variance
     public String toString() {
         StringBuilder sb = new StringBuilder();
 
-        sb.append(String.format("X-Means distortion: %.5f\n", distortion));
-        sb.append(String.format("Clusters of %d data points of dimension %d:\n", y.length, centroids[0].length));
+        sb.append(String.format("X-Means distortion: %.5f%n", distortion));
+        sb.append(String.format("Clusters of %d data points of dimension %d:%n", y.length, centroids[0].length));
         for (int i = 0; i < k; i++) {
             int r = (int) Math.round(1000.0 * size[i] / y.length);
-            sb.append(String.format("%3d\t%5d (%2d.%1d%%)\n", i, size[i], r / 10, r % 10));
+            sb.append(String.format("%3d\t%5d (%2d.%1d%%)%n", i, size[i], r / 10, r % 10));
         }
 
         return sb.toString();

File: core/src/main/java/smile/sequence/HMM.java
Patch:
@@ -714,7 +714,7 @@ private double[][] estimateGamma(double[][][] xi) {
     @Override
     public String toString() {
         StringBuilder sb = new StringBuilder();
-        sb.append(String.format("HMM (%d states, %d emission symbols)\n", numStates, numSymbols));
+        sb.append(String.format("HMM (%d states, %d emission symbols)%n", numStates, numSymbols));
 
         sb.append("\tInitial state probability:\n\t\t");
         for (int i = 0; i < numStates; i++) {

File: core/src/main/java/smile/vq/NeuralGas.java
Patch:
@@ -264,11 +264,11 @@ public int predict(double[] x) {
     public String toString() {
         StringBuilder sb = new StringBuilder();
         
-        sb.append(String.format("Neural Gas distortion: %.5f\n", distortion));
-        sb.append(String.format("Clusters of %d data points of dimension %d:\n", y.length, centroids[0].length));
+        sb.append(String.format("Neural Gas distortion: %.5f%n", distortion));
+        sb.append(String.format("Clusters of %d data points of dimension %d:%n", y.length, centroids[0].length));
         for (int i = 0; i < k; i++) {
             int r = (int) Math.round(1000.0 * size[i] / y.length);
-            sb.append(String.format("%3d\t%5d (%2d.%1d%%)\n", i, size[i], r / 10, r % 10));
+            sb.append(String.format("%3d\t%5d (%2d.%1d%%)%n", i, size[i], r / 10, r % 10));
         }
         
         return sb.toString();

File: core/src/test/java/smile/classification/DecisionTreeTest.java
Patch:
@@ -145,7 +145,7 @@ public void testUSPS() {
                 }
             }
 
-            System.out.format("USPS error rate = %.2f%%\n", 100.0 * error / testx.length);
+            System.out.format("USPS error rate = %.2f%%%n", 100.0 * error / testx.length);
             assertEquals(328, error);
         } catch (Exception ex) {
             System.err.println(ex);
@@ -200,12 +200,12 @@ public void testUSPSNominal() {
                 }
             }
 
-            System.out.format("USPS error rate = %.2f%%\n", 100.0 * error / testx.length);
+            System.out.format("USPS error rate = %.2f%%%n", 100.0 * error / testx.length);
             
             double[] importance = tree.importance();
             int[] index = QuickSort.sort(importance);
             for (int i = importance.length; i-- > 0; ) {
-                System.out.format("%s importance is %.4f\n", train.attributes()[index[i]], importance[i]);
+                System.out.format("%s importance is %.4f%n", train.attributes()[index[i]], importance[i]);
             }
             
             assertEquals(324, error);

File: core/src/test/java/smile/classification/FLDTest.java
Patch:
@@ -112,7 +112,7 @@ public void testUSPS() {
                 }
             }
 
-            System.out.format("USPS error rate = %.2f%%\n", 100.0 * error / testx.length);
+            System.out.format("USPS error rate = %.2f%%%n", 100.0 * error / testx.length);
             assertEquals(521, error);
         } catch (Exception ex) {
             System.err.println(ex);

File: core/src/test/java/smile/classification/KNNTest.java
Patch:
@@ -177,7 +177,7 @@ public void testSegment() throws ParseException {
                 }
             }
 
-            System.out.format("Segment error rate = %.2f%%\n", 100.0 * error / testx.length);
+            System.out.format("Segment error rate = %.2f%%%n", 100.0 * error / testx.length);
             assertEquals(39, error);
         } catch (IOException ex) {
             System.err.println(ex);
@@ -210,7 +210,7 @@ public void testUSPS() {
                 }
             }
 
-            System.out.format("USPS error rate = %.2f%%\n", 100.0 * error / testx.length);
+            System.out.format("USPS error rate = %.2f%%%n", 100.0 * error / testx.length);
             assertEquals(113, error);
         } catch (Exception ex) {
             System.err.println(ex);

File: core/src/test/java/smile/classification/LDATest.java
Patch:
@@ -115,7 +115,7 @@ public void testUSPS() {
                 }
             }
 
-            System.out.format("USPS error rate = %.2f%%\n", 100.0 * error / testx.length);
+            System.out.format("USPS error rate = %.2f%%%n", 100.0 * error / testx.length);
             assertEquals(256, error);
         } catch (Exception ex) {
             System.err.println(ex);

File: core/src/test/java/smile/classification/LogisticRegressionTest.java
Patch:
@@ -152,7 +152,7 @@ public void testSegment() {
                 }
             }
 
-            System.out.format("Segment error rate = %.2f%%\n", 100.0 * error / testx.length);
+            System.out.format("Segment error rate = %.2f%%%n", 100.0 * error / testx.length);
             assertEquals(48, error);
         } catch (Exception ex) {
             System.err.println(ex);
@@ -185,7 +185,7 @@ public void testUSPS() {
                 }
             }
 
-            System.out.format("USPS error rate = %.2f%%\n", 100.0 * error / testx.length);
+            System.out.format("USPS error rate = %.2f%%%n", 100.0 * error / testx.length);
             assertEquals(188, error);
         } catch (Exception ex) {
             System.err.println(ex);

File: core/src/test/java/smile/classification/RBFNetworkTest.java
Patch:
@@ -124,7 +124,7 @@ public void testSegment() {
                 }
             }
 
-            System.out.format("Segment error rate = %.2f%%\n", 100.0 * error / testx.length);
+            System.out.format("Segment error rate = %.2f%%%n", 100.0 * error / testx.length);
             assertTrue(error <= 210);
         } catch (Exception ex) {
             System.err.println(ex);
@@ -159,7 +159,7 @@ public void testUSPS() {
                 }
             }
 
-            System.out.format("USPS error rate = %.2f%%\n", 100.0 * error / testx.length);
+            System.out.format("USPS error rate = %.2f%%%n", 100.0 * error / testx.length);
             assertTrue(error <= 150);
         } catch (Exception ex) {
             System.err.println(ex);

File: core/src/test/java/smile/classification/RDATest.java
Patch:
@@ -234,7 +234,7 @@ public void testUSPS() {
                 }
             }
 
-            System.out.format("USPS error rate = %.2f%%\n", 100.0 * error / testx.length);
+            System.out.format("USPS error rate = %.2f%%%n", 100.0 * error / testx.length);
             assertEquals(235, error);
         } catch (Exception ex) {
             System.err.println(ex);

File: core/src/test/java/smile/classification/SVMTest.java
Patch:
@@ -158,7 +158,7 @@ public void testSegment() {
                 }
             }
 
-            System.out.format("Segment error rate = %.2f%%\n", 100.0 * error / testx.length);
+            System.out.format("Segment error rate = %.2f%%%n", 100.0 * error / testx.length);
             assertTrue(error < 70);
         } catch (Exception ex) {
             ex.printStackTrace();
@@ -193,7 +193,7 @@ public void testUSPS() {
                 }
             }
 
-            System.out.format("USPS error rate = %.2f%%\n", 100.0 * error / testx.length);
+            System.out.format("USPS error rate = %.2f%%%n", 100.0 * error / testx.length);
             assertTrue(error < 95);
             
             System.out.println("USPS one more epoch...");
@@ -210,7 +210,7 @@ public void testUSPS() {
                     error++;
                 }
             }
-            System.out.format("USPS error rate = %.2f%%\n", 100.0 * error / testx.length);
+            System.out.format("USPS error rate = %.2f%%%n", 100.0 * error / testx.length);
             assertTrue(error < 95);
         } catch (Exception ex) {
             ex.printStackTrace();

File: core/src/test/java/smile/clustering/BIRCHTest.java
Patch:
@@ -90,7 +90,7 @@ public void testUSPS() {
             
             double r = rand.measure(y, p);
             double r2 = ari.measure(y, p);
-            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
             assertTrue(r > 0.65);
             assertTrue(r2 > 0.20);
             
@@ -101,7 +101,7 @@ public void testUSPS() {
             
             r = rand.measure(testy, p);
             r2 = ari.measure(testy, p);
-            System.out.format("Testing rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+            System.out.format("Testing rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
             assertTrue(r > 0.65);
             assertTrue(r2 > 0.20);
         } catch (Exception ex) {

File: core/src/test/java/smile/clustering/CLARANSTest.java
Patch:
@@ -76,7 +76,7 @@ public void testUSPS() {
 
             double r = rand.measure(y, clarans.getClusterLabel());
             double r2 = ari.measure(y, clarans.getClusterLabel());
-            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
             assertTrue(r > 0.8);
             assertTrue(r2 > 0.28);
             
@@ -87,7 +87,7 @@ public void testUSPS() {
             
             r = rand.measure(testy, p);
             r2 = ari.measure(testy, p);
-            System.out.format("Testing rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+            System.out.format("Testing rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
             assertTrue(r > 0.8);
             assertTrue(r2 > 0.25);
         } catch (Exception ex) {

File: core/src/test/java/smile/clustering/DBScanTest.java
Patch:
@@ -116,7 +116,7 @@ public void testToy() {
         double r = rand.measure(y1, y2);
         double r2 = ari.measure(y1, y2);
         System.out.println("The number of clusters: " + dbscan.getNumClusters());
-        System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+        System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
         assertTrue(r > 0.40);
         assertTrue(r2 > 0.15);
     }

File: core/src/test/java/smile/clustering/DENCLUETest.java
Patch:
@@ -98,7 +98,7 @@ public void testToy() {
         double r = rand.measure(label, denclue.getClusterLabel());
         double r2 = ari.measure(label, denclue.getClusterLabel());
         System.out.println("The number of clusters: " + denclue.getNumClusters());
-        System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+        System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
         assertTrue(r > 0.54);
         assertTrue(r2 > 0.2);
     }

File: core/src/test/java/smile/clustering/DeterministicAnnealingTest.java
Patch:
@@ -75,7 +75,7 @@ public void testUSPS() {
             RandIndex rand = new RandIndex();
             double r = rand.measure(y, annealing.getClusterLabel());
             double r2 = ari.measure(y, annealing.getClusterLabel());
-            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
             assertTrue(r > 0.75);
             assertTrue(r2 > 0.25);
             
@@ -86,7 +86,7 @@ public void testUSPS() {
             
             r = rand.measure(testy, p);
             r2 = ari.measure(testy, p);
-            System.out.format("Testing rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+            System.out.format("Testing rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
             assertTrue(r > 0.75);
             assertTrue(r2 > 0.3);
         } catch (Exception ex) {

File: core/src/test/java/smile/clustering/GMeansTest.java
Patch:
@@ -75,7 +75,7 @@ public void testUSPS() {
             
             double r = rand.measure(y, gmeans.getClusterLabel());
             double r2 = ari.measure(y, gmeans.getClusterLabel());
-            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
             assertTrue(r > 0.85);
             assertTrue(r2 > 0.4);
             
@@ -86,7 +86,7 @@ public void testUSPS() {
             
             r = rand.measure(testy, p);
             r2 = ari.measure(testy, p);
-            System.out.format("Testing rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+            System.out.format("Testing rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
             assertTrue(r > 0.85);
             assertTrue(r2 > 0.4);
         } catch (Exception ex) {

File: core/src/test/java/smile/clustering/MECTest.java
Patch:
@@ -76,7 +76,7 @@ public void testUSPS() {
             
             double r = rand.measure(y, mec.getClusterLabel());
             double r2 = ari.measure(y, mec.getClusterLabel());
-            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
             assertTrue(r > 0.85);
             assertTrue(r2 > 0.35);
             
@@ -87,7 +87,7 @@ public void testUSPS() {
             
             r = rand.measure(testy, p);
             r2 = ari.measure(testy, p);
-            System.out.format("Testing rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+            System.out.format("Testing rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
             assertTrue(r > 0.85);
             assertTrue(r2 > 0.35);
         } catch (Exception ex) {

File: core/src/test/java/smile/clustering/SIBTest.java
Patch:
@@ -71,7 +71,7 @@ public void testParseNG20() throws Exception {
             RandIndex rand = new RandIndex();
             double r = rand.measure(y, sib.getClusterLabel());
             double r2 = ari.measure(y, sib.getClusterLabel());
-            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
             assertTrue(r > 0.85);
             assertTrue(r2 > 0.2);
             
@@ -82,7 +82,7 @@ public void testParseNG20() throws Exception {
             
             r = rand.measure(testy, p);
             r2 = ari.measure(testy, p);
-            System.out.format("Testing rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+            System.out.format("Testing rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
             assertTrue(r > 0.85);
             assertTrue(r2 > 0.2);
         } catch (Exception ex) {

File: core/src/test/java/smile/clustering/SpectralClusteringTest.java
Patch:
@@ -72,7 +72,7 @@ public void testUSPS() {
             RandIndex rand = new RandIndex();
             double r = rand.measure(y, spectral.getClusterLabel());
             double r2 = ari.measure(y, spectral.getClusterLabel());
-            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
             assertTrue(r > 0.85);
             assertTrue(r2 > 0.45);            
         } catch (Exception ex) {
@@ -100,7 +100,7 @@ public void testUSPSNystrom() {
             RandIndex rand = new RandIndex();
             double r = rand.measure(y, spectral.getClusterLabel());
             double r2 = ari.measure(y, spectral.getClusterLabel());
-            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
             assertTrue(r > 0.8);
             assertTrue(r2 > 0.35);            
         } catch (Exception ex) {

File: core/src/test/java/smile/clustering/XMeansTest.java
Patch:
@@ -76,7 +76,7 @@ public void testUSPS() {
             
             double r = rand.measure(y, xmeans.getClusterLabel());
             double r2 = ari.measure(y, xmeans.getClusterLabel());
-            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
             assertTrue(r > 0.85);
             assertTrue(r2 > 0.4);
             
@@ -87,7 +87,7 @@ public void testUSPS() {
             
             r = rand.measure(testy, p);
             r2 = ari.measure(testy, p);
-            System.out.format("Testing rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+            System.out.format("Testing rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
             assertTrue(r > 0.85);
             assertTrue(r2 > 0.4);
         } catch (Exception ex) {

File: core/src/test/java/smile/feature/SumSquaresRatioTest.java
Patch:
@@ -125,7 +125,7 @@ public void testLearn() {
             }
 
             double accuracy = new Accuracy().measure(testy, prediction);
-            System.out.format("SSR %.2f%%\n", 100 * accuracy);
+            System.out.format("SSR %.2f%%%n", 100 * accuracy);
         } catch (Exception ex) {
             System.err.println(ex);
         }

File: core/src/test/java/smile/neighbor/CoverTreeStringSpeedTest.java
Patch:
@@ -52,14 +52,14 @@ public CoverTreeStringSpeedTest() {
         }
 
         double time = (System.currentTimeMillis() - start) / 1000.0;
-        System.out.format("Loading data: %.2fs\n", time);
+        System.out.format("Loading data: %.2fs%n", time);
 
         String[] data = words.toArray(new String[words.size()]);
 
         start = System.currentTimeMillis();
         cover = new CoverTree<String>(data, new EditDistance(50, true));
         time = (System.currentTimeMillis() - start) / 1000.0;
-        System.out.format("Building cover tree: %.2fs\n", time);
+        System.out.format("Building cover tree: %.2fs%n", time);
     }
 
     @BeforeClass
@@ -91,6 +91,6 @@ public void testNaiveSpeed() {
             neighbors.clear();
         }
         double time = (System.currentTimeMillis() - start) / 1000.0;
-        System.out.format("Cover tree string search: %.2fs\n", time);
+        System.out.format("Cover tree string search: %.2fs%n", time);
     }
 }
\ No newline at end of file

File: core/src/test/java/smile/projection/GHATest.java
Patch:
@@ -135,7 +135,7 @@ public void testLearn() {
             error /= USArrests.length;
 
             if (iter % 100 == 0) {
-                System.out.format("Iter %3d, Error = %.5g\n", iter, error);
+                System.out.format("Iter %3d, Error = %.5g%n", iter, error);
             }
         }
 

File: core/src/test/java/smile/regression/RegressionTreeTest.java
Patch:
@@ -132,7 +132,7 @@ public void test(String dataset, String url, int response) {
                 }
             }
 
-            System.out.format("10-CV RMSE = %.4f \t AbsoluteDeviation = %.4f\n", Math.sqrt(rss/n), ad/n);
+            System.out.format("10-CV RMSE = %.4f \t AbsoluteDeviation = %.4f%n", Math.sqrt(rss/n), ad/n);
          } catch (Exception ex) {
              System.err.println(ex);
          }
@@ -186,12 +186,12 @@ public void testCPU() {
             }
 
             RegressionTree tree = new RegressionTree(data.attributes(), trainx, trainy, 20);
-            System.out.format("RMSE = %.4f\n", Validation.test(tree, testx, testy));
+            System.out.format("RMSE = %.4f%n", Validation.test(tree, testx, testy));
             
             double[] importance = tree.importance();
             index = QuickSort.sort(importance);
             for (int i = importance.length; i-- > 0; ) {
-                System.out.format("%s importance is %.4f\n", data.attributes()[index[i]], importance[i]);
+                System.out.format("%s importance is %.4f%n", data.attributes()[index[i]], importance[i]);
             }
         } catch (Exception ex) {
             System.err.println(ex);

File: core/src/test/java/smile/regression/RidgeRegressionTest.java
Patch:
@@ -144,7 +144,7 @@ public void testPredict() {
                 rss += r * r;
             }
 
-            System.out.format("LOOCV MSE with lambda %.2f = %.3f\n", 0.01*lambda, rss/n);
+            System.out.format("LOOCV MSE with lambda %.2f = %.3f%n", 0.01*lambda, rss/n);
         }
     }
 

File: core/src/test/java/smile/validation/BootstrapTest.java
Patch:
@@ -105,8 +105,8 @@ public void testOrthogonal() {
             }
         }
 
-        System.out.format("Train coverage: %d\t%d\t%d\n", Math.min(trainhit), Math.median(trainhit), Math.max(trainhit));
-        System.out.format("Test coverage: %d\t%d\t%d\n", Math.min(testhit), Math.median(testhit), Math.max(testhit));
+        System.out.format("Train coverage: %d\t%d\t%d%n", Math.min(trainhit), Math.median(trainhit), Math.max(trainhit));
+        System.out.format("Test coverage: %d\t%d\t%d%n", Math.min(testhit), Math.median(testhit), Math.max(testhit));
 
         for (int j = 0; j < n; j++) {
             assertTrue(trainhit[j] > 60);

File: core/src/test/java/smile/vq/GrowingNeuralGasTest.java
Patch:
@@ -91,7 +91,7 @@ public void testUSPS() {
             
             double r = rand.measure(y, p);
             double r2 = ari.measure(y, p);
-            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
             assertTrue(r > 0.85);
             assertTrue(r2 > 0.40);
             
@@ -102,7 +102,7 @@ public void testUSPS() {
             
             r = rand.measure(testy, p);
             r2 = ari.measure(testy, p);
-            System.out.format("Testing rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+            System.out.format("Testing rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
             assertTrue(r > 0.85);
             assertTrue(r2 > 0.40);
         } catch (Exception ex) {

File: core/src/test/java/smile/vq/NeuralGasTest.java
Patch:
@@ -75,7 +75,7 @@ public void testUSPS() {
             RandIndex rand = new RandIndex();
             double r = rand.measure(y, gas.getClusterLabel());
             double r2 = ari.measure(y, gas.getClusterLabel());
-            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
             assertTrue(r > 0.88);
             assertTrue(r2 > 0.45);
             
@@ -86,7 +86,7 @@ public void testUSPS() {
             
             r = rand.measure(testy, p);
             r2 = ari.measure(testy, p);
-            System.out.format("Testing rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+            System.out.format("Testing rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
             assertTrue(r > 0.88);
             assertTrue(r2 > 0.45);
         } catch (Exception ex) {

File: core/src/test/java/smile/vq/NeuralMapTest.java
Patch:
@@ -89,7 +89,7 @@ public void testUSPS() {
             
             double r = rand.measure(y, p);
             double r2 = ari.measure(y, p);
-            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
             //assertTrue(r > 0.65);
             //assertTrue(r2 > 0.18);
             
@@ -100,7 +100,7 @@ public void testUSPS() {
             
             r = rand.measure(testy, p);
             r2 = ari.measure(testy, p);
-            System.out.format("Testing rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+            System.out.format("Testing rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
             //assertTrue(r > 0.65);
             //assertTrue(r2 > 0.18);
         } catch (Exception ex) {

File: core/src/test/java/smile/vq/SOMTest.java
Patch:
@@ -77,7 +77,7 @@ public void testUSPS() {
             RandIndex rand = new RandIndex();
             double r = rand.measure(y, label);
             double r2 = ari.measure(y, label);
-            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+            System.out.format("Training rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
             assertTrue(r > 0.88);
             assertTrue(r2 > 0.45);
             
@@ -88,7 +88,7 @@ public void testUSPS() {
             
             r = rand.measure(testy, p);
             r2 = ari.measure(testy, p);
-            System.out.format("Testing rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
+            System.out.format("Testing rand index = %.2f%%\tadjusted rand index = %.2f%%%n", 100.0 * r, 100.0 * r2);
             assertTrue(r > 0.88);
             assertTrue(r2 > 0.45);
         } catch (Exception ex) {

File: core/src/test/java/smile/wavelet/BestLocalizedWaveletTest.java
Patch:
@@ -53,7 +53,7 @@ public void testFilter() {
         System.out.println("filter");
         int[] order = {14, 18, 20};
         for (int p : order) {
-            System.out.format("p = %d\n", p);
+            System.out.format("p = %d%n", p);
             double[] a = {.2, -.4, -.6, -.5, -.8, -.4, -.9, 0, -.2, .1, -.1, .1, .7, .9, 0, .3};
             double[] b = a.clone();
             Wavelet instance = new BestLocalizedWavelet(p);

File: core/src/test/java/smile/wavelet/CoifletWaveletTest.java
Patch:
@@ -52,7 +52,7 @@ public void tearDown() {
     public void testFilter() {
         System.out.println("filter");
         for (int p = 6; p <= 30; p += 6) {
-            System.out.format("p = %d\n", p);
+            System.out.format("p = %d%n", p);
             double[] a = {.2, -.4, -.6, -.5, -.8, -.4, -.9, 0, -.2, .1, -.1, .1, .7, .9, 0, .3};
             double[] b = a.clone();
             Wavelet instance = new CoifletWavelet(p);

File: core/src/test/java/smile/wavelet/DaubechiesWaveletTest.java
Patch:
@@ -55,7 +55,7 @@ public void tearDown() {
     public void testFilter() {
         System.out.println("filter");
         for (int p = 2; p <= 20; p += 2) {
-            System.out.format("p = %d\n", p);
+            System.out.format("p = %d%n", p);
             double[] a = {.2, -.4, -.6, -.5, -.8, -.4, -.9, 0, -.2, .1, -.1, .1, .7, .9, 0, .3};
             double[] b = a.clone();
             Wavelet instance = new DaubechiesWavelet(p);

File: core/src/test/java/smile/wavelet/SymmletWaveletTest.java
Patch:
@@ -52,7 +52,7 @@ public void tearDown() {
     public void testFilter() {
         System.out.println("filter");
         for (int p = 8; p <= 20; p += 2) {
-            System.out.format("p = %d\n", p);
+            System.out.format("p = %d%n", p);
             double[] a = {.2, -.4, -.6, -.5, -.8, -.4, -.9, 0, -.2, .1, -.1, .1, .7, .9, 0, .3};
             double[] b = a.clone();
             Wavelet instance = new SymmletWavelet(p);

File: demo/src/main/java/smile/demo/clustering/CLARANSDemo.java
Patch:
@@ -55,7 +55,7 @@ public JComponent learn() {
         try {
             numLocal = Integer.parseInt(numLocalField.getText().trim());
             if (numLocal < 5) {
-                JOptionPane.showMessageDialog(this, "Toll smal NumLocal: " + numLocal, "Error", JOptionPane.ERROR_MESSAGE);
+                JOptionPane.showMessageDialog(this, "Too small NumLocal: " + numLocal, "Error", JOptionPane.ERROR_MESSAGE);
                 return null;
             }
         } catch (Exception e) {

File: plot/src/main/java/smile/swing/table/DoubleArrayCellRenderer.java
Patch:
@@ -35,6 +35,7 @@ public DoubleArrayCellRenderer() {
     public void setValue(Object value) {
         if (value == null) {
             setText("");
+            return;
         }
         
         double[] data = (double[]) value;

File: plot/src/main/java/smile/swing/table/FloatArrayCellRenderer.java
Patch:
@@ -35,6 +35,7 @@ public FloatArrayCellRenderer() {
     public void setValue(Object value) {
         if (value == null) {
             setText("");
+            return;
         }
         
         float[] data = (float[]) value;

File: plot/src/main/java/smile/swing/table/FontCellRenderer.java
Patch:
@@ -16,7 +16,6 @@
 package smile.swing.table;
 
 import java.awt.Font;
-
 import javax.swing.table.DefaultTableCellRenderer;
 
 /**
@@ -45,6 +44,7 @@ public FontCellRenderer(String text) {
     public void setValue(Object value) {
         if (value == null) {
             setText("");
+            return;
         }
         
         Font font = (Font) value;

File: plot/src/main/java/smile/swing/table/IntegerArrayCellRenderer.java
Patch:
@@ -35,6 +35,7 @@ public IntegerArrayCellRenderer() {
     public void setValue(Object value) {
         if (value == null) {
             setText("");
+            return;
         }
         
         int[] data = (int[]) value;

File: plot/src/main/java/smile/swing/table/LongArrayCellRenderer.java
Patch:
@@ -35,6 +35,7 @@ public LongArrayCellRenderer() {
     public void setValue(Object value) {
         if (value == null) {
             setText("");
+            return;
         }
         
         long[] data = (long[]) value;

File: plot/src/main/java/smile/swing/table/ShortArrayCellRenderer.java
Patch:
@@ -35,6 +35,7 @@ public ShortArrayCellRenderer() {
     public void setValue(Object value) {
         if (value == null) {
             setText("");
+            return;
         }
         
         short[] data = (short[]) value;

File: plot/src/main/java/smile/swing/table/DoubleArrayCellRenderer.java
Patch:
@@ -35,6 +35,7 @@ public DoubleArrayCellRenderer() {
     public void setValue(Object value) {
         if (value == null) {
             setText("");
+            return;
         }
         
         double[] data = (double[]) value;

File: plot/src/main/java/smile/swing/table/FloatArrayCellRenderer.java
Patch:
@@ -35,6 +35,7 @@ public FloatArrayCellRenderer() {
     public void setValue(Object value) {
         if (value == null) {
             setText("");
+            return;
         }
         
         float[] data = (float[]) value;

File: plot/src/main/java/smile/swing/table/FontCellRenderer.java
Patch:
@@ -15,9 +15,8 @@
  *******************************************************************************/
 package smile.swing.table;
 
-import java.awt.Font;
-
 import javax.swing.table.DefaultTableCellRenderer;
+import java.awt.*;
 
 /**
  * Font renderer in JTable.
@@ -45,6 +44,7 @@ public FontCellRenderer(String text) {
     public void setValue(Object value) {
         if (value == null) {
             setText("");
+            return;
         }
         
         Font font = (Font) value;

File: plot/src/main/java/smile/swing/table/IntegerArrayCellRenderer.java
Patch:
@@ -35,6 +35,7 @@ public IntegerArrayCellRenderer() {
     public void setValue(Object value) {
         if (value == null) {
             setText("");
+            return;
         }
         
         int[] data = (int[]) value;

File: plot/src/main/java/smile/swing/table/LongArrayCellRenderer.java
Patch:
@@ -35,6 +35,7 @@ public LongArrayCellRenderer() {
     public void setValue(Object value) {
         if (value == null) {
             setText("");
+            return;
         }
         
         long[] data = (long[]) value;

File: plot/src/main/java/smile/swing/table/ShortArrayCellRenderer.java
Patch:
@@ -35,6 +35,7 @@ public ShortArrayCellRenderer() {
     public void setValue(Object value) {
         if (value == null) {
             setText("");
+            return;
         }
         
         short[] data = (short[]) value;

File: core/src/main/java/smile/classification/DecisionTree.java
Patch:
@@ -265,7 +265,7 @@ public enum SplitRule {
     /**
      * Classification tree node.
      */
-    class Node {
+    class Node implements Serializable {
 
         /**
          * Predicted class label for this node.

File: plot/src/main/java/smile/swing/table/ButtonCellRenderer.java
Patch:
@@ -56,7 +56,7 @@ public class ButtonCellRenderer extends AbstractCellEditor
 	implements TableCellRenderer, TableCellEditor, ActionListener, MouseListener
 {
     private JTable table;
-    private Action action;
+    private transient Action action;
     private int mnemonic;
     private Border originalBorder;
     private Border focusBorder;

File: core/src/main/java/smile/classification/DecisionTree.java
Patch:
@@ -265,7 +265,7 @@ public enum SplitRule {
     /**
      * Classification tree node.
      */
-    class Node {
+    class Node implements Serializable {
 
         /**
          * Predicted class label for this node.

File: plot/src/main/java/smile/swing/table/ButtonCellRenderer.java
Patch:
@@ -56,7 +56,7 @@ public class ButtonCellRenderer extends AbstractCellEditor
 	implements TableCellRenderer, TableCellEditor, ActionListener, MouseListener
 {
     private JTable table;
-    private Action action;
+    private transient Action action;
     private int mnemonic;
     private Border originalBorder;
     private Border focusBorder;

File: core/src/main/java/smile/mds/IsotonicMDS.java
Patch:
@@ -33,7 +33,7 @@
  * @author Haifeng Li
  */
 public class IsotonicMDS {
-    private static final Logger logger = LoggerFactory.getLogger(SammonMapping.class);
+    private static final Logger logger = LoggerFactory.getLogger(IsotonicMDS.class);
 
     /**
      * The final stress achieved.

File: core/src/main/java/smile/regression/LASSO.java
Patch:
@@ -156,7 +156,7 @@ public static class Trainer extends RegressionTrainer<double[]> {
         /**
          * The maximum number of IPM (Newton) iterations.
          */
-        private int maxIter = 500;
+        private int maxIter = 1000;
 
         /**
          * Constructor.
@@ -212,7 +212,7 @@ public LASSO train(double[][] x, double[] y) {
      * @param lambda the shrinkage/regularization parameter.
      */
     public LASSO(double[][] x, double[] y, double lambda) {
-        this(x, y, lambda, 1E-3, 5000);
+        this(x, y, lambda, 1E-4, 1000);
     }
     
     /**

File: core/src/main/java/smile/classification/RandomForest.java
Patch:
@@ -403,7 +403,7 @@ public Tree call() {
             double accuracy = 1.0;
             if (oob != 0) {
                 accuracy = (double) correct / oob;
-                logger.info("Random forest tree OOB accuracy: {}%", accuracy);
+                logger.info("Random forest tree OOB accuracy: {}", String.format("%.2f%%", 100 * accuracy));
             } else {
                 logger.error("Random forest has a tree trained without OOB samples.");
             }

File: math/src/main/java/smile/math/matrix/LUDecomposition.java
Patch:
@@ -24,7 +24,7 @@
  * and a permutation vector piv of length m so that A(piv,:) = L*U.
  * If m &lt; n, then L is m-by-m and U is m-by-n.
  * <p>
- * The LU decompostion with pivoting always exists, even if the matrix is
+ * The LU decomposition with pivoting always exists, even if the matrix is
  * singular. The primary use of the LU decomposition is in the solution of
  * square systems of simultaneous linear equations if it is not singular.
  * <p>

File: core/src/main/java/smile/classification/AdaBoost.java
Patch:
@@ -217,8 +217,8 @@ public AdaBoost(Attribute[] attributes, double[][] x, int[] y, int ntrees, int m
             throw new IllegalArgumentException(String.format("The sizes of X and Y don't match: %d != %d", x.length, y.length));
         }
 
-        if (maxNodes < 1) {
-            throw new IllegalArgumentException("Invalid number of trees: " + maxNodes);
+        if (ntrees < 1) {
+            throw new IllegalArgumentException("Invalid number of trees: " + ntrees);
         }
         
         if (maxNodes < 2) {

File: core/src/main/java/smile/validation/FMeasure.java
Patch:
@@ -18,7 +18,7 @@
 
 /**
  * The F-score (or F-measure) considers both the precision and the recall of the test
- * to compute the score. THe precision p is the number of correct positive results
+ * to compute the score. The precision p is the number of correct positive results
  * divided by the number of all positive results, and the recall r is the number of
  * correct positive results divided by the number of positive results that should
  * have been returned.

File: math/src/main/java/smile/math/matrix/EigenValueDecomposition.java
Patch:
@@ -906,10 +906,9 @@ private static int error_bound(boolean[] enough, double[] ritz, double[] bnd, in
 
         logger.info("Lancozs method found {} converged eigenvalues of the {}-by-{} matrix", neig, step + 1, step + 1);
         if (neig != 0) {
-            logger.info("ritz: {}", ritz);
             for (int i = 0, n = 0; i <= step; i++) {
                 if (bnd[i] <= 16.0 * Math.EPSILON * Math.abs(ritz[i])) {
-                    logger.info("ritz[{}] = ", i, ritz[i]);
+                    logger.info("ritz[{}] = {}", i, ritz[i]);
                 }
             }
         }

File: core/src/main/java/smile/projection/GHA.java
Patch:
@@ -26,7 +26,7 @@
  * <p>
  * It guarantees that GHA finds the first k eigenvectors of the covariance matrix,
  * assuming that the associated eigenvalues are distinct. The convergence theorem
- * is forumulated in terms of a time-varying learning rate &eta;. In practice, the
+ * is formulated in terms of a time-varying learning rate &eta;. In practice, the
  * learning rate &eta; is chosen to be a small constant, in which case convergence is
  * guaranteed with mean-squared error in synaptic weights of order &eta;.
  * <p>

File: core/src/main/java/smile/projection/PCA.java
Patch:
@@ -109,7 +109,7 @@ public PCA(double[][] data) {
      * is larger than the data dimension and cor = false, SVD is employed for
      * efficiency. Otherwise, eigen decomposition on covariance or correlation
      * matrix is performed.
-     * @param cor true if use correlation matrix instead of covariance matrix if ture.
+     * @param cor true use correlation matrix instead of covariance matrix if true.
      */
     public PCA(double[][] data, boolean cor) {
         int m = data.length;

File: core/src/main/java/smile/classification/LDA.java
Patch:
@@ -189,7 +189,7 @@ public LDA(double[][] x, int[] y, double[] priori, double tol) {
             double sum = 0.0;
             for (double pr : priori) {
                 if (pr <= 0.0 || pr >= 1.0) {
-                    throw new IllegalArgumentException("Invlaid priori probability: " + pr);
+                    throw new IllegalArgumentException("Invalid priori probability: " + pr);
                 }
                 sum += pr;
             }

File: core/src/main/java/smile/classification/NaiveBayes.java
Patch:
@@ -288,13 +288,13 @@ public NaiveBayes train(double[][] x, int[] y) {
      */
     public NaiveBayes(double[] priori, Distribution[][] condprob) {
         if (priori.length != condprob.length) {
-            throw new IllegalArgumentException("The number of priori probabilites and that of the classes are not same.");
+            throw new IllegalArgumentException("The number of priori probabilities and that of the classes are not same.");
         }
 
         double sum = 0.0;
         for (double pr : priori) {
             if (pr <= 0.0 || pr >= 1.0) {
-                throw new IllegalArgumentException("Invlaid priori probability: " + pr);
+                throw new IllegalArgumentException("Invalid priori probability: " + pr);
             }
             sum += pr;
         }
@@ -405,7 +405,7 @@ public NaiveBayes(Model model, double[] priori, int p, double sigma) {
         double sum = 0.0;
         for (double pr : priori) {
             if (pr <= 0.0 || pr >= 1.0) {
-                throw new IllegalArgumentException("Invlaid priori probability: " + pr);
+                throw new IllegalArgumentException("Invalid priori probability: " + pr);
             }
             sum += pr;
         }

File: core/src/main/java/smile/classification/NeuralNetwork.java
Patch:
@@ -353,7 +353,7 @@ public Trainer setWeightDecay(double lambda) {
          */
         public Trainer setNumEpochs(int epochs) {
             if (epochs < 1) {
-                throw new IllegalArgumentException("Invlaid numer of epochs of stochastic learning:" + epochs);
+                throw new IllegalArgumentException("Invalid numer of epochs of stochastic learning:" + epochs);
             }
         
             this.epochs = epochs;

File: core/src/main/java/smile/classification/QDA.java
Patch:
@@ -179,7 +179,7 @@ public QDA(double[][] x, int[] y, double[] priori, double tol) {
             double sum = 0.0;
             for (double pr : priori) {
                 if (pr <= 0.0 || pr >= 1.0) {
-                    throw new IllegalArgumentException("Invlaid priori probability: " + pr);
+                    throw new IllegalArgumentException("Invalid priori probability: " + pr);
                 }
                 sum += pr;
             }

File: core/src/main/java/smile/classification/RDA.java
Patch:
@@ -184,7 +184,7 @@ public RDA(double[][] x, int[] y, double[] priori, double alpha, double tol) {
             double sum = 0.0;
             for (double pr : priori) {
                 if (pr <= 0.0 || pr >= 1.0) {
-                    throw new IllegalArgumentException("Invlaid priori probability: " + pr);
+                    throw new IllegalArgumentException("Invalid priori probability: " + pr);
                 }
                 sum += pr;
             }

File: core/src/main/java/smile/classification/RandomForest.java
Patch:
@@ -193,7 +193,7 @@ public Trainer setSplitRule(DecisionTree.SplitRule rule) {
          */
         public Trainer setNumTrees(int ntrees) {
             if (ntrees < 1) {
-                throw new IllegalArgumentException("Invlaid number of trees: " + ntrees);
+                throw new IllegalArgumentException("Invalid number of trees: " + ntrees);
             }
 
             this.ntrees = ntrees;

File: core/src/main/java/smile/classification/SVM.java
Patch:
@@ -267,7 +267,7 @@ public Trainer(MercerKernel<T> kernel, double C, double[] weight, Multiclass str
          */
         public Trainer setTolerance(double tol) {
             if (tol <= 0.0) {
-                throw new IllegalArgumentException("Invlaid tolerance of convergence test:" + tol);
+                throw new IllegalArgumentException("Invalid tolerance of convergence test:" + tol);
             }
 
             this.tol = tol;
@@ -280,7 +280,7 @@ public Trainer setTolerance(double tol) {
          */
         public Trainer setNumEpochs(int epochs) {
             if (epochs < 1) {
-                throw new IllegalArgumentException("Invlaid numer of epochs of stochastic learning:" + epochs);
+                throw new IllegalArgumentException("Invalid numer of epochs of stochastic learning:" + epochs);
             }
         
             this.epochs = epochs;
@@ -1045,7 +1045,7 @@ public SVM(MercerKernel<T> kernel, double C, double[] weight, Multiclass strateg
      */
     public SVM setTolerance(double tol) {
         if (tol <= 0.0) {
-            throw new IllegalArgumentException("Invlaid tolerance of convergence test:" + tol);
+            throw new IllegalArgumentException("Invalid tolerance of convergence test:" + tol);
         }
         
         this.tol = tol;

File: core/src/main/java/smile/clustering/DENCLUE.java
Patch:
@@ -90,7 +90,7 @@ public DENCLUE(double[][] data, double sigma, int m) {
         }
         
         if (m <= 0) {
-            throw new IllegalArgumentException("Invlaid number of selected samples: " + m);
+            throw new IllegalArgumentException("Invalid number of selected samples: " + m);
         }
         
         if (m < 10) {

File: core/src/main/java/smile/feature/GAFeatureSelection.java
Patch:
@@ -149,7 +149,7 @@ public BitString[] learn(int size, int generation, ClassifierTrainer<double[]> t
         }
         
         if (generation <= 0) {
-            throw new IllegalArgumentException("Invlid number of generations to go: " + generation);
+            throw new IllegalArgumentException("Invalid number of generations to go: " + generation);
         }
         
         if (x.length != y.length) {
@@ -336,7 +336,7 @@ public BitString[] learn(int size, int generation, RegressionTrainer<double[]> t
         }
         
         if (generation <= 0) {
-            throw new IllegalArgumentException("Invlid number of generations to go: " + generation);
+            throw new IllegalArgumentException("Invalid number of generations to go: " + generation);
         }
         
         if (x.length != y.length) {

File: core/src/main/java/smile/gap/GeneticAlgorithm.java
Patch:
@@ -353,7 +353,7 @@ public T evolve(int generation) {
      */
     public T evolve(int generation, double threshold) {
         if (generation <= 0) {
-            throw new IllegalArgumentException("Invlid number of generations to go: " + generation);
+            throw new IllegalArgumentException("Invalid number of generations to go: " + generation);
         }
         
         // Calculate the fitness of each chromosome.

File: core/src/main/java/smile/regression/RandomForest.java
Patch:
@@ -148,7 +148,7 @@ public Trainer(Attribute[] attributes, int ntrees) {
          */
         public Trainer setNumTrees(int ntrees) {
             if (ntrees < 1) {
-                throw new IllegalArgumentException("Invlaid number of trees: " + ntrees);
+                throw new IllegalArgumentException("Invalid number of trees: " + ntrees);
             }
 
             this.ntrees = ntrees;

File: core/src/main/java/smile/regression/SVR.java
Patch:
@@ -180,7 +180,7 @@ public Trainer(MercerKernel<T> kernel, double eps, double C) {
          */
         public Trainer setTolerance(double tol) {
             if (tol <= 0.0) {
-                throw new IllegalArgumentException("Invlaid tolerance of convergence test:" + tol);
+                throw new IllegalArgumentException("Invalid tolerance of convergence test:" + tol);
             }
 
             this.tol = tol;
@@ -262,7 +262,7 @@ public SVR(T[] x, double[] y, double[] weight, MercerKernel<T> kernel, double ep
         }
         
         if (tol <= 0.0) {
-            throw new IllegalArgumentException("Invlaid tolerance of convergence test:" + tol);
+            throw new IllegalArgumentException("Invalid tolerance of convergence test:" + tol);
         }
         
         this.kernel = kernel;

File: plot/src/main/java/smile/plot/Histogram.java
Patch:
@@ -268,7 +268,7 @@ private static double[][] histogram(double[] data, int k, boolean prob) {
     private static double[][] histogram(int[] data, double[] breaks, boolean prob) {
         int k = breaks.length - 1;
         if (k <= 1) {
-            throw new IllegalArgumentException("Invlaid number of bins: " + k);
+            throw new IllegalArgumentException("Invalid number of bins: " + k);
         }
         
         double[][] hist = smile.math.Histogram.histogram(data, breaks);
@@ -298,7 +298,7 @@ private static double[][] histogram(int[] data, double[] breaks, boolean prob) {
     private static double[][] histogram(double[] data, double[] breaks, boolean prob) {
         int k = breaks.length - 1;
         if (k <= 1) {
-            throw new IllegalArgumentException("Invlaid number of bins: " + k);
+            throw new IllegalArgumentException("Invalid number of bins: " + k);
         }
         
         double[][] hist = smile.math.Histogram.histogram(data, breaks);

File: core/src/main/java/smile/feature/Nominal2Binary.java
Patch:
@@ -90,11 +90,11 @@ public Attribute[] attributes() {
     @Override
     public double f(double[] object, int id) {
         if (object.length != attributes.length) {
-            throw new IllegalArgumentException(String.format("Invalide object size %d, expected %d", object.length, attributes.length));            
+            throw new IllegalArgumentException(String.format("Invalid object size %d, expected %d", object.length, attributes.length));
         }
         
         if (id < 0 || id >= features.length) {
-            throw new IllegalArgumentException("Invalide feature id: " + id);
+            throw new IllegalArgumentException("Invalid feature id: " + id);
         }
         
         if (object[map[id]] == value[id]) {

File: math/src/main/java/smile/math/matrix/SparseMatrix.java
Patch:
@@ -65,7 +65,7 @@ public class SparseMatrix implements IMatrix {
      * Constructor.
      * @param nrows the number of rows in the matrix.
      * @param ncols the number of columns in the matrix.
-     * @param size the number of nonzero entries in the matrix.
+     * @param nvals the number of nonzero entries in the matrix.
      */
     private SparseMatrix(int nrows, int ncols, int nvals) {
         this.nrows = nrows;

File: nlp/src/main/java/smile/nlp/collocation/AprioriPhraseExtractor.java
Patch:
@@ -42,7 +42,7 @@
  */
 public class AprioriPhraseExtractor {
 
-    /**
+    /** Extracts n-gram phrases.
      * 
      * @param sentences A collection of sentences (already split).
      * @param maxNGramSize The maximum length of n-gram

File: nlp/src/main/java/smile/nlp/tokenizer/SimpleTokenizer.java
Patch:
@@ -106,7 +106,7 @@ public SimpleTokenizer() {
 
     /**
      * Constructor.
-     * @param splitContraction if true, most punctuation is from adjoining words.
+     * @param splitContraction if true, split adjoining words.
      */
     public SimpleTokenizer(boolean splitContraction) {
         this.splitContraction = splitContraction;

File: nlp/src/test/java/smile/nlp/collocation/AprioriPhraseExtractorTest.java
Patch:
@@ -76,9 +76,6 @@ public void testExtract() throws FileNotFoundException {
             for (String s : SimpleSentenceSplitter.getInstance().split(paragraph)) {
                 String[] sentence = tokenizer.split(s);
                 for (int i = 0; i < sentence.length; i++) {
-                    if (stemmer.stripPluralParticiple(sentence[i]).toLowerCase().equals("")) {
-                        System.out.println(Arrays.toString(sentence));
-                    }
                     sentence[i] = stemmer.stripPluralParticiple(sentence[i]).toLowerCase();
                 }
                 sentences.add(sentence);

File: core/src/main/java/smile/neighbor/KDTree.java
Patch:
@@ -297,7 +297,6 @@ private void search(double[] q, Node node, Neighbor<double[], E> neighbor) {
      *
      * @param q    the query key.
      * @param node the root of subtree.
-     * @param k    the number of neighbors to find.
      * @param heap the heap object to store/update the kNNs found during the search.
      */
     private void search(double[] q, Node node, HeapSelect<Neighbor<double[], E>> heap) {

File: core/src/main/java/smile/clustering/GrowingNeuralGas.java
Patch:
@@ -126,7 +126,6 @@ class Node implements Comparable<Node> {
 
         /**
          * Constructor.
-         * @param d the dimensionality of reference vector.
          */
         Node(double[] w) {
             this.w = w;
@@ -205,7 +204,7 @@ public GrowingNeuralGas(int d) {
      * of lambda, insert a new neuron.
      * @param alpha decrease error variables by multiplying them with alpha
      * during inserting a new neuron.
-     * @param beta decrease all error variables by multiply them with de.
+     * @param beta decrease all error variables by multiply them with beta.
      */
     public GrowingNeuralGas(int d, double epsBest, double epsNeighbor, int maxEdgeAge, int lambda, double alpha, double beta) {
         this.d = d;

File: core/src/main/java/smile/clustering/NeuralGas.java
Patch:
@@ -71,7 +71,6 @@ class Neuron implements Comparable<Neuron> {
 
         /**
          * Constructor.
-         * @param d the dimensionality of reference vector.
          */
         Neuron(double[] w) {
             this.w = w;

File: core/src/main/java/smile/clustering/KMeans.java
Patch:
@@ -24,7 +24,7 @@
 import smile.util.MulticoreExecutor;
 
 /**
- * K-Means learn aims to partition n observations into k clusters in which
+ * K-Means clustering. The algorithm partitions n observations into k clusters in which
  * each observation belongs to the cluster with the nearest mean.
  * Although finding an exact solution to the k-means problem for arbitrary
  * input is NP-hard, the standard approach to finding an approximate solution

File: core/src/main/java/smile/clustering/XMeans.java
Patch:
@@ -170,7 +170,6 @@ public XMeans(double[][] data, int kmax) {
 
     /**
      * Calculates the BIC for single cluster.
-     * @param k the number of clusters.
      * @param n the total number of samples.
      * @param d the dimensionality of data.
      * @param distortion the distortion of clusters.

File: core/src/main/java/smile/clustering/linkage/SingleLinkage.java
Patch:
@@ -35,7 +35,7 @@
 public class SingleLinkage extends Linkage {
     /**
      * Constructor.
-     * @param proximity  The proximity matrix to store the distance measure of
+     * @param proximity The proximity matrix to store the distance measure of
      * dissimilarity. To save space, we only need the lower half of matrix.
      */
     public SingleLinkage(double[][] proximity) {

File: core/src/test/java/smile/association/ARMTest.java
Patch:
@@ -160,8 +160,7 @@ public void testLearnKosarak() {
         List<int[]> dataList = new ArrayList<int[]>(1000);
 
         try {
-            InputStream stream = smile.data.parser.IOUtils.getTestDataReader("transaction/kosarak.dat");
-            BufferedReader input = new BufferedReader(new InputStreamReader(stream));
+            BufferedReader input = smile.data.parser.IOUtils.getTestDataReader("transaction/kosarak.dat");
 
             String line;
             for (int nrow = 0; (line = input.readLine()) != null; nrow++) {

File: core/src/test/java/smile/association/ARMTest.java
Patch:
@@ -117,8 +117,7 @@ public void testLearnPima() {
         List<int[]> dataList = new ArrayList<int[]>(1000);
 
         try {
-            InputStream stream = getClass().getResourceAsStream("/smile/data/transaction/pima.D38.N768.C2");
-            BufferedReader input = new BufferedReader(new InputStreamReader(stream));
+            BufferedReader input = smile.data.parser.IOUtils.getTestDataReader("transaction/pima.D38.N768.C2");
 
             String line;
             for (int nrow = 0; (line = input.readLine()) != null; nrow++) {
@@ -161,7 +160,7 @@ public void testLearnKosarak() {
         List<int[]> dataList = new ArrayList<int[]>(1000);
 
         try {
-            InputStream stream = getClass().getResourceAsStream("/smile/data/transaction/kosarak.dat");
+            InputStream stream = smile.data.parser.IOUtils.getTestDataReader("transaction/kosarak.dat");
             BufferedReader input = new BufferedReader(new InputStreamReader(stream));
 
             String line;

File: core/src/main/java/smile/association/FPGrowth.java
Patch:
@@ -118,7 +118,7 @@ public FPGrowth(int[][] itemsets, int minSupport) {
     }
 
     /**
-     * Add an item set into the object.
+     * Add an item set into the database.
      * @param itemset an item set, which should NOT contain duplicated items.
      * Note that it is reordered after the call.
      */
@@ -243,7 +243,6 @@ public Long call() {
      * <LI> If new local FP tree is not empty repeat mining operation.
      * </OL>
      * Otherwise end.
-     * @param header the header table item to start.
      * @param itemset the current item sets as generated so far (null at start).
      */
     private long grow(PrintStream out, List<ItemSet> list, TotalSupportTree ttree, FPTree fptree, int[] itemset) {
@@ -271,7 +270,6 @@ private long grow(PrintStream out, List<ItemSet> list, TotalSupportTree ttree, F
      * <LI> If new local FP tree is not empty repeat mining operation.
      * </OL>
      * Otherwise end.
-     * @param header the header table item to start.
      * @param itemset the current item sets as generated so far (null at start).
      */
     private long grow(PrintStream out, List<ItemSet> list, TotalSupportTree ttree, FPTree fptree, int[] itemset, int[] localItemSupport, int[] prefixItemset) {

File: plot/src/main/java/smile/plot/Wireframe.java
Patch:
@@ -73,7 +73,7 @@ public void paint(Graphics g) {
     
     /**
      * Create a wire frame plot canvas.
-     * @param vertices an m x n x 2 or m x n x 3 array which are coordinates of m x n grid.
+     * @param vertices a n-by-2 or n-by-3 array which are coordinates of n vertices.
      */
     public static PlotCanvas plot(double[][] vertices, int[][] edges) {
         double[] lowerBound = Math.colMin(vertices);
@@ -90,7 +90,7 @@ public static PlotCanvas plot(double[][] vertices, int[][] edges) {
     /**
      * Create a 2D grid plot canvas.
      * @param id the id of the plot.
-     * @param vertices an m x n x 2 array which are coordinates of m x n grid.
+     * @param vertices a n-by-2 or n-by-3 array which are coordinates of n vertices.
      * @param edges an m-by-2 array of which each row is the vertex indices of two
      * end points of each edge.
      */

File: demo/src/main/java/smile/demo/plot/HexmapDemo.java
Patch:
@@ -72,7 +72,7 @@ public HexmapDemo() {
         canvas.setTitle("redgreen");
         add(canvas);
         canvas = Hexmap.plot(z, Palette.heat(256));
-        canvas.setTitle("Hex");
+        canvas.setTitle("heat");
         add(canvas);
         canvas = Hexmap.plot(z, Palette.terrain(256));
         canvas.setTitle("terrain");

File: data/src/main/java/smile/data/parser/microarray/package-info.java
Patch:
@@ -19,7 +19,7 @@
  * collection of microscopic DNA spots attached to a solid surface. Scientists
  * use DNA microarrays to measure the expression levels of large numbers of
  * genes simultaneously or to genotype multiple regions of a genome. Each DNA
- * spot contains picomoles (10?12 moles) of a specific DNA sequence, known as
+ * spot contains picomoles (10 - 12 moles) of a specific DNA sequence, known as
  * probes (or reporters). These can be a short section of a gene or other DNA
  * element that are used to hybridize a cDNA or cRNA sample (called target)
  * under high-stringency conditions. Probe-target hybridization is usually

File: core/src/main/java/smile/wavelet/Wavelet.java
Patch:
@@ -21,7 +21,7 @@
 /**
  * A wavelet is a wave-like oscillation with an amplitude that starts out at
  * zero, increases, and then decreases back to zero. Like the fast Fourier
- * transform (FFT), the discrete wavelet trainsform (DWT) is a fast, linear
+ * transform (FFT), the discrete wavelet transform (DWT) is a fast, linear
  * operation that operates on a data vector whose length is an integer power
  * of 2, transforming it into a numerically different vector of the same length.
  * The wavelet transform is invertible and in fact orthogonal. Both FFT and DWT

File: core/src/main/java/smile/regression/RandomForest.java
Patch:
@@ -598,7 +598,7 @@ public double[] test(double[][] x, double[] y) {
      * Test the model on a validation dataset.
      * 
      * @param x the test data set.
-     * @param y the test data labels.
+     * @param y the test data output values.
      * @param measures the performance measures of regression.
      * @return performance measures with first 1, 2, ..., regression trees.
      */

File: core/src/main/java/smile/regression/GradientTreeBoost.java
Patch:
@@ -649,7 +649,7 @@ public double[] test(double[][] x, double[] y) {
      * Test the model on a validation dataset.
      * 
      * @param x the test data set.
-     * @param y the test data labels.
+     * @param y the test data output values.
      * @param measures the performance measures of regression.
      * @return performance measures with first 1, 2, ..., regression trees.
      */

File: core/src/main/java/smile/regression/RandomForest.java
Patch:
@@ -302,7 +302,6 @@ public RegressionTree call() {
                 }
             }
 
-
             RegressionTree tree = new RegressionTree(attributes, x, y, maxNodes, nodeSize, mtry, order, samples, null);
 
             for (int i = 0; i < n; i++) {

File: core/src/test/java/smile/regression/RandomForestTest.java
Patch:
@@ -92,7 +92,7 @@ public void testPredict() {
             double[] trainy = Math.slice(y, loocv.train[i]);
             
             try {
-                RandomForest forest = new RandomForest(trainx, trainy, 300, 2, 3, n);
+                RandomForest forest = new RandomForest(trainx, trainy, 300, n, 3, 2);
 
                 double r = y[loocv.test[i]] - forest.predict(longley[loocv.test[i]]);
                 rss += r * r;
@@ -126,7 +126,7 @@ public void test(String dataset, String url, int response) {
                 double[][] testx = Math.slice(datax, cv.test[i]);
                 double[] testy = Math.slice(datay, cv.test[i]);
 
-                RandomForest forest = new RandomForest(data.attributes(), trainx, trainy, 200, trainx[0].length/3, 5, n);
+                RandomForest forest = new RandomForest(data.attributes(), trainx, trainy, 200, n, 5, trainx[0].length/3);
                 System.out.format("OOB error rate = %.4f\n", forest.error());
 
                 for (int j = 0; j < testx.length; j++) {
@@ -189,7 +189,7 @@ public void testCPU() {
                 testy[i-m] = datay[index[i]];                
             }
 
-            RandomForest forest = new RandomForest(data.attributes(), trainx, trainy, 100, trainx[0].length / 3, 5, n);
+            RandomForest forest = new RandomForest(data.attributes(), trainx, trainy, 100, n, 5, trainx[0].length / 3);
             System.out.format("RMSE = %.4f\n", Validation.test(forest, testx, testy));
             
             double[] rmse = forest.test(testx, testy);

File: core/src/test/java/smile/classification/DecisionTreeTest.java
Patch:
@@ -209,7 +209,7 @@ public void testUSPSNominal() {
                 System.out.format("%s importance is %.4f\n", train.attributes()[index[i]], importance[i]);
             }
             
-            assertEquals(329, error);
+            assertEquals(324, error);
         } catch (Exception ex) {
             System.err.println(ex);
         }

File: core/src/main/java/smile/regression/RegressionTree.java
Patch:
@@ -1060,6 +1060,7 @@ public RegressionTree(int numFeatures, int[][] x, double[] y, int maxNodes, int
         this.maxNodes = maxNodes;
         this.nodeSize = nodeSize;
         this.numFeatures = numFeatures;
+        this.mtry = numFeatures;
         importance = new double[numFeatures];
         
         // Priority queue for best-first tree growing.

File: core/src/main/java/smile/sequence/CRF.java
Patch:
@@ -740,10 +740,10 @@ public Object call() {
                 
                 // Perform training.
                 if (x != null) {
-                    RegressionTree tree = new RegressionTree(attributes, x, y, maxLeaves, order, samples, null);
+                    RegressionTree tree = new RegressionTree(attributes, x, y, maxLeaves, 5, attributes.length, order, samples, null);
                     potential.add(tree);
                 } else {
-                    RegressionTree tree = new RegressionTree(numFeatures + numClasses + 1, sparseX, y, maxLeaves, samples, null);
+                    RegressionTree tree = new RegressionTree(numFeatures + numClasses + 1, sparseX, y, maxLeaves, 5, samples, null);
                     potential.add(tree);
                 }
                 

File: data/src/test/java/smile/data/parser/DelimitedTextParserTest.java
Patch:
@@ -63,9 +63,9 @@ public void testParse() throws Exception {
             double[][] x = usps.toArray(new double[usps.size()][]);
             int[] y = usps.toArray(new int[usps.size()]);
             
-            assertEquals(Attribute.Type.NOMINAL, usps.response().type);
+            assertEquals(Attribute.Type.NOMINAL, usps.response().getType());
             for (Attribute attribute : usps.attributes()) {
-                assertEquals(Attribute.Type.NUMERIC, attribute.type);
+                assertEquals(Attribute.Type.NUMERIC, attribute.getType());
             }
 
             assertEquals(7291, usps.size());

File: data/src/test/java/smile/data/parser/microarray/GCTParserTest.java
Patch:
@@ -63,8 +63,8 @@ public void testParse() throws Exception {
             String[] id = data.toArray(new String[data.size()]);
             
             for (Attribute attribute : data.attributes()) {
-                assertEquals(Attribute.Type.NUMERIC, attribute.type);
-                System.out.println(attribute.name);
+                assertEquals(Attribute.Type.NUMERIC, attribute.getType());
+                System.out.println(attribute.getName());
             }
 
             assertEquals(12564, data.size());

File: data/src/test/java/smile/data/parser/microarray/PCLParserTest.java
Patch:
@@ -63,8 +63,8 @@ public void testParse() throws Exception {
             String[] id = data.toArray(new String[data.size()]);
             
             for (Attribute attribute : data.attributes()) {
-                assertEquals(Attribute.Type.NUMERIC, attribute.type);
-                System.out.println(attribute.name);
+                assertEquals(Attribute.Type.NUMERIC, attribute.getType());
+                System.out.println(attribute.getName());
             }
 
             assertEquals(6694, data.size());

File: data/src/test/java/smile/data/parser/microarray/RESParserTest.java
Patch:
@@ -63,8 +63,8 @@ public void testParse() throws Exception {
             String[] id = data.toArray(new String[data.size()]);
             
             for (Attribute attribute : data.attributes()) {
-                assertEquals(Attribute.Type.NUMERIC, attribute.type);
-                System.out.println(attribute.name + "\t" + attribute.description);
+                assertEquals(Attribute.Type.NUMERIC, attribute.getType());
+                System.out.println(attribute.getName() + "\t" + attribute.getDescription());
             }
 
             assertEquals(7129, data.size());

File: data/src/test/java/smile/data/parser/microarray/TXTParserTest.java
Patch:
@@ -63,8 +63,8 @@ public void testParse() throws Exception {
             String[] id = data.toArray(new String[data.size()]);
             
             for (Attribute attribute : data.attributes()) {
-                assertEquals(Attribute.Type.NUMERIC, attribute.type);
-                System.out.println(attribute.name);
+                assertEquals(Attribute.Type.NUMERIC, attribute.getType());
+                System.out.println(attribute.getName());
             }
 
             assertEquals(6694, data.size());

File: core/src/main/java/smile/classification/AdaBoost.java
Patch:
@@ -398,7 +398,6 @@ public int predict(double[] x, double[] posteriori) {
             for (int i = 0; i < k; i++) posteriori[i] /= sum;
             return y > 0 ? 1 : 0;
         } else {
-            double[] y = new double[k];
             for (int i = 0; i < trees.length; i++) {
                 posteriori[trees[i].predict(x)] += alpha[i];
             }

File: core/src/main/java/smile/classification/DecisionTree.java
Patch:
@@ -141,7 +141,7 @@ public class DecisionTree implements Classifier<double[]> {
      * Random number generator for training, used in training. Math.random uses a static
      * object, which will cause troubles in training ensemble methods.
      */
-    private transient Random random = new Random();
+    private transient Random random = new Random(Thread.currentThread().getId() * System.currentTimeMillis());
 
     /**
      * Trainer for decision tree classifiers.

File: core/src/main/java/smile/regression/RegressionTree.java
Patch:
@@ -118,7 +118,7 @@ public class RegressionTree implements Regression<double[]> {
      * Random number generator for training, used in training. Math.random uses a static
      * object, which will cause troubles in training ensemble methods.
      */
-    private transient Random random = new Random();
+    private transient Random random = new Random(Thread.currentThread().getId() * System.currentTimeMillis());
 
     /**
      * Trainer for regression tree.

File: SmileMath/src/main/java/smile/math/kernel/HellingerKernel.java
Patch:
@@ -42,7 +42,7 @@ public double k(double[] x, double[] y) {
         
         double sum = 0;
         for (int i = 0; i < x.length; i++) {
-            sum += Math.sqr(x[i] * y[i]);
+            sum += Math.sqrt(x[i] * y[i]);
         }
 
         return sum;

File: SmileMath/src/test/java/smile/stat/distribution/LogNormalDistributionTest.java
Patch:
@@ -59,8 +59,8 @@ public void testLogNormalDistribution() {
         for (int i = 0; i < data.length; i++)
             data[i] = instance.rand();
         LogNormalDistribution est = new LogNormalDistribution(data);
-        assertEquals(3, est.getMu(), 1E-1);
-        assertEquals(2.1, est.getSigma(), 1E-1);
+        assertEquals(3, est.getMu(), 1.5E-1);
+        assertEquals(2.1, est.getSigma(), 1.5E-1);
     }
 
     /**

File: SmileMath/src/test/java/smile/stat/distribution/MultivariateGaussianDistributionTest.java
Patch:
@@ -87,10 +87,10 @@ public void testMultivariateGaussianDistribution() {
         }
         MultivariateGaussianDistribution est = new MultivariateGaussianDistribution(data, true);
         for (int i = 0; i < mu.length; i++) {
-            assertEquals(mu[i], est.mean()[i], 1E-1);
+            assertEquals(mu[i], est.mean()[i], 1.5E-1);
         }
         for (int i = 0; i < mu.length; i++) {
-            assertEquals(sigma[0][i], est.cov()[i][i], 1E-1);
+            assertEquals(sigma[0][i], est.cov()[i][i], 1.5E-1);
             for (int j = 0; j < mu.length; j++) {
                 if (i != j) {
                     assertEquals(0, est.cov()[i][j], 1E-10);

File: Smile/src/main/java/smile/neighbor/LSH.java
Patch:
@@ -530,8 +530,9 @@ public Neighbor<double[], E>[] knn(double[] q, int k) {
         if (hit < k) {
             @SuppressWarnings("unchecked")
             Neighbor<double[], E>[] n2 = (Neighbor<double[], E>[]) java.lang.reflect.Array.newInstance(neighbor.getClass(), hit);
+            int start = k - hit;
             for (int i = 0; i < hit; i++) {
-                n2[i] = neighbors[i + 1];
+                n2[i] = neighbors[i + start];
             }
             neighbors = n2;
         }

File: Smile/src/main/java/smile/neighbor/MPLSH.java
Patch:
@@ -1009,8 +1009,9 @@ public Neighbor<double[], E>[] knn(double[] q, int k, double recall, int T) {
         if (hit < k) {
             @SuppressWarnings("unchecked")
             Neighbor<double[], E>[] n2 = (Neighbor<double[], E>[]) java.lang.reflect.Array.newInstance(neighbor.getClass(), hit);
+            int start = k - hit;
             for (int i = 0; i < hit; i++) {
-                n2[i] = neighbors[i + 1];
+                n2[i] = neighbors[i + start];
             }
             neighbors = n2;
         }

File: Smile/src/test/java/smile/manifold/IsoMapTest.java
Patch:
@@ -1063,7 +1063,7 @@ public void testLearn() {
         double[][] data = new double[1000][];
         System.arraycopy(dat, 0, data, 0, data.length);
         
-        IsoMap isomap = new IsoMap(data, 2, 7);
+        IsoMap isomap = new IsoMap(data, 2, 7, false);
 
         double sign = Math.signum(points[0][0] / isomap.getCoordinates()[0][0]);
         for (int i = 0; i < points.length; i++) {
@@ -1075,6 +1075,7 @@ public void testLearn() {
             points[i][1] *= sign;
         }
 
+        // This is the results of standard Isomap.
         assertTrue(Math.equals(points, isomap.getCoordinates(), 1E-6));
     }
 }

File: SmileData/src/test/java/smile/data/SparseDatasetTest.java
Patch:
@@ -9,7 +9,6 @@
 import org.junit.Before;
 import org.junit.BeforeClass;
 import org.junit.Test;
-import smile.math.matrix.LUDecomposition;
 import smile.math.matrix.SparseMatrix;
 import static org.junit.Assert.*;
 

File: Smile/test/smile/clustering/BIRCHTest.java
Patch:
@@ -92,7 +92,7 @@ public void testUSPS() {
             r2 = ari.measure(testy, p);
             System.out.format("Testing rand index = %.2f%%\tadjusted rand index = %.2f%%\n", 100.0 * r, 100.0 * r2);
             assertTrue(r > 0.65);
-            assertTrue(r2 > 0.25);
+            assertTrue(r2 > 0.24);
         } catch (Exception ex) {
             System.err.println(ex);
         }

File: Smile/src/smile/util/SmileUtils.java
Patch:
@@ -150,7 +150,7 @@ public static GaussianRadialBasis[] learnGaussianRadialBasis(double[][] x, doubl
         int[] ni = kmeans.getClusterSize();
         GaussianRadialBasis[] rbf = new GaussianRadialBasis[k];
         for (int i = 0; i < k; i++) {
-            if (ni[i] >= 5 || sigma[i] == 0.0) {
+            if (ni[i] >= 5 || sigma[i] != 0.0) {
                 sigma[i] = Math.sqrt(sigma[i] / ni[i]);
             } else {
                 sigma[i] = Double.POSITIVE_INFINITY;
@@ -164,7 +164,7 @@ public static GaussianRadialBasis[] learnGaussianRadialBasis(double[][] x, doubl
                 }
                 sigma[i] /= 2.0;
             }
-            
+
             rbf[i] = new GaussianRadialBasis(r * sigma[i]);            
         }
         

File: SmileMath/src/smile/math/matrix/QRDecomposition.java
Patch:
@@ -159,7 +159,7 @@ public CholeskyDecomposition toCholesky() {
             L[i][i] = Rdiagonal[i];
 
             for (int j = 0; j < i; j++) {
-                    L[i][j] = QR[j][i];
+                L[i][j] = QR[j][i];
             }
         }
 

File: Smile/src/smile/regression/RandomForest.java
Patch:
@@ -332,7 +332,7 @@ public RandomForest(Attribute[] attributes, double[][] x, double[] y, int T, int
         try {
             trees = MulticoreExecutor.run(tasks);
         } catch (Exception ex) {
-            System.err.println(ex);
+            ex.printStackTrace();
 
             trees = new ArrayList<RegressionTree>(T);
             for (int i = 0; i < T; i++) {

File: Smile/src/smile/classification/NaiveBayes.java
Patch:
@@ -222,7 +222,7 @@ public Trainer(Model model, double[] priori, int p) {
                 if (prob <= 0.0 || prob >= 1.0) {
                     throw new IllegalArgumentException("Invalid priori probability: " + prob);
                 }
-                   sum += prob;
+                sum += prob;
             }
             
             if (Math.abs(sum - 1.0) > 1E-10) {

File: Smile/src/smile/classification/NaiveBayes.java
Patch:
@@ -220,8 +220,9 @@ public Trainer(Model model, double[] priori, int p) {
             double sum = 0.0;
             for (double prob : priori) {
                 if (prob <= 0.0 || prob >= 1.0) {
-                    throw new IllegalArgumentException("Invlaid priori probability: " + prob);
+                    throw new IllegalArgumentException("Invalid priori probability: " + prob);
                 }
+                   sum += prob;
             }
             
             if (Math.abs(sum - 1.0) > 1E-10) {

File: SmilePlot/src/smile/swing/Table.java
Patch:
@@ -51,7 +51,6 @@
 import smile.swing.table.TableCopyPasteAdapter;
 
 /**
- *
  * Customized JTable with optional row number header. It also provides the
  * renderer and editor Color and Font. It also provides a renderer for
  * Float/Double of special values (such as NaN and Infinity).

File: SmilePlot/src/smile/plot/PlotCanvas.java
Patch:
@@ -928,7 +928,7 @@ private JDialog createPropertyDialog() {
 
         // There is a known issue with JTables whereby the changes made in a
         // cell editor are not committed when focus is lost.
-        // This can result in the table staying in ‘edit mode’ with the stale
+        // This can result in the table staying in 'edit mode' with the stale
         // value in the cell being edited still showing, although the change
         // has not actually been committed to the model.
         //

File: SmilePlot/src/smile/swing/AlphaIcon.java
Patch:
@@ -10,8 +10,7 @@
  * An Icon wrapper that paints the contained icon with a specified transparency.
  * <P>
  * <B>Note:</B> This class is not suitable for wrapping an
- * <CODE>ImageIcon</CODE> that holds an animated image. To show an animated
- * ImageIcon with transparency, use the companion class {@link AlphaImageIcon}.
+ * <CODE>ImageIcon</CODE> that holds an animated image.
  */
 public class AlphaIcon implements Icon {
 

File: Smile/src/smile/association/FPGrowth.java
Patch:
@@ -44,7 +44,7 @@
  * <h2>References</h2>
  * <ol>
  * <li> Jiawei Han, Jian Pei, Yiwen Yin, and Runying Mao. Mining frequent patterns without candidate generation. Data Mining and Knowledge Discovery 8:53-87, 2004.</li>
- * <li> G�sta Grahne and Jianfei Zhu. Fast algorithms for frequent itemset mining using FP-trees. IEEE TRANS. ON KNOWLEDGE AND DATA ENGINEERING 17(10):1347-1362, 2005.</li>
+ * <li> Gosta Grahne and Jianfei Zhu. Fast algorithms for frequent itemset mining using FP-trees. IEEE TRANS. ON KNOWLEDGE AND DATA ENGINEERING 17(10):1347-1362, 2005.</li>
  * <li> Christian Borgelt. An Implementation of the FP-growth Algorithm. OSDM, 1-5, 2005.</li>
  * </ol>
  * 

File: Smile/src/smile/classification/NaiveBayes.java
Patch:
@@ -60,7 +60,7 @@
  * 
  * <h2>References</h2>
  * <ol>
- * <li> Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch�tze. Introduction to Information Retrieval, Chapter 13, 2009.</li>
+ * <li> Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schutze. Introduction to Information Retrieval, Chapter 13, 2009.</li>
  * </ol>
  * 
  * @author Haifeng Li

File: Smile/src/smile/classification/SVM.java
Patch:
@@ -60,10 +60,10 @@
  * 
  * <h2>References</h2>
  * <ol>
- * <li> Christopher J. C. Burges. A Tutorial on Support Vector Machines for Pattern Recognition. Data Mining and Knowledge Discovery 2:121�167, 1998.</li>
+ * <li> Christopher J. C. Burges. A Tutorial on Support Vector Machines for Pattern Recognition. Data Mining and Knowledge Discovery 2:121-167, 1998.</li>
  * <li> John Platt. Sequential Minimal Optimization: A Fast Algorithm for Training Support Vector Machines.</li>
- * <li> Rong-En Fan, Pai-Hsuen, and Chih-Jen Lin. Working Set Selection Using Second Order Information for Training Support Vector Machines. JMLR, 6:1889�1918, 2005.</li>
- * <li> Antoine Bordes, Seyda Ertekin, Jason Weston and L�on Bottou. Fast Kernel Classifiers with Online and Active Learning, Journal of Machine Learning Research, 6:1579-1619, 2005.</li>
+ * <li> Rong-En Fan, Pai-Hsuen, and Chih-Jen Lin. Working Set Selection Using Second Order Information for Training Support Vector Machines. JMLR, 6:1889-1918, 2005.</li>
+ * <li> Antoine Bordes, Seyda Ertekin, Jason Weston and Leon Bottou. Fast Kernel Classifiers with Online and Active Learning, Journal of Machine Learning Research, 6:1579-1619, 2005.</li>
  * <li> Tobias Glasmachers and Christian Igel. Second Order SMO Improves SVM Online and Active Learning.</li>
  * <li> Chih-Chung Chang and Chih-Jen Lin. LIBSVM: a Library for Support Vector Machines.</li>
  * </ol>

File: Smile/src/smile/classification/package-info.java
Patch:
@@ -108,7 +108,7 @@
  * on model parameters.</dd>
  * <dt>Bias-variance tradeoff</dt>
  * <dd>Mean squared error (MSE) can be broken down into two components:
- * variance and squared bias, known as the bias�variance decomposition.
+ * variance and squared bias, known as the bias-variance decomposition.
  * Thus in order to minimize the MSE, we need to minimize both the bias and
  * the variance. However, this is not trivial. Therefore, there is a tradeoff
  * between bias and variance.</dd>

File: Smile/src/smile/clustering/DBScan.java
Patch:
@@ -67,8 +67,8 @@
  *
  * <h2>References</h2>
  * <ol>
- * <li> Martin Ester, Hans-Peter Kriegel, J�rg Sander, Xiaowei Xu (1996-). A density-based algorithm for discovering clusters in large spatial databases with noise". KDD, 1996. </li>
- * <li> J�rg Sander, Martin Ester, Hans-Peter  Kriegel, Xiaowei Xu. (1998). Density-Based Clustering in Spatial Databases: The Algorithm GDBSCAN and Its Applications. 1998. </li>
+ * <li> Martin Ester, Hans-Peter Kriegel, Jorg Sander, Xiaowei Xu (1996-). A density-based algorithm for discovering clusters in large spatial databases with noise". KDD, 1996. </li>
+ * <li> Jorg Sander, Martin Ester, Hans-Peter  Kriegel, Xiaowei Xu. (1998). Density-Based Clustering in Spatial Databases: The Algorithm GDBSCAN and Its Applications. 1998. </li>
  * </ol>
  * 
  * @param <T> the type of input object.

File: Smile/src/smile/clustering/KMeans.java
Patch:
@@ -44,7 +44,7 @@
  * <h2>References</h2>
  * <ol>
  * <li> Tapas Kanungo, David M. Mount, Nathan S. Netanyahu, Christine D. Piatko, Ruth Silverman, and Angela Y. Wu. An Efficient k-Means Clustering Algorithm: Analysis and Implementation. IEEE TRANS. PAMI, 2002.</li>
- * <li> D. Arthur and S. Vassilvitskii. "K-means++: the advantages of careful seeding". ACM-SIAM symposium on Discrete algorithms, 1027�1035, 2007.</li>
+ * <li> D. Arthur and S. Vassilvitskii. "K-means++: the advantages of careful seeding". ACM-SIAM symposium on Discrete algorithms, 1027-1035, 2007.</li>
  * <li> Anna D. Peterson, Arka P. Ghosh and Ranjan Maitra. A systematic evaluation of different methods for initializing the K-means clustering algorithm. 2010.</li>
  * </ol>
  * 

File: Smile/src/smile/clustering/PartitionClustering.java
Patch:
@@ -124,7 +124,7 @@ static enum DistanceMethod {
      * 
      * <h2>References</h2>
      * <ol>
-     * <li> D. Arthur and S. Vassilvitskii. "K-means++: the advantages of careful seeding". ACM-SIAM symposium on Discrete algorithms, 1027�1035, 2007.</li>
+     * <li> D. Arthur and S. Vassilvitskii. "K-means++: the advantages of careful seeding". ACM-SIAM symposium on Discrete algorithms, 1027-1035, 2007.</li>
      * <li> Anna D. Peterson, Arka P. Ghosh and Ranjan Maitra. A systematic evaluation of different methods for initializing the K-means clustering algorithm. 2010.</li>
      * </ol>
      * 
@@ -231,7 +231,7 @@ static int[] seed(double[][] data, int k, DistanceMethod method) {
      * 
      * <h2>References</h2>
      * <ol>
-     * <li> D. Arthur and S. Vassilvitskii. "K-means++: the advantages of careful seeding". ACM-SIAM symposium on Discrete algorithms, 1027�1035, 2007.</li>
+     * <li> D. Arthur and S. Vassilvitskii. "K-means++: the advantages of careful seeding". ACM-SIAM symposium on Discrete algorithms, 1027-1035, 2007.</li>
      * <li> Anna D. Peterson, Arka P. Ghosh and Ranjan Maitra. A systematic evaluation of different methods for initializing the K-means clustering algorithm. 2010.</li>
      * </ol>
      * 

File: Smile/src/smile/gap/GeneticAlgorithm.java
Patch:
@@ -103,8 +103,8 @@
  * <p>
  * The number of iterations t during each fitness assessment is a knob that
  * adjusts the degree of exploitation in the algorithm. If t is very large,
- * then we�re doing more hill-climbing and thus more exploiting; whereas if
- * t is very small, then we�re spending more time in the outer algorithm and
+ * then we're doing more hill-climbing and thus more exploiting; whereas if
+ * t is very small, then we're spending more time in the outer algorithm and
  * thus doing more exploring.
  * 
  * @author Haifeng Li

File: Smile/src/smile/mds/package-info.java
Patch:
@@ -6,7 +6,7 @@
 /**
  * Multidimensional scaling. MDS is a set of related statistical techniques
  * often used in information visualization for exploring similarities or
- * dissimilarities in data. An MDS algorithm starts with a matrix of item�item
+ * dissimilarities in data. An MDS algorithm starts with a matrix of item-item
  * similarities, then assigns a location to each item in N-dimensional space.
  * For sufficiently small N, the resulting locations may be displayed in a
  * graph or 3D visualization.

File: Smile/src/smile/projection/GHA.java
Patch:
@@ -32,7 +32,7 @@
  *
  * <h2>References</h2>
  * <ol>
- * <li> Terence D. Sanger. Optimal unsupervised learning in a single-layer linear feedforward neural network. Neural Networks 2(6):459�473, 1989.</li>
+ * <li> Terence D. Sanger. Optimal unsupervised learning in a single-layer linear feedforward neural network. Neural Networks 2(6):459-473, 1989.</li>
  * <li> Simon Haykin. Neural Networks: A Comprehensive Foundation (2 ed.). 1998. </li>
  * </ol>
  * 

File: Smile/src/smile/projection/KPCA.java
Patch:
@@ -29,7 +29,7 @@
  *
  * <h2>References</h2>
  * <ol>
- * <li>Bernhard Sch�lkopf, Alexander Smola, and Klaus-Robert M�ller. Nonlinear Component Analysis as a Kernel Eigenvalue Problem. Neural Computation, 1998.</li>
+ * <li>Bernhard Scholkopf, Alexander Smola, and Klaus-Robert Muller. Nonlinear Component Analysis as a Kernel Eigenvalue Problem. Neural Computation, 1998.</li>
  * </ol>
  *
  * @see smile.math.kernel.MercerKernel

File: Smile/src/smile/regression/GaussianProcessRegression.java
Patch:
@@ -48,7 +48,7 @@
  * <h2>References</h2>
  * <ol>
  * <li> Carl Edward Rasmussen and Chris Williams. Gaussian Processes for Machine Learning, 2006.</li>
- * <li> Joaquin Qui�onero-candela,  Carl Edward Ramussen,  Christopher K. I. Williams. Approximation Methods for Gaussian Process Regression. 2007. </li>
+ * <li> Joaquin Quinonero-candela,  Carl Edward Ramussen,  Christopher K. I. Williams. Approximation Methods for Gaussian Process Regression. 2007. </li>
  * <li> T. Poggio and F. Girosi. Networks for approximation and learning. Proc. IEEE 78(9):1484-1487, 1990. </li>
  * <li> Kai Zhang and James T. Kwok. Clustered Nystrom Method for Large Scale Manifold Learning and Dimension Reduction. IEEE Transactions on Neural Networks, 2010. </li>
  * <li> </li>

File: Smile/src/smile/regression/RegressionTree.java
Patch:
@@ -148,7 +148,7 @@ public Trainer(Attribute[] attributes, int J) {
         /**
          * Constructor.
          * 
-         * @param attributes the attributes of independent variable.
+         * @param numFeatures the number of features.
          * @param J the maximum number of leaf nodes in the tree.
          */
         public Trainer(int numFeatures, int J) {
@@ -1022,8 +1022,6 @@ public RegressionTree(int numFeatures, int[][] x, double[] y, int J) {
      * @param numFeatures the number of sparse binary features.
      * @param x the training instances. 
      * @param y the response variable.
-     * @param order  the index of training values in ascending order. Note
-     * that only numeric attributes need be sorted.
      * @param J the maximum number of leaf nodes in the tree.
      * @param samples the sample set of instances for stochastic learning.
      * samples[i] should be 0 or 1 to indicate if the instance is used for training.

File: Smile/src/smile/wavelet/CoifletWavelet.java
Patch:
@@ -9,7 +9,7 @@
  * Coiflet wavelets. Coiflet wavelets have scaling functions with vanishing
  * moments. The wavelet is near symmetric, their wavelet functions have N / 3
  * vanishing moments and scaling functions N / 3 ? 1, and has been used in
- * many applications using Calder�n-Zygmund Operators.
+ * many applications using Calderon-Zygmund Operators.
  *
  * @author Haifeng Li
  */

File: SmileMath/src/smile/math/distance/DynamicTimeWarping.java
Patch:
@@ -8,7 +8,7 @@
 /**
  * Dynamic time warping is an algorithm for measuring similarity between two
  * sequences which may vary in time or speed. DTW has been applied to video,
- * audio, and graphics � indeed, any data which can be turned into a linear
+ * audio, and graphics - indeed, any data which can be turned into a linear
  * representation can be analyzed with DTW. A well known application has been
  * automatic speech recognition, to cope with different speaking speeds.
  * <p>

File: SmileMath/src/smile/math/distance/EditDistance.java
Patch:
@@ -13,7 +13,7 @@
  * strings is given by the minimum number of operations needed to transform one
  * string into the other, where an operation is an insertion, deletion, or
  * substitution of a single character. A generalization of the Levenshtein
- * distance (Damerau�Levenshtein distance) allows the transposition of two
+ * distance (Damerau-Levenshtein distance) allows the transposition of two
  * characters as an operation.
  * <p>
  * Given two strings x and y of length m and n (suppose n &ge; m), this
@@ -88,7 +88,7 @@ public EditDistance(int maxStringLength) {
     }
 
     /**
-     * Constructor. Damerau�Levenshtein distance.
+     * Constructor. Damerau-Levenshtein distance.
      * @param maxStringLength the maximum length of strings that will be
      * feed to this algorithm.
      * @param damerau if true, calculate Damerau-Levenshtein distance instead

File: SmileMath/src/smile/math/distance/LeeDistance.java
Patch:
@@ -8,7 +8,7 @@
 /**
  * In coding theory, the Lee distance is a distance between two strings
  * x<sub>1</sub>x<sub>2</sub>...x<sub>n</sub> and y<sub>1</sub>y<sub>2</sub>...y<sub>n</sub>
- * of equal length n over the q-ary alphabet {0,1,�,q-1} of size q &ge; 2, defined as
+ * of equal length n over the q-ary alphabet {0,1,...,q-1} of size q &ge; 2, defined as
  * <p>
  * sum min(|x<sub>i</sub>-y<sub>i</sub>|, q-|x<sub>i</sub>-y<sub>i</sub>|)
  * <p>

File: SmileMath/src/smile/math/matrix/EigenValueDecomposition.java
Patch:
@@ -35,11 +35,11 @@
  * Let A be a real n-by-n matrix with strictly positive entries a<sub>ij</sub>
  * > 0. Then the following statements hold.
  * <ol>
- * <li> There is a positive real number r, called the Perron�Frobenius
+ * <li> There is a positive real number r, called the Perron-Frobenius
  * eigenvalue, such that r is an eigenvalue of A and any other eigenvalue &lambda;
  * (possibly complex) is strictly smaller than r in absolute value,
  * |&lambda;| < r.
- * <li> The Perron�Frobenius eigenvalue is simple: r is a simple root of the
+ * <li> The Perron-Frobenius eigenvalue is simple: r is a simple root of the
  * characteristic polynomial of A. Consequently, both the right and the left
  * eigenspace associated to r is one-dimensional.
  * <li> There exists a left eigenvector v of A associated with r (row vector)
@@ -319,7 +319,6 @@ public static double[] pagerank(IMatrix A, double[] v) {
     /**
      * Calculate the page rank vector.
      * @param A the matrix supporting matrix vector multiplication operation.
-     * @param p on input, it is the non-zero initial guess of the page rank, usually 1/n.
      * @param v the teleportation vector.
      * @param damping the damper factor.
      * @param tol the desired convergence tolerance.

File: SmileMath/src/smile/math/matrix/SparseMatrix.java
Patch:
@@ -25,8 +25,6 @@
  * of pointers are indexes where each column starts. This format is efficient
  * for arithmetic operations, column slicing, and matrix-vector products.
  * One typically uses SparseDataset for construction of SparseMatrix.
- * 
- * @see smile.data.SparseDataset
  *
  * @author Haifeng Li
  */

File: SmileMath/src/smile/stat/distribution/GaussianDistribution.java
Patch:
@@ -23,7 +23,7 @@
  * then their linear combination will also be normally distributed.
  * The converse is also true: if X<sub>1</sub> and X<sub>2</sub> are independent and their sum X<sub>1</sub> + X<sub>2</sub>
  * is distributed normally, then both X<sub>1</sub> and X<sub>2</sub> must also be normal, which is
- * known as the Cram�r�s theorem. Of all probability distributions over the
+ * known as the Cramer's theorem. Of all probability distributions over the
  * reals with mean &mu; and variance &sigma;<sup>2</sup>, the normal
  * distribution N(&mu;, &sigma;<sup>2</sup>) is the one with the maximum entropy.
  * <p>
@@ -45,7 +45,7 @@
  * <li> The binomial distribution B(n, p) is approximately normal N(np, np(1-p)) for large n and for p not too close to zero or one.
  * <li> The Poisson(&lambda;) distribution is approximately normal N(&lambda;, &lambda;) for large values of &lambda;.
  * <li> The chi-squared distribution &Chi;<sup>2</sup>(k) is approximately normal N(k, 2k) for large k.
- * <li> The Student�s t-distribution t(&nu;) is approximately normal N(0, 1) when &nu; is large.
+ * <li> The Student's t-distribution t(&nu;) is approximately normal N(0, 1) when &nu; is large.
  * </ul>
  * 
  * @author Haifeng Li

