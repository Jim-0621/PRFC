File: frontend/server/src/test/java/org/pytorch/serve/device/utils/AppleUtilTest.java
Patch:
@@ -76,7 +76,7 @@ public void testExtractAcceleratorId() {
     public void testExtractAccelerators() {
         List<JsonObject> accelerators = appleUtil.extractAccelerators(sampleOutputJson);
 
-        assertEquals(accelerators.size(), 7);
+        assertEquals(accelerators.size(), 1);
         assertEquals(accelerators.get(0).get("sppci_model").getAsString(), "Apple M1");
     }
 
@@ -88,7 +88,7 @@ public void testSmiOutputToUpdatedAccelerators() {
         ArrayList<Accelerator> updatedAccelerators =
                 appleUtil.smiOutputToUpdatedAccelerators(sampleOutputJson.toString(), parsedGpuIds);
 
-        assertEquals(updatedAccelerators.size(), 7);
+        assertEquals(updatedAccelerators.size(), 1);
         Accelerator accelerator = updatedAccelerators.get(0);
         assertEquals(accelerator.getAcceleratorModel(), "Apple M1");
         assertEquals(accelerator.getVendor(), AcceleratorVendor.APPLE);
@@ -112,7 +112,7 @@ public String[] getUtilizationSmiCommand() {
         ArrayList<Accelerator> availableAccelerators =
                 spyAppleUtil.getAvailableAccelerators(availableAcceleratorIds);
 
-        assertEquals(availableAccelerators.size(), 7);
+        assertEquals(availableAccelerators.size(), 1);
         Accelerator accelerator = availableAccelerators.get(0);
         assertEquals(accelerator.getAcceleratorModel(), "Apple M1");
         assertEquals(accelerator.getVendor(), AcceleratorVendor.APPLE);

File: frontend/server/src/test/java/org/pytorch/serve/util/ConfigManagerTest.java
Patch:
@@ -118,7 +118,8 @@ public void testNumGpuM1() throws ReflectiveOperationException, IOException {
         String mac_arm64_cpu_only = System.getenv().getOrDefault("TS_MAC_ARM64_CPU_ONLY", "False");
         if (arch.equals("aarch64")) {
             if (mac_arm64_cpu_only.equals("True")) {
-                Assert.assertEquals(configManager.getNumberOfGpu(), 0);
+                // Mac M1 returns 1 accelerator device
+                Assert.assertEquals(configManager.getNumberOfGpu(), 1);
             } else {
                 Assert.assertTrue(configManager.getNumberOfGpu() > 0);
             }

File: frontend/server/src/main/java/org/pytorch/serve/util/ApiUtils.java
Patch:
@@ -374,7 +374,8 @@ public static String getWorkerStatus() {
         } else if ((numWorking == 0) && (numScaled > 0)) {
             response = "Unhealthy";
         }
-        // TODO: Check if its OK to send other 2xx errors to ALB for "Partial Healthy" and
+        // TODO: Check if its OK to send other 2xx errors to ALB for "Partial Healthy"
+        // and
         // "Unhealthy"
         return response;
     }

File: frontend/server/src/main/java/org/pytorch/serve/wlm/WorkerLifeCycle.java
Patch:
@@ -135,7 +135,9 @@ private void startWorkerPython(int port, String deviceIds)
                 attachRunner(argl, envp, port, deviceIds);
             } else {
                 if (deviceIds != null) {
-                    envp.add("CUDA_VISIBLE_DEVICES=" + deviceIds);
+                    String visibleDeviceEnvName =
+                            configManager.systemInfo.getVisibleDevicesEnvName();
+                    envp.add(visibleDeviceEnvName + "=" + deviceIds);
                 }
                 argl.add(EnvironmentUtils.getPythonRunTime(model));
             }

File: frontend/server/src/main/java/org/pytorch/serve/openapi/OpenApiUtils.java
Patch:
@@ -203,7 +203,7 @@ private static Operation getSetDefaultOperation() {
         MediaType error = getErrorResponse();
 
         operation.addResponse(
-                new Response("200", "Default vesion succsesfully updated for model", status));
+                new Response("200", "Default version successfully updated for model", status));
         operation.addResponse(
                 new Response("404", "Model not found or Model version not found", error));
         operation.addResponse(new Response("500", "Internal Server Error", error));

File: frontend/server/src/main/java/org/pytorch/serve/util/ApiUtils.java
Patch:
@@ -98,7 +98,7 @@ public static String setDefault(String modelName, String newModelVersion)
         ModelManager modelManager = ModelManager.getInstance();
         modelManager.setDefaultVersion(modelName, newModelVersion);
         String msg =
-                "Default vesion succsesfully updated for model \""
+                "Default version successfully updated for model \""
                         + modelName
                         + "\" to \""
                         + newModelVersion

File: frontend/server/src/test/java/org/pytorch/serve/ModelServerTest.java
Patch:
@@ -429,7 +429,7 @@ public void testSetDefaultVersionNoop() throws InterruptedException {
         StatusResponse resp = JsonUtils.GSON.fromJson(TestUtils.getResult(), StatusResponse.class);
         Assert.assertEquals(
                 resp.getStatus(),
-                "Default vesion succsesfully updated for model \"noopversioned\" to \"1.2.1\"");
+                "Default version successfully updated for model \"noopversioned\" to \"1.2.1\"");
     }
 
     @Test(

File: frontend/archive/src/test/java/org/pytorch/serve/archive/model/ModelConfigTest.java
Patch:
@@ -21,6 +21,7 @@ public void testValidYamlConfig() throws InvalidModelException, IOException {
         Assert.assertEquals(modelConfig.getBatchSize(), 1);
         Assert.assertEquals(modelConfig.getMaxBatchDelay(), 100);
         Assert.assertEquals(modelConfig.getResponseTimeout(), 120);
+        Assert.assertEquals(modelConfig.getStartupTimeout(), 120);
         Assert.assertEquals(modelConfig.getDeviceType(), ModelConfig.DeviceType.GPU);
         Assert.assertEquals(modelConfig.getParallelLevel(), 4);
         Assert.assertEquals(modelConfig.getParallelType(), ModelConfig.ParallelType.PP);
@@ -42,6 +43,7 @@ public void testInvalidYamlConfig() throws InvalidModelException, IOException {
         Assert.assertEquals(modelConfig.getBatchSize(), 1);
         Assert.assertEquals(modelConfig.getMaxBatchDelay(), 100);
         Assert.assertEquals(modelConfig.getResponseTimeout(), 120);
+        Assert.assertEquals(modelConfig.getStartupTimeout(), 120);
         Assert.assertNotEquals(modelConfig.getDeviceType(), ModelConfig.DeviceType.GPU);
         Assert.assertEquals(modelConfig.getParallelLevel(), 0);
         Assert.assertNotEquals(modelConfig.getParallelType(), ModelConfig.ParallelType.PPTP);

File: frontend/server/src/main/java/org/pytorch/serve/ModelServer.java
Patch:
@@ -270,6 +270,7 @@ private void initModelStore() throws InvalidSnapshotException, IOException {
                                 -1 * RegisterModelRequest.DEFAULT_BATCH_SIZE,
                                 -1 * RegisterModelRequest.DEFAULT_MAX_BATCH_DELAY,
                                 configManager.getDefaultResponseTimeout(),
+                                configManager.getDefaultStartupTimeout(),
                                 defaultModelName,
                                 false,
                                 false,

File: frontend/server/src/main/java/org/pytorch/serve/workflow/api/http/WorkflowMgmtRequestHandler.java
Patch:
@@ -153,6 +153,7 @@ private void handleRegisterWorkflows(
                                 registerWFRequest.getWorkflowName(),
                                 registerWFRequest.getWorkflowUrl(),
                                 registerWFRequest.getResponseTimeout(),
+                                registerWFRequest.getStartupTimeout(),
                                 true,
                                 registerWFRequest.getS3SseKms());
 

File: frontend/server/src/test/java/org/pytorch/serve/EnsembleTest.java
Patch:
@@ -169,6 +169,7 @@ public void testWorkflowYaml() throws Exception {
                                     "test.war",
                                     "file:///Users/demo/git/serve/frontend/server/src/test/resources/test.war",
                                     300,
+                                    300,
                                     true);
         } catch (Exception e) {
             System.out.println(e.getMessage());

File: frontend/server/src/main/java/org/pytorch/serve/util/ApiUtils.java
Patch:
@@ -414,6 +414,7 @@ private static DescribeModelResponse createModelResponse(
         resp.setUseVenv(model.isUseVenv());
         resp.setStateful(model.isSequenceBatching());
         resp.setSequenceMaxIdleMSec(model.getSequenceMaxIdleMSec());
+        resp.setSequenceTimeoutMSec(model.getSequenceTimeoutMSec());
         resp.setMaxNumSequence(model.getMaxNumSequence());
         resp.setMaxSequenceJobQueueSize(model.getMaxSequenceJobQueueSize());
 

File: frontend/archive/src/main/java/org/pytorch/serve/archive/model/ModelArchive.java
Patch:
@@ -54,13 +54,13 @@ public static ModelArchive downloadModel(
             throw new ModelNotFoundException("empty url");
         }
 
-        String marFileName = ArchiveUtils.getFilenameFromUrl(url);
-        File modelLocation = new File(modelStore, marFileName);
-
         if (url.contains("..")) {
             throw new ModelNotFoundException("Relative path is not allowed in url: " + url);
         }
 
+        String marFileName = ArchiveUtils.getFilenameFromUrl(url);
+        File modelLocation = new File(modelStore, marFileName);
+
         try {
             ArchiveUtils.downloadArchive(
                     allowedUrls, modelLocation, marFileName, url, s3SseKmsEnabled);

File: frontend/server/src/main/java/org/pytorch/serve/ServerInitializer.java
Patch:
@@ -19,7 +19,7 @@
 import org.pytorch.serve.servingsdk.impl.PluginsManager;
 import org.pytorch.serve.util.ConfigManager;
 import org.pytorch.serve.util.ConnectorType;
-import org.pytorch.serve.util.TokenType;
+import org.pytorch.serve.util.TokenAuthorization.TokenType;
 import org.pytorch.serve.workflow.api.http.WorkflowInferenceRequestHandler;
 import org.pytorch.serve.workflow.api.http.WorkflowMgmtRequestHandler;
 import org.slf4j.Logger;

File: frontend/server/src/main/java/org/pytorch/serve/util/ConfigManager.java
Patch:
@@ -498,7 +498,7 @@ public int getNumberOfGpu() {
         return getIntProperty(TS_NUMBER_OF_GPU, 0);
     }
 
-    public boolean getModelControlMode() {
+    public boolean isModelApiEnabled() {
         return Boolean.parseBoolean(getProperty(TS_ENABLE_MODEL_API, "false"));
     }
 
@@ -832,7 +832,7 @@ public String dumpConfigurations() {
                 + "\nSystem metrics command: "
                 + (getSystemMetricsCmd().isEmpty() ? "default" : getSystemMetricsCmd())
                 + "\nModel API enabled: "
-                + (getModelControlMode() ? "true" : "false");
+                + (isModelApiEnabled() ? "true" : "false");
     }
 
     public boolean useNativeIo() {

File: frontend/server/src/test/java/org/pytorch/serve/ModelServerTest.java
Patch:
@@ -46,6 +46,7 @@
 import org.pytorch.serve.util.ConfigManager;
 import org.pytorch.serve.util.ConnectorType;
 import org.pytorch.serve.util.JsonUtils;
+import org.pytorch.serve.util.TokenAuthorization;
 import org.pytorch.serve.wlm.Model;
 import org.testng.Assert;
 import org.testng.SkipException;
@@ -77,7 +78,9 @@ public void beforeSuite()
                     InvalidSnapshotException {
         ConfigManager.init(new ConfigManager.Arguments());
         configManager = ConfigManager.getInstance();
+        configManager.setProperty("disable_token_authorization", "true");
         configManager.setProperty("metrics_mode", "prometheus");
+        TokenAuthorization.init();
         PluginsManager.getInstance().initialize();
         MetricCache.init();
 

File: frontend/server/src/main/java/org/pytorch/serve/wlm/Model.java
Patch:
@@ -311,9 +311,10 @@ private boolean addJobInGroup(Job job) {
                     logger.info("added jobGroup for sequenceId:{}", job.getGroupId());
                 } else {
                     logger.warn(
-                            "Skip the requestId: {} for sequence: {} due to exceeding maxNumSequence: {}",
+                            "Skip the requestId: {} for sequence: {} due to jobGroups size: {} exceeding maxNumSequence: {}",
                             job.getJobId(),
                             job.getGroupId(),
+                            jobGroups.size(),
                             maxNumSequence);
                     return false;
                 }

File: frontend/server/src/main/java/org/pytorch/serve/wlm/BatchAggregator.java
Patch:
@@ -74,12 +74,15 @@ public boolean sendResponse(ModelWorkerResponse message) {
         if (message.getCode() == 200) {
             if (jobs.isEmpty()) {
                 // this is from initial load.
+                logger.info("Jobs is empty. This is from initial load....");
                 return true;
             }
             for (Predictions prediction : message.getPredictions()) {
                 String jobId = prediction.getRequestId();
                 Job job = jobs.get(jobId);
 
+                logger.info("Sending response for jobId {}", jobId);
+
                 if (job == null) {
                     throw new IllegalStateException(
                             "Unexpected job in sendResponse() with 200 status code: " + jobId);

File: frontend/server/src/main/java/org/pytorch/serve/ModelServer.java
Patch:
@@ -40,6 +40,7 @@
 import org.pytorch.serve.archive.model.ModelNotFoundException;
 import org.pytorch.serve.grpcimpl.GRPCInterceptor;
 import org.pytorch.serve.grpcimpl.GRPCServiceFactory;
+import org.pytorch.serve.http.TokenAuthorizationHandler;
 import org.pytorch.serve.http.messages.RegisterModelRequest;
 import org.pytorch.serve.metrics.MetricCache;
 import org.pytorch.serve.metrics.MetricManager;
@@ -86,7 +87,7 @@ public static void main(String[] args) {
             ConfigManager.Arguments arguments = new ConfigManager.Arguments(cmd);
             ConfigManager.init(arguments);
             ConfigManager configManager = ConfigManager.getInstance();
-            configManager.setupToken();
+            TokenAuthorizationHandler.setupToken();
             PluginsManager.getInstance().initialize();
             MetricCache.init();
             InternalLoggerFactory.setDefaultFactory(Slf4JLoggerFactory.INSTANCE);

File: frontend/server/src/main/java/org/pytorch/serve/ModelServer.java
Patch:
@@ -86,6 +86,7 @@ public static void main(String[] args) {
             ConfigManager.Arguments arguments = new ConfigManager.Arguments(cmd);
             ConfigManager.init(arguments);
             ConfigManager configManager = ConfigManager.getInstance();
+            configManager.setupToken();
             PluginsManager.getInstance().initialize();
             MetricCache.init();
             InternalLoggerFactory.setDefaultFactory(Slf4JLoggerFactory.INSTANCE);

File: frontend/archive/src/main/java/org/pytorch/serve/archive/model/ModelConfig.java
Patch:
@@ -60,7 +60,7 @@ public class ModelConfig {
      */
     private long sequenceMaxIdleMSec;
     /**
-     * the job queue size of an inference sequence of this stateful model. The default value is 1.
+     * the job queue size of one inference sequence of this stateful model. The default value is 1.
      */
     private int maxSequenceJobQueueSize = 1;
     /** the max number of sequences can be accepted. The default value is 1. */

File: frontend/server/src/main/java/org/pytorch/serve/grpcimpl/InferenceImpl.java
Patch:
@@ -229,7 +229,7 @@ private void prediction(
             if (workerCmd == WorkerCommands.STREAMPREDICT2) {
                 String sequenceId = request.getSequenceId();
                 if ("".equals(sequenceId)) {
-                    sequenceId = String.format("ts-%s", UUID.randomUUID());
+                    sequenceId = String.format("ts-seq-%s", UUID.randomUUID());
                     inputData.updateHeaders(
                             ConfigManager.getInstance().getTsHeaderKeySequenceStart(), "true");
                 }

File: frontend/server/src/main/java/org/pytorch/serve/wlm/ContinuousBatching.java
Patch:
@@ -19,6 +19,7 @@ public ContinuousBatching(Model model) {
         super(model);
     }
 
+    @Override
     public BaseModelRequest getRequest(String threadName, WorkerState state)
             throws InterruptedException, ExecutionException {
         int batchQuota = model.getBatchSize() - jobs.size();
@@ -60,6 +61,7 @@ public BaseModelRequest getRequest(String threadName, WorkerState state)
      * @return - true: either a non-stream response or last stream response is sent - false: a
      *     stream response (not include the last stream) is sent
      */
+    @Override
     public boolean sendResponse(ModelWorkerResponse message) {
         // TODO: Handle prediction level code
         if (message.getCode() == 200) {
@@ -98,7 +100,7 @@ public boolean sendResponse(ModelWorkerResponse message) {
                         prediction
                                 .getHeaders()
                                 .get(org.pytorch.serve.util.messages.RequestInput.TS_STREAM_NEXT);
-                if (streamNext != null && streamNext.equals("false")) {
+                if (streamNext == null || (streamNext != null && streamNext.equals("false"))) {
                     jobs.remove(jobId);
                 } else if (!job.isOpen()) {
                     jobs.remove(job.getJobId());

File: frontend/server/src/main/java/org/pytorch/serve/wlm/Model.java
Patch:
@@ -382,7 +382,8 @@ public void pollInferJob(
         }
 
         long begin = System.currentTimeMillis();
-        for (int i = 0; i < batchSize - 1; ++i) {
+        batchSize = pollNoWait ? batchSize : batchSize - 1;
+        for (int i = 0; i < batchSize; ++i) {
             if (pollNoWait) {
                 j = jobsQueue.poll();
             } else {

File: frontend/server/src/main/java/org/pytorch/serve/wlm/WorkerThread.java
Patch:
@@ -541,9 +541,8 @@ public void retry() {
         if (backoffIdx < BACK_OFF.length - 1) {
             ++backoffIdx;
         }
-        if (aggregator instanceof SequenceBatchAggregator) {
-            ((SequenceBatchAggregator) aggregator).startEventDispatcher();
-        }
+        aggregator.startEventDispatcher();
+
         manager.getScheduler()
                 .schedule(() -> manager.submitTask(this), BACK_OFF[backoffIdx], TimeUnit.SECONDS);
         logger.info("Retry worker: {} in {} seconds.", workerId, BACK_OFF[backoffIdx]);

File: frontend/server/src/main/java/org/pytorch/serve/wlm/WorkLoadManager.java
Patch:
@@ -229,7 +229,7 @@ private void addThreads(
 
             BatchAggregator aggregator;
 
-            if (model.isStateful()) {
+            if (model.isSequenceBatching()) {
                 aggregator = new SequenceBatchAggregator(model);
             } else if (model.isContinuousBatching()) {
                 aggregator = new ContinuousBatching(model);

File: frontend/server/src/main/java/org/pytorch/serve/util/ConfigManager.java
Patch:
@@ -875,7 +875,8 @@ private static int getAvailableGpu() {
                         }
                     }
                 }
-                throw new AssertionError("Unexpected response.");
+                // No MPS devices detected
+                return 0;
             } else {
                 Process process =
                         Runtime.getRuntime().exec("nvidia-smi --query-gpu=index --format=csv");

File: frontend/server/src/main/java/org/pytorch/serve/wlm/WorkerThread.java
Patch:
@@ -481,7 +481,6 @@ public void shutdown() {
             }
         }
         backendChannel.clear();
-        lifeCycle.terminateIOStreams();
         Thread thread = currentThread.getAndSet(null);
         if (thread != null) {
             thread.interrupt();

File: frontend/server/src/main/java/org/pytorch/serve/wlm/WorkerLifeCycle.java
Patch:
@@ -231,11 +231,9 @@ private void startWorkerCPP(int port, String runtimeType, String deviceIds)
         argl.add(runtimeType);
         argl.add("--model_dir");
         argl.add(modelPath.getAbsolutePath());
-        argl.add("--logger_config_path");
         if (ConfigManager.getInstance().getTsCppLogConfig() != null) {
+            argl.add("--logger_config_path");
             argl.add(ConfigManager.getInstance().getTsCppLogConfig());
-        } else {
-            argl.add(configManager.getModelServerHome() + "/ts/cpp/resources/logging.config");
         }
         argl.add("--metrics_config_path");
         argl.add(configManager.getMetricsConfigPath());

File: frontend/server/src/main/java/org/pytorch/serve/http/api/rest/ApiDescriptionRequestHandler.java
Patch:
@@ -30,7 +30,6 @@ public void handleRequest(
             String[] segments)
             throws ModelException, DownloadArchiveException, WorkflowException,
                     WorkerInitializationException {
-
         if (isApiDescription(segments)) {
             String path = decoder.path();
             if (("/".equals(path) && HttpMethod.OPTIONS.equals(req.method()))

File: frontend/server/src/main/java/org/pytorch/serve/workflow/api/http/WorkflowInferenceRequestHandler.java
Patch:
@@ -80,6 +80,7 @@ public void handleRequest(
             String[] segments)
             throws ModelException, DownloadArchiveException, WorkflowException,
                     WorkerInitializationException {
+
         if ("wfpredict".equalsIgnoreCase(segments[1])) {
             if (segments.length < 3) {
                 throw new ResourceNotFoundException();

File: frontend/server/src/main/java/org/pytorch/serve/workflow/api/http/WorkflowMgmtRequestHandler.java
Patch:
@@ -63,6 +63,7 @@ public void handleRequest(
             String[] segments)
             throws ModelException, DownloadArchiveException, WorkflowException,
                     WorkerInitializationException {
+
         if (isManagementReq(segments)) {
             if (!"workflows".equals(segments[1])) {
                 throw new ResourceNotFoundException();

File: frontend/server/src/test/java/org/pytorch/serve/ModelServerTest.java
Patch:
@@ -1169,7 +1169,7 @@ public void testModelWithInvalidCustomPythonDependency()
         Assert.assertEquals(TestUtils.getHttpStatus(), HttpResponseStatus.BAD_REQUEST);
         Assert.assertEquals(
                 resp.getMessage(),
-                "Custom pip package installation failed for custom_invalid_python_dep");
+                "Custom pip package installation failed for model custom_invalid_python_dep");
         TestUtils.setConfiguration(configManager, "install_py_dep_per_model", "false");
         channel.close().sync();
     }

File: frontend/server/src/test/java/org/pytorch/serve/WorkflowTest.java
Patch:
@@ -424,7 +424,7 @@ public void testWorkflowWithInvalidCustomPythonDependencyModel()
         Assert.assertEquals(TestUtils.getHttpStatus(), HttpResponseStatus.INTERNAL_SERVER_ERROR);
         Assert.assertEquals(
                 resp.getMessage(),
-                "Workflow custom_invalid_python_dep has failed to register. Failures: [Workflow Node custom_invalid_python_dep__custom_invalid_python_dep failed to register. Details: Custom pip package installation failed for custom_invalid_python_dep__custom_invalid_python_dep]");
+                "Workflow custom_invalid_python_dep has failed to register. Failures: [Workflow Node custom_invalid_python_dep__custom_invalid_python_dep failed to register. Details: Custom pip package installation failed for model custom_invalid_python_dep__custom_invalid_python_dep]");
         TestUtils.setConfiguration(configManager, "install_py_dep_per_model", "false");
         channel.close().sync();
     }

File: frontend/server/src/main/java/org/pytorch/serve/http/messages/RegisterModelRequest.java
Patch:
@@ -55,8 +55,7 @@ public RegisterModelRequest(QueryStringDecoder decoder) {
                         "initial_workers",
                         ConfigManager.getInstance().getConfiguredDefaultWorkersPerModel());
         synchronous = Boolean.parseBoolean(NettyUtils.getParameter(decoder, "synchronous", "true"));
-        responseTimeout =
-                NettyUtils.getIntParameter(decoder, "response_timeout", -1 * DEFAULT_BATCH_SIZE);
+        responseTimeout = NettyUtils.getIntParameter(decoder, "response_timeout", -1);
         modelUrl = NettyUtils.getParameter(decoder, "url", null);
         s3SseKms = Boolean.parseBoolean(NettyUtils.getParameter(decoder, "s3_sse_kms", "false"));
     }

File: frontend/server/src/main/java/org/pytorch/serve/wlm/ModelManager.java
Patch:
@@ -76,8 +76,8 @@ public ModelArchive registerModel(String url, String defaultModelName)
                 null,
                 null,
                 null,
-                1,
-                100,
+                -1 * RegisterModelRequest.DEFAULT_BATCH_SIZE,
+                -1 * RegisterModelRequest.DEFAULT_MAX_BATCH_DELAY,
                 configManager.getDefaultResponseTimeout(),
                 defaultModelName,
                 false,

File: frontend/archive/src/main/java/org/pytorch/serve/archive/model/Manifest.java
Patch:
@@ -137,7 +137,9 @@ public enum RuntimeType {
         @SerializedName("python")
         PYTHON("python"),
         @SerializedName("python3")
-        PYTHON3("python3");
+        PYTHON3("python3"),
+        @SerializedName("LSP")
+        LSP("LSP");
 
         String value;
 

File: frontend/server/src/main/java/org/pytorch/serve/ensemble/WorkflowManifest.java
Patch:
@@ -119,7 +119,9 @@ public enum RuntimeType {
         @SerializedName("python")
         PYTHON("python"),
         @SerializedName("python3")
-        PYTHON3("python3");
+        PYTHON3("python3"),
+        @SerializedName("LSP")
+        LSP("LSP");
 
         String value;
 

File: frontend/server/src/main/java/org/pytorch/serve/wlm/WorkerThread.java
Patch:
@@ -227,6 +227,9 @@ public void run() {
                     long begin = System.currentTimeMillis();
                     for (int i = 0; i < repeats; i++) {
                         reply = replies.poll(responseTimeout, TimeUnit.SECONDS);
+                        if (req.getCommand() != WorkerCommands.LOAD) {
+                            break;
+                        }
                     }
 
                     long duration = System.currentTimeMillis() - begin;

File: frontend/server/src/main/java/org/pytorch/serve/wlm/WorkerThread.java
Patch:
@@ -80,6 +80,7 @@ public class WorkerThread implements Runnable {
     private WorkerLifeCycle lifeCycle;
     private int responseTimeout;
     private long recoveryStartTS; // 0: default value. no recovery needed, in healthy mode
+    private BaseModelRequest req = null;
 
     public WorkerThread(
             ConfigManager configManager,
@@ -182,7 +183,7 @@ public void run() {
         Thread thread = Thread.currentThread();
         thread.setName(getWorkerName());
         currentThread.set(thread);
-        BaseModelRequest req = null;
+        req = null;
         int status = HttpURLConnection.HTTP_INTERNAL_ERROR;
 
         try {
@@ -202,12 +203,11 @@ public void run() {
                 List<CompletableFuture<Void>> futureRequests = new ArrayList<>(repeats);
                 for (int i = 0; backendChannel.size() > 0 && i < repeats; i++) {
                     int idx = i;
-                    BaseModelRequest request = req;
                     futureRequests.add(
                             CompletableFuture.runAsync(
                                     () -> {
                                         try {
-                                            backendChannel.get(idx).writeAndFlush(request).sync();
+                                            backendChannel.get(idx).writeAndFlush(req).sync();
                                         } catch (InterruptedException e) {
                                             logger.error("Failed to send request to backend", e);
                                         }

File: frontend/server/src/main/java/org/pytorch/serve/util/ConfigManager.java
Patch:
@@ -210,6 +210,8 @@ private ConfigManager(Arguments args) throws IOException {
         String workflowStore = args.getWorkflowStore();
         if (workflowStore != null) {
             prop.setProperty(TS_WORKFLOW_STORE, workflowStore);
+        } else if (prop.getProperty(TS_WORKFLOW_STORE) == null) {
+            prop.setProperty(TS_WORKFLOW_STORE, prop.getProperty(TS_MODEL_STORE));
         }
 
         String[] models = args.getModels();

File: frontend/server/src/main/java/org/pytorch/serve/util/messages/WorkerCommands.java
Patch:
@@ -14,7 +14,9 @@ public enum WorkerCommands {
     @SerializedName("describe")
     DESCRIBE("describe"),
     @SerializedName("streampredict")
-    STREAMPREDICT("streampredict");
+    STREAMPREDICT("streampredict"),
+    @SerializedName("streampredict2")
+    STREAMPREDICT2("streampredict2");
 
     private String command;
 

File: frontend/server/src/main/java/org/pytorch/serve/util/codec/ModelRequestEncoder.java
Patch:
@@ -76,7 +76,7 @@ private void encodeRequest(RequestInput req, ByteBuf out) {
         out.writeInt(buf.length);
         out.writeBytes(buf);
 
-        if (req.isCached()) {
+        if (req.isCachedInBackend()) {
             out.writeInt(-1); // End of List
             out.writeInt(-1); // End of List
             return;
@@ -92,7 +92,6 @@ private void encodeRequest(RequestInput req, ByteBuf out) {
             encodeParameter(input, out);
         }
         out.writeInt(-1); // End of List
-        req.setCached(true);
     }
 
     private void encodeParameter(InputParameter parameter, ByteBuf out) {

File: frontend/server/src/main/java/org/pytorch/serve/util/messages/RequestInput.java
Patch:
@@ -73,11 +73,11 @@ public void setClientExpireTS(long clientTimeoutInMills) {
         }
     }
 
-    public boolean isCached() {
+    public boolean isCachedInBackend() {
         return cached;
     }
 
-    public void setCached(boolean cached) {
+    public void setCachedInBackend(boolean cached) {
         this.cached = cached;
     }
 }

File: frontend/server/src/main/java/org/pytorch/serve/wlm/WorkerThread.java
Patch:
@@ -313,6 +313,7 @@ public void run() {
                     i++) {
                 backendChannel.get(i).disconnect();
             }
+            backendChannel.clear();
             currentThread.set(null);
             Integer exitValue = lifeCycle.getExitValue();
 
@@ -462,6 +463,7 @@ public void shutdown() {
                 backendChannel.get(i).close();
             }
         }
+        backendChannel.clear();
         lifeCycle.terminateIOStreams();
         Thread thread = currentThread.getAndSet(null);
         if (thread != null) {

File: frontend/server/src/main/java/org/pytorch/serve/util/codec/ModelResponseDecoder.java
Patch:
@@ -3,6 +3,7 @@
 import io.netty.buffer.ByteBuf;
 import io.netty.channel.ChannelHandlerContext;
 import io.netty.handler.codec.ByteToMessageDecoder;
+import io.netty.handler.codec.http.multipart.HttpPostRequestDecoder.NotEnoughDataDecoderException;
 import java.util.ArrayList;
 import java.util.List;
 import org.pytorch.serve.util.messages.ModelWorkerResponse;
@@ -82,6 +83,7 @@ protected void decode(ChannelHandlerContext ctx, ByteBuf in, List<Object> out) {
             resp.setPredictions(predictions);
             out.add(resp);
             completed = true;
+        } catch (NotEnoughDataDecoderException e) {
         } finally {
             if (!completed) {
                 in.resetReaderIndex();

File: frontend/server/src/main/java/org/pytorch/serve/wlm/WorkerThread.java
Patch:
@@ -182,6 +182,8 @@ public void run() {
         currentThread.set(thread);
         BaseModelRequest req = null;
         int status = HttpURLConnection.HTTP_INTERNAL_ERROR;
+        // in case of retry
+        aggregator.cleanJobs();
 
         try {
             connect();
@@ -205,8 +207,6 @@ public void run() {
                     backendChannel.get(i).writeAndFlush(req).sync();
                 }
 
-                boolean isStreaming =
-                        req.getCommand() == WorkerCommands.STREAMPREDICT ? true : false;
                 ModelWorkerResponse reply = null;
 
                 boolean jobDone = false;

File: frontend/server/src/test/java/org/pytorch/serve/TestUtils.java
Patch:
@@ -77,7 +77,8 @@ public static void init() {
 
             HttpsURLConnection.setDefaultSSLSocketFactory(context.getSocketFactory());
 
-            HttpsURLConnection.setDefaultHostnameVerifier((s, sslSession) -> true);
+            HttpsURLConnection.setDefaultHostnameVerifier(
+                    HttpsURLConnection.getDefaultHostnameVerifier());
         } catch (GeneralSecurityException e) {
             // ignore
         }

File: frontend/server/src/main/java/org/pytorch/serve/servingsdk/impl/ModelServerRequest.java
Patch:
@@ -8,6 +8,7 @@
 import java.util.List;
 import java.util.Map;
 import org.pytorch.serve.servingsdk.http.Request;
+import org.pytorch.serve.util.NettyUtils;
 
 public class ModelServerRequest implements Request {
     private FullHttpRequest req;
@@ -45,6 +46,6 @@ public String getContentType() {
 
     @Override
     public ByteArrayInputStream getInputStream() {
-        return new ByteArrayInputStream(req.content().array());
+        return new ByteArrayInputStream(NettyUtils.getBytes(req.content()));
     }
 }

File: frontend/archive/src/main/java/org/pytorch/serve/archive/model/ModelConfig.java
Patch:
@@ -40,8 +40,8 @@ public class ModelConfig {
     /** the maximum seconds of a worker recovery's timeout. default: 5 min */
     private int maxRetryTimeoutInSec = 300;
     /**
-     * the client timeout in millions second. The inference request will be dropped once it is
-     * timeout. default: 0 which means no timeout (ie. clientExpireTS default value Long.MAX_VALUE.
+     * the client timeout in milliseconds. The inference request will be dropped once it is timeout.
+     * default: 0 which means no timeout (ie. clientExpireTS default value Long.MAX_VALUE.
      */
     private long clientTimeoutInMills;
     /**

File: frontend/archive/src/main/java/org/pytorch/serve/archive/model/ModelArchive.java
Patch:
@@ -8,7 +8,6 @@
 import java.util.List;
 import java.util.Map;
 import org.apache.commons.io.FileUtils;
-import org.apache.commons.io.FilenameUtils;
 import org.pytorch.serve.archive.DownloadArchiveException;
 import org.pytorch.serve.archive.utils.ArchiveUtils;
 import org.pytorch.serve.archive.utils.InvalidArchiveURLException;
@@ -55,7 +54,7 @@ public static ModelArchive downloadModel(
             throw new ModelNotFoundException("empty url");
         }
 
-        String marFileName = FilenameUtils.getName(url);
+        String marFileName = ArchiveUtils.getFilenameFromUrl(url);
         File modelLocation = new File(modelStore, marFileName);
         try {
             ArchiveUtils.downloadArchive(
@@ -165,7 +164,7 @@ public void validate() throws InvalidModelException {
 
     public static void removeModel(String modelStore, String marURL) {
         if (ArchiveUtils.isValidURL(marURL)) {
-            String marFileName = FilenameUtils.getName(marURL);
+            String marFileName = ArchiveUtils.getFilenameFromUrl(marURL);
             File modelLocation = new File(modelStore, marFileName);
             FileUtils.deleteQuietly(modelLocation);
         }

File: frontend/archive/src/main/java/org/pytorch/serve/archive/workflow/WorkflowArchive.java
Patch:
@@ -13,7 +13,6 @@
 import java.nio.file.Files;
 import java.util.List;
 import org.apache.commons.io.FileUtils;
-import org.apache.commons.io.FilenameUtils;
 import org.pytorch.serve.archive.DownloadArchiveException;
 import org.pytorch.serve.archive.utils.ArchiveUtils;
 import org.pytorch.serve.archive.utils.InvalidArchiveURLException;
@@ -53,7 +52,7 @@ public static WorkflowArchive downloadWorkflow(
             throw new WorkflowNotFoundException("Workflow store has not been configured.");
         }
 
-        String warFileName = FilenameUtils.getName(url);
+        String warFileName = ArchiveUtils.getFilenameFromUrl(url);
         File workflowLocation = new File(workflowStore, warFileName);
 
         try {
@@ -144,7 +143,7 @@ public void validate() throws InvalidWorkflowException {
 
     public static void removeWorkflow(String workflowStore, String warURL) {
         if (ArchiveUtils.isValidURL(warURL)) {
-            String warFileName = FilenameUtils.getName(warURL);
+            String warFileName = ArchiveUtils.getFilenameFromUrl(warURL);
             File workflowLocation = new File(workflowStore, warFileName);
             FileUtils.deleteQuietly(workflowLocation);
         }

File: frontend/server/src/main/java/org/pytorch/serve/util/ApiUtils.java
Patch:
@@ -12,13 +12,13 @@
 import java.util.concurrent.CompletableFuture;
 import java.util.concurrent.ExecutionException;
 import java.util.function.Function;
-import org.apache.commons.io.FilenameUtils;
 import org.pytorch.serve.archive.DownloadArchiveException;
 import org.pytorch.serve.archive.model.Manifest;
 import org.pytorch.serve.archive.model.ModelArchive;
 import org.pytorch.serve.archive.model.ModelException;
 import org.pytorch.serve.archive.model.ModelNotFoundException;
 import org.pytorch.serve.archive.model.ModelVersionNotFoundException;
+import org.pytorch.serve.archive.utils.ArchiveUtils;
 import org.pytorch.serve.http.BadRequestException;
 import org.pytorch.serve.http.InternalServerException;
 import org.pytorch.serve.http.InvalidModelVersionException;
@@ -183,7 +183,7 @@ public static StatusResponse handleRegister(
                             s3SseKms);
         } catch (FileAlreadyExistsException e) {
             throw new InternalServerException(
-                    "Model file already exists " + FilenameUtils.getName(modelUrl), e);
+                    "Model file already exists " + ArchiveUtils.getFilenameFromUrl(modelUrl), e);
         } catch (IOException | InterruptedException e) {
             throw new InternalServerException("Failed to save model: " + modelUrl, e);
         }

File: frontend/server/src/main/java/org/pytorch/serve/wlm/Model.java
Patch:
@@ -12,9 +12,9 @@
 import java.util.concurrent.TimeUnit;
 import java.util.concurrent.atomic.AtomicInteger;
 import java.util.concurrent.locks.ReentrantLock;
-import org.apache.commons.io.FilenameUtils;
 import org.pytorch.serve.archive.model.ModelArchive;
 import org.pytorch.serve.archive.model.ModelConfig;
+import org.pytorch.serve.archive.utils.ArchiveUtils;
 import org.pytorch.serve.job.Job;
 import org.pytorch.serve.util.ConfigManager;
 import org.pytorch.serve.util.messages.WorkerCommands;
@@ -130,7 +130,7 @@ public JsonObject getModelState(boolean isDefaultVersion) {
 
         JsonObject modelInfo = new JsonObject();
         modelInfo.addProperty(DEFAULT_VERSION, isDefaultVersion);
-        modelInfo.addProperty(MAR_NAME, FilenameUtils.getName(getModelUrl()));
+        modelInfo.addProperty(MAR_NAME, ArchiveUtils.getFilenameFromUrl(getModelUrl()));
         modelInfo.addProperty(MIN_WORKERS, getMinWorkers());
         modelInfo.addProperty(MAX_WORKERS, getMaxWorkers());
         modelInfo.addProperty(BATCH_SIZE, getBatchSize());

File: frontend/server/src/main/java/org/pytorch/serve/wlm/Model.java
Patch:
@@ -77,7 +77,7 @@ public Model(ModelArchive modelArchive, int queueSize) {
                         (modelArchive.getModelConfig().getDeviceType() == ModelConfig.DeviceType.GPU
                                         && ConfigManager.getInstance().getNumberOfGpu() > 0)
                                 ? ModelConfig.DeviceType.GPU
-                                : deviceType;
+                                : ModelConfig.DeviceType.CPU;
             }
 
             deviceIds = modelArchive.getModelConfig().getDeviceIds();

File: frontend/server/src/main/java/org/pytorch/serve/util/ConfigManager.java
Patch:
@@ -188,9 +188,6 @@ private ConfigManager(Arguments args) throws IOException {
             }
         }
 
-        // This is a workaround to avoid DeepSpeed JIT issue during pip installation
-        System.setProperty("DS_BUILD_OPS", "1");
-
         resolveEnvVarVals(prop);
 
         String modelStore = args.getModelStore();

File: frontend/server/src/main/java/org/pytorch/serve/wlm/WorkerLifeCycle.java
Patch:
@@ -191,6 +191,7 @@ private void attachRunner(
             envp.add("CUDA_VISIBLE_DEVICES=" + deviceIds);
         }
         ModelConfig.TorchRun torchRun = model.getModelArchive().getModelConfig().getTorchRun();
+        envp.add(String.format("OMP_NUM_THREADS=%d", torchRun.getOmpNumberThreads()));
         argl.add("torchrun");
         argl.add("--nnodes");
         argl.add(String.valueOf(torchRun.getNnodes()));
@@ -216,6 +217,8 @@ private void attachRunner(
             argl.add("--master-port");
             argl.add(String.valueOf(torchRun.getMasterPort()));
         }
+        argl.add("--max-restarts");
+        argl.add(String.valueOf(1));
     }
 
     public synchronized void terminateIOStreams() {

File: frontend/server/src/test/java/org/pytorch/serve/TestUtils.java
Patch:
@@ -50,9 +50,9 @@ public final class TestUtils {
     private static Channel metricsChannel;
     private static String tsInferLatencyPattern =
             "ts_inference_latency_microseconds\\{"
-                    + "ModelName=\"%s\","
-                    + "ModelVersion=\"%s\","
-                    + "Hostname=\".+\",\\}\\s\\d+(\\.\\d+)";
+                    + "model_name=\"%s\","
+                    + "model_version=\"%s\","
+                    + "hostname=\".+\",\\}\\s\\d+(\\.\\d+)";
 
     private TestUtils() {}
 

File: frontend/server/src/main/java/org/pytorch/serve/ModelServer.java
Patch:
@@ -38,6 +38,7 @@
 import org.pytorch.serve.archive.model.ModelNotFoundException;
 import org.pytorch.serve.grpcimpl.GRPCInterceptor;
 import org.pytorch.serve.grpcimpl.GRPCServiceFactory;
+import org.pytorch.serve.metrics.MetricCache;
 import org.pytorch.serve.metrics.MetricManager;
 import org.pytorch.serve.servingsdk.ModelServerEndpoint;
 import org.pytorch.serve.servingsdk.annotations.Endpoint;
@@ -82,6 +83,7 @@ public static void main(String[] args) {
             ConfigManager.init(arguments);
             ConfigManager configManager = ConfigManager.getInstance();
             PluginsManager.getInstance().initialize();
+            MetricCache.init();
             InternalLoggerFactory.setDefaultFactory(Slf4JLoggerFactory.INSTANCE);
             ModelServer modelServer = new ModelServer(configManager);
 

File: frontend/server/src/test/java/org/pytorch/serve/ModelServerTest.java
Patch:
@@ -39,6 +39,7 @@
 import org.pytorch.serve.http.messages.ListModelsResponse;
 import org.pytorch.serve.metrics.Dimension;
 import org.pytorch.serve.metrics.Metric;
+import org.pytorch.serve.metrics.MetricCache;
 import org.pytorch.serve.metrics.MetricManager;
 import org.pytorch.serve.servingsdk.impl.PluginsManager;
 import org.pytorch.serve.snapshot.InvalidSnapshotException;
@@ -76,7 +77,9 @@ public void beforeSuite()
                     InvalidSnapshotException {
         ConfigManager.init(new ConfigManager.Arguments());
         configManager = ConfigManager.getInstance();
+        configManager.setProperty("metrics_mode", "prometheus");
         PluginsManager.getInstance().initialize();
+        MetricCache.init();
 
         InternalLoggerFactory.setDefaultFactory(Slf4JLoggerFactory.INSTANCE);
 

File: frontend/server/src/test/java/org/pytorch/serve/TestUtils.java
Patch:
@@ -50,9 +50,9 @@ public final class TestUtils {
     private static Channel metricsChannel;
     private static String tsInferLatencyPattern =
             "ts_inference_latency_microseconds\\{"
-                    + "uuid=\"[\\w]{8}(-[\\w]{4}){3}-[\\w]{12}\","
-                    + "model_name=\"%s\","
-                    + "model_version=\"%s\",\\}\\s\\d+(\\.\\d+)";
+                    + "ModelName=\"%s\","
+                    + "ModelVersion=\"%s\","
+                    + "Hostname=\".+\",\\}\\s\\d+(\\.\\d+)";
 
     private TestUtils() {}
 

File: frontend/server/src/test/java/org/pytorch/serve/WorkflowTest.java
Patch:
@@ -19,6 +19,7 @@
 import org.apache.commons.io.FileUtils;
 import org.pytorch.serve.http.ErrorResponse;
 import org.pytorch.serve.http.StatusResponse;
+import org.pytorch.serve.metrics.MetricCache;
 import org.pytorch.serve.servingsdk.impl.PluginsManager;
 import org.pytorch.serve.snapshot.InvalidSnapshotException;
 import org.pytorch.serve.util.ConfigManager;
@@ -51,6 +52,7 @@ public void beforeClass()
         ConfigManager.init(new ConfigManager.Arguments());
         configManager = ConfigManager.getInstance();
         PluginsManager.getInstance().initialize();
+        MetricCache.init();
 
         InternalLoggerFactory.setDefaultFactory(Slf4JLoggerFactory.INSTANCE);
         configManager.setInitialWorkerPort(10000);

File: frontend/server/src/main/java/org/pytorch/serve/job/RestJob.java
Patch:
@@ -147,9 +147,6 @@ private void responseInference(
                 resp.headers().set(e.getKey(), e.getValue());
             }
         }
-        if (resp instanceof DefaultFullHttpResponse) {
-            ((DefaultFullHttpResponse) resp).content().writeBytes(body);
-        }
 
         /*
          * We can load the models based on the configuration file.Since this Job is
@@ -161,6 +158,7 @@ private void responseInference(
             if (numStreams == 0) { // non-stream response
                 MetricAggregator.handleInferenceMetric(
                         getModelName(), getModelVersion(), getScheduled() - getBegin(), inferTime);
+                ((DefaultFullHttpResponse) resp).content().writeBytes(body);
                 NettyUtils.sendHttpResponse(ctx, resp, true);
             } else if (numStreams == -1) { // the last response in a stream
                 MetricAggregator.handleInferenceMetric(
@@ -169,6 +167,7 @@ private void responseInference(
                 ctx.writeAndFlush(LastHttpContent.EMPTY_LAST_CONTENT);
             } else if (numStreams == 1) { // the first response in a stream
                 NettyUtils.sendHttpResponse(ctx, resp, true);
+                ctx.writeAndFlush(new DefaultHttpContent(Unpooled.wrappedBuffer(body)));
             } else if (numStreams > 1) { // the 2nd+ response in a stream
                 ctx.writeAndFlush(new DefaultHttpContent(Unpooled.wrappedBuffer(body)));
             }

File: frontend/server/src/main/java/org/pytorch/serve/util/ConfigManager.java
Patch:
@@ -189,6 +189,9 @@ private ConfigManager(Arguments args) throws IOException {
             }
         }
 
+        // This is a workaround to avoid DeepSpeed JIT issue during pip installation
+        System.setProperty("DS_BUILD_OPS", "1");
+
         resolveEnvVarVals(prop);
 
         String modelStore = args.getModelStore();

File: frontend/server/src/main/java/org/pytorch/serve/job/GRPCJob.java
Patch:
@@ -109,7 +109,8 @@ public void response(
     @Override
     public void sendError(int status, String error) {
         Status responseStatus = GRPCUtils.getGRPCStatusCode(status);
-        if (this.getCmd() == WorkerCommands.PREDICT) {
+        if (this.getCmd() == WorkerCommands.PREDICT
+                || this.getCmd() == WorkerCommands.STREAMPREDICT) {
             predictionResponseObserver.onError(
                     responseStatus
                             .withDescription(error)

File: frontend/archive/src/main/java/org/pytorch/serve/archive/model/s3/BinaryUtils.java
Patch:
@@ -36,7 +36,7 @@ public static String toHex(byte[] data) {
      */
     public static byte[] fromHex(String hexData) {
         byte[] result = new byte[(hexData.length() + 1) / 2];
-        String hexNumber = null;
+        String hexNumber;
         int stringOffset = 0;
         int byteOffset = 0;
         while (stringOffset < hexData.length()) {

File: frontend/archive/src/main/java/org/pytorch/serve/archive/model/s3/HttpUtils.java
Patch:
@@ -22,7 +22,7 @@ private HttpUtils() {}
     public static void copyURLToFile(URL endpointUrl, File modelLocation, boolean s3SseKmsEnabled)
             throws IOException {
         // for a simple GET, we have no body so supply the precomputed 'empty' hash
-        Map<String, String> headers = null;
+        Map<String, String> headers;
         if (s3SseKmsEnabled) {
             String awsAccessKey = System.getenv("AWS_ACCESS_KEY_ID");
             String awsSecretKey = System.getenv("AWS_SECRET_ACCESS_KEY");

File: frontend/archive/src/main/java/org/pytorch/serve/archive/workflow/WorkflowArchive.java
Patch:
@@ -86,7 +86,7 @@ private static WorkflowArchive load(String url, File dir, boolean extracted)
         boolean failed = true;
         try {
             File manifestFile = new File(dir, "WAR-INF/" + MANIFEST_FILE);
-            Manifest manifest = null;
+            Manifest manifest;
             if (manifestFile.exists()) {
                 manifest = readFile(manifestFile, Manifest.class);
             } else {

File: frontend/server/src/main/java/org/pytorch/serve/http/HttpRequestHandlerChain.java
Patch:
@@ -20,6 +20,7 @@
 import org.pytorch.serve.servingsdk.impl.ModelServerResponse;
 import org.pytorch.serve.util.NettyUtils;
 import org.pytorch.serve.wlm.ModelManager;
+import org.pytorch.serve.wlm.WorkerInitializationException;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -45,7 +46,7 @@ public abstract void handleRequest(
             QueryStringDecoder decoder,
             String[] segments)
             throws ModelNotFoundException, ModelException, DownloadArchiveException,
-                    WorkflowException;
+                    WorkflowException, WorkerInitializationException;
 
     private void run(
             ModelServerEndpoint endpoint,

File: frontend/server/src/main/java/org/pytorch/serve/http/api/rest/ApiDescriptionRequestHandler.java
Patch:
@@ -12,6 +12,7 @@
 import org.pytorch.serve.openapi.OpenApiUtils;
 import org.pytorch.serve.util.ConnectorType;
 import org.pytorch.serve.util.NettyUtils;
+import org.pytorch.serve.wlm.WorkerInitializationException;
 
 public class ApiDescriptionRequestHandler extends HttpRequestHandlerChain {
 
@@ -27,7 +28,8 @@ public void handleRequest(
             FullHttpRequest req,
             QueryStringDecoder decoder,
             String[] segments)
-            throws ModelException, DownloadArchiveException, WorkflowException {
+            throws ModelException, DownloadArchiveException, WorkflowException,
+                    WorkerInitializationException {
 
         if (isApiDescription(segments)) {
             String path = decoder.path();

File: frontend/server/src/main/java/org/pytorch/serve/http/api/rest/InferenceRequestHandler.java
Patch:
@@ -31,6 +31,7 @@
 import org.pytorch.serve.util.messages.RequestInput;
 import org.pytorch.serve.wlm.Model;
 import org.pytorch.serve.wlm.ModelManager;
+import org.pytorch.serve.wlm.WorkerInitializationException;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -54,7 +55,8 @@ public void handleRequest(
             FullHttpRequest req,
             QueryStringDecoder decoder,
             String[] segments)
-            throws ModelException, DownloadArchiveException, WorkflowException {
+            throws ModelException, DownloadArchiveException, WorkflowException,
+                    WorkerInitializationException {
         if (isInferenceReq(segments)) {
             if (endpointMap.getOrDefault(segments[1], null) != null) {
                 handleCustomEndpoint(ctx, req, segments, decoder);

File: frontend/server/src/main/java/org/pytorch/serve/http/api/rest/PrometheusMetricsRequestHandler.java
Patch:
@@ -25,6 +25,7 @@
 import org.pytorch.serve.archive.workflow.WorkflowException;
 import org.pytorch.serve.http.HttpRequestHandlerChain;
 import org.pytorch.serve.util.NettyUtils;
+import org.pytorch.serve.wlm.WorkerInitializationException;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -44,7 +45,8 @@ public void handleRequest(
             FullHttpRequest req,
             QueryStringDecoder decoder,
             String[] segments)
-            throws ModelException, DownloadArchiveException, WorkflowException {
+            throws ModelException, DownloadArchiveException, WorkflowException,
+                    WorkerInitializationException {
         if (segments.length >= 2 && "metrics".equals(segments[1])) {
             ByteBuf resBuf = Unpooled.directBuffer();
             List<String> params =

File: frontend/server/src/main/java/org/pytorch/serve/wlm/WorkerLifeCycle.java
Patch:
@@ -124,6 +124,9 @@ public void startWorker(int port) throws WorkerInitializationException, Interrup
         argl.add(connector.isUds() ? "--sock-name" : "--port");
         argl.add(connector.getSocketPath());
 
+        argl.add("--metrics-config");
+        argl.add(configManager.getMetricsConfigPath());
+
         String[] envp =
                 EnvironmentUtils.getEnvString(
                         workingDir.getAbsolutePath(),

File: frontend/server/src/main/java/org/pytorch/serve/util/ApiUtils.java
Patch:
@@ -35,6 +35,7 @@
 import org.pytorch.serve.wlm.Model;
 import org.pytorch.serve.wlm.ModelManager;
 import org.pytorch.serve.wlm.ModelVersionedRefs;
+import org.pytorch.serve.wlm.WorkerState;
 import org.pytorch.serve.wlm.WorkerThread;
 
 public final class ApiUtils {
@@ -376,7 +377,8 @@ private static DescribeModelResponse createModelResponse(
         for (WorkerThread worker : workers) {
             String workerId = worker.getWorkerId();
             long startTime = worker.getStartTime();
-            boolean isRunning = worker.isRunning();
+            boolean isRunning =
+                    worker.isRunning() && worker.getState() == WorkerState.WORKER_MODEL_LOADED;
             int gpuId = worker.getGpuId();
             long memory = worker.getMemory();
             int pid = worker.getPid();

File: frontend/server/src/main/java/org/pytorch/serve/wlm/ModelManager.java
Patch:
@@ -125,7 +125,7 @@ public ModelArchive registerModel(
             manifest.getModel().setModelVersion("1.0");
             manifest.getModel().setModelName(modelName);
             manifest.getModel().setHandler(new File(handler).getName());
-
+            manifest.getModel().setEnvelope(configManager.getTsServiceEnvelope());
             File f = new File(handler.substring(0, handler.lastIndexOf(':')));
             archive = new ModelArchive(manifest, url, f.getParentFile(), true);
         } else {

File: frontend/server/src/main/java/org/pytorch/serve/wlm/ModelManager.java
Patch:
@@ -125,7 +125,7 @@ public ModelArchive registerModel(
             manifest.getModel().setModelVersion("1.0");
             manifest.getModel().setModelName(modelName);
             manifest.getModel().setHandler(new File(handler).getName());
-
+            manifest.getModel().setEnvelope(configManager.getTsServiceEnvelope());
             File f = new File(handler.substring(0, handler.lastIndexOf(':')));
             archive = new ModelArchive(manifest, url, f.getParentFile(), true);
         } else {

File: frontend/server/src/main/java/org/pytorch/serve/metrics/Metric.java
Patch:
@@ -4,6 +4,7 @@
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.List;
+import java.util.concurrent.TimeUnit;
 import java.util.regex.Matcher;
 import java.util.regex.Pattern;
 
@@ -47,6 +48,8 @@ public Metric(
         this.unit = unit;
         this.hostName = hostName;
         this.dimensions = Arrays.asList(dimensions);
+        this.timestamp =
+                String.valueOf(TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()));
     }
 
     public String getHostName() {

File: frontend/server/src/main/java/org/pytorch/serve/ensemble/DagExecutor.java
Patch:
@@ -1,5 +1,6 @@
 package org.pytorch.serve.ensemble;
 
+import com.google.common.util.concurrent.ThreadFactoryBuilder;
 import java.util.ArrayList;
 import java.util.HashSet;
 import java.util.List;
@@ -14,10 +15,9 @@
 import java.util.concurrent.ExecutorService;
 import java.util.concurrent.Executors;
 import java.util.concurrent.Future;
+import java.util.concurrent.ThreadFactory;
 import java.util.concurrent.TimeUnit;
 import java.util.concurrent.TimeoutException;
-import java.util.concurrent.ThreadFactory;
-import com.google.common.util.concurrent.ThreadFactoryBuilder;
 import org.pytorch.serve.archive.model.ModelNotFoundException;
 import org.pytorch.serve.archive.model.ModelVersionNotFoundException;
 import org.pytorch.serve.http.InternalServerException;

File: frontend/server/src/main/java/org/pytorch/serve/metrics/Metric.java
Patch:
@@ -4,6 +4,7 @@
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.List;
+import java.util.concurrent.TimeUnit;
 import java.util.regex.Matcher;
 import java.util.regex.Pattern;
 
@@ -47,6 +48,8 @@ public Metric(
         this.unit = unit;
         this.hostName = hostName;
         this.dimensions = Arrays.asList(dimensions);
+        this.timestamp =
+                String.valueOf(TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()));
     }
 
     public String getHostName() {

File: frontend/server/src/main/java/org/pytorch/serve/workflow/api/http/WorkflowInferenceRequestHandler.java
Patch:
@@ -13,7 +13,6 @@
 import org.pytorch.serve.archive.DownloadArchiveException;
 import org.pytorch.serve.archive.model.ModelException;
 import org.pytorch.serve.archive.workflow.WorkflowException;
-import org.pytorch.serve.archive.workflow.WorkflowNotFoundException;
 import org.pytorch.serve.http.BadRequestException;
 import org.pytorch.serve.http.HttpRequestHandlerChain;
 import org.pytorch.serve.http.ResourceNotFoundException;
@@ -58,7 +57,7 @@ public void handleRequest(
 
     private void handlePredictions(
             ChannelHandlerContext ctx, FullHttpRequest req, String[] segments)
-            throws WorkflowNotFoundException {
+            throws WorkflowException {
         RequestInput input = parseRequest(ctx, req);
         logger.info(input.toString());
         String wfName = segments[2];

File: frontend/server/src/main/java/org/pytorch/serve/metrics/Metric.java
Patch:
@@ -4,6 +4,7 @@
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.List;
+import java.util.concurrent.TimeUnit;
 import java.util.regex.Matcher;
 import java.util.regex.Pattern;
 
@@ -47,6 +48,8 @@ public Metric(
         this.unit = unit;
         this.hostName = hostName;
         this.dimensions = Arrays.asList(dimensions);
+        this.timestamp =
+                String.valueOf(TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()));
     }
 
     public String getHostName() {

File: frontend/server/src/main/java/org/pytorch/serve/wlm/ModelManager.java
Patch:
@@ -125,7 +125,7 @@ public ModelArchive registerModel(
             manifest.getModel().setModelVersion("1.0");
             manifest.getModel().setModelName(modelName);
             manifest.getModel().setHandler(new File(handler).getName());
-
+            manifest.getModel().setEnvelope(configManager.getTsServiceEnvelope());
             File f = new File(handler.substring(0, handler.lastIndexOf(':')));
             archive = new ModelArchive(manifest, url, f.getParentFile(), true);
         } else {

File: frontend/archive/src/test/java/org/pytorch/serve/archive/model/ModelArchiveTest.java
Patch:
@@ -143,7 +143,7 @@ public void archiveTest() throws ModelException, IOException, DownloadArchiveExc
                 ModelNotFoundException.class,
                 () ->
                         ModelArchive.downloadModel(
-                                ALLOWED_URLS_LIST, "src/test/resources/", "models"));
+                                ALLOWED_URLS_LIST, "src/test/resources", "noop_no_archive"));
 
         archive.clean();
     }

File: frontend/archive/src/test/java/org/pytorch/serve/archive/model/ModelArchiveTest.java
Patch:
@@ -143,7 +143,7 @@ public void archiveTest() throws ModelException, IOException, DownloadArchiveExc
                 ModelNotFoundException.class,
                 () ->
                         ModelArchive.downloadModel(
-                                ALLOWED_URLS_LIST, "src/test/resources/", "models"));
+                                ALLOWED_URLS_LIST, "src/test/resources", "noop_no_archive"));
 
         archive.clean();
     }

File: frontend/server/src/main/java/org/pytorch/serve/job/Job.java
Patch:
@@ -39,7 +39,7 @@ public WorkerCommands getCmd() {
     }
 
     public boolean isControlCmd() {
-        return !WorkerCommands.PREDICT.equals(cmd);
+        return !WorkerCommands.PREDICT.equals(cmd) && !WorkerCommands.DESCRIBE.equals(cmd);
     }
 
     public RequestInput getPayload() {

File: frontend/server/src/main/java/org/pytorch/serve/util/messages/WorkerCommands.java
Patch:
@@ -10,7 +10,9 @@ public enum WorkerCommands {
     @SerializedName("unload")
     UNLOAD("unload"),
     @SerializedName("stats")
-    STATS("stats");
+    STATS("stats"),
+    @SerializedName("describe")
+    DESCRIBE("describe");
 
     private String command;
 

File: frontend/server/src/test/java/org/pytorch/serve/SnapshotTest.java
Patch:
@@ -136,7 +136,7 @@ public void testNoSnapshotOnDescribeModel() throws InterruptedException {
         Channel channel = TestUtils.getInferenceChannel(configManager);
         TestUtils.setResult(null);
         TestUtils.setLatch(new CountDownLatch(1));
-        TestUtils.describeModel(channel, "noop_v1.0", null);
+        TestUtils.describeModel(channel, "noop_v1.0", null, false);
         TestUtils.getLatch().await();
         validateNoSnapshot();
     }

File: frontend/server/src/main/java/org/pytorch/serve/job/Job.java
Patch:
@@ -39,7 +39,7 @@ public WorkerCommands getCmd() {
     }
 
     public boolean isControlCmd() {
-        return !WorkerCommands.PREDICT.equals(cmd);
+        return !WorkerCommands.PREDICT.equals(cmd) && !WorkerCommands.DESCRIBE.equals(cmd);
     }
 
     public RequestInput getPayload() {

File: frontend/server/src/main/java/org/pytorch/serve/util/messages/WorkerCommands.java
Patch:
@@ -10,7 +10,9 @@ public enum WorkerCommands {
     @SerializedName("unload")
     UNLOAD("unload"),
     @SerializedName("stats")
-    STATS("stats");
+    STATS("stats"),
+    @SerializedName("describe")
+    DESCRIBE("describe");
 
     private String command;
 

File: frontend/server/src/test/java/org/pytorch/serve/SnapshotTest.java
Patch:
@@ -136,7 +136,7 @@ public void testNoSnapshotOnDescribeModel() throws InterruptedException {
         Channel channel = TestUtils.getInferenceChannel(configManager);
         TestUtils.setResult(null);
         TestUtils.setLatch(new CountDownLatch(1));
-        TestUtils.describeModel(channel, "noop_v1.0", null);
+        TestUtils.describeModel(channel, "noop_v1.0", null, false);
         TestUtils.getLatch().await();
         validateNoSnapshot();
     }

File: frontend/server/src/main/java/org/pytorch/serve/job/Job.java
Patch:
@@ -39,7 +39,7 @@ public WorkerCommands getCmd() {
     }
 
     public boolean isControlCmd() {
-        return !WorkerCommands.PREDICT.equals(cmd);
+        return !WorkerCommands.PREDICT.equals(cmd) && !WorkerCommands.DESCRIBE.equals(cmd);
     }
 
     public RequestInput getPayload() {

File: frontend/server/src/main/java/org/pytorch/serve/util/messages/WorkerCommands.java
Patch:
@@ -10,7 +10,9 @@ public enum WorkerCommands {
     @SerializedName("unload")
     UNLOAD("unload"),
     @SerializedName("stats")
-    STATS("stats");
+    STATS("stats"),
+    @SerializedName("describe")
+    DESCRIBE("describe");
 
     private String command;
 

File: frontend/server/src/test/java/org/pytorch/serve/SnapshotTest.java
Patch:
@@ -136,7 +136,7 @@ public void testNoSnapshotOnDescribeModel() throws InterruptedException {
         Channel channel = TestUtils.getInferenceChannel(configManager);
         TestUtils.setResult(null);
         TestUtils.setLatch(new CountDownLatch(1));
-        TestUtils.describeModel(channel, "noop_v1.0", null);
+        TestUtils.describeModel(channel, "noop_v1.0", null, false);
         TestUtils.getLatch().await();
         validateNoSnapshot();
     }

File: frontend/server/src/main/java/org/pytorch/serve/metrics/MetricCollector.java
Patch:
@@ -32,9 +32,11 @@ public MetricCollector(ConfigManager configManager) {
     public void run() {
         try {
             // Collect System level Metrics
-            String[] args = new String[2];
+            String[] args = new String[4];
             args[0] = configManager.getPythonExecutable();
             args[1] = "ts/metrics/metric_collector.py";
+            args[2] = "--gpu";
+            args[3] = String.valueOf(ConfigManager.getInstance().getNumberOfGpu());
             File workingDir = new File(configManager.getModelServerHome());
 
             String[] envp = EnvironmentUtils.getEnvString(workingDir.getAbsolutePath(), null, null);

File: frontend/server/src/main/java/org/pytorch/serve/metrics/MetricCollector.java
Patch:
@@ -32,9 +32,11 @@ public MetricCollector(ConfigManager configManager) {
     public void run() {
         try {
             // Collect System level Metrics
-            String[] args = new String[2];
+            String[] args = new String[4];
             args[0] = configManager.getPythonExecutable();
             args[1] = "ts/metrics/metric_collector.py";
+            args[2] = "--gpu";
+            args[3] = String.valueOf(ConfigManager.getInstance().getNumberOfGpu());
             File workingDir = new File(configManager.getModelServerHome());
 
             String[] envp = EnvironmentUtils.getEnvString(workingDir.getAbsolutePath(), null, null);

File: frontend/server/src/main/java/org/pytorch/serve/metrics/MetricCollector.java
Patch:
@@ -32,9 +32,11 @@ public MetricCollector(ConfigManager configManager) {
     public void run() {
         try {
             // Collect System level Metrics
-            String[] args = new String[2];
+            String[] args = new String[4];
             args[0] = configManager.getPythonExecutable();
             args[1] = "ts/metrics/metric_collector.py";
+            args[2] = "--gpu";
+            args[3] = String.valueOf(ConfigManager.getInstance().getNumberOfGpu());
             File workingDir = new File(configManager.getModelServerHome());
 
             String[] envp = EnvironmentUtils.getEnvString(workingDir.getAbsolutePath(), null, null);

File: frontend/server/src/main/java/org/pytorch/serve/metrics/MetricCollector.java
Patch:
@@ -32,9 +32,11 @@ public MetricCollector(ConfigManager configManager) {
     public void run() {
         try {
             // Collect System level Metrics
-            String[] args = new String[2];
+            String[] args = new String[4];
             args[0] = configManager.getPythonExecutable();
             args[1] = "ts/metrics/metric_collector.py";
+            args[2] = "--gpu";
+            args[3] = String.valueOf(ConfigManager.getInstance().getNumberOfGpu());
             File workingDir = new File(configManager.getModelServerHome());
 
             String[] envp = EnvironmentUtils.getEnvString(workingDir.getAbsolutePath(), null, null);

File: frontend/server/src/main/java/org/pytorch/serve/ModelServer.java
Patch:
@@ -433,7 +433,7 @@ private boolean validEndpoint(Annotation a, EndpointTypes type) {
                 && ((Endpoint) a).endpointType().equals(type);
     }
 
-    private HashMap<String, ModelServerEndpoint> ModelServerEndpoint(EndpointTypes type) {
+    private HashMap<String, ModelServerEndpoint> registerEndpoints(EndpointTypes type) {
         ServiceLoader<ModelServerEndpoint> loader = ServiceLoader.load(ModelServerEndpoint.class);
         HashMap<String, ModelServerEndpoint> ep = new HashMap<>();
         for (ModelServerEndpoint mep : loader) {

File: frontend/server/src/main/java/org/pytorch/serve/wlm/WorkerLifeCycle.java
Patch:
@@ -71,8 +71,8 @@ public boolean isLauncherAvailable()
             String[] cmdList = new String[cmd.size()];
             cmdList = cmd.toArray(cmdList);
 
-            Process process = Runtime.getRuntime().exec(cmdList);
-            int ret = process.waitFor();
+            Process processLauncher = Runtime.getRuntime().exec(cmdList);
+            int ret = processLauncher.waitFor();
             launcherAvailable = (ret == 0);
         } catch (IOException | InterruptedException e) {
             throw new WorkerInitializationException("Failed to start launcher", e);

File: frontend/server/src/main/java/org/pytorch/serve/wlm/WorkerLifeCycle.java
Patch:
@@ -68,10 +68,10 @@ public boolean isLauncherAvailable()
             String dummyCmd = "hostname";
             cmd.add(dummyCmd);
 
-            String[] cmd_ = new String[cmd.size()];
-            cmd_ = cmd.toArray(cmd_);
+            String[] cmdList = new String[cmd.size()];
+            cmdList = cmd.toArray(cmdList);
 
-            Process process = Runtime.getRuntime().exec(cmd_);
+            Process process = Runtime.getRuntime().exec(cmdList);
             int ret = process.waitFor();
             launcherAvailable = (ret == 0);
         } catch (IOException | InterruptedException e) {

File: frontend/server/src/main/java/org/pytorch/serve/metrics/Metric.java
Patch:
@@ -4,6 +4,7 @@
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.List;
+import java.util.concurrent.TimeUnit;
 import java.util.regex.Matcher;
 import java.util.regex.Pattern;
 
@@ -47,6 +48,7 @@ public Metric(
         this.unit = unit;
         this.hostName = hostName;
         this.dimensions = Arrays.asList(dimensions);
+        this.timestamp = String.valueOf(TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()));
     }
 
     public String getHostName() {

File: frontend/server/src/main/java/org/pytorch/serve/metrics/MetricCollector.java
Patch:
@@ -34,9 +34,9 @@ public void run() {
             // Collect System level Metrics
             String[] args = new String[2];
             args[0] = configManager.getPythonExecutable();
-            args[1] = String.format(
-                    "ts/metrics/metric_collector.py --gpu %d",
-                    ConfigManager.getInstance().getNumberOfGpu());
+            args[1] = "ts/metrics/metric_collector.py";
+            args[2] = "--gpu";
+            args[3] = String.valueOf(ConfigManager.getInstance().getNumberOfGpu());
             File workingDir = new File(configManager.getModelServerHome());
 
             String[] envp = EnvironmentUtils.getEnvString(workingDir.getAbsolutePath(), null, null);

File: frontend/server/src/main/java/org/pytorch/serve/openapi/OpenApiUtils.java
Patch:
@@ -34,8 +34,8 @@ public static String listApis(ConnectorType type) {
         return JsonUtils.GSON_PRETTY.toJson(openApi);
     }
     /**
-     * The /v1/models/{model_name}:predict prediction api is used to access torchserve from
-     * kfserving v1 predictor
+     * The /v1/models/{model_name}:predict prediction api is used to access torchserve from kserve
+     * v1 protocol and /v2/models/{model_name}/infer is for kserve v2 protocol.
      */
     private static void listInferenceApis(OpenApi openApi) {
         openApi.addPath("/", getApiDescriptionPath("apiDescription", false));

File: frontend/server/src/main/java/org/pytorch/serve/job/GRPCJob.java
Patch:
@@ -17,8 +17,8 @@
 
 public class GRPCJob extends Job {
     private static final Logger logger = LoggerFactory.getLogger(Job.class);
-    private static final org.apache.log4j.Logger loggerTsMetrics =
-            org.apache.log4j.Logger.getLogger(ConfigManager.MODEL_SERVER_METRICS_LOGGER);
+    private static final Logger loggerTsMetrics =
+            LoggerFactory.getLogger(ConfigManager.MODEL_SERVER_METRICS_LOGGER);
     private static final Dimension DIMENSION = new Dimension("Level", "Host");
 
     private StreamObserver<PredictionResponse> predictionResponseObserver;
@@ -55,6 +55,7 @@ public void response(
                         TimeUnit.MILLISECONDS.convert(
                                 getScheduled() - getBegin(), TimeUnit.NANOSECONDS));
         loggerTsMetrics.info(
+                "{}",
                 new Metric(
                         "QueueTime",
                         queueTime,

File: frontend/server/src/main/java/org/pytorch/serve/job/RestJob.java
Patch:
@@ -23,8 +23,8 @@
 public class RestJob extends Job {
 
     private static final Logger logger = LoggerFactory.getLogger(Job.class);
-    private static final org.apache.log4j.Logger loggerTsMetrics =
-            org.apache.log4j.Logger.getLogger(ConfigManager.MODEL_SERVER_METRICS_LOGGER);
+    private static final Logger loggerTsMetrics =
+            LoggerFactory.getLogger(ConfigManager.MODEL_SERVER_METRICS_LOGGER);
     private static final Dimension DIMENSION = new Dimension("Level", "Host");
 
     private ChannelHandlerContext ctx;
@@ -87,6 +87,7 @@ public void response(
                         TimeUnit.MILLISECONDS.convert(
                                 getScheduled() - getBegin(), TimeUnit.NANOSECONDS));
         loggerTsMetrics.info(
+                "{}",
                 new Metric(
                         "QueueTime",
                         queueTime,

File: frontend/server/src/main/java/org/pytorch/serve/metrics/MetricCollector.java
Patch:
@@ -20,8 +20,8 @@
 public class MetricCollector implements Runnable {
 
     private static final Logger logger = LoggerFactory.getLogger(MetricCollector.class);
-    private static final org.apache.log4j.Logger loggerMetrics =
-            org.apache.log4j.Logger.getLogger(ConfigManager.MODEL_SERVER_METRICS_LOGGER);
+    private static final Logger loggerMetrics =
+            LoggerFactory.getLogger(ConfigManager.MODEL_SERVER_METRICS_LOGGER);
     private ConfigManager configManager;
 
     public MetricCollector(ConfigManager configManager) {
@@ -77,7 +77,7 @@ public void run() {
                     if (metric == null) {
                         logger.warn("Parse metrics failed: " + line);
                     } else {
-                        loggerMetrics.info(metric);
+                        loggerMetrics.info("{}", metric);
                         metricsSystem.add(metric);
                     }
                 }

File: frontend/server/src/main/java/org/pytorch/serve/wlm/WorkerLifeCycle.java
Patch:
@@ -46,7 +46,7 @@ public static boolean isIpexInstalled(){
             int ret = process.waitFor();
             ipexInstalled = (ret == 0);
         } catch (IOException | InterruptedException e) {}
-        return ipexInstalled = (ret == 0);;
+        return ipexInstalled;
     }
 
     public void startWorker(int port) throws WorkerInitializationException, InterruptedException {

File: frontend/server/src/main/java/org/pytorch/serve/wlm/WorkerLifeCycle.java
Patch:
@@ -54,8 +54,8 @@ public void startWorker(int port) throws WorkerInitializationException, Interrup
         if (configManager.isCPULauncherEnabled()) {
             argl.add("-m");
             argl.add("intel_extension_for_pytorch.cpu.launch");
-	    argl.add("----ninstance")
-	    argl.add("1")
+	    argl.add("----ninstance");
+	    argl.add("1");
             String largs = configManager.getCPULauncherArgs();
             if (largs != null && largs.length() > 1) {
                 String[] argarray = largs.split(" ");

File: frontend/server/src/main/java/org/pytorch/serve/openapi/OpenApiUtils.java
Patch:
@@ -41,6 +41,7 @@ private static void listInferenceApis(OpenApi openApi) {
         openApi.addPath("/", getApiDescriptionPath("apiDescription", false));
         openApi.addPath("/ping", getPingPath());
         openApi.addPath("/v1/models/{model_name}:predict", getPredictionsPath(false));
+        openApi.addPath("/v2/models/{model_name}/infer", getPredictionsPath(false));
         openApi.addPath("/predictions/{model_name}", getPredictionsPath(false));
         openApi.addPath("/predictions/{model_name}/{model_version}", getPredictionsPath(true));
         openApi.addPath("/api-description", getApiDescriptionPath("api-description", true));
@@ -65,6 +66,7 @@ public static String getModelApi(Model model) {
 
         openApi.addPath("/prediction/" + modelName, getModelPath(modelName));
         openApi.addPath("/v1/models/{model_name}:predict", getModelPath(modelName));
+        openApi.addPath("/v2/models/{model_name}/infer", getModelPath(modelName));
 
         return JsonUtils.GSON_PRETTY.toJson(openApi);
     }

File: frontend/server/src/main/java/org/pytorch/serve/openapi/OpenApiUtils.java
Patch:
@@ -34,8 +34,8 @@ public static String listApis(ConnectorType type) {
         return JsonUtils.GSON_PRETTY.toJson(openApi);
     }
     /**
-     * The /v1/models/{model_name}:predict prediction api is used to access torchserve from
-     * kserve v1 predictor
+     * The /v1/models/{model_name}:predict prediction api is used to access torchserve from kserve
+     * v1 protocol and /v2/models/{model_name}/infer is for kserve v2 protocol.
      */
     private static void listInferenceApis(OpenApi openApi) {
         openApi.addPath("/", getApiDescriptionPath("apiDescription", false));

File: frontend/server/src/main/java/org/pytorch/serve/openapi/OpenApiUtils.java
Patch:
@@ -35,7 +35,7 @@ public static String listApis(ConnectorType type) {
     }
     /**
      * The /v1/models/{model_name}:predict prediction api is used to access torchserve from
-     * kfserving v1 predictor
+     * kserve v1 predictor
      */
     private static void listInferenceApis(OpenApi openApi) {
         openApi.addPath("/", getApiDescriptionPath("apiDescription", false));

File: frontend/server/src/main/java/org/pytorch/serve/openapi/OpenApiUtils.java
Patch:
@@ -41,6 +41,7 @@ private static void listInferenceApis(OpenApi openApi) {
         openApi.addPath("/", getApiDescriptionPath("apiDescription", false));
         openApi.addPath("/ping", getPingPath());
         openApi.addPath("/v1/models/{model_name}:predict", getPredictionsPath(false));
+        openApi.addPath("/v2/models/{model_name}/infer", getPredictionsPath(false));
         openApi.addPath("/predictions/{model_name}", getPredictionsPath(false));
         openApi.addPath("/predictions/{model_name}/{model_version}", getPredictionsPath(true));
         openApi.addPath("/api-description", getApiDescriptionPath("api-description", true));
@@ -65,6 +66,7 @@ public static String getModelApi(Model model) {
 
         openApi.addPath("/prediction/" + modelName, getModelPath(modelName));
         openApi.addPath("/v1/models/{model_name}:predict", getModelPath(modelName));
+        openApi.addPath("/v2/models/{model_name}/infer", getModelPath(modelName));
 
         return JsonUtils.GSON_PRETTY.toJson(openApi);
     }

File: frontend/server/src/main/java/org/pytorch/serve/http/api/rest/InferenceRequestHandler.java
Patch:
@@ -25,6 +25,7 @@
 import org.pytorch.serve.openapi.OpenApiUtils;
 import org.pytorch.serve.servingsdk.ModelServerEndpoint;
 import org.pytorch.serve.util.ApiUtils;
+import org.pytorch.serve.util.ConfigManager;
 import org.pytorch.serve.util.NettyUtils;
 import org.pytorch.serve.util.messages.InputParameter;
 import org.pytorch.serve.util.messages.RequestInput;
@@ -252,7 +253,8 @@ private static RequestInput parseRequest(
         if (HttpPostRequestDecoder.isMultipart(req)
                 || HttpHeaderValues.APPLICATION_X_WWW_FORM_URLENCODED.contentEqualsIgnoreCase(
                         contentType)) {
-            HttpDataFactory factory = new DefaultHttpDataFactory(6553500);
+            HttpDataFactory factory =
+                    new DefaultHttpDataFactory(ConfigManager.getInstance().getMaxRequestSize());
             HttpPostRequestDecoder form = new HttpPostRequestDecoder(factory, req);
             try {
                 while (form.hasNext()) {

File: frontend/server/src/main/java/org/pytorch/serve/util/codec/ModelRequestEncoder.java
Patch:
@@ -60,6 +60,7 @@ protected void encode(ChannelHandlerContext ctx, BaseModelRequest msg, ByteBuf o
             out.writeInt(buf.length);
             out.writeBytes(buf);
 
+            out.writeBoolean(request.isLimitMaxImagePixels());
         } else if (msg instanceof ModelInferenceRequest) {
             out.writeByte('I');
             ModelInferenceRequest request = (ModelInferenceRequest) msg;

File: frontend/server/src/main/java/org/pytorch/serve/wlm/WorkerThread.java
Patch:
@@ -182,6 +182,7 @@ public void run() {
                 req = aggregator.getRequest(workerId, state);
 
                 long wtStartTime = System.currentTimeMillis();
+                logger.info("Flushing req. to backend at: " + wtStartTime);
                 backendChannel.writeAndFlush(req).sync();
 
                 long begin = System.currentTimeMillis();

File: frontend/server/src/main/java/org/pytorch/serve/workflow/api/http/WorkflowInferenceRequestHandler.java
Patch:
@@ -18,6 +18,7 @@
 import org.pytorch.serve.http.HttpRequestHandlerChain;
 import org.pytorch.serve.http.ResourceNotFoundException;
 import org.pytorch.serve.http.StatusResponse;
+import org.pytorch.serve.util.ConfigManager;
 import org.pytorch.serve.util.NettyUtils;
 import org.pytorch.serve.util.messages.InputParameter;
 import org.pytorch.serve.util.messages.RequestInput;
@@ -96,7 +97,8 @@ private static RequestInput parseRequest(ChannelHandlerContext ctx, FullHttpRequ
         if (HttpPostRequestDecoder.isMultipart(req)
                 || HttpHeaderValues.APPLICATION_X_WWW_FORM_URLENCODED.contentEqualsIgnoreCase(
                         contentType)) {
-            HttpDataFactory factory = new DefaultHttpDataFactory(6553500);
+            HttpDataFactory factory =
+                    new DefaultHttpDataFactory(ConfigManager.getInstance().getMaxRequestSize());
             HttpPostRequestDecoder form = new HttpPostRequestDecoder(factory, req);
             try {
                 while (form.hasNext()) {

File: frontend/server/src/main/java/org/pytorch/serve/http/api/rest/InferenceRequestHandler.java
Patch:
@@ -25,6 +25,7 @@
 import org.pytorch.serve.openapi.OpenApiUtils;
 import org.pytorch.serve.servingsdk.ModelServerEndpoint;
 import org.pytorch.serve.util.ApiUtils;
+import org.pytorch.serve.util.ConfigManager;
 import org.pytorch.serve.util.NettyUtils;
 import org.pytorch.serve.util.messages.InputParameter;
 import org.pytorch.serve.util.messages.RequestInput;
@@ -252,7 +253,8 @@ private static RequestInput parseRequest(
         if (HttpPostRequestDecoder.isMultipart(req)
                 || HttpHeaderValues.APPLICATION_X_WWW_FORM_URLENCODED.contentEqualsIgnoreCase(
                         contentType)) {
-            HttpDataFactory factory = new DefaultHttpDataFactory(6553500);
+            HttpDataFactory factory =
+                    new DefaultHttpDataFactory(ConfigManager.getInstance().getMaxRequestSize());
             HttpPostRequestDecoder form = new HttpPostRequestDecoder(factory, req);
             try {
                 while (form.hasNext()) {

File: frontend/server/src/main/java/org/pytorch/serve/util/codec/ModelRequestEncoder.java
Patch:
@@ -60,6 +60,7 @@ protected void encode(ChannelHandlerContext ctx, BaseModelRequest msg, ByteBuf o
             out.writeInt(buf.length);
             out.writeBytes(buf);
 
+            out.writeBoolean(request.isLimitMaxImagePixels());
         } else if (msg instanceof ModelInferenceRequest) {
             out.writeByte('I');
             ModelInferenceRequest request = (ModelInferenceRequest) msg;

File: frontend/server/src/main/java/org/pytorch/serve/wlm/WorkerThread.java
Patch:
@@ -182,6 +182,7 @@ public void run() {
                 req = aggregator.getRequest(workerId, state);
 
                 long wtStartTime = System.currentTimeMillis();
+                logger.info("Flushing req. to backend at: " + wtStartTime);
                 backendChannel.writeAndFlush(req).sync();
 
                 long begin = System.currentTimeMillis();

File: frontend/server/src/main/java/org/pytorch/serve/workflow/api/http/WorkflowInferenceRequestHandler.java
Patch:
@@ -18,6 +18,7 @@
 import org.pytorch.serve.http.HttpRequestHandlerChain;
 import org.pytorch.serve.http.ResourceNotFoundException;
 import org.pytorch.serve.http.StatusResponse;
+import org.pytorch.serve.util.ConfigManager;
 import org.pytorch.serve.util.NettyUtils;
 import org.pytorch.serve.util.messages.InputParameter;
 import org.pytorch.serve.util.messages.RequestInput;
@@ -96,7 +97,8 @@ private static RequestInput parseRequest(ChannelHandlerContext ctx, FullHttpRequ
         if (HttpPostRequestDecoder.isMultipart(req)
                 || HttpHeaderValues.APPLICATION_X_WWW_FORM_URLENCODED.contentEqualsIgnoreCase(
                         contentType)) {
-            HttpDataFactory factory = new DefaultHttpDataFactory(6553500);
+            HttpDataFactory factory =
+                    new DefaultHttpDataFactory(ConfigManager.getInstance().getMaxRequestSize());
             HttpPostRequestDecoder form = new HttpPostRequestDecoder(factory, req);
             try {
                 while (form.hasNext()) {

File: frontend/server/src/main/java/org/pytorch/serve/util/ApiUtils.java
Patch:
@@ -402,7 +402,7 @@ public static RestJob addRESTInferenceJob(
     public static String getInferenceErrorResponseMessage(String modelName, String modelVersion) {
         String responseMessage = "Model \"" + modelName;
 
-        if (modelVersion == null) {
+        if (modelVersion != null) {
             responseMessage += "\" Version " + modelVersion;
         }
 

File: frontend/server/src/test/java/org/pytorch/serve/WorkflowTest.java
Patch:
@@ -454,7 +454,7 @@ public void testPredictionMemoryError() throws InterruptedException {
         TestUtils.setLatch(new CountDownLatch(1));
         Assert.assertNotNull(channel);
 
-        TestUtils.unregisterWorkflow(channel, "pred-err", true);
+        TestUtils.unregisterWorkflow(channel, "pred-err", false);
         TestUtils.getLatch().await();
         Assert.assertEquals(TestUtils.getHttpStatus(), HttpResponseStatus.OK);
     }

File: frontend/server/src/main/java/org/pytorch/serve/util/ApiUtils.java
Patch:
@@ -402,7 +402,7 @@ public static RestJob addRESTInferenceJob(
     public static String getInferenceErrorResponseMessage(String modelName, String modelVersion) {
         String responseMessage = "Model \"" + modelName;
 
-        if (modelVersion == null) {
+        if (modelVersion != null) {
             responseMessage += "\" Version " + modelVersion;
         }
 

File: frontend/server/src/test/java/org/pytorch/serve/WorkflowTest.java
Patch:
@@ -454,7 +454,7 @@ public void testPredictionMemoryError() throws InterruptedException {
         TestUtils.setLatch(new CountDownLatch(1));
         Assert.assertNotNull(channel);
 
-        TestUtils.unregisterWorkflow(channel, "pred-err", true);
+        TestUtils.unregisterWorkflow(channel, "pred-err", false);
         TestUtils.getLatch().await();
         Assert.assertEquals(TestUtils.getHttpStatus(), HttpResponseStatus.OK);
     }

File: frontend/server/src/test/java/org/pytorch/serve/WorkflowTest.java
Patch:
@@ -454,7 +454,7 @@ public void testPredictionMemoryError() throws InterruptedException {
         TestUtils.setLatch(new CountDownLatch(1));
         Assert.assertNotNull(channel);
 
-        TestUtils.unregisterWorkflow(channel, "pred-err", true);
+        TestUtils.unregisterWorkflow(channel, "pred-err", false);
         TestUtils.getLatch().await();
         Assert.assertEquals(TestUtils.getHttpStatus(), HttpResponseStatus.OK);
     }

File: frontend/server/src/main/java/org/pytorch/serve/util/ApiUtils.java
Patch:
@@ -402,7 +402,7 @@ public static RestJob addRESTInferenceJob(
     public static String getInferenceErrorResponseMessage(String modelName, String modelVersion) {
         String responseMessage = "Model \"" + modelName;
 
-        if (modelVersion == null) {
+        if (modelVersion != null) {
             responseMessage += "\" Version " + modelVersion;
         }
 

File: frontend/server/src/test/java/org/pytorch/serve/WorkflowTest.java
Patch:
@@ -454,7 +454,7 @@ public void testPredictionMemoryError() throws InterruptedException {
         TestUtils.setLatch(new CountDownLatch(1));
         Assert.assertNotNull(channel);
 
-        TestUtils.unregisterWorkflow(channel, "pred-err", true);
+        TestUtils.unregisterWorkflow(channel, "pred-err", false);
         TestUtils.getLatch().await();
         Assert.assertEquals(TestUtils.getHttpStatus(), HttpResponseStatus.OK);
     }

File: frontend/server/src/main/java/org/pytorch/serve/util/ApiUtils.java
Patch:
@@ -402,7 +402,7 @@ public static RestJob addRESTInferenceJob(
     public static String getInferenceErrorResponseMessage(String modelName, String modelVersion) {
         String responseMessage = "Model \"" + modelName;
 
-        if (modelVersion == null) {
+        if (modelVersion != null) {
             responseMessage += "\" Version " + modelVersion;
         }
 

File: frontend/server/src/main/java/org/pytorch/serve/util/ApiUtils.java
Patch:
@@ -402,7 +402,7 @@ public static RestJob addRESTInferenceJob(
     public static String getInferenceErrorResponseMessage(String modelName, String modelVersion) {
         String responseMessage = "Model \"" + modelName;
 
-        if (modelVersion == null) {
+        if (modelVersion != null) {
             responseMessage += "\" Version " + modelVersion;
         }
 

File: frontend/server/src/main/java/org/pytorch/serve/util/ApiUtils.java
Patch:
@@ -402,7 +402,7 @@ public static RestJob addRESTInferenceJob(
     public static String getInferenceErrorResponseMessage(String modelName, String modelVersion) {
         String responseMessage = "Model \"" + modelName;
 
-        if (modelVersion == null) {
+        if (modelVersion != null) {
             responseMessage += "\" Version " + modelVersion;
         }
 

File: frontend/server/src/main/java/org/pytorch/serve/util/ApiUtils.java
Patch:
@@ -402,7 +402,7 @@ public static RestJob addRESTInferenceJob(
     public static String getInferenceErrorResponseMessage(String modelName, String modelVersion) {
         String responseMessage = "Model \"" + modelName;
 
-        if (modelVersion == null) {
+        if (modelVersion != null) {
             responseMessage += "\" Version " + modelVersion;
         }
 

File: frontend/server/src/main/java/org/pytorch/serve/util/ApiUtils.java
Patch:
@@ -402,7 +402,7 @@ public static RestJob addRESTInferenceJob(
     public static String getInferenceErrorResponseMessage(String modelName, String modelVersion) {
         String responseMessage = "Model \"" + modelName;
 
-        if (modelVersion == null) {
+        if (modelVersion != null) {
             responseMessage += "\" Version " + modelVersion;
         }
 

File: frontend/server/src/main/java/org/pytorch/serve/wlm/WorkLoadManager.java
Patch:
@@ -165,7 +165,7 @@ private void addThreads(
             int gpuId = -1;
 
             if (maxGpu > 0) {
-                gpuId = ConfigManager.gpuIds.get(
+                gpuId = ConfigManager.getGpuIds().get(
                         gpuCounter.accumulateAndGet(maxGpu, (prev, maxGpuId) -> ++prev % maxGpuId));
             }
 

File: frontend/server/src/main/java/org/pytorch/serve/util/ConfigManager.java
Patch:
@@ -745,7 +745,7 @@ private static int getAvailableGpu() {
                 }
             }
 
-            return gpuIds.size() - 1;
+            return gpuIds.size();
         } catch (IOException | InterruptedException e) {
             return 0;
         }

File: frontend/server/src/main/java/org/pytorch/serve/wlm/WorkLoadManager.java
Patch:
@@ -165,7 +165,8 @@ private void addThreads(
             int gpuId = -1;
 
             if (maxGpu > 0) {
-                gpuId = gpuCounter.accumulateAndGet(maxGpu, (prev, maxGpuId) -> ++prev % maxGpuId);
+                gpuId = ConfigManager.gpuIds.get(
+                        gpuCounter.accumulateAndGet(maxGpu, (prev, maxGpuId) -> ++prev % maxGpuId));
             }
 
             BatchAggregator aggregator = new BatchAggregator(model);

File: frontend/server/src/test/java/org/pytorch/serve/ModelServerTest.java
Patch:
@@ -1097,7 +1097,7 @@ public void testSuccessBatch() throws InterruptedException {
         TestUtils.setLatch(new CountDownLatch(1));
         TestUtils.setHttpStatus(null);
 
-        for (int i = 0; i < batch_size; i++) {
+        for (int i = 0; i < batchSize; i++) {
             DefaultFullHttpRequest req =
                     new DefaultFullHttpRequest(
                             HttpVersion.HTTP_1_1, HttpMethod.POST, "/predictions/noop");

File: frontend/server/src/test/java/org/pytorch/serve/ModelServerTest.java
Patch:
@@ -1069,8 +1069,8 @@ public void testPredictionMemoryError() throws InterruptedException {
             alwaysRun = true,
             dependsOnMethods = {"testPredictionMemoryError"})
     public void testSuccessBatch() throws InterruptedException {
-        int batch_size = 4;
-        int max_batch_delay = 10000;
+        int batchSize = 4;
+        int maxBatchDelay = 10000;
         Channel channel = TestUtils.connect(ConnectorType.MANAGEMENT_CONNECTOR, configManager);
         Assert.assertNotNull(channel);
 
@@ -1079,7 +1079,7 @@ public void testSuccessBatch() throws InterruptedException {
         TestUtils.setLatch(new CountDownLatch(1));
 
         TestUtils.registerModel(
-                channel, "noop.mar", "noop", true, false, batch_size, max_batch_delay);
+                channel, "noop.mar", "noop", true, false, batchSize, maxBatchDelay);
         TestUtils.getLatch().await();
 
         StatusResponse status =

File: frontend/server/src/main/java/org/pytorch/serve/wlm/WorkerThread.java
Patch:
@@ -182,6 +182,7 @@ public void run() {
                 req = aggregator.getRequest(workerId, state);
 
                 long wtStartTime = System.currentTimeMillis();
+                logger.info("Flushing req. to backend at: " + wtStartTime);
                 backendChannel.writeAndFlush(req).sync();
 
                 long begin = System.currentTimeMillis();

File: frontend/server/src/main/java/org/pytorch/serve/http/api/rest/InferenceRequestHandler.java
Patch:
@@ -25,6 +25,7 @@
 import org.pytorch.serve.openapi.OpenApiUtils;
 import org.pytorch.serve.servingsdk.ModelServerEndpoint;
 import org.pytorch.serve.util.ApiUtils;
+import org.pytorch.serve.util.ConfigManager;
 import org.pytorch.serve.util.NettyUtils;
 import org.pytorch.serve.util.messages.InputParameter;
 import org.pytorch.serve.util.messages.RequestInput;
@@ -252,7 +253,7 @@ private static RequestInput parseRequest(
         if (HttpPostRequestDecoder.isMultipart(req)
                 || HttpHeaderValues.APPLICATION_X_WWW_FORM_URLENCODED.contentEqualsIgnoreCase(
                         contentType)) {
-            HttpDataFactory factory = new DefaultHttpDataFactory(6553500);
+            HttpDataFactory factory = new DefaultHttpDataFactory(ConfigManager.getInstance().getMaxRequestSize());
             HttpPostRequestDecoder form = new HttpPostRequestDecoder(factory, req);
             try {
                 while (form.hasNext()) {

File: frontend/server/src/main/java/org/pytorch/serve/workflow/api/http/WorkflowInferenceRequestHandler.java
Patch:
@@ -18,6 +18,7 @@
 import org.pytorch.serve.http.HttpRequestHandlerChain;
 import org.pytorch.serve.http.ResourceNotFoundException;
 import org.pytorch.serve.http.StatusResponse;
+import org.pytorch.serve.util.ConfigManager;
 import org.pytorch.serve.util.NettyUtils;
 import org.pytorch.serve.util.messages.InputParameter;
 import org.pytorch.serve.util.messages.RequestInput;
@@ -96,7 +97,7 @@ private static RequestInput parseRequest(ChannelHandlerContext ctx, FullHttpRequ
         if (HttpPostRequestDecoder.isMultipart(req)
                 || HttpHeaderValues.APPLICATION_X_WWW_FORM_URLENCODED.contentEqualsIgnoreCase(
                         contentType)) {
-            HttpDataFactory factory = new DefaultHttpDataFactory(6553500);
+            HttpDataFactory factory = new DefaultHttpDataFactory(ConfigManager.getInstance().getMaxRequestSize());
             HttpPostRequestDecoder form = new HttpPostRequestDecoder(factory, req);
             try {
                 while (form.hasNext()) {

File: frontend/server/src/main/java/org/pytorch/serve/openapi/OpenApiUtils.java
Patch:
@@ -41,6 +41,7 @@ private static void listInferenceApis(OpenApi openApi) {
         openApi.addPath("/", getApiDescriptionPath("apiDescription", false));
         openApi.addPath("/ping", getPingPath());
         openApi.addPath("/v1/models/{model_name}:predict", getPredictionsPath(false));
+        openApi.addPath("/v2/models/{model_name}/infer", getPredictionsPath(false));
         openApi.addPath("/predictions/{model_name}", getPredictionsPath(false));
         openApi.addPath("/predictions/{model_name}/{model_version}", getPredictionsPath(true));
         openApi.addPath("/api-description", getApiDescriptionPath("api-description", true));
@@ -65,6 +66,7 @@ public static String getModelApi(Model model) {
 
         openApi.addPath("/prediction/" + modelName, getModelPath(modelName));
         openApi.addPath("/v1/models/{model_name}:predict", getModelPath(modelName));
+        openApi.addPath("/v2/models/{model_name}/infer", getModelPath(modelName));
 
         return JsonUtils.GSON_PRETTY.toJson(openApi);
     }

File: frontend/server/src/main/java/org/pytorch/serve/workflow/api/http/WorkflowInferenceRequestHandler.java
Patch:
@@ -45,10 +45,10 @@ public void handleRequest(
             QueryStringDecoder decoder,
             String[] segments)
             throws ModelException, DownloadArchiveException, WorkflowException {
-        if (segments.length < 3) {
-            throw new ResourceNotFoundException();
-        }
         if ("wfpredict".equalsIgnoreCase(segments[1])) {
+            if (segments.length < 3) {
+                throw new ResourceNotFoundException();
+            }
             handlePredictions(ctx, req, segments);
         } else {
             chain.handleRequest(ctx, req, decoder, segments);

File: frontend/server/src/main/java/org/pytorch/serve/workflow/api/http/WorkflowInferenceRequestHandler.java
Patch:
@@ -45,10 +45,10 @@ public void handleRequest(
             QueryStringDecoder decoder,
             String[] segments)
             throws ModelException, DownloadArchiveException, WorkflowException {
-        if (segments.length < 3) {
-            throw new ResourceNotFoundException();
-        }
         if ("wfpredict".equalsIgnoreCase(segments[1])) {
+            if (segments.length < 3) {
+                throw new ResourceNotFoundException();
+            }
             handlePredictions(ctx, req, segments);
         } else {
             chain.handleRequest(ctx, req, decoder, segments);

File: frontend/archive/src/main/java/org/pytorch/serve/archive/DownloadArchiveException.java
Patch:
@@ -1,6 +1,6 @@
 package org.pytorch.serve.archive;
 
-public class DownloadModelException extends ModelException {
+public class DownloadArchiveException extends Exception {
 
     private static final long serialVersionUID = 1L;
 
@@ -10,7 +10,7 @@ public class DownloadModelException extends ModelException {
      * @param message The detail message (which is saved for later retrieval by the {@link
      *     #getMessage()} method)
      */
-    public DownloadModelException(String message) {
+    public DownloadArchiveException(String message) {
         super(message);
     }
 
@@ -26,7 +26,7 @@ public DownloadModelException(String message) {
      *     method). (A null value is permitted, and indicates that the cause is nonexistent or
      *     unknown.)
      */
-    public DownloadModelException(String message, Throwable cause) {
+    public DownloadArchiveException(String message, Throwable cause) {
         super(message, cause);
     }
 }

File: frontend/archive/src/main/java/org/pytorch/serve/archive/model/InvalidModelException.java
Patch:
@@ -1,4 +1,4 @@
-package org.pytorch.serve.archive;
+package org.pytorch.serve.archive.model;
 
 public class InvalidModelException extends ModelException {
 

File: frontend/archive/src/main/java/org/pytorch/serve/archive/model/Manifest.java
Patch:
@@ -1,4 +1,4 @@
-package org.pytorch.serve.archive;
+package org.pytorch.serve.archive.model;
 
 import com.google.gson.annotations.SerializedName;
 

File: frontend/archive/src/main/java/org/pytorch/serve/archive/model/ModelException.java
Patch:
@@ -1,4 +1,4 @@
-package org.pytorch.serve.archive;
+package org.pytorch.serve.archive.model;
 
 public class ModelException extends Exception {
 

File: frontend/archive/src/main/java/org/pytorch/serve/archive/model/ModelNotFoundException.java
Patch:
@@ -1,4 +1,4 @@
-package org.pytorch.serve.archive;
+package org.pytorch.serve.archive.model;
 
 public class ModelNotFoundException extends ModelException {
 

File: frontend/archive/src/main/java/org/pytorch/serve/archive/model/ModelVersionNotFoundException.java
Patch:
@@ -1,4 +1,4 @@
-package org.pytorch.serve.archive;
+package org.pytorch.serve.archive.model;
 
 public class ModelVersionNotFoundException extends ModelException {
 

File: frontend/archive/src/main/java/org/pytorch/serve/archive/utils/HexUtils.java
Patch:
@@ -1,4 +1,4 @@
-package org.pytorch.serve.archive;
+package org.pytorch.serve.archive.utils;
 
 public final class HexUtils {
 

File: frontend/server/src/main/java/org/pytorch/serve/grpcimpl/InferenceImpl.java
Patch:
@@ -7,8 +7,8 @@
 import java.net.HttpURLConnection;
 import java.util.Map;
 import java.util.UUID;
-import org.pytorch.serve.archive.ModelNotFoundException;
-import org.pytorch.serve.archive.ModelVersionNotFoundException;
+import org.pytorch.serve.archive.model.ModelNotFoundException;
+import org.pytorch.serve.archive.model.ModelVersionNotFoundException;
 import org.pytorch.serve.grpc.inference.InferenceAPIsServiceGrpc.InferenceAPIsServiceImplBase;
 import org.pytorch.serve.grpc.inference.PredictionResponse;
 import org.pytorch.serve.grpc.inference.PredictionsRequest;

File: frontend/server/src/main/java/org/pytorch/serve/http/InvalidRequestHandler.java
Patch:
@@ -3,13 +3,13 @@
 import io.netty.channel.ChannelHandlerContext;
 import io.netty.handler.codec.http.FullHttpRequest;
 import io.netty.handler.codec.http.QueryStringDecoder;
-import org.pytorch.serve.archive.ModelException;
+import org.pytorch.serve.archive.model.ModelException;
 
 public class InvalidRequestHandler extends HttpRequestHandlerChain {
     public InvalidRequestHandler() {}
 
     @Override
-    protected void handleRequest(
+    public void handleRequest(
             ChannelHandlerContext ctx,
             FullHttpRequest req,
             QueryStringDecoder decoder,

File: frontend/server/src/main/java/org/pytorch/serve/http/messages/DescribeModelResponse.java
Patch:
@@ -1,4 +1,4 @@
-package org.pytorch.serve.http;
+package org.pytorch.serve.http.messages;
 
 import java.util.ArrayList;
 import java.util.Date;

File: frontend/server/src/main/java/org/pytorch/serve/http/messages/KFV1ModelReadyResponse.java
Patch:
@@ -1,4 +1,4 @@
-package org.pytorch.serve.http;
+package org.pytorch.serve.http.messages;
 
 public class KFV1ModelReadyResponse {
 

File: frontend/server/src/main/java/org/pytorch/serve/http/messages/ListModelsResponse.java
Patch:
@@ -1,4 +1,4 @@
-package org.pytorch.serve.http;
+package org.pytorch.serve.http.messages;
 
 import java.util.ArrayList;
 import java.util.List;

File: frontend/server/src/main/java/org/pytorch/serve/util/JsonUtils.java
Patch:
@@ -8,12 +8,14 @@ public final class JsonUtils {
     public static final Gson GSON_PRETTY =
             new GsonBuilder()
                     .setDateFormat("yyyy-MM-dd'T'HH:mm:ss.SSS'Z'")
+                    .disableHtmlEscaping()
                     .setPrettyPrinting()
                     .create();
 
     public static final Gson GSON_PRETTY_EXPOSED =
             new GsonBuilder()
                     .excludeFieldsWithoutExposeAnnotation()
+                    .disableHtmlEscaping()
                     .setDateFormat("yyyy-MM-dd'T'HH:mm:ss.SSS'Z'")
                     .setPrettyPrinting()
                     .create();

File: frontend/server/src/main/java/org/pytorch/serve/util/codec/ModelResponseDecoder.java
Patch:
@@ -53,6 +53,9 @@ protected void decode(ChannelHandlerContext ctx, ByteBuf in, List<Object> out) {
                 prediction.setContentType(CodecUtils.readString(in, len));
 
                 // Set per request response code
+                if (in.readableBytes() < 4) {
+                    return;
+                }
                 int httpStatusCode = in.readInt();
                 prediction.setStatusCode(httpStatusCode);
 

File: frontend/server/src/main/java/org/pytorch/serve/util/messages/EnvironmentUtils.java
Patch:
@@ -5,7 +5,7 @@
 import java.util.HashMap;
 import java.util.Map;
 import java.util.regex.Pattern;
-import org.pytorch.serve.archive.Manifest;
+import org.pytorch.serve.archive.model.Manifest;
 import org.pytorch.serve.util.ConfigManager;
 import org.pytorch.serve.wlm.Model;
 

File: frontend/server/src/main/java/org/pytorch/serve/wlm/ModelVersionedRefs.java
Patch:
@@ -3,7 +3,7 @@
 import java.util.Map;
 import java.util.Set;
 import java.util.concurrent.ConcurrentHashMap;
-import org.pytorch.serve.archive.ModelVersionNotFoundException;
+import org.pytorch.serve.archive.model.ModelVersionNotFoundException;
 import org.pytorch.serve.http.ConflictStatusException;
 import org.pytorch.serve.http.InvalidModelVersionException;
 import org.slf4j.Logger;

File: frontend/server/src/test/java/org/pytorch/serve/SnapshotTest.java
Patch:
@@ -53,15 +53,15 @@ public class SnapshotTest {
     public void beforeSuite()
             throws InterruptedException, IOException, GeneralSecurityException,
                     InvalidSnapshotException {
-        System.setProperty("tsConfigFile", "src/test/resources/config.properties");
+        System.setProperty("tsConfigFile", "src/test/resources/config_snapshot.properties");
         FileUtils.cleanDirectory(new File(System.getProperty("LOG_LOCATION"), "config"));
 
         ConfigManager.init(new ConfigManager.Arguments());
         configManager = ConfigManager.getInstance();
         PluginsManager.getInstance().initialize();
 
         InternalLoggerFactory.setDefaultFactory(Slf4JLoggerFactory.INSTANCE);
-        configManager.setIniitialWorkerPort(9500);
+        configManager.setInitialWorkerPort(9500);
         server = new ModelServer(configManager);
         server.startRESTserver();
     }

File: frontend/server/src/test/java/org/pytorch/serve/util/ConfigManagerTest.java
Patch:
@@ -97,6 +97,8 @@ public void testNoEnvVars() throws ReflectiveOperationException, IOException {
         ConfigManager configManager = ConfigManager.getInstance();
         Assert.assertEquals("false", configManager.getEnableEnvVarsConfig());
         Assert.assertEquals(120, configManager.getDefaultResponseTimeout());
+        Assert.assertEquals(4, configManager.getJsonIntValue("noop", "1.0", "batchSize", 1));
+        Assert.assertEquals(4, configManager.getJsonIntValue("vgg16", "1.0", "maxWorkers", 1));
         modifyEnv("TS_DEFAULT_RESPONSE_TIMEOUT", "120");
     }
 }

File: frontend/server/src/main/java/org/pytorch/serve/workflow/api/http/WorkflowInferenceRequestHandler.java
Patch:
@@ -45,10 +45,10 @@ public void handleRequest(
             QueryStringDecoder decoder,
             String[] segments)
             throws ModelException, DownloadArchiveException, WorkflowException {
-        if (segments.length < 3) {
-            throw new ResourceNotFoundException();
-        }
         if ("wfpredict".equalsIgnoreCase(segments[1])) {
+            if (segments.length < 3) {
+                throw new ResourceNotFoundException();
+            }
             handlePredictions(ctx, req, segments);
         } else {
             chain.handleRequest(ctx, req, decoder, segments);

File: frontend/server/src/test/java/org/pytorch/serve/SnapshotTest.java
Patch:
@@ -53,7 +53,7 @@ public class SnapshotTest {
     public void beforeSuite()
             throws InterruptedException, IOException, GeneralSecurityException,
                     InvalidSnapshotException {
-        System.setProperty("tsConfigFile", "src/test/resources/config.properties");
+        System.setProperty("tsConfigFile", "src/test/resources/config_snapshot.properties");
         FileUtils.cleanDirectory(new File(System.getProperty("LOG_LOCATION"), "config"));
 
         ConfigManager.init(new ConfigManager.Arguments());

File: frontend/server/src/test/java/org/pytorch/serve/util/ConfigManagerTest.java
Patch:
@@ -97,6 +97,8 @@ public void testNoEnvVars() throws ReflectiveOperationException, IOException {
         ConfigManager configManager = ConfigManager.getInstance();
         Assert.assertEquals("false", configManager.getEnableEnvVarsConfig());
         Assert.assertEquals(120, configManager.getDefaultResponseTimeout());
+        Assert.assertEquals(4, configManager.getJsonIntValue("noop", "1.0", "batchSize", 1));
+        Assert.assertEquals(4, configManager.getJsonIntValue("vgg16", "1.0", "maxWorkers", 1));
         modifyEnv("TS_DEFAULT_RESPONSE_TIMEOUT", "120");
     }
 }

File: frontend/server/src/main/java/org/pytorch/serve/util/codec/ModelResponseDecoder.java
Patch:
@@ -53,6 +53,9 @@ protected void decode(ChannelHandlerContext ctx, ByteBuf in, List<Object> out) {
                 prediction.setContentType(CodecUtils.readString(in, len));
 
                 // Set per request response code
+                if (in.readableBytes() < 4) {
+                    return;
+                }
                 int httpStatusCode = in.readInt();
                 prediction.setStatusCode(httpStatusCode);
 

File: frontend/server/src/main/java/org/pytorch/serve/wlm/ModelManager.java
Patch:
@@ -13,7 +13,6 @@
 import java.util.Map;
 import java.util.Map.Entry;
 import java.util.Set;
-import java.util.StringBuilder;
 import java.util.concurrent.CompletableFuture;
 import java.util.concurrent.ConcurrentHashMap;
 import java.util.concurrent.ExecutionException;
@@ -238,7 +237,7 @@ private void setupModelDependencies(Model model)
                 while ((line = brdr.readLine()) != null) {
                     outputString.append(line);
                 }
-                String errorString = new StringBuilder();
+                StringBuilder errorString = new StringBuilder();
                 // process's stderr is ErrorStream for caller process
                 brdr = new BufferedReader(new InputStreamReader(process.getErrorStream()));
                 while ((line = brdr.readLine()) != null) {

File: frontend/server/src/main/java/org/pytorch/serve/wlm/ModelManager.java
Patch:
@@ -1,5 +1,5 @@
 package org.pytorch.serve.wlm;
-  
+
 import com.google.gson.JsonObject;
 import java.io.BufferedReader;
 import java.io.File;
@@ -493,4 +493,4 @@ public Set<Entry<String, ModelVersionedRefs>> getAllModels() {
     public int getNumRunningWorkers(ModelVersionName modelVersionName) {
         return wlm.getNumRunningWorkers(modelVersionName);
     }
-}
\ No newline at end of file
+}

File: frontend/server/src/main/java/org/pytorch/serve/wlm/ModelManager.java
Patch:
@@ -4,7 +4,6 @@
 import java.io.BufferedReader;
 import java.io.File;
 import java.io.IOException;
-import java.io.InputStream;
 import java.io.InputStreamReader;
 import java.net.HttpURLConnection;
 import java.nio.file.Path;

File: frontend/server/src/main/java/org/pytorch/serve/util/ConfigManager.java
Patch:
@@ -54,8 +54,6 @@ public final class ConfigManager {
     // NOTE: Variables which can be configured through environment variables **SHOULD** have a
     // "TS_" prefix
 
-    private org.slf4j.Logger logger = LoggerFactory.getLogger(ConfigManager.class);
-
     private static final String TS_DEBUG = "debug";
     private static final String TS_INFERENCE_ADDRESS = "inference_address";
     private static final String TS_MANAGEMENT_ADDRESS = "management_address";
@@ -803,7 +801,7 @@ public int getJsonIntValue(String modelName, String version, String element, int
                         value = defaultVal;
                     }
                 } catch (ClassCastException | IllegalStateException e) {
-                    logger.error(
+                    Logger.getRootLogger().error(
                             "Invalid value for model: "
                                     + modelName
                                     + ":"

File: frontend/server/src/main/java/org/pytorch/serve/ModelServer.java
Patch:
@@ -496,6 +496,7 @@ public void stop() {
             try {
                 future.channel().close().sync();
             } catch (InterruptedException ignore) {
+                ignore.printStackTrace(); // NOPMD
             }
 
             SnapshotManager.getInstance().saveShutdownSnapshot();

File: frontend/server/src/main/java/org/pytorch/serve/util/ApiUtils.java
Patch:
@@ -116,6 +116,7 @@ public static StatusResponse registerModel(RegisterModelRequest registerModelReq
         int maxBatchDelay = registerModelRequest.getMaxBatchDelay();
         int initialWorkers = registerModelRequest.getInitialWorkers();
         int responseTimeout = registerModelRequest.getResponseTimeout();
+        boolean s3SseKms = registerModelRequest.getS3SseKms();
         if (responseTimeout == -1) {
             responseTimeout = ConfigManager.getInstance().getDefaultResponseTimeout();
         }
@@ -141,7 +142,8 @@ public static StatusResponse registerModel(RegisterModelRequest registerModelReq
                             batchSize,
                             maxBatchDelay,
                             responseTimeout,
-                            null);
+                            null,
+                            s3SseKms);
         } catch (FileAlreadyExistsException e) {
             throw new InternalServerException(
                     "Model file already exists " + FilenameUtils.getName(modelUrl), e);

File: frontend/server/src/main/java/org/pytorch/serve/util/ApiUtils.java
Patch:
@@ -116,6 +116,7 @@ public static StatusResponse registerModel(RegisterModelRequest registerModelReq
         int maxBatchDelay = registerModelRequest.getMaxBatchDelay();
         int initialWorkers = registerModelRequest.getInitialWorkers();
         int responseTimeout = registerModelRequest.getResponseTimeout();
+        boolean s3SseKms = registerModelRequest.getS3SseKms();
         if (responseTimeout == -1) {
             responseTimeout = ConfigManager.getInstance().getDefaultResponseTimeout();
         }
@@ -141,7 +142,8 @@ public static StatusResponse registerModel(RegisterModelRequest registerModelReq
                             batchSize,
                             maxBatchDelay,
                             responseTimeout,
-                            null);
+                            null,
+                            s3SseKms);
         } catch (FileAlreadyExistsException e) {
             throw new InternalServerException(
                     "Model file already exists " + FilenameUtils.getName(modelUrl), e);

File: frontend/server/src/main/java/org/pytorch/serve/ServerInitializer.java
Patch:
@@ -7,6 +7,7 @@
 import io.netty.handler.codec.http.HttpServerCodec;
 import io.netty.handler.ssl.SslContext;
 import org.pytorch.serve.http.ApiDescriptionRequestHandler;
+import org.pytorch.serve.http.ExtendedSSLHandler;
 import org.pytorch.serve.http.HttpRequestHandler;
 import org.pytorch.serve.http.HttpRequestHandlerChain;
 import org.pytorch.serve.http.InferenceRequestHandler;
@@ -48,7 +49,7 @@ public void initChannel(Channel ch) {
 
         int maxRequestSize = ConfigManager.getInstance().getMaxRequestSize();
         if (sslCtx != null) {
-            pipeline.addLast("ssl", sslCtx.newHandler(ch.alloc()));
+            pipeline.addLast("ssl", new ExtendedSSLHandler(sslCtx, connectorType));
         }
         pipeline.addLast("http", new HttpServerCodec());
         pipeline.addLast("aggregator", new HttpObjectAggregator(maxRequestSize));

File: frontend/server/src/main/java/org/pytorch/serve/ServerInitializer.java
Patch:
@@ -7,6 +7,7 @@
 import io.netty.handler.codec.http.HttpServerCodec;
 import io.netty.handler.ssl.SslContext;
 import org.pytorch.serve.http.ApiDescriptionRequestHandler;
+import org.pytorch.serve.http.ExtendedSSLHandler;
 import org.pytorch.serve.http.HttpRequestHandler;
 import org.pytorch.serve.http.HttpRequestHandlerChain;
 import org.pytorch.serve.http.InferenceRequestHandler;
@@ -48,7 +49,7 @@ public void initChannel(Channel ch) {
 
         int maxRequestSize = ConfigManager.getInstance().getMaxRequestSize();
         if (sslCtx != null) {
-            pipeline.addLast("ssl", sslCtx.newHandler(ch.alloc()));
+            pipeline.addLast("ssl", new ExtendedSSLHandler(sslCtx, connectorType));
         }
         pipeline.addLast("http", new HttpServerCodec());
         pipeline.addLast("aggregator", new HttpObjectAggregator(maxRequestSize));

File: frontend/server/src/main/java/org/pytorch/serve/util/ConfigManager.java
Patch:
@@ -760,7 +760,7 @@ public boolean isSSLEnabled(ConnectorType connectorType) {
         return "https".equalsIgnoreCase(protocol);
     }
 
-    public int getIniitialWorkerPort() {
+    public int getInitialWorkerPort() {
         return Integer.parseInt(prop.getProperty(TS_INITIAL_WORKER_PORT, "9000"));
     }
 

File: frontend/server/src/main/java/org/pytorch/serve/ServerInitializer.java
Patch:
@@ -7,6 +7,7 @@
 import io.netty.handler.codec.http.HttpServerCodec;
 import io.netty.handler.ssl.SslContext;
 import org.pytorch.serve.http.ApiDescriptionRequestHandler;
+import org.pytorch.serve.http.ExtendedSSLHandler;
 import org.pytorch.serve.http.HttpRequestHandler;
 import org.pytorch.serve.http.HttpRequestHandlerChain;
 import org.pytorch.serve.http.InferenceRequestHandler;
@@ -48,7 +49,7 @@ public void initChannel(Channel ch) {
 
         int maxRequestSize = ConfigManager.getInstance().getMaxRequestSize();
         if (sslCtx != null) {
-            pipeline.addLast("ssl", sslCtx.newHandler(ch.alloc()));
+            pipeline.addLast("ssl", new ExtendedSSLHandler(sslCtx, connectorType));
         }
         pipeline.addLast("http", new HttpServerCodec());
         pipeline.addLast("aggregator", new HttpObjectAggregator(maxRequestSize));

File: frontend/server/src/main/java/org/pytorch/serve/ServerInitializer.java
Patch:
@@ -7,6 +7,7 @@
 import io.netty.handler.codec.http.HttpServerCodec;
 import io.netty.handler.ssl.SslContext;
 import org.pytorch.serve.http.ApiDescriptionRequestHandler;
+import org.pytorch.serve.http.ExtendedSSLHandler;
 import org.pytorch.serve.http.HttpRequestHandler;
 import org.pytorch.serve.http.HttpRequestHandlerChain;
 import org.pytorch.serve.http.InferenceRequestHandler;
@@ -48,7 +49,7 @@ public void initChannel(Channel ch) {
 
         int maxRequestSize = ConfigManager.getInstance().getMaxRequestSize();
         if (sslCtx != null) {
-            pipeline.addLast("ssl", sslCtx.newHandler(ch.alloc()));
+            pipeline.addLast("ssl", new ExtendedSSLHandler(sslCtx, connectorType));
         }
         pipeline.addLast("http", new HttpServerCodec());
         pipeline.addLast("aggregator", new HttpObjectAggregator(maxRequestSize));

File: frontend/server/src/main/java/org/pytorch/serve/ServerInitializer.java
Patch:
@@ -7,6 +7,7 @@
 import io.netty.handler.codec.http.HttpServerCodec;
 import io.netty.handler.ssl.SslContext;
 import org.pytorch.serve.http.ApiDescriptionRequestHandler;
+import org.pytorch.serve.http.ExtendedSSLHandler;
 import org.pytorch.serve.http.HttpRequestHandler;
 import org.pytorch.serve.http.HttpRequestHandlerChain;
 import org.pytorch.serve.http.InferenceRequestHandler;
@@ -48,7 +49,7 @@ public void initChannel(Channel ch) {
 
         int maxRequestSize = ConfigManager.getInstance().getMaxRequestSize();
         if (sslCtx != null) {
-            pipeline.addLast("ssl", sslCtx.newHandler(ch.alloc()));
+            pipeline.addLast("ssl", new ExtendedSSLHandler(sslCtx, connectorType));
         }
         pipeline.addLast("http", new HttpServerCodec());
         pipeline.addLast("aggregator", new HttpObjectAggregator(maxRequestSize));

File: frontend/server/src/main/java/org/pytorch/serve/ServerInitializer.java
Patch:
@@ -7,6 +7,7 @@
 import io.netty.handler.codec.http.HttpServerCodec;
 import io.netty.handler.ssl.SslContext;
 import org.pytorch.serve.http.ApiDescriptionRequestHandler;
+import org.pytorch.serve.http.ExtendedSSLHandler;
 import org.pytorch.serve.http.HttpRequestHandler;
 import org.pytorch.serve.http.HttpRequestHandlerChain;
 import org.pytorch.serve.http.InferenceRequestHandler;
@@ -48,7 +49,7 @@ public void initChannel(Channel ch) {
 
         int maxRequestSize = ConfigManager.getInstance().getMaxRequestSize();
         if (sslCtx != null) {
-            pipeline.addLast("ssl", sslCtx.newHandler(ch.alloc()));
+            pipeline.addLast("ssl", new ExtendedSSLHandler(sslCtx, connectorType));
         }
         pipeline.addLast("http", new HttpServerCodec());
         pipeline.addLast("aggregator", new HttpObjectAggregator(maxRequestSize));

File: frontend/server/src/main/java/org/pytorch/serve/ModelServer.java
Patch:
@@ -3,6 +3,7 @@
 import io.grpc.Server;
 import io.grpc.ServerBuilder;
 import io.grpc.ServerInterceptors;
+import io.grpc.netty.shaded.io.grpc.netty.NettyServerBuilder;
 import io.netty.bootstrap.ServerBootstrap;
 import io.netty.channel.ChannelFuture;
 import io.netty.channel.ChannelFutureListener;
@@ -381,7 +382,8 @@ public void startGRPCServers() throws IOException {
     private Server startGRPCServer(ConnectorType connectorType) throws IOException {
 
         ServerBuilder<?> s =
-                ServerBuilder.forPort(configManager.getGRPCPort(connectorType))
+                NettyServerBuilder.forPort(configManager.getGRPCPort(connectorType))
+                        .maxInboundMessageSize(configManager.getMaxRequestSize())
                         .addService(
                                 ServerInterceptors.intercept(
                                         GRPCServiceFactory.getgRPCService(connectorType),

File: frontend/server/src/test/java/org/pytorch/serve/SnapshotTest.java
Patch:
@@ -28,8 +28,8 @@
 import java.util.concurrent.CountDownLatch;
 import org.apache.commons.io.FileUtils;
 import org.pytorch.serve.servingsdk.impl.PluginsManager;
+import org.pytorch.serve.servingsdk.snapshot.Snapshot;
 import org.pytorch.serve.snapshot.InvalidSnapshotException;
-import org.pytorch.serve.snapshot.Snapshot;
 import org.pytorch.serve.util.ConfigManager;
 import org.pytorch.serve.util.ConnectorType;
 import org.pytorch.serve.wlm.Model;
@@ -399,6 +399,7 @@ private void validateSnapshot(String expectedSnapshot) {
         }
 
         updateSnapshot(actualProp);
+
         assert actualProp.equals(expectedProp);
     }
 

File: frontend/modelarchive/src/main/java/org/pytorch/serve/archive/ModelArchive.java
Patch:
@@ -52,7 +52,7 @@ public static ModelArchive downloadModel(
             String modelStore,
             String url)
             throws ModelException, FileAlreadyExistsException, IOException {
-        downloadModel(allowedUrls, modelStore, url, false);
+        return downloadModel(allowedUrls, modelStore, url, false);
     }
 
     public static ModelArchive downloadModel(

File: frontend/server/src/test/java/org/pytorch/serve/WorkflowTest.java
Patch:
@@ -446,8 +446,6 @@ public void testPredictionMemoryError() throws InterruptedException {
         TestUtils.getLatch().await();
 
         Assert.assertEquals(TestUtils.getHttpStatus(), HttpResponseStatus.INTERNAL_SERVER_ERROR);
-        ErrorResponse resp = JsonUtils.GSON.fromJson(TestUtils.getResult(), ErrorResponse.class);
-        Assert.assertEquals(resp.getMessage(), "pred-err -  Out of resources");
         channel.close().sync();
 
         // Unload the workflow

File: frontend/archive/src/main/java/org/pytorch/serve/archive/DownloadArchiveException.java
Patch:
@@ -1,6 +1,6 @@
 package org.pytorch.serve.archive;
 
-public class DownloadModelException extends ModelException {
+public class DownloadArchiveException extends Exception {
 
     private static final long serialVersionUID = 1L;
 
@@ -10,7 +10,7 @@ public class DownloadModelException extends ModelException {
      * @param message The detail message (which is saved for later retrieval by the {@link
      *     #getMessage()} method)
      */
-    public DownloadModelException(String message) {
+    public DownloadArchiveException(String message) {
         super(message);
     }
 
@@ -26,7 +26,7 @@ public DownloadModelException(String message) {
      *     method). (A null value is permitted, and indicates that the cause is nonexistent or
      *     unknown.)
      */
-    public DownloadModelException(String message, Throwable cause) {
+    public DownloadArchiveException(String message, Throwable cause) {
         super(message, cause);
     }
 }

File: frontend/archive/src/main/java/org/pytorch/serve/archive/model/InvalidModelException.java
Patch:
@@ -1,4 +1,4 @@
-package org.pytorch.serve.archive;
+package org.pytorch.serve.archive.model;
 
 public class InvalidModelException extends ModelException {
 

File: frontend/archive/src/main/java/org/pytorch/serve/archive/model/Manifest.java
Patch:
@@ -1,4 +1,4 @@
-package org.pytorch.serve.archive;
+package org.pytorch.serve.archive.model;
 
 import com.google.gson.annotations.SerializedName;
 

File: frontend/archive/src/main/java/org/pytorch/serve/archive/model/ModelException.java
Patch:
@@ -1,4 +1,4 @@
-package org.pytorch.serve.archive;
+package org.pytorch.serve.archive.model;
 
 public class ModelException extends Exception {
 

File: frontend/archive/src/main/java/org/pytorch/serve/archive/model/ModelNotFoundException.java
Patch:
@@ -1,4 +1,4 @@
-package org.pytorch.serve.archive;
+package org.pytorch.serve.archive.model;
 
 public class ModelNotFoundException extends ModelException {
 

File: frontend/archive/src/main/java/org/pytorch/serve/archive/model/ModelVersionNotFoundException.java
Patch:
@@ -1,4 +1,4 @@
-package org.pytorch.serve.archive;
+package org.pytorch.serve.archive.model;
 
 public class ModelVersionNotFoundException extends ModelException {
 

File: frontend/archive/src/main/java/org/pytorch/serve/archive/utils/HexUtils.java
Patch:
@@ -1,4 +1,4 @@
-package org.pytorch.serve.archive;
+package org.pytorch.serve.archive.utils;
 
 public final class HexUtils {
 

File: frontend/server/src/main/java/org/pytorch/serve/http/InvalidRequestHandler.java
Patch:
@@ -3,13 +3,13 @@
 import io.netty.channel.ChannelHandlerContext;
 import io.netty.handler.codec.http.FullHttpRequest;
 import io.netty.handler.codec.http.QueryStringDecoder;
-import org.pytorch.serve.archive.ModelException;
+import org.pytorch.serve.archive.model.ModelException;
 
 public class InvalidRequestHandler extends HttpRequestHandlerChain {
     public InvalidRequestHandler() {}
 
     @Override
-    protected void handleRequest(
+    public void handleRequest(
             ChannelHandlerContext ctx,
             FullHttpRequest req,
             QueryStringDecoder decoder,

File: frontend/server/src/main/java/org/pytorch/serve/http/messages/DescribeModelResponse.java
Patch:
@@ -1,4 +1,4 @@
-package org.pytorch.serve.http;
+package org.pytorch.serve.http.messages;
 
 import java.util.ArrayList;
 import java.util.Date;

File: frontend/server/src/main/java/org/pytorch/serve/ModelServer.java
Patch:
@@ -3,6 +3,7 @@
 import io.grpc.Server;
 import io.grpc.ServerBuilder;
 import io.grpc.ServerInterceptors;
+import io.grpc.netty.shaded.io.grpc.netty.NettyServerBuilder;
 import io.netty.bootstrap.ServerBootstrap;
 import io.netty.channel.ChannelFuture;
 import io.netty.channel.ChannelFutureListener;
@@ -381,7 +382,8 @@ public void startGRPCServers() throws IOException {
     private Server startGRPCServer(ConnectorType connectorType) throws IOException {
 
         ServerBuilder<?> s =
-                ServerBuilder.forPort(configManager.getGRPCPort(connectorType))
+                NettyServerBuilder.forPort(configManager.getGRPCPort(connectorType))
+                        .maxInboundMessageSize(configManager.getMaxRequestSize())
                         .addService(
                                 ServerInterceptors.intercept(
                                         GRPCServiceFactory.getgRPCService(connectorType),

File: frontend/server/src/test/java/org/pytorch/serve/SnapshotTest.java
Patch:
@@ -28,8 +28,8 @@
 import java.util.concurrent.CountDownLatch;
 import org.apache.commons.io.FileUtils;
 import org.pytorch.serve.servingsdk.impl.PluginsManager;
+import org.pytorch.serve.servingsdk.snapshot.Snapshot;
 import org.pytorch.serve.snapshot.InvalidSnapshotException;
-import org.pytorch.serve.snapshot.Snapshot;
 import org.pytorch.serve.util.ConfigManager;
 import org.pytorch.serve.util.ConnectorType;
 import org.pytorch.serve.wlm.Model;
@@ -399,6 +399,7 @@ private void validateSnapshot(String expectedSnapshot) {
         }
 
         updateSnapshot(actualProp);
+
         assert actualProp.equals(expectedProp);
     }
 

File: frontend/server/src/test/java/org/pytorch/serve/SnapshotTest.java
Patch:
@@ -28,8 +28,8 @@
 import java.util.concurrent.CountDownLatch;
 import org.apache.commons.io.FileUtils;
 import org.pytorch.serve.servingsdk.impl.PluginsManager;
+import org.pytorch.serve.servingsdk.snapshot.Snapshot;
 import org.pytorch.serve.snapshot.InvalidSnapshotException;
-import org.pytorch.serve.snapshot.Snapshot;
 import org.pytorch.serve.util.ConfigManager;
 import org.pytorch.serve.util.ConnectorType;
 import org.pytorch.serve.wlm.Model;
@@ -399,6 +399,7 @@ private void validateSnapshot(String expectedSnapshot) {
         }
 
         updateSnapshot(actualProp);
+
         assert actualProp.equals(expectedProp);
     }
 

File: frontend/server/src/main/java/org/pytorch/serve/ModelServer.java
Patch:
@@ -3,6 +3,7 @@
 import io.grpc.Server;
 import io.grpc.ServerBuilder;
 import io.grpc.ServerInterceptors;
+import io.grpc.netty.shaded.io.grpc.netty.NettyServerBuilder;
 import io.netty.bootstrap.ServerBootstrap;
 import io.netty.channel.ChannelFuture;
 import io.netty.channel.ChannelFutureListener;
@@ -381,7 +382,8 @@ public void startGRPCServers() throws IOException {
     private Server startGRPCServer(ConnectorType connectorType) throws IOException {
 
         ServerBuilder<?> s =
-                ServerBuilder.forPort(configManager.getGRPCPort(connectorType))
+                NettyServerBuilder.forPort(configManager.getGRPCPort(connectorType))
+                        .maxInboundMessageSize(configManager.getMaxRequestSize())
                         .addService(
                                 ServerInterceptors.intercept(
                                         GRPCServiceFactory.getgRPCService(connectorType),

File: frontend/server/src/test/java/org/pytorch/serve/SnapshotTest.java
Patch:
@@ -28,8 +28,8 @@
 import java.util.concurrent.CountDownLatch;
 import org.apache.commons.io.FileUtils;
 import org.pytorch.serve.servingsdk.impl.PluginsManager;
+import org.pytorch.serve.servingsdk.snapshot.Snapshot;
 import org.pytorch.serve.snapshot.InvalidSnapshotException;
-import org.pytorch.serve.snapshot.Snapshot;
 import org.pytorch.serve.util.ConfigManager;
 import org.pytorch.serve.util.ConnectorType;
 import org.pytorch.serve.wlm.Model;
@@ -399,6 +399,7 @@ private void validateSnapshot(String expectedSnapshot) {
         }
 
         updateSnapshot(actualProp);
+
         assert actualProp.equals(expectedProp);
     }
 

File: frontend/server/src/test/java/org/pytorch/serve/SnapshotTest.java
Patch:
@@ -28,8 +28,8 @@
 import java.util.concurrent.CountDownLatch;
 import org.apache.commons.io.FileUtils;
 import org.pytorch.serve.servingsdk.impl.PluginsManager;
+import org.pytorch.serve.servingsdk.snapshot.Snapshot;
 import org.pytorch.serve.snapshot.InvalidSnapshotException;
-import org.pytorch.serve.snapshot.Snapshot;
 import org.pytorch.serve.util.ConfigManager;
 import org.pytorch.serve.util.ConnectorType;
 import org.pytorch.serve.wlm.Model;
@@ -399,6 +399,7 @@ private void validateSnapshot(String expectedSnapshot) {
         }
 
         updateSnapshot(actualProp);
+
         assert actualProp.equals(expectedProp);
     }
 

File: frontend/server/src/test/java/org/pytorch/serve/SnapshotTest.java
Patch:
@@ -28,8 +28,8 @@
 import java.util.concurrent.CountDownLatch;
 import org.apache.commons.io.FileUtils;
 import org.pytorch.serve.servingsdk.impl.PluginsManager;
+import org.pytorch.serve.servingsdk.snapshot.Snapshot;
 import org.pytorch.serve.snapshot.InvalidSnapshotException;
-import org.pytorch.serve.snapshot.Snapshot;
 import org.pytorch.serve.util.ConfigManager;
 import org.pytorch.serve.util.ConnectorType;
 import org.pytorch.serve.wlm.Model;
@@ -399,6 +399,7 @@ private void validateSnapshot(String expectedSnapshot) {
         }
 
         updateSnapshot(actualProp);
+
         assert actualProp.equals(expectedProp);
     }
 

File: frontend/server/src/main/java/org/pytorch/serve/http/ExtendedSSLHandler.java
Patch:
@@ -6,14 +6,13 @@
 import io.netty.handler.ssl.OptionalSslHandler;
 import io.netty.handler.ssl.SslContext;
 import io.netty.handler.ssl.SslHandler;
+import java.util.List;
 import org.pytorch.serve.util.ConfigManager;
 import org.pytorch.serve.util.ConnectorType;
 import org.pytorch.serve.util.NettyUtils;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import java.util.List;
-
 public class ExtendedSSLHandler extends OptionalSslHandler {
     private static final Logger logger = LoggerFactory.getLogger(ExtendedSSLHandler.class);
     /**

File: frontend/server/src/main/java/org/pytorch/serve/http/ExtendedSSLHandler.java
Patch:
@@ -40,8 +40,8 @@ protected void decode(ChannelHandlerContext context, ByteBuf in, List<Object> ou
             logger.error("Recieved HTTP request!");
             NettyUtils.sendJsonResponse(
                     context,
-                    new StatusResponse("This TorchServe instance only accepts HTTPS requests"),
-                    HttpResponseStatus.FORBIDDEN);
+                    new StatusResponse("This TorchServe instance only accepts HTTPS requests",
+                            HttpResponseStatus.FORBIDDEN.code()));
         }
     }
-}
+}
\ No newline at end of file

File: frontend/server/src/test/java/org/pytorch/serve/SnapshotTest.java
Patch:
@@ -28,8 +28,8 @@
 import java.util.concurrent.CountDownLatch;
 import org.apache.commons.io.FileUtils;
 import org.pytorch.serve.servingsdk.impl.PluginsManager;
+import org.pytorch.serve.servingsdk.snapshot.Snapshot;
 import org.pytorch.serve.snapshot.InvalidSnapshotException;
-import org.pytorch.serve.snapshot.Snapshot;
 import org.pytorch.serve.util.ConfigManager;
 import org.pytorch.serve.util.ConnectorType;
 import org.pytorch.serve.wlm.Model;
@@ -399,6 +399,7 @@ private void validateSnapshot(String expectedSnapshot) {
         }
 
         updateSnapshot(actualProp);
+
         assert actualProp.equals(expectedProp);
     }
 

File: frontend/modelarchive/src/test/java/org/pytorch/serve/archive/ModelArchiveTest.java
Patch:
@@ -173,7 +173,7 @@ public void testNullModelstore() throws ModelException, IOException {
 
     @Test(
             expectedExceptions = ModelNotFoundException.class,
-            expectedExceptionsMessageRegExp = "Model not found in model store: noop1\\.mar")
+            expectedExceptionsMessageRegExp = "Model not found at: noop1.mar")
     public void testMarFileNotexist() throws ModelException, IOException {
         String modelStore = "src/test/resources/models";
         ModelArchive.downloadModel(ALLOWED_URLS_LIST, modelStore, "noop1.mar");

File: frontend/server/src/main/java/org/pytorch/serve/wlm/BatchAggregator.java
Patch:
@@ -1,8 +1,8 @@
 package org.pytorch.serve.wlm;
 
-import io.netty.handler.codec.http.HttpResponseStatus;
 import java.util.LinkedHashMap;
 import java.util.Map;
+import org.pytorch.serve.job.Job;
 import org.pytorch.serve.util.messages.BaseModelRequest;
 import org.pytorch.serve.util.messages.ModelInferenceRequest;
 import org.pytorch.serve.util.messages.ModelLoadModelRequest;
@@ -83,15 +83,15 @@ public void sendResponse(ModelWorkerResponse message) {
                 if (j == null) {
                     throw new IllegalStateException("Unexpected job: " + reqId);
                 }
-                j.sendError(HttpResponseStatus.valueOf(message.getCode()), message.getMessage());
+                j.sendError(message.getCode(), message.getMessage());
             }
             if (!jobs.isEmpty()) {
                 throw new IllegalStateException("Not all jobs get response.");
             }
         }
     }
 
-    public void sendError(BaseModelRequest message, String error, HttpResponseStatus status) {
+    public void sendError(BaseModelRequest message, String error, int status) {
         if (message instanceof ModelLoadModelRequest) {
             logger.warn("Load model failed: {}, error: {}", message.getModelName(), error);
             return;

File: frontend/server/src/main/java/org/pytorch/serve/wlm/Model.java
Patch:
@@ -12,6 +12,7 @@
 import java.util.concurrent.locks.ReentrantLock;
 import org.apache.commons.io.FilenameUtils;
 import org.pytorch.serve.archive.ModelArchive;
+import org.pytorch.serve.job.Job;
 import org.pytorch.serve.util.ConfigManager;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;

File: frontend/server/src/test/java/org/pytorch/serve/SnapshotTest.java
Patch:
@@ -63,7 +63,7 @@ public void beforeSuite()
         InternalLoggerFactory.setDefaultFactory(Slf4JLoggerFactory.INSTANCE);
         configManager.setIniitialWorkerPort(9500);
         server = new ModelServer(configManager);
-        server.start();
+        server.startRESTserver();
     }
 
     @AfterClass
@@ -264,7 +264,7 @@ public void testStartTorchServeWithLastSnapshot()
         ConfigManager.init(new ConfigManager.Arguments());
         configManager = ConfigManager.getInstance();
         server = new ModelServer(configManager);
-        server.start();
+        server.startRESTserver();
         Channel channel = null;
         for (int i = 0; i < 5; ++i) {
             channel = TestUtils.connect(ConnectorType.INFERENCE_CONNECTOR, configManager);
@@ -289,7 +289,7 @@ public void testRestartTorchServeWithSnapshotAsConfig()
         ConfigManager.init(new ConfigManager.Arguments());
         configManager = ConfigManager.getInstance();
         server = new ModelServer(configManager);
-        server.start();
+        server.startRESTserver();
         Channel channel = null;
         for (int i = 0; i < 5; ++i) {
             channel = TestUtils.connect(ConnectorType.INFERENCE_CONNECTOR, configManager);

File: frontend/modelarchive/src/test/java/org/pytorch/serve/archive/ModelArchiveTest.java
Patch:
@@ -173,7 +173,7 @@ public void testNullModelstore() throws ModelException, IOException {
 
     @Test(
             expectedExceptions = ModelNotFoundException.class,
-            expectedExceptionsMessageRegExp = "Model not found in model store: noop1\\.mar")
+            expectedExceptionsMessageRegExp = "Model not found at: noop1.mar")
     public void testMarFileNotexist() throws ModelException, IOException {
         String modelStore = "src/test/resources/models";
         ModelArchive.downloadModel(ALLOWED_URLS_LIST, modelStore, "noop1.mar");

File: frontend/server/src/main/java/org/pytorch/serve/ModelServer.java
Patch:
@@ -3,6 +3,7 @@
 import io.grpc.Server;
 import io.grpc.ServerBuilder;
 import io.grpc.ServerInterceptors;
+import io.grpc.netty.shaded.io.grpc.netty.NettyServerBuilder;
 import io.netty.bootstrap.ServerBootstrap;
 import io.netty.channel.ChannelFuture;
 import io.netty.channel.ChannelFutureListener;
@@ -381,7 +382,8 @@ public void startGRPCServers() throws IOException {
     private Server startGRPCServer(ConnectorType connectorType) throws IOException {
 
         ServerBuilder<?> s =
-                ServerBuilder.forPort(configManager.getGRPCPort(connectorType))
+                NettyServerBuilder.forPort(configManager.getGRPCPort(connectorType))
+                        .maxInboundMessageSize(configManager.getMaxRequestSize())
                         .addService(
                                 ServerInterceptors.intercept(
                                         GRPCServiceFactory.getgRPCService(connectorType),

File: frontend/server/src/main/java/org/pytorch/serve/util/ConfigManager.java
Patch:
@@ -286,9 +286,9 @@ public Connector getListener(ConnectorType connectorType) {
     public int getGRPCPort(ConnectorType connectorType) {
         String port;
         if (connectorType == ConnectorType.MANAGEMENT_CONNECTOR) {
-            port = prop.getProperty(TS_GRPC_MANAGEMENT_PORT, "9091");
+            port = prop.getProperty(TS_GRPC_MANAGEMENT_PORT, "7071");
         } else {
-            port = prop.getProperty(TS_GRPC_INFERENCE_PORT, "9090");
+            port = prop.getProperty(TS_GRPC_INFERENCE_PORT, "7070");
         }
         return Integer.parseInt(port);
     }

File: frontend/server/src/main/java/org/pytorch/serve/ServerInitializer.java
Patch:
@@ -49,7 +49,7 @@ public void initChannel(Channel ch) {
 
         int maxRequestSize = ConfigManager.getInstance().getMaxRequestSize();
         if (sslCtx != null) {
-            pipeline.addLast("ssl", new ExtendedSSLHandler(sslCtx));
+            pipeline.addLast("ssl", new ExtendedSSLHandler(sslCtx, connectorType));
         }
         pipeline.addLast("http", new HttpServerCodec());
         pipeline.addLast("aggregator", new HttpObjectAggregator(maxRequestSize));

File: frontend/server/src/main/java/org/pytorch/serve/ModelServer.java
Patch:
@@ -3,6 +3,7 @@
 import io.grpc.Server;
 import io.grpc.ServerBuilder;
 import io.grpc.ServerInterceptors;
+import io.grpc.netty.shaded.io.grpc.netty.NettyServerBuilder;
 import io.netty.bootstrap.ServerBootstrap;
 import io.netty.channel.ChannelFuture;
 import io.netty.channel.ChannelFutureListener;
@@ -392,7 +393,8 @@ public void startGRPCServers() throws IOException {
     private Server startGRPCServer(ConnectorType connectorType) throws IOException {
 
         ServerBuilder<?> s =
-                ServerBuilder.forPort(configManager.getGRPCPort(connectorType))
+                NettyServerBuilder.forPort(configManager.getGRPCPort(connectorType))
+                        .maxInboundMessageSize(configManager.getMaxRequestSize())
                         .addService(
                                 ServerInterceptors.intercept(
                                         GRPCServiceFactory.getgRPCService(connectorType),

File: frontend/server/src/main/java/org/pytorch/serve/ModelServer.java
Patch:
@@ -3,6 +3,7 @@
 import io.grpc.Server;
 import io.grpc.ServerBuilder;
 import io.grpc.ServerInterceptors;
+import io.grpc.netty.shaded.io.grpc.netty.NettyServerBuilder;
 import io.netty.bootstrap.ServerBootstrap;
 import io.netty.channel.ChannelFuture;
 import io.netty.channel.ChannelFutureListener;
@@ -381,7 +382,8 @@ public void startGRPCServers() throws IOException {
     private Server startGRPCServer(ConnectorType connectorType) throws IOException {
 
         ServerBuilder<?> s =
-                ServerBuilder.forPort(configManager.getGRPCPort(connectorType))
+                NettyServerBuilder.forPort(configManager.getGRPCPort(connectorType))
+                        .maxInboundMessageSize(configManager.getMaxRequestSize())
                         .addService(
                                 ServerInterceptors.intercept(
                                         GRPCServiceFactory.getgRPCService(connectorType),

File: frontend/server/src/main/java/org/pytorch/serve/job/RestJob.java
Patch:
@@ -109,7 +109,7 @@ public void sendError(int status, String error) {
             NettyUtils.sendError(
                     ctx, HttpResponseStatus.valueOf(status), new InternalServerException(error));
         } else if (responsePromise != null) {
-            responsePromise.complete(error.getBytes());
+            responsePromise.completeExceptionally(new InternalServerException(error));
         }
 
         logger.debug(

File: frontend/server/src/main/java/org/pytorch/serve/http/HttpRequestHandler.java
Patch:
@@ -9,6 +9,7 @@
 import org.pytorch.serve.archive.model.ModelException;
 import org.pytorch.serve.archive.model.ModelNotFoundException;
 import org.pytorch.serve.archive.model.ModelVersionNotFoundException;
+import org.pytorch.serve.archive.workflow.WorkflowNotFoundException;
 import org.pytorch.serve.util.NettyUtils;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -44,7 +45,8 @@ protected void channelRead0(ChannelHandlerContext ctx, FullHttpRequest req) {
             handlerChain.handleRequest(ctx, req, decoder, segments);
         } catch (ResourceNotFoundException
                 | ModelNotFoundException
-                | ModelVersionNotFoundException e) {
+                | ModelVersionNotFoundException
+                | WorkflowNotFoundException e) {
             logger.trace("", e);
             NettyUtils.sendError(ctx, HttpResponseStatus.NOT_FOUND, e);
         } catch (BadRequestException | ModelException | DownloadArchiveException e) {

File: frontend/server/src/main/java/org/pytorch/serve/http/HttpRequestHandlerChain.java
Patch:
@@ -12,6 +12,7 @@
 import org.pytorch.serve.archive.DownloadArchiveException;
 import org.pytorch.serve.archive.model.ModelException;
 import org.pytorch.serve.archive.model.ModelNotFoundException;
+import org.pytorch.serve.archive.workflow.WorkflowException;
 import org.pytorch.serve.servingsdk.ModelServerEndpoint;
 import org.pytorch.serve.servingsdk.ModelServerEndpointException;
 import org.pytorch.serve.servingsdk.impl.ModelServerContext;
@@ -43,7 +44,8 @@ public abstract void handleRequest(
             FullHttpRequest req,
             QueryStringDecoder decoder,
             String[] segments)
-            throws ModelNotFoundException, ModelException, DownloadArchiveException;
+            throws ModelNotFoundException, ModelException, DownloadArchiveException,
+                    WorkflowException;
 
     private void run(
             ModelServerEndpoint endpoint,

File: frontend/server/src/main/java/org/pytorch/serve/http/api/rest/ApiDescriptionRequestHandler.java
Patch:
@@ -6,6 +6,7 @@
 import io.netty.handler.codec.http.QueryStringDecoder;
 import org.pytorch.serve.archive.DownloadArchiveException;
 import org.pytorch.serve.archive.model.ModelException;
+import org.pytorch.serve.archive.workflow.WorkflowException;
 import org.pytorch.serve.http.HttpRequestHandlerChain;
 import org.pytorch.serve.http.MethodNotAllowedException;
 import org.pytorch.serve.openapi.OpenApiUtils;
@@ -26,7 +27,7 @@ public void handleRequest(
             FullHttpRequest req,
             QueryStringDecoder decoder,
             String[] segments)
-            throws ModelException, DownloadArchiveException {
+            throws ModelException, DownloadArchiveException, WorkflowException {
 
         if (isApiDescription(segments)) {
             String path = decoder.path();

File: frontend/server/src/main/java/org/pytorch/serve/http/api/rest/InferenceRequestHandler.java
Patch:
@@ -16,6 +16,7 @@
 import org.pytorch.serve.archive.model.ModelException;
 import org.pytorch.serve.archive.model.ModelNotFoundException;
 import org.pytorch.serve.archive.model.ModelVersionNotFoundException;
+import org.pytorch.serve.archive.workflow.WorkflowException;
 import org.pytorch.serve.http.BadRequestException;
 import org.pytorch.serve.http.HttpRequestHandlerChain;
 import org.pytorch.serve.http.ResourceNotFoundException;
@@ -52,7 +53,7 @@ public void handleRequest(
             FullHttpRequest req,
             QueryStringDecoder decoder,
             String[] segments)
-            throws ModelException, DownloadArchiveException {
+            throws ModelException, DownloadArchiveException, WorkflowException {
         if (isInferenceReq(segments)) {
             if (endpointMap.getOrDefault(segments[1], null) != null) {
                 handleCustomEndpoint(ctx, req, segments, decoder);

File: frontend/server/src/main/java/org/pytorch/serve/http/api/rest/ManagementRequestHandler.java
Patch:
@@ -16,6 +16,7 @@
 import org.pytorch.serve.archive.model.ModelException;
 import org.pytorch.serve.archive.model.ModelNotFoundException;
 import org.pytorch.serve.archive.model.ModelVersionNotFoundException;
+import org.pytorch.serve.archive.workflow.WorkflowException;
 import org.pytorch.serve.http.HttpRequestHandlerChain;
 import org.pytorch.serve.http.InternalServerException;
 import org.pytorch.serve.http.MethodNotAllowedException;
@@ -52,7 +53,7 @@ public void handleRequest(
             FullHttpRequest req,
             QueryStringDecoder decoder,
             String[] segments)
-            throws ModelException, DownloadArchiveException {
+            throws ModelException, DownloadArchiveException, WorkflowException {
         if (isManagementReq(segments)) {
             if (endpointMap.getOrDefault(segments[1], null) != null) {
                 handleCustomEndpoint(ctx, req, segments, decoder);

File: frontend/server/src/main/java/org/pytorch/serve/http/api/rest/PrometheusMetricsRequestHandler.java
Patch:
@@ -22,6 +22,7 @@
 import java.util.List;
 import org.pytorch.serve.archive.DownloadArchiveException;
 import org.pytorch.serve.archive.model.ModelException;
+import org.pytorch.serve.archive.workflow.WorkflowException;
 import org.pytorch.serve.http.HttpRequestHandlerChain;
 import org.pytorch.serve.util.NettyUtils;
 import org.slf4j.Logger;
@@ -43,7 +44,7 @@ public void handleRequest(
             FullHttpRequest req,
             QueryStringDecoder decoder,
             String[] segments)
-            throws ModelException, DownloadArchiveException {
+            throws ModelException, DownloadArchiveException, WorkflowException {
         if (segments.length >= 2 && "metrics".equals(segments[1])) {
             ByteBuf resBuf = Unpooled.directBuffer();
             List<String> params =

File: frontend/server/src/main/java/org/pytorch/serve/workflow/api/http/WorkflowInferenceRequestHandler.java
Patch:
@@ -12,6 +12,7 @@
 import java.util.Map;
 import org.pytorch.serve.archive.DownloadArchiveException;
 import org.pytorch.serve.archive.model.ModelException;
+import org.pytorch.serve.archive.workflow.WorkflowException;
 import org.pytorch.serve.http.BadRequestException;
 import org.pytorch.serve.http.HttpRequestHandlerChain;
 import org.pytorch.serve.http.ResourceNotFoundException;
@@ -42,7 +43,7 @@ public void handleRequest(
             FullHttpRequest req,
             QueryStringDecoder decoder,
             String[] segments)
-            throws ModelException, DownloadArchiveException {
+            throws ModelException, DownloadArchiveException, WorkflowException {
         if (segments.length < 3) {
             throw new ResourceNotFoundException();
         }

File: frontend/server/src/main/java/org/pytorch/serve/wlm/ModelManager.java
Patch:
@@ -122,7 +122,7 @@ public ModelArchive registerModel(
             manifest.getModel().setModelName(modelName);
             manifest.getModel().setHandler(handler);
             File f = new File(handler.split(":")[0]);
-            archive = new ModelArchive(manifest, url, f.getParentFile(), true);
+            archive = new ModelArchive(manifest, url, f.getAbsoluteFile().getParentFile(), true);
         } else {
             archive = createModelArchive(modelName, url, handler, runtime, defaultModelName);
         }

File: frontend/server/src/test/java/org/pytorch/serve/ModelServerTest.java
Patch:
@@ -930,7 +930,7 @@ public void testLoadModelFromFileURI() throws InterruptedException, IOException
         File destinationFile = new File(destination);
         String fileUrl = "";
         FileUtils.copyFile(sourceFile, destinationFile);
-        fileUrl = "file://" + parent + "/archive/mnist1.mar";
+        fileUrl = "file:///" + parent + "/archive/mnist1.mar";
         testLoadModel(fileUrl, "mnist1", "1.0");
         Assert.assertTrue(new File(configManager.getModelStore(), "mnist1.mar").exists());
         FileUtils.deleteQuietly(destinationFile);

File: frontend/archive/src/test/java/org/pytorch/serve/archive/model/ModelArchiveTest.java
Patch:
@@ -96,7 +96,7 @@ public void testLocalFile()
         File destinationFile = new File(destination);
         FileUtils.copyFile(sourceFile, destinationFile);
 
-        String fileUrl = "file://" + parent + "/archive/mnist1.mar";
+        String fileUrl = "file:///" + parent + "/archive/mnist1.mar";
         ModelArchive archive = ModelArchive.downloadModel(ALLOWED_URLS_LIST, modelStore, fileUrl);
 
         File modelLocation = new File(modelStore + "/mnist1.mar");

File: frontend/archive/src/test/java/org/pytorch/serve/archive/model/ModelArchiveTest.java
Patch:
@@ -176,7 +176,7 @@ public void testNullModelstore() throws ModelException, IOException, DownloadArc
 
     @Test(
             expectedExceptions = ModelNotFoundException.class,
-            expectedExceptionsMessageRegExp = "Model not found in model store: noop1\\.mar")
+            expectedExceptionsMessageRegExp = "Model not found at: noop1.mar")
     public void testMarFileNotexist() throws ModelException, IOException, DownloadArchiveException {
         String modelStore = "src/test/resources/models";
         ModelArchive.downloadModel(ALLOWED_URLS_LIST, modelStore, "noop1.mar");

File: frontend/modelarchive/src/test/java/org/pytorch/serve/archive/ModelArchiveTest.java
Patch:
@@ -173,7 +173,7 @@ public void testNullModelstore() throws ModelException, IOException {
 
     @Test(
             expectedExceptions = ModelNotFoundException.class,
-            expectedExceptionsMessageRegExp = "Model not found in model store: noop1\\.mar")
+            expectedExceptionsMessageRegExp = "Model not found at: noop1.mar")
     public void testMarFileNotexist() throws ModelException, IOException {
         String modelStore = "src/test/resources/models";
         ModelArchive.downloadModel(ALLOWED_URLS_LIST, modelStore, "noop1.mar");

File: frontend/server/src/main/java/org/pytorch/serve/util/ConfigManager.java
Patch:
@@ -285,9 +285,9 @@ public Connector getListener(ConnectorType connectorType) {
     public int getGRPCPort(ConnectorType connectorType) {
         String port;
         if (connectorType == ConnectorType.MANAGEMENT_CONNECTOR) {
-            port = prop.getProperty(TS_GRPC_MANAGEMENT_PORT, "9091");
+            port = prop.getProperty(TS_GRPC_MANAGEMENT_PORT, "7071");
         } else {
-            port = prop.getProperty(TS_GRPC_INFERENCE_PORT, "9090");
+            port = prop.getProperty(TS_GRPC_INFERENCE_PORT, "7070");
         }
         return Integer.parseInt(port);
     }

File: frontend/modelarchive/src/main/java/org/pytorch/serve/archive/ModelArchive.java
Patch:
@@ -56,7 +56,6 @@ public static ModelArchive downloadModel(
 
         String marFileName = FilenameUtils.getName(url);
         File modelLocation = new File(modelStore, marFileName);
-
         if (checkAllowedUrl(allowedUrls, url)) {
             if (modelLocation.exists()) {
                 throw new FileAlreadyExistsException(marFileName);

File: frontend/modelarchive/src/test/java/org/pytorch/serve/archive/ModelArchiveTest.java
Patch:
@@ -93,8 +93,9 @@ public void testLocalFile() throws ModelException, IOException, InterruptedExcep
         File destinationFile = new File(destination);
         FileUtils.copyFile(sourceFile, destinationFile);
 
-        String fileUrl = "file://" + parent + "/modelarchive/mnist1.mar";
+        String fileUrl = "file:///" + parent + "/modelarchive/mnist1.mar";
         ModelArchive archive = ModelArchive.downloadModel(ALLOWED_URLS_LIST, modelStore, fileUrl);
+
         File modelLocation = new File(modelStore + "/mnist1.mar");
         Assert.assertTrue(modelLocation.exists());
         ModelArchive.removeModel(modelStore, fileUrl);
@@ -191,6 +192,6 @@ public void testFileAlreadyExist() throws ModelException, IOException {
     public void testMalformLocalURL() throws ModelException, IOException, InterruptedException {
         String modelStore = "src/test/resources/models";
         ModelArchive.downloadModel(
-                ALLOWED_URLS_LIST, modelStore, "file://" + modelStore + "/mnist1.mar");
+                ALLOWED_URLS_LIST, modelStore, "file:///" + modelStore + "/mnist1.mar");
     }
 }

File: frontend/server/src/main/java/org/pytorch/serve/wlm/BatchAggregator.java
Patch:
@@ -1,8 +1,8 @@
 package org.pytorch.serve.wlm;
 
-import io.netty.handler.codec.http.HttpResponseStatus;
 import java.util.LinkedHashMap;
 import java.util.Map;
+import org.pytorch.serve.job.Job;
 import org.pytorch.serve.util.messages.BaseModelRequest;
 import org.pytorch.serve.util.messages.ModelInferenceRequest;
 import org.pytorch.serve.util.messages.ModelLoadModelRequest;
@@ -83,15 +83,15 @@ public void sendResponse(ModelWorkerResponse message) {
                 if (j == null) {
                     throw new IllegalStateException("Unexpected job: " + reqId);
                 }
-                j.sendError(HttpResponseStatus.valueOf(message.getCode()), message.getMessage());
+                j.sendError(message.getCode(), message.getMessage());
             }
             if (!jobs.isEmpty()) {
                 throw new IllegalStateException("Not all jobs get response.");
             }
         }
     }
 
-    public void sendError(BaseModelRequest message, String error, HttpResponseStatus status) {
+    public void sendError(BaseModelRequest message, String error, int status) {
         if (message instanceof ModelLoadModelRequest) {
             logger.warn("Load model failed: {}, error: {}", message.getModelName(), error);
             return;

File: frontend/server/src/main/java/org/pytorch/serve/wlm/Model.java
Patch:
@@ -12,6 +12,7 @@
 import java.util.concurrent.locks.ReentrantLock;
 import org.apache.commons.io.FilenameUtils;
 import org.pytorch.serve.archive.ModelArchive;
+import org.pytorch.serve.job.Job;
 import org.pytorch.serve.util.ConfigManager;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;

File: frontend/server/src/test/java/org/pytorch/serve/TestUtils.java
Patch:
@@ -236,7 +236,7 @@ public static Channel connect(ConnectorType connectorType, ConfigManager configM
 
     public static Channel connect(
             ConnectorType connectorType, ConfigManager configManager, int readTimeOut) {
-        Logger logger = LoggerFactory.getLogger(ModelServerTest.class);
+        Logger logger = LoggerFactory.getLogger(TestUtils.class);
 
         final Connector connector = configManager.getListener(connectorType);
         try {

File: frontend/server/src/main/java/org/pytorch/serve/wlm/BatchAggregator.java
Patch:
@@ -1,8 +1,8 @@
 package org.pytorch.serve.wlm;
 
-import io.netty.handler.codec.http.HttpResponseStatus;
 import java.util.LinkedHashMap;
 import java.util.Map;
+import org.pytorch.serve.job.Job;
 import org.pytorch.serve.util.messages.BaseModelRequest;
 import org.pytorch.serve.util.messages.ModelInferenceRequest;
 import org.pytorch.serve.util.messages.ModelLoadModelRequest;
@@ -83,15 +83,15 @@ public void sendResponse(ModelWorkerResponse message) {
                 if (j == null) {
                     throw new IllegalStateException("Unexpected job: " + reqId);
                 }
-                j.sendError(HttpResponseStatus.valueOf(message.getCode()), message.getMessage());
+                j.sendError(message.getCode(), message.getMessage());
             }
             if (!jobs.isEmpty()) {
                 throw new IllegalStateException("Not all jobs get response.");
             }
         }
     }
 
-    public void sendError(BaseModelRequest message, String error, HttpResponseStatus status) {
+    public void sendError(BaseModelRequest message, String error, int status) {
         if (message instanceof ModelLoadModelRequest) {
             logger.warn("Load model failed: {}, error: {}", message.getModelName(), error);
             return;

File: frontend/server/src/main/java/org/pytorch/serve/wlm/Model.java
Patch:
@@ -12,6 +12,7 @@
 import java.util.concurrent.locks.ReentrantLock;
 import org.apache.commons.io.FilenameUtils;
 import org.pytorch.serve.archive.ModelArchive;
+import org.pytorch.serve.job.Job;
 import org.pytorch.serve.util.ConfigManager;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;

File: frontend/server/src/test/java/org/pytorch/serve/ModelServerTest.java
Patch:
@@ -79,7 +79,7 @@ public void beforeSuite()
         InternalLoggerFactory.setDefaultFactory(Slf4JLoggerFactory.INSTANCE);
 
         server = new ModelServer(configManager);
-        server.start();
+        server.startRESTserver();
         String version = configManager.getProperty("version", null);
         try (InputStream is = new FileInputStream("src/test/resources/inference_open_api.json")) {
             listInferenceApisResult =

File: frontend/server/src/test/java/org/pytorch/serve/SnapshotTest.java
Patch:
@@ -63,7 +63,7 @@ public void beforeSuite()
         InternalLoggerFactory.setDefaultFactory(Slf4JLoggerFactory.INSTANCE);
         configManager.setIniitialWorkerPort(9500);
         server = new ModelServer(configManager);
-        server.start();
+        server.startRESTserver();
     }
 
     @AfterClass
@@ -264,7 +264,7 @@ public void testStartTorchServeWithLastSnapshot()
         ConfigManager.init(new ConfigManager.Arguments());
         configManager = ConfigManager.getInstance();
         server = new ModelServer(configManager);
-        server.start();
+        server.startRESTserver();
         Channel channel = null;
         for (int i = 0; i < 5; ++i) {
             channel = TestUtils.connect(ConnectorType.INFERENCE_CONNECTOR, configManager);
@@ -289,7 +289,7 @@ public void testRestartTorchServeWithSnapshotAsConfig()
         ConfigManager.init(new ConfigManager.Arguments());
         configManager = ConfigManager.getInstance();
         server = new ModelServer(configManager);
-        server.start();
+        server.startRESTserver();
         Channel channel = null;
         for (int i = 0; i < 5; ++i) {
             channel = TestUtils.connect(ConnectorType.INFERENCE_CONNECTOR, configManager);

File: frontend/server/src/main/java/org/pytorch/serve/util/ConfigManager.java
Patch:
@@ -712,6 +712,7 @@ public boolean isSSLEnabled() {
         String protocol = matcher.group(2);
 
         return "https".equalsIgnoreCase(protocol);
+    }
 
     public int getIniitialWorkerPort() {
         return Integer.parseInt(prop.getProperty(TS_INITIAL_WORKER_PORT, "9000"));

File: frontend/server/src/main/java/org/pytorch/serve/http/ManagementRequestHandler.java
Patch:
@@ -176,7 +176,9 @@ private DescribeModelResponse createModelResponse(
             boolean isRunning = worker.isRunning();
             int gpuId = worker.getGpuId();
             long memory = worker.getMemory();
-            resp.addWorker(workerId, startTime, isRunning, gpuId, memory);
+            int pid = worker.getPid();
+            String gpuUsage = worker.getGpuUsage();
+            resp.addWorker(workerId, startTime, isRunning, gpuId, memory, pid, gpuUsage);
         }
 
         return resp;

File: frontend/server/src/main/java/org/pytorch/serve/util/ConnectorType.java
Patch:
@@ -3,5 +3,6 @@
 public enum ConnectorType {
     INFERENCE_CONNECTOR,
     MANAGEMENT_CONNECTOR,
-    BOTH
+    METRICS_CONNECTOR,
+    ALL
 }

File: frontend/server/src/main/java/org/pytorch/serve/util/NettyUtils.java
Patch:
@@ -240,7 +240,7 @@ public static InputParameter getFormData(InterfaceHttpData data) {
             case Attribute:
                 Attribute attribute = (Attribute) data;
                 try {
-                    return new InputParameter(name, attribute.getValue());
+                    return new InputParameter(name, getBytes(attribute.getByteBuf()));
                 } catch (IOException e) {
                     throw new AssertionError(e);
                 }

File: serving-sdk/src/main/java/org/pytorch/serve/servingsdk/annotations/helpers/EndpointTypes.java
Patch:
@@ -5,5 +5,5 @@
  * Types of ModelServer endpoints
  */
 public enum EndpointTypes {
-    NONE, INFERENCE, MANAGEMENT;
+    NONE, INFERENCE, MANAGEMENT, METRIC;
 }

File: frontend/server/src/main/java/org/pytorch/serve/openapi/OpenApiUtils.java
Patch:
@@ -37,6 +37,7 @@ public static String listApis(ConnectorType type) {
     private static void listInferenceApis(OpenApi openApi) {
         openApi.addPath("/", getApiDescriptionPath("apiDescription", false));
         openApi.addPath("/ping", getPingPath());
+        openApi.addPath("/v1/models/{model_name}:predict", getPredictionsPath(false));
         openApi.addPath("/predictions/{model_name}", getPredictionsPath(false));
         openApi.addPath("/predictions/{model_name}/{model_version}", getPredictionsPath(true));
         openApi.addPath("/api-description", getApiDescriptionPath("api-description", true));
@@ -60,6 +61,7 @@ public static String getModelApi(Model model) {
         openApi.setInfo(info);
 
         openApi.addPath("/prediction/" + modelName, getModelPath(modelName));
+        openApi.addPath("/v1/models/{model_name}:predict", getModelPath(modelName));
 
         return JsonUtils.GSON_PRETTY.toJson(openApi);
     }

File: frontend/server/src/main/java/org/pytorch/serve/openapi/OpenApiUtils.java
Patch:
@@ -37,6 +37,7 @@ public static String listApis(ConnectorType type) {
     private static void listInferenceApis(OpenApi openApi) {
         openApi.addPath("/", getApiDescriptionPath("apiDescription", false));
         openApi.addPath("/ping", getPingPath());
+        openApi.addPath("/v1/models/{model_name}:predict", getPredictionsPath(false));
         openApi.addPath("/predictions/{model_name}", getPredictionsPath(false));
         openApi.addPath("/predictions/{model_name}/{model_version}", getPredictionsPath(true));
         openApi.addPath("/api-description", getApiDescriptionPath("api-description", true));
@@ -60,6 +61,7 @@ public static String getModelApi(Model model) {
         openApi.setInfo(info);
 
         openApi.addPath("/prediction/" + modelName, getModelPath(modelName));
+        openApi.addPath("/v1/models/{model_name}:predict", getModelPath(modelName));
 
         return JsonUtils.GSON_PRETTY.toJson(openApi);
     }

File: frontend/server/src/main/java/org/pytorch/serve/ensemble/Dag.java
Patch:
@@ -175,7 +175,6 @@ public ArrayList<NodeOutput> execute(RequestInput input, ArrayList<String> topoS
             }
         }
 
-        executorService.shutdown();
         return outputs;
     }
 

File: frontend/server/src/main/java/org/pytorch/serve/ensemble/Dag.java
Patch:
@@ -163,7 +163,8 @@ public ArrayList<NodeOutput> execute(RequestInput input, ArrayList<String> topoS
 
                 for (String newNodeName : dagMap.get(nodeName).get("outDegree")) {
                     List<InputParameter> params = new ArrayList<>();
-                    params.add(new InputParameter("body", output.getData().toString()));
+                    byte[] response = (byte[]) output.getData();
+                    params.add(new InputParameter("body", response));
                     input.setParameters(params);
                     nodes.get(newNodeName).updateInputDataMap("input", input);
                     inDegreeMap.replace(newNodeName, inDegreeMap.get(newNodeName) - 1);

File: frontend/server/src/main/java/org/pytorch/serve/ensemble/Node.java
Patch:
@@ -1,6 +1,5 @@
 package org.pytorch.serve.ensemble;
 
-import io.netty.handler.codec.http.FullHttpResponse;
 import java.util.HashMap;
 import java.util.Map;
 import java.util.concurrent.Callable;
@@ -75,7 +74,7 @@ public NodeOutput call() throws Exception {
     private NodeOutput invokeModel() {
         try {
             // TODO remove hard coding for model version
-            CompletableFuture<FullHttpResponse> respFuture = new CompletableFuture<>();
+            CompletableFuture<byte[]> respFuture = new CompletableFuture<>();
             RestJob job =
                     ApiUtils.addInferenceJob(
                             null,
@@ -84,7 +83,7 @@ private NodeOutput invokeModel() {
                             (RequestInput) inputDataMap.get("input"));
             job.setResponsePromise(respFuture);
             try {
-                FullHttpResponse resp = respFuture.get();
+                byte[] resp = respFuture.get();
 
                 return new NodeOutput(this.getName(), resp);
             } catch (InterruptedException | ExecutionException e) {

File: frontend/server/src/main/java/org/pytorch/serve/ensemble/Node.java
Patch:
@@ -80,7 +80,7 @@ private NodeOutput invokeModel() {
                     ApiUtils.addInferenceJob(
                             null,
                             workflowModel.getName(),
-                            "1.0",
+                            null,
                             (RequestInput) inputDataMap.get("input"));
             job.setResponsePromise(respFuture);
             try {

File: frontend/server/src/main/java/org/pytorch/serve/workflow/api/http/WorkflowMgmtRequestHandler.java
Patch:
@@ -133,8 +133,7 @@ private void handleRegisterWorkflows(
                                     registerWFRequest.getWorkflowUrl(),
                                     registerWFRequest.getResponseTimeout(),
                                     true);
-        } catch (Throwable e) {
-            e.printStackTrace();
+        } catch (ConflictStatusException e) {
             status.setHttpResponseCode(HttpURLConnection.HTTP_INTERNAL_ERROR);
             status.setStatus("Error while registering workflow. Details: " + e.getMessage());
             status.setE(e);

File: frontend/server/src/main/java/org/pytorch/serve/workflow/api/http/WorkflowInferenceRequestHandler.java
Patch:
@@ -5,6 +5,7 @@
 import io.netty.handler.codec.http.multipart.DefaultHttpDataFactory;
 import io.netty.handler.codec.http.multipart.HttpDataFactory;
 import io.netty.handler.codec.http.multipart.HttpPostRequestDecoder;
+import java.util.Map;
 import org.pytorch.serve.archive.DownloadArchiveException;
 import org.pytorch.serve.archive.model.ModelException;
 import org.pytorch.serve.http.BadRequestException;
@@ -18,8 +19,6 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import java.util.Map;
-
 /**
  * A class handling inbound HTTP requests to the workflow inference API.
  *

File: frontend/server/src/main/java/org/pytorch/serve/workflow/api/http/WorkflowMgmtRequestHandler.java
Patch:
@@ -133,7 +133,8 @@ private void handleRegisterWorkflows(
                                     registerWFRequest.getWorkflowUrl(),
                                     registerWFRequest.getResponseTimeout(),
                                     true);
-        } catch (ConflictStatusException e) {
+        } catch (Throwable e) {
+            e.printStackTrace();
             status.setHttpResponseCode(HttpURLConnection.HTTP_INTERNAL_ERROR);
             status.setStatus("Error while registering workflow. Details: " + e.getMessage());
             status.setE(e);

File: frontend/server/src/main/java/org/pytorch/serve/workflow/messages/RegisterWorkflowRequest.java
Patch:
@@ -16,7 +16,7 @@ public class RegisterWorkflowRequest {
 
     public RegisterWorkflowRequest(QueryStringDecoder decoder) {
         workflowName = NettyUtils.getParameter(decoder, "workflow_name", null);
-        responseTimeout = NettyUtils.getIntParameter(decoder, "response_timeout", -1);
+        responseTimeout = NettyUtils.getIntParameter(decoder, "response_timeout", 120);
         workflowUrl = NettyUtils.getParameter(decoder, "url", null);
     }
 

File: frontend/server/src/main/java/org/pytorch/serve/ensemble/Dag.java
Patch:
@@ -138,7 +138,7 @@ public ArrayList<NodeOutput> execute(RequestInput input, ArrayList<String> topoS
         Set<String> executing = new HashSet<>();
 
         for (String s : zeroInDegree) {
-            nodes.get(s).updateInputDataMap("start", "0");
+            nodes.get(s).updateInputDataMap("input", input);
         }
 
         ArrayList<NodeOutput> outputs = null;
@@ -164,7 +164,7 @@ public ArrayList<NodeOutput> execute(RequestInput input, ArrayList<String> topoS
                 }
 
                 for (String newNodeName : dagMap.get(nodeName).get("outDegree")) {
-                    nodes.get(newNodeName).updateInputDataMap(nodeName, output.getData());
+                    nodes.get(newNodeName).updateInputDataMap("input", output.getData());
                     inDegreeMap.replace(newNodeName, inDegreeMap.get(newNodeName) - 1);
                     if (inDegreeMap.get(newNodeName) == 0) {
                         zeroInDegree.add(newNodeName);

File: frontend/server/src/main/java/org/pytorch/serve/workflow/WorkflowManager.java
Patch:
@@ -237,4 +237,4 @@ public WorkFlow getWorkflow(String workflowName) {
     }
 
     public void predict(ChannelHandlerContext ctx, String wfName, RequestInput input) {}
-}
\ No newline at end of file
+}

File: frontend/server/src/main/java/org/pytorch/serve/workflow/messages/RegisterWorkflowRequest.java
Patch:
@@ -43,5 +43,4 @@ public String getWorkflowUrl() {
     public void setWorkflowUrl(String workflowUrl) {
         this.workflowUrl = workflowUrl;
     }
-
-}
\ No newline at end of file
+}

File: frontend/server/src/main/java/org/pytorch/serve/workflow/messages/RegisterWorkflowRequest.java
Patch:
@@ -44,4 +44,4 @@ public void setWorkflowUrl(String workflowUrl) {
         this.workflowUrl = workflowUrl;
     }
 
-}
+}
\ No newline at end of file

File: frontend/archive/src/main/java/org/pytorch/serve/archive/model/ModelArchive.java
Patch:
@@ -44,7 +44,6 @@ public static ModelArchive downloadModel(
 
         String marFileName = FilenameUtils.getName(url);
         File modelLocation = new File(modelStore, marFileName);
-
         try {
             ArchiveUtils.downloadArchive(allowedUrls, modelLocation, marFileName, url);
         } catch (InvalidArchiveURLException e) {

File: frontend/archive/src/test/java/org/pytorch/serve/archive/ModelArchiveTest.java
Patch:
@@ -101,6 +101,7 @@ public void testLocalFile()
 
         String fileUrl = "file://" + parent + "/archive/mnist1.mar";
         ModelArchive archive = ModelArchive.downloadModel(ALLOWED_URLS_LIST, modelStore, fileUrl);
+
         File modelLocation = new File(modelStore + "/mnist1.mar");
         Assert.assertTrue(modelLocation.exists());
         ModelArchive.removeModel(modelStore, fileUrl);
@@ -199,6 +200,6 @@ public void testMalformLocalURL()
             throws ModelException, IOException, InterruptedException, DownloadArchiveException {
         String modelStore = "src/test/resources/models";
         ModelArchive.downloadModel(
-                ALLOWED_URLS_LIST, modelStore, "file://" + modelStore + "/mnist1.mar");
+                ALLOWED_URLS_LIST, modelStore, "file:///" + modelStore + "/mnist1.mar");
     }
 }

File: serving-sdk/src/main/java/org/pytorch/serve/servingsdk/annotations/helpers/EndpointTypes.java
Patch:
@@ -5,5 +5,5 @@
  * Types of ModelServer endpoints
  */
 public enum EndpointTypes {
-    NONE, INFERENCE, MANAGEMENT;
+    NONE, INFERENCE, MANAGEMENT, METRIC;
 }

File: frontend/server/src/main/java/org/pytorch/serve/workflow/api/http/WorkflowMgmtRequestHandler.java
Patch:
@@ -154,7 +154,7 @@ private void handleRegisterWorkflows(
     }
 
     private void handleUnregisterWorkflow(ChannelHandlerContext ctx, String workflowName) {
-        WorkflowManager.getInstance().unregisterWorkflow(workflowName, null);
+        WorkflowManager.getInstance().unregisterWorkflow(workflowName);
         String msg = "Workflow \"" + workflowName + "\" unregistered";
         NettyUtils.sendJsonResponse(ctx, new StatusResponse(msg, HttpResponseStatus.OK.code()));
     }

File: frontend/server/src/main/java/org/pytorch/serve/workflow/api/http/WorkflowMgmtRequestHandler.java
Patch:
@@ -154,7 +154,7 @@ private void handleRegisterWorkflows(
     }
 
     private void handleUnregisterWorkflow(ChannelHandlerContext ctx, String workflowName) {
-        WorkflowManager.getInstance().unregisterWorkflow(workflowName);
+        WorkflowManager.getInstance().unregisterWorkflow(workflowName, null);
         String msg = "Workflow \"" + workflowName + "\" unregistered";
         NettyUtils.sendJsonResponse(ctx, new StatusResponse(msg, HttpResponseStatus.OK.code()));
     }

File: frontend/server/src/main/java/org/pytorch/serve/wlm/ModelManager.java
Patch:
@@ -133,9 +133,9 @@ public ModelArchive registerModel(
 
         try {
             createVersionedModel(tempModel, versionId);
-        } catch (ConflictStatusException E) {
+        } catch (ConflictStatusException e) {
             if (!ignoreDuplicate) {
-                throw E;
+                throw e;
             }
         }
 

File: frontend/server/src/main/java/org/pytorch/serve/workflow/WorkflowManager.java
Patch:
@@ -3,8 +3,9 @@
 import io.netty.channel.ChannelHandlerContext;
 import java.io.IOException;
 import java.net.HttpURLConnection;
+import java.util.ArrayList;
+import java.util.List;
 import java.util.Map;
-import java.util.Vector;
 import java.util.concurrent.ConcurrentHashMap;
 import java.util.concurrent.ExecutionException;
 import org.pytorch.serve.archive.DownloadArchiveException;
@@ -70,7 +71,7 @@ public StatusResponse registerWorkflow(
             WorkFlow workflow = createWorkflow(archive);
 
             Map<String, Node> nodes = workflow.getDag().getNodes();
-            Vector<StatusResponse> responses = new Vector<StatusResponse>();
+            List<StatusResponse> responses = new ArrayList<>();
             for (Map.Entry<String, Node> entry : nodes.entrySet()) {
                 Node node = entry.getValue();
                 WorkflowModel wfm = node.getWorkflowModel();

File: frontend/server/src/main/java/org/pytorch/serve/workflow/api/http/WorkflowInferenceRequestHandler.java
Patch:
@@ -56,6 +56,7 @@ public void handleRequest(
     private void handlePredictions(
             ChannelHandlerContext ctx, FullHttpRequest req, String[] segments) {
         RequestInput input = parseRequest(ctx, req);
+        logger.info(input.toString());
         String wfName = segments[2];
         if (wfName == null) {
             throw new BadRequestException("Parameter workflow_name is required.");

File: frontend/server/src/main/java/org/pytorch/serve/ensemble/Dag.java
Patch:
@@ -31,7 +31,7 @@ public void addNode(Node node) {
         dagMap.put(node.getName(), degreeMap);
     }
 
-    public boolean isNodeExist(Node node) {
+    public boolean checkNodeExist(Node node) {
         return nodes.containsKey(node.getName());
     }
 
@@ -40,10 +40,10 @@ public boolean hasEdgeTo(Node from, Node to) {
     }
 
     public void addEdge(Node from, Node to) throws InvalidDAGException {
-        if (!isNodeExist(from)) {
+        if (!checkNodeExist(from)) {
             addNode(from);
         }
-        if (!isNodeExist(to)) {
+        if (!checkNodeExist(to)) {
             addNode(to);
         }
 

File: frontend/server/src/main/java/org/pytorch/serve/workflow/api/http/WorkflowMgmtRequestHandler.java
Patch:
@@ -79,8 +79,7 @@ public void handleRequest(
     private boolean isManagementReq(String[] segments) {
         return segments.length == 0
                 || ((segments.length >= 2 && segments.length <= 4)
-                        && segments[1].equals("workflows"))
-                || endpointMap.containsKey(segments[1]);
+                        && segments[1].equals("workflows"));
     }
 
     private void handleListWorkflows(ChannelHandlerContext ctx, QueryStringDecoder decoder) {

File: frontend/server/src/test/java/org/pytorch/serve/ModelServerTest.java
Patch:
@@ -1320,7 +1320,7 @@ public void testRegisterModelMalformedUrl() throws InterruptedException {
 
         Assert.assertEquals(resp.getCode(), HttpResponseStatus.BAD_REQUEST.code());
         Assert.assertEquals(
-                resp.getMessage(), "Failed to download model from: http://localhost:aaaa");
+                resp.getMessage(), "Failed to download archive from: http://localhost:aaaa");
     }
 
     @Test(
@@ -1343,7 +1343,7 @@ public void testRegisterModelConnectionFailed() throws InterruptedException {
         Assert.assertEquals(resp.getCode(), HttpResponseStatus.BAD_REQUEST.code());
         Assert.assertEquals(
                 resp.getMessage(),
-                "Failed to download model from: http://localhost:18888/fake.mar");
+                "Failed to download archive from: http://localhost:18888/fake.mar");
     }
 
     @Test(
@@ -1366,7 +1366,7 @@ public void testRegisterModelHttpError() throws InterruptedException {
         Assert.assertEquals(resp.getCode(), HttpResponseStatus.BAD_REQUEST.code());
         Assert.assertEquals(
                 resp.getMessage(),
-                "Failed to download model from: https://localhost:8443/fake.mar");
+                "Failed to download archive from: https://localhost:8443/fake.mar");
     }
 
     @Test(

File: frontend/server/src/main/java/org/pytorch/serve/ensemble/WorkFlow.java
Patch:
@@ -42,8 +42,8 @@ public WorkFlow(WorkflowArchive workflowArchive)
                         this.workflowArchive.getManifest().getWorkflow().getHandler());
         this.models = new HashMap<String, WorkflowModel>();
         @SuppressWarnings("unchecked")
-        LinkedHashMap<String, Object> spec = (LinkedHashMap<String, Object>) this.readSpecFile(specFile);
-        this.workflowSpec = spec;
+        this.workflowSpec = (LinkedHashMap<String, Object>) this.readSpecFile(specFile);
+        
 
         @SuppressWarnings("unchecked")
         LinkedHashMap<String, Object> modelsInfo =
@@ -110,7 +110,7 @@ public WorkFlow(WorkflowArchive workflowArchive)
 
             @SuppressWarnings("unchecked")
             ArrayList<String> values = (ArrayList<String>) entry.getValue();
-            for (String toModelName : values ) {
+            for (String toModelName : values) {
                 WorkflowModel toWfm;
                 if (!models.containsKey(toModelName)) {
                     toWfm =

File: frontend/server/src/main/java/org/pytorch/serve/workflow/WorkflowManager.java
Patch:
@@ -133,6 +133,9 @@ public void unregisterWorkflow(String workflowName) {
                             })
                     .start();
         }
+
+        workflowMap.remove(workflowName);
+        WorkflowArchive.removeWorkflow(workflowName, workflow.getWorkflowArchive().getUrl());
     }
 
     public WorkFlow getWorkflow(String workflowName) {

File: frontend/server/src/main/java/org/pytorch/serve/ensemble/WorkFlow.java
Patch:
@@ -42,7 +42,8 @@ public WorkFlow(WorkflowArchive workflowArchive)
                         this.workflowArchive.getManifest().getWorkflow().getHandler());
         this.models = new HashMap<String, WorkflowModel>();
         @SuppressWarnings("unchecked")
-        LinkedHashMap<String, Object> spec = (LinkedHashMap<String, Object>) this.readSpecFile(specFile);
+        LinkedHashMap<String, Object> spec =
+                (LinkedHashMap<String, Object>) this.readSpecFile(specFile);
         this.workflowSpec = spec;
 
         @SuppressWarnings("unchecked")
@@ -110,7 +111,7 @@ public WorkFlow(WorkflowArchive workflowArchive)
 
             @SuppressWarnings("unchecked")
             ArrayList<String> values = (ArrayList<String>) entry.getValue();
-            for (String toModelName : values ) {
+            for (String toModelName : values) {
                 WorkflowModel toWfm;
                 if (!models.containsKey(toModelName)) {
                     toWfm =

File: frontend/server/src/main/java/org/pytorch/serve/workflow/WorkflowManager.java
Patch:
@@ -133,6 +133,9 @@ public void unregisterWorkflow(String workflowName) {
                             })
                     .start();
         }
+
+        workflowMap.remove(workflowName);
+        WorkflowArchive.removeWorkflow(workflowName, workflow.getWorkflowArchive().getUrl());
     }
 
     public WorkFlow getWorkflow(String workflowName) {

File: frontend/server/src/main/java/org/pytorch/serve/workflow/messages/RegisterWorkflowRequest.java
Patch:
@@ -43,5 +43,4 @@ public String getWorkflowUrl() {
     public void setWorkflowUrl(String workflowUrl) {
         this.workflowUrl = workflowUrl;
     }
-
 }

File: frontend/server/src/main/java/org/pytorch/serve/ensemble/WorkflowInt.java
Patch:
@@ -1,5 +1,5 @@
-// package org.pytorch.serve.ensemble;
-//
+package org.pytorch.serve.ensemble;
+
 // public interface WorkflowManager {
 //
 //    public Status registerWorkflow(File warFile);

File: frontend/server/src/main/java/org/pytorch/serve/wlm/ModelManager.java
Patch:
@@ -134,7 +134,9 @@ public ModelArchive registerModel(
         try {
             createVersionedModel(tempModel, versionId);
         } catch (ConflictStatusException E) {
-            if (!ignoreDuplicate) throw E;
+            if (!ignoreDuplicate) {
+                throw E;
+            }
         }
 
         logger.info("Model {} loaded.", tempModel.getModelName());

File: frontend/server/src/main/java/org/pytorch/serve/workflow/WorkflowManager.java
Patch:
@@ -23,7 +23,7 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-public class WorkflowManager {
+public final class WorkflowManager {
 
     private static final Logger logger = LoggerFactory.getLogger(WorkflowManager.class);
     private static WorkflowManager workflowManager;

File: frontend/server/src/test/java/org/pytorch/serve/ModelServerTest.java
Patch:
@@ -797,12 +797,12 @@ public void testLoadModelFromFileURI() throws InterruptedException, IOException
         String parent = curDirFile.getParent();
 
         String source = configManager.getModelStore() + "/mnist.mar";
-        String destination = parent + "/modelarchive/mnist1.mar";
+        String destination = parent + "/archive/mnist1.mar";
         File sourceFile = new File(source);
         File destinationFile = new File(destination);
         String fileUrl = "";
         FileUtils.copyFile(sourceFile, destinationFile);
-        fileUrl = "file://" + parent + "/modelarchive/mnist1.mar";
+        fileUrl = "file://" + parent + "/archive/mnist1.mar";
         testLoadModel(fileUrl, "mnist1", "1.0");
         Assert.assertTrue(new File(configManager.getModelStore(), "mnist1.mar").exists());
         FileUtils.deleteQuietly(destinationFile);

File: frontend/server/src/main/java/org/pytorch/serve/ensemble/InvalidDAGException.java
Patch:
@@ -1,6 +1,9 @@
 package org.pytorch.serve.ensemble;
 
 public class InvalidDAGException extends Exception {
+
+    private static final long serialVersionUID = 1L;
+
     public InvalidDAGException(String msg) {
         super(msg);
     }

File: frontend/server/src/main/java/org/pytorch/serve/workflow/messages/RegisterWorkflowRequest.java
Patch:
@@ -15,7 +15,7 @@ public class RegisterWorkflowRequest {
     private String workflowUrl;
 
     public RegisterWorkflowRequest(QueryStringDecoder decoder) {
-        workflowName = NettyUtils.getParameter(decoder, "name", null);
+        workflowName = NettyUtils.getParameter(decoder, "workflow_name", null);
         responseTimeout = NettyUtils.getIntParameter(decoder, "response_timeout", -1);
         workflowUrl = NettyUtils.getParameter(decoder, "url", null);
     }

File: frontend/archive/src/main/java/org/pytorch/serve/archive/model/ModelArchive.java
Patch:
@@ -48,8 +48,7 @@ public static ModelArchive downloadModel(
         try {
             ArchiveUtils.downloadArchive(allowedUrls, modelLocation, marFileName, url);
         } catch (InvalidArchiveURLException e) {
-            throw new ModelNotFoundException(
-                    "Given URL " + url + " does not match any allowed URL(s)");
+            throw new ModelNotFoundException(e.getMessage()); // NOPMD
         }
 
         if (url.contains("..")) {

File: frontend/archive/src/main/java/org/pytorch/serve/archive/utils/ArchiveUtils.java
Patch:
@@ -20,13 +20,13 @@
 
 public final class ArchiveUtils {
 
-    private ArchiveUtils() {}
-
     public static final Gson GSON = new GsonBuilder().setPrettyPrinting().create();
 
     private static final Pattern VALID_URL_PATTERN =
             Pattern.compile("file?://.*|http(s)?://.*", Pattern.CASE_INSENSITIVE);
 
+    private ArchiveUtils() {}
+
     public static <T> T readFile(File file, Class<T> type)
             throws InvalidModelException, IOException {
         try (Reader r =

File: frontend/server/src/main/java/org/pytorch/serve/ensemble/WorkflowInt.java
Patch:
@@ -1,5 +1,5 @@
-// package org.pytorch.serve.ensemble;
-//
+package org.pytorch.serve.ensemble;
+
 // public interface WorkflowManager {
 //
 //    public Status registerWorkflow(File warFile);

File: frontend/server/src/main/java/org/pytorch/serve/wlm/ModelManager.java
Patch:
@@ -134,7 +134,9 @@ public ModelArchive registerModel(
         try {
             createVersionedModel(tempModel, versionId);
         } catch (ConflictStatusException E) {
-            if (!ignoreDuplicate) throw E;
+            if (!ignoreDuplicate) {
+                throw E;
+            }
         }
 
         logger.info("Model {} loaded.", tempModel.getModelName());

File: frontend/server/src/main/java/org/pytorch/serve/workflow/WorkflowManager.java
Patch:
@@ -23,7 +23,7 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-public class WorkflowManager {
+public final class WorkflowManager {
 
     private static final Logger logger = LoggerFactory.getLogger(WorkflowManager.class);
     private static WorkflowManager workflowManager;

File: frontend/archive/src/main/java/org/pytorch/serve/archive/model/ModelArchive.java
Patch:
@@ -1,7 +1,6 @@
 package org.pytorch.serve.archive.model;
 
 import java.io.File;
-import java.io.FileNotFoundException;
 import java.io.IOException;
 import java.io.InputStream;
 import java.nio.file.FileAlreadyExistsException;
@@ -11,6 +10,7 @@
 import org.apache.commons.io.FilenameUtils;
 import org.pytorch.serve.archive.DownloadArchiveException;
 import org.pytorch.serve.archive.utils.ArchiveUtils;
+import org.pytorch.serve.archive.utils.InvalidArchiveURLException;
 import org.pytorch.serve.archive.utils.ZipUtils;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -47,7 +47,7 @@ public static ModelArchive downloadModel(
 
         try {
             ArchiveUtils.downloadArchive(allowedUrls, modelLocation, marFileName, url);
-        } catch (FileNotFoundException e) {
+        } catch (InvalidArchiveURLException e) {
             throw new ModelNotFoundException(
                     "Given URL " + url + " does not match any allowed URL(s)");
         }

File: frontend/archive/src/main/java/org/pytorch/serve/archive/workflow/WorkflowArchive.java
Patch:
@@ -4,7 +4,6 @@
 import com.google.gson.GsonBuilder;
 import com.google.gson.JsonParseException;
 import java.io.File;
-import java.io.FileNotFoundException;
 import java.io.IOException;
 import java.io.InputStream;
 import java.io.InputStreamReader;
@@ -17,6 +16,7 @@
 import org.apache.commons.io.FilenameUtils;
 import org.pytorch.serve.archive.DownloadArchiveException;
 import org.pytorch.serve.archive.utils.ArchiveUtils;
+import org.pytorch.serve.archive.utils.InvalidArchiveURLException;
 import org.pytorch.serve.archive.utils.ZipUtils;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -55,7 +55,7 @@ public static WorkflowArchive downloadWorkflow(
 
         try {
             ArchiveUtils.downloadArchive(allowedUrls, workflowLocation, warFileName, url);
-        } catch (FileNotFoundException e) {
+        } catch (InvalidArchiveURLException e) {
             throw new WorkflowNotFoundException(
                     "Given URL " + url + " does not match any allowed URL(s)");
         }

File: frontend/server/src/main/java/org/pytorch/serve/ensemble/WorkFlow.java
Patch:
@@ -4,7 +4,6 @@
 import java.nio.charset.StandardCharsets;
 import java.nio.file.Files;
 import java.util.*;
-import org.pytorch.serve.archive.model.InvalidModelException;
 import org.pytorch.serve.archive.workflow.InvalidWorkflowException;
 import org.pytorch.serve.archive.workflow.WorkflowArchive;
 import org.yaml.snakeyaml.Yaml;
@@ -24,7 +23,8 @@ public class WorkFlow {
     private Dag dag = new Dag();
     private File handlerFile;
 
-    public WorkFlow(WorkflowArchive workflowArchive) throws IOException, InvalidDAGException, InvalidWorkflowException {
+    public WorkFlow(WorkflowArchive workflowArchive)
+            throws IOException, InvalidDAGException, InvalidWorkflowException {
         this.workflowArchive = workflowArchive;
         File specFile =
                 new File(

File: frontend/server/src/main/java/org/pytorch/serve/ensemble/WorkFlow.java
Patch:
@@ -34,7 +34,7 @@ public class WorkFlow {
     private File specFile;
     private File handlerFile;
 
-    public WorkFlow(WorkflowArchive workflowArchive) throws Exception {
+    public WorkFlow(WorkflowArchive workflowArchive) throws InvalidModelException, IOException {
         this.workflowArchive = workflowArchive;
         this.specFile =
                 new File(

File: frontend/server/src/main/java/org/pytorch/serve/workflow/api/http/WorkflowMgmtRequestHandler.java
Patch:
@@ -91,6 +91,7 @@ private void handleRegisterWorkflows(
 
         try {
             RegisterWorkflowRequest registerWFRequest = parseRequest(req, decoder);
+
             status =
                     WorkflowManager.getInstance()
                             .registerWorkflow(

File: frontend/server/src/main/java/org/pytorch/serve/ensemble/WorkFlow.java
Patch:
@@ -75,7 +75,8 @@ public WorkFlow(WorkflowArchive workflowArchive) throws Exception {
             }
         }
 
-        LinkedHashMap<String, Object> dagInfo = (LinkedHashMap<String, Object>) this.workflowSpec.get("dag");
+        LinkedHashMap<String, Object> dagInfo =
+                (LinkedHashMap<String, Object>) this.workflowSpec.get("dag");
 
         for (Map.Entry<String, Object> entry : dagInfo.entrySet()) {
             String modelName = entry.getKey();
@@ -175,5 +176,4 @@ public void setBatchSizeDelay(int batchSizeDelay) {
     public String getWorkflowDag() {
         return this.workflowSpec.get("dag").toString();
     }
-
 }

File: frontend/server/src/main/java/org/pytorch/serve/workflow/WorkflowManager.java
Patch:
@@ -97,7 +97,7 @@ public ArrayList<DescribeWorkflowResponse> getWorkflowDescription(String wfName)
 
     public void unregisterWorkflow(String wfName) {}
 
-    public WorkFlow getWorkflow(String workflowName){
+    public WorkFlow getWorkflow(String workflowName) {
         return workflowMap.get(workflowName);
     }
 }

File: frontend/server/src/main/java/org/pytorch/serve/workflow/api/http/WorkflowMgmtRequestHandler.java
Patch:
@@ -146,7 +146,8 @@ private void sendResponse(ChannelHandlerContext ctx, StatusResponse statusRespon
         }
     }
 
-    private static DescribeWorkflowResponse createWorkflowResponse(String workflowName, WorkFlow workflow){
+    private static DescribeWorkflowResponse createWorkflowResponse(
+            String workflowName, WorkFlow workflow) {
         DescribeWorkflowResponse response = new DescribeWorkflowResponse();
         response.setWorkflowName(workflowName);
         response.setWorkflowUrl(workflow.getWorkflowArchive().getUrl());
@@ -157,5 +158,4 @@ private static DescribeWorkflowResponse createWorkflowResponse(String workflowNa
         response.setWorkflowDag(workflow.getWorkflowDag());
         return response;
     }
-
 }

File: frontend/server/src/main/java/org/pytorch/serve/workflow/WorkflowManager.java
Patch:
@@ -14,8 +14,8 @@
 import org.pytorch.serve.http.StatusResponse;
 import org.pytorch.serve.util.ApiUtils;
 import org.pytorch.serve.util.ConfigManager;
-import org.pytorch.serve.workflow.api.http.DescribeWorkflowResponse;
-import org.pytorch.serve.workflow.api.http.ListWorkflowResponse;
+import org.pytorch.serve.workflow.messages.DescribeWorkflowResponse;
+import org.pytorch.serve.workflow.messages.ListWorkflowResponse;
 
 public class WorkflowManager {
     private static WorkflowManager workflowManager;

File: frontend/server/src/main/java/org/pytorch/serve/workflow/api/http/DescribeWorkflowResponse.java
Patch:
@@ -1,3 +0,0 @@
-package org.pytorch.serve.workflow.api.http;
-
-public class DescribeWorkflowResponse {}

File: frontend/server/src/main/java/org/pytorch/serve/workflow/api/http/ListWorkflowResponse.java
Patch:
@@ -1,3 +0,0 @@
-package org.pytorch.serve.workflow.api.http;
-
-public class ListWorkflowResponse {}

File: frontend/server/src/main/java/org/pytorch/serve/workflow/messages/DescribeWorkflowResponse.java
Patch:
@@ -0,0 +1,3 @@
+package org.pytorch.serve.workflow.messages;
+
+public class DescribeWorkflowResponse {}

File: frontend/server/src/main/java/org/pytorch/serve/workflow/messages/ListWorkflowResponse.java
Patch:
@@ -0,0 +1,3 @@
+package org.pytorch.serve.workflow.messages;
+
+public class ListWorkflowResponse {}

File: frontend/server/src/main/java/org/pytorch/serve/workflow/messages/RegisterWorkflowRequest.java
Patch:
@@ -1,4 +1,4 @@
-package org.pytorch.serve.workflow.api.http;
+package org.pytorch.serve.workflow.messages;
 
 import com.google.gson.annotations.SerializedName;
 import io.netty.handler.codec.http.QueryStringDecoder;

File: frontend/modelarchive/src/main/java/org/pytorch/serve/archive/ModelArchive.java
Patch:
@@ -56,7 +56,6 @@ public static ModelArchive downloadModel(
 
         String marFileName = FilenameUtils.getName(url);
         File modelLocation = new File(modelStore, marFileName);
-
         if (checkAllowedUrl(allowedUrls, url)) {
             if (modelLocation.exists()) {
                 throw new FileAlreadyExistsException(marFileName);

File: frontend/server/src/main/java/org/pytorch/serve/wlm/ModelManager.java
Patch:
@@ -152,6 +152,8 @@ private ModelArchive createModelArchive(
             archive.getManifest().getModel().setHandler(configManager.getTsDefaultServiceHandler());
         }
 
+        archive.getManifest().getModel().setEnvelope(configManager.getTsServiceEnvelope());
+
         archive.validate();
 
         return archive;

File: frontend/server/src/main/java/org/pytorch/serve/wlm/ModelManager.java
Patch:
@@ -155,6 +155,8 @@ private ModelArchive createModelArchive(
             archive.getManifest().getModel().setHandler(configManager.getTsDefaultServiceHandler());
         }
 
+        archive.getManifest().getModel().setEnvelope(configManager.getTsServiceEnvelope());
+
         archive.validate();
 
         return archive;

File: frontend/modelarchive/src/main/java/org/pytorch/serve/archive/ModelArchive.java
Patch:
@@ -79,7 +79,7 @@ public static ModelArchive downloadModel(
 
         if (modelLocation.isFile()) {
             try (InputStream is = Files.newInputStream(modelLocation.toPath())) {
-                File unzipDir = unzip(is, null);
+                File unzipDir = unzip(is, null, "models");
                 return load(url, unzipDir, true);
             }
         }
@@ -136,9 +136,9 @@ private static <T> T readFile(File file, Class<T> type)
         }
     }
 
-    public static File unzip(InputStream is, String eTag) throws IOException {
+    public static File unzip(InputStream is, String eTag, String type) throws IOException {
         File tmpDir = FileUtils.getTempDirectory();
-        File modelDir = new File(tmpDir, "models");
+        File modelDir = new File(tmpDir, type);
         FileUtils.forceMkdir(modelDir);
 
         File tmp = File.createTempFile("model", ".download");

File: frontend/server/src/main/java/org/pytorch/serve/ModelServer.java
Patch:
@@ -228,7 +228,8 @@ private void initModelStore() throws InvalidSnapshotException, IOException {
                                 1,
                                 100,
                                 configManager.getDefaultResponseTimeout(),
-                                defaultModelName);
+                                defaultModelName,
+                                false);
                 modelManager.updateModel(
                         archive.getModelName(), archive.getModelVersion(), workers, workers, true);
                 startupModels.add(archive.getModelName());

File: frontend/server/src/main/java/org/pytorch/serve/ensemble/Dag.java
Patch:
@@ -16,7 +16,6 @@ public class Dag {
     CompletionService<NodeOutput> executorCompletionService= new ExecutorCompletionService<>(executorService);
     List<Future<Integer>> futures = new ArrayList<Future<Integer>>();
 
-
     public void addNode(Node<?> node){
         nodes.put(node.getName(), node);
         Map<String, Set<String>> degreeMap = new HashMap<>();
@@ -92,6 +91,9 @@ public Map<String, Integer> getOutDegreeMap(){
         return getDegreeMap("outDegree");
     }
 
+    public Map<String, Node<?>> getNodes() {
+        return nodes;
+    }
 
     public ArrayList<String> topoSort() throws Exception {
 
@@ -130,7 +132,6 @@ public ArrayList<String> topoSort() throws Exception {
                 zeroInDegree.remove(nodeName);
                 topoSortedList.add(nodeName);
 
-
                 for (String newNodeName : dagMap.get(nodeName).get("outDegree")) {
                     nodes.get(newNodeName).updateInputDataMap(nodeName, output.getData());
                     inDegreeMap.replace(newNodeName, inDegreeMap.get(newNodeName) - 1);

File: frontend/server/src/main/java/org/pytorch/serve/util/ApiUtils.java
Patch:
@@ -141,7 +141,8 @@ public static StatusResponse registerModel(RegisterModelRequest registerModelReq
                             batchSize,
                             maxBatchDelay,
                             responseTimeout,
-                            null);
+                            null,
+                            false);
         } catch (FileAlreadyExistsException e) {
             throw new InternalServerException(
                     "Model file already exists " + FilenameUtils.getName(modelUrl), e);

File: frontend/modelarchive/src/test/java/org/pytorch/serve/archive/ModelArchiveTest.java
Patch:
@@ -93,8 +93,9 @@ public void testLocalFile() throws ModelException, IOException, InterruptedExcep
         File destinationFile = new File(destination);
         FileUtils.copyFile(sourceFile, destinationFile);
 
-        String fileUrl = "file://" + parent + "/modelarchive/mnist1.mar";
+        String fileUrl = "file:///" + parent + "/modelarchive/mnist1.mar";
         ModelArchive archive = ModelArchive.downloadModel(ALLOWED_URLS_LIST, modelStore, fileUrl);
+
         File modelLocation = new File(modelStore + "/mnist1.mar");
         Assert.assertTrue(modelLocation.exists());
         ModelArchive.removeModel(modelStore, fileUrl);
@@ -191,6 +192,6 @@ public void testFileAlreadyExist() throws ModelException, IOException {
     public void testMalformLocalURL() throws ModelException, IOException, InterruptedException {
         String modelStore = "src/test/resources/models";
         ModelArchive.downloadModel(
-                ALLOWED_URLS_LIST, modelStore, "file://" + modelStore + "/mnist1.mar");
+                ALLOWED_URLS_LIST, modelStore, "file:///" + modelStore + "/mnist1.mar");
     }
 }

File: frontend/server/src/test/java/org/pytorch/serve/TestUtils.java
Patch:
@@ -236,7 +236,7 @@ public static Channel connect(ConnectorType connectorType, ConfigManager configM
 
     public static Channel connect(
             ConnectorType connectorType, ConfigManager configManager, int readTimeOut) {
-        Logger logger = LoggerFactory.getLogger(ModelServerTest.class);
+        Logger logger = LoggerFactory.getLogger(TestUtils.class);
 
         final Connector connector = configManager.getListener(connectorType);
         try {

File: serving-sdk/src/main/java/org/pytorch/serve/servingsdk/annotations/helpers/EndpointTypes.java
Patch:
@@ -5,5 +5,5 @@
  * Types of ModelServer endpoints
  */
 public enum EndpointTypes {
-    NONE, INFERENCE, MANAGEMENT;
+    NONE, INFERENCE, MANAGEMENT, METRIC;
 }

File: serving-sdk/src/main/java/org/pytorch/serve/servingsdk/annotations/helpers/EndpointTypes.java
Patch:
@@ -5,5 +5,5 @@
  * Types of ModelServer endpoints
  */
 public enum EndpointTypes {
-    NONE, INFERENCE, MANAGEMENT;
+    NONE, INFERENCE, MANAGEMENT, METRIC;
 }

File: frontend/server/src/main/java/org/pytorch/serve/http/HttpRequestHandlerChain.java
Patch:
@@ -37,7 +37,7 @@ public HttpRequestHandlerChain setNextHandler(HttpRequestHandlerChain nextHandle
         return chain;
     }
 
-    protected abstract void handleRequest(
+    public abstract void handleRequest(
             ChannelHandlerContext ctx,
             FullHttpRequest req,
             QueryStringDecoder decoder,

File: frontend/server/src/main/java/org/pytorch/serve/http/InvalidRequestHandler.java
Patch:
@@ -9,7 +9,7 @@ public class InvalidRequestHandler extends HttpRequestHandlerChain {
     public InvalidRequestHandler() {}
 
     @Override
-    protected void handleRequest(
+    public void handleRequest(
             ChannelHandlerContext ctx,
             FullHttpRequest req,
             QueryStringDecoder decoder,

File: frontend/server/src/main/java/org/pytorch/serve/http/api/rest/PrometheusMetricsRequestHandler.java
Patch:
@@ -1,4 +1,4 @@
-package org.pytorch.serve.http;
+package org.pytorch.serve.http.api.rest;
 
 import io.netty.buffer.ByteBuf;
 import io.netty.buffer.ByteBufOutputStream;
@@ -21,6 +21,7 @@
 import java.util.HashSet;
 import java.util.List;
 import org.pytorch.serve.archive.ModelException;
+import org.pytorch.serve.http.HttpRequestHandlerChain;
 import org.pytorch.serve.util.NettyUtils;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -36,7 +37,7 @@ public PrometheusMetricsRequestHandler() {
     }
 
     @Override
-    protected void handleRequest(
+    public void handleRequest(
             ChannelHandlerContext ctx,
             FullHttpRequest req,
             QueryStringDecoder decoder,

File: frontend/server/src/main/java/org/pytorch/serve/http/messages/DescribeModelResponse.java
Patch:
@@ -1,4 +1,4 @@
-package org.pytorch.serve.http;
+package org.pytorch.serve.http.messages;
 
 import java.util.ArrayList;
 import java.util.Date;

File: frontend/server/src/main/java/org/pytorch/serve/http/messages/ListModelsResponse.java
Patch:
@@ -1,4 +1,4 @@
-package org.pytorch.serve.http;
+package org.pytorch.serve.http.messages;
 
 import java.util.ArrayList;
 import java.util.List;

File: frontend/server/src/main/java/org/pytorch/serve/wlm/BatchAggregator.java
Patch:
@@ -1,8 +1,8 @@
 package org.pytorch.serve.wlm;
 
-import io.netty.handler.codec.http.HttpResponseStatus;
 import java.util.LinkedHashMap;
 import java.util.Map;
+import org.pytorch.serve.job.Job;
 import org.pytorch.serve.util.messages.BaseModelRequest;
 import org.pytorch.serve.util.messages.ModelInferenceRequest;
 import org.pytorch.serve.util.messages.ModelLoadModelRequest;
@@ -83,15 +83,15 @@ public void sendResponse(ModelWorkerResponse message) {
                 if (j == null) {
                     throw new IllegalStateException("Unexpected job: " + reqId);
                 }
-                j.sendError(HttpResponseStatus.valueOf(message.getCode()), message.getMessage());
+                j.sendError(message.getCode(), message.getMessage());
             }
             if (!jobs.isEmpty()) {
                 throw new IllegalStateException("Not all jobs get response.");
             }
         }
     }
 
-    public void sendError(BaseModelRequest message, String error, HttpResponseStatus status) {
+    public void sendError(BaseModelRequest message, String error, int status) {
         if (message instanceof ModelLoadModelRequest) {
             logger.warn("Load model failed: {}, error: {}", message.getModelName(), error);
             return;

File: frontend/server/src/main/java/org/pytorch/serve/wlm/Model.java
Patch:
@@ -12,6 +12,7 @@
 import java.util.concurrent.locks.ReentrantLock;
 import org.apache.commons.io.FilenameUtils;
 import org.pytorch.serve.archive.ModelArchive;
+import org.pytorch.serve.job.Job;
 import org.pytorch.serve.util.ConfigManager;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;

File: frontend/server/src/main/java/org/pytorch/serve/workflow/api/http/DescribeWorkflowResponse.java
Patch:
@@ -0,0 +1,3 @@
+package org.pytorch.serve.workflow.api.http;
+
+public class DescribeWorkflowResponse {}

File: frontend/server/src/main/java/org/pytorch/serve/workflow/api/http/ListWorkflowResponse.java
Patch:
@@ -0,0 +1,3 @@
+package org.pytorch.serve.workflow.api.http;
+
+public class ListWorkflowResponse {}

File: frontend/server/src/test/java/org/pytorch/serve/SnapshotTest.java
Patch:
@@ -61,7 +61,7 @@ public void beforeSuite()
 
         InternalLoggerFactory.setDefaultFactory(Slf4JLoggerFactory.INSTANCE);
         server = new ModelServer(configManager);
-        server.start();
+        server.startRESTserver();
     }
 
     @AfterClass
@@ -260,7 +260,7 @@ public void testStartTorchServeWithLastSnapshot()
         ConfigManager.init(new ConfigManager.Arguments());
         configManager = ConfigManager.getInstance();
         server = new ModelServer(configManager);
-        server.start();
+        server.startRESTserver();
         Channel channel = null;
         for (int i = 0; i < 5; ++i) {
             channel = TestUtils.connect(ConnectorType.INFERENCE_CONNECTOR, configManager);
@@ -285,7 +285,7 @@ public void testRestartTorchServeWithSnapshotAsConfig()
         ConfigManager.init(new ConfigManager.Arguments());
         configManager = ConfigManager.getInstance();
         server = new ModelServer(configManager);
-        server.start();
+        server.startRESTserver();
         Channel channel = null;
         for (int i = 0; i < 5; ++i) {
             channel = TestUtils.connect(ConnectorType.INFERENCE_CONNECTOR, configManager);

File: frontend/server/src/main/java/org/pytorch/serve/wlm/BatchAggregator.java
Patch:
@@ -1,8 +1,8 @@
 package org.pytorch.serve.wlm;
 
-import io.netty.handler.codec.http.HttpResponseStatus;
 import java.util.LinkedHashMap;
 import java.util.Map;
+import org.pytorch.serve.job.Job;
 import org.pytorch.serve.util.messages.BaseModelRequest;
 import org.pytorch.serve.util.messages.ModelInferenceRequest;
 import org.pytorch.serve.util.messages.ModelLoadModelRequest;
@@ -83,15 +83,15 @@ public void sendResponse(ModelWorkerResponse message) {
                 if (j == null) {
                     throw new IllegalStateException("Unexpected job: " + reqId);
                 }
-                j.sendError(HttpResponseStatus.valueOf(message.getCode()), message.getMessage());
+                j.sendError(message.getCode(), message.getMessage());
             }
             if (!jobs.isEmpty()) {
                 throw new IllegalStateException("Not all jobs get response.");
             }
         }
     }
 
-    public void sendError(BaseModelRequest message, String error, HttpResponseStatus status) {
+    public void sendError(BaseModelRequest message, String error, int status) {
         if (message instanceof ModelLoadModelRequest) {
             logger.warn("Load model failed: {}, error: {}", message.getModelName(), error);
             return;

File: frontend/server/src/main/java/org/pytorch/serve/wlm/Model.java
Patch:
@@ -12,6 +12,7 @@
 import java.util.concurrent.locks.ReentrantLock;
 import org.apache.commons.io.FilenameUtils;
 import org.pytorch.serve.archive.ModelArchive;
+import org.pytorch.serve.job.Job;
 import org.pytorch.serve.util.ConfigManager;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;

File: frontend/server/src/test/java/org/pytorch/serve/SnapshotTest.java
Patch:
@@ -61,7 +61,7 @@ public void beforeSuite()
 
         InternalLoggerFactory.setDefaultFactory(Slf4JLoggerFactory.INSTANCE);
         server = new ModelServer(configManager);
-        server.start();
+        server.startRESTserver();
     }
 
     @AfterClass
@@ -260,7 +260,7 @@ public void testStartTorchServeWithLastSnapshot()
         ConfigManager.init(new ConfigManager.Arguments());
         configManager = ConfigManager.getInstance();
         server = new ModelServer(configManager);
-        server.start();
+        server.startRESTserver();
         Channel channel = null;
         for (int i = 0; i < 5; ++i) {
             channel = TestUtils.connect(ConnectorType.INFERENCE_CONNECTOR, configManager);
@@ -285,7 +285,7 @@ public void testRestartTorchServeWithSnapshotAsConfig()
         ConfigManager.init(new ConfigManager.Arguments());
         configManager = ConfigManager.getInstance();
         server = new ModelServer(configManager);
-        server.start();
+        server.startRESTserver();
         Channel channel = null;
         for (int i = 0; i < 5; ++i) {
             channel = TestUtils.connect(ConnectorType.INFERENCE_CONNECTOR, configManager);

File: frontend/server/src/main/java/org/pytorch/serve/http/ManagementRequestHandler.java
Patch:
@@ -176,7 +176,9 @@ private DescribeModelResponse createModelResponse(
             boolean isRunning = worker.isRunning();
             int gpuId = worker.getGpuId();
             long memory = worker.getMemory();
-            resp.addWorker(workerId, startTime, isRunning, gpuId, memory);
+            int pid = worker.getPid();
+            String gpuUsage = worker.getGpuUsage();
+            resp.addWorker(workerId, startTime, isRunning, gpuId, memory, pid, gpuUsage);
         }
 
         return resp;

File: frontend/server/src/main/java/org/pytorch/serve/util/NettyUtils.java
Patch:
@@ -240,7 +240,7 @@ public static InputParameter getFormData(InterfaceHttpData data) {
             case Attribute:
                 Attribute attribute = (Attribute) data;
                 try {
-                    return new InputParameter(name, attribute.getValue());
+                    return new InputParameter(name, getBytes(attribute.getByteBuf()));
                 } catch (IOException e) {
                     throw new AssertionError(e);
                 }

File: frontend/server/src/main/java/org/pytorch/serve/http/ManagementRequestHandler.java
Patch:
@@ -176,7 +176,9 @@ private DescribeModelResponse createModelResponse(
             boolean isRunning = worker.isRunning();
             int gpuId = worker.getGpuId();
             long memory = worker.getMemory();
-            resp.addWorker(workerId, startTime, isRunning, gpuId, memory);
+            int pid = worker.getPid();
+            String gpuUsage = worker.getGpuUsage();
+            resp.addWorker(workerId, startTime, isRunning, gpuId, memory, pid, gpuUsage);
         }
 
         return resp;

File: frontend/server/src/main/java/org/pytorch/serve/util/NettyUtils.java
Patch:
@@ -240,7 +240,7 @@ public static InputParameter getFormData(InterfaceHttpData data) {
             case Attribute:
                 Attribute attribute = (Attribute) data;
                 try {
-                    return new InputParameter(name, attribute.getValue());
+                    return new InputParameter(name, getBytes(attribute.getByteBuf()));
                 } catch (IOException e) {
                     throw new AssertionError(e);
                 }

File: frontend/server/src/main/java/org/pytorch/serve/util/NettyUtils.java
Patch:
@@ -248,7 +248,7 @@ public static InputParameter getFormData(InterfaceHttpData data) {
             case Attribute:
                 Attribute attribute = (Attribute) data;
                 try {
-                    return new InputParameter(name, attribute.getValue());
+                    return new InputParameter(name, getBytes(attribute.getByteBuf()));
                 } catch (IOException e) {
                     throw new AssertionError(e);
                 }

File: frontend/server/src/main/java/org/pytorch/serve/wlm/WorkerThread.java
Patch:
@@ -9,11 +9,11 @@
 import io.netty.channel.ChannelPipeline;
 import io.netty.channel.EventLoopGroup;
 import io.netty.channel.SimpleChannelInboundHandler;
-import java.io.IOException;
-import java.net.HttpURLConnection;
 import java.io.BufferedReader;
+import java.io.IOException;
 import java.io.InputStream;
 import java.io.InputStreamReader;
+import java.net.HttpURLConnection;
 import java.net.SocketAddress;
 import java.nio.charset.StandardCharsets;
 import java.util.UUID;

File: frontend/server/src/main/java/org/pytorch/serve/http/InferenceRequestHandler.java
Patch:
@@ -14,6 +14,7 @@
 import org.pytorch.serve.archive.ModelException;
 import org.pytorch.serve.archive.ModelNotFoundException;
 import org.pytorch.serve.archive.ModelVersionNotFoundException;
+import org.pytorch.serve.metrics.api.MetricAggregator;
 import org.pytorch.serve.openapi.OpenApiUtils;
 import org.pytorch.serve.servingsdk.ModelServerEndpoint;
 import org.pytorch.serve.util.NettyUtils;
@@ -110,7 +111,6 @@ private void handlePredictions(
         if (segments.length == 4) {
             modelVersion = segments[3];
         }
-
         predict(ctx, req, null, segments[2], modelVersion);
     }
 
@@ -177,6 +177,7 @@ private void predict(
             return;
         }
 
+        MetricAggregator.handleInferenceMetric(modelName, modelVersion);
         Job job = new Job(ctx, modelName, modelVersion, WorkerCommands.PREDICT, input);
         if (!ModelManager.getInstance().addJob(job)) {
             String responseMessage =

File: frontend/server/src/main/java/org/pytorch/serve/util/ConnectorType.java
Patch:
@@ -3,5 +3,6 @@
 public enum ConnectorType {
     INFERENCE_CONNECTOR,
     MANAGEMENT_CONNECTOR,
-    BOTH
+    METRICS_CONNECTOR,
+    ALL
 }

File: frontend/server/src/main/java/org/pytorch/serve/wlm/ModelManager.java
Patch:
@@ -134,7 +134,9 @@ private ModelArchive createModelArchive(
             Manifest.RuntimeType runtime,
             String defaultModelName)
             throws FileAlreadyExistsException, ModelException, IOException {
-        ModelArchive archive = ModelArchive.downloadModel(configManager.getModelStore(), url);
+        ModelArchive archive =
+                ModelArchive.downloadModel(
+                        configManager.getAllowedUrls(), configManager.getModelStore(), url);
         if (modelName == null || modelName.isEmpty()) {
             if (archive.getModelName() == null || archive.getModelName().isEmpty()) {
                 archive.getManifest().getModel().setModelName(defaultModelName);

File: frontend/modelarchive/src/test/java/org/pytorch/serve/archive/ModelArchiveTest.java
Patch:
@@ -57,8 +57,7 @@ public void testAllowedMultiUrls() throws ModelException, IOException {
         final List<String> customUrlPatternList =
                 Arrays.asList(
                         "http(s)?://s3.amazonaws.com.*",
-                        "https://torchserve.pytorch.org/mar_files/.*",
-                        "https://www.dropbox.com/s/.*/.*");
+                        "https://torchserve.pytorch.org/mar_files/.*");
         ModelArchive.downloadModel(
                 customUrlPatternList,
                 modelStore,

File: frontend/server/src/main/java/org/pytorch/serve/http/ManagementRequestHandler.java
Patch:
@@ -176,7 +176,9 @@ private DescribeModelResponse createModelResponse(
             boolean isRunning = worker.isRunning();
             int gpuId = worker.getGpuId();
             long memory = worker.getMemory();
-            resp.addWorker(workerId, startTime, isRunning, gpuId, memory);
+            int pid = worker.getPid();
+            String gpuUsage = worker.getGpuUsage();
+            resp.addWorker(workerId, startTime, isRunning, gpuId, memory, pid, gpuUsage);
         }
 
         return resp;

File: frontend/server/src/test/java/org/pytorch/serve/ModelServerTest.java
Patch:
@@ -681,8 +681,6 @@ public void testPredictionsEchoNoMultipart()
 
         ByteBuffer allBytes = ByteBuffer.allocate(0x100);
         IntStream.range(0, 0x100).forEach(i -> allBytes.put((byte) i));
-        // https://github.com/netty/netty/issues/10284
-        allBytes.put(' ', (byte) 0);
 
         Charset charset = StandardCharsets.ISO_8859_1;
         HttpPostRequestEncoder.EncoderMode mode = HttpPostRequestEncoder.EncoderMode.RFC1738;

File: frontend/server/src/main/java/org/pytorch/serve/http/InferenceRequestHandler.java
Patch:
@@ -14,6 +14,7 @@
 import org.pytorch.serve.archive.ModelException;
 import org.pytorch.serve.archive.ModelNotFoundException;
 import org.pytorch.serve.archive.ModelVersionNotFoundException;
+import org.pytorch.serve.metrics.api.MetricAggregator;
 import org.pytorch.serve.openapi.OpenApiUtils;
 import org.pytorch.serve.servingsdk.ModelServerEndpoint;
 import org.pytorch.serve.util.NettyUtils;
@@ -110,7 +111,6 @@ private void handlePredictions(
         if (segments.length == 4) {
             modelVersion = segments[3];
         }
-
         predict(ctx, req, null, segments[2], modelVersion);
     }
 
@@ -177,6 +177,7 @@ private void predict(
             return;
         }
 
+        MetricAggregator.handleInferenceMetric(modelName, modelVersion);
         Job job = new Job(ctx, modelName, modelVersion, WorkerCommands.PREDICT, input);
         if (!ModelManager.getInstance().addJob(job)) {
             String responseMessage =

File: frontend/server/src/main/java/org/pytorch/serve/util/ConnectorType.java
Patch:
@@ -3,5 +3,6 @@
 public enum ConnectorType {
     INFERENCE_CONNECTOR,
     MANAGEMENT_CONNECTOR,
-    BOTH
+    METRICS_CONNECTOR,
+    ALL
 }

File: frontend/server/src/main/java/org/pytorch/serve/wlm/ModelManager.java
Patch:
@@ -134,7 +134,9 @@ private ModelArchive createModelArchive(
             Manifest.RuntimeType runtime,
             String defaultModelName)
             throws FileAlreadyExistsException, ModelException, IOException {
-        ModelArchive archive = ModelArchive.downloadModel(configManager.getModelStore(), url);
+        ModelArchive archive =
+                ModelArchive.downloadModel(
+                        configManager.getAllowedUrls(), configManager.getModelStore(), url);
         if (modelName == null || modelName.isEmpty()) {
             if (archive.getModelName() == null || archive.getModelName().isEmpty()) {
                 archive.getManifest().getModel().setModelName(defaultModelName);

File: frontend/server/src/test/java/org/pytorch/serve/ModelServerTest.java
Patch:
@@ -704,9 +704,7 @@ public void testModelRegisterWithDefaultWorkers()
             dependsOnMethods = {"testModelRegisterWithDefaultWorkers"})
     public void testLoadModelFromURL() throws InterruptedException {
         testLoadModel(
-                "https://torchserve.s3.amazonaws.com/mar_files/squeezenet1_1.mar",
-                "squeezenet",
-                "1.0");
+                "https://torchserve.pytorch.org/mar_files/squeezenet1_1.mar", "squeezenet", "1.0");
         Assert.assertTrue(new File(configManager.getModelStore(), "squeezenet1_1.mar").exists());
     }
 

File: frontend/server/src/main/java/org/pytorch/serve/ModelServer.java
Patch:
@@ -32,8 +32,8 @@
 import org.apache.commons.cli.ParseException;
 import org.pytorch.serve.archive.ModelArchive;
 import org.pytorch.serve.archive.ModelException;
+import org.pytorch.serve.grpcimpl.GRPCInterceptor;
 import org.pytorch.serve.grpcimpl.GRPCServiceFactory;
-import org.pytorch.serve.grpcimpl.gRPCInterceptor;
 import org.pytorch.serve.metrics.MetricManager;
 import org.pytorch.serve.servingsdk.ModelServerEndpoint;
 import org.pytorch.serve.servingsdk.annotations.Endpoint;
@@ -376,7 +376,7 @@ private Server startGRPCServer(ConnectorType connectorType) throws IOException {
                         .addService(
                                 ServerInterceptors.intercept(
                                         GRPCServiceFactory.getgRPCService(connectorType),
-                                        new gRPCInterceptor()));
+                                        new GRPCInterceptor()));
 
         if (configManager.isGRPCSSLEnabled()) {
             s.useTransportSecurity(

File: frontend/server/src/main/java/org/pytorch/serve/grpcimpl/GRPCInterceptor.java
Patch:
@@ -11,7 +11,7 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-public class gRPCInterceptor implements ServerInterceptor {
+public class GRPCInterceptor implements ServerInterceptor {
 
     private static final Logger logger = LoggerFactory.getLogger("ACCESS_LOG");
 

File: frontend/server/src/main/java/org/pytorch/serve/wlm/ModelManager.java
Patch:
@@ -130,7 +130,7 @@ private ModelArchive createModelArchive(
             String handler,
             Manifest.RuntimeType runtime,
             String defaultModelName)
-            throws FileAlreadyExistsException, ModelException, IOException {
+            throws ModelException, IOException {
         ModelArchive archive =
                 ModelArchive.downloadModel(
                         configManager.getAllowedUrls(), configManager.getModelStore(), url);

File: frontend/server/src/main/java/org/pytorch/serve/util/ApiUtils.java
Patch:
@@ -101,7 +101,8 @@ public static String setDefault(String modelName, String newModelVersion)
     }
 
     public static StatusResponse registerModel(RegisterModelRequest registerModelRequest)
-            throws ModelException, ExecutionException, InterruptedException {
+            throws ModelException, InternalServerException, ExecutionException,
+                    InterruptedException {
         String modelUrl = registerModelRequest.getModelUrl();
         if (modelUrl == null) {
             throw new BadRequestException("Parameter url is required.");

File: frontend/server/src/main/java/org/pytorch/serve/http/messages/RegisterModelRequest.java
Patch:
@@ -53,7 +53,7 @@ public RegisterModelRequest(QueryStringDecoder decoder) {
 
     public RegisterModelRequest(org.pytorch.serve.grpc.management.RegisterModelRequest request) {
         modelName = GRPCUtils.getRegisterParam(request.getModelName(), null);
-        runtime = GRPCUtils.getRegisterParam(request.getRuntime().name(), null);
+        runtime = GRPCUtils.getRegisterParam(request.getRuntime(), null);
         handler = GRPCUtils.getRegisterParam(request.getHandler(), null);
         batchSize = GRPCUtils.getRegisterParam(request.getBatchSize(), 1);
         maxBatchDelay = GRPCUtils.getRegisterParam(request.getMaxBatchDelay(), 100);

File: frontend/server/src/main/java/org/pytorch/serve/wlm/ModelManager.java
Patch:
@@ -134,7 +134,9 @@ private ModelArchive createModelArchive(
             Manifest.RuntimeType runtime,
             String defaultModelName)
             throws FileAlreadyExistsException, ModelException, IOException {
-        ModelArchive archive = ModelArchive.downloadModel(configManager.getModelStore(), url);
+        ModelArchive archive =
+                ModelArchive.downloadModel(
+                        configManager.getAllowedUrls(), configManager.getModelStore(), url);
         if (modelName == null || modelName.isEmpty()) {
             if (archive.getModelName() == null || archive.getModelName().isEmpty()) {
                 archive.getManifest().getModel().setModelName(defaultModelName);

File: frontend/modelarchive/src/main/java/org/pytorch/serve/archive/ModelArchive.java
Patch:
@@ -29,7 +29,7 @@ public class ModelArchive {
     public static final Gson GSON = new GsonBuilder().setPrettyPrinting().create();
 
     private static final Pattern URL_PATTERN =
-            Pattern.compile("http(s)?://.*", Pattern.CASE_INSENSITIVE);
+            Pattern.compile("file?://.*|http(s)?://.*", Pattern.CASE_INSENSITIVE);
 
     private static final String MANIFEST_FILE = "MANIFEST.json";
 

File: frontend/server/src/test/java/org/pytorch/serve/ModelServerTest.java
Patch:
@@ -1498,6 +1498,8 @@ public void testTSValidPort()
         FileUtils.deleteQuietly(new File(System.getProperty("LOG_LOCATION"), "config"));
         configManagerValidPort.setProperty("inference_address", "https://127.0.0.1:42523");
         configManagerValidPort.setProperty("metrics_address", "https://127.0.0.1:42524");
+        configManagerValidPort.setProperty("grpc_inference_port", "10010");
+        configManagerValidPort.setProperty("grpc_management_port", "10011");
         ModelServer serverValidPort = new ModelServer(configManagerValidPort);
         serverValidPort.start();
 

File: frontend/modelarchive/src/main/java/org/pytorch/serve/archive/ModelArchive.java
Patch:
@@ -29,7 +29,7 @@ public class ModelArchive {
     public static final Gson GSON = new GsonBuilder().setPrettyPrinting().create();
 
     private static final Pattern URL_PATTERN =
-            Pattern.compile("http(s)?://.*", Pattern.CASE_INSENSITIVE);
+            Pattern.compile("file?://.*|http(s)?://.*", Pattern.CASE_INSENSITIVE);
 
     private static final String MANIFEST_FILE = "MANIFEST.json";
 

File: frontend/server/src/main/java/org/pytorch/serve/http/InferenceRequestHandler.java
Patch:
@@ -14,6 +14,7 @@
 import org.pytorch.serve.archive.ModelException;
 import org.pytorch.serve.archive.ModelNotFoundException;
 import org.pytorch.serve.archive.ModelVersionNotFoundException;
+import org.pytorch.serve.metrics.api.MetricAggregator;
 import org.pytorch.serve.openapi.OpenApiUtils;
 import org.pytorch.serve.servingsdk.ModelServerEndpoint;
 import org.pytorch.serve.util.NettyUtils;
@@ -110,7 +111,6 @@ private void handlePredictions(
         if (segments.length == 4) {
             modelVersion = segments[3];
         }
-
         predict(ctx, req, null, segments[2], modelVersion);
     }
 
@@ -177,6 +177,7 @@ private void predict(
             return;
         }
 
+        MetricAggregator.handleInferenceMetric(modelName, modelVersion);
         Job job = new Job(ctx, modelName, modelVersion, WorkerCommands.PREDICT, input);
         if (!ModelManager.getInstance().addJob(job)) {
             String responseMessage =

File: frontend/server/src/main/java/org/pytorch/serve/util/ConnectorType.java
Patch:
@@ -3,5 +3,6 @@
 public enum ConnectorType {
     INFERENCE_CONNECTOR,
     MANAGEMENT_CONNECTOR,
-    BOTH
+    METRICS_CONNECTOR,
+    ALL
 }

File: frontend/server/src/main/java/org/pytorch/serve/ModelServer.java
Patch:
@@ -182,7 +182,7 @@ private void initModelStore() throws InvalidSnapshotException, IOException {
                                 workers,
                                 true);
                         startupModels.add(archive.getModelName());
-                    } catch (ModelException | IOException e) {
+                    } catch (ModelException | IOException | InterruptedException e) {
                         logger.warn("Failed to load model: " + file.getAbsolutePath(), e);
                     }
                 }
@@ -222,7 +222,7 @@ private void initModelStore() throws InvalidSnapshotException, IOException {
                 modelManager.updateModel(
                         archive.getModelName(), archive.getModelVersion(), workers, workers, true);
                 startupModels.add(archive.getModelName());
-            } catch (ModelException | IOException e) {
+            } catch (ModelException | IOException | InterruptedException e) {
                 logger.warn("Failed to load model: " + url, e);
             }
         }

File: frontend/server/src/main/java/org/pytorch/serve/snapshot/SnapshotManager.java
Patch:
@@ -135,7 +135,7 @@ private void initModels(Snapshot snapshot) {
 
         } catch (IOException e) {
             logger.error("Error while retrieving snapshot details. Details: {}", e.getMessage());
-        } catch (ModelException e) {
+        } catch (ModelException | InterruptedException e) {
             logger.error("Error while registering model. Details: {}", e.getMessage());
         }
     }

File: frontend/server/src/main/java/org/pytorch/serve/ModelServer.java
Patch:
@@ -182,7 +182,7 @@ private void initModelStore() throws InvalidSnapshotException, IOException {
                                 workers,
                                 true);
                         startupModels.add(archive.getModelName());
-                    } catch (ModelException | IOException e) {
+                    } catch (ModelException | IOException | InterruptedException e) {
                         logger.warn("Failed to load model: " + file.getAbsolutePath(), e);
                     }
                 }
@@ -222,7 +222,7 @@ private void initModelStore() throws InvalidSnapshotException, IOException {
                 modelManager.updateModel(
                         archive.getModelName(), archive.getModelVersion(), workers, workers, true);
                 startupModels.add(archive.getModelName());
-            } catch (ModelException | IOException e) {
+            } catch (ModelException | IOException | InterruptedException e) {
                 logger.warn("Failed to load model: " + url, e);
             }
         }

File: frontend/server/src/main/java/org/pytorch/serve/servingsdk/impl/ModelServerResponse.java
Patch:
@@ -22,7 +22,7 @@ public void setStatus(int i) {
 
     @Override
     public void setStatus(int i, String s) {
-        response.setStatus(HttpResponseStatus.valueOf(i, s));
+        response.setStatus(new HttpResponseStatus(i, s));
     }
 
     @Override

File: frontend/server/src/main/java/org/pytorch/serve/snapshot/SnapshotManager.java
Patch:
@@ -135,7 +135,7 @@ private void initModels(Snapshot snapshot) {
 
         } catch (IOException e) {
             logger.error("Error while retrieving snapshot details. Details: {}", e.getMessage());
-        } catch (ModelException e) {
+        } catch (ModelException | InterruptedException e) {
             logger.error("Error while registering model. Details: {}", e.getMessage());
         }
     }

File: frontend/server/src/main/java/org/pytorch/serve/wlm/Job.java
Patch:
@@ -79,7 +79,7 @@ public void response(
         HttpResponseStatus status =
                 (statusPhrase == null)
                         ? HttpResponseStatus.valueOf(statusCode)
-                        : HttpResponseStatus.valueOf(statusCode, statusPhrase);
+                        : new HttpResponseStatus(statusCode, statusPhrase);
         FullHttpResponse resp = new DefaultFullHttpResponse(HttpVersion.HTTP_1_1, status, false);
 
         if (contentType != null && contentType.length() > 0) {

File: frontend/modelarchive/src/main/java/org/pytorch/serve/archive/ModelArchive.java
Patch:
@@ -111,7 +111,6 @@ private static ModelArchive load(String url, File dir, boolean extracted)
             if (manifestFile.exists()) {
                 manifest = readFile(manifestFile, Manifest.class);
             } else {
-
                 manifest = new Manifest();
             }
 

File: frontend/server/src/main/java/org/pytorch/serve/ModelServer.java
Patch:
@@ -182,7 +182,7 @@ private void initModelStore() throws InvalidSnapshotException, IOException {
                                 workers,
                                 true);
                         startupModels.add(archive.getModelName());
-                    } catch (ModelException | IOException e) {
+                    } catch (ModelException | IOException | InterruptedException e) {
                         logger.warn("Failed to load model: " + file.getAbsolutePath(), e);
                     }
                 }
@@ -222,7 +222,7 @@ private void initModelStore() throws InvalidSnapshotException, IOException {
                 modelManager.updateModel(
                         archive.getModelName(), archive.getModelVersion(), workers, workers, true);
                 startupModels.add(archive.getModelName());
-            } catch (ModelException | IOException e) {
+            } catch (ModelException | IOException | InterruptedException e) {
                 logger.warn("Failed to load model: " + url, e);
             }
         }

File: frontend/server/src/main/java/org/pytorch/serve/http/ManagementRequestHandler.java
Patch:
@@ -228,7 +228,7 @@ private void handleRegisterModel(
         } catch (FileAlreadyExistsException e) {
             throw new InternalServerException(
                     "Model file already exists " + FilenameUtils.getName(modelUrl), e);
-        } catch (IOException e) {
+        } catch (IOException | InterruptedException e) {
             throw new InternalServerException("Failed to save model: " + modelUrl, e);
         }
 

File: frontend/server/src/main/java/org/pytorch/serve/snapshot/SnapshotManager.java
Patch:
@@ -135,7 +135,7 @@ private void initModels(Snapshot snapshot) {
 
         } catch (IOException e) {
             logger.error("Error while retrieving snapshot details. Details: {}", e.getMessage());
-        } catch (ModelException e) {
+        } catch (ModelException | InterruptedException e) {
             logger.error("Error while registering model. Details: {}", e.getMessage());
         }
     }

File: frontend/modelarchive/src/main/java/org/pytorch/serve/archive/ModelArchive.java
Patch:
@@ -93,7 +93,6 @@ private static ModelArchive load(String url, File dir, boolean extracted)
             if (manifestFile.exists()) {
                 manifest = readFile(manifestFile, Manifest.class);
             } else {
-
                 manifest = new Manifest();
             }
 

File: frontend/server/src/main/java/org/pytorch/serve/ModelServer.java
Patch:
@@ -182,7 +182,7 @@ private void initModelStore() throws InvalidSnapshotException, IOException {
                                 workers,
                                 true);
                         startupModels.add(archive.getModelName());
-                    } catch (ModelException | IOException e) {
+                    } catch (ModelException | IOException | InterruptedException e) {
                         logger.warn("Failed to load model: " + file.getAbsolutePath(), e);
                     }
                 }
@@ -222,7 +222,7 @@ private void initModelStore() throws InvalidSnapshotException, IOException {
                 modelManager.updateModel(
                         archive.getModelName(), archive.getModelVersion(), workers, workers, true);
                 startupModels.add(archive.getModelName());
-            } catch (ModelException | IOException e) {
+            } catch (ModelException | IOException | InterruptedException e) {
                 logger.warn("Failed to load model: " + url, e);
             }
         }

File: frontend/server/src/main/java/org/pytorch/serve/http/ManagementRequestHandler.java
Patch:
@@ -228,7 +228,7 @@ private void handleRegisterModel(
         } catch (FileAlreadyExistsException e) {
             throw new InternalServerException(
                     "Model file already exists " + FilenameUtils.getName(modelUrl), e);
-        } catch (IOException e) {
+        } catch (IOException | InterruptedException e) {
             throw new InternalServerException("Failed to save model: " + modelUrl, e);
         }
 

File: frontend/server/src/main/java/org/pytorch/serve/snapshot/SnapshotManager.java
Patch:
@@ -135,7 +135,7 @@ private void initModels(Snapshot snapshot) {
 
         } catch (IOException e) {
             logger.error("Error while retrieving snapshot details. Details: {}", e.getMessage());
-        } catch (ModelException e) {
+        } catch (ModelException | InterruptedException e) {
             logger.error("Error while registering model. Details: {}", e.getMessage());
         }
     }

File: frontend/modelarchive/src/main/java/org/pytorch/serve/archive/ModelArchive.java
Patch:
@@ -93,7 +93,6 @@ private static ModelArchive load(String url, File dir, boolean extracted)
             if (manifestFile.exists()) {
                 manifest = readFile(manifestFile, Manifest.class);
             } else {
-
                 manifest = new Manifest();
             }
 

File: frontend/server/src/main/java/org/pytorch/serve/ModelServer.java
Patch:
@@ -182,7 +182,7 @@ private void initModelStore() throws InvalidSnapshotException, IOException {
                                 workers,
                                 true);
                         startupModels.add(archive.getModelName());
-                    } catch (ModelException | IOException e) {
+                    } catch (ModelException | IOException | InterruptedException e) {
                         logger.warn("Failed to load model: " + file.getAbsolutePath(), e);
                     }
                 }
@@ -222,7 +222,7 @@ private void initModelStore() throws InvalidSnapshotException, IOException {
                 modelManager.updateModel(
                         archive.getModelName(), archive.getModelVersion(), workers, workers, true);
                 startupModels.add(archive.getModelName());
-            } catch (ModelException | IOException e) {
+            } catch (ModelException | IOException | InterruptedException e) {
                 logger.warn("Failed to load model: " + url, e);
             }
         }

File: frontend/server/src/main/java/org/pytorch/serve/http/ManagementRequestHandler.java
Patch:
@@ -228,7 +228,7 @@ private void handleRegisterModel(
         } catch (FileAlreadyExistsException e) {
             throw new InternalServerException(
                     "Model file already exists " + FilenameUtils.getName(modelUrl), e);
-        } catch (IOException e) {
+        } catch (IOException | InterruptedException e) {
             throw new InternalServerException("Failed to save model: " + modelUrl, e);
         }
 

File: frontend/server/src/main/java/org/pytorch/serve/snapshot/SnapshotManager.java
Patch:
@@ -135,7 +135,7 @@ private void initModels(Snapshot snapshot) {
 
         } catch (IOException e) {
             logger.error("Error while retrieving snapshot details. Details: {}", e.getMessage());
-        } catch (ModelException e) {
+        } catch (ModelException | InterruptedException e) {
             logger.error("Error while registering model. Details: {}", e.getMessage());
         }
     }

File: frontend/server/src/main/java/org/pytorch/serve/util/ConfigManager.java
Patch:
@@ -309,7 +309,7 @@ public int getDefaultWorkers() {
         if (workers == 0) {
             workers = Runtime.getRuntime().availableProcessors();
         }
-        setProperty("default_workers_per_model", Integer.toString(workers));
+
         return workers;
     }
 

File: frontend/server/src/main/java/org/pytorch/serve/servingsdk/impl/ModelServerResponse.java
Patch:
@@ -22,7 +22,7 @@ public void setStatus(int i) {
 
     @Override
     public void setStatus(int i, String s) {
-        response.setStatus(HttpResponseStatus.valueOf(i, s));
+        response.setStatus(new HttpResponseStatus(i, s));
     }
 
     @Override

File: frontend/server/src/main/java/org/pytorch/serve/snapshot/FSSnapshotSerializer.java
Patch:
@@ -40,6 +40,7 @@ public void saveSnapshot(Snapshot snapshot) throws IOException {
         if (snapshotFile.exists()) {
             logger.error(
                     "Snapshot " + snapshot.getName() + " already exists. Not saving the sanpshot.");
+            return;
         }
 
         String snapshotJson = GSON.toJson(snapshot, Snapshot.class);

File: frontend/server/src/main/java/org/pytorch/serve/wlm/Job.java
Patch:
@@ -79,7 +79,7 @@ public void response(
         HttpResponseStatus status =
                 (statusPhrase == null)
                         ? HttpResponseStatus.valueOf(statusCode)
-                        : HttpResponseStatus.valueOf(statusCode, statusPhrase);
+                        : new HttpResponseStatus(statusCode, statusPhrase);
         FullHttpResponse resp = new DefaultFullHttpResponse(HttpVersion.HTTP_1_1, status, false);
 
         if (contentType != null && contentType.length() > 0) {

File: frontend/server/src/main/java/org/pytorch/serve/wlm/WorkLoadManager.java
Patch:
@@ -95,6 +95,9 @@ public CompletableFuture<HttpResponseStatus> modelChanged(Model model, boolean i
                 threads = workers.remove(model.getModelVersionName());
                 if (threads == null) {
                     future.complete(HttpResponseStatus.OK);
+                    if (!isStartup) {
+                        SnapshotManager.getInstance().saveSnapshot();
+                    }
                     return future;
                 }
             } else {

File: frontend/server/src/main/java/org/pytorch/serve/util/ConfigManager.java
Patch:
@@ -309,7 +309,7 @@ public int getDefaultWorkers() {
         if (workers == 0) {
             workers = Runtime.getRuntime().availableProcessors();
         }
-        setProperty("default_workers_per_model", Integer.toString(workers));
+
         return workers;
     }
 

File: frontend/server/src/main/java/org/pytorch/serve/util/ConfigManager.java
Patch:
@@ -309,7 +309,7 @@ public int getDefaultWorkers() {
         if (workers == 0) {
             workers = Runtime.getRuntime().availableProcessors();
         }
-        setProperty("default_workers_per_model", Integer.toString(workers));
+
         return workers;
     }
 

File: frontend/server/src/main/java/org/pytorch/serve/servingsdk/impl/ModelServerResponse.java
Patch:
@@ -22,7 +22,7 @@ public void setStatus(int i) {
 
     @Override
     public void setStatus(int i, String s) {
-        response.setStatus(HttpResponseStatus.valueOf(i, s));
+        response.setStatus(new HttpResponseStatus(i, s));
     }
 
     @Override

File: frontend/server/src/main/java/org/pytorch/serve/util/ConfigManager.java
Patch:
@@ -424,7 +424,7 @@ public SslContext getSslContext() throws IOException, GeneralSecurityException {
         }
 
         return SslContextBuilder.forServer(privateKey, chain)
-                .protocols("TLSv1.2")
+                .protocols(new String[] {"TLSv1.2"})
                 .ciphers(supportedCiphers)
                 .build();
     }

File: frontend/server/src/main/java/org/pytorch/serve/wlm/Job.java
Patch:
@@ -79,7 +79,7 @@ public void response(
         HttpResponseStatus status =
                 (statusPhrase == null)
                         ? HttpResponseStatus.valueOf(statusCode)
-                        : HttpResponseStatus.valueOf(statusCode, statusPhrase);
+                        : new HttpResponseStatus(statusCode, statusPhrase);
         FullHttpResponse resp = new DefaultFullHttpResponse(HttpVersion.HTTP_1_1, status, false);
 
         if (contentType != null && contentType.length() > 0) {

File: frontend/server/src/main/java/org/pytorch/serve/servingsdk/impl/ModelServerResponse.java
Patch:
@@ -22,7 +22,7 @@ public void setStatus(int i) {
 
     @Override
     public void setStatus(int i, String s) {
-        response.setStatus(HttpResponseStatus.valueOf(i, s));
+        response.setStatus(new HttpResponseStatus(i, s));
     }
 
     @Override

File: frontend/server/src/main/java/org/pytorch/serve/util/ConfigManager.java
Patch:
@@ -424,7 +424,7 @@ public SslContext getSslContext() throws IOException, GeneralSecurityException {
         }
 
         return SslContextBuilder.forServer(privateKey, chain)
-                .protocols("TLSv1.2")
+                .protocols(new String[] {"TLSv1.2"})
                 .ciphers(supportedCiphers)
                 .build();
     }

File: frontend/server/src/main/java/org/pytorch/serve/wlm/Job.java
Patch:
@@ -79,7 +79,7 @@ public void response(
         HttpResponseStatus status =
                 (statusPhrase == null)
                         ? HttpResponseStatus.valueOf(statusCode)
-                        : HttpResponseStatus.valueOf(statusCode, statusPhrase);
+                        : new HttpResponseStatus(statusCode, statusPhrase);
         FullHttpResponse resp = new DefaultFullHttpResponse(HttpVersion.HTTP_1_1, status, false);
 
         if (contentType != null && contentType.length() > 0) {

File: frontend/server/src/main/java/org/pytorch/serve/servingsdk/impl/ModelServerResponse.java
Patch:
@@ -22,7 +22,7 @@ public void setStatus(int i) {
 
     @Override
     public void setStatus(int i, String s) {
-        response.setStatus(HttpResponseStatus.valueOf(i, s));
+        response.setStatus(new HttpResponseStatus(i, s));
     }
 
     @Override

File: frontend/server/src/main/java/org/pytorch/serve/util/ConfigManager.java
Patch:
@@ -430,7 +430,7 @@ public SslContext getSslContext() throws IOException, GeneralSecurityException {
         }
 
         return SslContextBuilder.forServer(privateKey, chain)
-                .protocols("TLSv1.2")
+                .protocols(new String[] {"TLSv1.2"})
                 .ciphers(supportedCiphers)
                 .build();
     }

File: frontend/server/src/main/java/org/pytorch/serve/wlm/Job.java
Patch:
@@ -79,7 +79,7 @@ public void response(
         HttpResponseStatus status =
                 (statusPhrase == null)
                         ? HttpResponseStatus.valueOf(statusCode)
-                        : HttpResponseStatus.valueOf(statusCode, statusPhrase);
+                        : new HttpResponseStatus(statusCode, statusPhrase);
         FullHttpResponse resp = new DefaultFullHttpResponse(HttpVersion.HTTP_1_1, status, false);
 
         if (contentType != null && contentType.length() > 0) {

File: frontend/server/src/main/java/org/pytorch/serve/servingsdk/impl/ModelServerResponse.java
Patch:
@@ -22,7 +22,7 @@ public void setStatus(int i) {
 
     @Override
     public void setStatus(int i, String s) {
-        response.setStatus(HttpResponseStatus.valueOf(i, s));
+        response.setStatus(new HttpResponseStatus(i, s));
     }
 
     @Override

File: frontend/server/src/main/java/org/pytorch/serve/util/ConfigManager.java
Patch:
@@ -424,7 +424,7 @@ public SslContext getSslContext() throws IOException, GeneralSecurityException {
         }
 
         return SslContextBuilder.forServer(privateKey, chain)
-                .protocols("TLSv1.2")
+                .protocols(new String[] {"TLSv1.2"})
                 .ciphers(supportedCiphers)
                 .build();
     }

File: frontend/server/src/main/java/org/pytorch/serve/wlm/Job.java
Patch:
@@ -79,7 +79,7 @@ public void response(
         HttpResponseStatus status =
                 (statusPhrase == null)
                         ? HttpResponseStatus.valueOf(statusCode)
-                        : HttpResponseStatus.valueOf(statusCode, statusPhrase);
+                        : new HttpResponseStatus(statusCode, statusPhrase);
         FullHttpResponse resp = new DefaultFullHttpResponse(HttpVersion.HTTP_1_1, status, false);
 
         if (contentType != null && contentType.length() > 0) {

File: frontend/server/src/main/java/org/pytorch/serve/util/ConfigManager.java
Patch:
@@ -309,7 +309,7 @@ public int getDefaultWorkers() {
         if (workers == 0) {
             workers = Runtime.getRuntime().availableProcessors();
         }
-        setProperty("default_workers_per_model", Integer.toString(workers));
+
         return workers;
     }
 

File: frontend/server/src/main/java/org/pytorch/serve/snapshot/FSSnapshotSerializer.java
Patch:
@@ -40,6 +40,7 @@ public void saveSnapshot(Snapshot snapshot) throws IOException {
         if (snapshotFile.exists()) {
             logger.error(
                     "Snapshot " + snapshot.getName() + " already exists. Not saving the sanpshot.");
+            return;
         }
 
         String snapshotJson = GSON.toJson(snapshot, Snapshot.class);

File: frontend/server/src/main/java/org/pytorch/serve/wlm/WorkLoadManager.java
Patch:
@@ -95,6 +95,9 @@ public CompletableFuture<HttpResponseStatus> modelChanged(Model model, boolean i
                 threads = workers.remove(model.getModelVersionName());
                 if (threads == null) {
                     future.complete(HttpResponseStatus.OK);
+                    if (!isStartup) {
+                        SnapshotManager.getInstance().saveSnapshot();
+                    }
                     return future;
                 }
             } else {

File: frontend/server/src/main/java/org/pytorch/serve/snapshot/FSSnapshotSerializer.java
Patch:
@@ -40,6 +40,7 @@ public void saveSnapshot(Snapshot snapshot) throws IOException {
         if (snapshotFile.exists()) {
             logger.error(
                     "Snapshot " + snapshot.getName() + " already exists. Not saving the sanpshot.");
+            return;
         }
 
         String snapshotJson = GSON.toJson(snapshot, Snapshot.class);

File: frontend/server/src/main/java/org/pytorch/serve/wlm/WorkLoadManager.java
Patch:
@@ -95,6 +95,9 @@ public CompletableFuture<HttpResponseStatus> modelChanged(Model model, boolean i
                 threads = workers.remove(model.getModelVersionName());
                 if (threads == null) {
                     future.complete(HttpResponseStatus.OK);
+                    if (!isStartup) {
+                        SnapshotManager.getInstance().saveSnapshot();
+                    }
                     return future;
                 }
             } else {

File: frontend/server/src/main/java/org/pytorch/serve/util/ConfigManager.java
Patch:
@@ -309,7 +309,7 @@ public int getDefaultWorkers() {
         if (workers == 0) {
             workers = Runtime.getRuntime().availableProcessors();
         }
-        setProperty("default_workers_per_model", Integer.toString(workers));
+
         return workers;
     }
 

File: frontend/server/src/main/java/org/pytorch/serve/snapshot/FSSnapshotSerializer.java
Patch:
@@ -40,6 +40,7 @@ public void saveSnapshot(Snapshot snapshot) throws IOException {
         if (snapshotFile.exists()) {
             logger.error(
                     "Snapshot " + snapshot.getName() + " already exists. Not saving the sanpshot.");
+            return;
         }
 
         String snapshotJson = GSON.toJson(snapshot, Snapshot.class);

File: frontend/server/src/test/java/org/pytorch/serve/SnapshotTest.java
Patch:
@@ -411,7 +411,7 @@ private void updateSnapshot(Properties prop) {
         String snapshotJson = GSON.toJson(snapshot, Snapshot.class);
         prop.put("model_snapshot", snapshotJson);
         prop.put("tsConfigFile", "dummyconfig");
-        prop.put("NUM_WORKERS", 4);
+        prop.put("default_workers_per_model", 4);
         prop.put("number_of_gpu", 4);
         prop.put("version", "0.1.1");
     }

File: frontend/server/src/main/java/org/pytorch/serve/snapshot/FSSnapshotSerializer.java
Patch:
@@ -40,6 +40,7 @@ public void saveSnapshot(Snapshot snapshot) throws IOException {
         if (snapshotFile.exists()) {
             logger.error(
                     "Snapshot " + snapshot.getName() + " already exists. Not saving the sanpshot.");
+            return;
         }
 
         String snapshotJson = GSON.toJson(snapshot, Snapshot.class);

File: frontend/server/src/main/java/org/pytorch/serve/wlm/WorkLoadManager.java
Patch:
@@ -95,6 +95,9 @@ public CompletableFuture<HttpResponseStatus> modelChanged(Model model, boolean i
                 threads = workers.remove(model.getModelVersionName());
                 if (threads == null) {
                     future.complete(HttpResponseStatus.OK);
+                    if (!isStartup) {
+                        SnapshotManager.getInstance().saveSnapshot();
+                    }
                     return future;
                 }
             } else {

File: frontend/server/src/main/java/org/pytorch/serve/snapshot/FSSnapshotSerializer.java
Patch:
@@ -40,6 +40,7 @@ public void saveSnapshot(Snapshot snapshot) throws IOException {
         if (snapshotFile.exists()) {
             logger.error(
                     "Snapshot " + snapshot.getName() + " already exists. Not saving the sanpshot.");
+            return;
         }
 
         String snapshotJson = GSON.toJson(snapshot, Snapshot.class);

File: frontend/server/src/test/java/org/pytorch/serve/SnapshotTest.java
Patch:
@@ -411,7 +411,7 @@ private void updateSnapshot(Properties prop) {
         String snapshotJson = GSON.toJson(snapshot, Snapshot.class);
         prop.put("model_snapshot", snapshotJson);
         prop.put("tsConfigFile", "dummyconfig");
-        prop.put("NUM_WORKERS", 4);
+        prop.put("default_workers_per_model", 4);
         prop.put("number_of_gpu", 4);
         prop.put("version", "0.1.1");
     }

File: frontend/server/src/main/java/org/pytorch/serve/snapshot/FSSnapshotSerializer.java
Patch:
@@ -40,6 +40,7 @@ public void saveSnapshot(Snapshot snapshot) throws IOException {
         if (snapshotFile.exists()) {
             logger.error(
                     "Snapshot " + snapshot.getName() + " already exists. Not saving the sanpshot.");
+            return;
         }
 
         String snapshotJson = GSON.toJson(snapshot, Snapshot.class);

File: frontend/server/src/test/java/org/pytorch/serve/SnapshotTest.java
Patch:
@@ -411,7 +411,7 @@ private void updateSnapshot(Properties prop) {
         String snapshotJson = GSON.toJson(snapshot, Snapshot.class);
         prop.put("model_snapshot", snapshotJson);
         prop.put("tsConfigFile", "dummyconfig");
-        prop.put("NUM_WORKERS", 4);
+        prop.put("default_workers_per_model", 4);
         prop.put("number_of_gpu", 4);
         prop.put("version", "0.1.1");
     }

File: frontend/server/src/main/java/org/pytorch/serve/snapshot/FSSnapshotSerializer.java
Patch:
@@ -40,6 +40,7 @@ public void saveSnapshot(Snapshot snapshot) throws IOException {
         if (snapshotFile.exists()) {
             logger.error(
                     "Snapshot " + snapshot.getName() + " already exists. Not saving the sanpshot.");
+            return;
         }
 
         String snapshotJson = GSON.toJson(snapshot, Snapshot.class);

File: frontend/server/src/test/java/org/pytorch/serve/SnapshotTest.java
Patch:
@@ -411,7 +411,7 @@ private void updateSnapshot(Properties prop) {
         String snapshotJson = GSON.toJson(snapshot, Snapshot.class);
         prop.put("model_snapshot", snapshotJson);
         prop.put("tsConfigFile", "dummyconfig");
-        prop.put("NUM_WORKERS", 4);
+        prop.put("default_workers_per_model", 4);
         prop.put("number_of_gpu", 4);
         prop.put("version", "0.1.1");
     }

File: frontend/server/src/main/java/org/pytorch/serve/snapshot/FSSnapshotSerializer.java
Patch:
@@ -40,6 +40,7 @@ public void saveSnapshot(Snapshot snapshot) throws IOException {
         if (snapshotFile.exists()) {
             logger.error(
                     "Snapshot " + snapshot.getName() + " already exists. Not saving the sanpshot.");
+            return;
         }
 
         String snapshotJson = GSON.toJson(snapshot, Snapshot.class);

File: frontend/server/src/test/java/org/pytorch/serve/SnapshotTest.java
Patch:
@@ -411,7 +411,7 @@ private void updateSnapshot(Properties prop) {
         String snapshotJson = GSON.toJson(snapshot, Snapshot.class);
         prop.put("model_snapshot", snapshotJson);
         prop.put("tsConfigFile", "dummyconfig");
-        prop.put("NUM_WORKERS", 4);
+        prop.put("default_workers_per_model", 4);
         prop.put("number_of_gpu", 4);
         prop.put("version", "0.1.1");
     }

File: frontend/server/src/main/java/org/pytorch/serve/wlm/WorkLoadManager.java
Patch:
@@ -95,6 +95,9 @@ public CompletableFuture<HttpResponseStatus> modelChanged(Model model, boolean i
                 threads = workers.remove(model.getModelVersionName());
                 if (threads == null) {
                     future.complete(HttpResponseStatus.OK);
+                    if (!isStartup) {
+                        SnapshotManager.getInstance().saveSnapshot();
+                    }
                     return future;
                 }
             } else {

File: frontend/server/src/main/java/org/pytorch/serve/wlm/WorkLoadManager.java
Patch:
@@ -95,6 +95,9 @@ public CompletableFuture<HttpResponseStatus> modelChanged(Model model, boolean i
                 threads = workers.remove(model.getModelVersionName());
                 if (threads == null) {
                     future.complete(HttpResponseStatus.OK);
+                    if (!isStartup) {
+                        SnapshotManager.getInstance().saveSnapshot();
+                    }
                     return future;
                 }
             } else {

File: frontend/server/src/main/java/org/pytorch/serve/wlm/WorkLoadManager.java
Patch:
@@ -95,6 +95,9 @@ public CompletableFuture<HttpResponseStatus> modelChanged(Model model, boolean i
                 threads = workers.remove(model.getModelVersionName());
                 if (threads == null) {
                     future.complete(HttpResponseStatus.OK);
+                    if (!isStartup) {
+                        SnapshotManager.getInstance().saveSnapshot();
+                    }
                     return future;
                 }
             } else {

File: frontend/server/src/main/java/org/pytorch/serve/http/HttpRequestHandlerChain.java
Patch:
@@ -11,15 +11,15 @@
 import java.util.Map;
 import org.pytorch.serve.archive.ModelException;
 import org.pytorch.serve.archive.ModelNotFoundException;
+import org.pytorch.serve.servingsdk.ModelServerEndpoint;
+import org.pytorch.serve.servingsdk.ModelServerEndpointException;
 import org.pytorch.serve.servingsdk.impl.ModelServerContext;
 import org.pytorch.serve.servingsdk.impl.ModelServerRequest;
 import org.pytorch.serve.servingsdk.impl.ModelServerResponse;
 import org.pytorch.serve.util.NettyUtils;
 import org.pytorch.serve.wlm.ModelManager;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
-import software.amazon.ai.mms.servingsdk.ModelServerEndpoint;
-import software.amazon.ai.mms.servingsdk.ModelServerEndpointException;
 
 public abstract class HttpRequestHandlerChain {
     private static final Logger logger = LoggerFactory.getLogger(HttpRequestHandler.class);

File: frontend/server/src/main/java/org/pytorch/serve/servingsdk/impl/ModelServerContext.java
Patch:
@@ -3,10 +3,10 @@
 import java.util.HashMap;
 import java.util.Map;
 import java.util.Properties;
+import org.pytorch.serve.servingsdk.Context;
+import org.pytorch.serve.servingsdk.Model;
 import org.pytorch.serve.util.ConfigManager;
 import org.pytorch.serve.wlm.ModelManager;
-import software.amazon.ai.mms.servingsdk.Context;
-import software.amazon.ai.mms.servingsdk.Model;
 
 public class ModelServerContext implements Context {
     @Override

File: frontend/server/src/main/java/org/pytorch/serve/servingsdk/impl/ModelServerModel.java
Patch:
@@ -2,9 +2,9 @@
 
 import java.util.ArrayList;
 import java.util.List;
+import org.pytorch.serve.servingsdk.Model;
+import org.pytorch.serve.servingsdk.Worker;
 import org.pytorch.serve.wlm.ModelManager;
-import software.amazon.ai.mms.servingsdk.Model;
-import software.amazon.ai.mms.servingsdk.Worker;
 
 public class ModelServerModel implements Model {
     private final org.pytorch.serve.wlm.Model model;

File: frontend/server/src/main/java/org/pytorch/serve/servingsdk/impl/ModelServerRequest.java
Patch:
@@ -7,7 +7,7 @@
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
-import software.amazon.ai.mms.servingsdk.http.Request;
+import org.pytorch.serve.servingsdk.http.Request;
 
 public class ModelServerRequest implements Request {
     private FullHttpRequest req;

File: frontend/server/src/main/java/org/pytorch/serve/servingsdk/impl/ModelServerResponse.java
Patch:
@@ -5,7 +5,7 @@
 import io.netty.handler.codec.http.HttpHeaderNames;
 import io.netty.handler.codec.http.HttpResponseStatus;
 import java.io.OutputStream;
-import software.amazon.ai.mms.servingsdk.http.Response;
+import org.pytorch.serve.servingsdk.http.Response;
 
 public class ModelServerResponse implements Response {
 

File: frontend/server/src/main/java/org/pytorch/serve/servingsdk/impl/ModelWorker.java
Patch:
@@ -1,8 +1,8 @@
 package org.pytorch.serve.servingsdk.impl;
 
+import org.pytorch.serve.servingsdk.Worker;
 import org.pytorch.serve.wlm.WorkerState;
 import org.pytorch.serve.wlm.WorkerThread;
-import software.amazon.ai.mms.servingsdk.Worker;
 
 public class ModelWorker implements Worker {
     private boolean running;

File: frontend/server/src/main/java/org/pytorch/serve/servingsdk/impl/PluginsManager.java
Patch:
@@ -5,11 +5,11 @@
 import java.util.Map;
 import java.util.ServiceLoader;
 import org.pytorch.serve.http.InvalidPluginException;
+import org.pytorch.serve.servingsdk.ModelServerEndpoint;
+import org.pytorch.serve.servingsdk.annotations.Endpoint;
+import org.pytorch.serve.servingsdk.annotations.helpers.EndpointTypes;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
-import software.amazon.ai.mms.servingsdk.ModelServerEndpoint;
-import software.amazon.ai.mms.servingsdk.annotations.Endpoint;
-import software.amazon.ai.mms.servingsdk.annotations.helpers.EndpointTypes;
 
 public final class PluginsManager {
 

File: frontend/server/src/main/java/org/pytorch/serve/snapshot/FSSnapshotSerializer.java
Patch:
@@ -25,7 +25,7 @@ public class FSSnapshotSerializer implements SnapshotSerializer {
 
     private Logger logger = LoggerFactory.getLogger(FSSnapshotSerializer.class);
     private ConfigManager configManager = ConfigManager.getInstance();
-    private static final String TS_MODEL_SNAPSHOT = "model_snapshot";
+    private static final String MODEL_SNAPSHOT = "model_snapshot";
     public static final Gson GSON = new GsonBuilder().setPrettyPrinting().create();
 
     @Override
@@ -43,7 +43,7 @@ public void saveSnapshot(Snapshot snapshot) throws IOException {
         }
 
         String snapshotJson = GSON.toJson(snapshot, Snapshot.class);
-        prop.put(TS_MODEL_SNAPSHOT, snapshotJson);
+        prop.put(MODEL_SNAPSHOT, snapshotJson);
         try (OutputStream os = Files.newOutputStream(snapshotFile.toPath())) {
             OutputStreamWriter osWriter = new OutputStreamWriter(os, StandardCharsets.UTF_8);
             prop.store(osWriter, "Saving snapshot");

File: frontend/server/src/main/java/org/pytorch/serve/util/Connector.java
Patch:
@@ -105,7 +105,7 @@ public static Connector parse(String binding, boolean management) {
         } else {
             port = Integer.parseInt(listeningPort);
         }
-        if (port >= Short.MAX_VALUE) {
+        if (port >= Short.MAX_VALUE * 2 + 1) {
             throw new IllegalArgumentException("Invalid port number: " + binding);
         }
         return new Connector(port, false, host, String.valueOf(port), ssl, management);

File: frontend/server/src/main/java/org/pytorch/serve/util/codec/ModelRequestEncoder.java
Patch:
@@ -14,6 +14,9 @@
 
 @ChannelHandler.Sharable
 public class ModelRequestEncoder extends MessageToByteEncoder<BaseModelRequest> {
+    public ModelRequestEncoder(boolean preferDirect) {
+        super(preferDirect);
+    }
 
     @Override
     protected void encode(ChannelHandlerContext ctx, BaseModelRequest msg, ByteBuf out) {

File: frontend/server/src/main/java/org/pytorch/serve/wlm/ModelVersionedRefs.java
Patch:
@@ -29,7 +29,7 @@ public ModelVersionedRefs() {
      * @return None
      */
     public void addVersionModel(Model model, String versionId)
-            throws InvalidModelVersionException, ConflictStatusException {
+            throws ModelVersionNotFoundException, ConflictStatusException {
         logger.debug("Adding new version {} for model {}", versionId, model.getModelName());
 
         if (versionId == null) {
@@ -64,10 +64,10 @@ public String getDefaultVersion() {
      * @param A valid String obj with version to set default
      * @return None
      */
-    public void setDefaultVersion(String versionId) throws InvalidModelVersionException {
+    public void setDefaultVersion(String versionId) throws ModelVersionNotFoundException {
         Model model = this.modelsVersionMap.get(versionId);
         if (model == null) {
-            throw new InvalidModelVersionException("Can't set default to: " + versionId);
+            throw new ModelVersionNotFoundException("Can't set default to: " + versionId);
         }
 
         logger.debug("Setting default version to {} for model {}", versionId, model.getModelName());

File: serving-sdk/src/main/java/org/pytorch/serve/servingsdk/Context.java
Patch:
@@ -1,5 +1,5 @@
 
-package software.amazon.ai.mms.servingsdk;
+package org.pytorch.serve.servingsdk;
 
 import java.util.Map;
 import java.util.Properties;

File: serving-sdk/src/main/java/org/pytorch/serve/servingsdk/Model.java
Patch:
@@ -1,5 +1,5 @@
 
-package software.amazon.ai.mms.servingsdk;
+package org.pytorch.serve.servingsdk;
 
 import java.util.List;
 

File: serving-sdk/src/main/java/org/pytorch/serve/servingsdk/ModelServerEndpoint.java
Patch:
@@ -1,8 +1,8 @@
 
-package software.amazon.ai.mms.servingsdk;
+package org.pytorch.serve.servingsdk;
 
-import software.amazon.ai.mms.servingsdk.http.Request;
-import software.amazon.ai.mms.servingsdk.http.Response;
+import org.pytorch.serve.servingsdk.http.Request;
+import org.pytorch.serve.servingsdk.http.Response;
 
 import java.io.IOException;
 

File: serving-sdk/src/main/java/org/pytorch/serve/servingsdk/ModelServerEndpointException.java
Patch:
@@ -1,6 +1,6 @@
 
 
-package software.amazon.ai.mms.servingsdk;
+package org.pytorch.serve.servingsdk;
 
 /**
  * Runtime exception for custom model server endpoint plugins

File: serving-sdk/src/main/java/org/pytorch/serve/servingsdk/Worker.java
Patch:
@@ -1,6 +1,6 @@
 
 
-package software.amazon.ai.mms.servingsdk;
+package org.pytorch.serve.servingsdk;
 
 /**
  * Describe the model worker

File: serving-sdk/src/main/java/org/pytorch/serve/servingsdk/annotations/Endpoint.java
Patch:
@@ -1,7 +1,7 @@
 
-package software.amazon.ai.mms.servingsdk.annotations;
+package org.pytorch.serve.servingsdk.annotations;
 
-import software.amazon.ai.mms.servingsdk.annotations.helpers.EndpointTypes;
+import org.pytorch.serve.servingsdk.annotations.helpers.EndpointTypes;
 
 import java.lang.annotation.Documented;
 import java.lang.annotation.ElementType;

File: serving-sdk/src/main/java/org/pytorch/serve/servingsdk/annotations/helpers/EndpointTypes.java
Patch:
@@ -1,5 +1,5 @@
 
-package software.amazon.ai.mms.servingsdk.annotations.helpers;
+package org.pytorch.serve.servingsdk.annotations.helpers;
 
 /**
  * Types of ModelServer endpoints

File: serving-sdk/src/main/java/org/pytorch/serve/servingsdk/http/Request.java
Patch:
@@ -1,6 +1,6 @@
 
 
-package software.amazon.ai.mms.servingsdk.http;
+package org.pytorch.serve.servingsdk.http;
 
 import java.io.IOException;
 import java.io.InputStream;

File: serving-sdk/src/main/java/org/pytorch/serve/servingsdk/http/Response.java
Patch:
@@ -1,6 +1,6 @@
 
 
-package software.amazon.ai.mms.servingsdk.http;
+package org.pytorch.serve.servingsdk.http;
 
 import java.io.IOException;
 import java.io.OutputStream;

File: frontend/server/src/main/java/org/pytorch/serve/util/codec/ModelRequestEncoder.java
Patch:
@@ -14,6 +14,9 @@
 
 @ChannelHandler.Sharable
 public class ModelRequestEncoder extends MessageToByteEncoder<BaseModelRequest> {
+    public ModelRequestEncoder(boolean preferDirect) {
+        super(preferDirect);
+    }
 
     @Override
     protected void encode(ChannelHandlerContext ctx, BaseModelRequest msg, ByteBuf out) {

File: frontend/server/src/main/java/org/pytorch/serve/util/codec/ModelRequestEncoder.java
Patch:
@@ -14,6 +14,9 @@
 
 @ChannelHandler.Sharable
 public class ModelRequestEncoder extends MessageToByteEncoder<BaseModelRequest> {
+    public ModelRequestEncoder(boolean preferDirect) {
+        super(preferDirect);
+    }
 
     @Override
     protected void encode(ChannelHandlerContext ctx, BaseModelRequest msg, ByteBuf out) {

File: frontend/server/src/main/java/org/pytorch/serve/util/codec/ModelRequestEncoder.java
Patch:
@@ -14,6 +14,9 @@
 
 @ChannelHandler.Sharable
 public class ModelRequestEncoder extends MessageToByteEncoder<BaseModelRequest> {
+    public ModelRequestEncoder(boolean preferDirect) {
+        super(preferDirect);
+    }
 
     @Override
     protected void encode(ChannelHandlerContext ctx, BaseModelRequest msg, ByteBuf out) {

File: frontend/modelarchive/src/main/java/org/pytorch/serve/archive/ModelArchive.java
Patch:
@@ -55,9 +55,11 @@ public static ModelArchive downloadModel(String modelStore, String url)
         File modelLocation = new File(modelStore, marFileName);
 
         if (URL_PATTERN.matcher(url).matches()) {
+            // Required if you are re-executing test cases specially build
             if (modelLocation.exists()) {
-                throw new FileAlreadyExistsException(marFileName);
+                modelLocation.delete();
             }
+
             try {
                 FileUtils.copyURLToFile(new URL(url), modelLocation);
             } catch (IOException e) {

File: frontend/server/src/main/java/org/pytorch/serve/http/ExtendedSSLHandler.java
Patch:
@@ -15,7 +15,7 @@
 public class ExtendedSSLHandler extends OptionalSslHandler {
     private static final Logger logger = LoggerFactory.getLogger(ExtendedSSLHandler.class);
     /** the length of the ssl record header (in bytes) */
-    static final int SSL_RECORD_HEADER_LENGTH = 5;
+    private static final int SSL_RECORD_HEADER_LENGTH = 5;
 
     public ExtendedSSLHandler(SslContext sslContext) {
         super(sslContext);

File: frontend/server/src/main/java/org/pytorch/serve/snapshot/FSSnapshotSerializer.java
Patch:
@@ -25,7 +25,7 @@ public class FSSnapshotSerializer implements SnapshotSerializer {
 
     private Logger logger = LoggerFactory.getLogger(FSSnapshotSerializer.class);
     private ConfigManager configManager = ConfigManager.getInstance();
-    private static final String TS_MODEL_SNAPSHOT = "model_snapshot";
+    private static final String MODEL_SNAPSHOT = "model_snapshot";
     public static final Gson GSON = new GsonBuilder().setPrettyPrinting().create();
 
     @Override
@@ -43,7 +43,7 @@ public void saveSnapshot(Snapshot snapshot) throws IOException {
         }
 
         String snapshotJson = GSON.toJson(snapshot, Snapshot.class);
-        prop.put(TS_MODEL_SNAPSHOT, snapshotJson);
+        prop.put(MODEL_SNAPSHOT, snapshotJson);
         try (OutputStream os = Files.newOutputStream(snapshotFile.toPath())) {
             OutputStreamWriter osWriter = new OutputStreamWriter(os, StandardCharsets.UTF_8);
             prop.store(osWriter, "Saving snapshot");

File: frontend/server/src/main/java/org/pytorch/serve/openapi/OpenApiUtils.java
Patch:
@@ -4,6 +4,7 @@
 import java.util.ArrayList;
 import java.util.List;
 import org.pytorch.serve.archive.Manifest;
+import org.pytorch.serve.util.ConfigManager;
 import org.pytorch.serve.util.ConnectorType;
 import org.pytorch.serve.util.JsonUtils;
 import org.pytorch.serve.wlm.Model;
@@ -18,7 +19,8 @@ public static String listApis(ConnectorType type) {
         info.setTitle("TorchServe APIs");
         info.setDescription(
                 "TorchServe is a flexible and easy to use tool for serving deep learning models");
-        info.setVersion("1.0.0");
+        ConfigManager config = ConfigManager.getInstance();
+        info.setVersion(config.getProperty("version", null));
         openApi.setInfo(info);
 
         if (ConnectorType.BOTH.equals(type) || ConnectorType.INFERENCE_CONNECTOR.equals(type)) {

File: frontend/server/src/main/java/org/pytorch/serve/util/Connector.java
Patch:
@@ -105,7 +105,7 @@ public static Connector parse(String binding, boolean management) {
         } else {
             port = Integer.parseInt(listeningPort);
         }
-        if (port >= Short.MAX_VALUE) {
+        if (port >= Short.MAX_VALUE * 2 + 1) {
             throw new IllegalArgumentException("Invalid port number: " + binding);
         }
         return new Connector(port, false, host, String.valueOf(port), ssl, management);

File: frontend/server/src/test/java/org/pytorch/serve/SnapshotTest.java
Patch:
@@ -399,6 +399,7 @@ private void updateSnapshot(Properties prop) {
         prop.put("tsConfigFile", "dummyconfig");
         prop.put("NUM_WORKERS", 4);
         prop.put("number_of_gpu", 4);
+        prop.put("version", "0.1.1");
     }
 
     private String getLastSnapshot() {

File: frontend/server/src/test/java/org/pytorch/serve/util/ConfigManagerTest.java
Patch:
@@ -105,7 +105,8 @@ public void test()
 
     @Test
     public void testNoEnvVars()
-            throws IllegalAccessException, NoSuchFieldException, ClassNotFoundException {
+            throws IllegalAccessException, NoSuchFieldException, ClassNotFoundException,
+                    IOException {
         System.setProperty("tsConfigFile", "src/test/resources/config_test_env.properties");
         modifyEnv("TS_DEFAULT_RESPONSE_TIMEOUT", "130");
         ConfigManager.Arguments args = new ConfigManager.Arguments();

File: frontend/server/src/main/java/org/pytorch/serve/openapi/OpenApiUtils.java
Patch:
@@ -4,6 +4,7 @@
 import java.util.ArrayList;
 import java.util.List;
 import org.pytorch.serve.archive.Manifest;
+import org.pytorch.serve.util.ConfigManager;
 import org.pytorch.serve.util.ConnectorType;
 import org.pytorch.serve.util.JsonUtils;
 import org.pytorch.serve.wlm.Model;
@@ -18,7 +19,8 @@ public static String listApis(ConnectorType type) {
         info.setTitle("TorchServe APIs");
         info.setDescription(
                 "TorchServe is a flexible and easy to use tool for serving deep learning models");
-        info.setVersion("1.0.0");
+        ConfigManager config = ConfigManager.getInstance();
+        info.setVersion(config.getProperty("version", null));
         openApi.setInfo(info);
 
         if (ConnectorType.BOTH.equals(type) || ConnectorType.INFERENCE_CONNECTOR.equals(type)) {

File: frontend/server/src/test/java/org/pytorch/serve/SnapshotTest.java
Patch:
@@ -399,6 +399,7 @@ private void updateSnapshot(Properties prop) {
         prop.put("tsConfigFile", "dummyconfig");
         prop.put("NUM_WORKERS", 4);
         prop.put("number_of_gpu", 4);
+        prop.put("version", "0.1.1");
     }
 
     private String getLastSnapshot() {

File: frontend/server/src/test/java/org/pytorch/serve/util/ConfigManagerTest.java
Patch:
@@ -105,7 +105,8 @@ public void test()
 
     @Test
     public void testNoEnvVars()
-            throws IllegalAccessException, NoSuchFieldException, ClassNotFoundException {
+            throws IllegalAccessException, NoSuchFieldException, ClassNotFoundException,
+                    IOException {
         System.setProperty("tsConfigFile", "src/test/resources/config_test_env.properties");
         modifyEnv("TS_DEFAULT_RESPONSE_TIMEOUT", "130");
         ConfigManager.Arguments args = new ConfigManager.Arguments();

File: frontend/server/src/main/java/org/pytorch/serve/util/Connector.java
Patch:
@@ -105,7 +105,7 @@ public static Connector parse(String binding, boolean management) {
         } else {
             port = Integer.parseInt(listeningPort);
         }
-        if (port >= Short.MAX_VALUE) {
+        if (port >= Short.MAX_VALUE * 2 + 1) {
             throw new IllegalArgumentException("Invalid port number: " + binding);
         }
         return new Connector(port, false, host, String.valueOf(port), ssl, management);

File: frontend/server/src/main/java/org/pytorch/serve/wlm/WorkLoadManager.java
Patch:
@@ -113,7 +113,9 @@ public CompletableFuture<HttpResponseStatus> modelChanged(Model model, boolean i
 
                     Process workerProcess = lifecycle.getProcess();
 
-                    if (workerProcess.isAlive()) {
+                    // Need to check worker process here since thread.shutdown() -> lifecycle.exit()
+                    // -> This may nullify process object per destroyForcibly doc.
+                    if (workerProcess != null && workerProcess.isAlive()) {
                         boolean workerDestroyed = false;
                         workerProcess.destroyForcibly();
                         try {

File: frontend/server/src/main/java/org/pytorch/serve/wlm/WorkLoadManager.java
Patch:
@@ -113,7 +113,9 @@ public CompletableFuture<HttpResponseStatus> modelChanged(Model model, boolean i
 
                     Process workerProcess = lifecycle.getProcess();
 
-                    if (workerProcess.isAlive()) {
+                    // Need to check worker process here since thread.shutdown() -> lifecycle.exit()
+                    // -> This may nullify process object per destroyForcibly doc.
+                    if (workerProcess != null && workerProcess.isAlive()) {
                         boolean workerDestroyed = false;
                         workerProcess.destroyForcibly();
                         try {

File: frontend/server/src/main/java/org/pytorch/serve/wlm/ModelVersionedRefs.java
Patch:
@@ -29,7 +29,7 @@ public ModelVersionedRefs() {
      * @return None
      */
     public void addVersionModel(Model model, String versionId)
-            throws InvalidModelVersionException, ConflictStatusException {
+            throws ModelVersionNotFoundException, ConflictStatusException {
         logger.debug("Adding new version {} for model {}", versionId, model.getModelName());
 
         if (versionId == null) {
@@ -64,10 +64,10 @@ public String getDefaultVersion() {
      * @param A valid String obj with version to set default
      * @return None
      */
-    public void setDefaultVersion(String versionId) throws InvalidModelVersionException {
+    public void setDefaultVersion(String versionId) throws ModelVersionNotFoundException {
         Model model = this.modelsVersionMap.get(versionId);
         if (model == null) {
-            throw new InvalidModelVersionException("Can't set default to: " + versionId);
+            throw new ModelVersionNotFoundException("Can't set default to: " + versionId);
         }
 
         logger.debug("Setting default version to {} for model {}", versionId, model.getModelName());

File: frontend/server/src/test/java/org/pytorch/serve/util/ConfigManagerTest.java
Patch:
@@ -9,12 +9,12 @@
 import java.util.Collections;
 import java.util.List;
 import java.util.Map;
-import org.junit.Assert;
 import org.pytorch.serve.TestUtils;
 import org.pytorch.serve.metrics.Dimension;
 import org.pytorch.serve.metrics.Metric;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
+import org.testng.Assert;
 import org.testng.annotations.Test;
 
 public class ConfigManagerTest {

File: frontend/server/src/test/java/org/pytorch/serve/util/ConfigManagerTest.java
Patch:
@@ -9,12 +9,12 @@
 import java.util.Collections;
 import java.util.List;
 import java.util.Map;
-import org.junit.Assert;
 import org.pytorch.serve.TestUtils;
 import org.pytorch.serve.metrics.Dimension;
 import org.pytorch.serve.metrics.Metric;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
+import org.testng.Assert;
 import org.testng.annotations.Test;
 
 public class ConfigManagerTest {

File: frontend/server/src/test/java/org/pytorch/serve/util/ConfigManagerTest.java
Patch:
@@ -9,12 +9,12 @@
 import java.util.Collections;
 import java.util.List;
 import java.util.Map;
-import org.junit.Assert;
 import org.pytorch.serve.TestUtils;
 import org.pytorch.serve.metrics.Dimension;
 import org.pytorch.serve.metrics.Metric;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
+import org.testng.Assert;
 import org.testng.annotations.Test;
 
 public class ConfigManagerTest {

File: frontend/server/src/main/java/org/pytorch/serve/wlm/ModelVersionedRefs.java
Patch:
@@ -29,7 +29,7 @@ public ModelVersionedRefs() {
      * @return None
      */
     public void addVersionModel(Model model, String versionId)
-            throws InvalidModelVersionException, ConflictStatusException {
+            throws ModelVersionNotFoundException, ConflictStatusException {
         logger.debug("Adding new version {} for model {}", versionId, model.getModelName());
 
         if (versionId == null) {
@@ -64,10 +64,10 @@ public String getDefaultVersion() {
      * @param A valid String obj with version to set default
      * @return None
      */
-    public void setDefaultVersion(String versionId) throws InvalidModelVersionException {
+    public void setDefaultVersion(String versionId) throws ModelVersionNotFoundException {
         Model model = this.modelsVersionMap.get(versionId);
         if (model == null) {
-            throw new InvalidModelVersionException("Can't set default to: " + versionId);
+            throw new ModelVersionNotFoundException("Can't set default to: " + versionId);
         }
 
         logger.debug("Setting default version to {} for model {}", versionId, model.getModelName());

File: frontend/server/src/main/java/org/pytorch/serve/wlm/ModelVersionedRefs.java
Patch:
@@ -29,7 +29,7 @@ public ModelVersionedRefs() {
      * @return None
      */
     public void addVersionModel(Model model, String versionId)
-            throws InvalidModelVersionException, ConflictStatusException {
+            throws ModelVersionNotFoundException, ConflictStatusException {
         logger.debug("Adding new version {} for model {}", versionId, model.getModelName());
 
         if (versionId == null) {
@@ -64,10 +64,10 @@ public String getDefaultVersion() {
      * @param A valid String obj with version to set default
      * @return None
      */
-    public void setDefaultVersion(String versionId) throws InvalidModelVersionException {
+    public void setDefaultVersion(String versionId) throws ModelVersionNotFoundException {
         Model model = this.modelsVersionMap.get(versionId);
         if (model == null) {
-            throw new InvalidModelVersionException("Can't set default to: " + versionId);
+            throw new ModelVersionNotFoundException("Can't set default to: " + versionId);
         }
 
         logger.debug("Setting default version to {} for model {}", versionId, model.getModelName());

File: frontend/server/src/main/java/org/pytorch/serve/ModelServer.java
Patch:
@@ -29,6 +29,9 @@
 import org.pytorch.serve.archive.ModelArchive;
 import org.pytorch.serve.archive.ModelException;
 import org.pytorch.serve.metrics.MetricManager;
+import org.pytorch.serve.servingsdk.ModelServerEndpoint;
+import org.pytorch.serve.servingsdk.annotations.Endpoint;
+import org.pytorch.serve.servingsdk.annotations.helpers.EndpointTypes;
 import org.pytorch.serve.servingsdk.impl.PluginsManager;
 import org.pytorch.serve.snapshot.InvalidSnapshotException;
 import org.pytorch.serve.snapshot.SnapshotManager;
@@ -40,9 +43,6 @@
 import org.pytorch.serve.wlm.WorkLoadManager;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
-import software.amazon.ai.mms.servingsdk.ModelServerEndpoint;
-import software.amazon.ai.mms.servingsdk.annotations.Endpoint;
-import software.amazon.ai.mms.servingsdk.annotations.helpers.EndpointTypes;
 
 public class ModelServer {
 

File: frontend/server/src/main/java/org/pytorch/serve/http/HttpRequestHandlerChain.java
Patch:
@@ -11,15 +11,15 @@
 import java.util.Map;
 import org.pytorch.serve.archive.ModelException;
 import org.pytorch.serve.archive.ModelNotFoundException;
+import org.pytorch.serve.servingsdk.ModelServerEndpoint;
+import org.pytorch.serve.servingsdk.ModelServerEndpointException;
 import org.pytorch.serve.servingsdk.impl.ModelServerContext;
 import org.pytorch.serve.servingsdk.impl.ModelServerRequest;
 import org.pytorch.serve.servingsdk.impl.ModelServerResponse;
 import org.pytorch.serve.util.NettyUtils;
 import org.pytorch.serve.wlm.ModelManager;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
-import software.amazon.ai.mms.servingsdk.ModelServerEndpoint;
-import software.amazon.ai.mms.servingsdk.ModelServerEndpointException;
 
 public abstract class HttpRequestHandlerChain {
     private static final Logger logger = LoggerFactory.getLogger(HttpRequestHandler.class);

File: frontend/server/src/main/java/org/pytorch/serve/http/InferenceRequestHandler.java
Patch:
@@ -14,6 +14,7 @@
 import org.pytorch.serve.archive.ModelException;
 import org.pytorch.serve.archive.ModelNotFoundException;
 import org.pytorch.serve.openapi.OpenApiUtils;
+import org.pytorch.serve.servingsdk.ModelServerEndpoint;
 import org.pytorch.serve.util.NettyUtils;
 import org.pytorch.serve.util.messages.InputParameter;
 import org.pytorch.serve.util.messages.RequestInput;
@@ -23,7 +24,6 @@
 import org.pytorch.serve.wlm.ModelManager;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
-import software.amazon.ai.mms.servingsdk.ModelServerEndpoint;
 
 /**
  * A class handling inbound HTTP requests to the management API.

File: frontend/server/src/main/java/org/pytorch/serve/http/ManagementRequestHandler.java
Patch:
@@ -23,14 +23,14 @@
 import org.pytorch.serve.archive.ModelNotFoundException;
 import org.pytorch.serve.archive.ModelVersionNotFoundException;
 import org.pytorch.serve.http.messages.RegisterModelRequest;
+import org.pytorch.serve.servingsdk.ModelServerEndpoint;
 import org.pytorch.serve.snapshot.SnapshotManager;
 import org.pytorch.serve.util.ConfigManager;
 import org.pytorch.serve.util.JsonUtils;
 import org.pytorch.serve.util.NettyUtils;
 import org.pytorch.serve.wlm.Model;
 import org.pytorch.serve.wlm.ModelManager;
 import org.pytorch.serve.wlm.WorkerThread;
-import software.amazon.ai.mms.servingsdk.ModelServerEndpoint;
 
 /**
  * A class handling inbound HTTP requests to the management API.

File: frontend/server/src/main/java/org/pytorch/serve/servingsdk/impl/ModelServerContext.java
Patch:
@@ -3,10 +3,10 @@
 import java.util.HashMap;
 import java.util.Map;
 import java.util.Properties;
+import org.pytorch.serve.servingsdk.Context;
+import org.pytorch.serve.servingsdk.Model;
 import org.pytorch.serve.util.ConfigManager;
 import org.pytorch.serve.wlm.ModelManager;
-import software.amazon.ai.mms.servingsdk.Context;
-import software.amazon.ai.mms.servingsdk.Model;
 
 public class ModelServerContext implements Context {
     @Override

File: frontend/server/src/main/java/org/pytorch/serve/servingsdk/impl/ModelServerModel.java
Patch:
@@ -2,9 +2,9 @@
 
 import java.util.ArrayList;
 import java.util.List;
+import org.pytorch.serve.servingsdk.Model;
+import org.pytorch.serve.servingsdk.Worker;
 import org.pytorch.serve.wlm.ModelManager;
-import software.amazon.ai.mms.servingsdk.Model;
-import software.amazon.ai.mms.servingsdk.Worker;
 
 public class ModelServerModel implements Model {
     private final org.pytorch.serve.wlm.Model model;

File: frontend/server/src/main/java/org/pytorch/serve/servingsdk/impl/ModelServerRequest.java
Patch:
@@ -7,7 +7,7 @@
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
-import software.amazon.ai.mms.servingsdk.http.Request;
+import org.pytorch.serve.servingsdk.http.Request;
 
 public class ModelServerRequest implements Request {
     private FullHttpRequest req;

File: frontend/server/src/main/java/org/pytorch/serve/servingsdk/impl/ModelServerResponse.java
Patch:
@@ -5,7 +5,7 @@
 import io.netty.handler.codec.http.HttpHeaderNames;
 import io.netty.handler.codec.http.HttpResponseStatus;
 import java.io.OutputStream;
-import software.amazon.ai.mms.servingsdk.http.Response;
+import org.pytorch.serve.servingsdk.http.Response;
 
 public class ModelServerResponse implements Response {
 

File: frontend/server/src/main/java/org/pytorch/serve/servingsdk/impl/ModelWorker.java
Patch:
@@ -1,8 +1,8 @@
 package org.pytorch.serve.servingsdk.impl;
 
+import org.pytorch.serve.servingsdk.Worker;
 import org.pytorch.serve.wlm.WorkerState;
 import org.pytorch.serve.wlm.WorkerThread;
-import software.amazon.ai.mms.servingsdk.Worker;
 
 public class ModelWorker implements Worker {
     private boolean running;

File: frontend/server/src/main/java/org/pytorch/serve/servingsdk/impl/PluginsManager.java
Patch:
@@ -5,11 +5,11 @@
 import java.util.Map;
 import java.util.ServiceLoader;
 import org.pytorch.serve.http.InvalidPluginException;
+import org.pytorch.serve.servingsdk.ModelServerEndpoint;
+import org.pytorch.serve.servingsdk.annotations.Endpoint;
+import org.pytorch.serve.servingsdk.annotations.helpers.EndpointTypes;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
-import software.amazon.ai.mms.servingsdk.ModelServerEndpoint;
-import software.amazon.ai.mms.servingsdk.annotations.Endpoint;
-import software.amazon.ai.mms.servingsdk.annotations.helpers.EndpointTypes;
 
 public final class PluginsManager {
 

File: frontend/server/src/main/java/org/pytorch/serve/wlm/WorkLoadManager.java
Patch:
@@ -123,7 +123,7 @@ public CompletableFuture<HttpResponseStatus> modelChanged(Model model, boolean i
                                             TimeUnit.SECONDS);
                         } catch (InterruptedException e) {
                             logger.warn(
-                                    "WorkerThread interrupted during waitFor, possible asynch resource cleanup.");
+                                    "WorkerThread interrupted during waitFor, possible async resource cleanup.");
                             future.complete(HttpResponseStatus.INTERNAL_SERVER_ERROR);
                             return future;
                         }

File: frontend/server/src/main/java/org/pytorch/serve/wlm/WorkerThread.java
Patch:
@@ -161,7 +161,8 @@ public void run() {
                 req = null;
             }
         } catch (InterruptedException e) {
-            if (state == WorkerState.WORKER_SCALED_DOWN) {
+            logger.debug("System state is : " + state);
+            if (state == WorkerState.WORKER_SCALED_DOWN || state == WorkerState.WORKER_STOPPED) {
                 logger.debug("Shutting down the thread .. Scaling down.");
             } else {
                 logger.debug(

File: serving-sdk/src/main/java/org/pytorch/serve/servingsdk/Context.java
Patch:
@@ -1,5 +1,5 @@
 
-package software.amazon.ai.mms.servingsdk;
+package org.pytorch.serve.servingsdk;
 
 import java.util.Map;
 import java.util.Properties;

File: serving-sdk/src/main/java/org/pytorch/serve/servingsdk/Model.java
Patch:
@@ -1,5 +1,5 @@
 
-package software.amazon.ai.mms.servingsdk;
+package org.pytorch.serve.servingsdk;
 
 import java.util.List;
 

File: serving-sdk/src/main/java/org/pytorch/serve/servingsdk/ModelServerEndpoint.java
Patch:
@@ -1,8 +1,8 @@
 
-package software.amazon.ai.mms.servingsdk;
+package org.pytorch.serve.servingsdk;
 
-import software.amazon.ai.mms.servingsdk.http.Request;
-import software.amazon.ai.mms.servingsdk.http.Response;
+import org.pytorch.serve.servingsdk.http.Request;
+import org.pytorch.serve.servingsdk.http.Response;
 
 import java.io.IOException;
 

File: serving-sdk/src/main/java/org/pytorch/serve/servingsdk/ModelServerEndpointException.java
Patch:
@@ -1,6 +1,6 @@
 
 
-package software.amazon.ai.mms.servingsdk;
+package org.pytorch.serve.servingsdk;
 
 /**
  * Runtime exception for custom model server endpoint plugins

File: serving-sdk/src/main/java/org/pytorch/serve/servingsdk/Worker.java
Patch:
@@ -1,6 +1,6 @@
 
 
-package software.amazon.ai.mms.servingsdk;
+package org.pytorch.serve.servingsdk;
 
 /**
  * Describe the model worker

File: serving-sdk/src/main/java/org/pytorch/serve/servingsdk/annotations/Endpoint.java
Patch:
@@ -1,7 +1,7 @@
 
-package software.amazon.ai.mms.servingsdk.annotations;
+package org.pytorch.serve.servingsdk.annotations;
 
-import software.amazon.ai.mms.servingsdk.annotations.helpers.EndpointTypes;
+import org.pytorch.serve.servingsdk.annotations.helpers.EndpointTypes;
 
 import java.lang.annotation.Documented;
 import java.lang.annotation.ElementType;

File: serving-sdk/src/main/java/org/pytorch/serve/servingsdk/annotations/helpers/EndpointTypes.java
Patch:
@@ -1,5 +1,5 @@
 
-package software.amazon.ai.mms.servingsdk.annotations.helpers;
+package org.pytorch.serve.servingsdk.annotations.helpers;
 
 /**
  * Types of ModelServer endpoints

File: serving-sdk/src/main/java/org/pytorch/serve/servingsdk/http/Request.java
Patch:
@@ -1,6 +1,6 @@
 
 
-package software.amazon.ai.mms.servingsdk.http;
+package org.pytorch.serve.servingsdk.http;
 
 import java.io.IOException;
 import java.io.InputStream;

File: serving-sdk/src/main/java/org/pytorch/serve/servingsdk/http/Response.java
Patch:
@@ -1,6 +1,6 @@
 
 
-package software.amazon.ai.mms.servingsdk.http;
+package org.pytorch.serve.servingsdk.http;
 
 import java.io.IOException;
 import java.io.OutputStream;

File: frontend/server/src/main/java/org/pytorch/serve/wlm/WorkLoadManager.java
Patch:
@@ -123,7 +123,7 @@ public CompletableFuture<HttpResponseStatus> modelChanged(Model model, boolean i
                                             TimeUnit.SECONDS);
                         } catch (InterruptedException e) {
                             logger.warn(
-                                    "WorkerThread interrupted during waitFor, possible asynch resource cleanup.");
+                                    "WorkerThread interrupted during waitFor, possible async resource cleanup.");
                             future.complete(HttpResponseStatus.INTERNAL_SERVER_ERROR);
                             return future;
                         }

File: frontend/server/src/main/java/org/pytorch/serve/wlm/WorkLoadManager.java
Patch:
@@ -123,7 +123,7 @@ public CompletableFuture<HttpResponseStatus> modelChanged(Model model, boolean i
                                             TimeUnit.SECONDS);
                         } catch (InterruptedException e) {
                             logger.warn(
-                                    "WorkerThread interrupted during waitFor, possible asynch resource cleanup.");
+                                    "WorkerThread interrupted during waitFor, possible async resource cleanup.");
                             future.complete(HttpResponseStatus.INTERNAL_SERVER_ERROR);
                             return future;
                         }

File: frontend/server/src/main/java/org/pytorch/serve/ModelServer.java
Patch:
@@ -29,6 +29,9 @@
 import org.pytorch.serve.archive.ModelArchive;
 import org.pytorch.serve.archive.ModelException;
 import org.pytorch.serve.metrics.MetricManager;
+import org.pytorch.serve.servingsdk.ModelServerEndpoint;
+import org.pytorch.serve.servingsdk.annotations.Endpoint;
+import org.pytorch.serve.servingsdk.annotations.helpers.EndpointTypes;
 import org.pytorch.serve.servingsdk.impl.PluginsManager;
 import org.pytorch.serve.snapshot.InvalidSnapshotException;
 import org.pytorch.serve.snapshot.SnapshotManager;
@@ -40,9 +43,6 @@
 import org.pytorch.serve.wlm.WorkLoadManager;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
-import software.amazon.ai.mms.servingsdk.ModelServerEndpoint;
-import software.amazon.ai.mms.servingsdk.annotations.Endpoint;
-import software.amazon.ai.mms.servingsdk.annotations.helpers.EndpointTypes;
 
 public class ModelServer {
 

File: frontend/server/src/main/java/org/pytorch/serve/http/HttpRequestHandlerChain.java
Patch:
@@ -11,15 +11,15 @@
 import java.util.Map;
 import org.pytorch.serve.archive.ModelException;
 import org.pytorch.serve.archive.ModelNotFoundException;
+import org.pytorch.serve.servingsdk.ModelServerEndpoint;
+import org.pytorch.serve.servingsdk.ModelServerEndpointException;
 import org.pytorch.serve.servingsdk.impl.ModelServerContext;
 import org.pytorch.serve.servingsdk.impl.ModelServerRequest;
 import org.pytorch.serve.servingsdk.impl.ModelServerResponse;
 import org.pytorch.serve.util.NettyUtils;
 import org.pytorch.serve.wlm.ModelManager;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
-import software.amazon.ai.mms.servingsdk.ModelServerEndpoint;
-import software.amazon.ai.mms.servingsdk.ModelServerEndpointException;
 
 public abstract class HttpRequestHandlerChain {
     private static final Logger logger = LoggerFactory.getLogger(HttpRequestHandler.class);

File: frontend/server/src/main/java/org/pytorch/serve/http/InferenceRequestHandler.java
Patch:
@@ -14,6 +14,7 @@
 import org.pytorch.serve.archive.ModelException;
 import org.pytorch.serve.archive.ModelNotFoundException;
 import org.pytorch.serve.openapi.OpenApiUtils;
+import org.pytorch.serve.servingsdk.ModelServerEndpoint;
 import org.pytorch.serve.util.NettyUtils;
 import org.pytorch.serve.util.messages.InputParameter;
 import org.pytorch.serve.util.messages.RequestInput;
@@ -23,7 +24,6 @@
 import org.pytorch.serve.wlm.ModelManager;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
-import software.amazon.ai.mms.servingsdk.ModelServerEndpoint;
 
 /**
  * A class handling inbound HTTP requests to the management API.

File: frontend/server/src/main/java/org/pytorch/serve/http/ManagementRequestHandler.java
Patch:
@@ -23,14 +23,14 @@
 import org.pytorch.serve.archive.ModelNotFoundException;
 import org.pytorch.serve.archive.ModelVersionNotFoundException;
 import org.pytorch.serve.http.messages.RegisterModelRequest;
+import org.pytorch.serve.servingsdk.ModelServerEndpoint;
 import org.pytorch.serve.snapshot.SnapshotManager;
 import org.pytorch.serve.util.ConfigManager;
 import org.pytorch.serve.util.JsonUtils;
 import org.pytorch.serve.util.NettyUtils;
 import org.pytorch.serve.wlm.Model;
 import org.pytorch.serve.wlm.ModelManager;
 import org.pytorch.serve.wlm.WorkerThread;
-import software.amazon.ai.mms.servingsdk.ModelServerEndpoint;
 
 /**
  * A class handling inbound HTTP requests to the management API.

File: frontend/server/src/main/java/org/pytorch/serve/servingsdk/impl/ModelServerContext.java
Patch:
@@ -3,10 +3,10 @@
 import java.util.HashMap;
 import java.util.Map;
 import java.util.Properties;
+import org.pytorch.serve.servingsdk.Context;
+import org.pytorch.serve.servingsdk.Model;
 import org.pytorch.serve.util.ConfigManager;
 import org.pytorch.serve.wlm.ModelManager;
-import software.amazon.ai.mms.servingsdk.Context;
-import software.amazon.ai.mms.servingsdk.Model;
 
 public class ModelServerContext implements Context {
     @Override

File: frontend/server/src/main/java/org/pytorch/serve/servingsdk/impl/ModelServerModel.java
Patch:
@@ -2,9 +2,9 @@
 
 import java.util.ArrayList;
 import java.util.List;
+import org.pytorch.serve.servingsdk.Model;
+import org.pytorch.serve.servingsdk.Worker;
 import org.pytorch.serve.wlm.ModelManager;
-import software.amazon.ai.mms.servingsdk.Model;
-import software.amazon.ai.mms.servingsdk.Worker;
 
 public class ModelServerModel implements Model {
     private final org.pytorch.serve.wlm.Model model;

File: frontend/server/src/main/java/org/pytorch/serve/servingsdk/impl/ModelServerRequest.java
Patch:
@@ -7,7 +7,7 @@
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
-import software.amazon.ai.mms.servingsdk.http.Request;
+import org.pytorch.serve.servingsdk.http.Request;
 
 public class ModelServerRequest implements Request {
     private FullHttpRequest req;

File: frontend/server/src/main/java/org/pytorch/serve/servingsdk/impl/ModelServerResponse.java
Patch:
@@ -5,7 +5,7 @@
 import io.netty.handler.codec.http.HttpHeaderNames;
 import io.netty.handler.codec.http.HttpResponseStatus;
 import java.io.OutputStream;
-import software.amazon.ai.mms.servingsdk.http.Response;
+import org.pytorch.serve.servingsdk.http.Response;
 
 public class ModelServerResponse implements Response {
 

File: frontend/server/src/main/java/org/pytorch/serve/servingsdk/impl/ModelWorker.java
Patch:
@@ -1,8 +1,8 @@
 package org.pytorch.serve.servingsdk.impl;
 
+import org.pytorch.serve.servingsdk.Worker;
 import org.pytorch.serve.wlm.WorkerState;
 import org.pytorch.serve.wlm.WorkerThread;
-import software.amazon.ai.mms.servingsdk.Worker;
 
 public class ModelWorker implements Worker {
     private boolean running;

File: frontend/server/src/main/java/org/pytorch/serve/servingsdk/impl/PluginsManager.java
Patch:
@@ -5,11 +5,11 @@
 import java.util.Map;
 import java.util.ServiceLoader;
 import org.pytorch.serve.http.InvalidPluginException;
+import org.pytorch.serve.servingsdk.ModelServerEndpoint;
+import org.pytorch.serve.servingsdk.annotations.Endpoint;
+import org.pytorch.serve.servingsdk.annotations.helpers.EndpointTypes;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
-import software.amazon.ai.mms.servingsdk.ModelServerEndpoint;
-import software.amazon.ai.mms.servingsdk.annotations.Endpoint;
-import software.amazon.ai.mms.servingsdk.annotations.helpers.EndpointTypes;
 
 public final class PluginsManager {
 

File: frontend/server/src/main/java/org/pytorch/serve/wlm/Model.java
Patch:
@@ -172,7 +172,9 @@ public void pollBatch(String threadId, long waitTime, Map<String, Job> jobsRepo)
             }
             logger.trace("sending jobs, size: {}", jobsRepo.size());
         } finally {
-            lock.unlock();
+            if (lock.isHeldByCurrentThread()) {
+                lock.unlock();
+            }
         }
     }
 

File: frontend/server/src/main/java/org/pytorch/serve/wlm/ModelManager.java
Patch:
@@ -171,6 +171,8 @@ public HttpResponseStatus unregisterModel(String modelName, String versionId) {
             if (vmodel.getAllVersions().size() == 0) {
                 modelsNameMap.remove(modelName);
             }
+
+            ModelArchive.removeModel(configManager.getModelStore(), model.getModelUrl());
         } catch (ModelVersionNotFoundException e) {
             logger.warn("Model {} version {} not found.", modelName, versionId);
             httpResponseStatus = HttpResponseStatus.BAD_REQUEST;

File: frontend/server/src/main/java/org/pytorch/serve/wlm/WorkerThread.java
Patch:
@@ -161,7 +161,8 @@ public void run() {
                 req = null;
             }
         } catch (InterruptedException e) {
-            if (state == WorkerState.WORKER_SCALED_DOWN) {
+            logger.debug("System state is : " + state);
+            if (state == WorkerState.WORKER_SCALED_DOWN || state == WorkerState.WORKER_STOPPED) {
                 logger.debug("Shutting down the thread .. Scaling down.");
             } else {
                 logger.debug(

File: serving-sdk/src/main/java/org/pytorch/serve/servingsdk/Context.java
Patch:
@@ -1,5 +1,5 @@
 
-package software.amazon.ai.mms.servingsdk;
+package org.pytorch.serve.servingsdk;
 
 import java.util.Map;
 import java.util.Properties;

File: serving-sdk/src/main/java/org/pytorch/serve/servingsdk/Model.java
Patch:
@@ -1,5 +1,5 @@
 
-package software.amazon.ai.mms.servingsdk;
+package org.pytorch.serve.servingsdk;
 
 import java.util.List;
 

File: serving-sdk/src/main/java/org/pytorch/serve/servingsdk/ModelServerEndpoint.java
Patch:
@@ -1,8 +1,8 @@
 
-package software.amazon.ai.mms.servingsdk;
+package org.pytorch.serve.servingsdk;
 
-import software.amazon.ai.mms.servingsdk.http.Request;
-import software.amazon.ai.mms.servingsdk.http.Response;
+import org.pytorch.serve.servingsdk.http.Request;
+import org.pytorch.serve.servingsdk.http.Response;
 
 import java.io.IOException;
 

File: serving-sdk/src/main/java/org/pytorch/serve/servingsdk/ModelServerEndpointException.java
Patch:
@@ -1,6 +1,6 @@
 
 
-package software.amazon.ai.mms.servingsdk;
+package org.pytorch.serve.servingsdk;
 
 /**
  * Runtime exception for custom model server endpoint plugins

File: serving-sdk/src/main/java/org/pytorch/serve/servingsdk/Worker.java
Patch:
@@ -1,6 +1,6 @@
 
 
-package software.amazon.ai.mms.servingsdk;
+package org.pytorch.serve.servingsdk;
 
 /**
  * Describe the model worker

File: serving-sdk/src/main/java/org/pytorch/serve/servingsdk/annotations/Endpoint.java
Patch:
@@ -1,7 +1,7 @@
 
-package software.amazon.ai.mms.servingsdk.annotations;
+package org.pytorch.serve.servingsdk.annotations;
 
-import software.amazon.ai.mms.servingsdk.annotations.helpers.EndpointTypes;
+import org.pytorch.serve.servingsdk.annotations.helpers.EndpointTypes;
 
 import java.lang.annotation.Documented;
 import java.lang.annotation.ElementType;

File: serving-sdk/src/main/java/org/pytorch/serve/servingsdk/annotations/helpers/EndpointTypes.java
Patch:
@@ -1,5 +1,5 @@
 
-package software.amazon.ai.mms.servingsdk.annotations.helpers;
+package org.pytorch.serve.servingsdk.annotations.helpers;
 
 /**
  * Types of ModelServer endpoints

File: serving-sdk/src/main/java/org/pytorch/serve/servingsdk/http/Request.java
Patch:
@@ -1,6 +1,6 @@
 
 
-package software.amazon.ai.mms.servingsdk.http;
+package org.pytorch.serve.servingsdk.http;
 
 import java.io.IOException;
 import java.io.InputStream;

File: serving-sdk/src/main/java/org/pytorch/serve/servingsdk/http/Response.java
Patch:
@@ -1,6 +1,6 @@
 
 
-package software.amazon.ai.mms.servingsdk.http;
+package org.pytorch.serve.servingsdk.http;
 
 import java.io.IOException;
 import java.io.OutputStream;

File: frontend/server/src/main/java/org/pytorch/serve/wlm/Model.java
Patch:
@@ -172,7 +172,9 @@ public void pollBatch(String threadId, long waitTime, Map<String, Job> jobsRepo)
             }
             logger.trace("sending jobs, size: {}", jobsRepo.size());
         } finally {
-            lock.unlock();
+            if (lock.isHeldByCurrentThread()) {
+                lock.unlock();
+            }
         }
     }
 

File: frontend/server/src/main/java/org/pytorch/serve/wlm/WorkerThread.java
Patch:
@@ -161,7 +161,8 @@ public void run() {
                 req = null;
             }
         } catch (InterruptedException e) {
-            if (state == WorkerState.WORKER_SCALED_DOWN) {
+            logger.debug("System state is : " + state);
+            if (state == WorkerState.WORKER_SCALED_DOWN || state == WorkerState.WORKER_STOPPED) {
                 logger.debug("Shutting down the thread .. Scaling down.");
             } else {
                 logger.debug(

File: frontend/modelarchive/src/test/java/org/pytorch/serve/archive/ModelArchiveTest.java
Patch:
@@ -38,6 +38,7 @@ public void test() throws ModelException, IOException {
                         modelStore,
                         "https://s3.amazonaws.com/model-server/models/squeezenet_v1.1/squeezenet_v1.1.model");
         Assert.assertEquals(archive.getModelName(), null);
+        FileUtils.deleteQuietly(new File(modelStore, "squeezenet_v1.1.model"));
 
         ModelArchive.downloadModel(modelStore, "/../noop-v1.0");
     }

File: frontend/server/src/test/java/org/pytorch/serve/util/ConfigManagerTest.java
Patch:
@@ -110,9 +110,11 @@ public void testNoEnvVars()
         modifyEnv("TS_DEFAULT_RESPONSE_TIMEOUT", "130");
         ConfigManager.Arguments args = new ConfigManager.Arguments();
         args.setModels(new String[] {"noop_v0.1"});
+        args.setSnapshotDisabled(true);
         ConfigManager.init(args);
         ConfigManager configManager = ConfigManager.getInstance();
         Assert.assertEquals("false", configManager.getEnableEnvVarsConfig());
         Assert.assertEquals(120, configManager.getDefaultResponseTimeout());
+        modifyEnv("TS_DEFAULT_RESPONSE_TIMEOUT", "120");
     }
 }

File: frontend/server/src/main/java/org/pytorch/serve/http/ManagementRequestHandler.java
Patch:
@@ -141,7 +141,7 @@ private void handleDescribeModel(
         ArrayList<DescribeModelResponse> resp = new ArrayList<DescribeModelResponse>();
 
         if ("all".equals(modelVersion)) {
-            for (Map.Entry<Double, Model> m : modelManager.getAllModelVersions(modelName)) {
+            for (Map.Entry<String, Model> m : modelManager.getAllModelVersions(modelName)) {
                 resp.add(createModelResponse(modelManager, modelName, m.getValue()));
             }
         } else {
@@ -380,8 +380,8 @@ private void setDefaultModelVersion(
         if (httpResponseStatus == HttpResponseStatus.NOT_FOUND) {
             throw new ModelNotFoundException("Model not found: " + modelName);
         } else if (httpResponseStatus == HttpResponseStatus.FORBIDDEN) {
-            throw new InternalServerException(
-                    "Cannot set version " + newModelVersion + " as default for model " + modelName);
+            throw new InvalidModelVersionException(
+                    "Model version " + newModelVersion + " does not exist for model " + modelName);
         }
         String msg =
                 "Default vesion succsesfully updated for model \""

File: frontend/server/src/main/java/org/pytorch/serve/snapshot/SnapshotManager.java
Patch:
@@ -54,10 +54,10 @@ private void saveSnapshot(String snapshotName) {
             int modelCount = 0;
             for (Map.Entry<String, Model> m : defModels.entrySet()) {
 
-                Set<Entry<Double, Model>> versionModels =
+                Set<Entry<String, Model>> versionModels =
                         modelManager.getAllModelVersions(m.getKey());
                 Map<String, ModelSnapshot> modelInfoMap = new HashMap<>();
-                for (Entry<Double, Model> versionedModel : versionModels) {
+                for (Entry<String, Model> versionedModel : versionModels) {
                     ModelSnapshot model = new ModelSnapshot();
                     String version = String.valueOf(versionedModel.getKey());
                     model.setBatchSize(versionedModel.getValue().getBatchSize());

File: frontend/server/src/test/java/org/pytorch/serve/SnapshotTest.java
Patch:
@@ -214,7 +214,7 @@ private void testSecondModelVersionSnapshot(Channel channel) throws InterruptedE
     private void testSetDefaultSnapshot(Channel channel) throws InterruptedException {
         TestUtils.setResult(null);
         TestUtils.setLatch(new CountDownLatch(1));
-        String requestURL = "/models/noopversioned/1.11/set-default";
+        String requestURL = "/models/noopversioned/1.2.1/set-default";
 
         HttpRequest req =
                 new DefaultFullHttpRequest(HttpVersion.HTTP_1_1, HttpMethod.PUT, requestURL);

File: frontend/server/src/main/java/org/pytorch/serve/snapshot/SnapshotManager.java
Patch:
@@ -54,10 +54,10 @@ private void saveSnapshot(String snapshotName) {
             int modelCount = 0;
             for (Map.Entry<String, Model> m : defModels.entrySet()) {
 
-                Set<Entry<Double, Model>> versionModels =
+                Set<Entry<String, Model>> versionModels =
                         modelManager.getAllModelVersions(m.getKey());
                 Map<String, ModelSnapshot> modelInfoMap = new HashMap<>();
-                for (Entry<Double, Model> versionedModel : versionModels) {
+                for (Entry<String, Model> versionedModel : versionModels) {
                     ModelSnapshot model = new ModelSnapshot();
                     String version = String.valueOf(versionedModel.getKey());
                     model.setBatchSize(versionedModel.getValue().getBatchSize());

File: frontend/server/src/test/java/org/pytorch/serve/SnapshotTest.java
Patch:
@@ -214,7 +214,7 @@ private void testSecondModelVersionSnapshot(Channel channel) throws InterruptedE
     private void testSetDefaultSnapshot(Channel channel) throws InterruptedException {
         TestUtils.setResult(null);
         TestUtils.setLatch(new CountDownLatch(1));
-        String requestURL = "/models/noopversioned/1.11/set-default";
+        String requestURL = "/models/noopversioned/1.2.1/set-default";
 
         HttpRequest req =
                 new DefaultFullHttpRequest(HttpVersion.HTTP_1_1, HttpMethod.PUT, requestURL);

File: frontend/modelarchive/src/test/java/org/pytorch/serve/archive/ModelArchiveTest.java
Patch:
@@ -38,6 +38,7 @@ public void test() throws ModelException, IOException {
                         modelStore,
                         "https://s3.amazonaws.com/model-server/models/squeezenet_v1.1/squeezenet_v1.1.model");
         Assert.assertEquals(archive.getModelName(), null);
+        FileUtils.deleteQuietly(new File(modelStore, "squeezenet_v1.1.model"));
 
         ModelArchive.downloadModel(modelStore, "/../noop-v1.0");
     }

File: frontend/server/src/test/java/org/pytorch/serve/util/ConfigManagerTest.java
Patch:
@@ -110,9 +110,11 @@ public void testNoEnvVars()
         modifyEnv("TS_DEFAULT_RESPONSE_TIMEOUT", "130");
         ConfigManager.Arguments args = new ConfigManager.Arguments();
         args.setModels(new String[] {"noop_v0.1"});
+        args.setSnapshotDisabled(true);
         ConfigManager.init(args);
         ConfigManager configManager = ConfigManager.getInstance();
         Assert.assertEquals("false", configManager.getEnableEnvVarsConfig());
         Assert.assertEquals(120, configManager.getDefaultResponseTimeout());
+        modifyEnv("TS_DEFAULT_RESPONSE_TIMEOUT", "120");
     }
 }

File: frontend/server/src/main/java/org/pytorch/serve/http/ManagementRequestHandler.java
Patch:
@@ -373,8 +373,8 @@ private void setDefaultModelVersion(
         if (httpResponseStatus == HttpResponseStatus.NOT_FOUND) {
             throw new ModelNotFoundException("Model not found: " + modelName);
         } else if (httpResponseStatus == HttpResponseStatus.FORBIDDEN) {
-            throw new InternalServerException(
-                    "Cannot set version " + newModelVersion + " as default for model " + modelName);
+            throw new InvalidModelVersionException(
+                    "Model version " + newModelVersion + " does not exist for model " + modelName);
         }
         String msg =
                 "Default vesion succsesfully updated for model \""

File: frontend/server/src/main/java/org/pytorch/serve/wlm/ModelManager.java
Patch:
@@ -192,8 +192,7 @@ public HttpResponseStatus setDefaultVersion(String modelName, String newModelVer
         try {
             vmodel.setDefaultVersion(newModelVersion);
         } catch (InvalidModelVersionException e) {
-            logger.warn(
-                    "Cannot set version {} as default for model {}", newModelVersion, modelName);
+            logger.warn("Model version {} does not exist for model {}", newModelVersion, modelName);
             httpResponseStatus = HttpResponseStatus.FORBIDDEN;
         }
 

File: frontend/server/src/main/java/org/pytorch/serve/http/ManagementRequestHandler.java
Patch:
@@ -138,7 +138,7 @@ private void handleDescribeModel(
         ArrayList<DescribeModelResponse> resp = new ArrayList<DescribeModelResponse>();
 
         if ("all".equals(modelVersion)) {
-            for (Map.Entry<Double, Model> m : modelManager.getAllModelVersions(modelName)) {
+            for (Map.Entry<String, Model> m : modelManager.getAllModelVersions(modelName)) {
                 resp.add(createModelResponse(modelManager, modelName, m.getValue()));
             }
         } else {

File: frontend/server/src/main/java/org/pytorch/serve/wlm/ModelManager.java
Patch:
@@ -286,7 +286,7 @@ public void modelWorkerStatus(final String modelName, final ChannelHandlerContex
                     int numWorking = 0;
                     int numScaled = 0;
                     ModelVersionedRefs vmodel = modelsNameMap.get(modelName);
-                    for (Map.Entry<Double, Model> m : vmodel.getAllVersions()) {
+                    for (Map.Entry<String, Model> m : vmodel.getAllVersions()) {
                         numScaled += m.getValue().getMinWorkers();
                         numWorking += wlm.getNumRunningWorkers(m.getValue().getModelVersionName());
                     }
@@ -332,7 +332,7 @@ public Model getModel(String modelName, String versionId) {
         return vmodel.getVersionModel(versionId);
     }
 
-    public Set<Entry<Double, Model>> getAllModelVersions(String modelName)
+    public Set<Entry<String, Model>> getAllModelVersions(String modelName)
             throws ModelNotFoundException {
         ModelVersionedRefs vmodel = modelsNameMap.get(modelName);
         if (vmodel == null) {

File: frontend/server/src/test/java/org/pytorch/serve/ModelServerTest.java
Patch:
@@ -152,7 +152,7 @@ public void test()
         testLoadModelWithInitialWorkers(managementChannel, "noop.mar", "noopversioned");
         testLoadModelWithInitialWorkers(managementChannel, "noop_v2.mar", "noopversioned");
         testDescribeModel(managementChannel, "noopversioned", null, "1.11");
-        testDescribeModel(managementChannel, "noopversioned", "all", "1.11");
+        testDescribeModel(managementChannel, "noopversioned", "all", "1.21");
         testDescribeModel(managementChannel, "noopversioned", "1.11", "1.11");
         testPredictions(channel, "noopversioned", "OK", "1.21");
         testSetDefault(managementChannel, "noopversioned", "1.21");

File: frontend/server/src/main/java/org/pytorch/serve/snapshot/SnapshotManager.java
Patch:
@@ -19,7 +19,7 @@
 
 public final class SnapshotManager {
 
-    private static final Logger logger = LoggerFactory.getLogger(ModelManager.class);
+    private static final Logger logger = LoggerFactory.getLogger(SnapshotManager.class);
 
     private static SnapshotManager snapshotManager;
 

File: frontend/server/src/main/java/org/pytorch/serve/wlm/ModelVersionedRefs.java
Patch:
@@ -55,8 +55,9 @@ public void addVersionModel(Model model, String versionId)
                             + model.getModelName());
         }
 
-        // TODO what if user wants to keep existing default as it is?
-        this.setDefaultVersion(versionId);
+        if (this.defaultVersion == null) {
+            this.setDefaultVersion(versionId);
+        }
     }
 
     /**

File: frontend/server/src/main/java/org/pytorch/serve/ModelServer.java
Patch:
@@ -73,6 +73,7 @@ public static void main(String[] args) {
             Runtime.getRuntime()
                     .addShutdownHook(
                             new Thread() {
+                                @Override
                                 public void run() {
                                     modelServer.stop();
                                 }

File: frontend/server/src/main/java/org/pytorch/serve/util/ConfigManager.java
Patch:
@@ -705,7 +705,7 @@ public static Options getOptions() {
                     Option.builder("ncs")
                             .longOpt("no-config-snapshot")
                             .argName("NO-CONFIG-SNAPSHOT")
-                            .desc("cofig")
+                            .desc("disable torchserve snapshot")
                             .build());
             return options;
         }

File: frontend/server/src/test/java/org/pytorch/serve/util/ConfigManagerTest.java
Patch:
@@ -110,6 +110,7 @@ public void testNoEnvVars()
         modifyEnv("TS_DEFAULT_RESPONSE_TIMEOUT", "130");
         ConfigManager.Arguments args = new ConfigManager.Arguments();
         args.setModels(new String[] {"noop_v0.1"});
+        args.setSnapshotDisabled(true);
         ConfigManager.init(args);
         ConfigManager configManager = ConfigManager.getInstance();
         Assert.assertEquals("false", configManager.getEnableEnvVarsConfig());

File: frontend/server/src/main/java/org/pytorch/serve/http/ApiDescriptionRequestHandler.java
Patch:
@@ -39,7 +39,8 @@ protected void handleRequest(
     }
 
     private boolean isApiDescription(String[] segments) {
-        return segments.length == 0 || segments[1].equals("api-description");
+        return segments.length == 0
+                || (segments.length == 2 && segments[1].equals("api-description"));
     }
 
     private void handleApiDescription(ChannelHandlerContext ctx) {

File: frontend/server/src/main/java/org/pytorch/serve/util/ConfigManager.java
Patch:
@@ -117,6 +117,7 @@ private ConfigManager(Arguments args) {
         if (filePath == null) {
             filePath = args.getTsConfigFile();
             if (filePath == null) {
+                filePath = System.getProperty("tsConfigFile", "config.properties");
                 if (filePath == null) {
                     filePath = getLastSnapshot();
                     if (filePath == null) {

File: frontend/server/src/test/java/org/pytorch/serve/util/ConfigManagerTest.java
Patch:
@@ -106,10 +106,10 @@ public void test()
     @Test
     public void testNoEnvVars()
             throws IllegalAccessException, NoSuchFieldException, ClassNotFoundException {
+        System.setProperty("tsConfigFile", "src/test/resources/config_test_env.properties");
         modifyEnv("TS_DEFAULT_RESPONSE_TIMEOUT", "130");
         ConfigManager.Arguments args = new ConfigManager.Arguments();
         args.setModels(new String[] {"noop_v0.1"});
-        args.setTsConfigFile("src/test/resources/config_test_env.properties");
         ConfigManager.init(args);
         ConfigManager configManager = ConfigManager.getInstance();
         Assert.assertEquals("false", configManager.getEnableEnvVarsConfig());

File: frontend/server/src/test/java/org/pytorch/serve/util/ConfigManagerTest.java
Patch:
@@ -106,10 +106,10 @@ public void test()
     @Test
     public void testNoEnvVars()
             throws IllegalAccessException, NoSuchFieldException, ClassNotFoundException {
-        System.setProperty("tsConfigFile", "src/test/resources/config_test_env.properties");
         modifyEnv("TS_DEFAULT_RESPONSE_TIMEOUT", "130");
         ConfigManager.Arguments args = new ConfigManager.Arguments();
         args.setModels(new String[] {"noop_v0.1"});
+        args.setTsConfigFile("src/test/resources/config_test_env.properties");
         ConfigManager.init(args);
         ConfigManager configManager = ConfigManager.getInstance();
         Assert.assertEquals("false", configManager.getEnableEnvVarsConfig());

File: frontend/server/src/main/java/org/pytorch/serve/snapshot/SnapshotSerializer.java
Patch:
@@ -2,11 +2,10 @@
 
 import java.io.IOException;
 import java.util.List;
-import org.pytorch.serve.http.ConflictStatusException;
 
 public interface SnapshotSerializer {
 
-    public void saveSnapshot(Snapshot snapshot) throws IOException, ConflictStatusException;
+    public void saveSnapshot(Snapshot snapshot) throws IOException;
 
     public Snapshot getSnapshot(String modelSnapshot) throws IOException;
 

File: frontend/server/src/main/java/org/pytorch/serve/util/ConfigManager.java
Patch:
@@ -463,7 +463,7 @@ private String getLastSnapshot() {
                     latestSnapshotPath = lastFilePath.get().toString();
                 }
             } catch (IOException e) {
-                e.printStackTrace();
+                e.printStackTrace(); // NOPMD
             }
         }
 

File: frontend/modelarchive/src/main/java/org/pytorch/serve/archive/ModelArchive.java
Patch:
@@ -62,7 +62,7 @@ public static ModelArchive downloadModel(String modelStore, String url)
                 FileUtils.copyURLToFile(new URL(url), modelLocation);
             } catch (IOException e) {
                 FileUtils.deleteQuietly(modelLocation);
-                throw e;
+                throw new IOException("Invalid model url: " + url, e);
             }
         }
 

File: frontend/modelarchive/src/test/java/org/pytorch/serve/archive/ModelArchiveTest.java
Patch:
@@ -43,7 +43,7 @@ public void test() throws ModelException, IOException {
         ModelArchive.downloadModel(modelStore, "/../noop-v1.0");
     }
 
-    @Test(expectedExceptions = IOException.class)
+    @Test(expectedExceptions = DownloadModelException.class)
     public void testInvalidURL() throws ModelException, IOException {
         String modelStore = "src/test/resources/models";
         // load model for s3 --> This will fail as this model is not compatible with
@@ -54,7 +54,7 @@ public void testInvalidURL() throws ModelException, IOException {
                 "https://s3.amazonaws.com/model-server/models/squeezenet_v1.1/squeezenet_v1.1.mod");
     }
 
-    @Test(expectedExceptions = IOException.class)
+    @Test(expectedExceptions = DownloadModelException.class)
     public void testMalformURL() throws ModelException, IOException {
         String modelStore = "src/test/resources/models";
         // load model for s3 --> This will fail as this model is not compatible with

File: frontend/server/src/main/java/org/pytorch/serve/util/ConfigManager.java
Patch:
@@ -459,7 +459,9 @@ private String getLastSnapshot() {
                         Files.list(configPath)
                                 .filter(f -> !Files.isDirectory(f))
                                 .max(Comparator.comparingLong(f -> f.toFile().lastModified()));
-                if (lastFilePath.isPresent()) latestSnapshotPath = lastFilePath.get().toString();
+                if (lastFilePath.isPresent()) {
+                    latestSnapshotPath = lastFilePath.get().toString();
+                }
             } catch (IOException e) {
                 e.printStackTrace();
             }

File: frontend/modelarchive/src/test/java/org/pytorch/serve/archive/ModelArchiveTest.java
Patch:
@@ -38,11 +38,12 @@ public void test() throws ModelException, IOException {
                         modelStore,
                         "https://s3.amazonaws.com/model-server/models/squeezenet_v1.1/squeezenet_v1.1.model");
         Assert.assertEquals(archive.getModelName(), null);
+        FileUtils.deleteQuietly(new File(modelStore, "squeezenet_v1.1.model"));
 
         ModelArchive.downloadModel(modelStore, "/../noop-v1.0");
     }
 
-    @Test(expectedExceptions = DownloadModelException.class)
+    @Test(expectedExceptions = IOException.class)
     public void testInvalidURL() throws ModelException, IOException {
         String modelStore = "src/test/resources/models";
         // load model for s3 --> This will fail as this model is not compatible with
@@ -53,7 +54,7 @@ public void testInvalidURL() throws ModelException, IOException {
                 "https://s3.amazonaws.com/model-server/models/squeezenet_v1.1/squeezenet_v1.1.mod");
     }
 
-    @Test(expectedExceptions = DownloadModelException.class)
+    @Test(expectedExceptions = IOException.class)
     public void testMalformURL() throws ModelException, IOException {
         String modelStore = "src/test/resources/models";
         // load model for s3 --> This will fail as this model is not compatible with

File: frontend/server/src/main/java/org/pytorch/serve/ModelServer.java
Patch:
@@ -90,7 +90,7 @@ public void startAndWait() throws InterruptedException, IOException, GeneralSecu
             MetricManager.scheduleMetrics(configManager);
             System.out.println("Model server started."); // NOPMD
 
-            SnapshotManager.getInstance().saveSnapshot("startup");
+            SnapshotManager.getInstance().saveStartupSnapshot();
 
             channelFutures.get(0).sync();
         } catch (InvalidPropertiesFormatException e) {
@@ -356,7 +356,7 @@ public void stop() {
             future.channel().close();
         }
 
-        SnapshotManager.getInstance().saveSnapshot("shutdown");
+        SnapshotManager.getInstance().saveShutdownSnapshot();
         serverGroups.shutdown(true);
         serverGroups.init();
     }

File: frontend/server/src/main/java/org/pytorch/serve/http/ManagementRequestHandler.java
Patch:
@@ -241,7 +241,7 @@ private void handleRegisterModel(
         final String msg = "Model \"" + modelName + "\" registered";
         if (initialWorkers <= 0) {
             NettyUtils.sendJsonResponse(ctx, new StatusResponse(msg));
-            SnapshotManager.getInstance().saveSnapshot("snapshot");
+            SnapshotManager.getInstance().saveSnapshot();
             return;
         }
 
@@ -280,7 +280,7 @@ private void handleUnregisterModel(
                     "Cannot remove default version for model " + modelName);
         }
         String msg = "Model \"" + modelName + "\" unregistered";
-        SnapshotManager.getInstance().saveSnapshot("snapshot");
+        SnapshotManager.getInstance().saveSnapshot();
         NettyUtils.sendJsonResponse(ctx, new StatusResponse(msg));
     }
 

File: frontend/server/src/main/java/org/pytorch/serve/wlm/WorkLoadManager.java
Patch:
@@ -92,10 +92,10 @@ public CompletableFuture<HttpResponseStatus> modelChanged(Model model, boolean i
             if (minWorker == 0) {
                 threads = workers.remove(model.getModelVersionName());
                 if (threads == null) {
-                    future.complete(HttpResponseStatus.OK);
                     if (!isStartup) {
-                        SnapshotManager.getInstance().saveSnapshot("snapshot");
+                        SnapshotManager.getInstance().saveSnapshot();
                     }
+                    future.complete(HttpResponseStatus.OK);
                     return future;
                 }
             } else {
@@ -140,7 +140,7 @@ public CompletableFuture<HttpResponseStatus> modelChanged(Model model, boolean i
                 future.complete(HttpResponseStatus.OK);
             }
             if (!isStartup) {
-                SnapshotManager.getInstance().saveSnapshot("snapshot");
+                SnapshotManager.getInstance().saveSnapshot();
             }
             return future;
         }

File: frontend/server/src/main/java/org/pytorch/serve/metrics/Metric.java
Patch:
@@ -11,7 +11,7 @@ public class Metric {
 
     private static final Pattern PATTERN =
             Pattern.compile(
-                    "\\s*(\\w+)\\.(\\w+):([0-9\\-,.e]+)\\|#([^|]*)\\|#hostname:([^,]+),([^,]+)(,(.*))?");
+                    "\\s*([\\w\\s]+)\\.([\\w\\s]+):([0-9\\-,.e]+)\\|#([^|]*)\\|#hostname:([^,]+),([^,]+)(,(.*))?");
 
     @SerializedName("MetricName")
     private String metricName;

File: frontend/server/src/main/java/org/pytorch/serve/metrics/Metric.java
Patch:
@@ -11,7 +11,7 @@ public class Metric {
 
     private static final Pattern PATTERN =
             Pattern.compile(
-                    "\\s*(\\w+)\\.(\\w+):([0-9\\-,.e]+)\\|#([^|]*)\\|#hostname:([^,]+),([^,]+)(,(.*))?");
+                    "\\s*([\\w\\s]+)\\.([\\w\\s]+):([0-9\\-,.e]+)\\|#([^|]*)\\|#hostname:([^,]+),([^,]+)(,(.*))?");
 
     @SerializedName("MetricName")
     private String metricName;

File: frontend/server/src/main/java/org/pytorch/serve/http/ManagementRequestHandler.java
Patch:
@@ -136,7 +136,7 @@ private void handleDescribeModel(
         ModelManager modelManager = ModelManager.getInstance();
         ArrayList<DescribeModelResponse> resp = new ArrayList<DescribeModelResponse>();
 
-        if (modelVersion != null && ("all").equals(modelVersion)) {
+        if ("all".equals(modelVersion)) {
             for (Map.Entry<Double, Model> m : modelManager.getAllModelVersions(modelName)) {
                 resp.add(createModelResponse(modelManager, modelName, m.getValue()));
             }

File: frontend/server/src/test/java/org/pytorch/serve/ModelServerTest.java
Patch:
@@ -696,7 +696,8 @@ private void testModelRegisterWithDefaultWorkers(Channel mgmtChannel)
         mgmtChannel.writeAndFlush(req);
 
         latch.await();
-        DescribeModelResponse[] resp = JsonUtils.GSON.fromJson(result, DescribeModelResponse[].class);
+        DescribeModelResponse[] resp =
+                JsonUtils.GSON.fromJson(result, DescribeModelResponse[].class);
         Assert.assertEquals(httpStatus, HttpResponseStatus.OK);
         Assert.assertEquals(resp[0].getMinWorkers(), 1);
         unloadTests(mgmtChannel, "noop_default_model_workers");

File: frontend/server/src/test/java/org/pytorch/serve/ModelServerTest.java
Patch:
@@ -696,9 +696,9 @@ private void testModelRegisterWithDefaultWorkers(Channel mgmtChannel)
         mgmtChannel.writeAndFlush(req);
 
         latch.await();
-        DescribeModelResponse resp = JsonUtils.GSON.fromJson(result, DescribeModelResponse.class);
+        DescribeModelResponse[] resp = JsonUtils.GSON.fromJson(result, DescribeModelResponse[].class);
         Assert.assertEquals(httpStatus, HttpResponseStatus.OK);
-        Assert.assertEquals(resp.getMinWorkers(), 1);
+        Assert.assertEquals(resp[0].getMinWorkers(), 1);
         unloadTests(mgmtChannel, "noop_default_model_workers");
         setConfiguration("default_workers_per_model", "0");
     }

File: frontend/server/src/main/java/org/pytorch/serve/http/ManagementRequestHandler.java
Patch:
@@ -245,6 +245,8 @@ private void handleUnregisterModel(
             throw new InternalServerException("Interrupted while cleaning resources: " + modelName);
         } else if (httpResponseStatus == HttpResponseStatus.REQUEST_TIMEOUT) {
             throw new RequestTimeoutException("Timed out while cleaning resources: " + modelName);
+        }else if (httpResponseStatus == HttpResponseStatus.FORBIDDEN) {
+            throw new InternalServerException("Cannot remove default version " + modelVersion + " for model " + modelName);
         }
         String msg = "Model \"" + modelName + "\" unregistered";
         NettyUtils.sendJsonResponse(ctx, new StatusResponse(msg));

File: frontend/server/src/main/java/org/pytorch/serve/wlm/ModelVersionedRefs.java
Patch:
@@ -125,7 +125,7 @@ public Model removeVersionModel(String versionId)
      * @param A String specifying a valid version Id
      * @return On Success - a Model Obj previously registered On Failure - null
      */
-    public Model getVersionModel(String versionId) throws InvalidModelVersionException {
+    public Model getVersionModel(String versionId){
         Model model = null;
         if (versionId != null) {
             validateVersionId(versionId);
@@ -143,7 +143,7 @@ public Model getVersionModel(String versionId) throws InvalidModelVersionExcepti
      * @param None
      * @return On Success - a Model Obj corresponding to the default Model obj On Failure - null
      */
-    public Model getDefaultModel() throws InvalidModelVersionException {
+    public Model getDefaultModel(){
         // TODO should not throw invalid here as it has been already validated??
         return this.modelsVersionMap.get(this.defaultVersion);
     }

File: frontend/server/src/main/java/org/pytorch/serve/http/ManagementRequestHandler.java
Patch:
@@ -152,7 +152,7 @@ private void handleDescribeModel(
         resp.setModelVersion(manifest.getModel().getModelVersion());
         resp.setRuntime(manifest.getRuntime().getValue());
 
-        List<WorkerThread> workers = modelManager.getWorkers(modelName);
+        List<WorkerThread> workers = modelManager.getWorkers(model.getModelVersionName());
         for (WorkerThread worker : workers) {
             String workerId = worker.getWorkerId();
             long startTime = worker.getStartTime();

File: frontend/server/src/main/java/org/pytorch/serve/http/ManagementRequestHandler.java
Patch:
@@ -291,7 +291,7 @@ private void updateModelWorkers(
         }
         future.thenApply(
                         v -> {
-                            boolean status = modelManager.scaleRequestStatus(modelName);
+                            boolean status = modelManager.scaleRequestStatus(modelName, modelVersion);
                             if (HttpResponseStatus.OK.equals(v)) {
                                 if (status) {
                                     NettyUtils.sendJsonResponse(

File: frontend/server/src/main/java/org/pytorch/serve/openapi/OpenApiUtils.java
Patch:
@@ -18,7 +18,7 @@ public static String listApis(ConnectorType type) {
         info.setTitle("TorchServe APIs");
         info.setDescription(
                 "TorchServe is a flexible and easy to use tool for serving deep learning models");
-        info.setVersion("0.1.0");
+        info.setVersion("1.0.0");
         openApi.setInfo(info);
 
         if (ConnectorType.BOTH.equals(type) || ConnectorType.INFERENCE_CONNECTOR.equals(type)) {

File: frontend/server/src/main/java/org/pytorch/serve/http/InferenceRequestHandler.java
Patch:
@@ -166,7 +166,7 @@ private void predict(
             ModelManager modelManager = ModelManager.getInstance();
 
             // ModelVersionedRefs model = modelManager.getModel(modelName, modelVersion);
-            Model model = modelManager.getModels().get(modelName);
+            Model model = modelManager.getModel(modelName, modelVersion);
             if (model == null) {
                 throw new ModelNotFoundException("Model not found: " + modelName);
             }

File: frontend/modelarchive/src/main/java/org/pytorch/serve/archive/ModelArchive.java
Patch:
@@ -296,9 +296,9 @@ public void validate() throws InvalidModelException {
                 throw new InvalidModelException("Model name is not defined.");
             }
 
-            if (model.getHandler() == null) {
-                throw new InvalidModelException("Model handler is not defined.");
-            }
+//            if (model.getHandler() == null) {
+//                throw new InvalidModelException("Model handler is not defined.");
+//            }
 
             if (manifest.getRuntime() == null) {
                 throw new InvalidModelException("Runtime is not defined or invalid.");

File: frontend/server/src/main/java/org/pytorch/serve/util/ConfigManager.java
Patch:
@@ -84,7 +84,7 @@ public final class ConfigManager {
     public static final String MODEL_METRICS_LOGGER = "MODEL_METRICS";
     public static final String MODEL_LOGGER = "MODEL_LOG";
     public static final String MODEL_SERVER_METRICS_LOGGER = "TS_METRICS";
-    
+
     public static final String PYTHON_EXECUTABLE = "python";
 
     private Pattern blacklistPattern;

