File: dinky-metadata/dinky-metadata-paimon/src/main/java/org/dinky/metadata/driver/PaimonDriver.java
Patch:
@@ -99,7 +99,7 @@ public JdbcSelectResult query(QueryData queryData) {
                         LinkedHashMap<String, Object> rowList = new LinkedHashMap<>();
                         for (int i = 0; i < row.getFieldCount(); i++) {
                             String name = fieldTypes.get(i).name();
-                            Object data = PaimonTypeConvert.SafeGetRowData(fieldTypes.get(i), row, i);
+                            Object data = PaimonTypeConvert.getRowDataSafe(fieldTypes.get(i), row, i);
                             rowList.put(name, data);
                         }
                         datas.add(rowList);

File: dinky-core/src/main/java/org/dinky/explainer/Explainer.java
Patch:
@@ -24,6 +24,7 @@
 import org.dinky.data.exception.DinkyException;
 import org.dinky.data.job.JobStatement;
 import org.dinky.data.job.JobStatementType;
+import org.dinky.data.job.SqlCategory;
 import org.dinky.data.job.SqlType;
 import org.dinky.data.model.LineageRel;
 import org.dinky.data.result.ExplainResult;
@@ -212,7 +213,7 @@ public List<LineageRel> getLineage(String statement) {
             try {
                 if (sqlType.equals(SqlType.INSERT)) {
                     lineageRelList.addAll(executor.getLineage(sql));
-                } else if (!sqlType.equals(SqlType.SELECT) && !sqlType.equals(SqlType.PRINT)) {
+                } else if (SqlCategory.DDL.equals(sqlType.getCategory())) {
                     jobRunnerFactory.getJobRunner(item.getStatementType()).run(item);
                 }
             } catch (Exception e) {

File: dinky-core/src/main/java/org/dinky/executor/ExecutorFactory.java
Patch:
@@ -38,7 +38,7 @@ public static Executor getDefaultExecutor() {
     }
 
     public static Executor buildExecutor(ExecutorConfig executorConfig, DinkyClassLoader classLoader) {
-        if (executorConfig.isRemote()) {
+        if (executorConfig.isRemote() && !executorConfig.isPlan()) {
             return buildRemoteExecutor(executorConfig, classLoader);
         } else {
             return buildLocalExecutor(executorConfig, classLoader);

File: dinky-core/src/main/java/org/dinky/job/Job.java
Patch:
@@ -54,7 +54,7 @@ public class Job {
     private Executor executor;
     private boolean useGateway;
     private List<String> jids;
-    private boolean isPipeline = true;
+    private boolean isPipeline = false;
 
     @Getter
     public enum JobStatus {

File: dinky-core/src/main/java/org/dinky/job/runner/JobPipelineRunner.java
Patch:
@@ -91,6 +91,7 @@ public Optional<JobClient> execute(JobStatement jobStatement) throws Exception {
 
     @Override
     public void run(JobStatement jobStatement) throws Exception {
+        jobManager.getJob().setPipeline(true);
         if (ExecuteJarParseStrategy.INSTANCE.match(jobStatement.getStatement())) {
             JobJarRunner jobJarRunner = new JobJarRunner(jobManager);
             jobJarRunner.run(jobStatement);

File: dinky-client/dinky-client-base/src/main/java/org/dinky/executor/CustomTableEnvironment.java
Patch:
@@ -96,6 +96,7 @@ default void addJar(File... jarPath) {
             addConfiguration(PipelineOptions.JARS, pathList);
         } else {
             CollUtil.addAll(jars, pathList);
+            addConfiguration(PipelineOptions.JARS, jars);
         }
     }
 

File: dinky-admin/src/main/java/org/dinky/ws/GlobalWebSocket.java
Patch:
@@ -94,7 +94,9 @@ public enum EventType {
     private static final Map<Session, RequestDTO> TOPICS = new ConcurrentHashMap<>();
 
     @OnOpen
-    public void onOpen(Session session) {}
+    public void onOpen(Session session) {
+        session.setMaxIdleTimeout(30000);
+    }
 
     @OnClose
     public void onClose(Session session) {

File: dinky-core/src/main/java/org/dinky/explainer/mock/MockStatementExplainer.java
Patch:
@@ -165,14 +165,14 @@ private String getSinkMockDdlStatement(String tableName, String columns) {
 
     /**
      * generate table identifier with mocked prefix info
-     * @param tableName table name
+     * @param tableIdentifier table identifier
      * @return table identifier with mocked prefix info
      */
-    private List<String> generateMockedTableIdentifier(String tableName) {
+    private List<String> generateMockedTableIdentifier(String tableIdentifier) {
         List<String> names = new ArrayList<>();
         names.add("default_catalog");
         names.add("default_database");
-        names.add("mock_sink_" + tableName);
+        names.add("mock_sink_" + tableIdentifier.replace(".", "_"));
         return names;
     }
 }

File: dinky-admin/src/main/java/org/dinky/service/impl/JobInstanceServiceImpl.java
Patch:
@@ -41,7 +41,6 @@
 import org.dinky.data.model.mapping.ClusterInstanceMapping;
 import org.dinky.data.result.ProTableResult;
 import org.dinky.data.vo.task.JobInstanceVo;
-import org.dinky.executor.ExecutorConfig;
 import org.dinky.explainer.lineage.LineageBuilder;
 import org.dinky.explainer.lineage.LineageResult;
 import org.dinky.job.FlinkJobTask;
@@ -295,7 +294,7 @@ public void refreshJobByTaskIds(Integer... taskIds) {
     @Override
     public LineageResult getLineage(Integer id) {
         History history = getJobInfoDetail(id).getHistory();
-        return LineageBuilder.getColumnLineageByLogicalPlan(history.getStatement(), ExecutorConfig.DEFAULT);
+        return LineageBuilder.getColumnLineageByLogicalPlan(history.getStatement(), history.getConfigJson());
     }
 
     @Override

File: dinky-core/src/main/java/org/dinky/api/FlinkAPI.java
Patch:
@@ -112,7 +112,7 @@ private String getResult(String route) {
     }
 
     private JsonNode post(String route, String body) {
-        String url = NetConstant.SLASH + route;
+        String url = address + NetConstant.SLASH + route;
         if (!address.startsWith(NetConstant.HTTP) && !address.startsWith(NetConstant.HTTPS)) {
             url = NetConstant.HTTP + url;
         }

File: dinky-admin/src/main/java/org/dinky/service/impl/TaskServiceImpl.java
Patch:
@@ -1101,10 +1101,10 @@ public List<TaskDTO> getUserTasks(Integer userId) {
     private Boolean hasTaskOperatePermission(Integer firstLevelOwner, List<Integer> secondLevelOwners) {
         boolean isFirstLevelOwner = firstLevelOwner != null && firstLevelOwner == StpUtil.getLoginIdAsInt();
         if (TaskOwnerLockStrategyEnum.OWNER.equals(
-                SystemConfiguration.getInstances().getTaskOwnerLockStrategy())) {
+                SystemConfiguration.getInstances().getTaskOwnerLockStrategy().getValue())) {
             return isFirstLevelOwner;
         } else if (TaskOwnerLockStrategyEnum.OWNER_AND_MAINTAINER.equals(
-                SystemConfiguration.getInstances().getTaskOwnerLockStrategy())) {
+                SystemConfiguration.getInstances().getTaskOwnerLockStrategy().getValue())) {
             return isFirstLevelOwner
                     || (secondLevelOwners != null && secondLevelOwners.contains(StpUtil.getLoginIdAsInt()));
         }

File: dinky-metadata/dinky-metadata-postgresql/src/main/java/org/dinky/metadata/convert/PostgreSqlTypeConvert.java
Patch:
@@ -65,7 +65,9 @@ public PostgreSqlTypeConvert() {
 
     private static Optional<ColumnType> convertDecimalOrNumeric(
             Column column, DriverConfig<AbstractJdbcConfig> driverConfig) {
-        // 该字段的精度
+        if (column.getPrecision() == null) {
+            return Optional.of(ColumnType.DECIMAL);
+        }
         int intValue = column.getPrecision().intValue();
         if (intValue > 38) {
             return Optional.of(ColumnType.STRING);

File: dinky-admin/src/main/java/org/dinky/utils/MavenUtil.java
Patch:
@@ -168,7 +168,9 @@ public static String getMavenCommandLineByMvn(
         commandLine.add("-Dclassworlds.conf=" + StrUtil.wrap(mavenHome + "/bin/m2.conf", "\""));
         commandLine.add("-s " + settingsPath);
         commandLine.add("-f " + projectDir);
-        commandLine.add(StrUtil.wrap(StrUtil.replace(args, "\"", "\\*"), "\""));
+        if (StrUtil.isNotBlank(args)) {
+            commandLine.add(StrUtil.wrap(StrUtil.replace(args, "\"", "\\*"), "\""));
+        }
         commandLine.add(StrUtil.join(" ", goals));
         return StrUtil.join(" ", commandLine);
     }

File: dinky-client/dinky-client-base/src/main/java/org/dinky/executor/CustomTableEnvironment.java
Patch:
@@ -92,7 +92,7 @@ default void addJar(File... jarPath) {
         List<String> pathList =
                 Arrays.stream(URLUtil.getURLs(jarPath)).map(URL::toString).collect(Collectors.toList());
         List<String> jars = configuration.get(PipelineOptions.JARS);
-        if (jars == null) {
+        if (CollUtil.isEmpty(jars)) {
             addConfiguration(PipelineOptions.JARS, pathList);
         } else {
             CollUtil.addAll(jars, pathList);

File: dinky-common/src/main/java/org/dinky/data/job/JobStatementType.java
Patch:
@@ -24,4 +24,5 @@ public enum JobStatementType {
     DDL,
     SQL,
     PIPELINE,
+    EXECUTE_JAR
 }

File: dinky-common/src/main/java/org/dinky/data/job/SqlType.java
Patch:
@@ -66,6 +66,8 @@ public enum SqlType {
 
     RESET("RESET", "^RESET.*", SqlCategory.DDL),
 
+    EXECUTE_JAR("EXECUTE_JAR", "^EXECUTE\\s+JAR\\s+WITH.*", SqlCategory.DML),
+
     EXECUTE("EXECUTE", "^EXECUTE.*", SqlCategory.DML),
 
     ADD_JAR("ADD_JAR", "^ADD\\s+JAR\\s+\\S+", SqlCategory.DDL),

File: dinky-core/src/main/java/org/dinky/executor/Executor.java
Patch:
@@ -213,6 +213,8 @@ public JobStatementPlan parseStatementIntoJobStatementPlan(String[] statements)
             SqlType operationType = Operations.getOperationType(statement);
             if (operationType.equals(SqlType.SET) || operationType.equals(SqlType.RESET)) {
                 jobStatementPlan.addJobStatement(statement, JobStatementType.SET, operationType);
+            } else if (operationType.equals(SqlType.EXECUTE_JAR)) {
+                jobStatementPlan.addJobStatement(statement, JobStatementType.EXECUTE_JAR, operationType);
             } else if (operationType.equals(SqlType.EXECUTE)) {
                 jobStatementPlan.addJobStatement(statement, JobStatementType.PIPELINE, operationType);
             } else if (operationType.equals(SqlType.PRINT)) {

File: dinky-core/src/main/java/org/dinky/job/JobStatementPlan.java
Patch:
@@ -108,7 +108,8 @@ private void checkEmptyStatement() {
                 throw new DinkyException("The statement cannot be empty. Please check your statements.");
             }
             if (jobStatement.getStatementType().equals(JobStatementType.SQL)
-                    || jobStatement.getStatementType().equals(JobStatementType.PIPELINE)) {
+                    || jobStatement.getStatementType().equals(JobStatementType.PIPELINE)
+                    || jobStatement.getStatementType().equals(JobStatementType.EXECUTE_JAR)) {
                 hasSqlStatement = true;
             }
         }

File: dinky-alert/dinky-alert-dingtalk/src/main/java/org/dinky/alert/dingtalk/params/DingTalkParams.java
Patch:
@@ -22,15 +22,13 @@
 import java.util.ArrayList;
 import java.util.List;
 
-import com.fasterxml.jackson.annotation.JsonCreator;
-
 import lombok.AllArgsConstructor;
 import lombok.Data;
 import lombok.NoArgsConstructor;
 
 @Data
 @NoArgsConstructor
-@AllArgsConstructor(onConstructor = @__(@JsonCreator))
+@AllArgsConstructor
 public class DingTalkParams {
 
     private String webhook = "";

File: dinky-alert/dinky-alert-email/src/main/java/org/dinky/alert/email/params/EmailParams.java
Patch:
@@ -22,15 +22,13 @@
 import java.util.ArrayList;
 import java.util.List;
 
-import com.fasterxml.jackson.annotation.JsonCreator;
-
 import lombok.AllArgsConstructor;
 import lombok.Data;
 import lombok.NoArgsConstructor;
 
 @Data
 @NoArgsConstructor
-@AllArgsConstructor(onConstructor = @__(@JsonCreator))
+@AllArgsConstructor
 public class EmailParams {
 
     private List<String> receivers = new ArrayList<>();

File: dinky-alert/dinky-alert-feishu/src/main/java/org/dinky/alert/feishu/params/FeiShuParams.java
Patch:
@@ -22,14 +22,12 @@
 import java.util.ArrayList;
 import java.util.List;
 
-import com.fasterxml.jackson.annotation.JsonCreator;
-
 import lombok.AllArgsConstructor;
 import lombok.Data;
 import lombok.NoArgsConstructor;
 
 @Data
-@NoArgsConstructor(onConstructor = @__(@JsonCreator))
+@NoArgsConstructor
 @AllArgsConstructor
 public class FeiShuParams {
 

File: dinky-alert/dinky-alert-http/src/main/java/org/dinky/alert/http/params/HttpParams.java
Patch:
@@ -23,14 +23,12 @@
 
 import java.util.List;
 
-import com.fasterxml.jackson.annotation.JsonCreator;
-
 import lombok.AllArgsConstructor;
 import lombok.Data;
 import lombok.NoArgsConstructor;
 
 @Data
-@NoArgsConstructor(onConstructor = @__(@JsonCreator))
+@NoArgsConstructor
 @AllArgsConstructor
 public class HttpParams {
 

File: dinky-alert/dinky-alert-wechat/src/main/java/org/dinky/alert/wechat/params/WechatParams.java
Patch:
@@ -22,14 +22,12 @@
 import java.util.ArrayList;
 import java.util.List;
 
-import com.fasterxml.jackson.annotation.JsonCreator;
-
 import lombok.AllArgsConstructor;
 import lombok.Data;
 import lombok.NoArgsConstructor;
 
 @Data
-@NoArgsConstructor(onConstructor = @__(@JsonCreator))
+@NoArgsConstructor
 @AllArgsConstructor
 public class WechatParams {
 

File: dinky-scheduler/src/main/java/org/dinky/scheduler/client/ProjectClient.java
Patch:
@@ -70,7 +70,7 @@ public Project createDinkyProject() {
                 .form(map)
                 .timeout(5000)
                 .execute();
-        if (httpResponse.getStatus() != 200) {
+        if (!httpResponse.isOk()) {
             SystemConfiguration.getInstances().getDolphinschedulerEnable().setValue(false);
             logger.error("DolphInScheduler connection failed, Reason: {}", httpResponse.getStatus());
             return null;
@@ -101,7 +101,7 @@ public Project getDinkyProject() {
                 .timeout(5000)
                 .execute();
 
-        if (httpResponse.getStatus() != 200) {
+        if (!httpResponse.isOk()) {
             SystemConfiguration.getInstances().getDolphinschedulerEnable().setValue(false);
             logger.error("DolphInScheduler connection failed, Reason: {}", httpResponse.getStatus());
             return null;

File: dinky-admin/src/main/java/org/dinky/configure/MybatisPlusConfig.java
Patch:
@@ -80,7 +80,7 @@ public class MybatisPlusConfig {
             "dinky_task_version");
 
     @Bean
-    @Profile("pgsql")
+    @Profile("postgresql")
     public PostgreSQLQueryInterceptor postgreSQLQueryInterceptor() {
         return new PostgreSQLQueryInterceptor();
     }
@@ -91,7 +91,7 @@ public PostgreSQLQueryInterceptor postgreSQLQueryInterceptor() {
      * @return {@linkplain PostgreSQLPrepareInterceptor}
      */
     @Bean
-    @Profile("pgsql")
+    @Profile("postgresql")
     public PostgreSQLPrepareInterceptor postgreSQLPrepareInterceptor() {
         return new PostgreSQLPrepareInterceptor();
     }

File: dinky-admin/src/main/java/org/dinky/controller/CatalogueController.java
Patch:
@@ -128,6 +128,9 @@ public Result<String> upload(MultipartFile file, @PathVariable Integer id) {
             dataType = "Catalogue",
             dataTypeClass = Catalogue.class)
     public Result<Void> saveOrUpdateCatalogue(@RequestBody Catalogue catalogue) {
+        if (catalogueService.checkNameIsExistByParentId(catalogue)) {
+            return Result.failed(Status.NAME_IS_EXIST);
+        }
         if (catalogueService.saveOrUpdateOrRename(catalogue)) {
             return Result.succeed(Status.SAVE_SUCCESS);
         } else {

File: dinky-common/src/main/java/org/dinky/data/enums/Status.java
Patch:
@@ -196,6 +196,7 @@ public enum Status {
     CATALOGUE_NOT_EXIST(12017, "catalogue.not.exist"),
     CATALOGUE_IS_EXIST(12018, "catalogue.is.exist"),
     TASK_NAME_NOT_MATCH_CATALOGUE_NAME(12019, "task.name.not.match.catalogue.name"),
+    NAME_IS_EXIST(12021, "A task and a directory cannot have the same name under the same parent directory."),
 
     /**
      * alert instance

File: dinky-admin/src/main/java/org/dinky/service/catalogue/factory/CatalogueTreeSortFactory.java
Patch:
@@ -55,7 +55,6 @@ public CatalogueTreeSortStrategy getStrategy(String strategyName) {
         CatalogueTreeSortStrategy catalogueTreeSortStrategy =
                 Safes.of(catalogueTreeSortStrategyMap).get(strategyName);
         if (Objects.isNull(catalogueTreeSortStrategy)) {
-            log.warn("Strategy {} is not defined. Use DefaultStrategy", strategyName);
             catalogueTreeSortStrategy =
                     Safes.of(catalogueTreeSortStrategyMap).get(CatalogueSortConstant.STRATEGY_DEFAULT);
         }

File: dinky-admin/src/main/java/org/dinky/service/catalogue/impl/CatalogueServiceImpl.java
Patch:
@@ -121,7 +121,6 @@ public class CatalogueServiceImpl extends SuperServiceImpl<CatalogueMapper, Cata
      */
     @Override
     public List<Catalogue> getCatalogueTree(CatalogueTreeQueryDTO catalogueTreeQueryDto) {
-        log.info("getCatalogueTree, catalogueTreeQueryDto: {}", catalogueTreeQueryDto);
         List<Catalogue> catalogueTree = buildCatalogueTree(this.list());
         // sort
         CatalogueTreeSortStrategy strategy = catalogueTreeSortFactory.getStrategy(catalogueTreeQueryDto.getSortValue());

File: dinky-admin/src/main/java/org/dinky/service/impl/UDFServiceImpl.java
Patch:
@@ -24,11 +24,11 @@
 import org.dinky.data.model.udf.UDFManage;
 import org.dinky.data.vo.CascaderVO;
 import org.dinky.data.vo.UDFManageVO;
+import org.dinky.function.FlinkUDFDiscover;
 import org.dinky.function.data.model.UDF;
 import org.dinky.mapper.UDFManageMapper;
 import org.dinky.service.UDFService;
 import org.dinky.service.resource.ResourcesService;
-import org.dinky.trans.Operations;
 import org.dinky.utils.UDFUtils;
 
 import org.apache.flink.table.catalog.FunctionLanguage;
@@ -182,7 +182,7 @@ public List<UDFManage> getUDFFromUdfManage() {
     @Override
     public List<CascaderVO> getAllUdfsToCascader(List<UDF> userDefinedReleaseUdfs) {
         // Get all UDFs of static UDFs and dynamic UDFs
-        List<UDF> staticUdfs = Operations.getCustomStaticUdfs();
+        List<UDF> staticUdfs = FlinkUDFDiscover.getCustomStaticUDFs();
 
         // get all UDFs of UDFManage table
         List<UDF> udfManageDynamic = getUDFFromUdfManage().stream()

File: dinky-core/src/main/java/org/dinky/explainer/Explainer.java
Patch:
@@ -41,6 +41,7 @@
 import org.dinky.job.builder.JobUDFBuilder;
 import org.dinky.trans.Operations;
 import org.dinky.utils.DinkyClassLoaderUtil;
+import org.dinky.utils.LogUtil;
 import org.dinky.utils.SqlUtil;
 
 import org.apache.flink.runtime.rest.messages.JobPlanInfo;
@@ -141,8 +142,9 @@ public ExplainResult explainSql(String statement) {
             jobStatementPlan.buildFinalStatement();
             jobManager.setJobStatementPlan(jobStatementPlan);
         } catch (Exception e) {
+            String error = LogUtil.getError("Exception in parsing FlinkSQL:\n" + SqlUtil.addLineNumber(statement), e);
             SqlExplainResult.Builder resultBuilder = SqlExplainResult.Builder.newBuilder();
-            resultBuilder.error(e.getMessage()).parseTrue(false);
+            resultBuilder.error(error).parseTrue(false);
             sqlExplainRecords.add(resultBuilder.build());
             log.error("Failed parseStatements:", e);
             return new ExplainResult(false, sqlExplainRecords.size(), sqlExplainRecords);

File: dinky-admin/src/main/java/org/dinky/context/ConsoleContextHolder.java
Patch:
@@ -121,7 +121,7 @@ public boolean clearProcessLog(String processName) {
         if (FileUtil.exist(filePath)) {
             return FileUtil.del(filePath);
         }
-        return false;
+        return true;
     }
 
     /**

File: dinky-core/src/main/java/org/dinky/executor/Executor.java
Patch:
@@ -275,7 +275,7 @@ public void initPyUDF(String executable, String... udfPyFilePath) {
     private void addJar(String... jarPath) {
         Configuration configuration = tableEnvironment.getRootConfiguration();
         List<String> jars = configuration.get(PipelineOptions.JARS);
-        if (jars == null) {
+        if (CollUtil.isEmpty(jars)) {
             tableEnvironment.addConfiguration(PipelineOptions.JARS, CollUtil.newArrayList(jarPath));
         } else {
             CollUtil.addAll(jars, jarPath);

File: dinky-gateway/src/main/java/org/dinky/gateway/config/K8sConfig.java
Patch:
@@ -75,5 +75,5 @@ public class K8sConfig {
             dataType = "Map<String, String>",
             example = "{\"key1\": \"value1\", \"key2\": \"value2\"}",
             notes = "Ingress configuration properties")
-    private Map<String, String> ingressConfig = Maps.newHashMap();
+    private Map<String, Object> ingressConfig = Maps.newHashMap();
 }

File: dinky-client/dinky-client-base/src/main/java/org/dinky/executor/CustomTableEnvironment.java
Patch:
@@ -22,6 +22,7 @@
 import org.dinky.data.model.LineageRel;
 import org.dinky.data.result.SqlExplainResult;
 
+import org.apache.calcite.sql.SqlNode;
 import org.apache.flink.configuration.ConfigOption;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.configuration.PipelineOptions;
@@ -70,6 +71,8 @@ public interface CustomTableEnvironment
 
     Configuration getRootConfiguration();
 
+    SqlNode parseSql(String sql);
+
     default JobGraph getJobGraphFromInserts(List<String> statements) {
         return getStreamGraphFromInserts(statements).getJobGraph();
     }

File: dinky-core/src/main/java/org/dinky/constant/FlinkConstant.java
Patch:
@@ -29,4 +29,7 @@ public interface FlinkConstant {
     String LOCAL_HOST = "localhost";
     /** changlog op */
     String OP = "op";
+
+    /** flink 默认端口 **/
+    Integer FLINK_REST_DEFAULT_PORT = 8081;
 }

File: dinky-core/src/main/java/org/dinky/job/builder/JobExecuteBuilder.java
Patch:
@@ -65,6 +65,8 @@ public void run() throws Exception {
                 }
                 GatewayResult gatewayResult = null;
                 config.addGatewayConfig(executor.getSetConfig());
+                config.addGatewayConfig(
+                        executor.getCustomTableEnvironment().getConfig().getConfiguration());
                 config.getGatewayConfig().setSql(jobParam.getParsedSql());
 
                 if (runMode.isApplicationMode()) {

File: dinky-admin/src/main/java/org/dinky/service/impl/TaskServiceImpl.java
Patch:
@@ -355,8 +355,6 @@ public JobResult submitTask(TaskSubmitDto submitDto) throws Exception {
     public JobResult debugTask(TaskDTO task) throws Exception {
         // Debug mode need return result
         task.setUseResult(true);
-        // Debug mode need execute
-        task.setStatementSet(task.isMockSinkFunction());
         // mode check
         if (GatewayType.get(task.getType()).isDeployCluster()) {
             throw new BusException(Status.MODE_IS_NOT_ALLOW_SELECT.getMessage());

File: dinky-core/src/main/java/org/dinky/explainer/Explainer.java
Patch:
@@ -160,9 +160,6 @@ public JobParam pretreatStatements(String[] statements) {
             } else if (transSqlTypeSet.contains(operationType)) {
                 trans.add(new StatementParam(statement, operationType));
                 statementList.add(statement);
-                if (!useStatementSet) {
-                    break;
-                }
             } else if (operationType.equals(SqlType.EXECUTE)) {
                 execute.add(new StatementParam(statement, operationType));
             } else if (operationType.equals(SqlType.PRINT)) {

File: dinky-core/src/main/java/org/dinky/job/Job.java
Patch:
@@ -54,6 +54,7 @@ public class Job {
     private Executor executor;
     private boolean useGateway;
     private List<String> jids;
+    private boolean isPipeline = true;
 
     @Getter
     public enum JobStatus {
@@ -113,7 +114,8 @@ public JobResult getJobResult() {
                 error,
                 result,
                 startTime,
-                endTime);
+                endTime,
+                isPipeline);
     }
 
     public boolean isFailed() {

File: dinky-core/src/main/java/org/dinky/job/JobManager.java
Patch:
@@ -108,6 +108,7 @@ public class JobManager {
     private boolean useGateway = false;
     private boolean isPlanMode = false;
     private boolean useStatementSet = false;
+    private boolean useMockSinkFunction = false;
     private boolean useRestAPI = false;
     private GatewayType runMode = GatewayType.LOCAL;
     private JobParam jobParam = null;
@@ -214,6 +215,7 @@ public void init() {
             handler = JobHandler.build();
         }
         useStatementSet = config.isStatementSet();
+        useMockSinkFunction = config.isMockSinkFunction();
         useRestAPI = SystemConfiguration.getInstances().isUseRestAPI();
         executorConfig = config.getExecutorSetting();
         executorConfig.setPlan(isPlanMode);

File: dinky-admin/src/main/java/org/dinky/data/model/Resources.java
Patch:
@@ -54,7 +54,7 @@
 @AllArgsConstructor
 public class Resources extends Model<Resources> {
 
-    @TableId(type = IdType.ASSIGN_ID)
+    @TableId(type = IdType.AUTO)
     @ApiModelProperty(value = "ID", dataType = "Integer", example = "1", notes = "Unique identifier for the resource")
     private Integer id;
 

File: dinky-admin/src/main/java/org/dinky/service/resource/impl/ResourceServiceImpl.java
Patch:
@@ -86,6 +86,8 @@ public boolean syncRemoteDirectoryStructure() {
                                 x.setType(resources.getType());
                                 x.setUserId(resources.getUserId());
                             }
+                            x.setId(Math.abs(x.getId()));
+                            x.setPid(Math.abs(x.getPid()));
                         })
                         .collect(Collectors.toList());
         // not delete root directory

File: dinky-cdc/dinky-cdc-core/src/main/java/org/dinky/cdc/sql/catalog/SQLCatalogSinkBuilder.java
Patch:
@@ -66,7 +66,7 @@ public void addTableSink(
         String catalogName = config.getSink().get("catalog.name");
         String sinkSchemaName = getSinkSchemaName(table);
         String tableName = getSinkTableName(table);
-        String sinkTableName = catalogName + "." + sinkSchemaName + "." + tableName;
+        String sinkTableName = catalogName + ".`" + sinkSchemaName + "`.`" + tableName + "`";
         // Because the name of the view on Flink is not allowed to have -, it needs to be replaced with - here_
         String viewName = replaceViewNameMiddleLineToUnderLine("VIEW_" + table.getSchemaTableNameWithUnderline());
 

File: dinky-cdc/dinky-cdc-core/src/main/java/org/dinky/cdc/utils/FlinkStatementUtil.java
Patch:
@@ -36,7 +36,7 @@ private FlinkStatementUtil() {}
 
     public static String getCDCInsertSql(Table table, String targetName, String sourceName, FlinkCDCConfig config) {
         StringBuilder sb = new StringBuilder("INSERT INTO ");
-        sb.append("`").append(targetName).append("`");
+        sb.append(targetName);
         sb.append(" SELECT\n");
         for (int i = 0; i < table.getColumns().size(); i++) {
             sb.append("    ");

File: dinky-admin/src/main/java/org/dinky/data/model/ext/JobAlertData.java
Patch:
@@ -143,7 +143,7 @@ public class JobAlertData {
     private static String buildTaskUrl(JobInstance jobInstance) {
         return StrFormatter.format(
                 "{}/#/devops/job-detail?id={}",
-                SystemConfiguration.getInstances().getDinkyAddr(),
+                SystemConfiguration.getInstances().getDinkyAddr().getValue(),
                 jobInstance.getTaskId());
     }
 

File: dinky-common/src/main/java/org/dinky/data/model/SystemConfiguration.java
Patch:
@@ -123,10 +123,10 @@ public static Configuration.OptionBuilder key(Status status) {
             .stringType()
             .defaultValue(System.getProperty("dinkyAddr"))
             .note(Status.SYS_ENV_SETTINGS_DINKYADDR_NOTE);
-    private final Configuration<String> dinkyToken = key(Status.SYS_ENV_SETTINGS_DINKYTOKEN_NOTE)
+    private final Configuration<String> dinkyToken = key(Status.SYS_ENV_SETTINGS_DINKYTOKEN)
             .stringType()
             .defaultValue("efda1551-7958-4e0f-80a8-dfd107df3e38")
-            .note(Status.SYS_ENV_SETTINGS_DINKYTOKEN);
+            .note(Status.SYS_ENV_SETTINGS_DINKYTOKEN_NOTE);
 
     private final Configuration<Integer> jobReSendDiffSecond = key(Status.SYS_ENV_SETTINGS_JOB_RESEND_DIFF_SECOND)
             .intType()

File: dinky-cdc/dinky-cdc-core/src/main/java/org/dinky/cdc/sql/SQLSinkBuilder.java
Patch:
@@ -68,8 +68,9 @@ protected void initTypeConverterList() {
 
     private String addSourceTableView(
             CustomTableEnvironment customTableEnvironment, DataStream<Row> rowDataDataStream, Table table) {
-        // 上游表名称
-        String viewName = "VIEW_" + table.getSchemaTableNameWithUnderline();
+        // Because the name of the view on Flink is not allowed to have -, it needs to be replaced with - here_
+        String viewName = replaceViewNameMiddleLineToUnderLine("VIEW_" + table.getSchemaTableNameWithUnderline());
+
         customTableEnvironment.createTemporaryView(
                 viewName, customTableEnvironment.fromChangelogStream(rowDataDataStream));
         logger.info("Create {} temporaryView successful...", viewName);

File: dinky-cdc/dinky-cdc-core/src/main/java/org/dinky/cdc/sql/catalog/SQLCatalogSinkBuilder.java
Patch:
@@ -67,7 +67,8 @@ public void addTableSink(
         String sinkSchemaName = getSinkSchemaName(table);
         String tableName = getSinkTableName(table);
         String sinkTableName = catalogName + "." + sinkSchemaName + "." + tableName;
-        String viewName = "VIEW_" + table.getSchemaTableNameWithUnderline();
+        // Because the name of the view on Flink is not allowed to have -, it needs to be replaced with - here_
+        String viewName = replaceViewNameMiddleLineToUnderLine("VIEW_" + table.getSchemaTableNameWithUnderline());
 
         customTableEnvironment.createTemporaryView(
                 viewName, customTableEnvironment.fromChangelogStream(rowDataDataStream));

File: dinky-metadata/dinky-metadata-oracle/src/main/java/org/dinky/metadata/convert/OracleTypeConvert.java
Patch:
@@ -36,8 +36,8 @@ public OracleTypeConvert() {
         this.convertMap.clear();
         register("char", ColumnType.STRING);
         register("date", ColumnType.LOCAL_DATETIME);
-        register("time", ColumnType.TIME);
         register("timestamp", ColumnType.TIMESTAMP);
+        register("time", ColumnType.TIME);
         register("number", OracleTypeConvert::convertNumber);
         register("float", ColumnType.JAVA_LANG_FLOAT);
         register("clob", ColumnType.STRING);

File: dinky-admin/src/main/java/org/dinky/job/handler/JobAlertHandler.java
Patch:
@@ -249,7 +249,8 @@ private void executeAlertAction(Facts facts, AlertRuleDTO alertRuleDTO) throws E
 
         if (!Asserts.isNull(task.getAlertGroup())) {
             // 获取任务的责任人和维护人对应的用户信息|Get the responsible person and maintainer of the task
-            User ownerInfo = userCache.get(task.getFirstLevelOwner());
+            Integer owner = task.getFirstLevelOwner();
+            User ownerInfo = owner == null ? null : userCache.get(owner);
             List<User> maintainerInfo = Lists.newArrayList();
             if (CollectionUtils.isNotEmpty(task.getSecondLevelOwners())) {
                 for (Integer secondLevelOwner : task.getSecondLevelOwners()) {

File: dinky-admin/src/main/java/org/dinky/service/impl/TaskServiceImpl.java
Patch:
@@ -201,9 +201,7 @@ public TaskDTO prepareTask(TaskSubmitDto submitDto) {
 
     @ProcessStep(type = ProcessStepType.SUBMIT_EXECUTE)
     public JobResult executeJob(TaskDTO task) throws Exception {
-        JobResult jobResult = BaseTask.getTask(task).execute();
-        log.info("execute job finished,status is {}", jobResult.getStatus());
-        return jobResult;
+        return executeJob(task, false);
     }
 
     @ProcessStep(type = ProcessStepType.SUBMIT_EXECUTE)

File: dinky-admin/src/main/java/org/dinky/service/task/BaseTask.java
Patch:
@@ -20,6 +20,7 @@
 package org.dinky.service.task;
 
 import org.dinky.config.Dialect;
+import org.dinky.context.TaskContextHolder;
 import org.dinky.data.annotations.SupportDialect;
 import org.dinky.data.dto.TaskDTO;
 import org.dinky.data.exception.NotSupportExplainExcepition;
@@ -64,6 +65,7 @@ public static BaseTask getTask(TaskDTO taskDTO) {
             if (annotation != null) {
                 for (Dialect dialect : annotation.value()) {
                     if (dialect.isDialect(taskDTO.getDialect())) {
+                        TaskContextHolder.setDialect(dialect);
                         return (BaseTask) ReflectUtil.newInstance(clazz, taskDTO);
                     }
                 }

File: dinky-admin/src/main/java/org/dinky/url/RsURLStreamHandlerFactory.java
Patch:
@@ -33,7 +33,7 @@
 
 @Profile("!test")
 public class RsURLStreamHandlerFactory implements URLStreamHandlerFactory {
-    private final List<String> notContains = Arrays.asList("jar", "file");
+    private final List<String> notContains = Arrays.asList("jar", "file", "http", "https");
 
     @Override
     public URLStreamHandler createURLStreamHandler(String protocol) {

File: dinky-core/src/main/java/org/dinky/executor/Executor.java
Patch:
@@ -220,8 +220,8 @@ public void initPyUDF(String executable, String... udfPyFilePath) {
         }
 
         Configuration configuration = tableEnvironment.getConfig().getConfiguration();
-        configuration.setString(PythonOptions.PYTHON_FILES, String.join(",", udfPyFilePath));
-        configuration.setString(PythonOptions.PYTHON_CLIENT_EXECUTABLE, executable);
+        configuration.set(PythonOptions.PYTHON_FILES, String.join(",", udfPyFilePath));
+        configuration.set(PythonOptions.PYTHON_CLIENT_EXECUTABLE, executable);
     }
 
     private void addJar(String... jarPath) {

File: dinky-core/src/main/java/org/dinky/job/builder/JobJarStreamGraphBuilder.java
Patch:
@@ -45,6 +45,7 @@
 import org.apache.flink.api.common.Plan;
 import org.apache.flink.api.dag.Pipeline;
 import org.apache.flink.configuration.Configuration;
+import org.apache.flink.configuration.PipelineOptions;
 import org.apache.flink.core.execution.JobClient;
 import org.apache.flink.runtime.jobgraph.JobGraph;
 import org.apache.flink.runtime.jobgraph.SavepointConfigOptions;
@@ -117,6 +118,7 @@ public void run() throws Exception {
     }
 
     private GatewayResult submitGateway() throws Exception {
+        configuration.set(PipelineOptions.JARS, getUris(job.getStatement()));
         config.addGatewayConfig(configuration);
         config.getGatewayConfig().setSql(job.getStatement());
         return Gateway.build(config.getGatewayConfig()).submitJar(jobManager.getUdfPathContextHolder());

File: dinky-core/src/main/java/org/dinky/explainer/Explainer.java
Patch:
@@ -21,6 +21,7 @@
 
 import org.dinky.assertion.Asserts;
 import org.dinky.data.enums.GatewayType;
+import org.dinky.data.exception.DinkyException;
 import org.dinky.data.model.LineageRel;
 import org.dinky.data.result.ExplainResult;
 import org.dinky.data.result.SqlExplainResult;
@@ -415,8 +416,8 @@ public List<LineageRel> getLineage(String statement) {
                     executor.executeSql(sql);
                 }
             } catch (Exception e) {
-                log.error(e.getMessage());
-                return lineageRelList;
+                log.error("Exception occurred while fetching lineage information", e);
+                throw new DinkyException("Exception occurred while fetching lineage information", e);
             }
         }
         return lineageRelList;

File: dinky-common/src/main/java/org/dinky/data/enums/Status.java
Patch:
@@ -193,7 +193,6 @@ public enum Status {
     MODE_IS_NOT_ALLOW_SELECT(12014, "mode.is.not.allow.select"),
     OPERATE_NOT_SUPPORT_QUERY(12015, "operate.not.support.query"),
     TASK_NOT_OPERATE_PERMISSION(12016, "task.not.operate.permission"),
-    TASK_SQL_NO_EXECUTABLE(12017, "task.sql.no.executable"),
 
     /**
      * alert instance

File: dinky-metadata/dinky-metadata-base/src/main/java/org/dinky/metadata/convert/AbstractTypeConvert.java
Patch:
@@ -50,7 +50,7 @@ public ColumnType convert(Column column, DriverConfig<T> driverConfig) {
         }
         for (Map.Entry<String, BiFunction<Column, DriverConfig<T>, Optional<ColumnType>>> entry :
                 convertMap.entrySet()) {
-            if (column.getType().contains(entry.getKey())) {
+            if (column.getType().toLowerCase().contains(entry.getKey())) {
                 Optional<ColumnType> columnType = entry.getValue().apply(column, driverConfig);
                 if (columnType.isPresent()) {
                     return columnType.get();

File: dinky-client/dinky-client-1.19/src/main/java/org/dinky/executor/ClusterDescriptorAdapterImpl.java
Patch:
@@ -30,6 +30,8 @@
 import java.util.List;
 import java.util.stream.Collectors;
 
+import cn.hutool.core.util.URLUtil;
+
 public class ClusterDescriptorAdapterImpl extends ClusterDescriptorAdapter {
 
     public ClusterDescriptorAdapterImpl() {}
@@ -41,7 +43,7 @@ public ClusterDescriptorAdapterImpl(YarnClusterDescriptor yarnClusterDescriptor)
     @Override
     public void addShipFiles(List<File> shipFiles) {
         yarnClusterDescriptor.addShipFiles(shipFiles.stream()
-                .map(file -> new Path("file://" + file.getPath()))
+                .map(file -> new Path(URLUtil.getURL(file).toString()))
                 .collect(Collectors.toList()));
     }
 

File: dinky-core/src/main/java/org/dinky/job/JobConfig.java
Patch:
@@ -205,7 +205,9 @@ public void setAddress(String address) {
             if (colonIndex == -1) {
                 this.address = address + NetConstant.COLON + configJson.get(RestOptions.PORT.key());
             } else {
-                this.address = address.replaceAll("(?<=:)\\d{0,6}$", configJson.get(RestOptions.PORT.key()));
+                String port =
+                        configJson.getOrDefault(RestOptions.BIND_PORT.key(), configJson.get(RestOptions.PORT.key()));
+                this.address = address.replaceAll("(?<=:)\\d{0,6}$", port);
             }
         } else {
             this.address = address;

File: dinky-admin/src/main/java/org/dinky/data/dto/StudioLineageDTO.java
Patch:
@@ -53,4 +53,7 @@ public class StudioLineageDTO extends AbstractStatementDTO {
             example = "1",
             notes = "The identifier of the target database")
     private Integer databaseId;
+
+    @ApiModelProperty(value = "Task ID", dataType = "Integer", example = "1", notes = "The identifier of the task")
+    private Integer taskId;
 }

File: dinky-admin/src/main/java/org/dinky/service/impl/JobInstanceServiceImpl.java
Patch:
@@ -41,6 +41,7 @@
 import org.dinky.data.model.mapping.ClusterInstanceMapping;
 import org.dinky.data.result.ProTableResult;
 import org.dinky.data.vo.task.JobInstanceVo;
+import org.dinky.executor.ExecutorConfig;
 import org.dinky.explainer.lineage.LineageBuilder;
 import org.dinky.explainer.lineage.LineageResult;
 import org.dinky.job.FlinkJobTask;
@@ -258,7 +259,7 @@ public void refreshJobByTaskIds(Integer... taskIds) {
     @Override
     public LineageResult getLineage(Integer id) {
         History history = getJobInfoDetail(id).getHistory();
-        return LineageBuilder.getColumnLineageByLogicalPlan(history.getStatement());
+        return LineageBuilder.getColumnLineageByLogicalPlan(history.getStatement(), ExecutorConfig.DEFAULT);
     }
 
     @Override

File: dinky-admin/src/main/java/org/dinky/service/impl/TaskServiceImpl.java
Patch:
@@ -991,7 +991,9 @@ public LineageResult getTaskLineage(Integer id) {
                         task.getStatement(), task.getDialect().toLowerCase(), dataBase.getDriverConfig());
             }
         } else {
-            return LineageBuilder.getColumnLineageByLogicalPlan(buildEnvSql(task));
+            task.setStatement(buildEnvSql(task) + task.getStatement());
+            JobConfig jobConfig = task.getJobConfig();
+            return LineageBuilder.getColumnLineageByLogicalPlan(task.getStatement(), jobConfig.getExecutorSetting());
         }
     }
 

File: dinky-core/src/test/java/org/dinky/core/LineageTest.java
Patch:
@@ -19,6 +19,7 @@
 
 package org.dinky.core;
 
+import org.dinky.executor.ExecutorConfig;
 import org.dinky.explainer.lineage.LineageBuilder;
 import org.dinky.explainer.lineage.LineageResult;
 
@@ -55,7 +56,7 @@ public void sumTest() {
                 + " 'connector' = 'print'\n"
                 + ");\n"
                 + "insert into TT select a||c A ,b||c B from ST";
-        LineageResult result = LineageBuilder.getColumnLineageByLogicalPlan(sql);
+        LineageResult result = LineageBuilder.getColumnLineageByLogicalPlan(sql, ExecutorConfig.DEFAULT);
         LOGGER.info("end");
     }
 }

File: dinky-admin/src/main/java/org/dinky/sse/git/GetJarsStepSse.java
Patch:
@@ -57,7 +57,7 @@ public GetJarsStepSse(
     public void exec() {
         List<File> jars = MavenUtil.getJars((File) params.get("pom"));
         List<String> pathList = uploadResources(jars);
-        addFileMsg(pathList);
+        addMsg(String.join("\n", pathList));
 
         params.put("jarPath", pathList);
     }

File: dinky-admin/src/main/java/org/dinky/sse/git/HeadStepSse.java
Patch:
@@ -68,11 +68,11 @@ private void checkJava() {
         String mavenHome = MavenUtil.getMavenHome();
 
         if (StrUtil.isBlank(mavenHome)) {
-            addFileMsg(Status.GIT_MAVEN_HOME_NOT_SET.getMessage());
+            addFileMsgLog(Status.GIT_MAVEN_HOME_NOT_SET.getMessage());
             setFinish(false);
         }
         String mavenVersionMsg = MavenUtil.getMavenVersion();
-        mavenVersionMsg += "\n Your Maven Home is: " + mavenHome;
+        mavenVersionMsg += "Your Maven Home is: " + mavenHome;
         addFileMsgLog(mavenVersionMsg);
     }
 

File: dinky-admin/src/main/java/org/dinky/sse/git/MavenStepSse.java
Patch:
@@ -79,7 +79,7 @@ public void exec() {
                 null,
                 getLogFile().getAbsolutePath(),
                 CollUtil.newArrayList("clean", "package"),
-                StrUtil.split(gitProject.getBuildArgs(), " "),
+                gitProject.getBuildArgs(),
                 this::addFileMsgLog);
         params.put("pom", pom);
         Assert.isTrue(state, "maven build failed");

File: dinky-admin/src/main/java/org/dinky/utils/GitProjectStepSseFactory.java
Patch:
@@ -63,7 +63,7 @@ public static void build(GitProject gitProject, Dict params) {
         List<SseEmitter> emitterList = new ArrayList<>();
 
         StepSse headStepSse = getHeadStepPlan(gitProject.getCodeType(), sleep, emitterList, params);
-        cachedThreadPool.execute(headStepSse::main);
+        cachedThreadPool.execute(headStepSse::run);
         gitProject.setBuildStep(1);
         gitProject.setBuildState(1);
         gitProject.setLastBuild(DateUtil.date());

File: dinky-gateway/src/main/java/org/dinky/gateway/kubernetes/operator/api/FlinkDeploymentSpec.java
Patch:
@@ -42,6 +42,9 @@
 public class FlinkDeploymentSpec {
     private JobSpec job;
     private Long restartNonce;
+    /** Ingress specs. */
+    private IngressSpec ingress;
+
     private Map<String, String> flinkConfiguration;
     private String image;
     private String imagePullPolicy;

File: dinky-common/src/main/java/org/dinky/data/constant/NetConstant.java
Patch:
@@ -23,6 +23,8 @@ public final class NetConstant {
 
     /** http:// */
     public static final String HTTP = "http://";
+    /** https:// */
+    public static final String HTTPS = "https://";
     /** 冒号: */
     public static final String COLON = ":";
     /** 斜杠/ */

File: dinky-client/dinky-client-1.19/src/main/java/org/dinky/gateway/FlinkSqlClient.java
Patch:
@@ -116,7 +116,9 @@ public static EmbeddedGateway create(DefaultContext defaultContext) {
             Configuration restConfig = new Configuration();
             // always use localhost
             restConfig.set(SqlGatewayRestOptions.ADDRESS, ADDRESS);
+            restConfig.set(SqlGatewayRestOptions.BIND_ADDRESS, ADDRESS);
             restConfig.set(SqlGatewayRestOptions.PORT, port.getPort());
+            restConfig.set(SqlGatewayRestOptions.BIND_PORT, port.getPort() + "");
             defaultConfig.addAll(
                     restConfig,
                     SqlGatewayEndpointFactoryUtils.getSqlGatewayOptionPrefix(SqlGatewayRestEndpointFactory.IDENTIFIER));

File: dinky-common/src/main/java/org/dinky/data/enums/Status.java
Patch:
@@ -193,6 +193,7 @@ public enum Status {
     MODE_IS_NOT_ALLOW_SELECT(12014, "mode.is.not.allow.select"),
     OPERATE_NOT_SUPPORT_QUERY(12015, "operate.not.support.query"),
     TASK_NOT_OPERATE_PERMISSION(12016, "task.not.operate.permission"),
+    TASK_SQL_NO_EXECUTABLE(12017, "task.sql.no.executable"),
 
     /**
      * alert instance

File: dinky-core/src/main/java/org/dinky/data/result/ResultBuilder.java
Patch:
@@ -45,7 +45,7 @@ static ResultBuilder build(
             case SHOW:
             case DESC:
             case DESCRIBE:
-                return new ShowResultBuilder();
+                return new ShowResultBuilder(id);
             case INSERT:
                 return new InsertResultBuilder();
             default:

File: dinky-admin/src/main/java/org/dinky/configure/AppConfig.java
Patch:
@@ -85,7 +85,8 @@ public void addInterceptors(InterceptorRegistry registry) {
                     }
                 }))
                 .addPathPatterns("/api/**", "/openapi/**")
-                .excludePathPatterns("/api/login", "/api/ldap/ldapEnableStatus", "/download/**", "/druid/**");
+                .excludePathPatterns(
+                        "/api/login", "/api/ldap/ldapEnableStatus", "/download/**", "/druid/**", "/api/version");
 
         registry.addInterceptor(new TenantInterceptor())
                 .addPathPatterns("/api/**")

File: dinky-connectors/dinky-connector-jdbc-1.14/src/main/java/org/apache/flink/connector/jdbc/dialect/ClickHouseDialect.java
Patch:
@@ -60,7 +60,7 @@ public String getLimitClause(long limit) {
 
     @Override
     public Optional<String> defaultDriverName() {
-        return Optional.of("ru.yandex.clickhouse.ClickHouseDriver");
+        return Optional.of("com.clickhouse.jdbc.ClickHouseDriver");
     }
 
     @Override

File: dinky-metadata/dinky-metadata-clickhouse/src/main/java/org/dinky/metadata/driver/ClickHouseDriver.java
Patch:
@@ -65,7 +65,7 @@ public class ClickHouseDriver extends AbstractJdbcDriver {
 
     @Override
     String getDriverClass() {
-        return "ru.yandex.clickhouse.ClickHouseDriver";
+        return "com.clickhouse.jdbc.ClickHouseDriver";
     }
 
     @Override

File: dinky-gateway/src/main/java/org/dinky/gateway/sqlgateway/cli/SqlClientAdapter.java
Patch:
@@ -82,6 +82,8 @@ public void startClient() {
                 .orElseThrow(() -> new IllegalArgumentException("Flink Cluster address is not set."));
         configuration.set(RestOptions.BIND_ADDRESS, clusterAddress.getHostName());
         configuration.set(RestOptions.BIND_PORT, clusterAddress.getPort() + "");
+        configuration.set(RestOptions.ADDRESS, clusterAddress.getHostName());
+        configuration.set(RestOptions.PORT, clusterAddress.getPort());
         configuration.set(DeploymentOptions.TARGET, RemoteExecutor.NAME);
         //            当客户端程序终止时，作业执行也会终止。
         configuration.set(DeploymentOptions.ATTACHED, true);

File: dinky-admin/src/main/java/org/dinky/data/dto/StudioMetaStoreDTO.java
Patch:
@@ -22,6 +22,8 @@
 import org.dinky.data.enums.GatewayType;
 import org.dinky.job.JobConfig;
 
+import com.google.common.collect.Maps;
+
 import io.swagger.annotations.ApiModel;
 import io.swagger.annotations.ApiModelProperty;
 import lombok.Getter;
@@ -67,6 +69,7 @@ public class StudioMetaStoreDTO extends AbstractStatementDTO {
     public JobConfig getJobConfig() {
         return JobConfig.builder()
                 .type(GatewayType.LOCAL.getLongValue())
+                .configJson(Maps.newHashMap())
                 .useResult(true)
                 .useChangeLog(false)
                 .useAutoCancel(false)

File: dinky-core/src/main/java/org/dinky/job/JobManager.java
Patch:
@@ -256,6 +256,8 @@ public JobResult executeJarSql(String statement) throws Exception {
                 .map(t -> executor.pretreatStatement(t))
                 .collect(Collectors.toList());
         statement = String.join(";\n", statements);
+        jobParam =
+                Explainer.build(executor, useStatementSet, this).pretreatStatements(SqlUtil.getStatements(statement));
         job = Job.build(runMode, config, executorConfig, executor, statement, useGateway);
         ready();
         try {

File: dinky-admin/src/main/java/org/dinky/utils/SqliteUtil.java
Patch:
@@ -27,8 +27,6 @@
 import java.sql.Statement;
 import java.util.List;
 
-import org.jetbrains.annotations.NotNull;
-
 import lombok.extern.slf4j.Slf4j;
 
 @Slf4j
@@ -85,7 +83,7 @@ public void write(String tableName, List<String> columns, List<List<String>> val
         }
     }
 
-    private static @NotNull String createInsertSql(String tableName, List<String> columns) {
+    private static String createInsertSql(String tableName, List<String> columns) {
         StringBuilder columnNames = new StringBuilder();
         StringBuilder placeholders = new StringBuilder();
         for (int i = 0; i < columns.size(); i++) {

File: dinky-common/src/main/java/org/dinky/utils/SqlUtil.java
Patch:
@@ -61,7 +61,7 @@ public static String removeNote(String sql) {
             // Remove the special-space characters
             sql = sql.replaceAll("\u00A0", " ").replaceAll("[\r\n]+", "\n");
             // Remove annotations Support '--aa' , '/**aaa*/' , '//aa' , '#aaa'
-            Pattern p = Pattern.compile("(?ms)('(?:''|[^'])*')|--.*?$|/\\*[^+].*?\\*/|");
+            Pattern p = Pattern.compile("(?ms)('(?:[^'])*')|--.*?$|/\\*[^+].*?\\*/|");
             String presult = p.matcher(sql).replaceAll("$1");
             return presult.trim();
         }

File: dinky-admin/src/main/java/org/dinky/service/impl/TaskServiceImpl.java
Patch:
@@ -226,6 +226,9 @@ public JobConfig buildJobSubmitConfig(TaskDTO task) {
             log.info("Init savePoint");
             config.setSavePointPath(savepoints.getPath());
             config.getConfigJson().put("execution.savepoint.path", savepoints.getPath()); // todo: 写工具类处理相关配置
+        } else {
+            // When disabling checkpoints, delete the checkpoint path
+            config.setSavePointPath(null);
         }
         if (GatewayType.get(task.getType()).isDeployCluster()) {
             log.info("Init gateway config, type:{}", task.getType());

File: dinky-client/dinky-client-base/src/main/java/org/dinky/trans/dml/ExecuteJarOperation.java
Patch:
@@ -90,12 +90,12 @@ public static Pipeline getStreamGraph(
             Configuration configuration = tEnv.getConfig().getConfiguration();
             File file =
                     Opt.ofBlankAble(submitParam.getUri()).map(URLUtils::toFile).orElse(null);
+            String submitArgs = Opt.ofBlankAble(submitParam.getArgs()).orElse("");
             if (!PackagedProgramUtils.isPython(submitParam.getMainClass())) {
                 tEnv.addJar(file);
             } else {
                 // python submit
-                submitParam.setArgs("--python " + file.getAbsolutePath() + " "
-                        + Opt.ofBlankAble(submitParam.getArgs()).orElse(""));
+                submitParam.setArgs("--python " + file.getAbsolutePath() + " " + submitArgs);
                 file = null;
             }
 
@@ -104,7 +104,7 @@ public static Pipeline getStreamGraph(
                     .setEntryPointClassName(submitParam.getMainClass())
                     .setConfiguration(configuration)
                     .setSavepointRestoreSettings(savepointRestoreSettings)
-                    .setArguments(extractArgs(submitParam.getArgs().trim()).toArray(new String[0]))
+                    .setArguments(extractArgs(submitArgs.trim()).toArray(new String[0]))
                     .setUserClassPaths(classpaths)
                     .build();
             int parallelism = StrUtil.isNumeric(submitParam.getParallelism())

File: dinky-gateway/src/main/java/org/dinky/gateway/kubernetes/KubernetesGateway.java
Patch:
@@ -93,8 +93,10 @@ protected void initConfig() {
             logger.warn("load locale config yaml failed：{},Skip config it", e.getMessage());
         }
 
-        addConfigParas(flinkConfig.getConfiguration());
+        // -------------------Note: the sequence can not be changed, priority problem----------------
         addConfigParas(k8sConfig.getConfiguration());
+        addConfigParas(flinkConfig.getConfiguration());
+        // -------------------------------------------
         addConfigParas(DeploymentOptions.TARGET, getType().getLongValue());
         addConfigParas(KubernetesConfigOptions.CLUSTER_ID, flinkConfig.getJobName());
         addConfigParas(

File: dinky-function/src/main/java/org/dinky/function/compiler/JVMPackage.java
Patch:
@@ -35,7 +35,6 @@
 import cn.hutool.core.io.FileUtil;
 import cn.hutool.core.util.StrUtil;
 
-/** @since 0.6.8 */
 public class JVMPackage implements FunctionPackage {
 
     @Override
@@ -57,7 +56,7 @@ public String[] pack(List<UDF> udfList, Integer missionId) {
         for (int i = 0; i < classNameList.size(); i++) {
             String className = classNameList.get(i);
             String classFile = StrUtil.replace(className, ".", "/") + ".class";
-            String absoluteFilePath = PathConstant.getUdfCompilerJavaPath(missionId, classFile);
+            String absoluteFilePath = PathConstant.getUdfCompilerPath(FunctionLanguage.JAVA, classFile);
 
             clazzs[i] = classFile;
             fileInputStreams[i] = FileUtil.getInputStream(absoluteFilePath);

File: dinky-function/src/main/java/org/dinky/function/compiler/JavaCompiler.java
Patch:
@@ -23,6 +23,7 @@
 import org.dinky.function.data.model.UDF;
 
 import org.apache.flink.configuration.ReadableConfig;
+import org.apache.flink.table.catalog.FunctionLanguage;
 
 import lombok.extern.slf4j.Slf4j;
 
@@ -46,7 +47,7 @@ public synchronized boolean compiler(UDF udf, ReadableConfig conf, Integer missi
         // TODO 改为ProcessStep注释
         log.info("Compiling java code, class: {}", udf.getClassName());
         CustomStringJavaCompiler compiler = new CustomStringJavaCompiler(udf.getCode());
-        boolean res = compiler.compilerToTmpPath(PathConstant.getUdfCompilerJavaPath(missionId));
+        boolean res = compiler.compilerToTmpPath(PathConstant.getUdfCompilerPath(FunctionLanguage.JAVA));
         String className = compiler.getFullClassName();
         if (res) {
             log.info("class compiled successfully:{}", className);

File: dinky-function/src/main/java/org/dinky/function/compiler/ScalaCompiler.java
Patch:
@@ -39,7 +39,7 @@ public boolean compiler(UDF udf, ReadableConfig conf, Integer missionId) {
 
         String className = udf.getClassName();
         log.info("正在编译 scala 代码 , class: " + className);
-        if (CustomStringScalaCompiler.getInterpreter(missionId).compileString(udf.getCode())) {
+        if (CustomStringScalaCompiler.getInterpreter().compileString(udf.getCode())) {
             log.info("scala class编译成功:" + className);
             return true;
         } else {

File: dinky-function/src/main/java/org/dinky/function/util/UDFUtil.java
Patch:
@@ -214,7 +214,7 @@ public static Map<String, List<String>> buildJar(List<UDF> codeList) {
                 }
             } else if (udf.getFunctionLanguage() == FunctionLanguage.SCALA) {
                 String className = udf.getClassName();
-                if (CustomStringScalaCompiler.getInterpreter(null).compileString(udf.getCode())) {
+                if (CustomStringScalaCompiler.getInterpreter().compileString(udf.getCode())) {
                     log.info("scala class compile successful:{}", className);
                     ClassPool.push(ClassEntity.build(className, udf.getCode()));
                     successList.add(className);

File: dinky-admin/src/main/java/org/dinky/data/model/ClusterInstance.java
Patch:
@@ -96,7 +96,7 @@ public class ClusterInstance extends SuperEntity<ClusterInstance> {
             dataType = "Boolean",
             example = "test",
             notes = "is auto registers, if this record from projob/application mode , it will be true")
-    private Boolean autoRegisters;
+    private boolean autoRegisters;
 
     @ApiModelProperty(
             value = "clusterConfigurationId",

File: dinky-admin/src/main/java/org/dinky/job/handler/ClearJobHistoryHandler.java
Patch:
@@ -98,7 +98,7 @@ public void clearJobHistory(Integer maxRetainDays, Integer maxRetainCount) {
                     clusterDeleteWrapper
                             .lambda()
                             .in(true, ClusterInstance::getId, clusterDeleteIds)
-                            .eq(ClusterInstance::getAutoRegisters, true);
+                            .eq(ClusterInstance::isAutoRegisters, true);
                     clusterService.remove(clusterDeleteWrapper);
                 }
             }

File: dinky-admin/src/main/java/org/dinky/service/impl/ClusterInstanceServiceImpl.java
Patch:
@@ -178,7 +178,7 @@ public Boolean deleteClusterInstanceById(Integer id) {
         ClusterInstance clusterInstance = getById(id);
         // if cluster instance is not null and cluster instance is health, can not delete, must kill cluster instance
         // first
-        if (Asserts.isNotNull(clusterInstance) && checkHealth(clusterInstance)) {
+        if (Asserts.isNotNull(clusterInstance) && checkHealth(clusterInstance) && clusterInstance.isAutoRegisters()) {
             throw new BusException(Status.CLUSTER_INSTANCE_HEALTH_NOT_DELETE);
         }
         return removeById(id);
@@ -258,7 +258,7 @@ public ClusterInstance deploySessionCluster(Integer id) {
     public List<ClusterInstance> selectListByKeyWord(String searchKeyWord, boolean isAutoCreate) {
         return getBaseMapper()
                 .selectList(new LambdaQueryWrapper<ClusterInstance>()
-                        .and(true, i -> i.eq(ClusterInstance::getAutoRegisters, isAutoCreate))
+                        .and(true, i -> i.eq(ClusterInstance::isAutoRegisters, isAutoCreate))
                         .and(true, i -> i.like(ClusterInstance::getName, searchKeyWord)
                                 .or()
                                 .like(ClusterInstance::getAlias, searchKeyWord)

File: dinky-function/src/main/java/org/dinky/function/util/UDFUtil.java
Patch:
@@ -154,7 +154,7 @@ public static String templateParse(String dialect, String template, String class
         }
     }
 
-    public static String[] initJavaUDF(List<UDF> udf, GatewayType gatewayType, Integer missionId) {
+    public static String[] initJavaUDF(List<UDF> udf, Integer missionId) {
         return FunctionFactory.initUDF(
                         CollUtil.newArrayList(
                                 CollUtil.filterNew(udf, x -> x.getFunctionLanguage() != FunctionLanguage.PYTHON)),

File: dinky-function/src/main/java/org/dinky/function/data/model/UDF.java
Patch:
@@ -22,13 +22,15 @@
 import org.apache.flink.table.catalog.FunctionLanguage;
 
 import lombok.Builder;
+import lombok.EqualsAndHashCode;
 import lombok.Getter;
 import lombok.Setter;
 
 /** @since 0.6.8 */
 @Getter
 @Setter
 @Builder
+@EqualsAndHashCode
 public class UDF {
 
     /** 函数名 */

File: dinky-admin/src/main/java/org/dinky/context/TenantContextHolder.java
Patch:
@@ -40,8 +40,7 @@ public static void set(Object value) {
     }
 
     public static Object get() {
-        return Optional.ofNullable(TENANT_CONTEXT.get())
-                .orElseThrow(() -> new IllegalStateException("current Tenant context is not set"));
+        return TENANT_CONTEXT.get();
     }
 
     public static void clear() {

File: dinky-common/src/main/java/org/dinky/data/enums/ColumnType.java
Patch:
@@ -38,6 +38,7 @@ public enum ColumnType {
     INT("int", "INT NOT NULL"),
     JAVA_LANG_LONG("java.lang.Long", "BIGINT"),
     LONG("long", "BIGINT NOT NULL"),
+    BIG_INTEGER("java.math.BigInteger", "BIGINT"),
     JAVA_LANG_FLOAT("java.lang.Float", "FLOAT"),
     FLOAT("float", "FLOAT NOT NULL"),
     JAVA_LANG_DOUBLE("java.lang.Double", "DOUBLE"),

File: dinky-admin/src/main/java/org/dinky/context/TenantContextHolder.java
Patch:
@@ -40,7 +40,8 @@ public static void set(Object value) {
     }
 
     public static Object get() {
-        return TENANT_CONTEXT.get();
+        return Optional.ofNullable(TENANT_CONTEXT.get())
+                .orElseThrow(() -> new IllegalStateException("current Tenant context is not set"));
     }
 
     public static void clear() {

File: dinky-admin/src/main/java/org/flywaydb/database/mysql/MySQLDatabase.java
Patch:
@@ -172,7 +172,7 @@ public String getRawCreateScript(Table table, boolean baseline) {
                 + "    `success` BOOL NOT NULL,\n"
                 + "    CONSTRAINT "
                 + getConstraintName(table.getName()) + " PRIMARY KEY (`installed_rank`)\n" + ")"
-                + tablespace + " ENGINE=InnoDB" + baselineMarker
+                + tablespace + " DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_general_ci ENGINE=InnoDB" + baselineMarker
                 + ";\n"
                 + "CREATE INDEX `"
                 + table.getName() + "_s_idx` ON " + table + " (`success`);";

File: dinky-client/dinky-client-1.19/src/main/java/org/dinky/executor/ClusterDescriptorAdapterImpl.java
Patch:
@@ -41,9 +41,7 @@ public ClusterDescriptorAdapterImpl(YarnClusterDescriptor yarnClusterDescriptor)
     @Override
     public void addShipFiles(List<File> shipFiles) {
         yarnClusterDescriptor.addShipFiles(shipFiles.stream()
-                .map(file -> {
-                    return new Path(file.getPath());
-                })
+                .map(file -> new Path("file://" + file.getPath()))
                 .collect(Collectors.toList()));
     }
 

File: dinky-common/src/main/java/org/dinky/context/FreeMarkerHolder.java
Patch:
@@ -49,6 +49,7 @@ public class FreeMarkerHolder {
      */
     public FreeMarkerHolder() {
         configuration.setTemplateLoader(stringLoader);
+        configuration.setNumberFormat("computer");
         configuration.setDefaultEncoding("UTF-8");
         configuration.setTemplateExceptionHandler(TemplateExceptionHandler.RETHROW_HANDLER);
     }

File: dinky-admin/src/main/java/org/dinky/data/model/Task.java
Patch:
@@ -55,6 +55,7 @@ public class Task extends SuperEntity<Task> {
     @ApiModelProperty(value = "Dialect", dataType = "String", notes = "Dialect for the task")
     private String dialect;
 
+    @TableField(fill = FieldFill.INSERT)
     @ApiModelProperty(
             value = "Tenant ID",
             dataType = "Integer",

File: dinky-admin/src/main/java/org/dinky/mybatis/properties/MybatisPlusFillProperties.java
Patch:
@@ -49,4 +49,6 @@ public class MybatisPlusFillProperties {
     private String updaterField = "updater";
 
     private String operatorField = "operator";
+
+    private String tenantIdField = "tenantId";
 }

File: dinky-admin/src/main/java/org/dinky/data/dto/CatalogueTaskDTO.java
Patch:
@@ -22,7 +22,6 @@
 import org.dinky.data.model.ext.TaskExtConfig;
 import org.dinky.data.typehandler.ListTypeHandler;
 
-import java.util.ArrayList;
 import java.util.List;
 
 import com.baomidou.mybatisplus.annotation.TableField;
@@ -99,7 +98,7 @@ public class CatalogueTaskDTO {
             dataType = "List",
             notes = "list of secondary responsible persons' ids")
     @TableField(typeHandler = ListTypeHandler.class)
-    private List<Integer> secondLevelOwners = new ArrayList<>();
+    private List<Integer> secondLevelOwners;
 
     @ApiModelProperty(value = "Task", dataType = "TaskDTO", notes = "The task information")
     private TaskDTO task;

File: dinky-admin/src/main/java/org/dinky/data/dto/TaskDTO.java
Patch:
@@ -26,7 +26,6 @@
 import org.dinky.data.typehandler.ListTypeHandler;
 import org.dinky.job.JobConfig;
 
-import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
@@ -232,7 +231,7 @@ public class TaskDTO extends AbstractStatementDTO {
             dataType = "List",
             notes = "list of secondary responsible persons' ids")
     @TableField(typeHandler = ListTypeHandler.class)
-    private List<Integer> secondLevelOwners = new ArrayList<>();
+    private List<Integer> secondLevelOwners;
 
     public JobConfig getJobConfig() {
 

File: dinky-admin/src/main/java/org/dinky/data/dto/TaskSaveDTO.java
Patch:
@@ -27,7 +27,6 @@
 
 import org.apache.ibatis.type.JdbcType;
 
-import java.util.ArrayList;
 import java.util.List;
 
 import javax.validation.constraints.NotNull;
@@ -162,7 +161,7 @@ public class TaskSaveDTO {
             dataType = "List",
             notes = "list of secondary responsible persons' ids")
     @TableField(typeHandler = ListTypeHandler.class)
-    private List<Integer> secondLevelOwners = new ArrayList<>();
+    private List<Integer> secondLevelOwners;
 
     public Task toTaskEntity() {
         Task task = new Task();

File: dinky-admin/src/main/java/org/dinky/data/model/Task.java
Patch:
@@ -26,7 +26,6 @@
 
 import org.apache.ibatis.type.JdbcType;
 
-import java.util.ArrayList;
 import java.util.List;
 
 import com.baomidou.mybatisplus.annotation.FieldFill;
@@ -184,7 +183,7 @@ public class Task extends SuperEntity<Task> {
             dataType = "List",
             notes = "list of secondary responsible persons' ids")
     @TableField(typeHandler = ListTypeHandler.class)
-    private List<Integer> secondLevelOwners = new ArrayList<>();
+    private List<Integer> secondLevelOwners;
 
     public Task(Integer id, Integer jobInstanceId) {
         this.jobInstanceId = jobInstanceId;

File: dinky-admin/src/main/java/org/dinky/service/catalogue/strategy/impl/CatalogueTreeSortCreateTimeStrategy.java
Patch:
@@ -51,7 +51,7 @@ public class CatalogueTreeSortCreateTimeStrategy implements CatalogueTreeSortStr
      */
     @Override
     public List<Catalogue> sort(List<Catalogue> catalogueTree, SortTypeEnum sortTypeEnum) {
-        log.info(
+        log.debug(
                 "sort catalogue tree based on creation time. catalogueTree: {}, sortTypeEnum: {}",
                 catalogueTree,
                 sortTypeEnum);

File: dinky-admin/src/main/java/org/dinky/service/catalogue/strategy/impl/CatalogueTreeSortDefaultStrategy.java
Patch:
@@ -51,7 +51,7 @@ public class CatalogueTreeSortDefaultStrategy implements CatalogueTreeSortStrate
      */
     @Override
     public List<Catalogue> sort(List<Catalogue> catalogueTree, SortTypeEnum sortTypeEnum) {
-        log.info("sort catalogue tree based on id. catalogueTree: {}, sortTypeEnum: {}", catalogueTree, sortTypeEnum);
+        log.debug("sort catalogue tree based on id. catalogueTree: {}, sortTypeEnum: {}", catalogueTree, sortTypeEnum);
         return recursionSortCatalogues(catalogueTree, sortTypeEnum);
     }
 

File: dinky-admin/src/main/java/org/dinky/service/catalogue/strategy/impl/CatalogueTreeSortFirstLetterStrategy.java
Patch:
@@ -53,7 +53,7 @@ public class CatalogueTreeSortFirstLetterStrategy implements CatalogueTreeSortSt
      */
     @Override
     public List<Catalogue> sort(List<Catalogue> catalogueTree, SortTypeEnum sortTypeEnum) {
-        log.info(
+        log.debug(
                 "sort catalogue tree based on first letter. catalogueTree: {}, sortTypeEnum: {}",
                 catalogueTree,
                 sortTypeEnum);

File: dinky-core/src/main/java/org/dinky/executor/Executor.java
Patch:
@@ -153,6 +153,7 @@ private void initClassloader(DinkyClassLoader classLoader) {
     protected void init(DinkyClassLoader classLoader) {
         initClassloader(classLoader);
         this.dinkyClassLoader = classLoader;
+        Thread.currentThread().setContextClassLoader(classLoader);
         if (executorConfig.isValidParallelism()) {
             environment.setParallelism(executorConfig.getParallelism());
         }

File: dinky-core/src/main/java/org/dinky/job/builder/JobExecuteBuilder.java
Patch:
@@ -65,6 +65,8 @@ public void run() throws Exception {
                 }
                 GatewayResult gatewayResult = null;
                 config.addGatewayConfig(executor.getSetConfig());
+                config.getGatewayConfig().setSql(jobParam.getParsedSql());
+
                 if (runMode.isApplicationMode()) {
                     gatewayResult = Gateway.build(config.getGatewayConfig())
                             .submitJar(executor.getDinkyClassLoader().getUdfPathContextHolder());

File: dinky-gateway/src/main/java/org/dinky/gateway/kubernetes/operator/KubernetsOperatorGateway.java
Patch:
@@ -94,6 +94,7 @@ public boolean onJobFinishCallback(String status) {
     }
 
     public boolean deleteCluster() {
+        kubernetsConfiguration = config.getKubernetesConfig().getConfiguration();
         initConfig();
         initMetadata();
         getK8sClientHelper().getKubernetesClient().resource(flinkDeployment).delete();
@@ -160,7 +161,7 @@ private void initResource() {
     private void initSpec() {
         String flinkVersion = flinkConfig.getFlinkVersion();
         String image = kubernetsConfiguration.get("kubernetes.container.image");
-        String serviceAccount = kubernetsConfiguration.get("kubernetes.service.account");
+        String serviceAccount = kubernetsConfiguration.get("kubernetes.service-account");
 
         logger.info("\nflinkVersion is : {} \n image is : {}", flinkVersion, image);
 

File: dinky-gateway/src/main/java/org/dinky/gateway/kubernetes/utils/K8sClientHelper.java
Patch:
@@ -49,6 +49,7 @@
 import io.fabric8.kubernetes.client.Config;
 import io.fabric8.kubernetes.client.DefaultKubernetesClient;
 import io.fabric8.kubernetes.client.KubernetesClient;
+import io.fabric8.kubernetes.client.utils.Serialization;
 import lombok.Data;
 import lombok.extern.slf4j.Slf4j;
 
@@ -132,6 +133,7 @@ public Deployment createDinkyResource() {
         resources.forEach(resource ->
                 resource.getMetadata().setOwnerReferences(Collections.singletonList(deploymentOwnerReference)));
         // create resources
+        resources.forEach(resource -> log.info(Serialization.asYaml(resource)));
         kubernetesClient.resourceList(resources).createOrReplace();
         return deployment;
     }

File: dinky-admin/src/main/java/org/dinky/controller/APIController.java
Patch:
@@ -47,7 +47,6 @@
 
 import com.fasterxml.jackson.databind.node.ObjectNode;
 
-import cn.dev33.satoken.annotation.SaCheckLogin;
 import io.swagger.annotations.Api;
 import io.swagger.annotations.ApiImplicitParam;
 import io.swagger.annotations.ApiOperation;
@@ -63,7 +62,6 @@
 @Api(tags = "OpenAPI & Task API Controller")
 @RequestMapping("/openapi")
 @RequiredArgsConstructor
-@SaCheckLogin
 public class APIController {
 
     private final TaskService taskService;

File: dinky-admin/src/main/java/org/dinky/controller/SystemController.java
Patch:
@@ -33,6 +33,7 @@
 import org.springframework.web.bind.annotation.RequestParam;
 import org.springframework.web.bind.annotation.RestController;
 
+import cn.dev33.satoken.annotation.SaCheckLogin;
 import cn.dev33.satoken.annotation.SaCheckPermission;
 import io.swagger.annotations.Api;
 import io.swagger.annotations.ApiImplicitParam;
@@ -81,6 +82,7 @@ public Result<String> getRootLog() {
     @GetMapping("/readFile")
     @ApiOperation("Read File By File Path")
     @ApiImplicitParam(name = "path", value = "File Path", required = true, dataType = "String")
+    @SaCheckLogin
     public Result<String> readFile(@RequestParam String path) {
         return Result.data(systemService.readFile(path));
     }

File: dinky-admin/src/main/java/org/dinky/context/MetricsContextHolder.java
Patch:
@@ -55,6 +55,7 @@ public static MetricsContextHolder getInstances() {
 
     public void sendAsync(String key, MetricsVO o) {
         CompletableFuture.runAsync(() -> {
+                    Thread.currentThread().setContextClassLoader(MetricsContextHolder.class.getClassLoader());
                     metricsVOS.add(o);
                     long duration = System.currentTimeMillis() - lastDumpTime;
                     synchronized (metricsVOS) {

File: dinky-metadata/dinky-metadata-oracle/src/main/java/org/dinky/metadata/convert/OracleTypeConvert.java
Patch:
@@ -36,6 +36,7 @@ public OracleTypeConvert() {
         this.convertMap.clear();
         register("char", ColumnType.STRING);
         register("date", ColumnType.LOCAL_DATETIME);
+        register("time", ColumnType.TIME);
         register("timestamp", ColumnType.TIMESTAMP);
         register("number", OracleTypeConvert::convertNumber);
         register("float", ColumnType.JAVA_LANG_FLOAT);

File: dinky-admin/src/main/java/org/dinky/service/DataBaseService.java
Patch:
@@ -55,7 +55,7 @@ public interface DataBaseService extends ISuperService<DataBase> {
      * @param dataBase {@link DataBase}
      * @return {@link Boolean}
      */
-    Boolean checkHeartBeat(DataBase dataBase);
+    String checkHeartBeat(DataBase dataBase);
 
     /**
      * save or update database

File: dinky-common/src/main/java/org/dinky/data/enums/Status.java
Patch:
@@ -173,6 +173,7 @@ public enum Status {
 
     DATASOURCE_EXIST_RELATIONSHIP(11006, "datasource.exist.relationship"),
 
+    DATASOURCE_CONNECT_ERROR(11007, "datasource.connect.error"),
     /**
      * job or task about
      */

File: dinky-client/dinky-client-base/src/main/java/org/dinky/url/ResourceFileSystem.java
Patch:
@@ -21,14 +21,14 @@
 
 import org.dinky.resource.BaseResourceManager;
 
+import org.apache.flink.api.common.io.InputStreamFSInputWrapper;
 import org.apache.flink.core.fs.BlockLocation;
 import org.apache.flink.core.fs.FSDataInputStream;
 import org.apache.flink.core.fs.FSDataOutputStream;
 import org.apache.flink.core.fs.FileStatus;
 import org.apache.flink.core.fs.FileSystem;
 import org.apache.flink.core.fs.FileSystemKind;
 import org.apache.flink.core.fs.Path;
-import org.apache.flink.core.fs.local.LocalDataInputStream;
 import org.apache.flink.core.fs.local.LocalFileStatus;
 
 import java.io.File;
@@ -91,7 +91,7 @@ public FSDataInputStream open(Path f, int bufferSize) throws IOException {
 
     @Override
     public FSDataInputStream open(Path f) throws IOException {
-        return new LocalDataInputStream(getFile(f));
+        return new InputStreamFSInputWrapper(BASE_RESOURCE_MANAGER.readFile(f.getPath()));
     }
 
     @Override
@@ -142,6 +142,6 @@ public FileSystemKind getKind() {
     }
 
     public static ResourceFileSystem getSharedInstance() {
-        return INSTANCE;
+        return getInstance();
     }
 }

File: dinky-core/src/main/java/org/dinky/explainer/Explainer.java
Patch:
@@ -215,7 +215,7 @@ public ExplainResult explainSql(String statement) {
                 String error = StrFormatter.format(
                         "Exception in executing FlinkSQL:\n{}\n{}",
                         SqlUtil.addLineNumber(item.getValue()),
-                        e.getMessage());
+                        LogUtil.getError(e));
                 resultBuilder
                         .error(error)
                         .explainTrue(false)

File: dinky-client/dinky-client-base/src/main/java/org/dinky/trans/ddl/AddFileOperation.java
Patch:
@@ -30,13 +30,13 @@
 /**
  * @since 0.7.0
  */
-public class AddFilerOperation extends AbstractOperation implements ExtendOperation {
+public class AddFileOperation extends AbstractOperation implements ExtendOperation {
 
-    public AddFilerOperation(String statement) {
+    public AddFileOperation(String statement) {
         super(statement);
     }
 
-    public AddFilerOperation() {}
+    public AddFileOperation() {}
 
     @Override
     public Optional<? extends TableResult> execute(CustomTableEnvironment tEnv) {

File: dinky-client/dinky-client-base/src/main/java/org/dinky/trans/parse/AddFileSqlParseStrategy.java
Patch:
@@ -20,7 +20,7 @@
 package org.dinky.trans.parse;
 
 import org.dinky.data.exception.DinkyException;
-import org.dinky.trans.ddl.AddFilerOperation;
+import org.dinky.trans.ddl.AddFileOperation;
 import org.dinky.utils.URLUtils;
 
 import org.apache.flink.table.operations.Operation;
@@ -86,7 +86,7 @@ public static Set<File> getAllFilePath(String statements) {
 
     @Override
     public Operation convert(String statement) {
-        return new AddFilerOperation(statement);
+        return new AddFileOperation(statement);
     }
 
     @Override

File: dinky-cdc/dinky-cdc-core/src/main/java/org/dinky/cdc/AbstractSinkBuilder.java
Patch:
@@ -238,7 +238,7 @@ public void addSink(
 
     protected List<Operation> createInsertOperations(
             CustomTableEnvironment customTableEnvironment, Table table, String viewName, String tableName) {
-        String cdcSqlInsert = FlinkStatementUtil.getCDCInsertSql(table, tableName, viewName);
+        String cdcSqlInsert = FlinkStatementUtil.getCDCInsertSql(table, tableName, viewName, config);
         logger.info(cdcSqlInsert);
 
         List<Operation> operations = customTableEnvironment.getParser().parse(cdcSqlInsert);

File: dinky-admin/src/main/java/org/dinky/service/impl/UserServiceImpl.java
Patch:
@@ -182,7 +182,7 @@ public Result<UserDTO> loginUser(LoginDTO loginDTO) {
             user = loginDTO.isLdapLogin() ? ldapLogin(loginDTO) : localLogin(loginDTO);
         } catch (AuthException e) {
             // Handle authentication exceptions and return the corresponding error status
-            return Result.authorizeFailed(Status.USER_NOT_EXIST, loginDTO.getUsername());
+            return Result.authorizeFailed(e.getStatus());
         }
 
         // Check if the user is enabled

File: dinky-admin/src/main/java/org/dinky/service/impl/CatalogueServiceImpl.java
Patch:
@@ -222,6 +222,9 @@ private Task initTaskValue(CatalogueTaskDTO catalogueTask) {
                 task.setAlertGroupId(-1); // -1 is disabled
             }
         }
+        if (!Opt.ofNullable(task.getStep()).isPresent()) {
+            task.setStep(JobLifeCycle.DEVELOP.getValue());
+        }
         return task;
     }
 

File: dinky-admin/src/main/java/org/dinky/configure/cache/PaimonCache.java
Patch:
@@ -37,6 +37,7 @@
 
 import cn.hutool.cache.impl.TimedCache;
 import cn.hutool.core.convert.Convert;
+import cn.hutool.core.date.DateUtil;
 
 public class PaimonCache extends AbstractValueAdaptingCache {
     private static final Class<CacheData> clazz = CacheData.class;
@@ -45,7 +46,7 @@ public class PaimonCache extends AbstractValueAdaptingCache {
     /**
      * TIMEOUT CACHE
      */
-    private final cn.hutool.cache.Cache<Object, Object> cache = new TimedCache<>(1000 * 60 * 30);
+    private final cn.hutool.cache.Cache<Object, Object> cache = new TimedCache<>(1000 * 60 * 10);
 
     public PaimonCache(String cacheName) {
         super(true);
@@ -98,6 +99,7 @@ public void put(Object key, Object value) {
         cache.put(strKey, value);
         PaimonUtil.createOrGetTable(TABLE_NAME, clazz);
         CacheData cacheData = CacheData.builder()
+                .cacheTime(DateUtil.format(DateUtil.date(), "yyyy-MM-dd HH:mm"))
                 .cacheName(cacheName)
                 .key(strKey)
                 .data(serialize(value))

File: dinky-admin/src/main/java/org/dinky/controller/DataSourceController.java
Patch:
@@ -78,6 +78,7 @@ public class DataSourceController {
      * @param dataBaseDTO {@link DataBaseDTO}
      * @return {@link Result}< {@link Void}>
      */
+    @CacheEvict(cacheNames = "metadata_schema", key = "#dataBaseDTO.id")
     @PutMapping("/saveOrUpdate")
     @Log(title = "Insert Or Update DataBase", businessType = BusinessType.INSERT_OR_UPDATE)
     @ApiOperation("Insert Or Update DataBase")

File: dinky-admin/src/main/java/org/dinky/data/model/ClusterInstance.java
Patch:
@@ -21,8 +21,6 @@
 
 import org.dinky.mybatis.model.SuperEntity;
 
-import com.baomidou.mybatisplus.annotation.FieldFill;
-import com.baomidou.mybatisplus.annotation.TableField;
 import com.baomidou.mybatisplus.annotation.TableName;
 
 import io.swagger.annotations.ApiModel;
@@ -46,7 +44,6 @@ public class ClusterInstance extends SuperEntity<ClusterInstance> {
     @ApiModelProperty(value = "name", required = true, dataType = "String", example = "test")
     private Integer tenantId;
 
-    @TableField(fill = FieldFill.INSERT)
     @ApiModelProperty(
             value = "alias",
             required = true,

File: dinky-admin/src/main/java/org/dinky/mybatis/properties/MybatisPlusFillProperties.java
Patch:
@@ -49,6 +49,4 @@ public class MybatisPlusFillProperties {
     private String updaterField = "updater";
 
     private String operatorField = "operator";
-
-    private String name = "name";
 }

File: dinky-admin/src/main/java/org/dinky/data/dto/APIExecuteSqlDTO.java
Patch:
@@ -128,7 +128,7 @@ public JobConfig getJobConfig() {
                 .useRemote(true)
                 .address(address)
                 .jobName(jobName)
-                .fragment(getFragment())
+                .fragment(isFragment())
                 .statementSet(useStatementSet)
                 .maxRowNum(maxRowNum)
                 .checkpoint(checkPoint)

File: dinky-admin/src/main/java/org/dinky/data/dto/APIExplainSqlDTO.java
Patch:
@@ -59,7 +59,7 @@ public JobConfig getJobConfig() {
         return JobConfig.builder()
                 .type(GatewayType.LOCAL.getLongValue())
                 .useRemote(false)
-                .fragment(getFragment())
+                .fragment(isFragment())
                 .statementSet(useStatementSet)
                 .parallelism(parallelism)
                 .configJson(configuration)

File: dinky-admin/src/main/java/org/dinky/data/dto/AbstractStatementDTO.java
Patch:
@@ -42,7 +42,7 @@ public class AbstractStatementDTO {
     private Integer envId;
 
     @ApiModelProperty(value = "Fragment Flag", dataType = "boolean", example = "false", notes = "是否为片段")
-    private Boolean fragment = false;
+    private boolean fragment = false;
 
     @ApiModelProperty(
             value = "Variables",

File: dinky-admin/src/main/java/org/dinky/data/dto/StudioMetaStoreDTO.java
Patch:
@@ -70,7 +70,7 @@ public JobConfig getJobConfig() {
                 .useResult(true)
                 .useChangeLog(false)
                 .useAutoCancel(false)
-                .fragment(getFragment())
+                .fragment(isFragment())
                 .statementSet(false)
                 .batchModel(false)
                 .maxRowNum(0)

File: dinky-admin/src/main/java/org/dinky/service/impl/TaskServiceImpl.java
Patch:
@@ -285,7 +285,7 @@ public JobConfig buildJobConfig(TaskDTO task) {
     public String buildEnvSql(AbstractStatementDTO task) {
         log.info("Start initialize FlinkSQLEnv:");
         String sql = CommonConstant.LineSep;
-        if (task.getFragment()) {
+        if (task.isFragment()) {
             String flinkWithSql = dataBaseService.getEnabledFlinkWithSql();
             if (Asserts.isNotNullString(flinkWithSql)) {
                 sql += flinkWithSql + CommonConstant.LineSep;

File: dinky-admin/src/main/java/org/dinky/data/model/SqlGeneration.java
Patch:
@@ -40,11 +40,11 @@ public class SqlGeneration {
             value = "Flink SQL Create Statement",
             dataType = "String",
             notes = "Flink SQL statement for creating a table or view")
-    private String flinkSqlCreate;
+    private String flinkSqlCreate = "";
 
     @ApiModelProperty(value = "SQL Select Statement", dataType = "String", notes = "SQL SELECT statement")
-    private String sqlSelect;
+    private String sqlSelect = "";
 
     @ApiModelProperty(value = "SQL Create Statement", dataType = "String", notes = "SQL statement for creating a table")
-    private String sqlCreate;
+    private String sqlCreate = "";
 }

File: dinky-core/src/main/java/org/dinky/constant/FlinkSQLConstant.java
Patch:
@@ -28,7 +28,7 @@ public class FlinkSQLConstant {
     private FlinkSQLConstant() {}
 
     /** 分隔符 */
-    public static final String SEPARATOR = ";\\n";
+    public static final String SEPARATOR = ";\n";
 
     /** The define identifier of FlinkSQL Variable */
     public static final String VARIABLES = ":=";

File: dinky-admin/src/main/java/org/dinky/data/dto/TaskSaveDTO.java
Patch:
@@ -143,6 +143,9 @@ public class TaskSaveDTO {
     @ApiModelProperty(value = "Statement", dataType = "String", notes = "SQL statement for the task")
     private String statement;
 
+    @ApiModelProperty(value = "Step", dataType = "Integer", example = "1", notes = "Step for the task")
+    private Integer step;
+
     public Task toTaskEntity() {
         Task task = new Task();
         BeanUtil.copyProperties(this, task);

File: dinky-metadata/dinky-metadata-sqlserver/src/main/java/org/dinky/metadata/convert/SqlServerTypeConvert.java
Patch:
@@ -50,7 +50,7 @@ public SqlServerTypeConvert() {
         register("datetime", ColumnType.TIMESTAMP);
         register("date", ColumnType.LOCAL_DATE);
         register("time", ColumnType.LOCALTIME);
-        register("timestamp", ColumnType.BYTES);
+        register("timestamp", ColumnType.STRING);
         register("binary", ColumnType.BYTES);
         register("varbinary", ColumnType.BYTES);
         register("image", ColumnType.BYTES);

File: dinky-client/dinky-client-base/src/main/java/org/dinky/trans/dml/ExecuteJarOperation.java
Patch:
@@ -92,6 +92,7 @@ public static StreamGraph getStreamGraph(JarSubmitParam submitParam, CustomTable
                     .setArguments(RunTimeUtil.handleCmds(submitParam.getArgs()))
                     .build();
             Pipeline pipeline = PackagedProgramUtils.getPipelineFromProgram(program, configuration, 1, true);
+            program.close();
             Assert.isTrue(pipeline instanceof StreamGraph, "can not translate");
             return (StreamGraph) pipeline;
         } catch (Exception e) {

File: dinky-app/dinky-app-base/src/main/java/org/dinky/app/flinksql/Submitter.java
Patch:
@@ -111,6 +111,7 @@ public static void submit(AppParamConfig config) throws SQLException {
                 .type(appTask.getType())
                 .checkpoint(appTask.getCheckPoint())
                 .parallelism(appTask.getParallelism())
+                .useSqlFragment(appTask.getFragment())
                 .useStatementSet(appTask.getStatementSet())
                 .useBatchModel(appTask.getBatchModel())
                 .savePointPath(appTask.getSavePointPath())

File: dinky-admin/src/main/java/org/dinky/data/model/ext/JobAlertData.java
Patch:
@@ -185,7 +185,7 @@ public static JobAlertData buildData(JobInfoDetail jobInfoDetail) {
 
         if (jobDataDto.isError()) {
             builder.errorMsg(jobDataDto.getErrorMsg());
-        } else if (exceptions != null && ExceptionRule.isException(id, exceptions)) {
+        } else if (exceptions != null && ExceptionRule.isException(exceptions)) {
             // The error message is too long to send an alarm,
             // and only the first line of abnormal information is used
             String err = Optional.ofNullable(exceptions.getRootException())
@@ -198,8 +198,8 @@ public static JobAlertData buildData(JobInfoDetail jobInfoDetail) {
         }
 
         if (checkpoints != null) {
-            builder.checkpointCostTime(CheckpointsRule.checkpointTime(id, checkpoints))
-                    .isCheckpointFailed(CheckpointsRule.checkFailed(id, checkpoints));
+            builder.checkpointCostTime(CheckpointsRule.checkpointTime(checkpoints))
+                    .isCheckpointFailed(CheckpointsRule.checkFailed(checkpoints));
             if (checkpoints.getCounts() != null) {
                 builder.checkpointFailedCount(checkpoints.getCounts().getNumberFailedCheckpoints())
                         .checkpointCompleteCount(checkpoints.getCounts().getNumberCompletedCheckpoints());

File: dinky-admin/src/main/java/org/dinky/service/impl/JobInstanceServiceImpl.java
Patch:
@@ -170,7 +170,9 @@ public JobInfoDetail getJobInfoDetailInfo(JobInstance jobInstance) {
             if (Asserts.isNotNull(history.getClusterConfigurationId())) {
                 ClusterConfiguration clusterConfig =
                         clusterConfigurationService.getClusterConfigById(history.getClusterConfigurationId());
-                jobInfoDetail.setClusterConfiguration(ClusterConfigurationDTO.fromBean(clusterConfig));
+                if (clusterConfig != null) {
+                    jobInfoDetail.setClusterConfiguration(ClusterConfigurationDTO.fromBean(clusterConfig));
+                }
             }
         }
 

File: dinky-common/src/main/java/org/dinky/data/model/SystemConfiguration.java
Patch:
@@ -68,7 +68,7 @@ public static Configuration.OptionBuilder key(Status status) {
             .note(Status.SYS_FLINK_SETTINGS_USERESTAPI_NOTE);
     private final Configuration<String> sqlSeparator = key(Status.SYS_FLINK_SETTINGS_SQLSEPARATOR)
             .stringType()
-            .defaultValue(";\\s*(?:\\n|--.*)")
+            .defaultValue(";\\n")
             .note(Status.SYS_FLINK_SETTINGS_SQLSEPARATOR_NOTE);
     private final Configuration<Integer> jobIdWait = key(Status.SYS_FLINK_SETTINGS_JOBIDWAIT)
             .intType()

File: dinky-common/src/main/java/org/dinky/utils/SqlUtil.java
Patch:
@@ -45,7 +45,8 @@ public static String[] getStatements(String sql, String sqlSeparator) {
             return new String[0];
         }
 
-        String[] splits = sql.replace("\r\n", "\n").split(sqlSeparator);
+        final String localSqlSeparator = ";\\s*(?:\\n|--.*)";
+        String[] splits = sql.replace("\r\n", "\n").split(localSqlSeparator);
         String lastStatement = splits[splits.length - 1].trim();
         if (lastStatement.endsWith(SEMICOLON)) {
             splits[splits.length - 1] = lastStatement.substring(0, lastStatement.length() - 1);

File: dinky-admin/src/main/java/org/dinky/service/impl/LdapServiceImpl.java
Patch:
@@ -73,7 +73,7 @@ public User authenticate(LoginDTO loginDTO) throws AuthException {
         if (result.size() == 0) {
             log.info(String.format(
                     "No results found for search, base: '%s'; filter: '%s'", configuration.getLdapBaseDn(), filter));
-            throw new AuthException(Status.USER_NOT_EXIST);
+            throw new AuthException(Status.USER_NOT_EXIST, loginDTO.getUsername());
         } else if (result.size() > 1) {
             log.error(String.format(
                     "IncorrectResultSize, base: '%s'; filter: '%s'", configuration.getLdapBaseDn(), filter));

File: dinky-common/src/main/java/org/dinky/utils/SplitUtil.java
Patch:
@@ -35,7 +35,7 @@
 public class SplitUtil {
     public static final String ENABLE = "enable";
     public static final String MATCH_NUMBER_REGEX = "match_number_regex";
-    public static final String MAX_MATCH_VALUE = "match_number_regex";
+    public static final String MAX_MATCH_VALUE = "max_match_value";
     public static final String MATCH_WAY = "match_way";
 
     public static boolean contains(String regex, String sourceData) {

File: dinky-common/src/main/java/org/dinky/utils/SqlUtil.java
Patch:
@@ -60,7 +60,7 @@ public static String removeNote(String sql) {
             // Remove the special-space characters
             sql = sql.replaceAll("\u00A0", " ").replaceAll("[\r\n]+", "\n");
             // Remove annotations Support '--aa' , '/**aaa*/' , '//aa' , '#aaa'
-            Pattern p = Pattern.compile("(?ms)('(?:''|[^'])*')|--.*?$|//.*?$|/\\*[^+].*?\\*/|#.*?$|");
+            Pattern p = Pattern.compile("(?ms)('(?:''|[^'])*')|--.*?$|/\\*[^+].*?\\*/|");
             String presult = p.matcher(sql).replaceAll("$1");
             return presult.trim();
         }

File: dinky-metadata/dinky-metadata-postgresql/src/main/java/org/dinky/metadata/convert/PostgreSqlTypeConvert.java
Patch:
@@ -30,7 +30,7 @@ public class PostgreSqlTypeConvert extends AbstractJdbcTypeConvert {
 
     public PostgreSqlTypeConvert() {
         this.convertMap.clear();
-        register("smallint", ColumnType.SHORT, ColumnType.JAVA_LANG_SHORT);
+        register("smallint", ColumnType.INT, ColumnType.INTEGER);
         register("int2", ColumnType.SHORT, ColumnType.JAVA_LANG_SHORT);
         register("smallserial", ColumnType.SHORT, ColumnType.JAVA_LANG_SHORT);
         register("serial2", ColumnType.SHORT, ColumnType.JAVA_LANG_SHORT);

File: dinky-core/src/main/java/org/dinky/api/FlinkAPI.java
Patch:
@@ -94,7 +94,7 @@ private JsonNode get(String route) {
     }
 
     /**
-     * get请求获取jobManger/TaskManager的日志 (结果为字符串并不是json格式)
+     * get请求获取jobManager/TaskManager的日志 (结果为字符串并不是json格式)
      *
      * @param route route
      * @return {@link String}

File: dinky-admin/src/main/java/org/dinky/configure/AppConfig.java
Patch:
@@ -81,8 +81,7 @@ public void addInterceptors(InterceptorRegistry registry) {
                 }))
                 .addPathPatterns("/api/**")
                 .excludePathPatterns(
-                        "/api/login", "/api/ldap/ldapEnableStatus",
-                        "/druid/**", "/openapi/**");
+                        "/api/login", "/api/ldap/ldapEnableStatus", "/download/**", "/druid/**", "/openapi/**");
 
         registry.addInterceptor(new TenantInterceptor())
                 .addPathPatterns("/api/**")

File: dinky-client/dinky-client-base/src/main/java/org/dinky/executor/CustomTableEnvironment.java
Patch:
@@ -84,7 +84,7 @@ default void addJar(File... jarPath) {
                 Arrays.stream(URLUtil.getURLs(jarPath)).map(URL::toString).collect(Collectors.toList());
         List<String> jars = configuration.get(PipelineOptions.JARS);
         if (jars == null) {
-            configuration.set(PipelineOptions.JARS, pathList);
+            addConfiguration(PipelineOptions.JARS, pathList);
         } else {
             CollUtil.addAll(jars, pathList);
         }

File: dinky-function/src/main/java/org/dinky/function/util/UDFUtil.java
Patch:
@@ -95,7 +95,7 @@
 public class UDFUtil {
 
     public static final String FUNCTION_SQL_REGEX =
-            "^CREATE\\s+(?:(?:TEMPORARY|TEMPORARY\\s+SYSTEM)\\s+)?FUNCTION\\s+(?:IF\\s+NOT\\s+EXISTS\\s+)?(\\S+)\\s+AS\\s+'(\\S+)'\\s*(?:LANGUAGE\\s+(?:JAVA|SCALA|PYTHON)\\s+)?(?:USING\\s+JAR\\s+'(\\S+)'\\s*(?:,\\s*JAR\\s+'(\\S+)'\\s*)*)?";
+            "^CREATE\\s+(?:(?:TEMPORARY|TEMPORARY\\s+SYSTEM)\\s+)?FUNCTION\\s+(?:IF\\s+NOT\\s+EXISTS\\s+)?(\\S+)\\s+AS\\s+'(\\S+)'\\s*(?:LANGUAGE\\s+(?:JAVA|SCALA|PYTHON)\\s*)?(?:USING\\s+JAR\\s+'(\\S+)'\\s*(?:,\\s*JAR\\s+'(\\S+)'\\s*)*)?";
     public static final Pattern PATTERN = Pattern.compile(FUNCTION_SQL_REGEX, Pattern.CASE_INSENSITIVE);
 
     public static final String SESSION = "SESSION";
@@ -118,7 +118,7 @@ public class UDFUtil {
      */
     protected static final Map<String, Integer> UDF_MD5_MAP = new HashMap<>();
 
-    public static final String PYTHON_UDF_ATTR = "(\\S)\\s+=\\s+ud(?:f|tf|af|taf)";
+    public static final String PYTHON_UDF_ATTR = "(\\S+)\\s*=\\s*ud(?:f|tf|af|taf)";
     public static final String PYTHON_UDF_DEF = "@ud(?:f|tf|af|taf).*\\n+def\\s+(.*)\\(.*\\):";
     public static final String SCALA_UDF_CLASS = "class\\s+(\\w+)(\\s*\\(.*\\)){0,1}\\s+extends";
     public static final String SCALA_UDF_PACKAGE = "package\\s+(.*);";
@@ -343,7 +343,7 @@ public static UDF toUDF(String statement, DinkyClassLoader classLoader) {
             String gitPackage = UdfCodePool.getGitPackage(className);
 
             if (StrUtil.isNotBlank(gitPackage) && FileUtil.exist(gitPackage)) {
-                if (FileUtil.getSuffix(gitPackage).equals("jar")) {
+                if ("jar".equals(FileUtil.getSuffix(gitPackage))) {
                     udfPathContextHolder.addUdfPath(new File(gitPackage));
                 } else {
                     udfPathContextHolder.addPyUdfPath(new File(gitPackage));

File: dinky-gateway/src/main/java/org/dinky/gateway/kubernetes/KubernetesGateway.java
Patch:
@@ -40,6 +40,7 @@
 import org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient;
 import org.apache.flink.kubernetes.kubeclient.FlinkKubeClient;
 import org.apache.flink.kubernetes.kubeclient.FlinkKubeClientFactory;
+import org.apache.flink.python.PythonOptions;
 import org.apache.http.util.TextUtils;
 
 import java.lang.reflect.Method;
@@ -101,6 +102,8 @@ private void initConfig() {
         preparPodTemplate(k8sConfig.getKubeConfig(), KubernetesConfigOptions.KUBE_CONFIG_FILE);
 
         if (getType().isApplicationMode()) {
+            // remove python file
+            configuration.removeConfig(PythonOptions.PYTHON_FILES);
             resetCheckpointInApplicationMode(flinkConfig.getJobName());
         }
     }

File: dinky-client/dinky-client-base/src/main/java/org/dinky/sql/FlinkQuery.java
Patch:
@@ -19,6 +19,8 @@
 
 package org.dinky.sql;
 
+import org.dinky.data.model.SystemConfiguration;
+
 /**
  * FlinkQuery
  *
@@ -27,7 +29,7 @@
 public class FlinkQuery {
 
     public static String separator() {
-        return ";\n";
+        return SystemConfiguration.getInstances().getSqlSeparator();
     }
 
     public static String defaultCatalog() {

File: dinky-common/src/main/java/org/dinky/data/model/SystemConfiguration.java
Patch:
@@ -68,7 +68,7 @@ public static Configuration.OptionBuilder key(Status status) {
             .note(Status.SYS_FLINK_SETTINGS_USERESTAPI_NOTE);
     private final Configuration<String> sqlSeparator = key(Status.SYS_FLINK_SETTINGS_SQLSEPARATOR)
             .stringType()
-            .defaultValue(";\\n")
+            .defaultValue(";\\s*(?:\\n|--.*)")
             .note(Status.SYS_FLINK_SETTINGS_SQLSEPARATOR_NOTE);
     private final Configuration<Integer> jobIdWait = key(Status.SYS_FLINK_SETTINGS_JOBIDWAIT)
             .intType()

File: dinky-common/src/main/java/org/dinky/utils/SqlUtil.java
Patch:
@@ -45,7 +45,7 @@ public static String[] getStatements(String sql, String sqlSeparator) {
             return new String[0];
         }
 
-        String[] splits = sql.replace(";\r\n", ";\n").split(sqlSeparator);
+        String[] splits = sql.replace("\r\n", "\n").split(sqlSeparator);
         String lastStatement = splits[splits.length - 1].trim();
         if (lastStatement.endsWith(SEMICOLON)) {
             splits[splits.length - 1] = lastStatement.substring(0, lastStatement.length() - 1);

File: dinky-cdc/dinky-cdc-core/src/main/java/org/dinky/cdc/utils/FlinkStatementUtil.java
Patch:
@@ -35,7 +35,7 @@ private FlinkStatementUtil() {}
 
     public static String getCDCInsertSql(Table table, String targetName, String sourceName) {
         StringBuilder sb = new StringBuilder("INSERT INTO ");
-        sb.append(targetName);
+        sb.append("`").append(targetName).append("`");
         sb.append(" SELECT\n");
         for (int i = 0; i < table.getColumns().size(); i++) {
             sb.append("    ");

File: dinky-core/src/main/java/org/dinky/trans/ddl/CreateCDCSourceOperation.java
Patch:
@@ -123,9 +123,9 @@ public TableResult execute(Executor executor) {
                     // 分库分表所有表结构都是一样的，取出列表中第一个表名即可
                     String schemaTableName = table.getSchemaTableNameList().get(0);
                     // 真实的表名
+                    String realSchemaName = schemaTableName.split("\\.")[0];
                     String tableName = schemaTableName.split("\\.")[1];
-                    table.setColumns(driver.listColumnsSortByPK(schemaName, tableName));
-                    table.setColumns(driver.listColumnsSortByPK(schemaName, table.getName()));
+                    table.setColumns(driver.listColumnsSortByPK(realSchemaName, tableName));
                     schemaList.add(schema);
 
                     if (null != sinkDriver) {

File: dinky-app/dinky-app-base/src/main/java/org/dinky/app/util/FlinkAppUtil.java
Patch:
@@ -79,7 +79,6 @@ public static void monitorFlinkTask(Executor executor, int taskId) {
     public static void monitorFlinkTask(JobClient jobClient, int taskId) {
         boolean isRun = true;
         String jobId = jobClient.getJobID().toHexString();
-
         try {
             while (isRun) {
                 String jobStatus = jobClient.getJobStatus().get().toString();

File: dinky-admin/src/main/java/org/dinky/utils/GitRepository.java
Patch:
@@ -124,6 +124,7 @@ public void write(String str) {
                                 super.write(str);
                             }
                         }))
+                        .setCredentialsProvider(new UsernamePasswordCredentialsProvider(username, password))
                         .call();
                 git.close();
             } else {

File: dinky-cdc/dinky-cdc-core/src/main/java/org/dinky/cdc/sql/SQLSinkBuilder.java
Patch:
@@ -71,7 +71,8 @@ private String addSourceTableView(
             CustomTableEnvironment customTableEnvironment, DataStream<Row> rowDataDataStream, Table table) {
         // 上游表名称
         String viewName = "VIEW_" + table.getSchemaTableNameWithUnderline();
-        customTableEnvironment.createTemporaryView(viewName, rowDataDataStream);
+        customTableEnvironment.createTemporaryView(
+                viewName, customTableEnvironment.fromChangelogStream(rowDataDataStream));
         logger.info("Create {} temporaryView successful...", viewName);
         return viewName;
     }

File: dinky-cdc/dinky-cdc-core/src/main/java/org/dinky/cdc/sql/catalog/SQLCatalogSinkBuilder.java
Patch:
@@ -70,7 +70,8 @@ public void addTableSink(
         String sinkTableName = catalogName + "." + sinkSchemaName + "." + tableName;
         String viewName = "VIEW_" + table.getSchemaTableNameWithUnderline();
 
-        customTableEnvironment.createTemporaryView(viewName, rowDataDataStream);
+        customTableEnvironment.createTemporaryView(
+                viewName, customTableEnvironment.fromChangelogStream(rowDataDataStream));
         logger.info("Create {} temporaryView successful...", viewName);
 
         createInsertOperations(customTableEnvironment, table, viewName, sinkTableName);

File: dinky-admin/src/main/java/org/dinky/data/dto/AbstractStatementDTO.java
Patch:
@@ -19,6 +19,7 @@
 
 package org.dinky.data.dto;
 
+import java.util.HashMap;
 import java.util.Map;
 
 import io.swagger.annotations.ApiModel;
@@ -41,12 +42,12 @@ public class AbstractStatementDTO {
     private Integer envId;
 
     @ApiModelProperty(value = "Fragment Flag", dataType = "boolean", example = "false", notes = "是否为片段")
-    private Boolean fragment;
+    private Boolean fragment = false;
 
     @ApiModelProperty(
             value = "Variables",
             dataType = "Map<String, String>",
             example = "{\"key\": \"value\"}",
             notes = "变量集合")
-    private Map<String, String> variables;
+    private Map<String, String> variables = new HashMap<>();
 }

File: dinky-metadata/dinky-metadata-mysql/src/main/java/org/dinky/metadata/driver/MySqlDriver.java
Patch:
@@ -108,7 +108,7 @@ private String genTable(Table table) {
 
                     final String dv = column.getDefaultValue();
                     String defaultValue = Asserts.isNotNull(dv)
-                            ? String.format(" DEFAULT %s", dv.isEmpty() ? "\"\"" : dv)
+                            ? String.format(" DEFAULT '%s'", dv.isEmpty() ? "''" : dv)
                             : String.format("%s NULL ", !column.isNullable() ? " NOT " : "");
 
                     return String.format(

File: dinky-alert/dinky-alert-email/src/main/java/org/dinky/alert/email/params/EmailParams.java
Patch:
@@ -44,5 +44,5 @@ public class EmailParams {
     private boolean sslEnable;
     private String user;
     private String password;
-    private String smtpSslTrust;
+    private String smtpSslTrust = "";
 }

File: dinky-core/src/main/java/org/dinky/executor/Executor.java
Patch:
@@ -209,7 +209,7 @@ private void addJar(String... jarPath) {
         Configuration configuration = tableEnvironment.getRootConfiguration();
         List<String> jars = configuration.get(PipelineOptions.JARS);
         if (jars == null) {
-            configuration.set(PipelineOptions.JARS, CollUtil.newArrayList(jarPath));
+            tableEnvironment.addConfiguration(PipelineOptions.JARS, CollUtil.newArrayList(jarPath));
         } else {
             CollUtil.addAll(jars, jarPath);
         }

File: dinky-gateway/src/main/java/org/dinky/gateway/kubernetes/KubernetesGateway.java
Patch:
@@ -50,6 +50,7 @@
 import cn.hutool.core.lang.Assert;
 import cn.hutool.core.text.StrFormatter;
 import cn.hutool.core.util.ReflectUtil;
+import io.fabric8.kubernetes.client.Config;
 import io.fabric8.kubernetes.client.DefaultKubernetesClient;
 import io.fabric8.kubernetes.client.KubernetesClient;
 
@@ -120,7 +121,7 @@ private void initKubeClient() {
         if (TextUtils.isEmpty(k8sConfig.getKubeConfig())) {
             kubernetesClient = new DefaultKubernetesClient();
         } else {
-            kubernetesClient = DefaultKubernetesClient.fromConfig(k8sConfig.getKubeConfig());
+            kubernetesClient = new DefaultKubernetesClient(Config.fromKubeconfig(k8sConfig.getKubeConfig()));
         }
     }
 

File: dinky-admin/src/main/java/org/dinky/job/handler/JobRefreshHandler.java
Patch:
@@ -122,8 +122,7 @@ public static boolean refreshJob(JobInfoDetail jobInfoDetail, boolean needSave)
         } else {
             jobInfoDetail.setJobDataDto(jobDataDto);
             FlinkJobDetailInfo flinkJobDetailInfo = jobDataDto.getJob();
-            //            The YARN running status is no longer monitored
-            jobInstance.setStatus(flinkJobDetailInfo.getState());
+            jobInstance.setStatus(getJobStatus(jobInfoDetail).getValue());
             jobInstance.setDuration(flinkJobDetailInfo.getDuration());
             jobInstance.setCreateTime(TimeUtil.toLocalDateTime(flinkJobDetailInfo.getStartTime()));
             // if the job is still running the end-time is -1
@@ -244,7 +243,8 @@ private static JobStatus getJobStatus(JobInfoDetail jobInfoDetail) {
 
         ClusterConfigurationDTO clusterCfg = jobInfoDetail.getClusterConfiguration();
 
-        if (!Asserts.isNull(clusterCfg)) {
+        if (!Asserts.isNull(clusterCfg)
+                && GatewayType.YARN_PER_JOB.getLongValue().equals(clusterCfg.getType())) {
             try {
                 String appId = jobInfoDetail.getClusterInstance().getName();
 

File: dinky-client/dinky-client-base/src/main/java/org/dinky/executor/CustomTableEnvironment.java
Patch:
@@ -41,7 +41,6 @@
 import java.util.Arrays;
 import java.util.Collections;
 import java.util.List;
-import java.util.Map;
 import java.util.stream.Collectors;
 
 import com.fasterxml.jackson.databind.node.ObjectNode;
@@ -67,8 +66,6 @@ public interface CustomTableEnvironment
 
     SqlExplainResult explainSqlRecord(String statement, ExplainDetail... extraDetails);
 
-    boolean parseAndLoadConfiguration(String statement, Map<String, Object> setMap);
-
     StreamExecutionEnvironment getStreamExecutionEnvironment();
 
     Planner getPlanner();

File: dinky-admin/src/main/java/org/dinky/service/TaskService.java
Patch:
@@ -116,7 +116,7 @@ public interface TaskService extends ISuperService<Task> {
      * @param task The {@link TaskDTO} object representing the task to cancel.
      * @return true if the task job is successfully cancelled, false otherwise.
      */
-    boolean cancelTaskJob(TaskDTO task, boolean withSavePoint);
+    boolean cancelTaskJob(TaskDTO task, boolean withSavePoint, boolean forceCancel);
 
     /**
      * Get the stream graph of the given task job.

File: dinky-core/src/main/java/org/dinky/job/builder/JobTransBuilder.java
Patch:
@@ -183,8 +183,8 @@ private GatewayResult submitByGateway(List<String> inserts) {
 
         GatewayResult gatewayResult = null;
 
-        // Use gateway need to build gateway config, include flink configeration.
-        config.addGatewayConfig(executor.getSetConfig());
+        // Use gateway need to build gateway config, include flink configuration.
+        config.addGatewayConfig(executor.getCustomTableEnvironment().getConfig().getConfiguration());
 
         if (runMode.isApplicationMode()) {
             // Application mode need to submit dinky-app.jar that in the hdfs or image.

File: dinky-gateway/src/main/java/org/dinky/gateway/config/FlinkConfig.java
Patch:
@@ -107,8 +107,6 @@ public class FlinkConfig {
 
     private static final ObjectMapper mapper = new ObjectMapper();
 
-    public static final String DEFAULT_SAVEPOINT_PREFIX = "hdfs:///flink/savepoints/";
-
     public FlinkConfig() {}
 
     public FlinkConfig(Map<String, String> configuration) {

File: dinky-admin/src/main/java/org/dinky/interceptor/TenantInterceptor.java
Patch:
@@ -35,6 +35,7 @@
 import javax.servlet.http.HttpServletRequest;
 import javax.servlet.http.HttpServletResponse;
 
+import org.jetbrains.annotations.NotNull;
 import org.springframework.web.servlet.AsyncHandlerInterceptor;
 
 import cn.dev33.satoken.SaManager;
@@ -47,7 +48,7 @@
 public class TenantInterceptor implements AsyncHandlerInterceptor {
 
     @Override
-    public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler)
+    public boolean preHandle(HttpServletRequest request, @NotNull HttpServletResponse response, @NotNull Object handler)
             throws Exception {
         boolean isPass = false;
         Cookie[] cookies = request.getCookies();

File: dinky-admin/src/main/java/org/dinky/service/task/FlinkJarSqlTask.java
Patch:
@@ -25,7 +25,6 @@
 import org.dinky.data.result.SqlExplainResult;
 import org.dinky.job.JobResult;
 
-import java.util.ArrayList;
 import java.util.List;
 
 import com.fasterxml.jackson.databind.node.ObjectNode;
@@ -38,7 +37,7 @@ public FlinkJarSqlTask(TaskDTO task) {
 
     @Override
     public List<SqlExplainResult> explain() {
-        return new ArrayList<>();
+        return jobManager.explainSql(task.getStatement()).getSqlExplainResults();
     }
 
     @Override

File: dinky-admin/src/main/java/org/dinky/configure/cache/PaimonCache.java
Patch:
@@ -21,10 +21,9 @@
 
 import org.dinky.data.constant.PaimonTableConstant;
 import org.dinky.data.paimon.CacheData;
+import org.dinky.shaded.paimon.data.BinaryString;
 import org.dinky.utils.PaimonUtil;
 
-import org.apache.paimon.data.BinaryString;
-
 import java.util.Arrays;
 import java.util.Collections;
 import java.util.List;

File: dinky-admin/src/main/java/org/dinky/utils/PaimonTypeUtil.java
Patch:
@@ -19,8 +19,8 @@
 
 package org.dinky.utils;
 
-import org.apache.paimon.types.DataType;
-import org.apache.paimon.types.DataTypes;
+import org.dinky.shaded.paimon.types.DataType;
+import org.dinky.shaded.paimon.types.DataTypes;
 
 import java.math.BigDecimal;
 import java.time.LocalDateTime;

File: dinky-admin/src/main/java/org/dinky/service/impl/ClusterInstanceServiceImpl.java
Patch:
@@ -231,7 +231,7 @@ public ClusterInstance deploySessionCluster(Integer id) {
                     gatewayResult.getWebURL().replace("http://", ""),
                     gatewayResult.getId(),
                     clusterCfg.getName() + "_" + LocalDateTime.now(),
-                    clusterCfg.getName() + LocalDateTime.now(),
+                    gatewayConfig.getType().getLongValue(),
                     id,
                     null));
         }

File: dinky-core/src/main/java/org/dinky/job/JobConfig.java
Patch:
@@ -225,6 +225,7 @@ public void buildGatewayConfig(FlinkClusterConfig config) {
 
         gatewayConfig = GatewayConfig.build(config);
         gatewayConfig.setTaskId(getTaskId());
+        gatewayConfig.setType(GatewayType.get(getType()));
     }
 
     public void addGatewayConfig(Map<String, String> config) {

File: dinky-client/dinky-client-base/src/main/java/org/dinky/trans/parse/ExecuteJarParseStrategy.java
Patch:
@@ -36,7 +36,7 @@
 import cn.hutool.core.util.StrUtil;
 
 public class ExecuteJarParseStrategy extends AbstractRegexParseStrategy {
-    private static final String PATTERN_STR = "^EXECUTE\\s+JAR\\s+WITH\\s?\\(\\s+(.*\\s+)+\\)";
+    private static final String PATTERN_STR = "^EXECUTE\\s+JAR\\s+WITH\\s*\\(.+\\)";
     private static final Pattern PATTERN = Pattern.compile(PATTERN_STR, Pattern.CASE_INSENSITIVE | Pattern.DOTALL);
     public static final ExecuteJarParseStrategy INSTANCE = new ExecuteJarParseStrategy();
 

File: dinky-admin/src/main/java/org/dinky/aop/exception/WebExceptionHandler.java
Patch:
@@ -48,6 +48,7 @@
 import cn.dev33.satoken.exception.NotLoginException;
 import cn.hutool.core.map.MapUtil;
 import cn.hutool.core.util.StrUtil;
+import lombok.extern.slf4j.Slf4j;
 
 /**
  * WebExceptionHandler
@@ -56,11 +57,13 @@
  */
 @RestControllerAdvice
 @Order(Ordered.HIGHEST_PRECEDENCE)
+@Slf4j
 public class WebExceptionHandler {
 
     @ExceptionHandler
     @ResponseBody
     public Result<Void> busException(BusException e) {
+        log.error("BusException:", e);
         if (StrUtil.isEmpty(e.getMsg())) {
             return Result.failed(I18n.getMessage(e.getCode(), e.getMessage()));
         }

File: dinky-admin/src/main/java/org/dinky/controller/CatalogueController.java
Patch:
@@ -147,7 +147,7 @@ public Result<List<Catalogue>> getCatalogueTree() {
             dataType = "CatalogueTaskDTO",
             dataTypeClass = CatalogueTaskDTO.class)
     public Result<Catalogue> createTask(@RequestBody CatalogueTaskDTO catalogueTaskDTO) {
-        if (catalogueService.checkCatalogueTaskNameIsExist(catalogueTaskDTO.getName())) {
+        if (catalogueService.checkCatalogueTaskNameIsExistById(catalogueTaskDTO.getName(), catalogueTaskDTO.getId())) {
             return Result.failed(Status.TASK_IS_EXIST);
         }
         Catalogue catalogue = catalogueService.saveOrUpdateCatalogueAndTask(catalogueTaskDTO);

File: dinky-admin/src/main/java/org/dinky/service/CatalogueService.java
Patch:
@@ -135,7 +135,8 @@ public interface CatalogueService extends ISuperService<Catalogue> {
     /**
      * Check if the catalogue task name is exist
      * @param name catalogue task name
+     * @param id catalogue task id
      * @return true if the catalogue task name is exist
      */
-    boolean checkCatalogueTaskNameIsExist(String name);
+    boolean checkCatalogueTaskNameIsExistById(String name, Integer id);
 }

File: dinky-client/dinky-client-base/src/main/java/org/dinky/trans/parse/ExecuteJarParseStrategy.java
Patch:
@@ -36,7 +36,7 @@
 import cn.hutool.core.util.StrUtil;
 
 public class ExecuteJarParseStrategy extends AbstractRegexParseStrategy {
-    private static final String PATTERN_STR = "^EXECUTE\\s+JAR\\s+WITH\\s+\\(\\s+(.*\\s+)+\\)";
+    private static final String PATTERN_STR = "^EXECUTE\\s+JAR\\s+WITH\\s?\\(\\s+(.*\\s+)+\\)";
     private static final Pattern PATTERN = Pattern.compile(PATTERN_STR, Pattern.CASE_INSENSITIVE | Pattern.DOTALL);
     public static final ExecuteJarParseStrategy INSTANCE = new ExecuteJarParseStrategy();
 

File: dinky-admin/src/main/java/org/dinky/service/impl/SavepointsServiceImpl.java
Patch:
@@ -30,7 +30,7 @@
 
 import org.springframework.stereotype.Service;
 
-import com.baomidou.mybatisplus.core.conditions.query.QueryWrapper;
+import com.baomidou.mybatisplus.core.conditions.query.LambdaQueryWrapper;
 
 /**
  * SavepointsServiceImpl
@@ -42,7 +42,7 @@ public class SavepointsServiceImpl extends SuperServiceImpl<SavepointsMapper, Sa
 
     @Override
     public List<Savepoints> listSavepointsByTaskId(Integer taskId) {
-        return list(new QueryWrapper<Savepoints>().eq("task_id", taskId));
+        return list(new LambdaQueryWrapper<>(Savepoints.class).eq(Savepoints::getTaskId, taskId));
     }
 
     @Override

File: dinky-admin/src/main/java/org/dinky/service/impl/StudioServiceImpl.java
Patch:
@@ -101,7 +101,7 @@ public SelectResult getJobData(String jobId) {
     public LineageResult getLineage(StudioLineageDTO studioCADTO) {
         // TODO 添加ProcessStep
         if (Asserts.isNotNullString(studioCADTO.getDialect())
-                && !Dialect.FLINK_SQL.equalsVal(studioCADTO.getDialect())) {
+                && !Dialect.FLINK_SQL.isDialect(studioCADTO.getDialect())) {
             if (Asserts.isNull(studioCADTO.getDatabaseId())) {
                 log.error("Job's data source not selected!");
                 return null;
@@ -111,7 +111,7 @@ public LineageResult getLineage(StudioLineageDTO studioCADTO) {
                 log.error("Job's data source does not exist!");
                 return null;
             }
-            if (Dialect.DORIS.equalsVal(studioCADTO.getDialect())) {
+            if (Dialect.DORIS.isDialect(studioCADTO.getDialect())) {
                 return SQLLineageBuilder.getSqlLineage(studioCADTO.getStatement(), "mysql", dataBase.getDriverConfig());
             } else {
                 return SQLLineageBuilder.getSqlLineage(

File: dinky-admin/src/main/java/org/dinky/service/task/BaseTask.java
Patch:
@@ -69,7 +69,7 @@ public static BaseTask getTask(TaskDTO taskDTO) {
             SupportDialect annotation = clazz.getAnnotation(SupportDialect.class);
             if (annotation != null) {
                 for (Dialect dialect : annotation.value()) {
-                    if (dialect.getValue().equalsIgnoreCase(taskDTO.getDialect())) {
+                    if (dialect.isDialect(taskDTO.getDialect())) {
                         return (BaseTask) ReflectUtil.newInstance(clazz, taskDTO);
                     }
                 }

File: dinky-admin/src/main/java/org/dinky/utils/UDFUtils.java
Patch:
@@ -25,7 +25,6 @@
 
 import org.apache.flink.table.catalog.FunctionLanguage;
 
-/** @since 0.6.8 */
 public class UDFUtils extends UDFUtil {
 
     public static UDF taskToUDF(Task task) {

File: dinky-common/src/main/java/org/dinky/config/Dialect.java
Patch:
@@ -58,7 +58,7 @@ public String getValue() {
         return value;
     }
 
-    public boolean equalsVal(String valueText) {
+    public boolean isDialect(String valueText) {
         return Asserts.isEqualsIgnoreCase(value, valueText);
     }
 

File: dinky-admin/src/main/java/org/dinky/service/impl/TaskServiceImpl.java
Patch:
@@ -325,7 +325,7 @@ public JobResult submitTask(TaskSubmitDto submitDto) throws Exception {
         taskDTO.setStatementSet(true);
         JobResult jobResult = taskServiceBean.executeJob(taskDTO);
         if ((jobResult.getStatus() == Job.JobStatus.FAILED)) {
-            throw new BusException(jobResult.getError());
+            throw new RuntimeException(jobResult.getError());
         }
         log.info("Job Submit success");
         Task task = new Task(submitDto.getId(), jobResult.getJobInstanceId());

File: dinky-gateway/src/main/java/org/dinky/gateway/kubernetes/KubernetesSessionGateway.java
Patch:
@@ -24,7 +24,6 @@
 import org.dinky.gateway.enums.GatewayType;
 import org.dinky.gateway.result.GatewayResult;
 import org.dinky.gateway.result.KubernetesResult;
-import org.dinky.utils.LogUtil;
 
 import org.apache.flink.client.deployment.ClusterSpecification;
 import org.apache.flink.client.program.ClusterClient;
@@ -67,7 +66,7 @@ public GatewayResult deployCluster(FlinkUdfPathContextHolder udfPathContextHolde
             result.setWebURL(clusterClient.getWebInterfaceURL());
             result.success();
         } catch (Exception e) {
-            result.fail(LogUtil.getError(e));
+            throw new RuntimeException(e);
         }
         return result;
     }

File: dinky-gateway/src/main/java/org/dinky/gateway/yarn/YarnPerJobGateway.java
Patch:
@@ -24,7 +24,6 @@
 import org.dinky.gateway.enums.GatewayType;
 import org.dinky.gateway.result.GatewayResult;
 import org.dinky.gateway.result.YarnResult;
-import org.dinky.utils.LogUtil;
 
 import org.apache.flink.client.deployment.ClusterSpecification;
 import org.apache.flink.client.program.ClusterClient;
@@ -98,7 +97,7 @@ public GatewayResult submitJobGraph(JobGraph jobGraph) {
             }
             result.success();
         } catch (Exception e) {
-            result.fail(LogUtil.getError(e));
+            throw new RuntimeException(e);
         }
         return result;
     }

File: dinky-gateway/src/main/java/org/dinky/gateway/yarn/YarnSessionGateway.java
Patch:
@@ -24,7 +24,6 @@
 import org.dinky.gateway.enums.GatewayType;
 import org.dinky.gateway.result.GatewayResult;
 import org.dinky.gateway.result.YarnResult;
-import org.dinky.utils.LogUtil;
 
 import org.apache.flink.client.deployment.ClusterSpecification;
 import org.apache.flink.client.program.ClusterClient;
@@ -63,7 +62,7 @@ public GatewayResult deployCluster(FlinkUdfPathContextHolder udfPathContextHolde
             result.setWebURL(clusterClient.getWebInterfaceURL());
             result.success();
         } catch (Exception e) {
-            result.fail(LogUtil.getError(e));
+            throw new RuntimeException(e);
         }
         return result;
     }

File: dinky-admin/src/main/java/org/dinky/job/handler/JobRefreshHandler.java
Patch:
@@ -122,8 +122,8 @@ public static boolean refreshJob(JobInfoDetail jobInfoDetail, boolean needSave)
         } else {
             jobInfoDetail.setJobDataDto(jobDataDto);
             FlinkJobDetailInfo flinkJobDetailInfo = jobDataDto.getJob();
-
-            jobInstance.setStatus(getJobStatus(jobInfoDetail).getValue());
+            //            The YARN running status is no longer monitored
+            jobInstance.setStatus(flinkJobDetailInfo.getState());
             jobInstance.setDuration(flinkJobDetailInfo.getDuration());
             jobInstance.setCreateTime(TimeUtil.toLocalDateTime(flinkJobDetailInfo.getStartTime()));
             // if the job is still running the end-time is -1

File: dinky-admin/src/main/java/org/dinky/configure/Knife4jConfig.java
Patch:
@@ -86,7 +86,7 @@ private ApiInfo apiInfo() {
                 .version(dinkyVersion)
                 .licenseUrl("https://www.apache.org/licenses/LICENSE-2.0")
                 .license("Apache 2.0")
-                .contact(new Contact("Dinky 开源", "http://www.dlink.top/", ""))
+                .contact(new Contact("Dinky 开源", "http://www.dinky.org.cn/", ""))
                 .build();
     }
     /** 增加如下配置可解决Spring Boot 6.x 与Swagger 3.0.0 不兼容问题 */

File: dinky-common/src/main/java/org/dinky/data/flink/checkpoint/CheckPointOverView.java
Patch:
@@ -111,7 +111,7 @@ public Counts(
 
     @Data
     @NoArgsConstructor
-    public final class StatsSummaryDto {
+    public static final class StatsSummaryDto {
 
         public static final String FIELD_NAME_MINIMUM = "min";
 

File: dinky-client/dinky-client-base/src/main/java/org/dinky/trans/dml/ExecuteJarOperation.java
Patch:
@@ -60,7 +60,7 @@ public Optional<? extends TableResult> execute(CustomTableEnvironment tEnv) {
         return Optional.of(TABLE_RESULT_OK);
     }
 
-    protected StreamGraph getStreamGraph(CustomTableEnvironment tEnv) {
+    public StreamGraph getStreamGraph(CustomTableEnvironment tEnv) {
         JarSubmitParam submitParam = JarSubmitParam.build(statement);
         return getStreamGraph(submitParam, tEnv);
     }

File: dinky-app/dinky-app-1.14/src/main/java/org/dinky/app/MainApp.java
Patch:
@@ -22,7 +22,6 @@
 import org.dinky.app.constant.AppParamConstant;
 import org.dinky.app.db.DBUtil;
 import org.dinky.app.flinksql.Submitter;
-import org.dinky.app.util.FlinkAppUtil;
 import org.dinky.data.app.AppParamConfig;
 import org.dinky.utils.JsonUtils;
 
@@ -52,7 +51,5 @@ public static void main(String[] args) throws Exception {
         log.info("dinky app is Ready to run, config is {}", appConfig);
         DBUtil.init(appConfig);
         Submitter.submit(appConfig);
-        log.info("Start Monitor Job");
-        FlinkAppUtil.monitorFlinkTask(Submitter.executor, appConfig.getTaskId());
     }
 }

File: dinky-app/dinky-app-1.15/src/main/java/org/dinky/app/MainApp.java
Patch:
@@ -22,7 +22,6 @@
 import org.dinky.app.constant.AppParamConstant;
 import org.dinky.app.db.DBUtil;
 import org.dinky.app.flinksql.Submitter;
-import org.dinky.app.util.FlinkAppUtil;
 import org.dinky.data.app.AppParamConfig;
 import org.dinky.utils.JsonUtils;
 
@@ -52,7 +51,5 @@ public static void main(String[] args) throws Exception {
         log.info("dinky app is Ready to run, config is {}", appConfig);
         DBUtil.init(appConfig);
         Submitter.submit(appConfig);
-        log.info("Start Monitor Job");
-        FlinkAppUtil.monitorFlinkTask(Submitter.executor, appConfig.getTaskId());
     }
 }

File: dinky-app/dinky-app-1.16/src/main/java/org/dinky/app/MainApp.java
Patch:
@@ -22,7 +22,6 @@
 import org.dinky.app.constant.AppParamConstant;
 import org.dinky.app.db.DBUtil;
 import org.dinky.app.flinksql.Submitter;
-import org.dinky.app.util.FlinkAppUtil;
 import org.dinky.data.app.AppParamConfig;
 import org.dinky.utils.JsonUtils;
 
@@ -52,7 +51,5 @@ public static void main(String[] args) throws Exception {
         log.info("dinky app is Ready to run, config is {}", appConfig);
         DBUtil.init(appConfig);
         Submitter.submit(appConfig);
-        log.info("Start Monitor Job");
-        FlinkAppUtil.monitorFlinkTask(Submitter.executor, appConfig.getTaskId());
     }
 }

File: dinky-app/dinky-app-1.17/src/main/java/org/dinky/app/MainApp.java
Patch:
@@ -22,7 +22,6 @@
 import org.dinky.app.constant.AppParamConstant;
 import org.dinky.app.db.DBUtil;
 import org.dinky.app.flinksql.Submitter;
-import org.dinky.app.util.FlinkAppUtil;
 import org.dinky.data.app.AppParamConfig;
 import org.dinky.utils.JsonUtils;
 
@@ -52,7 +51,5 @@ public static void main(String[] args) throws Exception {
         log.info("dinky app is Ready to run, config is {}", appConfig);
         DBUtil.init(appConfig);
         Submitter.submit(appConfig);
-        log.info("Start Monitor Job");
-        FlinkAppUtil.monitorFlinkTask(Submitter.executor, appConfig.getTaskId());
     }
 }

File: dinky-app/dinky-app-1.18/src/main/java/org/dinky/app/MainApp.java
Patch:
@@ -22,7 +22,6 @@
 import org.dinky.app.constant.AppParamConstant;
 import org.dinky.app.db.DBUtil;
 import org.dinky.app.flinksql.Submitter;
-import org.dinky.app.util.FlinkAppUtil;
 import org.dinky.data.app.AppParamConfig;
 import org.dinky.utils.JsonUtils;
 
@@ -52,7 +51,5 @@ public static void main(String[] args) throws Exception {
         log.info("dinky app is Ready to run, config is {}", appConfig);
         DBUtil.init(appConfig);
         Submitter.submit(appConfig);
-        log.info("Start Monitor Job");
-        FlinkAppUtil.monitorFlinkTask(Submitter.executor, appConfig.getTaskId());
     }
 }

File: dinky-client/dinky-client-base/src/main/java/org/dinky/trans/parse/SetSqlParseStrategy.java
Patch:
@@ -50,8 +50,8 @@ public SetSqlParseStrategy() {
     public static Map<String, List<String>> getInfo(String statement) {
         // SET(\s+(\S+)\s*=(.*))?
         List<SqlSegment> segments = new ArrayList<>();
-        segments.add(new SqlSegment("(set)\\s+(.+)(\\s*=)", "[.]"));
-        segments.add(new SqlSegment("(=)\\s*(.*)($)", ","));
+        segments.add(new SqlSegment("(set)\\s+'?([^']+)'?(\\s*=)", "[.]"));
+        segments.add(new SqlSegment("(=)\\s*'?([^']+)'?($)", ","));
         return SqlSegmentUtil.splitSql2Segment(segments, statement);
     }
 

File: dinky-scheduler/src/main/java/org/dinky/scheduler/client/ProcessClient.java
Patch:
@@ -46,6 +46,7 @@
 import cn.hutool.http.HttpRequest;
 import cn.hutool.http.HttpResponse;
 import cn.hutool.json.JSONObject;
+import cn.hutool.json.JSONUtil;
 
 /**
  * 工作流定义
@@ -88,7 +89,7 @@ public List<ProcessDefinition> getProcessDefinition(Long projectCode, String pro
             ProcessDefinition processDefinition = MyJSONUtil.toBean(jsonObject, ProcessDefinition.class);
             // The locations of processDefinition is json string
             List<DagNodeLocation> locations = jsonObject.getBeanList("locations", DagNodeLocation.class);
-            processDefinition.setLocations(locations);
+            processDefinition.setLocations(JSONUtil.toJsonStr(locations));
             lists.add(processDefinition);
         }
         return lists;

File: dinky-scheduler/src/main/java/org/dinky/scheduler/model/ProcessDefinition.java
Patch:
@@ -23,7 +23,6 @@
 import org.dinky.scheduler.enums.ProcessExecutionTypeEnum;
 import org.dinky.scheduler.enums.ReleaseState;
 
-import java.util.ArrayList;
 import java.util.Date;
 import java.util.List;
 import java.util.Map;
@@ -88,7 +87,7 @@ public class ProcessDefinition {
     private String projectName;
 
     @ApiModelProperty(value = "位置")
-    private List<DagNodeLocation> locations = new ArrayList<>();
+    private String locations;
 
     @ApiModelProperty(value = "计划发布状态 online/offline")
     private ReleaseState scheduleReleaseState;

File: dinky-client/dinky-client-1.14/src/main/java/org/apache/flink/yarn/YarnClusterDescriptor.java
Patch:
@@ -26,6 +26,8 @@
 import static org.apache.flink.util.Preconditions.checkNotNull;
 import static org.apache.flink.yarn.YarnConfigKeys.LOCAL_RESOURCE_DESCRIPTOR_SEPARATOR;
 
+import org.dinky.utils.ClassPathUtils;
+
 import org.apache.flink.annotation.VisibleForTesting;
 import org.apache.flink.api.common.cache.DistributedCache;
 import org.apache.flink.api.java.tuple.Tuple2;
@@ -854,7 +856,7 @@ private ApplicationReport startAppMaster(
         }
 
         // normalize classpath by sorting
-        Collections.sort(systemClassPaths);
+        ClassPathUtils.sort(systemClassPaths);
         Collections.sort(userClassPaths);
 
         // classpath assembler

File: dinky-client/dinky-client-1.16/src/main/java/org/apache/flink/yarn/YarnClusterDescriptor.java
Patch:
@@ -29,6 +29,8 @@
 import static org.apache.flink.yarn.YarnConfigKeys.ENV_FLINK_CLASSPATH;
 import static org.apache.flink.yarn.YarnConfigKeys.LOCAL_RESOURCE_DESCRIPTOR_SEPARATOR;
 
+import org.dinky.utils.ClassPathUtils;
+
 import org.apache.flink.annotation.VisibleForTesting;
 import org.apache.flink.api.common.cache.DistributedCache;
 import org.apache.flink.api.java.tuple.Tuple2;
@@ -894,7 +896,7 @@ private ApplicationReport startAppMaster(
         }
 
         // normalize classpath by sorting
-        Collections.sort(systemClassPaths);
+        ClassPathUtils.sort(systemClassPaths);
         Collections.sort(userClassPaths);
 
         // classpath assembler

File: dinky-client/dinky-client-1.17/src/main/java/org/apache/flink/yarn/YarnClusterDescriptor.java
Patch:
@@ -29,6 +29,8 @@
 import static org.apache.flink.yarn.YarnConfigKeys.ENV_FLINK_CLASSPATH;
 import static org.apache.flink.yarn.YarnConfigKeys.LOCAL_RESOURCE_DESCRIPTOR_SEPARATOR;
 
+import org.dinky.utils.ClassPathUtils;
+
 import org.apache.flink.annotation.VisibleForTesting;
 import org.apache.flink.api.common.cache.DistributedCache;
 import org.apache.flink.api.java.tuple.Tuple2;
@@ -897,7 +899,7 @@ private ApplicationReport startAppMaster(
         }
 
         // normalize classpath by sorting
-        Collections.sort(systemClassPaths);
+        ClassPathUtils.sort(systemClassPaths);
         Collections.sort(userClassPaths);
 
         // classpath assembler

File: dinky-client/dinky-client-1.18/src/main/java/org/apache/flink/yarn/YarnClusterDescriptor.java
Patch:
@@ -29,6 +29,8 @@
 import static org.apache.flink.yarn.YarnConfigKeys.ENV_FLINK_CLASSPATH;
 import static org.apache.flink.yarn.YarnConfigKeys.LOCAL_RESOURCE_DESCRIPTOR_SEPARATOR;
 
+import org.dinky.utils.ClassPathUtils;
+
 import org.apache.flink.annotation.VisibleForTesting;
 import org.apache.flink.api.common.cache.DistributedCache;
 import org.apache.flink.api.java.tuple.Tuple2;
@@ -905,7 +907,7 @@ private ApplicationReport startAppMaster(
         }
 
         // normalize classpath by sorting
-        Collections.sort(systemClassPaths);
+        ClassPathUtils.sort(systemClassPaths);
         Collections.sort(userClassPaths);
 
         // classpath assembler

File: dinky-core/src/main/java/org/dinky/executor/AppBatchExecutor.java
Patch:
@@ -37,7 +37,7 @@ public AppBatchExecutor(ExecutorConfig executorConfig, DinkyClassLoader classLoa
             Configuration configuration = Configuration.fromMap(executorConfig.getConfig());
             this.environment = StreamExecutionEnvironment.getExecutionEnvironment(configuration);
         } else {
-            this.environment = StreamExecutionEnvironment.createLocalEnvironment();
+            this.environment = StreamExecutionEnvironment.getExecutionEnvironment();
         }
         init(classLoader);
     }

File: dinky-admin/src/main/java/org/dinky/service/impl/PrintTableServiceImpl.java
Patch:
@@ -115,6 +115,7 @@ private static DatagramSocket getDatagramSocket(int port) {
             InetAddress host = null;
             try {
                 host = InetAddress.getByName("0.0.0.0");
+                log.info("PrintTableListener:DatagramSocket init success, host: {}, port: {}", host, port);
                 return new DatagramSocket(port, host);
             } catch (SocketException | UnknownHostException e) {
                 log.error(

File: dinky-client/dinky-client-base/src/main/java/org/dinky/constant/CustomerConfigureOptions.java
Patch:
@@ -42,4 +42,7 @@ public class CustomerConfigureOptions {
 
     public static final ConfigOption<String> DINKY_HOST =
             key("dinky.dinkyHost").stringType().noDefaultValue().withDescription("dinky local address");
+
+    public static final ConfigOption<Integer> DINKY_PORT =
+            key("dinky.dinkyPort").intType().defaultValue(7125).withDescription("dinky local port");
 }

File: dinky-core/src/main/java/org/dinky/connector/printnet/sink/PrintNetSinkFunction.java
Patch:
@@ -55,6 +55,7 @@ public PrintNetSinkFunction(
 
         try {
             this.targetAddress = InetAddress.getByName(hostname);
+            log.info("PrintNetSinkFunction target address: {}, port: {}", hostname, port);
         } catch (UnknownHostException e) {
             log.error("Unknown host: {}", hostname);
             throw new RuntimeException(e);
@@ -68,7 +69,6 @@ public void open(Configuration parameters) throws Exception {
             serializer.open(null);
         }
 
-        //        StreamingRuntimeContext context = (StreamingRuntimeContext) getRuntimeContext();
         socket = new DatagramSocket();
     }
 

File: dinky-client/dinky-client-base/src/main/java/org/dinky/parser/SqlType.java
Patch:
@@ -29,7 +29,7 @@
 public enum SqlType {
     SELECT("SELECT", "^SELECT.*"),
 
-    CREATE("CREATE", "^CREATE(?!.*AS SELECT).*$"),
+    CREATE("CREATE", "^CREATE(?!\\s+TABLE.*AS SELECT).*$"),
 
     DROP("DROP", "^DROP.*"),
 

File: dinky-admin/src/main/java/org/dinky/data/model/ext/TaskExtConfig.java
Patch:
@@ -74,7 +74,9 @@ public List<String> getCustomConfigKeys() {
     @JsonIgnore
     public Map<String, String> getCustomConfigMaps() {
         return Asserts.isNotNullCollection(customConfig)
-                ? customConfig.stream().collect(Collectors.toMap(ConfigItem::getKey, ConfigItem::getValue))
+                ? customConfig.stream()
+                        .filter(item -> item.getKey() != null && item.getValue() != null)
+                        .collect(Collectors.toMap(ConfigItem::getKey, ConfigItem::getValue))
                 : new HashMap<>();
     }
 

File: dinky-client/dinky-client-1.18/src/main/java/org/dinky/executor/CustomTableEnvironmentImpl.java
Patch:
@@ -23,6 +23,7 @@
 import org.dinky.data.model.LineageRel;
 import org.dinky.data.result.SqlExplainResult;
 import org.dinky.operations.CustomNewParserImpl;
+import org.dinky.operations.DinkyExecutableOperation;
 import org.dinky.trans.ddl.CustomSetOperation;
 import org.dinky.utils.LineageContext;
 
@@ -108,7 +109,8 @@ public static CustomTableEnvironmentImpl create(
     @Override
     public boolean parseAndLoadConfiguration(String statement, Map<String, Object> setMap) {
         List<Operation> operations = getParser().parse(statement);
-        for (Operation operation : operations) {
+        for (Operation outterOperation : operations) {
+            Operation operation = ((DinkyExecutableOperation) outterOperation).getInnerOperation();
             if (operation instanceof SetOperation) {
                 callSet((SetOperation) operation, getStreamExecutionEnvironment(), setMap);
                 return true;

File: dinky-admin/src/main/java/org/dinky/init/SystemInit.java
Patch:
@@ -26,7 +26,6 @@
 import org.dinky.daemon.pool.ScheduleThreadPool;
 import org.dinky.daemon.task.DaemonTask;
 import org.dinky.daemon.task.DaemonTaskConfig;
-import org.dinky.data.exception.BusException;
 import org.dinky.data.exception.DinkyException;
 import org.dinky.data.model.Configuration;
 import org.dinky.data.model.SystemConfiguration;
@@ -204,7 +203,7 @@ private void aboutDolphinSchedulerInitOperation(Object v) {
                 }
             } catch (Exception e) {
                 log.error("Error in DolphinScheduler: ", e);
-                throw new BusException(
+                log.error(
                         "get or create DolphinScheduler project failed, please check the config of DolphinScheduler!");
             }
         }

File: dinky-core/src/main/java/org/dinky/executor/VariableManager.java
Patch:
@@ -83,7 +83,7 @@ private static void loadExpressionVariableClass() {
                 log.info("load class : {}", fullClassName);
             } catch (ClassNotFoundException e) {
                 log.error(
-                        "The class [{}] that needs to be loaded may not be loaded by dinky or there is no jar file of this class under dinky's lib/plugins. Please check, and try again. {}",
+                        "The class [{}] that needs to be loaded may not be loaded by dinky or there is no jar file of this class under dinky's lib/plugins/extends. Please check, and try again. {}",
                         fullClassName,
                         e.getMessage(),
                         e);

File: dinky-scheduler/src/main/java/org/dinky/scheduler/client/TaskClient.java
Patch:
@@ -138,7 +138,7 @@ public TaskDefinition getTaskDefinition(Long projectCode, Long taskCode) {
                         SystemConfiguration.getInstances()
                                 .getDolphinschedulerToken()
                                 .getValue())
-                .timeout(5000)
+                .timeout(20000)
                 .execute()
                 .body();
 

File: dinky-scheduler/src/main/java/org/dinky/scheduler/model/DagNodeLocation.java
Patch:
@@ -26,6 +26,8 @@
 @Data
 public class DagNodeLocation implements Serializable {
 
+    private static final long serialVersionUID = -5243356147439794746L;
+
     private long taskCode;
     private long x;
     private long y;

File: dinky-scheduler/src/main/java/org/dinky/scheduler/model/TaskDefinition.java
Patch:
@@ -74,6 +74,9 @@ public class TaskDefinition {
     @ApiModelProperty(value = "运行标志 yes 正常/no 禁止执行")
     private Flag flag;
 
+    @ApiModelProperty(value = "Cache run: yes/no ")
+    private Flag isCache;
+
     @ApiModelProperty(value = "优先级")
     private Priority taskPriority;
 

File: dinky-scheduler/src/main/java/org/dinky/scheduler/model/TaskRequest.java
Patch:
@@ -53,6 +53,9 @@ public class TaskRequest {
     @ApiModelProperty(value = "运行标志 yes 正常/no 禁止执行")
     private String flag;
 
+    @ApiModelProperty(value = "Cache run: yes/no")
+    private String isCache;
+
     @ApiModelProperty(value = "任务参数 默认DINKY参数")
     private String taskParams;
 

File: dinky-admin/src/main/java/org/dinky/controller/ClusterInstanceController.java
Patch:
@@ -85,7 +85,9 @@ public class ClusterInstanceController {
             mode = SaMode.OR)
     public Result<Void> saveOrUpdateClusterInstance(@RequestBody ClusterInstanceDTO clusterInstanceDTO)
             throws Exception {
-        clusterInstanceDTO.setAutoRegisters(false);
+        if (clusterInstanceDTO.getAutoRegisters() == null) {
+            clusterInstanceDTO.setAutoRegisters(false);
+        }
         clusterInstanceService.registersCluster(clusterInstanceDTO);
         return Result.succeed(Status.SAVE_SUCCESS);
     }

File: dinky-admin/src/main/java/org/dinky/controller/TaskController.java
Patch:
@@ -106,8 +106,8 @@ public Result<JobResult> debugTask(@RequestBody TaskDTO task) throws Exception {
     @GetMapping("/cancel")
     @Log(title = "Cancel Flink Job", businessType = BusinessType.TRIGGER)
     @ApiOperation("Cancel Flink Job")
-    public Result<Void> cancel(@RequestParam Integer id) {
-        if (taskService.cancelTaskJob(taskService.getTaskInfoById(id))) {
+    public Result<Void> cancel(@RequestParam Integer id, @RequestParam(defaultValue = "false") boolean withSavePoint) {
+        if (taskService.cancelTaskJob(taskService.getTaskInfoById(id), withSavePoint)) {
             return Result.succeed(Status.EXECUTE_SUCCESS);
         } else {
             return Result.failed(Status.EXECUTE_FAILED);

File: dinky-admin/src/main/java/org/dinky/service/TaskService.java
Patch:
@@ -114,7 +114,7 @@ public interface TaskService extends ISuperService<Task> {
      * @param task The {@link TaskDTO} object representing the task to cancel.
      * @return true if the task job is successfully cancelled, false otherwise.
      */
-    boolean cancelTaskJob(TaskDTO task);
+    boolean cancelTaskJob(TaskDTO task, boolean withSavePoint);
 
     /**
      * Get the stream graph of the given task job.

File: dinky-admin/src/main/java/org/dinky/service/impl/TaskServiceImpl.java
Patch:
@@ -367,15 +367,15 @@ public JobResult restartTask(Integer id, String savePointPath) throws Exception
         if (!Dialect.isCommonSql(task.getDialect()) && Asserts.isNotNull(task.getJobInstanceId())) {
             String status = jobInstanceService.getById(task.getJobInstanceId()).getStatus();
             if (!JobStatus.isDone(status)) {
-                cancelTaskJob(task);
+                cancelTaskJob(task, true);
             }
         }
         return submitTask(
                 TaskSubmitDto.builder().id(id).savePointPath(savePointPath).build());
     }
 
     @Override
-    public boolean cancelTaskJob(TaskDTO task) {
+    public boolean cancelTaskJob(TaskDTO task, boolean withSavePoint) {
         if (Dialect.isCommonSql(task.getDialect())) {
             return true;
         }
@@ -385,7 +385,7 @@ public boolean cancelTaskJob(TaskDTO task) {
         Assert.notNull(clusterInstance, Status.CLUSTER_NOT_EXIST.getMessage());
 
         JobManager jobManager = JobManager.build(buildJobConfig(task));
-        return jobManager.cancel(jobInstance.getJid());
+        return jobManager.cancel(jobInstance.getJid(), withSavePoint);
     }
 
     @Override

File: dinky-cdc/dinky-cdc-plus/src/main/java/org/dinky/cdc/doris/DorisSchemaEvolutionSinkBuilder.java
Patch:
@@ -33,6 +33,7 @@
 import org.apache.doris.flink.cfg.DorisReadOptions;
 import org.apache.doris.flink.sink.DorisSink;
 import org.apache.doris.flink.sink.writer.JsonDebeziumSchemaSerializer;
+import org.apache.doris.flink.sink.writer.serializer.DorisRecordSerializer;
 import org.apache.flink.streaming.api.datastream.DataStreamSource;
 import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
@@ -164,7 +165,7 @@ public void processElement(Map map, Context ctx, Collector<String> out) throws E
             builder.setDorisReadOptions(DorisReadOptions.builder().build())
                     .setDorisExecutionOptions(executionBuilder.build())
                     .setDorisOptions(dorisOptions)
-                    .setSerializer(JsonDebeziumSchemaSerializer.builder()
+                    .setSerializer((DorisRecordSerializer<String>) JsonDebeziumSchemaSerializer.builder()
                             .setDorisOptions(dorisOptions)
                             .build());
 

File: dinky-cdc/dinky-cdc-plus/src/main/java/org/dinky/cdc/doris/DorisSinkBuilder.java
Patch:
@@ -30,6 +30,7 @@
 import org.apache.doris.flink.cfg.DorisReadOptions;
 import org.apache.doris.flink.sink.DorisSink;
 import org.apache.doris.flink.sink.writer.RowDataSerializer;
+import org.apache.doris.flink.sink.writer.serializer.DorisRecordSerializer;
 import org.apache.flink.streaming.api.datastream.DataStream;
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
 import org.apache.flink.table.data.RowData;
@@ -184,7 +185,7 @@ public void addSink(
         DorisSink.Builder<RowData> builder = DorisSink.builder();
         builder.setDorisReadOptions(readOptionBuilder.build())
                 .setDorisExecutionOptions(executionBuilder.build())
-                .setSerializer(RowDataSerializer.builder()
+                .setSerializer((DorisRecordSerializer<RowData>) RowDataSerializer.builder()
                         .setFieldNames(columnNames)
                         .setType("json")
                         .enableDelete(true)

File: dinky-client/dinky-client-base/src/main/java/org/dinky/parser/SqlType.java
Patch:
@@ -56,6 +56,7 @@ public enum SqlType {
     RESET("RESET", "^RESET.*"),
 
     EXECUTE("EXECUTE", "^EXECUTE.*"),
+
     ADD_JAR("ADD_JAR", "^ADD\\s+JAR\\s+\\S+"),
     ADD("ADD", "^ADD\\s+CUSTOMJAR\\s+\\S+"),
 

File: dinky-core/src/main/java/org/dinky/job/JobConfig.java
Patch:
@@ -227,12 +227,12 @@ public void buildGatewayConfig(FlinkClusterConfig config) {
         gatewayConfig.setTaskId(getTaskId());
     }
 
-    public void addGatewayConfig(Map<String, Object> config) {
+    public void addGatewayConfig(Map<String, String> config) {
         if (Asserts.isNull(gatewayConfig)) {
             gatewayConfig = new GatewayConfig();
         }
-        for (Map.Entry<String, Object> entry : config.entrySet()) {
-            gatewayConfig.getFlinkConfig().getConfiguration().put(entry.getKey(), (String) entry.getValue());
+        for (Map.Entry<String, String> entry : config.entrySet()) {
+            gatewayConfig.getFlinkConfig().getConfiguration().put(entry.getKey(), entry.getValue());
         }
     }
 

File: dinky-core/src/main/java/org/dinky/utils/KerberosUtil.java
Patch:
@@ -48,7 +48,7 @@ private static void reset() {
         }
     }
 
-    public static void authenticate(Map<String, Object> configuration) {
+    public static void authenticate(Map<String, String> configuration) {
         configuration.forEach((k, v) -> logger.debug("Flink configuration key: [{}], value: [{}]", k, v));
         String krb5ConfPath = (String) configuration.getOrDefault("java.security.krb5.conf", "");
         String keytabPath = (String) configuration.getOrDefault("security.kerberos.login.keytab", "");

File: dinky-admin/src/main/java/org/dinky/service/DataBaseService.java
Patch:
@@ -196,4 +196,6 @@ public interface DataBaseService extends ISuperService<DataBase> {
     JobResult executeCommonSql(SqlDTO sqlDTO);
 
     List<DataBase> selectListByKeyWord(String keyword);
+
+    JobResult StreamExecuteCommonSql(SqlDTO sqlDTO);
 }

File: dinky-metadata/dinky-metadata-base/src/main/java/org/dinky/metadata/driver/Driver.java
Patch:
@@ -37,6 +37,7 @@
 import java.util.Optional;
 import java.util.ServiceLoader;
 import java.util.Set;
+import java.util.stream.Stream;
 
 import cn.hutool.core.text.StrFormatter;
 
@@ -235,4 +236,6 @@ default Set<Table> getSplitTables(List<String> tableRegList, Map<String, String>
     }
 
     List<Map<String, String>> getSplitSchemaList();
+
+    Stream<JdbcSelectResult> StreamExecuteSql(String statement, Integer maxRowNum);
 }

File: dinky-admin/src/main/java/org/dinky/controller/TaskController.java
Patch:
@@ -144,9 +144,9 @@ public Result<SavePointResult> savepoint(@RequestParam Integer taskId, @RequestP
     public Result<Boolean> changeTaskLife(@RequestParam Integer taskId, @RequestParam Integer lifeCycle)
             throws SqlExplainExcepition {
         if (taskService.changeTaskLifeRecyle(taskId, JobLifeCycle.get(lifeCycle))) {
-            return Result.succeed(Status.PUBLISH_SUCCESS);
+            return Result.succeed(lifeCycle == 2 ? Status.PUBLISH_SUCCESS : Status.OFFLINE_SUCCESS);
         } else {
-            return Result.failed(Status.PUBLISH_FAILED);
+            return Result.failed(lifeCycle == 2 ? Status.PUBLISH_FAILED : Status.OFFLINE_FAILED);
         }
     }
 

File: dinky-admin/src/main/java/org/dinky/service/resource/impl/ResourceServiceImpl.java
Patch:
@@ -276,13 +276,11 @@ public boolean remove(Integer id) {
                 () -> new BusException(Status.ROOT_DIR_NOT_ALLOW_DELETE));
         try {
             if (id < 1) {
-                getBaseResourceManager().remove("/");
                 // todo 删除主目录，实际是清空
                 remove(new LambdaQueryWrapper<Resources>().ne(Resources::getId, 0));
             }
             Resources byId = getById(id);
             if (isExistsChildren(id)) {
-                getBaseResourceManager().remove(byId.getFullName());
                 if (byId.getIsDirectory()) {
                     List<Resources> resourceByPidToChildren =
                             getResourceByPidToChildren(new ArrayList<>(), byId.getId());
@@ -291,7 +289,6 @@ public boolean remove(Integer id) {
                 List<Resources> resourceByPidToParent = getResourceByPidToParent(new ArrayList<>(), byId.getPid());
                 resourceByPidToParent.forEach(x -> x.setSize(x.getSize() - byId.getSize()));
                 updateBatchById(resourceByPidToParent);
-                getBaseResourceManager().remove(byId.getFullName());
                 return removeById(id);
             }
             return removeById(id);

File: dinky-common/src/main/java/org/dinky/data/model/ResourcesModelEnum.java
Patch:
@@ -20,6 +20,7 @@
 package org.dinky.data.model;
 
 public enum ResourcesModelEnum {
+    LOCAL,
     HDFS,
     OSS
 }

File: dinky-admin/src/main/java/org/dinky/controller/AlertInstanceController.java
Patch:
@@ -191,10 +191,10 @@ public Result<List<AlertInstance>> listEnabledAll() {
             paramType = "body",
             required = true,
             dataTypeClass = AlertInstanceDTO.class)
-    public Result<Void> sendAlertMsgTest(@RequestBody AlertInstanceDTO alertInstanceDTO) {
+    public Result<String> sendAlertMsgTest(@RequestBody AlertInstanceDTO alertInstanceDTO) {
         AlertResult alertResult = alertInstanceService.testAlert(alertInstanceDTO);
         if (alertResult.getSuccess()) {
-            return Result.succeed(Status.SEND_TEST_SUCCESS);
+            return Result.succeed(alertResult.getMessage(), Status.SEND_TEST_SUCCESS);
         } else {
             return Result.failed(Status.SEND_TEST_FAILED);
         }

File: dinky-alert/dinky-alert-base/src/main/java/org/dinky/alert/AlertBaseConstant.java
Patch:
@@ -39,8 +39,8 @@ public class AlertBaseConstant {
     public static final String AT_USERS = "atUsers";
     public static final String PASSWORD = "password";
 
-    public static final String ALERT_TEMPLATE_TITLE = "Job Alert Test Title";
-    public static final String ALERT_TEMPLATE_MSG = "\n- **Job Name:** <font color='gray'>Test Job</font>\n"
+    public static final String ALERT_TEMPLATE_TITLE = "Dinky Job Alert Test Title";
+    public static final String ALERT_TEMPLATE_MSG = "\n- **Job Name:** <font color='gray'>Dinky Test Job</font>\n"
             + "- **Job Status:** <font color='red'>FAILED</font>\n"
             + "- **Alert Time:** 2023-01-01 12:00:00\n"
             + "- **Start Time:** 2023-01-01 12:00:00\n"

File: dinky-alert/dinky-alert-http/src/main/java/org/dinky/alert/http/HttpConstants.java
Patch:
@@ -19,11 +19,9 @@
 
 package org.dinky.alert.http;
 
-/** DingTalkConstants */
+/** Http Constants */
 public class HttpConstants {
     public static final String TYPE = "Http";
-    public static final String ALERT_TEMPLATE_TITLE = "title";
-    public static final String ALERT_TEMPLATE_CONTENT = "content";
 
     public static final String REQUEST_TYPE_POST = "POST";
     public static final String REQUEST_TYPE_GET = "GET";

File: dinky-admin/src/main/java/org/dinky/context/SseSessionContextHolder.java
Patch:
@@ -159,7 +159,7 @@ public static void sendTopic(String topic, Object content) {
                     SseDataVo data = new SseDataVo(sessionKey, topic, content);
                     sendSse(sessionKey, data);
                 } catch (Exception e) {
-                    log.error("Error sending sse data", e);
+                    log.error("Error sending sse data:{}", e.getMessage());
                     onError(sessionKey, e);
                 }
             }

File: dinky-admin/src/main/java/org/dinky/service/impl/JobInstanceServiceImpl.java
Patch:
@@ -151,9 +151,10 @@ public JobInfoDetail getJobInfoDetail(Integer id) {
 
     @Override
     public JobInfoDetail getJobInfoDetailInfo(JobInstance jobInstance) {
+        Asserts.checkNull(jobInstance, Status.JOB_INSTANCE_NOT_EXIST.getMessage());
+
         JobInfoDetail jobInfoDetail = new JobInfoDetail(jobInstance.getId());
 
-        Asserts.checkNull(jobInstance, Status.JOB_INSTANCE_NOT_EXIST.getMessage());
         jobInfoDetail.setInstance(jobInstance);
 
         ClusterInstance clusterInstance = clusterInstanceService.getById(jobInstance.getClusterId());

File: dinky-admin/src/main/java/org/dinky/data/model/ext/TaskExtConfig.java
Patch:
@@ -20,6 +20,7 @@
 package org.dinky.data.model.ext;
 
 import org.dinky.assertion.Asserts;
+import org.dinky.data.ext.ConfigItem;
 
 import java.io.Serializable;
 import java.util.ArrayList;

File: dinky-alert/dinky-alert-feishu/src/test/java/org/dinky/alert/feishu/FeiShuSenderTest.java
Patch:
@@ -52,7 +52,7 @@ public void testSend() {
         FeiShuAlert feiShuAlert = new FeiShuAlert();
         AlertConfig alertConfig = new AlertConfig();
 
-        alertConfig.setType("FeiShu");
+        alertConfig.setType(FeiShuConstants.TYPE);
         alertConfig.setParam(feiShuConfig);
         feiShuAlert.setConfig(alertConfig);
 

File: dinky-common/src/main/java/org/dinky/data/ext/ConfigItem.java
Patch:
@@ -17,7 +17,7 @@
  *
  */
 
-package org.dinky.data.model.ext;
+package org.dinky.data.ext;
 
 import java.io.Serializable;
 

File: dinky-admin/src/main/java/org/dinky/controller/TaskController.java
Patch:
@@ -22,7 +22,6 @@
 import org.dinky.data.annotations.ExecuteProcess;
 import org.dinky.data.annotations.Log;
 import org.dinky.data.annotations.ProcessId;
-import org.dinky.data.dto.DebugDTO;
 import org.dinky.data.dto.TaskDTO;
 import org.dinky.data.dto.TaskRollbackVersionDTO;
 import org.dinky.data.dto.TaskSaveDTO;
@@ -96,8 +95,8 @@ public Result<JobResult> submitTask(@ProcessId @RequestParam Integer id) throws
             dataType = "DebugDTO",
             paramType = "body")
     @ExecuteProcess(type = ProcessType.FLINK_SUBMIT)
-    public Result<JobResult> debugTask(@RequestBody DebugDTO debugDTO) throws Exception {
-        JobResult result = taskService.debugTask(debugDTO);
+    public Result<JobResult> debugTask(@RequestBody TaskDTO task) throws Exception {
+        JobResult result = taskService.debugTask(task);
         if (result.isSuccess()) {
             return Result.succeed(result, Status.DEBUG_SUCCESS);
         }

File: dinky-admin/src/main/java/org/dinky/service/TaskService.java
Patch:
@@ -20,7 +20,6 @@
 package org.dinky.service;
 
 import org.dinky.data.dto.AbstractStatementDTO;
-import org.dinky.data.dto.DebugDTO;
 import org.dinky.data.dto.TaskDTO;
 import org.dinky.data.dto.TaskRollbackVersionDTO;
 import org.dinky.data.dto.TaskSubmitDto;
@@ -75,11 +74,11 @@ public interface TaskService extends ISuperService<Task> {
     /**
      * Debug the given task and return the job result.
      *
-     * @param debugDTO The param of preview task.
+     * @param task The param of preview task.
      * @return A {@link JobResult} object representing the result of the submitted task.
      * @throws ExcuteException If there is an error debugging the task.
      */
-    JobResult debugTask(DebugDTO debugDTO) throws Exception;
+    JobResult debugTask(TaskDTO task) throws Exception;
 
     /**
      * Restart the given task and return the job result.

File: dinky-client/dinky-client-1.14/src/main/java/org/dinky/executor/AbstractCustomTableEnvironment.java
Patch:
@@ -47,6 +47,7 @@ public TableEnvironment getTableEnvironment() {
         return streamTableEnvironment;
     }
 
+    @Override
     public StreamExecutionEnvironment getStreamExecutionEnvironment() {
         return ((StreamTableEnvironmentImpl) streamTableEnvironment).execEnv();
     }
@@ -56,9 +57,11 @@ public ClassLoader getUserClassLoader() {
         return userClassLoader;
     }
 
+    @Override
     public Planner getPlanner() {
         return ((StreamTableEnvironmentImpl) streamTableEnvironment).getPlanner();
     }
 
+    @Override
     public abstract <T> void addConfiguration(ConfigOption<T> option, T value);
 }

File: dinky-admin/src/main/java/org/dinky/aop/exception/UnKnownExceptionHandler.java
Patch:
@@ -24,6 +24,7 @@
 import org.springframework.core.annotation.Order;
 import org.springframework.web.bind.annotation.ControllerAdvice;
 import org.springframework.web.bind.annotation.ExceptionHandler;
+import org.springframework.web.bind.annotation.ResponseBody;
 
 import lombok.extern.slf4j.Slf4j;
 
@@ -33,7 +34,8 @@
 public class UnKnownExceptionHandler {
 
     @ExceptionHandler
-    public Result<Exception> unknownException(Exception e) {
+    @ResponseBody
+    public Result<String> unknownException(Exception e) {
         log.error(e.getMessage(), e);
         return Result.exception(e.getMessage(), e);
     }

File: dinky-admin/src/main/java/org/dinky/data/result/Result.java
Patch:
@@ -21,6 +21,7 @@
 
 import org.dinky.data.enums.CodeEnum;
 import org.dinky.data.enums.Status;
+import org.dinky.utils.LogUtil;
 
 import java.io.Serializable;
 import java.text.MessageFormat;
@@ -206,8 +207,8 @@ public static <T> Result<T> authorizeFailed(String msg) {
         return of(null, CodeEnum.AUTHORIZE_ERROR.getCode(), msg);
     }
 
-    public static Result<Exception> exception(String msg, Exception e) {
-        return of(e, CodeEnum.EXCEPTION.getCode(), msg);
+    public static Result<String> exception(String msg, Exception e) {
+        return of(LogUtil.getError(e), CodeEnum.EXCEPTION.getCode(), msg);
     }
 
     public static <T> Result<T> paramsError(Status status, Object... args) {

File: dinky-admin/src/main/java/org/dinky/aop/exception/UnKnownExceptionHandler.java
Patch:
@@ -34,6 +34,7 @@ public class UnKnownExceptionHandler {
 
     @ExceptionHandler
     public Result<Exception> unknownException(Exception e) {
+        log.error(e.getMessage(), e);
         return Result.exception(e.getMessage(), e);
     }
 }

File: dinky-admin/src/main/java/org/dinky/service/impl/UserServiceImpl.java
Patch:
@@ -177,19 +177,19 @@ public Result<UserDTO> loginUser(LoginDTO loginDTO) {
             user = loginDTO.isLdapLogin() ? ldapLogin(loginDTO) : localLogin(loginDTO);
         } catch (AuthException e) {
             // Handle authentication exceptions and return the corresponding error status
-            return Result.failed(e.getStatus() + e.getMessage());
+            return Result.authorizeFailed(e.getStatus() + e.getMessage());
         }
 
         // Check if the user is enabled
         if (!user.getEnabled()) {
             loginLogService.saveLoginLog(user, Status.USER_DISABLED_BY_ADMIN);
-            return Result.failed(Status.USER_DISABLED_BY_ADMIN);
+            return Result.authorizeFailed(Status.USER_DISABLED_BY_ADMIN);
         }
 
         UserDTO userInfo = refreshUserInfo(user);
         if (Asserts.isNullCollection(userInfo.getTenantList())) {
             loginLogService.saveLoginLog(user, Status.USER_NOT_BINDING_TENANT);
-            return Result.failed(Status.USER_NOT_BINDING_TENANT);
+            return Result.authorizeFailed(Status.USER_NOT_BINDING_TENANT);
         }
 
         // Perform login using StpUtil (Assuming it handles the session management)

File: dinky-common/src/main/java/org/dinky/data/enums/CodeEnum.java
Patch:
@@ -29,7 +29,8 @@ public enum CodeEnum {
     ERROR(1),
 
     EXCEPTION(5),
-    NOTLOGIN(401);
+    PARAMS_ERROR(6),
+    AUTHORIZE_ERROR(7);
 
     private Integer code;
 

File: dinky-admin/src/main/java/org/dinky/service/impl/ClusterInstanceServiceImpl.java
Patch:
@@ -36,7 +36,7 @@
 import org.dinky.mybatis.service.impl.SuperServiceImpl;
 import org.dinky.service.ClusterConfigurationService;
 import org.dinky.service.ClusterInstanceService;
-import org.dinky.utils.IpUtils;
+import org.dinky.utils.IpUtil;
 import org.dinky.utils.URLUtils;
 
 import java.time.LocalDateTime;
@@ -100,7 +100,7 @@ public String buildEnvironmentAddress(JobConfig config) {
                 port = Integer.valueOf(flinkConfig.get("rest.port"));
             } else {
                 port = URLUtils.getRandomPort();
-                while (!IpUtils.isPortAvailable(port)) {
+                while (!IpUtil.isPortAvailable(port)) {
                     port = URLUtils.getRandomPort();
                 }
             }

File: dinky-core/src/main/java/org/dinky/explainer/Explainer.java
Patch:
@@ -41,6 +41,7 @@
 import org.dinky.trans.Operations;
 import org.dinky.trans.parse.AddJarSqlParseStrategy;
 import org.dinky.utils.DinkyClassLoaderUtil;
+import org.dinky.utils.IpUtil;
 import org.dinky.utils.LogUtil;
 import org.dinky.utils.SqlUtil;
 import org.dinky.utils.URLUtils;
@@ -152,7 +153,7 @@ public JobParam pretreatStatements(String[] statements) {
                 PrintStatementExplainer printStatementExplainer = new PrintStatementExplainer(statement);
 
                 Map<String, String> config = this.executor.getExecutorConfig().getConfig();
-                String host = config.getOrDefault("dinky.dinkyHost", "127.0.0.1");
+                String host = config.getOrDefault("dinky.dinkyHost", IpUtil.getHostIp());
                 int port = Integer.parseInt(config.getOrDefault("dinky.dinkyPrintPort", "7125"));
                 String[] tableNames = printStatementExplainer.getTableNames();
                 for (String tableName : tableNames) {

File: dinky-common/src/main/java/org/dinky/data/enums/Status.java
Patch:
@@ -124,6 +124,7 @@ public enum Status {
     KICK_OUT(10024, "kick.out"),
     TOKEN_FREEZED(10025, "token.freezed"),
     NO_PREFIX(10026, "no.prefix"),
+    USER_SUPERADMIN_CANNOT_DELETE(10027, "user.superadmin.cannot.delete"),
 
     // role
     ROLE_ALREADY_EXISTS(10101, "role.already.exists"),

File: dinky-admin/src/main/java/org/dinky/service/impl/ClusterInstanceServiceImpl.java
Patch:
@@ -201,6 +201,7 @@ public ClusterInstance deploySessionCluster(Integer id) {
         }
         GatewayConfig gatewayConfig =
                 GatewayConfig.build(FlinkClusterConfig.create(clusterCfg.getType(), clusterCfg.getConfigJson()));
+        gatewayConfig.setType(gatewayConfig.getType().getSessionType());
         GatewayResult gatewayResult = JobManager.deploySessionCluster(gatewayConfig);
         return registersCluster(ClusterInstanceDTO.autoRegistersClusterDTO(
                 gatewayResult.getWebURL().replace("http://", ""),

File: dinky-core/src/main/java/org/dinky/job/JobManager.java
Patch:
@@ -240,6 +240,7 @@ public ObjectNode getJarStreamGraphJson(String statement) {
     @ProcessStep(type = ProcessStepType.SUBMIT_EXECUTE)
     public JobResult executeJarSql(String statement) throws Exception {
         job = Job.build(runMode, config, executorConfig, executor, statement, useGateway);
+        ready();
         StreamGraph streamGraph =
                 JobJarStreamGraphBuilder.build(this).getJarStreamGraph(statement, getDinkyClassLoader());
         try {

File: dinky-alert/dinky-alert-email/src/main/java/org/dinky/alert/email/EmailSender.java
Patch:
@@ -78,8 +78,6 @@ public AlertResult send(String title, String content) {
             return alertResult;
         }
 
-        Thread.currentThread().setContextClassLoader(getClass().getClassLoader());
-
         try {
             String sendResult = MailUtil.send(
                     getMailAccount(),

File: dinky-client/dinky-client-1.14/src/main/java/org/dinky/executor/CustomTableEnvironmentImpl.java
Patch:
@@ -119,6 +119,7 @@ public CustomTableEnvironmentImpl(
                 executor,
                 isStreamingMode,
                 userClassLoader));
+        Thread.currentThread().setContextClassLoader(userClassLoader);
         this.executor = executor;
         injectParser(new CustomParserImpl(getPlanner().getParser()));
         injectExtendedExecutor(new CustomExtendedOperationExecutorImpl(this));

File: dinky-client/dinky-client-1.15/src/main/java/org/dinky/executor/CustomTableEnvironmentImpl.java
Patch:
@@ -108,6 +108,7 @@ public CustomTableEnvironmentImpl(
                 executor,
                 isStreamingMode,
                 userClassLoader));
+        Thread.currentThread().setContextClassLoader(userClassLoader);
         injectParser(new CustomParserImpl(getPlanner().getParser()));
         injectExtendedExecutor(new CustomExtendedOperationExecutorImpl(this));
     }

File: dinky-common/src/main/java/org/dinky/data/enums/Status.java
Patch:
@@ -367,6 +367,8 @@ public enum Status {
     SYS_RESOURCE_SETTINGS_HDFS_ROOT_USER_NOTE(173, "sys.resource.settings.hdfs.root.user.note"),
     SYS_RESOURCE_SETTINGS_HDFS_FS_DEFAULTFS(174, "sys.resource.settings.hdfs.fs.defaultFS"),
     SYS_RESOURCE_SETTINGS_HDFS_FS_DEFAULTFS_NOTE(175, "sys.resource.settings.hdfs.fs.defaultFS.note"),
+    SYS_RESOURCE_SETTINGS_PATH_STYLE_ACCESS(176, "sys.resource.settings.oss.path.style.access"),
+    SYS_RESOURCE_SETTINGS_PATH_STYLE_ACCESS_NOTE(177, "sys.resource.settings.oss.path.style.access.note"),
 
     /**
      * gateway config

File: dinky-client/dinky-client-1.14/src/main/java/org/dinky/executor/AbstractCustomTableEnvironment.java
Patch:
@@ -19,6 +19,7 @@
 
 package org.dinky.executor;
 
+import org.apache.flink.configuration.ConfigOption;
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
 import org.apache.flink.table.api.TableEnvironment;
 import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;
@@ -58,4 +59,6 @@ public ClassLoader getUserClassLoader() {
     public Planner getPlanner() {
         return ((StreamTableEnvironmentImpl) streamTableEnvironment).getPlanner();
     }
+
+    public abstract <T> void addConfiguration(ConfigOption<T> option, T value);
 }

File: dinky-client/dinky-client-1.15/src/main/java/org/dinky/executor/AbstractCustomTableEnvironment.java
Patch:
@@ -19,6 +19,7 @@
 
 package org.dinky.executor;
 
+import org.apache.flink.configuration.ConfigOption;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
 import org.apache.flink.table.api.TableEnvironment;
@@ -61,4 +62,6 @@ public ClassLoader getUserClassLoader() {
     public Configuration getRootConfiguration() {
         return (Configuration) this.getConfig().getRootConfiguration();
     }
+
+    public abstract <T> void addConfiguration(ConfigOption<T> option, T value);
 }

File: dinky-core/src/main/java/org/dinky/executor/AppBatchExecutor.java
Patch:
@@ -44,6 +44,6 @@ public AppBatchExecutor(ExecutorConfig executorConfig, DinkyClassLoader classLoa
 
     @Override
     CustomTableEnvironment createCustomTableEnvironment(ClassLoader classLoader) {
-        return CustomTableEnvironmentImpl.createBatch(environment);
+        return CustomTableEnvironmentImpl.createBatch(environment, classLoader);
     }
 }

File: dinky-core/src/main/java/org/dinky/executor/LocalBatchExecutor.java
Patch:
@@ -63,6 +63,6 @@ public LocalBatchExecutor(ExecutorConfig executorConfig, DinkyClassLoader classL
 
     @Override
     CustomTableEnvironment createCustomTableEnvironment(ClassLoader classLoader) {
-        return CustomTableEnvironmentImpl.createBatch(environment);
+        return CustomTableEnvironmentImpl.createBatch(environment, classLoader);
     }
 }

File: dinky-core/src/main/java/org/dinky/executor/RemoteBatchExecutor.java
Patch:
@@ -46,6 +46,6 @@ public RemoteBatchExecutor(ExecutorConfig executorConfig, DinkyClassLoader class
 
     @Override
     CustomTableEnvironment createCustomTableEnvironment(ClassLoader classLoader) {
-        return CustomTableEnvironmentImpl.createBatch(environment);
+        return CustomTableEnvironmentImpl.createBatch(environment, classLoader);
     }
 }

File: dinky-gateway/src/main/java/org/dinky/gateway/config/K8sConfig.java
Patch:
@@ -64,4 +64,7 @@ public class K8sConfig {
             example = "tm-pod-template.yaml",
             notes = "YAML file containing the pod template for TaskManagers in Flink jobs")
     private String tmPodTemplate;
+
+    @ApiModelProperty(value = "KubeConfig", dataType = "String", example = "kubeconfig.yaml", notes = "KubeConfig file")
+    private String kubeConfig;
 }

File: dinky-gateway/src/main/java/org/dinky/gateway/kubernetes/KubernetesApplicationGateway.java
Patch:
@@ -91,8 +91,7 @@ public GatewayResult submitJar(FlinkUdfPathContextHolder udfPathContextHolder) {
         } catch (Exception ex) {
             throw new RuntimeException(ex);
         } finally {
-            client.close();
-            kubernetesClient.close();
+            close();
         }
     }
 

File: dinky-gateway/src/main/java/org/dinky/gateway/yarn/YarnGateway.java
Patch:
@@ -34,6 +34,7 @@
 
 import org.apache.flink.client.deployment.ClusterRetrieveException;
 import org.apache.flink.client.program.ClusterClient;
+import org.apache.flink.configuration.CoreOptions;
 import org.apache.flink.configuration.DeploymentOptions;
 import org.apache.flink.configuration.GlobalConfiguration;
 import org.apache.flink.configuration.SecurityOptions;
@@ -89,6 +90,7 @@ public void init() {
     private void initConfig() {
         final ClusterConfig clusterConfig = config.getClusterConfig();
         configuration = GlobalConfiguration.loadConfiguration(clusterConfig.getFlinkConfigPath());
+        configuration.set(CoreOptions.CLASSLOADER_RESOLVE_ORDER, "parent-first");
 
         final FlinkConfig flinkConfig = config.getFlinkConfig();
         if (Asserts.isNotNull(flinkConfig.getConfiguration())) {

File: dinky-admin/src/main/java/org/dinky/context/SseSessionContextHolder.java
Patch:
@@ -173,7 +173,7 @@ public static void sendTopic(String topic, Object content) {
      * @param content    The SSE data to send.
      * @throws IOException If an I/O error occurs while sending the data.
      */
-    public static void sendSse(String sessionKey, SseDataVo content) throws IOException {
+    public static void sendSse(String sessionKey, SseDataVo content) throws Exception {
         if (exists(sessionKey)) {
             sessionMap.get(sessionKey).getEmitter().send(content);
         } else {

File: dinky-admin/src/main/java/org/dinky/service/impl/PrintTableServiceImpl.java
Patch:
@@ -114,7 +114,7 @@ public void start() {
         private static DatagramSocket getDatagramSocket(int port) {
             InetAddress host = null;
             try {
-                host = InetAddress.getLocalHost();
+                host = InetAddress.getByName("0.0.0.0");
                 return new DatagramSocket(port, host);
             } catch (SocketException | UnknownHostException e) {
                 log.error(

File: dinky-admin/src/main/java/org/dinky/service/impl/CatalogueServiceImpl.java
Patch:
@@ -291,6 +291,7 @@ public boolean copyTask(Catalogue catalogue) {
         Task newTask = new Task();
         BeanUtil.copyProperties(oldTask, newTask);
         newTask.setId(null);
+        newTask.setJobInstanceId(null);
         newTask.setType(oldTask.getType());
         // 设置复制后的作业名称为：原名称+自增序列
         size = size + 1;

File: dinky-client/dinky-client-base/src/main/java/org/dinky/executor/CustomTableEnvironment.java
Patch:
@@ -33,7 +33,6 @@
 import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;
 import org.apache.flink.table.api.internal.TableEnvironmentInternal;
 import org.apache.flink.table.delegation.Planner;
-import org.apache.flink.table.operations.Operation;
 import org.apache.flink.types.Row;
 
 import java.io.File;
@@ -83,8 +82,6 @@ default List<LineageRel> getLineage(String statement) {
 
     <T> void createTemporaryView(String s, DataStream<Row> dataStream, List<String> columnNameList);
 
-    void executeCTAS(Operation operation);
-
     default void addJar(File... jarPath) {
         Configuration configuration = this.getRootConfiguration();
         List<String> pathList =

File: dinky-app/dinky-app-1.14/src/main/java/org/dinky/app/MainApp.java
Patch:
@@ -57,7 +57,7 @@ public static void main(String[] args) throws Exception {
             log.error("exectue app failed : ", e);
         } finally {
             log.info("Start Monitor Job");
-            FlinkAppUtil.monitorFlinkTask(appConfig.getTaskId());
+            FlinkAppUtil.monitorFlinkTask(Submitter.executor, appConfig.getTaskId());
         }
     }
 }

File: dinky-app/dinky-app-1.15/src/main/java/org/dinky/app/MainApp.java
Patch:
@@ -57,7 +57,7 @@ public static void main(String[] args) throws Exception {
             log.error("exectue app failed : ", e);
         } finally {
             log.info("Start Monitor Job");
-            FlinkAppUtil.monitorFlinkTask(appConfig.getTaskId());
+            FlinkAppUtil.monitorFlinkTask(Submitter.executor, appConfig.getTaskId());
         }
     }
 }

File: dinky-app/dinky-app-1.16/src/main/java/org/dinky/app/MainApp.java
Patch:
@@ -57,7 +57,7 @@ public static void main(String[] args) throws Exception {
             log.error("exectue app failed : ", e);
         } finally {
             log.info("Start Monitor Job");
-            FlinkAppUtil.monitorFlinkTask(appConfig.getTaskId());
+            FlinkAppUtil.monitorFlinkTask(Submitter.executor, appConfig.getTaskId());
         }
     }
 }

File: dinky-app/dinky-app-1.17/src/main/java/org/dinky/app/MainApp.java
Patch:
@@ -57,7 +57,7 @@ public static void main(String[] args) throws Exception {
             log.error("exectue app failed : ", e);
         } finally {
             log.info("Start Monitor Job");
-            FlinkAppUtil.monitorFlinkTask(appConfig.getTaskId());
+            FlinkAppUtil.monitorFlinkTask(Submitter.executor, appConfig.getTaskId());
         }
     }
 }

File: dinky-app/dinky-app-1.18/src/main/java/org/dinky/app/MainApp.java
Patch:
@@ -57,7 +57,7 @@ public static void main(String[] args) throws Exception {
             log.error("exectue app failed : ", e);
         } finally {
             log.info("Start Monitor Job");
-            FlinkAppUtil.monitorFlinkTask(appConfig.getTaskId());
+            FlinkAppUtil.monitorFlinkTask(Submitter.executor, appConfig.getTaskId());
         }
     }
 }

File: dinky-app/dinky-app-base/src/main/java/org/dinky/app/flinksql/Submitter.java
Patch:
@@ -92,6 +92,7 @@
  */
 public class Submitter {
     private static final Logger log = LoggerFactory.getLogger(Submitter.class);
+    public static Executor executor = null;
 
     private static void initSystemConfiguration() throws SQLException {
         SystemConfiguration systemConfiguration = SystemConfiguration.getInstances();
@@ -156,7 +157,7 @@ public static void submit(AppParamConfig config) throws SQLException {
                 // .config(JsonUtils.toMap(appTask.getConfigJson()))
                 .build();
 
-        Executor executor = ExecutorFactory.buildAppStreamExecutor(
+        executor = ExecutorFactory.buildAppStreamExecutor(
                 executorConfig, new WeakReference<>(DinkyClassLoader.build()).get());
 
         // 加载第三方jar //TODO 这里有问题，需要修一修

File: dinky-client/dinky-client-1.16/src/test/java/org/dinky/utils/LineageContextTest.java
Patch:
@@ -121,7 +121,7 @@ private void analyzeLineage(String sql, String[][] expectedArray) {
     }
 
     private void analyzeFunction(String sql, String[] expectedArray) {
-        Set<FunctionResult> actualSet = context.analyzeFunction(sql);
+        Set<FunctionResult> actualSet = context.analyzeFunction(tableEnv, sql);
         Set<FunctionResult> expectedSet = FunctionResult.build(CATALOG_NAME, DEFAULT_DATABASE, expectedArray);
         assertEquals(expectedSet, actualSet);
     }

File: dinky-admin/src/main/java/org/dinky/job/handler/JobRefreshHandler.java
Patch:
@@ -117,7 +117,7 @@ public static boolean refreshJob(JobInfoDetail jobInfoDetail, boolean needSave)
             jobInstance.setError(jobDataDto.getErrorMsg());
             jobInfoDetail.getJobDataDto().setError(true);
             jobInfoDetail.getJobDataDto().setErrorMsg(jobDataDto.getErrorMsg());
-            if (TimeUtil.localDateTimeToLong(jobInstance.getFinishTime()) < 1) {
+            if (jobInstance.getFinishTime() == null || TimeUtil.localDateTimeToLong(jobInstance.getFinishTime()) < 1) {
                 jobInstance.setFinishTime(LocalDateTime.now());
             }
         } else {

File: dinky-core/src/main/java/org/dinky/job/builder/JobUDFBuilder.java
Patch:
@@ -29,7 +29,6 @@
 import org.dinky.function.util.UDFUtil;
 import org.dinky.job.JobBuilder;
 import org.dinky.job.JobManager;
-import org.dinky.utils.LogUtil;
 import org.dinky.utils.URLUtils;
 
 import java.io.File;
@@ -118,8 +117,7 @@ public void run() throws Exception {
 
             UDFUtil.addConfigurationClsAndJars(jarList, CollUtil.newArrayList(URLUtils.getURLs(otherPluginsFiles)));
         } catch (Exception e) {
-            log.error("add configuration failed;reason:{}", LogUtil.getError(e));
-            throw new RuntimeException(e);
+            throw new RuntimeException("add configuration failed: ", e);
         }
 
         log.info(StrUtil.format("A total of {} UDF have been Init.", udfList.size() + pyUdfFile.size()));

File: dinky-admin/src/main/java/org/dinky/service/impl/CatalogueServiceImpl.java
Patch:
@@ -294,7 +294,7 @@ public boolean copyTask(Catalogue catalogue) {
         newTask.setType(oldTask.getType());
         // 设置复制后的作业名称为：原名称+自增序列
         size = size + 1;
-        newTask.setName(oldTask.getName() + "_" + size);
+        newTask.setName(oldTask.getName() + "-" + size);
         newTask.setStep(JobLifeCycle.DEVELOP.getValue());
         taskService.save(newTask);
 

File: dinky-core/src/main/java/org/dinky/explainer/lineage/LineageBuilder.java
Patch:
@@ -22,6 +22,7 @@
 import org.dinky.data.model.LineageRel;
 import org.dinky.executor.ExecutorFactory;
 import org.dinky.explainer.Explainer;
+import org.dinky.job.JobManager;
 
 import java.util.ArrayList;
 import java.util.HashMap;
@@ -36,7 +37,7 @@
 public class LineageBuilder {
 
     public static LineageResult getColumnLineageByLogicalPlan(String statement) {
-        Explainer explainer = new Explainer(ExecutorFactory.getDefaultExecutor(), false);
+        Explainer explainer = new Explainer(ExecutorFactory.getDefaultExecutor(), false, new JobManager());
         List<LineageRel> lineageRelList = explainer.getLineage(statement);
         List<LineageRelation> relations = new ArrayList<>();
         Map<String, LineageTable> tableMap = new HashMap<>();

File: dinky-core/src/main/java/org/dinky/explainer/Explainer.java
Patch:
@@ -20,7 +20,6 @@
 package org.dinky.explainer;
 
 import org.dinky.assertion.Asserts;
-import org.dinky.classloader.DinkyClassLoader;
 import org.dinky.constant.FlinkSQLConstant;
 import org.dinky.data.model.LineageRel;
 import org.dinky.data.model.SystemConfiguration;
@@ -127,7 +126,7 @@ public JobParam pretreatStatements(String[] statements) {
             if (operationType.equals(SqlType.ADD)) {
                 AddJarSqlParseStrategy.getAllFilePath(statement)
                         .forEach(t -> jobManager.getUdfPathContextHolder().addOtherPlugins(t));
-                ((DinkyClassLoader) executor.getCustomTableEnvironment().getUserClassLoader())
+                (executor.getDinkyClassLoader())
                         .addURLs(URLUtils.getURLs(
                                 jobManager.getUdfPathContextHolder().getOtherPluginsFiles()));
             } else if (operationType.equals(SqlType.ADD_JAR)) {

File: dinky-admin/src/main/java/org/dinky/job/handler/JobMetricsHandler.java
Patch:
@@ -54,8 +54,8 @@ public class JobMetricsHandler {
      * Send to MetricsContextHolder asynchronously at the end of the method.  </br>
      * Thus, the operation of writing the Flink indicator is completed. </br>
      */
-    public static void writeFlinkMetrics(JobInfoDetail jobInfoDetail) {
-        Map<String, Map<String, String>> customMetricsList = jobInfoDetail.getCustomMetricsMap();
+    public static void refeshAndWriteFlinkMetrics(
+            JobInfoDetail jobInfoDetail, Map<String, Map<String, String>> customMetricsList) {
         String[] jobManagerUrls =
                 jobInfoDetail.getClusterInstance().getJobManagerHost().split(",");
         String jobId = jobInfoDetail.getInstance().getJid();

File: dinky-admin/src/main/java/org/dinky/service/MonitorService.java
Patch:
@@ -19,13 +19,13 @@
 
 package org.dinky.service;
 
+import org.dinky.data.MetricsLayoutVo;
 import org.dinky.data.dto.MetricsLayoutDTO;
 import org.dinky.data.model.Metrics;
 import org.dinky.data.vo.MetricsVO;
 
 import java.util.Date;
 import java.util.List;
-import java.util.Map;
 
 import org.springframework.transaction.annotation.Transactional;
 import org.springframework.web.servlet.mvc.method.annotation.SseEmitter;
@@ -65,7 +65,7 @@ public interface MonitorService extends IService<Metrics> {
      *
      * @return A map where the keys are layout names and the values are lists of {@link Metrics} objects representing the metrics in each layout.
      */
-    Map<String, List<Metrics>> getMetricsLayout();
+    List<MetricsLayoutVo> getMetricsLayout();
 
     /**
      * Get the metrics layout by name.
@@ -81,5 +81,5 @@ public interface MonitorService extends IService<Metrics> {
      * @param taskId The ID of the task to get the job metrics for.
      * @return A list of {@link Metrics} objects representing the job metrics for the specified task ID.
      */
-    List<Metrics> getJobMetrics(Integer taskId);
+    List<Metrics> getMetricsLayoutByTaskId(Integer taskId);
 }

File: dinky-admin/src/main/java/org/dinky/service/impl/TaskServiceImpl.java
Patch:
@@ -240,7 +240,7 @@ public JobConfig buildJobConfig(TaskDTO task) {
         try {
             config.setAddress(clusterInstanceService.buildEnvironmentAddress(config));
         } catch (Exception e) {
-            log.error("Init remote cluster error", e);
+            log.error("Init remote cluster error:{}", e.getMessage());
         }
         return config;
     }

File: dinky-client/dinky-client-base/src/main/java/org/dinky/trans/parse/SetSqlParseStrategy.java
Patch:
@@ -20,7 +20,7 @@
 package org.dinky.trans.parse;
 
 import org.dinky.parser.SqlSegment;
-import org.dinky.trans.ddl.SetOperation;
+import org.dinky.trans.ddl.CustomSetOperation;
 import org.dinky.utils.SqlSegmentUtil;
 
 import org.apache.flink.table.operations.Operation;
@@ -51,13 +51,13 @@ public static Map<String, List<String>> getInfo(String statement) {
         // SET(\s+(\S+)\s*=(.*))?
         List<SqlSegment> segments = new ArrayList<>();
         segments.add(new SqlSegment("(set)\\s+(.+)(\\s*=)", "[.]"));
-        segments.add(new SqlSegment("(=)\\s*(.*)( ENDOFSQL)", ","));
+        segments.add(new SqlSegment("(=)\\s*(.*)($)", ","));
         return SqlSegmentUtil.splitSql2Segment(segments, statement);
     }
 
     @Override
     public Operation convert(String statement) {
-        return new SetOperation(statement);
+        return new CustomSetOperation(statement);
     }
 
     @Override

File: dinky-client/dinky-client-1.14/src/main/java/org/dinky/executor/CustomTableEnvironmentImpl.java
Patch:
@@ -121,7 +121,8 @@ public CustomTableEnvironmentImpl(
         this.executor = executor;
     }
 
-    public static CustomTableEnvironmentImpl create(StreamExecutionEnvironment executionEnvironment) {
+    public static CustomTableEnvironmentImpl create(
+            StreamExecutionEnvironment executionEnvironment, ClassLoader classLoader) {
         return create(executionEnvironment, EnvironmentSettings.newInstance().build(), TableConfig.getDefault());
     }
 

File: dinky-client/dinky-client-1.15/src/main/java/org/dinky/executor/CustomTableEnvironmentImpl.java
Patch:
@@ -110,7 +110,8 @@ public CustomTableEnvironmentImpl(
                 userClassLoader));
     }
 
-    public static CustomTableEnvironmentImpl create(StreamExecutionEnvironment executionEnvironment) {
+    public static CustomTableEnvironmentImpl create(
+            StreamExecutionEnvironment executionEnvironment, ClassLoader classLoader) {
         return create(executionEnvironment, EnvironmentSettings.newInstance().build());
     }
 

File: dinky-client/dinky-client-base/src/main/java/org/dinky/executor/CustomTableEnvironment.java
Patch:
@@ -73,6 +73,8 @@ public interface CustomTableEnvironment
 
     Planner getPlanner();
 
+    ClassLoader getUserClassLoader();
+
     Configuration getRootConfiguration();
 
     default List<LineageRel> getLineage(String statement) {

File: dinky-core/src/main/java/org/dinky/job/Job.java
Patch:
@@ -87,7 +87,7 @@ public Job(
         this.useGateway = useGateway;
     }
 
-    public static Job init(
+    public static Job build(
             GatewayType type,
             JobConfig jobConfig,
             ExecutorConfig executorConfig,
@@ -98,7 +98,6 @@ public static Job init(
         if (!useGateway) {
             job.setJobManagerAddress(executorConfig.getJobManagerAddress());
         }
-        JobContextHolder.setJob(job);
         return job;
     }
 

File: dinky-core/src/main/java/org/dinky/job/JobBuilder.java
Patch:
@@ -43,7 +43,7 @@ public JobBuilder(JobManager jobManager) {
         this.useStatementSet = jobManager.isUseStatementSet();
         this.useGateway = jobManager.isUseGateway();
         this.sqlSeparator = jobManager.getSqlSeparator();
-        this.job = JobContextHolder.getJob();
+        this.job = jobManager.getJob();
     }
 
     public abstract void run() throws Exception;

File: dinky-core/src/main/java/org/dinky/job/JobHandler.java
Patch:
@@ -30,7 +30,7 @@
  */
 public interface JobHandler {
 
-    boolean init();
+    boolean init(Job job);
 
     boolean ready();
 

File: dinky-core/src/main/java/org/dinky/job/builder/JobExecuteBuilder.java
Patch:
@@ -66,7 +66,8 @@ public void run() throws Exception {
                 GatewayResult gatewayResult = null;
                 config.addGatewayConfig(executor.getSetConfig());
                 if (runMode.isApplicationMode()) {
-                    gatewayResult = Gateway.build(config.getGatewayConfig()).submitJar();
+                    gatewayResult = Gateway.build(config.getGatewayConfig())
+                            .submitJar(executor.getDinkyClassLoader().getUdfPathContextHolder());
                 } else {
                     StreamGraph streamGraph = executor.getStreamGraph();
                     streamGraph.setJobName(config.getJobName());

File: dinky-core/src/main/java/org/dinky/job/builder/JobTransBuilder.java
Patch:
@@ -200,7 +200,8 @@ private GatewayResult submitByGateway(List<String> inserts) {
 
         if (runMode.isApplicationMode()) {
             // Application mode need to submit dinky-app.jar that in the hdfs or image.
-            gatewayResult = Gateway.build(config.getGatewayConfig()).submitJar();
+            gatewayResult = Gateway.build(config.getGatewayConfig())
+                    .submitJar(executor.getDinkyClassLoader().getUdfPathContextHolder());
         } else {
             JobGraph jobGraph = executor.getJobGraphFromInserts(inserts);
             // Perjob mode need to set savepoint restore path, when recovery from savepoint.

File: dinky-core/src/test/java/org/dinky/interceptor/CdcSourceTests.java
Patch:
@@ -19,6 +19,7 @@
 
 package org.dinky.interceptor;
 
+import org.dinky.classloader.DinkyClassLoader;
 import org.dinky.executor.Executor;
 import org.dinky.executor.ExecutorConfig;
 import org.dinky.executor.ExecutorFactory;
@@ -49,7 +50,7 @@ public void printTest() throws Exception {
                 .toString();
 
         ExecutorConfig executorConfig = ExecutorConfig.DEFAULT;
-        Executor executor = ExecutorFactory.buildLocalExecutor(executorConfig);
+        Executor executor = ExecutorFactory.buildLocalExecutor(executorConfig, DinkyClassLoader.build());
         executor.executeSql(statement);
         executor.execute("");
     }

File: dinky-gateway/src/main/java/org/dinky/gateway/AbstractGateway.java
Patch:
@@ -20,6 +20,7 @@
 package org.dinky.gateway;
 
 import org.dinky.assertion.Asserts;
+import org.dinky.context.FlinkUdfPathContextHolder;
 import org.dinky.data.enums.JobStatus;
 import org.dinky.gateway.config.GatewayConfig;
 import org.dinky.gateway.enums.ActionType;
@@ -182,7 +183,7 @@ public GatewayResult submitJobGraph(JobGraph jobGraph) {
     }
 
     @Override
-    public GatewayResult submitJar() {
+    public GatewayResult submitJar(FlinkUdfPathContextHolder udfPathContextHolder) {
         throw new GatewayException("Couldn't deploy Flink Cluster with User Application Jar.");
     }
 
@@ -207,7 +208,7 @@ public void killCluster() {
     }
 
     @Override
-    public GatewayResult deployCluster() {
+    public GatewayResult deployCluster(FlinkUdfPathContextHolder udfPathContextHolder) {
         logger.error("Could not deploy the Flink cluster");
         return null;
     }

File: dinky-gateway/src/main/java/org/dinky/gateway/Gateway.java
Patch:
@@ -20,6 +20,7 @@
 package org.dinky.gateway;
 
 import org.dinky.assertion.Asserts;
+import org.dinky.context.FlinkUdfPathContextHolder;
 import org.dinky.data.enums.JobStatus;
 import org.dinky.gateway.config.GatewayConfig;
 import org.dinky.gateway.enums.GatewayType;
@@ -70,7 +71,7 @@ static Gateway build(GatewayConfig config) {
 
     GatewayResult submitJobGraph(JobGraph jobGraph);
 
-    GatewayResult submitJar();
+    GatewayResult submitJar(FlinkUdfPathContextHolder udfPathContextHolder);
 
     SavePointResult savepointCluster();
 
@@ -88,5 +89,5 @@ static Gateway build(GatewayConfig config) {
 
     boolean onJobFinishCallback(String status);
 
-    GatewayResult deployCluster();
+    GatewayResult deployCluster(FlinkUdfPathContextHolder udfPathContextHolder);
 }

File: dinky-gateway/src/main/java/org/dinky/gateway/kubernetes/KubernetesApplicationGateway.java
Patch:
@@ -20,6 +20,7 @@
 package org.dinky.gateway.kubernetes;
 
 import org.dinky.assertion.Asserts;
+import org.dinky.context.FlinkUdfPathContextHolder;
 import org.dinky.data.model.SystemConfiguration;
 import org.dinky.gateway.config.AppConfig;
 import org.dinky.gateway.enums.GatewayType;
@@ -74,7 +75,7 @@ public GatewayType getType() {
      * @throws RuntimeException if an error occurs during submission.
      */
     @Override
-    public GatewayResult submitJar() {
+    public GatewayResult submitJar(FlinkUdfPathContextHolder udfPathContextHolder) {
         try {
             logger.info("Start submit k8s application.");
             ClusterClientProvider<String> clusterClient = deployApplication();

File: dinky-gateway/src/main/java/org/dinky/gateway/kubernetes/KubernetesSessionGateway.java
Patch:
@@ -20,6 +20,7 @@
 package org.dinky.gateway.kubernetes;
 
 import org.dinky.assertion.Asserts;
+import org.dinky.context.FlinkUdfPathContextHolder;
 import org.dinky.gateway.enums.GatewayType;
 import org.dinky.gateway.result.GatewayResult;
 import org.dinky.gateway.result.KubernetesResult;
@@ -43,7 +44,7 @@ public GatewayType getType() {
     }
 
     @Override
-    public GatewayResult deployCluster() {
+    public GatewayResult deployCluster(FlinkUdfPathContextHolder udfPathContextHolder) {
         if (Asserts.isNull(client)) {
             init();
         }

File: dinky-gateway/src/main/java/org/dinky/gateway/kubernetes/operator/KubetnetsApplicationOperatorGateway.java
Patch:
@@ -20,6 +20,7 @@
 package org.dinky.gateway.kubernetes.operator;
 
 import org.dinky.assertion.Asserts;
+import org.dinky.context.FlinkUdfPathContextHolder;
 import org.dinky.gateway.enums.GatewayType;
 import org.dinky.gateway.kubernetes.operator.api.FlinkDeployment;
 import org.dinky.gateway.result.GatewayResult;
@@ -50,7 +51,7 @@ public GatewayType getType() {
     }
 
     @Override
-    public GatewayResult submitJar() {
+    public GatewayResult submitJar(FlinkUdfPathContextHolder udfPathContextHolder) {
         // TODO 改为ProcessStep注释
         logger.info("start submit flink jar use {}", getType());
 

File: dinky-gateway/src/main/java/org/dinky/gateway/yarn/YarnGateway.java
Patch:
@@ -278,15 +278,15 @@ public void killCluster() {
         }
     }
 
-    protected YarnClusterDescriptor createYarnClusterDescriptorWithJar() {
+    protected YarnClusterDescriptor createYarnClusterDescriptorWithJar(FlinkUdfPathContextHolder udfPathContextHolder) {
         YarnClusterDescriptor yarnClusterDescriptor = createInitYarnClusterDescriptor();
 
         if (Asserts.isNotNull(config.getJarPaths())) {
             yarnClusterDescriptor.addShipFiles(
                     Arrays.stream(config.getJarPaths()).map(FileUtil::file).collect(Collectors.toList()));
-            yarnClusterDescriptor.addShipFiles(new ArrayList<>(FlinkUdfPathContextHolder.getPyUdfFile()));
+            yarnClusterDescriptor.addShipFiles(new ArrayList<>(udfPathContextHolder.getPyUdfFile()));
         }
-        Set<File> otherPluginsFiles = FlinkUdfPathContextHolder.getOtherPluginsFiles();
+        Set<File> otherPluginsFiles = udfPathContextHolder.getOtherPluginsFiles();
 
         if (CollUtil.isNotEmpty(otherPluginsFiles)) {
             yarnClusterDescriptor.addShipFiles(CollUtil.newArrayList(otherPluginsFiles));

File: dinky-gateway/src/main/java/org/dinky/gateway/yarn/YarnSessionGateway.java
Patch:
@@ -20,6 +20,7 @@
 package org.dinky.gateway.yarn;
 
 import org.dinky.assertion.Asserts;
+import org.dinky.context.FlinkUdfPathContextHolder;
 import org.dinky.gateway.enums.GatewayType;
 import org.dinky.gateway.result.GatewayResult;
 import org.dinky.gateway.result.YarnResult;
@@ -44,7 +45,7 @@ public GatewayType getType() {
     }
 
     @Override
-    public GatewayResult deployCluster() {
+    public GatewayResult deployCluster(FlinkUdfPathContextHolder udfPathContextHolder) {
         if (Asserts.isNull(yarnClient)) {
             init();
         }
@@ -53,7 +54,7 @@ public GatewayResult deployCluster() {
                 createClusterSpecificationBuilder();
 
         YarnResult result = YarnResult.build(getType());
-        try (YarnClusterDescriptor yarnClusterDescriptor = createYarnClusterDescriptorWithJar()) {
+        try (YarnClusterDescriptor yarnClusterDescriptor = createYarnClusterDescriptorWithJar(udfPathContextHolder)) {
             ClusterClientProvider<ApplicationId> clusterClientProvider = yarnClusterDescriptor.deploySessionCluster(
                     clusterSpecificationBuilder.createClusterSpecification());
             ClusterClient<ApplicationId> clusterClient = clusterClientProvider.getClusterClient();

File: dinky-admin/src/main/java/org/dinky/utils/FlinkConfigOptionsUtils.java
Patch:
@@ -91,6 +91,8 @@ public static String parsedBinlogGroup(String name) {
     }
 
     public static String[] getConfigOptionsClass() {
-        return ResourceUtil.readUtf8Str("FlinkConfClass").replace("\r", "").split("\n");
+        return ResourceUtil.readUtf8Str("dinky-loader/FlinkConfClass")
+                .replace("\r", "")
+                .split("\n");
     }
 }

File: dinky-admin/src/main/java/org/dinky/job/handler/JobRefreshHandler.java
Patch:
@@ -95,8 +95,10 @@ public static boolean refreshJob(JobInfoDetail jobInfoDetail, boolean needSave)
         JobDataDto jobDataDto = jobInfoDetail.getJobDataDto();
         String oldStatus = jobInstance.getStatus();
 
-        // Local mode without rest port parameter unable to monitor
+        // Cluster information is missing and cannot be monitored
         if (Asserts.isNull(jobInfoDetail.getClusterInstance())) {
+            jobInstance.setStatus(JobStatus.UNKNOWN.getValue());
+            jobInstanceService.updateById(jobInstance);
             return true;
         }
         // Update the value of JobData from the flink api while ignoring the null value to prevent

File: dinky-admin/src/main/java/org/dinky/job/handler/SystemMetricsHandler.java
Patch:
@@ -28,6 +28,7 @@
 import org.dinky.data.vo.MetricsVO;
 
 import java.time.LocalDateTime;
+import java.time.format.DateTimeFormatter;
 
 import lombok.extern.slf4j.Slf4j;
 
@@ -47,6 +48,7 @@ public static void refresh() {
         metrics.setContent(metricsTotal);
         metrics.setHeartTime(now);
         metrics.setModel(MetricsType.LOCAL.getType());
+        metrics.setDate(now.format(DateTimeFormatter.ofPattern("yyyy-MM-dd")));
         MetricsContextHolder.getInstances().sendAsync(metrics.getModel(), metrics);
 
         log.debug("Collecting jvm information ends.");

File: dinky-admin/src/main/java/org/dinky/service/impl/MonitorServiceImpl.java
Patch:
@@ -65,7 +65,7 @@ public class MonitorServiceImpl extends ServiceImpl<MetricsMapper, Metrics> impl
     private final Executor scheduleRefreshMonitorDataExecutor;
 
     @Override
-    public List<MetricsVO> getData(Date startTime, Date endTime, List<String> jobIds) {
+    public List<MetricsVO> getData(Date startTime, Date endTime, List<String> models) {
         endTime = Opt.ofNullable(endTime).orElse(DateUtil.date());
         Timestamp startTS = Timestamp.fromLocalDateTime(DateUtil.toLocalDateTime(startTime));
         Timestamp endTS = Timestamp.fromLocalDateTime(DateUtil.toLocalDateTime(endTime));
@@ -78,7 +78,7 @@ public List<MetricsVO> getData(Date startTime, Date endTime, List<String> jobIds
             Predicate greaterOrEqual = p.greaterOrEqual(0, startTS);
             Predicate lessOrEqual = p.lessOrEqual(0, endTS);
             Predicate local =
-                    p.in(1, jobIds.stream().map(BinaryString::fromString).collect(Collectors.toList()));
+                    p.in(1, models.stream().map(BinaryString::fromString).collect(Collectors.toList()));
             return CollUtil.newArrayList(local, greaterOrEqual, lessOrEqual);
         };
         List<MetricsVO> metricsVOList =

File: dinky-common/src/main/java/org/dinky/data/metrics/MetricsTotal.java
Patch:
@@ -19,13 +19,15 @@
 
 package org.dinky.data.metrics;
 
+import java.io.Serializable;
+
 import cn.hutool.core.lang.Singleton;
 import lombok.Getter;
 import lombok.Setter;
 
 @Getter
 @Setter
-public class MetricsTotal {
+public class MetricsTotal implements Serializable {
 
     public static volatile MetricsTotal instance = Singleton.get(MetricsTotal.class);
 

File: dinky-cdc/dinky-cdc-core/src/main/java/org/dinky/cdc/SinkBuilderFactory.java
Patch:
@@ -48,7 +48,7 @@ private SinkBuilderFactory() {}
     public static SinkBuilder buildSinkBuilder(FlinkCDCConfig config) {
 
         if (Asserts.isNull(config) || Asserts.isNullString(config.getSink().get("connector"))) {
-            throw new FlinkClientException("set Sink connector。");
+            throw new FlinkClientException("set Sink connector.");
         }
         return SINK_BUILDER_MAP
                 .getOrDefault(config.getSink().get("connector"), SQLSinkBuilder::new)

File: dinky-cdc/dinky-cdc-core/src/main/java/org/dinky/cdc/utils/FlinkStatementUtil.java
Patch:
@@ -138,7 +138,7 @@ private static String getSinkConfigurationString(
         String configurationString =
                 SqlUtil.replaceAllParam(config.getSinkConfigurationString(), "schemaName", sinkSchemaName);
         configurationString = SqlUtil.replaceAllParam(configurationString, "tableName", sinkTableName);
-        if (configurationString.contains("${pkList}")) {
+        if (configurationString.contains("#{pkList}")) {
             configurationString = SqlUtil.replaceAllParam(configurationString, "pkList", pkList);
         }
         return configurationString;

File: dinky-common/src/main/java/org/dinky/utils/SqlUtil.java
Patch:
@@ -68,7 +68,7 @@ public static String removeNote(String sql) {
     }
 
     public static String replaceAllParam(String sql, String name, String value) {
-        return sql.replaceAll("\\$\\{" + name + "\\}", value);
+        return sql.replaceAll("#\\{" + name + "\\}", value);
     }
 
     /**

File: dinky-gateway/src/main/java/org/dinky/gateway/enums/GatewayType.java
Patch:
@@ -58,7 +58,7 @@ public static GatewayType get(String value) {
                 return type;
             }
         }
-        return GatewayType.YARN_APPLICATION;
+        return GatewayType.LOCAL;
     }
 
     public static GatewayType getSessionType(String value) {

File: dinky-admin/src/main/java/org/dinky/init/SystemInit.java
Patch:
@@ -201,6 +201,7 @@ private void initDaemon() {
         };
         metricsListener.accept(metricsSysEnable);
         metricsListener.accept(sysGatherTiming);
+        metricsSysEnable.runChangeEvent();
 
         // Init clear job history task
         DaemonTask clearJobHistoryTask = DaemonTask.build(new DaemonTaskConfig(ClearJobHistoryTask.TYPE));

File: dinky-admin/src/main/java/org/dinky/job/handler/JobMetricsHandler.java
Patch:
@@ -27,6 +27,7 @@
 import org.dinky.utils.TimeUtil;
 
 import java.time.LocalDateTime;
+import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Map;
 import java.util.concurrent.CompletableFuture;
@@ -93,7 +94,8 @@ private static void fetchFlinkMetrics(String v, Map<String, String> m, String[]
         String metricsName = StrUtil.join(",", m.keySet());
         String urlParam =
                 StrFormatter.format("/jobs/{}/vertices/{}/metrics?get={}", jid, v, URLUtil.encode(metricsName));
-        HttpUtils.asyncRequest(Arrays.asList(urlList), urlParam, NetConstant.READ_TIME_OUT, x -> {
+        ArrayList<String> list = new ArrayList<String>(Arrays.asList(urlList));
+        HttpUtils.asyncRequest(list, urlParam, NetConstant.READ_TIME_OUT, x -> {
             JSONArray array = JSONUtil.parseArray(x.body());
             array.forEach(y -> {
                 JSONObject jsonObject = JSONUtil.parseObj(y);

File: dinky-alert/dinky-alert-email/src/test/java/org/dinky/alert/email/EmailSenderTest.java
Patch:
@@ -22,6 +22,7 @@
 import org.dinky.alert.AlertBaseConstant;
 import org.dinky.alert.AlertResult;
 
+import java.security.GeneralSecurityException;
 import java.util.Arrays;
 import java.util.HashMap;
 import java.util.Map;
@@ -58,9 +59,9 @@ public void initEmailConfig() {
 
     @Ignore
     @Test
-    public void testTextSendMails() {
+    public void testTextSendMails() throws GeneralSecurityException {
         AlertResult alertResult =
                 emailSender.send(AlertBaseConstant.ALERT_TEMPLATE_TITLE, AlertBaseConstant.ALERT_TEMPLATE_MSG);
-        Assert.assertEquals(true, alertResult.getSuccess()); // 格式需要调整
+        Assert.assertEquals(true, alertResult.getSuccess());
     }
 }

File: dinky-admin/src/main/java/org/dinky/data/dto/AlertInstanceDTO.java
Patch:
@@ -21,6 +21,8 @@
 
 import org.dinky.mybatis.annotation.Save;
 
+import java.util.Map;
+
 import javax.validation.constraints.NotNull;
 
 import io.swagger.annotations.ApiModel;
@@ -68,5 +70,5 @@ public class AlertInstanceDTO {
             required = true,
             dataType = "String",
             example = "{\"webhook\":\"https://oapi.dingtalk.com/robot/send?access_token=xxxxxx\"}")
-    private String params;
+    private Map<String, Object> params;
 }

File: dinky-admin/src/main/java/org/dinky/job/handler/JobAlertHandler.java
Patch:
@@ -210,8 +210,8 @@ private void executeAlertAction(Facts facts, AlertRuleDTO alertRuleDTO) {
      */
     private void sendAlert(
             AlertInstance alertInstance, int jobInstanceId, int alertGid, String title, String alertMsg) {
-        Map<String, String> params = JsonUtils.toMap(alertInstance.getParams());
-        AlertConfig alertConfig = AlertConfig.build(alertInstance.getName(), alertInstance.getType(), params);
+        AlertConfig alertConfig =
+                AlertConfig.build(alertInstance.getName(), alertInstance.getType(), alertInstance.getParams());
         Alert alert = Alert.build(alertConfig);
         AlertResult alertResult = alert.send(title, alertMsg);
 

File: dinky-alert/dinky-alert-dingtalk/src/main/java/org/dinky/alert/dingtalk/DingTalkConstants.java
Patch:
@@ -27,7 +27,6 @@ public class DingTalkConstants extends AlertBaseConstant {
     public static final String ALERT_TEMPLATE_TITLE = "title";
     public static final String ALERT_TEMPLATE_CONTENT = "content";
     public static final String ALERT_TEMPLATE_KEYWORD = "keyword";
-    public static final String ALERT_TEMPLATE_AT_MOBILE = "atMobile";
     public static final String ALERT_TEMPLATE_AT_MOBILES = "atMobiles";
-    public static final String ALERT_TEMPLATE_AT_ALL = "atAll";
+    public static final String ALERT_TEMPLATE_AT_ALL = "isAtAll";
 }

File: dinky-alert/dinky-alert-wechat/src/main/java/org/dinky/alert/wechat/WeChatConstants.java
Patch:
@@ -25,11 +25,10 @@
 public class WeChatConstants extends AlertBaseConstant {
     public static final String TYPE = "WeChat";
     /** WeChat alert baseconstant */
-    public static final String WECHAT_PUSH_URL =
-            "https://qyapi.weixin.qq.com/cgi-bin/message/send?access_token={token}";
+    public static final String WECHAT_PUSH_URL = "https://qyapi.weixin.qq.com/cgi-bin/message/send?access_token=%s";
 
     public static final String WECHAT_TOKEN_URL =
-            "https://qyapi.weixin.qq.com/cgi-bin/gettoken?corpid={corpId}&corpsecret={secret}";
+            "https://qyapi.weixin.qq.com/cgi-bin/gettoken?corpid=%s&corpsecret=%s";
     public static final String CORP_ID = "corpId";
     public static final String TEAM_SEND_MSG = "teamSendMsg";
     public static final String AGENT_ID = "agentId";

File: dinky-core/src/main/java/org/dinky/explainer/sqllineage/SQLLineageBuilder.java
Patch:
@@ -24,8 +24,8 @@
 import org.dinky.explainer.lineage.LineageRelation;
 import org.dinky.explainer.lineage.LineageResult;
 import org.dinky.explainer.lineage.LineageTable;
+import org.dinky.metadata.config.DriverConfig;
 import org.dinky.metadata.driver.Driver;
-import org.dinky.metadata.driver.DriverConfig;
 
 import java.util.ArrayList;
 import java.util.HashMap;

File: dinky-metadata/dinky-metadata-clickhouse/src/main/java/org/dinky/metadata/convert/ClickHouseTypeConvert.java
Patch:
@@ -26,7 +26,7 @@
  *
  * @since 2021/7/21 17:15
  */
-public class ClickHouseTypeConvert extends AbstractTypeConvert {
+public class ClickHouseTypeConvert extends AbstractJdbcTypeConvert {
 
     // Use mysql now,and welcome to fix it.
     public ClickHouseTypeConvert() {

File: dinky-metadata/dinky-metadata-clickhouse/src/main/java/org/dinky/metadata/driver/ClickHouseDriver.java
Patch:
@@ -24,6 +24,7 @@
 import org.dinky.data.model.Table;
 import org.dinky.data.result.SqlExplainResult;
 import org.dinky.metadata.ast.Clickhouse20CreateTableStatement;
+import org.dinky.metadata.config.AbstractJdbcConfig;
 import org.dinky.metadata.convert.ClickHouseTypeConvert;
 import org.dinky.metadata.convert.ITypeConvert;
 import org.dinky.metadata.parser.Clickhouse20StatementParser;
@@ -67,7 +68,7 @@ public IDBQuery getDBQuery() {
     }
 
     @Override
-    public ITypeConvert getTypeConvert() {
+    public ITypeConvert<AbstractJdbcConfig> getTypeConvert() {
         return new ClickHouseTypeConvert();
     }
 

File: dinky-metadata/dinky-metadata-doris/src/main/java/org/dinky/metadata/convert/DorisTypeConvert.java
Patch:
@@ -21,7 +21,7 @@
 
 import org.dinky.data.enums.ColumnType;
 
-public class DorisTypeConvert extends AbstractTypeConvert {
+public class DorisTypeConvert extends AbstractJdbcTypeConvert {
 
     public DorisTypeConvert() {
         this.convertMap.clear();

File: dinky-metadata/dinky-metadata-doris/src/main/java/org/dinky/metadata/driver/DorisDriver.java
Patch:
@@ -19,6 +19,7 @@
 
 package org.dinky.metadata.driver;
 
+import org.dinky.metadata.config.AbstractJdbcConfig;
 import org.dinky.metadata.convert.DorisTypeConvert;
 import org.dinky.metadata.convert.ITypeConvert;
 import org.dinky.metadata.query.DorisQuery;
@@ -44,7 +45,7 @@ public IDBQuery getDBQuery() {
     }
 
     @Override
-    public ITypeConvert getTypeConvert() {
+    public ITypeConvert<AbstractJdbcConfig> getTypeConvert() {
         return new DorisTypeConvert();
     }
 

File: dinky-metadata/dinky-metadata-hive/src/main/java/org/dinky/metadata/convert/HiveTypeConvert.java
Patch:
@@ -21,7 +21,7 @@
 
 import org.dinky.data.enums.ColumnType;
 
-public class HiveTypeConvert extends AbstractTypeConvert {
+public class HiveTypeConvert extends AbstractJdbcTypeConvert {
 
     public HiveTypeConvert() {
         this.convertMap.clear();

File: dinky-metadata/dinky-metadata-hive/src/main/java/org/dinky/metadata/driver/HiveDriver.java
Patch:
@@ -23,6 +23,7 @@
 import org.dinky.data.model.Column;
 import org.dinky.data.model.Schema;
 import org.dinky.data.model.Table;
+import org.dinky.metadata.config.AbstractJdbcConfig;
 import org.dinky.metadata.constant.HiveConstant;
 import org.dinky.metadata.convert.HiveTypeConvert;
 import org.dinky.metadata.convert.ITypeConvert;
@@ -287,7 +288,7 @@ public IDBQuery getDBQuery() {
     }
 
     @Override
-    public ITypeConvert getTypeConvert() {
+    public ITypeConvert<AbstractJdbcConfig> getTypeConvert() {
         return new HiveTypeConvert();
     }
 

File: dinky-metadata/dinky-metadata-oracle/src/main/java/org/dinky/metadata/convert/OracleTypeConvert.java
Patch:
@@ -21,7 +21,7 @@
 
 import org.dinky.data.enums.ColumnType;
 import org.dinky.data.model.Column;
-import org.dinky.metadata.driver.DriverConfig;
+import org.dinky.metadata.config.DriverConfig;
 
 import java.util.Optional;
 
@@ -30,7 +30,7 @@
  *
  * @since 2021/7/21 16:00
  */
-public class OracleTypeConvert extends AbstractTypeConvert {
+public class OracleTypeConvert extends AbstractJdbcTypeConvert {
 
     public OracleTypeConvert() {
         this.convertMap.clear();

File: dinky-metadata/dinky-metadata-oracle/src/main/java/org/dinky/metadata/driver/OracleDriver.java
Patch:
@@ -23,6 +23,7 @@
 import org.dinky.data.model.Column;
 import org.dinky.data.model.QueryData;
 import org.dinky.data.model.Table;
+import org.dinky.metadata.config.AbstractJdbcConfig;
 import org.dinky.metadata.convert.ITypeConvert;
 import org.dinky.metadata.convert.OracleTypeConvert;
 import org.dinky.metadata.query.IDBQuery;
@@ -59,7 +60,7 @@ public IDBQuery getDBQuery() {
     }
 
     @Override
-    public ITypeConvert getTypeConvert() {
+    public ITypeConvert<AbstractJdbcConfig> getTypeConvert() {
         return new OracleTypeConvert();
     }
 

File: dinky-metadata/dinky-metadata-phoenix/src/main/java/org/dinky/metadata/convert/PhoenixTypeConvert.java
Patch:
@@ -21,7 +21,7 @@
 
 import org.dinky.data.enums.ColumnType;
 
-public class PhoenixTypeConvert extends AbstractTypeConvert {
+public class PhoenixTypeConvert extends AbstractJdbcTypeConvert {
 
     public PhoenixTypeConvert() {
         this.convertMap.clear();

File: dinky-metadata/dinky-metadata-postgresql/src/main/java/org/dinky/metadata/convert/PostgreSqlTypeConvert.java
Patch:
@@ -26,7 +26,7 @@
  *
  * @since 2021/7/22 9:33
  */
-public class PostgreSqlTypeConvert extends AbstractTypeConvert {
+public class PostgreSqlTypeConvert extends AbstractJdbcTypeConvert {
 
     public PostgreSqlTypeConvert() {
         this.convertMap.clear();

File: dinky-metadata/dinky-metadata-postgresql/src/main/java/org/dinky/metadata/driver/PostgreSqlDriver.java
Patch:
@@ -23,6 +23,7 @@
 import org.dinky.data.model.Column;
 import org.dinky.data.model.QueryData;
 import org.dinky.data.model.Table;
+import org.dinky.metadata.config.AbstractJdbcConfig;
 import org.dinky.metadata.convert.ITypeConvert;
 import org.dinky.metadata.convert.PostgreSqlTypeConvert;
 import org.dinky.metadata.query.IDBQuery;
@@ -52,7 +53,7 @@ public IDBQuery getDBQuery() {
     }
 
     @Override
-    public ITypeConvert getTypeConvert() {
+    public ITypeConvert<AbstractJdbcConfig> getTypeConvert() {
         return new PostgreSqlTypeConvert();
     }
 

File: dinky-metadata/dinky-metadata-presto/src/main/java/org/dinky/metadata/driver/PrestoDriver.java
Patch:
@@ -24,6 +24,7 @@
 import org.dinky.data.model.QueryData;
 import org.dinky.data.model.Schema;
 import org.dinky.data.model.Table;
+import org.dinky.metadata.config.AbstractJdbcConfig;
 import org.dinky.metadata.constant.PrestoConstant;
 import org.dinky.metadata.convert.ITypeConvert;
 import org.dinky.metadata.convert.PrestoTypeConvert;
@@ -323,7 +324,7 @@ public IDBQuery getDBQuery() {
     }
 
     @Override
-    public ITypeConvert getTypeConvert() {
+    public ITypeConvert<AbstractJdbcConfig> getTypeConvert() {
         return new PrestoTypeConvert();
     }
 

File: dinky-metadata/dinky-metadata-sqlserver/src/main/java/org/dinky/metadata/convert/SqlServerTypeConvert.java
Patch:
@@ -21,7 +21,7 @@
 
 import org.dinky.data.enums.ColumnType;
 
-public class SqlServerTypeConvert extends AbstractTypeConvert {
+public class SqlServerTypeConvert extends AbstractJdbcTypeConvert {
 
     public SqlServerTypeConvert() {
         this.convertMap.clear();

File: dinky-metadata/dinky-metadata-sqlserver/src/main/java/org/dinky/metadata/driver/SqlServerDriver.java
Patch:
@@ -23,6 +23,7 @@
 import org.dinky.data.model.Column;
 import org.dinky.data.model.QueryData;
 import org.dinky.data.model.Table;
+import org.dinky.metadata.config.AbstractJdbcConfig;
 import org.dinky.metadata.constant.SqlServerConstant;
 import org.dinky.metadata.convert.ITypeConvert;
 import org.dinky.metadata.convert.SqlServerTypeConvert;
@@ -42,7 +43,7 @@ public IDBQuery getDBQuery() {
     }
 
     @Override
-    public ITypeConvert getTypeConvert() {
+    public ITypeConvert<AbstractJdbcConfig> getTypeConvert() {
         return new SqlServerTypeConvert();
     }
 

File: dinky-metadata/dinky-metadata-starrocks/src/main/java/org/dinky/metadata/convert/StarRocksTypeConvert.java
Patch:
@@ -21,7 +21,7 @@
 
 import org.dinky.data.enums.ColumnType;
 
-public class StarRocksTypeConvert extends AbstractTypeConvert {
+public class StarRocksTypeConvert extends AbstractJdbcTypeConvert {
 
     public StarRocksTypeConvert() {
         this.convertMap.clear();

File: dinky-metadata/dinky-metadata-starrocks/src/main/java/org/dinky/metadata/driver/StarRocksDriver.java
Patch:
@@ -19,6 +19,7 @@
 
 package org.dinky.metadata.driver;
 
+import org.dinky.metadata.config.AbstractJdbcConfig;
 import org.dinky.metadata.convert.ITypeConvert;
 import org.dinky.metadata.convert.StarRocksTypeConvert;
 import org.dinky.metadata.query.IDBQuery;
@@ -40,7 +41,7 @@ public IDBQuery getDBQuery() {
     }
 
     @Override
-    public ITypeConvert getTypeConvert() {
+    public ITypeConvert<AbstractJdbcConfig> getTypeConvert() {
         return new StarRocksTypeConvert();
     }
 

File: dinky-admin/src/main/java/org/dinky/data/model/job/History.java
Patch:
@@ -76,6 +76,9 @@ public class History implements Serializable {
     @ApiModelProperty(value = "Job Manager Address", dataType = "String")
     private String jobManagerAddress;
 
+    @ApiModelProperty(value = "batchModel", dataType = "Boolean")
+    private Boolean batchModel;
+
     @ApiModelProperty(value = "Status", dataType = "Integer")
     private Integer status;
 

File: dinky-admin/src/main/java/org/dinky/service/impl/JobInstanceServiceImpl.java
Patch:
@@ -189,10 +189,10 @@ public JobInfoDetail refreshJobInfoDetail(Integer jobInstanceId, boolean isForce
         DaemonTaskConfig daemonTaskConfig = DaemonTaskConfig.build(FlinkJobTask.TYPE, jobInstanceId);
         DaemonTask daemonTask = FlinkJobThreadPool.getInstance().getByTaskConfig(daemonTaskConfig);
 
-        if (daemonTask != null) {
-            daemonTask.dealTask();
+        if (daemonTask != null && !isForce) {
             return ((FlinkJobTask) daemonTask).getJobInfoDetail();
         } else if (isForce) {
+            FlinkJobThreadPool.getInstance().removeByTaskConfig(daemonTaskConfig);
             daemonTask = DaemonTask.build(daemonTaskConfig);
             daemonTask.dealTask();
             JobInfoDetail jobInfoDetail = ((FlinkJobTask) daemonTask).getJobInfoDetail();

File: dinky-admin/src/main/java/org/dinky/service/impl/MonitorServiceImpl.java
Patch:
@@ -115,7 +115,9 @@ public void saveFlinkMetricLayout(String layout, List<MetricsLayoutDTO> metricsL
         if (CollUtil.isEmpty(metricsList)) {
             return;
         }
-        saveBatch(BeanUtil.copyToList(metricsList, Metrics.class));
+        List<MetricsLayoutDTO> list =
+                metricsList.stream().peek(m -> m.setLayoutName(layout)).collect(Collectors.toList());
+        saveBatch(BeanUtil.copyToList(list, Metrics.class));
     }
 
     @Override

File: dinky-admin/src/main/java/org/dinky/controller/HistoryController.java
Patch:
@@ -59,7 +59,7 @@ public class HistoryController {
      * @param para
      * @return
      */
-    @PostMapping
+    @PostMapping("/list")
     @ApiOperation("Query History List")
     @ApiImplicitParam(name = "para", value = "Query Parameters", dataType = "JsonNode", paramType = "body")
     public ProTableResult<History> listHistory(@RequestBody JsonNode para) {

File: dinky-admin/src/main/java/org/dinky/data/model/ext/JobAlertData.java
Patch:
@@ -29,7 +29,6 @@
 import org.dinky.data.model.job.JobInstance;
 import org.dinky.data.options.JobAlertRuleOptions;
 import org.dinky.job.JobConfig;
-import org.dinky.utils.JsonUtils;
 import org.dinky.utils.TimeUtil;
 
 import com.fasterxml.jackson.annotation.JsonProperty;
@@ -150,7 +149,7 @@ public static JobAlertData buildData(JobInfoDetail jobInfoDetail) {
         builder.alertTime(TimeUtil.nowStr());
 
         JobDataDto jobDataDto = jobInfoDetail.getJobDataDto();
-        JobConfig job = JsonUtils.parseObject(jobInfoDetail.getHistory().getConfigJson(), JobConfig.class);
+        JobConfig job = jobInfoDetail.getHistory().getConfigJson();
         ClusterInstance clusterInstance = jobInfoDetail.getClusterInstance();
         CheckPointOverView checkpoints = jobDataDto.getCheckpoints();
         FlinkJobExceptionsDetail exceptions = jobDataDto.getExceptions();

File: dinky-admin/src/main/java/org/dinky/data/typehandler/JSONObjectHandler.java
Patch:
@@ -28,6 +28,7 @@
 import org.dinky.data.model.mapping.ClusterConfigurationMapping;
 import org.dinky.data.model.mapping.ClusterInstanceMapping;
 import org.dinky.gateway.model.FlinkClusterConfig;
+import org.dinky.job.JobConfig;
 
 import org.apache.ibatis.type.JdbcType;
 import org.apache.ibatis.type.MappedJdbcTypes;
@@ -49,7 +50,8 @@
     ClusterInstanceMapping.class,
     ClusterConfigurationMapping.class,
     FlinkClusterConfig.class,
-    TaskExtConfig.class
+    TaskExtConfig.class,
+    JobConfig.class
 })
 public class JSONObjectHandler<T> extends AbstractJsonTypeHandler<T> {
 

File: dinky-admin/src/main/java/org/dinky/job/handler/Job2MysqlHandler.java
Patch:
@@ -44,7 +44,6 @@
 import org.dinky.service.JobHistoryService;
 import org.dinky.service.JobInstanceService;
 import org.dinky.service.TaskService;
-import org.dinky.utils.JsonUtils;
 
 import java.time.LocalDateTime;
 
@@ -87,12 +86,11 @@ public boolean init() {
         }
         history.setJobManagerAddress(job.getJobManagerAddress());
         history.setJobName(job.getJobConfig().getJobName());
-        history.setSession(job.getJobConfig().getSession());
         history.setStatus(job.getStatus().ordinal());
         history.setStatement(job.getStatement());
         history.setStartTime(job.getStartTime());
         history.setTaskId(job.getJobConfig().getTaskId());
-        history.setConfigJson(JsonUtils.toJsonString(job.getJobConfig()));
+        history.setConfigJson(job.getJobConfig());
         historyService.save(history);
 
         job.setId(history.getId());

File: dinky-admin/src/main/java/org/dinky/service/impl/JobInstanceServiceImpl.java
Patch:
@@ -50,7 +50,6 @@
 import org.dinky.service.JobHistoryService;
 import org.dinky.service.JobInstanceService;
 import org.dinky.service.MonitorService;
-import org.dinky.utils.JsonUtils;
 
 import java.util.List;
 import java.util.Map;
@@ -164,7 +163,7 @@ public JobInfoDetail getJobInfoDetailInfo(JobInstance jobInstance) {
         jobInfoDetail.setClusterInstance(clusterInstance);
 
         History history = historyService.getById(jobInstance.getHistoryId());
-        history.setConfig(JsonUtils.parseObject(history.getConfigJson()));
+        history.setConfigJson(history.getConfigJson());
         jobInfoDetail.setHistory(history);
         if (Asserts.isNotNull(history.getClusterConfigurationId())) {
             ClusterConfiguration clusterConfig =

File: dinky-core/src/test/java/org/dinky/core/JobManagerTest.java
Patch:
@@ -49,7 +49,6 @@ public void cancelJobSelect() throws Exception {
                 .useResult(true)
                 .useChangeLog(true)
                 .useAutoCancel(true)
-                .session("s1")
                 .clusterId(2)
                 .jobName("Test")
                 .fragment(false)

File: dinky-admin/src/main/java/org/dinky/service/TaskService.java
Patch:
@@ -66,8 +66,7 @@ public interface TaskService extends ISuperService<Task> {
     /**
      * Submit the given task and return the job result.
      *
-     * @param id The ID of the task to submit.
-     * @param savePointPath The path of the save point for the job execution.
+     * @param submitDto The param of the task to submit.
      * @return A {@link JobResult} object representing the result of the submitted task.
      * @throws ExcuteException If there is an error executing the task.
      */

File: dinky-admin/src/main/java/org/dinky/controller/TaskController.java
Patch:
@@ -95,6 +95,7 @@ public Result<JobResult> submitTask(@ProcessId @RequestParam Integer id) throws
             required = true,
             dataType = "DebugDTO",
             paramType = "body")
+    @ExecuteProcess(type = ProcessType.FLINK_SUBMIT)
     public Result<JobResult> debugTask(@RequestBody DebugDTO debugDTO) throws Exception {
         JobResult result = taskService.debugTask(debugDTO);
         if (result.isSuccess()) {
@@ -159,7 +160,7 @@ public Result<List<SqlExplainResult>> explainSql(@RequestBody TaskDTO taskDTO) t
     @PostMapping("/getJobPlan")
     @ApiOperation("Get Job Plan")
     @ExecuteProcess(type = ProcessType.FLINK_JOB_PLAN)
-    public Result<ObjectNode> getJobPlan(@ProcessId @RequestBody TaskDTO taskDTO) {
+    public Result<ObjectNode> getJobPlan(@RequestBody TaskDTO taskDTO) {
         ObjectNode jobPlan = null;
         jobPlan = taskService.getJobPlan(taskDTO);
         return Result.succeed(jobPlan, Status.EXECUTE_SUCCESS);

File: dinky-admin/src/main/java/org/dinky/data/dto/DebugDTO.java
Patch:
@@ -19,6 +19,8 @@
 
 package org.dinky.data.dto;
 
+import org.dinky.data.annotations.ProcessId;
+
 import io.swagger.annotations.ApiModel;
 import io.swagger.annotations.ApiModelProperty;
 import lombok.Getter;
@@ -37,6 +39,7 @@ public class DebugDTO {
             dataType = "Integer",
             example = "1",
             notes = "The ID of Task which is debugged")
+    @ProcessId
     private Integer id;
 
     @ApiModelProperty(

File: dinky-common/src/main/java/org/dinky/data/annotations/ProcessId.java
Patch:
@@ -25,7 +25,7 @@
 import java.lang.annotation.RetentionPolicy;
 import java.lang.annotation.Target;
 
-@Target(ElementType.PARAMETER)
+@Target({ElementType.PARAMETER, ElementType.FIELD})
 @Retention(RetentionPolicy.RUNTIME)
 @Inherited
 public @interface ProcessId {}

File: dinky-admin/src/main/java/org/dinky/service/impl/TaskServiceImpl.java
Patch:
@@ -246,7 +246,7 @@ public String buildEnvSql(AbstractStatementDTO task) {
             // The order cannot be wrong here,
             // and the variables from the parameter have the highest priority
             Map<String, String> variables = fragmentVariableService.listEnabledVariables();
-            variables.putAll(task.getVariables());
+            variables.putAll(Optional.ofNullable(task.getVariables()).orElse(new HashMap<>()));
             task.setVariables(variables);
         }
         int envId = Optional.ofNullable(task.getEnvId()).orElse(-1);

File: dinky-admin/src/main/java/org/dinky/controller/TaskController.java
Patch:
@@ -26,6 +26,7 @@
 import org.dinky.data.dto.TaskDTO;
 import org.dinky.data.dto.TaskRollbackVersionDTO;
 import org.dinky.data.dto.TaskSaveDTO;
+import org.dinky.data.dto.TaskSubmitDto;
 import org.dinky.data.enums.BusinessType;
 import org.dinky.data.enums.JobLifeCycle;
 import org.dinky.data.enums.ProcessType;
@@ -76,7 +77,8 @@ public class TaskController {
     @Log(title = "Submit Task", businessType = BusinessType.SUBMIT)
     @ExecuteProcess(type = ProcessType.FLINK_SUBMIT)
     public Result<JobResult> submitTask(@ProcessId @RequestParam Integer id) throws Exception {
-        JobResult jobResult = taskService.submitTask(id, null);
+        JobResult jobResult =
+                taskService.submitTask(TaskSubmitDto.builder().id(id).build());
         if (jobResult.isSuccess()) {
             return Result.succeed(jobResult, Status.EXECUTE_SUCCESS);
         } else {

File: dinky-admin/src/main/java/org/dinky/data/dto/TaskDTO.java
Patch:
@@ -56,9 +56,6 @@ public class TaskDTO extends AbstractStatementDTO {
             notes = "The execution mode for the SQL query")
     private String type;
 
-    @ApiModelProperty(value = "Check Point", dataType = "Integer", example = "1", notes = "Check point for the task")
-    private Integer checkPoint;
-
     @ApiModelProperty(
             value = "Save Point Strategy",
             dataType = "Integer",

File: dinky-admin/src/main/java/org/dinky/service/TaskService.java
Patch:
@@ -23,6 +23,7 @@
 import org.dinky.data.dto.DebugDTO;
 import org.dinky.data.dto.TaskDTO;
 import org.dinky.data.dto.TaskRollbackVersionDTO;
+import org.dinky.data.dto.TaskSubmitDto;
 import org.dinky.data.enums.JobLifeCycle;
 import org.dinky.data.exception.ExcuteException;
 import org.dinky.data.exception.NotSupportExplainExcepition;
@@ -70,7 +71,7 @@ public interface TaskService extends ISuperService<Task> {
      * @return A {@link JobResult} object representing the result of the submitted task.
      * @throws ExcuteException If there is an error executing the task.
      */
-    JobResult submitTask(Integer id, String savePointPath) throws Exception;
+    JobResult submitTask(TaskSubmitDto submitDto) throws Exception;
 
     /**
      * Debug the given task and return the job result.

File: dinky-admin/src/main/java/org/dinky/init/SystemInit.java
Patch:
@@ -34,6 +34,7 @@
 import org.dinky.function.constant.PathConstant;
 import org.dinky.function.pool.UdfCodePool;
 import org.dinky.job.FlinkJobTask;
+import org.dinky.oss.OssTemplate;
 import org.dinky.scheduler.client.ProjectClient;
 import org.dinky.scheduler.exception.SchedulerException;
 import org.dinky.scheduler.model.Project;
@@ -46,7 +47,6 @@
 import org.dinky.service.resource.impl.OssResourceManager;
 import org.dinky.url.RsURLStreamHandlerFactory;
 import org.dinky.utils.JsonUtils;
-import org.dinky.utils.OssTemplate;
 import org.dinky.utils.UDFUtils;
 
 import org.apache.catalina.webresources.TomcatURLStreamHandlerFactory;
@@ -98,9 +98,9 @@ public class SystemInit implements ApplicationRunner {
     public void run(ApplicationArguments args) {
         TenantContextHolder.ignoreTenant();
         initResources();
-
         List<Tenant> tenants = tenantService.list();
         sysConfigService.initSysConfig();
+
         for (Tenant tenant : tenants) {
             taskService.initDefaultFlinkSQLEnv(tenant.getId());
         }

File: dinky-admin/src/main/java/org/dinky/service/resource/impl/OssResourceManager.java
Patch:
@@ -21,8 +21,8 @@
 
 import org.dinky.data.exception.BusException;
 import org.dinky.data.exception.DinkyException;
+import org.dinky.oss.OssTemplate;
 import org.dinky.service.resource.BaseResourceManager;
-import org.dinky.utils.OssTemplate;
 
 import java.io.File;
 import java.io.InputStream;

File: dinky-common/src/main/java/org/dinky/oss/OssTemplate.java
Patch:
@@ -17,7 +17,7 @@
  *
  */
 
-package org.dinky.utils;
+package org.dinky.oss;
 
 import org.dinky.data.properties.OssProperties;
 

File: dinky-gateway/src/main/java/org/dinky/gateway/yarn/YarnApplicationGateway.java
Patch:
@@ -64,7 +64,6 @@ public GatewayResult submitJar() {
 
         AppConfig appConfig = config.getAppConfig();
         configuration.set(PipelineOptions.JARS, Collections.singletonList(appConfig.getUserJarPath()));
-
         configuration.setString(
                 "python.files",
                 FlinkUdfPathContextHolder.getPyUdfFile().stream()

File: dinky-admin/src/main/java/org/dinky/service/impl/TaskServiceImpl.java
Patch:
@@ -444,8 +444,8 @@ public boolean changeTaskLifeRecyle(Integer taskId, JobLifeCycle lifeCycle) thro
 
     @Override
     public boolean saveOrUpdateTask(Task task) {
-
-        if (JobLifeCycle.PUBLISH.equalsValue(task.getStep())) {
+        Task byId = getById(task.getId());
+        if (byId != null && JobLifeCycle.PUBLISH.equalsValue(byId.getStep())) {
             throw new BusException(Status.TASK_IS_ONLINE.getMessage());
         }
 

File: dinky-admin/src/main/java/org/dinky/data/dto/APIExecuteSqlDTO.java
Patch:
@@ -128,7 +128,7 @@ public JobConfig getJobConfig() {
                 .useRemote(true)
                 .address(address)
                 .jobName(jobName)
-                .fragment(isFragment())
+                .fragment(getFragment())
                 .statementSet(useStatementSet)
                 .maxRowNum(maxRowNum)
                 .checkpoint(checkPoint)

File: dinky-admin/src/main/java/org/dinky/data/dto/APIExplainSqlDTO.java
Patch:
@@ -59,7 +59,7 @@ public JobConfig getJobConfig() {
         return JobConfig.builder()
                 .type(GatewayType.LOCAL.getLongValue())
                 .useRemote(false)
-                .fragment(isFragment())
+                .fragment(getFragment())
                 .statementSet(useStatementSet)
                 .parallelism(parallelism)
                 .configJson(configuration)

File: dinky-admin/src/main/java/org/dinky/data/dto/AbstractStatementDTO.java
Patch:
@@ -41,7 +41,7 @@ public class AbstractStatementDTO {
     private Integer envId;
 
     @ApiModelProperty(value = "Fragment Flag", dataType = "boolean", example = "false", notes = "是否为片段")
-    private boolean fragment = false;
+    private Boolean fragment;
 
     @ApiModelProperty(
             value = "Variables",

File: dinky-admin/src/main/java/org/dinky/data/dto/StudioMetaStoreDTO.java
Patch:
@@ -70,7 +70,7 @@ public JobConfig getJobConfig() {
                 .useResult(true)
                 .useChangeLog(false)
                 .useAutoCancel(false)
-                .fragment(isFragment())
+                .fragment(getFragment())
                 .statementSet(false)
                 .batchModel(false)
                 .maxRowNum(0)

File: dinky-admin/src/main/java/org/dinky/service/impl/TaskServiceImpl.java
Patch:
@@ -227,7 +227,7 @@ public JobConfig buildJobConfig(TaskDTO task) {
     public String buildEnvSql(AbstractStatementDTO task) {
         log.info("Start initialize FlinkSQLEnv:");
         String sql = CommonConstant.LineSep;
-        if (task.isFragment()) {
+        if (task.getFragment()) {
             String flinkWithSql = dataBaseService.getEnabledFlinkWithSql();
             if (Asserts.isNotNullString(flinkWithSql)) {
                 sql += flinkWithSql + CommonConstant.LineSep;

File: dinky-app/dinky-app-1.16/src/main/java/org/dinky/app/MainApp.java
Patch:
@@ -54,8 +54,7 @@ public static void main(String[] args) throws Exception {
             DBUtil.init(appConfig);
             Submitter.submit(appConfig);
         } catch (Exception e) {
-            log.error("exectue app failed with config: {}", appConfig);
-            throw e;
+            log.error("exectue app failed : ", e);
         } finally {
             log.info("Start Monitor Job");
             FlinkAppUtil.monitorFlinkTask(appConfig.getTaskId());

File: dinky-admin/src/main/java/org/dinky/data/dto/AssignRoleDTO.java
Patch:
@@ -17,7 +17,7 @@
  *
  */
 
-package org.dinky.data.params;
+package org.dinky.data.dto;
 
 import java.util.List;
 
@@ -27,8 +27,8 @@
 
 /** assign role params */
 @Data
-@ApiModel(value = "AssignRoleParams", description = "Parameters for Assigning Roles to a User")
-public class AssignRoleParams {
+@ApiModel(value = "AssignRoleDTO", description = "Parameters for Assigning Roles to a User")
+public class AssignRoleDTO {
 
     @ApiModelProperty(value = "User ID", dataType = "Integer", notes = "ID of the user to assign roles to")
     private Integer userId;

File: dinky-admin/src/main/java/org/dinky/data/dto/AssignUserToTenantDTO.java
Patch:
@@ -17,7 +17,7 @@
  *
  */
 
-package org.dinky.data.params;
+package org.dinky.data.dto;
 
 import java.util.List;
 
@@ -31,8 +31,8 @@
 @Data
 @AllArgsConstructor
 @NoArgsConstructor
-@ApiModel(value = "AssignUserToTenantParams", description = "Parameters for Assigning Users to a Tenant")
-public class AssignUserToTenantParams {
+@ApiModel(value = "AssignUserToTenantDTO", description = "Parameters for Assigning Users to a Tenant")
+public class AssignUserToTenantDTO {
 
     @ApiModelProperty(value = "Tenant ID", dataType = "Integer", notes = "ID of the tenant to assign users to")
     private Integer tenantId;

File: dinky-admin/src/main/java/org/dinky/service/GitProjectService.java
Patch:
@@ -20,9 +20,9 @@
 package org.dinky.service;
 
 import org.dinky.data.dto.GitProjectDTO;
+import org.dinky.data.dto.GitProjectSortJarDTO;
 import org.dinky.data.dto.TreeNodeDTO;
 import org.dinky.data.model.GitProject;
-import org.dinky.data.params.GitProjectSortJarParams;
 import org.dinky.mybatis.service.ISuperService;
 
 import java.util.List;
@@ -50,10 +50,10 @@ public interface GitProjectService extends ISuperService<GitProject> {
     /**
      * drag sort jar level
      *
-     * @param gitProjectSortJarParams
+     * @param gitProjectSortJarDTO
      * @return
      */
-    Boolean dragendSortJar(GitProjectSortJarParams gitProjectSortJarParams);
+    Boolean dragendSortJar(GitProjectSortJarDTO gitProjectSortJarDTO);
 
     /**
      * Get the Git pool as a map of strings.

File: dinky-admin/src/main/java/org/dinky/service/TenantService.java
Patch:
@@ -19,8 +19,8 @@
 
 package org.dinky.service;
 
+import org.dinky.data.dto.AssignUserToTenantDTO;
 import org.dinky.data.model.rbac.Tenant;
-import org.dinky.data.params.AssignUserToTenantParams;
 import org.dinky.data.result.Result;
 import org.dinky.mybatis.service.ISuperService;
 
@@ -78,8 +78,8 @@ public interface TenantService extends ISuperService<Tenant> {
     /**
      * assignUserToTenant users to tenant
      *
-     * @param assignUserToTenantParams {@link AssignUserToTenantParams}
+     * @param assignUserToTenantDTO {@link AssignUserToTenantDTO}
      * @return {@link Result} of {@link Void}
      */
-    Result<Void> assignUserToTenant(AssignUserToTenantParams assignUserToTenantParams);
+    Result<Void> assignUserToTenant(AssignUserToTenantDTO assignUserToTenantDTO);
 }

File: dinky-admin/src/main/java/org/dinky/service/UserService.java
Patch:
@@ -19,14 +19,14 @@
 
 package org.dinky.service;
 
+import org.dinky.data.dto.AssignRoleDTO;
 import org.dinky.data.dto.LoginDTO;
 import org.dinky.data.dto.ModifyPasswordDTO;
 import org.dinky.data.dto.UserDTO;
 import org.dinky.data.model.rbac.Role;
 import org.dinky.data.model.rbac.RowPermissions;
 import org.dinky.data.model.rbac.Tenant;
 import org.dinky.data.model.rbac.User;
-import org.dinky.data.params.AssignRoleParams;
 import org.dinky.data.result.Result;
 import org.dinky.data.vo.UserVo;
 import org.dinky.mybatis.service.ISuperService;
@@ -91,10 +91,10 @@ public interface UserService extends ISuperService<User> {
     /**
      * grant role
      *
-     * @param assignRoleParams {@link AssignRoleParams}
+     * @param assignRoleDTO {@link AssignRoleDTO}
      * @return {@link Result}<{@link Void}>
      */
-    Result<Void> assignRole(AssignRoleParams assignRoleParams);
+    Result<Void> assignRole(AssignRoleDTO assignRoleDTO);
 
     /**
      * choose tenant

File: dinky-admin/src/main/java/org/dinky/job/FlinkJobTask.java
Patch:
@@ -33,10 +33,12 @@
 
 import org.springframework.context.annotation.DependsOn;
 
+import lombok.Data;
 import lombok.extern.slf4j.Slf4j;
 
 @DependsOn("springContextUtils")
 @Slf4j
+@Data
 public class FlinkJobTask implements DaemonTask {
 
     private DaemonTaskConfig config;

File: dinky-admin/src/main/java/org/dinky/job/handler/JobMetricsHandler.java
Patch:
@@ -24,6 +24,7 @@
 import org.dinky.data.model.ext.JobInfoDetail;
 import org.dinky.data.vo.MetricsVO;
 import org.dinky.utils.HttpUtils;
+import org.dinky.utils.TimeUtil;
 
 import java.time.LocalDateTime;
 import java.util.Arrays;
@@ -67,6 +68,7 @@ public static void writeFlinkMetrics(JobInfoDetail jobInfoDetail) {
         metricsVO.setContent(customMetricsList);
         metricsVO.setHeartTime(LocalDateTime.now());
         metricsVO.setModel(jobId);
+        metricsVO.setDate(TimeUtil.nowStr("yyyy-MM-dd"));
         MetricsContextHolder.getInstances().sendAsync(metricsVO.getModel(), metricsVO);
     }
 

File: dinky-daemon/src/main/java/org/dinky/daemon/entity/TaskWorker.java
Patch:
@@ -50,12 +50,12 @@ public TaskWorker(TaskQueue<DaemonTask> queue) {
     public void run() {
         log.debug("TaskWorker run:" + Thread.currentThread().getName());
         while (running) {
-            DaemonTask daemonTask = queue.dequeue();
+            DaemonTask daemonTask = queue.getNext();
             if (daemonTask != null) {
                 try {
                     boolean done = daemonTask.dealTask();
-                    if (!done) {
-                        queue.enqueue(daemonTask);
+                    if (done) {
+                        queue.removeByTask(daemonTask);
                     }
                 } catch (Exception e) {
                     log.error(e.getMessage(), e);

File: dinky-daemon/src/main/java/org/dinky/daemon/pool/ThreadPool.java
Patch:
@@ -20,7 +20,6 @@
 package org.dinky.daemon.pool;
 
 import org.dinky.daemon.task.DaemonTask;
-import org.dinky.daemon.task.DaemonTaskConfig;
 
 /**
  * @operate
@@ -31,8 +30,6 @@ public interface ThreadPool {
     // 执行任务
     void execute(DaemonTask daemonTask);
 
-    DaemonTask dequeueByTask(DaemonTaskConfig daemonTask);
-
     // 关闭连接池
     void shutdown();
 

File: dinky-admin/src/main/java/org/dinky/controller/DataBaseController.java
Patch:
@@ -78,7 +78,7 @@ public class DataBaseController {
      * @param dataBaseDTO {@link DataBaseDTO}
      * @return {@link Result}< {@link Void}>
      */
-    @PutMapping
+    @PutMapping("/saveOrUpdate")
     @Log(title = "Insert Or Update DataBase", businessType = BusinessType.INSERT_OR_UPDATE)
     @ApiOperation("Insert Or Update DataBase")
     @ApiImplicitParam(

File: dinky-admin/src/main/java/org/dinky/sse/git/GetJarsStepSse.java
Patch:
@@ -66,7 +66,7 @@ private List<String> uploadResources(List<File> jars) {
         GitProject gitProject = (GitProject) params.get("gitProject");
 
         ResourcesService resourcesService = SpringUtil.getBean(ResourcesService.class);
-        TreeNodeDTO gitFolder = resourcesService.createFolderOrGet(0, "git", "");
+        TreeNodeDTO gitFolder = resourcesService.createFolderOrGet(1, "git", "");
         TreeNodeDTO treeNodeDTO =
                 resourcesService.createFolderOrGet(Convert.toInt(gitFolder.getId()), gitProject.getName(), "");
         return jars.stream()

File: dinky-admin/src/main/java/org/dinky/sse/git/PythonZipStepSse.java
Patch:
@@ -64,7 +64,7 @@ public void exec() {
 
     private String uploadResources(File zipFile, GitProject gitProject) {
         ResourcesService resourcesService = SpringUtil.getBean(ResourcesService.class);
-        TreeNodeDTO gitFolder = resourcesService.createFolderOrGet(0, "git", "");
+        TreeNodeDTO gitFolder = resourcesService.createFolderOrGet(1, "git", "");
         TreeNodeDTO treeNodeDTO =
                 resourcesService.createFolderOrGet(Convert.toInt(gitFolder.getId()), gitProject.getName(), "");
         resourcesService.uploadFile(Convert.toInt(treeNodeDTO.getId()), "", zipFile);

File: dinky-admin/src/main/java/org/dinky/configure/AppConfig.java
Patch:
@@ -93,6 +93,7 @@ public void addInterceptors(InterceptorRegistry registry) {
                 .addPathPatterns("/api/namespace/**")
                 .addPathPatterns("/api/savepoints/**")
                 .addPathPatterns("/api/statement/**")
+                .addPathPatterns("/api/studio/**")
                 .addPathPatterns("/api/task/**")
                 .addPathPatterns("/api/role/**")
                 .addPathPatterns("/api/fragment/**")

File: dinky-common/src/main/java/org/dinky/data/constant/DirConstant.java
Patch:
@@ -21,7 +21,7 @@
 
 import java.io.File;
 
-import cn.hutool.core.io.FileUtil;
+import cn.hutool.system.SystemUtil;
 
 /**
  * DirConstant
@@ -31,7 +31,7 @@
 public class DirConstant {
 
     public static final String FILE_SEPARATOR = File.separator;
-    public static final String ROOT_PATH = FileUtil.getUserHomeDir().getAbsolutePath();
+    public static final String ROOT_PATH = System.getProperty(SystemUtil.USER_DIR);
     public static final String LOG_DIR_PATH = ROOT_PATH + FILE_SEPARATOR + "logs";
     public static final String ROOT_LOG_PATH = LOG_DIR_PATH + FILE_SEPARATOR + "dinky.log";
 }

File: dinky-executor/src/main/java/org/dinky/executor/VariableManager.java
Patch:
@@ -146,7 +146,7 @@ public Object getVariable(String variableName) {
                 return variables.get(variableName);
             }
             // use jexl to parse variable value
-            return ENGINE.eval(variableName, ENGINE_CONTEXT);
+            return ENGINE.eval(variableName, ENGINE_CONTEXT, null);
         } catch (Exception e) {
             throw new CatalogException(format("The variable of sql %s does not exist.", variableName));
         }

File: dinky-admin/src/main/java/org/dinky/aop/LogAspect.java
Patch:
@@ -20,7 +20,7 @@
 package org.dinky.aop;
 
 import org.dinky.context.UserInfoContextHolder;
-import org.dinky.data.annotation.Log;
+import org.dinky.data.annotations.Log;
 import org.dinky.data.enums.BusinessStatus;
 import org.dinky.data.model.OperateLog;
 import org.dinky.data.model.User;
@@ -63,7 +63,7 @@
 @Component
 public class LogAspect {
 
-    @Pointcut("@annotation(org.dinky.data.annotation.Log)")
+    @Pointcut("@annotation(org.dinky.data.annotations.Log)")
     public void logPointCut() {}
 
     /**

File: dinky-admin/src/main/java/org/dinky/configure/schedule/metrics/GatherSysIndicator.java
Patch:
@@ -21,7 +21,7 @@
 
 import org.dinky.configure.schedule.BaseSchedule;
 import org.dinky.context.MetricsContextHolder;
-import org.dinky.data.annotation.GaugeM;
+import org.dinky.data.annotations.GaugeM;
 import org.dinky.data.enums.MetricsType;
 import org.dinky.data.metrics.BaseMetrics;
 import org.dinky.data.metrics.Cpu;

File: dinky-admin/src/main/java/org/dinky/controller/APIController.java
Patch:
@@ -19,7 +19,7 @@
 
 package org.dinky.controller;
 
-import org.dinky.data.annotation.Log;
+import org.dinky.data.annotations.Log;
 import org.dinky.data.dto.TaskDTO;
 import org.dinky.data.enums.BusinessType;
 import org.dinky.data.enums.Status;

File: dinky-admin/src/main/java/org/dinky/controller/AlertGroupController.java
Patch:
@@ -19,7 +19,7 @@
 
 package org.dinky.controller;
 
-import org.dinky.data.annotation.Log;
+import org.dinky.data.annotations.Log;
 import org.dinky.data.constant.PermissionConstants;
 import org.dinky.data.enums.BusinessType;
 import org.dinky.data.enums.Status;

File: dinky-admin/src/main/java/org/dinky/controller/AlertHistoryController.java
Patch:
@@ -19,7 +19,7 @@
 
 package org.dinky.controller;
 
-import org.dinky.data.annotation.Log;
+import org.dinky.data.annotations.Log;
 import org.dinky.data.enums.BusinessType;
 import org.dinky.data.enums.Status;
 import org.dinky.data.model.AlertHistory;

File: dinky-admin/src/main/java/org/dinky/controller/AlertInstanceController.java
Patch:
@@ -21,7 +21,7 @@
 
 import org.dinky.alert.AlertPool;
 import org.dinky.alert.AlertResult;
-import org.dinky.data.annotation.Log;
+import org.dinky.data.annotations.Log;
 import org.dinky.data.constant.PermissionConstants;
 import org.dinky.data.dto.AlertInstanceDTO;
 import org.dinky.data.enums.BusinessType;

File: dinky-admin/src/main/java/org/dinky/controller/AlertRuleController.java
Patch:
@@ -19,7 +19,7 @@
 
 package org.dinky.controller;
 
-import org.dinky.data.annotation.Log;
+import org.dinky.data.annotations.Log;
 import org.dinky.data.constant.PermissionConstants;
 import org.dinky.data.enums.BusinessType;
 import org.dinky.data.enums.Status;

File: dinky-admin/src/main/java/org/dinky/controller/AlertTemplateController.java
Patch:
@@ -19,7 +19,7 @@
 
 package org.dinky.controller;
 
-import org.dinky.data.annotation.Log;
+import org.dinky.data.annotations.Log;
 import org.dinky.data.constant.PermissionConstants;
 import org.dinky.data.enums.BusinessType;
 import org.dinky.data.enums.Status;

File: dinky-admin/src/main/java/org/dinky/controller/CatalogueController.java
Patch:
@@ -19,7 +19,7 @@
 
 package org.dinky.controller;
 
-import org.dinky.data.annotation.Log;
+import org.dinky.data.annotations.Log;
 import org.dinky.data.dto.CatalogueTaskDTO;
 import org.dinky.data.enums.BusinessType;
 import org.dinky.data.enums.Status;

File: dinky-admin/src/main/java/org/dinky/controller/ClusterConfigurationController.java
Patch:
@@ -19,7 +19,7 @@
 
 package org.dinky.controller;
 
-import org.dinky.data.annotation.Log;
+import org.dinky.data.annotations.Log;
 import org.dinky.data.constant.PermissionConstants;
 import org.dinky.data.dto.ClusterConfigurationDTO;
 import org.dinky.data.enums.BusinessType;

File: dinky-admin/src/main/java/org/dinky/controller/ClusterInstanceController.java
Patch:
@@ -19,7 +19,7 @@
 
 package org.dinky.controller;
 
-import org.dinky.data.annotation.Log;
+import org.dinky.data.annotations.Log;
 import org.dinky.data.constant.PermissionConstants;
 import org.dinky.data.dto.ClusterInstanceDTO;
 import org.dinky.data.enums.BusinessType;

File: dinky-admin/src/main/java/org/dinky/controller/DataBaseController.java
Patch:
@@ -20,7 +20,7 @@
 package org.dinky.controller;
 
 import org.dinky.assertion.Asserts;
-import org.dinky.data.annotation.Log;
+import org.dinky.data.annotations.Log;
 import org.dinky.data.constant.CommonConstant;
 import org.dinky.data.constant.PermissionConstants;
 import org.dinky.data.dto.DataBaseDTO;

File: dinky-admin/src/main/java/org/dinky/controller/DocumentController.java
Patch:
@@ -19,7 +19,7 @@
 
 package org.dinky.controller;
 
-import org.dinky.data.annotation.Log;
+import org.dinky.data.annotations.Log;
 import org.dinky.data.constant.PermissionConstants;
 import org.dinky.data.enums.BusinessType;
 import org.dinky.data.enums.Status;

File: dinky-admin/src/main/java/org/dinky/controller/DownloadController.java
Patch:
@@ -20,7 +20,7 @@
 package org.dinky.controller;
 
 import org.dinky.assertion.Asserts;
-import org.dinky.data.annotation.Log;
+import org.dinky.data.annotations.Log;
 import org.dinky.data.enums.BusinessType;
 import org.dinky.data.exception.BusException;
 import org.dinky.data.model.FlinkUdfManifest;

File: dinky-admin/src/main/java/org/dinky/controller/FragmentController.java
Patch:
@@ -19,7 +19,7 @@
 
 package org.dinky.controller;
 
-import org.dinky.data.annotation.Log;
+import org.dinky.data.annotations.Log;
 import org.dinky.data.constant.PermissionConstants;
 import org.dinky.data.enums.BusinessType;
 import org.dinky.data.enums.Status;

File: dinky-admin/src/main/java/org/dinky/controller/GitController.java
Patch:
@@ -19,7 +19,7 @@
 
 package org.dinky.controller;
 
-import org.dinky.data.annotation.Log;
+import org.dinky.data.annotations.Log;
 import org.dinky.data.constant.PermissionConstants;
 import org.dinky.data.dto.GitProjectDTO;
 import org.dinky.data.dto.TreeNodeDTO;

File: dinky-admin/src/main/java/org/dinky/controller/JobInstanceController.java
Patch:
@@ -21,7 +21,7 @@
 
 import org.dinky.api.FlinkAPI;
 import org.dinky.assertion.Asserts;
-import org.dinky.data.annotation.Log;
+import org.dinky.data.annotations.Log;
 import org.dinky.data.enums.BusinessType;
 import org.dinky.data.model.ID;
 import org.dinky.data.model.JobInfoDetail;

File: dinky-admin/src/main/java/org/dinky/controller/LdapController.java
Patch:
@@ -19,7 +19,7 @@
 
 package org.dinky.controller;
 
-import org.dinky.data.annotation.Log;
+import org.dinky.data.annotations.Log;
 import org.dinky.data.dto.LoginDTO;
 import org.dinky.data.dto.UserDTO;
 import org.dinky.data.enums.BusinessType;

File: dinky-admin/src/main/java/org/dinky/controller/MenuController.java
Patch:
@@ -19,7 +19,7 @@
 
 package org.dinky.controller;
 
-import org.dinky.data.annotation.Log;
+import org.dinky.data.annotations.Log;
 import org.dinky.data.constant.PermissionConstants;
 import org.dinky.data.dto.MenuDTO;
 import org.dinky.data.dto.RoleMenuDTO;

File: dinky-admin/src/main/java/org/dinky/controller/MonitorController.java
Patch:
@@ -19,7 +19,7 @@
 
 package org.dinky.controller;
 
-import org.dinky.data.annotation.Log;
+import org.dinky.data.annotations.Log;
 import org.dinky.data.dto.MetricsLayoutDTO;
 import org.dinky.data.enums.BusinessType;
 import org.dinky.data.enums.MetricsType;

File: dinky-admin/src/main/java/org/dinky/controller/ResourceController.java
Patch:
@@ -19,7 +19,7 @@
 
 package org.dinky.controller;
 
-import org.dinky.data.annotation.Log;
+import org.dinky.data.annotations.Log;
 import org.dinky.data.constant.PermissionConstants;
 import org.dinky.data.dto.ResourcesDTO;
 import org.dinky.data.dto.TreeNodeDTO;

File: dinky-admin/src/main/java/org/dinky/controller/RoleController.java
Patch:
@@ -19,7 +19,7 @@
 
 package org.dinky.controller;
 
-import org.dinky.data.annotation.Log;
+import org.dinky.data.annotations.Log;
 import org.dinky.data.constant.PermissionConstants;
 import org.dinky.data.dto.RoleDTO;
 import org.dinky.data.enums.BusinessType;

File: dinky-admin/src/main/java/org/dinky/controller/RoleMenuController.java
Patch:
@@ -19,7 +19,7 @@
 
 package org.dinky.controller;
 
-import org.dinky.data.annotation.Log;
+import org.dinky.data.annotations.Log;
 import org.dinky.data.constant.PermissionConstants;
 import org.dinky.data.dto.AssignMenuToRoleDTO;
 import org.dinky.data.enums.BusinessType;

File: dinky-admin/src/main/java/org/dinky/controller/RowPermissionsController.java
Patch:
@@ -19,7 +19,7 @@
 
 package org.dinky.controller;
 
-import org.dinky.data.annotation.Log;
+import org.dinky.data.annotations.Log;
 import org.dinky.data.constant.PermissionConstants;
 import org.dinky.data.enums.BusinessType;
 import org.dinky.data.enums.Status;

File: dinky-admin/src/main/java/org/dinky/controller/StudioController.java
Patch:
@@ -20,7 +20,7 @@
 package org.dinky.controller;
 
 import org.dinky.assertion.Asserts;
-import org.dinky.data.annotation.Log;
+import org.dinky.data.annotations.Log;
 import org.dinky.data.dto.StudioDDLDTO;
 import org.dinky.data.dto.StudioLineageDTO;
 import org.dinky.data.dto.StudioMetaStoreDTO;

File: dinky-admin/src/main/java/org/dinky/controller/SysConfigController.java
Patch:
@@ -19,7 +19,7 @@
 
 package org.dinky.controller;
 
-import org.dinky.data.annotation.Log;
+import org.dinky.data.annotations.Log;
 import org.dinky.data.enums.BusinessType;
 import org.dinky.data.enums.Status;
 import org.dinky.data.model.Configuration;

File: dinky-admin/src/main/java/org/dinky/controller/TaskController.java
Patch:
@@ -19,8 +19,8 @@
 
 package org.dinky.controller;
 
-import org.dinky.data.annotation.Log;
 import org.dinky.data.annotations.ExecuteProcess;
+import org.dinky.data.annotations.Log;
 import org.dinky.data.annotations.ProcessId;
 import org.dinky.data.dto.DebugDTO;
 import org.dinky.data.dto.TaskDTO;

File: dinky-admin/src/main/java/org/dinky/controller/TaskVersionController.java
Patch:
@@ -19,7 +19,7 @@
 
 package org.dinky.controller;
 
-import org.dinky.data.annotation.Log;
+import org.dinky.data.annotations.Log;
 import org.dinky.data.dto.TaskVersionHistoryDTO;
 import org.dinky.data.enums.BusinessType;
 import org.dinky.data.model.TaskVersion;

File: dinky-admin/src/main/java/org/dinky/controller/TenantController.java
Patch:
@@ -19,7 +19,7 @@
 
 package org.dinky.controller;
 
-import org.dinky.data.annotation.Log;
+import org.dinky.data.annotations.Log;
 import org.dinky.data.constant.PermissionConstants;
 import org.dinky.data.enums.BusinessType;
 import org.dinky.data.model.Tenant;

File: dinky-admin/src/main/java/org/dinky/controller/TokenController.java
Patch:
@@ -19,7 +19,7 @@
 
 package org.dinky.controller;
 
-import org.dinky.data.annotation.Log;
+import org.dinky.data.annotations.Log;
 import org.dinky.data.constant.PermissionConstants;
 import org.dinky.data.enums.BusinessType;
 import org.dinky.data.enums.Status;

File: dinky-admin/src/main/java/org/dinky/controller/UDFTemplateController.java
Patch:
@@ -19,7 +19,7 @@
 
 package org.dinky.controller;
 
-import org.dinky.data.annotation.Log;
+import org.dinky.data.annotations.Log;
 import org.dinky.data.constant.PermissionConstants;
 import org.dinky.data.enums.BusinessType;
 import org.dinky.data.enums.Status;

File: dinky-admin/src/main/java/org/dinky/controller/UserController.java
Patch:
@@ -20,7 +20,7 @@
 package org.dinky.controller;
 
 import org.dinky.assertion.Asserts;
-import org.dinky.data.annotation.Log;
+import org.dinky.data.annotations.Log;
 import org.dinky.data.constant.PermissionConstants;
 import org.dinky.data.dto.ModifyPasswordDTO;
 import org.dinky.data.enums.BusinessType;

File: dinky-admin/src/main/java/org/dinky/data/model/AlertRule.java
Patch:
@@ -36,7 +36,7 @@
 @Data
 @TableName("dinky_alert_rules")
 @ApiModel(value = "AlertRule", description = "AlertRule")
-public class AlertRule extends SuperEntity {
+public class AlertRule extends SuperEntity<AlertRule> {
 
     @ApiModelProperty(value = "rule", required = true, dataType = "String", example = "rule")
     String rule;

File: dinky-admin/src/main/java/org/dinky/service/task/BaseTask.java
Patch:
@@ -20,7 +20,7 @@
 package org.dinky.service.task;
 
 import org.dinky.config.Dialect;
-import org.dinky.data.annotation.SupportDialect;
+import org.dinky.data.annotations.SupportDialect;
 import org.dinky.data.dto.TaskDTO;
 import org.dinky.data.exception.NotSupportExplainExcepition;
 import org.dinky.data.result.SelectResult;

File: dinky-admin/src/main/java/org/dinky/service/task/CommonSqlTask.java
Patch:
@@ -20,7 +20,7 @@
 package org.dinky.service.task;
 
 import org.dinky.config.Dialect;
-import org.dinky.data.annotation.SupportDialect;
+import org.dinky.data.annotations.SupportDialect;
 import org.dinky.data.dto.SqlDTO;
 import org.dinky.data.dto.TaskDTO;
 import org.dinky.data.result.ResultPool;

File: dinky-admin/src/main/java/org/dinky/service/task/FlinkJarSqlTask.java
Patch:
@@ -20,7 +20,7 @@
 package org.dinky.service.task;
 
 import org.dinky.config.Dialect;
-import org.dinky.data.annotation.SupportDialect;
+import org.dinky.data.annotations.SupportDialect;
 import org.dinky.data.dto.TaskDTO;
 import org.dinky.data.result.SqlExplainResult;
 import org.dinky.job.JobResult;

File: dinky-admin/src/main/java/org/dinky/service/task/FlinkSqlTask.java
Patch:
@@ -20,7 +20,7 @@
 package org.dinky.service.task;
 
 import org.dinky.config.Dialect;
-import org.dinky.data.annotation.SupportDialect;
+import org.dinky.data.annotations.SupportDialect;
 import org.dinky.data.dto.TaskDTO;
 import org.dinky.data.result.SqlExplainResult;
 import org.dinky.job.JobManager;

File: dinky-admin/src/main/java/org/dinky/service/task/UdfTask.java
Patch:
@@ -20,7 +20,7 @@
 package org.dinky.service.task;
 
 import org.dinky.config.Dialect;
-import org.dinky.data.annotation.SupportDialect;
+import org.dinky.data.annotations.SupportDialect;
 import org.dinky.data.dto.TaskDTO;
 import org.dinky.data.model.Task;
 import org.dinky.function.FunctionFactory;

File: dinky-common/src/main/java/org/dinky/data/annotations/GaugeM.java
Patch:
@@ -17,7 +17,7 @@
  *
  */
 
-package org.dinky.data.annotation;
+package org.dinky.data.annotations;
 
 import java.lang.annotation.ElementType;
 import java.lang.annotation.Inherited;

File: dinky-common/src/main/java/org/dinky/data/metrics/Cpu.java
Patch:
@@ -19,7 +19,7 @@
 
 package org.dinky.data.metrics;
 
-import org.dinky.data.annotation.GaugeM;
+import org.dinky.data.annotations.GaugeM;
 
 import cn.hutool.core.bean.BeanUtil;
 import cn.hutool.system.oshi.CpuInfo;

File: dinky-common/src/main/java/org/dinky/data/metrics/Jvm.java
Patch:
@@ -19,7 +19,7 @@
 
 package org.dinky.data.metrics;
 
-import org.dinky.data.annotation.GaugeM;
+import org.dinky.data.annotations.GaugeM;
 
 import java.lang.management.ManagementFactory;
 import java.lang.management.MemoryUsage;

File: dinky-common/src/main/java/org/dinky/data/metrics/Mem.java
Patch:
@@ -19,7 +19,7 @@
 
 package org.dinky.data.metrics;
 
-import org.dinky.data.annotation.GaugeM;
+import org.dinky.data.annotations.GaugeM;
 
 import cn.hutool.system.oshi.OshiUtil;
 import lombok.AllArgsConstructor;

File: dinky-admin/src/main/java/org/dinky/service/resource/impl/ResourceServiceImpl.java
Patch:
@@ -169,7 +169,7 @@ public File getFile(Integer id) {
         Resources resources = getById(id);
         Assert.notNull(resources, () -> new BusException(Status.RESOURCE_DIR_OR_FILE_NOT_EXIST));
         Assert.isFalse(resources.getSize() > ALLOW_MAX_CAT_CONTENT_SIZE, () -> new BusException("file is too large!"));
-        return URLUtils.toFile("rs:" + resources.getFullName());
+        return URLUtils.toFile("rs://" + resources.getFullName());
     }
 
     @Transactional(rollbackFor = Exception.class)

File: dinky-function/src/main/java/org/dinky/function/util/UDFUtil.java
Patch:
@@ -394,7 +394,7 @@ public static List<String> getPythonUdfList(String pythonPath, String udfFile) {
                 continue;
             }
             Configuration configuration = new Configuration();
-            configuration.set(PythonOptions.PYTHON_FILES, udfFile + ".zip");
+            configuration.set(PythonOptions.PYTHON_FILES, udfFile);
             configuration.set(PythonOptions.PYTHON_CLIENT_EXECUTABLE, pythonPath);
             configuration.set(PythonOptions.PYTHON_EXECUTABLE, pythonPath);
             try {
@@ -413,7 +413,7 @@ private static List<String> execPyAndGetUdfNameList(String pyPath, String pyFile
             String shell =
                     StrUtil.join(" ", Arrays.asList(Opt.ofBlankAble(pyPath).orElse("python3"), pyFile, checkPyFile));
 
-            return StrUtil.split(RuntimeUtil.execForStr(shell), ",");
+            return StrUtil.split(StrUtil.trim(RuntimeUtil.execForStr(shell)), ",");
         } catch (Exception e) {
             throw new DinkyException(e);
         }

File: dinky-client/dinky-client-1.14/src/main/java/org/dinky/executor/CustomTableEnvironmentImpl.java
Patch:
@@ -360,7 +360,7 @@ public <T> void createTemporaryView(String path, DataStream<T> dataStream, Strin
     @Override
     public List<LineageRel> getLineage(String statement) {
         LineageContext lineageContext = new LineageContext((TableEnvironmentImpl) streamTableEnvironment);
-        return lineageContext.getLineage(statement);
+        return lineageContext.analyzeLineage(statement);
     }
 
     @Override

File: dinky-client/dinky-client-1.15/src/main/java/org/dinky/executor/CustomTableEnvironmentImpl.java
Patch:
@@ -319,7 +319,7 @@ public <T> void createTemporaryView(String path, DataStream<T> dataStream, Strin
     @Override
     public List<LineageRel> getLineage(String statement) {
         LineageContext lineageContext = new LineageContext((TableEnvironmentImpl) streamTableEnvironment);
-        return lineageContext.getLineage(statement);
+        return lineageContext.analyzeLineage(statement);
     }
 
     @Override

File: dinky-client/dinky-client-1.16/src/main/java/org/dinky/executor/CustomTableEnvironmentImpl.java
Patch:
@@ -39,7 +39,6 @@
 import org.apache.flink.table.api.ExplainDetail;
 import org.apache.flink.table.api.TableException;
 import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;
-import org.apache.flink.table.api.internal.TableEnvironmentImpl;
 import org.apache.flink.table.operations.CreateTableASOperation;
 import org.apache.flink.table.operations.ExplainOperation;
 import org.apache.flink.table.operations.ModifyOperation;
@@ -255,8 +254,8 @@ private void setConfiguration(StreamExecutionEnvironment environment, Map<String
 
     @Override
     public List<LineageRel> getLineage(String statement) {
-        LineageContext lineageContext = new LineageContext((TableEnvironmentImpl) streamTableEnvironment);
-        return lineageContext.getLineage(statement);
+        LineageContext lineageContext = new LineageContext(this);
+        return lineageContext.analyzeLineage(statement);
     }
 
     @Override

File: dinky-client/dinky-client-1.17/src/main/java/org/dinky/executor/CustomTableEnvironmentImpl.java
Patch:
@@ -40,7 +40,6 @@
 import org.apache.flink.table.api.ExplainFormat;
 import org.apache.flink.table.api.TableException;
 import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;
-import org.apache.flink.table.api.internal.TableEnvironmentImpl;
 import org.apache.flink.table.operations.CreateTableASOperation;
 import org.apache.flink.table.operations.ExplainOperation;
 import org.apache.flink.table.operations.ModifyOperation;
@@ -251,8 +250,8 @@ private void setConfiguration(StreamExecutionEnvironment environment, Map<String
 
     @Override
     public List<LineageRel> getLineage(String statement) {
-        LineageContext lineageContext = new LineageContext((TableEnvironmentImpl) streamTableEnvironment);
-        return lineageContext.getLineage(statement);
+        LineageContext lineageContext = new LineageContext(this);
+        return lineageContext.analyzeLineage(statement);
     }
 
     @Override

File: dinky-admin/src/main/java/org/dinky/controller/UDFController.java
Patch:
@@ -88,7 +88,7 @@ public Result<List<Resources>> udfResourcesList() {
      * @return Result
      */
     @PostMapping("/addOrUpdateByResourceId")
-    public Result<Void> saveOrUpdate(@RequestBody CommonDTO<List<Integer>> dto) {
+    public Result<Void> addOrUpdateByResourceId(@RequestBody CommonDTO<List<Integer>> dto) {
         udfService.addOrUpdateByResourceId(dto.getData());
         return Result.succeed();
     }

File: dinky-admin/src/main/java/org/dinky/data/dto/TaskRollbackVersionDTO.java
Patch:
@@ -31,7 +31,7 @@
 public class TaskRollbackVersionDTO implements Serializable {
 
     @ApiModelProperty(value = "ID", dataType = "Integer", example = "1", notes = "The identifier of the task")
-    private Integer id;
+    private Integer taskId;
 
     @ApiModelProperty(
             value = "Version ID",

File: dinky-admin/src/main/java/org/dinky/service/TaskVersionService.java
Patch:
@@ -38,6 +38,7 @@ public interface TaskVersionService extends ISuperService<TaskVersion> {
      * Create a snapshot of a task version.
      *
      * @param task A {@link TaskDTO} object representing the task to create a snapshot for.
+     * @return
      */
-    void createTaskVersionSnapshot(TaskDTO task);
+    Integer createTaskVersionSnapshot(TaskDTO task);
 }

File: dinky-cdc/dinky-cdc-core/src/main/java/org/dinky/cdc/SinkBuilderFactory.java
Patch:
@@ -20,6 +20,7 @@
 package org.dinky.cdc;
 
 import org.dinky.assertion.Asserts;
+import org.dinky.cdc.kafka.KafkaSinkBuilder;
 import org.dinky.cdc.sql.SQLSinkBuilder;
 import org.dinky.cdc.sql.catalog.SQLCatalogSinkBuilder;
 import org.dinky.data.model.FlinkCDCConfig;
@@ -59,6 +60,7 @@ private static Map<String, Supplier<SinkBuilder>> getPlusSinkBuilder() {
         Map<String, Supplier<SinkBuilder>> map = new HashMap<>();
         map.put(SQLSinkBuilder.KEY_WORD, SQLSinkBuilder::new);
         map.put(SQLCatalogSinkBuilder.KEY_WORD, SQLCatalogSinkBuilder::new);
+        map.put(KafkaSinkBuilder.KEY_WORD, KafkaSinkBuilder::new);
 
         final ServiceLoader<SinkBuilder> loader = ServiceLoader.load(SinkBuilder.class);
 

File: dinky-admin/src/main/java/org/dinky/controller/SchedulerController.java
Patch:
@@ -207,6 +207,9 @@ public Result<String> createTaskDefinition(
             return Result.failed(Status.DS_WORK_FLOW_DEFINITION_TASK_NAME_EXIST, processName, taskName);
         }
 
+        Long taskCode = taskClient.genTaskCode(projectCode);
+        taskRequest.setCode(taskCode);
+
         String taskDefinitionJsonObj = JSONUtil.toJsonStr(taskRequest);
         taskClient.createTaskDefinition(projectCode, process.getCode(), upstreamCodes, taskDefinitionJsonObj);
 

File: dinky-metadata/dinky-metadata-base/src/main/java/org/dinky/metadata/convert/ITypeConvert.java
Patch:
@@ -76,6 +76,7 @@ default Object convertValue(ResultSet results, String columnName, String javaTyp
             case "blob":
                 return results.getBlob(columnName);
             case "boolean":
+            case "bool":
             case "bit":
                 return results.getBoolean(columnName);
             case "byte":

File: dinky-admin/src/main/java/org/dinky/context/ConsoleContextHolder.java
Patch:
@@ -195,7 +195,7 @@ public void finishedProcess(String processName, ProcessStatus status, Throwable
         if (e != null) {
             appendLog(processName, null, LogUtil.getError(e.getCause()), true);
         }
-        String filePath = String.format("%s/tmp/log/%s.json", System.getProperty("user.dir"), process.getTitle());
+        String filePath = String.format("%s/tmp/log/%s.json", System.getProperty("user.dir"), processName);
         if (FileUtil.exist(filePath)) {
             Assert.isTrue(FileUtil.del(filePath));
         }

File: dinky-admin/src/main/java/org/dinky/data/model/mapping/ClusterConfigurationMapping.java
Patch:
@@ -20,6 +20,7 @@
 package org.dinky.data.model.mapping;
 
 import org.dinky.data.model.ClusterConfiguration;
+import org.dinky.gateway.model.FlinkClusterConfig;
 
 import java.time.LocalDateTime;
 
@@ -75,7 +76,7 @@ public class ClusterConfigurationMapping {
             dataType = "String",
             example = "test",
             notes = "cluster config json")
-    private String configJson;
+    private FlinkClusterConfig configJson;
 
     @ApiModelProperty(
             value = "isAvailable",

File: dinky-admin/src/main/java/org/dinky/job/handler/JobRefreshHandler.java
Patch:
@@ -55,7 +55,6 @@
 import com.alibaba.fastjson2.JSON;
 import com.fasterxml.jackson.databind.JsonNode;
 
-import cn.hutool.json.JSONObject;
 import cn.hutool.json.JSONUtil;
 import lombok.RequiredArgsConstructor;
 import lombok.extern.slf4j.Slf4j;
@@ -249,8 +248,8 @@ private static void handleJobDone(JobInfoDetail jobInfoDetail) {
 
         if (GatewayType.isDeployCluster(clusterType)) {
             JobConfig jobConfig = new JobConfig();
-            String configJson = jobDataDto.getClusterConfiguration().getConfigJson();
-            jobConfig.buildGatewayConfig(new JSONObject(configJson).toBean(FlinkClusterConfig.class));
+            FlinkClusterConfig configJson = jobDataDto.getClusterConfiguration().getConfigJson();
+            jobConfig.buildGatewayConfig(configJson);
             jobConfig.getGatewayConfig().setType(GatewayType.get(clusterType));
             jobConfig.getGatewayConfig().getFlinkConfig().setJobName(jobInstance.getName());
             Gateway.build(jobConfig.getGatewayConfig()).onJobFinishCallback(jobInstance.getStatus());

File: dinky-admin/src/main/java/org/dinky/service/TaskService.java
Patch:
@@ -26,6 +26,7 @@
 import org.dinky.data.enums.JobLifeCycle;
 import org.dinky.data.exception.ExcuteException;
 import org.dinky.data.exception.NotSupportExplainExcepition;
+import org.dinky.data.exception.SqlExplainExcepition;
 import org.dinky.data.model.JobModelOverview;
 import org.dinky.data.model.JobTypeOverView;
 import org.dinky.data.model.Task;
@@ -162,7 +163,7 @@ public interface TaskService extends ISuperService<Task> {
      * @param lifeCycle The new life cycle of the task.
      * @return true if the life cycle is successfully changed, false otherwise.
      */
-    boolean changeTaskLifeRecyle(Integer taskId, JobLifeCycle lifeCycle);
+    boolean changeTaskLifeRecyle(Integer taskId, JobLifeCycle lifeCycle) throws SqlExplainExcepition;
 
     /**
      * Save or update the given task.

File: dinky-app/dinky-app-base/src/main/java/org/dinky/app/flinksql/Submitter.java
Patch:
@@ -103,7 +103,7 @@ public static void submit(AppParamConfig config) throws SQLException {
     public static String buildSql(AppTask appTask) throws SQLException {
         StringBuilder sb = new StringBuilder();
         // build env task
-        if (Asserts.isNotNull(appTask.getEnvId())) {
+        if (Asserts.isNotNull(appTask.getEnvId()) && appTask.getEnvId() > 0) {
             AppTask envTask = DBUtil.getTask(appTask.getEnvId());
             if (Asserts.isNotNullString(envTask.getStatement())) {
                 log.info("use statement is enable, load env:{}", envTask.getName());

File: dinky-admin/src/main/java/org/dinky/aop/ProcessAspect.java
Patch:
@@ -101,7 +101,7 @@ public Object processStepAround(ProceedingJoinPoint joinPoint, ProcessStep proce
         ProcessStepType processStepType = processStep.type();
         ProcessStepEntity step = contextHolder.registerProcessStep(processStepType, MDC.get(PROCESS_NAME), parentStep);
         MDC.put(PROCESS_STEP, step.getKey());
-        contextHolder.appendLog(processName, step.getKey(), "Start Process Step:" + step.getType());
+        contextHolder.appendLog(processName, step.getKey(), "Start Process Step:" + step.getType(), true);
 
         try {
             result = joinPoint.proceed();

File: dinky-admin/src/main/java/org/dinky/job/handler/JobRefreshHandler.java
Patch:
@@ -228,9 +228,10 @@ private static JobStatus getJobStatus(JobInfoDetail jobInfoDetail) {
                 Gateway gateway = Gateway.build(gatewayConfig);
                 return gateway.getJobStatusById(appId);
             } catch (NotSupportGetStatusException ignored) {
+                // if the gateway does not support get status, then use the api to get job status
+                // ignore to do something here
             }
         }
-
         JobDataDto jobDataDto = jobInfoDetail.getJobDataDto();
         String status = jobDataDto.getJob().getState();
         return JobStatus.get(status);

File: dinky-admin/src/main/java/org/dinky/sse/LogSseAppender.java
Patch:
@@ -69,7 +69,7 @@ public void append(LogEvent event) {
             String processName = contextData.getValue(ProcessAspect.PROCESS_NAME);
             String processStepPid = contextData.getValue(ProcessAspect.PROCESS_STEP);
             String log = getStringLayout().toSerializable(event);
-            ConsoleContextHolder.getInstances().appendLog(processName, processStepPid, log);
+            ConsoleContextHolder.getInstances().appendLog(processName, processStepPid, log, true);
         }
     }
 

File: dinky-gateway/src/main/java/org/dinky/gateway/AbstractGateway.java
Patch:
@@ -25,6 +25,7 @@
 import org.dinky.gateway.enums.ActionType;
 import org.dinky.gateway.enums.GatewayType;
 import org.dinky.gateway.exception.GatewayException;
+import org.dinky.gateway.exception.NotSupportGetStatusException;
 import org.dinky.gateway.model.JobInfo;
 import org.dinky.gateway.result.GatewayResult;
 import org.dinky.gateway.result.SavePointResult;
@@ -172,7 +173,7 @@ protected <T> SavePointResult runClusterSavePointResult(
 
     @Override
     public JobStatus getJobStatusById(String id) {
-        return JobStatus.UNKNOWN;
+        throw new NotSupportGetStatusException(StrFormatter.format("{} is not support get status.", getType()));
     }
 
     @Override

File: dinky-gateway/src/main/java/org/dinky/gateway/result/GatewayResult.java
Patch:
@@ -30,7 +30,7 @@ public interface GatewayResult {
 
     String getId();
 
-    void setId(String id);
+    GatewayResult setId(String id);
 
     String getWebURL();
 

File: dinky-gateway/src/main/java/org/dinky/gateway/result/SavePointResult.java
Patch:
@@ -69,8 +69,9 @@ public String getId() {
     }
 
     @Override
-    public void setId(String id) {
+    public GatewayResult setId(String id) {
         this.appId = id;
+        return this;
     }
 
     @Override

File: dinky-gateway/src/main/java/org/dinky/gateway/result/YarnResult.java
Patch:
@@ -71,8 +71,9 @@ public String getId() {
     }
 
     @Override
-    public void setId(String id) {
+    public GatewayResult setId(String id) {
         this.appId = id;
+        return this;
     }
 
     public String getWebURL() {

File: dinky-admin/src/main/java/org/dinky/controller/MenuController.java
Patch:
@@ -72,7 +72,7 @@ public class MenuController {
             dataType = "MenuDTO",
             paramType = "body",
             required = true,
-            dataTypeClass = Menu.class)
+            dataTypeClass = MenuDTO.class)
     @SaCheckPermission(
             value = {
                 PermissionConstants.AUTH_MENU_ADD_ROOT,

File: dinky-admin/src/main/java/org/dinky/controller/RoleController.java
Patch:
@@ -79,7 +79,7 @@ public class RoleController {
             required = true,
             dataType = "RoleDTO",
             paramType = "body",
-            dataTypeClass = Role.class)
+            dataTypeClass = RoleDTO.class)
     @SaCheckPermission(
             value = {PermissionConstants.AUTH_ROLE_ADD, PermissionConstants.AUTH_ROLE_EDIT},
             mode = SaMode.OR)

File: dinky-admin/src/main/java/org/dinky/data/dto/MenuDTO.java
Patch:
@@ -45,6 +45,9 @@
 @ApiModel(value = "MenuDTO", description = "API Menu Data Transfer Object")
 public class MenuDTO {
 
+    @ApiModelProperty(value = "ID", dataType = "Integer", example = "1", notes = "Unique identifier for the menu")
+    private Integer id;
+
     @ApiModelProperty(value = "Parent ID", dataType = "Integer", example = "0", notes = "ID of the parent menu")
     private Integer parentId;
 

File: dinky-admin/src/main/java/org/dinky/data/dto/ClusterInstanceDTO.java
Patch:
@@ -42,6 +42,9 @@
 @ApiModel(value = "ClusterInstanceDTO", description = "API Cluster Instance Data Transfer Object")
 public class ClusterInstanceDTO {
 
+    @ApiModelProperty(value = "id", required = true, dataType = "Integer", example = "id")
+    private Integer id;
+
     @ApiModelProperty(value = "Name", required = true, dataType = "String", example = "Name")
     private String name;
 

File: dinky-admin/src/main/java/org/dinky/configure/Knife4jConfig.java
Patch:
@@ -19,11 +19,12 @@
 
 package org.dinky.configure;
 
+import org.dinky.DinkyVersion;
+
 import java.util.ArrayList;
 import java.util.Collection;
 import java.util.List;
 
-import org.springframework.beans.factory.annotation.Value;
 import org.springframework.boot.actuate.autoconfigure.endpoint.web.CorsEndpointProperties;
 import org.springframework.boot.actuate.autoconfigure.endpoint.web.WebEndpointProperties;
 import org.springframework.boot.actuate.autoconfigure.web.server.ManagementPortType;
@@ -61,8 +62,7 @@
 @Import(BeanValidatorPluginsConfiguration.class)
 public class Knife4jConfig {
 
-    @Value("${dinky.version}")
-    private String dinkyVersion;
+    private final String dinkyVersion = DinkyVersion.getVersion();
 
     private final OpenApiExtensionResolver openApiExtensionResolver;
 

File: dinky-admin/src/main/java/org/dinky/service/impl/TaskServiceImpl.java
Patch:
@@ -256,8 +256,6 @@ public String buildEnvSql(AbstractStatementDTO task) {
     @Override
     @ProcessStep(type = ProcessStepType.SUBMIT_TASK)
     public JobResult submitTask(Integer id, String savePointPath) throws Exception {
-        initTenantByTaskId(id);
-
         TaskDTO taskDTO = this.getTaskInfoById(id);
 
         if (StringUtils.isNotBlank(savePointPath)) {

File: dinky-core/src/main/java/org/dinky/job/JobResult.java
Patch:
@@ -128,7 +128,7 @@ public JobResult(
         this.jobConfig = jobConfig;
         this.jobManagerAddress = jobManagerAddress;
         this.status = status;
-        this.success = (status == (Job.JobStatus.SUCCESS)) ? true : false;
+        this.success = status == (Job.JobStatus.SUCCESS);
         this.statement = statement;
         this.jobId = jobId;
         this.error = error;

File: dinky-executor/src/main/java/org/dinky/trans/ddl/CreateCDCSourceOperation.java
Patch:
@@ -105,7 +105,7 @@ public TableResult build(Executor executor) {
             final List<String> schemaTableNameList = new ArrayList<>();
             if (SplitUtil.isEnabled(cdcSource.getSplit())) {
                 DriverConfig driverConfig = DriverConfig.build(cdcBuilder.parseMetaDataConfig());
-                Driver driver = Driver.build(driverConfig);
+                Driver driver = Driver.buildNewConnection(driverConfig);
 
                 // 这直接传正则过去
                 schemaTableNameList.addAll(tableRegList.stream()

File: dinky-admin/src/main/java/org/dinky/utils/MavenUtil.java
Patch:
@@ -52,7 +52,7 @@
 @Slf4j
 public class MavenUtil {
     static final String javaExecutor = FileUtil.file(
-                    FileUtil.file(SystemUtil.getJavaRuntimeInfo().getHomeDir()).getParentFile(), "/bin/java")
+                    FileUtil.file(SystemUtil.getJavaRuntimeInfo().getHomeDir()), "/bin/java")
             .getAbsolutePath();
     private static final String EXECTOR = SystemUtil.getOsInfo().isWindows() ? "mvn.cmd" : "mvn";
 

File: dinky-admin/src/main/java/org/dinky/service/impl/StudioServiceImpl.java
Patch:
@@ -88,7 +88,7 @@ private IResult executeMSFlinkSql(StudioMetaStoreDTO studioMetaStoreDTO) {
 
     @Override
     public JdbcSelectResult getCommonSqlData(Integer taskId) {
-        return ResultPool.getCommonSqlCache(taskId);
+        return (JdbcSelectResult) ResultPool.getCommonSqlCache(taskId);
     }
 
     @Override

File: dinky-admin/src/main/java/org/dinky/service/impl/TaskServiceImpl.java
Patch:
@@ -325,6 +325,9 @@ public JobResult restartTask(Integer id, String savePointPath) throws Exception
 
     @Override
     public boolean cancelTaskJob(TaskDTO task) {
+        if (Dialect.isCommonSql(task.getDialect())) {
+            return true;
+        }
         JobInstance jobInstance = jobInstanceService.getById(task.getJobInstanceId());
         Assert.notNull(jobInstance, Status.JOB_INSTANCE_NOT_EXIST.getMessage());
         ClusterInstance clusterInstance = clusterInstanceService.getById(jobInstance.getClusterId());

File: dinky-client/dinky-client-1.18/src/main/java/org/apache/calcite/rel/metadata/RelMdColumnOrigins.java
Patch:
@@ -288,7 +288,7 @@ public RexNode visitLocalRef(RexLocalRef localRef) {
                         .get(iOutputColumn)
                         .getName();
                 List<String> fieldList =
-                        table.catalogTable().getResolvedSchema().getColumnNames();
+                        table.contextResolvedTable().getResolvedSchema().getColumnNames();
 
                 int index = -1;
                 for (int i = 0; i < fieldList.size(); i++) {

File: dinky-client/dinky-client-1.18/src/main/java/org/dinky/executor/TableSchemaField.java
Patch:
@@ -21,8 +21,9 @@
 
 import org.apache.flink.table.types.DataType;
 
-/** @since 2021/6/7 22:06 */
+/** @since 2022/11/04 */
 public class TableSchemaField {
+
     private String name;
     private DataType type;
 

File: dinky-admin/src/main/java/org/dinky/context/SseSessionContextHolder.java
Patch:
@@ -67,13 +67,13 @@ public static SseEmitter connectSession(String sessionKey) {
             log.warn("Session key already exists: {}", sessionKey);
             closeSse(sessionKey);
         }
-        SseEmitter sseEmitter = new SseEmitter(60 * 1000L);
+        SseEmitter sseEmitter = new SseEmitter(60 * 1000L * 10);
         sseEmitter.onError(err -> onError(sessionKey, err));
         sseEmitter.onTimeout(() -> onTimeout(sessionKey));
         sseEmitter.onCompletion(() -> onCompletion(sessionKey));
         try {
             // Set the client reconnection interval, 0 to reconnect immediately
-            sseEmitter.send(SseEmitter.event().reconnectTime(0));
+            sseEmitter.send(SseEmitter.event().reconnectTime(1000));
         } catch (IOException e) {
             throw new RuntimeException(e);
         }

File: dinky-admin/src/main/java/org/dinky/service/impl/TaskServiceImpl.java
Patch:
@@ -160,8 +160,6 @@ private String[] buildParams(int id) {
                 .url(dsProperties.getUrl())
                 .username(dsProperties.getUsername())
                 .password(dsProperties.getPassword())
-                .dinkyAddr(SystemConfiguration.getInstances().getDinkyAddr().getValue())
-                .split(SystemConfiguration.getInstances().getSqlSeparator())
                 .build();
         String encodeParam = Base64.getEncoder()
                 .encodeToString(JsonUtils.toJsonString(appParamConfig).getBytes());
@@ -179,7 +177,7 @@ public void preCheckTask(TaskDTO task) throws TaskNotDoneException, SqlExplainEx
                 && task.getJobInstanceId() > 0) {
             JobInstance jobInstance = jobInstanceService.getById(task.getJobInstanceId());
             if (jobInstance != null && !JobStatus.isDone(jobInstance.getStatus())) {
-                throw new TaskNotDoneException(Status.TASK_STATUS_IS_NOT_DONE.getMessage());
+                throw new BusException(Status.TASK_STATUS_IS_NOT_DONE.getMessage());
             }
         }
 

File: dinky-common/src/main/java/org/dinky/data/enums/Status.java
Patch:
@@ -369,6 +369,7 @@ public enum Status {
     PROCESS_SUBMIT_BUILDCONFIG(193, "process.submit.buildConfig"),
     PROCESS_SUBMIT_EXECUTECOMMSQL(194, "process.submit.execute.commSql"),
     PROCESS_SUBMIT_EXECUTEFLINKSQL(195, "process.submit.execute.flinkSql"),
+    PROCESS_REGISTER_EXITS(196, "process.register.exits"),
     ;
     private final int code;
     private final String key;

File: dinky-admin/src/main/java/org/dinky/data/enums/BusinessType.java
Patch:
@@ -36,6 +36,9 @@ public enum BusinessType {
     /** 提交 */
     SUBMIT,
 
+    /** Debug */
+    DEBUG,
+
     /** 执行 */
     EXECUTE,
 

File: dinky-common/src/main/java/org/dinky/data/enums/ProcessStepType.java
Patch:
@@ -26,6 +26,7 @@
 @Getter
 public enum ProcessStepType {
     SUBMIT_TASK("SUBMIT_TASK", Status.PROCESS_SUBMIT_SUBMITTASK),
+    SUBMIT_DEBUG("DEBUG_TASK", Status.PROCESS_SUBMIT_SUBMITTASK),
     SUBMIT_PRECHECK("SUBMIT_PRECHECK", Status.PROCESS_SUBMIT_CHECKSQL),
     SUBMIT_EXECUTE("SUBMIT_EXECUTE", Status.PROCESS_SUBMIT_EXECUTE),
     SUBMIT_BUILD_CONFIG("SUBMIT_BUILD_CONFIG", Status.PROCESS_SUBMIT_BUILDCONFIG),

File: dinky-common/src/main/java/org/dinky/data/enums/Status.java
Patch:
@@ -85,7 +85,8 @@ public enum Status {
     MOVE_FAILED(9028, "move.failed"),
     TEST_CONNECTION_SUCCESS(9029, "test.connection.success"),
     TEST_CONNECTION_FAILED(9030, "test.connection.failed"),
-
+    DEBUG_SUCCESS(9031, "debug.success"),
+    DEBUG_FAILED(9032, "debug.failed"),
     /**
      * user,tenant,role
      */

File: dinky-admin/src/main/java/org/dinky/aop/UdfClassLoaderAspect.java
Patch:
@@ -21,8 +21,8 @@
 
 import org.dinky.classloader.DinkyClassLoader;
 import org.dinky.context.DinkyClassLoaderContextHolder;
+import org.dinky.data.exception.DinkyException;
 import org.dinky.job.JobResult;
-import org.dinky.process.exception.DinkyException;
 
 import org.aspectj.lang.ProceedingJoinPoint;
 import org.aspectj.lang.annotation.Around;

File: dinky-admin/src/main/java/org/dinky/context/SseSessionContextHolder.java
Patch:
@@ -62,7 +62,7 @@ public static Set<String> subscribeTopic(String sessionId, List<String> topics)
      * @return The SseEmitter for the session.
      */
     public static SseEmitter connectSession(String sessionKey) {
-        log.info("New session wants to connect: {}", sessionKey);
+        log.debug("New session wants to connect: {}", sessionKey);
         if (exists(sessionKey)) {
             log.warn("Session key already exists: {}", sessionKey);
             closeSse(sessionKey);
@@ -97,7 +97,7 @@ public static boolean exists(String sessionKey) {
      * @param sessionKey The session key of the timed-out session.
      */
     public static void onTimeout(String sessionKey) {
-        log.info("Type: SseSession Timeout, Session ID: {}", sessionKey);
+        log.debug("Type: SseSession Timeout, Session ID: {}", sessionKey);
         closeSse(sessionKey);
     }
 
@@ -142,7 +142,7 @@ public static void onError(String sessionKey, Throwable throwable) {
      * @param sessionKey The session key of the completed session.
      */
     public static void onCompletion(String sessionKey) {
-        log.info("Type: SseSession Completion, Session ID: {}", sessionKey);
+        log.debug("Type: SseSession Completion, Session ID: {}", sessionKey);
         closeSse(sessionKey);
     }
 

File: dinky-admin/src/main/java/org/dinky/controller/ProcessController.java
Patch:
@@ -20,9 +20,9 @@
 package org.dinky.controller;
 
 import org.dinky.context.ConsoleContextHolder;
+import org.dinky.data.model.ProcessEntity;
 import org.dinky.data.result.ProTableResult;
 import org.dinky.data.result.Result;
-import org.dinky.process.model.ProcessEntity;
 
 import org.springframework.web.bind.annotation.GetMapping;
 import org.springframework.web.bind.annotation.RequestMapping;

File: dinky-admin/src/main/java/org/dinky/controller/TaskController.java
Patch:
@@ -20,10 +20,13 @@
 package org.dinky.controller;
 
 import org.dinky.data.annotation.Log;
+import org.dinky.data.annotations.ExecuteProcess;
+import org.dinky.data.annotations.ProcessId;
 import org.dinky.data.dto.TaskDTO;
 import org.dinky.data.dto.TaskRollbackVersionDTO;
 import org.dinky.data.enums.BusinessType;
 import org.dinky.data.enums.JobLifeCycle;
+import org.dinky.data.enums.ProcessType;
 import org.dinky.data.enums.Status;
 import org.dinky.data.exception.NotSupportExplainExcepition;
 import org.dinky.data.model.Task;
@@ -33,9 +36,6 @@
 import org.dinky.gateway.enums.SavePointType;
 import org.dinky.gateway.result.SavePointResult;
 import org.dinky.job.JobResult;
-import org.dinky.process.annotations.ExecuteProcess;
-import org.dinky.process.annotations.ProcessId;
-import org.dinky.process.enums.ProcessType;
 import org.dinky.service.TaskService;
 
 import java.util.List;

File: dinky-admin/src/main/java/org/dinky/data/exception/SqlExplainExcepition.java
Patch:
@@ -19,8 +19,6 @@
 
 package org.dinky.data.exception;
 
-import org.dinky.process.exception.ExcuteException;
-
 public class SqlExplainExcepition extends ExcuteException {
     public SqlExplainExcepition(String message) {
         super(message);

File: dinky-admin/src/main/java/org/dinky/data/exception/TaskNotDoneException.java
Patch:
@@ -19,8 +19,6 @@
 
 package org.dinky.data.exception;
 
-import org.dinky.process.exception.ExcuteException;
-
 public class TaskNotDoneException extends ExcuteException {
     public TaskNotDoneException(String message) {
         super(message);

File: dinky-admin/src/main/java/org/dinky/init/SystemInit.java
Patch:
@@ -25,6 +25,7 @@
 import org.dinky.context.TenantContextHolder;
 import org.dinky.daemon.task.DaemonFactory;
 import org.dinky.daemon.task.DaemonTaskConfig;
+import org.dinky.data.exception.DinkyException;
 import org.dinky.data.model.JobInstance;
 import org.dinky.data.model.SystemConfiguration;
 import org.dinky.data.model.Task;
@@ -33,7 +34,6 @@
 import org.dinky.function.constant.PathConstant;
 import org.dinky.function.pool.UdfCodePool;
 import org.dinky.job.FlinkJobTask;
-import org.dinky.process.exception.DinkyException;
 import org.dinky.scheduler.client.ProjectClient;
 import org.dinky.scheduler.exception.SchedulerException;
 import org.dinky.scheduler.model.Project;

File: dinky-admin/src/main/java/org/dinky/service/TaskService.java
Patch:
@@ -23,6 +23,7 @@
 import org.dinky.data.dto.TaskDTO;
 import org.dinky.data.dto.TaskRollbackVersionDTO;
 import org.dinky.data.enums.JobLifeCycle;
+import org.dinky.data.exception.ExcuteException;
 import org.dinky.data.exception.NotSupportExplainExcepition;
 import org.dinky.data.model.JobModelOverview;
 import org.dinky.data.model.JobTypeOverView;
@@ -33,7 +34,6 @@
 import org.dinky.gateway.result.SavePointResult;
 import org.dinky.job.JobResult;
 import org.dinky.mybatis.service.ISuperService;
-import org.dinky.process.exception.ExcuteException;
 
 import java.util.List;
 

File: dinky-admin/src/main/java/org/dinky/service/impl/ClusterInstanceServiceImpl.java
Patch:
@@ -68,6 +68,7 @@ public FlinkClusterInfo checkHeartBeat(String hosts, String host) {
 
     @Override
     public String getJobManagerAddress(ClusterInstance clusterInstance) {
+        // TODO 这里判空逻辑有问题，clusterInstance有可能为null
         Assert.check(clusterInstance);
         FlinkClusterInfo info =
                 FlinkCluster.testFlinkJobManagerIP(clusterInstance.getHosts(), clusterInstance.getJobManagerHost());

File: dinky-admin/src/main/java/org/dinky/service/impl/DataBaseServiceImpl.java
Patch:
@@ -20,9 +20,11 @@
 package org.dinky.service.impl;
 
 import org.dinky.assertion.Asserts;
+import org.dinky.data.annotations.ProcessStep;
 import org.dinky.data.constant.CommonConstant;
 import org.dinky.data.dto.SqlDTO;
 import org.dinky.data.dto.TaskDTO;
+import org.dinky.data.enums.ProcessStepType;
 import org.dinky.data.enums.Status;
 import org.dinky.data.model.Column;
 import org.dinky.data.model.DataBase;
@@ -36,8 +38,6 @@
 import org.dinky.metadata.driver.Driver;
 import org.dinky.metadata.result.JdbcSelectResult;
 import org.dinky.mybatis.service.impl.SuperServiceImpl;
-import org.dinky.process.annotations.ProcessStep;
-import org.dinky.process.enums.ProcessStepType;
 import org.dinky.service.DataBaseService;
 
 import org.apache.commons.lang3.StringUtils;

File: dinky-admin/src/main/java/org/dinky/service/impl/GitProjectServiceImpl.java
Patch:
@@ -22,12 +22,12 @@
 import org.dinky.data.dto.GitAnalysisJarDTO;
 import org.dinky.data.dto.GitProjectDTO;
 import org.dinky.data.dto.TreeNodeDTO;
+import org.dinky.data.exception.DinkyException;
 import org.dinky.data.model.GitProject;
 import org.dinky.data.params.GitProjectSortJarParams;
 import org.dinky.function.pool.UdfCodePool;
 import org.dinky.mapper.GitProjectMapper;
 import org.dinky.mybatis.service.impl.SuperServiceImpl;
-import org.dinky.process.exception.DinkyException;
 import org.dinky.service.GitProjectService;
 import org.dinky.utils.GitRepository;
 import org.dinky.utils.TreeUtil;

File: dinky-admin/src/main/java/org/dinky/service/impl/MonitorServiceImpl.java
Patch:
@@ -20,11 +20,11 @@
 package org.dinky.service.impl;
 
 import org.dinky.data.dto.MetricsLayoutDTO;
+import org.dinky.data.exception.DinkyException;
 import org.dinky.data.metrics.Jvm;
 import org.dinky.data.model.Metrics;
 import org.dinky.data.vo.MetricsVO;
 import org.dinky.mapper.MetricsMapper;
-import org.dinky.process.exception.DinkyException;
 import org.dinky.service.MonitorService;
 import org.dinky.utils.PaimonUtil;
 

File: dinky-admin/src/main/java/org/dinky/service/impl/TaskServiceImpl.java
Patch:
@@ -22,12 +22,14 @@
 import org.dinky.assertion.Asserts;
 import org.dinky.config.Dialect;
 import org.dinky.context.TenantContextHolder;
+import org.dinky.data.annotations.ProcessStep;
 import org.dinky.data.constant.CommonConstant;
 import org.dinky.data.dto.AbstractStatementDTO;
 import org.dinky.data.dto.TaskDTO;
 import org.dinky.data.dto.TaskRollbackVersionDTO;
 import org.dinky.data.enums.JobLifeCycle;
 import org.dinky.data.enums.JobStatus;
+import org.dinky.data.enums.ProcessStepType;
 import org.dinky.data.enums.Status;
 import org.dinky.data.exception.BusException;
 import org.dinky.data.exception.NotSupportExplainExcepition;
@@ -65,8 +67,6 @@
 import org.dinky.job.JobResult;
 import org.dinky.mapper.TaskMapper;
 import org.dinky.mybatis.service.impl.SuperServiceImpl;
-import org.dinky.process.annotations.ProcessStep;
-import org.dinky.process.enums.ProcessStepType;
 import org.dinky.service.AlertGroupService;
 import org.dinky.service.CatalogueService;
 import org.dinky.service.ClusterConfigurationService;
@@ -238,7 +238,7 @@ public String buildEnvSql(AbstractStatementDTO task) {
             task.setVariables(fragmentVariableService.listEnabledVariables());
         }
         int envId = Optional.ofNullable(task.getEnvId()).orElse(-1);
-        if (envId != -1) {
+        if (envId >= 0) {
             TaskDTO envTask = this.getTaskInfoById(task.getEnvId());
             if (Asserts.isNotNull(envTask) && Asserts.isNotNullString(envTask.getStatement())) {
                 sql += envTask.getStatement() + CommonConstant.LineSep;

File: dinky-admin/src/main/java/org/dinky/service/resource/impl/OssResourceManager.java
Patch:
@@ -20,7 +20,7 @@
 package org.dinky.service.resource.impl;
 
 import org.dinky.data.exception.BusException;
-import org.dinky.process.exception.DinkyException;
+import org.dinky.data.exception.DinkyException;
 import org.dinky.service.resource.BaseResourceManager;
 import org.dinky.utils.OssTemplate;
 

File: dinky-admin/src/main/java/org/dinky/sse/git/AnalysisUdfClassStepSse.java
Patch:
@@ -20,9 +20,9 @@
 package org.dinky.sse.git;
 
 import org.dinky.data.dto.GitAnalysisJarDTO;
+import org.dinky.data.exception.DinkyException;
 import org.dinky.data.model.GitProject;
 import org.dinky.function.util.UDFUtil;
-import org.dinky.process.exception.DinkyException;
 import org.dinky.sse.StepSse;
 
 import java.io.File;

File: dinky-admin/src/main/java/org/dinky/sse/git/AnalysisUdfPythonStepSse.java
Patch:
@@ -20,10 +20,10 @@
 package org.dinky.sse.git;
 
 import org.dinky.data.dto.GitAnalysisJarDTO;
+import org.dinky.data.exception.DinkyException;
 import org.dinky.data.model.GitProject;
 import org.dinky.data.model.SystemConfiguration;
 import org.dinky.function.util.UDFUtil;
-import org.dinky.process.exception.DinkyException;
 import org.dinky.sse.StepSse;
 
 import java.io.File;

File: dinky-admin/src/main/java/org/dinky/utils/GitProjectStepSseFactory.java
Patch:
@@ -20,9 +20,9 @@
 package org.dinky.utils;
 
 import org.dinky.context.GitBuildContextHolder;
+import org.dinky.data.exception.DinkyException;
 import org.dinky.data.model.GitProject;
 import org.dinky.data.result.StepResult;
-import org.dinky.process.exception.DinkyException;
 import org.dinky.sse.DoneStepSse;
 import org.dinky.sse.StepSse;
 import org.dinky.sse.git.AnalysisUdfClassStepSse;

File: dinky-admin/src/main/java/org/dinky/utils/GitRepository.java
Patch:
@@ -20,8 +20,8 @@
 package org.dinky.utils;
 
 import org.dinky.data.dto.GitProjectDTO;
+import org.dinky.data.exception.DinkyException;
 import org.dinky.function.constant.PathConstant;
-import org.dinky.process.exception.DinkyException;
 
 import java.io.File;
 import java.io.StringWriter;

File: dinky-admin/src/main/java/org/dinky/utils/MavenUtil.java
Patch:
@@ -19,9 +19,9 @@
 
 package org.dinky.utils;
 
+import org.dinky.data.exception.DinkyException;
 import org.dinky.data.model.SystemConfiguration;
 import org.dinky.function.constant.PathConstant;
-import org.dinky.process.exception.DinkyException;
 
 import java.io.File;
 import java.io.IOException;

File: dinky-common/src/main/java/org/dinky/data/annotations/ExecuteProcess.java
Patch:
@@ -17,9 +17,9 @@
  *
  */
 
-package org.dinky.process.annotations;
+package org.dinky.data.annotations;
 
-import org.dinky.process.enums.ProcessType;
+import org.dinky.data.enums.ProcessType;
 
 import java.lang.annotation.ElementType;
 import java.lang.annotation.Inherited;

File: dinky-common/src/main/java/org/dinky/data/annotations/ProcessId.java
Patch:
@@ -17,7 +17,7 @@
  *
  */
 
-package org.dinky.process.annotations;
+package org.dinky.data.annotations;
 
 import java.lang.annotation.ElementType;
 import java.lang.annotation.Inherited;

File: dinky-common/src/main/java/org/dinky/data/annotations/ProcessStep.java
Patch:
@@ -17,9 +17,9 @@
  *
  */
 
-package org.dinky.process.annotations;
+package org.dinky.data.annotations;
 
-import org.dinky.process.enums.ProcessStepType;
+import org.dinky.data.enums.ProcessStepType;
 
 import java.lang.annotation.ElementType;
 import java.lang.annotation.Inherited;

File: dinky-common/src/main/java/org/dinky/data/enums/ProcessStatus.java
Patch:
@@ -17,7 +17,7 @@
  *
  */
 
-package org.dinky.process.enums;
+package org.dinky.data.enums;
 
 import org.dinky.assertion.Asserts;
 

File: dinky-common/src/main/java/org/dinky/data/enums/ProcessStepType.java
Patch:
@@ -17,10 +17,9 @@
  *
  */
 
-package org.dinky.process.enums;
+package org.dinky.data.enums;
 
 import org.dinky.assertion.Asserts;
-import org.dinky.data.enums.Status;
 
 import lombok.Getter;
 

File: dinky-common/src/main/java/org/dinky/data/enums/ProcessType.java
Patch:
@@ -17,7 +17,7 @@
  *
  */
 
-package org.dinky.process.enums;
+package org.dinky.data.enums;
 
 import org.dinky.assertion.Asserts;
 

File: dinky-common/src/main/java/org/dinky/data/exception/DinkyException.java
Patch:
@@ -17,7 +17,7 @@
  *
  */
 
-package org.dinky.process.exception;
+package org.dinky.data.exception;
 
 /** @since 0.7.0 */
 public class DinkyException extends RuntimeException {

File: dinky-common/src/main/java/org/dinky/data/exception/ExcuteException.java
Patch:
@@ -17,7 +17,7 @@
  *
  */
 
-package org.dinky.process.exception;
+package org.dinky.data.exception;
 
 public class ExcuteException extends Exception {
 

File: dinky-core/src/main/java/org/dinky/job/JobManager.java
Patch:
@@ -29,6 +29,9 @@
 import org.dinky.context.CustomTableEnvironmentContext;
 import org.dinky.context.FlinkUdfPathContextHolder;
 import org.dinky.context.RowLevelPermissionsContext;
+import org.dinky.data.annotations.ProcessStep;
+import org.dinky.data.enums.ProcessStepType;
+import org.dinky.data.exception.DinkyException;
 import org.dinky.data.model.FlinkUdfManifest;
 import org.dinky.data.model.SystemConfiguration;
 import org.dinky.data.result.ErrorResult;
@@ -58,9 +61,6 @@
 import org.dinky.interceptor.FlinkInterceptorResult;
 import org.dinky.parser.SqlType;
 import org.dinky.parser.check.AddJarSqlParser;
-import org.dinky.process.annotations.ProcessStep;
-import org.dinky.process.enums.ProcessStepType;
-import org.dinky.process.exception.DinkyException;
 import org.dinky.trans.ExecuteJarParseStrategy;
 import org.dinky.trans.Operations;
 import org.dinky.trans.dml.ExecuteJarOperation;

File: dinky-core/src/main/java/org/dinky/utils/DinkyClassLoaderUtil.java
Patch:
@@ -22,8 +22,8 @@
 import org.dinky.assertion.Asserts;
 import org.dinky.context.DinkyClassLoaderContextHolder;
 import org.dinky.context.FlinkUdfPathContextHolder;
+import org.dinky.data.exception.DinkyException;
 import org.dinky.job.JobConfig;
-import org.dinky.process.exception.DinkyException;
 
 import org.apache.flink.configuration.PipelineOptions;
 

File: dinky-executor/src/main/java/org/dinky/parser/check/AddJarSqlParser.java
Patch:
@@ -19,7 +19,7 @@
 
 package org.dinky.parser.check;
 
-import org.dinky.process.exception.DinkyException;
+import org.dinky.data.exception.DinkyException;
 import org.dinky.utils.URLUtils;
 
 import java.io.File;

File: dinky-function/src/main/java/org/dinky/function/util/UDFUtil.java
Patch:
@@ -23,6 +23,7 @@
 import org.dinky.config.Dialect;
 import org.dinky.context.DinkyClassLoaderContextHolder;
 import org.dinky.context.FlinkUdfPathContextHolder;
+import org.dinky.data.exception.DinkyException;
 import org.dinky.function.FunctionFactory;
 import org.dinky.function.compiler.CustomStringJavaCompiler;
 import org.dinky.function.compiler.CustomStringScalaCompiler;
@@ -32,7 +33,6 @@
 import org.dinky.gateway.enums.GatewayType;
 import org.dinky.pool.ClassEntity;
 import org.dinky.pool.ClassPool;
-import org.dinky.process.exception.DinkyException;
 
 import org.apache.flink.client.python.PythonFunctionFactory;
 import org.apache.flink.configuration.Configuration;

File: dinky-function/src/test/java/org/dinky/function/util/UDFUtilTest.java
Patch:
@@ -22,7 +22,7 @@
 import static org.junit.jupiter.api.Assertions.assertFalse;
 import static org.junit.jupiter.api.Assertions.assertTrue;
 
-import org.dinky.process.exception.DinkyException;
+import org.dinky.data.exception.DinkyException;
 
 import java.io.IOException;
 import java.io.InputStreamReader;

File: dinky-client/dinky-client-base/src/main/java/org/dinky/constant/CustomerConfigureOptions.java
Patch:
@@ -39,4 +39,7 @@ public class CustomerConfigureOptions {
 
     public static final ConfigOption<String> REST_FORMAT_TYPE =
             key("rest.formatType").stringType().defaultValue("DEFAULT").withDescription("for savepoint format type");
+
+    public static final ConfigOption<String> DINKY_HOST =
+            key("dinky.dinkyHost").stringType().noDefaultValue().withDescription("dinky local address");
 }

File: dinky-core/src/test/java/org/dinky/explainer/print_table/PrintStatementExplainerTest.java
Patch:
@@ -17,7 +17,7 @@
  *
  */
 
-package org.dinky.explainer.printTable;
+package org.dinky.explainer.print_table;
 
 import static org.junit.jupiter.api.Assertions.assertArrayEquals;
 

File: dinky-admin/src/main/java/org/dinky/job/FlinkJobTask.java
Patch:
@@ -55,6 +55,7 @@ public class FlinkJobTask implements DaemonTask {
     @Override
     public DaemonTask setConfig(DaemonTaskConfig config) {
         this.config = config;
+        TenantContextHolder.set(1);
         this.jobInfoDetail = jobInstanceService.getJobInfoDetail(config.getId());
         return this;
     }

File: dinky-admin/src/main/java/org/dinky/aop/ProcessAspect.java
Patch:
@@ -87,7 +87,7 @@ public Object processStepAround(ProceedingJoinPoint joinPoint, ProcessStep proce
         String parentStep = MDC.get(PROCESS_STEP);
         ProcessStepType processStepType = processStep.type();
         MDC.put(PROCESS_STEP, processStepType.getValue());
-        contextHolder.registerProcessStep(processStepType, MDC.get(PROCESS_NAME));
+        contextHolder.registerProcessStep(processStepType, MDC.get(PROCESS_NAME), parentStep);
 
         try {
             result = joinPoint.proceed();
@@ -120,7 +120,7 @@ private Object getProcessId(ProceedingJoinPoint joinPoint) {
             Annotation[] paramAnn = annotations[i];
             for (Annotation annotation : paramAnn) {
                 if (annotation instanceof ProcessId) {
-                    return param;
+                    return String.valueOf(param.hashCode());
                 }
             }
         }

File: dinky-core/src/main/java/org/dinky/explainer/lineage/LineageBuilder.java
Patch:
@@ -36,7 +36,7 @@
 public class LineageBuilder {
 
     public static LineageResult getColumnLineageByLogicalPlan(String statement) {
-        Explainer explainer = new Explainer(ExecutorFactory.getExecutor(), false);
+        Explainer explainer = new Explainer(ExecutorFactory.getDefaultExecutor(), false);
         List<LineageRel> lineageRelList = explainer.getLineage(statement);
         List<LineageRelation> relations = new ArrayList<>();
         Map<String, LineageTable> tableMap = new HashMap<>();

File: dinky-executor/src/main/java/org/dinky/executor/ExecutorFactory.java
Patch:
@@ -27,7 +27,9 @@
  **/
 public final class ExecutorFactory {
 
-    public static Executor getExecutor() {
+    private ExecutorFactory() {}
+
+    public static Executor getDefaultExecutor() {
         return new LocalStreamExecutor(ExecutorConfig.DEFAULT);
     }
 

File: dinky-executor/src/main/java/org/dinky/executor/LocalStreamExecutor.java
Patch:
@@ -30,7 +30,7 @@
 import cn.hutool.core.io.FileUtil;
 
 /**
- * LocalStreamExecuter
+ * LocalStreamExecutor
  *
  * @since 2021/5/25 13:48
  */

File: dinky-executor/src/test/java/org/dinky/interceptor/FlinkInterceptorTest.java
Patch:
@@ -38,7 +38,7 @@ public void replaceFragmentTest() {
         String statement = "nullif1:=NULLIF(1, 0) as val;\n"
                 + "nullif2:=NULLIF(0, 0) as val$null;\n"
                 + "select ${nullif1},${nullif2}";
-        String pretreatStatement = FlinkInterceptor.pretreatStatement(ExecutorFactory.getExecutor(), statement);
+        String pretreatStatement = FlinkInterceptor.pretreatStatement(ExecutorFactory.getDefaultExecutor(), statement);
         Assert.assertEquals("select NULLIF(1, 0) as val,NULLIF(0, 0) as val$null", pretreatStatement);
     }
 }

File: dinky-executor/src/main/java/org/dinky/constant/FlinkSQLConstant.java
Patch:
@@ -25,7 +25,6 @@
  * @since 2021/5/25 15:51
  */
 public interface FlinkSQLConstant {
-
     /** 分隔符 */
     String SEPARATOR = ";\n";
     /** DDL 类型 */
@@ -41,6 +40,9 @@ public interface FlinkSQLConstant {
     /** The define identifier of FlinkSQL Date Variable */
     String INNER_DATE_KEY = "_CURRENT_DATE_";
 
+    /** The define identifier of FlinkSQL Timestamp Variable */
+    String INNER_TIMESTAMP_KEY = "_CURRENT_TIMESTAMP_";
+
     /** 内置日期变量格式 确定后不能修改 */
     String INNER_DATE_FORMAT = "yyyy-MM-dd";
 }

File: dinky-admin/src/main/java/org/dinky/controller/ResourceController.java
Patch:
@@ -140,7 +140,6 @@ public Result<Void> uploadFile(Integer pid, String desc, @RequestParam("file") M
     @ApiImplicitParam(name = "id", value = "Resource ID", required = true, dataType = "Integer", paramType = "query")
     @SaCheckPermission(PermissionConstants.REGISTRATION_RESOURCE_DELETE)
     public Result<Void> remove(Integer id) {
-        resourcesService.remove(id);
-        return Result.succeed();
+        return resourcesService.remove(id) ? Result.succeed() : Result.failed();
     }
 }

File: dinky-admin/src/main/java/org/dinky/service/resource/ResourcesService.java
Patch:
@@ -86,7 +86,7 @@ public interface ResourcesService extends IService<Resources> {
      */
     void uploadFile(Integer pid, String desc, MultipartFile file);
 
-    void remove(Integer id);
+    boolean remove(Integer id);
 
     /**
      * 递归获取所有的资源，从pid到0

File: dinky-admin/src/main/java/org/dinky/aop/LogAspect.java
Patch:
@@ -117,7 +117,6 @@ protected void handleCommonLogic(final JoinPoint joinPoint, final Exception e, O
 
             if (e != null) {
                 operLog.setStatus(BusinessStatus.FAIL.ordinal());
-                log.error("pre doAfterThrowing Exception:{}", e.getMessage());
                 operLog.setErrorMsg(StringUtils.substring(e.getMessage(), 0, 2000));
             }
             operLog.setStatus(BusinessStatus.SUCCESS.ordinal());
@@ -137,8 +136,7 @@ protected void handleCommonLogic(final JoinPoint joinPoint, final Exception e, O
 
         } catch (Exception exp) {
             // 记录本地异常日志
-            log.error("pre doAfterThrowing Exception:{}", exp.getMessage());
-            exp.printStackTrace();
+            log.error("pre doAfterThrowing Exception:", exp);
         }
     }
 

File: dinky-admin/src/main/java/org/dinky/configure/schedule/metrics/GatherSysIndicator.java
Patch:
@@ -93,7 +93,7 @@ public void updateState() {
         metrics.setContent(metricsTotal);
         metrics.setHeartTime(now);
         metrics.setModel(MetricsType.LOCAL.getType());
-        MetricsContextHolder.sendAsync(metrics);
+        MetricsContextHolder.getInstances().sendAsync(metrics.getModel(), metrics);
 
         log.debug("Collecting jvm information ends.");
     }

File: dinky-admin/src/main/java/org/dinky/controller/APIController.java
Patch:
@@ -30,7 +30,6 @@
 import org.dinky.gateway.enums.SavePointType;
 import org.dinky.gateway.result.SavePointResult;
 import org.dinky.job.JobResult;
-import org.dinky.process.exception.ExcuteException;
 import org.dinky.service.JobInstanceService;
 import org.dinky.service.TaskService;
 
@@ -68,7 +67,7 @@ public class APIController {
     @PostMapping("/submitTask")
     @ApiOperation("Submit Task")
     //    @Log(title = "Submit Task", businessType = BusinessType.SUBMIT)
-    public Result<JobResult> submitTask(@RequestBody TaskDTO taskDTO) throws ExcuteException {
+    public Result<JobResult> submitTask(@RequestBody TaskDTO taskDTO) throws Exception {
         JobResult jobResult = taskService.submitTask(taskDTO.getId(), null);
         if (jobResult.isSuccess()) {
             return Result.succeed(jobResult, Status.EXECUTE_SUCCESS);
@@ -90,7 +89,7 @@ public Result<Boolean> cancel(@RequestParam Integer id) {
     @GetMapping(value = "/restartTask")
     @ApiOperation("Restart Task")
     //    @Log(title = "Restart Task", businessType = BusinessType.REMOTE_OPERATION)
-    public Result<JobResult> restartTask(@RequestParam Integer id, String savePointPath) throws ExcuteException {
+    public Result<JobResult> restartTask(@RequestParam Integer id, String savePointPath) throws Exception {
         return Result.succeed(taskService.restartTask(id, savePointPath));
     }
 

File: dinky-admin/src/main/java/org/dinky/job/handler/JobMetricsHandler.java
Patch:
@@ -66,7 +66,7 @@ public static void writeFlinkMetrics(JobInfoDetail jobInfoDetail) {
         AsyncUtil.waitAll(array);
 
         MetricsVO metricsVO = new MetricsVO(customMetricsList, jobId, LocalDateTime.now());
-        MetricsContextHolder.sendAsync(metricsVO);
+        MetricsContextHolder.getInstances().sendAsync(metricsVO.getModel(), metricsVO);
     }
 
     /**

File: dinky-admin/src/main/java/org/dinky/service/TaskService.java
Patch:
@@ -67,7 +67,7 @@ public interface TaskService extends ISuperService<Task> {
      * @return A {@link JobResult} object representing the result of the submitted task.
      * @throws ExcuteException If there is an error executing the task.
      */
-    JobResult submitTask(Integer id, String savePointPath) throws ExcuteException;
+    JobResult submitTask(Integer id, String savePointPath) throws Exception;
 
     /**
      * Restart the given task and return the job result.
@@ -77,7 +77,7 @@ public interface TaskService extends ISuperService<Task> {
      * @return A {@link JobResult} object representing the result of the restarted task.
      * @throws ExcuteException If there is an error restarting the task.
      */
-    JobResult restartTask(Integer id, String savePointPath) throws ExcuteException;
+    JobResult restartTask(Integer id, String savePointPath) throws Exception;
 
     /**
      * Savepoint the given task job and return the savepoint result.

File: dinky-admin/src/main/java/org/dinky/service/impl/DataBaseServiceImpl.java
Patch:
@@ -36,6 +36,8 @@
 import org.dinky.metadata.driver.Driver;
 import org.dinky.metadata.result.JdbcSelectResult;
 import org.dinky.mybatis.service.impl.SuperServiceImpl;
+import org.dinky.process.annotations.ProcessStep;
+import org.dinky.process.enums.ProcessStepType;
 import org.dinky.service.DataBaseService;
 
 import org.apache.commons.lang3.StringUtils;
@@ -265,6 +267,7 @@ public List<SqlExplainResult> explainCommonSql(TaskDTO task) {
     }
 
     @Override
+    @ProcessStep(type = ProcessStepType.SUBMIT_EXECUTE_COMMON_SQL)
     public JobResult executeCommonSql(SqlDTO sqlDTO) {
         JobResult result = new JobResult();
         result.setStatement(sqlDTO.getStatement());

File: dinky-admin/src/main/java/org/dinky/service/impl/MonitorServiceImpl.java
Patch:
@@ -93,7 +93,7 @@ public List<MetricsVO> getData(Date startTime, Date endTime, List<String> jobIds
 
     @Override
     public SseEmitter sendLatestData(SseEmitter sseEmitter, LocalDateTime lastDate, List<String> keys) {
-        MetricsContextHolder.addSse(keys, sseEmitter, lastDate);
+        MetricsContextHolder.getInstances().addSse(keys, sseEmitter, lastDate);
         return sseEmitter;
     }
 

File: dinky-common/src/main/java/org/dinky/utils/LogUtil.java
Patch:
@@ -42,7 +42,6 @@ public static String getError(Throwable e) {
                 PrintWriter pw = new PrintWriter(sw)) {
             e.printStackTrace(pw);
             error = sw.toString();
-            logger.error(error);
         } catch (IOException ioe) {
             ioe.printStackTrace();
         } finally {
@@ -56,8 +55,7 @@ public static String getError(String msg, Throwable e) {
                 PrintWriter pw = new PrintWriter(sw)) {
             e.printStackTrace(pw);
             LocalDateTime now = LocalDateTime.now();
-            error = now.toString() + ": " + msg + " \nError message:\n " + sw.toString();
-            logger.error(error);
+            error = now + ": " + msg + " \nError message:\n " + sw;
         } catch (IOException ioe) {
             ioe.printStackTrace();
         } finally {

File: dinky-core/src/test/java/org/dinky/core/JobManagerTest.java
Patch:
@@ -43,7 +43,7 @@ public class JobManagerTest {
 
     @Ignore
     @Test
-    public void cancelJobSelect() {
+    public void cancelJobSelect() throws Exception {
         JobConfig config = JobConfig.builder()
                 .type(GatewayType.YARN_SESSION.getLongValue())
                 .useResult(true)

File: dinky-admin/src/main/java/org/dinky/data/dto/ClusterConfigurationDTO.java
Patch:
@@ -22,7 +22,6 @@
 import org.dinky.data.model.ClusterConfiguration;
 import org.dinky.gateway.model.FlinkClusterConfig;
 import org.dinky.mybatis.annotation.Save;
-import org.dinky.utils.JsonUtils;
 
 import javax.validation.constraints.NotNull;
 
@@ -89,7 +88,7 @@ public static ClusterConfigurationDTO fromBean(ClusterConfiguration bean) {
     public ClusterConfiguration toBean() {
         ClusterConfiguration clusterConfiguration = new ClusterConfiguration();
         BeanUtil.copyProperties(this, clusterConfiguration);
-        clusterConfiguration.setConfigJson(JsonUtils.toJsonString(this.getConfig()));
+        clusterConfiguration.setConfigJson(this.getConfig());
         return clusterConfiguration;
     }
 }

File: dinky-admin/src/main/java/org/dinky/data/dto/StudioDDLDTO.java
Patch:
@@ -65,7 +65,7 @@ public class StudioDDLDTO {
     private boolean useRemote;
 
     @ApiModelProperty(
-            value = "Cluster ID",
+            value = "ClusterInstance ID",
             dataType = "Integer",
             example = "1",
             notes = "The identifier of the cluster")

File: dinky-admin/src/main/java/org/dinky/data/dto/TaskVersionConfigureDTO.java
Patch:
@@ -76,7 +76,7 @@ public class TaskVersionConfigureDTO implements Serializable {
     private Boolean batchModel;
 
     @ApiModelProperty(
-            value = "Flink Cluster ID",
+            value = "Flink ClusterInstance ID",
             dataType = "Integer",
             example = "3",
             notes = "The ID of the Flink cluster")

File: dinky-admin/src/main/java/org/dinky/data/model/History.java
Patch:
@@ -50,7 +50,7 @@ public class History implements Serializable {
     @ApiModelProperty(value = "Tenant ID", dataType = "Integer", example = "1", required = true)
     private Integer tenantId;
 
-    @ApiModelProperty(value = "Cluster ID", dataType = "Integer")
+    @ApiModelProperty(value = "ClusterInstance ID", dataType = "Integer")
     private Integer clusterId;
 
     @ApiModelProperty(value = "Cluster Configuration ID", dataType = "Integer")

File: dinky-admin/src/main/java/org/dinky/data/model/HomeResource.java
Patch:
@@ -27,7 +27,7 @@
 @ApiModel(value = "HomeResource", description = "Home Resource Information")
 public class HomeResource {
 
-    @ApiModelProperty(value = "Flink Cluster Count", dataType = "Integer")
+    @ApiModelProperty(value = "Flink ClusterInstance Count", dataType = "Integer")
     private Integer flinkClusterCount;
 
     @ApiModelProperty(value = "Flink Config Count", dataType = "Integer")

File: dinky-admin/src/main/java/org/dinky/data/model/JobInfoDetail.java
Patch:
@@ -44,8 +44,8 @@ public class JobInfoDetail {
     @ApiModelProperty(value = "Job Instance", notes = "Details about the job instance")
     private JobInstance instance;
 
-    @ApiModelProperty(value = "Cluster", notes = "Details about the cluster")
-    private Cluster cluster;
+    @ApiModelProperty(value = "ClusterInstance", notes = "Details about the cluster Instance")
+    private ClusterInstance clusterInstance;
 
     @ApiModelProperty(value = "Cluster Configuration", notes = "Details about the cluster configuration")
     private ClusterConfigurationDTO clusterConfiguration;

File: dinky-admin/src/main/java/org/dinky/data/model/JobInstance.java
Patch:
@@ -76,10 +76,10 @@ public class JobInstance implements Serializable {
     private Integer step;
 
     @ApiModelProperty(
-            value = "Cluster ID",
+            value = "ClusterInstance ID",
             dataType = "Integer",
             example = "1",
-            notes = "Cluster ID associated with the job instance")
+            notes = "ClusterInstance ID associated with the job instance")
     private Integer clusterId;
 
     @ApiModelProperty(value = "JID", dataType = "String", notes = "JID of the job instance")

File: dinky-admin/src/main/java/org/dinky/data/model/Task.java
Patch:
@@ -19,7 +19,7 @@
 
 package org.dinky.data.model;
 
-import org.dinky.data.typehandler.TaskExtConfigTypeHandler;
+import org.dinky.data.typehandler.JSONObjectHandler;
 import org.dinky.mybatis.model.SuperEntity;
 
 import org.apache.ibatis.type.JdbcType;
@@ -94,7 +94,7 @@ public class Task extends SuperEntity<Task> {
     private Boolean batchModel;
 
     @ApiModelProperty(
-            value = "Cluster ID",
+            value = "ClusterInstance ID",
             dataType = "Integer",
             example = "2001",
             notes = "ID of the cluster associated with the task")
@@ -139,7 +139,7 @@ public class Task extends SuperEntity<Task> {
             value = "Configuration JSON",
             dataType = "TaskExtConfig",
             notes = "Extended configuration in JSON format for the task")
-    @TableField(typeHandler = TaskExtConfigTypeHandler.class, jdbcType = JdbcType.VARCHAR)
+    @TableField(typeHandler = JSONObjectHandler.class, jdbcType = JdbcType.VARCHAR)
     private TaskExtConfig configJson;
 
     @ApiModelProperty(value = "Note", dataType = "String", notes = "Additional notes for the task")

File: dinky-admin/src/main/java/org/dinky/job/handler/JobMetricsHandler.java
Patch:
@@ -53,7 +53,8 @@ public class JobMetricsHandler {
      */
     public static void writeFlinkMetrics(JobInfoDetail jobInfoDetail) {
         Map<String, Map<String, String>> customMetricsList = jobInfoDetail.getCustomMetricsMap();
-        String[] jobManagerUrls = jobInfoDetail.getCluster().getJobManagerHost().split(",");
+        String[] jobManagerUrls =
+                jobInfoDetail.getClusterInstance().getJobManagerHost().split(",");
         String jobId = jobInfoDetail.getInstance().getJid();
 
         // Create a CompletableFuture array for concurrent acquisition of indicator data

File: dinky-admin/src/main/java/org/dinky/mapper/ClusterInstanceMapper.java
Patch:
@@ -19,7 +19,7 @@
 
 package org.dinky.mapper;
 
-import org.dinky.data.model.Cluster;
+import org.dinky.data.model.ClusterInstance;
 import org.dinky.mybatis.mapper.SuperMapper;
 
 import org.apache.ibatis.annotations.Mapper;
@@ -28,6 +28,6 @@
 
 /** ClusterInstanceMapper */
 @Mapper
-public interface ClusterInstanceMapper extends SuperMapper<Cluster> {
-    List<Cluster> listSessionEnable();
+public interface ClusterInstanceMapper extends SuperMapper<ClusterInstance> {
+    List<ClusterInstance> listSessionEnable();
 }

File: dinky-admin/src/main/java/org/dinky/service/impl/JobInstanceServiceImpl.java
Patch:
@@ -27,8 +27,8 @@
 import org.dinky.data.dto.JobDataDto;
 import org.dinky.data.enums.JobStatus;
 import org.dinky.data.enums.Status;
-import org.dinky.data.model.Cluster;
 import org.dinky.data.model.ClusterConfiguration;
+import org.dinky.data.model.ClusterInstance;
 import org.dinky.data.model.History;
 import org.dinky.data.model.JobInfoDetail;
 import org.dinky.data.model.JobInstance;
@@ -156,8 +156,8 @@ public JobInfoDetail getJobInfoDetailInfo(JobInstance jobInstance) {
         Asserts.checkNull(jobInstance, Status.JOB_INSTANCE_NOT_EXIST.getMessage());
         jobInfoDetail.setInstance(jobInstance);
 
-        Cluster cluster = clusterInstanceService.getById(jobInstance.getClusterId());
-        jobInfoDetail.setCluster(cluster);
+        ClusterInstance clusterInstance = clusterInstanceService.getById(jobInstance.getClusterId());
+        jobInfoDetail.setClusterInstance(clusterInstance);
 
         History history = historyService.getById(jobInstance.getHistoryId());
         history.setConfig(JsonUtils.parseObject(history.getConfigJson()));

File: dinky-alert/dinky-alert-base/src/main/java/org/dinky/alert/rules/ExceptionRule.java
Patch:
@@ -17,7 +17,7 @@
  *
  */
 
-package org.dinky.alert.Rules;
+package org.dinky.alert.rules;
 
 import org.dinky.data.flink.exceptions.FlinkJobExceptionsDetail;
 

File: dinky-gateway/src/main/java/org/dinky/gateway/model/FlinkClusterConfig.java
Patch:
@@ -24,7 +24,6 @@
 import org.dinky.gateway.config.FlinkConfig;
 import org.dinky.gateway.config.K8sConfig;
 import org.dinky.gateway.enums.GatewayType;
-import org.dinky.utils.JsonUtils;
 
 import java.util.Optional;
 
@@ -70,8 +69,7 @@ public class FlinkClusterConfig {
             notes = "Configuration settings for Kubernetes (if applicable)")
     private K8sConfig kubernetesConfig;
 
-    public static FlinkClusterConfig create(String type, String json) {
-        FlinkClusterConfig flinkClusterConfig = JsonUtils.parseObject(json, FlinkClusterConfig.class);
+    public static FlinkClusterConfig create(String type, FlinkClusterConfig flinkClusterConfig) {
         Optional.ofNullable(flinkClusterConfig).ifPresent(config -> config.setType(GatewayType.get(type)));
         return flinkClusterConfig;
     }

File: dinky-executor/src/main/java/org/dinky/trans/ddl/CreateTemporalTableFunctionOperation.java
Patch:
@@ -58,7 +58,7 @@ public Optional<? extends TableResult> execute(Executor executor) {
                 .from(temporalTable.getTableName())
                 .createTemporalTableFunction(timeColumn, targetColumn);
 
-        if (temporalTable.getFunctionType().toUpperCase().equals("TEMPORARY SYSTEM")) {
+        if (temporalTable.getFunctionType().equalsIgnoreCase("TEMPORARY SYSTEM")) {
             customTableEnvironmentImpl.createTemporarySystemFunction(temporalTable.getFunctionName(), ttf);
         } else {
             customTableEnvironmentImpl.createTemporaryFunction(temporalTable.getFunctionName(), ttf);

File: dinky-admin/src/main/java/org/dinky/service/TaskService.java
Patch:
@@ -101,4 +101,6 @@ public interface TaskService extends ISuperService<Task> {
     List<JobTypeOverView> getTaskOnlineRate();
 
     JobModelOverview getJobStreamingOrBatchModelOverview();
+
+    List<String> getPrintTables(String statement);
 }

File: dinky-admin/src/main/java/org/dinky/data/model/TaskVersion.java
Patch:
@@ -22,7 +22,7 @@
 import org.dinky.data.dto.TaskVersionConfigureDTO;
 
 import java.io.Serializable;
-import java.util.Date;
+import java.time.LocalDateTime;
 
 import com.baomidou.mybatisplus.annotation.TableField;
 import com.baomidou.mybatisplus.annotation.TableName;
@@ -83,5 +83,5 @@ public class TaskVersion implements Serializable {
 
     @ApiModelProperty(value = "Create Time", dataType = "Date", notes = "Timestamp when the version was created")
     @TableField(value = "create_time")
-    private Date createTime;
+    private LocalDateTime createTime;
 }

File: dinky-admin/src/main/java/org/dinky/job/handler/JobAlertHandler.java
Patch:
@@ -29,14 +29,14 @@
 import org.dinky.context.SpringContextUtils;
 import org.dinky.daemon.pool.DefaultThreadPool;
 import org.dinky.data.dto.AlertRuleDTO;
+import org.dinky.data.dto.TaskDTO;
 import org.dinky.data.enums.Status;
 import org.dinky.data.model.AlertGroup;
 import org.dinky.data.model.AlertHistory;
 import org.dinky.data.model.AlertInstance;
 import org.dinky.data.model.JobInfoDetail;
 import org.dinky.data.model.JobInstance;
 import org.dinky.data.model.SystemConfiguration;
-import org.dinky.data.model.Task;
 import org.dinky.data.options.AlertRuleOptions;
 import org.dinky.service.AlertGroupService;
 import org.dinky.service.AlertHistoryService;
@@ -214,7 +214,7 @@ private Rule buildRule(AlertRuleDTO alertRuleDTO) {
     private void executeAlertAction(Facts facts, AlertRuleDTO alertRuleDTO) {
         JobInfoDetail jobInfoDetail = facts.get(AlertRuleOptions.JOB_ALERT_RULE_JOB_DETAIL);
         JobInstance jobInstance = jobInfoDetail.getInstance();
-        Task task = taskService.getById(jobInfoDetail.getInstance().getTaskId());
+        TaskDTO task = taskService.getTaskInfoById(jobInfoDetail.getInstance().getTaskId());
 
         String taskUrl = StrFormatter.format(
                 "{}/#/devops/job-detail?id={}",

File: dinky-admin/src/main/java/org/dinky/service/SavepointsService.java
Patch:
@@ -19,6 +19,7 @@
 
 package org.dinky.service;
 
+import org.dinky.data.dto.TaskDTO;
 import org.dinky.data.model.Savepoints;
 import org.dinky.mybatis.service.ISuperService;
 
@@ -36,4 +37,6 @@ public interface SavepointsService extends ISuperService<Savepoints> {
     Savepoints getLatestSavepointByTaskId(Integer taskId);
 
     Savepoints getEarliestSavepointByTaskId(Integer taskId);
+
+    Savepoints getSavePointWithStrategy(TaskDTO task);
 }

File: dinky-admin/src/main/java/org/dinky/service/TaskVersionService.java
Patch:
@@ -19,6 +19,7 @@
 
 package org.dinky.service;
 
+import org.dinky.data.dto.TaskDTO;
 import org.dinky.data.model.TaskVersion;
 import org.dinky.mybatis.service.ISuperService;
 
@@ -32,4 +33,6 @@ public interface TaskVersionService extends ISuperService<TaskVersion> {
      * @return java.util.List<org.dinky.data.model.TaskVersion>
      */
     List<TaskVersion> getTaskVersionByTaskId(Integer taskId);
+
+    void createTaskVersionSnapshot(TaskDTO task);
 }

File: dinky-admin/src/main/java/org/dinky/service/UserService.java
Patch:
@@ -149,6 +149,8 @@ public interface UserService extends ISuperService<User> {
      */
     List<RowPermissions> getCurrentRoleSelectPermissions();
 
+    void buildRowPermission();
+
     /** user loginout */
     void outLogin();
 

File: dinky-common/src/main/java/org/dinky/config/Dialect.java
Patch:
@@ -77,7 +77,7 @@ public static Dialect get(String value) {
      * @param value {@link Dialect}
      * @return If is flink sql, return false, otherwise return true.
      */
-    public static boolean notFlinkSql(String value) {
+    public static boolean isCommonSql(String value) {
         Dialect dialect = Dialect.get(value);
         switch (dialect) {
             case SQL:

File: dinky-core/src/main/java/org/dinky/job/JobManager.java
Patch:
@@ -176,7 +176,7 @@ public boolean init() {
             useGateway = GatewayType.isDeployCluster(config.getType());
             handler = JobHandler.build();
         }
-        useStatementSet = config.isUseStatementSet();
+        useStatementSet = config.isStatementSet();
         useRestAPI = SystemConfiguration.getInstances().isUseRestAPI();
         sqlSeparator = SystemConfiguration.getInstances().getSqlSeparator();
         executorConfig = config.getExecutorSetting();

File: dinky-admin/src/main/java/org/dinky/data/dto/JobDataDto.java
Patch:
@@ -19,10 +19,10 @@
 
 package org.dinky.data.dto;
 
+import org.dinky.data.flink.config.FlinkJobConfigInfo;
+import org.dinky.data.flink.exceptions.FlinkJobExceptionsDetail;
+import org.dinky.data.flink.job.FlinkJobDetailInfo;
 import org.dinky.data.model.JobHistory;
-import org.dinky.data.model.flink.config.FlinkJobConfigInfo;
-import org.dinky.data.model.flink.exceptions.FlinkJobExceptionsDetail;
-import org.dinky.data.model.flink.job.FlinkJobDetailInfo;
 import org.dinky.utils.JsonUtils;
 
 import java.time.LocalDateTime;

File: dinky-common/src/main/java/org/dinky/data/flink/backpressure/FlinkJobNodeBackPressure.java
Patch:
@@ -17,7 +17,7 @@
  *
  */
 
-package org.dinky.data.model.flink.backpressure;
+package org.dinky.data.flink.backpressure;
 
 import java.io.Serializable;
 import java.util.List;

File: dinky-common/src/main/java/org/dinky/data/flink/config/ExecutionConfig.java
Patch:
@@ -17,7 +17,7 @@
  *
  */
 
-package org.dinky.data.model.flink.config;
+package org.dinky.data.flink.config;
 
 import java.io.Serializable;
 

File: dinky-common/src/main/java/org/dinky/data/flink/config/FlinkJobConfigInfo.java
Patch:
@@ -17,7 +17,7 @@
  *
  */
 
-package org.dinky.data.model.flink.config;
+package org.dinky.data.flink.config;
 
 import java.io.Serializable;
 

File: dinky-common/src/main/java/org/dinky/data/flink/exceptions/FlinkJobExceptionsDetail.java
Patch:
@@ -17,7 +17,7 @@
  *
  */
 
-package org.dinky.data.model.flink.exceptions;
+package org.dinky.data.flink.exceptions;
 
 import java.io.Serializable;
 import java.util.List;

File: dinky-common/src/main/java/org/dinky/data/flink/job/FlinkJobDetailInfo.java
Patch:
@@ -17,7 +17,7 @@
  *
  */
 
-package org.dinky.data.model.flink.job;
+package org.dinky.data.flink.job;
 
 import java.io.Serializable;
 import java.util.List;

File: dinky-common/src/main/java/org/dinky/data/flink/job/FlinkJobPlan.java
Patch:
@@ -17,7 +17,7 @@
  *
  */
 
-package org.dinky.data.model.flink.job;
+package org.dinky.data.flink.job;
 
 import java.io.Serializable;
 import java.util.List;

File: dinky-common/src/main/java/org/dinky/data/flink/job/FlinkJobPlanNode.java
Patch:
@@ -17,10 +17,10 @@
  *
  */
 
-package org.dinky.data.model.flink.job;
+package org.dinky.data.flink.job;
 
-import org.dinky.data.model.flink.backpressure.FlinkJobNodeBackPressure;
-import org.dinky.data.model.flink.watermark.FlinkJobNodeWaterMark;
+import org.dinky.data.flink.backpressure.FlinkJobNodeBackPressure;
+import org.dinky.data.flink.watermark.FlinkJobNodeWaterMark;
 
 import java.io.Serializable;
 import java.util.List;

File: dinky-common/src/main/java/org/dinky/data/flink/job/FlinkJobPlanNodeInput.java
Patch:
@@ -17,7 +17,7 @@
  *
  */
 
-package org.dinky.data.model.flink.job;
+package org.dinky.data.flink.job;
 
 import java.io.Serializable;
 

File: dinky-common/src/main/java/org/dinky/data/flink/job/FlinkJobVertex.java
Patch:
@@ -17,7 +17,7 @@
  *
  */
 
-package org.dinky.data.model.flink.job;
+package org.dinky.data.flink.job;
 
 import java.io.Serializable;
 import java.util.Map;

File: dinky-common/src/main/java/org/dinky/data/flink/watermark/FlinkJobNodeWaterMark.java
Patch:
@@ -17,7 +17,7 @@
  *
  */
 
-package org.dinky.data.model.flink.watermark;
+package org.dinky.data.flink.watermark;
 
 import java.io.Serializable;
 

File: dinky-admin/src/main/java/org/dinky/init/EnvInit.java
Patch:
@@ -47,8 +47,8 @@ public void run(ApplicationArguments args) throws Exception {
                         + "Application 'Dinky' is running! Access URLs:\n\t"
                         + "Local: \t\thttp://localhost:{}\n\t"
                         + "External: \thttp://{}:{}\n\t"
-                        + "Doc: \thttp://{}:{}/doc.html\n"
-                        + "Druid Monitor: \thttp://{}:{}/druid/index.html\n"
+                        + "Doc: \thttp://{}:{}/doc.html\n\t"
+                        + "Druid Monitor: \thttp://{}:{}/druid/index.html\n\t"
                         + "Actuator: \thttp://{}:{}/actuator\n"
                         + "----------------------------------------------------------",
                 port,

File: dinky-client/dinky-client-base/src/main/java/org/dinky/executor/CustomTableEnvironment.java
Patch:
@@ -32,6 +32,7 @@
 import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;
 import org.apache.flink.table.api.internal.TableEnvironmentInternal;
 import org.apache.flink.table.delegation.Planner;
+import org.apache.flink.table.operations.Operation;
 import org.apache.flink.types.Row;
 
 import java.util.Collections;
@@ -71,4 +72,6 @@ default List<LineageRel> getLineage(String statement) {
     }
 
     <T> void createTemporaryView(String s, DataStream<Row> dataStream, List<String> columnNameList);
+
+    void executeCTAS(Operation operation);
 }

File: dinky-client/dinky-client-base/src/main/java/org/dinky/flink/checkpoint/CheckpointRead.java
Patch:
@@ -112,7 +112,6 @@ private static Map<String, CheckPointReadTable> readState(
                                     new RegisteredOperatorStateBackendMetaInfo<>(stateMetaInfoSnapshot));
                             ;
                             deserializeOperatorStateValues(partitionableListState, in, value);
-                            partitionableListState.get().forEach(System.out::println);
                             // get checkpoint data
                             CheckpointReadFactory.getTable(partitionableListState)
                                     .ifPresent(tableVO -> map.put(key, tableVO));

File: dinky-admin/src/main/java/org/dinky/data/dto/ClusterConfigurationDTO.java
Patch:
@@ -22,7 +22,7 @@
 import org.dinky.data.model.ClusterConfiguration;
 import org.dinky.gateway.model.FlinkClusterConfig;
 import org.dinky.mybatis.annotation.Save;
-import org.dinky.utils.JSONUtil;
+import org.dinky.utils.JsonUtils;
 
 import javax.validation.constraints.NotNull;
 
@@ -89,7 +89,7 @@ public static ClusterConfigurationDTO fromBean(ClusterConfiguration bean) {
     public ClusterConfiguration toBean() {
         ClusterConfiguration clusterConfiguration = new ClusterConfiguration();
         BeanUtil.copyProperties(this, clusterConfiguration);
-        clusterConfiguration.setConfigJson(JSONUtil.toJsonString(this.getConfig()));
+        clusterConfiguration.setConfigJson(JsonUtils.toJsonString(this.getConfig()));
         return clusterConfiguration;
     }
 }

File: dinky-admin/src/main/java/org/dinky/init/SystemInit.java
Patch:
@@ -45,7 +45,7 @@
 import org.dinky.service.resource.impl.HdfsResourceManager;
 import org.dinky.service.resource.impl.OssResourceManager;
 import org.dinky.url.RsURLStreamHandlerFactory;
-import org.dinky.utils.JSONUtil;
+import org.dinky.utils.JsonUtils;
 import org.dinky.utils.OssTemplate;
 import org.dinky.utils.UDFUtils;
 
@@ -242,7 +242,7 @@ public void registerUDF() {
     public void updateGitBuildState() {
         String path = PathConstant.TMP_PATH + "/build.list";
         if (FileUtil.exist(path)) {
-            List<Integer> runningList = JSONUtil.toList(FileUtil.readUtf8String(path), Integer.class);
+            List<Integer> runningList = JsonUtils.toList(FileUtil.readUtf8String(path), Integer.class);
             gitProjectService.list().stream()
                     .filter(x -> x.getBuildState().equals(1))
                     .filter(x -> runningList.contains(x.getId()))

File: dinky-admin/src/main/java/org/dinky/job/FlinkJobTask.java
Patch:
@@ -27,7 +27,7 @@
 import org.dinky.data.model.JobInfoDetail;
 import org.dinky.job.handler.JobAlertHandler;
 import org.dinky.job.handler.JobMetricsHandler;
-import org.dinky.job.handler.JobRefeshHandler;
+import org.dinky.job.handler.JobRefreshHandler;
 import org.dinky.service.JobInstanceService;
 
 import java.util.Objects;
@@ -75,7 +75,7 @@ public boolean dealTask() {
         volatilityBalance();
         TenantContextHolder.set(1);
 
-        boolean isDone = JobRefeshHandler.refeshJob(jobInfoDetail, isNeedSave());
+        boolean isDone = JobRefreshHandler.refreshJob(jobInfoDetail, isNeedSave());
         JobAlertHandler.getInstance().check(jobInfoDetail);
         JobMetricsHandler.writeFlinkMetrics(jobInfoDetail);
         return isDone;

File: dinky-admin/src/main/java/org/dinky/service/impl/AlertInstanceServiceImpl.java
Patch:
@@ -29,7 +29,7 @@
 import org.dinky.mybatis.service.impl.SuperServiceImpl;
 import org.dinky.service.AlertGroupService;
 import org.dinky.service.AlertInstanceService;
-import org.dinky.utils.JSONUtil;
+import org.dinky.utils.JsonUtils;
 
 import org.apache.commons.collections4.CollectionUtils;
 import org.apache.commons.collections4.MapUtils;
@@ -68,7 +68,7 @@ public List<AlertInstance> listEnabledAll() {
     @Override
     public AlertResult testAlert(AlertInstance alertInstance) {
         AlertConfig alertConfig = AlertConfig.build(
-                alertInstance.getName(), alertInstance.getType(), JSONUtil.toMap(alertInstance.getParams()));
+                alertInstance.getName(), alertInstance.getType(), JsonUtils.toMap(alertInstance.getParams()));
         Alert alert = Alert.buildTest(alertConfig);
 
         String msg = "\n- **Job Name :** <font color='gray'>Test Job</font>\n"

File: dinky-admin/src/main/java/org/dinky/service/impl/TaskServiceImpl.java
Patch:
@@ -965,7 +965,7 @@ private FlinkClusterConfig buildGatewayCfgObj(JobConfig config) {
         //        if (config.isJarTask()) {
         //            JSONObject clusterObj = new JSONObject(flinkClusterCfg);
         //            JSONObject taskObj = new JSONObject(task.getStatement());
-        //            return JSONUtil.merge(clusterObj,taskObj).toBean(FlinkClusterConfig.class);
+        //            return JsonUtils.merge(clusterObj,taskObj).toBean(FlinkClusterConfig.class);
         //        }
         return flinkClusterCfg;
     }

File: dinky-alert/dinky-alert-base/src/main/java/org/dinky/utils/ExcelUtils.java
Patch:
@@ -63,7 +63,7 @@ public static void genExcelFile(String content, String title, String xlsFilePath
             throw new AlertException("Create xlsx directory error");
         }
 
-        List<LinkedHashMap> itemsList = JSONUtil.toList(content, LinkedHashMap.class);
+        List<LinkedHashMap> itemsList = JsonUtils.toList(content, LinkedHashMap.class);
 
         if (CollectionUtils.isEmpty(itemsList)) {
             logger.error("itemsList is null");

File: dinky-alert/dinky-alert-dingtalk/src/main/java/org/dinky/alert/dingtalk/DingTalkSender.java
Patch:
@@ -24,7 +24,7 @@
 import org.dinky.assertion.Asserts;
 import org.dinky.data.model.ProxyConfig;
 import org.dinky.utils.HttpUtils;
-import org.dinky.utils.JSONUtil;
+import org.dinky.utils.JsonUtils;
 
 import org.apache.commons.codec.binary.Base64;
 
@@ -144,7 +144,7 @@ private AlertResult checkMsgResult(String result) {
             logger.info("send ding talk msg error,ding talk server resp is null");
             return alertResult;
         }
-        AlertSendResponse response = JSONUtil.parseObject(result, AlertSendResponse.class);
+        AlertSendResponse response = JsonUtils.parseObject(result, AlertSendResponse.class);
         if (null == response) {
             alertResult.setMessage("send ding talk msg fail");
             logger.info("send ding talk msg error,resp error");

File: dinky-alert/dinky-alert-feishu/src/main/java/org/dinky/alert/feishu/FeiShuSender.java
Patch:
@@ -23,7 +23,7 @@
 import org.dinky.assertion.Asserts;
 import org.dinky.data.model.ProxyConfig;
 import org.dinky.utils.HttpUtils;
-import org.dinky.utils.JSONUtil;
+import org.dinky.utils.JsonUtils;
 
 import org.apache.commons.codec.binary.Base64;
 
@@ -149,7 +149,7 @@ public static AlertResult checkSendMsgResult(String result) {
             logger.info("send fei shu msg error,fei shu server resp is null");
             return alertResult;
         }
-        FeiShuSendMsgResponse sendMsgResponse = JSONUtil.parseObject(result, FeiShuSendMsgResponse.class);
+        FeiShuSendMsgResponse sendMsgResponse = JsonUtils.parseObject(result, FeiShuSendMsgResponse.class);
 
         if (null == sendMsgResponse) {
             alertResult.setMessage("send fei shu msg fail");

File: dinky-alert/dinky-alert-wechat/src/main/java/org/dinky/alert/wechat/WeChatSender.java
Patch:
@@ -24,7 +24,7 @@
 import org.dinky.alert.AlertResult;
 import org.dinky.alert.AlertSendResponse;
 import org.dinky.utils.HttpUtils;
-import org.dinky.utils.JSONUtil;
+import org.dinky.utils.JsonUtils;
 
 import org.apache.http.HttpEntity;
 import org.apache.http.client.methods.CloseableHttpResponse;
@@ -133,7 +133,7 @@ private String getToken() {
                     resp = EntityUtils.toString(entity, WeChatConstants.CHARSET);
                     EntityUtils.consume(entity);
                 }
-                HashMap<String, Object> map = JSONUtil.parseObject(resp, HashMap.class);
+                HashMap<String, Object> map = JsonUtils.parseObject(resp, HashMap.class);
                 if (map != null && null != map.get(WeChatConstants.ACCESS_TOKEN)) {
                     return map.get(WeChatConstants.ACCESS_TOKEN).toString();
                 } else {
@@ -154,7 +154,7 @@ private static AlertResult checkWeChatSendMsgResult(String result) {
             logger.error("send we chat msg error,resp is null");
             return alertResult;
         }
-        AlertSendResponse sendMsgResponse = JSONUtil.parseObject(result, AlertSendResponse.class);
+        AlertSendResponse sendMsgResponse = JsonUtils.parseObject(result, AlertSendResponse.class);
         if (null == sendMsgResponse) {
             alertResult.setMessage("we chat send fail");
             logger.error("send we chat msg error,resp error");

File: dinky-cdc/dinky-cdc-core/src/main/java/org/dinky/cdc/AbstractSinkBuilder.java
Patch:
@@ -26,7 +26,7 @@
 import org.dinky.data.model.Schema;
 import org.dinky.data.model.Table;
 import org.dinky.executor.CustomTableEnvironment;
-import org.dinky.utils.JSONUtil;
+import org.dinky.utils.JsonUtils;
 
 import org.apache.flink.api.common.functions.FilterFunction;
 import org.apache.flink.api.common.functions.FlatMapFunction;
@@ -182,7 +182,7 @@ protected FlatMapFunction<Map, RowData> sinkRowDataFunction(
                 logger.error(
                         "SchemaTable: {} - Row: {} - Exception: {}",
                         schemaTableName,
-                        JSONUtil.toJsonString(value),
+                        JsonUtils.toJsonString(value),
                         e.toString());
                 throw e;
             }

File: dinky-cdc/dinky-cdc-core/src/main/java/org/dinky/cdc/sql/AbstractSqlSinkBuilder.java
Patch:
@@ -26,7 +26,7 @@
 import org.dinky.data.model.Schema;
 import org.dinky.data.model.Table;
 import org.dinky.executor.CustomTableEnvironment;
-import org.dinky.utils.JSONUtil;
+import org.dinky.utils.JsonUtils;
 import org.dinky.utils.LogUtil;
 
 import org.apache.flink.api.common.functions.FlatMapFunction;
@@ -89,7 +89,7 @@ protected FlatMapFunction<Map, Row> sqlSinkRowFunction(
                 logger.error(
                         "SchemaTable: {} - Row: {} - Exception {}",
                         schemaTableName,
-                        JSONUtil.toJsonString(value),
+                        JsonUtils.toJsonString(value),
                         e.toString());
                 throw e;
             }

File: dinky-connectors/dinky-connector-pulsar-1.14/src/main/java/org/dinky/connector/pulsar/PulsarSinkFunction.java
Patch:
@@ -21,7 +21,7 @@
 
 import org.dinky.connector.pulsar.util.PulsarConnectionHolder;
 import org.dinky.connector.pulsar.util.PulsarProducerHolder;
-import org.dinky.utils.JSONUtil;
+import org.dinky.utils.JsonUtils;
 
 import org.apache.flink.annotation.Internal;
 import org.apache.flink.api.common.functions.RuntimeContext;
@@ -220,7 +220,7 @@ public String getKey(String strValue) {
         // JSONObject jsonObject = JSONObject.parseObject(strValue);
         // JSONObject jsonObject = JSONUtil.parseObject(strValue);
         // String key = jsonObject.getString("key");
-        ObjectNode jsonNodes = JSONUtil.parseObject(strValue);
+        ObjectNode jsonNodes = JsonUtils.parseObject(strValue);
         String key = String.valueOf(jsonNodes.get("key"));
         return key == null ? "" : key;
     }

File: dinky-core/src/main/java/org/dinky/data/constant/FlinkRestAPIConstant.java
Patch:
@@ -64,4 +64,7 @@ public final class FlinkRestAPIConstant {
     public static final String THREAD_DUMP = "/thread-dump";
 
     public static final String GET = "?get=";
+    public static final String BACKPRESSURE = "/backpressure";
+
+    public static final String WATERMARKS = "/watermarks";
 }

File: dinky-gateway/src/main/java/org/dinky/gateway/model/FlinkClusterConfig.java
Patch:
@@ -24,7 +24,7 @@
 import org.dinky.gateway.config.FlinkConfig;
 import org.dinky.gateway.config.K8sConfig;
 import org.dinky.gateway.enums.GatewayType;
-import org.dinky.utils.JSONUtil;
+import org.dinky.utils.JsonUtils;
 
 import java.util.Optional;
 
@@ -71,7 +71,7 @@ public class FlinkClusterConfig {
     private K8sConfig kubernetesConfig;
 
     public static FlinkClusterConfig create(String type, String json) {
-        FlinkClusterConfig flinkClusterConfig = JSONUtil.parseObject(json, FlinkClusterConfig.class);
+        FlinkClusterConfig flinkClusterConfig = JsonUtils.parseObject(json, FlinkClusterConfig.class);
         Optional.ofNullable(flinkClusterConfig).ifPresent(config -> config.setType(GatewayType.get(type)));
         return flinkClusterConfig;
     }

File: dinky-metadata/dinky-metadata-clickhouse/src/test/java/org/dinky/metadata/ClickHouseTest.java
Patch:
@@ -69,7 +69,7 @@ public void connectTest() {
     @Test
     public void schemaTest() {
         List<Schema> schemasAndTables = getDriver().getSchemasAndTables();
-        // LOGGER.info(JSONUtil.toJsonString(schemasAndTables));
+        // LOGGER.info(JsonUtils.toJsonString(schemasAndTables));
         // LOGGER.info("end...");
     }
 
@@ -78,7 +78,7 @@ public void schemaTest() {
     public void columnTest() {
         Driver driver = getDriver();
         List<Column> columns = driver.listColumns("xxx", "xxx");
-        // LOGGER.info(JSONUtil.toJsonString(columns));
+        // LOGGER.info(JsonUtils.toJsonString(columns));
         // LOGGER.info("end...");
     }
 
@@ -87,7 +87,7 @@ public void columnTest() {
     public void queryTest() {
         Driver driver = getDriver();
         JdbcSelectResult query = driver.query("select * from xxx", 10);
-        // LOGGER.info(JSONUtil.toJsonString(query));
+        // LOGGER.info(JsonUtils.toJsonString(query));
         // LOGGER.info("end...");
     }
 }

File: dinky-admin/src/main/java/org/dinky/controller/RoleMenuController.java
Patch:
@@ -20,6 +20,7 @@
 package org.dinky.controller;
 
 import org.dinky.data.annotation.Log;
+import org.dinky.data.constant.PermissionConstants;
 import org.dinky.data.dto.AssignMenuToRoleDto;
 import org.dinky.data.enums.BusinessType;
 import org.dinky.data.result.Result;
@@ -30,6 +31,7 @@
 import org.springframework.web.bind.annotation.RequestMapping;
 import org.springframework.web.bind.annotation.RestController;
 
+import cn.dev33.satoken.annotation.SaCheckPermission;
 import io.swagger.annotations.Api;
 import io.swagger.annotations.ApiImplicitParam;
 import io.swagger.annotations.ApiOperation;
@@ -62,6 +64,7 @@ public class RoleMenuController {
             required = true,
             dataType = "AssignMenuToRoleDto",
             paramType = "body")
+    @SaCheckPermission(PermissionConstants.AUTH_ROLE_ASSIGN_MENU)
     public Result<Void> assignMenuToRole(@RequestBody AssignMenuToRoleDto assignMenuToRoleDto) {
         return roleMenuService.assignMenuToRole(assignMenuToRoleDto);
     }

File: dinky-admin/src/main/java/org/dinky/controller/SysConfigController.java
Patch:
@@ -37,6 +37,7 @@
 import org.springframework.web.bind.annotation.RequestParam;
 import org.springframework.web.bind.annotation.RestController;
 
+import cn.dev33.satoken.annotation.SaIgnore;
 import cn.hutool.core.lang.Dict;
 import cn.hutool.core.map.MapUtil;
 import io.swagger.annotations.Api;
@@ -81,6 +82,7 @@ public Result<Void> modifyConfig(@RequestBody Dict params) {
      */
     @GetMapping("/getAll")
     @ApiOperation("Query All System Config List")
+    @SaIgnore
     public Result<Map<String, List<Configuration<?>>>> getAll() {
         Map<String, List<Configuration<?>>> all = sysConfigService.getAll();
         Map<String, List<Configuration<?>>> map =

File: dinky-admin/src/main/java/org/dinky/controller/UploadFileRecordController.java
Patch:
@@ -34,6 +34,7 @@
 
 import com.baomidou.mybatisplus.core.conditions.query.QueryWrapper;
 
+import cn.dev33.satoken.annotation.SaCheckLogin;
 import cn.hutool.json.JSONUtil;
 import io.swagger.annotations.Api;
 import io.swagger.annotations.ApiOperation;
@@ -43,6 +44,7 @@
 @Slf4j
 @Api(tags = "File Upload Controller")
 @RestController
+@SaCheckLogin
 @RequestMapping("/api/uploadFileRecord")
 public class UploadFileRecordController {
 

File: dinky-admin/src/main/java/org/dinky/service/impl/MonitorServiceImpl.java
Patch:
@@ -34,7 +34,6 @@
 import org.apache.paimon.predicate.Predicate;
 import org.apache.paimon.predicate.PredicateBuilder;
 
-import java.io.IOException;
 import java.time.LocalDateTime;
 import java.util.ArrayList;
 import java.util.Date;
@@ -106,8 +105,6 @@ public SseEmitter sendJvmInfo(SseEmitter sseEmitter) {
                     sseEmitter.send(JSONUtil.toJsonStr(Jvm.of()));
                     ThreadUtil.sleep(10000);
                 }
-            } catch (IOException e) {
-                sseEmitter.complete();
             } catch (Exception e) {
                 e.printStackTrace();
                 sseEmitter.complete();

File: dinky-gateway/src/main/java/org/dinky/gateway/kubernetes/KubernetesSessionGateway.java
Patch:
@@ -48,7 +48,6 @@ public GatewayResult deployCluster() {
             init();
         }
 
-        combineFlinkConfig();
         ClusterSpecification.ClusterSpecificationBuilder clusterSpecificationBuilder =
                 createClusterSpecificationBuilder();
 

File: dinky-gateway/src/main/java/org/dinky/gateway/yarn/YarnGateway.java
Patch:
@@ -122,7 +122,7 @@ private void initConfig() {
 
         if (getType().isApplicationMode()) {
             configuration.set(YarnConfigOptions.APPLICATION_TYPE, "Dinky Flink");
-            resetCheckpointInApplicationMode();
+            resetCheckpointInApplicationMode(flinkConfig.getJobName());
         }
 
         YarnLogConfigUtil.setLogConfigFileInConfig(configuration, clusterConfig.getFlinkConfigPath());

File: dinky-admin/src/main/java/org/dinky/data/model/JobInfoDetail.java
Patch:
@@ -19,6 +19,7 @@
 
 package org.dinky.data.model;
 
+import org.dinky.data.dto.ClusterConfigurationDTO;
 import org.dinky.data.dto.JobDataDto;
 
 import java.util.HashMap;
@@ -47,7 +48,7 @@ public class JobInfoDetail {
     private Cluster cluster;
 
     @ApiModelProperty(value = "Cluster Configuration", notes = "Details about the cluster configuration")
-    private ClusterConfiguration clusterConfiguration;
+    private ClusterConfigurationDTO clusterConfiguration;
 
     @ApiModelProperty(value = "History", notes = "Details about the history")
     private History history;

File: dinky-admin/src/main/java/org/dinky/job/handler/JobRefeshHandler.java
Patch:
@@ -23,9 +23,9 @@
 import org.dinky.assertion.Asserts;
 import org.dinky.context.SpringContextUtils;
 import org.dinky.data.constant.FlinkRestResultConstant;
+import org.dinky.data.dto.ClusterConfigurationDTO;
 import org.dinky.data.dto.JobDataDto;
 import org.dinky.data.enums.JobStatus;
-import org.dinky.data.model.ClusterConfiguration;
 import org.dinky.data.model.JobInfoDetail;
 import org.dinky.data.model.JobInstance;
 import org.dinky.gateway.Gateway;
@@ -170,13 +170,13 @@ public static JobDataDto getJobHistory(Integer id, String jobManagerHost, String
      */
     private static JobStatus getJobStatus(JobInfoDetail jobInfoDetail) {
 
-        ClusterConfiguration clusterCfg = jobInfoDetail.getClusterConfiguration();
+        ClusterConfigurationDTO clusterCfg = jobInfoDetail.getClusterConfiguration();
 
         if (!Asserts.isNull(clusterCfg)) {
             try {
                 String appId = jobInfoDetail.getCluster().getName();
 
-                GatewayConfig gatewayConfig = GatewayConfig.build(clusterCfg.getFlinkClusterCfg());
+                GatewayConfig gatewayConfig = GatewayConfig.build(clusterCfg.getConfig());
                 gatewayConfig.getClusterConfig().setAppId(appId);
                 gatewayConfig
                         .getFlinkConfig()

File: dinky-admin/src/main/java/org/dinky/service/ClusterConfigurationService.java
Patch:
@@ -19,6 +19,7 @@
 
 package org.dinky.service;
 
+import org.dinky.data.dto.ClusterConfigurationDTO;
 import org.dinky.data.model.ClusterConfiguration;
 import org.dinky.gateway.model.FlinkClusterConfig;
 import org.dinky.gateway.result.TestResult;
@@ -39,7 +40,7 @@ public interface ClusterConfigurationService extends ISuperService<ClusterConfig
 
     FlinkClusterConfig getFlinkClusterCfg(Integer id);
 
-    TestResult testGateway(ClusterConfiguration clusterConfiguration);
+    TestResult testGateway(ClusterConfigurationDTO clusterConfiguration);
 
     Boolean modifyClusterConfigStatus(Integer id);
 }

File: dinky-admin/src/main/java/org/dinky/service/impl/JobInstanceServiceImpl.java
Patch:
@@ -23,6 +23,7 @@
 import org.dinky.context.TenantContextHolder;
 import org.dinky.daemon.task.DaemonFactory;
 import org.dinky.daemon.task.DaemonTaskConfig;
+import org.dinky.data.dto.ClusterConfigurationDTO;
 import org.dinky.data.dto.JobDataDto;
 import org.dinky.data.enums.JobStatus;
 import org.dinky.data.enums.Status;
@@ -162,9 +163,9 @@ public JobInfoDetail getJobInfoDetailInfo(JobInstance jobInstance) {
         history.setConfig(JSONUtil.parseObject(history.getConfigJson()));
         jobInfoDetail.setHistory(history);
         if (Asserts.isNotNull(history.getClusterConfigurationId())) {
-            ClusterConfiguration clusterConfigById =
+            ClusterConfiguration clusterConfig =
                     clusterConfigurationService.getClusterConfigById(history.getClusterConfigurationId());
-            jobInfoDetail.setClusterConfiguration(clusterConfigById);
+            jobInfoDetail.setClusterConfiguration(ClusterConfigurationDTO.fromBean(clusterConfig));
         }
 
         JobDataDto jobDataDto = jobHistoryService.getJobHistoryDto(jobInstance.getId());

File: dinky-admin/src/main/java/org/dinky/configure/schedule/metrics/GatherSysIndicator.java
Patch:
@@ -19,8 +19,8 @@
 
 package org.dinky.configure.schedule.metrics;
 
-import org.dinky.configure.MetricConfig;
 import org.dinky.configure.schedule.BaseSchedule;
+import org.dinky.context.MetricsContextHolder;
 import org.dinky.data.annotation.GaugeM;
 import org.dinky.data.enums.MetricsType;
 import org.dinky.data.metrics.BaseMetrics;
@@ -93,7 +93,7 @@ public void updateState() {
         metrics.setContent(metricsTotal);
         metrics.setHeartTime(now);
         metrics.setModel(MetricsType.LOCAL.getType());
-        MetricConfig.getMetricsQueue().add(metrics);
+        MetricsContextHolder.sendAsync(metrics);
 
         log.debug("Collecting jvm information ends.");
     }

File: dinky-admin/src/main/java/org/dinky/job/FlinkJobTask.java
Patch:
@@ -26,6 +26,7 @@
 import org.dinky.daemon.task.DaemonTaskConfig;
 import org.dinky.data.model.JobInfoDetail;
 import org.dinky.job.handler.JobAlertHandler;
+import org.dinky.job.handler.JobMetricsHandler;
 import org.dinky.job.handler.JobRefeshHandler;
 import org.dinky.service.JobInstanceService;
 
@@ -71,13 +72,12 @@ public DaemonTask setConfig(DaemonTaskConfig config) {
      */
     @Override
     public boolean dealTask() {
-        // TODO: 目前只是完成作业刷新，告警与监控需要继续完善
         volatilityBalance();
         TenantContextHolder.set(1);
 
         boolean isDone = JobRefeshHandler.refeshJob(jobInfoDetail, isNeedSave());
         JobAlertHandler.getInstance().check(jobInfoDetail);
-
+        JobMetricsHandler.writeFlinkMetrics(jobInfoDetail);
         return isDone;
     }
 

File: dinky-admin/src/main/java/org/dinky/job/Job2MysqlHandler.java
Patch:
@@ -200,7 +200,7 @@ public boolean success() {
                 .build();
         jobHistoryService.save(jobHistory);
 
-        DaemonFactory.addTask(DaemonTaskConfig.build(FlinkJobTask.TYPE, jobInstance.getId()));
+        DaemonFactory.refeshOraddTask(DaemonTaskConfig.build(FlinkJobTask.TYPE, jobInstance.getId()));
         return true;
     }
 

File: dinky-admin/src/main/java/org/dinky/job/handler/JobRefeshHandler.java
Patch:
@@ -104,9 +104,9 @@ public static boolean refeshJob(JobInfoDetail jobInfoDetail, boolean needSave) {
             jobInstance.setStatus(getJobStatus(jobInfoDetail).getValue());
             jobInstance.setDuration(
                     job.get(FlinkRestResultConstant.JOB_DURATION).asLong());
-            jobInstance.setCreateTime(TimeUtil.longToLocalDateTime(startTime));
+            jobInstance.setCreateTime(TimeUtil.toLocalDateTime(startTime));
             // if the job is still running the end-time is -1
-            jobInstance.setFinishTime(TimeUtil.longToLocalDateTime(endTime));
+            jobInstance.setFinishTime(TimeUtil.toLocalDateTime(endTime));
         }
         jobInstance.setUpdateTime(LocalDateTime.now());
 

File: dinky-admin/src/main/java/org/dinky/service/JobInstanceService.java
Patch:
@@ -49,6 +49,8 @@ public interface JobInstanceService extends ISuperService<JobInstance> {
 
     JobInfoDetail refreshJobInfoDetail(Integer jobInstanceId);
 
+    void refreshJobByTaskIds(Integer... taskIds);
+
     LineageResult getLineage(Integer id);
 
     JobInstance getJobInstanceByTaskId(Integer id);

File: dinky-alert/dinky-alert-base/src/main/java/org/dinky/alert/Rules/CheckpointsRule.java
Patch:
@@ -37,7 +37,6 @@ public class CheckpointsRule {
     public CheckpointsRule() {
         checkpointsCache = CacheBuilder.newBuilder()
                 .expireAfterAccess(60, TimeUnit.SECONDS)
-                .recordStats()
                 .build(CacheLoader.from(key -> null));
     }
 

File: dinky-alert/dinky-alert-base/src/main/java/org/dinky/alert/Rules/ExceptionRule.java
Patch:
@@ -33,7 +33,6 @@ public class ExceptionRule {
     public ExceptionRule() {
         hisTime = CacheBuilder.newBuilder()
                 .expireAfterAccess(60, TimeUnit.SECONDS)
-                .recordStats()
                 .build(CacheLoader.from(key -> null));
     }
 

File: dinky-daemon/src/main/java/org/dinky/daemon/task/DaemonFactory.java
Patch:
@@ -87,7 +87,7 @@ public static void start(List<DaemonTaskConfig> configList) {
      * @param config
      * add task
      * */
-    public static void addTask(DaemonTaskConfig config) {
+    public static void refeshOraddTask(DaemonTaskConfig config) {
         DefaultThreadPool.getInstance().execute(DaemonTask.build(config));
     }
 }

File: dinky-admin/src/main/java/org/dinky/controller/AlertRuleController.java
Patch:
@@ -19,13 +19,13 @@
 
 package org.dinky.controller;
 
-import org.dinky.configure.schedule.Alert.JobAlerts;
 import org.dinky.data.annotation.Log;
 import org.dinky.data.enums.BusinessType;
 import org.dinky.data.enums.Status;
 import org.dinky.data.model.AlertRule;
 import org.dinky.data.result.ProTableResult;
 import org.dinky.data.result.Result;
+import org.dinky.job.handler.JobAlertHandler;
 import org.dinky.service.AlertRuleService;
 
 import java.util.List;
@@ -53,7 +53,6 @@
 public class AlertRuleController {
 
     private final AlertRuleService alertRuleService;
-    private final JobAlerts jobAlerts;
 
     @PostMapping("/list")
     @ApiOperation("Query alert rules list")
@@ -81,7 +80,7 @@ public ProTableResult<AlertRule> list(@RequestBody JsonNode para) {
     public Result<Boolean> saveOrUpdateAlertRule(@RequestBody AlertRule alertRule) {
         boolean saved = alertRuleService.saveOrUpdate(alertRule);
         if (saved) {
-            jobAlerts.refreshRulesData();
+            JobAlertHandler.getInstance().refreshRulesData();
             return Result.succeed(Status.MODIFY_SUCCESS);
         }
         return Result.failed(Status.MODIFY_FAILED);

File: dinky-admin/src/main/java/org/dinky/controller/JobInstanceController.java
Patch:
@@ -117,7 +117,7 @@ public Result<JobInfoDetail> getJobInfoDetail(@RequestParam Integer id) {
             paramType = "query",
             required = true)
     public Result<JobInfoDetail> refreshJobInfoDetail(@RequestParam Integer id) {
-        return Result.succeed(taskService.refreshJobInfoDetail(id), Status.RESTART_SUCCESS);
+        return Result.succeed(jobInstanceService.refreshJobInfoDetail(id), Status.RESTART_SUCCESS);
     }
 
     /** 获取单任务实例的血缘分析 */

File: dinky-admin/src/main/java/org/dinky/data/options/AlertRuleOptions.java
Patch:
@@ -31,7 +31,8 @@ public class AlertRuleOptions {
     public static final String JOB_ALERT_RULE_CHECK_POINTS = "checkPoints";
     public static final String JOB_ALERT_RULE_CLUSTER = "cluster";
     public static final String JOB_ALERT_RULE_EXCEPTIONS = "exceptions";
-    public static final String JOB_ALERT_RULE_REFRESH_RULES_DATA = "refreshRulesData";
+    public static final String JOB_ALERT_RULE_EXCEPTIONS_MSG = "exceptions_msg";
+    public static final String JOB_ALERT_RULE_EXCEPTION_CHECK = "exceptionRule";
     public static final String JOB_ALERT_RULE_CHECKPOINT_RULES = "checkpointRule";
     public static final String JOB_ALERT_RULE_TASK = "task";
     public static final String JOB_ALERT_RULE_TASK_URL = "taskUrl";

File: dinky-admin/src/main/java/org/dinky/service/JobHistoryService.java
Patch:
@@ -19,6 +19,7 @@
 
 package org.dinky.service;
 
+import org.dinky.data.dto.JobDataDto;
 import org.dinky.data.model.JobHistory;
 import org.dinky.mybatis.service.ISuperService;
 
@@ -33,7 +34,5 @@ public interface JobHistoryService extends ISuperService<JobHistory> {
 
     JobHistory getJobHistory(Integer id);
 
-    JobHistory getJobHistoryInfo(JobHistory jobHistory);
-
-    JobHistory refreshJobHistory(Integer id, String jobManagerHost, String jobId, boolean needSave);
+    JobDataDto getJobHistoryDto(Integer id);
 }

File: dinky-admin/src/main/java/org/dinky/service/JobInstanceService.java
Patch:
@@ -47,7 +47,7 @@ public interface JobInstanceService extends ISuperService<JobInstance> {
 
     JobInfoDetail getJobInfoDetailInfo(JobInstance jobInstance);
 
-    JobInfoDetail refreshJobInfoDetailInfo(JobInstance jobInstance);
+    JobInfoDetail refreshJobInfoDetail(Integer jobInstanceId);
 
     LineageResult getLineage(Integer id);
 

File: dinky-common/src/main/java/org/dinky/data/enums/JobStatus.java
Patch:
@@ -97,7 +97,6 @@ public static boolean isDone(String value) {
             case FAILED:
             case CANCELED:
             case FINISHED:
-            case UNKNOWN:
                 return true;
             default:
                 return false;
@@ -109,7 +108,6 @@ public boolean isDone() {
             case FAILED:
             case CANCELED:
             case FINISHED:
-            case UNKNOWN:
                 return true;
             default:
                 return false;

File: dinky-core/src/main/java/org/dinky/data/constant/FlinkRestResultConstant.java
Patch:
@@ -27,6 +27,9 @@
 public final class FlinkRestResultConstant {
 
     public static final String ERRORS = "errors";
+    public static final String ROOT_EXCEPTION = "root-exception";
     public static final String JOB_DURATION = "duration";
+    public static final String JOB_CREATE_TIME = "start-time";
+    public static final String JOB_FINISH_TIME = "end-time";
     public static final String JOB_STATE = "state";
 }

File: dinky-daemon/src/main/java/org/dinky/daemon/constant/FlinkTaskConstant.java
Patch:
@@ -22,7 +22,7 @@
 public interface FlinkTaskConstant {
 
     /** 检测停顿时间 */
-    int TIME_SLEEP = 1000;
+    int TIME_SLEEP = 1000 * 5;
 
     /** 启动线程轮询日志时间，用于设置work等信息 */
     int MAX_POLLING_GAP = 1000;

File: dinky-common/src/main/java/org/dinky/data/constant/NetConstant.java
Patch:
@@ -27,8 +27,8 @@ public final class NetConstant {
     public static final String COLON = ":";
     /** 斜杠/ */
     public static final String SLASH = "/";
-    /** 连接运行服务器超时时间 1000 */
-    public static final Integer SERVER_TIME_OUT_ACTIVE = 1000;
+    /** 连接运行服务器超时时间 10秒 */
+    public static final Integer SERVER_TIME_OUT_ACTIVE = 10000;
     /** 读取服务器超时时间 3000 */
     public static final Integer READ_TIME_OUT = 3000;
     /** 连接FLINK历史服务器超时时间 2000 */

File: dinky-alert/dinky-alert-email/src/main/java/org/dinky/alert/email/EmailConstants.java
Patch:
@@ -53,4 +53,6 @@ public class EmailConstants extends AlertBaseConstant {
     public static final String XLS_FILE_PATH = "xls.file.path";
     public static final String MAIL_TRANSPORT_PROTOCOL = "mail.transport.protocol";
     public static final String XLS_FILE_DEFAULT_PATH = "/tmp/xls";
+    public static final String ALERT_TEMPLATE_TITLE = "title";
+    public static final String ALERT_TEMPLATE_CONTENT = "content";
 }

File: dinky-alert/dinky-alert-feishu/src/main/java/org/dinky/alert/feishu/FeiShuAlert.java
Patch:
@@ -31,7 +31,6 @@
 
 /** FeiShuAlert */
 public class FeiShuAlert extends AbstractAlert {
-
     private static final Logger logger = LoggerFactory.getLogger(FeiShuAlert.class);
 
     @Override
@@ -47,6 +46,7 @@ public AlertResult send(String title, String content) {
             logger.info("Send FeiShu alert title: {}", title);
             return sender.send(built);
         } catch (TemplateException | IOException e) {
+            logger.error("{}'message send error, Reason:{}", getType(), e.getMessage());
             throw new RuntimeException(e);
         }
     }

File: dinky-alert/dinky-alert-sms/src/main/java/org/dinky/alert/sms/SmsSender.java
Patch:
@@ -39,7 +39,6 @@
 
 /** SmsSender todo: https://wind.kim/doc/start/springboot.html */
 public class SmsSender {
-
     private static final Logger logger = LoggerFactory.getLogger(SmsSender.class);
     private static SupplierConfig configLoader = null;
     private static BaseProviderFactory providerFactory = null;

File: dinky-alert/dinky-alert-base/src/main/java/org/dinky/alert/Rules/CheckpointsRule.java
Patch:
@@ -50,7 +50,7 @@ public CheckpointsRule() {
      */
     private boolean isExpire(JsonNode latest, String key, String ckKey) {
         JsonNode his = checkpointsCache.getIfPresent(key);
-        if (latest.get(ckKey) == null) {
+        if (latest.get(ckKey) == null || !latest.get(ckKey).has("trigger_timestamp")) {
             return true;
         }
         long latestTime = latest.get(ckKey).get("trigger_timestamp").asLong(-1);

File: dinky-common/src/main/java/org/dinky/data/model/Configuration.java
Patch:
@@ -173,9 +173,7 @@ public void addParameterCheck(Consumer<T> consumer) {
     }
 
     public void runParameterCheck() {
-        getParameterCheckConsumer().forEach(x -> {
-            x.accept(getValue());
-        });
+        getParameterCheckConsumer().forEach(x -> x.accept(getValue()));
     }
 
     public void runChangeEvent() {

File: dinky-executor/src/main/java/org/dinky/parser/CustomParserImpl.java
Patch:
@@ -67,6 +67,7 @@ public List<Operation> parse(String statement) {
         return command.map(Collections::singletonList).orElse(null);
     }
 
+    @Override
     public Parser getParser() {
         return parser;
     }

File: dinky-cdc/dinky-cdc-core/src/main/java/org/dinky/cdc/oracle/OracleCDCBuilder.java
Patch:
@@ -36,7 +36,7 @@
 import com.ververica.cdc.debezium.JsonDebeziumDeserializationSchema;
 
 /**
- * MysqlCDCBuilder
+ * OracleCDCBuilder
  *
  * @author wenmo
  * @since 2022/4/12 21:29

File: dinky-admin/src/main/java/org/dinky/controller/FlinkConfController.java
Patch:
@@ -68,7 +68,8 @@ public Result<List<CascaderVO>> loadDataByGroup() {
             "org.apache.flink.configuration.HistoryServerOptions",
             "org.apache.flink.configuration.MetricOptions",
             "org.apache.flink.configuration.NettyShuffleEnvironmentOptions",
-            "org.apache.flink.configuration.RestartStrategyOptions"
+            "org.apache.flink.configuration.RestartStrategyOptions",
+            "org.dinky.constant.CustomerConfigureOptions"
         };
         List<CascaderVO> dataList = new ArrayList<>();
         Arrays.stream(nameList).map(CascaderOptionsUtils::buildCascadeOptions).forEach(dataList::addAll);

File: dinky-admin/src/main/java/org/dinky/utils/CascaderOptionsUtils.java
Patch:
@@ -76,6 +76,7 @@ public static List<CascaderVO> buildCascadeOptions(String name) {
             cascaderVO.setLabel(parsedBinlogGroup);
             cascaderVO.setValue(parsedBinlogGroup);
             dataList.add(cascaderVO);
+            cache.put(name, dataList);
         } catch (ClassNotFoundException ignored) {
             logger.warning("get config option error, class not found: " + name);
         }

File: dinky-core/src/main/java/org/dinky/api/FlinkAPI.java
Patch:
@@ -53,7 +53,7 @@
 public class FlinkAPI {
     private static final Logger logger = LoggerFactory.getLogger(FlinkAPI.class);
 
-    public static final String REST_TARGET_DIRECTORY = "state.savepoints.dir";
+    public static final String REST_TARGET_DIRECTORY = "rest.target-directory";
     public static final String ERRORS = "errors";
     public static final String CANCEL_JOB = "cancel-job";
     public static final String DRAIN = "drain";

File: dinky-admin/src/main/java/org/dinky/service/impl/AlertGroupServiceImpl.java
Patch:
@@ -41,9 +41,11 @@
 import com.baomidou.mybatisplus.core.conditions.query.LambdaQueryWrapper;
 
 import cn.hutool.core.util.StrUtil;
+import lombok.RequiredArgsConstructor;
 
 /** AlertGroupServiceImpl */
 @Service
+@RequiredArgsConstructor
 public class AlertGroupServiceImpl extends SuperServiceImpl<AlertGroupMapper, AlertGroup> implements AlertGroupService {
 
     @Lazy

File: dinky-admin/src/main/java/org/dinky/service/impl/AlertHistoryServiceImpl.java
Patch:
@@ -31,8 +31,11 @@
 
 import com.baomidou.mybatisplus.core.conditions.query.LambdaQueryWrapper;
 
+import lombok.RequiredArgsConstructor;
+
 /** AlertHistoryServiceImpl */
 @Service
+@RequiredArgsConstructor
 public class AlertHistoryServiceImpl extends SuperServiceImpl<AlertHistoryMapper, AlertHistory>
         implements AlertHistoryService {
     /**

File: dinky-core/src/main/java/org/dinky/api/FlinkAPI.java
Patch:
@@ -53,7 +53,7 @@
 public class FlinkAPI {
     private static final Logger logger = LoggerFactory.getLogger(FlinkAPI.class);
 
-    public static final String REST_TARGET_DIRECTORY = "rest.target-directory";
+    public static final String REST_TARGET_DIRECTORY = "state.savepoints.dir";
     public static final String ERRORS = "errors";
     public static final String CANCEL_JOB = "cancel-job";
     public static final String DRAIN = "drain";

File: dinky-core/src/main/java/org/dinky/explainer/Explainer.java
Patch:
@@ -129,7 +129,8 @@ public JobParam pretreatStatements(String[] statements) {
                     || operationType.equals(SqlType.SELECT)
                     || operationType.equals(SqlType.SHOW)
                     || operationType.equals(SqlType.DESCRIBE)
-                    || operationType.equals(SqlType.DESC)) {
+                    || operationType.equals(SqlType.DESC)
+                    || operationType.equals(SqlType.CTAS)) {
                 trans.add(new StatementParam(statement, operationType));
                 statementList.add(statement);
                 if (!useStatementSet) {

File: dinky-common/src/main/java/org/dinky/utils/I18n.java
Patch:
@@ -19,7 +19,6 @@
 
 package org.dinky.utils;
 
-import java.nio.charset.StandardCharsets;
 import java.text.MessageFormat;
 import java.util.Arrays;
 import java.util.Locale;
@@ -52,7 +51,7 @@ public static void setLocale(Locale l) {
     public static String getMessage(String key) {
         bundle = ResourceBundle.getBundle(MESSAGES_BASE);
         String message = bundle.getString(key);
-        message = new String(message.getBytes(StandardCharsets.ISO_8859_1), StandardCharsets.UTF_8);
+        //        message = new String(message.getBytes(StandardCharsets.ISO_8859_1), StandardCharsets.UTF_8);
         return message;
     }
 

File: dinky-admin/src/main/java/org/dinky/data/dto/StudioExecuteDTO.java
Patch:
@@ -22,6 +22,7 @@
 import org.dinky.data.model.TaskExtConfig;
 import org.dinky.job.JobConfig;
 
+import java.util.HashMap;
 import java.util.Map;
 
 import lombok.Getter;
@@ -64,7 +65,8 @@ public class StudioExecuteDTO extends AbstractStatementDTO {
 
     public JobConfig getJobConfig() {
 
-        Map<String, String> parsedConfig = this.configJson.getCustomConfigMaps();
+        Map<String, String> parsedConfig =
+                this.configJson == null ? new HashMap<>(0) : this.configJson.getCustomConfigMaps();
 
         return new JobConfig(
                 type,

File: dinky-admin/src/main/java/org/dinky/service/PrintTableService.java
Patch:
@@ -21,7 +21,7 @@
 
 import org.springframework.web.servlet.mvc.method.annotation.SseEmitter;
 
-public interface WatchTableService {
+public interface PrintTableService {
     SseEmitter registerListenEntry(String table);
 
     void unRegisterListenEntry(String table);

File: dinky-admin/src/main/java/org/dinky/service/StatementService.java
Patch:
@@ -33,5 +33,5 @@ public interface StatementService extends ISuperService<Statement> {
 
     boolean insert(Statement statement);
 
-    List<String> getWatchTables(String statement);
+    List<String> getPrintTables(String statement);
 }

File: dinky-admin/src/test/java/org/dinky/service/impl/PrintTableServiceImplTest.java
Patch:
@@ -23,15 +23,15 @@
 
 import org.junit.jupiter.api.Test;
 
-class WatchTableServiceImplTest {
+class PrintTableServiceImplTest {
 
     @Test
     void getDestination() {
         String tableName = "`default_catalog`.`default_database`.`Orders`";
-        String result = WatchTableServiceImpl.getDestination(tableName);
+        String result = PrintTableServiceImpl.getDestination(tableName);
         assertEquals("/topic/table/`default_catalog`.`default_database`.`print_Orders`", result);
 
-        result = WatchTableServiceImpl.getDestination("Orders");
+        result = PrintTableServiceImpl.getDestination("Orders");
         assertEquals("/topic/table/`default_catalog`.`default_database`.`print_Orders`", result);
     }
 }

File: dinky-executor/src/main/java/org/dinky/parser/SqlType.java
Patch:
@@ -59,7 +59,7 @@ public enum SqlType {
     ADD_JAR("ADD_JAR", "^ADD\\s+JAR\\s+\\S+"),
     ADD("ADD", "^ADD\\s+CUSTOMJAR\\s+\\S+"),
 
-    WATCH("WATCH", "^WATCH.*"),
+    PRINT("PRINT", "^PRINT.*"),
 
     CTAS("CTAS", "^CREATE\\s.*AS\\sSELECT.*$"),
 

File: dinky-executor/src/test/java/org/dinky/parser/SqlTypeTest.java
Patch:
@@ -54,7 +54,7 @@ public void match() {
         test("EXECUTE...", SqlType.EXECUTE, true);
         test("ADD jar ...", SqlType.ADD_JAR, true);
         test("ADD customjar ...", SqlType.ADD, true);
-        test("WATCH...", SqlType.WATCH, true);
+        test("PRINT...", SqlType.PRINT, true);
 
         String sql = "CREATE TABLE print_OrdersView WITH ('connector' = 'printnet', 'port'='7125',"
                 + " 'hostName' = '172.26.16.1', 'sink.parallelism'='1')\n"

File: dinky-admin/src/main/java/org/dinky/service/impl/TaskServiceImpl.java
Patch:
@@ -947,7 +947,7 @@ private JobConfig buildJobConfig(Task task) {
         Savepoints savepoints = buildSavepoint(config);
         if (Asserts.isNotNull(savepoints)) {
             config.setSavePointPath(savepoints.getPath());
-            config.getConfigJson().put("execution.savepoint.path", savepoints.getPath());
+            config.getConfigJson().put("execution.savepoint.path", savepoints.getPath()); // todo: 写工具类处理相关配置
         }
 
         if (!GatewayType.get(task.getType()).isDeployCluster()) {

File: dinky-core/src/main/java/org/dinky/job/JobConfig.java
Patch:
@@ -275,7 +275,7 @@ public void buildGatewayConfig(FlinkClusterConfig config) {
                 .getFlinkConfig()
                 .getConfiguration()
                 .put(CoreOptions.DEFAULT_PARALLELISM.key(), String.valueOf(parallelism));
-        setUseRemote(false);
+        setUseRemote(false); // todo: remove
     }
 
     public void addGatewayConfig(Map<String, Object> config) {

File: dinky-core/src/main/java/org/dinky/job/JobManager.java
Patch:
@@ -186,9 +186,10 @@ public static JobManager buildPlanMode(JobConfig config) {
 
     private Executor createExecutor() {
         initEnvironmentSetting();
-        if (!runMode.equals(GatewayType.LOCAL) && !useGateway && config.isUseRemote()) {
-            executor = Executor.buildRemoteExecutor(environmentSetting, config.getExecutorSetting());
+        if (!runMode.isLocalExecute()) {
+            executor = Executor.buildRemoteExecutor(environmentSetting, config.getExecutorSetting()); // 构建远端执行器
         } else {
+            // 构建本地执行器
             if (ArrayUtil.isNotEmpty(config.getJarFiles())) {
                 config.getExecutorSetting()
                         .getConfig()

File: dinky-admin/src/main/java/org/dinky/configure/MetricConfig.java
Patch:
@@ -52,7 +52,7 @@ public class MetricConfig {
     /** Entering the lake every 10 states */
     @Scheduled(fixedRate = 10000)
     @PreDestroy
-    public void writeScheduled() {
+    public void writeScheduled() throws Exception {
         PaimonUtil.writeMetrics(new ArrayList<>(METRICS_QUEUE));
         METRICS_QUEUE.clear();
     }

File: dinky-admin/src/main/java/org/dinky/configure/schedule/metrics/GatherSysIndicator.java
Patch:
@@ -22,6 +22,7 @@
 import org.dinky.configure.MetricConfig;
 import org.dinky.configure.schedule.BaseSchedule;
 import org.dinky.data.annotation.GaugeM;
+import org.dinky.data.enums.MetricsType;
 import org.dinky.data.metrics.BaseMetrics;
 import org.dinky.data.metrics.Cpu;
 import org.dinky.data.metrics.Jvm;
@@ -42,7 +43,6 @@
 import cn.hutool.core.annotation.AnnotationUtil;
 import cn.hutool.core.lang.Opt;
 import cn.hutool.core.util.ReflectUtil;
-import cn.hutool.json.JSONUtil;
 import io.micrometer.core.instrument.Gauge;
 import io.micrometer.core.instrument.MeterRegistry;
 import lombok.RequiredArgsConstructor;
@@ -90,9 +90,9 @@ public void updateState() {
         metricsTotal.setMem(Mem.of());
 
         MetricsVO metrics = new MetricsVO();
-        metrics.setContent(JSONUtil.toJsonStr(metricsTotal));
+        metrics.setContent(metricsTotal);
         metrics.setHeartTime(now);
-        metrics.setModel("local");
+        metrics.setModel(MetricsType.LOCAL.getType());
         MetricConfig.getMetricsQueue().add(metrics);
 
         log.debug("Collecting jvm information ends.");

File: dinky-admin/src/main/java/org/dinky/controller/SseController.java
Patch:
@@ -49,7 +49,8 @@ public SseEmitter getLastUpdateData(Long lastTime) {
         SseEmitter emitter = new SseEmitterUTF8(TimeUnit.MINUTES.toMillis(30));
         return monitorService.sendLatestData(
                 emitter,
-                DateUtil.date(Opt.ofNullable(lastTime).orElse(DateUtil.date().getTime())));
+                DateUtil.date(Opt.ofNullable(lastTime).orElse(DateUtil.date().getTime())),
+                null);
     }
 
     @GetMapping(value = "/getJvmInfo", produces = MediaType.TEXT_EVENT_STREAM_VALUE)

File: dinky-admin/src/main/java/org/dinky/utils/PaimonUtil.java
Patch:
@@ -61,6 +61,7 @@
 import cn.hutool.core.util.ReflectUtil;
 import cn.hutool.core.util.StrUtil;
 import cn.hutool.core.util.URLUtil;
+import cn.hutool.json.JSONUtil;
 import lombok.extern.slf4j.Slf4j;
 
 @Slf4j
@@ -113,9 +114,9 @@ public static synchronized void writeMetrics(List<MetricsVO> metricsList) {
 
                 BinaryRow row = new BinaryRow(30);
                 BinaryRowWriter writer = new BinaryRowWriter(row);
-                writer.writeTimestamp(0, Timestamp.fromLocalDateTime(now), 0);
+                writer.writeTimestamp(0, Timestamp.fromLocalDateTime(now), 3);
                 writer.writeString(1, BinaryString.fromString(metrics.getModel()));
-                writer.writeString(2, BinaryString.fromString(metrics.getContent()));
+                writer.writeString(2, BinaryString.fromString(JSONUtil.toJsonStr(metrics.getContent())));
                 writer.writeString(3, BinaryString.fromString(now.format(DateTimeFormatter.ofPattern("yyyy-MM-dd"))));
                 write.write(row);
             }

File: dinky-core/src/main/java/org/dinky/data/constant/FlinkRestAPIConstant.java
Patch:
@@ -33,6 +33,7 @@ public final class FlinkRestAPIConstant {
     public static final String CONFIG = "/config";
 
     public static final String JOBS = "jobs/";
+    public static final String VERTICES = "/vertices/";
 
     public static final String JOBSLIST = "jobs/overview";
 

File: dinky-admin/src/main/java/org/dinky/aop/WebExceptionHandler.java
Patch:
@@ -23,7 +23,7 @@
 import org.dinky.data.enums.Status;
 import org.dinky.data.exception.BusException;
 import org.dinky.data.result.Result;
-import org.dinky.utils.I18nMsgUtils;
+import org.dinky.utils.I18n;
 
 import org.apache.commons.lang3.StringUtils;
 
@@ -64,7 +64,7 @@ public class WebExceptionHandler {
     @ExceptionHandler
     public Result<Void> busException(BusException e) {
         if (StrUtil.isEmpty(e.getMsg())) {
-            return Result.failed(I18nMsgUtils.getMsg(e.getCode(), e.getErrorArgs()));
+            return Result.failed(I18n.getMessage(e.getCode(), e.getMessage()));
         }
         return Result.failed(e.getMsg());
     }

File: dinky-admin/src/main/java/org/dinky/controller/AdminController.java
Patch:
@@ -25,7 +25,6 @@
 import org.dinky.data.model.Tenant;
 import org.dinky.data.result.Result;
 import org.dinky.service.UserService;
-import org.dinky.utils.I18nMsgUtils;
 
 import org.springframework.web.bind.annotation.DeleteMapping;
 import org.springframework.web.bind.annotation.GetMapping;
@@ -80,7 +79,7 @@ public Result<UserDTO> login(@RequestBody LoginDTO loginDTO) {
     @SaIgnore
     public Result<Void> outLogin() {
         userService.outLogin();
-        return Result.succeed(I18nMsgUtils.getMsg(Status.SIGN_OUT_SUCCESS));
+        return Result.succeed(Status.SIGN_OUT_SUCCESS.getMessage());
     }
 
     /**

File: dinky-admin/src/main/java/org/dinky/controller/DataBaseController.java
Patch:
@@ -180,7 +180,7 @@ public Result<Void> testConnect(@RequestBody DataBase database) {
     @ApiOperation("DataBase Check Heart Beat By Id")
     public Result<Void> checkHeartBeatByDataSourceId(@RequestParam Integer id) {
         DataBase dataBase = databaseService.getById(id);
-        Asserts.checkNotNull(dataBase, Status.DATASOURCE_NOT_EXIST.getMsg());
+        Asserts.checkNotNull(dataBase, Status.DATASOURCE_NOT_EXIST.getMessage());
         String error = "";
         try {
             databaseService.checkHeartBeat(dataBase);

File: dinky-admin/src/main/java/org/dinky/data/exception/AuthException.java
Patch:
@@ -34,12 +34,12 @@ public class AuthException extends Exception {
     private Status status;
 
     public AuthException(Status status) {
-        super(status.getMsg());
+        super(status.getMessage());
         this.status = status;
     }
 
     public AuthException(Throwable cause, Status status) {
-        super(status.getMsg(), cause);
+        super(status.getMessage(), cause);
         this.status = status;
     }
 }

File: dinky-admin/src/main/java/org/dinky/interceptor/LocaleChangeInterceptor.java
Patch:
@@ -21,6 +21,7 @@
 
 import org.dinky.assertion.Asserts;
 import org.dinky.data.constant.BaseConstant;
+import org.dinky.utils.I18n;
 
 import java.util.Locale;
 
@@ -42,11 +43,13 @@ public boolean preHandle(HttpServletRequest request, HttpServletResponse respons
         if (Asserts.isNotNull(cookie)) {
             // Proceed in cookie
             LocaleContextHolder.setLocale(parseLocaleValue(cookie.getValue()));
+            I18n.setLocale(LocaleContextHolder.getLocale());
         }
         // Proceed in header
         String newLocale = request.getHeader(BaseConstant.LOCALE_LANGUAGE_COOKIE);
         if (Asserts.isNotNull(newLocale)) {
             LocaleContextHolder.setLocale(parseLocaleValue(newLocale));
+            I18n.setLocale(LocaleContextHolder.getLocale());
         }
         return true;
     }

File: dinky-admin/src/main/java/org/dinky/service/impl/JobInstanceServiceImpl.java
Patch:
@@ -141,7 +141,7 @@ public JobInfoDetail getJobInfoDetail(Integer id) {
 
     @Override
     public JobInfoDetail getJobInfoDetailInfo(JobInstance jobInstance) {
-        Asserts.checkNull(jobInstance, Status.JOB_INSTANCE_NOT_EXIST.getMsg());
+        Asserts.checkNull(jobInstance, Status.JOB_INSTANCE_NOT_EXIST.getMessage());
         String key = jobInstance.getId().toString();
         FlinkJobTaskPool pool = FlinkJobTaskPool.INSTANCE;
         if (pool.containsKey(key)) {
@@ -164,7 +164,7 @@ public JobInfoDetail getJobInfoDetailInfo(JobInstance jobInstance) {
 
     @Override
     public JobInfoDetail refreshJobInfoDetailInfo(JobInstance jobInstance) {
-        Asserts.checkNull(jobInstance, Status.JOB_INSTANCE_NOT_EXIST.getMsg());
+        Asserts.checkNull(jobInstance, Status.JOB_INSTANCE_NOT_EXIST.getMessage());
         JobInfoDetail jobInfoDetail;
         FlinkJobTaskPool pool = FlinkJobTaskPool.INSTANCE;
         String key = jobInstance.getId().toString();
@@ -238,7 +238,7 @@ public ProTableResult<JobInstance> listJobInstances(JsonNode para) {
     @Override
     public void initTenantByJobInstanceId(Integer id) {
         Integer tenantId = baseMapper.getTenantByJobInstanceId(id);
-        Asserts.checkNull(tenantId, Status.JOB_INSTANCE_NOT_EXIST.getMsg());
+        Asserts.checkNull(tenantId, Status.JOB_INSTANCE_NOT_EXIST.getMessage());
         TenantContextHolder.set(tenantId);
     }
 }

File: dinky-admin/src/main/java/org/dinky/service/impl/LdapServiceImpl.java
Patch:
@@ -111,7 +111,7 @@ public User authenticate(LoginDTO loginDTO) throws AuthException {
     @Override
     public List<User> listUsers() {
         String filter = configuration.getLdapFilter().getValue();
-        Assert.notBlank(filter, Status.LDAP_FILTER_INCORRECT.getMsg());
+        Assert.notBlank(filter, Status.LDAP_FILTER_INCORRECT.getMessage());
 
         LdapTemplate ldapTemplate = new LdapTemplate(LdapContext.getLdapContext());
         List<User> result = ldapTemplate.search(

File: dinky-admin/src/main/java/org/dinky/service/impl/LoginLogServiceImpl.java
Patch:
@@ -75,7 +75,7 @@ public void saveLoginLog(User user, Status status, String ip) {
         loginLog.setAccessTime(LocalDateTime.now());
         loginLog.setIp(ip);
         loginLog.setStatus(status.getCode());
-        loginLog.setMsg(status.getMsg());
+        loginLog.setMsg(status.getMessage());
         saveOrUpdate(loginLog);
     }
 
@@ -92,7 +92,7 @@ public void saveLoginLog(User user, Status status) {
         loginLog.setAccessTime(LocalDateTime.now());
         loginLog.setIp(Ipv4Util.LOCAL_IP);
         loginLog.setStatus(status.getCode());
-        loginLog.setMsg(status.getMsg());
+        loginLog.setMsg(status.getMessage());
         saveOrUpdate(loginLog);
     }
 

File: dinky-admin/src/main/java/org/dinky/data/dto/StudioExecuteDTO.java
Patch:
@@ -62,7 +62,7 @@ public class StudioExecuteDTO extends AbstractStatementDTO {
     private Integer parallelism;
     private Integer savePointStrategy;
     private String savePointPath;
-    private Map<String, String> config = new HashMap<>();
+    private Map<String, String> configJson = new HashMap<>();
     private static final ObjectMapper mapper = new ObjectMapper();
 
     public JobConfig getJobConfig() {
@@ -87,7 +87,7 @@ public JobConfig getJobConfig() {
                 savePointStrategy,
                 savePointPath,
                 getVariables(),
-                config);
+                configJson);
     }
 
     public Integer getTaskId() {

File: dinky-admin/src/main/java/org/dinky/service/impl/TaskServiceImpl.java
Patch:
@@ -942,7 +942,7 @@ private JobConfig buildJobConfig(Task task) {
         Savepoints savepoints = buildSavepoint(config);
         if (Asserts.isNotNull(savepoints)) {
             config.setSavePointPath(savepoints.getPath());
-            config.getConfig().put("execution.savepoint.path", savepoints.getPath());
+            config.getConfigJson().put("execution.savepoint.path", savepoints.getPath());
         }
 
         if (!GatewayType.get(task.getType()).isDeployCluster()) {
@@ -977,7 +977,7 @@ private Savepoints buildSavepoint(JobConfig config) {
     private FlinkClusterConfig buildGatewayCfgObj(JobConfig config) {
         FlinkClusterConfig flinkClusterCfg = clusterCfgService.getFlinkClusterCfg(config.getClusterConfigurationId());
         flinkClusterCfg.getAppConfig().setUserJarParas(buildParas(config.getTaskId()));
-        flinkClusterCfg.getFlinkConfig().getConfiguration().putAll(config.getConfig());
+        flinkClusterCfg.getFlinkConfig().getConfiguration().putAll(config.getConfigJson());
 
         //        if (config.isJarTask()) {
         //            JSONObject clusterObj = new JSONObject(flinkClusterCfg);

File: dinky-core/src/test/java/org/dinky/job/JobConfigTest.java
Patch:
@@ -40,7 +40,7 @@ void setAddress() {
         Map<String, String> config = new HashMap<>();
         config.put(RestOptions.PORT.key(), "9999");
 
-        jobConfig.setConfig(config);
+        jobConfig.setConfigJson(config);
         jobConfig.setAddress("127.0.0.1:7777");
         assertEquals("127.0.0.1:9999", jobConfig.getAddress());
 

File: dinky-admin/src/main/java/org/dinky/mapper/UserMapper.java
Patch:
@@ -34,4 +34,6 @@
 public interface UserMapper extends SuperMapper<User> {
 
     Integer queryAdminUserByTenant(@Param("tenantId") Integer tenantId);
+
+    Integer recoveryUser(@Param("id") Integer userId);
 }

File: dinky-metadata/dinky-metadata-base/src/main/java/org/dinky/metadata/driver/AbstractDriver.java
Patch:
@@ -107,7 +107,7 @@ public String getSqlSelect(Table table) {
                 }
                 sb.append(String.format("`%s`  --  %s \n", columns.get(i).getName(), columnComment));
             } else {
-                sb.append(String.format("`%s` %\n", columns.get(i).getName()));
+                sb.append(String.format("`%s` \n", columns.get(i).getName()));
             }
         }
 

File: dinky-admin/src/main/java/org/dinky/controller/MenuController.java
Patch:
@@ -98,7 +98,7 @@ public Result<Void> deleteMenuById(@RequestParam("id") Integer id) {
      * @param roleId role id
      * @return {@link RoleMenuDto}
      */
-    @GetMapping(value = "/roleMenus/")
+    @GetMapping(value = "/roleMenus")
     @ApiOperation("Load Role Menu")
     public Result<RoleMenuDto> roleMenuTreeSelect(@RequestParam("id") Integer roleId) {
         List<Menu> menus = menuService.buildMenuTree(menuService.list());

File: dinky-admin/src/main/java/org/dinky/service/RoleService.java
Patch:
@@ -20,6 +20,7 @@
 package org.dinky.service;
 
 import org.dinky.data.model.Role;
+import org.dinky.data.model.User;
 import org.dinky.data.result.ProTableResult;
 import org.dinky.data.result.Result;
 import org.dinky.mybatis.service.ISuperService;
@@ -73,4 +74,6 @@ public interface RoleService extends ISuperService<Role> {
      * @return role list
      */
     List<Integer> selectRoleListByUserId(Integer userId);
+
+    List<User> getUserListByRoleId(Integer roleId);
 }

File: dinky-admin/src/main/java/org/dinky/service/impl/UserServiceImpl.java
Patch:
@@ -407,7 +407,7 @@ public List<User> getUserListByTenantId(int id) {
     @Override
     public Result<Void> modifyUserToTenantAdmin(Integer userId, Integer tenantId, Boolean tenantAdminFlag) {
         // query tenant admin user count
-        Long queryAdminUserByTenantCount = userTenantService.count(new LambdaQueryWrapper<UserTenant>()
+        long queryAdminUserByTenantCount = userTenantService.count(new LambdaQueryWrapper<UserTenant>()
                 .eq(UserTenant::getTenantId, tenantId)
                 .eq(UserTenant::getTenantAdminFlag, 1));
         if (queryAdminUserByTenantCount >= 1 && !tenantAdminFlag) {

File: dinky-admin/src/main/java/org/dinky/controller/UDFController.java
Patch:
@@ -35,6 +35,7 @@
 
 import org.springframework.transaction.annotation.Transactional;
 import org.springframework.web.bind.annotation.DeleteMapping;
+import org.springframework.web.bind.annotation.GetMapping;
 import org.springframework.web.bind.annotation.PostMapping;
 import org.springframework.web.bind.annotation.PutMapping;
 import org.springframework.web.bind.annotation.RequestBody;
@@ -65,7 +66,7 @@ public class UDFController {
      *
      * @return
      */
-    @PostMapping("/tree")
+    @GetMapping("/tree")
     @ApiOperation("Build UDF Tree")
     public Result<List<Object>> listUdfTemplates() {
         List<UDFTemplate> list = udfTemplateService.list();

File: dinky-metadata/dinky-metadata-postgresql/src/main/java/org/dinky/metadata/query/PostgreSqlQuery.java
Patch:
@@ -59,7 +59,7 @@ public String columnsSql(String schemaName, String tableName) {
                 + "     , col.ordinal_position                         as ordinal_position\n"
                 + "     , col.udt_name                                 as type\n"
                 + "     , (CASE  WHEN (SELECT COUNT(*) FROM pg_constraint AS PC WHERE b.attnum"
-                + " = PC.conkey[1] AND PC.contype = 'p' and PC.conrelid = c.oid) > 0 \n"
+                + " = ANY(PC.conkey) AND PC.contype = 'p' and PC.conrelid = c.oid) > 0 \n"
                 + "THEN 'PRI' ELSE '' END)                            AS key\n"
                 + "     , col_description(c.oid, col.ordinal_position) AS comment\n"
                 + "     , col.column_default                           AS column_default\n"

File: dinky-admin/src/main/java/org/dinky/service/impl/TaskVersionServiceImpl.java
Patch:
@@ -37,6 +37,8 @@ public class TaskVersionServiceImpl extends SuperServiceImpl<TaskVersionMapper,
     @Override
     public List<TaskVersion> getTaskVersionByTaskId(Integer taskId) {
 
-        return baseMapper.selectList(new LambdaQueryWrapper<TaskVersion>().eq(TaskVersion::getTaskId, taskId));
+        return baseMapper.selectList(new LambdaQueryWrapper<TaskVersion>()
+                .eq(TaskVersion::getTaskId, taskId)
+                .orderByDesc(true, TaskVersion::getVersionId));
     }
 }

File: dinky-admin/src/main/java/org/dinky/controller/MonitorController.java
Patch:
@@ -34,7 +34,6 @@
 import java.util.concurrent.TimeUnit;
 
 import org.springframework.http.MediaType;
-import org.springframework.web.bind.annotation.CrossOrigin;
 import org.springframework.web.bind.annotation.GetMapping;
 import org.springframework.web.bind.annotation.PutMapping;
 import org.springframework.web.bind.annotation.RequestBody;
@@ -66,7 +65,6 @@ public Result<List<MetricsVO>> getData(@RequestParam Long startTime, Long endTim
     }
 
     @GetMapping(value = "/getLastUpdateData", produces = MediaType.TEXT_EVENT_STREAM_VALUE)
-    @CrossOrigin("*")
     @PublicInterface
     @ApiOperation("Get Last Update Data")
     public SseEmitter getLastUpdateData(Long lastTime) {

File: dinky-admin/src/main/java/org/dinky/data/constant/BaseConstant.java
Patch:
@@ -77,4 +77,6 @@ public class BaseConstant {
     public static final String UNIQUE = "0";
 
     public static final String NOT_UNIQUE = "1";
+    /** admin user id */
+    public static final int ADMIN_ID = 1;
 }

File: dinky-admin/src/main/java/org/dinky/data/model/Menu.java
Patch:
@@ -79,7 +79,7 @@ public class Menu implements Serializable {
 
     private String icon;
 
-    private Integer type;
+    private String type;
 
     private Double orderNum;
 

File: dinky-admin/src/main/java/org/dinky/crypto/CryptoComponent.java
Patch:
@@ -22,6 +22,8 @@
 import org.jasypt.util.text.AES256TextEncryptor;
 import org.springframework.stereotype.Component;
 
+import com.google.common.base.Strings;
+
 @Component
 public class CryptoComponent {
 
@@ -31,7 +33,7 @@ public class CryptoComponent {
     public CryptoComponent(CryptoProperties cryptoProperties) {
         this.enabled = cryptoProperties.getEnabled();
         this.textEncryptor = new AES256TextEncryptor();
-        if (cryptoProperties.getEncryptionPassword() != null) {
+        if (!Strings.isNullOrEmpty(cryptoProperties.getEncryptionPassword())) {
             this.textEncryptor.setPassword(cryptoProperties.getEncryptionPassword());
         }
     }

File: dinky-admin/src/main/java/org/dinky/service/ClusterConfigurationService.java
Patch:
@@ -20,11 +20,11 @@
 package org.dinky.service;
 
 import org.dinky.data.model.ClusterConfiguration;
+import org.dinky.gateway.model.FlinkClusterConfig;
 import org.dinky.gateway.result.TestResult;
 import org.dinky.mybatis.service.ISuperService;
 
 import java.util.List;
-import java.util.Map;
 
 /**
  * ClusterConfigService
@@ -37,7 +37,7 @@ public interface ClusterConfigurationService extends ISuperService<ClusterConfig
 
     List<ClusterConfiguration> listEnabledAll();
 
-    Map<String, Object> getGatewayConfig(Integer id);
+    FlinkClusterConfig getFlinkClusterCfg(Integer id);
 
     TestResult testGateway(ClusterConfiguration clusterConfiguration);
 

File: dinky-common/src/main/java/org/dinky/data/constant/CommonConstant.java
Patch:
@@ -28,4 +28,7 @@ public final class CommonConstant {
 
     /** 实例健康 */
     public static final String HEALTHY = "1";
+
+    public static final String DINKY_APP_MAIN_CLASS = "org.dinky.app.MainApp";
+    public static final String LineSep = System.getProperty("line.separator");
 }

File: dinky-gateway/src/main/java/org/dinky/gateway/AbstractGateway.java
Patch:
@@ -62,7 +62,7 @@ public abstract class AbstractGateway implements Gateway {
 
     protected static final Logger logger = LoggerFactory.getLogger(AbstractGateway.class);
     protected GatewayConfig config;
-    protected Configuration configuration;
+    protected Configuration configuration = new Configuration();
 
     public AbstractGateway() {}
 

File: dinky-gateway/src/main/java/org/dinky/gateway/kubernetes/KubernetesApplicationGateway.java
Patch:
@@ -73,7 +73,7 @@ public GatewayResult submitJar() {
                 new ApplicationConfiguration(userJarParas, appConfig.getUserJarMainAppClass());
 
         KubernetesResult result = KubernetesResult.build(getType());
-        ;
+
         try (KubernetesClusterDescriptor kubernetesClusterDescriptor =
                 new KubernetesClusterDescriptor(configuration, client)) {
             ClusterClientProvider<String> clusterClientProvider =

File: dinky-admin/src/main/java/org/dinky/data/dto/TreeNodeDTO.java
Patch:
@@ -36,11 +36,14 @@
 @AllArgsConstructor
 @ToString
 public class TreeNodeDTO {
+    private Integer id;
     private String name;
     private String path;
     private String content;
+    private Integer parentId;
     private Long size;
     private boolean isLeaf;
+    private String desc;
     private List<TreeNodeDTO> children;
 
     public TreeNodeDTO(

File: dinky-common/src/main/java/org/dinky/utils/SqlUtil.java
Patch:
@@ -60,7 +60,8 @@ public static String removeNote(String sql) {
             // Remove the special-space characters
             sql = sql.replaceAll("\u00A0", " ").replaceAll("[\r\n]+", "\n");
             // Remove annotations Support '--aa' , '/**aaa*/' , '//aa' , '#aaa'
-            Pattern p = Pattern.compile("(?ms)('(?:''|[^'])*')|--.*?$|//.*?$|/\\*.*?\\*/|#.*?$|");
+            Pattern p =
+                    Pattern.compile("(?ms)('(?:''|[^'])*')|--.*?$|//.*?$|/\\*[^+].*?\\*/|#.*?$|");
             String presult = p.matcher(sql).replaceAll("$1");
             return presult.trim();
         }

File: dinky-client/dinky-client-base/src/main/java/org/dinky/executor/CustomTableEnvironment.java
Patch:
@@ -34,6 +34,7 @@
 import org.apache.flink.table.delegation.Planner;
 import org.apache.flink.types.Row;
 
+import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 
@@ -67,7 +68,7 @@ boolean parseAndLoadConfiguration(
     Configuration getRootConfiguration();
 
     default List<LineageRel> getLineage(String statement) {
-        return null;
+        return Collections.emptyList();
     }
 
     <T> void createTemporaryView(String s, DataStream<Row> dataStream, List<String> columnNameList);

File: dinky-common/src/main/java/org/dinky/utils/SqlUtil.java
Patch:
@@ -34,6 +34,8 @@ public class SqlUtil {
 
     private static final String SEMICOLON = ";";
 
+    private SqlUtil() {}
+
     public static String[] getStatements(String sql) {
         return getStatements(sql, SystemConfiguration.getInstances().getSqlSeparator());
     }

File: dinky-executor/src/main/java/org/dinky/parser/SingleSqlParserFactory.java
Patch:
@@ -19,6 +19,7 @@
 
 package org.dinky.parser;
 
+import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 import java.util.regex.Matcher;
@@ -33,6 +34,7 @@
  * @since 2021/6/14 16:49
  */
 public class SingleSqlParserFactory {
+    private SingleSqlParserFactory() {}
 
     protected static final Logger logger = LoggerFactory.getLogger(SingleSqlParserFactory.class);
 
@@ -61,7 +63,7 @@ public static Map<String, List<String>> generateParser(String sql) {
 
         if (tmp == null) {
             logger.error("sql: {} illegal.", sql);
-            return null;
+            return Collections.emptyMap();
         }
 
         return tmp.splitSql2Segment();

File: dinky-admin/src/main/java/org/dinky/context/LdapContext.java
Patch:
@@ -103,6 +103,7 @@ public User mapFromAttributes(Attributes attributes) throws NamingException {
                 user.setUsername(usernameAttr.get().toString());
                 user.setNickname(nicknameAttr.get().toString());
                 user.setUserType(UserType.LDAP.getCode());
+                user.setEnabled(true);
                 return user;
             }
 

File: dinky-alert/dinky-alert-email/src/main/java/org/dinky/alert/email/template/DefaultHTMLTemplate.java
Patch:
@@ -162,7 +162,7 @@ private String getMessageFromHtmlTemplate(String title, String content) {
 
         requireNonNull(content, "content must not null");
         String htmlTableThead =
-                StringUtils.isEmpty(title) ? "" : String.format("<thead>%s</thead>%n", title);
+                StringUtils.isEmpty(title) ? "" : String.format("<thead>%s</thead>\n", title);
 
         return EmailConstants.HTML_HEADER_PREFIX
                 + htmlTableThead

File: dinky-common/src/main/java/org/dinky/data/model/Table.java
Patch:
@@ -153,7 +153,7 @@ public String getFlinkDDL(String flinkConfig, String tableName) {
     @Transient
     public String getFlinkTableSql(String catalogName, String flinkConfig) {
         String createSql = getFlinkDDL(getFlinkTableWith(flinkConfig), name);
-        return String.format("DROP TABLE IF EXISTS %s;%n%s", name, createSql);
+        return String.format("DROP TABLE IF EXISTS %s;\n%s", name, createSql);
     }
 
     @Override

File: dinky-metadata/dinky-metadata-mysql/src/main/java/org/dinky/metadata/driver/MySqlDriver.java
Patch:
@@ -218,10 +218,10 @@ public String getSqlSelect(Table table) {
         if (Asserts.isNotNullString(table.getComment())) {
             sb.append(
                     String.format(
-                            " FROM `%s`.`%s`; -- %s%n",
+                            " FROM `%s`.`%s`; -- %s\n",
                             table.getSchema(), table.getName(), table.getComment()));
         } else {
-            sb.append(String.format(" FROM `%s`.`%s`;%n", table.getSchema(), table.getName()));
+            sb.append(String.format(" FROM `%s`.`%s`;\n", table.getSchema(), table.getName()));
         }
         return sb.toString();
     }

File: dinky-admin/src/main/java/org/dinky/controller/SystemController.java
Patch:
@@ -69,6 +69,6 @@ public Result<String> getRootLog() {
      */
     @GetMapping("/readFile")
     public Result<String> readFile(@RequestParam String path) {
-        return Result.succeed(systemService.readFile(path));
+        return Result.data(systemService.readFile(path));
     }
 }

File: dinky-admin/src/main/java/org/dinky/controller/GitController.java
Patch:
@@ -19,6 +19,7 @@
 
 package org.dinky.controller;
 
+import org.dinky.data.annotation.PublicInterface;
 import org.dinky.data.dto.GitProjectDTO;
 import org.dinky.data.dto.TreeNodeDTO;
 import org.dinky.data.enums.Status;
@@ -209,6 +210,7 @@ public Result<Void> build(@RequestParam("id") Integer id) {
      */
     @GetMapping(path = "/build-step-logs", produces = MediaType.TEXT_EVENT_STREAM_VALUE)
     @CrossOrigin("*")
+    @PublicInterface
     public SseEmitter buildStepLogs(@RequestParam("id") Integer id) {
         SseEmitter emitter = new SseEmitterUTF8(TimeUnit.MINUTES.toMillis(30));
         GitProject gitProject = gitProjectService.getById(id);

File: dinky-admin/src/main/java/org/dinky/controller/MonitorController.java
Patch:
@@ -19,6 +19,7 @@
 
 package org.dinky.controller;
 
+import org.dinky.data.annotation.PublicInterface;
 import org.dinky.data.result.Result;
 import org.dinky.data.vo.MetricsVO;
 import org.dinky.service.MonitorService;
@@ -57,6 +58,7 @@ public Result<List<MetricsVO>> getData(@RequestParam Long startTime, Long endTim
 
     @GetMapping(value = "/getLastUpdateData", produces = MediaType.TEXT_EVENT_STREAM_VALUE)
     @CrossOrigin("*")
+    @PublicInterface
     public SseEmitter getLastUpdateData(Long lastTime) {
         SseEmitter emitter = new SseEmitterUTF8(TimeUnit.MINUTES.toMillis(30));
         return monitorService.sendLatestData(

File: dinky-admin/src/main/java/org/dinky/utils/PaimonUtil.java
Patch:
@@ -60,14 +60,16 @@
 import cn.hutool.core.date.TimeInterval;
 import cn.hutool.core.util.ReflectUtil;
 import cn.hutool.core.util.StrUtil;
+import cn.hutool.core.util.URLUtil;
 import lombok.extern.slf4j.Slf4j;
 
 @Slf4j
 public class PaimonUtil {
     private static final String DINKY_DB = "dinky_db";
     private static final Map<Identifier, Schema> SCHEMA_MAP = new HashMap<>();
     private static final CatalogContext CONTEXT =
-            CatalogContext.create(new Path("file://" + PathConstant.TMP_PATH + "paimon"));
+            CatalogContext.create(
+                    new Path(URLUtil.toURI(URLUtil.url(PathConstant.TMP_PATH + "paimon"))));
     private static final Catalog CATALOG = CatalogFactory.createCatalog(CONTEXT);
     public static final Identifier METRICS_IDENTIFIER =
             Identifier.create(DINKY_DB, "dinky_metrics");

File: dinky-common/src/main/java/org/dinky/data/model/SystemConfiguration.java
Patch:
@@ -135,9 +135,6 @@ public static Configuration.OptionBuilder key(String key) {
                     .defaultValue("Dinky")
                     .note("The project name specified in DolphinScheduler, case insensitive");
 
-    private final Configuration<Boolean> ldapEnable =
-            key("ldap.settings.enable").booleanType().defaultValue(false).note("LDAP ON-OFF");
-
     private final Configuration<String> ldapUrl =
             key("ldap.settings.url").stringType().defaultValue("").note("ldap server address");
 
@@ -187,6 +184,9 @@ public static Configuration.OptionBuilder key(String key) {
     private final Configuration<String> ldapCastNickname =
             key("ldap.settings.castNickname").stringType().defaultValue("sn").note("");
 
+    private final Configuration<Boolean> ldapEnable =
+            key("ldap.settings.enable").booleanType().defaultValue(false).note("LDAP ON-OFF");
+
     /** Initialize after spring bean startup */
     public void initAfterBeanStarted() {
         if (StrUtil.isBlank(dinkyAddr.getDefaultValue())) {

File: dinky-common/src/main/java/org/dinky/data/enums/ColumnType.java
Patch:
@@ -55,8 +55,8 @@ public enum ColumnType {
     T("T[]", "ARRAY"),
     MAP("java.util.Map<K, V>", "MAP");
 
-    private String javaType;
-    private String flinkType;
+    private final String javaType;
+    private final String flinkType;
 
     ColumnType(String javaType, String flinkType) {
         this.javaType = javaType;

File: dinky-metadata/dinky-metadata-base/src/main/java/org/dinky/metadata/driver/Driver.java
Patch:
@@ -58,6 +58,7 @@ static Driver build(DriverConfig config) {
         if (DriverPool.exist(key)) {
             return getHealthDriver(key);
         }
+
         synchronized (Driver.class) {
             Optional<Driver> optionalDriver = Driver.get(config);
             if (!optionalDriver.isPresent()) {
@@ -71,7 +72,6 @@ static Driver build(DriverConfig config) {
     }
 
     static Driver buildUnconnected(DriverConfig config) {
-        String key = config.getName();
         synchronized (Driver.class) {
             Optional<Driver> optionalDriver = Driver.get(config);
             if (!optionalDriver.isPresent()) {
@@ -114,6 +114,7 @@ static Driver build(String connector, String url, String username, String passwo
                 type = "Greenplum";
             }
         }
+
         if (Asserts.isNull(type)) {
             throw new MetaDataException("缺少数据源类型:【" + connector + "】");
         }

File: dinky-cdc/dinky-cdc-core/src/main/java/org/dinky/cdc/SinkBuilder.java
Patch:
@@ -32,7 +32,7 @@ public interface SinkBuilder {
 
     SinkBuilder create(FlinkCDCConfig config);
 
-    DataStreamSource build(
+    DataStreamSource<String> build(
             CDCBuilder cdcBuilder,
             StreamExecutionEnvironment env,
             CustomTableEnvironment customTableEnvironment,

File: dinky-common/src/main/java/org/dinky/data/model/SystemConfiguration.java
Patch:
@@ -68,7 +68,7 @@ public static Configuration.OptionBuilder key(String key) {
     private Configuration<String> sqlSeparator =
             key("flink.settings.sqlSeparator")
                     .stringType()
-                    .defaultValue(";\n")
+                    .defaultValue(";\\n")
                     .note("FlinkSQL语句分割符");
     private Configuration<Integer> jobIdWait =
             key("flink.settings.jobIdWait")

File: dinky-metadata/dinky-metadata-base/src/main/java/org/dinky/metadata/driver/AbstractJdbcDriver.java
Patch:
@@ -124,7 +124,7 @@ public Driver setDriverConfig(DriverConfig config) {
     }
 
     protected void createDataSource(DruidDataSource ds, DriverConfig config) {
-        ds.setName(config.getName().replaceAll(":", ""));
+        ds.setName(config.getName().replaceAll("[^\\w]", ""));
         ds.setUrl(config.getUrl());
         ds.setDriverClassName(getDriverClass());
         ds.setUsername(config.getUsername());

File: dinky-metadata/dinky-metadata-base/src/main/java/org/dinky/metadata/driver/Driver.java
Patch:
@@ -117,6 +117,7 @@ static Driver build(String connector, String url, String username, String passwo
         if (Asserts.isNull(type)) {
             throw new MetaDataException("缺少数据源类型:【" + connector + "】");
         }
+
         DriverConfig driverConfig = new DriverConfig(url, type, url, username, password);
         return build(driverConfig);
     }

File: dinky-admin/src/main/java/org/dinky/data/dto/ModifyPasswordDTO.java
Patch:
@@ -31,6 +31,7 @@
 @Setter
 public class ModifyPasswordDTO {
 
+    private String id;
     private String username;
     private String password;
     private String newPassword;

File: dinky-admin/src/main/java/org/dinky/mapper/RowPermissionsMapper.java
Patch:
@@ -29,7 +29,7 @@
 
 /** role select permissions mapper interface */
 @Mapper
-public interface RoleSelectPermissionsMapper extends SuperMapper<RowPermissions> {
+public interface RowPermissionsMapper extends SuperMapper<RowPermissions> {
 
     /**
      * delete user role select permissions by role id

File: dinky-admin/src/main/java/org/dinky/service/impl/RoleSelectPermissionsServiceImpl.java
Patch:
@@ -20,7 +20,7 @@
 package org.dinky.service.impl;
 
 import org.dinky.data.model.RowPermissions;
-import org.dinky.mapper.RoleSelectPermissionsMapper;
+import org.dinky.mapper.RowPermissionsMapper;
 import org.dinky.mybatis.service.impl.SuperServiceImpl;
 import org.dinky.service.RowPermissionsService;
 
@@ -32,7 +32,7 @@
 
 @Service
 public class RoleSelectPermissionsServiceImpl
-        extends SuperServiceImpl<RoleSelectPermissionsMapper, RowPermissions>
+        extends SuperServiceImpl<RowPermissionsMapper, RowPermissions>
         implements RowPermissionsService {
 
     @Override

File: dinky-admin/src/main/java/org/dinky/service/impl/UserServiceImpl.java
Patch:
@@ -116,7 +116,7 @@ public Boolean modifyUser(User user) {
 
     @Override
     public Result<Void> modifyPassword(ModifyPasswordDTO modifyPasswordDTO) {
-        User user = getUserByUsername(modifyPasswordDTO.getUsername());
+        User user = getById(modifyPasswordDTO.getId());
         if (Asserts.isNull(user)) {
             return Result.failed(Status.USER_NOT_EXIST);
         }

File: dinky-admin/src/main/java/org/dinky/data/enums/UserType.java
Patch:
@@ -20,8 +20,8 @@
 package org.dinky.data.enums;
 
 public enum UserType {
-    LDAP(0, "LDAP"),
-    LOCAL(1, "LOCAL");
+    LDAP(1, "LDAP"),
+    LOCAL(0, "LOCAL");
 
     private final int code;
     private final String type;

File: dinky-admin/src/main/java/org/dinky/service/impl/UserServiceImpl.java
Patch:
@@ -98,6 +98,7 @@ public Result<Void> registerUser(User user) {
         user.setPassword(SaSecureUtil.md5(user.getPassword()));
         user.setEnabled(true);
         user.setIsDelete(false);
+        user.setUserType(UserType.LOCAL.getCode());
         if (save(user)) {
             return Result.succeed(Status.ADDED_SUCCESS);
         } else {

File: dinky-admin/src/main/java/org/dinky/data/dto/LoginDTO.java
Patch:
@@ -35,4 +35,5 @@ public class LoginDTO {
     private String password;
     private Integer tenantId;
     private boolean autoLogin;
+    private boolean ldapLogin;
 }

File: dinky-admin/src/main/java/org/dinky/data/model/User.java
Patch:
@@ -60,6 +60,8 @@ public class User implements Serializable {
 
     private String nickname;
 
+    private int userType;
+
     private String worknum;
 
     private byte[] avatar;

File: dinky-admin/src/main/java/org/dinky/aop/WebExceptionHandler.java
Patch:
@@ -61,7 +61,7 @@ public class WebExceptionHandler {
 
     @ExceptionHandler
     public Result<Void> busException(BusException e) {
-        if (StrUtil.isNotEmpty(e.getMsg())) {
+        if (StrUtil.isEmpty(e.getMsg())) {
             return Result.failed(I18nMsgUtils.getMsg(e.getCode(), e.getErrorArgs()));
         }
         return Result.failed(e.getMsg());

File: dinky-admin/src/main/java/org/dinky/configure/MybatisPlusConfig.java
Patch:
@@ -74,7 +74,7 @@ public class MybatisPlusConfig {
                     "dinky_savepoints",
                     "dinky_task",
                     "dinky_task_statement",
-                    //                    "dinky_git_project",
+                    "dinky_git_project",
                     "dinky_task_version");
 
     @Bean

File: dinky-admin/src/main/java/org/dinky/context/TenantContextHolder.java
Patch:
@@ -22,7 +22,7 @@
 /** TenantContextHolder */
 public class TenantContextHolder {
 
-    private static final ThreadLocal<Object> TENANT_CONTEXT = new ThreadLocal<>();
+    private static final ThreadLocal<Object> TENANT_CONTEXT = new InheritableThreadLocal<>();
 
     public static void set(Object value) {
         TENANT_CONTEXT.set(value);

File: dinky-admin/src/main/java/org/dinky/sse/git/MavenStepSse.java
Patch:
@@ -88,7 +88,7 @@ public void exec() {
                         getLogFile().getAbsolutePath(),
                         CollUtil.newArrayList("clean", "package"),
                         StrUtil.split(gitProject.getBuildArgs(), " "),
-                        this::addMsg);
+                        this::addFileMsgLog);
         params.put("pom", pom);
         Assert.isTrue(state, "maven build failed");
     }

File: dinky-admin/src/main/java/org/dinky/utils/MavenUtil.java
Patch:
@@ -99,14 +99,15 @@ public static boolean build(
         }
         String mavenCommandLine =
                 getMavenCommandLine(pom, mavenHome, localRepositoryDirectory, setting, goals, args);
+        Opt.ofNullable(consumer)
+                .ifPresent(c -> c.accept("Executing command: " + mavenCommandLine + "\n"));
 
         int waitValue =
                 RuntimeUtils.run(
                         mavenCommandLine,
                         s -> {
-                            s = DateUtil.date().toMsStr() + " - " + s;
+                            s = DateUtil.date().toMsStr() + " - " + s + "\n";
                             consumer.accept(s);
-                            FileUtil.writeUtf8String(s, logFile);
                         },
                         log::error);
         return waitValue == 0;

File: dinky-common/src/main/java/org/dinky/data/model/SystemConfiguration.java
Patch:
@@ -156,8 +156,9 @@ public void setConfiguration(Map<String, String> configMap) {
                     item.setValue(value);
                 });
         // initRun
-        CONFIGURATION_LIST.forEach(
-                c -> Opt.ofNullable(this.initMethod).ifPresent(x -> x.accept(c)));
+        for (Configuration<?> configuration : CONFIGURATION_LIST) {
+            Opt.ofNullable(this.initMethod).ifPresent(x -> x.accept(configuration));
+        }
     }
 
     public Map<String, List<Configuration<?>>> getAllConfiguration() {

File: dinky-admin/src/main/java/org/dinky/Dinky.java
Patch:
@@ -32,7 +32,7 @@
 import lombok.extern.slf4j.Slf4j;
 
 /**
- * Dinky 启动器
+ * Dinky Starter
  *
  * @since 2021/5/28
  */

File: dinky-admin/src/main/java/org/dinky/enums/Status.java
Patch:
@@ -66,6 +66,7 @@ public enum Status {
     DELETE_USER_BY_ID_ERROR(10093, "delete user by id error", "删除用户失败"),
     VERIFY_USERNAME_ERROR(10100, "verify username error", "用户名验证失败"),
     SIGN_OUT_ERROR(10123, "sign out error", "退出失败"),
+    SIGN_OUT_SUCESS(10124, "sign out sucess", "退出成功"),
     USER_DISABLED(10148, "The current user is disabled", "当前用户已停用"),
     CURRENT_LOGIN_USER_TENANT_NOT_EXIST(
             10181, "the tenant of the currently login user is not specified", "未指定当前登录用户的租户"),

File: dinky-alert/dinky-alert-wechat/src/main/java/org/dinky/alert/wechat/WeChatType.java
Patch:
@@ -25,8 +25,8 @@
  * @since 2022/2/23 21:36
  */
 public enum WeChatType {
-    APP(1, "应用"),
-    CHAT(2, "群聊");
+    APP(1, "app"),
+    CHAT(2, "wechat");
 
     private final int code;
     private final String value;

File: dinky-admin/src/main/java/org/dinky/dto/GitAnalysisJarDTO.java
Patch:
@@ -30,4 +30,5 @@
 public class GitAnalysisJarDTO {
     private String jarPath;
     private List<String> classList;
+    private Integer orderLine = 1;
 }

File: dinky-admin/src/main/java/org/dinky/utils/GitProjectStepSseFactory.java
Patch:
@@ -20,6 +20,7 @@
 package org.dinky.utils;
 
 import org.dinky.common.result.StepResult;
+import org.dinky.context.GitBuildContextHolder;
 import org.dinky.model.GitProject;
 import org.dinky.process.exception.DinkyException;
 import org.dinky.sse.DoneStepSse;
@@ -66,6 +67,7 @@ public static void build(GitProject gitProject, Dict params) {
         gitProject.setBuildState(1);
         gitProject.updateById();
 
+        GitBuildContextHolder.addRun(gitProject.getId());
         sseEmitterMap.put(gitProject.getId(), emitterList);
     }
 

File: dinky-admin/src/test/java/org/dinky/utils/GitRepositoryTests.java
Patch:
@@ -22,12 +22,14 @@
 import java.io.File;
 import java.util.List;
 
+import org.junit.jupiter.api.Disabled;
 import org.junit.jupiter.api.Test;
 
 /**
  * @author ZackYoung
  * @since 0.8.0
  */
+@Disabled
 public class GitRepositoryTests {
     // ssh-keygen -m PEM -t ecdsa
     GitRepository sshRepository =

File: dinky-admin/src/main/java/org/dinky/aop/UdfClassLoaderAspect.java
Patch:
@@ -60,6 +60,7 @@ public Object round(ProceedingJoinPoint proceedingJoinPoint) {
             if (!(e instanceof DinkyException)) {
                 throw new DinkyException(e);
             }
+            e.printStackTrace();
             throw (DinkyException) e;
         } finally {
             if (proceed instanceof JobResult) {

File: dinky-admin/src/main/java/org/dinky/controller/GitController.java
Patch:
@@ -71,7 +71,7 @@ public class GitController {
      * @return {@link Result} of {@link Void}
      */
     @PutMapping("/saveOrUpdate")
-    public Result<Void> saveOrUpdate(@Validated @RequestBody GitProject gitProject) {
+    public Result<Void> saveOrUpdate(@Validated @RequestBody GitProjectDTO gitProject) {
         gitProjectService.saveOrUpdate(gitProject);
         GitRepository gitRepository =
                 new GitRepository(BeanUtil.copyProperties(gitProject, GitProjectDTO.class));

File: dinky-admin/src/main/java/org/dinky/dto/GitProjectDTO.java
Patch:
@@ -64,4 +64,6 @@ public class GitProjectDTO {
 
     /** */
     @NotNull private Boolean enabled;
+
+    private Integer orderLine;
 }

File: dinky-admin/src/main/java/org/dinky/init/SystemInit.java
Patch:
@@ -31,6 +31,7 @@
 import org.dinky.scheduler.config.DolphinSchedulerProperties;
 import org.dinky.scheduler.exception.SchedulerException;
 import org.dinky.scheduler.model.Project;
+import org.dinky.service.GitProjectService;
 import org.dinky.service.JobInstanceService;
 import org.dinky.service.SysConfigService;
 import org.dinky.service.TaskService;
@@ -67,6 +68,7 @@ public class SystemInit implements ApplicationRunner {
     private final TaskService taskService;
     private final TenantService tenantService;
     private final DolphinSchedulerProperties dolphinSchedulerProperties;
+    private final GitProjectService gitProjectService;
     private static Project project;
 
     @Override
@@ -125,6 +127,7 @@ public void registerUDF() {
                 taskService.getAllUDF().stream()
                         .map(UDFUtils::taskToUDF)
                         .collect(Collectors.toList()));
+        UdfCodePool.updateGitPool(gitProjectService.getGitPool());
         TenantContextHolder.set(null);
     }
 }

File: dinky-admin/src/main/java/org/dinky/model/GitProject.java
Patch:
@@ -92,6 +92,9 @@ public class GitProject extends SuperEntity<GitProject> {
     @TableField(value = "udf_class_map_list")
     private String udfClassMapList;
 
+    @TableField(value = "order_line")
+    private Integer orderLine;
+
     @TableField(exist = false)
     private static final long serialVersionUID = 1L;
 

File: dinky-admin/src/main/java/org/dinky/service/GitProjectService.java
Patch:
@@ -25,6 +25,7 @@
 import org.dinky.model.GitProject;
 
 import java.util.List;
+import java.util.Map;
 
 /**
  * @author ZackYoung
@@ -38,6 +39,8 @@ public interface GitProjectService extends ISuperService<GitProject> {
      */
     void saveOrUpdate(GitProjectDTO gitProjectDTO);
 
+    Map<String, String> getGitPool();
+
     /**
      * 更新状态
      *

File: dinky-common/src/main/java/org/dinky/classloader/DinkyClassLoader.java
Patch:
@@ -19,7 +19,7 @@
 
 package org.dinky.classloader;
 
-import org.dinky.context.JarPathContextHolder;
+import org.dinky.context.FlinkUdfPathContextHolder;
 
 import java.io.File;
 import java.net.MalformedURLException;
@@ -84,7 +84,7 @@ public void addURL(String[] paths, List<String> notExistsFiles) {
                     return;
                 }
                 super.addURL(file.toURI().toURL());
-                JarPathContextHolder.addOtherPlugins(file);
+                FlinkUdfPathContextHolder.addOtherPlugins(file);
             } catch (MalformedURLException e) {
                 throw new RuntimeException(e);
             }

File: dinky-core/src/main/java/org/dinky/explainer/Explainer.java
Patch:
@@ -22,7 +22,7 @@
 import org.dinky.assertion.Asserts;
 import org.dinky.constant.FlinkSQLConstant;
 import org.dinky.context.DinkyClassLoaderContextHolder;
-import org.dinky.context.JarPathContextHolder;
+import org.dinky.context.FlinkUdfPathContextHolder;
 import org.dinky.executor.CustomTableEnvironment;
 import org.dinky.executor.Executor;
 import org.dinky.explainer.watchTable.WatchStatementExplainer;
@@ -113,9 +113,9 @@ public JobParam pretreatStatements(String[] statements) {
             SqlType operationType = Operations.getOperationType(statement);
             if (operationType.equals(SqlType.ADD)) {
                 AddJarSqlParser.getAllFilePath(statement)
-                        .forEach(JarPathContextHolder::addOtherPlugins);
+                        .forEach(FlinkUdfPathContextHolder::addOtherPlugins);
                 DinkyClassLoaderContextHolder.get()
-                        .addURL(URLUtils.getURLs(JarPathContextHolder.getOtherPluginsFiles()));
+                        .addURL(URLUtils.getURLs(FlinkUdfPathContextHolder.getOtherPluginsFiles()));
             } else if (operationType.equals(SqlType.ADD_JAR)) {
                 Configuration combinationConfig = getCombinationConfig();
                 FileSystem.initialize(combinationConfig, null);

File: dinky-executor/src/main/java/org/dinky/trans/ddl/AddJarOperation.java
Patch:
@@ -19,7 +19,7 @@
 
 package org.dinky.trans.ddl;
 
-import org.dinky.context.JarPathContextHolder;
+import org.dinky.context.FlinkUdfPathContextHolder;
 import org.dinky.executor.Executor;
 import org.dinky.parser.check.AddJarSqlParser;
 import org.dinky.trans.AbstractOperation;
@@ -54,6 +54,7 @@ public TableResult build(Executor executor) {
     }
 
     public void init() {
-        AddJarSqlParser.getAllFilePath(statement).forEach(JarPathContextHolder::addOtherPlugins);
+        AddJarSqlParser.getAllFilePath(statement)
+                .forEach(FlinkUdfPathContextHolder::addOtherPlugins);
     }
 }

File: dinky-cdc/dinky-cdc-core/src/main/java/org/dinky/cdc/oracle/OracleCDCBuilder.java
Patch:
@@ -41,6 +41,7 @@
 /**
  * MysqlCDCBuilder
  *
+ * @author wenmo
  * @since 2022/4/12 21:29
  */
 public class OracleCDCBuilder extends AbstractCDCBuilder implements CDCBuilder {

File: dinky-cdc/dinky-cdc-core/src/main/java/org/dinky/cdc/sqlserver/SqlServerCDCBuilder.java
Patch:
@@ -39,8 +39,8 @@
 
 import com.ververica.cdc.connectors.sqlserver.SqlServerSource;
 import com.ververica.cdc.connectors.sqlserver.table.StartupOptions;
+import com.ververica.cdc.debezium.JsonDebeziumDeserializationSchema;
 
-/** sql server CDC */
 public class SqlServerCDCBuilder extends AbstractCDCBuilder implements CDCBuilder {
 
     protected static final Logger logger = LoggerFactory.getLogger(SqlServerCDCBuilder.class);
@@ -104,8 +104,7 @@ public DataStreamSource<String> build(StreamExecutionEnvironment env) {
         } else {
             sourceBuilder.tableList(new String[0]);
         }
-        // sourceBuilder.deserializer(new JsonDebeziumDeserializationSchema());
-        sourceBuilder.deserializer(new SqlServerJsonDebeziumDeserializationSchema());
+        sourceBuilder.deserializer(new JsonDebeziumDeserializationSchema());
         if (Asserts.isNotNullString(config.getStartupMode())) {
             switch (config.getStartupMode().toLowerCase()) {
                 case "initial":

File: dinky-cdc/dinky-cdc-plus/src/main/java/org/dinky/cdc/doris/AdditionalColumnEntry.java
Patch:
@@ -22,7 +22,6 @@
 import java.io.Serializable;
 import java.util.Map;
 
-/** @since 2022/11/10 19:37 */
 public class AdditionalColumnEntry<K, V> implements Map.Entry<K, V>, Serializable {
 
     private static final long serialVersionUID = 45678994131L;

File: dinky-cdc/dinky-cdc-plus/src/main/java/org/dinky/cdc/doris/DorisExtendSinkBuilder.java
Patch:
@@ -54,7 +54,6 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-/** @since 2022/11/10 19:37 */
 public class DorisExtendSinkBuilder extends DorisSinkBuilder implements Serializable {
 
     public static final String KEY_WORD = "datastream-doris-ext";

File: dinky-cdc/dinky-cdc-plus/src/main/java/org/dinky/cdc/doris/DorisExtendSinkOptions.java
Patch:
@@ -22,7 +22,6 @@
 import org.apache.flink.configuration.ConfigOption;
 import org.apache.flink.configuration.ConfigOptions;
 
-/** @since 2022/11/10 19:37 */
 public class DorisExtendSinkOptions extends DorisSinkOptions {
 
     public static final ConfigOption<String> AdditionalColumns =

File: dinky-cdc/dinky-cdc-plus/src/main/java/org/dinky/cdc/doris/DorisSinkBuilder.java
Patch:
@@ -44,7 +44,6 @@
 import java.util.Properties;
 import java.util.UUID;
 
-/** DorisSinkBuilder */
 public class DorisSinkBuilder extends AbstractSinkBuilder implements Serializable {
 
     public static final String KEY_WORD = "datastream-doris";
@@ -168,7 +167,7 @@ public void addSink(
                             + getSinkTableName(table));
         } else {
             executionBuilder.setLabelPrefix(
-                    "dinky-"
+                    "dlink-"
                             + getSinkSchemaName(table)
                             + "_"
                             + getSinkTableName(table)

File: dinky-cdc/dinky-cdc-plus/src/main/java/org/dinky/cdc/doris/DorisSinkOptions.java
Patch:
@@ -22,7 +22,6 @@
 import org.apache.flink.configuration.ConfigOption;
 import org.apache.flink.configuration.ConfigOptions;
 
-/** DorisSinkOptions */
 public class DorisSinkOptions {
 
     public static final ConfigOption<String> FENODES =

File: dinky-cdc/dinky-cdc-plus/src/main/java/org/dinky/cdc/kafka/KafkaSinkJsonBuilder.java
Patch:
@@ -55,7 +55,6 @@
 import com.fasterxml.jackson.datatype.jsr310.JavaTimeModule;
 import com.fasterxml.jackson.datatype.jsr310.deser.LocalDateTimeDeserializer;
 
-/** @className: org.dinky.cdc.kafka.KafkaSinkSimpleBuilder */
 public class KafkaSinkJsonBuilder extends AbstractSinkBuilder implements Serializable {
 
     public static final String KEY_WORD = "datastream-kafka-json";

File: dinky-client/dinky-client-base/src/test/java/org/dinky/model/FlinkCDCConfigTest.java
Patch:
@@ -54,6 +54,7 @@ void getSinkConfigurationString() {
                         null,
                         null,
                         sinkConfig,
+                        null,
                         null);
         String sinkConfigureStr = flinkCDCConfig.getSinkConfigurationString();
         assertThat(

File: dinky-admin/src/main/java/org/dinky/configure/MybatisPlusConfig.java
Patch:
@@ -74,7 +74,7 @@ public class MybatisPlusConfig {
                     "dinky_savepoints",
                     "dinky_task",
                     "dinky_task_statement",
-                    "dinky_git_project",
+                    //                    "dinky_git_project",
                     "dinky_task_version");
 
     @Bean

File: dinky-admin/src/main/java/org/dinky/sse/DoneStepSse.java
Patch:
@@ -43,7 +43,5 @@ public DoneStepSse(
     }
 
     @Override
-    public void exec() {
-        addFileMsgLog("finished");
-    }
+    public void exec() {}
 }

File: dinky-admin/src/main/java/org/dinky/sse/git/AnalysisUdfClassStepSse.java
Patch:
@@ -85,6 +85,7 @@ public void exec() {
                             v.stream().map(Class::getName).collect(Collectors.toList()));
                     dataList.add(gitAnalysisJarDTO);
                 });
+
         String data = JSONUtil.toJsonStr(dataList);
         sendMsg(getList(null).set("data", data));
 

File: dinky-admin/src/main/java/org/dinky/sse/git/GitCloneStepSse.java
Patch:
@@ -50,12 +50,11 @@ public GitCloneStepSse(
 
     @Override
     public void exec() {
-        addFileMsgLog("git clone start");
         GitProject gitProject = (GitProject) params.get("gitProject");
 
         GitRepository gitRepository =
                 new GitRepository(BeanUtil.toBean(gitProject, GitProjectDTO.class));
-        gitRepository.cloneAndPull(gitProject.getName(), gitProject.getBranch());
-        addFileMsgLog("git clone finish");
+        gitRepository.cloneAndPull(
+                gitProject.getName(), gitProject.getBranch(), getLogFile(), this::addMsg);
     }
 }

File: dinky-admin/src/main/java/org/dinky/service/impl/WatchTableServiceImpl.java
Patch:
@@ -33,6 +33,7 @@
 import java.util.regex.Matcher;
 import java.util.regex.Pattern;
 
+import org.springframework.messaging.MessagingException;
 import org.springframework.messaging.simp.SimpMessagingTemplate;
 import org.springframework.stereotype.Service;
 
@@ -62,8 +63,8 @@ public void send(String message) {
             if (destinations != null) {
                 destinations.forEach(d -> this.messagingTemplate.convertAndSend(d, data[1]));
             }
-        } catch (Exception e) {
-            log.error(e.getMessage());
+        } catch (MessagingException e) {
+            log.error(e.toString());
         }
     }
 

File: dinky-executor/src/main/java/org/dinky/parser/SqlSegment.java
Patch:
@@ -35,7 +35,7 @@
  */
 public class SqlSegment {
 
-    private static final String Crlf = "|";
+    private static final String Crlf = new String(new byte[] {1});
 
     @SuppressWarnings("unused")
     private static final String FourSpace = "　　";
@@ -98,7 +98,7 @@ private void parseBody() {
             m.appendReplacement(sb, Crlf);
         }
         m.appendTail(sb);
-        String[] arr = sb.toString().split("[|]");
+        String[] arr = sb.toString().split("[" + Crlf + "]");
         bodyPieces = Lists.newArrayList(arr);
     }
 

File: dinky-admin/src/main/java/org/dinky/service/impl/UserServiceImpl.java
Patch:
@@ -246,9 +246,10 @@ public Result<Void> assignRole(AssignRoleParams assignRoleParams) {
             return Result.succeed(MessageResolverUtils.getMessage("user.assign.role.success"));
         } else {
             if (userRoleList.size() == 0) {
-                return Result.succeed(MessageResolverUtils.getMessage("user.assign.role.failed"));
+                return Result.succeed(
+                        MessageResolverUtils.getMessage("user.binding.role.deleteAll"));
             }
-            return Result.failed(MessageResolverUtils.getMessage("user.binding.role.deleteAll"));
+            return Result.failed(MessageResolverUtils.getMessage("user.assign.role.failed"));
         }
     }
 

File: dinky-client/dinky-client-base/src/main/java/org/dinky/executor/CustomTableEnvironment.java
Patch:
@@ -22,6 +22,7 @@
 import org.dinky.model.LineageRel;
 import org.dinky.result.SqlExplainResult;
 
+import org.apache.flink.configuration.Configuration;
 import org.apache.flink.runtime.jobgraph.JobGraph;
 import org.apache.flink.runtime.rest.messages.JobPlanInfo;
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
@@ -61,6 +62,8 @@ boolean parseAndLoadConfiguration(
 
     Planner getPlanner();
 
+    Configuration getRootConfiguration();
+
     default List<LineageRel> getLineage(String statement) {
         return null;
     }

File: dinky-executor/src/main/java/org/dinky/parser/SqlType.java
Patch:
@@ -56,8 +56,8 @@ public enum SqlType {
     RESET("RESET", "^RESET.*"),
 
     EXECUTE("EXECUTE", "^EXECUTE.*"),
-
-    ADD("ADD", "^ADD.*"),
+    ADD_JAR("ADD_JAR", "^ADD\\s+JAR\\s+\\S+"),
+    ADD("ADD", "^ADD\\s+CUSTOMJAR\\s+\\S+"),
 
     WATCH("WATCH", "^WATCH.*"),
 

File: dinky-executor/src/main/java/org/dinky/parser/check/AddJarSqlParser.java
Patch:
@@ -36,7 +36,7 @@
 /** @since 0.7.0 */
 public class AddJarSqlParser {
 
-    private static final String ADD_JAR = "(add\\s+jar)\\s+'(.*.jar)'";
+    private static final String ADD_JAR = "(add\\s+customjar)\\s+'(.*.jar)'";
     private static final Pattern ADD_JAR_PATTERN =
             Pattern.compile(ADD_JAR, Pattern.CASE_INSENSITIVE);
 

File: dinky-executor/src/main/java/org/dinky/trans/ddl/AddJarOperation.java
Patch:
@@ -30,7 +30,7 @@
 /** @since 0.7.0 */
 public class AddJarOperation extends AbstractOperation implements Operation {
 
-    private static final String KEY_WORD = "ADD JAR";
+    private static final String KEY_WORD = "ADD CUSTOMJAR";
 
     public AddJarOperation(String statement) {
         super(statement);

File: dinky-executor/src/test/java/org/dinky/parser/SqlTypeTest.java
Patch:
@@ -52,7 +52,8 @@ public void match() {
         test("SET...", SqlType.SET, true);
         test("RESET...", SqlType.RESET, true);
         test("EXECUTE...", SqlType.EXECUTE, true);
-        test("ADD...", SqlType.ADD, true);
+        test("ADD jar ...", SqlType.ADD_JAR, true);
+        test("ADD customjar ...", SqlType.ADD, true);
         test("WATCH...", SqlType.WATCH, true);
 
         String sql =

File: dinky-client/dinky-client-1.13/src/main/java/org/dinky/executor/CustomTableEnvironmentImpl.java
Patch:
@@ -346,6 +346,7 @@ private void callSet(
             setMap.put(key, value);
             Configuration configuration = Configuration.fromMap(confMap);
             environment.getConfig().configure(configuration, null);
+            environment.getCheckpointConfig().configure(configuration);
             getConfig().addConfiguration(configuration);
         }
     }
@@ -364,6 +365,7 @@ private void callReset(
             setMap.remove(key);
             Configuration configuration = Configuration.fromMap(confMap);
             environment.getConfig().configure(configuration, null);
+            environment.getCheckpointConfig().configure(configuration);
             getConfig().addConfiguration(configuration);
         } else {
             setMap.clear();

File: dinky-client/dinky-client-1.14/src/main/java/org/dinky/executor/CustomTableEnvironmentImpl.java
Patch:
@@ -341,6 +341,7 @@ private void callSet(
             setMap.put(key, value);
             Configuration configuration = Configuration.fromMap(confMap);
             environment.getConfig().configure(configuration, null);
+            environment.getCheckpointConfig().configure(configuration);
             getConfig().addConfiguration(configuration);
         }
     }
@@ -359,6 +360,7 @@ private void callReset(
             setMap.remove(key);
             Configuration configuration = Configuration.fromMap(confMap);
             environment.getConfig().configure(configuration, null);
+            environment.getCheckpointConfig().configure(configuration);
             getConfig().addConfiguration(configuration);
         } else {
             setMap.clear();

File: dinky-client/dinky-client-1.15/src/main/java/org/dinky/executor/CustomTableEnvironmentImpl.java
Patch:
@@ -295,6 +295,7 @@ private void callSet(
             setMap.put(key, value);
             Configuration configuration = Configuration.fromMap(confMap);
             environment.getConfig().configure(configuration, null);
+            environment.getCheckpointConfig().configure(configuration);
             getConfig().addConfiguration(configuration);
         }
     }
@@ -313,6 +314,7 @@ private void callReset(
             setMap.remove(key);
             Configuration configuration = Configuration.fromMap(confMap);
             environment.getConfig().configure(configuration, null);
+            environment.getCheckpointConfig().configure(configuration);
             getConfig().addConfiguration(configuration);
         } else {
             setMap.clear();

File: dinky-client/dinky-client-1.16/src/main/java/org/dinky/executor/CustomTableEnvironmentImpl.java
Patch:
@@ -254,6 +254,7 @@ private void setConfiguration(
             StreamExecutionEnvironment environment, Map<String, String> config) {
         Configuration configuration = Configuration.fromMap(config);
         environment.getConfig().configure(configuration, null);
+        environment.getCheckpointConfig().configure(configuration);
         getConfig().addConfiguration(configuration);
     }
 

File: dinky-core/src/main/java/org/dinky/explainer/Explainer.java
Patch:
@@ -350,7 +350,8 @@ public List<LineageRel> getLineage(String statement) {
                 SqlType operationType = Operations.getOperationType(sql);
                 if (operationType.equals(SqlType.INSERT)) {
                     lineageRelList.addAll(executor.getLineage(sql));
-                } else if (!operationType.equals(SqlType.SELECT)) {
+                } else if (!operationType.equals(SqlType.SELECT)
+                        && !operationType.equals(SqlType.WATCH)) {
                     executor.executeSql(sql);
                 }
             } catch (Exception e) {

File: dinky-admin/src/main/java/org/dinky/aop/UdfClassLoaderAspect.java
Patch:
@@ -60,6 +60,7 @@ public Object round(ProceedingJoinPoint proceedingJoinPoint) {
             if (!(e instanceof DinkyException)) {
                 throw new DinkyException(e);
             }
+            throw (DinkyException) e;
         } finally {
             if (proceed instanceof JobResult) {
                 ClassLoader lastContextClassLoader = Thread.currentThread().getContextClassLoader();

File: dinky-core/src/main/java/org/dinky/explainer/Explainer.java
Patch:
@@ -139,7 +139,7 @@ public JobParam pretreatStatements(String[] statements) {
             } else {
                 UDF udf = UDFUtil.toUDF(statement);
                 if (Asserts.isNotNull(udf)) {
-                    udfList.add(UDFUtil.toUDF(statement));
+                    udfList.add(udf);
                 }
                 ddl.add(new StatementParam(statement, operationType));
                 statementList.add(statement);

File: dinky-admin/src/main/java/org/dinky/common/result/Result.java
Patch:
@@ -44,8 +44,8 @@ public class Result<T> implements Serializable {
     private T datas;
     private Integer code;
     private String msg;
-
     private String time;
+    private boolean success;
 
     public Result(Integer code, String msg) {
         this.code = code;
@@ -89,7 +89,7 @@ public static <T> Result<T> data(T model) {
     }
 
     public static <T> Result<T> of(T datas, Integer code, String msg) {
-        return new Result<>(datas, code, msg, new DateTime().toString());
+        return new Result<>(datas, code, msg, new DateTime().toString(), code == 0);
     }
 
     public static <T> Result<T> failed(String msg) {

File: dinky-admin/src/main/java/org/dinky/constant/BaseConstant.java
Patch:
@@ -26,4 +26,6 @@ public class BaseConstant {
 
     /** default batch insert size */
     public static final Integer DEFAULT_BATCH_INSERT_SIZE = 1000;
+
+    public static final String YYYY_MM_DD_HH_MM_SS = "yyyy-MM-dd HH:mm:ss";
 }

File: dinky-client/dinky-client-1.13/src/main/java/org/dinky/utils/ObjectConvertUtil.java
Patch:
@@ -69,7 +69,7 @@ public static Object convertValue(Object value, LogicalType logicalType, ZoneId
                 return Instant.ofEpochMilli((long) value).atZone(sinkTimeZone).toLocalDateTime();
             }
         } else if (logicalType instanceof DecimalType) {
-            return new BigDecimal((String) value);
+            return new BigDecimal(String.valueOf(value));
         } else if (logicalType instanceof BigIntType) {
             if (value instanceof Integer) {
                 return ((Integer) value).longValue();

File: dinky-client/dinky-client-1.14/src/main/java/org/dinky/utils/ObjectConvertUtil.java
Patch:
@@ -69,7 +69,7 @@ public static Object convertValue(Object value, LogicalType logicalType, ZoneId
                 return Instant.ofEpochMilli((long) value).atZone(sinkTimeZone).toLocalDateTime();
             }
         } else if (logicalType instanceof DecimalType) {
-            return new BigDecimal((String) value);
+            return new BigDecimal(String.valueOf(value));
         } else if (logicalType instanceof BigIntType) {
             if (value instanceof Integer) {
                 return ((Integer) value).longValue();

File: dinky-client/dinky-client-1.15/src/main/java/org/dinky/utils/ObjectConvertUtil.java
Patch:
@@ -69,7 +69,7 @@ public static Object convertValue(Object value, LogicalType logicalType, ZoneId
                 return Instant.ofEpochMilli((long) value).atZone(sinkTimeZone).toLocalDateTime();
             }
         } else if (logicalType instanceof DecimalType) {
-            return new BigDecimal((String) value);
+            return new BigDecimal(String.valueOf(value));
         } else if (logicalType instanceof BigIntType) {
             if (value instanceof Integer) {
                 return ((Integer) value).longValue();

File: dinky-client/dinky-client-1.16/src/main/java/org/dinky/utils/ObjectConvertUtil.java
Patch:
@@ -69,7 +69,7 @@ public static Object convertValue(Object value, LogicalType logicalType, ZoneId
                 return Instant.ofEpochMilli((long) value).atZone(sinkTimeZone).toLocalDateTime();
             }
         } else if (logicalType instanceof DecimalType) {
-            return new BigDecimal((String) value);
+            return new BigDecimal(String.valueOf(value));
         } else if (logicalType instanceof BigIntType) {
             if (value instanceof Integer) {
                 return ((Integer) value).longValue();

File: dinky-admin/src/main/java/org/dinky/service/impl/StatementServiceImpl.java
Patch:
@@ -53,7 +53,7 @@ public List<String> getWatchTables(String statement) {
         // TODO: 2023/4/7 this function not support variable sql, because, JobManager and executor
         // couple function
         //  and status and task execute.
-        final String[] statements = SqlUtil.getStatements(statement);
+        final String[] statements = SqlUtil.getStatements(SqlUtil.removeNote(statement));
         return Arrays.stream(statements)
                 .filter(t -> SqlType.WATCH.equals(Operations.getOperationType(t)))
                 .flatMap(t -> Arrays.stream(WatchStatementExplainer.splitTableNames(t)))

File: dinky-client/dinky-client-1.12/src/main/java/org/dinky/cdc/AbstractSinkBuilder.java
Patch:
@@ -336,7 +336,8 @@ protected Object convertValue(Object value, LogicalType logicalType) {
             final DecimalType decimalType = ((DecimalType) logicalType);
             final int precision = decimalType.getPrecision();
             final int scale = decimalType.getScale();
-            return DecimalData.fromBigDecimal(new BigDecimal((String) value), precision, scale);
+            return DecimalData.fromBigDecimal(
+                    new BigDecimal(String.valueOf(value)), precision, scale);
         } else {
             return value;
         }

File: dinky-client/dinky-client-1.13/src/main/java/org/dinky/cdc/AbstractSinkBuilder.java
Patch:
@@ -383,7 +383,8 @@ protected Object convertValue(Object value, LogicalType logicalType) {
             final DecimalType decimalType = ((DecimalType) logicalType);
             final int precision = decimalType.getPrecision();
             final int scale = decimalType.getScale();
-            return DecimalData.fromBigDecimal(new BigDecimal((String) value), precision, scale);
+            return DecimalData.fromBigDecimal(
+                    new BigDecimal(String.valueOf(value)), precision, scale);
         } else {
             return value;
         }

File: dinky-client/dinky-client-1.14/src/main/java/org/dinky/cdc/AbstractSinkBuilder.java
Patch:
@@ -353,7 +353,8 @@ protected Object convertValue(Object value, LogicalType logicalType) {
             final DecimalType decimalType = ((DecimalType) logicalType);
             final int precision = decimalType.getPrecision();
             final int scale = decimalType.getScale();
-            return DecimalData.fromBigDecimal(new BigDecimal((String) value), precision, scale);
+            return DecimalData.fromBigDecimal(
+                    new BigDecimal(String.valueOf(value)), precision, scale);
         } else if (logicalType instanceof FloatType) {
             if (value instanceof Float) {
                 return value;

File: dinky-client/dinky-client-1.15/src/main/java/org/dinky/cdc/AbstractSinkBuilder.java
Patch:
@@ -353,7 +353,8 @@ protected Object convertValue(Object value, LogicalType logicalType) {
             final DecimalType decimalType = ((DecimalType) logicalType);
             final int precision = decimalType.getPrecision();
             final int scale = decimalType.getScale();
-            return DecimalData.fromBigDecimal(new BigDecimal((String) value), precision, scale);
+            return DecimalData.fromBigDecimal(
+                    new BigDecimal(String.valueOf(value)), precision, scale);
         } else if (logicalType instanceof FloatType) {
             if (value instanceof Float) {
                 return value;

File: dinky-client/dinky-client-1.16/src/main/java/org/dinky/cdc/AbstractSinkBuilder.java
Patch:
@@ -340,7 +340,8 @@ protected Object convertValue(Object value, LogicalType logicalType) {
             final DecimalType decimalType = ((DecimalType) logicalType);
             final int precision = decimalType.getPrecision();
             final int scale = decimalType.getScale();
-            return DecimalData.fromBigDecimal(new BigDecimal((String) value), precision, scale);
+            return DecimalData.fromBigDecimal(
+                    new BigDecimal(String.valueOf(value)), precision, scale);
         } else {
             return value;
         }

File: dinky-metadata/dinky-metadata-postgresql/src/main/java/org/dinky/metadata/convert/PostgreSqlTypeConvert.java
Patch:
@@ -92,6 +92,8 @@ public ColumnType convert(Column column) {
             columnType = ColumnType.BYTES;
         } else if (t.contains("array")) {
             columnType = ColumnType.T;
+        } else if (t.contains("jsonb") | t.contains("json")) {
+            columnType = ColumnType.STRING;
         }
         return columnType;
     }

File: dinky-metadata/dinky-metadata-postgresql/src/main/java/org/dinky/metadata/convert/PostgreSqlTypeConvert.java
Patch:
@@ -74,7 +74,7 @@ public ColumnType convert(Column column) {
             }
         } else if (t.contains("numeric") || t.contains("decimal")) {
             columnType = ColumnType.DECIMAL;
-        } else if (t.contains("boolean")) {
+        } else if (t.contains("boolean") || t.contains("bool")) {
             if (isNullable) {
                 columnType = ColumnType.JAVA_LANG_BOOLEAN;
             } else {

File: dinky-client/dinky-client-1.16/src/main/java/org/dinky/executor/ExtendedOperationExecutorWrapper.java
Patch:
@@ -42,7 +42,7 @@ public ExtendedOperationExecutorWrapper(
     public Optional<TableResultInternal> executeOperation(Operation operation) {
         Optional<TableResultInternal> customResult =
                 (Optional<TableResultInternal>) customOperationExecutor.executeOperation(operation);
-        if (customResult != null) {
+        if (customResult.isPresent()) {
             return customResult;
         }
 

File: dinky-core/src/main/java/org/dinky/explainer/Explainer.java
Patch:
@@ -133,7 +133,7 @@ public JobParam pretreatStatements(String[] statements) {
                 trans.add(
                         new StatementParam(
                                 watchStatementExplainer.getCreateStatement(tableName),
-                                SqlType.CTASS));
+                                SqlType.CTAS));
             } else {
                 UDF udf = UDFUtil.toUDF(statement);
                 if (Asserts.isNotNull(udf)) {

File: dinky-core/src/main/java/org/dinky/job/JobManager.java
Patch:
@@ -449,7 +449,7 @@ public JobResult executeSql(String statement) {
                     List<String> inserts = new ArrayList<>();
                     for (StatementParam item : jobParam.getTrans()) {
                         if (item.getType().equals(SqlType.INSERT)
-                                || item.getType().equals(SqlType.CTASS)) {
+                                || item.getType().equals(SqlType.CTAS)) {
                             inserts.add(item.getValue());
                         }
                     }

File: dinky-executor/src/main/java/org/dinky/parser/CustomParserImpl.java
Patch:
@@ -20,7 +20,7 @@
 package org.dinky.parser;
 
 import org.dinky.executor.CustomParser;
-import org.dinky.trans.CreateAggTableOperationNewParseStrategy;
+import org.dinky.trans.CreateTemporalTableFunctionParseStrategy;
 
 import org.apache.flink.table.operations.Operation;
 import org.apache.flink.table.planner.parse.ExtendedParseStrategy;
@@ -50,7 +50,7 @@ public static class DinkyExtendedParser extends ExtendedParser {
         public static final DinkyExtendedParser INSTANCE = new DinkyExtendedParser();
 
         private static final List<ExtendedParseStrategy> PARSE_STRATEGIES =
-                Arrays.asList(CreateAggTableOperationNewParseStrategy.INSTANCE);
+                Arrays.asList(CreateTemporalTableFunctionParseStrategy.INSTANCE);
 
         @Override
         public Optional<Operation> parse(String statement) {

File: dinky-executor/src/main/java/org/dinky/parser/SqlType.java
Patch:
@@ -62,7 +62,7 @@ public enum SqlType {
 
     WATCH("WATCH", "^WATCH.*"),
 
-    CTASS("CTAS", "^CREATE\\s.*AS\\sSELECT.*$"),
+    CTAS("CTAS", "^CREATE\\s.*AS\\sSELECT.*$"),
 
     UNKNOWN("UNKNOWN", "^UNKNOWN.*");
 

File: dinky-executor/src/test/java/org/dinky/parser/SqlTypeTest.java
Patch:
@@ -59,7 +59,7 @@ public void match() {
                 "CREATE TABLE print_OrdersView WITH ('connector' = 'printnet', 'port'='7125', 'hostName' = '172"
                         + ".26.16.1', 'sink.parallelism'='1')\n"
                         + "AS SELECT * FROM OrdersView; ";
-        test(sql, SqlType.CTASS, true);
+        test(sql, SqlType.CTAS, true);
         test(sql, SqlType.CREATE, false);
     }
 

File: dinky-core/src/main/java/org/dinky/explainer/Explainer.java
Patch:
@@ -133,7 +133,7 @@ public JobParam pretreatStatements(String[] statements) {
                 trans.add(
                         new StatementParam(
                                 watchStatementExplainer.getCreateStatement(tableName),
-                                SqlType.CTAS));
+                                SqlType.CTASS));
             } else {
                 UDF udf = UDFUtil.toUDF(statement);
                 if (Asserts.isNotNull(udf)) {

File: dinky-core/src/main/java/org/dinky/job/JobManager.java
Patch:
@@ -444,7 +444,8 @@ public JobResult executeSql(String statement) {
                 } else if (useStatementSet && !useGateway) {
                     List<String> inserts = new ArrayList<>();
                     for (StatementParam item : jobParam.getTrans()) {
-                        if (item.getType().isInsert() || item.getType().equals(SqlType.CTAS)) {
+                        if (item.getType().equals(SqlType.INSERT)
+                                || item.getType().equals(SqlType.CTASS)) {
                             inserts.add(item.getValue());
                         }
                     }

File: dinky-executor/src/main/java/org/dinky/trans/Operations.java
Patch:
@@ -76,7 +76,7 @@ private static Operation[] getAllOperations() {
     public static SqlType getOperationType(String sql) {
         String sqlTrim = sql.replaceAll(SQL_EMPTY_STR, "").trim().toUpperCase();
         return Arrays.stream(SqlType.values())
-                .filter(sqlType -> sqlTrim.startsWith(sqlType.getType()))
+                .filter(sqlType -> sqlType.match(sqlTrim))
                 .findFirst()
                 .orElse(SqlType.UNKNOWN);
     }

File: dinky-core/src/main/java/org/dinky/job/JobManager.java
Patch:
@@ -444,7 +444,7 @@ public JobResult executeSql(String statement) {
                 } else if (useStatementSet && !useGateway) {
                     List<String> inserts = new ArrayList<>();
                     for (StatementParam item : jobParam.getTrans()) {
-                        if (item.getType().isInsert()) {
+                        if (item.getType().isInsert() || item.getType().equals(SqlType.CTAS)) {
                             inserts.add(item.getValue());
                         }
                     }

File: dinky-executor/src/main/java/org/dinky/parser/SqlType.java
Patch:
@@ -42,8 +42,9 @@ public enum SqlType {
     RESET("RESET"),
     EXECUTE("EXECUTE"),
     ADD("ADD"),
-    UNKNOWN("UNKNOWN"),
-    WATCH("WATCH");
+    WATCH("WATCH"),
+    CTAS("CTAS"),
+    UNKNOWN("UNKNOWN");
 
     private String type;
 

File: dinky-executor/src/main/java/org/dinky/parser/SqlType.java
Patch:
@@ -43,7 +43,7 @@ public enum SqlType {
     EXECUTE("EXECUTE"),
     ADD("ADD"),
     UNKNOWN("UNKNOWN"),
-    ;
+    WATCH("WATCH");
 
     private String type;
 

File: dinky-admin/src/main/java/org/dinky/model/RoleNamespace.java
Patch:
@@ -31,9 +31,11 @@
 import lombok.Data;
 import lombok.EqualsAndHashCode;
 
+/** role namespace , it's deprecated will be removed in the future */
 @Data
 @EqualsAndHashCode(callSuper = false)
 @TableName("dinky_role_namespace")
+@Deprecated
 public class RoleNamespace implements Serializable {
 
     private static final long serialVersionUID = 304808291890721691L;

File: dinky-admin/src/main/java/org/dinky/service/impl/RoleNamespaceServiceImpl.java
Patch:
@@ -28,7 +28,9 @@
 
 import org.springframework.stereotype.Service;
 
+/** role namespace service impl , it's will be deprecated in the future */
 @Service
+@Deprecated
 public class RoleNamespaceServiceImpl extends SuperServiceImpl<RoleNamespaceMapper, RoleNamespace>
         implements RoleNamespaceService {
 

File: dinky-admin/src/main/java/org/dinky/controller/AdminController.java
Patch:
@@ -84,12 +84,12 @@ public Result<UserDTO> current() {
     }
 
     /**
-     * choose tenant
+     * choose tenant by tenantId
      *
      * @param tenantId
      * @return {@link Result}{@link Tenant}
      */
-    @GetMapping("/chooseTenant")
+    @PostMapping("/chooseTenant")
     public Result<Tenant> chooseTenant(@RequestParam("tenantId") Integer tenantId) {
         return userService.chooseTenant(tenantId);
     }

File: dinky-core/src/main/java/org/dinky/result/ResultRunnable.java
Patch:
@@ -150,6 +150,8 @@ private Map<String, Object> getFieldMap(List<String> columns, Row row) {
                 map.put(
                         column,
                         ((Instant) field).atZone(ZoneId.of(timeZone)).toLocalDateTime().toString());
+            } else if (field instanceof Boolean) {
+                map.put(column, field.toString());
             } else {
                 map.put(column, field);
             }

File: dinky-admin/src/main/java/org/dinky/service/impl/TaskServiceImpl.java
Patch:
@@ -1403,7 +1403,7 @@ private String getDuration(long jobStartTimeMills, long jobEndTimeMills) {
 
     @Override
     public void handleJobDone(JobInstance jobInstance) {
-        if (Asserts.isNull(jobInstance.getTaskId())) {
+        if (Asserts.isNull(jobInstance.getTaskId()) || Asserts.isNull(jobInstance.getType())) {
             return;
         }
 

File: dinky-executor/src/main/java/org/dinky/trans/Operations.java
Patch:
@@ -85,7 +85,7 @@ public static Operation buildOperation(String statement) {
         String sql = statement.replace("\n", " ").replaceAll("\\s+", " ").trim().toUpperCase();
 
         return Arrays.stream(ALL_OPERATIONS)
-                .filter(p -> sql.startsWith(p.getHandle()))
+                .filter(p -> p.getHandle() != null && sql.startsWith(p.getHandle()))
                 .findFirst()
                 .map(p -> p.create(statement))
                 .orElse(null);

File: dinky-client/dinky-client-1.16/src/main/java/org/dinky/executor/CustomTableEnvironmentImpl.java
Patch:
@@ -101,6 +101,7 @@ public static CustomTableEnvironmentImpl create(
             StreamExecutionEnvironment executionEnvironment, EnvironmentSettings settings) {
         StreamTableEnvironment streamTableEnvironment =
                 StreamTableEnvironment.create(executionEnvironment, settings);
+
         return new CustomTableEnvironmentImpl(streamTableEnvironment);
     }
 

File: dinky-executor/src/main/java/org/dinky/executor/Executor.java
Patch:
@@ -24,6 +24,7 @@
 import org.dinky.interceptor.FlinkInterceptor;
 import org.dinky.interceptor.FlinkInterceptorResult;
 import org.dinky.model.LineageRel;
+import org.dinky.parser.CustomParserImpl;
 import org.dinky.result.SqlExplainResult;
 
 import org.apache.flink.api.common.ExecutionConfig;
@@ -183,8 +184,9 @@ public void initEnvironment() {
 
     private void initExecutionEnvironment() {
         useSqlFragment = executorSetting.isUseSqlFragment();
-
         tableEnvironment = createCustomTableEnvironment();
+        tableEnvironment.injectParser(new CustomParserImpl());
+        tableEnvironment.injectExtendedExecutor(new CustomExtendedOperationExecutorImpl(this));
         Configuration configuration = tableEnvironment.getConfig().getConfiguration();
         if (executorSetting.isValidJobName()) {
             configuration.setString(PipelineOptions.NAME.key(), executorSetting.getJobName());

File: dinky-metadata/dinky-metadata-doris/src/main/java/org/dinky/metadata/driver/DorisDriver.java
Patch:
@@ -121,7 +121,7 @@ public Map<String, String> getFlinkColumnTypeConversion() {
         map.put("SMALLINT", "SMALLINT");
         map.put("INT", "INT");
         map.put("VARCHAR", "STRING");
-        map.put("TEXY", "STRING");
+        map.put("TEXT", "STRING");
         map.put("DATETIME", "TIMESTAMP");
         return map;
     }

File: dinky-metadata/dinky-metadata-hive/src/main/java/org/dinky/metadata/driver/HiveDriver.java
Patch:
@@ -312,8 +312,7 @@ public Map<String, String> getFlinkColumnTypeConversion() {
         map.put("SMALLINT", "SMALLINT");
         map.put("INT", "INT");
         map.put("VARCHAR", "STRING");
-        map.put("TEXY", "STRING");
-        map.put("INT", "INT");
+        map.put("TEXT", "STRING");
         map.put("DATETIME", "TIMESTAMP");
         return map;
     }

File: dinky-metadata/dinky-metadata-presto/src/main/java/org/dinky/metadata/driver/PrestoDriver.java
Patch:
@@ -354,8 +354,7 @@ public Map<String, String> getFlinkColumnTypeConversion() {
         map.put("SMALLINT", "SMALLINT");
         map.put("INT", "INT");
         map.put("VARCHAR", "STRING");
-        map.put("TEXY", "STRING");
-        map.put("INT", "INT");
+        map.put("TEXT", "STRING");
         map.put("DATETIME", "TIMESTAMP");
         return map;
     }

File: dinky-metadata/dinky-metadata-starrocks/src/main/java/org/dinky/metadata/driver/StarRocksDriver.java
Patch:
@@ -105,7 +105,7 @@ public Map<String, String> getFlinkColumnTypeConversion() {
         map.put("SMALLINT", "SMALLINT");
         map.put("INT", "INT");
         map.put("VARCHAR", "STRING");
-        map.put("TEXY", "STRING");
+        map.put("TEXT", "STRING");
         map.put("DATETIME", "TIMESTAMP");
         return map;
     }

File: dinky-core/src/main/java/org/dinky/api/FlinkAPI.java
Patch:
@@ -389,7 +389,7 @@ public JsonNode getTaskManagerLogList(String containerId) {
      * @param logName 日志名称
      * @return String
      */
-    public String getTaskManagerLogFileDeatil(String containerId, String logName) {
+    public String getTaskManagerLogFileDetail(String containerId, String logName) {
         return getResult(
                 FlinkRestAPIConstant.TASK_MANAGER
                         + containerId

File: dinky-core/src/test/java/org/dinky/core/FlinkRestAPITest.java
Patch:
@@ -166,7 +166,7 @@ public void getTaskManagerLogListTest() {
     public void getTaskManagerLogListToDetail() {
         String taskManagerLogDetail =
                 FlinkAPI.build(address)
-                        .getTaskManagerLogFileDeatil(
+                        .getTaskManagerLogFileDetail(
                                 "container_e46_1655948912029_0061_01_000002", "taskmanager.log");
         LOGGER.info(taskManagerLogDetail);
     }

File: dinky-admin/src/main/java/org/dinky/model/User.java
Patch:
@@ -30,6 +30,7 @@
 import com.baomidou.mybatisplus.annotation.IdType;
 import com.baomidou.mybatisplus.annotation.TableField;
 import com.baomidou.mybatisplus.annotation.TableId;
+import com.baomidou.mybatisplus.annotation.TableLogic;
 import com.baomidou.mybatisplus.annotation.TableName;
 
 import lombok.Data;
@@ -68,7 +69,7 @@ public class User implements Serializable {
 
     private Boolean enabled;
 
-    private Boolean isDelete;
+    @TableLogic private Boolean isDelete;
 
     @TableField(fill = FieldFill.INSERT)
     private LocalDateTime createTime;

File: dinky-core/src/main/java/org/dinky/job/JobManager.java
Patch:
@@ -211,7 +211,8 @@ private static void initGatewayConfig(JobConfig config) {
     public static boolean useGateway(String type) {
         return (GatewayType.YARN_PER_JOB.equalsValue(type)
                 || GatewayType.YARN_APPLICATION.equalsValue(type)
-                || GatewayType.KUBERNETES_APPLICATION.equalsValue(type));
+                || GatewayType.KUBERNETES_APPLICATION.equalsValue(type)
+                || GatewayType.KUBERNETES_APPLICATION_OPERATOR.equalsValue(type));
     }
 
     private Executor createExecutor() {

File: dinky-gateway/src/main/java/org/dinky/gateway/Gateway.java
Patch:
@@ -86,5 +86,7 @@ static Gateway build(GatewayConfig config) {
 
     void killCluster();
 
+    boolean onJobFinishCallback(String status);
+
     GatewayResult deployCluster();
 }

File: dinky-gateway/src/main/java/org/dinky/gateway/config/FlinkConfig.java
Patch:
@@ -48,6 +48,7 @@ public class FlinkConfig {
     private SavePointType savePointType;
     private String savePoint;
     private Map<String, String> configuration = new HashMap<>();
+    private Map<String, String> FlinkKubetnetsConfig = new HashMap<>();
 
     private static final ObjectMapper mapper = new ObjectMapper();
 

File: dinky-process/src/main/java/org/dinky/process/model/ProcessType.java
Patch:
@@ -34,6 +34,7 @@ public enum ProcessType {
     SQL_EXPLAIN("SQLExplain"),
     SQL_EXECUTE("SQLExecute"),
     SQL_SUBMIT("SQLSubmit"),
+    LINEAGE("Lineage"),
     UNKNOWN("Unknown");
 
     private String value;

File: dinky-admin/src/main/java/org/dinky/job/FlinkJobTask.java
Patch:
@@ -77,7 +77,7 @@ public void dealTask() {
             DefaultThreadPool.getInstance().execute(this);
         } else {
             taskService.handleJobDone(jobInstance);
-            FlinkJobTaskPool.getInstance().remove(config.getId().toString());
+            FlinkJobTaskPool.INSTANCE.remove(config.getId().toString());
         }
     }
 }

File: dinky-process/src/main/java/org/dinky/process/context/ProcessContextHolder.java
Patch:
@@ -51,7 +51,7 @@ public static void clear() {
     public static ProcessEntity registerProcess(ProcessEntity process) {
         Asserts.checkNull(process, "Process can not be null.");
         setProcess(process);
-        ProcessPool.getInstance().push(process.getName(), process);
+        ProcessPool.INSTANCE.put(process.getName(), process);
         return process;
     }
 }

File: dinky-admin/src/main/java/org/dinky/model/Task.java
Patch:
@@ -106,7 +106,7 @@ public class Task extends SuperEntity<Task> {
     private String clusterName;
 
     @TableField(exist = false)
-    private List<Savepoints> savepoints;
+    private List<Savepoints> savePoints;
 
     @TableField(exist = false)
     private List<Map<String, String>> config = new ArrayList<>();

File: dinky-admin/src/main/java/org/dinky/service/impl/TaskServiceImpl.java
Patch:
@@ -511,7 +511,9 @@ public boolean saveOrUpdateTask(Task task) {
     @Override
     public List<Task> listFlinkSQLEnv() {
         return this.list(
-                new QueryWrapper<Task>().eq("dialect", Dialect.FLINK_SQL_ENV).eq("enabled", 1));
+                new QueryWrapper<Task>()
+                        .eq("dialect", Dialect.FLINK_SQL_ENV.getValue())
+                        .eq("enabled", 1));
     }
 
     @Override

File: dinky-admin/src/main/java/org/dinky/service/impl/StudioServiceImpl.java
Patch:
@@ -373,7 +373,7 @@ public List<SessionInfo> listSession(String createUser) {
     @Override
     public LineageResult getLineage(StudioCADTO studioCADTO) {
         if (Asserts.isNotNullString(studioCADTO.getDialect())
-                && !Dialect.FLINKSQL.equalsVal(studioCADTO.getDialect())) {
+                && !Dialect.FLINK_SQL.equalsVal(studioCADTO.getDialect())) {
             if (Asserts.isNull(studioCADTO.getDatabaseId())) {
                 return null;
             }

File: dinky-common/src/main/java/org/dinky/constant/CommonConstant.java
Patch:
@@ -25,8 +25,8 @@
  * @author wenmo
  * @since 2021/5/28 9:35
  */
-public interface CommonConstant {
+public final class CommonConstant {
 
     /** 实例健康 */
-    String HEALTHY = "1";
+    public static final String HEALTHY = "1";
 }

File: dinky-core/src/main/java/org/dinky/job/Job.java
Patch:
@@ -38,8 +38,7 @@
  */
 @Getter
 @Setter
-public class Job {
-    private Integer id;
+public class Job {private Integer id;
     private Integer jobInstanceId;
     private JobConfig jobConfig;
     private String jobManagerAddress;

File: dinky-core/src/main/java/org/dinky/api/FlinkAPI.java
Patch:
@@ -50,9 +50,7 @@
  * @since 2021/6/24 13:56
  */
 @SuppressWarnings("AlibabaClassNamingShouldBeCamel")
-public class FlinkAPI {
-
-    private static final Logger logger = LoggerFactory.getLogger(FlinkAPI.class);
+public class FlinkAPI { private static final Logger logger = LoggerFactory.getLogger(FlinkAPI.class);
 
     public static final String REST_TARGET_DIRECTORY = "rest.target-directory";
     public static final String ERRORS = "errors";

File: dinky-admin/src/test/java/org/dinky/admin/AdminTest.java
Patch:
@@ -20,6 +20,7 @@
 package org.dinky.admin;
 
 import org.junit.Assert;
+import org.junit.Ignore;
 import org.junit.Test;
 
 import cn.dev33.satoken.secure.SaSecureUtil;
@@ -30,6 +31,7 @@
  * @author wenmo
  * @since 2021/6/14 17:03
  */
+@Ignore
 public class AdminTest {
 
     @Test

File: dinky-admin/src/test/java/org/dinky/security/SecurityAspectTest.java
Patch:
@@ -20,6 +20,7 @@
 package org.dinky.security;
 
 import org.junit.Assert;
+import org.junit.Ignore;
 import org.junit.Test;
 
 /**
@@ -28,6 +29,7 @@
  * @author wenmo
  * @since 2023/1/14 15:59
  */
+@Ignore
 public class SecurityAspectTest {
 
     @Test

File: dinky-admin/src/test/java/org/dinky/utils/DirUtilTest.java
Patch:
@@ -34,6 +34,7 @@
  * @author wenmo
  * @since 2022/10/14 22:00
  */
+@Ignore
 public class DirUtilTest {
 
     @Ignore

File: dinky-alert/dinky-alert-dingtalk/src/test/java/org/dinky/alert/dingtalk/DingTalkTest.java
Patch:
@@ -40,6 +40,7 @@
  * @author wenmo
  * @since 2022/2/23 20:18
  */
+@Ignore
 public class DingTalkTest {
 
     private static Map<String, String> config = new HashMap<>();

File: dinky-alert/dinky-alert-email/src/test/java/org/dinky/alert/email/EmailSenderTest.java
Patch:
@@ -41,6 +41,7 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+@Ignore
 public class EmailSenderTest {
 
     private static final Logger logger = LoggerFactory.getLogger(EmailSenderTest.class);

File: dinky-alert/dinky-alert-feishu/src/test/java/org/dinky/alert/feishu/FeiShuSenderTest.java
Patch:
@@ -36,6 +36,7 @@
  *
  * @date: 2022/4/2 @Description: 飞书消息发送 单元测试
  */
+@Ignore
 public class FeiShuSenderTest {
 
     private static Map<String, String> feiShuConfig = new HashMap<>();

File: dinky-alert/dinky-alert-wechat/src/test/java/org/dinky/alert/wechat/WeChatSenderTest.java
Patch:
@@ -32,6 +32,7 @@
 import org.junit.Test;
 
 /** WeChatSenderTest */
+@Ignore
 public class WeChatSenderTest {
 
     private static Map<String, String> weChatConfig = new HashMap<>();

File: dinky-catalog/dinky-catalog-mysql/dinky-catalog-mysql-1.14/src/test/java/org/dinky/flink/catalog/DinkyMysqlCatalogTest.java
Patch:
@@ -25,8 +25,10 @@
 import org.apache.flink.table.api.TableEnvironment;
 
 import org.junit.Before;
+import org.junit.Ignore;
 import org.junit.Test;
 
+@Ignore
 public class DinkyMysqlCatalogTest {
 
     protected static String url;

File: dinky-catalog/dinky-catalog-mysql/dinky-catalog-mysql-1.14/src/test/java/org/dinky/flink/catalog/org/dinky/flink/catalog/factory/DinkyMysqlCatalogFactoryTest.java
Patch:
@@ -34,8 +34,10 @@
 import java.util.Map;
 
 import org.junit.BeforeClass;
+import org.junit.Ignore;
 import org.junit.Test;
 
+@Ignore
 public class DinkyMysqlCatalogFactoryTest {
 
     protected static String url;

File: dinky-client/dinky-client-1.16/src/main/java/org/dinky/cdc/sql/SQLSinkBuilder.java
Patch:
@@ -237,8 +237,8 @@ public DataStreamSource build(
             StreamExecutionEnvironment env,
             CustomTableEnvironment customTableEnvironment,
             DataStreamSource<String> dataStreamSource) {
-        final String timeZone = config.getSink().get("timezone");
-        config.getSink().remove("timezone");
+        final String timeZone = config.getSink().get(FlinkCDCConfig.TIMEZONE);
+        config.getSink().remove(FlinkCDCConfig.TIMEZONE);
         if (Asserts.isNotNullString(timeZone)) {
             sinkTimeZone = ZoneId.of(timeZone);
         }

File: dinky-core/src/test/java/org/dinky/core/BatchTest.java
Patch:
@@ -32,6 +32,7 @@
  * @author wenmo
  * @since 2022/2/7 23:15
  */
+@Ignore
 public class BatchTest {
 
     @Ignore

File: dinky-core/src/test/java/org/dinky/core/FlinkSqlPlusTest.java
Patch:
@@ -32,6 +32,7 @@
  * @author wenmo
  * @since 2021/6/23 10:37
  */
+@Ignore
 public class FlinkSqlPlusTest {
 
     @Ignore

File: dinky-core/src/test/java/org/dinky/core/JobManagerTest.java
Patch:
@@ -36,6 +36,7 @@
  * @author wenmo
  * @since 2021/6/3
  */
+@Ignore
 public class JobManagerTest {
 
     @Ignore

File: dinky-core/src/test/java/org/dinky/core/LineageTest.java
Patch:
@@ -31,6 +31,7 @@
  * @author wenmo
  * @since 2022/3/15 23:08
  */
+@Ignore
 public class LineageTest {
 
     @Ignore

File: dinky-executor/src/test/java/org/dinky/interceptor/FlinkInterceptorTest.java
Patch:
@@ -22,6 +22,7 @@
 import org.dinky.executor.Executor;
 
 import org.junit.Assert;
+import org.junit.Ignore;
 import org.junit.Test;
 
 /**
@@ -30,6 +31,7 @@
  * @author wenmo
  * @since 2022/4/9 17:48
  */
+@Ignore
 public class FlinkInterceptorTest {
 
     @Test

File: dinky-metadata/dinky-metadata-clickhouse/src/test/java/org/dinky/metadata/ClickHouseTest.java
Patch:
@@ -37,6 +37,7 @@
  * @author heyang
  * @since 2022/4/21 1:06
  */
+@Ignore
 public class ClickHouseTest {
 
     private static final String IP = "127.0.0.1";

File: dinky-metadata/dinky-metadata-doris/src/test/java/org/dinky/metadata/DorisTest.java
Patch:
@@ -35,6 +35,7 @@
 import org.junit.Ignore;
 import org.junit.Test;
 
+@Ignore
 public class DorisTest {
 
     private Driver driver;

File: dinky-metadata/dinky-metadata-hive/src/test/java/org/dinky/metadata/HiveTest.java
Patch:
@@ -40,6 +40,7 @@
  * @author wenmo
  * @since 2021/7/20 15:32
  */
+@Ignore
 public class HiveTest {
 
     private static final String IP = "cdh1";

File: dinky-metadata/dinky-metadata-mysql/src/test/java/org/dinky/metadata/MysqlTest.java
Patch:
@@ -37,6 +37,7 @@
  * @author wenmo
  * @since 2021/7/20 15:32
  */
+@Ignore
 public class MysqlTest {
 
     private static final String IP = "127.0.0.1";

File: dinky-metadata/dinky-metadata-oracle/src/test/java/org/dinky/metadata/OracleTest.java
Patch:
@@ -37,6 +37,7 @@
  * @author wenmo
  * @since 2021/7/21 16:14
  */
+@Ignore
 public class OracleTest {
 
     private static final String IP = "127.0.0.1";

File: dinky-metadata/dinky-metadata-phoenix/src/test/java/org/dinky/metadata/PhoenixTest.java
Patch:
@@ -33,6 +33,7 @@
 import org.junit.Ignore;
 import org.junit.Test;
 
+@Ignore
 public class PhoenixTest {
 
     private Driver driver;

File: dinky-metadata/dinky-metadata-presto/src/test/java/org/dinky/PrestoTest.java
Patch:
@@ -35,6 +35,7 @@
 import org.junit.Ignore;
 import org.junit.Test;
 
+@Ignore
 public class PrestoTest {
 
     private Driver driver;

File: dinky-metadata/dinky-metadata-sqlserver/src/test/java/org/dinky/metadata/SqlServerTest.java
Patch:
@@ -35,6 +35,7 @@
 import org.junit.Ignore;
 import org.junit.Test;
 
+@Ignore
 public class SqlServerTest {
 
     private Driver driver;

File: dinky-alert/dinky-alert-base/src/main/java/org/dinky/utils/ExcelUtils.java
Patch:
@@ -17,10 +17,9 @@
  *
  */
 
-package org.dinky.alert.email;
+package org.dinky.utils;
 
 import org.dinky.alert.AlertException;
-import org.dinky.utils.JSONUtil;
 
 import org.apache.commons.collections4.CollectionUtils;
 import org.apache.poi.ss.usermodel.Cell;

File: dinky-alert/dinky-alert-base/src/main/java/org/dinky/utils/HttpRequestUtil.java
Patch:
@@ -17,7 +17,7 @@
  *
  */
 
-package org.dinky.alert.feishu;
+package org.dinky.utils;
 
 import org.apache.http.HttpHost;
 import org.apache.http.auth.AuthScope;

File: dinky-alert/dinky-alert-email/src/main/java/org/dinky/alert/email/EmailAlert.java
Patch:
@@ -37,7 +37,7 @@ public String getType() {
 
     @Override
     public AlertResult send(String title, String content) {
-        MailSender mailSender = new MailSender(getConfig().getParam());
-        return mailSender.send(title, content);
+        EmailSender emailSender = new EmailSender(getConfig().getParam());
+        return emailSender.send(title, content);
     }
 }

File: dinky-alert/dinky-alert-feishu/src/test/java/org/dinky/alert/feishu/FeiShuSenderTest.java
Patch:
@@ -62,7 +62,7 @@ public void initFeiShuConfig() {
         feiShuConfig.put(
                 FeiShuConstants.WEB_HOOK,
                 "https://open.feishu.cn/open-apis/bot/v2/hook/aea3cd7f-75b4-45cd-abea-2c0dc808f2a9");
-        feiShuConfig.put(FeiShuConstants.KEY_WORD, "Dinky 飞书WebHook 告警测试");
+        feiShuConfig.put(FeiShuConstants.KEYWORD, "Dinky 飞书WebHook 告警测试");
         feiShuConfig.put(FeiShuConstants.MSG_TYPE, "text");
         feiShuConfig.put(FeiShuConstants.AT_ALL, "true");
         feiShuConfig.put(FeiShuConstants.AT_USERS, "zhumingye");

File: dinky-admin/src/main/java/org/dinky/constant/DirConstant.java
Patch:
@@ -35,7 +35,6 @@ public class DirConstant {
 
     static {
         String separator = System.getProperty(FILE_SEPARATOR);
-        // String rootPath = new ApplicationHome(Dinky.class).getSource().getParent();
         String rootPath = new ApplicationHome().getDir().getPath();
         LOG_DIR_PATH = rootPath + separator + "logs";
         ROOT_LOG_PATH = LOG_DIR_PATH + separator + "dinky.log";

File: dinky-admin/src/main/java/org/dinky/context/SpringContextUtils.java
Patch:
@@ -60,7 +60,7 @@ public static boolean isSingleton(String name) {
         return applicationContext.isSingleton(name);
     }
 
-    public static Class<? extends Object> getType(String name) {
+    public static Class<?> getType(String name) {
         return applicationContext.getType(name);
     }
 }

File: dinky-admin/src/main/java/org/dinky/dto/APICancelDTO.java
Patch:
@@ -31,6 +31,7 @@
  * @author wenmo
  * @since 2021/12/12 18:53
  */
+@SuppressWarnings("AlibabaClassNamingShouldBeCamel")
 @Getter
 @Setter
 public class APICancelDTO {

File: dinky-admin/src/main/java/org/dinky/enums/Status.java
Patch:
@@ -92,7 +92,7 @@ public enum Status {
 
     /** alert group , alert instance */
     CREATE_ALERT_GROUP_ERROR(10027, "create alert group error", "创建告警组失败"),
-    QUERY_ALL_ALERTGROUP_ERROR(10028, "query all alertgroup error", "查询告警组失败"),
+    QUERY_ALL_ALERT_GROUP_ERROR(10028, "query all alert group error", "查询告警组失败"),
     UPDATE_ALERT_GROUP_ERROR(10030, "update alert group error", "更新告警组失败"),
     DELETE_ALERT_GROUP_ERROR(10031, "delete alert group error", "删除告警组失败"),
     ALERT_GROUP_GRANT_USER_ERROR(10032, "alert group grant user error", "告警组授权用户失败"),

File: dinky-admin/src/main/java/org/dinky/interceptor/LocaleChangeInterceptor.java
Patch:
@@ -31,10 +31,10 @@
 import org.springframework.context.i18n.LocaleContextHolder;
 import org.springframework.lang.Nullable;
 import org.springframework.util.StringUtils;
-import org.springframework.web.servlet.handler.HandlerInterceptorAdapter;
+import org.springframework.web.servlet.AsyncHandlerInterceptor;
 import org.springframework.web.util.WebUtils;
 
-public class LocaleChangeInterceptor extends HandlerInterceptorAdapter {
+public class LocaleChangeInterceptor implements AsyncHandlerInterceptor {
 
     @Override
     public boolean preHandle(

File: dinky-admin/src/main/java/org/dinky/interceptor/TenantInterceptor.java
Patch:
@@ -26,13 +26,13 @@
 import javax.servlet.http.HttpServletRequest;
 import javax.servlet.http.HttpServletResponse;
 
-import org.springframework.web.servlet.HandlerInterceptor;
+import org.springframework.web.servlet.AsyncHandlerInterceptor;
 
 import lombok.extern.slf4j.Slf4j;
 
 /** tenant interceptor */
 @Slf4j
-public class TenantInterceptor implements HandlerInterceptor {
+public class TenantInterceptor implements AsyncHandlerInterceptor {
 
     @Override
     public boolean preHandle(
@@ -43,6 +43,6 @@ public boolean preHandle(
                 .findFirst()
                 .ifPresent(t -> TenantContextHolder.set(Integer.valueOf(t.getValue())));
 
-        return HandlerInterceptor.super.preHandle(request, response, handler);
+        return AsyncHandlerInterceptor.super.preHandle(request, response, handler);
     }
 }

File: dinky-admin/src/main/java/org/dinky/result/TaskOperatingResult.java
Patch:
@@ -57,7 +57,7 @@ public TaskOperatingResult(
         this.taskOperatingSavepointSelect = taskOperatingSavepointSelect;
     }
 
-    public void parseResult(Result result) {
+    public void parseResult(Result<Void> result) {
         if (result == null) {
             return;
         }

File: dinky-metadata/dinky-metadata-mysql/src/main/java/org/dinky/metadata/convert/MySqlTypeConvert.java
Patch:
@@ -37,7 +37,8 @@ public ColumnType convert(Column column) {
         if (Asserts.isNull(column)) {
             return columnType;
         }
-        String t = column.getType().toLowerCase();
+        Integer length = Asserts.isNull(column.getLength()) ? 0 : column.getLength();
+        String t = Asserts.isNull(column.getType()) ? "" : column.getType().toLowerCase();
         boolean isNullable = !column.isKeyFlag() && column.isNullable();
         if (t.contains("numeric") || t.contains("decimal")) {
             columnType = ColumnType.DECIMAL;
@@ -60,7 +61,7 @@ public ColumnType convert(Column column) {
                 columnType = ColumnType.DOUBLE;
             }
         } else if (t.contains("boolean")
-                || (t.contains("tinyint") && column.getLength() == 1)
+                || (t.contains("tinyint") && length.equals(1))
                 || t.contains("bit")) {
             if (isNullable) {
                 columnType = ColumnType.JAVA_LANG_BOOLEAN;

File: dinky-core/src/test/java/org/dinky/core/BatchTest.java
Patch:
@@ -23,6 +23,7 @@
 import org.apache.flink.table.api.TableEnvironment;
 import org.apache.flink.table.api.TableResult;
 
+import org.junit.Ignore;
 import org.junit.Test;
 
 /**
@@ -33,6 +34,7 @@
  */
 public class BatchTest {
 
+    @Ignore
     @Test
     public void batchTest() {
         String source =

File: dinky-core/src/test/java/org/dinky/core/FlinkRestAPITest.java
Patch:
@@ -24,6 +24,7 @@
 
 import java.util.List;
 
+import org.junit.Ignore;
 import org.junit.Test;
 
 import com.fasterxml.jackson.databind.JsonNode;
@@ -34,6 +35,7 @@
  * @author wenmo
  * @since 2021/6/24 14:24
  */
+@Ignore
 public class FlinkRestAPITest {
 
     // private String address = "192.168.123.157:8081";

File: dinky-core/src/test/java/org/dinky/core/FlinkSqlPlusTest.java
Patch:
@@ -23,6 +23,7 @@
 
 import org.apache.flink.runtime.rest.messages.JobPlanInfo;
 
+import org.junit.Ignore;
 import org.junit.Test;
 
 /**
@@ -33,6 +34,7 @@
  */
 public class FlinkSqlPlusTest {
 
+    @Ignore
     @Test
     public void getJobPlanInfo() {
         String sql =

File: dinky-core/src/test/java/org/dinky/core/JobManagerTest.java
Patch:
@@ -27,6 +27,7 @@
 
 import java.util.HashMap;
 
+import org.junit.Ignore;
 import org.junit.Test;
 
 /**
@@ -37,6 +38,7 @@
  */
 public class JobManagerTest {
 
+    @Ignore
     @Test
     public void cancelJobSelect() {
 

File: dinky-core/src/test/java/org/dinky/core/LineageTest.java
Patch:
@@ -22,6 +22,7 @@
 import org.dinky.explainer.lineage.LineageBuilder;
 import org.dinky.explainer.lineage.LineageResult;
 
+import org.junit.Ignore;
 import org.junit.Test;
 
 /**
@@ -32,6 +33,7 @@
  */
 public class LineageTest {
 
+    @Ignore
     @Test
     public void sumTest() {
         String sql =

File: dinky-executor/src/test/java/org/dinky/executor/SqlManagerTest.java
Patch:
@@ -19,8 +19,6 @@
 
 package org.dinky.executor;
 
-import static org.junit.jupiter.api.Assertions.*;
-
 import org.junit.jupiter.api.Test;
 
 /** */

File: dinky-client/dinky-client-1.12/src/main/java/org/dinky/cdc/AbstractSinkBuilder.java
Patch:
@@ -307,9 +307,9 @@ public LogicalType getLogicalType(Column column) {
             case INTEGER:
                 return new IntType();
             case DATE:
-            case LOCALDATE:
+            case LOCAL_DATE:
                 return new DateType();
-            case LOCALDATETIME:
+            case LOCAL_DATETIME:
             case TIMESTAMP:
                 return new TimestampType();
             case BYTES:

File: dinky-client/dinky-client-1.13/src/main/java/org/dinky/cdc/AbstractSinkBuilder.java
Patch:
@@ -306,9 +306,9 @@ public LogicalType getLogicalType(Column column) {
             case INTEGER:
                 return new IntType();
             case DATE:
-            case LOCALDATE:
+            case LOCAL_DATE:
                 return new DateType();
-            case LOCALDATETIME:
+            case LOCAL_DATETIME:
             case TIMESTAMP:
                 return new TimestampType();
             case BYTES:

File: dinky-client/dinky-client-1.13/src/main/java/org/dinky/cdc/kafka/KafkaSinkJsonBuilder.java
Patch:
@@ -326,9 +326,9 @@ public LogicalType getLogicalType(Column column) {
                 case INTEGER:
                     return new IntType();
                 case DATE:
-                case LOCALDATE:
+                case LOCAL_DATE:
                     return new DateType();
-                case LOCALDATETIME:
+                case LOCAL_DATETIME:
                 case TIMESTAMP:
                     return new TimestampType();
                 case BYTES:

File: dinky-client/dinky-client-1.14/src/main/java/org/dinky/cdc/AbstractSinkBuilder.java
Patch:
@@ -317,9 +317,9 @@ public LogicalType getLogicalType(Column column) {
             case INTEGER:
                 return new IntType();
             case DATE:
-            case LOCALDATE:
+            case LOCAL_DATE:
                 return new DateType();
-            case LOCALDATETIME:
+            case LOCAL_DATETIME:
             case TIMESTAMP:
                 return new TimestampType();
             case BYTES:

File: dinky-client/dinky-client-1.15/src/main/java/org/dinky/cdc/AbstractSinkBuilder.java
Patch:
@@ -317,9 +317,9 @@ public LogicalType getLogicalType(Column column) {
             case INTEGER:
                 return new IntType();
             case DATE:
-            case LOCALDATE:
+            case LOCAL_DATE:
                 return new DateType();
-            case LOCALDATETIME:
+            case LOCAL_DATETIME:
             case TIMESTAMP:
                 return new TimestampType();
             case BYTES:

File: dinky-client/dinky-client-1.16/src/main/java/org/dinky/cdc/AbstractSinkBuilder.java
Patch:
@@ -311,9 +311,9 @@ public LogicalType getLogicalType(Column column) {
             case INTEGER:
                 return new IntType();
             case DATE:
-            case LOCALDATE:
+            case LOCAL_DATE:
                 return new DateType();
-            case LOCALDATETIME:
+            case LOCAL_DATETIME:
             case TIMESTAMP:
                 return new TimestampType();
             case BYTES:

File: dinky-common/src/main/java/org/dinky/model/ColumnType.java
Patch:
@@ -42,12 +42,12 @@ public enum ColumnType {
     JAVA_LANG_DOUBLE("java.lang.Double", "DOUBLE"),
     DOUBLE("double", "DOUBLE NOT NULL"),
     DATE("java.sql.Date", "DATE"),
-    LOCALDATE("java.time.LocalDate", "DATE"),
+    LOCAL_DATE("java.time.LocalDate", "DATE"),
     TIME("java.sql.Time", "TIME"),
     LOCALTIME("java.time.LocalTime", "TIME"),
     TIMESTAMP("java.sql.Timestamp", "TIMESTAMP"),
-    LOCALDATETIME("java.time.LocalDateTime", "TIMESTAMP"),
-    OFFSETDATETIME("java.time.OffsetDateTime", "TIMESTAMP WITH TIME ZONE"),
+    LOCAL_DATETIME("java.time.LocalDateTime", "TIMESTAMP"),
+    OFFSET_DATETIME("java.time.OffsetDateTime", "TIMESTAMP WITH TIME ZONE"),
     INSTANT("java.time.Instant", "TIMESTAMP_LTZ"),
     DURATION("java.time.Duration", "INVERVAL SECOND"),
     PERIOD("java.time.Period", "INTERVAL YEAR TO MONTH"),

File: dinky-metadata/dinky-metadata-oracle/src/main/java/org/dinky/metadata/convert/OracleTypeConvert.java
Patch:
@@ -42,7 +42,7 @@ public ColumnType convert(Column column) {
         if (t.contains("char")) {
             columnType = ColumnType.STRING;
         } else if (t.contains("date")) {
-            columnType = ColumnType.LOCALDATETIME;
+            columnType = ColumnType.LOCAL_DATETIME;
         } else if (t.contains("timestamp")) {
             columnType = ColumnType.TIMESTAMP;
         } else if (t.contains("number")) {

File: dinky-metadata/dinky-metadata-sqlserver/src/main/java/org/dinky/metadata/convert/SqlServerTypeConvert.java
Patch:
@@ -86,7 +86,7 @@ public ColumnType convert(Column column) {
             // 这里应该是纳秒
             columnType = ColumnType.TIMESTAMP;
         } else if (t.equalsIgnoreCase("date")) {
-            columnType = ColumnType.LOCALDATE;
+            columnType = ColumnType.LOCAL_DATE;
         } else if (t.equalsIgnoreCase("time")) {
             columnType = ColumnType.LOCALTIME;
         } else if (t.contains("timestamp")

File: dinky-metadata/dinky-metadata-postgresql/src/main/java/org/dinky/metadata/convert/PostgreSqlTypeConvert.java
Patch:
@@ -51,7 +51,7 @@ public ColumnType convert(Column column) {
             } else {
                 columnType = ColumnType.INT;
             }
-        } else if (t.contains("bigint") || t.contains("bigserial")) {
+        } else if (t.contains("bigint") || t.contains("int8") || t.contains("bigserial")) {
             if (isNullable) {
                 columnType = ColumnType.JAVA_LANG_LONG;
             } else {

File: dinky-client/dinky-client-1.16/src/main/java/org/dinky/cdc/sql/SQLSinkBuilder.java
Patch:
@@ -169,7 +169,7 @@ private void addTableSink(
         String sinkTableName = getSinkTableName(table);
         String pkList = StringUtils.join(getPKList(table), ".");
         String viewName = "VIEW_" + table.getSchemaTableNameWithUnderline();
-        customTableEnvironment.createTemporaryView(viewName, rowDataDataStream, StringUtils.join(columnNameList, ","));
+        customTableEnvironment.createTemporaryView(viewName, rowDataDataStream);
         logger.info("Create " + viewName + " temporaryView successful...");
         String flinkDDL = FlinkBaseUtil.getFlinkDDL(table, sinkTableName, config, sinkSchemaName, sinkTableName,
                 pkList);

File: dinky-admin/src/main/java/org/dinky/service/impl/TaskServiceImpl.java
Patch:
@@ -242,11 +242,11 @@ public JobResult submitTask(Integer id) {
         process.start();
         if (!config.isJarTask()) {
             JobResult jobResult = jobManager.executeSql(task.getStatement());
-            process.finish("Submit Flink SQL succeed.");
+            process.finish("Submit Flink SQL finished.");
             return jobResult;
         } else {
             JobResult jobResult = jobManager.executeJar();
-            process.finish("Submit Flink Jar succeed.");
+            process.finish("Submit Flink Jar finished.");
             return jobResult;
         }
     }

File: dlink-metadata/dlink-metadata-mysql/src/main/java/com/dlink/metadata/convert/MySqlTypeConvert.java
Patch:
@@ -30,6 +30,7 @@
  * @since 2021/7/20 15:21
  **/
 public class MySqlTypeConvert implements ITypeConvert {
+
     @Override
     public ColumnType convert(Column column) {
         ColumnType columnType = ColumnType.STRING;
@@ -58,7 +59,7 @@ public ColumnType convert(Column column) {
             } else {
                 columnType = ColumnType.DOUBLE;
             }
-        } else if (t.contains("boolean") || t.contains("tinyint(1)") || t.contains("bit")) {
+        } else if (t.contains("boolean") || (t.contains("tinyint") && column.getLength() == 1) || t.contains("bit")) {
             if (isNullable) {
                 columnType = ColumnType.JAVA_LANG_BOOLEAN;
             } else {

File: dlink-core/src/main/java/com/dlink/explainer/Explainer.java
Patch:
@@ -114,7 +114,8 @@ public static Explainer build(Executor executor, boolean useStatementSet, String
 
     public Explainer initialize(JobManager jobManager, JobConfig config, String statement) {
         jobManager.initClassLoader(config);
-        jobManager.initUDF(parseUDFFromStatements(SqlUtil.getStatements(statement, sqlSeparator)));
+        String[] statements = SqlUtil.getStatements(SqlUtil.removeNote(statement), sqlSeparator);
+        jobManager.initUDF(parseUDFFromStatements(statements));
         return this;
     }
 

File: dlink-core/src/main/java/com/dlink/job/JobManager.java
Patch:
@@ -290,7 +290,9 @@ private void addConfigurationClsAndJars(List<URL> jarList, List<URL> classpaths)
     }
 
     public void initUDF(List<UDF> udfList) {
-        initUDF(udfList, runMode, config.getTaskId());
+        if (Asserts.isNotNullCollection(udfList)) {
+            initUDF(udfList, runMode, config.getTaskId());
+        }
     }
 
     public void initUDF(List<UDF> udfList, GatewayType runMode, Integer taskId) {

File: dlink-admin/src/main/java/com/dlink/service/impl/TaskServiceImpl.java
Patch:
@@ -1002,7 +1002,6 @@ public JobInstance refreshJobInstance(Integer id, boolean isCoercive) {
                 && !status.equals(jobInfoDetail.getInstance().getStatus())) {
             jobStatusChanged = true;
             jobInfoDetail.getInstance().setFinishTime(LocalDateTime.now());
-            // handleJobDone(jobInfoDetail.getInstance());
         }
         if (isCoercive) {
             DaemonFactory.addTask(DaemonTaskConfig.build(FlinkJobTask.TYPE, jobInfoDetail.getInstance().getId()));

File: dlink-core/src/main/java/com/dlink/job/JobConfig.java
Patch:
@@ -35,6 +35,7 @@
 import org.apache.http.util.TextUtils;
 
 import java.util.ArrayList;
+import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 
@@ -83,6 +84,7 @@ public class JobConfig {
     private Map<String, String> config;
 
     public JobConfig() {
+        this.config = new HashMap<String, String>();
     }
 
     public void setAddress(String address) {

File: dlink-core/src/main/java/com/dlink/job/JobManager.java
Patch:
@@ -687,7 +687,7 @@ public SavePointResult savepoint(String jobId, String savePointType, String save
                     savePointType, null));
             return Gateway.build(config.getGatewayConfig()).savepointJob(savePoint);
         } else {
-            return FlinkAPI.build(config.getAddress()).savepoints(jobId, savePointType);
+            return FlinkAPI.build(config.getAddress()).savepoints(jobId, savePointType, config.getConfig());
         }
     }
 

File: dlink-core/src/test/java/com/dlink/core/FlinkRestAPITest.java
Patch:
@@ -42,7 +42,8 @@ public class FlinkRestAPITest {
     @Test
     public void savepointTest() {
         //JsonNode savepointInfo = FlinkAPI.build(address).getSavepointInfo("602ad9d03b872dba44267432d1a2a3b2","04044589477a973a32e7dd53e1eb20fd");
-        SavePointResult savepoints = FlinkAPI.build(address).savepoints("243b97597448edbd2e635fc3d25b1064", "trigger");
+        SavePointResult savepoints = FlinkAPI.build(address).savepoints("243b97597448edbd2e635fc3d25b1064", "trigger"
+                , null);
         System.out.println(savepoints.toString());
     }
 

File: dlink-core/src/main/java/com/dlink/explainer/Explainer.java
Patch:
@@ -549,7 +549,7 @@ public List<LineageRel> getLineage(String statement) {
                 SqlType operationType = Operations.getOperationType(sql);
                 if (operationType.equals(SqlType.INSERT)) {
                     lineageRelList.addAll(executor.getLineage(sql));
-                } else {
+                } else if (!operationType.equals(SqlType.SELECT)) {
                     executor.executeSql(sql);
                 }
             } catch (Exception e) {

File: dlink-client/dlink-client-1.11/src/main/java/com/dlink/cdc/mysql/MysqlCDCBuilder.java
Patch:
@@ -44,7 +44,7 @@
  **/
 public class MysqlCDCBuilder extends AbstractCDCBuilder implements CDCBuilder {
 
-    private static final String KEY_WORD = "mysql-cdc";
+    public static final String KEY_WORD = "mysql-cdc";
     private static final String METADATA_TYPE = "MySql";
 
     public MysqlCDCBuilder() {

File: dlink-client/dlink-client-1.12/src/main/java/com/dlink/cdc/mysql/MysqlCDCBuilder.java
Patch:
@@ -45,7 +45,7 @@
  **/
 public class MysqlCDCBuilder extends AbstractCDCBuilder implements CDCBuilder {
 
-    private static final String KEY_WORD = "mysql-cdc";
+    public static final String KEY_WORD = "mysql-cdc";
     private static final String METADATA_TYPE = "MySql";
 
     public MysqlCDCBuilder() {

File: dlink-client/dlink-client-1.13/src/main/java/com/dlink/cdc/mysql/MysqlCDCBuilder.java
Patch:
@@ -48,7 +48,7 @@
  **/
 public class MysqlCDCBuilder extends AbstractCDCBuilder implements CDCBuilder {
 
-    private static final String KEY_WORD = "mysql-cdc";
+    public static final String KEY_WORD = "mysql-cdc";
     private static final String METADATA_TYPE = "MySql";
 
     public MysqlCDCBuilder() {

File: dlink-client/dlink-client-1.13/src/main/java/com/dlink/cdc/oracle/OracleCDCBuilder.java
Patch:
@@ -46,7 +46,7 @@
  **/
 public class OracleCDCBuilder extends AbstractCDCBuilder implements CDCBuilder {
 
-    private static final String KEY_WORD = "oracle-cdc";
+    public static final String KEY_WORD = "oracle-cdc";
     private static final String METADATA_TYPE = "Oracle";
 
     public OracleCDCBuilder() {

File: dlink-client/dlink-client-1.14/src/main/java/com/dlink/cdc/doris/DorisSchemaEvolutionSinkBuilder.java
Patch:
@@ -60,7 +60,7 @@
  **/
 public class DorisSchemaEvolutionSinkBuilder extends AbstractSinkBuilder implements Serializable {
 
-    private static final String KEY_WORD = "datastream-doris-schema-evolution";
+    public static final String KEY_WORD = "datastream-doris-schema-evolution";
 
     public DorisSchemaEvolutionSinkBuilder() {
     }

File: dlink-client/dlink-client-1.14/src/main/java/com/dlink/cdc/mysql/MysqlCDCBuilder.java
Patch:
@@ -48,7 +48,7 @@
  **/
 public class MysqlCDCBuilder extends AbstractCDCBuilder implements CDCBuilder {
 
-    private static final String KEY_WORD = "mysql-cdc";
+    public static final String KEY_WORD = "mysql-cdc";
     private static final String METADATA_TYPE = "MySql";
 
     public MysqlCDCBuilder() {

File: dlink-client/dlink-client-1.14/src/main/java/com/dlink/cdc/oracle/OracleCDCBuilder.java
Patch:
@@ -46,7 +46,7 @@
  **/
 public class OracleCDCBuilder extends AbstractCDCBuilder implements CDCBuilder {
 
-    private static final String KEY_WORD = "oracle-cdc";
+    public static final String KEY_WORD = "oracle-cdc";
     private static final String METADATA_TYPE = "Oracle";
 
     public OracleCDCBuilder() {

File: dlink-client/dlink-client-1.14/src/main/java/com/dlink/cdc/postgres/PostgresCDCBuilder.java
Patch:
@@ -45,7 +45,7 @@
  **/
 public class PostgresCDCBuilder extends AbstractCDCBuilder implements CDCBuilder {
 
-    private static final String KEY_WORD = "postgres-cdc";
+    public static final String KEY_WORD = "postgres-cdc";
     private static final String METADATA_TYPE = "PostgreSql";
 
     public PostgresCDCBuilder() {

File: dlink-client/dlink-client-1.14/src/main/java/com/dlink/cdc/sqlserver/SqlServerCDCBuilder.java
Patch:
@@ -49,7 +49,7 @@ public class SqlServerCDCBuilder extends AbstractCDCBuilder implements CDCBuilde
 
     protected static final Logger logger = LoggerFactory.getLogger(SqlServerCDCBuilder.class);
 
-    private static final String KEY_WORD = "sqlserver-cdc";
+    public static final String KEY_WORD = "sqlserver-cdc";
     private static final String METADATA_TYPE = "SqlServer";
 
     public SqlServerCDCBuilder() {

File: dlink-client/dlink-client-1.15/src/main/java/com/dlink/cdc/doris/DorisSchemaEvolutionSinkBuilder.java
Patch:
@@ -60,7 +60,7 @@
  **/
 public class DorisSchemaEvolutionSinkBuilder extends AbstractSinkBuilder implements Serializable {
 
-    private static final String KEY_WORD = "datastream-doris-schema-evolution";
+    public static final String KEY_WORD = "datastream-doris-schema-evolution";
 
     public DorisSchemaEvolutionSinkBuilder() {
     }

File: dlink-client/dlink-client-1.15/src/main/java/com/dlink/cdc/mysql/MysqlCDCBuilder.java
Patch:
@@ -48,7 +48,7 @@
  **/
 public class MysqlCDCBuilder extends AbstractCDCBuilder {
 
-    private static final String KEY_WORD = "mysql-cdc";
+    public static final String KEY_WORD = "mysql-cdc";
     private static final String METADATA_TYPE = "MySql";
 
     public MysqlCDCBuilder() {

File: dlink-client/dlink-client-1.15/src/main/java/com/dlink/cdc/oracle/OracleCDCBuilder.java
Patch:
@@ -46,7 +46,7 @@
  **/
 public class OracleCDCBuilder extends AbstractCDCBuilder {
 
-    private static final String KEY_WORD = "oracle-cdc";
+    public static final String KEY_WORD = "oracle-cdc";
     private static final String METADATA_TYPE = "Oracle";
 
     public OracleCDCBuilder() {

File: dlink-client/dlink-client-1.16/src/main/java/com/dlink/cdc/mysql/MysqlCDCBuilder.java
Patch:
@@ -48,7 +48,7 @@
  **/
 public class MysqlCDCBuilder extends AbstractCDCBuilder {
 
-    private static final String KEY_WORD = "mysql-cdc";
+    public static final String KEY_WORD = "mysql-cdc";
     private static final String METADATA_TYPE = "MySql";
 
     public MysqlCDCBuilder() {

File: dlink-admin/src/main/java/com/dlink/service/impl/TaskServiceImpl.java
Patch:
@@ -428,9 +428,9 @@ public void initTenantByTaskId(Integer id) {
 
     @Override
     public boolean saveOrUpdateTask(Task task) {
-        if (CollUtil.isNotEmpty(task.getConfig()) && Dialect.isUDF(task.getDialect())
-                && Convert.toInt(task.getConfig().get(0).get("templateId"), 0) != 0) {
-            if (Asserts.isNullString(task.getStatement())) {
+        if (Dialect.isUDF(task.getDialect())) {
+            if (CollUtil.isNotEmpty(task.getConfig()) && Asserts.isNullString(task.getStatement())
+                    && Convert.toInt(task.getConfig().get(0).get("templateId"), 0) != 0) {
                 Map<String, String> config = task.getConfig().get(0);
                 UDFTemplate template = udfTemplateService.getById(config.get("templateId"));
                 if (template != null) {

File: dlink-client/dlink-client-1.14/src/main/java/org/apache/flink/table/types/extraction/ExtractionUtils.java
Patch:
@@ -752,7 +752,7 @@ private AssigningConstructor(Constructor<?> constructor, List<String> parameterN
             } else {
                 extractor = new ParameterExtractor((Method) executable);
             }
-            getClassReader(executable.getDeclaringClass()).accept(extractor, 0);
+//            getClassReader(executable.getDeclaringClass()).accept(extractor, 0);
 
             final List<String> extractedNames = extractor.getParameterNames();
             if (extractedNames.size() == 0) {

File: dlink-admin/src/main/java/com/dlink/service/impl/DataBaseServiceImpl.java
Patch:
@@ -205,7 +205,7 @@ public String getEnabledFlinkWithSql() {
     public boolean copyDatabase(DataBase database) {
         String name = UUID.randomUUID().toString().replaceAll("-", "").substring(0, 10);
         database.setId(null);
-        database.setName(database.getName().substring(0, 10) + "_" + name);
+        database.setName((database.getName().length() > 10 ? database.getName().substring(0, 10) : database.getName()) + "_" + name);
         database.setCreateTime(null);
         return this.save(database);
     }

File: dlink-executor/src/main/java/com/dlink/executor/SqlManager.java
Patch:
@@ -269,11 +269,11 @@ private String parseDateVar(String key) {
         int days = 0;
         try {
             if (key.contains("+")) {
-                int s = key.indexOf("+");
+                int s = key.indexOf("+") + 1;
                 String num = key.substring(s).trim();
                 days = Integer.parseInt(num);
             } else if (key.contains("-")) {
-                int s = key.indexOf("-");
+                int s = key.indexOf("-") + 1;
                 String num = key.substring(s).trim();
                 days = Integer.parseInt(num) * -1;
             }

File: dlink-admin/src/main/java/com/dlink/assertion/Tips.java
Patch:
@@ -29,7 +29,8 @@ public class Tips {
 
     public static final String TASK_NOT_EXIST = "作业不存在";
 
-    public static final String SAVEPOINT_IS_NULL = "保存点为空";
+    public static final String JOB_INSTANCE_NOT_EXIST = "作业实例不存在";
 
+    public static final String SAVEPOINT_IS_NULL = "保存点为空";
 
 }

File: dlink-admin/src/main/java/com/dlink/mapper/JobInstanceMapper.java
Patch:
@@ -24,6 +24,7 @@
 import com.dlink.model.JobInstanceCount;
 
 import org.apache.ibatis.annotations.Mapper;
+import org.apache.ibatis.annotations.Param;
 
 import java.util.List;
 
@@ -50,4 +51,6 @@ public interface JobInstanceMapper extends SuperMapper<JobInstance> {
 
     JobInstance getJobInstanceByTaskId(Integer id);
 
+    @InterceptorIgnore(tenantLine = "true")
+    Integer getTenantByJobInstanceId(@Param("id") Integer id);
 }

File: dlink-admin/src/main/java/com/dlink/service/JobInstanceService.java
Patch:
@@ -55,4 +55,6 @@ public interface JobInstanceService extends ISuperService<JobInstance> {
     JobInstance getJobInstanceByTaskId(Integer id);
 
     ProTableResult<JobInstance> listJobInstances(JsonNode para);
+
+    void initTenantByJobInstanceId(Integer id);
 }

File: dlink-admin/src/main/java/com/dlink/service/TaskService.java
Patch:
@@ -55,6 +55,8 @@ public interface TaskService extends ISuperService<Task> {
 
     Task getTaskInfoById(Integer id);
 
+    void initTenantByTaskId(Integer id);
+
     boolean saveOrUpdateTask(Task task);
 
     List<Task> listFlinkSQLEnv();
@@ -104,7 +106,7 @@ public interface TaskService extends ISuperService<Task> {
     Result queryAllCatalogue();
 
     Result<List<Task>> queryOnLineTaskByDoneStatus(List<JobLifeCycle> jobLifeCycle, List<JobStatus> jobStatuses,
-                                                   boolean includeNull, Integer catalogueId);
+            boolean includeNull, Integer catalogueId);
 
     void selectSavepointOnLineTask(TaskOperatingResult taskOperatingResult);
 

File: dlink-admin/src/main/java/com/dlink/service/impl/DataBaseServiceImpl.java
Patch:
@@ -135,7 +135,7 @@ public String getSqlSelect(Integer id, String schemaName, String tableName) {
         Driver driver = Driver.build(dataBase.getDriverConfig());
         List<Column> columns = driver.listColumns(schemaName, tableName);
         Table table = Table.build(tableName, schemaName, columns);
-        return table.getSqlSelect(dataBase.getName());
+        return driver.getSqlSelect(table);
     }
 
     @Override
@@ -165,7 +165,7 @@ public SqlGeneration getSqlGeneration(Integer id, String schemaName, String tabl
         Table table = driver.getTable(schemaName, tableName);
         SqlGeneration sqlGeneration = new SqlGeneration();
         sqlGeneration.setFlinkSqlCreate(table.getFlinkTableSql(dataBase.getName(), dataBase.getFlinkTemplate()));
-        sqlGeneration.setSqlSelect(table.getSqlSelect(dataBase.getName()));
+        sqlGeneration.setSqlSelect(driver.getSqlSelect(table));
         sqlGeneration.setSqlCreate(driver.getCreateTableSql(table));
         driver.close();
         return sqlGeneration;

File: dlink-common/src/main/java/com/dlink/model/Table.java
Patch:
@@ -236,7 +236,6 @@ public String getSqlSelect(String catalogName) {
                 sb.append("`" + columns.get(i).getName() + "`  --  " + columnComment + " \n");
             } else {
                 sb.append("`" + columns.get(i).getName() + "` \n");
-
             }
         }
         if (Asserts.isNotNullString(comment)) {

File: dlink-metadata/dlink-metadata-base/src/main/java/com/dlink/metadata/driver/Driver.java
Patch:
@@ -157,6 +157,8 @@ static Driver build(String connector, String url, String username, String passwo
 
     String getCreateTableSql(Table table);
 
+    String getSqlSelect(Table table);
+
     String getDropTableSql(Table table);
 
     String getTruncateTableSql(Table table);

File: dlink-admin/src/main/java/com/dlink/service/impl/TaskServiceImpl.java
Patch:
@@ -1287,7 +1287,7 @@ public void handleJobDone(JobInstance jobInstance) {
                 String exceptionUrl = "http://" + jobManagerHost + "/#/job/" + jobInstance.getJid() + "/exceptions";
 
                 for (AlertInstance alertInstance : alertGroup.getInstances()) {
-                    if (alertInstance == null) {
+                    if (alertInstance == null || (Asserts.isNotNull(alertInstance.getEnabled()) && !alertInstance.getEnabled())) {
                         continue;
                     }
                     Map<String, String> map = JSONUtil.toMap(alertInstance.getParams());

File: dlink-alert/dlink-alert-feishu/src/main/java/com.dlink.alert.feishu/FeiShuSender.java
Patch:
@@ -76,7 +76,7 @@ public final class FeiShuSender {
     FeiShuSender(Map<String, String> config) {
         url = config.get(FeiShuConstants.WEB_HOOK);
         msgType = config.get(FeiShuConstants.MSG_TYPE);
-        keyword = config.get(FeiShuConstants.KEY_WORD).replace("\r\n", "");
+        keyword = config.get(FeiShuConstants.KEY_WORD) != null ? config.get(FeiShuConstants.KEY_WORD).replace("\r\n", "") : "";
         enableProxy = Boolean.valueOf(config.get(FeiShuConstants.FEI_SHU_PROXY_ENABLE));
         secret = config.get(FeiShuConstants.SECRET);
         if (Boolean.TRUE.equals(enableProxy)) {

File: dlink-admin/src/main/java/com/dlink/controller/UDFController.java
Patch:
@@ -119,3 +119,4 @@ public Result deleteMul(@RequestBody JsonNode para) {
         }
     }
 }
+

File: dlink-app/dlink-app-1.11/src/main/java/com/dlink/app/MainApp.java
Patch:
@@ -41,6 +41,6 @@ public static void main(String[] args) throws IOException {
         String id = params.get(FlinkParamConstant.ID);
         Asserts.checkNullString(id, "请配置入参 id ");
         DBConfig dbConfig = DBConfig.build(params);
-        Submiter.submit(Integer.valueOf(id), dbConfig);
+        Submiter.submit(Integer.valueOf(id), dbConfig,params.get(FlinkParamConstant.DINKY_ADDR));
     }
 }

File: dlink-app/dlink-app-1.12/src/main/java/com/dlink/app/MainApp.java
Patch:
@@ -41,6 +41,6 @@ public static void main(String[] args) throws IOException {
         String id = params.get(FlinkParamConstant.ID);
         Asserts.checkNullString(id, "请配置入参 id ");
         DBConfig dbConfig = DBConfig.build(params);
-        Submiter.submit(Integer.valueOf(id), dbConfig);
+        Submiter.submit(Integer.valueOf(id), dbConfig,params.get(FlinkParamConstant.DINKY_ADDR));
     }
 }

File: dlink-app/dlink-app-1.13/src/main/java/com/dlink/app/MainApp.java
Patch:
@@ -41,6 +41,6 @@ public static void main(String[] args) throws IOException {
         String id = params.get(FlinkParamConstant.ID);
         Asserts.checkNullString(id, "请配置入参 id ");
         DBConfig dbConfig = DBConfig.build(params);
-        Submiter.submit(Integer.valueOf(id), dbConfig);
+        Submiter.submit(Integer.valueOf(id), dbConfig,params.get(FlinkParamConstant.DINKY_ADDR));
     }
 }

File: dlink-app/dlink-app-1.14/src/main/java/com/dlink/app/MainApp.java
Patch:
@@ -41,6 +41,6 @@ public static void main(String[] args) throws IOException {
         String id = params.get(FlinkParamConstant.ID);
         Asserts.checkNullString(id, "请配置入参 id ");
         DBConfig dbConfig = DBConfig.build(params);
-        Submiter.submit(Integer.valueOf(id), dbConfig);
+        Submiter.submit(Integer.valueOf(id), dbConfig, params.get(FlinkParamConstant.DINKY_ADDR));
     }
 }

File: dlink-app/dlink-app-1.15/src/main/java/com/dlink/app/MainApp.java
Patch:
@@ -41,6 +41,6 @@ public static void main(String[] args) throws IOException {
         String id = params.get(FlinkParamConstant.ID);
         Asserts.checkNullString(id, "请配置入参 id ");
         DBConfig dbConfig = DBConfig.build(params);
-        Submiter.submit(Integer.valueOf(id), dbConfig);
+        Submiter.submit(Integer.valueOf(id), dbConfig,params.get(FlinkParamConstant.DINKY_ADDR));
     }
 }

File: dlink-app/dlink-app-1.16/src/main/java/com/dlink/app/MainApp.java
Patch:
@@ -41,6 +41,6 @@ public static void main(String[] args) throws IOException {
         String id = params.get(FlinkParamConstant.ID);
         Asserts.checkNullString(id, "请配置入参 id ");
         DBConfig dbConfig = DBConfig.build(params);
-        Submiter.submit(Integer.valueOf(id), dbConfig);
+        Submiter.submit(Integer.valueOf(id), dbConfig,params.get(FlinkParamConstant.DINKY_ADDR));
     }
 }

File: dlink-client/dlink-client-base/src/main/java/com/dlink/constant/FlinkParamConstant.java
Patch:
@@ -31,6 +31,7 @@ public final class FlinkParamConstant {
     public static final String URL = "url";
     public static final String USERNAME = "username";
     public static final String PASSWORD = "password";
+    public static final String DINKY_ADDR = "dinkyAddr";
 
     public static final String SPLIT = ",";
 }

File: dlink-client/dlink-client-base/src/main/java/com/dlink/utils/FlinkBaseUtil.java
Patch:
@@ -49,6 +49,7 @@ public static Map<String, String> getParamsFromArgs(String[] args) {
         params.put(FlinkParamConstant.URL, parameters.get(FlinkParamConstant.URL, null));
         params.put(FlinkParamConstant.USERNAME, parameters.get(FlinkParamConstant.USERNAME, null));
         params.put(FlinkParamConstant.PASSWORD, parameters.get(FlinkParamConstant.PASSWORD, null));
+        params.put(FlinkParamConstant.DINKY_ADDR, parameters.get(FlinkParamConstant.DINKY_ADDR, null));
         return params;
     }
 

File: dlink-core/src/main/java/com/dlink/job/JobManager.java
Patch:
@@ -260,8 +260,6 @@ private void initUDF() {
         if (CollUtil.isEmpty(udfList)) {
             createExecutorWithSession();
             return;
-        } else if (runMode == GatewayType.KUBERNETES_APPLICATION) {
-            throw new RuntimeException("UDF 暂不支持k8s模式");
         }
         String[] jarPaths = UDFUtil.initJavaUDF(udfList, runMode, config.getTaskId());
 

File: dlink-client/dlink-client-1.11/src/main/java/com/dlink/cdc/AbstractCDCBuilder.java
Patch:
@@ -54,7 +54,7 @@ public void setConfig(FlinkCDCConfig config) {
 
     public List<String> getSchemaList() {
         List<String> schemaList = new ArrayList<>();
-        String schema = config.getSchema();
+        String schema = getSchema();
         if (Asserts.isNotNullString(schema)) {
             String[] schemas = schema.split(FlinkParamConstant.SPLIT);
             Collections.addAll(schemaList, schemas);
@@ -86,4 +86,6 @@ public List<String> getTableList() {
     public String getSchemaFieldName() {
         return "schema";
     }
+
+    public abstract String getSchema();
 }

File: dlink-client/dlink-client-1.12/src/main/java/com/dlink/cdc/AbstractCDCBuilder.java
Patch:
@@ -54,7 +54,7 @@ public void setConfig(FlinkCDCConfig config) {
 
     public List<String> getSchemaList() {
         List<String> schemaList = new ArrayList<>();
-        String schema = config.getSchema();
+        String schema = getSchema();
         if (Asserts.isNotNullString(schema)) {
             String[] schemas = schema.split(FlinkParamConstant.SPLIT);
             Collections.addAll(schemaList, schemas);
@@ -86,4 +86,6 @@ public List<String> getTableList() {
     public String getSchemaFieldName() {
         return "schema";
     }
+
+    public abstract String getSchema();
 }

File: dlink-client/dlink-client-1.13/src/main/java/com/dlink/cdc/AbstractCDCBuilder.java
Patch:
@@ -54,7 +54,7 @@ public void setConfig(FlinkCDCConfig config) {
 
     public List<String> getSchemaList() {
         List<String> schemaList = new ArrayList<>();
-        String schema = config.getSchema();
+        String schema = getSchema();
         if (Asserts.isNotNullString(schema)) {
             String[] schemas = schema.split(FlinkParamConstant.SPLIT);
             Collections.addAll(schemaList, schemas);
@@ -86,4 +86,6 @@ public List<String> getTableList() {
     public String getSchemaFieldName() {
         return "schema";
     }
+
+    public abstract String getSchema();
 }

File: dlink-client/dlink-client-1.14/src/main/java/com/dlink/cdc/AbstractCDCBuilder.java
Patch:
@@ -54,7 +54,7 @@ public void setConfig(FlinkCDCConfig config) {
 
     public List<String> getSchemaList() {
         List<String> schemaList = new ArrayList<>();
-        String schema = config.getSchema();
+        String schema = getSchema();
         if (Asserts.isNotNullString(schema)) {
             String[] schemas = schema.split(FlinkParamConstant.SPLIT);
             Collections.addAll(schemaList, schemas);
@@ -86,4 +86,6 @@ public List<String> getTableList() {
     public String getSchemaFieldName() {
         return "schema";
     }
+
+    public abstract String getSchema();
 }

File: dlink-client/dlink-client-1.15/src/main/java/com/dlink/cdc/AbstractCDCBuilder.java
Patch:
@@ -54,7 +54,7 @@ public void setConfig(FlinkCDCConfig config) {
 
     public List<String> getSchemaList() {
         List<String> schemaList = new ArrayList<>();
-        String schema = config.getSchema();
+        String schema = getSchema();
         if (Asserts.isNotNullString(schema)) {
             String[] schemas = schema.split(FlinkParamConstant.SPLIT);
             Collections.addAll(schemaList, schemas);
@@ -86,4 +86,6 @@ public List<String> getTableList() {
     public String getSchemaFieldName() {
         return "schema";
     }
+
+    public abstract String getSchema();
 }

File: dlink-client/dlink-client-1.16/src/main/java/com/dlink/cdc/AbstractCDCBuilder.java
Patch:
@@ -54,7 +54,7 @@ public void setConfig(FlinkCDCConfig config) {
 
     public List<String> getSchemaList() {
         List<String> schemaList = new ArrayList<>();
-        String schema = config.getSchema();
+        String schema = getSchema();
         if (Asserts.isNotNullString(schema)) {
             String[] schemas = schema.split(FlinkParamConstant.SPLIT);
             Collections.addAll(schemaList, schemas);
@@ -86,4 +86,6 @@ public List<String> getTableList() {
     public String getSchemaFieldName() {
         return "schema";
     }
+
+    public abstract String getSchema();
 }

File: dlink-admin/src/main/java/com/dlink/service/DataBaseService.java
Patch:
@@ -62,4 +62,6 @@ public interface DataBaseService extends ISuperService<DataBase> {
     List<String> listEnabledFlinkWith();
 
     String getEnabledFlinkWithSql();
+
+    boolean copyDatabase(DataBase database);
 }

File: dlink-admin/src/main/java/com/dlink/service/impl/TaskServiceImpl.java
Patch:
@@ -755,7 +755,10 @@ private boolean savepointJobInstance(Integer jobInstanceId, String savePointType
 
         if (GatewayType.KUBERNETES_APPLICATION.equalsValue(cluster.getType())) {
             Statement statement = statementService.getById(cluster.getTaskId());
+            Map<String, Object> clusterConfiguration =
+                    clusterConfigurationService.getGatewayConfig(cluster.getClusterConfigurationId());
             Map<String, Object> gatewayConfig = JSONUtil.toMap(statement.getStatement(), String.class, Object.class);
+            gatewayConfig.putAll(clusterConfiguration);
             jobConfig.buildGatewayConfig(gatewayConfig);
             jobConfig.getGatewayConfig().getClusterConfig().setAppId(cluster.getName());
             useGateway = true;

File: dlink-admin/src/main/java/com/dlink/service/impl/TaskServiceImpl.java
Patch:
@@ -871,7 +871,6 @@ else if (Dialect.KUBERNETES_APPLICATION.equalsVal(task.getDialect())
                 config.setSavePointPath(null);
         }
         config.setVariables(fragmentVariableService.listEnabledVariables());
-        config.getGatewayConfig().getAppConfig().setParallelism(task.getParallelism());
         return config;
     }
 

File: dlink-core/src/main/java/com/dlink/job/JobConfig.java
Patch:
@@ -30,6 +30,7 @@
 import com.dlink.gateway.config.SavePointStrategy;
 import com.dlink.session.SessionConfig;
 
+import org.apache.flink.configuration.CoreOptions;
 import org.apache.http.util.TextUtils;
 
 import java.util.ArrayList;
@@ -237,6 +238,7 @@ public void buildGatewayConfig(Map<String, Object> config) {
         if (config.containsKey("flinkConfig")
                 && Asserts.isNotNullMap((Map<String, String>) config.get("flinkConfig"))) {
             gatewayConfig.setFlinkConfig(FlinkConfig.build((Map<String, String>) config.get("flinkConfig")));
+            gatewayConfig.getFlinkConfig().getConfiguration().put(CoreOptions.DEFAULT_PARALLELISM.key(),String.valueOf(parallelism));
         }
         if (config.containsKey("kubernetesConfig")) {
             Map<String, String> kubernetesConfig = (Map<String, String>) config.get("kubernetesConfig");

File: dlink-gateway/src/main/java/com/dlink/gateway/config/AppConfig.java
Patch:
@@ -36,7 +36,6 @@ public class AppConfig {
     private String userJarPath;
     private String[] userJarParas;
     private String userJarMainAppClass;
-    private Integer parallelism;
 
     public AppConfig() {
     }

File: dlink-executor/src/main/java/com/dlink/executor/SqlManager.java
Patch:
@@ -220,7 +220,8 @@ private String replaceVariable(String statement) {
         while (m.find()) {
             String key = m.group(1);
             String value = this.getSqlFragment(key);
-            m.appendReplacement(sb, value == null ? "" : value);
+            m.appendReplacement(sb, "");
+            sb.append(value == null ? "" : value);
         }
         m.appendTail(sb);
         return sb.toString();

File: dlink-admin/src/main/java/com/dlink/Dlink.java
Patch:
@@ -21,6 +21,7 @@
 
 import org.springframework.boot.SpringApplication;
 import org.springframework.boot.autoconfigure.SpringBootApplication;
+import org.springframework.boot.autoconfigure.freemarker.FreeMarkerAutoConfiguration;
 import org.springframework.cache.annotation.EnableCaching;
 import org.springframework.transaction.annotation.EnableTransactionManagement;
 
@@ -31,7 +32,7 @@
  * @since 2021/5/28
  */
 @EnableTransactionManagement
-@SpringBootApplication
+@SpringBootApplication(exclude = FreeMarkerAutoConfiguration.class)
 @EnableCaching
 public class Dlink {
 

File: dlink-gateway/src/main/java/com/dlink/gateway/yarn/YarnGateway.java
Patch:
@@ -126,8 +126,8 @@ private void initConfig() {
                 e.printStackTrace();
             }
         }
-
         if (getType().isApplicationMode()) {
+            configuration.set(YarnConfigOptions.APPLICATION_TYPE,"Dinky Flink");
             String uuid = UUID.randomUUID().toString().replace("-", "");
             if (configuration.contains(CheckpointingOptions.CHECKPOINTS_DIRECTORY)) {
                 configuration.set(CheckpointingOptions.CHECKPOINTS_DIRECTORY,

File: dlink-admin/src/main/java/com/dlink/configure/MybatisPlusConfig.java
Patch:
@@ -62,7 +62,7 @@ public MybatisPlusInterceptor mybatisPlusInterceptor() {
             public Expression getTenantId() {
                 Integer tenantId = (Integer) TenantContextHolder.get();
                 if (tenantId == null) {
-                    log.warn("request context tenant id is null");
+                    // log.warn("request context tenant id is null");
                     return new NullValue();
                 }
                 return new LongValue(tenantId);

File: dlink-admin/src/main/java/com/dlink/service/TaskService.java
Patch:
@@ -103,13 +103,14 @@ public interface TaskService extends ISuperService<Task> {
 
     Result queryAllCatalogue();
 
-    Result<List<Task>> queryOnLineTaskByDoneStatus(List<JobLifeCycle> jobLifeCycle
-        , List<JobStatus> jobStatuses, boolean includeNull, Integer catalogueId);
+    Result<List<Task>> queryOnLineTaskByDoneStatus(List<JobLifeCycle> jobLifeCycle, List<JobStatus> jobStatuses,
+                                                   boolean includeNull, Integer catalogueId);
 
     void selectSavepointOnLineTask(TaskOperatingResult taskOperatingResult);
 
     void selectSavepointOffLineTask(TaskOperatingResult taskOperatingResult);
 
     Task getTaskByNameAndTenantId(String name, Integer tenantId);
 
+    JobStatus checkJobStatus(JobInfoDetail jobInfoDetail);
 }

File: dlink-gateway/src/main/java/com/dlink/gateway/Gateway.java
Patch:
@@ -25,6 +25,7 @@
 import com.dlink.gateway.result.GatewayResult;
 import com.dlink.gateway.result.SavePointResult;
 import com.dlink.gateway.result.TestResult;
+import com.dlink.model.JobStatus;
 
 import org.apache.flink.runtime.jobgraph.JobGraph;
 
@@ -83,4 +84,5 @@ static Gateway build(GatewayConfig config) {
 
     TestResult test();
 
+    JobStatus getJobStatusById(String id);
 }

File: dlink-client/dlink-client-1.13/src/main/java/org/apache/flink/shaded/guava30/com/google/common/util/concurrent/ThreadFactoryBuilder.java
Patch:
@@ -19,7 +19,7 @@
 
 package org.apache.flink.shaded.guava30.com.google.common.util.concurrent;
 
-import org.apache.flink.shaded.guava30.com.google.common.base.Preconditions;
+import org.apache.flink.shaded.guava18.com.google.common.base.Preconditions;
 
 import java.util.concurrent.Executors;
 import java.util.concurrent.ThreadFactory;

File: dlink-admin/src/main/java/com/dlink/service/impl/UserServiceImpl.java
Patch:
@@ -220,6 +220,9 @@ public Result grantRole(JsonNode para) {
             if (result) {
                 return Result.succeed("用户授权角色成功");
             } else {
+                if (userRoleList.size() == 0) {
+                    return Result.succeed("该用户绑定的角色已被全部删除");
+                }
                 return Result.failed("用户授权角色失败");
             }
         } else {

File: dlink-admin/src/main/java/com/dlink/exception/WebExceptionHandler.java
Patch:
@@ -68,7 +68,7 @@ public Result notLoginException(NotLoginException e) {
             (ServletRequestAttributes) RequestContextHolder.getRequestAttributes();
         HttpServletResponse response = servletRequestAttributes.getResponse();
         response.setStatus(CodeEnum.NOTLOGIN.getCode());
-        return Result.notLogin(MessageResolverUtils.getMessage("login.not-login"));
+        return Result.notLogin(MessageResolverUtils.getMessage("login.not.login"));
     }
 
     @ResponseStatus(HttpStatus.BAD_REQUEST) // 设置状态码为 400

File: dlink-admin/src/main/java/com/dlink/service/impl/UserServiceImpl.java
Patch:
@@ -242,7 +242,7 @@ public Result getTenants(String username) {
         Set<Integer> tenantIds = new HashSet<>();
         userTenants.forEach(userTenant -> tenantIds.add(userTenant.getTenantId()));
         List<Tenant> tenants = tenantService.getTenantByIds(tenantIds);
-        return Result.succeed(tenants, MessageResolverUtils.getMessage("response.get.successfully"));
+        return Result.succeed(tenants, MessageResolverUtils.getMessage("response.get.success"));
     }
 
 }

File: dlink-app/dlink-app-1.16/src/main/java/com/dlink/app/MainApp.java
Patch:
@@ -19,15 +19,15 @@
 
 package com.dlink.app;
 
-import java.io.IOException;
-import java.util.Map;
-
 import com.dlink.app.db.DBConfig;
 import com.dlink.app.flinksql.Submiter;
 import com.dlink.assertion.Asserts;
 import com.dlink.constant.FlinkParamConstant;
 import com.dlink.utils.FlinkBaseUtil;
 
+import java.io.IOException;
+import java.util.Map;
+
 /**
  * MainApp
  *

File: dlink-catalog/dlink-catalog-mysql/dlink-catalog-mysql-1.16/src/main/java/com/dlink/flink/catalog/factory/DlinkMysqlCatalogFactoryOptions.java
Patch:
@@ -19,12 +19,12 @@
 
 package com.dlink.flink.catalog.factory;
 
+import com.dlink.flink.catalog.DlinkMysqlCatalog;
+
 import org.apache.flink.annotation.Internal;
 import org.apache.flink.configuration.ConfigOption;
 import org.apache.flink.configuration.ConfigOptions;
 
-import com.dlink.flink.catalog.DlinkMysqlCatalog;
-
 /**
  * {@link ConfigOption}s for {@link DlinkMysqlCatalog}.
  */

File: dlink-client/dlink-client-1.11/src/main/java/com/dlink/cdc/SinkBuilderFactory.java
Patch:
@@ -22,6 +22,7 @@
 import com.dlink.assertion.Asserts;
 import com.dlink.cdc.doris.DorisSinkBuilder;
 import com.dlink.cdc.kafka.KafkaSinkBuilder;
+import com.dlink.cdc.sql.SQLSinkBuilder;
 import com.dlink.exception.FlinkClientException;
 import com.dlink.model.FlinkCDCConfig;
 
@@ -47,6 +48,6 @@ public static SinkBuilder buildSinkBuilder(FlinkCDCConfig config) {
                 return sinkBuilders[i].create(config);
             }
         }
-        throw new FlinkClientException("未匹配到对应 Sink 类型的【" + config.getSink().get("connector") + "】。");
+        return new SQLSinkBuilder().create(config);
     }
 }

File: dlink-client/dlink-client-1.12/src/main/java/com/dlink/cdc/doris/DorisSinkBuilder.java
Patch:
@@ -45,7 +45,7 @@
  * @author wenmo
  * @since 2022/4/20 19:20
  **/
-public class DorisSinkBuilder extends AbstractSinkBuilder implements SinkBuilder, Serializable {
+public class DorisSinkBuilder extends AbstractSinkBuilder implements Serializable {
 
     private static final String KEY_WORD = "datastream-doris";
     private static final long serialVersionUID = 8330362249137471854L;

File: dlink-client/dlink-client-1.12/src/main/java/com/dlink/cdc/mysql/MysqlCDCBuilder.java
Patch:
@@ -38,7 +38,6 @@
 
 import com.alibaba.ververica.cdc.connectors.mysql.MySQLSource;
 import com.alibaba.ververica.cdc.connectors.mysql.table.StartupOptions;
-import com.alibaba.ververica.cdc.debezium.StringDebeziumDeserializationSchema;
 
 /**
  * MysqlCDCBuilder
@@ -94,7 +93,7 @@ public DataStreamSource<String> build(StreamExecutionEnvironment env) {
         } else {
             sourceBuilder.tableList(new String[0]);
         }
-        sourceBuilder.deserializer(new StringDebeziumDeserializationSchema());
+        sourceBuilder.deserializer(new MysqlJsonDebeziumDeserializationSchema());
         sourceBuilder.debeziumProperties(properties);
         if (Asserts.isNotNullString(config.getStartupMode())) {
             switch (config.getStartupMode().toLowerCase()) {

File: dlink-client/dlink-client-1.13/src/main/java/com/dlink/cdc/oracle/OracleCDCBuilder.java
Patch:
@@ -34,8 +34,8 @@
 import java.util.Map;
 import java.util.Properties;
 
+import com.ververica.cdc.connectors.base.options.StartupOptions;
 import com.ververica.cdc.connectors.oracle.OracleSource;
-import com.ververica.cdc.connectors.oracle.table.StartupOptions;
 import com.ververica.cdc.debezium.JsonDebeziumDeserializationSchema;
 
 /**

File: dlink-client/dlink-client-1.14/src/main/java/com/dlink/cdc/oracle/OracleCDCBuilder.java
Patch:
@@ -34,8 +34,8 @@
 import java.util.Map;
 import java.util.Properties;
 
+import com.ververica.cdc.connectors.base.options.StartupOptions;
 import com.ververica.cdc.connectors.oracle.OracleSource;
-import com.ververica.cdc.connectors.oracle.table.StartupOptions;
 import com.ververica.cdc.debezium.JsonDebeziumDeserializationSchema;
 
 /**

File: dlink-client/dlink-client-1.15/src/main/java/com/dlink/cdc/oracle/OracleCDCBuilder.java
Patch:
@@ -34,8 +34,8 @@
 import java.util.Map;
 import java.util.Properties;
 
+import com.ververica.cdc.connectors.base.options.StartupOptions;
 import com.ververica.cdc.connectors.oracle.OracleSource;
-import com.ververica.cdc.connectors.oracle.table.StartupOptions;
 import com.ververica.cdc.debezium.JsonDebeziumDeserializationSchema;
 
 /**

File: dlink-client/dlink-client-1.16/src/main/java/com/dlink/cdc/CDCBuilder.java
Patch:
@@ -21,6 +21,7 @@
 
 import com.dlink.exception.SplitTableException;
 import com.dlink.model.FlinkCDCConfig;
+
 import org.apache.flink.streaming.api.datastream.DataStreamSource;
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
 

File: dlink-client/dlink-client-1.16/src/main/java/com/dlink/cdc/CDCBuilderFactory.java
Patch:
@@ -20,6 +20,7 @@
 package com.dlink.cdc;
 
 import com.dlink.assertion.Asserts;
+import com.dlink.cdc.mysql.MysqlCDCBuilder;
 import com.dlink.exception.FlinkClientException;
 import com.dlink.model.FlinkCDCConfig;
 
@@ -32,6 +33,7 @@
 public class CDCBuilderFactory {
 
     private static CDCBuilder[] cdcBuilders = {
+        new MysqlCDCBuilder()
     };
 
     public static CDCBuilder buildCDCBuilder(FlinkCDCConfig config) {

File: dlink-client/dlink-client-1.16/src/main/java/com/dlink/cdc/SinkBuilder.java
Patch:
@@ -22,6 +22,7 @@
 import com.dlink.executor.CustomTableEnvironment;
 import com.dlink.model.FlinkCDCConfig;
 import com.dlink.model.Table;
+
 import org.apache.flink.streaming.api.datastream.DataStreamSource;
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
 
@@ -37,7 +38,8 @@ public interface SinkBuilder {
 
     SinkBuilder create(FlinkCDCConfig config);
 
-    DataStreamSource build(CDCBuilder cdcBuilder, StreamExecutionEnvironment env, CustomTableEnvironment customTableEnvironment, DataStreamSource<String> dataStreamSource);
+    DataStreamSource build(CDCBuilder cdcBuilder, StreamExecutionEnvironment env,
+                           CustomTableEnvironment customTableEnvironment, DataStreamSource<String> dataStreamSource);
 
     String getSinkSchemaName(Table table);
 

File: dlink-client/dlink-client-1.16/src/main/java/com/dlink/cdc/SinkBuilderFactory.java
Patch:
@@ -20,6 +20,7 @@
 package com.dlink.cdc;
 
 import com.dlink.assertion.Asserts;
+import com.dlink.cdc.sql.SQLSinkBuilder;
 import com.dlink.exception.FlinkClientException;
 import com.dlink.model.FlinkCDCConfig;
 
@@ -44,6 +45,6 @@ public static SinkBuilder buildSinkBuilder(FlinkCDCConfig config) {
                 return sinkBuilders[i].create(config);
             }
         }
-        throw new FlinkClientException("未匹配到对应 Sink 类型的【" + config.getType() + "】。");
+        return new SQLSinkBuilder().create(config);
     }
 }

File: dlink-connectors/dlink-connector-jdbc-1.12/src/main/java/org/apache/flink/connector/jdbc/dialect/OracleDialect.java
Patch:
@@ -24,9 +24,7 @@
 import org.apache.flink.table.types.logical.LogicalTypeRoot;
 import org.apache.flink.table.types.logical.RowType;
 
-import java.util.ArrayList;
 import java.util.Arrays;
-import java.util.Collections;
 import java.util.List;
 import java.util.Optional;
 import java.util.Set;

File: dlink-client/dlink-client-1.14/src/main/java/com/dlink/cdc/kafka/KafkaSinkBuilder.java
Patch:
@@ -104,6 +104,7 @@ public DataStreamSource build(
                     )
                     .setDeliverGuarantee(DeliveryGuarantee.valueOf(env.getCheckpointingMode().name()))
                     .setKafkaProducerConfig(kafkaProducerConfig)
+                    .setTransactionalIdPrefix(kafkaProducerConfig.getProperty("transactional.id"))
                     .build();
             dataStreamSource.sinkTo(kafkaSink);
         } else {
@@ -149,6 +150,7 @@ public void processElement(Map map, ProcessFunction<Map, String>.Context ctx, Co
                             )
                             .setDeliverGuarantee(DeliveryGuarantee.valueOf(env.getCheckpointingMode().name()))
                             .setKafkaProducerConfig(kafkaProducerConfig)
+                            .setTransactionalIdPrefix(kafkaProducerConfig.getProperty("transactional.id") + "-" + topic)
                             .build();
                     process.getSideOutput(v).rebalance().sinkTo(kafkaSink).name(topic);
                 });

File: dlink-client/dlink-client-1.15/src/main/java/com/dlink/cdc/kafka/KafkaSinkBuilder.java
Patch:
@@ -104,6 +104,7 @@ public DataStreamSource build(
                     )
                     .setDeliverGuarantee(DeliveryGuarantee.valueOf(env.getCheckpointingMode().name()))
                     .setKafkaProducerConfig(kafkaProducerConfig)
+                    .setTransactionalIdPrefix(kafkaProducerConfig.getProperty("transactional.id"))
                     .build();
             dataStreamSource.sinkTo(kafkaSink);
         } else {
@@ -149,6 +150,7 @@ public void processElement(Map map, ProcessFunction<Map, String>.Context ctx, Co
                             )
                             .setDeliverGuarantee(DeliveryGuarantee.valueOf(env.getCheckpointingMode().name()))
                             .setKafkaProducerConfig(kafkaProducerConfig)
+                            .setTransactionalIdPrefix(kafkaProducerConfig.getProperty("transactional.id") + "-" + topic)
                             .build();
                     process.getSideOutput(v).rebalance().sinkTo(kafkaSink).name(topic);
                 });

File: dlink-client/dlink-client-1.15/src/main/java/com/dlink/cdc/SinkBuilder.java
Patch:
@@ -21,8 +21,8 @@
 
 import com.dlink.executor.CustomTableEnvironment;
 import com.dlink.model.FlinkCDCConfig;
-
 import com.dlink.model.Table;
+
 import org.apache.flink.streaming.api.datastream.DataStreamSource;
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
 

File: dlink-common/src/main/java/com/dlink/config/Dialect.java
Patch:
@@ -28,6 +28,7 @@
  * @since 2021/12/13
  **/
 public enum Dialect {
+
     FLINKSQL("FlinkSql"),
     FLINKJAR("FlinkJar"),
     FLINKSQLENV("FlinkSqlEnv"),
@@ -110,4 +111,3 @@ public static boolean isUDF(String value) {
         }
     }
 }
-

File: dlink-function/src/main/java/com/dlink/function/compiler/JVMPackage.java
Patch:
@@ -66,7 +66,7 @@ public String[] pack(List<UDF> udfList, Integer missionId) {
             fileInputStreams[i] = FileUtil.getInputStream(absoluteFilePath);
         }
 
-        String jarPath = PathConstant.getUdfPackagePath(missionId, PathConstant.UDF_JAR_NAME);
+        String jarPath = PathConstant.getUdfPackagePath(missionId) + PathConstant.UDF_JAR_NAME;
         // 编译好的文件打包jar
         File file = FileUtil.file(jarPath);
         FileUtil.del(file);

File: dlink-common/src/main/java/com/dlink/model/Column.java
Patch:
@@ -46,6 +46,7 @@ public class Column implements Serializable {
     private ColumnType javaType;
     private String columnFamily;
     private Integer position;
+    private Integer length;
     private Integer precision;
     private Integer scale;
     private String characterSet;

File: dlink-core/src/main/java/com/dlink/config/Dialect.java
Patch:
@@ -44,6 +44,7 @@ public enum Dialect {
     PHOENIX("Phoenix"),
     HIVE("Hive"),
     STARROCKS("StarRocks"),
+    PRESTO("Presto"),
     KUBERNETES_APPLICATION("KubernetesApplaction");
 
     private String value;
@@ -90,6 +91,7 @@ public static boolean notFlinkSql(String value) {
             case PHOENIX:
             case HIVE:
             case STARROCKS:
+            case PRESTO:
                 return true;
             default:
                 return false;

File: dlink-core/src/main/java/com/dlink/job/JobManager.java
Patch:
@@ -81,6 +81,8 @@
 
 import com.fasterxml.jackson.databind.node.ObjectNode;
 
+import cn.hutool.core.util.ArrayUtil;
+
 /**
  * JobManager
  *
@@ -164,7 +166,7 @@ public static JobManager build(JobConfig config) {
         manager.executor.initPyUDF(config.getPyFiles());
 
         if (config.getGatewayConfig() != null) {
-            config.getGatewayConfig().setJarPaths(config.getJarFiles());
+            config.getGatewayConfig().setJarPaths(ArrayUtil.append(config.getJarFiles(),config.getPyFiles()));
         }
         return manager;
     }

File: dlink-admin/src/main/java/com/dlink/init/SystemInit.java
Patch:
@@ -20,7 +20,6 @@
 package com.dlink.init;
 
 import com.dlink.assertion.Asserts;
-import com.dlink.context.RequestContext;
 import com.dlink.daemon.task.DaemonFactory;
 import com.dlink.daemon.task.DaemonTaskConfig;
 import com.dlink.job.FlinkJobTask;
@@ -76,7 +75,6 @@ public void run(ApplicationArguments args) throws Exception {
         List<Tenant> tenants = tenantService.list();
         sysConfigService.initSysConfig();
         for (Tenant tenant : tenants) {
-            RequestContext.set(tenant.getId());
             taskService.initDefaultFlinkSQLEnv(tenant.getId());
         }
         initTaskMonitor();

File: dlink-admin/src/main/java/com/dlink/interceptor/TenantInterceptor.java
Patch:
@@ -19,7 +19,7 @@
 
 package com.dlink.interceptor;
 
-import com.dlink.context.RequestContext;
+import com.dlink.context.TenantContextHolder;
 
 import javax.servlet.http.HttpServletRequest;
 import javax.servlet.http.HttpServletResponse;
@@ -41,7 +41,7 @@ public boolean preHandle(HttpServletRequest request, HttpServletResponse respons
                              Object handler) throws Exception {
         String tenantId = request.getHeader("tenantId");
         if (!StringUtils.isNullOrEmpty(tenantId)) {
-            RequestContext.set(Integer.valueOf(tenantId));
+            TenantContextHolder.set(Integer.valueOf(tenantId));
         }
         return HandlerInterceptor.super.preHandle(request, response, handler);
     }

File: dlink-admin/src/main/java/com/dlink/job/FlinkJobTask.java
Patch:
@@ -74,7 +74,7 @@ public void dealTask() {
         preDealTime = System.currentTimeMillis();
         JobInstance jobInstance = taskService.refreshJobInstance(config.getId(), false);
         if ((!JobStatus.isDone(jobInstance.getStatus())) || (Asserts.isNotNull(jobInstance.getFinishTime())
-            && Duration.between(jobInstance.getFinishTime(), LocalDateTime.now()).toMinutes() < 1)) {
+                && Duration.between(jobInstance.getFinishTime(), LocalDateTime.now()).toMinutes() < 1)) {
             DefaultThreadPool.getInstance().execute(this);
         } else {
             taskService.handleJobDone(jobInstance);

File: dlink-admin/src/main/java/com/dlink/service/JobHistoryService.java
Patch:
@@ -30,9 +30,12 @@
  **/
 public interface JobHistoryService extends ISuperService<JobHistory> {
 
+    JobHistory getByIdWithoutTenant(Integer id);
+
     JobHistory getJobHistory(Integer id);
 
     JobHistory getJobHistoryInfo(JobHistory jobHistory);
 
     JobHistory refreshJobHistory(Integer id, String jobManagerHost, String jobId, boolean needSave);
+
 }

File: dlink-admin/src/main/java/com/dlink/service/JobInstanceService.java
Patch:
@@ -38,6 +38,8 @@
  */
 public interface JobInstanceService extends ISuperService<JobInstance> {
 
+    JobInstance getByIdWithoutTenant(Integer id);
+
     JobInstanceStatus getStatusCount(boolean isHistory);
 
     List<JobInstance> listJobInstanceActive();

File: dlink-process/src/main/java/com/dlink/process/pool/ConsolePool.java
Patch:
@@ -55,7 +55,8 @@ public static void write(String str, Integer userId) {
         if (consoleEntityMap.containsKey(user)) {
             consoleEntityMap.get(user).append(str);
         } else {
-            consoleEntityMap.put(user, new StringBuilder(str));
+            StringBuilder sb = new StringBuilder("Dinky User Console:");
+            consoleEntityMap.put(user, sb.append(str));
         }
     }
 

File: dlink-executor/src/main/java/com/dlink/interceptor/FlinkInterceptor.java
Patch:
@@ -34,6 +34,8 @@
  * @since 2021/6/11 22:17
  */
 public class FlinkInterceptor {
+    private FlinkInterceptor() {
+    }
 
     public static String pretreatStatement(Executor executor, String statement) {
         statement = SqlUtil.removeNote(statement);

File: dlink-executor/src/test/java/com/dlink/interceptor/FlinkInterceptorTest.java
Patch:
@@ -17,10 +17,9 @@
  *
  */
 
-package com.dlink.core;
+package com.dlink.interceptor;
 
 import com.dlink.executor.Executor;
-import com.dlink.interceptor.FlinkInterceptor;
 
 import org.junit.Assert;
 import org.junit.Test;
@@ -39,6 +38,6 @@ public void replaceFragmentTest() {
             + "nullif2:=NULLIF(0, 0) as val$null;"
             + "select ${nullif1},${nullif2}";
         String pretreatStatement = FlinkInterceptor.pretreatStatement(Executor.build(), statement);
-        Assert.assertEquals("select NULLIF(1, 0) as val,NULLIF(0, 0) as val$null",pretreatStatement);
+        Assert.assertEquals("select NULLIF(1, 0) as val,NULLIF(0, 0) as val$null", pretreatStatement);
     }
 }

File: dlink-admin/src/main/java/com/dlink/service/impl/UDFServiceImpl.java
Patch:
@@ -94,12 +94,12 @@ public UDFPath initUDF(String statement, GatewayType gatewayType) {
     }
 
     private static String[] initPythonUDF(List<UDF> udfList) {
-        return (udfList.size() > 0) ? new String[] {UDFUtil.buildPy(udfList)} : new String[] {};
+        return udfList == null || udfList.isEmpty() ? new String[0] : new String[] {UDFUtil.buildPy(udfList)};
     }
 
     private static String[] initJavaUDF(List<UDF> udfList) {
         Opt<String> udfJarPath = Opt.empty();
-        if (udfList.size() > 0) {
+        if (!udfList.isEmpty()) {
             udfJarPath = Opt.ofBlankAble(UDFUtil.getUdfFileAndBuildJar(udfList));
         }
 

File: dlink-client/dlink-client-1.11/src/main/java/com/dlink/cdc/AbstractSinkBuilder.java
Patch:
@@ -103,8 +103,8 @@ protected Properties getProperties() {
         Properties properties = new Properties();
         Map<String, String> sink = config.getSink();
         for (Map.Entry<String, String> entry : sink.entrySet()) {
-            if (Asserts.isNotNullString(entry.getKey()) && entry.getKey().startsWith("sink.properties") && Asserts.isNotNullString(entry.getValue())) {
-                properties.setProperty(entry.getKey().replace("sink.properties.",""), entry.getValue());
+            if (Asserts.isNotNullString(entry.getKey()) && entry.getKey().startsWith("properties") && Asserts.isNotNullString(entry.getValue())) {
+                properties.setProperty(entry.getKey().replace("properties.",""), entry.getValue());
             }
         }
         return properties;

File: dlink-client/dlink-client-1.12/src/main/java/com/dlink/cdc/AbstractSinkBuilder.java
Patch:
@@ -103,8 +103,8 @@ protected Properties getProperties() {
         Properties properties = new Properties();
         Map<String, String> sink = config.getSink();
         for (Map.Entry<String, String> entry : sink.entrySet()) {
-            if (Asserts.isNotNullString(entry.getKey()) && entry.getKey().startsWith("sink.properties") && Asserts.isNotNullString(entry.getValue())) {
-                properties.setProperty(entry.getKey().replace("sink.properties.",""), entry.getValue());
+            if (Asserts.isNotNullString(entry.getKey()) && entry.getKey().startsWith("properties") && Asserts.isNotNullString(entry.getValue())) {
+                properties.setProperty(entry.getKey().replace("properties.",""), entry.getValue());
             }
         }
         return properties;

File: dlink-client/dlink-client-1.13/src/main/java/com/dlink/cdc/AbstractSinkBuilder.java
Patch:
@@ -103,8 +103,8 @@ protected Properties getProperties() {
         Properties properties = new Properties();
         Map<String, String> sink = config.getSink();
         for (Map.Entry<String, String> entry : sink.entrySet()) {
-            if (Asserts.isNotNullString(entry.getKey()) && entry.getKey().startsWith("sink.properties") && Asserts.isNotNullString(entry.getValue())) {
-                properties.setProperty(entry.getKey().replace("sink.properties.",""), entry.getValue());
+            if (Asserts.isNotNullString(entry.getKey()) && entry.getKey().startsWith("properties") && Asserts.isNotNullString(entry.getValue())) {
+                properties.setProperty(entry.getKey().replace("properties.",""), entry.getValue());
             }
         }
         return properties;

File: dlink-client/dlink-client-1.14/src/main/java/com/dlink/cdc/AbstractSinkBuilder.java
Patch:
@@ -105,8 +105,8 @@ protected Properties getProperties() {
         Properties properties = new Properties();
         Map<String, String> sink = config.getSink();
         for (Map.Entry<String, String> entry : sink.entrySet()) {
-            if (Asserts.isNotNullString(entry.getKey()) && entry.getKey().startsWith("sink.properties") && Asserts.isNotNullString(entry.getValue())) {
-                properties.setProperty(entry.getKey().replace("sink.properties.", ""), entry.getValue());
+            if (Asserts.isNotNullString(entry.getKey()) && entry.getKey().startsWith("properties") && Asserts.isNotNullString(entry.getValue())) {
+                properties.setProperty(entry.getKey().replace("properties.", ""), entry.getValue());
             }
         }
         return properties;

File: dlink-client/dlink-client-1.15/src/main/java/com/dlink/cdc/AbstractSinkBuilder.java
Patch:
@@ -103,8 +103,8 @@ protected Properties getProperties() {
         Properties properties = new Properties();
         Map<String, String> sink = config.getSink();
         for (Map.Entry<String, String> entry : sink.entrySet()) {
-            if (Asserts.isNotNullString(entry.getKey()) && entry.getKey().startsWith("sink.properties") && Asserts.isNotNullString(entry.getValue())) {
-                properties.setProperty(entry.getKey().replace("sink.properties.",""), entry.getValue());
+            if (Asserts.isNotNullString(entry.getKey()) && entry.getKey().startsWith("properties") && Asserts.isNotNullString(entry.getValue())) {
+                properties.setProperty(entry.getKey().replace("properties.",""), entry.getValue());
             }
         }
         return properties;

File: dlink-common/src/main/java/com/dlink/utils/SqlUtil.java
Patch:
@@ -41,7 +41,7 @@ public static String[] getStatements(String sql, String sqlSeparator) {
             return new String[0];
         }
 
-        String[] splits = sql.replaceAll(";\r\n", ";\n").split(sqlSeparator);
+        String[] splits = sql.replace(";\r\n", ";\n").split(sqlSeparator);
         String lastStatement = splits[splits.length - 1].trim();
         if (lastStatement.endsWith(SEMICOLON)) {
             splits[splits.length - 1] = lastStatement.substring(0, lastStatement.length() - 1);

File: dlink-metadata/dlink-metadata-postgresql/src/main/java/com/dlink/metadata/query/PostgreSqlQuery.java
Patch:
@@ -28,7 +28,7 @@
 public class PostgreSqlQuery extends AbstractDBQuery {
     @Override
     public String schemaAllSql() {
-        return "SELECT \"schema_name\" FROM information_schema.schemata where \"schema_name\" not in ('pg_toast','pg_temp_1','pg_toast_temp_1','pg_catalog','information_schema')";
+        return "SELECT nspname AS \"schema_name\" FROM pg_namespace WHERE nspname NOT LIKE 'pg_%' AND nspname != 'information_schema'";
     }
 
     @Override

File: dlink-admin/src/main/java/com/dlink/service/impl/StudioServiceImpl.java
Patch:
@@ -259,7 +259,7 @@ private List<SqlExplainResult> explainFlinkSql(StudioExecuteDTO studioExecuteDTO
         buildSession(config);
         process.infoSuccess();
         // To initialize java udf, but it has a bug in the product environment now.
-        // initUDF(config,studioExecuteDTO.getStatement());
+        config.setJarFiles(udfService.initUDF(studioExecuteDTO.getStatement()));
         process.start();
         JobManager jobManager = JobManager.buildPlanMode(config);
         List<SqlExplainResult> sqlExplainResults =
@@ -311,6 +311,7 @@ public ObjectNode getJobPlan(StudioExecuteDTO studioExecuteDTO) {
         // If you are using explainSql | getStreamGraph | getJobPlan, make the dialect change to local.
         config.buildLocal();
         buildSession(config);
+        config.setJarFiles(udfService.initUDF(studioExecuteDTO.getStatement()));
         JobManager jobManager = JobManager.buildPlanMode(config);
         String planJson = jobManager.getJobPlanJson(studioExecuteDTO.getStatement());
         ObjectMapper mapper = new ObjectMapper();

File: dlink-admin/src/main/java/com/dlink/controller/ClusterConfigurationController.java
Patch:
@@ -60,12 +60,13 @@ public class ClusterConfigurationController {
      */
     @PutMapping
     public Result saveOrUpdate(@RequestBody ClusterConfiguration clusterConfiguration) {
+        Integer id = clusterConfiguration.getId();
         TestResult testResult = clusterConfigurationService.testGateway(clusterConfiguration);
         clusterConfiguration.setIsAvailable(testResult.isAvailable());
         if (clusterConfigurationService.saveOrUpdate(clusterConfiguration)) {
-            return Result.succeed(Asserts.isNotNull(clusterConfiguration.getId()) ? "修改成功" : "新增成功");
+            return Result.succeed(Asserts.isNotNull(id) ? "修改成功" : "新增成功");
         } else {
-            return Result.failed(Asserts.isNotNull(clusterConfiguration.getId()) ? "修改失败" : "新增失败");
+            return Result.failed(Asserts.isNotNull(id) ? "修改失败" : "新增失败");
         }
     }
 

File: dlink-admin/src/main/java/com/dlink/controller/ClusterController.java
Patch:
@@ -60,8 +60,9 @@ public class ClusterController {
     @PutMapping
     public Result saveOrUpdate(@RequestBody Cluster cluster) throws Exception {
         cluster.setAutoRegisters(false);
+        Integer id = cluster.getId();
         clusterService.registersCluster(cluster);
-        return Result.succeed(Asserts.isNotNull(cluster.getId()) ? "修改成功" : "新增成功");
+        return Result.succeed(Asserts.isNotNull(id) ? "修改成功" : "新增成功");
     }
 
     /**

File: dlink-admin/src/main/java/com/dlink/controller/JarController.java
Patch:
@@ -130,7 +130,7 @@ public Result<Map<String, List<String>>> generateJar() {
         List<String> udfCodes = allUDF.stream().map(Task::getStatement).collect(Collectors.toList());
         Map<String, List<String>> resultMap = UDFUtil.buildJar(udfCodes);
         String msg = StrUtil.format("udf jar生成成功，jar文件在{}；\n本次成功 class:{}。\n失败 class:{}"
-                , PathConstant.UDF_JAR_TMP_PATH, resultMap.get("success"), resultMap.get("failed"));
+            , PathConstant.UDF_JAR_TMP_PATH, resultMap.get("success"), resultMap.get("failed"));
         return Result.succeed(resultMap, msg);
     }
 }

File: dlink-admin/src/main/java/com/dlink/dto/CatalogueTaskDTO.java
Patch:
@@ -34,6 +34,7 @@
 @Setter
 public class CatalogueTaskDTO {
     private Integer id;
+    private Integer tenantId;
     private Integer parentId;
     private boolean isLeaf;
     private String name;

File: dlink-admin/src/main/java/com/dlink/dto/LoginUTO.java
Patch:
@@ -33,5 +33,6 @@
 public class LoginUTO {
     private String username;
     private String password;
+    private Integer tenantId;
     private boolean autoLogin;
 }

File: dlink-admin/src/main/java/com/dlink/exception/WebExceptionHandler.java
Patch:
@@ -69,7 +69,7 @@ public Result notLoginException(NotLoginException e) {
         return Result.notLogin("该用户未登录!");
     }
 
-    @ResponseStatus(HttpStatus.BAD_REQUEST) //设置状态码为 400
+    @ResponseStatus(HttpStatus.BAD_REQUEST) // 设置状态码为 400
     @ExceptionHandler({MethodArgumentNotValidException.class})
     public Result<String> paramExceptionHandler(MethodArgumentNotValidException e) {
         BindingResult exceptions = e.getBindingResult();
@@ -93,5 +93,4 @@ public Result unknownException(Exception e) {
         logger.error("ERROR:", e);
         return Result.failed(e.getMessage());
     }
-
 }

File: dlink-admin/src/main/java/com/dlink/mapper/StatementMapper.java
Patch:
@@ -33,6 +33,4 @@
 @Mapper
 public interface StatementMapper extends SuperMapper<Statement> {
 
-    int insert(Statement statement);
-
 }

File: dlink-admin/src/main/java/com/dlink/mapper/TaskMapper.java
Patch:
@@ -41,4 +41,6 @@ public interface TaskMapper extends SuperMapper<Task> {
     List<Task> queryOnLineTaskByDoneStatus(@Param("parentIds") List<Integer> parentIds
             , @Param("stepIds") List<Integer> stepIds, @Param("includeNull") boolean includeNull
             , @Param("jobStatuses") List<String> jobStatuses);
+
+    Task getTaskByNameAndTenantId(@Param("name") String name, @Param("tenantId") Integer tenantId);
 }

File: dlink-admin/src/main/java/com/dlink/model/AlertGroup.java
Patch:
@@ -42,6 +42,8 @@ public class AlertGroup extends SuperEntity {
 
     private static final long serialVersionUID = 7027411164191682344L;
 
+    private Integer tenantId;
+
     private String alertInstanceIds;
 
     private String note;

File: dlink-admin/src/main/java/com/dlink/model/AlertHistory.java
Patch:
@@ -47,6 +47,8 @@ public class AlertHistory implements Serializable {
     @TableId(value = "id", type = IdType.AUTO)
     private Integer id;
 
+    private Integer tenantId;
+
     private Integer alertGroupId;
 
     private Integer jobInstanceId;

File: dlink-admin/src/main/java/com/dlink/model/AlertInstance.java
Patch:
@@ -38,6 +38,8 @@
 public class AlertInstance extends SuperEntity {
     private static final long serialVersionUID = -3435401513220527001L;
 
+    private Integer tenantId;
+
     private String type;
 
     private String params;

File: dlink-admin/src/main/java/com/dlink/model/Catalogue.java
Patch:
@@ -39,6 +39,8 @@ public class Catalogue extends SuperEntity {
 
     private static final long serialVersionUID = 4659379420249868394L;
 
+    private Integer tenantId;
+
     private Integer taskId;
 
     private String type;

File: dlink-admin/src/main/java/com/dlink/model/Cluster.java
Patch:
@@ -41,6 +41,8 @@ public class Cluster extends SuperEntity {
 
     private static final long serialVersionUID = 3104721227014487321L;
 
+    private Integer tenantId;
+
     @TableField(fill = FieldFill.INSERT)
     private String alias;
 

File: dlink-admin/src/main/java/com/dlink/model/ClusterConfiguration.java
Patch:
@@ -47,6 +47,8 @@ public class ClusterConfiguration extends SuperEntity {
 
     private static final long serialVersionUID = 5830130188542066241L;
 
+    private Integer tenantId;
+
     @TableField(fill = FieldFill.INSERT)
     private String alias;
 

File: dlink-admin/src/main/java/com/dlink/model/DataBase.java
Patch:
@@ -44,6 +44,8 @@ public class DataBase extends SuperEntity {
 
     private static final long serialVersionUID = -5002272138861566408L;
 
+    private Integer tenantId;
+
     @TableField(fill = FieldFill.INSERT)
     private String alias;
 

File: dlink-admin/src/main/java/com/dlink/model/Jar.java
Patch:
@@ -44,6 +44,8 @@ public class Jar extends SuperEntity {
     @TableField(fill = FieldFill.INSERT)
     private String alias;
 
+    private Integer tenantId;
+
     private String type;
 
     private String path;

File: dlink-admin/src/main/java/com/dlink/model/JobHistory.java
Patch:
@@ -45,6 +45,8 @@ public class JobHistory implements Serializable {
 
     private Integer id;
 
+    private Integer tenantId;
+
     @TableField(exist = false)
     private ObjectNode job;
 

File: dlink-admin/src/main/java/com/dlink/model/JobInstance.java
Patch:
@@ -48,6 +48,8 @@ public class JobInstance implements Serializable {
     @TableId(value = "id", type = IdType.AUTO)
     private Integer id;
 
+    private Integer tenantId;
+
     private String name;
 
     private Integer taskId;

File: dlink-admin/src/main/java/com/dlink/model/Savepoints.java
Patch:
@@ -50,6 +50,8 @@ public class Savepoints implements Serializable {
     @TableId(value = "id", type = IdType.AUTO)
     private Integer id;
 
+    private Integer tenantId;
+
     @NotNull(message = "作业ID不能为空", groups = {Save.class})
     private Integer taskId;
 

File: dlink-admin/src/main/java/com/dlink/model/Statement.java
Patch:
@@ -41,5 +41,7 @@ public class Statement implements Serializable {
 
     private Integer id;
 
+    private Integer tenantId;
+
     private String statement;
 }

File: dlink-admin/src/main/java/com/dlink/service/TaskService.java
Patch:
@@ -59,7 +59,7 @@ public interface TaskService extends ISuperService<Task> {
 
     List<Task> listFlinkSQLEnv();
 
-    Task initDefaultFlinkSQLEnv();
+    Task initDefaultFlinkSQLEnv(Integer tenantId);
 
     String exportSql(Integer id);
 
@@ -110,4 +110,6 @@ Result<List<Task>> queryOnLineTaskByDoneStatus(List<JobLifeCycle> jobLifeCycle
 
     void selectSavepointOffLineTask(TaskOperatingResult taskOperatingResult);
 
+    Task getTaskByNameAndTenantId(String name, Integer tenantId);
+
 }

File: dlink-admin/src/main/java/com/dlink/service/impl/CatalogueServiceImpl.java
Patch:
@@ -85,6 +85,7 @@ public Catalogue createCatalogueAndTask(CatalogueTaskDTO catalogueTaskDTO) {
         task.setDialect(catalogueTaskDTO.getDialect());
         taskService.saveOrUpdateTask(task);
         Catalogue catalogue = new Catalogue();
+        catalogue.setTenantId(catalogueTaskDTO.getTenantId());
         catalogue.setName(catalogueTaskDTO.getAlias());
         catalogue.setIsLeaf(true);
         catalogue.setTaskId(task.getId());
@@ -189,8 +190,8 @@ public boolean copyTask(Catalogue catalogue) {
         Statement statementServiceById = statementService.getById(catalogue.getTaskId());
         //新建作业的sql语句
         Statement statement = new Statement();
-        statement.setStatement(statementServiceById.getStatement());
         statement.setId(newTask.getId());
+        statement.setStatement(statementServiceById.getStatement());
         statementService.save(statement);
 
         Catalogue one = this.getOne(new LambdaQueryWrapper<Catalogue>().eq(Catalogue::getTaskId, catalogue.getTaskId()));

File: dlink-core/src/main/java/com/dlink/config/Dialect.java
Patch:
@@ -31,7 +31,7 @@ public enum Dialect {
 
     FLINKSQL("FlinkSql"), FLINKJAR("FlinkJar"), FLINKSQLENV("FlinkSqlEnv"), SQL("Sql"), JAVA("Java"),
     MYSQL("Mysql"), ORACLE("Oracle"), SQLSERVER("SqlServer"), POSTGRESQL("PostgreSql"), CLICKHOUSE("ClickHouse"),
-    DORIS("Doris"), PHOENIX("Phoenix"), HIVE("Hive"), STARROCKS("StarRocks");
+    DORIS("Doris"), PHOENIX("Phoenix"), HIVE("Hive"), STARROCKS("StarRocks"), KUBERNETES_APPLICATION("KubernetesApplaction");
 
     private String value;
 

File: dlink-metadata/dlink-metadata-base/src/main/java/com/dlink/metadata/driver/AbstractJdbcDriver.java
Patch:
@@ -120,7 +120,7 @@ public Driver setDriverConfig(DriverConfig config) {
         return this;
     }
 
-    private void createDataSource(DruidDataSource ds, DriverConfig config) {
+    protected void createDataSource(DruidDataSource ds, DriverConfig config) {
         ds.setName(config.getName().replaceAll(":", ""));
         ds.setUrl(config.getUrl());
         ds.setDriverClassName(getDriverClass());

File: dlink-app/dlink-app-base/src/main/java/com/dlink/app/flinksql/Submiter.java
Patch:
@@ -184,7 +184,6 @@ public static void submit(Integer id, DBConfig dbConfig) {
                 logger.error("执行失败, {}", e.getMessage(), e);
             }
         }
-        logger.info(LocalDateTime.now() + "任务提交成功");
-        System.out.println(LocalDateTime.now() + "任务提交成功");
+        logger.info("{}任务提交成功",LocalDateTime.now());
     }
 }

File: dlink-client/dlink-client-1.13/src/main/java/com/dlink/cdc/AbstractSinkBuilder.java
Patch:
@@ -77,7 +77,7 @@
  * @author wenmo
  * @since 2022/4/12 21:28
  **/
-public abstract class AbstractSinkBuilder {
+public abstract class AbstractSinkBuilder implements SinkBuilder {
 
     protected static final Logger logger = LoggerFactory.getLogger(AbstractSinkBuilder.class);
 
@@ -307,15 +307,15 @@ protected Object convertValue(Object value, LogicalType logicalType) {
         }
     }
 
-    protected String getSinkSchemaName(Table table) {
+    public String getSinkSchemaName(Table table) {
         String schemaName = table.getSchema();
         if (config.getSink().containsKey("sink.db")) {
             schemaName = config.getSink().get("sink.db");
         }
         return schemaName;
     }
 
-    protected String getSinkTableName(Table table) {
+    public String getSinkTableName(Table table) {
         String tableName = table.getName();
         if (config.getSink().containsKey("table.prefix.schema")) {
             if (Boolean.valueOf(config.getSink().get("table.prefix.schema"))) {

File: dlink-client/dlink-client-1.13/src/main/java/com/dlink/cdc/doris/DorisSinkBuilder.java
Patch:
@@ -43,7 +43,7 @@
  * @author wenmo
  * @since 2022/4/20 19:20
  **/
-public class DorisSinkBuilder extends AbstractSinkBuilder implements SinkBuilder, Serializable {
+public class DorisSinkBuilder extends AbstractSinkBuilder implements Serializable {
 
     private static final String KEY_WORD = "datastream-doris";
     private static final long serialVersionUID = 8330362249137471854L;

File: dlink-client/dlink-client-1.13/src/main/java/com/dlink/cdc/hudi/HudiSinkBuilder.java
Patch:
@@ -49,7 +49,7 @@
  * @author wenmo
  * @since 2022/4/22 23:50
  */
-public class HudiSinkBuilder extends AbstractSinkBuilder implements SinkBuilder, Serializable {
+public class HudiSinkBuilder extends AbstractSinkBuilder implements Serializable {
 
     private static final String KEY_WORD = "datastream-hudi";
     private static final long serialVersionUID = 5324199407472847422L;

File: dlink-client/dlink-client-1.13/src/main/java/com/dlink/cdc/kafka/KafkaSinkBuilder.java
Patch:
@@ -53,7 +53,7 @@
  * @author wenmo
  * @since 2022/4/12 21:29
  **/
-public class KafkaSinkBuilder extends AbstractSinkBuilder implements SinkBuilder, Serializable {
+public class KafkaSinkBuilder extends AbstractSinkBuilder implements Serializable {
 
     private static final String KEY_WORD = "datastream-kafka";
 

File: dlink-client/dlink-client-1.13/src/main/java/com/dlink/cdc/sql/SQLSinkBuilder.java
Patch:
@@ -75,7 +75,7 @@
  * @author wenmo
  * @since 2022/4/25 23:02
  */
-public class SQLSinkBuilder extends AbstractSinkBuilder implements SinkBuilder, Serializable {
+public class SQLSinkBuilder extends AbstractSinkBuilder implements Serializable {
 
     private static final String KEY_WORD = "sql";
     private static final long serialVersionUID = -3699685106324048226L;

File: dlink-client/dlink-client-1.14/src/main/java/com/dlink/cdc/AbstractSinkBuilder.java
Patch:
@@ -78,7 +78,7 @@
  * @author wenmo
  * @since 2022/4/12 21:28
  **/
-public abstract class AbstractSinkBuilder {
+public abstract class AbstractSinkBuilder implements SinkBuilder {
 
     protected static final Logger logger = LoggerFactory.getLogger(AbstractSinkBuilder.class);
 
@@ -342,15 +342,15 @@ protected Object convertValue(Object value, LogicalType logicalType) {
         }
     }
 
-    protected String getSinkSchemaName(Table table) {
+    public String getSinkSchemaName(Table table) {
         String schemaName = table.getSchema();
         if (config.getSink().containsKey("sink.db")) {
             schemaName = config.getSink().get("sink.db");
         }
         return schemaName;
     }
 
-    protected String getSinkTableName(Table table) {
+    public String getSinkTableName(Table table) {
         String tableName = table.getName();
         if (config.getSink().containsKey("table.prefix.schema")) {
             if (Boolean.valueOf(config.getSink().get("table.prefix.schema"))) {

File: dlink-client/dlink-client-1.14/src/main/java/com/dlink/cdc/doris/DorisSinkBuilder.java
Patch:
@@ -45,7 +45,7 @@
 /**
  * DorisSinkBuilder
  **/
-public class DorisSinkBuilder extends AbstractSinkBuilder implements SinkBuilder, Serializable {
+public class DorisSinkBuilder extends AbstractSinkBuilder implements Serializable {
 
     private static final String KEY_WORD = "datastream-doris";
     private static final long serialVersionUID = 8330362249137471854L;

File: dlink-client/dlink-client-1.14/src/main/java/com/dlink/cdc/kafka/KafkaSinkBuilder.java
Patch:
@@ -55,7 +55,7 @@
  * @author wenmo
  * @since 2022/4/12 21:29
  **/
-public class KafkaSinkBuilder extends AbstractSinkBuilder implements SinkBuilder, Serializable {
+public class KafkaSinkBuilder extends AbstractSinkBuilder implements Serializable {
 
     private static final String KEY_WORD = "datastream-kafka";
 

File: dlink-client/dlink-client-1.14/src/main/java/com/dlink/cdc/kafka/KafkaSinkJsonBuilder.java
Patch:
@@ -39,7 +39,7 @@
 /**
  * @className: com.dlink.cdc.kafka.KafkaSinkSimpleBuilder
  */
-public class KafkaSinkJsonBuilder extends AbstractSinkBuilder implements SinkBuilder, Serializable {
+public class KafkaSinkJsonBuilder extends AbstractSinkBuilder implements Serializable {
 
     private static final String KEY_WORD = "datastream-kafka-json";
     private transient ObjectMapper objectMapper;

File: dlink-client/dlink-client-1.14/src/main/java/com/dlink/cdc/sql/SQLSinkBuilder.java
Patch:
@@ -81,7 +81,8 @@
  * @author wenmo
  * @since 2022/4/25 23:02
  */
-public class SQLSinkBuilder extends AbstractSinkBuilder implements SinkBuilder, Serializable {
+public class SQLSinkBuilder extends AbstractSinkBuilder implements Serializable {
+
     private static final String KEY_WORD = "sql";
     private static final long serialVersionUID = -3699685106324048226L;
     private static AtomicInteger atomicInteger = new AtomicInteger(0);

File: dlink-client/dlink-client-1.14/src/main/java/com/dlink/cdc/starrocks/StarrocksSinkBuilder.java
Patch:
@@ -35,7 +35,7 @@
  * StarrocksSinkBuilder
  *
  **/
-public class StarrocksSinkBuilder extends AbstractSinkBuilder implements SinkBuilder, Serializable {
+public class StarrocksSinkBuilder extends AbstractSinkBuilder implements Serializable {
 
     private static final String KEY_WORD = "datastream-starrocks";
     private static final long serialVersionUID = 8330362249137431824L;

File: dlink-admin/src/main/java/com/dlink/Dlink.java
Patch:
@@ -21,6 +21,7 @@
 
 import org.springframework.boot.SpringApplication;
 import org.springframework.boot.autoconfigure.SpringBootApplication;
+import org.springframework.cache.annotation.EnableCaching;
 import org.springframework.transaction.annotation.EnableTransactionManagement;
 
 /**
@@ -31,6 +32,7 @@
  */
 @EnableTransactionManagement
 @SpringBootApplication
+@EnableCaching
 public class Dlink {
 
     public static void main(String[] args) {

File: dlink-admin/src/main/java/com/dlink/common/result/Result.java
Patch:
@@ -23,6 +23,7 @@
 
 import java.io.Serializable;
 
+import cn.hutool.core.date.DateTime;
 import lombok.AllArgsConstructor;
 import lombok.Data;
 import lombok.NoArgsConstructor;
@@ -41,6 +42,7 @@ public class Result<T> implements Serializable {
     private T datas;
     private Integer code;
     private String msg;
+    private String time;
 
     public static <T> Result<T> succeed(String msg) {
         return of(null, CodeEnum.SUCCESS.getCode(), msg);
@@ -55,7 +57,7 @@ public static <T> Result<T> succeed(T model) {
     }
 
     public static <T> Result<T> of(T datas, Integer code, String msg) {
-        return new Result<>(datas, code, msg);
+        return new Result<>(datas, code, msg,new DateTime().toString());
     }
 
     public static <T> Result<T> failed(String msg) {

File: dlink-admin/src/main/java/com/dlink/service/impl/TaskServiceImpl.java
Patch:
@@ -757,6 +757,7 @@ private JobConfig buildJobConfig(Task task) {
                 }
                 break;
             case CUSTOM:
+                config.setSavePointPath(config.getSavePointPath());
                 config.getConfig().put("execution.savepoint.path", config.getSavePointPath());
                 break;
             default:

File: dlink-core/src/main/java/com/dlink/job/JobManager.java
Patch:
@@ -376,7 +376,7 @@ public JobResult executeSql(String statement) {
                         streamGraph.setJobName(config.getJobName());
                         JobGraph jobGraph = streamGraph.getJobGraph();
                         if (Asserts.isNotNullString(config.getSavePointPath())) {
-                            jobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(config.getSavePointPath()));
+                            jobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(config.getSavePointPath(),true));
                         }
                         gatewayResult = Gateway.build(config.getGatewayConfig()).submitJobGraph(jobGraph);
                     }
@@ -434,7 +434,7 @@ private GatewayResult submitByGateway(List<String> inserts) {
             JobGraph jobGraph = executor.getJobGraphFromInserts(inserts);
             // Perjob mode need to set savepoint restore path, when recovery from savepoint.
             if (Asserts.isNotNullString(config.getSavePointPath())) {
-                jobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(config.getSavePointPath()));
+                jobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(config.getSavePointPath(),true));
             }
             // Perjob mode need to submit job graph.
             gatewayResult = Gateway.build(config.getGatewayConfig()).submitJobGraph(jobGraph);

File: dlink-metadata/dlink-metadata-base/src/main/java/com/dlink/metadata/driver/Driver.java
Patch:
@@ -23,6 +23,7 @@
 import com.dlink.exception.MetaDataException;
 import com.dlink.metadata.result.JdbcSelectResult;
 import com.dlink.model.Column;
+import com.dlink.model.QueryData;
 import com.dlink.model.Schema;
 import com.dlink.model.Table;
 import com.dlink.result.SqlExplainResult;
@@ -137,6 +138,8 @@ static Driver getHealthDriver(String key) {
 
     JdbcSelectResult query(String sql, Integer limit);
 
+    StringBuilder genQueryOption(QueryData queryData);
+
     JdbcSelectResult executeSql(String sql, Integer limit);
 
     List<SqlExplainResult> explain(String sql);

File: dlink-client/dlink-client-1.14/src/main/java/com/dlink/cdc/sqlserver/SqlServerCDCBuilder.java
Patch:
@@ -56,8 +56,8 @@ public DataStreamSource<String> build(StreamExecutionEnvironment env) {
         String database = config.getDatabase();
         Properties debeziumProperties = new Properties();
         // 为部分转换添加默认值
-        //debeziumProperties.setProperty("bigint.unsigned.handling.mode", "long");
-        //debeziumProperties.setProperty("decimal.handling.mode", "string");
+        debeziumProperties.setProperty("bigint.unsigned.handling.mode", "long");
+        debeziumProperties.setProperty("decimal.handling.mode", "string");
         for (Map.Entry<String, String> entry : config.getDebezium().entrySet()) {
             if (Asserts.isNotNullString(entry.getKey()) && Asserts.isNotNullString(entry.getValue())) {
                 debeziumProperties.setProperty(entry.getKey(), entry.getValue());

File: dlink-client/dlink-client-1.11/src/main/java/com/dlink/cdc/AbstractSinkBuilder.java
Patch:
@@ -193,7 +193,7 @@ public void flatMap(Map value, Collector<RowData> out) throws Exception {
                             default:
                         }
                     } catch (Exception e) {
-                        logger.error("SchameTable: {} - Row: {} - Exception: {}", schemaTableName, JSONUtil.toJsonString(value), e.getCause().getMessage());
+                        logger.error("SchameTable: {} - Row: {} - Exception:", schemaTableName, JSONUtil.toJsonString(value), e);
                         throw e;
                     }
                 }

File: dlink-client/dlink-client-1.11/src/main/java/com/dlink/cdc/sql/SQLSinkBuilder.java
Patch:
@@ -144,7 +144,7 @@ public void flatMap(Map value, Collector<Row> out) throws Exception {
                                 default:
                             }
                         } catch (Exception e) {
-                            logger.error("SchameTable: {} - Row: {} - Exception: {}", schemaTableName, JSONUtil.toJsonString(value), e.getCause().getMessage());
+                            logger.error("SchameTable: {} - Row: {} - Exception:", schemaTableName, JSONUtil.toJsonString(value), e);
                             throw e;
                         }
                     }

File: dlink-client/dlink-client-1.12/src/main/java/com/dlink/cdc/AbstractSinkBuilder.java
Patch:
@@ -193,7 +193,7 @@ public void flatMap(Map value, Collector<RowData> out) throws Exception {
                             default:
                         }
                     } catch (Exception e) {
-                        logger.error("SchameTable: {} - Row: {} - Exception: {}", schemaTableName, JSONUtil.toJsonString(value), e.getCause().getMessage());
+                        logger.error("SchameTable: {} - Row: {} - Exception:", schemaTableName, JSONUtil.toJsonString(value), e);
                         throw e;
                     }
                 }

File: dlink-client/dlink-client-1.12/src/main/java/com/dlink/cdc/sql/SQLSinkBuilder.java
Patch:
@@ -144,7 +144,7 @@ public void flatMap(Map value, Collector<Row> out) throws Exception {
                                 default:
                             }
                         } catch (Exception e) {
-                            logger.error("SchameTable: {} - Row: {} - Exception: {}", schemaTableName, JSONUtil.toJsonString(value), e.getCause().getMessage());
+                            logger.error("SchameTable: {} - Row: {} - Exception:", schemaTableName, JSONUtil.toJsonString(value), e);
                             throw e;
                         }
                     }

File: dlink-client/dlink-client-1.13/src/main/java/com/dlink/cdc/AbstractSinkBuilder.java
Patch:
@@ -193,7 +193,7 @@ public void flatMap(Map value, Collector<RowData> out) throws Exception {
                             default:
                         }
                     } catch (Exception e) {
-                        logger.error("SchameTable: {} - Row: {} - Exception: {}", schemaTableName, JSONUtil.toJsonString(value), e.getCause().getMessage());
+                        logger.error("SchameTable: {} - Row: {} - Exception:", schemaTableName, JSONUtil.toJsonString(value), e);
                         throw e;
                     }
                 }

File: dlink-client/dlink-client-1.13/src/main/java/com/dlink/cdc/sql/SQLSinkBuilder.java
Patch:
@@ -144,7 +144,7 @@ public void flatMap(Map value, Collector<Row> out) throws Exception {
                                 default:
                             }
                         } catch (Exception e) {
-                            logger.error("SchameTable: {} - Row: {} - Exception: {}", schemaTableName, JSONUtil.toJsonString(value), e.getCause().getMessage());
+                            logger.error("SchameTable: {} - Row: {} - Exception:", schemaTableName, JSONUtil.toJsonString(value), e);
                             throw e;
                         }
                     }

File: dlink-client/dlink-client-1.14/src/main/java/com/dlink/cdc/sql/SQLSinkBuilder.java
Patch:
@@ -145,7 +145,7 @@ public void flatMap(Map value, Collector<Row> out) throws Exception {
                             default:
                         }
                     } catch (Exception e) {
-                        logger.error("SchameTable: {} - Row: {} - Exception: {}", schemaTableName, JSONUtil.toJsonString(value), e.getCause().getMessage());
+                        logger.error("SchameTable: {} - Row: {} - Exception:", schemaTableName, JSONUtil.toJsonString(value),e);
                         throw e;
                     }
                 }

File: dlink-client/dlink-client-1.15/src/main/java/com/dlink/cdc/AbstractSinkBuilder.java
Patch:
@@ -193,7 +193,7 @@ public void flatMap(Map value, Collector<RowData> out) throws Exception {
                             default:
                         }
                     } catch (Exception e) {
-                        logger.error("SchameTable: {} - Row: {} - Exception: {}", schemaTableName, JSONUtil.toJsonString(value), e.getCause().getMessage());
+                        logger.error("SchameTable: {} - Row: {} - Exception:", schemaTableName, JSONUtil.toJsonString(value), e);
                         throw e;
                     }
                 }

File: dlink-client/dlink-client-1.15/src/main/java/com/dlink/cdc/sql/SQLSinkBuilder.java
Patch:
@@ -144,7 +144,7 @@ public void flatMap(Map value, Collector<Row> out) throws Exception {
                                 default:
                             }
                         } catch (Exception e) {
-                            logger.error("SchameTable: {} - Row: {} - Exception: {}", schemaTableName, JSONUtil.toJsonString(value), e.getCause().getMessage());
+                            logger.error("SchameTable: {} - Row: {} - Exception:", schemaTableName, JSONUtil.toJsonString(value), e);
                             throw e;
                         }
                     }

File: dlink-gateway/src/main/java/com/dlink/gateway/kubernetes/KubernetesApplicationGateway.java
Patch:
@@ -25,6 +25,7 @@
 import com.dlink.gateway.exception.GatewayException;
 import com.dlink.gateway.result.GatewayResult;
 import com.dlink.gateway.result.KubernetesResult;
+import com.dlink.model.SystemConfiguration;
 import com.dlink.utils.LogUtil;
 
 import org.apache.flink.client.deployment.ClusterSpecification;
@@ -82,7 +83,7 @@ public GatewayResult submitJar() {
             ClusterClientProvider<String> clusterClientProvider = kubernetesClusterDescriptor.deployApplicationCluster(clusterSpecification, applicationConfiguration);
             ClusterClient<String> clusterClient = clusterClientProvider.getClusterClient();
             Collection<JobStatusMessage> jobStatusMessages = clusterClient.listJobs().get();
-            int counts = 10;
+            int counts = SystemConfiguration.getInstances().getJobIdWait();
             while (jobStatusMessages.size() == 0 && counts > 0) {
                 Thread.sleep(1000);
                 counts--;

File: dlink-gateway/src/main/java/com/dlink/gateway/yarn/YarnApplicationGateway.java
Patch:
@@ -26,6 +26,7 @@
 import com.dlink.gateway.exception.GatewayException;
 import com.dlink.gateway.result.GatewayResult;
 import com.dlink.gateway.result.YarnResult;
+import com.dlink.model.SystemConfiguration;
 import com.dlink.utils.LogUtil;
 
 import org.apache.flink.client.deployment.ClusterSpecification;
@@ -96,7 +97,7 @@ public GatewayResult submitJar() {
                 applicationConfiguration);
             ClusterClient<ApplicationId> clusterClient = clusterClientProvider.getClusterClient();
             Collection<JobStatusMessage> jobStatusMessages = clusterClient.listJobs().get();
-            int counts = 30;
+            int counts = SystemConfiguration.getInstances().getJobIdWait();
             while (jobStatusMessages.size() == 0 && counts > 0) {
                 Thread.sleep(1000);
                 counts--;

File: dlink-gateway/src/main/java/com/dlink/gateway/yarn/YarnPerJobGateway.java
Patch:
@@ -25,6 +25,7 @@
 import com.dlink.gateway.exception.GatewayException;
 import com.dlink.gateway.result.GatewayResult;
 import com.dlink.gateway.result.YarnResult;
+import com.dlink.model.SystemConfiguration;
 import com.dlink.utils.LogUtil;
 
 import org.apache.flink.client.deployment.ClusterSpecification;
@@ -81,7 +82,7 @@ public GatewayResult submitJobGraph(JobGraph jobGraph) {
             result.setAppId(applicationId.toString());
             result.setWebURL(clusterClient.getWebInterfaceURL());
             Collection<JobStatusMessage> jobStatusMessages = clusterClient.listJobs().get();
-            int counts = 10;
+            int counts = SystemConfiguration.getInstances().getJobIdWait();
             while (jobStatusMessages.size() == 0 && counts > 0) {
                 Thread.sleep(1000);
                 counts--;

File: dlink-client/dlink-client-1.14/src/main/java/com/dlink/cdc/CDCBuilderFactory.java
Patch:
@@ -22,6 +22,7 @@
 import com.dlink.assertion.Asserts;
 import com.dlink.cdc.mysql.MysqlCDCBuilder;
 import com.dlink.cdc.oracle.OracleCDCBuilder;
+import com.dlink.cdc.postgres.PostgresCDCBuilder;
 import com.dlink.exception.FlinkClientException;
 import com.dlink.model.FlinkCDCConfig;
 
@@ -35,7 +36,8 @@ public class CDCBuilderFactory {
 
     private static CDCBuilder[] cdcBuilders = {
         new MysqlCDCBuilder(),
-        new OracleCDCBuilder()
+        new OracleCDCBuilder(),
+            new PostgresCDCBuilder()
     };
 
     public static CDCBuilder buildCDCBuilder(FlinkCDCConfig config) {

File: dlink-core/src/main/java/com/dlink/cluster/FlinkCluster.java
Patch:
@@ -17,15 +17,16 @@
  *
  */
 
-
 package com.dlink.cluster;
 
-import cn.hutool.core.io.IORuntimeException;
 import com.dlink.api.FlinkAPI;
 import com.dlink.assertion.Asserts;
+
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+import cn.hutool.core.io.IORuntimeException;
+
 /**
  * FlinkCluster
  *

File: dlink-core/src/main/java/com/dlink/cluster/FlinkClusterInfo.java
Patch:
@@ -17,7 +17,6 @@
  *
  */
 
-
 package com.dlink.cluster;
 
 import lombok.Getter;

File: dlink-core/src/main/java/com/dlink/config/Dialect.java
Patch:
@@ -17,7 +17,6 @@
  *
  */
 
-
 package com.dlink.config;
 
 import com.dlink.assertion.Asserts;

File: dlink-core/src/main/java/com/dlink/constant/FlinkHistoryConstant.java
Patch:
@@ -17,7 +17,6 @@
  *
  */
 
-
 package com.dlink.constant;
 
 public interface FlinkHistoryConstant {

File: dlink-core/src/main/java/com/dlink/constant/FlinkRestAPIConstant.java
Patch:
@@ -17,7 +17,6 @@
  *
  */
 
-
 package com.dlink.constant;
 
 /**

File: dlink-core/src/main/java/com/dlink/constant/FlinkRestResultConstant.java
Patch:
@@ -17,7 +17,6 @@
  *
  */
 
-
 package com.dlink.constant;
 
 /**

File: dlink-core/src/main/java/com/dlink/explainer/ca/CABuilder.java
Patch:
@@ -17,7 +17,6 @@
  *
  */
 
-
 package com.dlink.explainer.ca;
 
 import com.dlink.plus.FlinkSqlPlus;
@@ -104,7 +103,7 @@ private static void buildColumnCANodeChildren(List<ColumnCANode> children, Colum
         for (NodeRel nodeRel : columnCASRel) {
             if (columnId.equals(nodeRel.getSufId())) {
                 ColumnCA childca = (ColumnCA) result.getColumnCASMaps().get(nodeRel.getPreId());
-//                operation = operation.replaceAll(childca.getAlias().replaceAll("\\$","\\\\$"),childca.getOperation());
+                //operation = operation.replaceAll(childca.getAlias().replaceAll("\\$","\\\\$"),childca.getOperation());
                 operation = operation.replaceAll(childca.getAlias()
                         .replaceAll("\\)", ""), childca.getOperation());
                 buildColumnCANodeChildren(children, result, nodeRel.getPreId(), operation);

File: dlink-core/src/main/java/com/dlink/explainer/ca/CAGenerator.java
Patch:
@@ -17,7 +17,6 @@
  *
  */
 
-
 package com.dlink.explainer.ca;
 
 public interface CAGenerator {

File: dlink-core/src/main/java/com/dlink/explainer/ca/ColumnCAResult.java
Patch:
@@ -17,7 +17,6 @@
  *
  */
 
-
 package com.dlink.explainer.ca;
 
 import com.dlink.explainer.lineage.LineageColumnGenerator;

File: dlink-core/src/main/java/com/dlink/explainer/ca/ICA.java
Patch:
@@ -17,7 +17,6 @@
  *
  */
 
-
 package com.dlink.explainer.ca;
 
 public interface ICA {

File: dlink-core/src/main/java/com/dlink/explainer/ca/TableCAResult.java
Patch:
@@ -17,7 +17,6 @@
  *
  */
 
-
 package com.dlink.explainer.ca;
 
 import java.util.List;

File: dlink-core/src/main/java/com/dlink/explainer/lineage/LineageColumn.java
Patch:
@@ -17,7 +17,6 @@
  *
  */
 
-
 package com.dlink.explainer.lineage;
 
 /**

File: dlink-core/src/main/java/com/dlink/explainer/lineage/LineageRelation.java
Patch:
@@ -17,7 +17,6 @@
  *
  */
 
-
 package com.dlink.explainer.lineage;
 
 /**

File: dlink-core/src/main/java/com/dlink/explainer/lineage/LineageResult.java
Patch:
@@ -17,7 +17,6 @@
  *
  */
 
-
 package com.dlink.explainer.lineage;
 
 import java.util.List;

File: dlink-core/src/main/java/com/dlink/explainer/lineage/LineageTable.java
Patch:
@@ -17,7 +17,6 @@
  *
  */
 
-
 package com.dlink.explainer.lineage;
 
 import com.dlink.explainer.ca.TableCA;

File: dlink-core/src/main/java/com/dlink/explainer/lineage/LineageTableGenerator.java
Patch:
@@ -17,7 +17,6 @@
  *
  */
 
-
 package com.dlink.explainer.lineage;
 
 import com.dlink.assertion.Asserts;

File: dlink-core/src/main/java/com/dlink/explainer/sqlLineage/TreeNodeIterator.java
Patch:
@@ -17,9 +17,10 @@
  *
  */
 
-
 package com.dlink.explainer.sqlLineage;
+
 import java.util.Iterator;
+
 public class TreeNodeIterator<T> implements Iterator<TreeNode<T>> {
 
     private ProcessStages doNext;

File: dlink-core/src/main/java/com/dlink/explainer/trans/Field.java
Patch:
@@ -17,7 +17,6 @@
  *
  */
 
-
 package com.dlink.explainer.trans;
 
 /**

File: dlink-core/src/main/java/com/dlink/explainer/trans/SinkTrans.java
Patch:
@@ -17,10 +17,8 @@
  *
  */
 
-
 package com.dlink.explainer.trans;
 
-
 import com.dlink.utils.MapParseUtils;
 
 import java.util.ArrayList;
@@ -40,7 +38,7 @@ public class SinkTrans extends AbstractTrans implements Trans {
     private String table;
     private List<String> fields;
 
-    public final static String TRANS_TYPE = "Data Sink";
+    public static final String TRANS_TYPE = "Data Sink";
 
     public SinkTrans() {
     }

File: dlink-core/src/main/java/com/dlink/explainer/trans/Trans.java
Patch:
@@ -17,13 +17,12 @@
  *
  */
 
-
 package com.dlink.explainer.trans;
 
-import com.fasterxml.jackson.databind.JsonNode;
-
 import java.util.List;
 
+import com.fasterxml.jackson.databind.JsonNode;
+
 /**
  * Trans
  *

File: dlink-core/src/main/java/com/dlink/explainer/trans/TransGenerator.java
Patch:
@@ -17,18 +17,18 @@
  *
  */
 
-
 package com.dlink.explainer.trans;
 
 import com.dlink.assertion.Asserts;
-import com.fasterxml.jackson.databind.JsonNode;
-import com.fasterxml.jackson.databind.node.ObjectNode;
 
 import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 
+import com.fasterxml.jackson.databind.JsonNode;
+import com.fasterxml.jackson.databind.node.ObjectNode;
+
 /**
  * TransGenerator
  *

File: dlink-core/src/main/java/com/dlink/job/Job.java
Patch:
@@ -17,19 +17,19 @@
  *
  */
 
-
 package com.dlink.job;
 
 import com.dlink.executor.Executor;
 import com.dlink.executor.ExecutorSetting;
 import com.dlink.gateway.GatewayType;
 import com.dlink.result.IResult;
-import lombok.Getter;
-import lombok.Setter;
 
 import java.time.LocalDateTime;
 import java.util.List;
 
+import lombok.Getter;
+import lombok.Setter;
+
 /**
  * Job
  *

File: dlink-core/src/main/java/com/dlink/job/JobContextHolder.java
Patch:
@@ -17,7 +17,6 @@
  *
  */
 
-
 package com.dlink.job;
 
 /**

File: dlink-core/src/main/java/com/dlink/job/JobHandler.java
Patch:
@@ -17,7 +17,6 @@
  *
  */
 
-
 package com.dlink.job;
 
 import com.dlink.exception.JobException;

File: dlink-core/src/main/java/com/dlink/job/JobParam.java
Patch:
@@ -17,7 +17,6 @@
  *
  */
 
-
 package com.dlink.job;
 
 import java.util.ArrayList;
@@ -69,7 +68,7 @@ public List<StatementParam> getTrans() {
 
     public List<String> getTransStatement() {
         List<String> statementList = new ArrayList<>();
-        for(StatementParam statementParam: trans){
+        for (StatementParam statementParam: trans) {
             statementList.add(statementParam.getValue());
         }
         return statementList;

File: dlink-core/src/main/java/com/dlink/job/RunTime.java
Patch:
@@ -17,7 +17,6 @@
  *
  */
 
-
 package com.dlink.job;
 
 /**

File: dlink-core/src/main/java/com/dlink/job/StatementParam.java
Patch:
@@ -17,7 +17,6 @@
  *
  */
 
-
 package com.dlink.job;
 
 import com.dlink.parser.SqlType;

File: dlink-core/src/main/java/com/dlink/plus/FlinkSqlPlus.java
Patch:
@@ -17,20 +17,21 @@
  *
  */
 
-
 package com.dlink.plus;
 
 import com.dlink.executor.Executor;
 import com.dlink.explainer.Explainer;
 import com.dlink.explainer.ca.ColumnCAResult;
 import com.dlink.explainer.ca.TableCAResult;
 import com.dlink.result.SqlExplainResult;
-import com.fasterxml.jackson.databind.node.ObjectNode;
+
 import org.apache.flink.runtime.rest.messages.JobPlanInfo;
 
 import java.util.ArrayList;
 import java.util.List;
 
+import com.fasterxml.jackson.databind.node.ObjectNode;
+
 /**
  * FlinkSqlPlus
  *

File: dlink-core/src/main/java/com/dlink/plus/SqlResult.java
Patch:
@@ -17,7 +17,6 @@
  *
  */
 
-
 package com.dlink.plus;
 
 import org.apache.flink.table.api.TableResult;

File: dlink-core/src/main/java/com/dlink/result/DDLResultBuilder.java
Patch:
@@ -17,7 +17,6 @@
  *
  */
 
-
 package com.dlink.result;
 
 import org.apache.flink.table.api.TableResult;

File: dlink-core/src/main/java/com/dlink/result/ErrorResult.java
Patch:
@@ -17,7 +17,6 @@
  *
  */
 
-
 package com.dlink.result;
 
 import java.time.LocalDateTime;

File: dlink-core/src/main/java/com/dlink/result/InsertResult.java
Patch:
@@ -17,14 +17,13 @@
  *
  */
 
-
 package com.dlink.result;
 
+import java.time.LocalDateTime;
+
 import lombok.Getter;
 import lombok.Setter;
 
-import java.time.LocalDateTime;
-
 /**
  * InsertResult
  *

File: dlink-core/src/main/java/com/dlink/result/InsertResultBuilder.java
Patch:
@@ -17,7 +17,6 @@
  *
  */
 
-
 package com.dlink.result;
 
 import org.apache.flink.table.api.TableResult;

File: dlink-core/src/main/java/com/dlink/result/JobSubmitResult.java
Patch:
@@ -17,7 +17,6 @@
  *
  */
 
-
 package com.dlink.result;
 
 /**

File: dlink-core/src/main/java/com/dlink/result/ResultBuilder.java
Patch:
@@ -17,13 +17,12 @@
  *
  */
 
-
 package com.dlink.result;
 
-import org.apache.flink.table.api.TableResult;
-
 import com.dlink.parser.SqlType;
 
+import org.apache.flink.table.api.TableResult;
+
 /**
  * ResultBuilder
  *

File: dlink-core/src/main/java/com/dlink/result/ResultPool.java
Patch:
@@ -17,7 +17,6 @@
  *
  */
 
-
 package com.dlink.result;
 
 import java.util.HashMap;

File: dlink-core/src/main/java/com/dlink/result/RunResult.java
Patch:
@@ -17,7 +17,6 @@
  *
  */
 
-
 package com.dlink.result;
 
 import com.dlink.executor.ExecutorSetting;

File: dlink-core/src/main/java/com/dlink/result/SelectResultBuilder.java
Patch:
@@ -17,7 +17,6 @@
  *
  */
 
-
 package com.dlink.result;
 
 import org.apache.flink.table.api.TableResult;

File: dlink-core/src/main/java/com/dlink/result/SubmitResult.java
Patch:
@@ -17,7 +17,6 @@
  *
  */
 
-
 package com.dlink.result;
 
 import java.time.LocalDateTime;

File: dlink-core/src/main/java/com/dlink/session/ExecutorEntity.java
Patch:
@@ -17,15 +17,15 @@
  *
  */
 
-
 package com.dlink.session;
 
 import com.dlink.executor.Executor;
-import lombok.Getter;
-import lombok.Setter;
 
 import java.time.LocalDateTime;
 
+import lombok.Getter;
+import lombok.Setter;
+
 /**
  * FlinkEntity
  *

File: dlink-core/src/main/java/com/dlink/session/SessionConfig.java
Patch:
@@ -17,10 +17,10 @@
  *
  */
 
-
 package com.dlink.session;
 
 import com.dlink.executor.ExecutorSetting;
+
 import lombok.Getter;
 import lombok.Setter;
 

File: dlink-core/src/main/java/com/dlink/session/SessionInfo.java
Patch:
@@ -17,14 +17,13 @@
  *
  */
 
-
 package com.dlink.session;
 
+import java.time.LocalDateTime;
+
 import lombok.Getter;
 import lombok.Setter;
 
-import java.time.LocalDateTime;
-
 /**
  * SessionInfo
  *

File: dlink-core/src/main/java/com/dlink/session/SessionPool.java
Patch:
@@ -17,7 +17,6 @@
  *
  */
 
-
 package com.dlink.session;
 
 import com.dlink.constant.FlinkConstant;

File: dlink-gateway/src/main/java/com/dlink/gateway/yarn/YarnPerJobGateway.java
Patch:
@@ -70,7 +70,7 @@ public GatewayResult submitJobGraph(JobGraph jobGraph) {
         YarnClusterDescriptor yarnClusterDescriptor = new YarnClusterDescriptor(
                 configuration, yarnConfiguration, yarnClient, YarnClientYarnClusterInformationRetriever.create(yarnClient), true);
         try {
-            ClusterClientProvider<ApplicationId> clusterClientProvider = yarnClusterDescriptor.deployJobCluster(clusterSpecification, jobGraph, false);
+            ClusterClientProvider<ApplicationId> clusterClientProvider = yarnClusterDescriptor.deployJobCluster(clusterSpecification, jobGraph, true);
             ClusterClient<ApplicationId> clusterClient = clusterClientProvider.getClusterClient();
             ApplicationId applicationId = clusterClient.getClusterId();
             result.setAppId(applicationId.toString());

File: dlink-client/dlink-client-base/src/main/java/com/dlink/utils/FlinkBaseUtil.java
Patch:
@@ -75,7 +75,7 @@ public static String getFlinkDDL(Table table, String tableName, FlinkCDCConfig c
         sb.append("` (\n");
         List<String> pks = new ArrayList<>();
         for (int i = 0; i < table.getColumns().size(); i++) {
-            String type = table.getColumns().get(i).getJavaType().getFlinkType();
+            String type = table.getColumns().get(i).getFlinkType();
             sb.append("    ");
             if (i > 0) {
                 sb.append(",");

File: dlink-common/src/main/java/com/dlink/model/Table.java
Patch:
@@ -107,7 +107,7 @@ public String getFlinkDDL(String flinkConfig, String tableName) {
         sb.append("CREATE TABLE IF NOT EXISTS " + tableName + " (\n");
         List<String> pks = new ArrayList<>();
         for (int i = 0; i < columns.size(); i++) {
-            String type = columns.get(i).getJavaType().getFlinkType();
+            String type = columns.get(i).getFlinkType();
             sb.append("    ");
             if (i > 0) {
                 sb.append(",");
@@ -158,7 +158,7 @@ public String getFlinkTableSql(String catalogName, String flinkConfig) {
         sb.append("CREATE TABLE IF NOT EXISTS " + name + " (\n");
         List<String> pks = new ArrayList<>();
         for (int i = 0; i < columns.size(); i++) {
-            String type = columns.get(i).getJavaType().getFlinkType();
+            String type = columns.get(i).getFlinkType();
             sb.append("    ");
             if (i > 0) {
                 sb.append(",");

File: dlink-metadata/dlink-metadata-clickhouse/src/main/java/com/dlink/metadata/convert/ClickHouseTypeConvert.java
Patch:
@@ -92,7 +92,6 @@ public ColumnType convert(Column column) {
                 columnType = ColumnType.INT;
             }
         }
-        columnType.setPrecisionAndScale(column.getPrecision(), column.getScale());
         return columnType;
     }
 

File: dlink-metadata/dlink-metadata-doris/src/main/java/com/dlink/metadata/convert/DorisTypeConvert.java
Patch:
@@ -92,7 +92,6 @@ public ColumnType convert(Column column) {
                 columnType = ColumnType.DOUBLE;
             }
         }
-        columnType.setPrecisionAndScale(column.getPrecision(), column.getScale());
         return columnType;
     }
 

File: dlink-metadata/dlink-metadata-hive/src/main/java/com/dlink/metadata/convert/HiveTypeConvert.java
Patch:
@@ -94,7 +94,6 @@ public ColumnType convert(Column column) {
                 columnType = ColumnType.DOUBLE;
             }
         }
-        columnType.setPrecisionAndScale(column.getPrecision(), column.getScale());
         return columnType;
     }
 

File: dlink-metadata/dlink-metadata-mysql/src/main/java/com/dlink/metadata/convert/MySqlTypeConvert.java
Patch:
@@ -84,7 +84,6 @@ public ColumnType convert(Column column) {
                 columnType = ColumnType.INT;
             }
         }
-        columnType.setPrecisionAndScale(column.getPrecision(), column.getScale());
         return columnType;
     }
 

File: dlink-metadata/dlink-metadata-oracle/src/main/java/com/dlink/metadata/convert/OracleTypeConvert.java
Patch:
@@ -72,7 +72,6 @@ public ColumnType convert(Column column) {
         } else if (t.contains("blob")) {
             columnType = ColumnType.BYTES;
         }
-        columnType.setPrecisionAndScale(column.getPrecision(), column.getScale());
         return columnType;
     }
 

File: dlink-metadata/dlink-metadata-phoenix/src/main/java/com/dlink/metadata/convert/PhoenixTypeConvert.java
Patch:
@@ -79,7 +79,6 @@ public ColumnType convert(Column column) {
         } else if (t.contains("date")) {
             columnType = ColumnType.DATE;
         }
-        columnType.setPrecisionAndScale(column.getPrecision(), column.getScale());
         return columnType;
     }
 

File: dlink-metadata/dlink-metadata-postgresql/src/main/java/com/dlink/metadata/convert/PostgreSqlTypeConvert.java
Patch:
@@ -90,7 +90,6 @@ public ColumnType convert(Column column) {
         } else if (t.contains("array")) {
             columnType = ColumnType.T;
         }
-        columnType.setPrecisionAndScale(column.getPrecision(), column.getScale());
         return columnType;
     }
 

File: dlink-metadata/dlink-metadata-sqlserver/src/main/java/com/dlink/metadata/convert/SqlServerTypeConvert.java
Patch:
@@ -80,7 +80,6 @@ public ColumnType convert(Column column) {
         } else if (t.contains("timestamp") || t.contains("binary") || t.contains("varbinary") || t.contains("image")) {
             columnType = ColumnType.BYTES;
         }
-        columnType.setPrecisionAndScale(column.getPrecision(), column.getScale());
         return columnType;
     }
 

File: dlink-client/dlink-client-1.13/src/main/java/com/dlink/cdc/mysql/MysqlCDCBuilder.java
Patch:
@@ -121,7 +121,7 @@ public DataStreamSource<String> build(StreamExecutionEnvironment env) {
             sourceBuilder.tableList(new String[0]);
         }
 
-        sourceBuilder.deserializer(new JsonDebeziumDeserializationSchema());
+        sourceBuilder.deserializer(new MysqlJsonDebeziumDeserializationSchema());
         sourceBuilder.debeziumProperties(debeziumProperties);
         sourceBuilder.jdbcProperties(jdbcProperties);
 

File: dlink-client/dlink-client-1.14/src/main/java/com/dlink/cdc/mysql/MysqlCDCBuilder.java
Patch:
@@ -121,7 +121,7 @@ public DataStreamSource<String> build(StreamExecutionEnvironment env) {
             sourceBuilder.tableList(new String[0]);
         }
 
-        sourceBuilder.deserializer(new JsonDebeziumDeserializationSchema());
+        sourceBuilder.deserializer(new MysqlJsonDebeziumDeserializationSchema());
         sourceBuilder.debeziumProperties(debeziumProperties);
         sourceBuilder.jdbcProperties(jdbcProperties);
 

File: dlink-client/dlink-client-1.15/src/main/java/com/dlink/cdc/SinkBuilderFactory.java
Patch:
@@ -23,6 +23,7 @@
 import com.dlink.assertion.Asserts;
 import com.dlink.cdc.doris.DorisSinkBuilder;
 import com.dlink.cdc.kafka.KafkaSinkBuilder;
+import com.dlink.cdc.kafka.KafkaSinkJsonBuilder;
 import com.dlink.cdc.sql.SQLSinkBuilder;
 import com.dlink.exception.FlinkClientException;
 import com.dlink.model.FlinkCDCConfig;
@@ -37,6 +38,7 @@ public class SinkBuilderFactory {
 
     private static SinkBuilder[] sinkBuilders = {
         new KafkaSinkBuilder(),
+        new KafkaSinkJsonBuilder(),
         new DorisSinkBuilder(),
         new SQLSinkBuilder()
     };

File: dlink-client/dlink-client-1.15/src/main/java/com/dlink/cdc/mysql/MysqlCDCBuilder.java
Patch:
@@ -121,7 +121,7 @@ public DataStreamSource<String> build(StreamExecutionEnvironment env) {
             sourceBuilder.tableList(new String[0]);
         }
 
-        sourceBuilder.deserializer(new JsonDebeziumDeserializationSchema());
+        sourceBuilder.deserializer(new MysqlJsonDebeziumDeserializationSchema());
         sourceBuilder.debeziumProperties(debeziumProperties);
         sourceBuilder.jdbcProperties(jdbcProperties);
 

File: dlink-app/dlink-app-base/src/main/java/com/dlink/app/flinksql/Submiter.java
Patch:
@@ -103,7 +103,7 @@ public static void submit(Integer id, DBConfig dbConfig) {
             if (Asserts.isNotNullString(envId)) {
                 sb.append(envId);
             }
-            sb.append("\r\n");
+            sb.append("\n");
         }
         sb.append(getFlinkSQLStatement(id, dbConfig));
         List<String> statements = Submiter.getStatements(sb.toString());

File: dlink-client/dlink-client-base/src/main/java/com/dlink/sql/FlinkQuery.java
Patch:
@@ -29,7 +29,7 @@
 public class FlinkQuery {
 
     public static String separator() {
-        return ";\r\n";
+        return ";\n";
     }
 
     public static String defaultCatalog() {

File: dlink-common/src/main/java/com/dlink/model/SystemConfiguration.java
Patch:
@@ -81,7 +81,7 @@ public static SystemConfiguration getInstances() {
         "sqlSeparator",
         "FlinkSQL语句分割符",
         ValueType.STRING,
-        ";\r\n|;\n",
+        ";\n",
         "Flink SQL 的语句分割符"
     );
 

File: dlink-executor/src/main/java/com/dlink/constant/FlinkSQLConstant.java
Patch:
@@ -30,7 +30,7 @@ public interface FlinkSQLConstant {
     /**
      * 分隔符
      */
-    String SEPARATOR = ";";
+    String SEPARATOR = ";\n";
     /**
      * DDL 类型
      */

File: dlink-admin/src/main/java/com/dlink/model/JobHistory.java
Patch:
@@ -87,4 +87,7 @@ public class JobHistory implements Serializable {
 
     @TableField(fill = FieldFill.INSERT_UPDATE)
     private LocalDateTime updateTime;
+
+    @TableField(exist = false)
+    private boolean error;
 }

File: dlink-admin/src/main/java/com/dlink/service/impl/TaskServiceImpl.java
Patch:
@@ -785,13 +785,13 @@ public JobInstance refreshJobInstance(Integer id, boolean isCoercive) {
         }
         JobHistory jobHistoryJson = jobHistoryService.refreshJobHistory(id, jobInfoDetail.getCluster().getJobManagerHost(), jobInfoDetail.getInstance().getJid(), jobInfoDetail.isNeedSave());
         JobHistory jobHistory = jobHistoryService.getJobHistoryInfo(jobHistoryJson);
-        if (JobStatus.isDone(jobInfoDetail.getInstance().getStatus()) && Asserts.isNull(jobHistory.getJob())) {
+        jobInfoDetail.setJobHistory(jobHistory);
+        if (JobStatus.isDone(jobInfoDetail.getInstance().getStatus()) && (Asserts.isNull(jobHistory.getJob()) || jobHistory.isError())) {
             return jobInfoDetail.getInstance();
         }
-        jobInfoDetail.setJobHistory(jobHistory);
         String status = jobInfoDetail.getInstance().getStatus();
         boolean jobStatusChanged = false;
-        if (Asserts.isNull(jobInfoDetail.getJobHistory().getJob())) {
+        if (Asserts.isNull(jobInfoDetail.getJobHistory().getJob()) || jobInfoDetail.getJobHistory().isError()) {
             jobInfoDetail.getInstance().setStatus(JobStatus.UNKNOWN.getValue());
         } else {
             jobInfoDetail.getInstance().setDuration(jobInfoDetail.getJobHistory().getJob().get(FlinkRestResultConstant.JOB_DURATION).asLong() / 1000);

File: dlink-admin/src/main/java/com/dlink/utils/CopyrightUtil.java
Patch:
@@ -19,7 +19,8 @@
 
 package com.dlink.utils;
 
-import org.apache.log4j.Logger;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 import java.io.BufferedReader;
 import java.io.File;
@@ -34,7 +35,7 @@
  * copyright : 版权信息 Apache LICENSE-2.0
  */
 public class CopyrightUtil {
-	private static Logger log = Logger.getLogger(CopyrightUtil.class);
+	private static Logger log = LoggerFactory.getLogger(CopyrightUtil.class);
 	public  static String dir = "d:\\your-dir";
 	public  static String fileEndWithStr = ".less";
 	public  static String copyright = "/*\n" +

File: dlink-client/dlink-client-base/src/main/java/com/dlink/sql/FlinkQuery.java
Patch:
@@ -6,7 +6,7 @@
  * @author wenmo
  * @since 2022/7/18 18:43
  **/
-public abstract class AbstractFlinkQuery {
+public class FlinkQuery {
 
     public static String separator() {
         return ";\r\n";

File: dlink-admin/src/main/java/com/dlink/service/impl/UserServiceImpl.java
Patch:
@@ -100,7 +100,7 @@ public Result loginUser(String username, String password, boolean isRemember) {
 
     @Override
     public User getUserByUsername(String username) {
-        User user = getOne(new QueryWrapper<User>().eq("username", username).eq("is_delete", 0));
+        User user = getOne(new QueryWrapper<User>().eq("username", username));
         if (Asserts.isNotNull(user)) {
             user.setIsAdmin(Asserts.isEqualsIgnoreCase(username, "admin"));
         }

File: dlink-admin/src/main/java/com/dlink/service/TaskService.java
Patch:
@@ -10,7 +10,6 @@
 import com.dlink.model.JobInfoDetail;
 import com.dlink.model.JobInstance;
 import com.dlink.model.Task;
-import com.dlink.model.TaskVersion;
 import com.dlink.result.SqlExplainResult;
 
 /**
@@ -35,6 +34,8 @@ public interface TaskService extends ISuperService<Task> {
 
     List<Task> listFlinkSQLEnv();
 
+    Task initDefaultFlinkSQLEnv();
+
     String exportSql(Integer id);
 
     Task getUDFByClassName(String className);

File: dlink-connectors/dlink-connector-phoenix-1.13/src/main/java/org/apache/flink/connector/phoenix/internal/executor/TableSimpleStatementExecutor.java
Patch:
@@ -61,7 +61,8 @@ public void addToBatch(RowData record) throws SQLException {
 
     @Override
     public void executeBatch(Connection conn) throws SQLException {
-        st.executeBatch();
+        //st.executeBatch();
+        conn.commit();
     }
 
     @Override

File: dlink-connectors/dlink-connector-phoenix-1.13/src/main/java/org/apache/flink/connector/phoenix/table/PhoenixJdbcDynamicOutputFormatBuilder.java
Patch:
@@ -87,7 +87,7 @@ public PhoenixJdbcDynamicOutputFormatBuilder setFieldDataTypes(DataType[] fieldD
         if (dmlOptions.getKeyFields().isPresent() && dmlOptions.getKeyFields().get().length > 0) {
             // upsert query
             return new JdbcBatchingOutputFormat<>(
-                    new PhoneixJdbcConnectionProvider(jdbcOptions),
+                    new PhoneixJdbcConnectionProvider(jdbcOptions,jdbcOptions.getNamespaceMappingEnabled(),jdbcOptions.getMapSystemTablesToNamespace()),
                     executionOptions,
                     ctx ->
                             createBufferReduceExecutor(
@@ -101,7 +101,7 @@ public PhoenixJdbcDynamicOutputFormatBuilder setFieldDataTypes(DataType[] fieldD
                             .getInsertIntoStatement(
                                     dmlOptions.getTableName(), dmlOptions.getFieldNames());
             return new JdbcBatchingOutputFormat<>(
-                    new PhoneixJdbcConnectionProvider(jdbcOptions),
+                    new PhoneixJdbcConnectionProvider(jdbcOptions,jdbcOptions.getNamespaceMappingEnabled(),jdbcOptions.getMapSystemTablesToNamespace()),
                     executionOptions,
                     ctx ->
                             createSimpleBufferedExecutor(

File: dlink-connectors/dlink-connector-phoenix-1.13/src/main/java/org/apache/flink/connector/phoenix/table/PhoenixRowDataLookupFunction.java
Patch:
@@ -52,7 +52,7 @@
 import static org.apache.flink.util.Preconditions.checkArgument;
 import static org.apache.flink.util.Preconditions.checkNotNull;
 
-/** A lookup function for {@link JdbcDynamicTableSource}. */
+/** A lookup function for {@link PhoenixDynamicTableSource}. */
 @Internal
 public class PhoenixRowDataLookupFunction extends TableFunction<RowData> {
 
@@ -208,6 +208,7 @@ public void eval(Object... keys) {
     private void establishConnectionAndStatement() throws SQLException, ClassNotFoundException {
         Connection dbConn = connectionProvider.getOrEstablishConnection();
         statement = FieldNamedPreparedStatement.prepareStatement(dbConn, query, keyNames);
+        LOG.info("executor query SQL : "+query);
     }
 
     @Override

File: dlink-connectors/dlink-connector-phoenix-1.14/src/main/java/org/apache/flink/connector/phoenix/internal/executor/TableSimpleStatementExecutor.java
Patch:
@@ -61,7 +61,8 @@ public void addToBatch(RowData record) throws SQLException {
 
     @Override
     public void executeBatch(Connection conn) throws SQLException {
-        st.executeBatch();
+        //st.executeBatch();
+        conn.commit();
     }
 
     @Override

File: dlink-connectors/dlink-connector-phoenix-1.14/src/main/java/org/apache/flink/connector/phoenix/table/PhoenixDynamicTableFactory.java
Patch:
@@ -52,8 +52,8 @@ public class PhoenixDynamicTableFactory implements DynamicTableSourceFactory, Dy
     private static final ConfigOption<Duration> SINK_BUFFER_FLUSH_INTERVAL = ConfigOptions.key("sink.buffer-flush.interval").durationType().defaultValue(Duration.ofSeconds(1L)).withDescription("The flush interval mills, over this time, asynchronous threads will flush data.");
     private static final ConfigOption<Integer> SINK_MAX_RETRIES = ConfigOptions.key("sink.max-retries").intType().defaultValue(3).withDescription("The max retry times if writing records to database failed.");
 
-    public static final ConfigOption<Boolean> SCHEMA_NAMESPACE_MAPPING_ENABLE = ConfigOptions.key("phoenix.schema.isNamespaceMappingEnabled").booleanType().defaultValue(false).withDescription("The JDBC phoenix Schema isNamespaceMappingEnabled.");
-    public static final ConfigOption<Boolean> SCHEMA_MAP_SYSTEMTABLE_ENABLE = ConfigOptions.key("phoenix.schema.mapSystemTablesToNamespace").booleanType().defaultValue(false).withDescription("The JDBC phoenix mapSystemTablesToNamespace.");
+    public static final ConfigOption<Boolean> SCHEMA_NAMESPACE_MAPPING_ENABLE = ConfigOptions.key("phoenix.schema.isnamespacemappingenabled").booleanType().defaultValue(false).withDescription("The JDBC phoenix Schema isNamespaceMappingEnabled.");
+    public static final ConfigOption<Boolean> SCHEMA_MAP_SYSTEMTABLE_ENABLE = ConfigOptions.key("phoenix.schema.mapsystemtablestonamespace").booleanType().defaultValue(false).withDescription("The JDBC phoenix mapSystemTablesToNamespace.");
 
     @Override
     public DynamicTableSink createDynamicTableSink(Context context) {
@@ -73,7 +73,6 @@ public DynamicTableSource createDynamicTableSource(Context context) {
         helper.validate();
         this.validateConfigOptions(config);
         TableSchema physicalSchema = TableSchemaUtils.getPhysicalSchema(context.getCatalogTable().getSchema());
-        //return new JdbcDynamicTableSource(this.getJdbcOptions(helper.getOptions()), this.getJdbcReadOptions(helper.getOptions()), this.getJdbcLookupOptions(helper.getOptions()), physicalSchema);
         return new PhoenixDynamicTableSource(this.getJdbcOptions(helper.getOptions()), this.getJdbcReadOptions(helper.getOptions()), this.getJdbcLookupOptions(helper.getOptions()),physicalSchema);
 
     }

File: dlink-connectors/dlink-connector-phoenix-1.14/src/main/java/org/apache/flink/connector/phoenix/table/PhoenixJdbcDynamicOutputFormatBuilder.java
Patch:
@@ -87,7 +87,7 @@ public PhoenixJdbcDynamicOutputFormatBuilder setFieldDataTypes(DataType[] fieldD
         if (dmlOptions.getKeyFields().isPresent() && dmlOptions.getKeyFields().get().length > 0) {
             // upsert query
             return new JdbcBatchingOutputFormat<>(
-                    new PhoneixJdbcConnectionProvider(jdbcOptions),
+                    new PhoneixJdbcConnectionProvider(jdbcOptions,jdbcOptions.getNamespaceMappingEnabled(),jdbcOptions.getMapSystemTablesToNamespace()),
                     executionOptions,
                     ctx ->
                             createBufferReduceExecutor(
@@ -101,7 +101,7 @@ public PhoenixJdbcDynamicOutputFormatBuilder setFieldDataTypes(DataType[] fieldD
                             .getInsertIntoStatement(
                                     dmlOptions.getTableName(), dmlOptions.getFieldNames());
             return new JdbcBatchingOutputFormat<>(
-                    new PhoneixJdbcConnectionProvider(jdbcOptions),
+                    new PhoneixJdbcConnectionProvider(jdbcOptions,jdbcOptions.getNamespaceMappingEnabled(),jdbcOptions.getMapSystemTablesToNamespace()),
                     executionOptions,
                     ctx ->
                             createSimpleBufferedExecutor(

File: dlink-connectors/dlink-connector-phoenix-1.14/src/main/java/org/apache/flink/connector/phoenix/table/PhoenixRowDataLookupFunction.java
Patch:
@@ -52,7 +52,7 @@
 import static org.apache.flink.util.Preconditions.checkArgument;
 import static org.apache.flink.util.Preconditions.checkNotNull;
 
-/** A lookup function for {@link JdbcDynamicTableSource}. */
+/** A lookup function for {@link PhoenixRowDataLookupFunction}. */
 @Internal
 public class PhoenixRowDataLookupFunction extends TableFunction<RowData> {
 
@@ -208,6 +208,7 @@ public void eval(Object... keys) {
     private void establishConnectionAndStatement() throws SQLException, ClassNotFoundException {
         Connection dbConn = connectionProvider.getOrEstablishConnection();
         statement = FieldNamedPreparedStatement.prepareStatement(dbConn, query, keyNames);
+        LOG.info("executor query SQL : "+query);
     }
 
     @Override

File: dlink-admin/src/main/java/com/dlink/service/JobInstanceService.java
Patch:
@@ -26,6 +26,8 @@ public interface JobInstanceService extends ISuperService<JobInstance> {
 
     JobInfoDetail getJobInfoDetailInfo(JobInstance jobInstance);
 
+    JobInfoDetail refreshJobInfoDetailInfo(JobInstance jobInstance);
+
     LineageResult getLineage(Integer id);
 
     JobInstance getJobInstanceByTaskId(Integer id);

File: dlink-core/src/main/java/com/dlink/explainer/trans/TransGenerator.java
Patch:
@@ -58,7 +58,7 @@ private void setParentId(Map<Integer, Trans> nodemap) {
         for (Map.Entry<Integer, Trans> entry : nodemap.entrySet()) {
             Trans trans = entry.getValue();
             List<Predecessor> predecessors = trans.getPredecessors();
-            if (Asserts.isNull(predecessors)) {
+            if (Asserts.isNullCollection(predecessors)) {
                 continue;
             }
             for (int i = 0; i < predecessors.size(); i++) {

File: dlink-core/src/main/java/com/dlink/explainer/trans/TransGenerator.java
Patch:
@@ -58,7 +58,7 @@ private void setParentId(Map<Integer, Trans> nodemap) {
         for (Map.Entry<Integer, Trans> entry : nodemap.entrySet()) {
             Trans trans = entry.getValue();
             List<Predecessor> predecessors = trans.getPredecessors();
-            if (Asserts.isNull(predecessors)) {
+            if (Asserts.isNullCollection(predecessors)) {
                 continue;
             }
             for (int i = 0; i < predecessors.size(); i++) {

File: dlink-core/src/main/java/com/dlink/explainer/Explainer.java
Patch:
@@ -212,7 +212,7 @@ record = executor.explainSqlRecord(item.getValue());
                 for (StatementParam item : jobParam.getTrans()) {
                     SqlExplainResult record = new SqlExplainResult();
                     try {
-                        record.setExplain(executor.explainSql(item.getValue()));
+                        record = executor.explainSqlRecord(item.getValue());
                         record.setParseTrue(true);
                         record.setExplainTrue(true);
                     } catch (Exception e) {

File: dlink-core/src/main/java/com/dlink/explainer/Explainer.java
Patch:
@@ -212,7 +212,7 @@ record = executor.explainSqlRecord(item.getValue());
                 for (StatementParam item : jobParam.getTrans()) {
                     SqlExplainResult record = new SqlExplainResult();
                     try {
-                        record.setExplain(executor.explainSql(item.getValue()));
+                        record = executor.explainSqlRecord(item.getValue());
                         record.setParseTrue(true);
                         record.setExplainTrue(true);
                     } catch (Exception e) {

File: dlink-admin/src/main/java/com/dlink/service/impl/TaskServiceImpl.java
Patch:
@@ -525,7 +525,7 @@ public boolean savepointTask(Integer taskId, String savePointType) {
 
     private JobConfig buildJobConfig(Task task) {
         boolean isJarTask = Dialect.FLINKJAR.equalsVal(task.getDialect());
-        if (!isJarTask && task.getFragment()) {
+        if (!isJarTask && Asserts.isNotNull(task.getFragment())?task.getFragment():false) {
             String flinkWithSql = dataBaseService.getEnabledFlinkWithSql();
             if (Asserts.isNotNullString(flinkWithSql)) {
                 task.setStatement(flinkWithSql + "\r\n" + task.getStatement());

File: dlink-admin/src/main/java/com/dlink/service/impl/TaskServiceImpl.java
Patch:
@@ -525,7 +525,7 @@ public boolean savepointTask(Integer taskId, String savePointType) {
 
     private JobConfig buildJobConfig(Task task) {
         boolean isJarTask = Dialect.FLINKJAR.equalsVal(task.getDialect());
-        if (!isJarTask && task.getFragment()) {
+        if (!isJarTask && Asserts.isNotNull(task.getFragment())?task.getFragment():false) {
             String flinkWithSql = dataBaseService.getEnabledFlinkWithSql();
             if (Asserts.isNotNullString(flinkWithSql)) {
                 task.setStatement(flinkWithSql + "\r\n" + task.getStatement());

File: dlink-core/src/main/java/com/dlink/job/JobConfig.java
Patch:
@@ -168,9 +168,7 @@ public void buildGatewayConfig(Map<String, Object> config) {
                 config.get("flinkLibPath").toString(),
                 config.get("hadoopConfigPath").toString()));
         } else {
-            gatewayConfig.setClusterConfig(ClusterConfig.build(config.get("flinkConfigPath").toString(),
-                config.get("flinkLibPath").toString(),
-                ""));
+            gatewayConfig.setClusterConfig(ClusterConfig.build(config.get("flinkConfigPath").toString()));
         }
         AppConfig appConfig = new AppConfig();
         if (config.containsKey("userJarPath") && Asserts.isNotNullString((String) config.get("userJarPath"))) {

File: dlink-core/src/main/java/com/dlink/job/JobConfig.java
Patch:
@@ -168,9 +168,7 @@ public void buildGatewayConfig(Map<String, Object> config) {
                 config.get("flinkLibPath").toString(),
                 config.get("hadoopConfigPath").toString()));
         } else {
-            gatewayConfig.setClusterConfig(ClusterConfig.build(config.get("flinkConfigPath").toString(),
-                config.get("flinkLibPath").toString(),
-                ""));
+            gatewayConfig.setClusterConfig(ClusterConfig.build(config.get("flinkConfigPath").toString()));
         }
         AppConfig appConfig = new AppConfig();
         if (config.containsKey("userJarPath") && Asserts.isNotNullString((String) config.get("userJarPath"))) {

File: dlink-admin/src/main/java/com/dlink/service/impl/DataBaseServiceImpl.java
Patch:
@@ -129,6 +129,7 @@ public SqlGeneration getSqlGeneration(Integer id, String schemaName, String tabl
         sqlGeneration.setFlinkSqlCreate(table.getFlinkTableSql(dataBase.getName(), dataBase.getFlinkTemplate()));
         sqlGeneration.setSqlSelect(table.getSqlSelect(dataBase.getName()));
         sqlGeneration.setSqlCreate(driver.getCreateTableSql(table));
+        driver.close();
         return sqlGeneration;
     }
 

File: dlink-admin/src/main/java/com/dlink/service/impl/DataBaseServiceImpl.java
Patch:
@@ -129,6 +129,7 @@ public SqlGeneration getSqlGeneration(Integer id, String schemaName, String tabl
         sqlGeneration.setFlinkSqlCreate(table.getFlinkTableSql(dataBase.getName(), dataBase.getFlinkTemplate()));
         sqlGeneration.setSqlSelect(table.getSqlSelect(dataBase.getName()));
         sqlGeneration.setSqlCreate(driver.getCreateTableSql(table));
+        driver.close();
         return sqlGeneration;
     }
 

File: dlink-alert/dlink-alert-feishu/src/main/java/com.dlink.alert.feishu/FeiShuConstants.java
Patch:
@@ -8,7 +8,7 @@
 public final class FeiShuConstants {
     static final String TYPE = "FeiShu";
     static final String MARKDOWN_QUOTE = "> ";
-    static final String MARKDOWN_ENTER = "\n";
+    static final String MARKDOWN_ENTER = "/n";
     static final String WEB_HOOK = "webhook";
     static final String KEY_WORD = "keyword";
     static final String SECRET = "secret";
@@ -21,7 +21,7 @@ public final class FeiShuConstants {
     static final String AT_ALL = "isAtAll";
     static final String AT_USERS = "users";
     static final String FEI_SHU_TEXT_TEMPLATE = "{\"msg_type\":\"{msg_type}\",\"content\":{\"{msg_type}\":\"{msg} {users} \" }}";
-    static final String FEI_SHU_POST_TEMPLATE ="{\"msg_type\":\"{msg_type}\",\"content\":{\"{msg_type}\":{\"zh_cn\":{\"title\":\"{keyword}\",\"content\":[[{\"tag\":\"text\",\"text\":\"{msg}\"},{users}]]}}}}";
+    static final String FEI_SHU_POST_TEMPLATE ="{\"msg_type\":\"{msg_type}\",\"content\":{\"{msg_type}\":{\"zh_cn\":{\"title\":\"{keyword}\",\"content\":[[{\"tag\":\"text\",\"un_escape\": true,\"text\":\"{msg}\"},{users}]]}}}}";
 
     private FeiShuConstants() {
         throw new UnsupportedOperationException("This is a utility class and cannot be instantiated");

File: dlink-alert/dlink-alert-feishu/src/main/java/com.dlink.alert.feishu/FeiShuSender.java
Patch:
@@ -138,14 +138,13 @@ public static String formatContent(AlertMsg alertMsg) {
             }
 
             StringBuilder contents = new StringBuilder(100);
-            contents.append(String.format("`%s`/n ", alertMsg.getName()));
+            contents.append(String.format("`%s`"+FeiShuConstants.MARKDOWN_ENTER, alertMsg.getName()));
             for (Map map : list) {
                 for (Entry<String, Object> entry : (Iterable<Entry<String, Object>>) map.entrySet()) {
                     String key = entry.getKey();
                     String value = entry.getValue().toString();
                     contents.append(FeiShuConstants.MARKDOWN_QUOTE);
-                    contents.append(key + "：" + value);
-                    contents.append(" /n ");
+                    contents.append(key + "：" + value).append(FeiShuConstants.MARKDOWN_ENTER);
                 }
             }
             return contents.toString();

File: dlink-alert/dlink-alert-feishu/src/main/java/com.dlink.alert.feishu/FeiShuConstants.java
Patch:
@@ -8,7 +8,7 @@
 public final class FeiShuConstants {
     static final String TYPE = "FeiShu";
     static final String MARKDOWN_QUOTE = "> ";
-    static final String MARKDOWN_ENTER = "\n";
+    static final String MARKDOWN_ENTER = "/n";
     static final String WEB_HOOK = "webhook";
     static final String KEY_WORD = "keyword";
     static final String SECRET = "secret";
@@ -21,7 +21,7 @@ public final class FeiShuConstants {
     static final String AT_ALL = "isAtAll";
     static final String AT_USERS = "users";
     static final String FEI_SHU_TEXT_TEMPLATE = "{\"msg_type\":\"{msg_type}\",\"content\":{\"{msg_type}\":\"{msg} {users} \" }}";
-    static final String FEI_SHU_POST_TEMPLATE ="{\"msg_type\":\"{msg_type}\",\"content\":{\"{msg_type}\":{\"zh_cn\":{\"title\":\"{keyword}\",\"content\":[[{\"tag\":\"text\",\"text\":\"{msg}\"},{users}]]}}}}";
+    static final String FEI_SHU_POST_TEMPLATE ="{\"msg_type\":\"{msg_type}\",\"content\":{\"{msg_type}\":{\"zh_cn\":{\"title\":\"{keyword}\",\"content\":[[{\"tag\":\"text\",\"un_escape\": true,\"text\":\"{msg}\"},{users}]]}}}}";
 
     private FeiShuConstants() {
         throw new UnsupportedOperationException("This is a utility class and cannot be instantiated");

File: dlink-alert/dlink-alert-feishu/src/main/java/com.dlink.alert.feishu/FeiShuSender.java
Patch:
@@ -138,14 +138,13 @@ public static String formatContent(AlertMsg alertMsg) {
             }
 
             StringBuilder contents = new StringBuilder(100);
-            contents.append(String.format("`%s`/n ", alertMsg.getName()));
+            contents.append(String.format("`%s`"+FeiShuConstants.MARKDOWN_ENTER, alertMsg.getName()));
             for (Map map : list) {
                 for (Entry<String, Object> entry : (Iterable<Entry<String, Object>>) map.entrySet()) {
                     String key = entry.getKey();
                     String value = entry.getValue().toString();
                     contents.append(FeiShuConstants.MARKDOWN_QUOTE);
-                    contents.append(key + "：" + value);
-                    contents.append(" /n ");
+                    contents.append(key + "：" + value).append(FeiShuConstants.MARKDOWN_ENTER);
                 }
             }
             return contents.toString();

File: dlink-metadata/dlink-metadata-oracle/src/main/java/com/dlink/metadata/convert/OracleTypeConvert.java
Patch:
@@ -28,7 +28,7 @@ public ColumnType convert(Column column) {
             if (t.matches("number\\(+\\d\\)")) {
                 columnType = ColumnType.INTEGER;
             } else if (t.matches("number\\(+\\d{2}+\\)")) {
-                columnType = ColumnType.LONG;
+                columnType = ColumnType.JAVA_LANG_LONG;
             } else {
                 columnType = ColumnType.DECIMAL;
             }

File: dlink-metadata/dlink-metadata-oracle/src/main/java/com/dlink/metadata/convert/OracleTypeConvert.java
Patch:
@@ -28,7 +28,7 @@ public ColumnType convert(Column column) {
             if (t.matches("number\\(+\\d\\)")) {
                 columnType = ColumnType.INTEGER;
             } else if (t.matches("number\\(+\\d{2}+\\)")) {
-                columnType = ColumnType.LONG;
+                columnType = ColumnType.JAVA_LANG_LONG;
             } else {
                 columnType = ColumnType.DECIMAL;
             }

File: dlink-client/dlink-client-base/src/main/java/com/dlink/model/FlinkCDCConfig.java
Patch:
@@ -158,6 +158,7 @@ private boolean skip(String key) {
             case "table.suffix":
             case "table.upper":
             case "table.lower":
+            case "column.replace.line-break":
                 return true;
             default:
                 return false;

File: dlink-client/dlink-client-base/src/main/java/com/dlink/model/FlinkCDCConfig.java
Patch:
@@ -158,6 +158,7 @@ private boolean skip(String key) {
             case "table.suffix":
             case "table.upper":
             case "table.lower":
+            case "column.replace.line-break":
                 return true;
             default:
                 return false;

File: dlink-admin/src/main/java/com/dlink/controller/StudioController.java
Patch:
@@ -65,7 +65,8 @@ public Result getJobPlan(@RequestBody StudioExecuteDTO studioExecuteDTO) {
         try {
             return Result.succeed(studioService.getJobPlan(studioExecuteDTO), "获取作业计划成功");
         } catch (Exception e) {
-            return Result.failed("目前只支持获取 INSERT 语句的作业计划");
+            e.printStackTrace();
+            return Result.failed(e.getMessage());
         }
     }
 

File: dlink-admin/src/main/java/com/dlink/controller/StudioController.java
Patch:
@@ -65,7 +65,8 @@ public Result getJobPlan(@RequestBody StudioExecuteDTO studioExecuteDTO) {
         try {
             return Result.succeed(studioService.getJobPlan(studioExecuteDTO), "获取作业计划成功");
         } catch (Exception e) {
-            return Result.failed("目前只支持获取 INSERT 语句的作业计划");
+            e.printStackTrace();
+            return Result.failed(e.getMessage());
         }
     }
 

File: dlink-client/dlink-client-1.13/src/main/java/com/dlink/cdc/hudi/HudiSinkBuilder.java
Patch:
@@ -32,7 +32,7 @@
  */
 public class HudiSinkBuilder extends AbstractSinkBuilder implements SinkBuilder, Serializable {
 
-    private final static String KEY_WORD = "hudi";
+    private final static String KEY_WORD = "datastream-hudi";
     private static final long serialVersionUID = 5324199407472847422L;
 
     public HudiSinkBuilder() {

File: dlink-client/dlink-client-1.13/src/main/java/com/dlink/cdc/hudi/HudiSinkBuilder.java
Patch:
@@ -32,7 +32,7 @@
  */
 public class HudiSinkBuilder extends AbstractSinkBuilder implements SinkBuilder, Serializable {
 
-    private final static String KEY_WORD = "hudi";
+    private final static String KEY_WORD = "datastream-hudi";
     private static final long serialVersionUID = 5324199407472847422L;
 
     public HudiSinkBuilder() {

File: dlink-app/dlink-app-base/src/main/java/com/dlink/app/flinksql/Submiter.java
Patch:
@@ -41,7 +41,7 @@ private static String getTaskInfo(Integer id) throws SQLException {
         }
         return "select id, name, alias as jobName, type,check_point as checkpoint," +
                 "save_point_path as savePointPath, parallelism,fragment as useSqlFragment,statement_set as useStatementSet,config_json as config," +
-                " env_id as envId from dlink_task where id = " + id;
+                " env_id as envId,batch_model AS useBatchModel from dlink_task where id = " + id;
     }
 
     private static String getFlinkSQLStatement(Integer id, DBConfig config) {

File: dlink-executor/src/main/java/com/dlink/executor/AppBatchExecutor.java
Patch:
@@ -18,7 +18,7 @@ public AppBatchExecutor(ExecutorSetting executorSetting) {
         this.executorSetting = executorSetting;
         if (Asserts.isNotNull(executorSetting.getConfig())) {
             Configuration configuration = Configuration.fromMap(executorSetting.getConfig());
-            this.environment = StreamExecutionEnvironment.createLocalEnvironment(configuration);
+            this.environment = StreamExecutionEnvironment.getExecutionEnvironment(configuration);
         } else {
             this.environment = StreamExecutionEnvironment.createLocalEnvironment();
         }

File: dlink-app/dlink-app-base/src/main/java/com/dlink/app/flinksql/Submiter.java
Patch:
@@ -41,7 +41,7 @@ private static String getTaskInfo(Integer id) throws SQLException {
         }
         return "select id, name, alias as jobName, type,check_point as checkpoint," +
                 "save_point_path as savePointPath, parallelism,fragment as useSqlFragment,statement_set as useStatementSet,config_json as config," +
-                " env_id as envId from dlink_task where id = " + id;
+                " env_id as envId,batch_model AS useBatchModel from dlink_task where id = " + id;
     }
 
     private static String getFlinkSQLStatement(Integer id, DBConfig config) {

File: dlink-executor/src/main/java/com/dlink/executor/AppBatchExecutor.java
Patch:
@@ -18,7 +18,7 @@ public AppBatchExecutor(ExecutorSetting executorSetting) {
         this.executorSetting = executorSetting;
         if (Asserts.isNotNull(executorSetting.getConfig())) {
             Configuration configuration = Configuration.fromMap(executorSetting.getConfig());
-            this.environment = StreamExecutionEnvironment.createLocalEnvironment(configuration);
+            this.environment = StreamExecutionEnvironment.getExecutionEnvironment(configuration);
         } else {
             this.environment = StreamExecutionEnvironment.createLocalEnvironment();
         }

File: dlink-admin/src/main/java/com/dlink/model/AlertInstance.java
Patch:
@@ -19,7 +19,5 @@ public class AlertInstance extends SuperEntity {
 
     private String type;
 
-    private String name;
-
     private String params;
 }

File: dlink-admin/src/main/java/com/dlink/model/AlertInstance.java
Patch:
@@ -19,5 +19,7 @@ public class AlertInstance extends SuperEntity {
 
     private String type;
 
+    private String name;
+
     private String params;
 }

File: dlink-alert/dlink-alert-wechat/src/main/java/com/dlink/alert/wechat/WeChatSender.java
Patch:
@@ -81,7 +81,7 @@ public AlertResult send(String title, String content) {
 
         String data ="";
         if (sendType.equals(WeChatType.CHAT.getValue())) {
-            data = markdownByAlert(KeyWord, content ,userList);;
+            data = markdownByAlert(title, content ,userList);;
         }else{
             data = markdownByAlert(title, content, userList);
         }

File: dlink-client/dlink-client-1.13/src/main/java/com/dlink/cdc/mysql/MysqlCDCBuilder.java
Patch:
@@ -85,8 +85,6 @@ public DataStreamSource<String> build(StreamExecutionEnvironment env) {
                 case "LATEST":
                     sourceBuilder.startupOptions(StartupOptions.latest());
                     break;
-                default:
-                    sourceBuilder.startupOptions(StartupOptions.latest());
             }
         } else {
             sourceBuilder.startupOptions(StartupOptions.latest());

File: dlink-client/dlink-client-1.13/src/main/java/com/dlink/cdc/oracle/OracleCDCBuilder.java
Patch:
@@ -13,7 +13,6 @@
 import com.dlink.cdc.CDCBuilder;
 import com.dlink.constant.ClientConstant;
 import com.dlink.model.FlinkCDCConfig;
-import com.dlink.model.Table;
 import com.ververica.cdc.connectors.oracle.OracleSource;
 import com.ververica.cdc.connectors.oracle.table.StartupOptions;
 import com.ververica.cdc.debezium.JsonDebeziumDeserializationSchema;
@@ -78,8 +77,6 @@ public DataStreamSource<String> build(StreamExecutionEnvironment env) {
                 case "LATEST":
                     sourceBuilder.startupOptions(StartupOptions.latest());
                     break;
-                default:
-                    sourceBuilder.startupOptions(StartupOptions.latest());
             }
         } else {
             sourceBuilder.startupOptions(StartupOptions.latest());

File: dlink-client/dlink-client-base/src/main/java/com/dlink/model/FlinkCDCConfig.java
Patch:
@@ -133,7 +133,7 @@ public Map<String, String> getSink() {
 
     private boolean skip(String key) {
         switch (key) {
-            case "db":
+            case "sink.db":
             case "table.prefix":
             case "table.suffix":
             case "table.upper":

File: dlink-metadata/dlink-metadata-oracle/src/main/java/com/dlink/metadata/query/OracleQuery.java
Patch:
@@ -24,7 +24,8 @@ public String columnsSql(String schemaName, String tableName) {
                 + "(CASE WHEN A.DATA_PRECISION IS NULL THEN A.DATA_TYPE "
                 + "WHEN NVL(A.DATA_SCALE, 0) > 0 THEN A.DATA_TYPE||'('||A.DATA_PRECISION||','||A.DATA_SCALE||')' "
                 + "ELSE A.DATA_TYPE||'('||A.DATA_PRECISION||')' END) "
-                + "ELSE A.DATA_TYPE END DATA_TYPE, B.COMMENTS,A.NULLABLE,DECODE((select count(1) from all_constraints pc,all_cons_columns pcc"
+                + "ELSE A.DATA_TYPE END DATA_TYPE,A.DATA_PRECISION NUMERIC_PRECISION,A.DATA_SCALE NUMERIC_SCALE,"
+                + " B.COMMENTS,A.NULLABLE,DECODE((select count(1) from all_constraints pc,all_cons_columns pcc"
                 + "  where pcc.column_name = A.column_name"
                 + "  and pcc.constraint_name = pc.constraint_name"
                 + "  and pc.constraint_type ='P'"

File: dlink-client/dlink-client-1.13/src/main/java/com/dlink/cdc/mysql/MysqlCDCBuilder.java
Patch:
@@ -85,8 +85,6 @@ public DataStreamSource<String> build(StreamExecutionEnvironment env) {
                 case "LATEST":
                     sourceBuilder.startupOptions(StartupOptions.latest());
                     break;
-                default:
-                    sourceBuilder.startupOptions(StartupOptions.latest());
             }
         } else {
             sourceBuilder.startupOptions(StartupOptions.latest());

File: dlink-client/dlink-client-1.13/src/main/java/com/dlink/cdc/oracle/OracleCDCBuilder.java
Patch:
@@ -13,7 +13,6 @@
 import com.dlink.cdc.CDCBuilder;
 import com.dlink.constant.ClientConstant;
 import com.dlink.model.FlinkCDCConfig;
-import com.dlink.model.Table;
 import com.ververica.cdc.connectors.oracle.OracleSource;
 import com.ververica.cdc.connectors.oracle.table.StartupOptions;
 import com.ververica.cdc.debezium.JsonDebeziumDeserializationSchema;
@@ -78,8 +77,6 @@ public DataStreamSource<String> build(StreamExecutionEnvironment env) {
                 case "LATEST":
                     sourceBuilder.startupOptions(StartupOptions.latest());
                     break;
-                default:
-                    sourceBuilder.startupOptions(StartupOptions.latest());
             }
         } else {
             sourceBuilder.startupOptions(StartupOptions.latest());

File: dlink-client/dlink-client-base/src/main/java/com/dlink/model/FlinkCDCConfig.java
Patch:
@@ -133,7 +133,7 @@ public Map<String, String> getSink() {
 
     private boolean skip(String key) {
         switch (key) {
-            case "db":
+            case "sink.db":
             case "table.prefix":
             case "table.suffix":
             case "table.upper":

File: dlink-metadata/dlink-metadata-oracle/src/main/java/com/dlink/metadata/query/OracleQuery.java
Patch:
@@ -24,7 +24,8 @@ public String columnsSql(String schemaName, String tableName) {
                 + "(CASE WHEN A.DATA_PRECISION IS NULL THEN A.DATA_TYPE "
                 + "WHEN NVL(A.DATA_SCALE, 0) > 0 THEN A.DATA_TYPE||'('||A.DATA_PRECISION||','||A.DATA_SCALE||')' "
                 + "ELSE A.DATA_TYPE||'('||A.DATA_PRECISION||')' END) "
-                + "ELSE A.DATA_TYPE END DATA_TYPE, B.COMMENTS,A.NULLABLE,DECODE((select count(1) from all_constraints pc,all_cons_columns pcc"
+                + "ELSE A.DATA_TYPE END DATA_TYPE,A.DATA_PRECISION NUMERIC_PRECISION,A.DATA_SCALE NUMERIC_SCALE,"
+                + " B.COMMENTS,A.NULLABLE,DECODE((select count(1) from all_constraints pc,all_cons_columns pcc"
                 + "  where pcc.column_name = A.column_name"
                 + "  and pcc.constraint_name = pc.constraint_name"
                 + "  and pc.constraint_type ='P'"

File: dlink-metadata/dlink-metadata-clickhouse/src/main/java/com/dlink/metadata/query/ClickHouseQuery.java
Patch:
@@ -9,7 +9,7 @@
 public class ClickHouseQuery extends AbstractDBQuery {
     @Override
     public String schemaAllSql() {
-        return "show databases";
+        return "SELECT currentDatabase()";
     }
 
     @Override
@@ -25,7 +25,7 @@ public String columnsSql(String schemaName, String tableName) {
 
     @Override
     public String schemaName() {
-        return "db";
+        return "database";
     }
 
     @Override

File: dlink-admin/src/main/java/com/dlink/service/impl/TaskServiceImpl.java
Patch:
@@ -463,7 +463,7 @@ private boolean savepointJobInstance(Integer jobInstanceId, String savePointType
             return jobManager.cancel(jobId);
         }
         SavePointResult savePointResult = jobManager.savepoint(jobId, savePointType, null);
-        if (Asserts.isNotNull(savePointResult)) {
+        if (Asserts.isNotNull(savePointResult.getJobInfos())) {
             for (JobInfo item : savePointResult.getJobInfos()) {
                 if (Asserts.isEqualsIgnoreCase(jobId, item.getJobId()) && Asserts.isNotNull(jobConfig.getTaskId())) {
                     Savepoints savepoints = new Savepoints();

File: dlink-core/src/main/java/com/dlink/api/FlinkAPI.java
Patch:
@@ -97,6 +97,8 @@ public SavePointResult savepoints(String jobId, String savePointType) {
         Map<String, Object> paramMap = new HashMap<>();
         switch (type) {
             case CANCEL:
+                paramMap.put("cancel-job", true);
+                paramType = FlinkRestAPIConstant.SAVEPOINTS;
                 jobInfo.setStatus(JobInfo.JobStatus.CANCEL);
                 break;
             case STOP:

File: dlink-admin/src/main/java/com/dlink/service/impl/TaskServiceImpl.java
Patch:
@@ -463,7 +463,7 @@ private boolean savepointJobInstance(Integer jobInstanceId, String savePointType
             return jobManager.cancel(jobId);
         }
         SavePointResult savePointResult = jobManager.savepoint(jobId, savePointType, null);
-        if (Asserts.isNotNull(savePointResult)) {
+        if (Asserts.isNotNull(savePointResult.getJobInfos())) {
             for (JobInfo item : savePointResult.getJobInfos()) {
                 if (Asserts.isEqualsIgnoreCase(jobId, item.getJobId()) && Asserts.isNotNull(jobConfig.getTaskId())) {
                     Savepoints savepoints = new Savepoints();

File: dlink-core/src/main/java/com/dlink/api/FlinkAPI.java
Patch:
@@ -98,6 +98,7 @@ public SavePointResult savepoints(String jobId, String savePointType) {
         switch (type) {
             case CANCEL:
                 jobInfo.setStatus(JobInfo.JobStatus.CANCEL);
+                break;
             case STOP:
                 paramMap.put("drain", false);
                 paramType = FlinkRestAPIConstant.STOP;
@@ -127,7 +128,7 @@ public SavePointResult savepoints(String jobId, String savePointType) {
                     continue;
                 }
                 if (node.get("operation").has("failure-cause")) {
-                    String failureCause = node.get("operation").get("failure-cause").asText();
+                    String failureCause = node.get("operation").get("failure-cause").toString();
                     if (Asserts.isNotNullString(failureCause)) {
                         result.fail(failureCause);
                         break;

File: dlink-common/src/main/java/com/dlink/model/SystemConfiguration.java
Patch:
@@ -61,7 +61,7 @@ public static SystemConfiguration getInstances() {
             "sqlSeparator",
             "FlinkSQL语句分割符",
             ValueType.STRING,
-            ";",
+            ";\r\n",
             "Flink SQL 的语句分割符"
     );
 

File: dlink-common/src/main/java/com/dlink/model/JobLifeCycle.java
Patch:
@@ -34,15 +34,15 @@ public String getLabel() {
 
     public static JobLifeCycle get(Integer value) {
         for (JobLifeCycle item : JobLifeCycle.values()) {
-            if (item.getValue() == value) {
+            if (item.getValue().equals(value)) {
                 return item;
             }
         }
         return JobLifeCycle.UNKNOWN;
     }
 
     public boolean equalsValue(Integer step) {
-        if (value == step) {
+        if (value.equals(step)) {
             return true;
         }
         return false;

File: dlink-function/src/main/java/com/dlink/ud/udtaf/Top2WithRetract.java
Patch:
@@ -39,13 +39,13 @@ public void accumulate(Top2WithRetractAccumulator acc, Integer v) {
         }
     }
 
-    public void retract(Top2WithRetractAccumulator acc, Integer v){
-        if (v == acc.first) {
+    public void retract(Top2WithRetractAccumulator acc, Integer v) {
+        if (v.equals(acc.first)) {
             acc.oldFirst = acc.first;
             acc.oldSecond = acc.second;
             acc.first = acc.second;
             acc.second = Integer.MIN_VALUE;
-        } else if (v == acc.second) {
+        } else if (v.equals(acc.second)) {
             acc.oldSecond = acc.second;
             acc.second = Integer.MIN_VALUE;
         }

File: dlink-common/src/main/java/com/dlink/model/SystemConfiguration.java
Patch:
@@ -61,7 +61,7 @@ public static SystemConfiguration getInstances() {
             "sqlSeparator",
             "FlinkSQL语句分割符",
             ValueType.STRING,
-            ";",
+            ";\r\n",
             "Flink SQL 的语句分割符"
     );
 

File: dlink-common/src/main/java/com/dlink/model/JobLifeCycle.java
Patch:
@@ -34,15 +34,15 @@ public String getLabel() {
 
     public static JobLifeCycle get(Integer value) {
         for (JobLifeCycle item : JobLifeCycle.values()) {
-            if (item.getValue() == value) {
+            if (item.getValue().equals(value)) {
                 return item;
             }
         }
         return JobLifeCycle.UNKNOWN;
     }
 
     public boolean equalsValue(Integer step) {
-        if (value == step) {
+        if (value.equals(step)) {
             return true;
         }
         return false;

File: dlink-function/src/main/java/com/dlink/ud/udtaf/Top2WithRetract.java
Patch:
@@ -39,13 +39,13 @@ public void accumulate(Top2WithRetractAccumulator acc, Integer v) {
         }
     }
 
-    public void retract(Top2WithRetractAccumulator acc, Integer v){
-        if (v == acc.first) {
+    public void retract(Top2WithRetractAccumulator acc, Integer v) {
+        if (v.equals(acc.first)) {
             acc.oldFirst = acc.first;
             acc.oldSecond = acc.second;
             acc.first = acc.second;
             acc.second = Integer.MIN_VALUE;
-        } else if (v == acc.second) {
+        } else if (v.equals(acc.second)) {
             acc.oldSecond = acc.second;
             acc.second = Integer.MIN_VALUE;
         }

File: dlink-admin/src/main/java/com/dlink/controller/ClusterConfigurationController.java
Patch:
@@ -39,7 +39,7 @@ public class ClusterConfigurationController {
     @PutMapping
     public Result saveOrUpdate(@RequestBody ClusterConfiguration clusterConfiguration) {
         TestResult testResult = clusterConfigurationService.testGateway(clusterConfiguration);
-        clusterConfiguration.setAvailable(testResult.isAvailable());
+        clusterConfiguration.setIsAvailable(testResult.isAvailable());
         if (clusterConfigurationService.saveOrUpdate(clusterConfiguration)) {
             return Result.succeed(Asserts.isNotNull(clusterConfiguration.getId()) ? "修改成功" : "新增成功");
         } else {

File: dlink-admin/src/main/java/com/dlink/model/Cluster.java
Patch:
@@ -35,7 +35,7 @@ public class Cluster extends SuperEntity {
 
     private String note;
 
-    private boolean autoRegisters;
+    private Boolean autoRegisters;
 
     private Integer clusterConfigurationId;
 

File: dlink-admin/src/main/java/com/dlink/model/ClusterConfiguration.java
Patch:
@@ -33,7 +33,7 @@ public class ClusterConfiguration extends SuperEntity {
 
     private String configJson;
 
-    private boolean isAvailable;
+    private Boolean isAvailable;
 
     private String note;
 

File: dlink-admin/src/main/java/com/dlink/model/DataBase.java
Patch:
@@ -44,7 +44,7 @@ public class DataBase extends SuperEntity {
 
     private String dbVersion;
 
-    private boolean status;
+    private Boolean status;
 
     private LocalDateTime healthTime;
 

File: dlink-admin/src/main/java/com/dlink/model/Task.java
Patch:
@@ -44,11 +44,11 @@ public class Task extends SuperEntity {
 
     private Integer parallelism;
 
-    private boolean fragment;
+    private Boolean fragment;
 
-    private boolean statementSet;
+    private Boolean statementSet;
 
-    private boolean batchModel;
+    private Boolean batchModel;
 
     private Integer clusterId;
 

File: dlink-admin/src/main/java/com/dlink/model/User.java
Patch:
@@ -38,9 +38,9 @@ public class User implements Serializable {
 
     private String mobile;
 
-    private boolean enabled;
+    private Boolean enabled;
 
-    private boolean isDelete;
+    private Boolean isDelete;
 
     @TableField(fill = FieldFill.INSERT)
     private LocalDateTime createTime;
@@ -49,5 +49,5 @@ public class User implements Serializable {
     private LocalDateTime updateTime;
 
     @TableField(exist = false)
-    private boolean isAdmin;
+    private Boolean isAdmin;
 }

File: dlink-admin/src/main/java/com/dlink/service/impl/TaskServiceImpl.java
Patch:
@@ -487,7 +487,7 @@ public boolean savepointTask(Integer taskId, String savePointType) {
 
     private JobConfig buildJobConfig(Task task) {
         boolean isJarTask = Dialect.FLINKJAR.equalsVal(task.getDialect());
-        if (!isJarTask && task.isFragment()) {
+        if (!isJarTask && task.getFragment()) {
             String flinkWithSql = dataBaseService.getEnabledFlinkWithSql();
             if (Asserts.isNotNullString(flinkWithSql)) {
                 task.setStatement(flinkWithSql + "\r\n" + task.getStatement());

File: dlink-admin/src/main/java/com/dlink/controller/ClusterConfigurationController.java
Patch:
@@ -39,7 +39,7 @@ public class ClusterConfigurationController {
     @PutMapping
     public Result saveOrUpdate(@RequestBody ClusterConfiguration clusterConfiguration) {
         TestResult testResult = clusterConfigurationService.testGateway(clusterConfiguration);
-        clusterConfiguration.setAvailable(testResult.isAvailable());
+        clusterConfiguration.setIsAvailable(testResult.isAvailable());
         if (clusterConfigurationService.saveOrUpdate(clusterConfiguration)) {
             return Result.succeed(Asserts.isNotNull(clusterConfiguration.getId()) ? "修改成功" : "新增成功");
         } else {

File: dlink-admin/src/main/java/com/dlink/model/Cluster.java
Patch:
@@ -35,7 +35,7 @@ public class Cluster extends SuperEntity {
 
     private String note;
 
-    private boolean autoRegisters;
+    private Boolean autoRegisters;
 
     private Integer clusterConfigurationId;
 

File: dlink-admin/src/main/java/com/dlink/model/ClusterConfiguration.java
Patch:
@@ -33,7 +33,7 @@ public class ClusterConfiguration extends SuperEntity {
 
     private String configJson;
 
-    private boolean isAvailable;
+    private Boolean isAvailable;
 
     private String note;
 

File: dlink-admin/src/main/java/com/dlink/model/DataBase.java
Patch:
@@ -44,7 +44,7 @@ public class DataBase extends SuperEntity {
 
     private String dbVersion;
 
-    private boolean status;
+    private Boolean status;
 
     private LocalDateTime healthTime;
 

File: dlink-admin/src/main/java/com/dlink/model/Task.java
Patch:
@@ -44,11 +44,11 @@ public class Task extends SuperEntity {
 
     private Integer parallelism;
 
-    private boolean fragment;
+    private Boolean fragment;
 
-    private boolean statementSet;
+    private Boolean statementSet;
 
-    private boolean batchModel;
+    private Boolean batchModel;
 
     private Integer clusterId;
 

File: dlink-admin/src/main/java/com/dlink/model/User.java
Patch:
@@ -38,9 +38,9 @@ public class User implements Serializable {
 
     private String mobile;
 
-    private boolean enabled;
+    private Boolean enabled;
 
-    private boolean isDelete;
+    private Boolean isDelete;
 
     @TableField(fill = FieldFill.INSERT)
     private LocalDateTime createTime;
@@ -49,5 +49,5 @@ public class User implements Serializable {
     private LocalDateTime updateTime;
 
     @TableField(exist = false)
-    private boolean isAdmin;
+    private Boolean isAdmin;
 }

File: dlink-admin/src/main/java/com/dlink/service/impl/TaskServiceImpl.java
Patch:
@@ -487,7 +487,7 @@ public boolean savepointTask(Integer taskId, String savePointType) {
 
     private JobConfig buildJobConfig(Task task) {
         boolean isJarTask = Dialect.FLINKJAR.equalsVal(task.getDialect());
-        if (!isJarTask && task.isFragment()) {
+        if (!isJarTask && task.getFragment()) {
             String flinkWithSql = dataBaseService.getEnabledFlinkWithSql();
             if (Asserts.isNotNullString(flinkWithSql)) {
                 task.setStatement(flinkWithSql + "\r\n" + task.getStatement());

File: dlink-admin/src/main/java/com/dlink/controller/ClusterConfigurationController.java
Patch:
@@ -1,5 +1,6 @@
 package com.dlink.controller;
 
+import com.dlink.assertion.Asserts;
 import com.dlink.common.result.ProTableResult;
 import com.dlink.common.result.Result;
 import com.dlink.gateway.result.TestResult;
@@ -40,9 +41,9 @@ public Result saveOrUpdate(@RequestBody ClusterConfiguration clusterConfiguratio
         TestResult testResult = clusterConfigurationService.testGateway(clusterConfiguration);
         clusterConfiguration.setAvailable(testResult.isAvailable());
         if (clusterConfigurationService.saveOrUpdate(clusterConfiguration)) {
-            return Result.succeed("新增成功");
+            return Result.succeed(Asserts.isNotNull(clusterConfiguration.getId()) ? "修改成功" : "新增成功");
         } else {
-            return Result.failed("新增失败");
+            return Result.failed(Asserts.isNotNull(clusterConfiguration.getId()) ? "修改失败" : "新增失败");
         }
     }
 

File: dlink-admin/src/main/java/com/dlink/controller/ClusterConfigurationController.java
Patch:
@@ -1,5 +1,6 @@
 package com.dlink.controller;
 
+import com.dlink.assertion.Asserts;
 import com.dlink.common.result.ProTableResult;
 import com.dlink.common.result.Result;
 import com.dlink.gateway.result.TestResult;
@@ -40,9 +41,9 @@ public Result saveOrUpdate(@RequestBody ClusterConfiguration clusterConfiguratio
         TestResult testResult = clusterConfigurationService.testGateway(clusterConfiguration);
         clusterConfiguration.setAvailable(testResult.isAvailable());
         if (clusterConfigurationService.saveOrUpdate(clusterConfiguration)) {
-            return Result.succeed("新增成功");
+            return Result.succeed(Asserts.isNotNull(clusterConfiguration.getId()) ? "修改成功" : "新增成功");
         } else {
-            return Result.failed("新增失败");
+            return Result.failed(Asserts.isNotNull(clusterConfiguration.getId()) ? "修改失败" : "新增失败");
         }
     }
 

File: dlink-admin/src/main/java/com/dlink/exception/WebExceptionHandler.java
Patch:
@@ -1,8 +1,10 @@
 package com.dlink.exception;
 
 import cn.dev33.satoken.exception.NotLoginException;
+
 import com.dlink.common.result.Result;
 import com.dlink.model.CodeEnum;
+
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import org.springframework.web.bind.annotation.ControllerAdvice;
@@ -41,6 +43,6 @@ public Result notLoginException(NotLoginException e) {
     @ExceptionHandler
     public Result unknownException(Exception e) {
         logger.error("ERROR:", e);
-        return Result.failed("系统出现错误, 请联系平台管理员!");
+        return Result.failed(e.getMessage());
     }
 }

File: dlink-admin/src/main/java/com/dlink/exception/WebExceptionHandler.java
Patch:
@@ -1,8 +1,10 @@
 package com.dlink.exception;
 
 import cn.dev33.satoken.exception.NotLoginException;
+
 import com.dlink.common.result.Result;
 import com.dlink.model.CodeEnum;
+
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import org.springframework.web.bind.annotation.ControllerAdvice;
@@ -41,6 +43,6 @@ public Result notLoginException(NotLoginException e) {
     @ExceptionHandler
     public Result unknownException(Exception e) {
         logger.error("ERROR:", e);
-        return Result.failed("系统出现错误, 请联系平台管理员!");
+        return Result.failed(e.getMessage());
     }
 }

File: dlink-executor/src/main/java/com/dlink/executor/SqlManager.java
Patch:
@@ -200,7 +200,8 @@ private String replaceVariable(String statement) {
         while (m.find()) {
             String key = m.group(1);
             String value = this.getSqlFragment(key);
-            m.appendReplacement(sb, value == null ? "" : value);
+            m.appendReplacement(sb, "");
+            sb.append(value == null ? "" : value);
         }
         m.appendTail(sb);
         return sb.toString();

File: dlink-metadata/dlink-metadata-hive/src/main/java/com/dlink/metadata/driver/HiveDriver.java
Patch:
@@ -85,6 +85,7 @@ public List<Table> listTables(String schemaName) {
         IDBQuery dbQuery = getDBQuery();
         String sql = dbQuery.tablesSql(schemaName);
         try {
+            execute(String.format(HiveConstant.USE_DB, schemaName));
             preparedStatement = conn.prepareStatement(sql);
             results = preparedStatement.executeQuery();
             ResultSetMetaData metaData = results.getMetaData();
@@ -113,7 +114,7 @@ public List<Table> listTables(String schemaName) {
                     tableList.add(tableInfo);
                 }
             }
-        } catch (SQLException e) {
+        } catch (Exception e) {
             e.printStackTrace();
         } finally {
             close(preparedStatement, results);

File: dlink-metadata/dlink-metadata-hive/src/main/java/com/dlink/metadata/driver/HiveDriver.java
Patch:
@@ -85,6 +85,7 @@ public List<Table> listTables(String schemaName) {
         IDBQuery dbQuery = getDBQuery();
         String sql = dbQuery.tablesSql(schemaName);
         try {
+            execute(String.format(HiveConstant.USE_DB, schemaName));
             preparedStatement = conn.prepareStatement(sql);
             results = preparedStatement.executeQuery();
             ResultSetMetaData metaData = results.getMetaData();
@@ -113,7 +114,7 @@ public List<Table> listTables(String schemaName) {
                     tableList.add(tableInfo);
                 }
             }
-        } catch (SQLException e) {
+        } catch (Exception e) {
             e.printStackTrace();
         } finally {
             close(preparedStatement, results);

File: dlink-admin/src/main/java/com/dlink/service/AlertInstanceService.java
Patch:
@@ -16,5 +16,5 @@ public interface AlertInstanceService extends ISuperService<AlertInstance> {
 
     List<AlertInstance> listEnabledAll();
 
-    AlertResult getAlerTesttResult(AlertInstance alertInstance);
+    AlertResult testAlert(AlertInstance alertInstance);
 }

File: dlink-admin/src/main/java/com/dlink/service/AlertInstanceService.java
Patch:
@@ -16,5 +16,5 @@ public interface AlertInstanceService extends ISuperService<AlertInstance> {
 
     List<AlertInstance> listEnabledAll();
 
-    AlertResult getAlerTesttResult(AlertInstance alertInstance);
+    AlertResult testAlert(AlertInstance alertInstance);
 }

File: dlink-alert/dlink-alert-dingtalk/src/test/java/com/dlink/alert/dingtalk/DingTalkTest.java
Patch:
@@ -43,7 +43,7 @@ public class DingTalkTest {
     @Before
     public void initDingTalkConfig() {
 
-        config.put(DingTalkConstants.KEYWORD, "Dlinky-Fink 钉钉告警测试");
+        config.put(DingTalkConstants.KEYWORD, "Dinky-Fink 钉钉告警测试");
         config.put(DingTalkConstants.WEB_HOOK, "url");
         config.put(DingTalkConstants.MSG_TYPE, ShowType.MARKDOWN.getValue());
 
@@ -57,7 +57,7 @@ public void initDingTalkConfig() {
     public void sendMarkDownMsgTest() {
         AlertConfig config = AlertConfig.build("MarkDownTest", "DingTalk", DingTalkTest.config);
         Alert alert = Alert.build(config);
-        AlertResult result = alert.send("Dlinky钉钉告警测试", contentTest);
+        AlertResult result = alert.send("Dinky钉钉告警测试", contentTest);
         Assert.assertEquals(true, result.getSuccess());
     }
 
@@ -66,7 +66,7 @@ public void sendTextMsgTest() {
         config.put(DingTalkConstants.MSG_TYPE, ShowType.TEXT.getValue());
         AlertConfig config = AlertConfig.build("TextMsgTest", "DingTalk", DingTalkTest.config);
         Alert alert = Alert.build(config);
-        AlertResult result = alert.send("Dlinky钉钉告警测试", contentTest);
+        AlertResult result = alert.send("Dinky钉钉告警测试", contentTest);
         Assert.assertEquals(true, result.getSuccess());
     }
 

File: dlink-alert/dlink-alert-email/src/main/java/com/dlink/alert/email/EmailConstants.java
Patch:
@@ -78,7 +78,7 @@ public final class EmailConstants {
     public static final String HTML_HEADER_PREFIX = "<!DOCTYPE HTML PUBLIC '-//W3C//DTD HTML 4.01 Transitional//EN' 'http://www.w3.org/TR/html4/loose.dtd'>"
         + "<html>"
         + "<head>"
-        + "<title>Dlinky</title>"
+        + "<title>Dinky</title>"
         + "<meta name='Keywords' content=''>"
         + "<meta name='Description' content=''>"
         + "<style type=\"text/css\">"

File: dlink-alert/dlink-alert-email/src/test/java/com/dlink/alert/email/EmailSenderTest.java
Patch:
@@ -19,7 +19,7 @@ public class EmailSenderTest {
     private static Map<String, String> emailConfig = new HashMap<>();
     private static AlertTemplate alertTemplate;
 
-    String title = "Dlinky Email Alert";
+    String title = "Dinky Email Alert";
     String content = "[{\"id\":\"69\","
             + "\"name\":\"UserBehavior-0--1193959466\","
             + "\"Job name\": \"Start workflow\","

File: dlink-alert/dlink-alert-feishu/src/test/java/com.dlink.alert.feishu/FeiShuSenderTest.java
Patch:
@@ -40,7 +40,7 @@ public class FeiShuSenderTest {
     @Before
     public void initFeiShuConfig() {
         feiShuConfig.put(FeiShuConstants.WEB_HOOK, "https://open.feishu.cn/open-apis/bot/v2/hook/aea3cd7f13154854541dsadsadas08f2a9");
-        feiShuConfig.put(FeiShuConstants.KEY_WORD, "Dlinky 飞书WebHook 告警测试");
+        feiShuConfig.put(FeiShuConstants.KEY_WORD, "Dinky 飞书WebHook 告警测试");
         feiShuConfig.put(FeiShuConstants.MSG_TYPE,"text");
         feiShuConfig.put(FeiShuConstants.AT_ALL, "false");
         feiShuConfig.put(FeiShuConstants.AT_USERS, "user1,user2,user3");
@@ -49,7 +49,7 @@ public void initFeiShuConfig() {
     @Test
     public void testTextTypeSend() {
         AlertMsg alertMsg = new AlertMsg();
-        alertMsg.setName("Dlinky 飞书WebHook 告警测试");
+        alertMsg.setName("Dinky 飞书WebHook 告警测试");
         alertMsg.setContent(alertMsgContentTemplate);
         FeiShuSender feiShuSender = new FeiShuSender(feiShuConfig);
         AlertResult alertResult = feiShuSender.send(alertMsg.getName(),alertMsg.getContent());
@@ -60,7 +60,7 @@ public void testTextTypeSend() {
     public void testPostTypeSend() {
         feiShuConfig.put(FeiShuConstants.MSG_TYPE,"post");
         AlertMsg alertMsg = new AlertMsg();
-        alertMsg.setName("Dlinky 飞书WebHook 告警测试");
+        alertMsg.setName("Dinky 飞书WebHook 告警测试");
         alertMsg.setContent(alertMsgContentTemplate);
         FeiShuSender feiShuSender = new FeiShuSender(feiShuConfig);
         AlertResult alertResult = feiShuSender.send(alertMsg.getName(),alertMsg.getContent());

File: dlink-alert/dlink-alert-dingtalk/src/main/java/com/dlink/alert/dingtalk/DingTalkSender.java
Patch:
@@ -128,7 +128,7 @@ private String generateMsgJson(String title, String content) {
         items.put("msgtype", msgType);
         Map<String, Object> text = new HashMap<>();
         items.put(msgType, text);
-        if (ShowType.TABLE.getValue().equals(msgType)) {
+        if (ShowType.MARKDOWN.getValue().equals(msgType)) {
             generateMarkdownMsg(title, content, text);
         } else {
             generateTextMsg(title, content, text);

File: dlink-alert/dlink-alert-dingtalk/src/test/java/com/dlink/alert/dingtalk/DingTalkTest.java
Patch:
@@ -45,7 +45,7 @@ public void initDingTalkConfig() {
 
         config.put(DingTalkConstants.KEYWORD, "Dlinky-Fink 钉钉告警测试");
         config.put(DingTalkConstants.WEB_HOOK, "url");
-        config.put(DingTalkConstants.MSG_TYPE, ShowType.TABLE.getValue());
+        config.put(DingTalkConstants.MSG_TYPE, ShowType.MARKDOWN.getValue());
 
         config.put(DingTalkConstants.PROXY_ENABLE, "false");
         config.put(DingTalkConstants.PASSWORD, "password");

File: dlink-alert/dlink-alert-wechat/src/main/java/com/dlink/alert/wechat/WeChatSender.java
Patch:
@@ -182,7 +182,7 @@ private static String mkMarkDownAtUsers(List<String> userList){
 
     private String markdownByAlert(String title, String content,List<String> userList) {
         String result = "";
-        if (showType.equals(ShowType.TABLE.getValue())) {
+        if (showType.equals(ShowType.MARKDOWN.getValue())) {
             result = markdownTable(title, content,userList,sendType);
         } else if (showType.equals(ShowType.TEXT.getValue())) {
             result = markdownText(title, content,userList,sendType);

File: dlink-alert/dlink-alert-wechat/src/test/java/com/dlink/alert/wechat/WeChatSenderTest.java
Patch:
@@ -70,7 +70,7 @@ public void initWeChatConfig() {
         );
         weChatConfig.put(WeChatConstants.USERS, "all");
         weChatConfig.put(WeChatConstants.TEAM_SEND_MSG, "msg");
-        weChatConfig.put(WeChatConstants.SHOW_TYPE, ShowType.TABLE.getValue());// default is "table"
+        weChatConfig.put(WeChatConstants.SHOW_TYPE, ShowType.MARKDOWN.getValue());// default is "table"
         weChatConfig.put(WeChatConstants.SEND_TYPE, WeChatType.APP.getValue());
 
     }
@@ -96,7 +96,7 @@ public void testChatMarkDownMsg() throws IOException {
         weChatConfig.put(WeChatConstants.WEBHOOK, "https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=8xxxxxxxxxxxxxxxxx6fe13396c");
         weChatConfig.put(WeChatConstants.SEND_TYPE, WeChatType.CHAT.getValue());
         weChatConfig.put(WeChatConstants.USER_SEND_MSG,WeChatConstants.WEBHOOK_TEMPLATE);
-        weChatConfig.put(WeChatConstants.SHOW_TYPE, ShowType.TABLE.getValue());
+        weChatConfig.put(WeChatConstants.SHOW_TYPE, ShowType.MARKDOWN.getValue());
         weChatConfig.put(WeChatConstants.KEYWORD, "Dlinky企微WEBHOOK  MarkDown方式 告警测试");
         WeChatSender weChatSender = new WeChatSender(weChatConfig);
         AlertResult alertResult = weChatSender.send("TEXT-TEST", contentTest);

File: dlink-alert/dlink-alert-email/src/main/java/com/dlink/alert/email/MailSender.java
Patch:
@@ -1,9 +1,9 @@
 
 package com.dlink.alert.email;
 
+import com.dlink.alert.AlertException;
 import com.dlink.alert.AlertResult;
 import com.dlink.alert.ShowType;
-import com.dlink.alert.email.exception.AlertEmailException;
 import com.dlink.alert.email.template.AlertTemplate;
 import com.dlink.alert.email.template.DefaultHTMLTemplate;
 import com.sun.mail.smtp.SMTPProvider;
@@ -52,7 +52,7 @@ public final class MailSender {
     public MailSender(Map<String, String> config) {
         String receiversConfig = config.get(EmailConstants.NAME_PLUGIN_DEFAULT_EMAIL_RECEIVERS);
         if (receiversConfig == null || "".equals(receiversConfig)) {
-            throw new AlertEmailException(EmailConstants.NAME_PLUGIN_DEFAULT_EMAIL_RECEIVERS + mustNotNull);
+            throw new AlertException(EmailConstants.NAME_PLUGIN_DEFAULT_EMAIL_RECEIVERS + mustNotNull);
         }
 
         receivers = Arrays.asList(receiversConfig.split(","));

File: dlink-alert/dlink-alert-dingtalk/src/main/java/com/dlink/alert/dingtalk/DingTalkSender.java
Patch:
@@ -128,7 +128,7 @@ private String generateMsgJson(String title, String content) {
         items.put("msgtype", msgType);
         Map<String, Object> text = new HashMap<>();
         items.put(msgType, text);
-        if (ShowType.TABLE.getValue().equals(msgType)) {
+        if (ShowType.MARKDOWN.getValue().equals(msgType)) {
             generateMarkdownMsg(title, content, text);
         } else {
             generateTextMsg(title, content, text);

File: dlink-alert/dlink-alert-dingtalk/src/test/java/com/dlink/alert/dingtalk/DingTalkTest.java
Patch:
@@ -45,7 +45,7 @@ public void initDingTalkConfig() {
 
         config.put(DingTalkConstants.KEYWORD, "Dlinky-Fink 钉钉告警测试");
         config.put(DingTalkConstants.WEB_HOOK, "url");
-        config.put(DingTalkConstants.MSG_TYPE, ShowType.TABLE.getValue());
+        config.put(DingTalkConstants.MSG_TYPE, ShowType.MARKDOWN.getValue());
 
         config.put(DingTalkConstants.PROXY_ENABLE, "false");
         config.put(DingTalkConstants.PASSWORD, "password");

File: dlink-alert/dlink-alert-wechat/src/main/java/com/dlink/alert/wechat/WeChatSender.java
Patch:
@@ -182,7 +182,7 @@ private static String mkMarkDownAtUsers(List<String> userList){
 
     private String markdownByAlert(String title, String content,List<String> userList) {
         String result = "";
-        if (showType.equals(ShowType.TABLE.getValue())) {
+        if (showType.equals(ShowType.MARKDOWN.getValue())) {
             result = markdownTable(title, content,userList,sendType);
         } else if (showType.equals(ShowType.TEXT.getValue())) {
             result = markdownText(title, content,userList,sendType);

File: dlink-alert/dlink-alert-wechat/src/test/java/com/dlink/alert/wechat/WeChatSenderTest.java
Patch:
@@ -70,7 +70,7 @@ public void initWeChatConfig() {
         );
         weChatConfig.put(WeChatConstants.USERS, "all");
         weChatConfig.put(WeChatConstants.TEAM_SEND_MSG, "msg");
-        weChatConfig.put(WeChatConstants.SHOW_TYPE, ShowType.TABLE.getValue());// default is "table"
+        weChatConfig.put(WeChatConstants.SHOW_TYPE, ShowType.MARKDOWN.getValue());// default is "table"
         weChatConfig.put(WeChatConstants.SEND_TYPE, WeChatType.APP.getValue());
 
     }
@@ -96,7 +96,7 @@ public void testChatMarkDownMsg() throws IOException {
         weChatConfig.put(WeChatConstants.WEBHOOK, "https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=8xxxxxxxxxxxxxxxxx6fe13396c");
         weChatConfig.put(WeChatConstants.SEND_TYPE, WeChatType.CHAT.getValue());
         weChatConfig.put(WeChatConstants.USER_SEND_MSG,WeChatConstants.WEBHOOK_TEMPLATE);
-        weChatConfig.put(WeChatConstants.SHOW_TYPE, ShowType.TABLE.getValue());
+        weChatConfig.put(WeChatConstants.SHOW_TYPE, ShowType.MARKDOWN.getValue());
         weChatConfig.put(WeChatConstants.KEYWORD, "Dlinky企微WEBHOOK  MarkDown方式 告警测试");
         WeChatSender weChatSender = new WeChatSender(weChatConfig);
         AlertResult alertResult = weChatSender.send("TEXT-TEST", contentTest);

File: dlink-core/src/main/java/com/dlink/explainer/ca/CABuilder.java
Patch:
@@ -82,7 +82,7 @@ private static void buildColumnCANodeChildren(List<ColumnCANode> children, Colum
         Set<NodeRel> columnCASRel = result.getColumnCASRel();
         boolean hasChildren = false;
         for (NodeRel nodeRel : columnCASRel) {
-            if (columnId == nodeRel.getSufId()) {
+            if (columnId.equals(nodeRel.getSufId())) {
                 ColumnCA childca = (ColumnCA) result.getColumnCASMaps().get(nodeRel.getPreId());
 //                operation = operation.replaceAll(childca.getAlias().replaceAll("\\$","\\\\$"),childca.getOperation());
                 operation = operation.replaceAll(childca.getAlias()

File: dlink-core/src/main/java/com/dlink/explainer/ca/ColumnCAGenerator.java
Patch:
@@ -61,7 +61,7 @@ public void translate() {
                 }
                 for (int j = 0; j < this.columnCAS.size(); j++) {
                     ColumnCA columnCA = (ColumnCA) this.columnCAS.get(j);
-                    if (columnCA.getTableCA().getId() == tableCA.getId()) {
+                    if (columnCA.getTableCA().getId().equals(tableCA.getId())) {
                         buildColumnCAFields(tableCA, tableCA.getParentId(), columnCA);
                     }
                 }
@@ -118,7 +118,7 @@ private void paddingFields(TableCA tableCA) {
         for (int i = 0; i < this.sourceTableCAS.size(); i++) {
             if (this.sourceTableCAS.get(i) instanceof TableCA) {
                 TableCA sourceTableCA = (TableCA) this.sourceTableCAS.get(i);
-                if (sourceTableCA.getId() == tableCA.getId()) {
+                if (sourceTableCA.getId().equals(tableCA.getId())) {
                     tableCA.setFields(sourceTableCA.getFields());
                 }
             }
@@ -148,7 +148,7 @@ private void searchSelect(TableCA tableCA, ColumnCA columnCA, OperatorTrans tran
             Integer cid = null;
             for (int j = 0; j < this.columnCAS.size(); j++) {
                 ColumnCA columnCA1 = (ColumnCA) this.columnCAS.get(j);
-                if (columnCA1.getTableCA().getId() == tableCA.getId() && columnCA1.getName().equals(alias)) {
+                if (columnCA1.getTableCA().getId().equals(tableCA.getId()) && columnCA1.getName().equals(alias)) {
                     isHad = true;
                     cid = columnCA1.getId();
                     break;

File: dlink-core/src/main/java/com/dlink/explainer/ca/CABuilder.java
Patch:
@@ -82,7 +82,7 @@ private static void buildColumnCANodeChildren(List<ColumnCANode> children, Colum
         Set<NodeRel> columnCASRel = result.getColumnCASRel();
         boolean hasChildren = false;
         for (NodeRel nodeRel : columnCASRel) {
-            if (columnId == nodeRel.getSufId()) {
+            if (columnId.equals(nodeRel.getSufId())) {
                 ColumnCA childca = (ColumnCA) result.getColumnCASMaps().get(nodeRel.getPreId());
 //                operation = operation.replaceAll(childca.getAlias().replaceAll("\\$","\\\\$"),childca.getOperation());
                 operation = operation.replaceAll(childca.getAlias()

File: dlink-core/src/main/java/com/dlink/explainer/ca/ColumnCAGenerator.java
Patch:
@@ -61,7 +61,7 @@ public void translate() {
                 }
                 for (int j = 0; j < this.columnCAS.size(); j++) {
                     ColumnCA columnCA = (ColumnCA) this.columnCAS.get(j);
-                    if (columnCA.getTableCA().getId() == tableCA.getId()) {
+                    if (columnCA.getTableCA().getId().equals(tableCA.getId())) {
                         buildColumnCAFields(tableCA, tableCA.getParentId(), columnCA);
                     }
                 }
@@ -118,7 +118,7 @@ private void paddingFields(TableCA tableCA) {
         for (int i = 0; i < this.sourceTableCAS.size(); i++) {
             if (this.sourceTableCAS.get(i) instanceof TableCA) {
                 TableCA sourceTableCA = (TableCA) this.sourceTableCAS.get(i);
-                if (sourceTableCA.getId() == tableCA.getId()) {
+                if (sourceTableCA.getId().equals(tableCA.getId())) {
                     tableCA.setFields(sourceTableCA.getFields());
                 }
             }
@@ -148,7 +148,7 @@ private void searchSelect(TableCA tableCA, ColumnCA columnCA, OperatorTrans tran
             Integer cid = null;
             for (int j = 0; j < this.columnCAS.size(); j++) {
                 ColumnCA columnCA1 = (ColumnCA) this.columnCAS.get(j);
-                if (columnCA1.getTableCA().getId() == tableCA.getId() && columnCA1.getName().equals(alias)) {
+                if (columnCA1.getTableCA().getId().equals(tableCA.getId()) && columnCA1.getName().equals(alias)) {
                     isHad = true;
                     cid = columnCA1.getId();
                     break;

File: dlink-core/src/main/java/com/dlink/explainer/ca/CABuilder.java
Patch:
@@ -82,7 +82,7 @@ private static void buildColumnCANodeChildren(List<ColumnCANode> children, Colum
         Set<NodeRel> columnCASRel = result.getColumnCASRel();
         boolean hasChildren = false;
         for (NodeRel nodeRel : columnCASRel) {
-            if (columnId == nodeRel.getSufId()) {
+            if (columnId.equals(nodeRel.getSufId())) {
                 ColumnCA childca = (ColumnCA) result.getColumnCASMaps().get(nodeRel.getPreId());
 //                operation = operation.replaceAll(childca.getAlias().replaceAll("\\$","\\\\$"),childca.getOperation());
                 operation = operation.replaceAll(childca.getAlias()

File: dlink-core/src/main/java/com/dlink/explainer/ca/ColumnCAGenerator.java
Patch:
@@ -61,7 +61,7 @@ public void translate() {
                 }
                 for (int j = 0; j < this.columnCAS.size(); j++) {
                     ColumnCA columnCA = (ColumnCA) this.columnCAS.get(j);
-                    if (columnCA.getTableCA().getId() == tableCA.getId()) {
+                    if (columnCA.getTableCA().getId().equals(tableCA.getId())) {
                         buildColumnCAFields(tableCA, tableCA.getParentId(), columnCA);
                     }
                 }
@@ -118,7 +118,7 @@ private void paddingFields(TableCA tableCA) {
         for (int i = 0; i < this.sourceTableCAS.size(); i++) {
             if (this.sourceTableCAS.get(i) instanceof TableCA) {
                 TableCA sourceTableCA = (TableCA) this.sourceTableCAS.get(i);
-                if (sourceTableCA.getId() == tableCA.getId()) {
+                if (sourceTableCA.getId().equals(tableCA.getId())) {
                     tableCA.setFields(sourceTableCA.getFields());
                 }
             }
@@ -148,7 +148,7 @@ private void searchSelect(TableCA tableCA, ColumnCA columnCA, OperatorTrans tran
             Integer cid = null;
             for (int j = 0; j < this.columnCAS.size(); j++) {
                 ColumnCA columnCA1 = (ColumnCA) this.columnCAS.get(j);
-                if (columnCA1.getTableCA().getId() == tableCA.getId() && columnCA1.getName().equals(alias)) {
+                if (columnCA1.getTableCA().getId().equals(tableCA.getId()) && columnCA1.getName().equals(alias)) {
                     isHad = true;
                     cid = columnCA1.getId();
                     break;

File: dlink-admin/src/main/java/com/dlink/dto/StudioCADTO.java
Patch:
@@ -13,5 +13,6 @@
 @Setter
 public class StudioCADTO extends AbstractStatementDTO {
     // It's useless for the time being
+    private Boolean statementSet;
     private Integer type;
 }

File: dlink-admin/src/main/java/com/dlink/service/impl/JobInstanceServiceImpl.java
Patch:
@@ -114,7 +114,8 @@ public JobInfoDetail getJobInfoDetailInfo(JobInstance jobInstance) {
 
     @Override
     public LineageResult getLineage(Integer id) {
-        return LineageBuilder.getLineage(getJobInfoDetail(id).getHistory().getStatement());
+        History history = getJobInfoDetail(id).getHistory();
+        return LineageBuilder.getLineage(history.getStatement(), history.getConfig().get("useStatementSet").asBoolean());
     }
 
     @Override

File: dlink-admin/src/main/java/com/dlink/service/impl/StudioServiceImpl.java
Patch:
@@ -257,7 +257,7 @@ public List<SessionInfo> listSession(String createUser) {
     @Override
     public LineageResult getLineage(StudioCADTO studioCADTO) {
         addFlinkSQLEnv(studioCADTO);
-        return LineageBuilder.getLineage(studioCADTO.getStatement());
+        return LineageBuilder.getLineage(studioCADTO.getStatement(), studioCADTO.getStatementSet());
     }
 
     @Override

File: dlink-core/src/main/java/com/dlink/explainer/ca/TableCA.java
Patch:
@@ -67,7 +67,6 @@ public TableCA(OperatorTrans trans) {
         List<String> tableList = trans.getTable();
         this.id = trans.getId();
         this.parentId = trans.getParentId();
-        this.name = trans.getName();
         List<Field> select = trans.getSelect();
         List<String> fieldList = new ArrayList<>();
         for (Field field : select) {
@@ -94,6 +93,7 @@ public TableCA(OperatorTrans trans) {
                 this.table = strings[0];
             }
         }
+        this.name = this.catalog + "." + this.database + "." + this.table;
     }
 
     public static TableCA build(Trans trans) {

File: dlink-admin/src/main/java/com/dlink/dto/StudioCADTO.java
Patch:
@@ -13,5 +13,6 @@
 @Setter
 public class StudioCADTO extends AbstractStatementDTO {
     // It's useless for the time being
+    private Boolean statementSet;
     private Integer type;
 }

File: dlink-admin/src/main/java/com/dlink/service/impl/JobInstanceServiceImpl.java
Patch:
@@ -114,7 +114,8 @@ public JobInfoDetail getJobInfoDetailInfo(JobInstance jobInstance) {
 
     @Override
     public LineageResult getLineage(Integer id) {
-        return LineageBuilder.getLineage(getJobInfoDetail(id).getHistory().getStatement());
+        History history = getJobInfoDetail(id).getHistory();
+        return LineageBuilder.getLineage(history.getStatement(), history.getConfig().get("useStatementSet").asBoolean());
     }
 
     @Override

File: dlink-admin/src/main/java/com/dlink/service/impl/StudioServiceImpl.java
Patch:
@@ -257,7 +257,7 @@ public List<SessionInfo> listSession(String createUser) {
     @Override
     public LineageResult getLineage(StudioCADTO studioCADTO) {
         addFlinkSQLEnv(studioCADTO);
-        return LineageBuilder.getLineage(studioCADTO.getStatement());
+        return LineageBuilder.getLineage(studioCADTO.getStatement(), studioCADTO.getStatementSet());
     }
 
     @Override

File: dlink-core/src/main/java/com/dlink/explainer/ca/TableCA.java
Patch:
@@ -67,7 +67,6 @@ public TableCA(OperatorTrans trans) {
         List<String> tableList = trans.getTable();
         this.id = trans.getId();
         this.parentId = trans.getParentId();
-        this.name = trans.getName();
         List<Field> select = trans.getSelect();
         List<String> fieldList = new ArrayList<>();
         for (Field field : select) {
@@ -94,6 +93,7 @@ public TableCA(OperatorTrans trans) {
                 this.table = strings[0];
             }
         }
+        this.name = this.catalog + "." + this.database + "." + this.table;
     }
 
     public static TableCA build(Trans trans) {

File: dlink-admin/src/main/java/com/dlink/controller/UserController.java
Patch:
@@ -89,6 +89,7 @@ private static boolean checkAdmin(Integer id) {
     @PostMapping("/getOneById")
     public Result getOneById(@RequestBody User user) {
         user = userService.getById(user.getId());
+        user.setPassword(null);
         return Result.succeed(user, "获取成功");
     }
 

File: dlink-admin/src/main/java/com/dlink/service/impl/UserServiceImpl.java
Patch:
@@ -46,9 +46,6 @@ public boolean modifyUser(User user) {
         if (Asserts.isNull(user.getId())) {
             return false;
         }
-        if (Asserts.isNotNull(user.getPassword())) {
-            user.setPassword(SaSecureUtil.md5(user.getPassword()));
-        }
         return updateById(user);
     }
 

File: dlink-admin/src/main/java/com/dlink/controller/UserController.java
Patch:
@@ -89,6 +89,7 @@ private static boolean checkAdmin(Integer id) {
     @PostMapping("/getOneById")
     public Result getOneById(@RequestBody User user) {
         user = userService.getById(user.getId());
+        user.setPassword(null);
         return Result.succeed(user, "获取成功");
     }
 

File: dlink-admin/src/main/java/com/dlink/service/impl/UserServiceImpl.java
Patch:
@@ -46,9 +46,6 @@ public boolean modifyUser(User user) {
         if (Asserts.isNull(user.getId())) {
             return false;
         }
-        if (Asserts.isNotNull(user.getPassword())) {
-            user.setPassword(SaSecureUtil.md5(user.getPassword()));
-        }
         return updateById(user);
     }
 

File: dlink-alert/dlink-alert-wechat/src/main/java/com/dlink/alert/wechat/WeChatType.java
Patch:
@@ -9,7 +9,7 @@
 public enum WeChatType {
 
     APP(1, "应用"),
-    APPCHAT(2, "群聊");
+    CHAT(2, "群聊");
 
     private final int code;
     private final String value;

File: dlink-alert/dlink-alert-wechat/src/main/java/com/dlink/alert/wechat/WeChatType.java
Patch:
@@ -9,7 +9,7 @@
 public enum WeChatType {
 
     APP(1, "应用"),
-    APPCHAT(2, "群聊");
+    CHAT(2, "群聊");
 
     private final int code;
     private final String value;

File: dlink-admin/src/main/java/com/dlink/db/model/SuperEntity.java
Patch:
@@ -50,7 +50,7 @@ public class SuperEntity<T extends Model<?>> extends Model<T> implements Seriali
     private LocalDateTime updateTime;
 
     @Override
-    protected Serializable pkVal() {
+    public Serializable pkVal() {
         return this.id;
     }
 }

File: dlink-admin/src/main/java/com/dlink/db/model/SuperEntity.java
Patch:
@@ -50,7 +50,7 @@ public class SuperEntity<T extends Model<?>> extends Model<T> implements Seriali
     private LocalDateTime updateTime;
 
     @Override
-    protected Serializable pkVal() {
+    public Serializable pkVal() {
         return this.id;
     }
 }

File: dlink-core/src/main/java/com/dlink/job/JobManager.java
Patch:
@@ -505,6 +505,7 @@ public JobResult executeJar() {
             GatewayResult gatewayResult = Gateway.build(config.getGatewayConfig()).submitJar();
             job.setResult(InsertResult.success(gatewayResult.getAppId()));
             job.setJobId(gatewayResult.getAppId());
+            job.setJids(gatewayResult.getJids());
             job.setJobManagerAddress(formatAddress(gatewayResult.getWebURL()));
             job.setEndTime(LocalDateTime.now());
             job.setStatus(Job.JobStatus.SUCCESS);

File: dlink-gateway/src/main/java/com/dlink/gateway/yarn/YarnApplicationGateway.java
Patch:
@@ -73,7 +73,7 @@ public GatewayResult submitJar() {
                 applicationConfiguration);
             ClusterClient<ApplicationId> clusterClient = clusterClientProvider.getClusterClient();
             Collection<JobStatusMessage> jobStatusMessages = clusterClient.listJobs().get();
-            int counts = 10;
+            int counts = 30;
             while (jobStatusMessages.size() == 0 && counts > 0) {
                 Thread.sleep(1000);
                 counts--;
@@ -88,7 +88,6 @@ public GatewayResult submitJar() {
                     jids.add(jobStatusMessage.getJobId().toHexString());
                 }
                 result.setJids(jids);
-                logger.info("JIDS =" + StringUtils.join(jids, ","));
             }
             ApplicationId applicationId = clusterClient.getClusterId();
             result.setAppId(applicationId.toString());

File: dlink-core/src/main/java/com/dlink/job/JobManager.java
Patch:
@@ -505,6 +505,7 @@ public JobResult executeJar() {
             GatewayResult gatewayResult = Gateway.build(config.getGatewayConfig()).submitJar();
             job.setResult(InsertResult.success(gatewayResult.getAppId()));
             job.setJobId(gatewayResult.getAppId());
+            job.setJids(gatewayResult.getJids());
             job.setJobManagerAddress(formatAddress(gatewayResult.getWebURL()));
             job.setEndTime(LocalDateTime.now());
             job.setStatus(Job.JobStatus.SUCCESS);

File: dlink-gateway/src/main/java/com/dlink/gateway/yarn/YarnApplicationGateway.java
Patch:
@@ -73,7 +73,7 @@ public GatewayResult submitJar() {
                 applicationConfiguration);
             ClusterClient<ApplicationId> clusterClient = clusterClientProvider.getClusterClient();
             Collection<JobStatusMessage> jobStatusMessages = clusterClient.listJobs().get();
-            int counts = 10;
+            int counts = 30;
             while (jobStatusMessages.size() == 0 && counts > 0) {
                 Thread.sleep(1000);
                 counts--;
@@ -88,7 +88,6 @@ public GatewayResult submitJar() {
                     jids.add(jobStatusMessage.getJobId().toHexString());
                 }
                 result.setJids(jids);
-                logger.info("JIDS =" + StringUtils.join(jids, ","));
             }
             ApplicationId applicationId = clusterClient.getClusterId();
             result.setAppId(applicationId.toString());

File: dlink-connectors/dlink-connector-phoenix-1.13/src/main/java/org/apache/flink/connector/phoenix/internal/JdbcBatchingOutputFormat.java
Patch:
@@ -190,7 +190,7 @@ public synchronized void flush() throws IOException {
         for (int i = 0; i <= executionOptions.getMaxRetries(); i++) {
             try {
                 attemptFlush();
-                conn.commit();
+                //conn.commit();
                 batchCount = 0;
                 break;
             } catch (SQLException e) {
@@ -236,8 +236,8 @@ public synchronized void close() {
 
             if (batchCount > 0) {
                 try {
-                    flush();
                     LOG.info("关闭连接前 刷写数据 !!! batchCount: "+batchCount);
+                    flush();
                 } catch (Exception e) {
                     LOG.warn("Writing records to JDBC failed.", e);
                     throw new RuntimeException("Writing records to JDBC failed.", e);

File: dlink-connectors/dlink-connector-phoenix-1.13/src/main/java/org/apache/flink/connector/phoenix/internal/TableJdbcUpsertOutputFormat.java
Patch:
@@ -76,7 +76,7 @@ class TableJdbcUpsertOutputFormat
 
     @Override
     public void open(int taskNumber, int numTasks) throws IOException {
-        //super.open(taskNumber, numTasks);
+        super.open(taskNumber, numTasks);
         try {
             conn = connectionProvider.getOrEstablishConnection();
         } catch (Exception e) {

File: dlink-connectors/dlink-connector-phoenix-1.13/src/main/java/org/apache/flink/connector/phoenix/internal/executor/TableBufferedStatementExecutor.java
Patch:
@@ -38,7 +38,7 @@ public final class TableBufferedStatementExecutor implements JdbcBatchStatementE
     private final Function<RowData, RowData> valueTransform;
     private final List<RowData> buffer = new ArrayList<>();
 
-    public TableBufferedStatementExecutor(
+    public  TableBufferedStatementExecutor(
             JdbcBatchStatementExecutor<RowData> statementExecutor,
             Function<RowData, RowData> valueTransform) {
         this.statementExecutor = statementExecutor;

File: dlink-metadata/dlink-metadata-base/src/main/java/com/dlink/metadata/convert/ITypeConvert.java
Patch:
@@ -37,6 +37,7 @@ default Object convertValue(ResultSet results, String columnName, String javaTyp
             case "float":
                 return results.getFloat(columnName);
             case "bigint":
+                return results.getLong(columnName);
             case "decimal":
                 return results.getBigDecimal(columnName);
             case "date":

File: dlink-metadata/dlink-metadata-base/src/main/java/com/dlink/metadata/convert/ITypeConvert.java
Patch:
@@ -37,6 +37,7 @@ default Object convertValue(ResultSet results, String columnName, String javaTyp
             case "float":
                 return results.getFloat(columnName);
             case "bigint":
+                return results.getLong(columnName);
             case "decimal":
                 return results.getBigDecimal(columnName);
             case "date":

File: dlink-metadata/dlink-metadata-mysql/src/main/java/com/dlink/metadata/driver/MySqlDriver.java
Patch:
@@ -45,7 +45,7 @@ public String getDriverClass() {
     public Map<String, String> getFlinkColumnTypeConversion() {
         HashMap<String, String> map = new HashMap<>();
         map.put("VARCHAR", "STRING");
-        map.put("TEXY", "STRING");
+        map.put("TEXT", "STRING");
         map.put("INT", "INT");
         map.put("DATETIME", "TIMESTAMP");
         return map;

File: dlink-metadata/dlink-metadata-mysql/src/main/java/com/dlink/metadata/driver/MySqlDriver.java
Patch:
@@ -45,7 +45,7 @@ public String getDriverClass() {
     public Map<String, String> getFlinkColumnTypeConversion() {
         HashMap<String, String> map = new HashMap<>();
         map.put("VARCHAR", "STRING");
-        map.put("TEXY", "STRING");
+        map.put("TEXT", "STRING");
         map.put("INT", "INT");
         map.put("DATETIME", "TIMESTAMP");
         return map;

File: dlink-metadata/dlink-metadata-mysql/src/main/java/com/dlink/metadata/convert/MySqlTypeConvert.java
Patch:
@@ -35,6 +35,8 @@ public ColumnType convert(Column column) {
             return ColumnType.TIMESTAMP;
         } else if (t.contains("date")) {
             return ColumnType.DATE;
+        } else if (t.contains("timestamp")) {
+            return ColumnType.TIMESTAMP;
         } else if (t.contains("time")) {
             return ColumnType.TIME;
         } else if (t.contains("char") || t.contains("text")) {

File: dlink-core/src/test/java/com/dlink/core/LineageTest.java
Patch:
@@ -28,7 +28,7 @@ public void sumTest() {
                 ") WITH (\n" +
                 " 'connector' = 'print'\n" +
                 ");\n" +
-                "insert into TT select a||c A ,b B from ST";
+                "insert into TT select a||c A ,b||c B from ST";
         LineageResult result = LineageBuilder.getLineage(sql);
         System.out.println("end");
     }

File: dlink-core/src/main/java/com/dlink/config/Dialect.java
Patch:
@@ -12,7 +12,7 @@ public enum  Dialect {
 
     FLINKSQL("FlinkSql"),FLINKJAR("FlinkJar"),FLINKSQLENV("FlinkSqlEnv"),SQL("Sql"),JAVA("Java"),
     MYSQL("Mysql"),ORACLE("Oracle"),SQLSERVER("SqlServer"),POSTGRESQL("PostGreSql"),CLICKHOUSE("ClickHouse"),
-    DORIS("Doris");
+    DORIS("Doris"),PHOENIX("Phoenix");
 
     private String value;
 
@@ -42,7 +42,7 @@ public static Dialect get(String value){
     public static boolean isSql(String value){
         Dialect dialect = Dialect.get(value);
         switch (dialect){
-            case SQL:case MYSQL:case ORACLE:case SQLSERVER:case POSTGRESQL:case CLICKHOUSE:case DORIS:
+            case SQL:case MYSQL:case ORACLE:case SQLSERVER:case POSTGRESQL:case CLICKHOUSE:case DORIS: case PHOENIX:
                 return true;
             default:
                 return false;

File: dlink-metadata/dlink-metadata-phoenix/src/main/java/com/dlink/metadata/convert/PhoenixTypeConvert.java
Patch:
@@ -7,7 +7,7 @@
 public class PhoenixTypeConvert implements ITypeConvert {
     @Override
     public ColumnType convert(Column column) {
-        if (Asserts.isNull(column)) {
+        if (Asserts.isNull(column)||Asserts.isNull(column.getType())) {
             return ColumnType.STRING;
         }
         String t = column.getType().toLowerCase();

File: dlink-metadata/dlink-metadata-phoenix/src/test/java/com/dlink/metadata/PhoenixTest.java
Patch:
@@ -21,7 +21,7 @@ public void init() {
         DriverConfig config = new DriverConfig();
         config.setName("phoenix");
         config.setType("Phoenix");
-        config.setUrl("jdbc:phoenix:10.1.51.24:2181");
+        config.setUrl("jdbc:phoenix:zxbd-test-hbase:2181");
         try {
             driver = Driver.build(config);
         } catch (Exception e) {

File: dlink-core/src/main/java/com/dlink/config/Dialect.java
Patch:
@@ -12,7 +12,7 @@ public enum  Dialect {
 
     FLINKSQL("FlinkSql"),FLINKJAR("FlinkJar"),FLINKSQLENV("FlinkSqlEnv"),SQL("Sql"),JAVA("Java"),
     MYSQL("Mysql"),ORACLE("Oracle"),SQLSERVER("SqlServer"),POSTGRESQL("PostGreSql"),CLICKHOUSE("ClickHouse"),
-    DORIS("Doris");
+    DORIS("Doris"),PHOENIX("Phoenix");
 
     private String value;
 
@@ -42,7 +42,7 @@ public static Dialect get(String value){
     public static boolean isSql(String value){
         Dialect dialect = Dialect.get(value);
         switch (dialect){
-            case SQL:case MYSQL:case ORACLE:case SQLSERVER:case POSTGRESQL:case CLICKHOUSE:case DORIS:
+            case SQL:case MYSQL:case ORACLE:case SQLSERVER:case POSTGRESQL:case CLICKHOUSE:case DORIS: case PHOENIX:
                 return true;
             default:
                 return false;

File: dlink-metadata/dlink-metadata-phoenix/src/main/java/com/dlink/metadata/convert/PhoenixTypeConvert.java
Patch:
@@ -7,7 +7,7 @@
 public class PhoenixTypeConvert implements ITypeConvert {
     @Override
     public ColumnType convert(Column column) {
-        if (Asserts.isNull(column)) {
+        if (Asserts.isNull(column)||Asserts.isNull(column.getType())) {
             return ColumnType.STRING;
         }
         String t = column.getType().toLowerCase();

File: dlink-metadata/dlink-metadata-phoenix/src/test/java/com/dlink/metadata/PhoenixTest.java
Patch:
@@ -21,7 +21,7 @@ public void init() {
         DriverConfig config = new DriverConfig();
         config.setName("phoenix");
         config.setType("Phoenix");
-        config.setUrl("jdbc:phoenix:10.1.51.24:2181");
+        config.setUrl("jdbc:phoenix:zxbd-test-hbase:2181");
         try {
             driver = Driver.build(config);
         } catch (Exception e) {

File: dlink-common/src/main/java/com/dlink/model/Table.java
Patch:
@@ -129,10 +129,10 @@ public String getSqlSelect(String catalogName) {
                 sb.append(",");
             }
             String columnComment= columns.get(i).getComment();
-            if(columnComment.contains("\'") | columnComment.contains("\"")) {
-                columnComment = columnComment.replaceAll("\"|'","");
-            }
             if(Asserts.isNotNullString(columnComment)){
+                if (columnComment.contains("\'") | columnComment.contains("\"")) {
+                    columnComment = columnComment.replaceAll("\"|'", "");
+                }
                 sb.append("`"+columns.get(i).getName() + "`  --  " + columnComment + " \n");
             }else {
                 sb.append("`"+columns.get(i).getName() + "` \n");

File: dlink-metadata/dlink-metadata-base/src/main/java/com/dlink/metadata/driver/AbstractJdbcDriver.java
Patch:
@@ -208,7 +208,7 @@ public List<Column> listColumns(String schemaName, String tableName) {
                 if(columnList.contains(dbQuery.columnType())) {
                     field.setType(results.getString(dbQuery.columnType()));
                 }
-                if(columnList.contains(dbQuery.columnComment())) {
+                if(columnList.contains(dbQuery.columnComment())&& Asserts.isNotNull(results.getString(dbQuery.columnComment()))) {
                     String columnComment = results.getString(dbQuery.columnComment()).replaceAll("\"|'", "");
                     field.setComment(columnComment);
                 }

File: dlink-alert/dlink-alert-base/src/main/java/com/dlink/alert/ShowType.java
Patch:
@@ -8,7 +8,7 @@
  **/
 public enum  ShowType {
 
-    TABLE(0, "table"),
+    TABLE(0, "markdown"),
     TEXT(1, "text");
 
     private int code;

File: dlink-alert/dlink-alert-base/src/main/java/com/dlink/alert/ShowType.java
Patch:
@@ -8,7 +8,7 @@
  **/
 public enum  ShowType {
 
-    TABLE(0, "table"),
+    TABLE(0, "markdown"),
     TEXT(1, "text");
 
     private int code;

File: dlink-metadata/dlink-metadata-phoenix/src/main/java/com/dlink/metadata/convert/PhoenixTypeConvert.java
Patch:
@@ -92,7 +92,7 @@ public String convertToDB(String columnType) {
                 case "VARBINARY":
                     return "VARBINARY";
                 default:
-                    return "VARBINARY";
+                    return "VARCHAR";
             }
         }
     }

File: dlink-common/src/main/java/com/dlink/utils/LogUtil.java
Patch:
@@ -1,8 +1,7 @@
 package com.dlink.utils;
 
-
-import com.sun.org.slf4j.internal.Logger;
-import com.sun.org.slf4j.internal.LoggerFactory;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 import java.io.PrintWriter;
 import java.io.StringWriter;

File: dlink-metadata/dlink-metadata-clickhouse/src/main/java/com/dlink/metadata/driver/ClickHouseDriver.java
Patch:
@@ -15,6 +15,7 @@
 import com.dlink.metadata.query.IDBQuery;
 import com.dlink.model.Table;
 import com.dlink.result.SqlExplainResult;
+import com.dlink.utils.LogUtil;
 
 import java.sql.PreparedStatement;
 import java.sql.ResultSet;
@@ -128,8 +129,7 @@ public List<SqlExplainResult> explain(String sql){
                 sqlExplainResults.add(SqlExplainResult.success(type, current, explain.toString()));
             }
         } catch (Exception e) {
-            e.printStackTrace();
-            sqlExplainResults.add(SqlExplainResult.fail(current, e.getMessage()));
+            sqlExplainResults.add(SqlExplainResult.fail(current, LogUtil.getError(e)));
         } finally {
             close(preparedStatement, results);
             return sqlExplainResults;

File: dlink-gateway/src/main/java/com/dlink/gateway/kubernetes/KubernetesApplicationGateway.java
Patch:
@@ -6,6 +6,7 @@
 import com.dlink.gateway.exception.GatewayException;
 import com.dlink.gateway.result.GatewayResult;
 import com.dlink.gateway.result.KubernetesResult;
+import com.dlink.utils.LogUtil;
 import org.apache.flink.client.deployment.ClusterSpecification;
 import org.apache.flink.client.deployment.application.ApplicationConfiguration;
 import org.apache.flink.client.program.ClusterClient;
@@ -52,9 +53,7 @@ public GatewayResult submitJar() {
             result.setWebURL(clusterClient.getWebInterfaceURL());
             result.success();
         }catch (Exception e){
-            e.printStackTrace();
-            logger.error("任务提交时发生异常",e);
-            result.fail(e.getMessage());
+            result.fail(LogUtil.getError(e));
         }finally {
             kubernetesClusterDescriptor.close();
         }

File: dlink-gateway/src/main/java/com/dlink/gateway/yarn/YarnApplicationGateway.java
Patch:
@@ -7,6 +7,7 @@
 import com.dlink.gateway.exception.GatewayException;
 import com.dlink.gateway.result.GatewayResult;
 import com.dlink.gateway.result.YarnResult;
+import com.dlink.utils.LogUtil;
 import org.apache.flink.client.deployment.ClusterSpecification;
 import org.apache.flink.client.deployment.application.ApplicationConfiguration;
 import org.apache.flink.client.program.ClusterClient;
@@ -66,9 +67,7 @@ public GatewayResult submitJar() {
             result.setWebURL(clusterClient.getWebInterfaceURL());
             result.success();
         }catch (Exception e){
-            e.printStackTrace();
-            logger.error("任务提交时发生异常",e);
-            result.fail(e.getMessage());
+            result.fail(LogUtil.getError(e));
         }finally {
             yarnClusterDescriptor.close();
         }

File: dlink-gateway/src/main/java/com/dlink/gateway/yarn/YarnPerJobGateway.java
Patch:
@@ -6,6 +6,7 @@
 import com.dlink.gateway.exception.GatewayException;
 import com.dlink.gateway.result.GatewayResult;
 import com.dlink.gateway.result.YarnResult;
+import com.dlink.utils.LogUtil;
 import org.apache.flink.client.deployment.ClusterSpecification;
 import org.apache.flink.client.program.ClusterClient;
 import org.apache.flink.client.program.ClusterClientProvider;
@@ -51,9 +52,7 @@ public GatewayResult submitJobGraph(JobGraph jobGraph) {
             result.setWebURL(clusterClient.getWebInterfaceURL());
             result.success();
         }catch (Exception e){
-            e.printStackTrace();
-            logger.error("任务提交时发生异常",e);
-            result.fail(e.getMessage());
+            result.fail(LogUtil.getError(e));
         }finally {
             yarnClusterDescriptor.close();
         }

File: dlink-admin/src/main/java/com/dlink/job/Job2MysqlHandler.java
Patch:
@@ -3,6 +3,7 @@
 import cn.hutool.json.JSONUtil;
 import com.dlink.assertion.Asserts;
 import com.dlink.context.SpringContextUtils;
+import com.dlink.gateway.GatewayType;
 import com.dlink.model.Cluster;
 import com.dlink.model.History;
 import com.dlink.service.ClusterService;
@@ -66,7 +67,7 @@ public boolean success() {
         Job job = JobContextHolder.getJob();
         History history = new History();
         history.setId(job.getId());
-        if(Asserts.isNullString(job.getJobId())){
+        if(job.isUseGateway()&&Asserts.isNullString(job.getJobId())){
             job.setJobId("unknown-"+LocalDateTime.now().toString());
         }
         history.setJobId(job.getJobId());

File: dlink-admin/src/main/java/com/dlink/service/ClusterService.java
Patch:
@@ -32,5 +32,7 @@ public interface ClusterService extends ISuperService<Cluster> {
 
     Cluster registersCluster(Cluster cluster);
 
+    boolean enableCluster(Cluster cluster);
+
     int clearCluster();
 }

File: dlink-admin/src/main/java/com/dlink/service/impl/StudioServiceImpl.java
Patch:
@@ -64,7 +64,7 @@ public class StudioServiceImpl implements StudioService {
     private TaskService taskService;
 
     private void addFlinkSQLEnv(AbstractStatementDTO statementDTO){
-        if(Asserts.isNotNull(statementDTO.getEnvId())){
+        if(Asserts.isNotNull(statementDTO.getEnvId())&&statementDTO.getEnvId()!=0){
             Task task = taskService.getTaskInfoById(statementDTO.getEnvId());
             if(Asserts.isNotNull(task)&&Asserts.isNotNullString(task.getStatement())) {
                 statementDTO.setStatement(task.getStatement() + "\r\n" + statementDTO.getStatement());

File: dlink-admin/src/main/java/com/dlink/service/impl/TaskServiceImpl.java
Patch:
@@ -167,7 +167,7 @@ public Task getUDFByClassName(String className) {
 
     private JobConfig buildJobConfig(Task task){
         boolean isJarTask = isJarTask(task);
-        if(!isJarTask&&Asserts.isNotNull(task.getEnvId())){
+        if(!isJarTask&&Asserts.isNotNull(task.getEnvId())&&task.getEnvId()!=0){
             Task envTask = getTaskInfoById(task.getEnvId());
             if(Asserts.isNotNull(envTask)&&Asserts.isNotNullString(envTask.getStatement())) {
                 task.setStatement(envTask.getStatement() + "\r\n" + task.getStatement());

File: dlink-admin/src/main/java/com/dlink/service/impl/CatalogueServiceImpl.java
Patch:
@@ -88,6 +88,7 @@ public boolean toRename(Catalogue catalogue) {
         }else{
             Task task = new Task();
             task.setId(oldCatalogue.getTaskId());
+            task.setName(catalogue.getName());
             task.setAlias(catalogue.getName());
             taskService.updateById(task);
             this.updateById(catalogue);

File: dlink-admin/src/main/java/com/dlink/service/impl/CatalogueServiceImpl.java
Patch:
@@ -88,6 +88,7 @@ public boolean toRename(Catalogue catalogue) {
         }else{
             Task task = new Task();
             task.setId(oldCatalogue.getTaskId());
+            task.setName(catalogue.getName());
             task.setAlias(catalogue.getName());
             taskService.updateById(task);
             this.updateById(catalogue);

File: dlink-admin/src/main/java/com/dlink/service/impl/CatalogueServiceImpl.java
Patch:
@@ -88,7 +88,6 @@ public boolean toRename(Catalogue catalogue) {
         }else{
             Task task = new Task();
             task.setId(oldCatalogue.getTaskId());
-            task.setName(catalogue.getName());
             task.setAlias(catalogue.getName());
             taskService.updateById(task);
             this.updateById(catalogue);

File: dlink-admin/src/main/java/com/dlink/service/impl/CatalogueServiceImpl.java
Patch:
@@ -88,7 +88,6 @@ public boolean toRename(Catalogue catalogue) {
         }else{
             Task task = new Task();
             task.setId(oldCatalogue.getTaskId());
-            task.setName(catalogue.getName());
             task.setAlias(catalogue.getName());
             taskService.updateById(task);
             this.updateById(catalogue);

File: dlink-admin/src/main/java/com/dlink/service/impl/CatalogueServiceImpl.java
Patch:
@@ -88,6 +88,7 @@ public boolean toRename(Catalogue catalogue) {
         }else{
             Task task = new Task();
             task.setId(oldCatalogue.getTaskId());
+            task.setName(catalogue.getName());
             task.setAlias(catalogue.getName());
             taskService.updateById(task);
             this.updateById(catalogue);

File: dlink-admin/src/main/java/com/dlink/service/impl/CatalogueServiceImpl.java
Patch:
@@ -88,6 +88,7 @@ public boolean toRename(Catalogue catalogue) {
         }else{
             Task task = new Task();
             task.setId(oldCatalogue.getTaskId());
+            task.setName(catalogue.getName());
             task.setAlias(catalogue.getName());
             taskService.updateById(task);
             this.updateById(catalogue);

File: dlink-core/src/main/java/com/dlink/explainer/Explainer.java
Patch:
@@ -9,6 +9,7 @@
 import com.dlink.interceptor.FlinkInterceptor;
 import com.dlink.job.JobParam;
 import com.dlink.job.StatementParam;
+import com.dlink.model.SystemConfiguration;
 import com.dlink.parser.SqlType;
 import com.dlink.result.ExplainResult;
 import com.dlink.result.SqlExplainResult;
@@ -162,7 +163,7 @@ record = executor.explainSqlRecord(item.getValue());
                     }
                 }
                 if (inserts.size() > 0) {
-                    String sqlSet = String.join(FlinkSQLConstant.SEPARATOR, inserts);
+                    String sqlSet = String.join(";\r\n ", inserts);
                     try {
                         record.setExplain(executor.explainStatementSet(inserts));
                         record.setParseTrue(true);

File: dlink-admin/src/main/java/com/dlink/job/Job2MysqlHandler.java
Patch:
@@ -66,13 +66,15 @@ public boolean success() {
         Job job = JobContextHolder.getJob();
         History history = new History();
         history.setId(job.getId());
+        if(Asserts.isNullString(job.getJobId())){
+            job.setJobId("unknown-"+LocalDateTime.now().toString());
+        }
         history.setJobId(job.getJobId());
         history.setStatus(job.getStatus().ordinal());
         history.setEndTime(job.getEndTime());
         if(job.isUseGateway()){
             history.setJobManagerAddress(job.getJobManagerAddress());
         }
-//        history.setResult(JSONUtil.toJsonStr(job.getResult()));
         if(job.isUseGateway()){
             Cluster cluster = clusterService.registersCluster(Cluster.autoRegistersCluster(job.getJobManagerAddress(),
                     job.getJobId(),job.getJobConfig().getJobName()+ LocalDateTime.now(), job.getType().getLongValue(),

File: dlink-core/src/main/java/com/dlink/job/JobManager.java
Patch:
@@ -306,7 +306,7 @@ public JobResult executeSql(String statement) {
             LocalDateTime now = LocalDateTime.now();
             job.setEndTime(now);
             job.setStatus(Job.JobStatus.FAILED);
-            String error = now.toString() + ":" + "运行语句：\n" + currentSql + " \n时出现异常:" + e.getMessage() + " \n >>>堆栈信息<<<" + resMsg.toString();
+            String error = now.toString() + ":" + "Exception in executing FlinkSQL:\n" + currentSql + " \nError message: " + e.getMessage() + " \n >>> PrintStackTrace <<<" + resMsg.toString();
             job.setError(error);
             failed();
             close();
@@ -430,7 +430,7 @@ public JobResult executeJar() {
             LocalDateTime now = LocalDateTime.now();
             job.setEndTime(now);
             job.setStatus(Job.JobStatus.FAILED);
-            String error = now.toString() + ":" + "运行Jar：\n" + config.getGatewayConfig().getAppConfig().getUserJarPath() + " \n时出现异常:" + e.getMessage() + " \n >>>堆栈信息<<<" + resMsg.toString();
+            String error = now.toString() + ":" + "Exception in executing Jar：\n" + config.getGatewayConfig().getAppConfig().getUserJarPath() + " \nError message: " + e.getMessage() + " \n >>> PrintStackTrace <<<" + resMsg.toString();
             job.setError(error);
             failed();
             close();

File: dlink-core/src/main/java/com/dlink/job/JobManager.java
Patch:
@@ -95,8 +95,8 @@ private static void initGatewayConfig(JobConfig config){
     }
 
     public static boolean useGateway(String type){
-        return !(GatewayType.STANDALONE.equalsValue(type)||
-                GatewayType.YARN_SESSION.equalsValue(type));
+        return (GatewayType.YARN_PER_JOB.equalsValue(type)||
+                GatewayType.YARN_APPLICATION.equalsValue(type));
     }
 
     private Executor createExecutor() {

File: dlink-admin/src/main/java/com/dlink/model/Task.java
Patch:
@@ -40,6 +40,9 @@ public class Task extends SuperEntity{
     @TableField(exist = false)
     private String statement;
 
+    @TableField(exist = false)
+    private String clusterName;
+
     public ExecutorSetting getLocalExecutorSetting(){
         return new ExecutorSetting(Executor.LOCAL,checkPoint,parallelism,fragment,savePointPath,alias);
     }

File: dlink-admin/src/main/java/com/dlink/controller/CatalogueController.java
Patch:
@@ -98,8 +98,9 @@ public Result getCatalogueTreeData() throws Exception {
      */
     @PutMapping("/createTask")
     public Result createTask(@RequestBody CatalogueTaskDTO catalogueTaskDTO) throws Exception {
-        if(catalogueService.createCatalogueAndTask(catalogueTaskDTO)){
-            return Result.succeed("创建成功");
+        Catalogue catalogue = catalogueService.createCatalogueAndTask(catalogueTaskDTO);
+        if(catalogue.getId()!=null){
+            return Result.succeed(catalogue,"创建成功");
         }else {
             return Result.failed("创建失败");
         }

File: dlink-admin/src/main/java/com/dlink/service/CatalogueService.java
Patch:
@@ -16,7 +16,7 @@ public interface CatalogueService extends ISuperService<Catalogue> {
 
     List<Catalogue> getAllData();
 
-    boolean createCatalogueAndTask(CatalogueTaskDTO catalogueTaskDTO);
+    Catalogue createCatalogueAndTask(CatalogueTaskDTO catalogueTaskDTO);
 
     boolean toRename(Catalogue catalogue);
 

File: dlink-admin/src/main/java/com/dlink/service/impl/CatalogueServiceImpl.java
Patch:
@@ -36,7 +36,7 @@ public List<Catalogue> getAllData() {
 
     @Transactional(rollbackFor=Exception.class)
     @Override
-    public boolean createCatalogueAndTask(CatalogueTaskDTO catalogueTaskDTO) {
+    public Catalogue createCatalogueAndTask(CatalogueTaskDTO catalogueTaskDTO) {
         Task task = new Task();
         task.setName(catalogueTaskDTO.getName());
         task.setAlias(catalogueTaskDTO.getAlias());
@@ -46,7 +46,8 @@ public boolean createCatalogueAndTask(CatalogueTaskDTO catalogueTaskDTO) {
         catalogue.setIsLeaf(true);
         catalogue.setTaskId(task.getId());
         catalogue.setParentId(catalogueTaskDTO.getParentId());
-        return this.save(catalogue);
+        this.save(catalogue);
+        return catalogue;
     }
 
     @Transactional(rollbackFor=Exception.class)

